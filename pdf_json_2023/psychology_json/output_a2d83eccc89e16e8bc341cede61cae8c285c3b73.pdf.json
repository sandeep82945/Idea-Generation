{
    "abstractText": "Molecular property prediction is a critical task in computational drug discovery. While recent advances in Graph Neural Networks (GNNs) and Transformers have shown to be effective and promising, they face the following limitations: Transformer self-attention does not explicitly consider the underlying molecule structure while GNN feature representation alone is not sufficient to capture granular and hidden interactions and characteristics that distinguish similar molecules. To address these limitations, we propose SYNFUSION, a novel approach that synergistically combines pre-trained features from GNNs and Transformers. This approach provides a comprehensive molecular representation, capturing both the global molecule structure and the individual atom characteristics. Experimental results on MoleculeNet benchmarks demonstrate superior performance, surpassing previous models in 5 out of 7 classification datasets and 4 out of 6 regression datasets. The performance of SYN-FUSION has been compared with other Graph-Transformer models that have been jointly trained using a combination of transformer and graph features, and it is found that our approach is on par with those models in terms of performance. Extensive analysis of the learned fusion model across aspects such as loss, latent space, and weight distribution further validates the effectiveness of SYNFUSION. Finally, an ablation study unequivocally demonstrates that the synergy achieved by SYN-FUSION surpasses the performance of its individual model components and their ensemble, offering a substantial improvement in predicting molecular properties.",
    "authors": [
        {
            "affiliations": [],
            "name": "M V Sai Prakash"
        },
        {
            "affiliations": [],
            "name": "Siddartha Reddy"
        },
        {
            "affiliations": [],
            "name": "Ganesh Parab"
        },
        {
            "affiliations": [],
            "name": "Vishal Vaddina"
        },
        {
            "affiliations": [],
            "name": "Saisubramaniam Gopalakrishnan"
        }
    ],
    "id": "SP:689c6cace5b66d0d0a4260b8261e57db1935d4af",
    "references": [
        {
            "authors": [
                "G W Bemis",
                "M A Murcko"
            ],
            "title": "1996",
            "venue": "The properties of known drugs. 1. Molecular frameworks. J. Med. Chem. 39, 15 ",
            "year": 1996
        },
        {
            "authors": [
                "JAMES BLAKE"
            ],
            "title": "1886",
            "venue": "On the Connection between Chemical Constitution, and Physiological Action. Nature 34, 886 ",
            "year": 1886
        },
        {
            "authors": [
                "Nicola De Cao",
                "Thomas Kipf"
            ],
            "title": "MolGAN: An implicit generative model for small molecular graphs",
            "year": 2022
        },
        {
            "authors": [
                "An Chen",
                "Xu Zhang",
                "Zhen Zhou"
            ],
            "title": "Machine learning: Accelerating materials development for energy storage and conversion",
            "venue": "InfoMat 2,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Seyone Chithrananda",
                "Gabriel Grand",
                "Bharath Ramsundar"
            ],
            "title": "Chem- BERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction",
            "venue": "CoRR abs/2010.09885 (2020)",
            "year": 2020
        },
        {
            "authors": [
                "John Dearden"
            ],
            "title": "The History and Development of Quantitative Structure- Activity Relationships (QSARs)",
            "venue": "International Journal of Quantitative Structure- Property Relationships",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "In Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Carl Doersch",
                "Abhinav Gupta",
                "Alexei A. Efros"
            ],
            "title": "Unsupervised Visual Representation Learning by Context Prediction",
            "venue": "IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaomin Fang",
                "Lihang Liu",
                "Jieqiong Lei",
                "Donglong He",
                "Shanzhuo Zhang",
                "Jingbo Zhou",
                "Fan Wang",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "Geometry-enhanced molecular representation learning for property prediction",
            "venue": "Nature Machine Intelligence 4,",
            "year": 2022
        },
        {
            "authors": [
                "Evan N. Feinberg",
                "Debnil Sur",
                "Zhenqin Wu",
                "Brooke E. Husic",
                "Huanghao Mai",
                "Yang Li",
                "Saisai Sun",
                "Jianyi Yang",
                "Bharath Ramsundar",
                "Vijay S. Pande"
            ],
            "title": "PotentialNet for Molecular Property Prediction",
            "venue": "ACS Central Science 4,",
            "year": 2018
        },
        {
            "authors": [
                "Spyros Gidaris",
                "Praveer Singh",
                "Nikos Komodakis"
            ],
            "title": "Unsupervised Representation Learning by Predicting Image Rotations",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S. Schoenholz",
                "Patrick F. Riley",
                "Oriol Vinyals",
                "George E. Dahl"
            ],
            "title": "Neural Message Passing for Quantum Chemistry",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Oriol Vinyals"
            ],
            "title": "Qualitatively characterizing neural network optimization problems",
            "venue": "In 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar",
                "Bilal Piot",
                "koray kavukcuoglu",
                "Remi Munos",
                "Michal Valko"
            ],
            "title": "Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Weihua Hu",
                "Bowen Liu",
                "Joseph Gomes",
                "Marinka Zitnik",
                "Percy Liang",
                "Vijay S. Pande",
                "Jure Leskovec"
            ],
            "title": "Strategies for Pre-training Graph Neural Networks",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ross Irwin",
                "Spyridon Dimitriadis",
                "Jiazhen He",
                "Esben Jannik Bjerrum"
            ],
            "title": "Chemformer: a pre-trained transformer for computational chemistry",
            "venue": "Machine Learning: Science and Technology 3,",
            "year": 2022
        },
        {
            "authors": [
                "Yinghui Jiang",
                "Shuting Jin",
                "Xurui Jin",
                "Xianglu Xiao",
                "Wenfan Wu",
                "Xiangrong Liu",
                "Qiang Zhang",
                "Xiangxiang Zeng",
                "Guang Yang",
                "Zhangming Niu"
            ],
            "title": "Pharmacophoric-constrained heterogeneous graph transformer model for molecular property prediction",
            "venue": "Communications Chemistry",
            "year": 2023
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "year": 2017
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Mario Krenn",
                "Qianxiang Ai",
                "Senja Barthel",
                "Nessa Carson",
                "Angelo Frei",
                "Nathan C. Frey",
                "Pascal Friederich",
                "Th\u00e9ophile Gaudin",
                "Alberto Alexander Gayle",
                "Kevin Maik Jablonka",
                "Rafael F. Lameiro",
                "Dominik Lemm",
                "Alston Lo",
                "SeyedMohamadMoosavi",
                "Jos\u00e9 Manuel N\u00e1poles-Duarte",
                "AkshatKumar Nigam",
                "Robert Pollice",
                "Kohulan Rajan",
                "Ulrich Schatzschneider",
                "Philippe Schwaller",
                "Marta Skreta",
                "Berend Smit",
                "Felix Strieth-Kalthoff",
                "Chong Sun",
                "Gary Tom",
                "Guido Falk von Rudorff",
                "Andrew Wang",
                "Andrew D. White",
                "Adamo Young",
                "Rose Yu",
                "Al\u00e1n Aspuru-Guzik"
            ],
            "title": "SELFIES and the future of molecular string representations",
            "venue": "Patterns 3,",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein"
            ],
            "title": "Visualizing the Loss Landscape of Neural Nets",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Juncai Li",
                "Xiaofei Jiang"
            ],
            "title": "Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction",
            "venue": "Wireless Communications and Mobile Computing",
            "year": 2021
        },
        {
            "authors": [
                "Chengqiang Lu",
                "Qi Liu",
                "ChaoWang",
                "Zhenya Huang",
                "Peize Lin",
                "Lixin He"
            ],
            "title": "Molecular Property Prediction: A Multilevel Quantum Interactions Modeling Perspective",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence 33,",
            "year": 2019
        },
        {
            "authors": [
                "Kelong Mao",
                "Peilin Zhao",
                "Tingyang Xu",
                "Yu Rong",
                "Xi Xiao",
                "Junzhou Huang"
            ],
            "title": "Molecular Graph Enhanced Transformer for Retrosynthesis Prediction",
            "year": 2020
        },
        {
            "authors": [
                "H.L. Morgan"
            ],
            "title": "1965",
            "venue": "The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service. Journal of Chemical Documentation 5, 2 ",
            "year": 1965
        },
        {
            "authors": [
                "Fran\u00e7ois Mouvet",
                "Justin Villard",
                "Viacheslav Bolnykh",
                "Ursula Rothlisberger"
            ],
            "title": "Recent Advances in First-Principles Based Molecular Dynamics",
            "venue": "Accounts of Chemical Research 55,",
            "year": 2022
        },
        {
            "authors": [
                "Jo\u00e3o C.A. Oliveira",
                "Johanna Frey",
                "Shuo-Qing Zhang",
                "Li-Cheng Xu",
                "Xin Li",
                "Shu- Wen Li",
                "Xin Hong",
                "Lutz Ackermann"
            ],
            "title": "When machine learning meets molecular synthesis",
            "venue": "Trends in Chemistry 4,",
            "year": 2022
        },
        {
            "authors": [
                "Kenley M. Pelzer",
                "Lei Cheng",
                "Larry A. Curtiss"
            ],
            "title": "Effects of Functional Groups in Redox-Active Organic Molecules: A High-Throughput Screening Approach",
            "venue": "The Journal of Physical Chemistry C 121,",
            "year": 2016
        },
        {
            "authors": [
                "Mike Renier"
            ],
            "title": "2022. history and philosophy of computational chemistry",
            "year": 2022
        },
        {
            "authors": [
                "Kyle R. Roell",
                "David M. Reif",
                "Alison A. Motsinger-Reif"
            ],
            "title": "An Introduction to Terminology and Methodology of Chemical Synergy\u2014Perspectives from Across Disciplines",
            "venue": "Frontiers in Pharmacology",
            "year": 2017
        },
        {
            "authors": [
                "David Rogers",
                "Mathew Hahn"
            ],
            "title": "Extended-Connectivity Fingerprints",
            "venue": "Journal of Chemical Information and Modeling 50,",
            "year": 2010
        },
        {
            "authors": [
                "Yu Rong",
                "Yatao Bian",
                "Tingyang Xu",
                "Weiyang Xie",
                "YingWEI",
                "Wenbing Huang",
                "Junzhou Huang"
            ],
            "title": "Self-Supervised Graph Transformer on Large-Scale Molecular Data",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Rong",
                "Yatao Bian",
                "Tingyang Xu",
                "Weiyang Xie",
                "Ying Wei",
                "Wenbing Huang",
                "Junzhou Huang"
            ],
            "title": "Self-Supervised Graph Transformer on Large-Scale Molecular Data. In Advances in Neural Information Processing Systems",
            "venue": "Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Jie Shen",
                "Christos Nicolaou"
            ],
            "title": "Molecular property prediction: recent trends in the era of artificial intelligence. Drug Discovery Today: Technologies",
            "year": 2020
        },
        {
            "authors": [
                "Jessica Vamathevan",
                "Dominic Clark",
                "Paul Czodrowski",
                "Ian Dunham",
                "Edgardo Ferran",
                "George Lee",
                "Bin Li",
                "Anant Madabhushi",
                "Parantu Shah",
                "Michaela Spitzer",
                "Shanrong Zhao"
            ],
            "title": "Applications of machine learning in drug discovery and development",
            "venue": "Nat. Rev. Drug Discov. 18,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation Learning with Contrastive Predictive Coding",
            "year": 2019
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing Data using t-SNE",
            "venue": "Journal of Machine Learning Research 9,",
            "year": 2008
        },
        {
            "authors": [
                "Sheng Wang",
                "Yuzhi Guo",
                "Yuhong Wang",
                "Hongmao Sun",
                "Junzhou Huang"
            ],
            "title": "SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction",
            "venue": "In Proceedings of the 10th ACM International Conference on Bioinformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Xiao Wang",
                "Sean Flannery",
                "Daisuke Kihara"
            ],
            "title": "Protein Docking Model Evaluation by Graph Neural Networks",
            "venue": "Frontiers in Molecular Biosciences",
            "year": 2021
        },
        {
            "authors": [
                "Yuyang Wang",
                "Jianren Wang",
                "Zhonglin Cao",
                "Amir Barati Farimani"
            ],
            "title": "Molecular contrastive learning of representations via graph neural networks",
            "venue": "Nature Machine Intelligence 4,",
            "year": 2022
        },
        {
            "authors": [
                "David Weininger"
            ],
            "title": "SMILES, a chemical language and information system. 1. Introduction tomethodology and encoding rules",
            "venue": "Journal of Chemical Information and Computer Sciences",
            "year": 1988
        },
        {
            "authors": [
                "R Guy Woolley"
            ],
            "title": "1998",
            "venue": "Is there a quantum definition of a molecule? Journal of Mathematical Chemistry 23 ",
            "year": 1998
        },
        {
            "authors": [
                "Zhenqin Wu",
                "Bharath Ramsundar",
                "Evan N. Feinberg",
                "Joseph Gomes",
                "Caleb Geniesse",
                "Aneesh S. Pappu",
                "Karl Leswing",
                "Vijay Pande"
            ],
            "title": "MoleculeNet: a benchmark for molecular machine learning",
            "venue": "Chem. Sci",
            "year": 2018
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How Powerful are Graph Neural Networks",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Dongyu Xue",
                "Han Zhang",
                "Xiaohan Chen",
                "Dongling Xiao",
                "Yukang Gong",
                "Guohui Chuai",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Yu-Kun Li",
                "qi Liu"
            ],
            "title": "X-MOL: largescale pre-training for molecular understanding and diverse molecular analysis",
            "venue": "Science Bulletin",
            "year": 2022
        },
        {
            "authors": [
                "Dongyu Xue",
                "Han Zhang",
                "Dongling Xiao",
                "Yukang Gong",
                "Guohui Chuai",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Yukun Li",
                "Qi Liu"
            ],
            "title": "X-MOL: large-scale pre-training for molecular understanding and diverse molecular analysis",
            "venue": "bioRxiv (2021)",
            "year": 2021
        },
        {
            "authors": [
                "Kevin Yang",
                "Kyle Swanson",
                "Wengong Jin",
                "Connor Coley",
                "Philipp Eiden",
                "Hua Gao",
                "Angel Guzman-Perez",
                "Timothy Hopper",
                "Brian Kelley",
                "Miriam Mathea",
                "Andrew Palmer",
                "Volker Settels",
                "Tommi Jaakkola",
                "Klavs Jensen",
                "Regina Barzilay"
            ],
            "title": "Analyzing Learned Molecular Representations for Property Prediction",
            "venue": "Journal of Chemical Information and Modeling 59,",
            "year": 2019
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do Transformers Really Perform Badly for Graph Representation",
            "venue": "In Thirty-Fifth Conference on Neural Information Processing Systems. https://openreview.net/forum?id=OeWooOxFwDa",
            "year": 2021
        },
        {
            "authors": [
                "Xuan Zang",
                "Xianbing Zhao",
                "Buzhou Tang"
            ],
            "title": "Hierarchical Molecular Graph Self-Supervised Learning for property prediction",
            "venue": "Communications Chemistry",
            "year": 2023
        },
        {
            "authors": [
                "Xiao-Chen Zhang",
                "Cheng-Kun Wu",
                "Zhi-Jiang Yang",
                "Zhen-Xing Wu",
                "Jia- Cai Yi",
                "Chang-Yu Hsieh",
                "Ting-Jun Hou",
                "Dong-Sheng Cao"
            ],
            "title": "MG- BERT: leveraging unsupervised atomic representation learning for molecular property prediction",
            "venue": "Briefings in Bioinformatics 22,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Applied computing \u2192 Molecular sequence analysis; Bioinformatics; \u2022 Computing methodologies\u2192 Artificial intelligence; Transfer learning; Neural networks.\nKEYWORDS Graph Neural Networks, Transformer, Molecular Representation Learning, Molecular Property Prediction, Synergy\n\u2217Corresponding author 0Accepted as poster at ACM-BCB 2023 and as workshop talk at CNB-MAC \u201923."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Molecular property prediction [38] has rapidly evolved into an interdisciplinary field that leverages insights from chemistry, physics, and materials science. Predicting molecular properties is widely considered one of the most critical tasks in computational drug discovery. The ability to accurately predict molecular properties enables transformative applications in drug design, materials development, and reaction optimization [4, 11, 30]. Early approaches in molecular property prediction include quantum mechanics-based mathematical models describing atomic and molecular behavior [46], computational chemistry involving the study of chemical systems through computer simulations [32], and molecular mechanics and molecular dynamics [29] involving the simulation of larger and more complex molecules.\nWith the advent of computational methods, fingerprint techniques such as Morgan Fingerprint [28], Extended-Connectivity FingerPrints (ECFP) [34] have been developed for efficient molecular representation. Statistical methods andmachine learning models like Quantitative Structure-Property Relationships (QSPR) [2] and Quantitative Structure-Activity Relationships (QSAR) [7] predict properties based on the molecular structure using these techniques. By utilizing large datasets to learn complex structure-property relationships [22, 39], deep learning approaches surpass traditional methods across various molecular tasks. Two such widely adopted approaches include modeling the molecule as (i) a sequence of atoms using either Simplified Molecular Input Line Entry System (SMILES) [45] or SELF-referencing Embedded Strings (SELFIES) [21] and (ii) a graph-based structure using Graph Neural Networks (GNN) [37].\nSequence-based molecular property prediction has witnessed significant growth in recent years [6, 42, 54]. This approach leverages the inherent sequential nature of molecular structures and protein sequences to accurately predict properties, opening new possibilities in the realm of drug discovery and materials science. Graph neural networks (GNNs) model the natural representation of molecules as graphs and use neighborhood aggregation strategies to predict molecular properties such as solubility, toxicity, binding affinity, etc. [13, 51] Related tasks, including molecular generation\n[3], reaction prediction [27], and molecular docking [43] are other applications of GNNs.\nA pivotal challenge in property prediction is the limited availability of labeled data, an obstacle shared across different fields such as language and vision [9, 12]. The success of self-supervised learning in image and text domains [8, 15] has also been extended to molecular property prediction [53]. Contrastive learning [5] has been shown to be effective in learning better latent representations by pre-training a model to maximize the distance between positive and negative pairs from unlabeled data samples and learning downstream tasks with limited data [16, 35]. Masked Language Modelling (MLM) [8, 23] has been adopted as a pre-training strategy in sequence-to-sequence and discriminative cheminformatics tasks [17]. These approaches have proven beneficial for models to leverage the knowledge learned from larger datasets when fine-tuning smaller, focused tasks.\nBoth sequence-based transformers and graph-basedmodels learn richer representations when they are initially pre-trained on large datasets of molecules in a self-supervised manner, followed by supervised fine-tuning on smaller datasets with specific properties of interest. This work investigates the benefits of fusing the pretrained latent representations from both approaches and fine-tuning them towards the downstream task of property prediction.\nThe contributions of this work are as follows: (i) Learning the synergistic interaction between pre-trained features from graphs and transformers to create a more comprehensive molecular representation that captures both the global molecule structure and the characteristics of individual atoms, (ii) Conducting a detailed analysis of the learned synergistic fusion representation in various aspects, including loss, latent space, activation, and weights, through classification and regression case studies, and, (iii) an ablation study to showcase that the synergy effect of fusion is greater than the performances of the individual models and their ensemble."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Molecular fingerprinting methods commonly used in cheminformatics and computational chemistry such as Extended-Connectivity FingerPrints (ECFP) [34] encode the structural features of molecules into fingerprint representations for similarity-based analyses and machine learning tasks. ECFP generates a binary fingerprint for each molecule based on the structure, taking into account the connectivity of atoms and the presence of chemical groups. ECFP is fast but limited in its ability to capture the diversity of molecular structures, as it only considers the presence (1) or absence (0) of specific sub-structures within a molecule.\nThe Simplified Molecular Input Line Entry System (SMILES) [45] is a linear representation designed to encode molecular structure in a machine-readable format. SMILES uses a distinctive set of characters to represent atoms, bonds, and functional groups within a molecule. Due to its machine-interpretability, SMILES has become a key molecular descriptor for training machine learning models. Advanced machine learning algorithms, including deep neural networks, can utilize SMILES strings to learn predictive models for various molecular properties. Transformer-based models [6, 42, 54] have been employed formolecular property prediction, primarily relying on SMILES representations. Transformers utilize\nself-attention mechanisms that enable them to attend to different parts of the molecule and consider the relationships between atoms and bonds. However, while models such as Chemformer [17] and X-MOL [50], trained on SMILES data, have exhibited promising outcomes in classification and regression tasks, they have limitations in providing comprehensive insights into the underlying molecular structure, particularly in modeling the intricate connectivity patterns and spatial arrangements found in molecular graphs.\nGraph Neural Networks (GNNs) have demonstrated significant potential in predicting molecular properties, as evidenced by previous studies [13, 19, 48]. In GNNs, molecules are represented as graphs, with atoms serving as nodes and bonds as edges. These networks leverage the graph structure to learn representations of the molecules. A widely adopted GNN-based approach is the Message Passing Neural Network (MPNN) [13]. MPNN utilizes a recursive message-passing mechanism to propagate information throughout the molecular graph structure. Another approach is the Graph Convolutional Network (GCN) [20], which uses graph convolution operations to learn node embeddings. GIN [48] handles graph isomorphism (the similarity between two graphs despite differences in node labels or orderings) by employing an aggregation function that is independent of the ordering of nodes or edges.\nMolecules with similar overall structures can exhibit distinct functional groups or subtle variations that significantly impact their properties [31]. Self-supervised learning has gained significant attention in the field of molecular property prediction, enabling models to learn meaningful representations from unlabeled molecular data. Hu et al [16] introduces innovative strategies for molecule graphs, encompassing pre-training at both the node and graph levels. Contrastive learning [5, 40] is a machine learning technique that learns data representations by comparing pairs of instances. Instances that are similar are pulled closer together, while instances that are dissimilar are pushed further apart. In molecular property prediction, contrastive learning-based methods involve encoding molecular structures as feature vectors and comparing these vectors using a contrastive loss function. An illustration of this is MolCLR [44], which employs a contrastive loss function to learn representations of molecular structures that can subsequently be utilized for predicting molecular properties. MegaMolBART utilizes a transformer architecture based on BART [23] and is trained for small molecule drug discovery. It comprises a bidirectional encoder and an autoregressive decoder. The pretraining of MegaMolBART is built upon the foundation of Chemformer [17] and uses Masked Language Modelling and augmentation of SMILES input. Recent efforts have focused on integrating graph information into Transformer models. GROVER [36] uses a graph multi-head attention network with node vectors obtained through a specialized Graph Neural Network (GNN). Graphormer [52] is based on the standard Transformer, taking graph-structured encoded data directly as input and avoiding conversion to sequential formats. PharmHGT [18] utilizes various perspectives within the molecular graph for message passing, yielding distinct atom features, followed by attention aggregation for holistic molecule representation.\nTransformers excel in learning complex relationships and hidden dependencies across the entire molecule, including interactions between atoms and bonds. However, they do not explicitly\nconsider the underlying graph structure of the molecule. Alternatively, Graph Neural Networks (GNNs) offer a more appropriate framework for capturing the unique structural characteristics of molecules and have the potential to benefit by integrating transformer representations acquired through self-attention, effectively capturing long-range dependencies. Therefore, in this work, the pretrained features from graphs and transformers have been combined, and the synergistic interaction between the two representations has been learned through fusion. The result is a more comprehensive molecular representation that effectively captures the global structure of molecules and the specific characteristics of individual atoms. Additionally, a detailed analysis of the learned synergistic fusion representation has been conducted through case studies involving classification and regression tasks. To the best of our knowledge, previous works have not explored using a combination of pre-trained features and the resulting synergistic interaction for molecular property prediction."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "We propose a novel approach to learning the synergistic interaction between pre-trained features from graphs and transformers, termed Synergistic Fusion (SYN-FUSION). The overall framework is illustrated in Figure 1. The approach comprises two steps: In the first step, a single molecule is represented as a Graph and a\nSMILES string independently, and then encoded into two feature representations using a Graph Neural Network such as GIN and a Transformer based network such as MegaMolBART, respectively. In the second step, the two distinct features are concatenated and fused using a linear layer that learns the combined \u2019synergistic\u2019 information for enhanced downstream property prediction."
        },
        {
            "heading": "3.1 Graph Isomorphism Network",
            "text": "Graph Isomorphism Network (GIN) [48] learns a permutationinvariant representation of each input graph, achieved by applying a series of message-passing operations to the nodes and edges of the graph. Let G = (V, E) denote an undirected graph. Let V be the node feature matrix of the nodes in V and E be the edge feature matrix of the edges in E. The message-passing operation computes a new feature vector h for each node in V:\nh(\ud835\udc58+1) \ud835\udc56 = MLP(\ud835\udc58+1)\ud835\udc4e\ud835\udc61\ud835\udc5c\ud835\udc5a ( (1 + \ud835\udf16 (\ud835\udc58 ) ) h(\ud835\udc58 ) \ud835\udc56\n+ \u2211\ufe01\n\ud835\udc57\u2208N(\ud835\udc56 )\n( h(\ud835\udc58 ) \ud835\udc57 +MLP(\ud835\udc58+1) \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51 (e\ud835\udc56 \ud835\udc57 ) )\u00aa\u00ae\u00ac (1)\nwhere h0 = \ud835\udc49 , e\ud835\udc56 \ud835\udc57 is the edge feature vector of edge \ud835\udc52 \u2208 E connecting atoms \ud835\udc56 and \ud835\udc57 , \ud835\udc58 represents the \ud835\udc58\ud835\udc61\u210e layer, \ud835\udc40\ud835\udc3f\ud835\udc43 stands\nfor multi layered perception,\ud835\udc40\ud835\udc3f\ud835\udc43 (\ud835\udc58+1)\ud835\udc4e\ud835\udc61\ud835\udc5c\ud835\udc5a and\ud835\udc40\ud835\udc3f\ud835\udc43 (\ud835\udc58+1) \ud835\udc4f\ud835\udc5c\ud835\udc5b\ud835\udc51\nbond are the (\ud835\udc58 + 1)-th MLP layers on the atom- and bond-level respectively, N(\ud835\udc56) is the set of neighboring nodes of \ud835\udc56 , and \ud835\udf16 (\ud835\udc58 ) is a learnable parameter that helps to avoid over-smoothing of the features.\nThe pooling operation aggregates the feature vectors of all nodes in the graph into a single vector:\npooling (H) = mean (H) (2)\nwhere H = [h(\ud835\udc58\ud835\udc5b)1 , h (\ud835\udc58\ud835\udc5b) 2 , ..., h (\ud835\udc58\ud835\udc5b) \ud835\udc41\n] is the matrix of node features at the final layer kn, N is the number of nodes in G and mean(\u00b7) is the element-wise mean operator.\nThe final output of the GIN network is obtained by passing the pooled feature vector through a linear layer:\nzGIN = linear (pooling (H)) (3) where zGIN is the latent representation for the input graph G, and linear is a multi-layer perceptron.\nBy stacking multiple layers of message passing and pooling operations, GIN is able to learn a hierarchical representation of the input graph that is invariant to node ordering and is capable of capturing complex structural patterns."
        },
        {
            "heading": "3.2 Transformer - MegaMolBART",
            "text": "The Chemformer [17] paper proposes a pre-training method for molecular property prediction based on the Bidirectional and AutoRegressive Transformers (BART) [23] architecture. MegaMolBART employs an identical configuration for pre-training i.e. Masked Language Modeling (MLM). The goal of MLM is to predict the masked tokens based on the context provided by the other tokens in the sequence. The pre-training process commences by transforming each molecule in the batch into a non-canonical SMILES representation that aligns with the specific molecule. The SMILES strings are subsequently subjected to random masking, tokenization, and embedding into a vector sequence. This modified sequence is then fed into the bidirectional encoder, while the autoregressive decoder is tasked with predicting the initial SMILES sequence based on the same right-shifted sequence. A fully-connected layer is employed to process the decoder\u2019s output, generating a distribution across the model\u2019s vocabulary. To obtain the latent feature from MegaMolBART, only the encoder component is required.\nzMMB = \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc40\ud835\udc40\ud835\udc35 (\ud835\udc65) (4)\nwhere the bidirectional encoder of MegaMolBART, \ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc40\ud835\udc40\ud835\udc35 takes a SMILES representation of the molecule \ud835\udc65 as input and provides the corresponding latent representation zMMB."
        },
        {
            "heading": "3.3 Synergistic Fusion",
            "text": "Synergy refers to the phenomenonwhere the combined effect of two ormore substances is greater than the sum of their individual effects. It occurs when the interaction between the substances enhances or amplifies their overall impact [33]. Let us consider two substances, denoted as \ud835\udc34 and \ud835\udc35, which each possess distinct effects represented by variables \ud835\udc4b and \ud835\udc4c , respectively. The combined effect resulting from the interaction of \ud835\udc34 and \ud835\udc35 can be represented as \ud835\udc4d . If there exists synergy between substances \ud835\udc34 and \ud835\udc35, it can be expressed through the equation \ud835\udc4d > \ud835\udc4b + \ud835\udc4c . This equation denotes that the\ncombined effect (\ud835\udc4d ) surpasses the summation of the individual effects (\ud835\udc4b + \ud835\udc4c ), thereby indicating the presence of a synergistic interaction.\nLet us define the latent embeddings fromMegaMolBART encoder as zMMB and the latent embeddings from GIN as zGIN.\n3.3.1 Classification. For classification tasks using the cross-entropy loss, the objective function can be written as:\nL\ud835\udc50 = \u2212 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56=1 \ud835\udc36\u2211\ufe01 \ud835\udc57=1 \ud835\udc66\ud835\udc56 \ud835\udc57 log(\ud835\udc66\ud835\udc56 \ud835\udc57 ) (5)\nwhere \ud835\udc41 is the number of samples, \ud835\udc36 is the number of classes, \ud835\udc66\ud835\udc56 \ud835\udc57 is the true label for sample \ud835\udc56 and class \ud835\udc57 , and \ud835\udc66\ud835\udc56 \ud835\udc57 is the predicted probability of sample \ud835\udc56 belonging to class \ud835\udc57 .\nWe can now express the predicted probabilities as a function of the feature embeddings:\n\ud835\udc66\ud835\udc56 \ud835\udc57 = softmax ( W [ z(\ud835\udc56 )GIN, z (\ud835\udc56 ) MMB ] + b ) \ud835\udc56 \ud835\udc57\n(6)\nwhereW and b are the weight matrix and bias vector of the linear layer, [\u00b7, \u00b7] denotes concatenation, and softmax(\u00b7) is the softmax function\nFinally, we can write the objective function for classification as:\nL\ud835\udc50 = \u2212 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56=1 \ud835\udc36\u2211\ufe01 \ud835\udc57=1 \ud835\udc66\ud835\udc56 \ud835\udc57 log ( softmax ( W [ z(\ud835\udc56 )GIN, z (\ud835\udc56 ) MMB ] + b ) \ud835\udc56 \ud835\udc57 ) (7)\n3.3.2 Regression. For regression tasks using the Mean Squared Error loss, the objective function can be written as:\nL\ud835\udc5f = 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56=1 (\ud835\udc66\ud835\udc56 \u2212 \ud835\udc66\ud835\udc56 )2 (8)\nwhere \ud835\udc41 is the number of samples, \ud835\udc66\ud835\udc56 is the true value for sample \ud835\udc56 , and \ud835\udc66\ud835\udc56 is the predicted value.\nThe predicted value \ud835\udc66\ud835\udc56 can be defined as: \ud835\udc66\ud835\udc56 = ( W [ z(\ud835\udc56 )GIN, z (\ud835\udc56 ) MMB ] + b ) \ud835\udc56\n(9)\nFinally, we can write the objective function for regression as:\nL\ud835\udc5f = 1 \ud835\udc41 \ud835\udc41\u2211\ufe01 \ud835\udc56=1 ( \ud835\udc66\ud835\udc56 \u2212 ( W [ z(\ud835\udc56 )GIN, z (\ud835\udc56 ) MMB ] + b ) \ud835\udc56 )2 (10)"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Data",
            "text": "A series of experiments were conducted utilizingmultiple molecular benchmarks obtained from MoleculeNet [47]. These benchmarks encompass classification and regression tasks derived from diverse studies. To divide the datasets into training and testing sets, the scaffold split method [1] was employed. A scaffold refers to a molecular substructure that exists within a group of molecules and serves to define a chemical series or class. To ensure the evaluation of the model\u2019s generalization ability, this procedure maintains chemical distinctiveness between the training and test sets. The training set comprisedmolecules possessing a specific scaffold, while the test set comprised molecules lacking that specific scaffold. Using the scaffold split, the molecules in each dataset were divided into training,\nvalidation, and test sets, following an 8:1:1 ratio. The classification and regression results using scaffold split are provided in Table 1 and Table 2 respectively. For a fair and consistent comparison between SYN-FUSION and Chemformer, X-Mol, andMolBERTmodels, random splitting was used instead of scaffold split during evaluation (Table 3). This decision was due to the aforementioned models utilizing random splitting for their experiments, and adopting the same splitting methodology maintains methodological consistency across the comparative analysis."
        },
        {
            "heading": "4.2 Configuration",
            "text": "The SYN-FUSION framework utilized GIN and MegaMolBART as the GNN and Transformer architectures respectively. The experimental settings were adopted from [44]. For comparison purposes, two prior works that employed GIN were selected for fusion, namely, MolCLR [44] and Hu et. al [16]. Adam was employed as the optimizer, maintaining a fixed learning rate of 0.001 across all models. A batch size of 32 was used for training the models on each dataset. The selection of an appropriate activation function is crucial as it significantly impacts the model\u2019s learning capacity. ReLU and Softplus activation functions were used as in [44]. For comparison purposes, the results using Softplus are presented in this work as it outperformed ReLU in terms of predictive performance. During the training phase, each model was learned for 100 epochs on the training set, with model checkpoints and early stopping based on validation loss. The model checkpoint having the lowest validation loss was used to evaluate the test set."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "ROC-AUC (%) was used as the evaluation metric for all the classification datasets, following the recommendation by MoleculeNet. For regression datasets, Root Mean Square Error (RMSE) was employed as the metric for FreeSolv, ESOL, and Lipo datasets, while Mean Average Error (MAE) was used as the metric for the QM7, QM8, and QM9 datasets. For each method, the mean and standard deviation over three independent runs are presented."
        },
        {
            "heading": "4.4 Baseline Methods",
            "text": "SYN-FUSION underwent a comprehensive evaluation for molecular property prediction, comparing its performance with other GNN and Transformer baseline methods. The evaluation process comprised of Random Forest (RF) and Support Vector Machine (SVM) which take molecular FPs as the input, GNN architectures that integrate edge features during the aggregation process such as GCN [20] and GIN [48], capturing quantum interactions within molecules such as D-MPNN [51], node-level (self-supervised) and graph-level (supervised) pre-training approaches as described in Hu et. al [16], contrastive pre-training approach such as MolCLR [44]. Transformer-Graph Combination networks that use both selfattention and graph features for training such as Graphformer [52], pharmHGT [18] and GROVER [36]. Due to inconsistencies between the reported results in the MolCLR paper and the reproduction using their provided code repository, the scores obtained from rerun using their code are presented and compared.\nTransformer based models such as Chemformer [17], X-Mol [49], andMolBERT [25] were also included for comparison using random split as followed in their respective works. For a fair comparison, models that solely focus on 2D information were included, and models that incorporate 3D features like MGCN [26], GEM [10], etc. were excluded."
        },
        {
            "heading": "4.5 Results",
            "text": "4.5.1 Classification. The performance of SYN-FUSION was assessed on seven classification datasets. The comparison with GNN baselines using scaffold split is provided in Table 1. SYN-FUSION exhibited superior performance in 5 out of 7 such as ClinTox, HIV, SIDER, and MUV. SYN-FUSION has a relative improvement of (6.63, 0.4)% on BBBP, (-6.2, 4.3)% on Tox21, (19.89, 6.88)% on ClinTox, (4.36, 2.27)% on HIV, (-6.2, 2.4)% on BACE, (7.2, 0.15)% on SIDER, (9.21, 7.75)% on MUV datasets when comparing against its non-fusion counterparts Hu et. al [16] and MolCLR [44] approaches respectively. Also on ClinTox and SIDER datasets, SYN-FUSION outperforms the state-of-the-art model by 4.64% and 2.2% respectively. The results using random split are provided in Table 3. SYN-FUSION\ndemonstrated an improvement over X-Mol [49] by 0.8%, 3.4%, 0.3%, and 6% on BBBP, BACE, ClinTox, and HIV datasets respectively. The comparison involving Transformer-Graph combination networks is presented in Table 4. The SYN-FUSION model demonstrated performance improvements of 2%, 3.8%, and 4.4% on ClinTox, HIV, and SIDER datasets, respectively, compared to the previous best.\n4.5.2 Regression. The performance of SYN-FUSION was assessed on six regression benchmarks and the corresponding results are presented in Table 2. SYN-FUSION surpassed the performance of baseline methods on 4 out of the 6 datasets namely FreeSolv, ESOL,\nQM7, and QM9. These datasets encompass a diverse range of molecular properties, thereby providing a comprehensive evaluation of the fusion approach\u2019s capabilities. SYN-FUSION has a relative improvement of (-10.6, 25.97)% on FreeSolv, (21.3, 31)% on ESOL, (5.4, 8.86)% on Lipo, (38.74, 29.79)% on QM7, (2.09, 3.2)% on QM8, (55.23, 35.49)% on QM9 datasets when comparing on Hu et. al [16] and MolCLR [44] approaches respectively. Notably, the SYN-FUSION model demonstrated significant improvements over the previous best, achieving a remarkable 4.3% improvement on the ESOL dataset. In the random split experiments (Table 3) SYN-FUSION showcased substantial enhancements on ESOL, Lipo, and FreeSolv datasets,\nachieving improvements of 6.59%, 4.81%, and 7.59% respectively, surpassing the performance of all the baselines. The comparison involving Transformer-Graph combination networks is presented in Table 4. Although SYN-FUSION\u2019s performance does not surpass that of methods specifically trained using both graphs and transformers, our approach is comparable. Moreover, our method demonstrates greater practicality due to its efficiency in terms of reduced training time and computational resources required."
        },
        {
            "heading": "5 ANALYSIS ON SYNERGISTIC FUSION",
            "text": "We conducted an extensive analysis of our synergistic fusion approach, SYN-FUSION, to evaluate its performance from both quantitative and qualitative perspectives. The analysis covered various aspects, such as examining its latent space, activation maps, loss interpolation, and weight distribution. All experiments conducted in this section compared SYN-FUSION as the synergistic model with MolCLR in GNN and MegaMolBART in Transformer as its individual models."
        },
        {
            "heading": "5.1 Latent Space Visualization",
            "text": "Latent space visualization offers valuable insights into the encoded representations learned by amodel, enabling a better understanding of the learned distribution and patterns. t-SNE [41] plots were generated to compare the latent space representations of the proposed SYN-FUSION model and its individual components (MolCLR and MegaMolBART) on the ClinTox classification dataset, providing qualitative visualization for comparison as shown in Figure 2, with subfigures 2(a), 2(b), and 2(c) displaying the t-SNE plots of the embeddings derived from SYN-FUSION, MolCLR, and MegaMolBART, respectively. The red and blue points represent the projection of toxic and non-toxic molecule samples respectively. Figure 2(a) has a clear separation between the two classes, as the toxic samples cluster together at the top while the non-toxic molecules appear at the bottom. This observation indicates the successful learning and encoding of discriminative features pertaining to toxic and non-toxic molecules by SYN-FUSION, and the model\u2019s ability to make accurate predictions regarding the toxicity of newmolecules. In contrast, Figure 2(b) suggests that the latent representations of toxic and non-toxic molecules in MolCLR are intermingled instead of having a separation. This finding implies that MolCLR may face difficulties in accurately classifying molecules as toxic or non-toxic. MegaMolBART (Figure 2(c)) exhibits improved discrimination between toxic and non-toxic molecules, although there are still scattered instances of toxic molecules among the non-toxic ones, and the level of separation is not as pronounced as in SYN-FUSION. Confusion matrices obtained on evaluation of 1476 molecule samples from ClinTox presented in Figures 3(a), 3(b), and 3(c) indicate that better separation leads to fewer false predictions, and SYN-FUSION made fewer incorrect predictions in distinguishing between toxic and non-toxic molecules when compared to MolCLR and MegaMolBART."
        },
        {
            "heading": "5.2 1-D Activation Maps",
            "text": "Activation maps help to identify similar patterns across samples belonging to the same class and enable the model to distinguish between classes, leading to effective decision-making. To observe any learned patterns between toxic and non-toxic molecules, 100\nsamples from ClinTox were considered in equal proportions (50 toxic / 50 non-toxic), and 1-D activation maps were generated using the last layer of SYN-FUSION, MolCLR, and MegaMolBART models. Figure 4(a) showcases the stacked 1-D activation maps of SYN-FUSION, revealing distinct and clear activation patterns across samples in both classes, indicating that the model has learned to focus on relevant features for effective classification. In contrast, the activation maps of MolCLR in Figure 4(b) lack well-defined patterns, and it is difficult to differentiate the toxic from the non-toxic class. Activation maps generated by MegaMolBART, as depicted in Figure 4(c), demonstrate intermediate characteristics between the two models, revealing a few discernible patterns that are slightly more noticeable compared to MolCLR but not as prominent as SYNFUSION."
        },
        {
            "heading": "5.3 Loss Interpolation",
            "text": "Loss interpolation plots offer a concise visualization of the transition between different loss values, providing valuable insights into the behavior and convergence of optimization over the course of training. Notably, the presence of Monotonic Linear Interpolation in a model\u2019s loss trajectory signifies that the optimization of tasks is relatively easier [14]. Notable differences are observed in the loss trajectories of the models under comparison, as shown in Figure 5. Specifically, the loss curve of SYN-FUSION displays a remarkably high level of monotonicity, suggesting a smoother and more consistent optimization process, in contrast to the loss curves of MolCLR and MegaMolBART. Moreover, SYN-FUSION has lower initial and final loss values compared to the other two models, providing additional evidence of easier and better optimization. These findings substantiate the effectiveness of synergistic fusion surpassing individual models in terms of optimization and convergence."
        },
        {
            "heading": "5.4 Weight Histograms",
            "text": "A weight histogram shows the distribution of weights within a model, providing insights into the range and frequency of different weight values. Small weights (values close to 0.0) tend to yield sharper minimizers and exhibit greater sensitivity to perturbations [24]. Conversely, weight distribution with uniform variance (both positive and negative values) leads to flatter minima and contributes to better generalization. In light of this finding, the weight distributions of the final layer of SYN-FUSION, MolCLR, and MegaMolBART were investigated after completion of training, and the histogram of weights is presented in Figure 6. SYN-FUSION produces higher magnitude (both range and density) weights compared to MolCLR and MegaMolBART, indicating that fusion improves generalization and helps in easier and faster optimization. The impact of this phenomenon can be observed in the loss interpolation discussed in Section 5.3 Figure 5 where SYN-FUSION demonstrates a favorable initialization denoted by a lower initial loss value, undergoes a rapid minimization of the loss ending with a significantly lower final loss value."
        },
        {
            "heading": "6 ABLATION STUDY",
            "text": "In order to experimentally verify the impact of synergy, we conducted a comparative analysis between the combined effect (represented by SYN-FUSION) and the individual models (MolCLR and MegaMolBART), as well as their (sum) ensemble, on both classification and regression tasks. In the ensemble approach, we handled the predictions generated by each individual model differently depending on the task at hand. For classification, if both models provided identical predictions, we retained the prediction as is. However,\nin cases where the models offered differing predictions, we considered the prediction with higher confidence. For regression, we computed the average of the two individual model predictions. The results are illustrated in Figure 7. In the absence of SYN-FUSION, the AUC% drops from 94.69% by 5.19%-6.24% on ClinTox, and the RMSE increases from 0.89 by 0.15-0.39 on ESOL. This demonstrates the synergy effect - the combined effect achieved through fusion is greater than the individual models and their ensemble."
        },
        {
            "heading": "7 DISCUSSION AND CONCLUSION",
            "text": "Wepresent SYN-FUSION, a novel approach that synergistically combines pre-trained features from Graph Neural Networks (GNNs) and Transformers to create a comprehensive molecular representation. Our method effectively captures both the global structure of molecules and the characteristics of individual atoms, addressing the limitations of existing approaches. Experimental results on various molecular benchmarks demonstrate the superior performance of SYN-FUSION compared to previous models in both classification and regression tasks. Furthermore, a detailed analysis of the learned fusion model provides insights into its effectiveness through aspects such as loss, activation, and weight distribution. The conducted ablation study demonstrates that the fusion approach outperforms individual models and their ensemble, offering a substantial improvement in predicting molecular properties. This work contributes to the advancement of molecular representation\ntechniques, providing a promising solution for accurate molecule property prediction and generalization within the vast chemical space.We believe that the presented findings will make a substantial contribution to the field, and hold great potential for applications in drug discovery and chemical research."
        }
    ],
    "title": "Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction",
    "year": 2023
}