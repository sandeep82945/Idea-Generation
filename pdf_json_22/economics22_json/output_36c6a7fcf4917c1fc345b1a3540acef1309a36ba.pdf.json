{
    "abstractText": "In this paper, we propose a framework for achieving longterm fair sequential decision making. By conducting both the hard and soft interventions, we propose to take path-specific effects on the time-lagged causal graph as a quantitative tool for measuring long-term fairness. The problem of fair sequential decision making is then formulated as a constrained optimization problem with the utility as the objective and the long-term and short-term fairness as constraints. We show that such an optimization problem can be converted to a performative risk optimization. Finally, repeated risk minimization (RRM) is used for model training, and the convergence of RRM is theoretically analyzed. The empirical evaluation shows the effectiveness of the proposed algorithm on synthetic and semi-synthetic temporal datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yaowei Hu"
        },
        {
            "affiliations": [],
            "name": "Lu Zhang"
        }
    ],
    "id": "SP:0258a28292f146f5ed653e13ead7cee87a8bb473",
    "references": [
        {
            "authors": [
                "C. Avin",
                "I. Shpitser",
                "J. Pearl"
            ],
            "title": "Identifiability of path-specific effects",
            "venue": "Proceedings of the 19th international joint conference on Artificial intelligence, 357\u2013363.",
            "year": 2005
        },
        {
            "authors": [
                "A. Bower",
                "S.N. Kitchen",
                "L. Niss",
                "M.J. Strauss",
                "A. Vargas",
                "S. Venkatasubramanian"
            ],
            "title": "Fair pipelines",
            "venue": "arXiv preprint arXiv:1707.00391.",
            "year": 2017
        },
        {
            "authors": [
                "S. Caton",
                "C. Haas"
            ],
            "title": "Fairness in Machine Learning: A Survey",
            "venue": "ArXiv, abs/2010.04053.",
            "year": 2020
        },
        {
            "authors": [
                "J. Correa",
                "E. Bareinboim"
            ],
            "title": "A calculus for stochastic interventions: Causal effect identification and surrogate experiments",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, 10093\u201310100.",
            "year": 2020
        },
        {
            "authors": [
                "E. Creager",
                "D. Madras",
                "T. Pitassi",
                "R. Zemel"
            ],
            "title": "Causal modeling for fairness in dynamical systems",
            "venue": "International Conference on Machine Learning, 2185\u20132195. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "A. D\u2019Amour",
                "H. Srinivasan",
                "J. Atwood",
                "P. Baljekar",
                "D. Sculley",
                "Y. Halpern"
            ],
            "title": "Fairness is not static: deeper understanding of long term fairness via simulation studies",
            "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,",
            "year": 2020
        },
        {
            "authors": [
                "S. Diamond",
                "S. Boyd"
            ],
            "title": "CVXPY: A Pythonembedded modeling language for convex optimization",
            "venue": "Journal of Machine Learning Research, 17(83): 1\u20135.",
            "year": 2016
        },
        {
            "authors": [
                "C. Dwork",
                "C. Ilvento"
            ],
            "title": "Fairness under composition",
            "venue": "arXiv preprint arXiv:1806.06122.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ge",
                "S. Liu",
                "R. Gao",
                "Y. Xian",
                "Y. Li",
                "X. Zhao",
                "C. Pei",
                "F. Sun",
                "J. Ge",
                "W Ou"
            ],
            "title": "Towards Long-term Fairness in Recommendation",
            "venue": "In Proceedings of the 14th ACM International Conference on Web Search and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "M. Hardt",
                "E. Price",
                "N. Srebro"
            ],
            "title": "Equality of Opportunity in Supervised Learning",
            "venue": "NIPS.",
            "year": 2016
        },
        {
            "authors": [
                "L. Hu",
                "Y. Chen"
            ],
            "title": "A short-term intervention for long-term fairness in the labor market",
            "venue": "Proceedings of the 2018 World Wide Web Conference, 1389\u20131398.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Hu",
                "Y. Wu",
                "L. Zhang",
                "X. Wu"
            ],
            "title": "Fair Multiple Decision Making Through Soft Interventions",
            "venue": "Advances in Neural Information Processing Systems, 33.",
            "year": 2020
        },
        {
            "authors": [
                "S. Jabbari",
                "M. Joseph",
                "M. Kearns",
                "J. Morgenstern",
                "A. Roth"
            ],
            "title": "Fairness in reinforcement learning",
            "venue": "International Conference on Machine Learning, 1617\u20131626. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "K.D. Johnson",
                "D.P. Foster",
                "R.A. Stine"
            ],
            "title": "Impartial predictive modeling: Ensuring fairness in arbitrary models",
            "venue": "Statistical Science, 1.",
            "year": 2016
        },
        {
            "authors": [
                "S. Kannan",
                "A. Roth",
                "J. Ziani"
            ],
            "title": "Downstream effects of affirmative action",
            "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency, 240\u2013248.",
            "year": 2019
        },
        {
            "authors": [
                "M.J. Kusner",
                "J.R. Loftus",
                "C. Russell",
                "R. Silva"
            ],
            "title": "Counterfactual Fairness",
            "venue": "NIPS.",
            "year": 2017
        },
        {
            "authors": [
                "L.T. Liu",
                "S. Dean",
                "E. Rolf",
                "M. Simchowitz",
                "M. Hardt"
            ],
            "title": "Delayed impact of fair machine learning",
            "venue": "International Conference on Machine Learning, 3150\u20133158. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "S. L\u00f6we",
                "D. Madras",
                "R. Zemel",
                "M. Welling"
            ],
            "title": "Amortized causal discovery: Learning to infer causal graphs from time-series data",
            "venue": "arXiv preprint arXiv:2006.10833.",
            "year": 2020
        },
        {
            "authors": [
                "R. Nabi",
                "I. Shpitser"
            ],
            "title": "Fair inference on outcomes",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "R. Pamfil",
                "N. Sriwattanaworachai",
                "S. Desai",
                "P. Pilgerstorfer",
                "K. Georgatzis",
                "P. Beaumont",
                "B. Aragam"
            ],
            "title": "Dynotears: Structure learning from time-series data",
            "venue": "International Conference on Artificial Intelligence and Statistics, 1595\u20131605. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. Kopf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "PyTorch: An Imperative Style",
            "year": 2019
        },
        {
            "authors": [
                "J. Pearl"
            ],
            "title": "Causality",
            "venue": "Cambridge university press.",
            "year": 2009
        },
        {
            "authors": [
                "J. Perdomo",
                "T. Zrnic",
                "C. Mendler-D\u00fcnner",
                "M. Hardt"
            ],
            "title": "Performative prediction",
            "venue": "International Conference on Machine Learning, 7599\u20137609. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "J. Runge"
            ],
            "title": "Causal network reconstruction from time series: From theoretical assumptions to practical estimation",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(7): 075310.",
            "year": 2018
        },
        {
            "authors": [
                "J. Runge"
            ],
            "title": "Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information",
            "venue": "International Conference on Artificial Intelligence and Statistics, 938\u2013947. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "J. Runge"
            ],
            "title": "Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets",
            "venue": "Conference on Uncertainty in Artificial Intelligence, 1388\u20131397. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "J. Runge",
                "P. Nowack",
                "M. Kretschmer",
                "S. Flaxman",
                "D. Sejdinovic"
            ],
            "title": "Detecting and quantifying causal associations in large nonlinear time series datasets",
            "venue": "Science Advances, 5(11): eaau4996.",
            "year": 2019
        },
        {
            "authors": [
                "M. Wen",
                "O. Bastani",
                "U. Topcu"
            ],
            "title": "Algorithms for Fairness in Sequential Decision Making",
            "venue": "International Conference on Artificial Intelligence and Statistics, 1144\u2013 1152. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wu",
                "L. Zhang",
                "X. Wu"
            ],
            "title": "On convexity and bounds of fairness-aware classification",
            "venue": "The World Wide Web Conference, 3356\u20133362.",
            "year": 2019
        },
        {
            "authors": [
                "I.-C. Yeh",
                "C.-h. Lien"
            ],
            "title": "The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients",
            "venue": "Expert Systems with Applications, 36(2): 2473\u20132480.",
            "year": 2009
        },
        {
            "authors": [
                "R. Zemel",
                "Y. Wu",
                "K. Swersky",
                "T. Pitassi",
                "C. Dwork"
            ],
            "title": "Learning fair representations",
            "venue": "International conference on machine learning, 325\u2013333. PMLR.",
            "year": 2013
        },
        {
            "authors": [
                "L. Zhang",
                "Y. Wu",
                "X. Wu"
            ],
            "title": "Achieving NonDiscrimination in Data Release",
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.",
            "year": 2017
        },
        {
            "authors": [
                "L. Zhang",
                "Y. Wu",
                "X. Wu"
            ],
            "title": "A Causal Framework for Discovering and Removing Direct and Indirect Discrimination",
            "venue": "IJCAI.",
            "year": 2017
        },
        {
            "authors": [
                "L. Zhang",
                "Y. Wu",
                "X. Wu"
            ],
            "title": "Causal modeling-based discrimination discovery and removal: Criteria, bounds, and algorithms",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 31(11): 2035\u20132050.",
            "year": 2018
        },
        {
            "authors": [
                "X. Zhang",
                "M. Liu"
            ],
            "title": "Fairness in learning-based sequential decision algorithms: A survey",
            "venue": "arXiv preprint arXiv:2001.04861.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "R. Tu",
                "Y. Liu",
                "M. Liu",
                "H. Kjellstr\u00f6m",
                "K. Zhang",
                "C. Zhang"
            ],
            "title": "How do fair decisions fare in longterm qualification? arXiv preprint arXiv:2010.11300",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Fair machine learning has received increasing attention in the past years, especially in decision making tasks such as hiring (Caton and Haas 2020), college admissions (Zhang, Wu, and Wu 2017a) and bank loans (Johnson, Foster, and Stine 2016). Many algorithms for achieving fair decision making have been proposed based on various fairness notions (e.g. demographic parity (Zemel et al. 2013), equalized odds (Hardt, Price, and Srebro 2016) and counterfactual fairness (Kusner et al. 2017)). At present, the majority of studies on fair machine learning focus on the static or one-shot classification setting. However, in practice, decision making systems are usually operating in a dynamic manner such that the classifier makes sequential decisions over a period of time. In many situations, each decision made by the classifier may change the underlying data population and further affect subsequent decisions. For example, suppose a person applies to a bank for a loan and the bank estimates the risk of default according to his/her credit score. Then, the bank\u2019s decision on the loan application (e.g., whether to grant the loan and the interest rate assigned) may in turn affect the default risk and change the person\u2019s credit score (e.g., the credit score will decrease if the loan is granted but he/she defaults on the loan) which will affect his/her next loan application. If the bank\u2019s decision leads to a long-term decrease in the credit score, then it imposes a negative long-term effect\nCopyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\non future decisions for this person. Therefore, fair decision making should concern not only the fairness of a single decision but more importantly, whether a decision model can impose fair long-term effects on different groups. This notion of fairness is referred to as long-term fairness in recent studies (Liu et al. 2018; Hu and Chen 2018; Ge et al. 2021).\nThe challenge of achieving long-term fairness comes in two folds. Firstly, different from static settings, decisions made by models may change users\u2019 behaviors, and/or affect their status such as reputation, qualification, etc., and impact subsequent decisions via feedback loops. Without knowing how the population would be reshaped by decisions, enforcing any fairness constraint may create negative feedback loops and eventually harm fairness in the long run. Recent research has demonstrated that existing fairness criteria cannot guarantee fairness and sometimes undermine fairness even if only one time step is taken into consideration (Liu et al. 2018; Kannan, Roth, and Ziani 2019; D\u2019Amour et al. 2020; Creager et al. 2020). Secondly, due to the feedback loops, the deployment of the decision model will cause changes in the data distribution that is originally used for training. This can be viewed as a distribution shift problem as the distribution of the training data (i.e., distribution before the model deployment) is different from the distribution of the test data (i.e., distribution after the model deployment). Ignoring the distribution shift will critically affect the achievement of long-term fairness, as long-term fairness is affected by all decisions made by the model along the time.\nIn this paper, we propose a framework for achieving longterm fair sequential decision making by addressing both above challenges. We model the dynamics of the decisionmaking process by employing Pearl\u2019s Structural Causal Model (SCM) (Pearl 2009), in which the relations among user features and decisions and how those decisions affect the data distribution can be encoded in a probabilistic graphical model. Specifically, we leverage the time-lagged causal graph (Runge et al. 2019) to describe the causal relations over time, and adopt the soft intervention (Correa and Bareinboim 2020) for modeling the model deployment and inferring its impacts on the underlying population. Then, we measure long-term fairness as the path-specific effect on the time-lagged causal graph under both the hard intervention on the sensitive attribute and the soft intervention on the predicted decisions. A constrained optimization problem is ar X\niv :2\n20 4.\n01 81\n9v 1\n[ cs\n.L G\n] 4\nA pr\n2 02\n2\nformulated to strike a trade-off between long-term fairness and model utility, as well as certain short-term fairness requirement that may be stipulated by law or regulations. On the other hand, we show that the constrained optimization problem can be converted to a performative risk optimization problem (Perdomo et al. 2020). Then, we employ the repeated risk minimization (RRM) training technique (Perdomo et al. 2020) for dealing with the distribution shift problem. The performative optimality and stability properties of the proposed method are theoretically and empirically evaluated which shows its effectiveness.\nTo the best of our knowledge, this paper is the first to propose a causality-based long-fairness notion. The proposed learning framework is general such that it could incorporate different combinations of surrogate functions, utility loss functions, as well as causal paths regarding long-term fairness used to fit different applications. The experiment results show that the proposed method can achieve long-term fairness for multiple time steps, while the fairness performance deteriorates with time if no fairness constraint or static fairness constraints are used. Related Work. Fair machine learning research in past years has been focused on static settings with one-shot decisions being made. To extend fair machine learning to dynamic settings, some efforts have been devoted to a compound decision-making process called pipeline (Bower et al. 2017; Dwork and Ilvento 2018). In pipelines, individuals may drop out at any stage and classification in subsequent stages depends on the remaining cohort of individuals. In a more general setting, decisions made in the past will affect the underlying population, and then affect future decisions (Zhang and Liu 2020). In this setting, a number of studies have demonstrated the inadequacy of static fairness approaches in various scenarios, including credit lending (Liu et al. 2018), college admission (D\u2019Amour et al. 2020), labor market (Hu and Chen 2018). Creager et al. (2020) proposes to use causal directed acyclic graphs (DAGs) as a unifying framework to study fairness in dynamical systems, but does not propose an approach to achieve long-term fairness. On the other hand, some works (Jabbari et al. 2017; Zhang et al. 2020; Wen, Bastani, and Topcu 2021) study long-term fairness in the context of reinforcement learning. The most relevant work to this paper is (Hu et al. 2020) on fair multiple decision making, which also applies SCM and leverages soft interventions to model the deployment of decision models. However, (Hu et al. 2020) is still focused on the static fairness of each decision model separately other than the long-term fairness."
        },
        {
            "heading": "Preliminaries",
            "text": "Throughout this paper, variables and their values are denoted by uppercase and lowercase letters respectively, i.e., X and x. The sets of variables and their values are denoted by bold letters, i.e., X and x."
        },
        {
            "heading": "Structural Causal Model",
            "text": "Our work is based on Pearl\u2019s structural causal models (Pearl 2009) which describe the causal mechanisms of a system\nby a set of structural equations, i.e., x = fX(paX ,uX) for each X , where paX is a realization of a subset of endogenous variables, and uX is a realization of a set of exogenous variables. Each causal modelM is associated with a causal model graph G = \u3008V,E\u3009 where V is a set of nodes and E is a set of directed edges for representing the direct causal relations. This paper assumes the Markovian model in which all exogenous variables are mutually independent.\nQuantitatively measuring causal effects is facilitated with the (hard) intervention (Pearl 2009) which forces some variables to take certain values. Formally, the intervention that sets the value of X to x is denoted by do(x). In a SCM, intervention do(x) is defined as the substitution of equation x = fX(paX ,uX) with constant x. An intervention on a variable affects its descendants via causal paths. For an observed variable Y affected by X , its variant under intervention do(x) is denoted by Y (x). The distribution of Y (x), also referred to as the post-intervention distribution of Y , is denoted by P (Y (x)). The soft intervention (Pearl 2009) extends the hard intervention such that it forces variable X to take functional relationship g(z) in responding to some other variables Z, which is denoted by \u03c3 in (Correa and Bareinboim 2020). The soft intervention substitutes equation x = fX(paX ,uX) with a new function x = g(z). After performing the soft intervention, X will be associated with a new distribution determined by function g. In this paper, the function g is parameterized by \u03b8, and we denote the new distribution by P\u03b8(x|z). We also denote the distribution of Y after performing the soft intervention by P (Y (\u03b8))."
        },
        {
            "heading": "Fairness-aware Classification",
            "text": "The classification problem is to learn a functional mapping f : X 7\u2192 Y from the labeled training data {(xi, yi)}ni=1 where xi \u2208 X , yi \u2208 Y and Y = {\u22121, 1} , by minimizing the 0-1 loss function EX,Y [1[f(X) 6= Y ]] where 1[\u00b7] is an indicator function. In general, f is made up of another function h set up in the real number domain, i.e., h : X 7\u2192 R and 1[f(X) 6= Y ] = 1[Y h(X) \u2265 0]. Since directly minimizing the indicator is intractable, one can replace it with a smooth and differentiable surrogate function \u03c6. Then, the loss function can can be reformulated as EX,Y [\u03c6(Y h(X))]. Similarly, one can also formulate fairness constraints as smoothed expressions using surrogate functions. As a result, fair classification problems can be formulated as constrained optimization problems (Wu, Zhang, and Wu 2019; Hu et al. 2020). We follow the notations used in (Wu, Zhang, and Wu 2019; Hu et al. 2020) in our formulations."
        },
        {
            "heading": "Formulating Long-term Fairness",
            "text": "We start by formally formulating the long-term fairness in sequential decision making. Assume we have access to a temporal dataset D = {(S,Xt, Y t)}lt=1 where S is a timeinvariant protected attribute, Xt is a set of time-dependent unprotected attributes and Y t is a time-dependent class label. Note that this setting can be viewed as observing the data of a set of individuals at all time steps, or a more general situation where a population is subject to the decision cycles and the data is sampled at each time step. For ease\nof discussion, we assume both class label and protected attribute are binary variables, i.e., S = {s+, s\u2212} with s+ denoting the unprotected group and s\u2212 denoting the protected group, and Y = {1,\u22121} with 1 denoting the positive decision and \u22121 denoting the negative decision, but proposed concepts could be extended to multiple protected attributes and multiple/continuous labels situations. A predictive decision model h\u03b8(\u00b7) parameterized by \u03b8 is trained on D. Then, it is deployed to make predicted decisions Y\u0302 t from (S,Xt) repeatedly at each time, i.e., Y\u0302 t = 1 if h\u03b8(s,xt) \u2265 0 and Y\u0302 t = \u22121 otherwise, forming a sequential decision making process. Such sequential decision making process is common in practice. For example, a bank repeatedly makes lending decisions based on applicants\u2019 profile such as credit score, income, etc., and a predictive policing algorithm repeatedly makes decision about where to send police for patrolling based on the crime discovered in the neighborhood. The ultimate goal of long-term fair machine learning is to ensure that the model h\u03b8(\u00b7) is fair in a long-term stage denoted by t*. In this paper, we assume there is sufficient historical training data such that l \u2265 t\u2217."
        },
        {
            "heading": "Causality-based Long-term Fairness",
            "text": "We develop the long-term fairness notion by leveraging Pearl\u2019s SCM. First, we assume a time-lagged causal graph G for describing the causal relationship among variables over time. In recent years, structure learning algorithms have been proposed for constructing time-lagged causal graphs from data, including both constrained-based approaches (Runge 2018a,b, 2020) and continuous optimization-based approaches (Pamfil et al. 2020; Lo\u0308we et al. 2020) which can be leveraged to learn the time-lagged causal graph from data. Figure 1 shows a typical example of the time-lagged causal graph in our settings: the edge from S to X0 represents the bias in the distribution of X due to historical reasons; the edges from S and Xt to Y t represent that S and Xt are used as the input to compute Y\u0302 t; and the edges from Xt and Y t to Xt+1 represent how the distribution of X would be reshaped via feedback after each decision.\nNext, we formulate long-term fairness as path-specific effects that are transmitted in the time-lagged causal graph along certain paths. The path-specific effects reflect how the intervention affects each variable on the path in a topological order and hence are appropriate for capturing dynamics in sequential decision making. Similar to the indirect discrimination in static fair machine learning (Zhang, Wu, and Wu 2017b; Nabi and Shpitser 2018), we can also justify the use of the path-specific effect by the need to distinguish discriminatory effects from explainable effects. We consider discriminatory effects as those which are due to biased decisions made by the decision making system in the past and will continue to influence future decisions. Correspondingly, we consider explainable effects as those which are attributed to external factors and cannot be eliminated within the decision making system. To this end, we categorize unprotected attributes X into two disjoint subsets: irrelevant attributes Xi and relevant attributes Xr. We define irrelevant attributes as those which are justifiable in deci-\nsion making, and meanwhile evolved autonomously or/and altered by external factors only. We define the rest of attributes as relevant attributes, which could be unjustifiable in decision making or reshaped by the decision over time. Then, we define long-term fairness as the causal effect where the influence of the hard intervention on S is transmitted in the causal graph by passing through relevant attributes only. Note that the influence of the soft intervention on Y is still transmitted through all causal paths.\nFinally, we propose to adopt soft interventions as a key technique for modeling decision model deployment and inferring its impacts on the underlying population. We treat the deployment of the decision model at each time step as to perform a soft intervention on the decision variable. More specifically, we force the structural equation associated with Y t in the causal model to be replaced by the decision model h\u03b8(\u00b7) that outputs Y\u0302 t. Thus, the change to underlying population could be inferred as the post-intervention distribution after performing the soft intervention. Meanwhile, to quantify fairness as causal effects of the protected attribute on the decision, we perform hard intervention on the protected attribute in order to answer the \u201cwhat if\u201d question, i.e., \u201cwhat would the decision be if we intervene the gender of applications to female?\u201d As a result, we perform both hard intervention and soft intervention simultaneously for measuring long-term fairness as causal effects.\nSymbolically, denote by \u03c0 the set of causal paths from S to Y\u0302 t \u2217 through relevant attributes X1r, \u00b7 \u00b7 \u00b7 ,Xt \u2217 r and Y\u0302 1, \u00b7 \u00b7 \u00b7 , Y\u0302 t\u2217\u22121 but not through irrelevant attributes X0i , \u00b7 \u00b7 \u00b7 ,Xt \u2217\ni . Meanwhile, as we conduct path-specific hard intervention on S and soft interventions on Y to deploy decision model h\u03b8(\u00b7), we denote the post-intervention distribution of Y\u0302 t by Y\u0302 t(s\u03c0, \u03b8) which explicitly shows that the soft intervention depends on parameters \u03b8. Then, we can readily propose the quantitative notion for long-term fairness. Definition 1 (Long-term Fairness). The long-term fairness of a decision model h\u03b8(\u00b7) is measured by P (Y\u0302 t\u2217(s+\u03c0 , \u03b8)) \u2212 P (Y\u0302 t\u2217(s\u2212\u03c0 , \u03b8)) where \u03c0 is a set of paths from S to Y\u0302\nt\u2217 passing through X1r , Y\u0302 1, \u00b7 \u00b7 \u00b7 , Xt\u2217\u22121r , Y\u0302 t\u2217\u22121, Xt\u2217r , s\u03c0 represents the path-specific hard intervention and \u03b8 represents the soft intervention through all paths."
        },
        {
            "heading": "Loss Function and Short-term Fairness",
            "text": "In addition to long-term fairness, a desired fair decision model should also satisfy two other requirements. Firstly, it is a natural desire for a predictive decision model to maximize the institution utility, e.g., the loan granting model of a bank certainly wants to maximize the expected return from loans. Secondly, the decision model should also satisfy certain short-term fairness requirement at each time step to enforce local equality, which may be stipulated by law or regulations. For example, the Equal Credit Opportunity Act, 1974, prohibits lending decisions from being influenced by race, age, religion, etc. Similar to the direct discrimination in static fair machine learning, we consider a subset of relevant attributes X\u0303r \u2282 Xr which are unprotected but cannot be justifiably used in the decision making either directly or\nindirectly, referred to as the redlining attributes (Zhang, Wu, and Wu 2017b). Then, we measure the short-term fairness by the causal effect of S on Y\u0302 t along paths that pass through X\u0303r, i.e., S \u2192 X\u0303r \u2192 Y\u0302 t, as well as the direct edge S \u2192 Y\u0302 t at each time step t.\nWe note that trade-off may exist between fairness and utility, as well as between long-term and short-term fairness. The long-term fairness focuses on remedying past discrimination existed in the system, but has no constraint on the biases in the decision at each time step. The short-term fairness, on the other hand, cares about fairness in the decision making process at each time step, but pays no attention in correcting past discrimination in the population. One should combine long-term and short-term fairness to force the decision model to take into consideration both factors and to remove discrimination in the system gradually with time. Therefore, we similarly propose quantitative notions for short-term fairness and institution utility as follows.\nDefinition 2 (Short-term Fairness). The short-term fairness of a decision model h\u03b8(\u00b7) at time t is measured by the causal effect transmitted through paths involved in time t, i.e., P (Y\u0302 t(s+\u03c0t , \u03b8)) \u2212 P (Y\u0302 t(s\u2212\u03c0t , \u03b8)), where \u03c0 t = {S \u2192 X\u0303r \u2192 Y\u0302 t, S \u2192 Y\u0302 t} with redlining attributes X\u0303r, s\u03c0 is the path-specific hard intervention and \u03b8 represents the soft intervention.\nDefinition 3 (Institution Utility). The institution utility of decision model h\u03b8(\u00b7) is measured by the aggregate loss given by \u2211t\u2217 t=1 E[L(Y t, Y\u0302 t)] where L(\u00b7) is the loss function."
        },
        {
            "heading": "Learning Fair Decision Models",
            "text": "After formulating related notions, we are ready to formulate the fair sequential decision making problem given a timelagged causal graph. To ease the representation, in following discussions we consider the simplified causal graph shown in Figure 1 where only relevant attributes with no redlining attributes exist. In this case, the long-term fairness is captured by paths from S to Y\u0302 t \u2217 through X1, Y\u0302 1, \u00b7 \u00b7 \u00b7 ,Xt\u2217 as shown in red, and the short-term fairness is captured by the direct edge S \u2192 Y\u0302 t at each time t as shown in green. How-\never, all our discussions can be applied to our general formulation that includes both relevant and irrelevant features.\nThe goal is to learn a functional mapping h\u03b8 : (Xt, S) 7\u2192 Y t parameterized with \u03b8, i.e., Y\u0302 t = h\u03b8(Xt, S). Based on the discussions above, we formulate a constrained optimization problem which minimizes the loss while subject to longterm fairness and short-term fairness constrains simultaneously. The thresholds \u03c4l and \u03c4t control the strictness of constraints. Problem Formulation 1. The problem of fair sequential decision making is formulated as the constrained optimization:\nargmin \u03b8 t\u2217\u2211 t=1 E [ L(Y t, Y\u0302 t) ] s.t. P ( Y\u0302 t \u2217 (s+\u03c0 , \u03b8)=1 ) \u2212 P ( Y\u0302 t \u2217 (s\u2212\u03c0 , \u03b8)=1 ) \u2264 \u03c4l,\nP ( Y\u0302 t(s+\u03c0t , \u03b8)=1 ) \u2212 P ( Y\u0302 t(s\u2212\u03c0t , \u03b8)=1 ) \u2264 \u03c4t,\nt = 1, \u00b7 \u00b7 \u00b7 , t\u2217\nwhere \u03c4l and \u03c4t are thresholds for long-term fairness and short-term fairness constraints, respectively."
        },
        {
            "heading": "Formulating as Performative Risk Optimization",
            "text": "Solving the optimization problem in Problem Formulation 1 is not trivial. According to the path-specific effect inference (Avin, Shpitser, and Pearl 2005) and the definition of soft intervention (Correa and Bareinboim 2020), post-intervention probability P (Y\u0302 t\n\u2217 (s+\u03c0 , \u03b8) = 1) is given by\u2211\nX1,Y 1,\u00b7\u00b7\u00b7 ,Xt\u2217\n{ P (x1|s+)P\u03b8(y1|x1, s\u2212) \u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7P (xt \u2217 |xt \u2217\u22121, yt \u2217\u22121)P\u03b8(Y t\u2217 =1|xt \u2217 , s\u2212) } , (1)\nwhere P\u03b8(y|x, s\u2212) is a probabilistic function determined by h\u03b8(\u00b7). As a result, P (Y\u0302 t \u2217 (s+\u03c0 , \u03b8) = 1) is a complex nonlinear function of \u03b8, making Problem Formulation 1 difficult to solve. In the following, we show how Problem Formulation 1 is converted to a performative risk optimization problem and then propose an optimization algorithm by leveraging repeated risk minimization.\nFollowing the notation of convex optimization of classification, we denote by \u03c6 a convex surrogate function. Then, we can formulate the loss function as L(Y t, Y\u0302 t) = 1 [ Y th\u03b8(X t, S) < 0 ] = \u03c6 ( Y th\u03b8(X t, S) ) .\nWe can also apply the surrogate function to the fairness constraints. For any t, we have\nP ( Y\u0302 t(s+\u03c0 , \u03b8)=1 ) = \u2211 Xt P ( xt(s+\u03c0 , \u03b8) ) P\u03b8(Y t=1|x, s\u2212).\nSimilar to (Hu et al. 2020), we estimate P\u03b8(Y t = 1|x, s\u2212) by first treating it as 1 [h\u03b8 (xt, s\u2212) \u2265 0] and then replacing the indicator function by \u03c6(\u00b7):\nP ( Y\u0302 t(s+\u03c0 , \u03b8)=1 ) = \u2211 Xt P ( xt(s+\u03c0 , \u03b8) ) \u03c6 ( \u2212h\u03b8 ( xt, s\u2212 )) = E\nXt \u223c P (Xt(s+\u03c0 , \u03b8))\n[ \u03c6 ( \u2212h\u03b8 ( Xt, s\u2212 ))] .\nSimilarly, we have\n\u2212 P ( Y\u0302 t(s\u2212\u03c0 , \u03b8)=1 ) = P ( Y\u0302 t(s\u2212\u03c0 , \u03b8)=0 ) \u2212 1\n= E Xt \u223c P (Xt(s\u2212\u03c0 , \u03b8))\n[ \u03c6 ( h\u03b8 ( Xt, s\u2212 ))] \u2212 1.\nThen, we define utility loss lu(\u03b8), long-term fairness loss ll(\u03b8), and short-term fairness loss ls(\u03b8) as follows.\nlu(\u03b8) = t\u2217\u2211 t=1 E S,Xt, Y t \u223c P (S,Xt, Y t) [ \u03c6 ( Y th\u03b8(X t, S) )] ,\nll(\u03b8) = 1\n2\n{ E\nXt \u2217 \u223c P ( Xt \u2217 (s+\u03c0 , \u03b8)\n) [ \u03c6 ( \u2212h\u03b8 ( Xt \u2217 , s\u2212 ))] + E\nXt \u2217 \u223c P ( Xt \u2217 (s\u2212\u03c0 , \u03b8)\n) [ \u03c6 ( h\u03b8 ( Xt \u2217 , s\u2212 ))] \u2212 1\u2212 \u03c4l } ,\nls(\u03b8) = 1\nt\u2217 t\u2217\u2211 t=1 { E Xt \u223c P ( Xt(s\u2212\u03c0t , \u03b8) ) [ \u03c6 ( \u2212h\u03b8 ( Xt, s+ ))] + E\nXt \u223c P ( Xt(s\u2212\u03c0t , \u03b8)\n) [ \u03c6 ( h\u03b8 ( Xt, s\u2212 ))] \u2212 1\u2212 \u03c4t } .\nBy adding the long-term and short-term fairness losses as regularization terms into the objective function, we obtain an unconstrained optimization problem as given in Problem Formulation 2. The general formulation of the performative risk optimization can be given by argmin\u03b8 E\nZ\u223cD(\u03b8) l(Z; \u03b8)\nwhere Z represents the set of all attributes and outcome (Perdomo et al. 2020). Thus, Problem Formulation 2 can be considered as a performative risk optimization problem as all terms in the objective function are represented as expectations of the loss function over the distributions that depend on the loss function parameters. Compare with Problem Formulation 1, Problem Formulation 2 relaxes the fairness constraints and certain amount of violations to the constraints are allowed. However, Problem Formulation 2 can be solve more efficiently by leveraging the repeated risk minimization technique as shown in the next subsection. Problem Formulation 2. The problem of fair sequential decision making is reformulated as the performative risk optimization:\nargmin \u03b8 l(\u03b8) = \u03bbulu(\u03b8) + \u03bblll(\u03b8) + \u03bbsls(\u03b8) (2)\nwhere \u03bbu, \u03bbl and \u03bbs are weight parameters and satisfy \u03bbu+ \u03bbl + \u03bbs = 1."
        },
        {
            "heading": "The Algorithm of Repeated Risk Minimization",
            "text": "Repeated risk minimization (RRM) is an iterative algorithmic heuristic for solving the performative risk optimization problem. The procedure of the RRM is to start from an initial model and repeatedly find a model that minimizes the loss function on the distribution resulting from the previous model, which can symbolically represented as the update\nAlgorithm 1: Repeated Risk Minimization Input : Dataset D = {(S,Xt, Y t)}lt=1, time-lagged\ncausal graph G, convergence threshold \u03b4 Output: The stable model h\u03b8\n1 Train a classifier on D according to Eq. (2) without the soft intervention to obtain the initial parameter \u03b80; 2 i\u2190 0; 3 repeat 4 Sampled the post-intervention distributions\nP ( Xt \u2217 (s+\u03c0 , \u03b8i) ) and P ( Xt \u2217 (s\u2212\u03c0 , \u03b8i) ) ;\n5 Sampled the post-intervention distributions P ( Xt(s+\u03c0 , \u03b8i) ) and P ( Xt(s\u2212\u03c0 , \u03b8i) ) for each t; 6 Minimize l(\u03b8) according to Eq. (2) to obtain \u03b8i+1; 7 4 = \u2016\u03b8i+1 \u2212 \u03b8i\u20162; 8 i\u2192 i+ 1; 9 until4 < \u03b4;\n10 \u03b8 \u2190 \u03b8i; 11 return h\u03b8;\nrule \u03b8i+1 = argmin\u03b8 E Z\u223cD(\u03b8i) l(Z; \u03b8) (Perdomo et al. 2020). The RRM converges if the model that minimizes the loss remains unchanged from the previous model, i.e., \u03b8i+1 = \u03b8i.\nTo implement the RRM algorithm in our context with three different loss terms, we sample different distributions at each iteration. For computing lu(\u03b8), the data distribution does not change with the deployment of new models, and we always use the original dataset D to compute lu(\u03b8). For computing ll(\u03b8), the data distribution follows the post-intervention distribution P (Xt \u2217 (s+\u03c0 , \u03b8)) (resp. P (Xt \u2217 (s\u2212\u03c0 , \u03b8))). Thus, we sample the data according to the inference formula that is similar to Eq. (1) where a smooth probabilistic function P\u03b8(y|x, s) is used. Specifically, we first sample X1 according to the distribution P (X1|s+) (resp. P (X1|s\u2212)), and sample the decision for each sample according to P\u03b8(Y 1|x1, s\u2212). Then, we sample X2 according to the distribution P (X2|X1, Y 1) upon the samples obtained at the first time step. We repeat this process until time t\u2217 to obtain samples for Xt \u2217 for computing ll(\u03b8). For computing ls(\u03b8), we similarly sample the distributions P (Xt(s+\u03c0t , \u03b8)) and P (X\nt(s\u2212\u03c0t , \u03b8)) for each time step t. The procedure of our algorithm starts from an initial model h\u03b80 directly trained on D, and repeatedly train the model on the re-sample data at each iteration, until the model converges to performative stability. The pseudocode of this procedure is described in Algorithm 1."
        },
        {
            "heading": "Convergence Analysis of RRM",
            "text": "We now conduct performative stability analysis for our algorithm. The convergence of the RRM algorithm depends on the smoothness and convexity of the loss function, as well as the sensitivity of the distribution to the parameters (Perdomo et al. 2020). Specifically, given a general RRM formulation \u03b8i+1 = argmin\u03b8 E\nZ\u223cD(\u03b8i) l(Z; \u03b8), if loss function l(\u00b7) is \u03b2-\njointly smooth and \u03b3-strongly convex, and distributionD(\u03b8) is \u03b5-sensitive, then the RRM converges to a stable point if\n\u03b5 < \u03b2\u03b3 . We similarly analyze these factors for our problem and then give the theoretical convergence result. Lemma 1. If the surrogated loss function (\u03c6 \u25e6 h)(\u00b7) is \u03b3strongly convex, then l(\u00b7) is \u03b3-strongly convex.\nLemma 1 can be directly proven according to the sum rule of the gradient.\nNext, we study the sensitivity of the distributions. Consider the distribution P (Xt(s\u03c0, \u03b8)) for any t. Its sensitivity to \u03b8 depends on to what extend the decisions will impact the attributes via the feedback loop. By assuming that the change of the distribution over the attributes in respond to the change of \u03b8 is bounded by a constant, we present following lemma. Please refer to the appendix for detailed proof. Definition 4. For any t, attributes Xt+1 are c-sensitive if\n\u2016 \u2211 Y t \u2207\u03b8P\u03b8(yt|xt, s)P (xt+1|xt, yt)\u2016\n\u2264 c \u2211 Y t P (xt+1|xt, yt).\nLemma 2. For any t, suppose that Xt+1 are c-sensitive, then distribution P (Xt(s\u03c0, \u03b8)) is \u03b5-sensitive with \u03b5 \u2264 2mc(t \u2212 1), where m is the maximum ground distance between two values of Xt.\nAfter introducing the above two lemmas, we now present our main theoretical result. Theorem 1. Suppose that surrogated loss function (\u03c6\u25e6h)(\u00b7) is \u03b2-jointly smooth and \u03b3-strongly convex, and suppose that Xt+1 are c-sensitive for any t, then the repeated risk minimization converges to a stable point at a linear rate, if 2mc(t\u2217 \u2212 1) < \u03b2\u03b3 .\nThe proof of Theorem 1 is based on Theorem 3.5 in (Perdomo et al. 2020). Please refer to the appendix for details. In practice, this theoretical criterion of convergence may be difficult to meet. However, our experimental results show that our algorithm can converge under reasonable conditions."
        },
        {
            "heading": "Experiments",
            "text": "We conduct experiments on both synthetic and semisynthetic temporal datasets to evaluate the proposed algorithm. We show that our algorithm is effective in achieving both long-term and short-term fairness, while previous fair algorithms that do not consider the dynamics in sequential decision making actually do not mitigate or even exacerbate the short-term or long-term fairness. We consider three baselines in the experiments which treat the whole temporal dataset as a static dataset and train the decision model on it. Fairness constraints are added following the technique proposed in (Wu, Zhang, and Wu 2019). \u2022 Logistic Regression (LR): An unconstrained logistic re-\ngression model which takes user features and labels from all time steps as inputs and outputs. \u2022 Fair Model with Demographic Parity (FMDP): On the basis of the logistic regression model, fairness constraint is added to achieve demographic parity. \u2022 Fair Model with Equal Opportunity (FMEO): On the basis of the logistic regression model, fairness constraint is added to achieve equal opportunity."
        },
        {
            "heading": "Datasets",
            "text": "Synthetic Data. We simulate a process of bank loans following the time-lagged causal graph depicted in Figure 1, where S is the protected attribute like race, Xt represents the financial status of applicants, and Y t represents the decisions about whether or not to grant loans. At t = 1, we generate samples where both values of S are sampled with the equal probability, and the values of X1 are sampled using two different Gaussian distributions according to the value of S. Then at each time t, we sample predicted decisions Y\u0302 t and the values of Xt+1 as follows. Consider a groundtruth decision model h\u03b8\u2217(\u00b7) for deciding the probability of whether an individual would default on a loan, given by \u03c3(h\u03b8\u2217(\u00b7)) where \u03c3(\u00b7) is the sigmoid function. Then, we sample the predicted decision Y\u0302 t (as well as the actual repayment Y t which is sampled separately) from \u03c3(h\u03b8\u2217(\u00b7)) as:\nP (Y\u0302 t) = \u03c3(h\u03b8\u2217(X t, S)), Y\u0302 t \u223c 2 \u00b7 Bernoulli(P (Y\u0302 t))\u2212 1.\nThen, Xt+1 is generated according to the update rule below:\nXt+1 =  Xt \u2212 \u00b7 \u03b8t + b Y\u0302 t = 1, Y t = \u22121 Xt + \u00b7 \u03b8t + b Y\u0302 t = 1, Y t = 1 Xt + b Y\u0302 t = \u22121\n(3)\nwhere is a parameter that controls the sensitivity of the update to the predicted decisions, and b = S \u00b7b1+(1\u2212S)\u00b7b0 is a small base increment at each time step. In the simulation process, we generate a 5-step synthetic dataset with 5000 samples where parameters are set as = 0.5, b0 = 0.2, b1 = 1.0. Semi-synthetic Data. We use the Taiwan credit card dataset (Yeh and Lien 2009) as the initial data at t = 1. To form a balanced dataset, we extract 3000 samples and choose two features PAY AMT1 and PAY AMT2 that are appropriate in fitting into our update rule. Then, we generate a 4-step dataset using the same update rule as shown above."
        },
        {
            "heading": "Training and Evaluation",
            "text": "We conduct the training process following the RRM algorithm. At each iteration, we sample the data according to the current decision model and the causal graph. Similar to the data generation process, predicted decisions are sampled according to the probability given by \u03c3(h\u03b8(\u00b7)), and the feature values are sampled according to Eq. (3). In our experiments, we assume that the true update rule is known in order to remove errors introduced by causal graph construction. In practice, the causal graph learned from data may introduce additional errors.\nWe then design an evaluation process which simulates the real model deployment procedure and feedback loops. At each time step t, we use the trained decision model h\u03b8(\u00b7) to make decisions Y\u0302 t, and use the ground-truth model h\u03b8\u2217(\u00b7) to determine the repayment Y t. The accuracy is measured by comparing Y\u0302 t and Y t, the long-term fairness is measured based on the distribution of Y\u0302 t \u2217 in the evaluation, and the short-term fairness is measured based on the distribution of Y\u0302 t at different time steps according to proposed definitions.\nImplementation Details.1 For baselines FMDP and FMEO, they are formulated as constrained optimization forms which are directly solved by the CVXPY package (Diamond and Boyd 2016). For our algorithm, we use the logistic loss function for the surrogate function \u03c6 and the linear model for the decision model. All algorithms use the l2-regularization which can equip the logistic loss function with strong convexity. In our algorithm, ReLU activation function is adopted to ensure that the fairness constraints are always non-negative, and we adopt PyTorch (Paszke et al. 2019) to implement optimization with Adam optimizer."
        },
        {
            "heading": "Results",
            "text": "The results of the accuracy and fairness of the baselines and our algorithm on the synthetic dataset are shown in Table 1. As can be seen, our algorithm achieves the short-term fair-\n1The code and hyperparameter settings are available online: https://github.com/yaoweihu/Achieving-Long-term-Fairness.\nness at all time steps. More importantly, the long-term fairness is improved with time and approaches zero at t = 5. For other baselines, there is a clear trend that the long-term fairness continuously accumulates with time. This demonstrates that static fairness notions may harm fairness in the long run. The short-term fairness remains stable with time as it shows the bias in the model that is related to the protected attribute. The experiments on the semi-synthetic dataset produce similar results as shown in Table 2. We also observe a trade-off between accuracy and fairness meaning that some accuracy needs to be sacrificed in order to achieve fairness.\nWe also plot in Figure 2 the convergence results of our algorithms for different values. As mentioned earlier, the value of controls the sensitivity of Xt+1 to the update of \u03b8. Figure 2 shows that our algorithm converges when the value of is reasonably small, which is consistent with the results in (Perdomo et al. 2020). We observe similar results on the semi-synthetic dataset."
        },
        {
            "heading": "Conclusions and Future Work",
            "text": "We proposed a framework to achieve long-term fairness in sequential decision making. The decision-making process was modeled by a time-lagged causal graph, in which the hard intervention was performed on the protected attribute and soft interventions were performed on the decisions. We measured both long-term and short-term fairness as pathspecific effects. The problem of fair sequential decision making was formulated as a performative risk optimization problem, and repeated risk minimization is adopted to train the model on the datasets sampled from post-intervention distributions. The convergence of the proposed algorithm is analyzed theoretically. Finally, we verify the effectiveness of the proposed framework and algorithm by comparing it with the baselines on two synthetic datasets.\nPath-specific effects may be unidentifiable from the observational data if the \u201ckite structure\u201d presents in the causal graph (Avin, Shpitser, and Pearl 2005). The long-term fairness loss term ll(\u03b8) cannot be accurately estimated if P (Xt \u2217 (s+\u03c0 , \u03b8)) is unidentifiable if the paths in \u03c0 form a \u201ckite structure\u201d. We will adopt the bounding technique proposed in (Zhang, Wu, and Wu 2018) for unidentifiable pathspecific quantify, compute the lower and upper bounds of P (xt \u2217 (s+\u03c0 , \u03b8)) for each x, and then obtain the upper bound of ll(\u03b8). We leave this to our future work."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our research could benefit decision makers for achieving long-term fairness and balancing the trade-off between fairness and accuracy. The proposed method relaxes the constrained optimization problem (Problem Formulation 1) to an unconstrained optimization problem (Problem Formulation 2). There may be gaps between the two problems, i.e., the optimal solution to the unconstrained optimization problem may not be the optimal solution to the original constrained one. This may potentially result in solutions that achieve compromised fairness which is lower than user requirements."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported in part by NSF 1910284, 1920920, and 1946391."
        }
    ],
    "title": "Achieving Long-Term Fairness in Sequential Decision Making",
    "year": 2022
}