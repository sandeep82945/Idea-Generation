{
    "abstractText": "The recently proposed Sharpness-Aware Minimization (SAM) improves generalization by minimizing a perturbed loss defined as the maximum loss within a neighborhood in the parameter space. However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima. Instead, we define a surrogate gap, a measure equivalent to the dominant eigenvalue of Hessian at a local minimum when the radius of neighborhood (to derive the perturbed loss) is small. The surrogate gap is easy to compute and feasible for direct minimization during training. Based on the above observations, we propose Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), a novel improvement over SAM with negligible computation overhead. Conceptually, GSAM consists of two steps: 1) a gradient descent like SAM to minimize the perturbed loss, and 2) an ascent step in the orthogonal direction (after gradient decomposition) to minimize the surrogate gap and yet not affect the perturbed loss. GSAM seeks a region with both small loss (by step 1) and low sharpness (by step 2), giving rise to a model with high generalization capabilities. Theoretically, we show the convergence of GSAM and provably better generalization than SAM. Empirically, GSAM consistently improves generalization (e.g., +3.2% over SAM and +5.4% over AdamW on ImageNet top-1 accuracy for ViT-B/32). Code is released at https://sites.google.com/view/gsam-iclr22/home.",
    "authors": [
        {
            "affiliations": [],
            "name": "Juntang Zhuang"
        },
        {
            "affiliations": [],
            "name": "Boqing Gong"
        },
        {
            "affiliations": [],
            "name": "Liangzhe Yuan"
        },
        {
            "affiliations": [],
            "name": "Yin Cui"
        },
        {
            "affiliations": [],
            "name": "Hartwig Adam {bgong"
        },
        {
            "affiliations": [],
            "name": "Nicha C. Dvornek"
        },
        {
            "affiliations": [],
            "name": "Sekhar Tatikonda"
        },
        {
            "affiliations": [],
            "name": "James S. Duncan"
        },
        {
            "affiliations": [],
            "name": "Ting Liu"
        }
    ],
    "id": "SP:bf13cf664518ab06891cbd4169299a754f3432c6",
    "references": [
        {
            "authors": [
                "Randall Balestriero",
                "Jerome Pesenti",
                "Yann LeCun"
            ],
            "title": "Learning in high dimension always amounts to extrapolation",
            "venue": "arXiv preprint arXiv:2110.09485,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Beyer",
                "Olivier J. Henaff",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "Aaron van den Oord"
            ],
            "title": "Are we done with imagenet",
            "venue": "arXiv preprint arXiv:2002.05709,",
            "year": 2020
        },
        {
            "authors": [
                "L\u00e9on Bottou"
            ],
            "title": "Large-scale machine learning with stochastic gradient descent",
            "venue": "In Proceedings of COMPSTAT\u20192010,",
            "year": 2010
        },
        {
            "authors": [
                "Pratik Chaudhari",
                "Anna Choromanska",
                "Stefano Soatto",
                "Yann LeCun",
                "Carlo Baldassi",
                "Christian Borgs",
                "Jennifer Chayes",
                "Levent Sagun",
                "Riccardo Zecchina"
            ],
            "title": "Entropy-sgd: Biasing gradient descent into wide valleys",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2019
        },
        {
            "authors": [
                "Xiangning Chen",
                "Cho-Jui Hsieh",
                "Boqing Gong"
            ],
            "title": "When vision transformers outperform resnets without pretraining or strong data augmentations, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Dandelion Mane",
                "Vijay Vasudevan",
                "Quoc V Le"
            ],
            "title": "Autoaugment: Learning augmentation policies from data",
            "venue": "arXiv preprint arXiv:1805.09501,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Damian",
                "Tengyu Ma",
                "Jason Lee"
            ],
            "title": "Label noise sgd provably prefers flat global minimizers",
            "venue": "arXiv preprint arXiv:2106.06530,",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552,",
            "year": 2017
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "John Duchi",
                "Elad Hazan",
                "Yoram Singer"
            ],
            "title": "Adaptive subgradient methods for online learning and stochastic optimization",
            "venue": "Journal of machine learning research,",
            "year": 2011
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "arXiv preprint arXiv:2010.01412,",
            "year": 2020
        },
        {
            "authors": [
                "Xavier Gastaldi"
            ],
            "title": "Shake-shake regularization",
            "venue": "arXiv preprint arXiv:1705.07485,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Byeongho Heo",
                "Sanghyuk Chun",
                "Seong Joon Oh",
                "Dongyoon Han",
                "Sangdoo Yun",
                "Gyuwan Kim",
                "Youngjung Uh",
                "Jung-Woo Ha"
            ],
            "title": "Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights",
            "year": 2006
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Simplifying neural nets by discovering flat minima",
            "venue": "In Advances in neural information processing systems,",
            "year": 1995
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "arXiv preprint arXiv:1803.05407,",
            "year": 2018
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "arXiv preprint arXiv:1806.07572,",
            "year": 2018
        },
        {
            "authors": [
                "Yiding Jiang",
                "Behnam Neyshabur",
                "Hossein Mobahi",
                "Dilip Krishnan",
                "Samy Bengio"
            ],
            "title": "Fantastic generalization measures and where to find them",
            "year": 1912
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Richard Socher"
            ],
            "title": "Improving generalization performance by switching from adam to sgd",
            "venue": "arXiv preprint arXiv:1712.07628,",
            "year": 2017
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "arXiv preprint arXiv:1609.04836,",
            "year": 2016
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Jungmin Kwon",
                "Jeongseop Kim",
                "Hyunseo Park",
                "In Kwon Choi"
            ],
            "title": "Asam: Adaptive sharpnessaware minimization for scale-invariant learning of deep neural networks",
            "venue": "arXiv preprint arXiv:2102.11600,",
            "year": 2021
        },
        {
            "authors": [
                "Beatrice Laurent",
                "Pascal Massart"
            ],
            "title": "Adaptive estimation of a quadratic functional by model selection",
            "venue": "Annals of Statistics,",
            "year": 2000
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Colin Wei",
                "Tengyu Ma"
            ],
            "title": "Towards explaining the regularization effect of initial large learning rate in training neural networks",
            "year": 1907
        },
        {
            "authors": [
                "Tengyuan Liang",
                "Tomaso Poggio",
                "Alexander Rakhlin",
                "James Stokes"
            ],
            "title": "Fisher-rao metric, geometry, and complexity of neural networks",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian Stich",
                "Martin Jaggi"
            ],
            "title": "Extrapolation for large-batch training in deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Liyuan Liu",
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Jiawei Han"
            ],
            "title": "On the variance of the adaptive learning rate and beyond",
            "year": 1908
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Liangchen Luo",
                "Yuanhao Xiong",
                "Yan Liu",
                "Xu Sun"
            ],
            "title": "Adaptive gradient methods with dynamic bound of learning rate",
            "year": 1902
        },
        {
            "authors": [
                "David McAllester"
            ],
            "title": "Simplified pac-bayesian margin bounds",
            "venue": "In Learning theory and Kernel machines,",
            "year": 2003
        },
        {
            "authors": [
                "David A McAllester"
            ],
            "title": "Pac-bayesian model averaging",
            "venue": "In Proceedings of the twelfth annual conference on Computational learning theory, pp",
            "year": 1999
        },
        {
            "authors": [
                "RV Mises",
                "Hilda Pollaczek-Geiringer"
            ],
            "title": "Praktische verfahren der gleichungsaufl\u00f6sung",
            "venue": "ZAMMJournal of Applied Mathematics and Mechanics/Zeitschrift fu\u0308r Angewandte Mathematik und Mechanik,",
            "year": 1929
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey Hinton"
            ],
            "title": "When does label smoothing help",
            "venue": "arXiv preprint arXiv:1906.02629,",
            "year": 2019
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "Nathan Srebro"
            ],
            "title": "A pac-bayesian approach to spectrally-normalized margin bounds for neural networks",
            "venue": "arXiv preprint arXiv:1707.09564,",
            "year": 2017
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian Conference on Computer Vision, Graphics & Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar"
            ],
            "title": "Do imagenet classifiers generalize to imagenet",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sashank J Reddi",
                "Satyen Kale",
                "Sanjiv Kumar"
            ],
            "title": "On the convergence of adam and beyond",
            "venue": "arXiv preprint arXiv:1904.09237,",
            "year": 2019
        },
        {
            "authors": [
                "David E Rumelhart",
                "Geoffrey E Hinton",
                "Ronald J Williams"
            ],
            "title": "Learning internal representations by error propagation",
            "venue": "Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,",
            "year": 1985
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Daniel Keysers",
                "Jakob Uszkoreit",
                "Mario Lucic"
            ],
            "title": "Mlp-mixer: An all-mlp architecture for vision",
            "venue": "arXiv preprint arXiv:2105.01601,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Wei",
                "Jason Lee",
                "Qiang Liu",
                "Tengyu Ma"
            ],
            "title": "Regularization matters: Generalization and optimization of neural nets vs their induced kernel. 2019",
            "year": 2019
        },
        {
            "authors": [
                "Zeke Xie",
                "Li Yuan",
                "Zhanxing Zhu",
                "Masashi Sugiyama"
            ],
            "title": "Positive-negative momentum: Manipulating stochastic gradient noise to improve generalization",
            "venue": "arXiv preprint arXiv:2103.17182,",
            "year": 2021
        },
        {
            "authors": [
                "Xubo Yue",
                "Maher Nouiehed",
                "Raed Al Kontar"
            ],
            "title": "Salr: Sharpness-aware learning rates for improved generalization",
            "venue": "arXiv preprint arXiv:2011.05348,",
            "year": 2020
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Sashank Reddi",
                "Devendra Sachan",
                "Satyen Kale",
                "Sanjiv Kumar"
            ],
            "title": "Adaptive methods for nonconvex optimization",
            "venue": "In Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew D Zeiler"
            ],
            "title": "Adadelta: an adaptive learning rate method",
            "venue": "arXiv preprint arXiv:1212.5701,",
            "year": 2012
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "year": 2017
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "arXiv preprint arXiv:1710.09412,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Zhang",
                "James Lucas",
                "Jimmy Ba",
                "Geoffrey E Hinton"
            ],
            "title": "Lookahead optimizer: k steps forward, 1 step back",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yaowei Zheng",
                "Richong Zhang",
                "Yongyi Mao"
            ],
            "title": "Regularizing neural networks via adversarial model perturbation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Pan Zhou",
                "Jiashi Feng",
                "Chao Ma",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Towards theoretically understanding why sgd generalizes better than adam in deep learning",
            "year": 2010
        },
        {
            "authors": [
                "Juntang Zhuang",
                "Tommy Tang",
                "Yifan Ding",
                "Sekhar Tatikonda",
                "Nicha Dvornek",
                "Xenophon Papademetris",
                "James S Duncan"
            ],
            "title": "Adabelief optimizer: Adapting stepsizes by the belief in observed gradients",
            "year": 2010
        },
        {
            "authors": [
                "Foret"
            ],
            "title": "\u03c1true > \u03c1train set almost surely. (2) Second, we don\u2019t know the value of \u03c1true and can only guess it. In practice, we often guess a small value because training often diverges with large \u03c1 (as observed",
            "year": 2021
        },
        {
            "authors": [
                "Xie"
            ],
            "title": "surrogate gap increases with training steps for any fixed \u03b1, indicating that the training process gradually falls into local minimum in order to minimize the training loss. D RELATED WORKS Besides SAM and ASAM, other methods were proposed in the literature to improve generalization: Lin et al",
            "year": 2021
        },
        {
            "authors": [
                "gradient",
                "Damian"
            ],
            "title": "2020) proposed to adjust learning rate according to sharpness, and Zheng et al. (2021) proposed model perturbation with similar idea to SAM. Izmailov et al. (2018) proposed averaging weights to improve generalization, and Heo et al. (2020) restricted the norm of updated weights to improve generalization",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Modern neural networks are typically highly over-parameterized and easy to overfit to training data, yet the generalization performances on unseen data (test set) often suffer a gap from the training performance (Zhang et al., 2017a). Many studies try to understand the generalization of machine learning models, including the Bayesian perspective (McAllester, 1999; Neyshabur et al., 2017), the information perspective (Liang et al., 2019), the loss surface geometry perspective (Hochreiter & Schmidhuber, 1995; Jiang et al., 2019) and the kernel perspective (Jacot et al., 2018; Wei et al., 2019). Besides analyzing the properties of a model after training, some works study the influence of training and the optimization process, such as the implicit regularization of stochastic gradient descent (SGD) (Bottou, 2010; Zhou et al., 2020), the learning rate\u2019s regularization effect (Li et al., 2019), and the influence of the batch size (Keskar et al., 2016).\nThese studies have led to various modifications to the training process to improve generalization. Keskar & Socher (2017) proposed to use Adam in early training phases for fast convergence and then switch to SGD in late phases for better generalization. Izmailov et al. (2018) proposed to average weights to achieve a wider local minimum, which is expected to generalize better than sharp minima. A similar idea was later used in Lookahead (Zhang et al., 2019). Entropy-SGD (Chaudhari\n\u2217Work was done during an internship at Google\nar X\niv :2\n20 3.\n08 06\n5v 2\n[ cs\n.L G\n] 1\n9 M\nar 2\net al., 2019) derived the gradient of local entropy to avoid solutions in sharp valleys. Entropy-SGD has a nested Langevin iteration, inducing much higher computation costs than vanilla training.\nThe recently proposed Sharpness-Aware Minimization (SAM) (Foret et al., 2020) is a generic training scheme that improves generalization and has been shown especially effective for Vision Transformers (Dosovitskiy et al., 2020) when large-scale pre-training is unavailable (Chen et al., 2021). Suppose vanilla training minimizes loss f(w) (e.g., the cross-entropy loss for classification), where w is the parameter. SAM minimizes a perturbed loss defined as fp(w) , max||\u03b4||\u2264\u03c1 f(w + \u03b4), which is the maximum loss within radius \u03c1 centered at the model parameter w. Intuitively, vanilla training seeks a single point with a low loss, while SAM searches for a neighborhood within which the maximum loss is low. However, we show that a low perturbed loss fp could appear in both flat and sharp minima, implying that only minimizing fp is not always sharpness-aware.\nAlthough the perturbed loss fp(w) might disagree with sharpness, we find a surrogate gap defined as h(w) , fp(w) \u2212 f(w) agrees with sharpness \u2014 Lemma 3.3 shows that the surrogate gap h is an equivalent measure of the dominant eigenvalue of Hessian at a local minimum. Inspired by this observation, we propose the Surrogate Gap Guided Sharpness Aware Minimization (GSAM) which jointly minimizes the perturbed loss fp and the surrogate gap h: a low perturbed loss fp indicates a low training loss within the neighborhood, and a small surrogate gap h avoids solutions in sharp valleys and hence narrows the generalization gap between training and test performances (Thm. 5.3). When both criteria are satisfied, we find a generalizable model with good performances.\nGSAM consists of two steps for each update: 1) descend gradient\u2207fp(w) to minimize the perturbed loss fp (this step is exactly the same as SAM), and 2) decompose gradient \u2207f(w) of the original loss f(w) into components that are parallel and orthogonal to \u2207fp(w), i.e., \u2207f(w) = \u2207\u2016f(w) + \u2207\u22a5f(w), and perform an ascent step in \u2207\u22a5f(w) to minimize the surrogate gap h(w). Note that this ascent step does not change the perturbed loss fp because\u2207f\u22a5(w) \u22a5 \u2207fp(w) by construction. We summarize our contribution as follows:\n\u2022 We define surrogate gap, which measures the sharpness at local minima and is easy to compute. \u2022 We propose the GSAM method to improve the generalization of neural networks. GSAM is\nwidely applicable and incurs negligible computation overhead compared to SAM. \u2022 We demonstrate the convergence of GSAM and its provably better generalization than SAM. \u2022 We empirically validate GSAM over image classification tasks with various neural architec-\ntures, including ResNets (He et al., 2016), Vision Transformers (Dosovitskiy et al., 2020), and MLP-Mixers (Tolstikhin et al., 2021)."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 NOTATIONS",
            "text": "\u2022 f(w): A loss function f with parameter w \u2208 Rk, where k is the parameter dimension. \u2022 \u03c1t \u2208 R: A scalar value controlling the amplitude of perturbation at step t. \u2022 \u2208 R: A small positive constant (to avoid division by 0, = 10\u221212 by default).\n\u2022 wadvt , wt + \u03c1t \u2207f(wt) ||\u2207f(wt)||+ : The solution to max||w\u2032\u2212wt||\u2264\u03c1t f(w \u2032) when \u03c1t is small.\n\u2022 fp(wt) , max||\u03b4||\u2264\u03c1t f(wt + \u03b4) \u2248 f(wadvt ): The perturbed loss induced by f(wt). For each wt, fp(wt) returns the worst possible loss f within a ball of radius \u03c1t centered at wt. When \u03c1t is small, by Taylor expansion, the solution to the maximization problem is equivalent to a gradient ascent from wt to wadvt .\n\u2022 h(w) , fp(w)\u2212 f(w): The surrogate gap defined as the difference between fp(w) and f(w). \u2022 \u03b7t \u2208 R: Learning rate at step t. \u2022 \u03b1 \u2208 R: A constant value that controls the scaled learning rate of the ascent step in GSAM.\n\u2022 g(t), g(t)p \u2208 Rk: At the t-th step, the noisy observation of the gradients\u2207f(wt),\u2207fp(wt) of the original loss and perturbed loss, respectively.\n\u2022 \u2207f(wt) = \u2207f\u2016(wt) +\u2207f\u22a5(wt): Decompose \u2207f(wt) into parallel component \u2207f\u2016(wt) and vertical component\u2207f\u22a5(wt) by projection\u2207f(wt) onto\u2207fp(wt)."
        },
        {
            "heading": "2.2 SHARPNESS-AWARE MINIMIZATION",
            "text": "Conventional optimization of neural networks typically minimizes the training loss f(w) by gradient descent w.r.t. \u2207f(w) and searches for a single pointw with a low loss. However, this vanilla training often falls into a sharp valley of the loss surface, resulting in inferior generalization performance (Chaudhari et al., 2019). Instead of searching for a single point solution, SAM seeks a region with low losses so that small perturbation to the model weights does not cause significant performance degradation. SAM formulates the problem as:\nminw fp(w) where fp(w) , max||\u03b4||\u2264\u03c1 f(w + \u03b4) (1)\nwhere \u03c1 is a predefined constant controlling the radius of a neighborhood. This perturbed loss fp induced by f(w) is the maximum loss within the neighborhood. When the perturbed loss is minimized, the neighborhood corresponds to low losses (below the perturbed loss). For a small \u03c1, using Taylor expansion around w, the inner maximization in Eq. 1 turns into a linear constrained optimization with solution\narg max||\u03b4||\u2264\u03c1 f(w + \u03b4) = arg max||\u03b4||\u2264\u03c1 f(w) + \u03b4 >\u2207f(w) +O(\u03c12) = \u03c1 \u2207f(w)\n||\u2207f(w)|| (2)\nAs a result, the optimization problem of SAM reduces to\nminw fp(w) \u2248 minw f(wadv) where wadv , w + \u03c1 \u2207f(w)\n||\u2207f(w)||+ (3)\nwhere is a scalar (default: 1e-12) to avoid division by 0, and wadv is the \u201cperturbed weight\u201d with the highest loss within the neighborhood. Equivalently, SAM seeks a solution on the surface of the perturbed loss fp(w) rather than the original loss f(w) (Foret et al., 2020)."
        },
        {
            "heading": "3 THE SURROGATE GAP MEASURES THE SHARPNESS AT A LOCAL MINIMUM",
            "text": ""
        },
        {
            "heading": "3.1 THE PERTURBED LOSS IS NOT ALWAYS SHARPNESS-AWARE",
            "text": "Despite that SAM searches for a region of low losses, we show that a solution by SAM is not guaranteed to be flat. Throughout this paper we measure the sharpness at a local minimum of loss f(w) by the dominant eigenvalue \u03c3max (eigenvalue with the largest absolute value) of Hessian. For simplicity, we do not consider the influence of reparameterization on the geometry of loss surfaces, which is thoroughly discussed in (Laurent & Massart, 2000; Kwon et al., 2021).\nFigure 2: \u2207f is decomposed into parallel and vertical (\u2207f\u22a5) components by projection onto \u2207fp. \u2207fGSAM = \u2207fp \u2212 \u03b1\u2207f\u22a5\nAlgorithm 1 GSAM Algorithm For t = 1 to T\n0) \u03c1t schedule: \u03c1t = \u03c1min + (\u03c1max\u2212\u03c1min)(lr\u2212lrmin)\nlrmax\u2212lrmin\n1a) \u2206wt = \u03c1t \u2207f (t) ||\u2207f(t)||+ 1b) wadvt = wt + \u2206wt 2) Get\u2207f (t)p by back-propagation at wadvt . 3) \u2207f (t) = \u2207f (t)\u2016 +\u2207f (t) \u22a5 Decompose \u2207f (t) into compo-\nnents that are parallel and orthogonal to\u2207f (t)p . 4) Update weights:\nVanilla wt+1 = wt \u2212 \u03b7t\u2207f (t) SAM wt+1 = wt \u2212 \u03b7t\u2207f (t)p GSAM wt+1 = wt \u2212 \u03b7t(\u2207f (t)p \u2212 \u03b1\u2207f (t)\u22a5 )\nLemma 3.1. For some fixed \u03c1, consider two local minima w1 and w2, fp(w1) \u2264 fp(w2) 6=\u21d2 \u03c3max(w1) \u2264 \u03c3max(w2), where \u03c3max is the dominant eigenvalue of the Hessian.\nWe leave the proof to Appendix. Fig. 1 illustrates Lemma 3.1 with an example. Consider three local minima denoted as w1 to w3, and suppose the corresponding loss surfaces are flatter from w1 to w3. For some fixed \u03c1, we plot the perturbed loss fp and surrogate gap h , fp \u2212 f around each solution. Comparing w2 with w3: Suppose their vanilla losses are equal, f(w2) = f(w3), then fp(w2) > fp(w3) because the loss surface is flatter around w3, implying that SAM will prefer w3 to w2. Comparing w1 and w2: fp(w1) < fp(w2), and SAM will favor w1 over w2 because it only cares about the perturbed loss fp, even though the loss surface is sharper around w1 than w2."
        },
        {
            "heading": "3.2 THE SURROGATE GAP AGREES WITH SHARPNESS",
            "text": "We introduce the surrogate gap that agrees with sharpness, defined as:\nh(w) , max||\u03b4||\u2264\u03c1 f(w + \u03b4)\u2212 f(w) \u2248 f(wadv)\u2212 f(w) (4) Intuitively, the surrogate gap represents the difference between the maximum loss within the neighborhood and the loss at the center point. The surrogate gap has the following properties. Lemma 3.2. Suppose the perturbation amplitude \u03c1 is sufficiently small, then the approximation to the surrogate gap in Eq. 4 is always non-negative, h(w) \u2248 f(wadv)\u2212 f(w) \u2265 0,\u2200w. Lemma 3.3. For a local minimum w\u2217, consider the dominate eigenvalue \u03c3max of the Hessian of loss f as a measure of sharpness. Considering the neighborhood centered at w\u2217 with a small radius \u03c1, the surrogate gap h(w\u2217) is an equivalent measure of the sharpness: \u03c3max \u2248 2h(w\u2217)/\u03c12.\nThe proof is in Appendix. Lemma 3.2 tells that the surrogate gap is non-negative, and Lemma 3.3 shows that the loss surface is flatter as h gets closer to 0. The two lemmas together indicate that we can find a region with a flat loss surface by minimizing the surrogate gap h(w)."
        },
        {
            "heading": "4 SURROGATE GAP GUIDED SHARPNESS-AWARE MINIMIZATION",
            "text": ""
        },
        {
            "heading": "4.1 GENERAL IDEA: SIMULTANEOUSLY MINIMIZE THE PERTURBED LOSS AND SURROGATE GAP",
            "text": "Inspired by the analysis in Section 3, we propose Surrogate Gap Guided Sharpness-Aware Minimzation (GSAM) to simultaneously minimize two objectives, the perturbed loss fp and the surrogate gap h:\nminw ( fp(w), h(w) ) (5)\nIntuitively, by minimizng fp we search for a region with a low perturbed loss similar to SAM, and by minimizing h we search for a local minimum with a flat surface. A low perturbed loss implies\nlow training losses within the neighborhood, and a flat loss surface reduces the generalization gap between training and test performances (Chaudhari et al., 2019). When both are minimized, the solution gives rise to high accuracy and good generalization.\nPotential caveat in optimization It is tempting and yet sub-optimal to combine the objectives in Eq. 5 to arrive at minw fp(w)+\u03bbh(w), where \u03bb is some positive scalar. One caveat when solving this weighted combination is the potential conflict between the gradients of the two terms, i.e., \u2207fp(w) and \u2207h(w). We illustrate this conflict by Fig. 2, where \u2207h(w) = \u2207fp(w) \u2212 \u2207f(w) (the grey dashed arrow) has a negative inner product with \u2207fp(w) and \u2207f(w). Hence, the gradient descent for the surrogate gap could potentially increase the loss fp, harming the model\u2019s performance. We empirically validate this argument in Sec. 6.4."
        },
        {
            "heading": "4.2 GRADIENT DECOMPOSITION AND ASCENT FOR THE MULTI-OBJECTIVE OPTIMIZATION",
            "text": "Our primary goal is to minimize fp because otherwise a flat solution of high loss is meaningless, and the minimization of h should not increase fp. We propose to decompose \u2207f(wt) and \u2207h into components that are parallel and orthogonal to\u2207fp(wt), respectively (see Fig. 2):\n\u2207f(wt) = \u2207f\u2016(wt) +\u2207f\u22a5(wt) \u2207h(wt) = \u2207h\u2016(wt) +\u2207h\u22a5(wt) (6) \u2207h\u22a5(wt) = \u2212\u2207f\u22a5(wt)\nThe key is that updating in the direction of\u2207h\u22a5(wt) does not change the value of the perturbed loss fp(wt) because \u2207h\u22a5 \u22a5 \u2207fp by construction. Therefore, we propose to perform a descent step in the \u2207h\u22a5(wt) direction, which is equivalent to an ascent step in the \u2207f\u22a5(wt) direction (because \u2207h\u22a5 = \u2212\u2207f\u22a5 by the definition of h), and achieve two goals simultaneously \u2014 it keeps the value of fp(wt) intact and meanwhile decreases the surrogate gap h(wt) = fp(wt)\u2212f(wt) (by increasing f(wt) and not affect fp(wt)).\nThe full GSAM Algorithm is shown in Algo. 1 and Fig. 2, where g(t), g(t)p are noisy observations of \u2207f(wt) and\u2207fp(wt), respectively, and g(t)\u2016 , g (t) \u22a5 are noisy observations of\u2207f\u2016(wt) and\u2207f\u22a5(wt), respectively, by projecting g(t) onto g(t)p . We introduce a constant \u03b1 to scale the stepsize of the ascent step. Steps 1) to 2) are the same as SAM: At current point wt, step 1) takes a gradient ascent to wadvt followed by step 2) evaluating the gradient g (t) p at wadvt . Step 3) projects g\n(t) onto g(t)p , which requires negligible computation compared to the forward and backward passes. In step 4), \u2212\u03b7tg(t)p is the same as in SAM and minimizes the perturbed loss fp(wt) with gradient descent, and \u03b1\u03b7tg (t) \u22a5 performs an ascent step in the orthogonal direction of g (t) p to minimize the surrogate gap h(wt) ( equivalently increase f(wt) and keep fp(wt) intact). In coding, GSAM feeds the \u201csurrogate gradient\u201d\u2207fGSAMt , g (t) p \u2212 \u03b1g(t)\u22a5 to first-order gradient optimizers such as SGD and Adam.\nThe ascent step along g(t)\u22a5 does not harm convergence SAM demonstrates that minimizing fp makes the network generalize better than minimizing f . Even though our ascent step along g(t)\u22a5 increases f(w), it does not affect fp(w), so GSAM still decreases the perturbed loss fp in a way similar to SAM. In Thm. 5.1, we formally prove the convergence of GSAM. In Sec. 6 and Appendix C, we empirically validate that the loss decreases and accuracy increases with training.\nIllustration with a toy example We demonstrate different algorithms by a numerical toy example shown in Fig. 3. The trajectory of GSAM is closer to the ridge and tends to find a flat minimum. Intuitively, since the loss surface is smoother along the ridge than in sharp local minima, the surrogate gap h(w) is small near the ridge, and the ascent step in GSAM minimizes h to pushes the trajectory closer to the ridge. More concretely,\u2207f(wt) points to a sharp local solution and deviates from the ridge; in contrast, wadvt is closer to the ridge and \u2207f(wadvt ) is closer to the ridge descent direction than \u2207f(wt). Note that \u2207fGSAMt and \u2207f(wt) always lie at different sides of \u2207fp(wt) by construction (see Fig. 2), hence\u2207fGSAMt pushes the trajectory closer to the ridge than\u2207fp(wt) does. The trajectory of GSAM is like descent along the ridge and tends to find flat minima."
        },
        {
            "heading": "5 THEORETICAL PROPERTIES OF GSAM",
            "text": ""
        },
        {
            "heading": "5.1 CONVERGENCE DURING TRAINING",
            "text": "Theorem 5.1. Consider a non-convex function f(w) with Lipschitz-smooth constant L and lower bound fmin. Suppose we can access a noisy, bounded observation g(t) (||g(t)||2 \u2264 G,\u2200t) of the true gradient \u2207f(wt) at the t-th step. For some constant \u03b1, with learning rate \u03b7t = \u03b70/ \u221a t, and\nperturbation amplitude \u03c1t proportional to the learning rate, e.g., \u03c1t = \u03c10/ \u221a t, we have\n1\nT T\u2211 t=1 E \u2223\u2223\u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 C1 + C2 log T\u221a T ,\n1\nT T\u2211 t=1 E \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 C3 + C4 log T\u221a T\nwhere C1, C2, C3, C4 are some constants. Thm. 5.1 implies both fp and f converge in GSAM at rate O(log T/ \u221a T ) for non-convex stochastic optimization, matching the convergence rate of first-order gradient optimizers like Adam."
        },
        {
            "heading": "5.2 GENERALIZATION OF GSAM",
            "text": "In this section, we show the surrogate gap in GSAM is provably lower than SAM\u2019s, so GSAM is expected to find a smoother minimum with better generalization. Theorem 5.2 (PAC-Bayesian Theorem (McAllester, 2003)). Suppose the training set has m elements drawn i.i.d. from the true distribution, and denote the loss on the training set as f\u0302(w) = 1 m \u2211m i=1 f(w, xi), where we use xi to denote the (input, target) pair of the i-th element. Let w be learned from the training set. Suppose w is drawn from posterior distribution Q. Denote the prior distribution (independent of training) as P , then\nEw\u223cQExf(w, x) \u2264 Ew\u223cQf\u0302(w) + 4 \u221a(\nKL(Q||P) + log 2m a\n) /m with probability at least 1\u2212 a\nCorollary 5.2.1. Suppose perturbation \u03b4 is drawn from distribution \u03b4 \u223c N (0, b2Ik), \u03b4 \u2208 Rk, k is the dimension of w, then with probability at least ( 1\u2212 a )[ 1\u2212 e\u2212 ( \u03c1\u221a 2b \u2212 \u221a k )2]\nEw\u223cQExf(w, x) \u2264 h\u0302+ C + 4 \u221a(\nKL(Q||P) + log 2m a\n) /m (7)\nh\u0302 , max||\u03b4||2\u2264\u03c1 f\u0302(w + \u03b4)\u2212 f\u0302(w) = 1\nm m\u2211 i=1 [ max||\u03b4||2\u2264\u03c1 f(w + \u03b4, xi)\u2212 f(w, xi) ] (8)\nwhere C = f\u0302(w) is the empirical training loss, and h\u0302 is the surrogate gap evaluated on the training set.\nCorollary 5.2.1 implies that minimizing h\u0302 (right hand side of Eq. 7) is expected to achieve a tighter upper bound of the generalization performance (left hand side of Eq. 7). The third term on the right of Eq. 7 is typically hard to analyze and often simplified to L2 regularization (Foret et al., 2020). Note that fp = C + h\u0302 only holds when \u03c1train (the perturbation amplitude specified by users during training) equals \u03c1true (the ground truth value determined by underlying data distribution); when \u03c1train 6= \u03c1true, min(fp, h\u0302) is more effective than min(fp) in terms of minimizing generalization loss. A detailed discussion is in Appendix A.7. Theorem 5.3 (Unlike SAM, GSAM decreases the surrogate gap). Under the assumption in Thm. 5.1, Thm. 5.2 and Corollary 5.2.1, we assume the Hessian has a lower-bound |\u03c3|min on the absolute value of eigenvalue, and the variance of noisy observation g(t) is lower-bounded by c2. The surrogate gap h can be minimized by the ascent step along the orthogonal direction g(t)\u22a5 . During training we minimize the sample estimate of h. We use \u2206h\u0302t to denote the amount that the ascent step in GSAM decreases h\u0302 for the t-th step. Compared to SAM, the proposed method generates a total decrease in surrogate gap \u2211T t=1 \u2206h\u0302t, which is bounded by\n\u03b1c2\u03c120\u03b70|\u03c3|2min G2 \u2264 lim T\u2192\u221e T\u2211 t=1 \u2206h\u0302t \u2264 2.7\u03b1L2\u03b70\u03c120 (9)\nWe provide proof in the appendix. The lower-bound of \u2211T t=1 \u2206h\u0302t indicates that GSAM achieves a provably non-trivial decrease in the surrogate gap. Combined with Corollary 5.2.1, GSAM provably improves the generalization performance over SAM."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "6.1 GSAM IMPROVES TEST PERFORMANCE ON VARIOUS MODEL ARCHITECTURES",
            "text": "We conduct experiments with ResNets (He et al., 2016), Vision Transformers (ViTs) (Dosovitskiy et al., 2020) and MLP-Mixers (Tolstikhin et al., 2021). Following the settings by Chen et al. (2021), we train on the ImageNet-1k (Deng et al., 2009) training set using the Inception-style (Szegedy et al., 2015) pre-processing without extra training data or strong augmentation. For all models, we search for the best learning rate and weight decay for vanilla training, and then use the same values for the experiments with SAM and GSAM. For ResNets, we search for \u03c1 from 0.01 to 0.05 with a stepsize 0.01. For ViTs and Mixers, we search for \u03c1 from 0.05 to 0.6 with a stepsize 0.05. In GSAM, we search for \u03b1 in {0.01, 0.02, 0.03} for ResNets and \u03b1 in {0.1, 0.2, 0.3} for ViTs and Mixers. Considering that each step in SAM and GSAM requires twice the computation of vanilla training, we experiment with the vanilla training for twice the epochs of SAM and GSAM, but we observe no significant improvements from the longer training (Table 5 in appendix). We summarize the best hyper-parameters for each model in Appendix B.\nWe report the performances on ImageNet (Deng et al., 2009), ImageNet-v2 (Recht et al., 2019) and ImageNet-Real (Beyer et al., 2020) in Table 1. GSAM consistently improves over SAM and vanilla training (with SGD or AdamW): on ViT-B/32, GSAM achieves +5.4% improvement over AdamW and +3.2% over SAM in top-1 accuracy; on Mixer-B/32, GSAM achieves +11.1% over AdamW and +1.2% over SAM. We ignore the standard deviation since it is typically negligible (< 0.1%) compared to the improvements. We also test the generalization performance on out-of-distribution data (ImageNet-R and ImageNet-C), and the observation is consistent with that on ImageNet, e.g., +5.1% on ImageNet-R and +5.9% on ImageNet-C for Mixer-B/32."
        },
        {
            "heading": "6.2 GSAM FINDS A MINIMUM WHOSE HESSIAN HAS SMALL DOMINANT EIGENVALUES",
            "text": "Lemma 3.3 indicates that the surrogate gap h is an equivalent measure of the dominant eigenvalue of the Hessian, and minimizing h equivalently searches for a flat minimum. We empirically validate this in Fig. 4. As shown in the left subfigure, for some fixed \u03c1, increasing \u03b1 decreases the dominant value and improves generalization (test accuracy). In the middle subfigure, we plot the dominant\nTable 2: Results (%) of GSAM and min(fp + \u03bbh) on ViT-B/32\nDataset min(fp + \u03bbh) GSAM ImageNet 75.4 76.8 ImageNet-Real 81.1 82.7 ImageNet-v2 60.9 63.0 ImageNet-R 23.9 25.1\nTable 3: Transfer learning results (top-1 accuracy, %)\nViT-B/16 ViT-S/16\nVanilla SAM GSAM Vanilla SAM GSAM Cifar10 98.1 98.6 98.8 97.6 98.2 98.4 Cifar100 87.6 89.1 89.7 85.7 87.6 88.1 Flowers 88.5 91.8 91.2 86.4 91.5 90.3\nPets 91.9 93.1 94.4 90.4 92.9 93.5 mean 91.5 93.2 93.5 90.0 92.6 92.6\neigenvalues estimated by the surrogate gap, \u03c3max \u2248 2h/\u03c12 (Lemma 3.3). In the right subfigure, we directly calculate the dominant eigenvalues using the power-iteration (Mises & Pollaczek-Geiringer, 1929). The estimated dominant eigenvalues (middle) match the real eigenvalues \u03c3max (right) in terms of the trend that \u03c3max decreases with \u03b1 and \u03c1. Note that the surrogate gap h is derived over the whole training set, while the measured eigenvalues are over a subset to save computation. These results show that the ascent step in GSAM minimizes the dominant eigenvalue by minimizing the surrogate loss, validating Thm 5.3."
        },
        {
            "heading": "6.3 COMPARISON WITH METHODS IN THE LITERATURE",
            "text": "Section 6.1 compares GSAM to SAM and vanilla training. In this subsection, we further compare GSAM against Entropy-SGD (Chaudhari et al., 2019) and Adaptive-SAM (ASAM) (Kwon et al., 2021), which are designed to improve generalization. Note that Entropy-SGD uses SGD in the inner Langevin iteration and can be combined with other base optimizers such as AdamW as the outer loop. For Entropy-SGD, we find the hyper-parameter \u201cscope\u201d from 0.0 and 0.9, and search for the inner-loop iteration number between 1 and 14. For ASAM, we search for \u03c1 between 1 and 7 (10\u00d7 larger than in SAM) as recommended by the ASAM authors. Note that the only difference between ASAM and SAM is the derivation of the perturbation, so both can be combined with the proposed ascent step. As shown in Fig. 5, the proposed ascent step increases test accuracy when combined with both SAM and ASAM and outperforms Entropy-SGD and vanilla training."
        },
        {
            "heading": "6.4 ADDITIONAL STUDIES",
            "text": "GSAM outperforms a weighted combination of the perturbed loss and surrogate gap With an example in Fig. 2, we demonstrate that directly minimizing fp(w) + \u03bbh(w) as discussed in Sec. 4.1 is sub-optimal because \u2207h(w) could conflict with \u2207fp(w) and \u2207f(w). We empirically validate this argument on ViT-B/32. We search for \u03bb between 0.0 and 0.5 with a step 0.1 and search for \u03c1 in the same grid as SAM and GSAM. We report the best accuracy of each method. Top-1 accuracy in Table 2 show the superior performance of GSAM, validating our analysis.\nmin(fp, h) vs. min(f, h) GSAM solves min(fp, h) by descent in\u2207fp, decomposing\u2207f onto\u2207fp, and an ascent step in the orthogonal direction to increase f while keep fp intact. Alternatively, we can also optimize min(f, h) by descent in \u2207f , decomposing \u2207fp onto \u2207f , and a descent step in the orthogonal direction to decrease fp while keep f intact. The two GSAM variations perform similarly (see Fig. 6, right). We choose min(fp, h) mainly to make the minimal change to SAM.\nGSAM benefits transfer learning Using weights trained on ImageNet-1k, we finetune models with SGD on downstream tasks including the CIFAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford-\nflowers (Nilsback & Zisserman, 2008) and Oxford-IITPets (Parkhi et al., 2012). Results in Table 3 shows that GSAM leads to better transfer performance than vanilla training and SAM.\nGSAM remains effective under various data augmentations We plot the top-1 accuracy of a ViT-B/32 model under various Mixup (Zhang et al., 2017b) augmentations in Fig. 6 (left subfigure). Under different augmentations, GSAM consistently outperforms SAM and vanilla training.\nGSAM is compatible with different base optimizers GSAM is generic and applicable to various base optimizers. We compare vanilla training, SAM and GSAM using AdamW (Loshchilov & Hutter, 2017) and AdaBelief (Zhuang et al., 2020) with default hyper-parameters. Fig. 6 (middle subfigure) shows that GSAM performs the best, and SAM improves over vanilla training."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "We propose the surrogate gap as an equivalent measure of sharpness which is easy to compute and feasible to optimize. We propose the GSAM method, which improves the generalization over SAM at negligible computation cost. We show the convergence and provably better generalization of GSAM compared to SAM, and validate the superior performance of GSAM on various models."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "We would like to thank Xiangning Chen (UCLA) and Hossein Mobahi (Google) for discussions, Yi Tay (Google) for help with datasets, and Yeqing Li, Xianzhi Du, and Shawn Wang (Google) for help with TensorFlow implementation."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "This paper focuses on the development of optimization methodologies and can be applied to the training of different deep neural networks for a wide range of applications. Therefore, the ethical impact of our work would primarily be determined by the specific models that are trained using our new optimization strategy."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We provide the detailed proof of theoretical results in Appendix A and provide the data preprocessing and hyper-parameter settings in Appendix B. Together with the references to existing works and public codebases, we believe the paper contains sufficient details to ensure reproducibility. We plan to release the models trained by using GSAM upon publication."
        },
        {
            "heading": "A PROOFS",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF LEMMA. 3.1",
            "text": "Suppose \u03c1 is small, perform Taylor expansion around the local minima w, we have:\nf(w + \u03b4) = f(w) +\u2207f(w)>\u03b4 + 1 2 \u03b4>H\u03b4 +O(||\u03b4||3) (10)\nwhereH is the Hessian, and is positive semidefinite at a local minima. At a local minima,\u2207f(w) = 0, hence we have\nf(w + \u03b4) = f(w) + 1\n2 \u03b4>H\u03b4 +O(||\u03b4||3) (11)\nand\nfp(w) = max||\u03b4||\u2264\u03c1 f(w + \u03b4) = f(w) + 1\n2 \u03c12\u03c3max(H) +O(||\u03b4||3) (12)\nwhere \u03c3max is the dominate eigenvalue (eigenvalue with the largest absolute value). Now consider two local minima w1 and w2 with dominate eigenvalue \u03c31 and \u03c32 respectively, we have\nfp(w1) \u2248 f(w1) + 1\n2 \u03c12\u03c31 fp(w2) \u2248 f(w2) +\n1 2 \u03c12\u03c32\nWe have fp(w1) > fp(w2) 6=\u21d2 \u03c31 > \u03c32 and \u03c31 > \u03c32 6=\u21d2 fp(w1) > fp(w2) because the relation between f(w1) and f(w2) is undetermined."
        },
        {
            "heading": "A.2 PROOF OF LEMMA. 3.2",
            "text": "Since \u03c1 is small, we can perform Taylor expansion around w,\nh(w) = f(w + \u03b4)\u2212 f(w) = \u03b4>\u2207f(w) +O(\u03c12) = \u03c1||\u2207f(w)||2 +O(\u03c12) > 0 (13)\nwhere the last line is because \u03b4 is approximated as \u03b4 = \u03c1 \u2207f(w)||\u2207f(w)||2+ , hence has the same direction as\u2207f(w)."
        },
        {
            "heading": "A.3 PROOF OF LEMMA. 3.3",
            "text": "Since \u03c1 is small, we can approximate f(w) with a quadratic model around a local minima w:\nf(w + \u03b4) = f(w) + 1\n2 \u03b4>H\u03b4 +O(\u03c13)\nwhere H is the Hessian at w, assumed to be positive semidefinite at local minima. Normalize \u03b4 such that ||\u03b4||2 = \u03c1, Hence we have:\nh(w) = fp(w)\u2212 f(w) = max||\u03b4||2\u2264\u03c1 f(w + \u03b4)\u2212 f(w) = 1\n2 \u03c3max\u03c1\n2 +O(\u03c13) (14)\nwhere \u03c3max is the dominate eigenvalue of the hessian H , and first order term is 0 because the gradient is 0 at local minima. Therefore, we have \u03c3max \u2248 2h(w)/\u03c12."
        },
        {
            "heading": "A.4 PROOF OF THM. 5.1",
            "text": "For simplicity we consider the base optimizer is SGD. For other optimizers such as Adam, we can derive similar results by applying standard proof techniques in the literature to our proof.\nSTEP 1: CONVERGENCE W.R.T FUNCTION fp(w)\nFor simplicity of notation, we denote the update at step t as\ndt = \u2212\u03b7tg(t)p + \u03b7t\u03b1g (t) \u22a5 (15)\nBy L\u2212smoothness of f and the definition of fp(wt) = f(wadvt ), and definition of dt = wt+1 \u2212 wt and wadvt = wt + \u03b4t we have\nfp(wt+1) = f(w adv t+1) \u2264 f(wadvt ) + \u3008\u2207f(wadvt ), wadvt+1 \u2212 wadvt \u3009+\nL\n2 \u2223\u2223\u2223\u2223\u2223\u2223wadvt+1 \u2212 wadvt \u2223\u2223\u2223\u2223\u2223\u22232 (16) = f(wadvt ) + \u3008\u2207f(wadvt ), wt+1 + \u03b4t+1 \u2212 wt \u2212 \u03b4t\u3009\n+ L\n2 \u2223\u2223\u2223\u2223\u2223\u2223wt+1 + \u03b4t+1 \u2212 wt \u2212 \u03b4t\u2223\u2223\u2223\u2223\u2223\u22232 (17) \u2264 f(wadvt ) + \u3008\u2207f(wadvt ), dt\u3009+ L\n\u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 (18) + \u3008\u2207f(wadvt ), \u03b4t+1 \u2212 \u03b4t\u3009+ L\n\u2223\u2223\u2223\u2223\u2223\u2223\u03b4t+1 \u2212 \u03b4t\u2223\u2223\u2223\u2223\u2223\u22232 (19) STEP 1.0: BOUND EQ. 18\nWe first bound Eq. 18. Take expectation conditioned on observation up to step t (for simplicity of notation, we use E short for Ex to denote expectation over all possible data points) conditioned on observations up to step t, also by definition of dt, we have\nEfp(wt+1)\u2212 fp(wt) \u2264 \u2212\u03b7t\u3008\u2207fp(wt),Eg(t)p \u3009+ \u03b1\u03b7t\u3008\u2207fp(wt),Eg (t) \u22a5 \u3009 + L\u03b72tE \u2223\u2223\u2223\u2223\u2223\u2223\u2212 g(t)p + \u03b1g(t)\u22a5 \u2223\u2223\u2223\u2223\u2223\u22232\n2 (20) \u2264 \u2212\u03b7tE \u2223\u2223\u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223\u2223\u22232\n2 + 0 + (\u03b1+ 1)2G2\u03b72t (21)(\nSince Eg(t)\u22a5 is orthogonal to \u2207fp(wt) by construction, ||g(t)|| \u2264 G by assumption )\nSTEP 1.1: BOUND EQ. 19\nBy definition of \u03b4t, we have\n\u03b4t = \u03c1t g(t)\n||g(t)||+ (22)\n\u03b4t+1 = \u03c1t+1 g(t+1)\n||g(t+1)||+ (23)\nwhere g(t) is the gradient of f at wt evaluated with a noisy data sample. When learning rate \u03b7t is small, the update in weight dt is small, and expected gradient is\n\u2207f(wt+1) = \u2207f(wt + dt) = \u2207f(wt) +Hdt +O(||dt||2) (24) where H is the Hessian at wt. Therefore, we have\nE\u3008\u2207f(wadvt ), \u03b4t+1 \u2212 \u03b4t\u3009 = \u3008\u2207f(wadvt ), \u03c1tE g(t)\n||g(t)||+ \u2212 \u03c1t+1E\ng(t+1)\n||g(t+1)||+ \u3009 (25)\n\u2264 ||\u2207f(wadvt )||\u03c1t \u2223\u2223\u2223\u2223\u2223\u2223E g(t)||g(t)||+ \u2212 E g(t+1)||g(t+1)||+ \u2223\u2223\u2223\u2223\u2223\u2223 (26)\n\u2264 ||\u2207f(wadvt )||\u03c1t\u03c6t (27) where the first inequality is due to (1) \u03c1t is monotonically decreasing with t, and (2) triangle inequality that \u3008a, b\u3009 \u2264 ||a|| \u00b7 ||b||. \u03c6t is the angle between the unit vector in the direction of \u2207f(wt)\nand\u2207f(wt+1). The second inequality comes from that (1) \u2223\u2223\u2223\u2223\u2223\u2223 g||g||+ \u2223\u2223\u2223\u2223\u2223\u2223 < 1 strictly, so we can replace \u03b4t in Eq. 25 with a unit vector in corresponding directions multiplied by \u03c1t and get the upper bound, (2) the norm of difference in unit vectors can be upper bounded by the arc length on a unit circle.\nWhen learning rate \u03b7t and update stepsize dt is small, \u03c6t is also small. Using the limit that\ntanx = x+O(x2), sinx = x+O(x2), x\u2192 0 We have:\ntan\u03c6t = ||\u2207f(wt+1)\u2212\u2207f(wt)||\n||\u2207f(wt)|| +O(\u03c62t ) (28)\n= ||Hdt +O(||dt||2)|| ||\u2207f(wt)|| +O(\u03c62t ) (29)\n\u2264 \u03b7tL(1 + \u03b1) (30) where the last inequality is due to (1) max eigenvalue of H is upper bounded by L because f is L\u2212smooth, (2) ||dt|| = ||\u03b7t(g\u2016 + \u03b1g\u22a5)|| and Egt = \u2207f(wt).\nPlug into Eq. 27, also note that the perturbation amplitude \u03c1t is small so wt is close to wadvt , then we have E\u3008\u2207f(wadvt ), \u03b4t+1 \u2212 \u03b4t\u3009 \u2264 L(1 + \u03b1)G\u03c1t\u03b7t (31) Similarly, we have\nE \u2223\u2223\u2223\u2223\u2223\u2223\u03b4t+1 \u2212 \u03b4t\u2223\u2223\u2223\u2223\u2223\u22232 \u2264 \u03c12tE\u2223\u2223\u2223\u2223\u2223\u2223 g(t)||g(t)||+ \u2212 g(t+1)||g(t+1)||+ \u2223\u2223\u2223\u2223\u2223\u22232 (32)\n\u2264 \u03c12t\u03c62t (33) \u2264 \u03c12t\u03b72tL2(1 + \u03b1)2 (34)\nSTEP 1.2: TOTAL BOUND\nReuse results from Eq. 21 (replace Lp with 2L) and plug into Eq. 18, and plug Eq. 31 and Eq. 34 into Eq. 19, we have\nEfp(wt+1)\u2212 fp(wt) \u2264 \u2212\u03b7tE \u2223\u2223\u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223\u2223\u22232\n2 +\n2L(\u03b1+ 1)2\n2 G2\u03b72t\n+ L(1 + \u03b1)G\u03c1t\u03b7t + 2L3(1 + \u03b1)2\n2 \u03b72t \u03c1 2 t (35)\nPerform telescope sum, we have\nEfp(wT )\u2212 fp(w0) \u2264 \u2212 T\u2211 t=1 \u03b7tE||\u2207fp(wt)||2 + [ L(1 + \u03b1)2G2\u03b720 + L(1 + \u03b1)G\u03c10\u03b70 ] T\u2211 t=1 1 t\n+ L3(1 + \u03b1)2\u03b720\u03c1 2 0 T\u2211 t=1 1 t2 (36)\nHence\n\u03b7T T\u2211 t=1 E||\u2207fp(wt)||2 \u2264 T\u2211 t=1 \u03b7tE||\u2207fp(wt)||2 \u2264 fp(w0)\u2212 Efp(wT ) +D log T + \u03c02E 6 (37)\nwhere D = L(1 + \u03b1)2G2\u03b720 + L(1 + \u03b1)G\u03c10\u03b70, E = L 3(1 + \u03b1)2\u03b720\u03c1 2 0 (38) Note that \u03b7T = \u03b70\u221aT , we have\n1\nT T\u2211 t=1 E||\u2207fp(wt)||2 \u2264 fp(w0)\u2212 fmin + \u03c02E/6 \u03b70 1\u221a T + D \u03b70 log T\u221a T\n(39)\nwhich implies that GSAM enables fp to converge at a rate of O(log T/ \u221a T ), and all the constants here are well-bounded.\nSTEP 2: CONVERGENCE W.R.T. FUNCTION f(w)\nWe prove the risk for f(w) convergences for non-convex stochastic optimization case using SGD. Denote the update at step t as\ndt = \u2212\u03b7tg(t)p + \u03b1\u03b7tg (t) \u22a5 (40)\nBy smoothness of f , we have\nf(wt+1) \u2264 f(wt) + \u3008\u2207f(wt), dt\u3009+ L\n2 \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2\n(41)\n= f(wt) + \u3008\u2207f(wt),\u2212\u03b7tg(t)p + \u03b1\u03b7tg (t) \u22a5 \u3009+\nL\n2 \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2\n(42)\nFor simplicity, we introduce a scalar \u03b2t such that \u2207f\u2016(wt) = \u03b2t\u2207fp(wt) (43)\nwhere\u2207f\u2016(wt) is the projection of\u2207f(wt) onto\u2207fp(wt). When perturbation amplitude \u03c1 is small, we expect \u03b2t to be very close to 1.\nTake expectation conditioned on observations up to step t for both sides of Eq. 42, we have:\nEf(wt+1) \u2264 f(wt) + \u2329 \u2207f(wt),\u2212\n\u03b7t \u03b2t\n( \u2207f(wt)\u2212\u2207f\u22a5(wt) ) + \u03b1\u03b7tEg(t)\u22a5 \u232a + L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2 (44)\n= f(wt)\u2212 \u03b7t \u03b2t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2t + \u03b1 ) \u03b7t \u2329 \u2207f(wt),\u2207f\u22a5(wt) \u232a + L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2 (45)\n= f(wt)\u2212 \u03b7t \u03b2t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2t + \u03b1 ) \u03b7t \u2329 \u2207f(wt),\u2207f(wt) sin \u03b8t \u232a + L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2\n(46)( \u03b8t is the angle between\u2207fp(wt) and \u2207f(wt) ) = f(wt)\u2212\n\u03b7t \u03b2t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2t + \u03b1 ) \u03b7t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 (| tan \u03b8t|+O(\u03b82t )) + L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2\n(47)( sinx = x+O(x2), tanx = x+O(x2) when x\u2192 0. ) Also note when perturbation amplitude \u03c1t is small, we have\n\u2207fp(wt) = \u2207f(wt + \u03b4t) = \u2207f(wt) + \u03c1t\n||\u2207f(wt)||2 + H(wt)\u2207f(wt) +O(\u03c12t ) (48)\nwhere \u03b4t = \u03c1t \u2207f(wt) ||\u2207f(wt)||2 by definition, H(wt) is the Hessian. Hence we have\n| tan \u03b8t| \u2264 ||\u2207fp(wt)\u2212\u2207f(wt)|| ||\u2207f(wt)|| \u2264 \u03c1tL ||\u2207f(wt)||\n(49)\nwhere L is the Lipschitz constant of f , and L\u2212smoothness of f indicates the maximum absolute eigenvalue of H is upper bounded by L. Plug Eq. 49 into Eq. 47, we have\nEf(wt+1) \u2264 f(wt)\u2212 \u03b7t \u03b2t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2t + \u03b1 ) \u03b7t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 | tan \u03b8t|+ L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2 (50)\n\u2264 f(wt)\u2212 \u03b7t \u03b2t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2t + \u03b1 ) L\u03c1t\u03b7t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u2223 2 + L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232 2 (51)\n\u2264 f(wt)\u2212 \u03b7t \u03b2t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2t + \u03b1 ) L\u03c1t\u03b7tG+ L 2 E \u2223\u2223\u2223\u2223\u2223\u2223dt\u2223\u2223\u2223\u2223\u2223\u22232\n2 (52)( Assume gradient has bounded norm G. ) (53)\n\u2264 f(wt)\u2212 \u03b7t\n\u03b2max\n\u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + ( 1 \u03b2min + \u03b1 ) L\u03c1t\u03b7tG+ L\n2 E(\u03b1+ 1)2G2\u03b72t (54)(\n\u03b2t is close to 1 assuming \u03c1 is small, hence it\u2019s natural to assume 0 < \u03b2min \u2264 \u03b2t \u2264 \u03b2max )\nRe-arranging above formula, we have \u03b7t\n\u03b2max\n\u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 f(wt)\u2212 Ef(wt+1) + ( 1 \u03b2min + \u03b1 ) LG\u03b7t\u03c1t + L 2 (\u03b1+ 1)2G2\u03b72t (55)\nperform telescope sum and taking expectations on each step, we have\n1\n\u03b2max T\u2211 t=1 \u03b7t \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 f(w0)\u2212 Ef(wT ) + ( 1 \u03b2min + \u03b1 ) LG T\u2211 t=1 \u03b7t\u03c1t + L 2 (\u03b1+ 1)2G2 T\u2211 t=1 \u03b72t (56) Take the schedule to be \u03b7t = \u03b70\u221at and \u03c1t = \u03c10\u221a t , then we have\n\u03b70 \u03b2max 1\u221a T T\u2211 t=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 LHS (57)\n\u2264 RHS (58)\n\u2264 f(w0)\u2212 fmin + ( 1 \u03b2min + \u03b1 ) LG\u03b70\u03c10 T\u2211 t=1 1 t + L 2 (\u03b1+ 1)2G2\u03b720 T\u2211 t=1 1 t\n(59) \u2264 f(w0)\u2212 fmin + ( 1 \u03b2min + \u03b1 ) LG\u03b70\u03c10(1 + log T )\n+ L\n2 (\u03b1+ 1)2G2\u03b720(1 + log T ) (60)\nHence 1\nT T\u2211 t=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 C3\u221a T + C4 log T\u221a T\n(61)\nwhere C1, C4 are some constants. This implies the convergence rate w.r.t f(w) is O(log T/ \u221a T ).\nSTEP 3: CONVERGENCE W.R.T. SURROGATE GAP h(w)\nNote that we have proved convergence for fp(w) in step 1, and convergence for f(w) in step 3. Also note that \u2223\u2223\u2223\u2223\u2223\u2223\u2207h(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 = \u2223\u2223\u2223\u2223\u2223\u2223\u2207fp(wt)\u2212\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 2 \u2223\u2223\u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + 2 \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 (62)\nHence 1\nT T\u2211 t=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2207h(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 \u2264 2 T T\u2211 t=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2 + 2 T T\u2211 t=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223\u2223\u22232 2\n(63)\nalso converges at rate O(log T/ \u221a T ) because each item in the RHS converges at rate O(log T \u221a T )."
        },
        {
            "heading": "A.5 PROOF OF COROLLARY. 5.2.1",
            "text": "Using the results from Thm. 5.2, with probability at least 1\u2212 a, we have\nEw\u223cQExf(w, x) \u2264 Ew\u223cQf\u0302(w) + 4\n\u221a KL(Q||P) + log 2ma\nm (64)\nAssume \u03b4 \u223c N (0, b2Ik) where k is the dimension of model parameters, hence \u03b42 (element-wise square) follows a a Chi-square distribution. By Lemma.1 in Laurent & Massart (2000), we have\nP ( ||\u03b4||22 \u2212 kb2 \u2265 2b2 \u221a kt+ 2tb2 ) \u2264 exp(\u2212t) (65)\nhence with probability at least 1\u2212 1/ \u221a n, we have\n||\u03b4||22 \u2264 b2 ( 2 log \u221a n+ k + 2 \u221a k log \u221a n ) \u2264 2b2k ( 1 + \u221a log \u221a n\nk\n)2 \u2264 \u03c12 (66)\nTherefore, with probability at least 1\u2212 1/ \u221a n = 1\u2212 exp ( \u2212 ( \u03c1\u221a 2b \u2212 \u221a k )2)\nE\u03b4 f\u0302(w + \u03b4) \u2264 max||\u03b4||2\u2264\u03c1 f\u0302(w + \u03b4) (67)\nCombine Eq. 65 and Eq. 67, subtract the same constant C on both sides, and under the same assumption as in (Foret et al., 2020) that Ew\u223cQExf(w, x) \u2264 E\u03b4\u223cN (0,b2Ik)Ew\u223cQExf(w + \u03b4, x)we finish the proof."
        },
        {
            "heading": "A.6 PROOF OF THM. 5.3",
            "text": "STEP 1: A SUFFICIENT CONDITION THAT THE LOSS GAP IS EXPECTED TO DECREASE FOR"
        },
        {
            "heading": "EACH STEP",
            "text": "Take Taylor expansion, then the expected change of loss gap caused by descent step is\nE\u3008\u2207fp(wt)\u2212\u2207f(wt),\u2212\u03b7t\u2207fp(wt)\u3009 (68)( where Eg\u22a5 = \u2207f\u22a5(wt) ) = \u03b7t [ \u2212 \u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u222322 + \u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u22232\u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u22232 cos \u03b8t ] (69)\nwhere \u03b8t is the angle between vector \u2207fp(wt) and \u2207f(wt). The expected change of loss gap caused by ascent step is\nE\u3008\u2207fp(wt)\u2212\u2207f(wt), \u03b1\u03b7t\u2207f\u22a5(wt)\u3009 = \u2212\u03b1\u03b7t \u2223\u2223\u2223\u2223\u2207f\u22a5(wt)\u2223\u2223\u2223\u222322 < 0 (70)\nAbove results demonstrate that ascent step decreases the loss gap, while descent step might increase the loss gap. A sufficient (but not necessary) condition for E\u3008\u2207h(wt), dt\u3009 \u2264 0 requires \u03b1 to be large or | \u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u22232 cos \u03b8t \u2264 \u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223. In practice, the perturbation amplitude \u03c1 is small and we can\nassume \u03b8t is close to 0 and \u2223\u2223\u2223\u2223\u2207fp(wt)\u2223\u2223\u2223\u2223 is close to \u2223\u2223\u2223\u2223\u2207f(wt)\u2223\u2223\u2223\u2223, we can also set the parameter \u03b1 to be large in order to decrease the loss gap.\nSTEP 2: UPPER AND LOWER BOUND OF DECREASE IN LOSS GAP (BY THE ASCENT STEP IN ORTHOGONAL GRADIENT DIRECTION) COMPARED TO SAM.\nNext we give an estimate of the decrease in h\u0302 caused by our ascent step. We refer to Eq. 69 and Eq. 70 to analyze the change in loss gap caused by the descent and ascent (orthogonally) respectively. It can be seen that gradient descent step might not decrease loss gap, in fact they often increase loss gap in practice; while the ascent step is guaranteed to decrease the loss gap.\nThe decrease in loss gap is: \u2206h\u0302t = \u2212\u3008\u2207f\u0302p(wt)\u2212\u2207f\u0302(wt), \u03b1\u03b7t\u2207f\u0302\u22a5(wt)\u3009 = \u03b1\u03b7t \u2223\u2223\u2223\u2223\u2207f\u0302\u22a5(wt)\u2223\u2223\u2223\u222322 (71)\n= \u03b1\u03b7t \u2223\u2223\u2223\u2223\u2207f\u0302(wt)\u2223\u2223\u2223\u222322| tan \u03b8t|2 (72)\nT\u2211 t=1 \u2206h\u0302t \u2264 T\u2211 t=1\n\u03b1L2\u03b7t\u03c1 2 t (73)(\nBy Eq. 49 )\n(74)\n\u2264 T\u2211 t=1 \u03b1L2\u03b70\u03c1 2 0 1 t3/2 (75)\n\u2264 2.7\u03b1L2\u03b70\u03c120 (76)\nHence we derive an upper bound for \u2211T t=1 \u2206h\u0302t.\nNext we derive a lower bound for \u2211T t=1 \u2206h\u0302t Note that when \u03c1t is small, by Taylor expansion\n\u2207f\u0302p(wt) = \u2207f\u0302(wt + \u03b4t) = \u2207f\u0302(wt) + \u03c1t\n||\u2207f\u0302(wt)|| H\u0302(wt)\u2207f\u0302(wt) +O(\u03c12t ) (77)\nwhere H\u0302(wt) is the Hessian evaluated on training samples. Also when \u03c1t is small, the angle \u03b8t between \u2207f\u0302p(wt) and \u2207f\u0302(wt) is small, by the limit that\ntanx = x+O(x2), x\u2192 0 sinx = x+O(x2), x\u2192 0\nWe have | tan \u03b8t| = | sin \u03b8t|+O(\u03b82t ) = |\u03b8t|+O(\u03b82t ) Omitting high order term, we have\n| tan \u03b8t| \u2248 |\u03b8t| = ||\u2207f\u0302p(wt)\u2212\u2207f\u0302(wt)|| ||f\u0302(wt)|| = ||\u03c1tH\u0302(wt) +O(\u03c12t )|| ||\u2207f\u0302(wt)|| \u2265 \u03c1t|\u03c3|min G (78)\nwhere G is the upper-bound on norm of gradient, |\u03c3|min is the minimum absolute eigenvalue of the Hessian. The intuition is that as perturbation amplitude decreases, the angle \u03b8t decreases at a similar rate, though the scale constant might be different. Hence we have\nT\u2211 t=1 \u2206h\u0302t = T\u2211 t=1 \u03b1\u03b7t \u2223\u2223\u2223\u2223\u2207f\u0302(wt)\u2223\u2223\u2223\u222322| tan \u03b8t|2 +O(\u03b84t ) (79)\n\u2265 T\u2211 t=1 \u03b1\u03b7tc 2 (\u03c1t|\u03c3|min G )2 (80)\n= \u03b1c2\u03c120\u03b70|\u03c3|2min\nG2\nT\u2211 t=1 1 t3/2 (81)\n\u2265 \u03b1c 2\u03c120\u03b70|\u03c3|2min\nG2 (82)\nwhere c2 is the lower bound of ||\u2207f\u0302 ||2 (e.g. due to noise in data and gradient observation). Results above indicate that the decrease in loss gap caused by the ascent step is non-trivial, hence our proposed method efficiently improves generalization compared with SAM."
        },
        {
            "heading": "A.7 DISCUSSION ON COROLLARY 5.2.1",
            "text": "The comment \u201c\u2018The corollary gives a bound on the risk in terms of the perturbed training loss if one removes C from both sides\u201d\u2019 is correct. But there is a misunderstanding in the statement \u201c\u2018the perturbed training loss is small then the model has a small risk\u201d\u2019: it\u2019s only true when \u03c1train for training equals its real value \u03c1true determined by the data distribution; in practice, we never know \u03c1true. In the following we show that the minimization of both h and fp is better than simply minimizing fp when \u03c1true 6= \u03c1train. 1. First, we re-write the conclusion of Corollary 5.2.1 as\nEwExf(w, x) \u2264 fp +R = C + h\u0302+R = C + \u03c12\u03c3/2 +R+O(\u03c13)\nwith probability (1\u2212 a)[1\u2212 e\u2212( \u03c1\u221a 2b \u2212 \u221a k)2 ]\nwhere R is the regularization term, C is the training loss, \u03c3 is the dominant eigenvalue of Hessian. As in lemma 3.3, we perform Taylor-expansion and can ignore the high-order termO(\u03c13). We focus on\nfp = C + h\u0302 = C + \u03c1 2\u03c3/2\n2. When \u03c1true 6= \u03c1train, minimizing h achieves a lower risk than only minimizing fp. (1) Note that after training, C (training loss) is fixed, but h could vary with \u03c1 (e.g. when training on dataset A and testing on an unrelated dataset B, the training loss remains unchanged, but the risk would be huge and a large \u03c1 is required for a valid bound). (2) With an example, we show a low fp is insufficient for generalization, and a low \u03c3 is necessary:\nA Suppose we use \u03c1train for training, and consider two solutions with C1, \u03c31 (SAM) and C2, \u03c32 (GSAM). Suppose they have the same fp during training for some \u03c1train, so\nfp1 = C1 + \u03c31/2\u00d7 \u03c12train = C2 + \u03c32/2\u00d7 \u03c12train = fp2 Suppose C1 < C2 so \u03c31 > \u03c32.\nB When \u03c1true > \u03c1train, we have\nrisk bound 1 = C1 +\u03c31/2\u00d7 \u03c12true +R > risk bound 2 = C2 +\u03c32/2\u00d7 \u03c12true +R\nThis implies that a small \u03c3 helps generalization, but only a low fp1 (caused by a low C1 and high \u03c31) is insufficient for a good generalization.\nC Note that \u03c1train is fixed during training, so minimizing htrain during training is equivalently minimizing \u03c3 by Lemma 3.3\n3. Why we are often unlucky to have \u03c1true > \u03c1train (1) First, the test sets are almost surely outside the convex hull of the training set because \u201c\u2018interpolation almost surely never occurs in high-dimensional (> 100) cases\u201d\u2019 Balestriero et al. (2021). As a result, the variability of (train + test) sets is almost surely larger than the variability of (train) set. Since \u03c1 increases with data variability (see point 4 below), we have \u03c1true > \u03c1train set almost surely. (2) Second, we don\u2019t know the value of \u03c1true and can only guess it. In practice, we often guess a small value because training often diverges with large \u03c1 (as observed in Foret et al. (2020); Chen et al. (2021)).\n4. Why \u03c1 increases with data variability. In Corollary 5.2.1, we assume weight perturbation \u03b4 \u223c N (0, b2Ik). The meaning of b is the following. If we can randomly sample a fixed number of samples from the underlying distribution, then training the model from scratch (with a fixed seed for random initialization) gives rise to a set of weights. Repeating this process, we get many sets of weights, and their standard deviation is b. Since the number of training samples is limited and fixed, the more variability in data, the more variability in weights, and the larger b. Note that Corollary stated that the bound holds with probability proportional to [1 \u2212 e\u2212( \u03c1\u221a 2b \u2212 \u221a k)2\n]. In order for the result to hold with a fixed probability, \u03c1 must stay proportional to b, hence \u03c1 also increases with the variability of data."
        },
        {
            "heading": "B EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 TRAINING DETAILS",
            "text": "For ViT and Mixer, we search the learning rate in {1e-3, 3e-3, 1e-2, 3e-3}, and search weight decay in {0.003, 0.03, 0.3}. For ResNet, we search the learning rate in {1.6, 0.16, 0.016}, and search the weight decay in {0.001, 0.01,0.1}. For ViT and Mixer, we use the AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.999; for ResNet we use SGD with momentum= 0.9. We train ResNets for 90 epochs, and train ViTs and Mixers for 300 epochs following the settings in (Chen et al., 2021) and (Dosovitskiy et al., 2020). Considering that SAM and GSAM uses twice the computation of vanilla training for each step, for vanilla training we try 2\u00d7 longer training, and does not find significant improvement as in Table. 5.\nWe first search the optimal learning rate and weight decay for vanilla training, and keep these two hyper-parameters fixed for SAM and GSAM. For ViT and Mixer, we search \u03c1 in {0.1, 0.2, 0.3, 0.4, 0.5, 0.6} for SAM and GSAM; for ResNet, we search \u03c1 from 0.01 to 0.05 with a stepsize 0.01. For ASAM, we amplify \u03c1 by 10\u00d7 compared to SAM, as recommended by Kwon et al. (2021). For GSAM, we search \u03b1 in {0.1, 0.2, 0.3} throughout the paper. We report the best configuration of each individual model in Table. 4."
        },
        {
            "heading": "B.2 TRANSFER LEARNING EXPERIMENTS",
            "text": "Using weights trained on ImageNet-1k, we finetune models with SGD on downstream tasks including the CIFAR10/CIFAR100 (Krizhevsky et al., 2009), Oxford-flowers (Nilsback & Zisserman, 2008) and Oxford-IITPets (Parkhi et al., 2012). For all experiments, we use the SGD optimizer with no weight decay under a linear learning rate schedule and gradient clipping with global norm 1. We search the maximum learning rate in {0.001, 0.003, 0.01, 0.03}. On Cifar datasets, we train models for 10k steps with a warmup step of 500; on Oxford datasets, we train models for 500 steps with a wamup step of 100."
        },
        {
            "heading": "B.3 EXPERIMENTAL SETUP WITH ABLATION STUDIES ON DATA AUGMENTATION",
            "text": "We follow the settings in (Tolstikhin et al., 2021) to perform ablation studies on data augmentation. In the left subfigure of Fig. 6, \u201cLight\u201d refers to Inception-style data augmentation with random flip and crop of images, \u201cMedium\u201d refers to the mixup augmentation with probability 0.2 and RandAug magnitude 10; \u201cStrong\u201d refers to the mixup augmentation with probability 0.2 and RandAug magnitude 15."
        },
        {
            "heading": "C ABLATION STUDIES AND DISCUSSIONS",
            "text": "C.1 INFLUENCE OF \u03c1 AND \u03b1\nWe plot the performance of a ViT-B/32 model varying with \u03c1 (Fig. 7a) and \u03b1 (Fig. 7b). We empirically validate that fine-tuning \u03c1 in SAM can not achieve comparable performance with GSAM, as\nshown in Fig. 7a. Considering that GSAM has one more parameter \u03b1, we plot the accuracy varying with \u03b1 in Fig. 7b, and show that GSAM consistently outperforms SAM and vanilla training.\nC.2 CONSTANT \u03c1 V.S. DECAYED \u03c1t SCHEDULE\nNote that Thm. 5.1 assumes \u03c1t to decay with t in order to prove the convergence, while SAM uses a constant \u03c1 during training. To eliminate the influence of \u03c1t schedule, we conduct ablation study as in Table. 6. The ascent step in GSAM can be applied to both constant \u03c1 or a decayed \u03c1t schedule, and improves accuracy for both cases. Without ascent step, constant \u03c1 and decayed \u03c1t achieve similar performance. Results in Table. 6 implies that the ascent step in GSAM is the main reason for improvement of generalization performance.\nC.3 VISUALIZE THE TRAINING PROCESS\nIn the proof of Thm. 5.3, our analysis relies on assumption that \u03b8t is small. We empirically validated this assumption by plotting cos \u03b8t in Fig. 8, where \u03b8t is the angle between \u2207f(wt) and \u2207fp(wt). Note that the cosine value is calculated in the parameter space of dimension 8.8 \u00d7 107, and in high-dimensional space two random vectors are highly likely to be perpendicular. In Fig. 8 the cosine value is always above 0.9, indicating that\u2207f(wt) and\u2207fp(wt) point to very close directions considering the high dimension of parameters. This empirically validates our assumption that \u03b8t is small during training.\nWe also plot the surrogate gap during training in Fig. 9. As \u03b1 increases, the surrogate gap decreases, validating that the ascent step in GSAM efficiently minimizes the surrogate gap. Furthermore, the surrogate gap increases with training steps for any fixed \u03b1, indicating that the training process gradually falls into local minimum in order to minimize the training loss."
        },
        {
            "heading": "D RELATED WORKS",
            "text": "Besides SAM and ASAM, other methods were proposed in the literature to improve generalization: Lin et al. (2020) proposed extrapolation of gradient, Xie et al. (2021) proposed to manipulate the noise in gradient, and Damian et al. (2021) proved label noise improves generalization, Yue et al. (2020) proposed to adjust learning rate according to sharpness, and Zheng et al. (2021) proposed model perturbation with similar idea to SAM. Izmailov et al. (2018) proposed averaging weights to improve generalization, and Heo et al. (2020) restricted the norm of updated weights to improve generalization. Many of aforementioned methods can be combined with GSAM to further improve generalization.\nBesides modified training schemes, there are other two types of techniques to improve generalization: data augmentation and model regularization. Data augmentation typically generates new data from training samples; besides standard data augmentation such as flipping or rotation of images, recent data augmentations include label smoothing (Mu\u0308ller et al., 2019) and mixup (Mu\u0308ller et al., 2019) which trains on convex combinations of both inputs and labels, automatically learned augmentation (Cubuk et al., 2018), and cutout (DeVries & Taylor, 2017) which randomly masks out parts of an image. Model regularization typically applies auxiliary losses besides the training loss such as weight decay (Loshchilov & Hutter, 2017), other methods randomly modify the model architecture during training, such as dropout (Srivastava et al., 2014) and shake-shake regularization (Gastaldi, 2017). Note that the data augmentation and model regularization literature mentioned here typically train with the standard back-propagation (Rumelhart et al., 1985) and first-order gradient optimizers, and both techniques can be combined with GSAM.\nBesides SGD, Adam and AdaBelief, GSAM can be combined with other first-order gradient optimizers, such as AdaBound (Luo et al., 2019), RAdam (Liu et al., 2019), Yogi (Zaheer et al., 2018), AdaGrad (Duchi et al., 2011), AMSGrad (Reddi et al., 2019) and AdaDelta (Zeiler, 2012)."
        }
    ],
    "title": "SURROGATE GAP MINIMIZATION IMPROVES SHARPNESS-AWARE TRAINING",
    "year": 2022
}