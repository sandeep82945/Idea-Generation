{
    "abstractText": "We build on the view of the Exact Renormalization Group (ERG) as an instantiation of Optimal Transport described by a functional convection-diffusion equation. We provide a new information theoretic perspective for understanding the ERG through the intermediary of Bayesian Statistical Inference. This connection is facilitated by the Dynamical Bayesian Inference scheme, which encodes Bayesian inference in the form of a one parameter family of probability distributions solving an integro-differential equation derived from Bayes\u2019 law. In this note, we demonstrate how the Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion equation which we dub Bayesian Diffusion. Identifying the features that define Bayesian Diffusion, and mapping them onto the features that define the ERG, we obtain a dictionary outlining how renormalization can be understood as the inverse of statistical inference.",
    "authors": [
        {
            "affiliations": [],
            "name": "David S. Berman"
        },
        {
            "affiliations": [],
            "name": "Marc S. Klinger"
        }
    ],
    "id": "SP:899f7a79d39feb888bc12da9180b22b925db9919",
    "references": [
        {
            "authors": [
                "K.G. Wilson",
                "J. Kogut"
            ],
            "title": "The renormalization group and the \u01eb expansion, Physics reports",
            "year": 1974
        },
        {
            "authors": [
                "J. Polchinski"
            ],
            "title": "Renormalization and effective lagrangians",
            "venue": "Nuclear Physics B",
            "year": 1984
        },
        {
            "authors": [
                "J.I. Latorre",
                "T.R. Morris"
            ],
            "title": "Exact scheme independence",
            "venue": "Journal of High Energy Physics 2000 (2000),",
            "year": 2000
        },
        {
            "authors": [
                "S.J. Cotler"
            ],
            "title": "Rezchikov, Renormalization group flow as optimal transport",
            "venue": "arXiv preprint arXiv:2202.11737",
            "year": 2022
        },
        {
            "authors": [
                "M. Dashti",
                "A.M. Stuart"
            ],
            "title": "The bayesian approach to inverse problems, in Handbook of uncertainty quantification",
            "year": 2017
        },
        {
            "authors": [
                "D.S. Berman",
                "J.J. Heckman",
                "M. Klinger"
            ],
            "title": "On the dynamics of inference and learning, arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "C.C. Bagnuls"
            ],
            "title": "Bervillier, Exact renormalization group equations: an introductory review",
            "venue": "Physics Reports",
            "year": 2001
        },
        {
            "authors": [
                "O.J. Rosten"
            ],
            "title": "Fundamentals of the exact renormalization group",
            "venue": "Physics Reports",
            "year": 2012
        },
        {
            "authors": [
                "F.J. Wegner",
                "A. Houghton"
            ],
            "title": "Renormalization group equation for critical phenomena",
            "venue": "Physical Review A",
            "year": 1973
        },
        {
            "authors": [
                "F. Wegner"
            ],
            "title": "Some invariance properties of the renormalization group",
            "venue": "Journal of Physics C: Solid State Physics",
            "year": 1974
        },
        {
            "authors": [
                "T.R. Morris"
            ],
            "title": "The exact renormalization group and approximate solutions, International",
            "venue": "Journal of Modern Physics A",
            "year": 1994
        },
        {
            "authors": [
                "T.R. Morris"
            ],
            "title": "Derivative expansion of the exact renormalization group",
            "venue": "Physics Letters B",
            "year": 1994
        },
        {
            "authors": [
                "T.R. Morris"
            ],
            "title": "Elements of the continuous renormalization group",
            "venue": "Progress of Theoretical Physics Supplement",
            "year": 1998
        },
        {
            "authors": [
                "G. Da Prato"
            ],
            "title": "Kolmogorov equations for stochastic PDEs",
            "venue": "Springer Science & Business Media,",
            "year": 2004
        },
        {
            "authors": [
                "M. Fuhrman"
            ],
            "title": "Nonlinear kolmogorov equations in infinite dimensional spaces: the backward stochastic differential equations approach and applications to optimal control",
            "venue": "The Annals of Probability",
            "year": 2002
        },
        {
            "authors": [
                "S.E. Shreve"
            ],
            "title": "Stochastic calculus for finance II: Continuous-time models, vol",
            "year": 2004
        },
        {
            "authors": [
                "F. Santambrogio"
            ],
            "title": "Euclidean, metric, and Wasserstein} gradient flows: an overview",
            "venue": "Bulletin of Mathematical Sciences",
            "year": 2017
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Optimal transport: old and new",
            "year": 2009
        },
        {
            "authors": [
                "K. It\u00f4"
            ],
            "title": "On stochastic differential equations",
            "venue": "No. 4. American Mathematical Soc.,",
            "year": 1951
        },
        {
            "authors": [
                "K. It\u00f4",
                "P. Henry Jr."
            ],
            "title": "Diffusion processes and their sample paths",
            "venue": "Reprint of the 1974 edition. Springer Science & Business Media,",
            "year": 1996
        },
        {
            "authors": [
                "W. Coffey",
                "Y.P. Kalmykov"
            ],
            "title": "The Langevin equation: with applications to stochastic problems in physics, chemistry and electrical engineering, vol. 27",
            "venue": "World Scientific,",
            "year": 2012
        },
        {
            "authors": [
                "K. Sekimoto"
            ],
            "title": "Langevin equation and thermodynamics",
            "venue": "Progress of Theoretical Physics Supplement",
            "year": 1998
        },
        {
            "authors": [
                "B. Simon"
            ],
            "title": "Harmonic analysis",
            "venue": "American Mathematical Soc.,",
            "year": 2015
        },
        {
            "authors": [
                "G. Maruyama"
            ],
            "title": "The harmonic analysis of stationary stochastic processes, Memoirs of the Faculty of Science, Kyushu University",
            "venue": "Series A, Mathematics",
            "year": 1949
        },
        {
            "authors": [
                "S. Bochner"
            ],
            "title": "Harmonic analysis and the theory of probability",
            "venue": "Courier Corporation,",
            "year": 2005
        },
        {
            "authors": [
                "F. Santambrogio"
            ],
            "title": "Optimal transport for applied mathematicians",
            "venue": "Birka\u0308user, NY",
            "year": 2015
        },
        {
            "authors": [
                "E.B. Davies"
            ],
            "title": "Heat kernels and spectral theory",
            "venue": "No. 92. Cambridge university press,",
            "year": 1989
        },
        {
            "authors": [
                "R. Rudnicki",
                "K. Pich\u00f3r"
            ],
            "title": "Tyran-Kami\u0144ska, Markov semigroups and their applications, in Dynamics of Dissipation",
            "year": 2002
        },
        {
            "authors": [
                "L. Lorenzi",
                "M. Bertoldi"
            ],
            "title": "Analytical methods for Markov semigroups",
            "venue": "Chapman and Hall/CRC,",
            "year": 2006
        },
        {
            "authors": [
                "N. V"
            ],
            "title": "Kolokoltsov, Markov processes, semigroups and generators, in Markov Processes, Semigroups and Generators",
            "venue": "de Gruyter,",
            "year": 2011
        },
        {
            "authors": [
                "V.H. Hoang",
                "C. Schwab",
                "A.M. Stuart"
            ],
            "title": "Complexity analysis of accelerated mcmc methods for bayesian inversion",
            "venue": "Inverse Problems",
            "year": 2013
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational physics",
            "year": 2019
        },
        {
            "authors": [
                "C. Schillings",
                "C. Schwab"
            ],
            "title": "Scaling limits in computational bayesian inversion, ESAIM",
            "venue": "Mathematical Modelling and Numerical Analysis",
            "year": 2016
        },
        {
            "authors": [
                "M. Harper"
            ],
            "title": "Information geometry and evolutionary game theory, arXiv preprint arXiv:0911.1383",
            "year": 2009
        },
        {
            "authors": [
                "M. Harper"
            ],
            "title": "The replicator equation as an inference dynamic, arXiv preprint arXiv:0911.1763",
            "year": 2009
        },
        {
            "authors": [
                "G. Parisi"
            ],
            "title": "The theory of non-renormalizable interactions: The large n expansion",
            "venue": "Nuclear Physics B",
            "year": 1975
        },
        {
            "authors": [
                "D. Anderson",
                "K. Burnham",
                "G. White"
            ],
            "title": "Comparison of akaike information criterion and consistent akaike information criterion for model selection and statistical inference from capture-recapture studies",
            "venue": "Journal of Applied Statistics",
            "year": 1998
        },
        {
            "authors": [
                "V. Balasubramanian"
            ],
            "title": "Statistical inference, occam\u2019s razor, and statistical mechanics on the space of probability distributions",
            "venue": "Neural computation",
            "year": 1997
        },
        {
            "authors": [
                "J. Sohl-Dickstein",
                "E. Weiss",
                "N. Maheswaranathan",
                "S. Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "in International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "R.M. Neal"
            ],
            "title": "Annealed importance sampling, Statistics and computing",
            "year": 2001
        },
        {
            "authors": [
                "A. Ramesh",
                "P. Dhariwal",
                "A. Nichol",
                "C. Chu",
                "M. Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "J. Maldacena"
            ],
            "title": "The large-n limit of superconformal field theories and supergravity, International journal of theoretical physics",
            "year": 1999
        },
        {
            "authors": [
                "X. Dong",
                "D. Harlow",
                "A.C. Wall"
            ],
            "title": "Reconstruction of bulk operators within the entanglement wedge in gauge-gravity duality, Physical review letters",
            "year": 2016
        },
        {
            "authors": [
                "F. Pastawski",
                "B. Yoshida",
                "D. Harlow",
                "J. Preskill"
            ],
            "title": "Holographic quantum error-correcting codes: Toy models for the bulk/boundary correspondence",
            "venue": "Journal of High Energy Physics 2015 (2015),",
            "year": 2015
        },
        {
            "authors": [
                "N. Lashkari",
                "M. Van Raamsdonk"
            ],
            "title": "Canonical energy is quantum fisher information",
            "venue": "Journal of High Energy Physics 2016 (2016),",
            "year": 2016
        },
        {
            "authors": [
                "J. Cotler",
                "P. Hayden",
                "G. Penington",
                "G. Salton",
                "B. Swingle",
                "M. Walter"
            ],
            "title": "Entanglement wedge reconstruction via universal recovery channels",
            "venue": "Physical Review X",
            "year": 2019
        },
        {
            "authors": [
                "T. Faulkner"
            ],
            "title": "The holographic map as a conditional expectation, arXiv preprint arXiv:2008.04810 (2020)",
            "year": 2020
        },
        {
            "authors": [
                "M. Ohya",
                "D. Petz"
            ],
            "title": "Quantum entropy and its use",
            "venue": "Springer Science & Business Media,",
            "year": 2004
        },
        {
            "authors": [
                "C.W. Helstrom"
            ],
            "title": "Quantum detection and estimation theory",
            "venue": "Journal of Statistical Physics",
            "year": 1969
        },
        {
            "authors": [
                "C. B\u00e9ny",
                "T.J. Osborne"
            ],
            "title": "Information-geometric approach to the renormalization group",
            "venue": "Physical Review A",
            "year": 2015
        },
        {
            "authors": [
                "C. B\u00e9ny",
                "T.J. Osborne"
            ],
            "title": "The renormalization group via statistical inference",
            "venue": "New Journal of Physics",
            "year": 2015
        },
        {
            "authors": [
                "E.A. Carlen",
                "J. Maas"
            ],
            "title": "An analog of the 2-wasserstein metric in non-commutative probability under which the fermionic fokker\u2013planck equation is gradient flow for the entropy, Communications in mathematical physics",
            "year": 2014
        },
        {
            "authors": [
                "E.A. Carlen",
                "J. Maas"
            ],
            "title": "Gradient flow and entropy inequalities for quantum markov semigroups with detailed balance",
            "venue": "Journal of Functional Analysis",
            "year": 2017
        },
        {
            "authors": [
                "E.A. Carlen",
                "J. Maas"
            ],
            "title": "Non-commutative calculus, optimal transport and functional inequalities in dissipative quantum systems",
            "venue": "Journal of Statistical Physics",
            "year": 2020
        },
        {
            "authors": [
                "M. Nozaki",
                "S. Ryu",
                "T. Takayanagi"
            ],
            "title": "Holographic geometry of entanglement renormalization in quantum field theories",
            "venue": "Journal of High Energy Physics 2012 (2012),",
            "year": 2012
        },
        {
            "authors": [
                "B. Swingle"
            ],
            "title": "Entanglement renormalization and holography",
            "venue": "Physical Review D",
            "year": 2012
        },
        {
            "authors": [
                "E. Alvarez",
                "C. Gomez"
            ],
            "title": "Geometric holography, the renormalization group and the c-theorem",
            "venue": "Nuclear Physics B",
            "year": 1999
        },
        {
            "authors": [
                "R.G. Leigh",
                "O. Parrikar",
                "A.B. Weiss"
            ],
            "title": "Holographic geometry of the renormalization group and higher spin symmetries",
            "venue": "Physical Review D",
            "year": 2014
        },
        {
            "authors": [
                "A. Mollabashi",
                "M. Naozaki",
                "S. Ryu",
                "T. Takayanagi"
            ],
            "title": "Holographic geometry of cmera for quantum quenches and finite temperature",
            "venue": "Journal of High Energy Physics 2014 (2014),",
            "year": 2014
        },
        {
            "authors": [
                "G. Evenbly",
                "G. Vidal"
            ],
            "title": "Tensor network renormalization",
            "venue": "Physical review letters",
            "year": 2015
        },
        {
            "authors": [
                "B. A"
            ],
            "title": "Zamolodchikov, Irreversibility of the flux of the renormalization group in a 2d field theory, JETP lett",
            "year": 1986
        },
        {
            "authors": [
                "R.C. Myers",
                "A. Sinha"
            ],
            "title": "Seeing a c-theorem with holography",
            "venue": "Physical Review D",
            "year": 2010
        },
        {
            "authors": [
                "H. Casini",
                "M. Huerta"
            ],
            "title": "A c-theorem for entanglement entropy",
            "venue": "Journal of Physics A: Mathematical and Theoretical",
            "year": 2007
        },
        {
            "authors": [
                "H. Casini",
                "M. Huerta",
                "R.C. Myers",
                "A. Yale"
            ],
            "title": "Mutual information and the f-theorem",
            "venue": "Journal of High Energy Physics 2015 (2015),",
            "year": 2015
        },
        {
            "authors": [
                "H. Casini",
                "E. Test\u00e9"
            ],
            "title": "Torroba, Markov property of the conformal field theory vacuum and the a theorem, Physical review letters",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 2.\n11 37\n9v 1\n[ he\npth\n] 2\n1 D\nWe build on the view of the Exact Renormalization Group (ERG) as an instantiation of Optimal Transport described by a functional convection-diffusion equation. We provide a new information theoretic perspective for understanding the ERG through the intermediary of Bayesian Statistical Inference. This connection is facilitated by the Dynamical Bayesian Inference scheme, which encodes Bayesian inference in the form of a one parameter family of probability distributions solving an integro-differential equation derived from Bayes\u2019 law. In this note, we demonstrate how the Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion equation which we dub Bayesian Diffusion. Identifying the features that define Bayesian Diffusion, and mapping them onto the features that define the ERG, we obtain a dictionary outlining how renormalization can be understood as the inverse of statistical inference.\nContents"
        },
        {
            "heading": "1 Introduction 3",
            "text": ""
        },
        {
            "heading": "2 ERG equation as a Functional Diffusion Equation 5",
            "text": "2.1 Polchinski\u2019s Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Wegner-Morris Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Field Reparameterization and Scheme Independence . . . . . . . . . . . . . . 9 2.4 Wegner-Morris and Fokker-Planck . . . . . . . . . . . . . . . . . . . . . . . . 9 2.5 Renormalization Group Flow as Optimal Transport . . . . . . . . . . . . . . 14"
        },
        {
            "heading": "3 From Stochastic Differential Equations to Partial Differential Equations",
            "text": "and back again 16 3.1 Stochastic Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . 16 3.2 Continuity Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"
        },
        {
            "heading": "4 Dynamical Bayesian Inference and the Backward Equation 22",
            "text": "4.1 Bayesian Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.2 Dynamical Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.3 Bayesian Diffusion for Normal Data . . . . . . . . . . . . . . . . . . . . . . . 25 4.4 Bayesian Drift and Scheme Independence . . . . . . . . . . . . . . . . . . . . 27 4.5 Generic Bayesian Diffusion at Late T . . . . . . . . . . . . . . . . . . . . . . 29 4.6 A Partial Differential Equation for Bayesian Inference . . . . . . . . . . . . . 30"
        },
        {
            "heading": "5 The ERG flow/Dynamical Bayesian Inference Correspondence 32",
            "text": "5.1 Dynamical Bayesian Flow \u2192 ERG Flow . . . . . . . . . . . . . . . . . . . . 33 5.2 Dynamical Bayesian Flow \u2190 ERG Flow . . . . . . . . . . . . . . . . . . . . 34 5.3 A Dictionary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.4 Renormalizability and Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
        },
        {
            "heading": "6 Discussion 37",
            "text": ""
        },
        {
            "heading": "7 Acknowledgements 40",
            "text": ""
        },
        {
            "heading": "A Optimal Transport 41",
            "text": "B ERG and Error Correction 43"
        },
        {
            "heading": "1 Introduction",
            "text": "The Renormalization Group (RG) is used in physical settings to deduce how a theory changes when it is viewed at different scales. In Wilsonian RG, one regards the set of possible theories as a space coordinatized by a collection of coupling constants specifying all of the possible contributions to a classical action. An RG flow can then be understood as a one parameter family, or flow, in the space of theories generated by a vector field on theory space referred to as the Beta Function [1]. The flow is completely specified once the requisite initial data is provided, for example, fixing a theory in the UV from which the RG flow begins.\nA typical approach to RG is to deduce the beta function by sequentially coarse-graining the theory \u2013 that is integrating out degrees of freedom which become suppressed at lower energy scales. In field theoretic contexts, this approach makes use of perturbative techniques in order to perform the requisite functional integrals. From a formal perspective, however, it is possible to study the properties of renormalization as an abstract flow equation without immediately concerning ourselves with any complicated or even intractable calculations that may be required to realize the flow explicitly. This approach to RG goes under the name of the Exact Renormalization Group (ERG), and will be the primarily focus of our paper.\nIn our approach to ERG we shall regard a Quantum Field Theory (QFT) as equivalent to the specification of a probability distribution on the space of fields included in the theory, which we denote by F(M) and refer to as the sample space in standard probability theoretic nomenclature.1 From this perspective the space of theories is isomorphic to the space of probability distributions on the sample space F(M), denoted byM. An ERG flow is therefore equivalent to a one parameter family of probability distributions onM.\nFrom this point of view we can still make contact with the Wilsonian picture by regarding a Wilsonian Effective Action, written in terms of some collection of coupling constants, as specifying a parameteric family of probability distributions. In other words, Wilsonian RG corresponds to a particular coordinatization of the spaceM. Following the lead of [2\u20134] and others, it is natural to interpret the one parameter family of probability distributions generated by an ERG flow as being governed by a functional convection-diffusion equation on the sample space F(M). Regarding ERG as a flow on the space of probability distributions over a given sample space contextualizes renormalization in a language that is amenable to applications outside of the usual realm of physical theory. In particular, it suggests a manifestly information theoretic interpretation for renormalization. Uncovering and presenting the details of this interpretation is the primarily objective of this note.\n1To accommodate this perspective, we shall consider only Euclidean QFT.\nIn seeking an answer to the question, \u201cWhat is the Information Theoretic Interpretation of an ERG flow?\u201d we find it useful to consider a related question: \u201cWhat does it mean to invert an ERG flow?\u201d. Here, our understanding ERG as being governed by a diffusion equation provides us with some direction. A powerful method for formally inverting a diffusion process is Bayesian Inversion [5]. Bayesian Inversion is a probabilistic approach used to determine the initial data that was fed into a partial differential equation and subsequently generated an observed sequence of outcomes. As the name implies, the main tool employed in Bayesian Inversion is Bayes\u2019 Law. This lends some credence to the idea that Bayesian Inference may serve as an \u201cinverse process\u201d to ERG.\nIn our previous publication [6] we introduced Dynamical Bayesian Inference, which recasts the Bayesian Inference as a dynamical system. Like ERG, Dynamical Bayesian Inference generates a one parameter family of probability distributions satisfying a flow equation. Whereas in ERG, the flow equation is deduced by sequential application of a coarse-graining law, in Dynamical Bayes the flow equation is obtained by sequential application of Bayes\u2019 law with respect to a continuously growing set of observed data. In this respect, an ERG flow continuously loses information2 while an Dynamical Bayesian flow continuously gains information.\nIn this note we shall argue in favor of this interpretation. In particular, we will demonstrate that the Dynamical Bayesian Flow equation gives rise to a convection-diffusion equation describing the evolution of an associated posterior predictive distribution backwards against the collection of additional data. We refer to the process described by this equation as Bayesian Diffusion, or Backward Inference. We interpret the equation governing Bayesian Diffusion as defining an ERG flow, with a coarse-graining procedure given by the continuous discarding of observed data. By construction, this ERG is inverted by the forward Dynamical Bayesian inference flow which is obtained by reincorporating the lost data back into the model. Alternatively, starting from an ERG flow we identify the Dynamical Bayesian flow for which it is the Backward Inference process by drawing a correspondence between the partial differential equation governing ERG and the partial differential equation governing Bayesian diffusion. Ultimately, this correspondence suggests a fascinating information theoretic interpretation for ERG: it can be regarded as the one parameter family of probability distributions obtained by starting from a data generating model, and continuously throwing away information in the form of observed data.\nThe organization of the paper is given as follows: In section 2 we review the formulation of ERG as a functional differential equation, specifically in the Wegner-Morris form. We also demonstrate that the Wegner-Morris equation is equivalent to a Fokker-Planck equation\n2Hence why it is diffusive.\nwith a given potential function, establishing the correspondence between ERG and optimal transport. In section 3 we introduce Stochastic Differential Equations (SDEs), and discuss the relationship between SDEs and partial differential equations of the type appearing in ERG. In section 4, we review Dynamical Bayesian Inference, and derive the Bayesian Diffusion equations. Finally, in section 5, we establish the correspondence between Bayesian Diffusion and ERG explicitly. We conclude with section 6 in which we review our findings, and discuss future research directions."
        },
        {
            "heading": "2 ERG equation as a Functional Diffusion Equation",
            "text": "In this section we will provide an overview of functional renormalization with the objective of demonstrating how ERG can be understood as a functional differential equation. Our presentation will only focus on the aspects of ERG which are relevant for this work, for a more complete overview see [2, 4, 7, 8] and the references therein. The presentation here follows very closely with that in [4].\nLet us consider a theory with fields \u03a6 \u2208 F(M). Here we are using a notation in which capital letters correspond to random variables, and lower case letters to realizations. In this case \u03a6 is to be regarded as a random variable taking values in a space of continuous functions on a manifold M . The distribution over \u03a6 is a probability functional P [\u03c6(x)] \u221d e\u2212S[\u03c6(x)] where S[\u03c6(x)] is the Euclidean action. The idea of RG is that we are only capable of probing scales less than some cutoff \u039b. As we change our cutoff we obtain a family of probability distributions, P\u039b[\u03c6(x)] \u221d e\u2212S\u039b[\u03c6(x)] corresponding to effective descriptions for the field \u03a6 at each measurement threshold. ERG provides a description for the changes to the effective probability distribution in the form of a flow equation:\n\u2212 \u039b d d\u039b P\u039b[\u03c6] = F [ P\u039b[\u03c6], \u03b4P\u039b[\u03c6] \u03b4\u03c6 , \u03b42P\u039b[\u03c6] \u03b4\u03c6\u03b4\u03c6 , ... ] . (1)\nwhere here F is a functional of P\u039b[\u03c6] and all its functional derivatives specifying the particular ERG scheme."
        },
        {
            "heading": "2.1 Polchinski\u2019s Equation",
            "text": "The canonical example of ERG is Polchinski\u2019s approach [2]. We begin with the partition function of a scalar field with source J given by:\nZ\u039b[J ] :=\n\u222b D\u03c6 exp { \u22121 2 \u222b ddp (2\u03c0)d ( \u03c6(p)\u03c6(\u2212p)(p2 +m2)K\u22121\u039b (p2) + J(p)\u03c6(\u2212p) ) \u2212 Sint,\u039b[\u03c6] } .\n(2)\nHere Sint,\u039b is the interacting action, and K\u039b(p 2) is a cutoff function which differentially weights modes \u03c6(p) based on their momenta.\nPolchinski\u2019s idea was to consider a scale \u039bR < \u039b and integrate out modes down to \u039bR. To this end, we assume that J(p) has compact support in the sphere of momenta with radius \u039bR \u2212 \u01eb for some small \u01eb > 0. If \u039bR is only infinitesimally smaller than \u039b we can compute the differential change to Z\u039b[J ] on account of integrating out the shell of modes between \u039b and \u039bR. Then, we demand that:\n\u2212 \u039b d d\u039b Z\u039b[J ] = A\u039bZ\u039b[J ] . (3)\nwhere A\u039b is a constant for each value of \u039b. If (3) holds, any correlation functions below the changed scale (i.e. for which one can take functional derivatives with respect to J) will be unchanged. Hence, this form of RG respects the fact that measured correlation functions ought to be independent of the RG flow below the relevant momentum scale.\nExpanding out (3) we find:\n\u2212 \u039b d d\u039b Z\u039b[J ] =\n\u222b D\u03c6 ( 1\n2\n\u222b\nddp\n(2\u03c0)d \u03c6(p)\u03c6(\u2212p)(p2 +m2)\u039b\u2202K\n\u22121 \u039b (p 2) \u2202\u039b + \u039b \u2202Sint,\u039b[\u03c6] \u2202\u039b\n)\ne\u2212S\u039b[\u03c6,J ] .\n(4)\nAgain, K\u039b(p 2) is a function with prescribed \u039b dependence. The behavior governed by (3) is that of Sint,\u039b. Polchinski showed that one can consistently satisfy (3) by taking Sint,\u039b to satisfy the functional differential equation:\n\u2212 \u039b\u2202Sint,\u039b[\u03c6] \u2202\u039b = 1 2\n\u222b\nddp(2\u03c0)d(p2 +m2)\u22121\u039b \u2202K\u039b(p\n2)\n\u2202\u039b\n{\n\u03b42Sint,\u039b \u03b4\u03c6(p)\u03b4\u03c6(\u2212p) \u2212 \u03b4Sint,\u039b \u03b4\u03c6(p) \u03b4Sint,\u039b \u03b4\u03c6(\u2212p)\n}\n.\n(5)\nFollowing the approach of (1), we define the probability functional: P\u039b[\u03c6] = e \u2212S\u039b[\u03c6]/Z\u039b which is explicitly the probability distribution for the field \u03a6 at scale \u039b. Then, we can truly write the Polchinski equation in the form of a convection-diffusion equation as:\n\u2212 \u039b d d\u039b P\u039b[\u03c6] = 1 2\n\u222b\nddp(2\u03c0)d(p2 +m2)\u22121\u039b \u2202K\u039b(p\n2)\n\u2202\u039b\n\u03b42\n\u03b4\u03c6(p)\u03b4\u03c6(\u2212p)P\u039b[\u03c6]\n+ 1\n2\n\u222b\nddp(2\u03c0)d(p2 +m2)\u22121\u039b \u2202K\u039b(p\n2)\n\u2202\u039b\n\u03b4\n\u03b4\u03c6(p)\n(\n2(p2 +m2)\n(2\u03c0)dK\u039b(p2) \u03c6(p)P\u039b[\u03c6]\n)\n. (6)\nThroughout this note we shall emphasize the sense in which such equations can be understood as functional analogs of more well defined finite dimensional equations. For example, (6) ought to be compared to the finite dimensional equation:\nd\ndt pt(y) =\n\u2211\ni,j\ngij \u2202 \u2202yi \u2202 \u2202yj pt(y) + \u2211\ni,j\n\u2202\n\u2202yi (gijvj(y)pt(y)) , (7)\nwhere here we\u2019ve written sums explicitly to make the analogy clear. To move between the finite dimensional equation and the Polchinski equation, we have made us of the dictionary:"
        },
        {
            "heading": "2.2 Wegner-Morris Flows",
            "text": "We have now established that Polchinski\u2019s equation is simply an infinite dimensional convectiondiffusion equation. It can be shown, that there exists a family of such equations which satisfy the constraint (3). This family is defined by considering different choices for the metric and drift velocity appearing in Table 1. The choice of this data then corresponds to a choice of scheme for the ERG flow. The aforementioned family of ERG schemes is given explicit representation in terms of the Wegner-Morris flow equation [9\u201313]. As a one parameter family of probability distributions, the Wegner-Morris flow is governed by the equation:\n\u2212 \u039b d d\u039b P\u039b[\u03c6] =\n\u222b\nddx \u03b4\n\u03b4\u03c6(x) (\u03a8\u039b[\u03c6, x]P\u039b[\u03c6]) . (8)\nThe Wegner-Morris scheme is encapsulated in the kernel \u03a8\u039b[\u03c6, x]. Again, it is useful to\ncompare to a finite dimensional flow equation:\nd dt pt(y) = \u2211\ni\n\u2202\n\u2202yi ( V i(pt, y)pt ) . (9)\nHere \u03a8\u039b has been replaced by a vector field that depends not only on y but on the entire probability function pt. It is often natural to regard V as the gradient of some scalar function W (pt, y) which also depends on pt, i.e. V i = gij(y) \u2202\n\u2202yj W (pt, y). Then we can write:\nd\ndt pt(y) =\n\u2211\nij\n\u2202\n\u2202yi\n(\ngij(y) \u2202\n\u2202yj W (pt, y)pt\n)\n. (10)\nA natural interpretation of (8) is that it simply reparameterizes the field at each new\nscale. To be precise as \u039b changes we find:\n\u03c6\u2032(x) = \u03c6(x) + \u03b4\u039b\n\u039b \u03a8\u039b[\u03c6, x] . (11)\nThis implies that the Wegner-Morris flow conserves probability, it only changes the way that the probability is distributed. This is the reason why the Wegner-Morris family of flow equations satisfies (3). On account of this new interpretation, \u03a8\u039b is given the title of the Reparameterization Kernel.\nFollowing the intuition from the finite dimensional case, we can represent the Reparam-\neterization Kernel as a functional gradient:\n\u03a8\u039b[\u03c6, x] = \u2212 \u222b ddy 1\n2 C\u0307\u039b(x, y)\n\u03b4\u03a3\u039b[\u03c6]\n\u03b4\u03c6(y) . (12)\nHere C\u0307\u039b(x, y) is playing the same role it did in the Polchinski equation as an inverse metric on the sample space of fields, however we have not yet fixed its functional form and, indeed, it may differ from Polchinski\u2019s choice. In the literature, C\u0307\u039b(x, y) is called the ERG kernel.\nA popular choice for \u03a3\u039b is given by the difference between the renormalized action of the\ntheory and a second action, S\u0302\u039b, called the seed action:\n\u03a3\u039b[\u03c6] := S\u039b[\u03c6]\u2212 2S\u0302\u039b[\u03c6] . (13)\nThe factor of two is conventional. In this framework, an RG scheme is therefore specified entirely by a choice of C\u0307\u039b and S\u0302\u039b. For example, we can reconcile the Polchinski equation from the Wegner-Morris set up by taking:\nC\u0307\u039b(p 2) = (2\u03c0)d(p2 +m2)\u22121\u039b\n\u2202K\u039b(p 2)\n\u2202\u039b ; S\u0302\u039b[\u03c6] =\n1 2\n\u222b\nddp\n(2\u03c0)d (p2 +m2)K\u22121\u039b (p 2)\u03c6(p)\u03c6(\u2212p) . (14)"
        },
        {
            "heading": "2.3 Field Reparameterization and Scheme Independence",
            "text": "In a Quantum Field Theory, fields are not an observable quantity, but rather a device used to encode a theory. In this respect we should regard field paramterization as a choice of coordinates on the sample space of the theory, and require that the theory be invariant under diffeomorphisms of these coordinates. Following this line of thought, [3] showed that the field reparamterization appearing in (11) can be interpreted as a gauge redundancy, and, as a consequence, the drift component of the Wegner-Morris equation can be interpreted as a choice of gauge. This perspective can be made concrete by viewing the convective derivative of the resulting flow equation as a covariant derivative, with the vector field generating the drift playing the role of a gauge field. Changing the drift field changes the description of the ERG only cosmetically, in particular the expectation values of relevant operators are left invariant under a change of gauge. We shall adopt this perspective and regard the prescription of drift in an ERG as tantamount to a choice of scheme."
        },
        {
            "heading": "2.4 Wegner-Morris and Fokker-Planck",
            "text": "In the previous subsection we saw that ERG can be understood as a functional differential equation associated with the Wegner-Morris equation (9). In this section we shall demonstrate how the Wegner-Morris equation may be regarded as a form of the Fokker-Planck equation. For simplicity, we shall work in the context of probability theory on a differentiable sample space, S.\nGiven a function F : M \u2192 R on a (psuedo)-Riemannian manifold M with metric g, we define the gradient as follows: For any path \u03b3 : [0, 1] \u2192 M , with \u03b3(0) = x, the gradient of F at x with respect to the metric g is the tangent vector gradgF (x) such that:\nd\ndt \u03b3\u2217F\n\u2223 \u2223 \u2223 \u2223\nt=0\n= g(gradgF (x), \u03b3\u2217 d\ndt )\n\u2223 \u2223 \u2223 \u2223\nt=0\n. (15)\nAn equivalent definition that does not require the introduction of a path simply defines\nthe gradient in terms of the exterior derivative on M :\ndF (X) = g(gradgF,X); gradgF = (dF )\\ . (16)\nHere, the map \\ : T \u2217M \u2192 TM , corresponds to the usual notion of index raising via the metric, mapping one forms into vectors. To be precise, given \u03b1, \u03b2 \u2208 T \u2217M we have: \u03b1\\(\u03b2) = g\u22121(\u03b1, \u03b2).\nLetM = dens(S) denote the manifold of probability distribution functions on the sample space S. The tangent space toM at a point p is defined by:\nTpM := { \u03b7 \u2208 C\u221e(S) | \u222b\nS\nVolS \u03b7 = 0\n}\n. (17)\nThis is a good definition since functions, \u03b7 \u2208 TM, can be regarded as perturbations to probability densities which do not spoil the integral normalization:3\n\u222b\nS\n\u22c6(p+ \u03b7) =\n\u222b\nS\n\u22c6p+\n\u222b\nS\n\u22c6\u03b7 =\n\u222b\nS\n\u22c6p = 1 . (18)\nHence, perturbations p 7\u2192 p+ \u03b7 still belong to the manifoldM. Now, as a first exercise in understanding gradients on M, let us consider the most straightforward combination of metric and scalar function on this manifold. M is naturally equipped with an \u21132 metric of the form\nG\u21132(\u03b71, \u03b72) :=\n\u222b\nS\n\u03b71 \u2227 \u22c6\u03b72 . (19)\nMoreover, there is a natural functional to consider, namely the Dirichlet Energy Functional E :M\u2192 R defined by:4\nE [p] = 1 2\n\u222b\nS\ndp \u2227 \u22c6dp . (20)\nThe exterior derivative on M should be understood as the variational derivative with respect to the density p. Thus, by standard techniques, we can compute:\n\u03b4E [p] = \u222b\nS\nd\u03b4p \u2227 \u22c6dp = \u222b\nS\n(\u22121)d\u03b4p \u2227 d \u22c6 dp+ \u222b\nS\nd(\u03b4p \u2227 \u22c6dp) . (21)\nA probability distribution must necessarily have bounded support to satisfy an integral normalization condition, hence in this and any future computations we can safely take the boundary variation to zero. Thus, we arrive at the desired result:\n\u03b4E [p] = \u222b\nS\n(\u22121)d\u03b4p \u2227 d \u22c6 dp = G\u21132((\u22121)d \u22c6 d \u22c6 dp, \u03b4p) . (22)\nMatching this to the definition of the gradient (16), we conclude that the gradient of the Dirichley Energy Functional with respect to the \u21132 metric onM is equivalent to the Laplacian of p:\ngrad\u21132E [p] = \u2212\u2206p . (23) 3Here \u22c6 is the Hodge star on S. 4Here d is the exterior derivative on S not onM.\nAn immediate corollary of this fact is that it allows us to write the standard heat equation, \u2202 \u2202t pt = \u2206pt, in the form of a gradient flow as\n\u2202\n\u2202t pt = \u2212grad\u21132E [pt] . (24)\nRemarkably, if we choose a different metric for the space of probability distributionsM we can reconcile the heat equation as a gradient flow with respect to a different functional of the probability distribution. Of particular interest to us is the task of constructing a metric for which the potential is the differential entropy:\nS[p] = \u2212 \u222b\nS\np \u2227 \u22c6 ln(p) . (25)\nTo find such a metric, we begin by establishing the following isomorphism of TM: Given \u03b7 \u2208 TpM and \u03b7\u0302 \u2208 C\u221e(S) the following equation implicitly defines an isomorphism between them, up to a constant:\n\u03b7 = \u22c6dip(d\u03b7\u0302)\\VolS . (26)\nIn more familiar vector calculus notation, this equation takes the form\n\u03b7 = div(p grad \u03b7\u0302). (27)\nUsing this isomorphism, we may now define a new metric on M which is essentially a probability weighted version of the Dirichlet Energy Functional. In particular, we take:\nGW2(\u03b71, \u03b72) =\n\u222b\nS\np d\u03b7\u03021 \u2227 \u22c6d\u03b7\u03022 = \u2212 \u222b\nS\n\u03b71 \u2227 \u22c6\u03b7\u03022 = \u2212 \u222b\nS\n\u03b7\u03021 \u2227 \u22c6\u03b72 . (28)\nAs our notation suggests, this metric can be understood as the infinitesimal form of the Wasserstein two distance defined in Appendix A. A proof of this fact can be found in [4].\nNow, let us compute the exterior derivative of the differential entropy, and by extension its gradient with respect to the newly minted Wasserstein metric (28). Let \u03b7 = \u03b4p, denote an element of TM obtained by perturbing the density p. Moreover, let \u03b7\u0302 denote the element isomorphic to \u03b7 via (26). We may now complete the desired computation:\n\u03b4S[p] = \u2212 \u222b\nS\n(\n\u03b4p \u2227 \u22c6 ln(p) + p \u2227 \u22c6\u03b4p p\n)\n= \u2212 \u222b\nS\n\u03b7 \u2227 \u22c6(ln(p) + 1) (29)\n= \u2212 \u222b\nS\ndip(d\u03b7\u0302)\\VolS \u2227 (ln(p) + 1) Using (26) (30)\n= \u2212 \u222b\nS\n(\u22121)dip(d\u03b7\u0302)\\VolS \u2227 dp\np Integrating by parts (31)\n= \u2212 \u222b\nS\n(\u22121)dpd\u03b7\u0302 \u2227 \u22c6dp p\ni\u03c9\\VolS = \u22c6\u03c9 and\n\u222b\nS\n\u22c6\u03b1 \u2227 \u03b2 = \u222b\nS\n\u03b1 \u2227 \u22c6\u03b2 (32)\n= \u2212 \u222b\nS\n(\u22121)dd\u03b7\u0302 \u2227 \u22c6dp = \u2212 \u222b\nS\n\u03b7\u0302 \u2227 \u22c6\u2206p Integrating by parts once more (33)\n= GW2(\u2206p, \u03b7) . (34)\nHence, we have succeeded in showing that:\ngradW2S[p] = \u2206p . (35)\nmeaning we can indeed write the heat equation:\n\u2202\n\u2202t pt = gradW2S[pt] . (36)\nIn general, if we specify a functional, F : M \u2192 R, and follow the approach described above, the resulting gradient flow equation will read:\n\u2202\n\u2202t pt = gradW2F [pt] = \u22c6diptd( \u03b4F\u03b4p |pt )\\VolS . (37)\nFor our purposes, it will be interesting to consider functionals that are the sum of two pieces:\nF [p] = S[p] + V[p] = \u2212 \u222b\nS\np \u2227 \u22c6 ln(p) + \u222b\nS\np \u2227 \u22c6V . (38)\nHere, S[p] is the differential entropy, and thus this part of the functional will source a diffusion equation in (37). The second term should be interpreted as a potential, and will introduce drift into (37). Provided V : S \u2192 R is a function that does not depend on p, (37) takes the form of a convection-diffusion equation:\n\u2202\n\u2202t pt \u2212 \u22c6didp\\VolS \u2212 \u22c6diptdV\\VolS = 0 . (39)\nThis is precisely the Fokker-Planck equation! In vector calculus notation, it takes the\nform: \u2202\n\u2202t pt \u2212\u2206pt \u2212 div(p gradV ) = 0 . (40)\nIf the normalization Z = \u222b S e\u2212VVolS is finite, q = 1 Z e\u2212V is a probability distribution, which is the stationary state of (39). In this case, the gradient flow of F can equivalently be understood as the gradient flow of the KL-Divergence between pt and q. This follows from a simple calculation:\nF [pt] + ln(Z) = \u222b\nS\np \u2227 \u22c6(ln ( pt e\u2212V ) + ln(Z)) =\n\u222b\nS\npt \u2227 \u22c6 ln ( pt q ) = DKL(pt \u2016 q) . (41)\nBecause ln(Z) is independent of the distribution p, the variational derivatives of F and DKL(pt \u2016 q) will be equal: \u03b4F\u03b4p = \u03b4DKL(p\u2016q) \u03b4p . Thus, both of these functionals provide the same (37).\nWe are now prepared to make good on our promise and show that the Wegner-Morris equation is equivalent to the Fokker-Planck equation with a specified stationary distribution. To this end, let us write:\np = e\u2212Sp(x)\nZp , q =\ne\u2212Sq(x)\nZq . (42)\nwhere here Zd = \u222b\nS \u22c6d\u0302 is the normalization factor for the Boltzmann weight of the distribu-\ntion d, i.e. p\u0302 = e\u2212Sp. Let us also define\n\u03a3 = \u2212 ln ( p\u0302\nq\u0302\n)\n= Sq \u2212 Sp; \u03a8 = gradg\u03a3 . (43)\nwhich are the analogs of the renormalization scheme function, and the associated reparameterization kernel. Now, we need only compute the variation of the KL-Divergence:\n\u03b4DKL(p \u2016 q) = \u222b\nS\n(\u03b4p \u2227 \u22c6(ln(p)\u2212 ln(q) + 1) (44)\n=\n\u222b\nS\n\u03b4p \u2227 \u22c6 (\u2212Sp \u2212 ln(Zp) + 1 + Sq + ln(Zq)) (45)\n=\n\u222b\nS\n\u03b4p \u2227 \u22c6 (Sq \u2212 Sq) \u222b\nS\n\u03b4p \u2227 \u22c6const. = 0 s.t. \u222b\nS\n\u22c6(p+ \u03b4p) = 1 (46)\n=\n\u222b\nS\ndipd\u03b7\u0302\\VolS \u2227 \u03a3 Via the tangent space isomorphism with \u03b4p = \u03b7 (47)\n= (\u22121)d \u222b\nS\nipd\u03b7\u0302\\VolS \u2227 d\u03a3 Integrating by parts (48)\n= (\u22121)d \u222b\nS\npd\u03b7\u0302 \u2227 \u22c6d\u03a3 i\u03c9\\VolS = \u22c6\u03c9 and \u222b\nS\n\u22c6\u03b1 \u2227 \u03b2 = \u222b\nS\n\u03b1 \u2227 \u22c6\u03b2 (49)\n= \u2212(\u22121)d \u222b\nS\n\u03b7\u0302 \u2227 d(\u22c6p \u03a3) Integrating by parts (50)\n= \u2212GW2(\u22c6d(\u22c6p d\u03a3)), \u03b7\u0302) . (51)\nHence, we have shown that:\ngradW2DKL(p \u2016 q) = \u22c6d(\u22c6pd\u03a3) . (52)\nThus, the Fokker Planck equation associated with the gradient flow of F is precisely the Finite Dimensional Wegner-Morris equation:\n\u2202\n\u2202t pt = \u2212gradW2DKL(p \u2016 q) = \u2212 \u22c6 d(\u22c6p d\u03a3) =\n\u2202\n\u2202xi\n(\npgij \u2202\n\u2202xj \u03a3\n)\n= \u2202\n\u2202xi ( \u03c8i p ) . (53)"
        },
        {
            "heading": "2.5 Renormalization Group Flow as Optimal Transport",
            "text": "Relating RG flow to the Continuity Equation (53) is more or less an exercise in properly identifying the sample space of the RG probability distribution, and completing the analogies that follow from there. For simplicity, we have reproduced this exercise in the form of a dictionary between optimal transport for finite sample spaces and ERG:\n15"
        },
        {
            "heading": "3 From Stochastic Differential Equations to Partial Dif-",
            "text": "ferential Equations and back again\nHaving placed the Exact Renormalization Group flow equations squarely in the context of Partial Differential Equations, we would now like to take a brief detour into addressing some of the generic properties of these equations. In particular, we will be interested in understanding the relationship between continuity equations on the one hand, and stochastic differential equations on the other. For more information on the relationship between Stochastic Differential Equations, partial differential equations, and optimal transport see [14\u201321]."
        },
        {
            "heading": "3.1 Stochastic Differential Equations",
            "text": "A stochastic process is a one-parameter family of random variables, {Xt}t\u22650. For our purposes, we shall be interested in stochastic processes whose dynamics are governed by a stochastic differential equation of the Ito5 form [22\u201324]:\ndX it = m i(Xt, t)dt+ \u03c3 i j(Xt, t)dW j t . (54)\nHere mi is describes the drift, or mean rate of change, while \u03c3ij describes the diffusion, or mean variability. dW it corresponds to an independent increment of a Brownian motion, a random variable drawn from a standard normal distribution corresponding to the noisy motion of the process over a single increment of time.\nPhysicists may find it useful to compare (54) to the Langevin Equation\ndX i\ndt = \u00b5i(Xt) + \u03b7 i t . (55)\nwhich describes the dynamics of a quantity Xt governed by a law dX dt = \u00b5, but subject to random fluctuations, \u03b7 [25, 26]. To match the stochastic differential equation, we should take \u03b7t = (\u03b7 1 t , ..., \u03b7 n t ) to be a random variable with correlation structure\n\u3008\u03b7it\u03b7jt\u2032\u3009 = \u03b4kl\u03c3ik\u03c3jl \u03b4(t\u2212 t\u2032) . (56)\nGiven a function: f : [0, T ] \u00d7 S \u2192 R, we can determine its stochastic differential when evaluated over the process Xt by using the principal of quadratic variation. Quadratic\n5The Ito formulation is one of two major approaches to the subject of stochastic calculus with the other being the Stratonovich formalism. Here we use Ito\u2019s formulation because we feel it makes the relationship between Stochastic Differential Equations and Partial Differential Equations very clear.\nvariation dictates that the product of two increments of Brownian motion scales linearly in the interval between realizations of the stochastic process. This fact follows from the observation that the variance of a Weiner process is linear in time. Schematically, we encode the principal of quadratic variation in the form:\ndW it dW j t \u223c \u03b4ijdt+ (1\u2212 \u03b4ij)O(dt2) . (57)\nIf we now expand the differential of f(Xt, t) in a power series, retaining terms up to O(dt), we find:\ndf(Xt, t) =\n(\n\u2202f \u2202t +mi \u2202f \u2202xi + 1 2 \u03b4kl\u03c3ik\u03c3 j l\n\u22022f\n\u2202xi\u2202xj\n)\ndt+ \u2202f\n\u2202xi dW it . (58)\nThe second order derivative terms are the unique addition provided to us by quadratic variation. The realizations, {f(Xt, t)}t\u22650, now define a stochastic process in their own right, governed by the stochastic differential equation (58).\nA stochastic process, {Yt}t\u22650 is called a Martingale if it satisfies the equation\nE(Yt |{Ys}s\u2264\u03c4) = Y\u03c4 ; \u2200s \u2264 t . (59)\nOne can read (59) as specifying that the mean value of the stochastic process, Yt, has no tendency to change over time. Inspired by this interpretation, it is not hard to show that a stochastic process will be a Martingale if and only if it is described by a stochastic differential equation with vanishing drift. This leads to a beautiful connection between the theory of stochastic processes, and harmonic analysis [27\u201329]. Considering the stochastic process {f(Xt, t)}t\u22650, if we demand that the drift in (58) be set equal to zero, we find that f must satisfy the Partial Differential Equation:\n\u2202f \u2202t +mi \u2202f \u2202xi + 1 2 \u03b4kl\u03c3ik\u03c3 j l\n\u22022f\n\u2202xi\u2202xj = 0 . (60)\nMoreover, using (59), we see that this partial differential equation has a formal solution:\nf(Xt, t) = E(f(XT , T ) | {Xs}s\u2264t) . (61)\nwhere here T should be regarded as the terminal time of the stochastic process.\nLet\u2019s define the differential operator which appears in (60) as:\nA = mi \u2202 \u2202xi + 1 2 \u03b4kl\u03c3ik\u03c3 j l\n\u22022\n\u2202xi\u2202xj . (62)\nWith respect to the \u21132 inner product on the space of functions, we can define a formal adjoint\noperator: A\u2020 such that\nG\u21132(f,A(g)) = G\u21132(A\u2020(f), g) . (63)\nThe adjoint operator can be deduced by integrating by parts, and we find:\nA\u2020(f) = \u2212 \u2202 \u2202xi (mif) + 1 2\n\u22022\n\u2202xi\u2202xj (\u03b4kl\u03c3ik\u03c3 j l f) . (64)\nThe differential equation generated by the adjoint operator therefore reads:\n\u2202f \u2202t = \u2202 \u2202xi (mif)\u2212 1 2\n\u22022\n\u2202xi\u2202xj (\u03b4kl\u03c3ik\u03c3 j l f) . (65)\nThis is the Fokker-Planck equation! In fact it is slightly more general than the Fokker-Planck equation we uncovered through our analysis in section 2.4 because it allows for a stochastic process with non-trivial diffusion matrices \u03c3ij . To reconcile the Fokker-Planck equation (39), we should consider the Stochastic Differential equation:\ndX it = \u2212(gradgV )idt+ \u221a 2\u03b4ijdW j t . (66)\nHere, we again see the role of the potential V in sourcing the mean drift behavior of the stochastic process. That is, the stochastic behavior associated with the random variable described by a probability distribution solving the Fokker-Planck equation with potential function V , is itself a stochastic gradient flow with respect to that potential.\nThe pair of differential equations we have written have interpretations as \u201cForward\u201d and\n\u201cBackward\u201d processes. The equation generated by A: (\n\u2202\n\u2202t +A\n)\nf = 0 , (67)\nis called the Backward equation because its solution, i.e. (61), is specified by a terminal condition. On the contrary, the adjoint equation:\n(\n\u2202\n\u2202t \u2212A\u2020\n)\nf = 0 , (68)\nis called the Forward equation because its solution is specified by an initial condition: f(0, x)."
        },
        {
            "heading": "3.2 Continuity Equations",
            "text": "Let us now begin from the reverse perspective, and seek to understand how a continuity equation might be associated with a stochastic process. A measure on S is a top form whose\nintegral over all of S can be normalized to 1. For convenience we will refer to such a form as \u00b5 \u2208MS with\nMS := {\u00b5 \u2208 \u2126d(S)| \u222b\nS\n\u00b5 = 1} . (69)\nBy Hodge duality, a measure can be related to a distribution: p = \u22c6\u00b5, or \u00b5 = pVolS, which would bring us back into the notation used in the previous section. We will move back and forth between the measure based, and probability density based approaches more or less at will.\nTo write down a continuity equation, we consider a one parameter family of measures, or otherwise a trajectory through the spaceMS, \u00b5 : [0, 1]\u2192MS which we denote as \u00b5t(x) \u2208MS, governed by the differential equation:\n\u2202\n\u2202t \u00b5t + divt\u00b5t = 0 . (70)\nHere, vt : [0, 1]\u2192 TS is a time dependent vector field. Since \u00b5t is a top form, we can regard the second term above as the Lie Derivative and write:\n\u2202\n\u2202t \u00b5t + Lvt\u00b5t = 0 . (71)\nThus, a continuity equation has the immediate interpretation as a flow generated by the vector field vt [19]. Comparing to (39), we see that the vector field which generates a gradient flow with respect to the potential F is given by:\nvt = \u2212d ( \u03b4F \u03b4p (pt) ) \\ . (72)\nA measure \u00b5t that solves the continuity equation is referred to as a strong solution. However, we can also consider a weaker condition in which the integral of (70) against a compactly supported, C1(S) function is always zero [30]. That is, given \u03c8 : [0, 1]\u2192 C1(S), we demand:\n0 =\n\u222b\n[0,1]\u00d7S\n\u03c8t( \u2202\n\u2202t \u00b5t + divt\u00b5t) . (73)\nIntegrating by parts and using the fact that \u03c8t is compactly supported (73) implies that 6\n6We\u2019ve also used the fact that\nd\u03c8t \u2227 ivt\u00b5t = d\u03c8t(vt)\u00b5t , (74) where d\u03c8t(vt) is the pairing of d\u03c8t and vt in the sense of forms and tangent vectors.\n0 = \u2212 \u222b\n[0,1]\u00d7S\n\u00b5t( \u2202\u03c8t \u2202t + d\u03c8t(vt)) . (75)\nWe can interpret this equation as the condition:\n\u222b\n[0,1]\nE\u00b5t( \u2202\u03c8t \u2202t + d\u03c8t(vt)) = 0 . (76)\nProvided we can exchange the order of the t-derivative and the integral over S, we can reframe our analysis in terms of a t-independent function, \u03c6 : S \u2192 R, in which case we find\nd\ndt E\u00b5t(\u03c6) + E\u00b5t(d\u03c6(vt)) = 0 . (77)\nThis equation can be regarded as an Ehrenfest theorem specifying the mean dynamics of the field \u03c6. In particular, it says that \u03c6 will satisfy a gradient flow in the direction of vt, and thus we can regard (77) as the expectation value of the stochastic differential equation (54).\nThe heat equation is a special case of the continuity equation with the flow vector given\nby gradpt pt = grad(ln(pt)): To see that this is the case, let us write \u00b5t = ptVol. Then it is straightforward to show that7\ndi gradpt pt \u00b5t = digradpt ptVolS pt = digradptVolS = div(gradpt)VolS = \u2206ptVolS = \u2206\u00b5t . (79)\nHence in this case (70) becomes:\n\u2202\n\u2202t \u00b5t +\u2206\u00b5t = 0;\n\u2202\n\u2202t pt +\u2206pt = 0 . (80)\nThe fundamental solution to the heat equation with initial data p0 is written formally in\nterms of the heat kernel [31, 32]:\npt = e \u2212t\u2206p0 . (81)\nHere e\u2212t\u2206 is the operator obtained by exponentiating \u2206, and it acts on the initial data.8 In\n7The Laplacian operator is defined on differential forms as \u2206 = d \u22c6 d \u22c6 + \u22c6 d \u22c6 d. It is easy to show that the Laplacian of a top form, \u00b5t = ptVol, is related to the Laplacian of its associated scalar by:\n\u2206\u00b5t = (d \u22c6 d \u22c6+ \u22c6 d \u22c6 d)\u00b5t = d \u22c6 d \u22c6 \u00b5t = d \u22c6 dpt = \u22c6\u2206pt . (78)\n8It is easy to compute:\n\u2202 \u2202t (e\u2212t\u2206p0) = \u2212\u2206e\u2212t\u2206p0 . (82)\na position basis we can represent the heat kernel as an integral kernel of the form:\n\u3008x| e\u2212t\u2206 |y\u3009 = H(x, y, t) = (4\u03c0t)\u2212d/2e\u2212 \u2016x\u2212y\u2016 2 4t . (83)\nThus, we can interpret the heat kernel as the transition function for a Markov process corresponding to the probability density of translation from sample point x to sample point y in an interval 2t. This density of the form of a multivariate Gaussian, which we shall denote by9 H(x, y, t) = N (y, 2t1). A general continuity equation will be generated by a differential operator A, through the equation:\n(\n\u2202\n\u2202t +A\n)\npt = 0 . (84)\nWe can play the same game as before and solve this differential equation formally by writing pt = e \u2212tAp0. Again, we can choose a continuous basis and express the heat kernel of the operator A as an integral kernel:\n\u3008x| e\u2212tA |y\u3009 = H(x, y, t) . (85)\nIt remains natural to interpret H(x, y; t) as the transition probability density for a Markov process [33, 34]. The Markov process in question is defined by a continuous set of operators, {Pt}t\u22650. One can regard the operator Pt as the \u201ctime evolution operator\u201d associated with A, which, in this context, should be understood as the infinitesimal generator of time translations. In other words, Pt is obtained by exponentiating the operator A, Pt = e\u2212tA.\nGiven any measurable function, f : S \u2192 R, the action of the operator Pt translates f along the Markov chain. If we work in the positional basis, we can write:\nPt(f) = e \u2212tA(f) =\n\u222b\nS\nVolS(z)H(z, x, t)f(z) . (86)\nwhich is simply the convolution of the transition density H with f . This provides us with a very useful formula for determining the operator A:\nA(f) = lim t\u21920 Pt(f)\u2212 f t . (87)\nEquation (87) reverses the operator exponentiation by differentiating along the integral curve Pt(f).\nThe family of operators {Pt}t\u22650 can be regarded as a semigroup with the simple compo-\n9Here N (\u00b5,\u03a3) is the multivariate normal distribution with mean parameter \u00b5 and covariance matrix \u03a3.\nsition law [35]:\nPt+s(f) = Pt(Ps(f)) = Ps(Pt(f)) . (88)\nMoreover, the set of operators {Pt}t\u22650 define a stochastic process {Xt}t\u22650, satisfying the law [17]:\nE(f(Xs+t) | Xr\u2264t) = Ps(f)(Xt) . (89)\nThis expression is equivalent to the Martingale condition (61). Writing the operator A in the form (62), the stochastic process {Xt}t\u22650 is immediately identified with the Stochastic Process:\ndX it = m i(Xt, t)dt+ \u03c3 i jdW i t . (90)\nThus, we have succeeding in mapping our way back to a stochastic differential equation, this time beginning from a diffusion equation."
        },
        {
            "heading": "4 Dynamical Bayesian Inference and the Backward Equa-",
            "text": "tion\nThe formulation of the Exact Renormalization Group flow as an Optimal Transport problem, particularly in its relationship to extremization of a relative entropy, already suggests a deep connection between renormalization, diffusion, and information theory. We now turn our attention to the task of fleshing out this relationship, and, in doing so, providing an information theoretic conceptualization of renormalization through the intermediary of statistical inference. To accomplish this task, we will build on the language of Dynamical Bayesian Inference introduced in [6]. We will show that the dynamics of an inferred probability model defined by a continuously updating Bayesian scheme gives rise to an inverse process governed by a diffusion equation that can be brought into correspondence with ERG. Using this fact, we advocate for the perspective that renormalization can be understood as the inverse process of statistical inference."
        },
        {
            "heading": "4.1 Bayesian Inversion",
            "text": "Bayesian inference is a probabilistic, self-consistent approach to adjusting one\u2019s beliefs in the presence of new information. It originates from Bayes\u2019 Law, which encodes the probabilistic\nrelationship between two, potentially co-varying, random variables H \u2208 SH and E \u2208 SE :\nP (H | E) = P (E | H)P (H) P (E) . (91)\nIn Bayesian Inference, the variables H and E are respectively identified with the Hypothesis and the Evidence. In this language, one can attach a compelling geometric interpretation to Bayes\u2019 law as measuring the relative volumes of the region H \u2282 SH \u00d7 SE for which both H and E are realized, and the region O \u2282 SE for which the observed evidence is true. Thus, the interpretation of Bayesian Inference is that it measures the volume of hypotheses that consistent a given set of observed data, which we denote as the region H | O \u2282 SH \u00d7 SE . A particularly interesting form of Bayesian Inference arises in the context of so-called Bayesian Inversion problems [5, 36\u201341]. There, we are concerned with a pair of random variables, Y \u2208 SY and U \u2208 SU which are related by some process:\ny = G(u) + n (92)\nWe regard G : SU \u2192 SY as a deterministic function, and denote by N \u2208 SN \u2243 SY noise which can either be associated with the process itself, or the process of measuring its output.\nThe goal of the Bayesian Inversion problem is to determine the signal, u, that led to the measured output, y. However, it is often impractical to truly invert the process, thus it is more prudent to seek to place a probability distribution over the inputs.\nTo this end, we regard U \u2208 SU as having a prior measure \u03a00 = \u03c00(u)VolU(u) \u2208 MSU , and treat the noise N as a random variable independent from Y also possessing of a measure \u03c10 = p0(n)VolN (n) \u2208MSN . Then, we can define the conditional random variable Y | U as:\nY | U = G(U) +N . (93)\nIf we regard \u03c6 : SY \u2192 SN as a map from output to noise, such that\nn = \u03c6(y) = y \u2212G(u) . (94)\nwe can obtain a conditional measure for Y | U by pulling back the measure \u03c10 via the map \u03c6:\n\u03c1Y |U(y | u) = \u03c6\u2217(\u03c10(n)) = p0(y \u2212G(u)) det ( \u2202\u03c6i\n\u2202yj\n)\nVolN(y \u2212G(u)) = p0(y \u2212G(u))VolY (y) . (95)\nWe regard the pulled-back density p0(y \u2212 G(u)) as the likelihood function for an inference\nproblem, and often denote it by pY |U(y | u). We therefore obtain a joint measure:\n\u00b5Y,U(y, u) = \u03c1Y |U(y | u) \u2227 \u03a00(u) = p0(y \u2212G(u))\u03c00(u)VolY (y) \u2227VolU(u) (96)\nfor the random variable (Y, U) \u2208 SY \u00d7 SU . Provided the marginalization:\npY (y) =\n\u222b\nSU\np0(y \u2212G(u))\u03c00(u)VolU(u) (97)\nis greater than zero and finite, we can define the Bayesian posterior measure as:\n\u03a0y\u2217(u) = 1\npY (y\u2217) \u03c1Y |U(y\u2217 | u) \u2227 \u03a00(u) , (98)\nwhich is just a recapitulation of Bayes\u2019 law as written in the form (91). The notation \u03a0y\u2217(u) is meant to remind us that the posterior distribution depends on the realization of observed data y\u2217 \u2208 SY . We can also recognize pY (y) as the marginal density for the random variable Y modulo the prior distribution \u03c00.\nNext, we define the Bayesian Potential as:\n\u03a6(u; y) := \u2212 ln(p0(y \u2212G(u))) (99)\nwhich is nothing but the negative log likelihood. With this potential in hand we can write the Bayesian Inference condition suggestively as:\nd\u03a0y d\u03a00 (u) = 1 pY (y) e\u2212\u03a6(u;y) . (100)\nOn the left hand side we have the Radon-Nikodym derivative of the posterior measure with respect to the prior measure. On the right hand side we have what we would like to interpret as the stationary distribution modulo observations y. This equation can be interpreted as follows: Given any measurable function f : SU \u2192 R\nE\u03a0y (f(U)) = E\u03a00\n(\nd\u03a0y d\u03a00 (U)f(U)\n)\n(101)\nmeaning the expectation value with respect to the posterior measure is the same as the expectation value with respect to the prior provided the function is augmented by the evidence in the form of the stationary distribution e \u2212\u03a6(u;y)\npY (y) ."
        },
        {
            "heading": "4.2 Dynamical Bayes",
            "text": "Dynamical Bayesian Inference is an extension of the conventional approach to Bayesian Inference in which one implements Bayesian inference as an iterative process where swathes of evidence are collected, and the posterior distribution at the end of a given iteration is used as the prior distribution in the following iteration [6].\nUsing such an approach, one can regard Bayesian Inference as a dynamical system governed by a first order differential equation. To begin, we define a timelike variable, T , which essentially corresponds to the total number of data point observed. Inference up to the \u201ctime\u201d T therefore leverages evidence from the set of observed data {yt}0\u2264t\u2264T which we can regard as a continuous time stochastic process coming from the data generating measure \u00b5\u2217Y (y) = p \u2217 Y (y)VolY . 10 The equation satisfied by \u03c0T (u) is then given by:\n\u2202\n\u2202T \u03c0T (u) = \u2212\n( D(pY |U)\u2212 E\u03a0T (D(pY |U(y | U)) ) \u03c0T (u) , (102)\nwhere here\nD(p) = DKL(p \u2217 Y \u2016 p) (103)\nis the KL-Divergence between a distribution p on SY and the data generating distribution p\u2217Y . 11\nAmong the most significant insights of this approach is that, if we consider models that are within an \u01eb neighborhood of the true underlying signal, the 2n-point functions described by \u03a0T are very well approximated by power laws:\nC i1...i2n := E\u03a0T\n(\n2n \u220f\nj=1\n(U \u2212 u\u2217)ij ) = 1\nT n\n\u2211\np\u2208P22n\n\u220f\n(r,s)\u2208p\nIiris \u2223 \u2223 \u2223\n\u2223\nu\u2217\n. (104)\nHere, Iij is the inverse of the Fisher Metric arising from the family of distributions pY |U , evaluated at the data generating parameter u\u2217."
        },
        {
            "heading": "4.3 Bayesian Diffusion for Normal Data",
            "text": "To illustrate the Dynamical Bayesian Inference dynamic, it will be beneficial to work through the problem of performing Bayesian inference on the mean of normally distributed data with\n10We shall regard the data generating measure as belonging to the parametric family pY |U , associated with the true underlying signal value u\u2217. That is: p\u2217Y (y) = pY |U (y | u\u2217).\n11Remarkably, this equation is equivalent to the replicator dynamic which appears in evolutionary game theory provided the fitness of a particular model pY |U (y | u) is taken to be minus the KL-Divergence with the data generating model. For more see [42][43].\nknown variance, \u03c32. In this case, we take:\ny = G(u) + n (105)\nwhere n is some noise distributed according to a distribution N (0, \u03c32):\np0(n) = 1\u221a 2\u03c0\u03c32 e\u2212 1 2\u03c32 n2 (106)\nand G(u) = \u00b5 is a random draw from the distribution over means for the data y so that p0(y \u2212 \u00b5) = N (\u00b5, \u03c32).12 The data generating distribution is taken to belong to the same parametric family of distributions, only with a fixed but unknown \u201ctrue\u201d underlying mean parameter \u00b5\u2217 \u2013 p \u2217 Y (y) = p(y | \u00b5\u2217).\nThe governing equation of Dynamical Bayesian Inference can be solved formally as:\n\u03c0T (u) = exp (\u2212T D(u)) exp ( \u222b T\nT0\ndT \u2032 D(\u03b1T \u2032)\n)\n. (107)\nHere we\u2019ve used an abbreviated notation in which D(u) = DKL(u\u2217 \u2016 u) is the KL-divergence between two distributions of the same paramteric form with given parameter values u. Notice that only the first exponential depends on the variable u, hence we conclude that the role of the second exponential is simply to maintain the normalization of \u03c0T as a probability distribution. Thus, we can write:\n\u03c0T (u) = 1\nZ exp (\u2212TD(u)) (108)\nwhere\nZ =\n\u222b VolU exp (\u2212T D(u)) = exp ( \u2212 \u222b T\nT0\ndT \u2032 D(\u03b1T \u2032)\n)\n. (109)\nIn the case of the normal model with fixed variance, the KL-divergence is given by\nD(\u00b5) = 1 2\u03c32 (\u00b5\u2212 \u00b5\u2217)2. The T -Posterior is therefore given by:\n\u03c0T (\u00b5) = 1\nZ e\u2212\nT 2\u03c32 (\u00b5\u2212\u00b5\u2217)2 . (110)\nIt is straightforward to determine the normalization of this distribution by performing the requisite integral which is now Gaussian. When all is said and done, the T dependent posterior density is of the form:\n\u03c0T (\u00b5) = 1 \u221a 2\u03c0(\u03c32/T ) e \u2212 1 2(\u03c32/T ) (\u00b5\u2212\u00b5\u2217)2 . (111)\n12For notational simplicity, we shall denote this distribution by p(y | \u00b5).\nLet us now compare (111) to the standard density of a length t increment of a Weiner\nprocess with diffusivity parameter \u03c3:13\nfWt(x) = 1\u221a\n2\u03c0\u03c32t e\u2212\nx2\n2\u03c32t . (114)\nWe can recognize (111) as describing a shifted Brownian motion for the mean parameter with diffusivity \u03c3 in the \u201ctime\u201d parameter \u03c4 = 1 T :\n\u03c0\u03c4 (\u00b5) = 1\u221a\n2\u03c0\u03c32\u03c4 e\u2212\n(\u00b5\u2212\u00b5\u2217) 2\n2\u03c32\u03c4 . (115)\nThis observation lends credence to the idea that Bayesian inference can be associated with a diffusion process. Moreover, it provides an important insight: Bayesian Diffusion ensues backwards with respect to the performance of Bayesian Inference, in the timelike parameter, \u03c4 , which is the inverse of the Bayesian time, T , originally introduced.\nGiven the \u03c4 -posterior \u03c0\u03c4 , and some terminal data such as the parametric form of the data generating distribution, one can obtain the \u03c4 path of the posterior predictive distribution for future data by marginalizing. For the normal model this means:\np\u03c4 (y) =\n\u222b\nR\nd\u00b5 \u03c0\u03c4 (\u00b5)p(y | \u00b5) = \u222b\nR\nd\u00b5 \u03c0\u03c4 (\u00b5)p0(y \u2212 \u00b5) , (116)\nwhich we can recognize as the convolution of \u03c0\u03c4 and p0(y), and interpret as the action of the Green Function of the Heat Operator translating the model forward in \u03c4 (and backward in T )."
        },
        {
            "heading": "4.4 Bayesian Drift and Scheme Independence",
            "text": "We have now shown that the solution to the Dynamical Bayesian inference equation, (111), is also the solution to the standard diffusion equation when viewed as transforming backwards relative to the update time, T . As we shall now discuss, we can promote (111) to a solution to a drift-diffusion equation by an analogous argument to the one that appeared in section (2.3). In particular, we argue that drift in the context of Bayesian diffusion is also associated\n13The density fWt(x) solves the heat equation:\n\u2202 \u2202t fWt(x) \u2212 \u03c32\n\u22022\n\u2202x2 fWt(x) = 0. (112)\nFollowing the analysis of section 3, one can also recognize the stochastic process {fWt(Xt)}t\u22650 as a martingale adapted to the Weiner process\ndXt = \u03c3dWt. (113)\nwith a redundancy of description, in this case related to the specific sequence with which data is observed.\nThe reasoning behind this argument is most easily understood through an example. Suppose again that one is performing Bayesian inference on a system which is taken to follow a normal distribution with known variance, \u03c32, but unknown mean. To deduce the mean of the distribution governing the system, we observe a sequence of N independent, identically distributed random draws from the true underlying distribution, E = {Y1, ..., YN}. Starting with a normal prior, one finds that the mean of the posterior distribution after observing the first n pieces of data shall be given by:\n\u00b5(En) = 1\nn\nn \u2211\ni=1\nYi . (117)\nLet \u03c0 \u2208 Perm(N) be a permutation, and let \u03c0(E) = {Y\u03c0(1), ..., Y\u03c0(N)} denote the same set of evidence, but in a new order defined by \u03c0. We interpret this transformation as changing the sequence in which the data is obtained. Crucially, \u03c0(E) is still a set of N independent, identically distributed random variables drawn from the same data generating distribution. If we compute the maximum likelihood estimate based on the first n piece of data appearing in \u03c0(E) we find:\n\u03c0(\u00b5(En)) = \u00b5(\u03c0(E)n) = 1\nn\nn \u2211\ni=1\nY\u03c0(i) . (118)\nand hence it is true that \u03c0(\u00b5(En)) 6= \u00b5(En). However, if we compute the maximum likelihood estimate on account of all of the data contained in either set, we find:\n\u03c0(\u00b5(E)) = 1\nN\nN \u2211\ni=1\nY\u03c0(i) = 1\nN\nN \u2211\ni=1\nYi = \u00b5(E) . (119)\nThat is, the terminal maximum likelihood estimate is invariant with respect to the sequence in which the data is incorporated into the model.\nWe extrapolate this observation to the statement that the posterior distribution can be assigned an arbitrary path through the space of probability models, provided the terminal distribution remains consistent with large observation limit, that is the central tendency towards the data generating distribution. This is completely analogous to the role of drift in defining an ERG scheme: The invariant definition of an ERG flow is given in terms of the IR fixed point it describes, the path through the space of theories by which the theory moves from the UV to the IR is scheme dependent.\nOperationally, we make use of the sequencing freedom in the Bayesian inference to \u201cseed\u201d\nthe flow with information about the individual inference trajectory by specifying the \u03c4 path of the maximum a posteriori estimate (MAP). In the general case where we are given a set of signal parameters u \u2208 SU , we specify the trajectory of the MAP as a flow on the manifold SU generated by a vector field V : SU \u2192 TSU . That is:\n\u03b3 : R\u2192 SU (120)\nsuch that\n\u03b3\u2217 d\nd\u03c4 = V (\u03b3\u03c4 ) (121)\nor, in more standard notation:\nd\nd\u03c4 \u03b3\u03c4 = \u03b3\u0307\u03c4 = V (\u03b3\u03c4 ) (122)\nThe trajectory of the MAP arises from maximizing the log-likelihood of the data gener-\nating model. Thus, in many cases the MAP path, \u03b3\u03c4 will realize a gradient descent:\n\u03b3\u0307\u03c4 = \u2212gradu ln(pY (y \u2212G(\u03b3\u03c4 ))) = gradu\u03a6(u; y) \u2223 \u2223 \u2223\n\u2223\n\u03b3\u03c4\n= \u2212Iij(\u03b3\u03c4 ) \u2202Gk \u2202ui \u2202\u03a6 \u2202yk\n\u2223 \u2223 \u2223 \u2223\n\u03b3\u03c4\n\u2202\n\u2202uj , (123)\nwhere here Iij are the matrix components of the inverse Fisher metric, and we have implemented the chain rule to evaluate the derivative. It is very crucial to note that the gradient descent here ensues in the direction of increased data observation. That is, as T \u2192 \u221e, \u03b3 approaches the data generating parameter value, as desired."
        },
        {
            "heading": "4.5 Generic Bayesian Diffusion at Late T",
            "text": "The Normal Model is significant because it arises as the late T limit of any Dynamical Bayesian Inference scheme for which the parameters of the data generating distribution are equal to some fixed values. This observation arises naturally as an asymptotic limit of the solution to (102), and is a statement of the Central Limit Theorem. At late T , the KL-Divergence can be approximated by the quadratic form:\nDKL(u\u2217 \u2016 u) = 1\n2 Iij(u\u2212 u\u2217)i(u\u2212 u\u2217)j +O((\n1 T )2) (124)\nmeaning we can write the unnormalized posterior distribution as:\n\u031fT (u) = e \u2212T 2 Iij(u\u2212u\u2217) i(u\u2212u\u2217)j . (125)\nProvided I does not depend on u, the normalization is obtained by performing a Gaussian integral and we can write the posterior distribution as:\n\u03c0\u03c4 (u) = 1 \u221a\n|2\u03c0\u03c4I\u22121| e\u2212\n1 2\u03c4 Iij(u\u2212u\u2217) i(u\u2212u\u2217)j . (126)\nHere we have changed variables to \u03c4 in order to observe that this is the distribution of a multi-variate Brownian motion.\nUsing the gauge freedom discussed in the last section we can promote this solution to\none of the form:\n\u03c0\u03c4 (u) = 1 \u221a\n|2\u03c0\u03c4I\u22121| e\u2212\n1 2\u03c4 Iij(u\u2212\u03b3\u03c4 ) i(u\u2212\u03b3\u03c4 )j (127)\nwhere \u03b3\u03c4 is the trajectory of the MAP. As advertised, both (126) and (127) are consistent with the late T statistics (104)."
        },
        {
            "heading": "4.6 A Partial Differential Equation for Bayesian Inference",
            "text": "In light of the previous sections, we shall now show that one can derive a convection-diffusion equation describing the evolution of the posterior predictive distribution when it is updated according to Bayes\u2019 law. As we have come to recognize, the sense in which Bayesian inference describes a diffusion process is in moving backwards relative to the observation of new information. We will therefore work in the time parameter \u03c4 , defined as the inverse to the time parameter T which tracks the amount of data observed. Given the time \u03c4 posterior distribution, which we have argued is of the form of a modified heat kernel, we can define the time \u03c4 posterior predictive model for future data, p\u03c4 by marginalizing over the likelihood model:\np\u03c4 (y) =\n\u222b\nSU\nVolU(u)\u03c0\u03c4 (u)pY |U(y | u) . (128)\nIn section 3 we reviewed the relationship between the solution to a convention-diffusion equation and the convolution of given boundary value data with a heat kernel. Comparing (128) with (86), it is natural to regard the posterior distribution as a Markovian transition kernel measuring the probability of going from y \u2208 SY to y \u2212 G(u) \u2208 SY . From this perspective, we define the set of operators {P\u03c4}\u03c4\u22650 such that:\nP\u03c4 (p0)(y) := E\u03a0\u03c4 (p0(y \u2212G(U))) . (129)\nFollowing the approach outlined in (87), we can now deduce the diffusion equation gen-\nerated by the semi-group {P\u03c4}\u03c4\u22650. By Taylor Expansion we can compute:\nP\u03c4 (p0)(y) = E\u03a0\u03c4\n(\np0(y)\u2212 \u2202p0 \u2202yi\n\u2202Gi \u2202uj u\u0307j\u03c4 + 1 2 \u22022p0 \u2202yi\u2202yj \u2202Gi \u2202uk \u2202Gj \u2202ul u\u0307ku\u0307l\u03c4 2 +O((u\u0307\u03c4)3)\n)\n. (130)\nHere we regard u\u0307i\u03c4 as corresponding to the infinitesimal flow of the parameter ui in terms of the vector field \u03b3\u0307\u03c4 generating the flow of the MAP. That is, u\u0307 i\u03c4 = \u03b4ui. Following this interpretation:\nP\u03c4 (p0)(y) = E\u03a0\u03c4\n(\np0(y)\u2212 \u2202p0 \u2202yi\n\u2202Gi \u2202uj \u03b3\u0307\u03c4\u03c4 + 1 2 \u22022p0 \u2202yi\u2202yj \u2202Gi \u2202uk \u2202Gj \u2202ul \u03b4uk\u03b4ul +O((\u03b4u)3)\n)\n. (131)\nWe can now use (104) to compute the expectation values explicitly to the order specified in the expansion. Because we are describing the diffusion in terms of the variable \u03c4 = 1/T and we shall eventually be taking the limit as \u03c4 \u2192 0 we can use the T \u2192\u221e results described in equations (104) and (127). In particular, we make use of the fact that\nE\u03a0\u03c4\n(\n2n \u220f\nj=1\n\u03b4uij\n)\n= 1\nT n\n\u2211\np\u2208P22n\n\u220f\n(r,s)\u2208p\nIiris . (132)\nMeaning we can write:\nP\u03c4 (p0)(y) = p0(y)\u2212 \u2202Gi \u2202uj \u03b3\u0307j\u03c4 \u2202p0 \u2202yi \u03c4 + \u2202Gi \u2202uk \u2202Gj \u2202ul Ikl(\u03b3\u03c4) 1 2 \u22022p0 \u2202yi\u2202yj \u03c4 +O(\u03c4 2) . (133)\nWe can rewrite these terms in a slightly more illuminating way by writing:\n\u2202Gi \u2202uk \u2202Gj \u2202ul Ikl(\u03b3\u03c4 ) = (G\u2217I\u22121)ij\n\u2223 \u2223 \u2223 \u2223\n\u03b3\u03c4\n= Kij(\u03b3\u03c4) . (134)\nMoreover, if we assume the MAP follows a gradient flow with respect to the log-likelihood, we can also write:\n\u2202Gi \u2202uj \u03b3\u0307j\u03c4 = G\u2217gradu\u03a6(u; y)\n\u2223 \u2223 \u2223 \u2223\nu=\u03b3\u03c4\n= \u2212\u2202G i \u2202uj \u2202Gk \u2202ul Ijl \u2202\u03a6 \u2202yk = \u2212Kij(\u03b3\u03c4 ) \u2202\u03a6 \u2202yj := mi . (135)\nPutting everything together, we have therefore shown that:\nP\u03c4 (p0)(y)\u2212 p0(y) \u03c4 = \u2212mi\u2202p0 \u2202yi +Kij(\u03b3\u03c4) \u22022p0 \u2202yi\u2202yj +O(\u03c4 2) . (136)\nThus, taking the limit \u03c4 \u2192 0 we obtain the result:\n\u2202p0 \u2202\u03c4 = lim \u03c4\u21920 P\u03c4 (p0)\u2212 p0 \u03c4 = \u2212mi\u2202p0 \u2202yi +Kij(\u03b3\u03c4 ) \u22022p0 \u2202yi\u2202yj . (137)\nThis is precisely of the form of the Kolmogorov equation for a diffusion process with potential V = \u03a6(\u03b3\u03c4 ; y) = \u03a6\u03c4 !\nIt is tempting to interpret the matrix Kij as being associated with the induced Fisher metric in the space of probability distributions along the path of the MAP. From this perspective, we can regard Y\u03c4 , the random variable associated with the T dependent posterior predictive distribution, as a stochastic process on a curved space specified by the stochastic differential equation:\ndY\u03c4 = \u2212grad \u03a6\u03c4 d\u03c4 + dW\u03c4 . (138)\nThe potential is given by \u03a6\u03c4 , which is minus the log-likelihood associated with the time \u03c4 maximum likelihood estimate for the generating distribution. Thus, we have shown that a Dynamical Bayesian inference induces a gradient flow with respect to the log-likelihood of the data generating distribution."
        },
        {
            "heading": "5 The ERG flow/Dynamical Bayesian Inference Cor-",
            "text": "respondence\nWe are now prepared to build the dictionary relating ERG flow and Bayesian Inference. For simplicity, we shall consider one parameter families of probability distributions on finite dimensional sample spaces, however it is a simple exercise to generalize these insights to the infinite dimensional case as well. To begin, let us review the work we have presented in the previous sections.\nIn section 2, we recalled the Wegner-Morris formulation for ERG and demonstrated that it is equivalent to a one parameter family of probability distributions described by a gradient flow with respect to the relative entropy:\n\u2202pt \u2202t = \u2212gradW2DKL(pt \u2016 qt) = \u2202 \u2202xi\n(\nptg ij \u2202\n\u2202xj \u03a3\n)\n= \u2202\n\u2202xi ( pt\u03a8 i ) . (139)\nIn this equation we interpret the time parameter t as a logarithm of the scale \u2013 t = ln\u039b. The distribution qt = q\u0302t/Zq, with q\u0302t = e \u2212V for some potential V , can be regarded as specifying the ERG scheme through the fixing of a stationary point along the flow generated by (139). The choice of V defines the functional, \u03a3 = \u2212 ln (\np\u0302t q\u0302t\n)\n, and the reparameterization kernel,\n\u03a8 = gradg\u03a3, and hence is equivalent to a choice of scheme in the standard Wegner-Morris\nsense. As we have reviewed in section 3, the Fokker-Planck equation is equivalent to the Kolmogorov Forward equation for the Stochastic Process governed by Stochastic Differential Equation:\ndXt = \u2212(gradgV )dt+ \u221a 2dWt . (140)\nThis is intuitively satisfying since such an equation describes a stochastic gradient descent of the potential function V . Once initial data is supplied in the form of a UV theory, p0, (139) completely describes an ERG flow terminating at an IR fixed point.\nIn section 4, we introduced the notion of Dynamical Bayesian Inference. Dynamical Bayesian Inference describes a one parameter family of probability distributions obtained by implementing Bayes\u2019 law using data collected from a continuous time stochastic process. To quantify this family of distributions, we introduced the \u201ctime\u201d parameter, T , which corresponds to the amount of data incorporated into the model. In the direction of increasing T , the inferred probability model converges onto the distribution generating the observed data. In this respect, a Dynamical Bayesian flow has the complexion of an inversion in the ERG sense because it begins with an uninformed prior distribution but eventually converges to an informed distribution. In the language of ERG, this describes a flow from an IR theory to a UV theory. The pair consisting of an uniformed prior, along with the specification of a sufficiently complete set of data therefore defines a flow terminating with a UV theory. We take this as our definition of a Dynamical Bayesian Flow.\nThis picture suggests Dynamical Bayesian Inference and ERG flow are inverses of each other. If we can find a Dynamical Bayesian inference which begins with a prior distribution equal to the IR fixed point of an ERG flow, and which terminates at a data generating distribution equal to the UV initial data of said ERG flow, we will have obtained an inversion of the ERG flow. Let us now describe a strategy for determining pairs of ERG flows with Dynamical Bayesian Flows."
        },
        {
            "heading": "5.1 Dynamical Bayesian Flow \u2192 ERG Flow",
            "text": "Suppose we are given a Dynamical Bayesian flow and asked to determine an ERG flow which is inverse to it. Recall, we have defined a Dynamical Bayesian Flow as the pair, (q0, {Yt}Tt=1), where q0 is an uninformed prior distribution, and {Yt}Tt=1 is a set of data generated by a distribution p\u2217. Using this data, it is straightforward to define the corresponding ERG flow as the one parameter family of probability distributions obtained from the Dynamical Bayesian flow when viewed as evolving backwards with respect to the time parameter T . In other words, one takes terminal distribution of the Dynamical Bayesian flow, p\u2217, as defining\nthe UV initial data of the ERG flow, and obtains subsequent probability distributions along the ERG flow by removing items of data from the inferred model.\nIn section 4.6, we derived a partial differential equation governing just such a situation, in which the posterior predictive distribution evolves backwards against the collection of new data. The resulting partial differential equation describes a diffusion process which we dubbed Bayesian Diffusion: That is, one obtains a one parameter family of probability distributions {p\u03c4} such that p0 = p\u2217 and for which:\n\u2202p\u03c4 \u2202\u03c4 +mi \u2202p\u03c4 \u2202yi \u2212Kij(\u03b3\u03c4 ) \u22022p\u03c4 \u2202yi\u2202yj = 0 . (141)\nThis differential equation describes the evolution of a probability model governing the stochastic process Y\u03c4 which itself is governed by the stochastic differential equation\ndY\u03c4 = \u2212(gradK\u03a6)d\u03c4 + dW\u03c4 . (142)\nHere \u03b3\u03c4 is a trajectory in the model space of the theory describing the path of the maximum a posteriori parameter estimate generated by the sequence of observed data, K is the pullback of the Fisher Information Metric on the space of models defined in (134), and \u03a6 is the log-likelihood function associated with the Bayesian Inference scheme.\nTogether, the aforementioned items constitute a choice of scheme for the Dynamical Bayesian flow. Bayesian diffusion can therefore be associated with an ERG flow governed by the Wegner-Morris equation:\n\u2202p\u03c4 \u2202\u03c4 = \u2212gradW2DKL(p\u03c4 \u2016 q\u03c4 ) = \u2202 \u2202yi\n(\np\u03c4K ij \u2202\n\u2202yj \u03a3\n)\n= \u2202\n\u2202yi ( p\u03c4\u03a8 i )\n(143)\nwhere now the stationary distribution, scheme function, and reparameterization kernel are respectively given by:\nq\u03c4 = e\u2212\u03a6(\u03b3\u03c4 ;y)\nZq ; \u03a3 = \u03a6(\u03b3\u03c4 ; y)\u2212 \u03a6(u; y); \u03a8 = gradK\u03a3 . (144)\nThis data, along with the distribution p\u2217, defines an ERG flow in the Wegner-Morris sense which, by construction, is the inverse of the Dynamical Bayesian flow we began with."
        },
        {
            "heading": "5.2 Dynamical Bayesian Flow \u2190 ERG Flow",
            "text": "Conversely, suppose that we are given an ERG flow in a Wegner-Morris form and asked to determine a Dynamical Bayesian flow which is inverse to it. To do so, we first identify the Bayesian Diffusion process that the ERG flow corresponds to. Since we have shown\nthat ERG and Bayesian Diffusion are governed by the same equations this is as simple as translating between the ERG scheme and the Dynamical Bayesian scheme. Comparing (139) and (143) we deduce that the ERG associated with an optimal transport with potential V and sample space metric g is equivalent to a Bayesian diffusion in which V is taken as the log-likelihood function, and g is identified with the metric K. This data is sufficient to define a Bayesian inference problem. Notice, if we go all the way back to (13), we can finally provide a conceptual understanding of the seed action: The seed action sets the log-likelihood for the Bayesian Inference scheme related to the ERG flow it defines."
        },
        {
            "heading": "5.3 A Dictionary",
            "text": "We summarize the analysis of this final section in Table 3. It provides a dictionary translating between the Wegner-Morris equations relevant to finite sample space ERG, infinite dimensional sample space ERG, and Bayesian Diffusion."
        },
        {
            "heading": "5.4 Renormalizability and Scale",
            "text": "The dictionary in Table 3 provides a blueprint for interpreting ERG in the language of Statistical Inference. In this role, our work suggests new approaches to understanding and resolving many interesting problems inside and outside of field theory. As a demonstration, we shall use this final section to discuss the meaning of renormalizability in the context of an ERG flow related to the Bayesian Inference paradigm.\nAn important observation in (3) is the correspondence between the Fisher Metric in the inference context and C\u0307\u039b(x, y) in the ERG context. In the exact renormalization of a free field theory, C\u0307\u039b(x, y) is the regulated two point function, and therefore sets a running momentum scale for operators in the theory. The Fisher Metric, I, plays an analogous role in the Bayesian Inference scheme as a generalized two point function encoding a notion of scale through the covariance between operators.\nThe interpretation of the Fisher Metric as defining an energy scale is made very clear when we consider the inverse Bayesian flow as a diffusion process. LetM = dens(S) denote the manifold of probability distributions over a sample space S. Then, a Bayesian diffusion, or equivalently an Exact Renormalization Group flow, can be described by a drift-diffusion process generated by an operator L :M\u2192M, such that: (\n\u2202\n\u2202t + L\n)\npt = 0 . (145)\nGiven initial data14, p0 \u2208M, we can write the solution to (145) symbolically as:\npt = e \u2212tL (p0) . (146)\nwhere e\u2212tL is the heat kernel of L. To give more concrete meaning to (146), let us assume that L is a positive definite operator which can be diagonalized as\nL(\u03c8n) = \u03bb 2 n\u03c8n . (147)\nHere {\u03c8n} is a countably infinite set forming a basis forM, and \u03bbn is non-negative, but may be equal to zero. An arbitrary element p \u2208 M can be expanded as a series: p = \u2211n pn\u03c8n, where pn is the coordinate of p in the eigendirection \u03c8n.\nThe spectrum of L defines an emergent energy scale in the following sense: Consider the\naction of (146) as given by:\npt = \u2211\nn\ne\u2212t\u03bb 2 npn0\u03c8n . (148)\n(148) dictates that the projection of pt onto each mode, \u03c8n, is damped over time with a strength determined by on the \u201cenergy\u201d \u03bb2n. At time t, the effective description exponentially suppresses modes that have large eigenvalues relative to the operator L, and thus sequentially removes these modes in a generalized integration over effective \u201cmomentum shells\u201d. From equation (137), we can see that the operator L is generically a convection-diffusion operator with a diffusion matrix given by the Fisher Information. Further comparing to (6), we see that in ERG for physical theories the analogous role is played by the regulated two point function. This leads to the important conclusion that, in physical contexts, the emergent energy scale is in fact equivalent to a physical one.\nMore generally, interpreting the two point function, or covariance matrix, in a statistical inference problem as generating an emergent energy scale provides the foundation for an\n14For example, in an ERG we would provide a UV theory.\ninformation theoretic interpretation of the conditions for renormalizability. In Wilsonian RG, a theory is said to be non-renormalizable if the divergences present in higher order Feynman diagrams can only be canceled by the introduction of an infinite number of arbitrarily high energy couplings [44]. By contrast, a theory is said to be renormalizable if arbitrarily higher order Feynman diagrams can be computed by introducing only a finite number of operator sourced counterterms.\nIn the language of statistical inference the operator content of a theory is related to the problem of model selection [45, 46]. In the context of parametric statistics15, model selection can be reduced to determining the set of sufficient parameters needed to form a model that can accurately compute the expectation values of any observable associated with the system of interest. A natural framing of this problem is given in terms of n-point correlation functions for the random variable, Y , observed throughout the inference. It is sensible to restrict our attention to n-point functions since arbitrary observables can be constructed from them using a Taylor expansion. Put differently, a probability distribution can be reconstructed with knowledge of all its moments.\nIn view of the previous discussion, higher n-point functions can be interpreted as encoding information at higher values of the emergent energy scale. This inspires the interpretation that an inference model is \u201crenormalizable\u201d if there exists a finite N such that for any n > N , the n-point function can be computed with the information contained only inm-point functions with m \u2264 N . In other words, there exists an energy as measured through L, \u03bb2\u2217, above which all of the information in the theory is actually encoded in lower energy operators. This happens, for example, in a Gaussian theory in which all n-point functions higher than N = 2 can be formulated as sums of products of 2-point functions using Wick\u2019s theorem. If no such finite N exists, the inference problem is \u201cnonrenormalizable\u201d. A nonrenormalizable theory can therefore be understood as a theory in which an infinite number of n-point functions will be required to compute the expectation values of arbitrary observables. In other words, there is no energy scale above which information becomes encoded in the energy scales below it \u2013 every energy scale contributes, in some sense, independently to the theory."
        },
        {
            "heading": "6 Discussion",
            "text": "In this note, we have demonstrated that an ERG flow can be identified with a diffusion process that is inversely related to a Dynamical Bayesian Inference scheme. In particular,\n15As we have mentioned in the introduction, the relationship between Wilsonian RG and ERG is analogous to the relationship between Parametric and Non Parametric statistics.\nwe have argued that ERG flow can be understood as a one parameter family of probability distributions arising where data is continuously removed from the inferred probability model. We have motivated this interpretation by illustrating that the equations governing ERG and Bayesian Diffusion can be brought into direct correspondence with one another, as outlined in 5. The resulting dictionary provides a novel, fully information theoretic language for understanding ERG flow. It also provides an operational answer to the question of what it means to \u201cinvert\u201d an ERG flow.\nFrom a very general perspective, the solution to this problem can be framed in the following way. Given a preliminary probability distribution, p0, we imagine running our model through a noisy channel generated by a diffusion operator B. In other words, we produce a probability distribution, p\u03c4 , which solves the differential equation:\n\u2202p\u03c4 \u2202\u03c4 +B(p\u03c4 ) = 0 , (149)\nwith initial data p0. After a given period of time, t, we obtain a new probability distribution, pt = e \u2212tB(p0), which has lost some of the information previously contained in p0 to diffusion. In the case of an ERG flow viewed from the functional diffusion perspective, we can regard this loss of information as being generated by a coarse-graining scheme encoded in the operator B.\nWe then ask the question, can this diffusion process can be \u201cinverted\u201d? Since exact inversion may not be possible, we frame this problem in the form of an optimization scheme. Consider the set F consisting of all operators F , generating a one parameter flow of probability distributions, qT , such that:\n\u2202qT \u2202T + F (qT ) = 0 . (150)\nsubject to an initial condition q0. If we take the initial data of this process to be the terminal distribution of the diffusion process given by (149), q0 = e \u2212tB(p0), we can interpret the solution qt = e \u2212tF (q0) as a reconstruction algorithm for the initial data, p0. It is natural to identify the optimal reconstruction algorithm as the operator F\u2217 \u2208 F for which the relative entropy between the reconstructed distribution qt, and the initial data, p0, is minimal:\nF \u2217 := argmin F\u2208F DKL(p0 \u2016 e\u2212tF \u25e6 e\u2212tBp0) . (151)\nIn this language, we interpret the main result of our paper as dictating that, given an ERG generated by a diffusion operator B, the optimal reconstruction operator F \u2217 corresponds to a continuous Bayesian Inference scheme in which the information lost to coarse-graining is\nre-learned, and hence reincorporated into the model. This clarifies the sense in which an ERG is \u201cinvertible\u201d as long as we allow for the reconstruction of information ostensibly destroyed by diffusion.\nOne can visualize this process as follows: Imagine an experimenter performing a statistical inference experiment in which they observe a collection of data {Yi}Ti=1, generated from the distribution p0. Next, imagine we can place each of the observations along the real axis, distinguishing a series of points, each of which we label by a probability distribution {pT}\u221eT=0. The probability distribution at the T th point, pT , is obtained by incorporating all of the data to the left of T into a model using Bayes\u2019 law. Moving to the left along this axis corresponds to dis incorporating data from the model, and therefore induces a diffusion process and by extension an ERG scheme. Conversely, moving to the right along this axis corresponds to reincorporating lost data, and therefore inverts the ERG flow.\nFraming the relationship between ERG and Statistical Inference in terms of the reconstruction problem (151) suggests several interesting paths for future study. Firstly, the reconstruction problem is equivalent to a common problem encountered in Machine Learning when one wishes to sample data from an analytically intractable distribution, p0. An approach to this problem goes by the name of Diffusion Learning [47\u201350]. Diffusion learning is a two step process: first, one uses a diffusion operator, B, with a known fixed point to transform the initial data p0 into an analytically tractable form. Then one identifies a second diffusion operator, F , which optimally reconstructs the initial data without sacrificing analytic tractability. This routine is equivalent to (151) provided we restrict the set of allowed reconstruction algorithms to operators that generate diffusion processes. More generally, the information theoretic formulation of ERG constructed in this paper renders renormalization in a form that is amenable to applications outside of pure physics. We hope this will catalyze continued work, especially at the intersection of physics and data science, geared towards constructing and better understanding machine learning algorithms like diffusion learning.\nA second fascinating implementation of (151) appears in the study of Holography [51]. There, one is interested in reconstructing a bulk spacetime from the data contained in a quantum field theory on its conformal boundary [52, 53]. The relationship between our work and bulk reconstruction is very natural. In modern literature, bulk reconstruction is often interpreted through the language of Quantum Error Correction as the inversion of a quantum channel associated with the propagation of bulk data into a subregion of the conformal boundary [54\u201357]. Placed in this context, the bulk reconstuction problem is a non-commutative generalization of (151) in which one replaces probability distributions by density operators, and the maps B and F by Quantum Channels [58, 59].16 This suggests\n16See appendix B for a short discussion of the relationship between this paper and error correction.\nthat by studying a quantum version of the correspondence introduced in this paper, one might be able to shed light on some of the mysterious aspects of the AdS/CFT correspondence. Beyond this, there are also interesting questions which pertain to the generalization of our correspondence into the general language of non-commutative probability theory, C\u2217 operator algebras. These include understanding statistical inference for non-commutative operator algebras [60\u201362], the study of non-commutative diffusion processes [63\u201365], and the formulation of renormalization techniques that work beyond the scope of Euclidean QFT in the vein of entanglement renormalization and tensor networks [66\u201371].\nFinally, our picture of ERG can provide a powerful tool for constructing and interpreting theorems about renormalization. By formulating ERG flows as a related Statistical Inference problem, one obtains a stark accounting of the information/degrees of freedom contained in the renormalized theory at any point over the course of its flow. Such knowledge is of great use in computing the information lost between various points along an RG flow and, especially, in constructing and interpreting RG monotones [72\u201376]. We are hopeful that our conceptual approach to the ERG can expand renormalization in its role as a toolset for studying the space of Quantum Field Theories."
        },
        {
            "heading": "7 Acknowledgements",
            "text": "We thank Jonathan Heckman for collaboration on dynamical Bayes in [6] which led to many of the ideas in this paper. We are grateful to the participants of the \u201cString Data 2022\u201d conference where this work was first presented and subsequent comments from Semon Rezchikov and Miranda Cheng. We also wish to thank Alex Stapleton for related collaboration on forth-coming work on diffusion models and reconstruction channels in holography. Finally, we thank Samuel Goldman and Robert Leigh for enlightening discussions on Exact Renormalization. DSB acknowledges support from Pierre Andurand over the course of this research. MSK is supported through the Physics department at the University of Illinois at Urbana-Champaign."
        },
        {
            "heading": "A Optimal Transport",
            "text": "Optimal transport consists in redistributing the mass between two probability measures in order to accomplish a cost minimization. To be precise, let Y \u2208 S be a random variable in the sample space S. We regard S as an orientable differentiable manifold possessing a reference measure, VolS(y), which is simply the volume form on S. A probability measure can then be obtained by considering a measurable function: p : S \u2192 R+ which is normalized in the integral sense:\n\u222b\nS\n\u22c6p =\n\u222b\nS\np(y)VolS(y) = 1 (152)\nOptimal transport can then be stated in two equivalent forms. The Monge Formulation is as follows: Given a space S and two probability distributions p1 and p2 corresponding to measures \u00b51 = \u22c6p1 and \u00b52 = \u22c6p2 we seek a transport function T : S \u2192 S such that:\n1.\nT \u2217\u00b52 = \u00b51 (153)\nmeaning the transport function pulls back mass from \u00b52 to \u00b51, or\n\u222b\nT (U)\n\u00b52 =\n\u222b\nU\n\u00b51 (154)\nfor any subset U \u2282 S. Note, this is usually written in terms of the \u201cpushforward\u201d: T#\u00b51 = \u00b52 which is the pullback by the inverse map T \u22121 : S \u2192 S i.e. \u00b52 = (T\u22121)\u2217\u00b5\u2018.\n2. The transport function is selected to minimize the objective function:\nM [T ] =\n\u222b\nS\n\u00b51(y)c(y, T (y)) (155)\nwhere c : S \u00d7 S is some cost function which is typically associated with a distance on the sample space.\nAlternatively, we can state the optimal transport problem in the Kantorovich Formulation. In that case one begins with a joint measure \u03a0 on the sample space S\u00d7S that pushes forward (in the sense of integrating along the fiber, or simply marginalizing in the probabilistic sense) to the measure \u00b51 and \u00b52 respectively. That is: \u03a0(y1, y2) = \u03c0(y1, y2)VolS(y1)\u2227VolS(y2)\n1.\n\u00b51(y1) = VolS(y1) \u2227 \u222b\nS\nVolS(y2)\u03c0(y1, y2) \u00b52(y2) = VolS(y2) \u2227 \u222b\nS\nVolS(y1)\u03c0(y1, y2)\n(156)\nWe shall henceforth denote by \u0393(\u00b51, \u00b52) the set of joint measures which marginalize to \u00b51 and \u00b52.\n2. The joint measure \u03a0 is chosen so as to minimize the joint expectation value for the\ncost:\nK(\u03a0) =\n\u222b\nS\u00d7S\n\u03a0(y1, y2)c(y1, y2) (157)\nNotice, if we choose \u03c0(y1, y2) = p1(x)\u03b4(y \u2212 T (x)) the Kantorovich objective function is equivalent to the Monge function.\nThere is an important theorem which states that for the \u21132 cost function, c(y1, y2) = \u2016y1 \u2212 y2\u20162, there exists a smooth solution to the Monge problem. In particular, there will exist a smooth function f : S \u2192 R for which\nT i(y) = gij(y) \u2202\n\u2202yj f(y) (158)\nor\nT (y) = gradf(y) (159)\nRecall the relationship between \u00b51 and \u00b52: \u00b51 = T \u2217\u00b52 or\n\u00b51 = T \u2217(\u00b52) = det\n(\n\u2202T i(y)\n\u2202yj\n)\np2(T (y))VolS(y) (160)\nor, with respect to the probability distributions:\np1(y) = det\n(\n\u2202T i \u2202yj\n)\np2(T (y)) (161)\nHence, with respect to the solution we have specified in (158) we can write:\np1(y) = det\n(\n\u2202\n\u2202yj gik(y)\n\u2202\n\u2202yk f\n)\np2(gradf(y)) (162)\nor, put more suggestively: p1(y)\np2(gradf(y)) = \u2206f(y) (163)\nwhere here \u2206 is the Laplacian.\nThe Wasserstein Distance is a metric on the space of probability measures which is\ndefined as the solution to an optimal transport problem with a specified cost:\nWc(\u00b51, \u00b52) := inf\u03a0\u2208\u0393(\u00b51,\u00b52) \u222b\nS\u00d7S\n\u03a0(y1, y2)c(y1, y2) (164)\nOf particular interest to us will be the Wasserstein two distance, which is the Wasserstein distance defined with respect to the \u21132 cost function:\nW2(\u00b51, \u00b52) = ( inf\u03a0\u2208\u0393(\u00b51,\u00b52) \u222b\nS\u00d7S\n\u03a0(y1, y2)\u2016y1 \u2212 y2\u20162 )1/2\n(165)"
        },
        {
            "heading": "B ERG and Error Correction",
            "text": "In this section, we would like to note how the work we have presented in this paper connects to a related approach to understanding RG through the language of quantum error correction [55\u201357].\nGiven an operator algebra, A, corresponding to a set of observable degrees of freedom, a theory can be thought of as a state, or, in more physical language, as a density operator, which assigns to each operator an expectation value. We shall denote the set of states on A as A\u2217. A quantum channel, E : A\u2217 \u2192 B\u2217, is a completely positive, trace preserving, linear map from states on an operator algebra A to states on an operator algebra B. A quantum channel is a natural mathematical representation for the generator of an RG flow because of the Data Processing Inequality :\nDKL(\u03c1 \u2016 \u03c1\u2032) \u2265 DKL(E(\u03c1) \u2016 E(\u03c1\u2032)) (166)\nOne can interpret (166) as stating that the distinguishability of states is decreased under the action of any quantum channel. For this reason, a quantum channel is sometimes also referred to as a coarse-graining map, in analogy with an RG flow.\nA quantum channel is exactly reversible if and only if it is sufficient in the sense that no information is lost: that is for all \u03c1, \u03c1\u2032 \u2208 A\u2217 the data processing inequality is saturated and\nDKL(\u03c1 \u2016 \u03c1\u2032) = DKL(E(\u03c1) \u2016 E(\u03c1\u2032)) (167)\nIn this case, there will exist a complimentary channel, P\u03c1\u2032,E : B\u2217 \u2192 A\u2217, called the Petz Map such that P\u03c1\u2032,E \u25e6 E(\u03c1) = \u03c1.\nIn general, (167) will only be met for a subset of states, C\u2217 \u2282 A\u2217. The operator algebra associated with this set of states is denote by C, and called the Code Subspace. The operators\nwhich live inside the code-subspace are defined by the property that their expectation values are invariant under the flow generated by E . Leveraging the interpretation of a quantum channel as generating an RG Flow, we can therefore interpret the code subspace of E as corresponding to the set of relevant operators.\nIn this paper we have provided a concrete link between information theory and ERG through the intermediary of statistical inference by applying the concept of Bayesian inversion to the heat flow describing an ERG. Regarding Bayesian Inference as dual to ERG fits neatly into the Error Correcting picture discussed above. The Petz Map of a quantum channel E can be defined as its formal adjoint with respect to a non-commutative generalization of the Fisher Information Metric on A\u2217.17\ng\u03c1(X,P\u03c1,E(Y )) = gE(\u03c1)(E(X), Y ) (169)\nWhen the operator algebra in question is commutative the set of states becomes equivalent to the space of probability distributions, and the metric g\u03c1 reduces to the unique Fisher Metric on this space. A consequence of this fact is that the Petz Map for commutative operator algebras as obtained through (169), is equivalent the Bayesian posterior with prior \u03c1. We prove this statement now:\nLet A be a commutative algebra. For example, its elements may correspond to the set of measurable functions on a domain, S, or, in the parlance of Probability Theory, random variables on the sample space S. A state on A is then a probability measure on S, which is a measurable function p : S \u2192 R that is normalized in the integral sense:\nTrVol(p) =\n\u222b\nS\nVolS(x)p(x) = 1 (170)\nHere VolS is the reference measure on S, if S is an orientable differentiable manifold this is nothing but the volume form. The pairing of a state, p \u2208 A\u2217 and a random variable f \u2208 A is the computation of an expectation value:\np[f ] := Ep(f(X)) = TrVol(pf) =\n\u222b\nS\nVolS(x)p(x)f(x) (171)\nA channel between states on commutative algebras, E : A\u2217 \u2192 B\u2217, is a Stochastic Map. 17g\u03c1 : T\u03c1A\u2217 \u00d7 T\u03c1A\u2217 \u2192 R, such that g\u03c1(X,Y ) = Tr(X\u2126\u22121\u03c1 (Y )), where here \u2126\u22121\u03c1 is a map that corresponds\nto a non-commutative generalization of \u201cdivision by \u03c1\u201d. Explicitly,\n\u2126\u22121\u03c1 (Y ) = d\ndt\n\u2223 \u2223 \u2223 \u2223\nt=0\nlog(\u03c1+ tY ) (168)\nThe metric g\u03c1 takes the schematic form, Tr( XY \u03c1 ), which is equivalent to the usual form of the Fisher\nMetric.\nLet SA and SB denote the sample spaces associated with the algebras A and B, respectively. Then, we can associate to E a conditional probability distribution: pB|A : A\u00d7 B \u2192 R such that:\n\u222b\nSB\nVolB(y)pB|A(y | x) = 1; \u2200x\u2208SA (172)\nand \u222b\nR\u2282SB\nVolB(y)pB|A(y | x) = P(y \u2208 R | X = x) (173)\nMore to the point, given a marginal probability distribution pA \u2208 A\u2217, the action of the map E is given by the following integral:\nE(pA) = \u222b\nSA\nVolA(x)pB|A(y | x)pA(x) \u2208 B\u2217 (174)\nThe Fisher Metric on A\u2217 takes the form:18\ngpA(U, V ) = TrVolA\n(\nUV\npA\n)\n=\n\u222b\nSA\nVolA(x) U(x)V (x)\npA(x) (176)\nHere U, V : SA \u2192 R are elements of TpAA\u2217, which, in the vein of (17), we identify with measurable functions on the sample space which have zero weight when integrated over the sample space with respect to the reference measure. This guarantees that such random variables can be identified with perturbations to a probability density which maintain the integral normalization condition.\nWith all of this terminology in place, we are now prepared to understand the implication of the Petz Map for commutative algebras. To begin, let us remark that the Petz map, as a channel, PpA,E : B\u2217 \u2192 A\u2217, can be associated with a stochastic map qA|B : SA\u00d7SB \u2192 R with\nPpA,E(pB) = \u222b\nSB\nVolB(y)qA|B(x | y)pB(y) \u2208 A\u2217 (177)\n18This form of the Fisher Metric should be compared with the more standard form:\n(gpA)ij =\n\u222b\nSA\nVolA(x)pA(x | \u03b8) \u2202 log(pA(x | \u03b8)) \u2202\u03b8i \u2202 log(pA(x | \u03b8)) \u2202\u03b8j = \u222b\nSA\nVolA(x) 1 pA(x | \u03b8) \u2202pA(x | \u03b8) \u2202\u03b8i \u2202pA(x | \u03b8) \u2202\u03b8j\n(175) Here \u03b8 are a set of parameters specifying a probability density in a parametric family. In the first equality, one regards \u2113i = \u2202 log(pA(x|\u03b8)) \u2202\u03b8i as a basis for TpAA\u2217, with TpAA\u2217 identified as the set of random variables on SA with zero expectation value. The second equality arises form a simple algebraic manipulation of the first, but implies a different condition on TpAA\u2217, namely that is the set of measurable functions on SA that integrate to zero. The second condition is consistent with our chosen specification of TpAA\u2217, hence why we have chosen the form of the Fisher Metric in (176)\nThus, the adjoint condition:\ngpA(U,PpA,E(V )) = gE(pA)(E(U), V ) (178)\nimplies the following. First, the left hand side can be written:\ngpA(U,PpA,E(V )) = \u222b\nSA\nVolA(x) U(x)\n(\n\u222b\nSB VolB(y)qA|B(x | y)V (y)\n)\npA(x) (179)\n=\n\u222b\nSA\u00d7SB\nVolA(x) \u2227 VolB(y) U(x)qA|B(x | y)V (y)\npA(x) (180)\nSimilarly, the right hand side is of the form:\ngE(pA)(E(U), V ) = \u222b\nSB\nVolB(y)\n(\n\u222b\nSA VolA(x)pB|A(y | x)U(x)\n)\nV (y) \u222b\nSA VolA(x\u2032)pB|A(y | x\u2032)pA(x\u2032)\n(181)\n=\n\u222b\nSA\u00d7SB\nVolA(x) \u2227 VolB(y) U(x)pB|A(y | x)V (y) \u222b\nSA VolA(x\u2032)pB|A(y | x\u2032)pA(x\u2032)\n(182)\nEquating the left hand side and the right hand side for arbitrary functions U : SA \u2192 R and V : SB \u2192 R we therefore find:\nqA|B(x | y) pA(x) = pB|A(y | x) \u222b\nSA VolA(x\u2032)pB|A(y | x\u2032)pA(x\u2032)\n(183)\nThis is Bayes\u2019 Law."
        }
    ],
    "title": "The Inverse of Exact Renormalization Group Flows as Statistical Inference",
    "year": 2022
}