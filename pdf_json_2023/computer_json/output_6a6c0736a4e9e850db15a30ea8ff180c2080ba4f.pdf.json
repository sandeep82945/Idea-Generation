{
    "abstractText": "Generating synthetic datasets for training face recognition models is challenging because dataset generation entails more than creating high fidelity images. It involves generating multiple images of same subjects under different factors (e.g., variations in pose, illumination, expression, aging and occlusion) which follows the real image conditional distribution. Previous works have studied the generation of synthetic datasets using GAN or 3D models. In this work, we approach the problem from the aspect of combining subject appearance (ID) and external factor (style) conditions. These two conditions provide a direct way to control the inter-class and intra-class variations. To this end, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our novel Patch-wise style extractor and Time-step dependent ID loss enables DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition models trained on synthetic images from the proposed DCFace provide higher verification accuracies compared to previous works by 6.11% on average in 4 out of 5 test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code Link",
    "authors": [
        {
            "affiliations": [],
            "name": "Minchul Kim"
        },
        {
            "affiliations": [],
            "name": "Feng Liu"
        },
        {
            "affiliations": [],
            "name": "Anil Jain"
        },
        {
            "affiliations": [],
            "name": "Xiaoming Liu"
        }
    ],
    "id": "SP:6788a85e1a722e8d0c83f3dc6457106b4a3fd276",
    "references": [
        {
            "authors": [
                "Vishal Asnani",
                "Xi Yin",
                "Tal Hassner",
                "Sijia Liu",
                "Xiaoming Liu"
            ],
            "title": "Proactive image manipulation detection",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Gwangbin Bae",
                "Martin de La Gorce",
                "Tadas Baltrusaitis",
                "Charlie Hewitt",
                "Dong Chen",
                "Julien Valentin",
                "Roberto Cipolla",
                "Jingjing Shen"
            ],
            "title": "Digiface-1m: 1 million digital face images for face recognition",
            "venue": "In WACV,",
            "year": 2023
        },
        {
            "authors": [
                "Volker Blanz",
                "Thomas Vetter"
            ],
            "title": "A morphable model for the synthesis of 3D faces",
            "venue": "In SIGGRAPH,",
            "year": 1999
        },
        {
            "authors": [
                "Andreas Blattmann",
                "Robin Rombach",
                "Kaan Oktay",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Retrieval-augmented diffusion models",
            "venue": "arXiv preprint arXiv:2204.11824,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Brock",
                "Jeff Donahue",
                "Karen Simonyan"
            ],
            "title": "Large scale gan training for high fidelity natural image synthesis",
            "venue": "arXiv preprint arXiv:1809.11096,",
            "year": 2018
        },
        {
            "authors": [
                "Qiong Cao",
                "Li Shen",
                "Weidi Xie",
                "Omkar M Parkhi",
                "Andrew Zisserman"
            ],
            "title": "VGGFace2: A dataset for recognising faces across pose and age",
            "venue": "In FG,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiyi Cheng",
                "Xiatian Zhu",
                "Shaogang Gong"
            ],
            "title": "Lowresolution face recognition",
            "venue": "In ACCV,",
            "year": 2018
        },
        {
            "authors": [
                "Yunjey Choi",
                "Minje Choi",
                "Munyoung Kim",
                "Jung-Woo Ha",
                "Sunghun Kim",
                "Jaegul Choo"
            ],
            "title": "StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Jiankang Deng",
                "Shiyang Cheng",
                "Niannan Xue",
                "Yuxiang Zhou",
                "Stefanos Zafeiriou"
            ],
            "title": "UV-GAN: Adversarial facial uv map completion for pose-invariant face recognition",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR. Ieee,",
            "year": 2009
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "ArcFace: Additive angular margin loss for deep face recognition",
            "venue": "In CVPR, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Yu Deng",
                "Jiaolong Yang",
                "Dong Chen",
                "Fang Wen",
                "Xin Tong"
            ],
            "title": "Disentangled and controllable face image generation via 3D imitative-contrastive learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoidweighted linear units for neural network function approximation in reinforcement learning",
            "venue": "Neural Networks,",
            "year": 2018
        },
        {
            "authors": [
                "Joshua J Engelsma",
                "Steven A Grosz",
                "Anil K Jain"
            ],
            "title": "Printsgan: synthetic fingerprint generator",
            "venue": "TPAMI, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Baris Gecer",
                "Binod Bhattarai",
                "Josef Kittler",
                "Tae-Kyun Kim"
            ],
            "title": "Semi-supervised adversarial learning to generate photorealistic face images of new identities from 3D morphable model",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Zhenglin Geng",
                "Chen Cao",
                "Sergey Tulyakov"
            ],
            "title": "3D guided fine-grained face manipulation",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Sharath Girish",
                "Saksham Suri",
                "Sai Saketh Rambhatla",
                "Abhinav Shrivastava"
            ],
            "title": "Towards discovery and attribution of open-world gan generated images",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Yandong Guo",
                "Lei Zhang",
                "Yuxiao Hu",
                "Xiaodong He",
                "Jianfeng Gao"
            ],
            "title": "MS-Celeb-1M: A dataset and benchmark for large-scale face recognition",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash",
            "venue": "equilibrium. NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Qiyang Hu",
                "Attila Szab\u00f3",
                "Tiziano Portenier",
                "Paolo Favaro",
                "Matthias Zwicker"
            ],
            "title": "Disentangling factors of variation by mixing them",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Yuan-Ting Hu",
                "Jiahong Wang",
                "Raymond A Yeh",
                "Alexander G Schwing"
            ],
            "title": "Sail-vos 3d: A synthetic dataset and baselines for object detection and 3d mesh reconstruction from video data",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Gary Huang",
                "Marwan Mattar",
                "Honglak Lee",
                "Erik Learned-Miller"
            ],
            "title": "Learning to align from scratch",
            "venue": "NeurIPS,",
            "year": 2012
        },
        {
            "authors": [
                "Gary B Huang",
                "Marwan Mattar",
                "Tamara Berg",
                "Eric Learned-Miller"
            ],
            "title": "Labeled Faces in the Wild: A database forstudying face recognition in unconstrained environments",
            "venue": "In Workshop on Faces in\u2019Real-Life\u2019Images: Detection, Alignment, and Recognition,",
            "year": 2008
        },
        {
            "authors": [
                "Yuge Huang",
                "Yuhan Wang",
                "Ying Tai",
                "Xiaoming Liu",
                "Pengcheng Shen",
                "Shaoxin Li",
                "Jilin Li",
                "Feiyue Huang"
            ],
            "title": "CurricularFace: adaptive curriculum learning loss for deep face recognition",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoyi Jiang",
                "Michael Binkert",
                "Bernard Achermann",
                "Horst Bunke"
            ],
            "title": "Towards detection of glasses in facial images",
            "venue": "Pattern Analysis & Applications,",
            "year": 2000
        },
        {
            "authors": [
                "Nathan D Kalka",
                "Brianna Maze",
                "James A Duncan",
                "Kevin O\u2019Connor",
                "Stephen Elliott",
                "Kaleb Hebert",
                "Julia Bryan",
                "Anil K Jain"
            ],
            "title": "IJB\u2013S: IARPA Janus Surveillance Video Benchmark",
            "venue": "BTAS,",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In CVPR, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Hyeongwoo Kim",
                "Pablo Garrido",
                "Ayush Tewari",
                "Weipeng Xu",
                "Justus Thies",
                "Matthias Niessner",
                "Patrick P\u00e9rez",
                "Christian Richardt",
                "Michael Zollh\u00f6fer",
                "Christian Theobalt"
            ],
            "title": "Deep video portraits",
            "year": 2018
        },
        {
            "authors": [
                "Minchul Kim",
                "Anil K Jain",
                "Xiaoming Liu"
            ],
            "title": "AdaFace: Quality adaptive margin for face recognition",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Minchul Kim",
                "Feng Liu",
                "Anil Jain",
                "Xiaoming Liu"
            ],
            "title": "Cluster and aggregate: Face recognition with large probe",
            "venue": "set. NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "David Kupas",
                "Balazs Harangi"
            ],
            "title": "Solving the problem of imbalanced dataset with synthetic image generation for cell classification using deep learning",
            "venue": "In EMBC,",
            "year": 2021
        },
        {
            "authors": [
                "Tuomas Kynk\u00e4\u00e4nniemi",
                "Tero Karras",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Improved precision and recall metric for assessing generative models",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "HyunJae Lee",
                "Hyo-Eun Kim",
                "Hyeonseob Nam"
            ],
            "title": "Srm: A style-based recalibration module for convolutional neural networks",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Jianxin Lin",
                "Yingce Xia",
                "Tao Qin",
                "Zhibo Chen",
                "Tie- Yan Liu"
            ],
            "title": "Conditional image-to-image translation",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Feng Liu",
                "Minchul Kim",
                "Anil Jain",
                "Xiaoming Liu"
            ],
            "title": "Controllable and guided face synthesis for unconstrained face recognition",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Weiyang Liu",
                "Yandong Wen",
                "Zhiding Yu",
                "Ming Li",
                "Bhiksha Raj",
                "Le Song"
            ],
            "title": "SphereFace: Deep hypersphere embedding for face recognition",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Yaojie Liu",
                "Xiaoming Liu"
            ],
            "title": "Spoof trace disentanglement for generic face anti-spoofing",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Safa C. Medin",
                "Bernhard Egger",
                "Anoop Cherian",
                "Ye Wang",
                "Joshua B. Tenenbaum",
                "Xiaoming Liu",
                "Tim K. Marks"
            ],
            "title": "MOST-GAN: 3d morphable stylegan for disentangled face image manipulation",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Stylianos Moschoglou",
                "Athanasios Papaioannou",
                "Christos Sagonas",
                "Jiankang Deng",
                "Irene Kotsia",
                "Stefanos Zafeiriou"
            ],
            "title": "AGEDB: the first manually collected, in-the-wild age database",
            "venue": "In CVPRW, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Thu Nguyen-Phuoc",
                "Chuan Li",
                "Lucas Theis",
                "Christian Richardt",
                "Yong-Liang Yang"
            ],
            "title": "HoloGAN: Unsupervised learning of 3d representations from natural images",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Jingtan Piao",
                "Chen Qian",
                "Hongsheng Li"
            ],
            "title": "Semisupervised monocular 3D face reconstruction with end-toend shape-preserved domain transfer",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Konpat Preechakul",
                "Nattanat Chatthee",
                "Suttisak Wizadwongsa",
                "Supasorn Suwajanakorn"
            ],
            "title": "Diffusion autoencoders: Toward a meaningful and decodable representation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Albert Pumarola",
                "Antonio Agudo",
                "Aleix M Martinez",
                "Alberto Sanfeliu",
                "Francesc Moreno-Noguer"
            ],
            "title": "Ganimation: Anatomically-aware facial animation from a single image",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Haibo Qiu",
                "Baosheng Yu",
                "Dihong Gong",
                "Zhifeng Li",
                "Wei Liu",
                "Dacheng Tao"
            ],
            "title": "SynFace: Face recognition with synthetic data",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training",
            "venue": "gans. NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Soumyadip Sengupta",
                "Jun-Cheng Chen",
                "Carlos Castillo",
                "Vishal M Patel",
                "Rama Chellappa",
                "David W Jacobs"
            ],
            "title": "Frontal to profile face verification in the wild",
            "venue": "In WACV,",
            "year": 2016
        },
        {
            "authors": [
                "Zeyang Sha",
                "Zheng Li",
                "Ning Yu",
                "Yang Zhang"
            ],
            "title": "Defake: Detection and attribution of fake images generated by text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2210.06998,",
            "year": 2022
        },
        {
            "authors": [
                "Yujun Shen",
                "Bolei Zhou",
                "Ping Luo",
                "Xiaoou Tang"
            ],
            "title": "Facefeat-GAN: a two-stage approach for identity-preserving face synthesis",
            "venue": "arXiv preprint arXiv:1812.01288,",
            "year": 2018
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Conor Durkan",
                "Iain Murray",
                "Stefano Ermon"
            ],
            "title": "Maximum likelihood training of score-based diffusion models",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Improved techniques for training score-based generative models",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Joel Stehouwer",
                "Amin Jourabloo",
                "Yaojie Liu",
                "Xiaoming Liu"
            ],
            "title": "Noise modeling, synthesis and classification for generic object anti-spoofing",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Tiancheng Sun",
                "Jonathan T Barron",
                "Yun-Ta Tsai",
                "Zexiang Xu",
                "Xueming Yu",
                "Graham Fyffe",
                "Christoph Rhemann",
                "Jay Busch",
                "Paul E Debevec",
                "Ravi Ramamoorthi"
            ],
            "title": "Single image portrait relighting",
            "year": 2019
        },
        {
            "authors": [
                "Luan Tran",
                "Xi Yin",
                "Xiaoming Liu"
            ],
            "title": "Disentangled representation learning gan for pose-invariant face recognition",
            "venue": "In CVPR, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Tremblay",
                "Aayush Prakash",
                "David Acuna",
                "Mark Brophy",
                "Varun Jampani",
                "Cem Anil",
                "Thang To",
                "Eric Cameracci",
                "Shaad Boochoon",
                "Stan Birchfield"
            ],
            "title": "Training deep networks with synthetic data: Bridging the reality gap by domain randomization",
            "venue": "CVPRW,",
            "year": 2018
        },
        {
            "authors": [
                "Boris van Breugel",
                "Trent Kyono",
                "Jeroen Berrevoets",
                "Mihaela van der Schaar"
            ],
            "title": "Decaf: Generating fair synthetic data using causally-aware generative networks",
            "venue": "NeurIPS, 34:22221\u201322233,",
            "year": 2021
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of Machine Learning Research,",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Wang",
                "Yitong Wang",
                "Zheng Zhou",
                "Xing Ji",
                "Dihong Gong",
                "Jingchao Zhou",
                "Zhifeng Li",
                "Wei Liu"
            ],
            "title": "CosFace: Large margin cosine loss for deep face recognition",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Sheng-Yu Wang",
                "Oliver Wang",
                "Richard Zhang",
                "Andrew Owens",
                "Alexei A Efros"
            ],
            "title": "Cnn-generated images are surprisingly easy to spot... for now",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Tengfei Wang",
                "Ting Zhang",
                "Bo Zhang",
                "Hao Ouyang",
                "Dong Chen",
                "Qifeng Chen",
                "Fang Wen"
            ],
            "title": "Pretraining is all you need for image-to-image translation",
            "venue": "arXiv preprint arXiv:2205.12952,",
            "year": 2022
        },
        {
            "authors": [
                "Cameron Whitelam",
                "Emma Taborsky",
                "Austin Blanton",
                "Brianna Maze",
                "Jocelyn Adams",
                "Tim Miller",
                "Nathan Kalka",
                "Anil K Jain",
                "James A Duncan",
                "Kristen Allen"
            ],
            "title": "IARPA Janus Benchmark-B face dataset",
            "venue": "CVPRW,",
            "year": 2017
        },
        {
            "authors": [
                "Tianxing Wu"
            ],
            "title": "Realtime glasses detection",
            "venue": "https:// github.com/TianxingWu/realtime-glassesdetection,",
            "year": 2022
        },
        {
            "authors": [
                "Andre Brasil Vieira Wyzykowski",
                "Anil K Jain"
            ],
            "title": "Synthetic latent fingerprint generator",
            "venue": "In WACV,",
            "year": 2023
        },
        {
            "authors": [
                "Taihong Xiao",
                "Jiapeng Hong",
                "Jinwen Ma"
            ],
            "title": "Elegant: Exchanging latent encodings with GAN for transferring multiple face attributes",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Dong Yi",
                "Zhen Lei",
                "Shengcai Liao",
                "Stan Z Li"
            ],
            "title": "Learning face representation from scratch",
            "venue": "arXiv preprint arXiv:1411.7923,",
            "year": 2014
        },
        {
            "authors": [
                "Ning Yu",
                "Larry S Davis",
                "Mario Fritz"
            ],
            "title": "Attributing fake images to gans: Learning and analyzing gan fingerprints",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Kaipeng Zhang",
                "Zhanpeng Zhang",
                "Zhifeng Li",
                "Yu Qiao"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "Signal Processing Letters,",
            "year": 2016
        },
        {
            "authors": [
                "Tianyue Zheng",
                "Weihong Deng"
            ],
            "title": "Cross-Pose LFW: A database for studying cross-pose face recognition in unconstrained environments",
            "venue": "Beijing University of Posts and Telecommunications, Tech. Rep,",
            "year": 2018
        },
        {
            "authors": [
                "Tianyue Zheng",
                "Weihong Deng",
                "Jiani Hu"
            ],
            "title": "Cross-Age LFW: A database for studying cross-age face recognition in unconstrained environments",
            "venue": "CoRR, abs/1708.08197,",
            "year": 2017
        },
        {
            "authors": [
                "Zheng Zhu",
                "Guan Huang",
                "Jiankang Deng",
                "Yun Ye",
                "Junjie Huang",
                "Xinze Chen",
                "Jiagang Zhu",
                "Tian Yang",
                "Jiwen Lu",
                "Dalong Du"
            ],
            "title": "WebFace260M: A benchmark unveiling the power of million-scale deep face recognition",
            "venue": "In CVPR,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "What does it take to create a good training dataset for visual recognition? An ideal training dataset for recognition tasks would have 1) large inter-class variation, 2) large intra-class variation and 3) small label noise. In the context of face recognition (FR), it means, the dataset has a large number of unique subjects, large intra-subject variations, and reliable subject labels. For instance, large-scale face datasets such as WebFace4M [89] contain over 1M subjects and large number of images/subject. Both the number of subjects and the number of images per subject are important for training FR models [14,39]. Also, datasets amassed by crawling the web are not free from label noise [9, 89].\nIn various domains, synthetic datasets are traditionally\nused to help generalize deep models when only limited real datasets could be collected [17, 28, 72, 90] or when bias exists in the real dataset [42, 73]. Lately, more attention has been drawn to training with only synthetic datasets in the face domain, as synthetic data can avoid leaking the privacy of real individuals. This is important as real face datasets have been under scrutiny for their lack of informed consent, as web-crawling is the primary means of large-scale data collection [22, 29, 89]. Also, synthetic training datasets can remedy some long-standing issues in real datasets, e.g. the long tail distribution, demographic bias, etc.\nWhen it comes to generating synthetic training datasets, the following questions should be raised. (i) How many novel subjects can be synthesized (ii) How well can we mimic the distribution of real images in the target domain and (iii) How well can we consistently generate multiple images of the same subjects? We start with the hypothesis that face dataset generation can be formulated as a problem that maximizes these criteria together.\nPrevious efforts in generating synthetic face datasets touch on one of the three aspects but do not consider all of them together [5, 57]. SynFace [57] generates high-fidelity face images based on DiscoFaceGAN [15], coming close to real images in terms of FID metric [24]. However, we were\nar X\niv :2\n30 4.\n07 06\n0v 1\n[ cs\n.C V\n] 1\n4 A\npr 2\nsurprised to find that the actual number of unique subjects that can be generated by DiscoFaceGAN is less than 500, a finding that will be discussed in Sec. 3.1. The recent state of the art (SoTA), DigiFace [5], can generate 1M large-scale synthetic face images with many unique subjects based on 3D parametric model rendering. However, it falls short in matching the quality and style of real face images.\nWe propose a new data generation scheme that addresses all three criteria, i.e. the large number of novel subjects (uniqueness), real dataset style matching (diversity) and label consistency (consistency). In Fig. 1, we illustrate the high-level idea by showcasing some of our generated face samples. The key motivation of our paper is that the synthetic dataset generator needs to control the number of unique subjects, match the training dataset\u2019s style distribution and be consistent in the subject label.\nIn light of this, we formulate the face image generation as a dual condition inverse problem, retrieving the unknown image Y from the observable Identity condition Xid and Style condition Xsty . Specifically, Xid specifies how a person looks and Xsty specifies how Xid should be portrayed in an image. Xsty contains identity-independent information such as pose, expression, and image quality.\nOur choice of dual conditions (identity and style) is important in how we generate a synthetic dataset as ID and style conditions are controllable factors that govern the dataset\u2019s characteristics. To achieve this, we propose a two-stage generation paradigm. First, we generate a highquality face image Xid using a face image generator and sample a style image Xsty from a style bank. Secondly, we mix these two conditions using a dual condition generator which predicts an image that has the ID of Xid and a style of Xsty . An illustration is given in Fig. 2.\nTraining the mixing generator in stage 2 is not trivial as it would require a triplet of (XAid,X B sty , X A sty) where X A sty is a hypothetical combination of the ID of subject A and the style of subject B. To solve this problem, we propose a new dual condition generator that can learn from (XAid,X A sty), a tuple of same subject images that can always be obtained in a labeled dataset. The novelty lies in our style condi-\ntion extractor and ID loss which prevents the training from falling into a degenerate solution. We modify the diffusion model [25, 64] to take in dual conditions and apply an auxiliary time-dependent ID loss that can control the balance between sample diversity and label consistency.\nWe show that our Dual Condition Face Dataset Generator (DCFace) is capable of surpassing the previous methods in terms of FR performance, establishing a new benchmark in face recognition with synthetic face datasets. We also show the roles dataset subject uniqueness, diversity and consistency play in face recognition performance.\nThe followings are the contributions of the paper. \u2022 We propose a two-stage face dataset generator that\ncontrols subject uniqueness, diversity and consistency. \u2022 For this, we propose a dual condition generator that\nmixes the two independent conditions Xid and Xsty . \u2022 We propose uniqueness, consistency and diversity met-\nrics that quantify the respective properties of a given dataset, useful measures that allow one to compare datasets apart from the recognition performance. \u2022 We achieve SoTA in FR with 0.5M image synthetic training dataset by surpassing the previous methods by 6.11% on average in 5 popular test datasets."
        },
        {
            "heading": "2. Related Works",
            "text": "Face Recognition. Face Recognition (FR) is the task of matching query imagery to an enrolled identity database. SoTA FR models are trained on large-scale web-crawled datasets [14, 22, 89] with margin-based softmax losses [14, 31, 39, 47, 76]. The FR performance is measured on various benchmark datasets such as LFW [30], CFP-FP [61], CPLFW [87], AgeDB [51] and CALFW [88]. These datasets are designed to measure factors such as pose changes and age variations. Performance on these datasets for models trained on large-scale datasets such as WebFace260M is well above 97% [39] in verification accuracy.\nSynthetic Face Generation. Recent advances in generative models allow high fidelity synthetic face image generations [8,11,25,35\u201337,65]. GANs have been widely used to\nmanipulate, animate or enhance face images [11,15,27,45, 56, 70, 71, 83]. They typically learn disentangled representations in GAN latent space that control desired face properties. On the contrary, some works leverage the 3D face prior from 3D datasets (e.g., 3DMM [6]) for controllable synthesis [12,18,19,38,50,52,54,63]. These methods have advantages in the fine-grained control over face generation and 3D consistency yet lack in style or domain variation.\nRecent advances in the latent variable models such as diffusion or score-based models have shown great success in high-quality image generation with a more stable and simple objective of MSE loss [25, 53, 64\u201368]. Diffusion models have advanced the conditional image generation in tasks such as text-conditional image generation, inpainting, etc [7, 58, 59, 78]. We adopt the diffusion model as a backbone and explore how the two image characteristics, namely ID and style images, can control complementary information, the subject appearance and the style of an image.\nFace Recognition with Synthetic Dataset. Synthetic training datasets offer an advantage over real datasets with regards to ethical issues and class imbalance problems as large-scale face datasets have been criticized for lacking informed consent and reflecting racial biases [5, 14, 84, 89]. Despite the benefit, use of synthetic datasets as the sole training data is not widely adopted due to the resulting low recognition performance. In various domains such as face recognition [5, 46, 57], fingerprint recognition [17, 82], and anti-spoofing [48, 69], synthetic datasets have been shown to improve recognition when combined with real images.\nIn the face domain, SynFace [57] studied the efficacy of using DiscoFaceGAN [15] for synthetic face generation. Recently, DigiFace-1M [5] studied the efficacy of 3D model based face rendering in combination with image augmentations to create a synthetic dataset. We propose a face dataset generation method that can generate both a large number of subjects and diverse styles that are close to the real dataset."
        },
        {
            "heading": "3. Proposed Approach",
            "text": "We propose Dual Condition Face Dataset Generator (DCFace), a two-stage dataset generator (see Fig. 2). Stage 1 is the Condition Sampling Stage, generating a highquality ID image (Xid) of a novel subject and selects one arbitrary style image (Xsty) from the bank of real training data. Stage 2 is the Mixing Stage which combines the two images using the Dual Condition Generator.\nFor trainable models in each stage, Stage 1 requires training an ID image generator Gid. For the style bank, we can conveniently use any real face dataset that we wish generated samples to follow. Stage 2 requires training a dual condition mixer Gmix. Both Gid and Gmix are based on diffusion models [25]. We describe each component and the associated training procedure in the following subsections."
        },
        {
            "heading": "3.1. Preliminary",
            "text": "Diffusion models [25,64] are a class of denoising generative models that are trained to predict an image from random noise through a gradual denoising process. One notable difference from the class of GAN-based generators [21] is in the objective function and the sampling procedure. The forward process as expressed in Eq. 1 corrupts the input X using variance controlled Gaussian noise over t time-steps,\nq (Xt|Xt\u22121) = N ( Xt; \u221a 1\u2212 \u03b2tXt\u22121, \u03b2tI ) , (1)\nand the denoising is done by training a model \u03f5\u03b8(Xt, t) to predict the initial noise \u03f5 with an L2 objective,\nL = Et,X0,\u03f5 [\u2225\u2225\u03f5\u03b8(\u221a\u03b1tX0 +\u221a1\u2212 \u03b1t\u03f5\ufe38 \ufe37\ufe37 \ufe38\nXt\n, t)\u2212 \u03f5 \u2225\u22252 2 ] . (2)\n\u03b2t and \u03b1t are pre-set variance scheduling scalars. The denoising diffusion model (DDPM) has shown success in producing diverse samples in text-conditioned image generation [58]. We find that in unconditional face generation, DDPM is also capable of generating many unique subjects. For instance, Fig. 3 compares DiscoFaceGAN [15] with DDPM [25] in their capacity to generate different subjects for every sample. It shows that DDPM [25] is a good model choice for Gid and Gmix as it can generate many unique subjects. For Gid, we adopt the unconditional DDPM trained on FFHQ [36], having observed that it is capable of generating a large number of unique subject images.\n3.2. Dual Condition Generator Gmix The two-stage data generation requires Dual Condition Generator Gmix which is a conditional DDPM. Two conditions Xid and Xsty are injected into the denoiser \u03f5\u03b8(Xt, t, Eid(Xid), Esty(Xsty)) using trainable feature\nextractors Eid and Esty and cross-attentions. Gmix is responsible for the operation XAid +X B sty \u2192XAsty , a mixing of an image of a novel subject A and an arbitrary style image of different subject B.\nNaive training would require the reference image XAsty , an image of subject A in the style of XBsty . This reference is absent in the labeled training dataset. As such, we modify the operation to XAid + X A sty \u2192XAsty , using two different images from the same subject as illustrated in Fig. 4(a). But this formulation is prone to a trivial solution of ignoring XAid, making the ID condition unused during test time. To mitigate this issue, we propose the following two elements. Patch-wise Style Extractor Esty . The motivation of Style Extractor is to map an image Xsty to a feature that contains little ID information, forcing Gmix to rely on Xid for ID information. In prior works such as StyleGAN, 1st and 2nd order statistics of a feature are shown to resemble the image style [36, 40, 44]. Yet, resulting statistics are reduced in spatial dimensions and consequently without spatially local informations such as pose.\nWe propose a module that can extract style information without losing spatial information. Specifically, consider a pretrained and fixed face recognition model Fs and its intermediate feature Fs(Xsty) = Isty \u2208RC\u00d7H\u00d7W . We divide the feature into a k \u00d7k grid. For each element in the grid Ikisty \u2208 RC\u00d7 H k\u00d7 W k , we perform non-linear mapping on the mean and variance of Ikisty . Specifically,\nI\u0302ki = BN(Conv(ReLU(Dropout(Ikisty)))), (3)\n\u00b5kisty = SpatialMean(\u0302I ki), \u03c3kisty = SpatialStd(\u0302I ki), (4) ski = LN ( (W1 \u2299 \u00b5kisty +W2 \u2299 \u03c3kisty) + Pemb ) , (5)\nEsty(Xsty) := s = [s 1, s2, ski ..., sk\u00d7k, s\u2032], (6)\nwhere s\u2032 corresponds to Ikisty being a global feature, where k = 1. The final output s is a concatenation of all style vectors for each patch. Each ski is a mean and variance of local information which is constrained from containing full pixel-level details with the ID information. And Pemb is a learned position embedding to let the model differentiate different patch locations. BN and LN are BatchNorm [32] and LayerNorm [4]. Fs is a shallow CNN taken from the early layers of a pretrained FR model. It is fixed and not updated to prevent it from optimizing Isty , serving only to create style information. By varying the grid size k\u00d7k, we can represent style at different spatial locations. An illustration of Esty can be found in Fig. 4(b). Time-step Dependent ID Loss. To train Dual Condition Generator Gmix, the original DDPM objective of L2 loss, Eq. 2 is not sufficient to guarantee the consistency in subject identity between the ID condition Xid and the prediction, X\u03020. To ensure the ID consistency, one could devise a loss function to maximize the similarity between Xid and the predicted denoised image X\u03020, in the ID feature space using a pretrained FR model, F . Specifically, following the Eq.15 of DDPM [25], one-step prediction of the original image is\nX\u03020 = (Xt \u2212 \u221a 1\u2212 \u03b1\u0304t\u03f5\u03b8(Xt, t,Xid,Xsty))/ \u221a \u03b1\u0304t. (7)\nA simple ID loss to increase cosine similarity (CS) is Lnaive1 = \u2212CS ( F (Xid), F (X\u03020)) ) . (8)\nHowever, this loss is in conflict with MSE loss and is empirically observed to reduce the predicted image quality. This is because the FR model, F is not invariant to image style; some style of Xid has to match in order to completely reduce Lnaive1. In contrast, one could also use\nLnaive2 = \u2212CS ( F (Xsty), F (X\u03020)) ) , (9)\nas during training the label of Xsty and Xid are the same. However, Lnaive2 causes the model to depend on Xsty for ID information. Thus, during evaluation, when Xsty and Xid are different subjects, the label consistency in the generated dataset is compromised. We show this in Tab. 2.\nInstead, we propose to interpolate between F (Xid) and F (Xsty) across diffusion time-steps. Specifically, LID =\u2212 \u03b3tCS ( F (Xid), F (X\u03020)) ) \u2212 (1\u2212 \u03b3t)CS ( F (Xsty), F (X\u03020)) ) , (10)\nwhere \u03b3t = tT is a time-dependent weight that linearly changes from 0 to 1. When t = T , \u03f5\u03b8 is predicting Xt\u22121 from random noise, and we let the model fully exploit the ID information of Xid. Gradually as t increases, we let the model\u2019s prediction walk into the direction of Xsty . Note that during training, the actual label of Xsty and Xid are the same. So the interpolation in the loss forces the prediction to be the same in identity but gradually shifting in style toward Xsty . This loss allows \u03f5\u03b8(Xt, t,Xid,Xsty)) to play different roles depending on t. For t \u2248 T , \u03f5\u03b8 will exploit Xid to infer front-view ID rich image. And as t \u2192 0, it will change the image\u2019s style to match the style of Xsty . The final loss is LMSE + \u03bbLID with \u03bb as a scaling parameter. Eid and Conditioning Mechanism. Following the success text-conditional image generation and inpainting using DDPM [55, 58, 78], we adopt a similar architecture for inserting conditions into the model. We concatenate Eid(Xid) and Esty(Xsty) and put in \u03f5\u03b8 using cross-attention and adaptive group normalization layers (AdaGN) [55]. Eid is a CNN, with the same architecture as a small FR model (e.g. ResNet50). And Eid is trained end-to-end with \u03f5\u03b8 to extract useful ID feature for \u03f5\u03b8. More training details can be found in Supp."
        },
        {
            "heading": "3.3. Condition Sampling Strategy",
            "text": "ID Image Sampling. For sampling ID images, we generate 200, 000 facial images from GID, from which we remove faces that are wearing sunglasses or too similar to the subjects in CASIA-WebFace with the Cosine Similarity threshold of 0.3 using Feval. We are left with 105, 446\nimages. Then we narrow them down to 62, 570 images that are unique according to uniqueness, Eq. 11 using Feval and r = 0.3. Then we explore two different options, 1) random sampling and 2) gender/ethnicity balanced sampling as Gid has a skewed distribution towards White subjects as shown in Tab. 1. We use [2] to classify the ethnicity and use [33, 80] to detect sunglasses. We denote the sampling option 1 as random and 2 as balance. Style Image Sampling. For style sampling, for each Xid, we randomly sample Xsty from the style bank. We denote this option as random. We also explore the option of sampling Xsty from the pool of images whose gender/ethnicity matches that of Xid. We denote this option as match."
        },
        {
            "heading": "4. Dataset Evaluation",
            "text": "In evaluating the synthesized dataset, one often adopts 1) FID [24] for evaluating the distribution similarity to the real images and 2) subsequent recognition performance. In this section, we propose three class-dependent metrics that aid us in understanding the property of generated labeled datasets. We let Feval be an recognition model used for evaluating synthesized face datasets. Note that this is different from F in ID loss. F is a model for training loss and Feval is for evaluating metrics. The more generalizable Feval is, the more accurate the metrics become in capturing the identity and diversity of the synthesized dataset.\nLet yc be a class label, and fi = Feval(Xi). Let d(fi, fj) be the distance between two images in Feval feature space. Uniqueness. Consider the following non-overlapping rball in Feval space,\nU={fi : d(fi, fj) > r, j < i, i, j \u2208 {1, .., N}}, (11)\nwhere d(fi, fj) is the cosine distance. Then |U | is the count of unique subjects determined by the threshold r in an un-\nlabeled dataset. Note that the set U is equivalent to sequentially adding a r-ball into Feval-space until you cannot add more without collision. |U | is subject to both r and Feval. In FR, r is a threshold in the FR model that is set to determine match or non-match.\nFor a labeled synthetic dataset, one generates multiple feature sets {f ci } for the same label. To count the number of unique subjects, we calculate the number of unique centers, f c = 1Nc \u2211Nc i f c i for c \u2208 {1, ..., C}, where C is the number of subjects and Nc is the number of images per subject. Then we define the number of unique subjects in a labeled dataset with |Uc| where Uc is\nUc={fc :d(f cn , fcm)>r,m<n, n,m\u2208{1, .., C}}, (12)\nFor the metric, we use Uclass = |Uc|/C, the ratio between the number of unique subjects and the number of labels. Intra-class Consistency. It measures how consistent the generated samples are in adhering to the label condition, as\nCintra = 1\nC C\u2211 c=1 1 Nc Nc\u2211 i=1 d(f ci , f c) < r, (13)\nwhich is the ratio of individual features f ci being close to the class center f c. For a given threshold r, higher values of Cintra mean the samples are more likely to be the same subject under the same label. Intra-class Diversity. It measures how diverse the generated samples are under the same label condition. Note that the diversity is in the style of an image, not in the subject\u2019s identity. We define the style space as a vector space defined by Inception Network [60] features pretrained on ImageNet [13] following the convention of [43], denoting the real and generated image inception vectors as {sci}, {s\u0302cj}.\nFor intra-class diversity, we measure how many real images fall into the style space manifold defined by the generated images under the same label condition. We compute this by extending the Improved Recall Metric [43], from comparing the unconditional distributions of real and fake images to comparing the label-conditional distributions. Specifically, for a set of real and generated feature vectors {sci}, {s\u0302cj} under the same label condition yc, we define knearest feature distance rk as rk = d ( s\u0302cj\u2212NNk ( s\u0302cj , {s\u0302cj} )) , where NNk returns the k-nearest feature vector in {s\u0302cj} and\nI(sci , {s\u0302cj})=\n{ 1,\u2203s\u0302cj \u2208 {s\u0302cj} s.t. d ( sci \u2212 s\u0302cj ) \u2264 rk\n0, otherwise. (14)\nd(\u00b7) is an Euclidean distance. Then diversity is defined by\nDintra = 1\nC\n1\nN C\u2211 c=1 Nc\u2211 i=1 I(sci , {s\u0302cj}), (15)\nwhich is the fraction of real image styles manifold covered by the generated image style manifold as defined by\nk-nearest neighbor ball. If the style variation is small, then rk becomes small, reducing the chance of d ( sci \u2212 s\u0302cj ) \u2264 rk. We compute the recall per class to capture style variation conditional on the subject label.\nIn Fig. 5, we illustrate different scenarios of conditional generation and how these metrics can capture the shortcomings in each scenario. In Sec. 5 and Fig. 6, we measure the metrics on our generated datasets and compare with previous synthetic datasets [5, 57]. We find that FR performance is at best when consistency and diversity are balanced. Also, we find SynFace and DigiFace have high Cintra and low Dintra compared to our method in Fig. 5."
        },
        {
            "heading": "5. Experiments",
            "text": "For Gid which generates ID images, we adopt the publicly released unconditional DDPM [25] trained on FFHQ [36]. For Gmix, we train it on CASIA-WebFace [29] after initializing weights from Gid. Although using all of CASIA-WebFace is a valid setting, we split it into a 95-5 split between train and validation sets. The validation set is used as a real dataset in measuring the uniqueness, consistency and diversity metrics. Gmix is trained for 10 epochs with a batch-size of 256 using AdamW Optimizer [41, 49] with the learning rate of 0.001. Training takes 8 hours using two A100 GPUs. Once Gmix is trained, we use Gid, Gmix and a style bank to generate a synthetic labeled dataset. The style bank is the CASIA-WebFace training set. For sampling, we use DDIM [65] with 200 intervals. Generating 500K samples takes about 20 hours using one A100 GPU.\nTo train FR models, for a fair comparison, we adopt the training scheme of [5, 57] using IR-SE-50 [14] as a backbone and AdaFace [39] as a loss function. We evaluate the trained FR models on five datasets, LFW [30], CFP-FP [61], CPLFW [87], AgeDB [51] and CALFW [88]. CFP-FP and CPLFW are designed to measure the FR in the large pose variation and AgeDB and CALFW are for the large age variation. To measure the consistency, diversity and uniqueness during evaluation, we adopt Feval as IR101 [14] model trained on WebFace4M [89] with AdaFace [39] loss."
        },
        {
            "heading": "5.1. Model Ablation",
            "text": "To show the efficacy of our proposed modules, we ablate on 1) the grid size in Style extractor Esty , 2) Time-step dependent ID loss and 3) the ID loss backbone F \u2019s. The number of samples we generate for the ablation are 10K subjects with 50 images per subject, similar to CASIAWebFace image counts. We report the FR performance with the synthetic data by averaging the 5 validation set verification accuracies. To measure Uclass, Cintra and Dintra, we use 500 subjects with 20 real images from the held-out validation set of CASIA-WebFace and generate an equivalent number of images from each method.\nGrid Size. We choose 4 grid sizes ranging from 1\u00d71 to 7\u00d77. Note that 1\u00d71 corresponds to the style vector of a whole image. We expect to see higher spatial control in Xsty as the grid size increases. In Tab. 2, we report the three metrics Uclass, Cintra and Dintra. As the grid size increases, Esty features contain more fine-grained information, possibly related to ID, lowering the consistency. However, the diversity increases, making the conditional distribution similar to the real dataset. The subsequent FR performance using the model is the best in the setting 5\u00d75, which is a good compromise between consistency and diversity. In Fig. 7, we show the effect of the grid size with examples.\nID Loss. For ID loss, we compare LID with Lnaive1 and Lnaive2 in Tab. 2. Using Lnaive1 or Lnaive2 both suffer from lower FR performance, but for different reasons. Lnaive1 has low diversity because it is optimized to be similar to Xid of front-view high quality face images. Lnaive2 has low consistency because of the lack of dependence on Xid, making the resulting dataset with random labels. FR performance of 0.5 means the model diverged and is returning random predictions. LID, a linear interpolation of the Lnaive1 and Lnaive2 across time-steps results in the best performance.\nID Loss Backbone F . ID Loss requires a pretrained FR model, F . For all of our experiments, we use F as IR50 trained on CASIA-WebFace. But, we are curious if there is a benefit to have a better representation from F . For this, we\nablate Fbigger, a model pretrained on a larger dataset, WebFace4M [89]. Tab. 2 shows that a better FR backbone induce the generator to synthesize better datasets, even without explicitly showing WebFace4M images to generators. But for fairness in comparing to the real CASIA-WebFace dataset, we do not use Fbigger for subsequent analysis."
        },
        {
            "heading": "5.2. Sampling Ablation",
            "text": "Using the sampling strategy defined in Sec. 3.3, we ablate on the ID sampling options (random, balance) and style sampling methods (random, match) in Tab. 3. We find that either balancing the gender/ethnicity distribution or making the gender/ethnicity of style image equal to that of ID images does not bring significant performance gain.\nOn the other hand, to compensate for lower label consistency compared to the real dataset, we include the same Xid for 5 additional times for each label. This has the effect of oversampling Xid during training FR model. When we add the oversampling option to (balance, match) setting, we observe an average verification accuracy of 89.56%, 0.52% increase over the (random, random) setting."
        },
        {
            "heading": "5.3. Comparison with Previous Methods",
            "text": "For training FR models with synthetic datasets, we compare with SynFace [57] and DigiFace [5]. We compare 0.5M and 1.2M image count settings. The first setting corresponds to the size of the CASIA-WebFace real dataset. The second setting is to evaluate the effect of increasing the training dataset size. In Tab. 4, we show the verification accuracies of 5 validation sets. In 0.5M regime, our DCFace can surpass DigiFace in 4 out of 5 datasets with an improvement of 6.11% on average. In CFP-FP dataset with extremely large pose variation, DigiFace performs better, showing the merit of 3D consistent face synthesis using 3D models. DCFace has a good balance of consistency and diversity with many unique subjects, leading to a better FR performance in general. Note the larger style variation compared to SynFace and DigiFace in Fig. 7.\nThe last column of Tab. 4 shows the gap between synthetic and real, calculated as (REAL \u2212 SYN)/SYN, e.g. 5.65% = 94.62\u221289.5689.56 . It indicates how much improvement is needed to be on par with the real dataset. In 0.5M setting, DCFace reduces the gap to real performance by 57% over the SoTA. When we use more synthetic data as in 1.2M"
        },
        {
            "heading": "High Consistency Low Diversity",
            "text": ""
        },
        {
            "heading": "Low Consistency",
            "text": "regime, the synthetic dataset performance comes closer to that of the real dataset (3.74% in gap), a 60.9% improvement from the previous method (9.55% in gap)."
        },
        {
            "heading": "6. Conclusion",
            "text": "This paper presents a method for creating a synthetic training dataset for face recognition. Dataset generation is studied from the perspective of generating many unique subjects with large style diversity and label consistency. We propose the Dual Condition Face Generator to this end and show its large FR performance gain over previous methods on synthetic dataset generation. We believe our approach takes one step towards matching the performance of real training datasets with synthetic training datasets. Limitations. This work addresses the problem of generating label consistent and diverse datasets for face recognition model training. In our model ablation, we find that sacrificing label consistency for diversity to some degree is beneficial for the FR model training. However, this is not\nideal; for instance, our synthetic face generator lacks 3D consistency across pose, which is an advantage of generative models with 3D priors. Secondly, the goal of our research is to release a synthetic face dataset that alleviates the dependence on large-scale web-crawled images. As shown in our experiments, there is still some performance gap between real and synthetic training datasets. In this work, we take one step towards the goal and hope that the continued research will introduce a standalone synthetic face dataset. Acknowledgments. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via 2022-21102100004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Gov. is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\nReferences [1] TFace. https://github.com/Tencent/TFace.\ngit. Accessed: 2021-10-3. 7 [2] V\u0131\u0301tor Albiero. Face analysis pytorch. https://github.\ncom/vitoralbiero/face_analysis_pytorch, 2022. 5\n[3] Vishal Asnani, Xi Yin, Tal Hassner, Sijia Liu, and Xiaoming Liu. Proactive image manipulation detection. In CVPR, 2022. 7\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 4\n[5] Gwangbin Bae, Martin de La Gorce, Tadas Baltrusaitis, Charlie Hewitt, Dong Chen, Julien Valentin, Roberto Cipolla, and Jingjing Shen. Digiface-1m: 1 million digital face images for face recognition. In WACV, 2023. 1, 2, 3, 6, 7, 8, 4\n[6] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3D faces. In SIGGRAPH, 1999. 3\n[7] Andreas Blattmann, Robin Rombach, Kaan Oktay, and Bjo\u0308rn Ommer. Retrieval-augmented diffusion models. arXiv preprint arXiv:2204.11824, 2022. 3\n[8] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2\n[9] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In FG, 2018. 1\n[10] Zhiyi Cheng, Xiatian Zhu, and Shaogang Gong. Lowresolution face recognition. In ACCV, 2018. 7\n[11] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. StarGAN: Unified generative adversarial networks for multi-domain image-to-image translation. In CVPR, 2018. 2, 3\n[12] Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, and Stefanos Zafeiriou. UV-GAN: Adversarial facial uv map completion for pose-invariant face recognition. In CVPR, 2018. 3\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR. Ieee, 2009. 6\n[14] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive angular margin loss for deep face recognition. In CVPR, 2019. 1, 2, 3, 6, 8, 4\n[15] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face image generation via 3D imitative-contrastive learning. In CVPR, 2020. 1, 3, 4\n[16] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107, 2018. 2\n[17] Joshua J Engelsma, Steven A Grosz, and Anil K Jain. Printsgan: synthetic fingerprint generator. TPAMI, 2022. 1, 3\n[18] Baris Gecer, Binod Bhattarai, Josef Kittler, and Tae-Kyun Kim. Semi-supervised adversarial learning to generate pho-\ntorealistic face images of new identities from 3D morphable model. In ECCV, 2018. 3 [19] Zhenglin Geng, Chen Cao, and Sergey Tulyakov. 3D guided fine-grained face manipulation. In CVPR, 2019. 3 [20] Sharath Girish, Saksham Suri, Sai Saketh Rambhatla, and Abhinav Shrivastava. Towards discovery and attribution of open-world gan generated images. In ICCV, 2021. 7 [21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11), 2020. 3 [22] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. MS-Celeb-1M: A dataset and benchmark for large-scale face recognition. In ECCV, 2016. 1, 2 [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017. 1, 5 [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33, 2020. 2, 3, 4, 6, 1 [26] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [27] Qiyang Hu, Attila Szabo\u0301, Tiziano Portenier, Paolo Favaro, and Matthias Zwicker. Disentangling factors of variation by mixing them. In CVPR, 2018. 3 [28] Yuan-Ting Hu, Jiahong Wang, Raymond A Yeh, and Alexander G Schwing. Sail-vos 3d: A synthetic dataset and baselines for object detection and 3d mesh reconstruction from video data. In CVPR, 2021. 1 [29] Gary Huang, Marwan Mattar, Honglak Lee, and Erik Learned-Miller. Learning to align from scratch. NeurIPS, 25, 2012. 1, 6, 2, 7 [30] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled Faces in the Wild: A database forstudying face recognition in unconstrained environments. In Workshop on Faces in\u2019Real-Life\u2019Images: Detection, Alignment, and Recognition, 2008. 2, 6, 7 [31] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. CurricularFace: adaptive curriculum learning loss for deep face recognition. In CVPR, 2020. 2 [32] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 4 [33] Xiaoyi Jiang, Michael Binkert, Bernard Achermann, and Horst Bunke. Towards detection of glasses in facial images. Pattern Analysis & Applications, 3(1), 2000. 5 [34] Nathan D Kalka, Brianna Maze, James A Duncan, Kevin O\u2019Connor, Stephen Elliott, Kaleb Hebert, Julia Bryan, and Anil K Jain. IJB\u2013S: IARPA Janus Surveillance Video Benchmark. In BTAS, 2018. 7 [35] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018. 2\n[36] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. 2, 3, 4, 5, 6 [37] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020. 2 [38] Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick Pe\u0301rez, Christian Richardt, Michael Zollho\u0308fer, and Christian Theobalt. Deep video portraits. TOG, 2018. 3 [39] Minchul Kim, Anil K Jain, and Xiaoming Liu. AdaFace: Quality adaptive margin for face recognition. In CVPR, 2022. 1, 2, 6, 8, 3, 7 [40] Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. Cluster and aggregate: Face recognition with large probe set. NeurIPS, 2022. 4 [41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [42] David Kupas and Balazs Harangi. Solving the problem of imbalanced dataset with synthetic image generation for cell classification using deep learning. In EMBC, 2021. 1 [43] Tuomas Kynka\u0308a\u0308nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. NeurIPS, 32, 2019. 6 [44] HyunJae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration module for convolutional neural networks. In ICCV, 2019. 4 [45] Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and TieYan Liu. Conditional image-to-image translation. In CVPR, 2018. 3 [46] Feng Liu, Minchul Kim, Anil Jain, and Xiaoming Liu. Controllable and guided face synthesis for unconstrained face recognition. In ECCV, 2022. 3 [47] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. SphereFace: Deep hypersphere embedding for face recognition. In CVPR, 2017. 2 [48] Yaojie Liu and Xiaoming Liu. Spoof trace disentanglement for generic face anti-spoofing. TPAMI, 45(3), 2023. 3 [49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [50] Safa C. Medin, Bernhard Egger, Anoop Cherian, Ye Wang, Joshua B. Tenenbaum, Xiaoming Liu, and Tim K. Marks. MOST-GAN: 3d morphable stylegan for disentangled face image manipulation. In AAAI, 2022. 3 [51] Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. AGEDB: the first manually collected, in-the-wild age database. In CVPRW, 2017. 2, 6, 7 [52] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised learning of 3d representations from natural images. In ICCV, 2019. 3 [53] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, pages 8162\u20138171. PMLR, 2021. 3 [54] Jingtan Piao, Chen Qian, and Hongsheng Li. Semisupervised monocular 3D face reconstruction with end-toend shape-preserved domain transfer. In ICCV, 2019. 3\n[55] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In CVPR, 2022. 5\n[56] Albert Pumarola, Antonio Agudo, Aleix M Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer. Ganimation: Anatomically-aware facial animation from a single image. In ECCV, 2018. 3\n[57] Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao. SynFace: Face recognition with synthetic data. In ICCV, 2021. 1, 3, 6, 7, 8\n[58] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 3, 5, 2\n[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo\u0308rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3\n[60] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016. 6\n[61] Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, and David W Jacobs. Frontal to profile face verification in the wild. In WACV, 2016. 2, 6, 7\n[62] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. Defake: Detection and attribution of fake images generated by text-to-image diffusion models. arXiv preprint arXiv:2210.06998, 2022. 7\n[63] Yujun Shen, Bolei Zhou, Ping Luo, and Xiaoou Tang. Facefeat-GAN: a two-stage approach for identity-preserving face synthesis. arXiv preprint arXiv:1812.01288, 2018. 3\n[64] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2, 3\n[65] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2, 3, 6\n[66] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. NeurIPS, 34, 2021. 3\n[67] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 32, 2019. 3\n[68] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. NeurIPS, 33:12438\u2013 12448, 2020. 3\n[69] Joel Stehouwer, Amin Jourabloo, Yaojie Liu, and Xiaoming Liu. Noise modeling, synthesis and classification for generic object anti-spoofing. In CVPR, 2020. 3\n[70] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul E Debevec, and Ravi Ramamoorthi. Single image portrait relighting. TOG, 2019. 3\n[71] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning gan for pose-invariant face recognition. In CVPR, 2017. 3\n[72] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic data: Bridging the reality gap by domain randomization. In CVPRW, 2018. 1\n[73] Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar. Decaf: Generating fair synthetic data using causally-aware generative networks. NeurIPS, 34:22221\u201322233, 2021. 1\n[74] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 2008. 4\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2\n[76] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. CosFace: Large margin cosine loss for deep face recognition. In CVPR, 2018. 2\n[77] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot... for now. In CVPR, 2020. 7\n[78] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining is all you need for image-to-image translation. arXiv preprint arXiv:2205.12952, 2022. 3, 5\n[79] Cameron Whitelam, Emma Taborsky, Austin Blanton, Brianna Maze, Jocelyn Adams, Tim Miller, Nathan Kalka, Anil K Jain, James A Duncan, Kristen Allen, et al. IARPA Janus Benchmark-B face dataset. In CVPRW, 2017. 7\n[80] Tianxing Wu. Realtime glasses detection. https:// github.com/TianxingWu/realtime-glassesdetection, 2022. 5\n[81] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. 2\n[82] Andre Brasil Vieira Wyzykowski and Anil K Jain. Synthetic latent fingerprint generator. In WACV, 2023. 3\n[83] Taihong Xiao, Jiapeng Hong, and Jinwen Ma. Elegant: Exchanging latent encodings with GAN for transferring multiple face attributes. In ECCV, 2018. 3\n[84] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014. 3\n[85] Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake images to gans: Learning and analyzing gan fingerprints. In ICCV, 2019. 7\n[86] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. Signal Processing Letters, 2016. 7\n[87] Tianyue Zheng and Weihong Deng. Cross-Pose LFW: A database for studying cross-pose face recognition in unconstrained environments. Beijing University of Posts and Telecommunications, Tech. Rep, 5, 2018. 2, 6, 7\n[88] Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-Age LFW: A database for studying cross-age face recognition in unconstrained environments. CoRR, abs/1708.08197, 2017. 2, 6, 7\n[89] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, et al. WebFace260M: A benchmark unveiling the power of million-scale deep face recognition. In CVPR, 2021. 1, 2, 3, 6, 7, 4\n[90] Hasib Zunair and A Ben Hamza. Synthesis of covid-19 chest x-rays using unpaired image-to-image translation. Social network analysis and mining, 11(1), 2021. 1\nDCFace: Synthetic Face Generation with Dual Condition Diffusion Model Supplementary Material"
        },
        {
            "heading": "A. Training Details",
            "text": ""
        },
        {
            "heading": "A.1. Architecture Detals",
            "text": "The dual condition generator Gmix is a modification of DDPM [25] to incorporate two conditions. We insert two conditions Xid and Xsty into the denoising U-Net \u03f5\u03b8(Xt, t,Xid,Xsty). Conditioning images Xsty and Xid are mapped to features using Esty and Eid, respectively. According to Eq. 6 of the main paper, the style information Esty(Xsty) is the concatenation of style vectors at different k\u00d7k patch locations,\nEsty(Xsty) := s = [ s1, s2, ski ..., sk\u00d7k, s\u2032 ] \u2208 R(k 2+1)\u00d7C . (1)\nOn the other hand, ID information is a concatenation of features extracted from a trainable CNN (e.g. ResNet50 [23]), which produces an intermediate feature Iid of shape R7\u00d77\u00d7512and a feature vector fid of shape R512. Specifically,\nEid(Xid) := i = [Flatten(Iid),fid] + Pemb \u2208 R50\u00d7C , (2)\nwhere Flatten refers to removing the H\u00d7W spatial dimension and R50\u00d7C is from concatenating features of length 7\u22177 and 1. Pemb is a learnable position embedding for distinguishing each feature position for the subsequent cross-attention operation. Detailed illustrations of Esty(Xsty) and Eid(Xid) are shown in Fig. 1. C for the channel dimension of Esty(Xsty) and Eid(Xid) is 512.\nWhen Esty(Xsty) and Eid(Xid) is prepared, they together form (k2+1)+50 vectors of shape 512. These can be injected into the U-Net \u03f5\u03b8 by following the convention of the DDPM based text-conditional image generators [58]. Specifically, cross attention operation can be written as a modification of attention equation [75] with query Q, key K and value V with additional query Qc, key Kc.\nAttn(Q,K,V ) = SoftMax ( QWq (KWk) \u22ba\n\u221a d\n) WvV , (3)\nCross-Attn(Q,K,V ,Kc,Vc) = SoftMax ( QWq ([K,Kc]Wk) \u22ba\n\u221a d\n) Wv[V ,Vc], (4)\nwhere Wq,Wk and Wv are learnable weights and [\u00b7] refers to concatenation operation. In our case, Q = K = V are an arbitrary intermediate feature in the U-Net. And Kc = Vc are conditions generated by Esty(Xsty) and Eid(Xid), concatenated together. This operation allows the model to update the intermediate features with the conditions if necessary. We insert the cross-attention module in the last two DownSampling Residual Blocks in the U-Net, as shown in Fig. 2.\nTo increase the effect of Xid in the conditioning operation, we also add fid to the time-step embedding temb. As shown in the right side of Fig. 2, the Residual Block in the U-Net modulates the intermediate features according to the scaling vector provided by fid + temb. GNorm [81] refers to Group Normalization and SiLU refers to Sigmoid Linear Units [16]. Adding fid to temb for the Residual Block allows more paths for Xid to change the output of U-Net."
        },
        {
            "heading": "A.2. Training Hyper-Parameters",
            "text": "The final loss for training the model end-to-end is LMSE + \u03bbLID with \u03bb as a scaling parameter. We set \u03bb = 0.05 to compensate for the different scale between L2 and Cosine Similarity. All our input image sizes are 112\u00d7112, following the convention of SoTA face recognition model datasets [14, 29, 89]. And our code is implemented in Pytorch."
        },
        {
            "heading": "B. More Experiment Results",
            "text": ""
        },
        {
            "heading": "B.1. Adding Real Dataset",
            "text": "We include additional experiment results that involve adding real images. Although the motivation of the paper is to use an only-synthetic dataset to train a face recognition model, the performance comparison with an addition of a subset of the real dataset has its merits; it shows 1) whether the synthetic dataset is complementary to the real dataset and 2) whether the synthetic dataset can work as an augmentation for real images.\nTab. 1 shows the performance comparison between DigiFace [5] and our proposed DCFace when 1) a few real images are added and 2) both synthetic datasets are combined. The performance gap for DigiFace is large, jumping from 86.37 to 92.67 on average when 2K real subjects with 20 images per subject are added. In contrast, ours show a relatively less dramatic gain, 91.21 to 92.90 when few real images are added. This indicates that DigiFace [5] is quite different from the real images and ours is similar to the real images. This is in-line with our expectation as we have created a synthetic dataset that tries to mimic the style distribution of the training dataset, whereas DigiFace simulates image styles using 3D models."
        },
        {
            "heading": "B.2. Combining Multiple Synthetic Datasets",
            "text": "In the second to the last row of Tab. 1, when we combined the two synthetic datasets without the real images, the performance is the highest, reaching 93.06 on average. This result indicates that different synthetic datasets can be complementary when they are generated using different methods."
        },
        {
            "heading": "C. Analysis",
            "text": "C.1 Unique Subject Counts. In Fig. 3, we plot the number of unique subjects that can be sampled as we increase the sample size. The blue curve shows that the number of unique samples that can be generated by a DDPM of our choice does not saturate when we sample 200, 000 samples. At 200, 000 samples, the unique subjects are about 60, 000. And by extrapolating the curve, we estimate the number might reach 80, 000 with more samples. Our DDPM of choice is trained on FFHQ [36] dataset which contains 70, 000 unlabeled high-quality images. The orange line shows the number of unique samples that are sufficiently different from the subjects in the CASIA-WebFace dataset. The green line shows the number of unique samples left after filtering images that contain sunglasses. The flat region is due to the filtering stage reducing the total candidates. The plot shows that DDPM trained on FFHQ dataset can sufficiently generate a large number of unique and new samples that are different from CASIA-WebFace dataset. However, with more samples, eventually there is a limit to the number of unique samples that can be generated. When the number of total generated samples is 100, 000, one additional sample has approximately 24% chance of being unique, whereas, at 200, 000, the probability is 15%. The rate of sampling another unique subject decreases with more samples. The model used for evaluating the uniqueness is IR101 [14] trained on the WebFace4M [89] dataset. And we use the threshold of 0.3. We would like to note a typo in Sec. 3.3 of the main paper, where the number of unique subjects should be corrected from 62, 570 to 42, 763.\nC.2 Feature Plot. In Fig. 4, we show the 2D t-SNE [74] plot of synthetic images generated by 3 different methods (DiscoFaceGAN [15], DigiFace [5] and proposed DCFace). The red circles represent real images from CASIA-WebFace. We extract the features from each image using a pre-trained face recognition model, IR101 [14] trained on WebFace4M [89]. We show two settings we sample (a) 50 subjects with 1 image per subject and (b) 1 subject with 50 images per subject. Note that the proximity of DCFace image features is closer to CASIA-WebFace image features, highlighted in a circle. For each setting, we show the features extracted from an intermediate layer of IR101 and the last layer. As the layer becomes deeper, the features become suitable for recognition, as shown in the last column of the figure."
        },
        {
            "heading": "C.3 Comparison with Classifier Free Guidance.",
            "text": "When \u03f5(xt, c) learns to use the condition c, the difference \u03f5(xt, c)\u2212\u03f5(xt) can give further guidance during sampling to increase the dependence on c. But, in our case, the ID condition is the fine-grained facial difference that is hard to learn with MSE loss. Proposed Time-dependent ID loss, LID helps the model learn this directly. Row 3 vs 4 of Tab. 2 shows that LID is more effective than CFG.\nInterestingly, with a large guidance scale, CFG becomes harmful. CFG decreases diversity as pointed out by [26]. We observe that guidance with Xid leads to consistent ID but with little facial variation, the same phenomenon in DCFace with grid-size 1x1 in Esty , in Tab. 2 (main). Good FR datasets need both large intra and inter-subject variability and we combine Esty and LID to achieve this.\nC.4 FID Scores. Note that our generated data is not high-res images like FFHQ when compared to how SynFace is similar to FFHQ. (Tab. 3 row 5 vs 6). But, we point out that our aim is not to create HQ images but to create a database with realistic inter/intra-subject variations. In that regard, we have successfully approximated the distribution of the popular FR training dataset CASIA-WebFace (FID=13.67).\nHaving said this, we note FID is not comprehensive in evaluating labeled datasets. It cannot capture the label consistency nor directly relate to the FR performance. As such, SynFace/DigiFace do not report FID. We propose U,D,C metrics that enable holistic analysis of labeled datasets.\nC.5 Does DCFace change gender?. DCFace combines XID and Xsty , while adhering to the subject ID as defined by a pre-trained FR model. Factors weakly related to ID, such as age and hair style, can vary. Biometric ambiguity can occur due to makeup, wig, weight change, etc. even in real life. The perceived gender may change, but changes such as hair are less relevant to subject ID for the FR model.\nC.6 Why DCFace is better in U,D,C metrics?. We note DCFace is not better in all U,D,C. Fig. 6 (main) shows SynFace has the highest consistency (C). But, DCFace excels in the tradeoff between C and D. In other words, style similarity to the real dataset (i.e. D) is lacking in other datasets and it is as important as ID consistency. As such, U,D,C metrics reveal weak/strong points of synthetic datasets.\nD. Visualizations"
        },
        {
            "heading": "D.1. Time-step Visualizaton",
            "text": "Fig. 5 shows how DDPM generates output at each time-step. The far left column shows Xsty , the desired style of an image. The far right column shows Xid, the desired ID image of choice. In early time-steps, the network reconstructs the front-view image with an ID of Xid. And gradually, it interpolates the image into the desired style of Xsty . The gradual transition can be in the pose, hair-style, expression, etc.\nD.2. Interpolation\nIn Fig. 6, we show the plot of interpolation in Xsty . While keeping the same identity Xid, we take two style images Xsty1 and Xsty2. We interpolate with \u03b1 in \u03b1Estry(Xsty1) + (1 \u2212 \u03b1)Estry(Xsty2) with \u03b1 increasing linearly from 0 to 1. The interpolation is smooth, creating an intermediate pose and expression that did not exist before."
        },
        {
            "heading": "E. Miscelaneous",
            "text": "Similarity threshold. Threshold=0.3 is based on FR evaluation model having a threshold of 0.3080 for verification with TPR@FPR=0.01% : 97.17% on IJB-B [79]. FPR=0.01% is widely used in practice and the scale of similarity is (\u22121, 1). At threshold=0.3, FFHQ has 200 (2%) more unique subjects than DDPM, signaling a similar level of uniqueness. Style Extracting Model. We use the early layers of face recognition model for style extractor backbone. Our rationale for adopting the early layers of the FR model, as opposed to that of the ImageNet-trained model is that the early layers extract low-level features and we wanted features optimized with the face dataset. But, it is possible to take other models as long as it generates low-level features. Evaluation on Harder Datasets. We evaluate on harder datasets, IJB-B [79] (TPR@FPR=0.01%: 75.12) and TinyFace [10] (Rank1: 41.66). We include this result for future works to evaluate on harder datasets. Real and Generated Similarity Analysis. In addition to Fig.7 mathcing X\u0302id with CASIA-WebFace, matching all X\u03020 (generated) images against CASIA-WebFace at threshold=0.3, we get 0.0026% FMR. This implies that only a small fraction of CAISA-WebFace images are similar to the generated images."
        },
        {
            "heading": "F. Societal Concerns",
            "text": "We believe that the Machine Learning and Computer Vision community should strive together to minimize the negative societal impact. Our work falls into the category of 1) image generation using generative models and 2) synthetic labeled dataset generation. In the field of image generation, unfortunately, there are numerous well-known malicious applications of generative models. Fake images can be used to impersonate high-profile figures and create fake news. Conditional image generation models make the malicious use cases easier to adapt to different use cases because of user controllability. Fortunately, GAN-based generators produce subtle artifacts in the generated samples that allow the visual forgery detection [3,20,77,85]. With the recent advance in DDPM, the community is optimistic about detecting forgeries in diffusion models [62]. It is also known that proactive treatments on generated images increase the forgery detection performance [3], and as generative models become more sophisticated, proactive measures may be advised whenever possible.\nSynthetic dataset generation is, on the other hand, an effort to avoid infringing the privacy of individuals on the web. Large-scale face dataset is collected without informed consent and only a few evaluation datasets such as IJB-S [34] has IRB compliance for safe and ethical research. Collecting large-scale datasets with informed consent is prohibitively challenging and the community uses web-crawled datasets for the lack of an alternative option. Therefore, efforts to create synthetic datasets with synthetic subjects can be a practical solution to this problem. In our method, we still use real images to train the generative models. We hope that research in synthetic dataset generation will eventually replace real images, not just in the recognition task, but also in the generative tasks as well, removing the need for using real datasets in any form.\nG. Implementation Details and Code The code will be released at https://github.com/mk-minchul/dcface. For preprocessing the training data CASIA-WebFace [29], we reference AdaFace [39] and use MTCNN [86] for alignment and cropping faces. For the backbone model definition, TFace [1] and for evaluation of LFW [30], CFP-FP [61], CPLFW [87], AgeDB [51] and CALFW [88], we use AdaFace repository [39]."
        }
    ],
    "title": "DCFace: Synthetic Face Generation with Dual Condition Diffusion Model",
    "year": 2023
}