{
    "abstractText": "Reservoir computing (RC) is a simple and efficient model-free framework for forecasting the behavior of nonlinear dynamical systems from data. Here, we show that there exist commonly-studied systems for which leading RC frameworks struggle to learn the dynamics unless key information about the underlying system is already known. We focus on the important problem of basin prediction\u2014determining which attractor a system will converge to from its initial conditions. First, we show that the predictions of standard RC models (echo state networks) depend critically on warm-up time, requiring a warm-up trajectory containing almost the entire transient in order to identify the correct attractor. Accordingly, we turn to next-generation reservoir computing (NGRC), an attractive variant of RC that requires negligible warm-up time. By incorporating the exact nonlinearities in the original equations, we show that NGRC can accurately reconstruct intricate and high-dimensional basins of attraction, even with sparse training data (e.g., a single transient trajectory). Yet, a tiny uncertainty in the exact nonlinearity can render prediction accuracy no better than chance. Our results highlight the challenges faced by data-driven methods in learning the dynamics of multistable systems and suggest potential avenues to make these approaches more robust.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuanzhao Zhang"
        },
        {
            "affiliations": [],
            "name": "Sean P. Cornelius"
        }
    ],
    "id": "SP:5393ae2d8853bdfbfe4516a3e6de893cc10b25ed",
    "references": [
        {
            "authors": [
                "W. Maass",
                "T. Natschl\u00e4ger",
                "H. Markram"
            ],
            "title": "Real-time computing without stable states: A new framework for neural computation based on perturbations",
            "venue": "Neural Comput. 14, 2531 ",
            "year": 2002
        },
        {
            "authors": [
                "H. Jaeger",
                "H. Haas"
            ],
            "title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication",
            "venue": "Science 304, 78 ",
            "year": 2004
        },
        {
            "authors": [
                "M. Luko\u0161evi\u010dius",
                "H. Jaeger"
            ],
            "title": "Reservoir computing approaches to recurrent neural network training",
            "venue": "Comput. Sci. Rev. 3, 127 ",
            "year": 2009
        },
        {
            "authors": [
                "L. Appeltant",
                "M.C. Soriano"
            ],
            "title": "G",
            "venue": "Van der Sande, J. Danckaert, S. Massar, J. Dambre, B. Schrauwen, C. R. Mirasso, and I. Fischer, Information processing using a single dynamical node as complex system, Nat. Commun. 2, 468 ",
            "year": 2011
        },
        {
            "authors": [
                "D. Canaday",
                "A. Griffith",
                "D.J. Gauthier"
            ],
            "title": "Rapid time series prediction with a hardware-based reservoir computer",
            "venue": "Chaos 28, 123119 ",
            "year": 2018
        },
        {
            "authors": [
                "T.L. Carroll"
            ],
            "title": "Using reservoir computers to distinguish chaotic signals",
            "venue": "Phys. Rev. E 98, 052209 ",
            "year": 2018
        },
        {
            "authors": [
                "P.R. Vlachas",
                "J. Pathak",
                "B.R. Hunt",
                "T.P. Sapsis",
                "M. Girvan",
                "E. Ott",
                "P. Koumoutsakos"
            ],
            "title": "Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics",
            "venue": "Neural Networks 126, 191 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Rafayelyan",
                "J. Dong",
                "Y. Tan",
                "F. Krzakala",
                "S. Gigan"
            ],
            "title": "Large-Scale Optical Reservoir Computing for Spatiotemporal Chaotic Systems Prediction",
            "venue": "Phys. Rev. X 10, 041037 (2020). 033213-17 YUANZHAO ZHANG AND SEAN P. CORNELIUS PHYSICAL REVIEW RESEARCH 5, 033213 ",
            "year": 2023
        },
        {
            "authors": [
                "H. Fan",
                "J. Jiang",
                "C. Zhang",
                "X. Wang",
                "Y.-C. Lai"
            ],
            "title": "Long-term prediction of chaotic systems with machine learning",
            "venue": "Phys. Rev. Res. 2, 012080(R) ",
            "year": 2020
        },
        {
            "authors": [
                "G.A. Gottwald",
                "S. Reich"
            ],
            "title": "Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations",
            "venue": "Chaos 31, 101103 ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhong",
                "J. Tang",
                "X. Li",
                "B. Gao",
                "H. Qian",
                "H. Wu"
            ],
            "title": "Dynamic memristor-based reservoir computing for high-efficiency temporal signal processing",
            "venue": "Nat. Commun. 12, 408 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Pathak",
                "B. Hunt",
                "M. Girvan",
                "Z. Lu",
                "E. Ott"
            ],
            "title": "Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach",
            "venue": "Phys. Rev. Lett. 120, 024102 ",
            "year": 2018
        },
        {
            "authors": [
                "Z. Lu",
                "B.R. Hunt",
                "E. Ott"
            ],
            "title": "Attractor reconstruction by machine learning",
            "venue": "Chaos 28, 061104 ",
            "year": 2018
        },
        {
            "authors": [
                "L. Grigoryeva",
                "A. Hart",
                "J.-P. Ortega"
            ],
            "title": "Learning strange attractors with reservoir systems",
            "venue": "Nonlinearity 36, 4674 ",
            "year": 2023
        },
        {
            "authors": [
                "J. Pathak",
                "Z. Lu",
                "B.R. Hunt",
                "M. Girvan",
                "E. Ott"
            ],
            "title": "Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data",
            "venue": "Chaos 27, 121102 ",
            "year": 2017
        },
        {
            "authors": [
                "J.Z. Kim",
                "Z. Lu",
                "E. Nozari",
                "G.J. Pappas",
                "D.S. Bassett"
            ],
            "title": "Teaching recurrent neural networks to infer global temporal structure from local examples",
            "venue": "Nat. Mach. Intell. 3, 316 ",
            "year": 2021
        },
        {
            "authors": [
                "A. R\u00f6hm",
                "D.J. Gauthier",
                "I. Fischer"
            ],
            "title": "Model-free inference of unseen attractors: Reconstructing phase space features from a single noisy trajectory using reservoir computing",
            "venue": "Chaos 31, 103127 ",
            "year": 2021
        },
        {
            "authors": [
                "M. Roy",
                "S. Mandal",
                "C. Hens",
                "A. Prasad",
                "N. Kuznetsov",
                "M.D. Shrimali"
            ],
            "title": "Model-free prediction of multistability using echo state network",
            "venue": "Chaos 32, 101104 ",
            "year": 2022
        },
        {
            "authors": [
                "T. Arcomano",
                "I. Szunyogh",
                "A. Wikner",
                "J. Pathak",
                "B.R. Hunt",
                "E. Ott"
            ],
            "title": "A hybrid approach to atmospheric modeling that combines machine learning with a physics-based numerical model",
            "venue": "J. Adv. Model. Earth Syst. 14, e2021MS002712 ",
            "year": 2022
        },
        {
            "authors": [
                "P. Antonik",
                "M. Gulina",
                "J. Pauwels",
                "S. Massar"
            ],
            "title": "Using a reservoir computer to learn chaotic attractors",
            "venue": "with applications to chaos synchronization and cryptography, Phys. Rev. E 98, 012215 ",
            "year": 2018
        },
        {
            "authors": [
                "T. Weng",
                "H. Yang",
                "C. Gu",
                "J. Zhang",
                "M. Small"
            ],
            "title": "Synchronization of chaotic systems and their machine-learning models",
            "venue": "Phys. Rev. E 99, 042203 ",
            "year": 2019
        },
        {
            "authors": [
                "H. Fan",
                "L.-W. Kong",
                "Y.-C. Lai",
                "X. Wang"
            ],
            "title": "Anticipating synchronization with machine learning",
            "venue": "Phys. Rev. Res. 3, 023237 ",
            "year": 2021
        },
        {
            "authors": [
                "L.-W. Kong",
                "H.-W. Fan",
                "C. Grebogi",
                "Y.-C. Lai"
            ],
            "title": "Machine learning prediction of critical transition and system collapse",
            "venue": "Phys. Rev. Res. 3, 013090 ",
            "year": 2021
        },
        {
            "authors": [
                "D. Patel",
                "E. Ott"
            ],
            "title": "Using machine learning to anticipate tipping points and extrapolate to post-tipping dynamics of nonstationary dynamical systems",
            "venue": "Chaos 33, 023143 ",
            "year": 2023
        },
        {
            "authors": [
                "A. Banerjee",
                "J.D. Hart",
                "R. Roy",
                "E. Ott"
            ],
            "title": "Machine Learning Link Inference of Noisy Delay-Coupled Networks with Optoelectronic Experimental Tests",
            "venue": "Phys. Rev. X 11, 031014 ",
            "year": 2021
        },
        {
            "authors": [
                "T.L. Carroll",
                "L.M. Pecora"
            ],
            "title": "Network structure effects in reservoir computers",
            "venue": "Chaos 29, 083130 ",
            "year": 2019
        },
        {
            "authors": [
                "J. Jiang",
                "Y.-C. Lai"
            ],
            "title": "Model-free prediction of spatiotemporal dynamical systems with recurrent neural networks: Role of network spectral radius",
            "venue": "Phys. Rev. Res. 1, 033056 ",
            "year": 2019
        },
        {
            "authors": [
                "L. Gonon",
                "J.-P. Ortega"
            ],
            "title": "Reservoir computing universality with stochastic inputs",
            "venue": "IEEE Trans. Neural Netw. Learning Syst. 31, 100 ",
            "year": 2019
        },
        {
            "authors": [
                "A. Griffith",
                "A. Pomerance",
                "D.J. Gauthier"
            ],
            "title": "Forecasting chaotic systems with very low connectivity reservoir computers",
            "venue": "Chaos 29, 123108 ",
            "year": 2019
        },
        {
            "authors": [
                "T.L. Carroll"
            ],
            "title": "Do reservoir computers work best at the edge of chaos",
            "venue": "Chaos 30, 121109 ",
            "year": 2020
        },
        {
            "authors": [
                "R. Pyle",
                "N. Jovanovic",
                "D. Subramanian",
                "K.V. Palem",
                "A.B. Patel"
            ],
            "title": "Domain-driven models yield better predictions at lower cost than reservoir computers in lorenz systems",
            "venue": "Philos. Trans. R. Soc. A 379, 20200246 ",
            "year": 2021
        },
        {
            "authors": [
                "A.G. Hart",
                "J.L. Hook",
                "J.H. Dawes"
            ],
            "title": "Echo state networks trained by Tikhonov least squares are L2 (\u03bc) approximators of ergodic dynamical systems",
            "venue": "Physica D 421, 132882 ",
            "year": 2021
        },
        {
            "authors": [
                "J.A. Platt",
                "A. Wong",
                "R. Clark",
                "S.G. Penny",
                "H.D. Abarbanel"
            ],
            "title": "Robust forecasting using predictive generalized synchronization in reservoir computing",
            "venue": "Chaos 31, 123118 ",
            "year": 2021
        },
        {
            "authors": [
                "A. Flynn",
                "V.A. Tsachouridis",
                "A. Amann"
            ],
            "title": "Multifunctionality in a reservoir computer",
            "venue": "Chaos 31, 013125 ",
            "year": 2021
        },
        {
            "authors": [
                "T.L. Carroll"
            ],
            "title": "Optimizing memory in reservoir computers",
            "venue": "Chaos 32, 023123 ",
            "year": 2022
        },
        {
            "authors": [
                "J. Pathak",
                "A. Wikner",
                "R. Fussell",
                "S. Chandra",
                "B.R. Hunt",
                "M. Girvan",
                "E. Ott"
            ],
            "title": "Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model",
            "venue": "Chaos 28, 041101 ",
            "year": 2018
        },
        {
            "authors": [
                "A. Wikner",
                "J. Pathak",
                "B. Hunt",
                "M. Girvan",
                "T. Arcomano",
                "I. Szunyogh",
                "A. Pomerance",
                "E. Ott"
            ],
            "title": "Combining machine learning with knowledge-based modeling for scalable forecasting and subgrid-scale closure of large",
            "venue": "complex, spatiotemporal systems, Chaos 30, 053111 ",
            "year": 2020
        },
        {
            "authors": [
                "K. Srinivasan",
                "N. Coble",
                "J. Hamlin",
                "T. Antonsen",
                "E. Ott",
                "M. Girvan"
            ],
            "title": "Parallel Machine Learning for Forecasting the Dynamics of Complex Networks",
            "venue": "Phys. Rev. Lett. 128, 164101 ",
            "year": 2022
        },
        {
            "authors": [
                "W.A.S. Barbosa",
                "A. Griffith",
                "G.E. Rowlands",
                "L.C.G. Govia",
                "G.J. Ribeill",
                "M.-H. Nguyen",
                "T.A. Ohki",
                "D.J. Gauthier"
            ],
            "title": "Symmetry-aware reservoir computing",
            "venue": "Phys. Rev. E 104, 045307 ",
            "year": 2021
        },
        {
            "authors": [
                "D.J. Gauthier",
                "E. Bollt",
                "A. Griffith",
                "W.A. Barbosa"
            ],
            "title": "Next generation reservoir computing",
            "venue": "Nat. Commun. 12, 5564 ",
            "year": 2021
        },
        {
            "authors": [
                "E. Bollt"
            ],
            "title": "On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD",
            "venue": "Chaos 31, 013108 ",
            "year": 2021
        },
        {
            "authors": [
                "D.J. Gauthier",
                "I. Fischer",
                "A. R\u00f6hm"
            ],
            "title": "Learning unseen coexisting attractors",
            "venue": "Chaos 32, 113107 ",
            "year": 2022
        },
        {
            "authors": [
                "J.J. Hopfield"
            ],
            "title": "Neural networks and physical systems with emergent collective computational abilities",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 1982
        },
        {
            "authors": [
                "H. Li",
                "Z. Xu",
                "G. Taylor",
                "C. Studer",
                "T. Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), Vol. 31 ",
            "year": 2018
        },
        {
            "authors": [
                "A.E. Teschendorff",
                "A.P. Feinberg"
            ],
            "title": "Statistical mechanics meets single-cell biology",
            "venue": "Nat. Rev. Genet. 22, 459 ",
            "year": 2021
        },
        {
            "authors": [
                "D.A. Rand",
                "A. Raju",
                "M. S\u00e1ez",
                "F. Corson",
                "E.D. Siggia"
            ],
            "title": "Geometry of gene regulatory dynamics",
            "venue": "Proc. Natl. Acad. Sci. USA 118, e2109729118 (2021). 033213-18 CATCH-22s OF RESERVOIR COMPUTING PHYSICAL REVIEW RESEARCH 5, 033213 ",
            "year": 2023
        },
        {
            "authors": [
                "G. Schiebinger",
                "J. Shu",
                "M. Tabaka",
                "B. Cleary",
                "V. Subramanian",
                "A. Solomon",
                "J. Gould",
                "S. Liu",
                "S. Lin"
            ],
            "title": "P",
            "venue": "Berube et al., Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming, Cell 176, 928 ",
            "year": 2019
        },
        {
            "authors": [
                "M. S\u00e1ez",
                "R. Blassberg",
                "E. Camacho-Aguilar",
                "E.D. Siggia",
                "D.A. Rand",
                "J. Briscoe"
            ],
            "title": "Statistically derived geometrical landscapes capture principles of decision-making dynamics during cell fate transitions",
            "venue": "Cell Syst. 13, 12 ",
            "year": 2022
        },
        {
            "authors": [
                "P.J. Menck",
                "J. Heitzig",
                "N. Marwan",
                "J. Kurths"
            ],
            "title": "How basin stability complements the linear-stability paradigm",
            "venue": "Nat. Phys. 9, 89 ",
            "year": 2013
        },
        {
            "authors": [
                "P.J. Menck",
                "J. Heitzig",
                "J. Kurths",
                "H. Joachim Schellnhuber"
            ],
            "title": "How dead ends undermine power grid stability",
            "venue": "Nat. Commun. 5, 3969 ",
            "year": 2014
        },
        {
            "authors": [
                "A.E. Motter",
                "M. Gruiz",
                "G. K\u00e1rolyi",
                "T. T\u00e9l"
            ],
            "title": "Doubly Transient Chaos: Generic Form of Chaos in Autonomous Dissipative Systems",
            "venue": "Phys. Rev. Lett. 111, 194101 ",
            "year": 2013
        },
        {
            "authors": [
                "M. Luko\u0161evi\u010dius"
            ],
            "title": "A practical guide to applying echo state networks",
            "venue": "Neural Networks: Tricks of the Trade ",
            "year": 2012
        },
        {
            "authors": [
                "S.A. Billings"
            ],
            "title": "Nonlinear System Identification: NARMAX Methods in the Time",
            "venue": "Frequency, and Spatio-Temporal Domains ",
            "year": 2013
        },
        {
            "authors": [
                "L. Jaurigue",
                "K. L\u00fcdge"
            ],
            "title": "Connecting reservoir computing with statistical forecasting and deep neural networks",
            "venue": "Nat. Commun. 13, 227 ",
            "year": 2022
        },
        {
            "authors": [
                "S.L. Brunton",
                "J.L. Proctor",
                "J.N. Kutz"
            ],
            "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "venue": "Proc. Natl. Acad. Sci. USA 113, 3932 ",
            "year": 2016
        },
        {
            "authors": [
                "A. Rahimi",
                "B. Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), Vol. 20 ",
            "year": 2007
        },
        {
            "authors": [
                "S. Shahi",
                "F.H. Fenton",
                "E.M. Cherry"
            ],
            "title": "Prediction of chaotic time series using recurrent neural networks and reservoir computing techniques: A comparative study",
            "venue": "Mach. Learn. Appl. 8, 100300 ",
            "year": 2022
        },
        {
            "authors": [
                "J.C. Butcher"
            ],
            "title": "Numerical Methods for Ordinary Differential Equations",
            "year": 2016
        },
        {
            "authors": [
                "D.A. Wiley",
                "S.H. Strogatz",
                "M. Girvan"
            ],
            "title": "The size of the sync basin",
            "venue": "Chaos 16, 015103 ",
            "year": 2006
        },
        {
            "authors": [
                "R. Delabays",
                "M. Tyloo",
                "P. Jacquod"
            ],
            "title": "The size of the sync basin revisited",
            "venue": "Chaos 27, 103109 ",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang",
                "S.H. Strogatz"
            ],
            "title": "Basins with Tentacles",
            "venue": "Phys. Rev. Lett. 127, 194101 ",
            "year": 2021
        },
        {
            "authors": [
                "E. Weinan"
            ],
            "title": "A proposal on machine learning via dynamical systems",
            "venue": "Commun. Math. Stat. 5, 1 ",
            "year": 2017
        },
        {
            "authors": [
                "R.T. Chen",
                "Y. Rubanova",
                "J. Bettencourt",
                "D.K. Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), Vol. 31 ",
            "year": 2018
        },
        {
            "authors": [
                "R. Guimer\u00e0",
                "I. Reichardt",
                "A. Aguilar-Mogas",
                "F.A. Massucci",
                "M. Miranda",
                "J. Pallar\u00e8s",
                "M. Sales-Pardo"
            ],
            "title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
            "venue": "Sci. Adv. 6, eaav6971 ",
            "year": 2020
        },
        {
            "authors": [
                "W. Gilpin"
            ],
            "title": "Deep reconstruction of strange attractors from time series",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), Vol. 33 ",
            "year": 2020
        },
        {
            "authors": [
                "G.E. Karniadakis",
                "I.G. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nat. Rev. Phys. 3, 422 ",
            "year": 2021
        },
        {
            "authors": [
                "N.H. Nelsen",
                "A.M. Stuart"
            ],
            "title": "The random feature model for input-output maps between Banach spaces",
            "venue": "SIAM J. Sci. Comput. 43, A3212 ",
            "year": 2021
        },
        {
            "authors": [
                "M. Belkin"
            ],
            "title": "Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation",
            "venue": "Acta Numerica 30, 203 ",
            "year": 2021
        },
        {
            "authors": [
                "M. Levine",
                "A. Stuart"
            ],
            "title": "A framework for machine learning of model error in dynamical systems",
            "venue": "Commun. Am. Math. Soc. 2, 283 ",
            "year": 2022
        },
        {
            "authors": [
                "S.L. Brunton",
                "M. Budi\u0161i\u0107",
                "E. Kaiser",
                "J.N. Kutz"
            ],
            "title": "Modern Koopman theory for dynamical systems",
            "venue": "SIAM Rev. 64, 229 ",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "PHYSICAL REVIEW RESEARCH 5, 033213 (2023)\nCatch-22s of reservoir computing\nYuanzhao Zhang 1 and Sean P. Cornelius 2 1Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, New Mexico 87501, USA\n2Department of Physics, Toronto Metropolitan University, Toronto, Ontario M5B 2K3, Canada\n(Received 19 March 2023; accepted 2 August 2023; published 25 September 2023)\nReservoir computing (RC) is a simple and efficient model-free framework for forecasting the behavior of nonlinear dynamical systems from data. Here, we show that there exist commonly-studied systems for which leading RC frameworks struggle to learn the dynamics unless key information about the underlying system is already known. We focus on the important problem of basin prediction\u2014determining which attractor a system will converge to from its initial conditions. First, we show that the predictions of standard RC models (echo state networks) depend critically on warm-up time, requiring a warm-up trajectory containing almost the entire transient in order to identify the correct attractor. Accordingly, we turn to next-generation reservoir computing (NGRC), an attractive variant of RC that requires negligible warm-up time. By incorporating the exact nonlinearities in the original equations, we show that NGRC can accurately reconstruct intricate and high-dimensional basins of attraction, even with sparse training data (e.g., a single transient trajectory). Yet, a tiny uncertainty in the exact nonlinearity can render prediction accuracy no better than chance. Our results highlight the challenges faced by data-driven methods in learning the dynamics of multistable systems and suggest potential avenues to make these approaches more robust.\nDOI: 10.1103/PhysRevResearch.5.033213\nI. INTRODUCTION\nReservoir computing (RC) [1\u201312] is a machine learning framework for time-series predictions based on recurrent neural networks. Because only the output layer needs to be modified, RC is extremely efficient to train. Despite its simplicity, recent studies have shown that RC can be extremely powerful when it comes to learning unknown dynamical systems from data [13]. Specifically, RC has been used to reconstruct attractors [14,15], calculate Lyapunov exponents [16], infer bifurcation diagrams [17], and even predict the basins of unseen attractors [18,19]. These advances open the possibilities of using RC to improve climate modeling [20], create digital twins [21], anticipate synchronization [22,23], predict tipping points [24,25], and infer network connections [26].\nSince the landmark paper demonstrating RC\u2019s ability to predict spatiotemporally chaotic systems from data [13], there has been a flurry of efforts to understand the success as well as identify limitations of RC [27\u201336]. As a result, more sophisticated architectures have been developed to extend the capability of the original framework, such as hybrid [37], parallel [38,39], and symmetry-aware [40] RC schemes.\nOne particularly promising variant of RC was proposed in 2021 and named next-generation reservoir computing\nPublished by the American Physical Society under the terms of the Creative Commons Attribution 4.0 International license. Further distribution of this work must maintain attribution to the author(s) and the published article\u2019s title, journal citation, and DOI.\n(NGRC) [41]. There, instead of having a nonlinear reservoir and a linear output layer, one has a linear reservoir and a nonlinear output layer [42]. These differences, although subtle, confer several advantages: First, NGRC requires no random matrices and thus has much fewer hyperparameters that need to be optimized. Moreover, each NGRC prediction needs exceedingly few data points to initiate (as opposed to thousands of data points in standard RC), which is especially useful when predicting the basins of attraction in multistable dynamical system [43].\nUnderstanding the basin structure is of fundamental importance for dynamical systems with multiple attractors. Such systems include neural networks [44,45], gene regulatory networks [46,47], differentiating cells [48,49], and power grids [50,51]. Basins of attraction provide a mapping from initial conditions to attractors and, in the face of noise or perturbations, tell us the robustness of each stable state. Despite their importance, basins have not been well studied from a machine learning perspective, with most methods for datadriven modeling of dynamical systems currently focusing on systems with a single attractor.\nIn this article, we show that the success of standard RC in predicting the dynamics of multistable systems can depend critically on having access to long initialization trajectories, while the performance of NGRC can be extremely sensitive to the choice of readout nonlinearity. It has been observed that for each new initial condition, a standard RC model needs to be \u201cwarmed up\u201d with thousands of data points before it can start making predictions [43]. In practice, such data will not exist for most initial conditions. Even when they do exist, we demonstrate that the warm-up time series would often have already approached the attractor, rendering\n2643-1564/2023/5(3)/033213(19) 033213-1 Published by the American Physical Society\npredictions unnecessary [52]. In contrast, NGRC can easily reproduce highly intermingled and high-dimensional basins with minimal warm-up, provided the exact nonlinearity in the underlying equations is known. However, a 1% uncertainty on that nonlinearity can already make the NGRC basin predictions barely outperform random guesses. Given this extreme sensitivity, even if one had partial (but imprecise) knowledge of the underlying system, a hybrid scheme combining NGRC and such knowledge would still struggle in making reliable predictions.\nThe rest of the paper is organized as follows. In Sec. II, we introduce the first model system under study\u2014the magnetic pendulum, which is representative of the difficulties of basin prediction in real nonlinear systems. In Secs. III\u2013IV, we apply standard RC to this system, showing that accurate predictions rely heavily on the length of the warm-up trajectory. We thus turn to next-generation reservoir computing, giving a brief overview of its implementation in Sec. V. We present our main results in Sec. VI, where we characterize the effect of readout nonlinearity on NGRC\u2019s ability to predict the basins of the magnetic pendulum. We further support our findings using coupled Kuramoto oscillators in Sec. VII, which can have a large number of coexisting high-dimensional basins. Finally, we discuss the implications of our results and suggest avenues for future research in Sec. VIII."
        },
        {
            "heading": "II. THE MAGNETIC PENDULUM",
            "text": "For concreteness, we focus on the magnetic pendulum [53] as a representative model. It is mechanistically simple\u2014being low-dimensional and generated by simple physical laws\u2014and yet captures all characteristics of the basin prediction problem in general: The system is multistable and predicting which attractor a given initial condition will go to is nontrivial.\nThe system consists of an iron bob suspended by a massless rod above three identical magnets, located at the vertices of an equilateral triangle in the (x, y) plane (Fig. 1). The bob moves under the influence of gravity, drag due to air friction, and the attractive forces of the magnets. For simplicity, we treat the magnets as magnetic point charges and assume that the length of the pendulum rod is much greater than the distance between the magnets, allowing us to describe the dynamics using a small-angle approximation.\nThe resulting dimensionless equations of motion for the pendulum bob are\nx\u0308 = \u2212\u03c920x \u2212 ax\u0307 + 3\u2211\ni=1\nx\u0303i \u2212 x D(x\u0303i, y\u0303i )3 , (1)\ny\u0308 = \u2212\u03c920y \u2212 ay\u0307 + 3\u2211\ni=1\ny\u0303i \u2212 y D(x\u0303i, y\u0303i )3 , (2)\nwhere (x\u0303i, y\u0303i ) are the coordinates of the ith magnet, \u03c90 is the pendulum\u2019s natural frequency, and a is the damping coefficient. Here, D(x\u0303, y\u0303) denotes the distance between the bob and a given point (x\u0303, y\u0303) in the magnets\u2019 plane,\nD(x\u0303, y\u0303) = \u221a\n(x\u0303 \u2212 x)2 + (y\u0303 \u2212 y)2 + h2, (3) where h is the bob\u2019s height above the plane. The system\u2019s fourdimensional state is thus x = (x, y, x\u0307, y\u0307)T .\nWe take the (x, y) coordinates of the magnets to be (1/ \u221a 3, 0), (\u22121/2\u221a3, \u22121/2), and (\u22121/2\u221a3, 1/2). Unless stated otherwise, we set \u03c90 = 0.5, a = 0.2, and h = 0.2 in our simulations. These values are representative of all cases for which the magnetic pendulum has exactly three stable fixed points, corresponding to the bob being at rest and pointed toward one of the three magnets.\nPrevious studies have largely focused on chaotic dynamics as a stress test of RC\u2019s capabilities [2,6,8,9,13,16,17,25,41]. Here we take a different approach. With nonzero damping, the magnetic pendulum dynamics is autonomous and dissipative, meaning all trajectories must eventually converge to a fixed point. Except on a set of initial conditions of measure zero, this will be one of the three stable fixed points identified earlier. Yet predicting which attractor a given initial condition will go to can be far from straightforward, with the pendulum wandering in an erratic transient before eventually settling to one of the three magnets [53]. This manifests as complicated basins of attraction with a \u201cpseudo\u201d (fat) fractal structure (Fig. 1). We can control the \u201cfractalness\u201d of the basins by, for example, varying the height of the pendulum h. This generates basins with tunable complexity to test the performance of (NG)RC.\nIII. IMPLEMENTATION OF STANDARD RC\nConsider a dynamical system whose n-dimensional state x obeys a set of n autonomous differential equations of the form\nx\u0307 = f (x). (4)\n033213-2\nIn general, the goal of reservoir computing is to approximate the flow of Eq. (4) in discrete time by a map of the form\nxt+1 = F(xt ). (5) Here, the index t runs over a set of discrete times separated by t time units of the real system, where t is a timescale hyperparameter generally chosen to be smaller than the characteristic timescale(s) of Eq. (4).\nIn standard RC, one views the state of the real system as a linear readout from an auxiliary reservoir system, whose state is an Nr-dimensional vector rt . Specifically,\nxt = W \u00b7 rt , (6) where W is an n \u00d7 Nr matrix of trainable output weights. The reservoir system is generally much higher dimensional (Nr n), and its dynamics obey\nrt+1 = (1 \u2212 \u03b1)rt + \u03b1 f (Wr \u00b7 rt + Win \u00b7 ut + b). (7) Here Wr is the Nr \u00d7 Nr reservoir matrix, Win is the Nr \u00d7 n input matrix, and b is an Nr-dimensional bias vector. The input ut is an n-dimensional vector that represents either a state of the real system (ut = xt ) during training or the model\u2019s own output (ut = W \u00b7 rt ) during prediction. The nonlinear activation function f is applied elementwise, where we adopt the standard choice of f (\u00b7) = tanh(\u00b7). Finally, 0 < \u03b1 1 is the so-called leaky coefficient, which controls the inertia of the reservoir dynamics.\nIn general, only the output matrix W is trained, with Wr , Win, and b generated randomly from appropriate ensembles. We follow best practices [54] and previous studies in generating these latter components, specifically:\n(i) Wr is the weighted adjacency matrix of a directed Erdo\u030bs-R\u00e9nyi graph on Nr nodes. The link probability is 0 < q 1, and we allow for self-loops. We first draw the link weights uniformly and independently from [\u22121, 1], and then normalize them so that Wr has a specified spectral radius \u03c1 > 0. Here, q and \u03c1 are hyperparameters.\n(ii) Win is a dense matrix, whose entries are initially drawn uniformly and independently from [\u22121, 1]. In the magnetic pendulum, the state xt [and hence the input term ut in Eq. (7)] is of the form (x, y, x\u0307, y\u0307)T . To allow for different characteristic scales of the position vs. velocity dynamics, we scale the first two columns of Win by sx and the last two columns by sv , where sx, sv > 0 are scale hyperparameters.\n(iii) b has its entries drawn uniformly and independently from [\u2212sb, sb], where sb > 0 is a scale hyperparameter.\nTraining. To train an RC model from a given initial condition x0, we first integrate the real dynamics (4) to obtain Ntrain additional states {xt }t=1,...,Ntrain . We then iterate the reservoir dynamics (7) for Ntrain times from r0 = 0, using the training data as inputs (ut = xt ). This produces a corresponding sequence of reservoir states, {rt }t=1,...,Ntrain . Finally, we solve for the output weights W that render Eq. (6) the best fit to the training data using Ridge regression with Tikhonov regularization,\nW = XRT (RRT + \u03bbI)\u22121. (8) Here, X (R) is a matrix whose columns are the xt (rt ) for t = 1, . . . , Ntrain, I is the identity matrix, and \u03bb > 0 is a\nregularization coefficient that prevents ill conditioning of the weights, which can be symptomatic of overfitting the data.\nPrediction. To simulate a trained RC model from a given initial condition x0, we first integrate the true dynamics (4) forward in time to obtain a total of Nwarm-up 0 states {xt }t=1,...,Nwarm-up . During the first Nwarm-up iterations of the discrete dynamics (7), the input term comes from the real trajectory, i.e., ut = xt . Thereafter, we replace the input with the model\u2019s own output at the previous iteration (ut = W \u00b7 rt ). This creates a closed-loop system from Eq. (7), which we iterate without further input from the real system."
        },
        {
            "heading": "IV. CRITICAL DEPENDENCE OF STANDARD RC ON WARM-UP TIME",
            "text": "Although standard RC is extremely powerful, it is known to demand large warm-up periods (Nwarm-up) in certain problems in order to be stable [18]. In principle, this could create a dilemma for the problem of basin prediction, as long warm-up trajectories from the real system will generally be unavailable for initial conditions unseen during training. And even if such data were available, the problem could be rendered moot if the required warm-up exceeds the transient period of the given initial condition [43]. Here, we systematically test RC\u2019s sensitivity to the warm-up time using the magnetic pendulum system.\nOur aim is to test standard RC under the most favorable conditions. Accordingly, we will train each RC model on a single initial condition x0 = (x0, y0, 0, 0)T of the magnetic pendulum, and ask it to reproduce only the trajectory from that initial condition. Likewise, before training, we systematically optimize the RC hyperparameters for that initial condition via Bayesian optimization, seeking to minimize an objective function that combines both training and validation error. For details of this process, we refer the reader to Appendix F.\nIn initial tests of our optimization procedure, we found it largely insensitive to the reservoir connectivity (q), with equally good training/validation performances achievable across a range of q from 0.01 to 1. We likewise found little impact of the regularization coefficient over several orders of magnitude, with the optimizer frequently pinning \u03bb at the supplied lower bound of 10\u22128. Thus, in the interest of more fully exploring the most important hyperparameters, we fix q = 0.03 and \u03bb = 10\u22128. We then optimize the remaining five continuous hyperparameters (\u03c1, sx, sv , sb, \u03b1) over the ranges specified in Table II.\nThroughout this section, we set t = 0.02, which is smaller than the characteristic timescales of the magnetic pendulum. We train each RC model on Ntrain = 4000 data points of the real system starting from the given initial condition, which when paired with the chosen t encompass both the transient dynamics and convergence to one of the attractors. We fix the reservoir size at Nr = 300, and we show that larger reservoir sizes do not alter our results in Appendix G.\nFigure 2 shows the performance of an ensemble of RC realizations with optimized hyperparameters for the initial condition (x0, y0) = (\u22121.2, 0.75). Specifically, we show the normalized root-mean-square error (NRMSE, see Appendix C) between the real and RC-predicted trajectory as a function of warm-up time (twarm-up = Nwarm-up \u00b7 t). In\n033213-3\nFig. 2(a) we observe a sharp transition around twarm-up = 6. Before this point, we consistently have NRMSE = O(1), meaning the RC error is comparable to the scale of the real trajectory. But after the transition, the error is always quite small (NRMSE 1).\nWe can gain physical insight about this \u201cforecastability transition\u201d by analyzing the total mechanical energy of the training trajectory,\nE = 12 (x\u03072 + y\u03072) + U (x, y). (9)\nHere U (x, y) is the potential corresponding to Eqs. (1) and (2), where we set U = 0 at the minima corresponding to the three attractors. Strikingly, the critical warm-up time occurs only shortly before the energy drops below a critical value U \u2217\u2014 defined as the height of the potential barriers between the three wells [Fig. 2(a)]. By this time, the system is unambiguously \u201ctrapped\u201d near a specific magnet, making only damped oscillations thereafter [Fig. 2(b)]. This suggests that even highly optimized RC models will fail to reproduce convergence to the\ncorrect attractor unless they have already been guided there by data from the real system.\nWe illustrate this further in Fig. 3, showing example predictions from one RC realization considered above under two different warm-up times: one above the critical value in Fig. 2, and one below. Indeed, with sufficient warm-up (left), the RC trajectory is a near-perfect match to the real one, both before and after the warm-up period. But if the warm-up time is even slightly less than the critical value (right), the model quickly diverges once the autonomous prediction begins. In this case, the model fails to reproduce convergence to any fixed-point attractor, let alone the correct one, instead oscillating wildly.\nThis pattern holds when we repeat our experiment for other initial conditions, re-optimizing hyperparameters and retraining an ensemble of RC models for each (Figs. 11\u2013 14 in Appendix G). In all cases, we see the same sharp drop in RC prediction error at a particular warm-up time (Figs. 11 and 13). Without at least this much warm-up time, the models fail to capture the real dynamics even qualitatively, often converging to an unphysical state with nonzero final velocity (Figs. 12 and 14). Although there exist initial conditions that require shorter warm-ups\u2014such as (x0, y0) = (1.0,\u22120.5)\u2014this is only because those initial conditions have shorter transients. Indeed, there are other initial conditions\u2014 such as (x0, y0) = (1.75, 1.6)\u2014that have longer transients and demand commensurately larger warm-up times (Figs. 13 and 14). In no case have we observed the RC dynamics staying faithful to the real system unless the warm-up is comparable to the transient period.\nNote that the breakdown of RC with insufficient warm-up time cannot be attributed to an insufficiently complex model vis-\u00e0-vis the only hyperparameter we have not optimized: the reservoir size (Nr). Indeed, we have repeated our experiment with reservoirs twice as large (Nr = 600). Even with optimized values of the other hyperparameters, we still see a sharp\n033213-4\ntransition in the NRMSE at a warm-up time comparable to the transient time (Fig. 15 in Appendix G).\nIn sum, we have shown that standard RC is unsuitable for basin prediction in this representative multistable system. Specifically, RC models can only reliably reproduce convergence to the correct attractor when they have been guided to its vicinity. This is true even with the benefit of highly tuned hyperparameters (Appendix E), and validation on only the initial condition seen during training.\nFor the remainder of the paper, we instead turn to next-generation reservoir computing (NGRC). Although it is known that every NGRC model implicitly defines the connectivity matrix and other parameters of a standard RC model [41,42], there is no guarantee that the two architectures would perform similarly in practice. In particular, NGRC is known to demand substantially less warm-up time [41], potentially avoiding the \u201ccatch-22\u201d identified here for standard RC. Can this cutting-edge framework succeed in learning the magnetic pendulum and other paradigmatic multistable systems?\nV. IMPLEMENTATION OF NGRC\nWe implement the NGRC framework following Refs. [41,43]. In NGRC, the update rule for the discrete dynamics is taken as\nxt+1 = xt + W \u00b7 gt , (10) where gt is an m-dimensional feature vector, calculated from the current state and k \u2212 1 past states, namely,\ngt = g(xt , xt\u22121, . . . , xt\u2212k+1). (11) Here, k 1 is a hyperparameter that governs the amount of memory in the NGRC model, and W is an n \u00d7 m matrix of trainable weights.\nWe elaborate on the functional form of the feature embedding g below. But in general, the features can be divided into three groups: (i) one constant (bias) feature; (ii) mlin = nk linear features, corresponding to the components of {xt , xt\u22121, . . . , xt\u2212k+1}; and finally (iii) mnonlin nonlinear features, each a nonlinear transformation of the linear features. The total number of features is thus m = 1 + mlin + mnonlin.\nTraining. Per Eq. (10), training an NGRC model amounts to finding values for the weights W that give the best fit for the discrete update rule\nyt = W \u00b7 gt , (12) where yt = xt+1 \u2212 xt . Accordingly, we calculate pairs of inputs (gt ) and next-step targets (yt ) over Ntraj 1 training trajectories from the real system (4), each of length Ntrain + k. We then solve for the values of W that best fit Eq. (12) in the least-squares sense via regularized Ridge regression, namely,\nW = YGT (GGT + \u03bbI)\u22121. (13) Here Y (G) is a matrix whose columns are the yt (gt ). The regularization coefficient \u03bb plays the same role as in standard RC [cf. Eq. (8)].\nPrediction. To simulate a trained NGRC model from a given initial condition x0, we first integrate the true dynamics (4) forward in time to obtain the additional k \u2212 1 states needed\nto perform the first discrete update according to Eqs. (10) and (11). This is the warm-up period for the NGRC model. Thereafter, we iterate Eqs. (10) and (11) as an autonomous dynamical system, with each output becoming part of the model\u2019s input at the next time step. Thus in contrast to training, the model receives no data from the real system during prediction except the k \u2212 1 \u201cwarm-up\u201d states.\nThere is a clear parallel between NGRC [41,42] and statistical forecasting methods [55] such as nonlinear vectorautoregression (NVAR). However, as noted in Ref. [56], the feature vectors of a typical NGRC model usually have far more terms than NVAR methods, as the latter was designed with interpretability in mind. It is the use of a library of many candidate features\u2014in addition to other details like the typical training methods employed\u2014that sets NGRC apart from classic statistical forecasting approaches. In this way, NGRC also resembles the sparse identification of nonlinear dynamics (SINDy) framework [57]. The differences here are the intended tasks (finding parsimonious models vs fitting the dynamics), the optimization schemes (LASSO vs Ridge regression), and NGRC\u2019s inclusion of delayed states (generally no delayed states for SINDy)."
        },
        {
            "heading": "VI. SENSITIVE DEPENDENCE OF NGRC PERFORMANCE",
            "text": ""
        },
        {
            "heading": "ON READOUT NONLINEARITY",
            "text": "The importance of careful feature selection is well appreciated for many machine learning frameworks [57,58]. Yet one major appeal of NGRC is that the choice of nonlinearity is considered to be of secondary importance; in many systems studied to date, one can often bypass the feature selection process by adopting some generic nonlinearities (e.g., low-order polynomials). Indeed, applications of NGRC to chaotic benchmark systems have shown good results even when the features do not include all nonlinearities in the underlying ODEs [41,59]. But can we expect this to be true in general? Here, we test NGRC\u2019s sensitivity to the choice of feature embedding g (i.e., readout nonlinearity) in the basin prediction problem. Specifically, we compare the performance of three candidate NGRC models, in which the nonlinearities are:\n(I) Polynomials, specifically all unique monomials formed by the 4k components of {xt , xt\u22121, . . . , xt\u2212k+1}, with degree between 2 and dmax.\n(II) As set of NRBF radial basis functions (RBF) applied to the position coordinates r = (x, y) of each of the k states. The RBFs have randomly-chosen centers and a kernel function with shape and scale similar to the magnetic force term.\n(III) The exact nonlinearities in the magnetic pendulum system. Namely, the x and y components of the magnetic force for each magnet, evaluated at each of the k states.\nThe details of each model are summarized in Table I. Recall that in addition to their unique nonlinear features, all models contain one constant feature (set to 1 without loss of generality) and 4k linear features.\nModels I\u2013III represent a hierarchy of increasing knowledge about the real system. In Model I, we assume complete ignorance, hoping that the real dynamics are well approximated by a truncated Taylor series. In Model II, we acknowledge that this is a Newtonian central force problem and even the\n033213-5\nshape/scale of that force, but plead ignorance about the locations of the point sources. Finally, in Model III, we assume perfect knowledge of the system that generated the time series. Between the linear and nonlinear features, Model III includes all terms in Eqs. (1) and (2).\nOur principal question is: How well each NGRC model can reproduce the basins of attraction of the magnetic pendulum and in turn predict its long-term behavior? We focus on the 2D region of initial conditions depicted in Fig. 1, in which the pendulum bob is released from rest at position (x0, y0), with \u22121.5 x0, y0 1.5. We train each model on Ntraj trajectories generated by Eqs. (1) and (2) from initial conditions sampled uniformly and independently from the same region. We then compare the basins predicted by each trained NGRC model with those of the real system (Appendix D). We define the error rate (p) as the fraction of initial conditions for which the basin predictions disagree.\nModel I (polynomial features). For NGRC models equipped with polynomial features, excellent training fits can be achieved (Figs. 8, 16, and 17). Despite this, the models struggle to reproduce the qualitative dynamics of the magnetic pendulum, let alone the basins of attraction.\nFigure 4(a) shows representative NGRC basin predictions made by Model I using k = 5, dmax = 3. For the vast majority of initial conditions, the NGRC trajectory does not converge to any of the three attractors, instead diverging to (numerical) infinity in finite time (black points in the middle panels of Fig. 4). Modest improvements can be obtained by including polynomials up to degree dmax = 5 (with k = 3) as shown in Fig. 4(b). But even here, the model succeeds only at learning the part of each basin in the immediate vicinity of each attractor.\nUnfortunately, eking out further improvements by increasing the complexity of the NGRC model becomes computationally prohibitive. When k = 3 and dmax = 5, for example, the model already has m = 6188 features. Likewise, the feature matrix G used in training has hundreds of millions of entries. With higher values of k and/or dmax, the model becomes too expensive to train and simulate on a standard computer.\nTo ensure the instability of the polynomial NGRC models is not caused by a poor choice of hyperparameters, we have repeated our experiments for a wide range of time resolutions t , training trajectory lengths Ntrain, numbers of training\ntrajectories Ntraj (Fig. 18 in Appendix G), and values for regularization coefficient \u03bb spanning ten orders of magnitude (Fig. 19 in Appendix G). The performance of Model I was not significantly improved in any case.\nModel II (radial basis features). For NGRC models using radial basis functions as the readout nonlinearity, the solutions no longer blow up as they did in Model I above. This is encouraging though perhaps unsurprising, as the RBFs are much closer to the nonlinearity in the original equations describing the magnetic pendulum system. Unfortunately, the\n033213-6\naccuracy of the NGRC models in predicting basins remains poor.\nFigure 5 shows representative NGRC basin predictions as the number of radial basis functions is increased from NRBF = 10 to NRBF = 1, 000. In all cases, fits to the training data are impeccable, with the root-mean-square error (RMSE) ranging from 0.003 (NRBF = 10) to 0.0005 (NRBF = 1, 000). As more and more RBFs are included, the predictions can be visibly improved, but this improvement is very slow. For example, at NRBF = 1, 000 [Fig. 5(f)], the trained model predicts the correct basin for only 53.4% of the initial conditions under study (p = 0.466). Moreover, most of this accuracy is attributable to the large central portions of the basins near the attractors, in which the dynamics are closest to linear. Outside of these regions, the NGRC basin map may appear fractal, but the basin predictions themselves are scarcely better than random guesses. This deprives us of accurate forecasts in precisely the regions of the phase space where the outcome is most in question.\nAs with the polynomial case above, we have repeated our experiments for a wide range of hyperparameters to rule out overfitting or poor model calibration (Figs. 18 and 19 in Appendix G). The accuracy of Model II cannot be meaningfully improved with any of these changes.\nModel III (exact nonlinearities). We next test NGRC models equipped with the exact form of the nonlinearity in the magnetic pendulum system, namely the force terms in Eqs. (1) and (2). This time, the NGRC models can perform exceptionally well. Figure 20 (in Appendix G) shows the error rate of NGRC basin predictions as a function of the time resolution t . Without any fine-tuning of the other hyperparameters, NGRC models already achieve a near-perfect accuracy of 98.6%, provided t is sufficiently small.\nAstonishingly, Model III\u2019s predictions remain highly accurate even when it is trained on a single trajectory (Ntraj = 1) from a randomly-selected initial condition. Here, NGRC can produce a map of all three basins that is very close to the ground truth (85.0% accuracy, Fig. 6), despite seeing data from only one basin during training. This echoes previous results reported for the Li-Sprott system [43], in which NGRC accurately reconstructed the basins of all three attractors (two chaotic, one quasiperiodic) from a single training trajectory. But how can we account for this night-and-day difference with\nthe more system-agnostic models (I and II), which showed poor performance despite 100-fold more training data?\nThe answer lies in the construction of the NGRC dynamics. In possession of the exact terms in the underlying differential equations, Eq. (10) can\u2014by a suitable choice of the weights W\u2014emulate the action of a numerical integration method from the linear-multistep family [60], whose order depends on k. When k = 1, for example, Eq. (10) can mimic an Euler step. Thus, with a sufficiently small step size ( t), it is not surprising that an NGRC model equipped with exact nonlinearities can accurately reproduce the dynamics of almost any differential equations.\nThis observation might explain the stellar performance of NGRC in forecasting specific chaotic dynamics like the Lorenz [41] and Li-Sprott systems [43]. The nonlinearities in these systems are quadratic, meaning that so long as dmax 2, Model I can exactly learn the underlying vector field. The only information to be learned is the coefficient (W) that appears before each (non)linear term (g) in the ODEs. This in turn could explain why a single training trajectory suffices to convey information about the phase space as a whole.\nModel III with uncertainty. Considering the wide gulf in performance between NGRC models equipped with exact nonlinearity and those equipped with polynomial/radial nonlinearity, it is natural to wonder whether there are some other\n033213-7\nsmart choices of nonlinear features that perform well enough without knowing the exact nonlinearity.\nTo explore this possibility, we consider a variant of Model III in which we introduce small uncertainties in the nonlinear features, perturbing the assumed coordinates of each magnet by small amounts drawn uniformly and independently between [\u2212\u03b4, \u03b4]. Here \u03b4 is a hyperparameter much smaller than the characteristic spatial scale in this system (\u03b4 1). We train the model on Ntraj = 100 trajectories from the (unperturbed) real system, then measure how NGRC models perform in the presence of uncertainty about the exact nonlinearity.\nIn Fig. 7, we see that even a \u223c1% mismatch (\u03b4 = 0.01) in the coordinates of the magnets (x\u0303i, y\u0303i ) is enough to make the accuracy of NGRC predictions plunge from almost 100% to below 50% (recall that even random guesses have an accuracy of 33.3%). This extreme sensitivity of NGRC performance to perturbations in the readout nonlinearity suggests that any function other than the exact nonlinearity is unlikely to enable reliable basin predictions in the NGRC model.\nTraining vs prediction divergence. In all models considered, we have seen that excellent fits to the training data do not guarantee accurate basin predictions for the rest of the phase space. But surprisingly, NGRC models can predict the wrong basin even for the precise initial conditions on which they were trained.\nFor each of Models I\u2013III, Fig. 8 shows one example training trajectory for which the model attains a near-perfect fit to the ground truth, but the NGRC trajectory from the same initial condition nonetheless goes to a different attractor. We can rationalize this discrepancy by considering the difference between the training and prediction phases as described in Sec. V. During training, NGRC is asked to calculate the next state given the k most recent states from the ground truth data. In contrast, during prediction, the model must make this forecast based on its own (autonomous) trajectory. This permits even tiny errors to compound over time, potentially driving the dynamics to the wrong attractor. Though Fig. 8 shows only one example for each model, these cases are quite common, regardless of the exact hyperparameters used [61].\nMoreover, in Fig. 17 in Appendix G, we show that even when the NGRC model predicts the correct attractor for a given training initial condition, the intervening transient dynamics can deviate significantly from the ground truth. This is especially common and pronounced for NGRC models with polynomial or radial nonlinearities [Figs. 17(a) and 17(b)]. In particular, the transient time\u2014how long it takes to come close to the given attractor\u2014can be much larger or smaller than in the real system. As such, reaching the correct attractor does not necessarily imply that an NGRC model has learned the true dynamics from a given training initial condition. To say nothing of the (uncountably many) other initial conditions unseen during training.\nInfluence of basin complexity. As motivated earlier, the magnetic pendulum is a hard-to-predict system because of its complicated basins of attraction, regardless of the exact parameter values used. And indeed, we see the same sensitivity of NGRC performance to readout nonlinearity for other parameter values, such as h = 0.3 and h = 0.4 (Fig. 21 in Appendix G).\nAs the height of the pendulum h is increased, the basins do tend to become less fractal-like. In Fig. 22 in Appendix G, we vary the value of h and show that NGRC models trained with polynomials fail even for the most regular basins (h = 0.4). On the other hand, NGRC models trained with radial basis functions see their performance improve significantly as the basins become simpler. As expected, NGRC models equipped with exact nonlinearity successfully capture the basins for all values of h studied."
        },
        {
            "heading": "VII. PREDICTING HIGH-DIMENSIONAL BASINS WITH NGRC",
            "text": "How general are the results presented in Sec. VI? Could the magnetic pendulum be pathological in some unexpected way, with low-order polynomials or other generic features sufficing as the readout nonlinearity for most dynamical systems of interest? To address this possibility, we investigate another paradigmatic multistable system\u2014identical Kuramoto oscillators with nearest-neighbor coupling [62\u201364],\n\u03b8\u0307i = sin(\u03b8i+1 \u2212 \u03b8i ) + sin(\u03b8i\u22121 \u2212 \u03b8i ), i = 1, . . . , n, (14) where we assume a periodic boundary condition, so \u03b8n+1 = \u03b81 and \u03b80 = \u03b8n. Here n is the number of oscillators and hence the dimension of the phase space, and \u03b8i(t ) \u2208 [0, 2\u03c0 ) is the phase of oscillator i at time t .\n033213-8\nAside from being well studied as a model system: the Kuramoto system has two nice features. First, its sine nonlinearities are more \u201ctame\u201d than the algebraic fractions in the magnetic pendulum, helping to untangle whether the sensitive dependence observed in Sec. VI afflicts only specific nonlinearities. Second, we can easily change the dimension of Eq. (14) by varying n, allowing us to test NGRC on highdimensional basins.\nFor n > 4, Eq. (14) has multiple attractors in the form of twisted states\u2014phase-locked configurations in which the oscillators\u2019 phases make q full twists around the unit circle, satisfying \u03b8i = 2\u03c0 iq/n + C. Here q is the winding number of the state [62]. Twisted states are fixed points of Eq. (14) for all q, but only those with |q| < n/4 are stable [63]. The corresponding basins of attraction can be highly complicated [64], though not fractal-like as in the magnetic pendulum system.\nSimilar to Sec. VI, we consider three classes of readout nonlinearities assuming increasing knowledge of the underlying system:\n(1) Monomials spanned by the nk oscillator states in = {\u03b8t , \u03b8t\u22121, . . . , \u03b8t\u2212k+1}, with degree between 2 and dmax.\n(2) Trigonometric functions of all scalars in , consisting of sin( \u03b8i ) and cos( \u03b8i ) for all i and for integers 1 max.\n(3) The exact nonlinearity in Eq. (14), namely sin(\u03b8i \u2212 \u03b8 j ) for all pairs of connected nodes i and j.\nTo test the performance of different NGRC models on the Kuramoto system, we first set n = 9 and use them to predict basins in a two-dimensional (2D) slice of the phase space.\nSpecifically, we look at slices spanned by \u03b80 + \u03b11P1 + \u03b12P2, \u03b1i \u2208 (\u2212\u03c0, \u03c0 ]. Here, P1 and P2 are n-dimensional binary orientation vectors, while \u03b80 is the base point at the center of the slice.\nFigure 9 shows results for orientation vectors given by\nP1 = [1, 0, 1, 0, 1, 0, 1, 0, 1], P2 = [0, 1, 0, 1, 0, 1, 0, 1, 0], with \u03b80 representing the two-twist state. We can see that NGRC models with polynomial nonlinearity and trigono-\n033213-9\nmetric nonlinearity fail utterly at capturing the simple ground-truth basins. This is despite an extensive search over the hyperparameters t , \u03bb, dmax, and max. On the other hand, the NGRC model with exact nonlinearity gives almost perfect predictions for a wide range of hyperparameters. The hyperparameters in Fig. 9 are chosen so that trajectories predicted by the polynomial-NGRC model do not blow up.\nNext, we show that the NGRC model with exact nonlinearity can predict basins in much higher dimensions and with more complicated geometries. In Fig. 10, we set n = 83 and choose \u03b80 to be a random point in the phase space. The n-dimensional binary orientation vectors P1 and P2 are constructed by randomly selecting n/2 components to be 1 and the rest of the components are 0. (The results are not sensitive to the particular realizations of P1 and P2.) Using the same hyperparameters as in Fig. 9, the NGRC model achieves an accuracy of 97.5%. Visually, one would be hard pressed to find any difference between the predicted basins and the ground truth."
        },
        {
            "heading": "VIII. DISCUSSION",
            "text": "When can we claim that a machine learning model like RC has \u201clearned\u201d a dynamical system? One basic requirement is a good training fit, but this is far from sufficient. Many (NG)RC models have extremely low training error, but fail completely during the prediction phase (Fig. 8). A stronger criterion germane to chaotic systems is that the predicted trajectory (beyond the training data) should reproduce the \u201cclimate\u201d of the strange attractor, for example replicating the Lyapunov exponents [16]. Here, we propose that the ability to accurately predict basins of attraction is another important test a model must pass before it can be trusted as a proxy of the underlying system. This applies as much to singleattractor systems as it does to multistable ones, as a model might produce spurious attractors not present in the original dynamics [35].\nHere, we have shown that there exist commonly-studied systems for which basin prediction presents a steep challenge\nto leading RC frameworks. In standard RC, the model must be warmed up by an overwhelming majority of the transient dynamics, essentially reaching the attractor before prediction can begin. In contrast, NGRC requires minimal warm-up data but is critically sensitive to the choice of readout nonlinearity, with its ability to make basin predictions contingent on having the exact features in the underlying dynamics. Though these frameworks face very different challenges, each presents a \u201ccatch-22\u201d: The dynamics cannot be learned unless key information about the system is already known.\nThe basin prediction problem poses distinct challenges from the problem of forecasting chaotic systems, a test (NG)RC has largely passed with flying colors [2,6,8,9,13,16,17,25,41]. In the latter case, the \u201cclimate\u201d of a strange attractor can still be accurately reproduced even after the short-term prediction has failed [16]. It is for this reason that\u2014in the most commonly-used benchmark systems (Lorenz-63, Lorenz-96, Kuramoto-Sivashinsky, etc.)\u2014the transients are often deemed uninteresting and discarded during training. But for multistable systems, to predict which attractor an initial condition will converge to, the transient dynamics are the whole story. Therefore, basin prediction can be even more challenging than forecasting chaos. This is true even in the idealized setting considered here, wherein the attractors are fixed points, and the state of the system is fully observed without noise. As such, we suggest that the magnetic pendulum and Kuramoto systems are ideal benchmarks for data-driven methods aiming to learn multistable nonlinear systems.\nIt has been established that both standard RC and NGRC are universal approximators, which in appropriate limits can achieve arbitrarily good fits to any system\u2019s dynamics [29,33]. But in practice, this is a rather weak guarantee. Unlike many other machine learning tasks, achieving a good fit to the flow of the real system [Eq. (5)] is only the first step; we must ultimately evolve the trained model as a dynamical system in its own right. This can invite a problem of stability, similar to the one faced by numerical integrators. Even when the fit to a system\u2019s flow is excellent, the autonomous dynamics of an (NG)RC model can be unstable, causing the prediction to diverge catastrophically from the true solution. How to ensure the stability of a trained (NG)RC model in the general case is a major open problem [54].\nThere are several exciting directions for future research that follow naturally from our results. First, RC\u2019s ability to extract global information about a nonlinear system from local transient trajectories is one of its most powerful assets. Currently, we lack a theory that characterizes conditions under which such extrapolations can be achieved by an RC model. Second, several factors could contribute to the difficulty of basin prediction for RC, including the nonlinearity in the underlying equations, the geometric complexity of the basins, and the nature of the attractors themselves. Can we untangle the effects of these factors? Finally, although standard RC requires relatively long initialization data, it tends to show more robustness towards the choice of nonlinearity (i.e., the reservoir activation function) compared to NGRC. Can we develop a new framework that combines standard RC\u2019s robustness with NGRC\u2019s efficiency and low data requirement?\n033213-10\nRC is elegant, efficient, and powerful; but to usher in a new era of model-free learning of complex dynamical systems [57,65\u201374], it needs to solve the catch-22 created by its fragile dependence on readout nonlinearity (NGRC) or its reliance on long initialization data for every new initial condition (standard RC)."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We thank D. Gauthier, M. Girvan, M. Levine, and A. Haji for insightful discussions. Y.Z. acknowledges support from the Schmidt Science Fellowship and Omidyar Fellowship. S.P.C. acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), grant RGPIN-2020-05015."
        },
        {
            "heading": "APPENDIX A: SOFTWARE IMPLEMENTATION",
            "text": "All simulations in this study were performed in Julia. For standard RC (Secs. III and IV), we employ the ReservoirComputing package in concert with the BayesianOptimization package for hyperparameter optimization. For NGRC (Secs. V\u2013VII), we use a custom implementation as described in Sec. V. Our source code is freely available online [75]."
        },
        {
            "heading": "APPENDIX B: NUMERICAL INTEGRATION",
            "text": "For the purpose of obtaining trajectories of the real system for training and validation, we use Julia\u2019s DifferentialEquations package to integrate all continuous equations of motion (4) using a ninth-order integration scheme (Vern9), with absolute and relative error tolerances both set to 10\u221210. We stress that the hyperparameter t has no relation to the numerical integration step size, which is determined adaptively to achieve the desired error\ntolerances. Instead, t simply represents the timescale at which we seek to model the real dynamics via (NG)RC, and hence the resolution at which we sample the continuous trajectories to generate training and validation data."
        },
        {
            "heading": "APPENDIX C: (NORMALIZED) ROOT-MEAN-SQUARE ERROR",
            "text": "Given an (NG)RC predicted trajectory x\u0303t and a corresponding trajectory of the real system xt \u2014each of length N\u2014we calculate the root-mean-square error (RMSE) as\nRMSE = \u221a 1\nN \u2211 t \u2016xt \u2212 x\u0303t\u20162, (C1)\nwhere \u2016 \u00b7 \u2016 denotes the Euclidean norm. To obtain a normalized version of this (NRMSE)\u2014which we use as part of the objective function to optimize standard RC hyperparameters (Appendix F)\u2014we first rescale each component of xt and x\u0303t by their range in the real system, e.g., xi,t \u2192 xi,txi,max \u2212 xi,min , (C2) where the maximum (xi,max) and minimum (xi,min) for dimension i = 1, . . . , n of the state space are calculated over the corresponding training data."
        },
        {
            "heading": "APPENDIX D: BASIN PREDICTION",
            "text": "We associate a given condition x0 with a basin of attraction by simulating the real (NGRC) dynamics for a total of T time units ( T/ t iterations). We then identify the closest stable fixed point at the end of the trajectory. In the magnetic pendulum, this is taken as the closest magnet. In the Kuramoto system, we calculate the winding number |q| and use it to identify the corresponding twisted state. We use T = 100 for both systems, which is sufficient for all initial conditions under study to approach one of the stable fixed points.\nAPPENDIX E: SUPPLEMENTAL TABLES"
        },
        {
            "heading": "APPENDIX F: HYPERPARAMETER OPTIMIZATION",
            "text": "Given an initial condition x0 = (x0, y0, x\u03070, y\u03070)T of the magnetic pendulum system, we identify an optimal set of RC hyperparameters using Bayesian optimization. The goal here is to find the minimizer p\u2217 of a (noisy) function F (p), i.e.,\np\u2217 = arg min p F (p). (F1)\nIn our setting, p = (\u03c1, sx, sv, sb, \u03b1)T is a vector of our optimizable hyperparameters, and F is a scalar objective function measuring the error between the real system and a trained RC model generated with those hyperparameters. Typically, this objective function incorporates the NRMSE (Appendix C) between the real and RC-predicted trajectories [30]. But what is the best choice?\nWe found that the NRMSE during training is a poor optimization objective. In the magnetic pendulum, the resulting RC dynamics tend to blow up during the subsequent autonomous prediction, rather than staying near the fixed point of the real system. Accordingly, we use an objective function that incorporates both training and validation NRMSE. Specifically, for a given set of hyperparameters p, we generate one random RC model and train it to the first Ntrain = 4000 steps of the real trajectory starting from x0. This yields a training NRMSE \u03b5train. We then simulate the trained RC model for an additional Nvalidation time steps, picking up where the training left off. This yields a validation NRMSE, \u03b5validation.\nWe then calculate F (p) as F (p) = log(\u03b5train) + log(\u03b5validation). (F2)\nWe find that this approach yields optimal RC models that have excellent training fits, but remain \u201cwell behaved\u201d (i.e., nearly stationary) beyond the training phase.\nAll hyperparameter optimization for standard RC was performed using the BayesianOptimization package in Julia. We model the landscape of F via Gaussian process regression to observed values of (p,F (p)). We employ the default squared-exponential (Gaussian) kernel, with tunable parameters corresponding to the standard deviation plus the length scale of each dimension of the hyperparameter space. We first bootstrap the kernel (fit its parameters) using 200 random sets of hyperparameters p generated log-uniformly between the bounds in Table II via Latin hypercube sampling. At every step of the process thereafter, we acquire a new candidate value of p via the commonly-used expected improvement strategy. We repeat this process for a total of 500 iterations, returning the observed minimizer of F (p). Every 50 iterations, we refit the kernel parameters via maximum a posteriori (MAP) estimation. To account for the stochasticity in F due to W, Win, and b, we generate 10 realizations of the RC model for each candidate set of hyperparameters p. Thus, over the course of the optimization, we evaluate F a total of 12 000 times\u20142000 for the initial bootstrapping period, and an additional 10 000 during the subsequent optimization.\nAPPENDIX G: SUPPLEMENTAL FIGURES\n033213-12\n033213-13\n033213-14\n033213-15\n033213-16\n[1] W. Maass, T. Natschl\u00e4ger, and H. Markram, Real-time computing without stable states: A new framework for neural computation based on perturbations, Neural Comput. 14, 2531 (2002).\n[2] H. Jaeger and H. Haas, Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication, Science 304, 78 (2004).\n[3] M. Luko\u0161evic\u030cius and H. Jaeger, Reservoir computing approaches to recurrent neural network training, Comput. Sci. Rev. 3, 127 (2009).\n[4] L. Appeltant, M. C. Soriano, G. Van der Sande, J. Danckaert, S. Massar, J. Dambre, B. Schrauwen, C. R. Mirasso, and I. Fischer, Information processing using a single dynamical node as complex system, Nat. Commun. 2, 468 (2011).\n[5] D. Canaday, A. Griffith, and D. J. Gauthier, Rapid time series prediction with a hardware-based reservoir computer, Chaos 28, 123119 (2018).\n[6] T. L. Carroll, Using reservoir computers to distinguish chaotic signals, Phys. Rev. E 98, 052209 (2018).\n[7] P. R. Vlachas, J. Pathak, B. R. Hunt, T. P. Sapsis, M. Girvan, E. Ott, and P. Koumoutsakos, Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics, Neural Networks 126, 191 (2020).\n[8] M. Rafayelyan, J. Dong, Y. Tan, F. Krzakala, and S. Gigan, Large-Scale Optical Reservoir Computing for Spatiotemporal Chaotic Systems Prediction, Phys. Rev. X 10, 041037 (2020).\n033213-17\n[9] H. Fan, J. Jiang, C. Zhang, X. Wang, and Y.-C. Lai, Long-term prediction of chaotic systems with machine learning, Phys. Rev. Res. 2, 012080(R) (2020).\n[10] G. A. Gottwald and S. Reich, Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations, Chaos 31, 101103 (2021).\n[11] Y. Zhong, J. Tang, X. Li, B. Gao, H. Qian, and H. Wu, Dynamic memristor-based reservoir computing for high-efficiency temporal signal processing, Nat. Commun. 12, 408 (2021).\n[12] K. Nakajima and I. Fischer, Reservoir Computing (Springer, New York, 2021)\n[13] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott, Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach, Phys. Rev. Lett. 120, 024102 (2018).\n[14] Z. Lu, B. R. Hunt, and E. Ott, Attractor reconstruction by machine learning, Chaos 28, 061104 (2018).\n[15] L. Grigoryeva, A. Hart, and J.-P. Ortega, Learning strange attractors with reservoir systems, Nonlinearity 36, 4674 (2023).\n[16] J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott, Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data, Chaos 27, 121102 (2017).\n[17] J. Z. Kim, Z. Lu, E. Nozari, G. J. Pappas, and D. S. Bassett, Teaching recurrent neural networks to infer global temporal structure from local examples, Nat. Mach. Intell. 3, 316 (2021).\n[18] A. R\u00f6hm, D. J. Gauthier, and I. Fischer, Model-free inference of unseen attractors: Reconstructing phase space features from a single noisy trajectory using reservoir computing, Chaos 31, 103127 (2021).\n[19] M. Roy, S. Mandal, C. Hens, A. Prasad, N. Kuznetsov, and M. D. Shrimali, Model-free prediction of multistability using echo state network, Chaos 32, 101104 (2022).\n[20] T. Arcomano, I. Szunyogh, A. Wikner, J. Pathak, B. R. Hunt, and E. Ott, A hybrid approach to atmospheric modeling that combines machine learning with a physics-based numerical model, J. Adv. Model. Earth Syst. 14, e2021MS002712 (2022).\n[21] P. Antonik, M. Gulina, J. Pauwels, and S. Massar, Using a reservoir computer to learn chaotic attractors, with applications to chaos synchronization and cryptography, Phys. Rev. E 98, 012215 (2018).\n[22] T. Weng, H. Yang, C. Gu, J. Zhang, and M. Small, Synchronization of chaotic systems and their machine-learning models, Phys. Rev. E 99, 042203 (2019).\n[23] H. Fan, L.-W. Kong, Y.-C. Lai, and X. Wang, Anticipating synchronization with machine learning, Phys. Rev. Res. 3, 023237 (2021).\n[24] L.-W. Kong, H.-W. Fan, C. Grebogi, and Y.-C. Lai, Machine learning prediction of critical transition and system collapse, Phys. Rev. Res. 3, 013090 (2021).\n[25] D. Patel and E. Ott, Using machine learning to anticipate tipping points and extrapolate to post-tipping dynamics of nonstationary dynamical systems, Chaos 33, 023143 (2023).\n[26] A. Banerjee, J. D. Hart, R. Roy, and E. Ott, Machine Learning Link Inference of Noisy Delay-Coupled Networks with Optoelectronic Experimental Tests, Phys. Rev. X 11, 031014 (2021).\n[27] T. L. Carroll and L. M. Pecora, Network structure effects in reservoir computers, Chaos 29, 083130 (2019).\n[28] J. Jiang and Y.-C. Lai, Model-free prediction of spatiotemporal dynamical systems with recurrent neural networks: Role of network spectral radius, Phys. Rev. Res. 1, 033056 (2019).\n[29] L. Gonon and J.-P. Ortega, Reservoir computing universality with stochastic inputs, IEEE Trans. Neural Netw. Learning Syst. 31, 100 (2019).\n[30] A. Griffith, A. Pomerance, and D. J. Gauthier, Forecasting chaotic systems with very low connectivity reservoir computers, Chaos 29, 123108 (2019).\n[31] T. L. Carroll, Do reservoir computers work best at the edge of chaos?, Chaos 30, 121109 (2020).\n[32] R. Pyle, N. Jovanovic, D. Subramanian, K. V. Palem, and A. B. Patel, Domain-driven models yield better predictions at lower cost than reservoir computers in lorenz systems, Philos. Trans. R. Soc. A 379, 20200246 (2021).\n[33] A. G. Hart, J. L. Hook, and J. H. Dawes, Echo state networks trained by Tikhonov least squares are L2 (\u03bc) approximators of ergodic dynamical systems, Physica D 421, 132882 (2021).\n[34] J. A. Platt, A. Wong, R. Clark, S. G. Penny, and H. D. Abarbanel, Robust forecasting using predictive generalized synchronization in reservoir computing, Chaos 31, 123118 (2021).\n[35] A. Flynn, V. A. Tsachouridis, and A. Amann, Multifunctionality in a reservoir computer, Chaos 31, 013125 (2021).\n[36] T. L. Carroll, Optimizing memory in reservoir computers, Chaos 32, 023123 (2022).\n[37] J. Pathak, A. Wikner, R. Fussell, S. Chandra, B. R. Hunt, M. Girvan, and E. Ott, Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model, Chaos 28, 041101 (2018).\n[38] A. Wikner, J. Pathak, B. Hunt, M. Girvan, T. Arcomano, I. Szunyogh, A. Pomerance, and E. Ott, Combining machine learning with knowledge-based modeling for scalable forecasting and subgrid-scale closure of large, complex, spatiotemporal systems, Chaos 30, 053111 (2020).\n[39] K. Srinivasan, N. Coble, J. Hamlin, T. Antonsen, E. Ott, and M. Girvan, Parallel Machine Learning for Forecasting the Dynamics of Complex Networks, Phys. Rev. Lett. 128, 164101 (2022).\n[40] W. A. S. Barbosa, A. Griffith, G. E. Rowlands, L. C. G. Govia, G. J. Ribeill, M.-H. Nguyen, T. A. Ohki, and D. J. Gauthier, Symmetry-aware reservoir computing, Phys. Rev. E 104, 045307 (2021).\n[41] D. J. Gauthier, E. Bollt, A. Griffith, and W. A. Barbosa, Next generation reservoir computing, Nat. Commun. 12, 5564 (2021).\n[42] E. Bollt, On explaining the surprising success of reservoir computing forecaster of chaos? The universal machine learning dynamical system with contrast to VAR and DMD, Chaos 31, 013108 (2021).\n[43] D. J. Gauthier, I. Fischer, and A. R\u00f6hm, Learning unseen coexisting attractors, Chaos 32, 113107 (2022).\n[44] J. J. Hopfield, Neural networks and physical systems with emergent collective computational abilities., Proc. Natl. Acad. Sci. USA 79, 2554 (1982).\n[45] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, Visualizing the loss landscape of neural nets, in Advances in Neural Information Processing Systems (NeurIPS), Vol. 31 (Curran Associates, Inc., 2018).\n[46] A. E. Teschendorff and A. P. Feinberg, Statistical mechanics meets single-cell biology, Nat. Rev. Genet. 22, 459 (2021).\n[47] D. A. Rand, A. Raju, M. S\u00e1ez, F. Corson, and E. D. Siggia, Geometry of gene regulatory dynamics, Proc. Natl. Acad. Sci. USA 118, e2109729118 (2021).\n033213-18\n[48] G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin, P. Berube et al., Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming, Cell 176, 928 (2019).\n[49] M. S\u00e1ez, R. Blassberg, E. Camacho-Aguilar, E. D. Siggia, D. A. Rand, and J. Briscoe, Statistically derived geometrical landscapes capture principles of decision-making dynamics during cell fate transitions, Cell Syst. 13, 12 (2022).\n[50] P. J. Menck, J. Heitzig, N. Marwan, and J. Kurths, How basin stability complements the linear-stability paradigm, Nat. Phys. 9, 89 (2013).\n[51] P. J. Menck, J. Heitzig, J. Kurths, and H. Joachim Schellnhuber, How dead ends undermine power grid stability, Nat. Commun. 5, 3969 (2014).\n[52] Note that the warm-up time series is different from the training data and is only used after training has been completed.\n[53] A. E. Motter, M. Gruiz, G. K\u00e1rolyi, and T. T\u00e9l, Doubly Transient Chaos: Generic Form of Chaos in Autonomous Dissipative Systems, Phys. Rev. Lett. 111, 194101 (2013).\n[54] M. Luko\u0161evic\u030cius, A practical guide to applying echo state networks, in Neural Networks: Tricks of the Trade (Springer, Berlin, 2012), pp. 659\u2013686.\n[55] S. A. Billings, Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains (John Wiley & Sons, Hoboken, NJ, 2013).\n[56] L. Jaurigue and K. L\u00fcdge, Connecting reservoir computing with statistical forecasting and deep neural networks, Nat. Commun. 13, 227 (2022).\n[57] S. L. Brunton, J. L. Proctor, and J. N. Kutz, Discovering governing equations from data by sparse identification of nonlinear dynamical systems, Proc. Natl. Acad. Sci. USA 113, 3932 (2016).\n[58] A. Rahimi and B. Recht, Random features for large-scale kernel machines, in Advances in Neural Information Processing Systems (NeurIPS), Vol. 20 (Curran Associates, Inc., 2007).\n[59] S. Shahi, F. H. Fenton, and E. M. Cherry, Prediction of chaotic time series using recurrent neural networks and reservoir computing techniques: A comparative study, Mach. Learn. Appl. 8, 100300 (2022).\n[60] J. C. Butcher, Numerical Methods for Ordinary Differential Equations (John Wiley & Sons, Hoboken, NJ, 2016).\n[61] We did not observe any other attractors other than the three ground-truth fixed points and infinity for all NGRC models con-\nsidered. The absence of more complicated attractors (compared to RC) is likely due to the simpler architecture of NGRC and the dissipativity of the real dynamics (which NGRC models can learn directly via the linear features).\n[62] D. A. Wiley, S. H. Strogatz, and M. Girvan, The size of the sync basin, Chaos 16, 015103 (2006).\n[63] R. Delabays, M. Tyloo, and P. Jacquod, The size of the sync basin revisited, Chaos 27, 103109 (2017).\n[64] Y. Zhang and S. H. Strogatz, Basins with Tentacles, Phys. Rev. Lett. 127, 194101 (2021). [65] E. Weinan, A proposal on machine learning via dynamical systems, Commun. Math. Stat. 5, 1 (2017).\n[66] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, Neural ordinary differential equations, in Advances in Neural Information Processing Systems (NeurIPS), Vol. 31 (Curran Associates, Inc., 2018), pp. 6572\u20136583.\n[67] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, Fourier neural operator for parametric partial differential equations, arXiv:2010.08895.\n[68] R. Guimer\u00e0, I. Reichardt, A. Aguilar-Mogas, F. A. Massucci, M. Miranda, J. Pallar\u00e8s, and M. Sales-Pardo, A Bayesian machine scientist to aid in the solution of challenging scientific problems, Sci. Adv. 6, eaav6971 (2020).\n[69] W. Gilpin, Deep reconstruction of strange attractors from time series, in Advances in Neural Information Processing Systems (NeurIPS), Vol. 33 (Curran Associates, Inc., 2020), pp. 204\u2013216.\n[70] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang, Physics-informed machine learning, Nat. Rev. Phys. 3, 422 (2021).\n[71] N. H. Nelsen and A. M. Stuart, The random feature model for input-output maps between Banach spaces, SIAM J. Sci. Comput. 43, A3212 (2021).\n[72] M. Belkin, Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation, Acta Numerica 30, 203 (2021).\n[73] M. Levine and A. Stuart, A framework for machine learning of model error in dynamical systems, Commun. Am. Math. Soc. 2, 283 (2022).\n[74] S. L. Brunton, M. Budi\u0161ic\u0301, E. Kaiser, and J. N. Kutz, Modern Koopman theory for dynamical systems, SIAM Rev. 64, 229 (2022).\n[75] Our source code can be found at https://github.com/spcornelius/ RCBasins.\n033213-19"
        }
    ],
    "title": "Catch-22s of reservoir computing",
    "year": 2023
}