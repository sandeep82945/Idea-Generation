{
    "abstractText": "This paper is concerned with optimizing the weights of the global minimum-variance portfolio (GMVP) in high-dimensional settings where both observation and population dimensions grow at a bounded ratio. Optimizing the GMVP weights is highly influenced by the data covariance matrix estimation. In a high-dimensional setting, it is well known that the sample covariance matrix is not a proper estimator of the true covariance matrix since it is not invertible when we have fewer observations than the data dimension. Even with more observations, the sample covariance matrix may not be well-conditioned. This paper determines the GMVP weights based on a regularized covariance matrix estimator to overcome the abovementioned difficulties. Unlike other methods, the proper selection of the regularization parameter is achieved by minimizing the mean squared error of an estimate of the noise vector that accounts for the uncertainty in the data mean estimation. Using random-matrix-theory tools, we derive a consistent estimator of the achievable mean squared error that allows us to find the optimal regularization parameter using a simple line search. Simulation results demonstrate the effectiveness of the proposed method when the data dimension is larger than, or of the same order of, the number of data samples. INDEX TERMS Portfolio optimization, global minimum-variance portfolio, GMVP, random matrix theory, RMT, consistent estimator.",
    "authors": [
        {
            "affiliations": [],
            "name": "MAAZ MAHADI"
        },
        {
            "affiliations": [],
            "name": "UBAID M. AL-SAGGAF"
        }
    ],
    "id": "SP:a43100f3ebf64b9efd302e873f907527d9c1b468",
    "references": [
        {
            "authors": [
                "T. Bodnar",
                "S. Dmytriv",
                "N. Parolya",
                "W. Schmid"
            ],
            "title": "Tests for the weights of the global minimum variance portfolio in a high-dimensional setting",
            "venue": "IEEE Transactions on Signal Processing, vol. 67, no. 17, pp. 4479\u20134493, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Huni",
                "A.B. Sibindi"
            ],
            "title": "An application of the markowitz\u2019s meanvariance framework in constructing optimal portfolios using the johannesburg securities exchange tradeable indices",
            "venue": "The Journal of Accounting and Management, vol. 10, no. 2, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Markowitz"
            ],
            "title": "Portfolio selection",
            "venue": "The Journal of Finance, vol. 7, no. 1, pp. 77\u201391, 1952. [Online]. Available: http://www.jstor.org/stable/2975974",
            "year": 1952
        },
        {
            "authors": [
                "R. Couillet",
                "M. Debbah"
            ],
            "title": "Signal processing in large systems: A new paradigm",
            "venue": "IEEE Signal Processing Magazine, vol. 30, no. 1, pp. 24\u201339, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "E. Ollila",
                "E. Raninen"
            ],
            "title": "Optimal shrinkage covariance matrix estimation under random sampling from elliptical distributions",
            "venue": "IEEE Transactions on Signal Processing, vol. 67, no. 10, pp. 2707\u20132719, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "O. Ledoit",
                "M. Wolf"
            ],
            "title": "A well-conditioned estimator for largedimensional covariance matrices",
            "venue": "Journal of multivariate analysis, vol. 88, no. 2, pp. 365\u2013411, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "L. Yang",
                "R. Couillet",
                "M.R. McKay"
            ],
            "title": "A robust statistics approach to minimum variance portfolio optimization",
            "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 24, pp. 6684\u20136697, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "E. Ollila",
                "D.P. Palomar",
                "F. Pascal"
            ],
            "title": "Shrinking the eigenvalues of mestimators of covariance matrix",
            "venue": "IEEE Transactions on Signal Processing, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.T. Cai",
                "J. Hu",
                "Y. Li",
                "X. Zheng"
            ],
            "title": "High-dimensional minimum variance portfolio estimation based on high-frequency data",
            "venue": "Journal of Econometrics, vol. 214, no. 2, pp. 482\u2013494, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Ballal",
                "A.S. Abdelrahman",
                "A.H. Muqaibel",
                "T.Y. Al-Naffouri"
            ],
            "title": "An adaptive regularization approach to portfolio optimization",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5175\u20135179.",
            "year": 2021
        },
        {
            "authors": [
                "F. Rubio",
                "X. Mestre",
                "D.P. Palomar"
            ],
            "title": "Performance analysis and optimal selection of large minimum variance portfolios under estimation risk",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 6, no. 4, pp. 337\u2013350, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "S. Chandrasekaran",
                "G. Golub",
                "M. Gu",
                "A.H. Sayed"
            ],
            "title": "Parameter estimation in the presence of bounded data uncertainties",
            "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 19, no. 1, pp. 235\u2013252, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "Y. Guo",
                "T. Hastie",
                "R. Tibshirani"
            ],
            "title": "Regularized linear discriminant analysis and its application in microarrays",
            "venue": "Biostatistics, vol. 8, no. 1, pp. 86\u2013100, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "A. Zollanvari",
                "E.R. Dougherty"
            ],
            "title": "Generalized consistent error estimator of linear discriminant analysis",
            "venue": "IEEE transactions on signal processing, vol. 63, no. 11, pp. 2804\u20132814, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Ballal",
                "M.A. Suliman",
                "T.Y. Al-Naffouri"
            ],
            "title": "Bounded perturbation regularization for linear least squares estimation",
            "venue": "IEEE Access, vol. 5, pp. 27 551\u201327 562, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Elkhalil",
                "A. Kammoun",
                "R. Couillet",
                "T.Y. Al-Naffouri",
                "M.-S. Alouini"
            ],
            "title": "A large dimensional study of regularized discriminant analysis",
            "venue": "IEEE Transactions on Signal Processing, vol. 68, pp. 2464\u20132479, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B.D. Carlson"
            ],
            "title": "Covariance matrix estimation errors and diagonal loading in adaptive arrays",
            "venue": "IEEE Transactions on Aerospace and Electronic systems, vol. 4, no. 4, pp. 397\u2013401, 1988.",
            "year": 1988
        },
        {
            "authors": [
                "J. Li",
                "P. Stoica",
                "Z. Wang"
            ],
            "title": "On robust capon beamforming and diagonal loading",
            "venue": "IEEE transactions on signal processing, vol. 51, no. 7, pp. 1702\u2013 1715, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "M. Mahadi",
                "T. Ballal",
                "M. Moinuddin",
                "T.Y. Al-Naffouri",
                "U. Al-Saggaf"
            ],
            "title": "Low-complexity robust beamforming for a moving source",
            "venue": "2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 1846\u20131850.",
            "year": 2020
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "A robust lcmp beamformer with limited snapshots",
            "venue": "2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 1831\u20131835.",
            "year": 2020
        },
        {
            "authors": [
                "M.A. Suliman",
                "H. Sifaou",
                "T. Ballal",
                "M.-S. Alouini",
                "T.Y. Al-Naffouri"
            ],
            "title": "Robust estimation in linear ill-posed problems with adaptive regularization scheme",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4504\u20134508.",
            "year": 2018
        },
        {
            "authors": [
                "S.M. Kay"
            ],
            "title": "Fundamentals of statistical signal processing: estimation theory",
            "venue": "Prentice-Hall, Inc.,",
            "year": 1993
        },
        {
            "authors": [
                "F. Rubio"
            ],
            "title": "Generalized consistent estimation in arbitrarily high dimensional signal processing",
            "venue": "Ph.D. dissertation, Universitat Politecnica de Catalunya, Barcelona, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "L.B. Niyazi",
                "A. Kammoun",
                "H. Dahrouj",
                "M.-S. Alouini",
                "T.Y. Al- Naffouri"
            ],
            "title": "Asymptotic analysis of an ensemble of randomly projected linear discriminants",
            "venue": "IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 3, pp. 914\u2013930, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Ledoit",
                "M. Wolf"
            ],
            "title": "Analytical nonlinear shrinkage of largedimensional covariance matrices",
            "venue": "The Annals of Statistics, vol. 48, no. 5, pp. 3043\u20133065, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "O. Ledoit",
                "M. Wolf"
            ],
            "title": "Numerical implementation of the quest function",
            "venue": "Computational Statistics & Data Analysis, vol. 115, pp. 199\u2013223, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Portfolio optimization, global minimum-variance portfolio, GMVP, random matrix theory, RMT, consistent estimator.\nI. INTRODUCTION\nDecision-making regarding investment in the stock market has become increasingly more complex because of the dynamic nature of the stocks available to investors and the advent of new unconventional and risky options [1]. Throughout the years, the portfolio optimization problem has attracted the attention of many signal-processing researchers due to its close relationship to the field. The portfolio optimization problem aims at achieving the maximum possible returns with the least volatility percentage [2]. The Economist, Harry Markowitz, introduced the modern portfolio theory, or meanvariance analysis (MVP), in [3]. Other portfolios, such as the global MVP and the maximum sharp ratio portfolio (MSRP) have been proposed as improvements to the MVP. Portfolio optimization utilizes the available financial data to reach conclusions regarding the allocation of wealth to each\nof the available stocks. The most important measurement in portfolio optimization is the data covariance matrix (CM).\nCM estimation in the classical signal processing framework relies on asymptotic statistics of some observations, n, which is assumed to grow largely compared to the population dimension, p, i.e., n/p \u2192 \u221e as n \u2192 \u221e [4]. However, many practical applications, such as finance, bioinformatics and data classification, require an estimate of the CM when the data dimension is large compared to the sample size [5]. In such cases, it is well known that the default estimator, i.e., the empirical sample covariance matrix (SCM), is usually illconditioned, leading to poor performance.\nIn the case of p > n, the SCM is not invertible; whereas for p < n, the SCM is invertible but might be ill-conditioned, which substantially increases estimation errors. In other words, for a large p, it is not practically guaranteed that\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthe number of observations is sufficient to develop a wellconditioned CM estimator [6]. Such scenarios have motivated researchers to look into estimation problems in the high-dimensional regime [4].\nIn scenarios with limited data, a regularized SCM (RSCM) estimator of the following general form is widely used [5]:\n\u03a3\u0302\u03b2,\u03b3 = \u03b2\u03a3\u0302 + \u03b3I, (1)\nwhere \u03a3\u0302 is the SCM defined in (7) (further ahead), \u03b2, \u03b3 \u2208 R+ are the regularization, or shrinkage, parameters. These parameters can be determined based on minimizing the mean squared error (MSE), which results in oracle shrinkage parameters, \u03b2o and \u03b3o, as follows [5], [6]:\n(\u03b2o, \u03b3o) = arg min \u03b2,\u03b3>0\nE [\u2225\u2225\u2225\u03a3\u0302\u03b2,\u03b3 \u2212\u03a3\u2225\u2225\u22252\nF\n] , (2)\nwhere \u2016.\u2016F denotes the Frobenius matrix norm. The estimation of (\u03b2o, \u03b3o) based on (2) depends on the true CM, \u03a3. To circumvent this issue, Ledoit and Wolf [6] proposed a distribution-free consistent estimator of (\u03b2o, \u03b3o) in highdimensional settings. The work in [5] assumes that the observations are from unspecified elliptically symmetric distribution. The consistent estimator proposed in [7] uses a hybrid CM estimator based on the Taylor\u2019s M-estimator and Ledoit-Wolf shrinkage estimator, which suits a global minimum variance portfolio (GMVP) influenced by outliers. A similar approach based on the M-estimator is proposed in [8], considering n > p with fully automated selection of the shrinkage parameters. The minimum variance portfolio estimator in [9] is based on certain sparsity assumptions imposed on the inverse of the CM. The work presented in [10] proposes a different RSCM estimator by manipulating the expression of the GMVP weights.\nIn this paper, we propose a single-parameter CM estimator. Instead of minimizing the MSE, as in (2), we minimize the MSE of the estimation of the sample noise vector. We utilize RMT tools to obtain a consistent estimator of this MSE. The value of the regularization parameter \u03b3 is selected as the one that minimizes the estimated MSE. By choosing to minimize the MSE of the noise vector\u2019s estimation, we implicitly consider the inaccuracy of estimating the true mean.\nA. CONTRIBUTIONS OF THE PAPER The contributions of this paper can be summarized as follows: \u2022 We propose a regularized sample covariance matrix\nestimator of the covariance matrix for the portfolio optimization problem based on estimating a noise vector that accounts for the uncertainty in estimating the true mean. \u2022 Under the assumption of the double asymptotic regime, we derive the asymptotic performance of the MSE of the estimated vector. \u2022 We derive a general consistent estimator of the MSE. \u2022 We utilize the derived consistent MSE estimator to opti-\nmally tune the regularization parameter associated with\nthe regularized sample covariance matrix estimator. The application of the proposed estimator results in our VBMSE estimator."
        },
        {
            "heading": "B. NOTATIONS",
            "text": "Uppercase boldface letters denote matrices, while lowercase boldface letters denote vectors. Scalars are denoted by normal lowercase letters. The superscript notation, (.)T denotes the transpose of a matrix or a vector, while E(.) denotes the expectation operator, and tr[.] is the trace of a matrix. R, R+, and C, respectively, denote real, positive-real, and complex fields of dimension specified by a superscript. The variable z denotes a complex variable. The notation x y denotes that x and y are asymptotically equivalent, i.e., |x \u2212 y| a.s.\u2212\u2212\u2192 0, where a.s. denotes almost-sure convergence. The l2 norm (of a vector), or the 2-induced norm (of a matrix) is denoted by \u2016.\u20162, and the identity matrix of dimension n is denoted by In."
        },
        {
            "heading": "II. GLOBAL MINIMUM VARIANCE PORTFOLIO",
            "text": "We consider a time series comprising y1,y2 \u00b7 \u00b7 \u00b7 ,yL logarithmic returns of p financial assets over a certain investment period. We assume that the elements of yt, (t = 1, 2, \u00b7 \u00b7 \u00b7 , L) are independent and identically distributed (i.i.d.) and are generated according to the following stochastic model [11]:\nyt = \u00b5t + \u03a3 1 2 t xt, (3)\nwhere \u00b5t \u2208 Rn\u00d71 and \u03a3t \u2208 Rp\u00d7p are the mean and the CM of the asset returns over the investment period, and xt is an i.i.d. random noise vector of zero mean and identity CM. For simplicity, we drop the subscript t from \u00b5t and \u03a3t. For the investment period of interest, we define w \u2208 Rp as the asset holdings vector, also known as the weight vector. The GMVP optimally minimizes the portfolio variance under single-period investment horizon, such that the weight vector is normalized by the outstanding wealth [11], i.e.,\nmin w\u2208Rp\nwT\u03a3w subject to 1Tp w = 1, (4)\nwhere 1p is a column vector of p 1\u2019s. The solution of (4) can be obtained by using the Lagrange-multipliers method, which results in the optimum weights [7]:\nwGMVP = \u03a3\u221211p\n1Tp \u03a3 \u221211p\n. (5)\nThe CM in (5) is unknown and should be estimated. As stated earlier, the SCM estimate does not perform well because it is usually ill-conditioned; hence, we apply the RSCM estimator and (5) becomes\nw\u0302GMVP = \u03a3\u0302 \u22121 RSCM1p\n1Tp \u03a3\u0302 \u22121 RSCM1p\n, (6)\nwhere \u03a3\u0302RSCM is the RSCM which can take the form of (1), for example. In the following section, we develop a RSCM estimator method and properly set the value of its regularization parameter."
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nIII. THE PROPOSED CONSISTENT VECTOR-BASED MSE ESTIMATOR The SCM, \u03a3\u0302, and the sample mean, \u00b5\u0302, can be estimated from the n past return observations as follows:\n\u03a3\u0302 = 1\nn\u2212 1 n\u2211 j=1 (yt\u2212j \u2212 \u00b5\u0302)(yt\u2212j \u2212 \u00b5\u0302)T , (7)\n\u00b5\u0302 = 1\nn n\u2211 j=1 yt\u2212j . (8)\nWe notice that computing \u03a3\u0302 using (7) involves evaluating the sample mean, not the true mean. This can worsen performance, especially for a small number of observations. Subtracting \u00b5\u0302 from both sides of (3), we obtain\nyt \u2212 \u00b5\u0302 = (\u03a3\u0302 1 2 + \u2206)xt + \u03b4, (9)\nwhere \u03b4 , \u00b5\u2212 \u00b5\u0302 and \u2206 = \u03a3 1 2 \u2212 \u03a3\u0302 1 2 . Eq. (9) can be viewed as a linear model with bounded uncertainties in both \u03a3 1 2 and \u00b5 [12]. We seek an estimate, x\u0302t that performs well for any allowed perturbation (\u2206, \u03b4) by formulating the following min-max problem [12]:\nmin x\u0302t max \u2206,\u03b4\n[ \u2016(\u03a3\u0302 1 2 + \u2206)x\u0302t \u2212 (yt \u2212 \u00b5\u0302\u2212 \u03b4)\u20162 ] . (10)\nA unique solution can exist which takes the form [12]\nx\u0302t = (\u03a3\u0302 + \u03b3I) \u22121\u03a3\u0302 1 2 (yt \u2212 \u00b5\u0302). (11)\nx\u0302t is a function of \u03b3, which when properly set leads to the best estimate of xt. It is easy to recognize that (\u03a3\u0302 + \u03b3I)\u22121 can be used as an estimator of the CM inverse, i.e., \u03a3\u0302 \u22121 RSCM = \u03a3\u0302 \u22121 \u03b3 = (\u03a3\u0302 + \u03b3I)\n\u22121. Such estimator is widely used in the literature, e.g., [13]\u2013[21]; to name a few. The optimal value of \u03b3 that estimates \u03a3\u0302 \u22121 \u03b3 is the one that minimizes the MSE for estimating xt. That is\nMSE(\u03b3) = E [ \u2016xt \u2212 x\u0302t\u201622 ] (12)\n= E [ \u2016xt \u2212 \u03a3\u0302 \u22121 \u03b3 \u03a3\u0302 1 2 (\u03a3 1 2 xt + \u03b4)\u201622 ] . (13)\nWe choose the optimal \u03b3o as follows:\n\u03b3o = arg min MSE(\u03b3). (14)\nThe choice of minimizing the MSE is reasonable because, under certain conditions, the minimization problem in (4) and the minimum MSE are equivalent [22], [23]. Unlike the other methods, it is remarkable that the uncertainty in estimating the mean is taken into account in (13). We expect the effect of the uncertainty in the mean estimation to be high when we have a limited number of observations. Also, unlike the methods that are based on (2), when we search for the optimal \u03b3 that minimizes (13), we actually estimate the inverse of the CM rather than estimating the CM itself. This is important\nbecause we use it in (6). We obtain the following normalized (by n) expression of the MSE (see Appendix B):\nMSE(\u03b3) = p n + n+ 1 n E [ 1 n tr [ \u03a3\u03a3\u0302 ( \u03a3\u0302 + \u03b3I )\u22122]\ufe38 \ufe37\ufe37 \ufe38 A(\u03b3) ]\n\u22122E [ 1 n tr [ \u03a3 1 2 \u03a3\u0302 1 2 ( \u03a3\u0302 + \u03b3I )\u22121]\ufe38 \ufe37\ufe37 \ufe38 B(\u03b3) ] . (15)\nWe observe that (15) is expressed in terms of the unknown quantity, \u03a3. In this case, using a direct plugin formula, i.e., substituting \u03a3 with \u03a3\u0302 results in\nM\u0302SEplugin(\u03b3) = p n + n+ 1 n E [ 1 n tr [ \u03a3\u0302 2( \u03a3\u0302 + \u03b3I )\u22122]] \u22122E\n[ 1 n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3I )\u22121]] . (16)\nHowever, the estimator in (16) is an inconsistent estimator in the regime where n and p grow at constant rate [24]. To clarify, Fig. 1 plots an example of the derived MSE(\u03b3)(15) and the plugin estimation method (16) versus a wide range values of \u03b3. It is clear that using the plugin strategy does not help obtain the minimum MSE suitably. Instead, as the figure depicts, the plugin estimation method selects an improper \u03b3 that corresponds to a high MSE.\nAs an alternative remedy , we seek a consistent estimator of (15) by leveraging tools from RMT. To this end, we need to first obtain an asymptotic expression of (15). To do so, the following assumption should hold true.\nAssumption 1: As p,n\u2192\u221e, p/n\u2192 c \u2208 (0,\u221e). Assumption 1 leads to the following theorem:\nTheorem 1: Under Assumption 1, MSE(\u03b3) in (15) asymptotically converges to\nMSE(\u03b3) p n + n+ 1 n . 1 n (\u03b4\u03031 + \u03b3\u03b4\u0303\n\u2032 1)tr [ \u03a32 ( \u03b4\u03031\u03a3 + \u03b3I )\u22122] \u2212 2 n < [ tr [ \u03a3 1 2 ( \u03b4\u03032\u03a3 1 2 \u2212 i\u221a\u03b3I )\u22121]] , (17)\nwhere \u03b4\u03031 is the unique positive solution to the following system of equations:{\n\u03b41 = 1 n tr [ \u03a3 ( \u03b4\u03031\u03a3 + \u03b3Ip )\u22121] ,\n\u03b4\u03031 = 1 n tr [ T ( \u03b41T + In )\u22121] ,\n(18)\nwhere T = diag([1, 1, \u00b7 \u00b7 \u00b7 , 1, 0]T ) \u2208 Rn\u00d7n; hence, \u03b4\u03031 can be written as follows:\n\u03b4\u03031 = 1\n1 + \u03b41 . (19)\nSimilarly, \u03b4\u03032 is obtained by solving{ \u03b42 = 1 n tr [ \u03a3 1 2 ( \u03b4\u03032\u03a3 1 2 \u2212 i\u221a\u03b3Ip )\u22121] ,\n\u03b4\u03032 = 1 n tr [ T ( \u03b42T + In )\u22121] .\n(20)\nand \u03b4\u03032 = 1\n1 + \u03b42 . (21)\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\np = n = 300, [\u03a3]i,j = 0.6 |i\u2212j| and \u00b5 = 1p.\nProof: see Appendix C. Now, we are in a position to reveal the consistent estimator of (15). Theorem 2: Under Assumption 1, the consistent estimator of (15) is given by (22)\nM\u0302SE(\u03b3) p n \u2212 2<(\u03b4\u03022) + n+ 1 n\n(1 + \u03b4\u03021) 2\n\u03b4\u0302\u20321 .[\n( \u02c6\u0303 \u03b41 + \u03b3 \u02c6\u0303 \u03b4\u20321)\u03b4\u0302 \u2032 1 \u2212\n1 n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip )\u22122]] , (22)\nwhere \u03b4\u03021 and \u03b4\u03022 are the consistent estimators of \u03b41 and \u03b42, respectively, and are given by\n\u03b4\u03021 = 1 n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip )\u22121] 1\u2212 1n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip\n)\u22121] , (23) \u03b4\u03022 = 1 n tr [ \u03a3\u0302 1 2 ( \u03a3\u0302 1 2 \u2212 i\u221a\u03b3Ip\n)\u22121] 1\u2212 1n tr [ \u03a3\u0302 1 2 ( \u03a3\u0302 1 2 \u2212 i\u221a\u03b3Ip\n)\u22121] . (24) Proof: see Appendix D.\nBack to Fig. 1, which compares the derived MSE with the asymptotic formula (17) and the consistently estimated MSE (22), it can be seen clearly that the consistent MSE is more suitable for obtaining the value of \u03b3 that minimizes (15).\nA closed-form solution for \u03b3 in (22) is infeasible, so we rely on using a line search, where we search for \u03b3 that minimizes (22) within a predefined range.\nA. SUMMARY OF THE PROPOSED VB-MSE (VECTOR BASED-MSE) METHOD FOR PORTFOLIO OPTIMIZATION 1. From the historical data estimate \u03a3\u0302 using Eq. (7). 2. Find the regularization parameter value, \u03b3o, that mini-\nmizes M\u0302SE(\u03b3) in (22) using a line search. 3. Use \u03b3o to compute \u03a3\u0302 \u22121 \u03b3o = (\u03a3\u0302 + \u03b3oI) \u22121. 4. Calculate w\u0302GMVP from (6) by using \u03a3\u0302 \u22121 RSCM = \u03a3\u0302 \u22121 \u03b3o ."
        },
        {
            "heading": "IV. PERFORMANCE EVALUATION",
            "text": "In this section, we present a simulation study to shed some light on the performance of our proposed method. First, we provide a simulation result that relates the proposed approach to other loss functions that quantify the estimator. Then, the convergence and time complexity of the proposed VB-MSE method, along with other competitive methods, is presented. Lastly, a simulation of the portfolio optimization problem is considered using synthetic and real data."
        },
        {
            "heading": "A. THE PROPOSED VB-MSE CONSISTENT ESTIMATOR AND QUANTIFYING THE ESTIMATOR",
            "text": "We relate our proposed estimator to other loss functions used in quantifying the closeness of an estimate to the true covariance matrix. Specifically, we use the minimum-variance and the inverse-Frobenius loss functions. The minimum-variance loss function is defined as [25]\nLMV ( \u03a3\u0302\u03b3 ,\u03a3 ) ,\ntr ( \u03a3\u0302 \u22121 \u03b3 \u03a3\u03a3\u0302 \u22121 \u03b3 ) /p[\ntr ( \u03a3\u0302 \u22121 \u03b3 ) /p ]2 \u2212 1tr (\u03a3\u22121) /p .\n(25)\nAn important performance measure that involves this loss function is the percentage relative improvement in average loss (PRIAL) defined as\nPRIALMV ( \u03a3\u0302\u03b3 ) :=\nE [ LMV ( \u03a3\u0302,\u03a3 )] \u2212 E [ LMV ( \u03a3\u0302\u03b3 ,\u03a3 )] E [ LMV ( \u03a3\u0302,\u03a3 )] \u2212 E [ LMV ( \u03a3\u0302 \u2217 ,\u03a3 )] \u00d7 100%, (26)\nwhere \u03a3\u0302 \u2217\nis the finite sample-optimal rotation-equivariant estimator, which is the closest estimator to \u03a3 according to the minimum variance loss [25]. Based on the rotationequivariant assumption, the eigenvectors of \u03a3\u0302 \u2217 and the SCM, \u03a3\u0302, are the same but their eigenvalues differ. All the methods used in this paper, including ours, belong to this class of estimators, i.e., rotation-equivariant estimators. We obtain the eigenvalue decomposition of the SCM from\n\u03a3\u0302 = UDUT . (27)\nThen, the finite sample-optimal estimator is [25]\n\u03a3\u0302 \u2217 = UD\u2217UT , (28)\nwhere D\u2217 is the matrix of eigenvalues that minimizes the minimum variance loss function; it is calculated as [25]\nD\u2217 = UT\u03a3U. (29)\nNote this finite-sample-optimal estimator is unattainable because it requires the knowledge of the true covariance matrix. From the definition (26), PRIALMV ( \u03a3\u0302 )\n= 0%. This means that the SCM represents a reference against which any loss reduction is measured. Similarly, PRIALMV ( \u03a3\u0302 \u2217) ="
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n100% is the maximum amount of loss reduction that can be achieved under the rotation-equivariant assumption. The PRIAL estimates how much of the possibility for variance reduction is captured by any other estimator [25].\nThe second loss function is the inverse-Frobenius loss function defined as [25]\nLIF ( \u03a3\u0302\u03b3 ,\u03a3 ) = \u2016\u03a3\u0302 \u22121 \u03b3 \u2212\u03a3 \u22121\u20162F (30)\n, 1\np tr\n[( \u03a3\u0302 \u22121 \u03b3 \u2212\u03a3 \u22121 )2]\n(31)\nIn Fig. 2, we plot the proposed VB-MSE consistent estimator (normalized) along with the inverse Frobenius loss (normalized) and PRIAL versus the regularization parameter value. As can be seen from the figure, the value of \u03b3 that minimizes the VB-MSE consistent estimator almost coincides with the one that minimizes the inverse-Frobenius loss and maximizes the PRIAL.\nIt is remarkable that although our proposed consistent MSE estimator involves only estimated quantities, its performance almost matches both the PRIAL and Frobenius that rely on the true covariance matrix.\nIn the following subsections, we provide a Monte Carlo simulation study of the VB-MSE estimator against other estimators. The competitive methods are the elliptical estimators ELL1-RSCM, ELL2-RSCM and ELL3-RSCM [5], [26], the Ledoit-Wolf estimator, LW-RSCM [6], [26], and the nonlinear estimator Quest 1 [27].\nThe results are generated from Gaussian data that follows the model (3) with [\u03a3]i,j = 0.6|i\u2212j|\u03c3, (\u03c3 = 1 \u00d7 10\u22124). We study convergence and time complexity."
        },
        {
            "heading": "B. CONVERGENCE",
            "text": "We consider the convergence under the large-dimensional asymptotic regime, where the data dimension, p, and the number of observations, n, grow to infinity with a ratio p/n that converges to some limit. In Fig. 3, we consider a ratio p/n that converges to 12 . We plot the PRIAL measure of each method versus p. It can be seen that the VB-MSE method has the highest PRIAL when p = 10, 20, and 50. For p \u2265 100, the nonlinear estimator Quest 1 wins the comparison. This result agrees with a previous study [25], which shows that the nonlinear estimators generally converge better than the linear estimators."
        },
        {
            "heading": "C. TIME COMPLEXITY",
            "text": "All the methods presented require computing the SCM, which is of complexity O(p2n). Both LW-RSCM and Ell2RSCM are computationally more efficient than Ell1-RSCM as pointed in [5], especially for the high-dimension setup. For the proposed method, we observe that implementing the VB-MSE estimator according to the steps presented in III-A requires computing (23), (24) and the last term in (22), each of O(p3) complexity. Also, the line search method used to obtain the regularization parameter in the VB-MSE approach increases the time complexity. The nonlinear shrinkage estimator, Quest 1, requires a complexity of O(p3) twice for extracting and recombining the eigenvalues and eigenvectors [25].\nWe can enhance the speed of the VB-MSE by observing\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthat we can write\ntr[(\u03a3\u0302(\u03a3\u0302 + \u03b3Ip)\u22121] = d\u2211 j=1 dj dj + \u03b3 , (32)\ntr[(\u03a3\u0302 1 2 (\u03a3\u0302 1 2 \u2212 i\u221a\u03b3Ip)\u22121] = d\u2211 j=1 \u221a dj\u221a dj \u2212 i \u221a \u03b3 , (33)\ntr[(\u03a3\u0302(\u03a3\u0302 + \u03b3Ip)\u22122] = d\u2211 j=1 dj (dj + \u03b3)2 . (34)\nThe formulas in (32)\u2013(34) can be used in (23), (24) and (13). Note that we need the eigenvalue decomposition (O(p3) complexity) one time.\nWe consider a runtime example using Matlab R2019a running on a 64-bit, Core(TM) i7-2600K 3.40 GHz Windows PC. The result is plotted in Fig. 4 which shows that the VBMSE method is faster than Quest 1, Ell1-RSCM and Ell3RSCM when 50 < p < 500.\nWe also consider a simulation example for a very highdimension scenario, p = 5000 and n = 10000, as shown in Table 1. While our proposed method (VB-MSE) performs slowly at this dimension, the PRIAL measure reveals a slight improvement compared to the remaining methods. Quest 1 method is not feasible for implementation at very high dimension [25].\nD. PORTFOLIO OPTIMIZATION SIMULATION USING SYNTHETIC DATA This section simulates the portfolio optimization problem using synthetic data generated from Gaussian distribution\n1 2 2021 4041 60 250Window 1\ntrain(i = 1)\ntest(i = 1)\ntrain(i = 2)\ntest(i = 2)\nDay\n1 2 21 4041 6061 80 250Window 2\ntrain(i = 1)\ntest(i = 1)\ntrain(i = 2)\ntest(i = 2)\nDay\n1\nFIGURE 5: Out-of-sample method for portfolio optimization.\nwith [\u03a3]i,j = 0.6|i\u2212j|\u03c3, (\u03c3 = 1\u00d7 10\u22125). As conventionally described in the financial literature, we implement the out-ofsample strategy defined in terms of a rolling window method (see [7]). At a particular day t, the training window for CM estimation is formed from the previous n days, i.e., from t\u2212n to t\u22121, to design the portfolio weights, w\u0302GMVP. The portfolio returns in the following 20 days are computed based on these weights. Next, the window is shifted 20 days forward and the returns for another 20 days are computed. The same procedure is repeated until the end of the data. Finally, the realized risk is computed as the standard deviation of the returns.\nFig. 5 illustrates an example of how to perform the outof-sample procedure. Assume that we aim to study the performance of each method over a year (250 working days). In Window 1, we perform the first iteration (i = 1) that we train the first available 20 days (Day 1 \u2013 Day 20) and use the following 20 days as test data (Day 21 \u2013 Day 40). The next iteration in Window 1 (i = 2) considers Day 21 \u2013 Day 40 for the training phase and uses data of Day 41 \u2013 Day 60 for testing, and so on. For Window 2, train data size is increased to 40 in each iteration. For example, the first iteration (i = 1), train data is taken from Day 1 \u2013 Day 40 and test data from Day 41 \u2013 Day 60. The second iteration (i = 2) collects data of Day 21 \u2013 Day 60 for training, and data of Day 61 \u2013 Day 80 for testing, etc. The subsequent windows follow similarly.\nFig. 6 simulates the aforementioned out-of-sample procedure considering two scenarios. The first scenario assumes data in (3) has zero mean (\u00b5 = 0), and the second scenario assumes \u00b5 6= 0. These two scenarios are shown in Fig. 6 (a), and Fig. 6 (b), respectively, with p = 200. As can be seen, the proposed VB-MSE method noticeably outperforms all the methods. The same observation can be reported when p = 300 in Fig. 6 (d), and Fig. 6 (e).To study the effect of the presence of mean, we plot the difference in the Frobenius loss between the two scenarios as shown in Fig. 6 (c) (p = 200) and in Fig. 6 (f) (p = 300). Both figures reveal that the difference is the smallest when using the VB-MSE method."
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThis result supports our previous claim that the proposed method should perform well because it incorporates the error in estimating the mean.\nE. PORTFOLIO OPTIMIZATION SIMULATION USING REAL DATA The following list describes the data from different stock market indices used in our evaluation:\n\u2022 Standard and Poor\u2019s 500 (S&P 500) index: This index includes 500 companies. The net returns of 484 stocks (p = 484) are obtained for 784 working days between 7 Jan. 2015 and 22 Dec. 2017. \u2022 Standard and Poor\u2019s 100 (S&P 100) index: The index is a subset of the S&P 500 that comprises 100 stocks. We consider two different periods to obtain the net returns from different stocks [28]. The first period is from 7 Jan. 2014 to 31 Dec. 2015 (501 trading days), where we fetch data of 97 stocks (p = 97). The second period is from 2 Jan., 2015 to 30 Dec. 2016 (504 trading days) that contains net returns of 97 stocks (p = 97). \u2022 NYSE Arca Major Market Index (XMI): This market index is made up of 20 Blue Chip industrial stocks of major U.S. corporations [29]. A full-length time series containing 503 working days from 4 Jan., 2016 to 29 Dec. 2017 is obtained for 19 stocks (p = 19). The second period is from 10 Jan. 2014 to 31 Dec. 2015 (498 working days). \u2022 Hang Seng Index (HSI): This market index comprises 50 stocks [30]. The returns of all the stocks (p = 50) is obtained from 1 Jan. 2016 to 27 Dec. 2017 (491 trading days).\nFig. 7 shows the annualized realized risk of the aforementioned market indices versus the number of training samples. We compare the proposed vector-based method, VB-MSE, against the elliptical estimators ELL1-RSCM, ELL2-RSCM and ELL3-RSCM [5], [26], the Ledoit-Wolf estimator, LWRSCM [6], [26], the nonlinear estimator Quest 1 [27] .\nFig. 7 (a) plots the result of the S&P 100 index from 2 Jan. 2015 to 30 Dec. 2016. As can be seen from the figure, the performance of the proposed VB-MSE method outperforms all other the methods except at n = 80 and 100, where it is slightly worse than Ell1-RSCM aand Ell3-RSCM. Similarly, VB-MSE has a superior performance in Fig. 7 (b), which plots the result from 7 Jan. 2014 to 31 Dec. 2015. However, at n = 20 and 80 Ell1-RSCM and Ell3-RSCM perform better. The realized risk for the HSI index is depicted in Fig. 7 (c) from 1 Jan. 2016 \u2013 27 Dec. 2017. The proposed method has comparable performance to Quest 1, Ell1-RSCM and Ell3-RSCM at n = 20, 40 and 60 but it outperforms all the methods for 100 < n \u2264 340. The results of the XMI index from 4 Jan. 2016 \u2013 29 Dec. 2017 and from 10 Jan. 2014 \u2013 31 Dec. 2015 are shown in Fig. 7 (d) and Fig. 7 (e), respectively. Overall, in both figures, VB-MSE is the best performing method. Finally, Fig. 7 (f) plots the realized risk of the S&P 500 index from 10 Jan. 2015 \u2013 31 Dec. 2017. The\nfigure shows clearly that the proposed method outperforms the other methods when 200 \u2264 n \u2264 400.\nFrom Fig. 7 (a) \u2013 (f), we can conclude that, on average, the proposed VB-MSE method compares favorably to all the benchmark methods tested in this paper. The method is also more consistent over the various datasets."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we have proposed a regularized covariance matrix estimator under high-dimensionality settings. Unlike the competitive methods, the proposed method exploited a linear model with bounded uncertainties in estimating the true covariance matrix and the mean. Based on this model, the estimation problem is reduced to minimizing the MSE of the noise vector. The proposed method searches for the optimal regularization parameter based on a consistent estimator of the MSE of the estimated vector. Portfolio optimization results from real financial data show that the proposed method performs reasonably well and outperforms a host of benchmark methods.\n."
        },
        {
            "heading": "APPENDIX A MATHEMATICAL TOOLS",
            "text": "For convenience, we write Equation (3) in matrix form\nY = \u03a3 1 2 X + \u00b5 1n, (35)\nwhere X = [x1x2 \u00b7 \u00b7 \u00b7xn] with xi \u223c N (0, Ip). We need to express the SCM in (7) in an appropriate matrix form as well, as follows:\n\u03a3\u0302 = 1\nn\u2212 1 BBT , (36)\nwhere B \u2208 Rp\u00d7n. It can be immediately recognized from (7) that B is\nB = Y \u2212 \u00b5\u03021Tn . (37)\nAlso, we can easily verify that\n\u00b5\u0302 = \u00b5 + 1\nn Z1n, (38)\nwhere Z , \u03a3 1 2 X. Finally, we perform the following operations to reach the model of \u03a3\u0302 at the end:\n\u03a3\u0302 = 1\nn\u2212 1\n( ZZT \u2212 Z1n1 T n n ZT )\n= 1\nn\u2212 1 Z\n( In \u2212 1n1 T n\nn\n) ZT\n= 1\nn\u2212 1 ZUTUTZT\n= 1\nn\u2212 1 \u03a3\n1 2 XUTUTXT\u03a3 1 2 (39)\n= 1\nn\u2212 1 \u03a3\n1 2 X\u0303TX\u0303T\u03a3 1 2 , (40)\nwhere U and T are the matrices of eigenvalue vectors and eigenvalues, respectively, of (In \u2212 1n1 T n\nn ) obtained using\nthe eigenvalue decomposition. The Gaussian distribution is\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n80 100 120 140 160 180 200 6.6\n6.8\n7\n7.2\n7.4\n7.6\nA n n . re\na liz\ne d r\nis k\n10 -3\nEll1-RSCM Ell2-RSCM Ell3-RSCM LW-RSCM Quest 1 VB-MSE Optimal\n(a)\n80 100 120 140 160 180 200 0.006\n0.008\n0.01\n0.012\n0.014\nA n n . re\na liz\ne d r\nis k\nEll1-RSCM Ell2-RSCM Ell3-RSCM LW-RSCM Quest 1 VB-MSE Optimal\n(b)\n80 100 120 140 160 180 200 0\n50\n100\n150 Difference in the Frobenius Loss\nEll1-RSCM Ell2-RSCM Ell3-RSCM LW-RSCM Quest 1 VB-MSE\n(c)\n50 100 150 200 250 300 5.4\n5.6\n5.8\n6\n6.2\nA n\nn .\nre a\nliz e\nd r\nis k\n10 -3\nEll1-RSCM Ell2-RSCM Ell3-RSCM LW-RSCM Quest 1 VB-MSE Optimal\n(d)\n50 100 150 200 250 300 5\n6\n7\n8\nA n n . re\na liz\ne d r\nis k\n10 -3\nEll1-RSCM Ell2-RSCM Ell3-RSCM LW-RSCM Quest 1 VB-MSE Optimal\n(e)\n50 100 150 200 250 300 0\n20\n40\n60\n80\n100\n120\n140 Difference in the Frobenius Loss\nEll1-RSCM Ell2-RSCM Ell3-RSCM LW-RSCM Quest 1 VB-MSE\n(f)\nFIGURE 6: Annualized realized risk versus training window length using synthetic data generated from Gaussian distribution.\ninvariant when multiplying by a unitary matrix; hence, X\u0303 has the same distribution as X [14], [16].\nThe model (40) is a well-established model is the RMT literature. Based on this model, for z \u2208 C\u2212R+ and bounded \u0398 \u2208 Rp\u00d7p the following relations, which will be used throughout the derivations, hold true (under Assumption 1) [11]:\ntr [ \u0398 ( \u03a3\u0302\u2212 zIp )\u22121] tr [ \u0398 ( \u03b4\u0303\u03a3\u2212 zIp )\u22121] (41)\ntr [ \u0398\u03a3\u0302 ( \u03a3\u0302\u2212 zIp )\u22121] \u03b4\u0303tr [ \u0398 ( \u03b4\u0303\u03a3\u2212 zIp )\u22121] (42)\ntr [ \u0398 ( \u03a3\u0302\u2212 zIp )\u22122] tr [ \u0398\u03a3 ( \u03b4\u0303\u03a3\u2212 zIp )\u22122] (43)\ntr [ \u0398\u03a3\u0302 ( \u03a3\u0302\u2212 zIp )\u22122] (\u03b4\u0303 \u2212 z\u03b4\u0303\u2032)tr [ \u0398\u03a3 ( \u03b4\u0303\u03a3\u2212 zIp )\u22122] .\n(44)\nAPPENDIX B DERIVING THE MSE FORMULA The MSE in (15) can be easily obtained from expanding (13) and computing the resulted terms. The first term results from E[xtxTt ] = Ip. The second term is computed as follows:\ntr [ E [ x\u0302tx\u0302 T ]] = tr [ E [ \u03a3\u0302 1 2 \u03a3\u0302 \u22121 \u03b3 y\u0303y\u0303 T \u03a3\u0302 \u22121 \u03b3 \u03a3\u0302 1 2 ]] . (45)\nwhere y\u0303 , (y \u2212 \u00b5\u0302) = ( \u03a3 1 2 xt + \u03b4 ) . Using the fact that the expectation and the trace are interchangeable and the cyclic property of traces, we can write\ntr [ E [ x\u0302tx\u0302 T t ]] (46)\n= tr [ E [ \u03a3\u0302 \u22121 \u03b3 \u03a3\u0302\u03a3\u0302 \u22121 \u03b3 ( \u03a3 1 2 xt + \u03b4 )( \u03a3 1 2 xt + \u03b4 )T]] .\n(47)\nAlso, using the eigenvalue decomposition of \u03a3\u0302, it is easy to prove that\n\u03a3\u0302 \u22121 \u03b3 \u03a3\u0302 = ( \u03a3\u0302 + \u03b3Ip )\u22121 \u03a3\u0302 (48)\n= \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip )\u22121 (49)\n= \u03a3\u0302 1 2 ( \u03a3\u0302 + \u03b3Ip )\u22121 \u03a3\u0302 1 2 . (50)\nHence,\ntr [ E [ x\u0302tx\u0302 T t ]] (51)\n= tr [ E [ \u03a3\u0302\u03a3\u0302 \u22122 \u03b3 ( \u03a3 1 2 xt + \u03b4 )( \u03a3 1 2 xt + \u03b4 )T]] . (52)\nObserving that xt, \u03b4 and \u03a3\u0302 are independent, and \u03b4 \u223c N (0, \u03a3n ), we obtain\ntr [ E [ x\u0302tx\u0302 T t ]] (53)\n= tr [ \u03a3\u03a3\u0302 ( \u03a3\u0302 + \u03b3I )\u22122] + 1 n tr [ \u03a3\u03a3\u0302 ( \u03a3\u0302 + \u03b3I )\u22122] (54)\n= n+ 1 n tr [ \u03a3\u03a3\u0302 ( \u03a3\u0302 + \u03b3I )\u22122] . (55)\nFinally, the third term is obtained from\ntr [ E [ x\u0302tx T t ]] (56)\n= tr [ E [ \u03a3\u0302 1 2 \u03a3\u0302 \u22121 \u03b3 (y \u2212 \u00b5\u0302)xTt ]] (57)\n= tr [ E [ \u03a3\u0302 1 2 \u03a3\u0302 \u22121 \u03b3 ( \u03a3 1 2 xt + \u03b4 ) xTt ]] (58)\n= tr [ \u03a3 1 2 \u03a3\u0302 1 2 \u03a3\u0302 \u22121 \u03b3 ] . (59)"
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAPPENDIX C PROOF OF THEOREM 1 E[A(\u03b3)] in (15) can be directly obtained from (43) with setting \u0398 = \u03a3 and z = \u2212\u03b3. The second term,B(\u03b3), resulted from adding and subtracting i \u221a \u03b3Ip with factoring \u03a3\u0302 \u22121 \u03b3 as follows:\ntr [ \u03a3 1 2 \u03a3\u0302 1 2 \u03a3\u0302 \u22121 \u03b3 ] = tr { \u03a3 1 2 [( \u03a3\u0302 1 2 + i \u221a \u03b3Ip ) \u2212 i\u221a\u03b3I ] (60)\n. [( \u03a3\u0302 1 2 + i \u221a \u03b3I )( \u03a3\u0302 1 2 \u2212 i\u221a\u03b3Ip )]\u22121} (61)\n= tr [ \u03a3 1 2 ( \u03a3\u0302 1 2 \u2212 i\u221a\u03b3I )\u22121] \u2212 i\u221a\u03b3tr [ \u03a3 1 2 \u03a3\u0302 \u22121 \u03b3 ] . (62)\nWe can further simplify (62) by noticing that the quantity on the left-hand side is a real quantity, so this implies\n= [ tr [ \u03a3 1 2 ( \u03a3\u0302 1 2 \u2212 i\u221a\u03b3I )\u22121]] = \u221a \u03b3tr [ \u03a3 1 2 \u03a3\u0302 \u22121 \u03b3 ] . (63)\nThus, we can express B(\u03b3) equivalently as tr [ \u03a3 1 2 \u03a3\u0302 1 2 \u03a3\u0302 \u22121 \u03b3 ] = < [ tr [ \u03a3 1 2 ( \u03a3\u0302 1 2 \u2212 i\u221a\u03b3I )\u22121]] , (64)\nso we can find E[B(\u03b3)] easily from (41) with setting \u0398 = \u03a3 1 2 , z = i \u221a \u03b3, and the third term in (17) resulted."
        },
        {
            "heading": "APPENDIX D PROOF OF THEOREM 2",
            "text": "The MSE(\u03b3) expressed in (17) converges to a sum of deterministic terms. To find a consistent estimator of (17), it is\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nsufficient to find a consistent estimator of each of these terms (Theorem 3.2.6 in [31]).\nThe consistent estimator, \u03b4\u03021, can be derived using (18) and (19) in (42) ( with \u0398 = Ip),\n1 n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip )\u22121] \u03b41 1 + \u03b41 . (65)\nThe consistent estimator, \u03b4\u03021, in (23) (that satisfies \u03b4\u0302 \u03b4) results immediately after rearranging (65). The derivation of \u03b4\u03022 follows similarly.\nTo derive the consistent estimator of \u03c6 , 1\nn\n[ \u03a32 ( \u03b4\u03031\u03a3 +\n\u03b3I )\u22122] we differentiate \u03b41 to obtain\n\u03b4\u20321 = \u03a8\n1\u2212 \u03c6(1 + \u03b41)\u22122 , (66)\nwhere \u03a8 , 1\nn tr[\u03a3(\u03b4\u03031\u03a3 + \u03b3Ip)\u22122]. Hence, we can estimate\n\u03c6 consistently, (i.e., \u03c6\u0302 \u03c6) as\n\u03c6\u0302 = \u03b4\u0302\u20321 \u2212 \u03a8\u0302\n\u03b4\u0302\u20321(1 + \u03b4\u03021) \u22122 , (67)\nwhere \u03a8\u0302 is the consistent estimator of \u03a8 can be estimated from (44) when \u0398 = I, z = \u2212\u03b3, as follows:\n\u03a8\u0302 =\n1 n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip )\u22122] ( \u02c6\u0303 \u03b41 + \u03b3 \u02c6\u0303 \u03b4\u20321) . (68)\nSubstituting in (67), we can obtain the second term as\n1\nn\n[ \u03a32 ( \u03b4\u03031\u03a3 + \u03b3I )\u22122] (1 + \u03b4\u03021) 2\n\u03b4\u0302\u20321\n[ \u03b4\u0302\u20321 \u2212 1 n tr [ \u03a3\u0302 ( \u03a3\u0302 + \u03b3Ip )\u22122] ( \u02c6\u0303 \u03b41 + \u03b3 \u02c6\u0303 \u03b4\u20321) ] . (69)\nREFERENCES [1] T. Bodnar, S. Dmytriv, N. Parolya, and W. Schmid, \u201cTests for the weights\nof the global minimum variance portfolio in a high-dimensional setting,\u201d IEEE Transactions on Signal Processing, vol. 67, no. 17, pp. 4479\u20134493, 2019. [2] S. Huni and A. B. Sibindi, \u201cAn application of the markowitz\u2019s meanvariance framework in constructing optimal portfolios using the johannesburg securities exchange tradeable indices,\u201d The Journal of Accounting and Management, vol. 10, no. 2, 2020. [3] H. Markowitz, \u201cPortfolio selection,\u201d The Journal of Finance, vol. 7, no. 1, pp. 77\u201391, 1952. [Online]. Available: http://www.jstor.org/stable/2975974 [4] R. Couillet and M. Debbah, \u201cSignal processing in large systems: A new paradigm,\u201d IEEE Signal Processing Magazine, vol. 30, no. 1, pp. 24\u201339, 2012. [5] E. Ollila and E. Raninen, \u201cOptimal shrinkage covariance matrix estimation under random sampling from elliptical distributions,\u201d IEEE Transactions on Signal Processing, vol. 67, no. 10, pp. 2707\u20132719, 2019. [6] O. Ledoit and M. Wolf, \u201cA well-conditioned estimator for largedimensional covariance matrices,\u201d Journal of multivariate analysis, vol. 88, no. 2, pp. 365\u2013411, 2004. [7] L. Yang, R. Couillet, and M. R. McKay, \u201cA robust statistics approach to minimum variance portfolio optimization,\u201d IEEE Transactions on Signal Processing, vol. 63, no. 24, pp. 6684\u20136697, 2015. [8] E. Ollila, D. P. Palomar, and F. Pascal, \u201cShrinking the eigenvalues of mestimators of covariance matrix,\u201d IEEE Transactions on Signal Processing, 2020.\n[9] T. T. Cai, J. Hu, Y. Li, and X. Zheng, \u201cHigh-dimensional minimum variance portfolio estimation based on high-frequency data,\u201d Journal of Econometrics, vol. 214, no. 2, pp. 482\u2013494, 2020. [10] T. Ballal, A. S. Abdelrahman, A. H. Muqaibel, and T. Y. Al-Naffouri, \u201cAn adaptive regularization approach to portfolio optimization,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5175\u20135179. [11] F. Rubio, X. Mestre, and D. P. Palomar, \u201cPerformance analysis and optimal selection of large minimum variance portfolios under estimation risk,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 6, no. 4, pp. 337\u2013350, 2012. [12] S. Chandrasekaran, G. Golub, M. Gu, and A. H. Sayed, \u201cParameter estimation in the presence of bounded data uncertainties,\u201d SIAM Journal on Matrix Analysis and Applications, vol. 19, no. 1, pp. 235\u2013252, 1998. [13] Y. Guo, T. Hastie, and R. Tibshirani, \u201cRegularized linear discriminant analysis and its application in microarrays,\u201d Biostatistics, vol. 8, no. 1, pp. 86\u2013100, 2007. [14] A. Zollanvari and E. R. Dougherty, \u201cGeneralized consistent error estimator of linear discriminant analysis,\u201d IEEE transactions on signal processing, vol. 63, no. 11, pp. 2804\u20132814, 2015. [15] T. Ballal, M. A. Suliman, and T. Y. Al-Naffouri, \u201cBounded perturbation regularization for linear least squares estimation,\u201d IEEE Access, vol. 5, pp. 27 551\u201327 562, 2017. [16] K. Elkhalil, A. Kammoun, R. Couillet, T. Y. Al-Naffouri, and M.-S. Alouini, \u201cA large dimensional study of regularized discriminant analysis,\u201d IEEE Transactions on Signal Processing, vol. 68, pp. 2464\u20132479, 2020. [17] B. D. Carlson, \u201cCovariance matrix estimation errors and diagonal loading in adaptive arrays,\u201d IEEE Transactions on Aerospace and Electronic systems, vol. 4, no. 4, pp. 397\u2013401, 1988. [18] J. Li, P. Stoica, and Z. Wang, \u201cOn robust capon beamforming and diagonal loading,\u201d IEEE transactions on signal processing, vol. 51, no. 7, pp. 1702\u2013 1715, 2003. [19] M. Mahadi, T. Ballal, M. Moinuddin, T. Y. Al-Naffouri, and U. Al-Saggaf, \u201cLow-complexity robust beamforming for a moving source,\u201d in 2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 1846\u20131850. [20] \u2014\u2014, \u201cA robust lcmp beamformer with limited snapshots,\u201d in 2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 1831\u20131835. [21] M. A. Suliman, H. Sifaou, T. Ballal, M.-S. Alouini, and T. Y. Al-Naffouri, \u201cRobust estimation in linear ill-posed problems with adaptive regularization scheme,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4504\u20134508. [22] S. M. Kay, Fundamentals of statistical signal processing: estimation theory. Prentice-Hall, Inc., 1993. [23] F. Rubio, \u201cGeneralized consistent estimation in arbitrarily high dimensional signal processing,\u201d Ph.D. dissertation, Universitat Politecnica de Catalunya, Barcelona, 2008. [24] L. B. Niyazi, A. Kammoun, H. Dahrouj, M.-S. Alouini, and T. Y. AlNaffouri, \u201cAsymptotic analysis of an ensemble of randomly projected linear discriminants,\u201d IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 3, pp. 914\u2013930, 2020. [25] O. Ledoit and M. Wolf, \u201cAnalytical nonlinear shrinkage of largedimensional covariance matrices,\u201d The Annals of Statistics, vol. 48, no. 5, pp. 3043\u20133065, 2020. [26] E. Ollila and E. Raninen. Matlab regularizedscm toolbox version 1.0. [Online]. Available: http://users.spa.aalto.fi/esollila/regscm/, [27] O. Ledoit and M. Wolf, \u201cNumerical implementation of the quest function,\u201d Computational Statistics & Data Analysis, vol. 115, pp. 199\u2013223, 2017. [28] [Online]. Available: https://www.spglobal.com/spdji/en/indices/equity/sp100 [29] [Online]. Available: https://www.nyse.com/quote/index/XMI [30] [Online]. Available: https://www.hsi.com.hk/eng [31] A. Takeshi, Advanced econometrics. Harvard university press, 1985."
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nMAAZ MAHADI received the B.Sc. degree (Hons.) in electrical engineering from the University of Khartoum, Sudan, the M.Sc. in electrical engineering from International Islamic University Malaysia, Kuala Lumpur, Malaysia, in 2014. He is currently a Ph.D. candidate at King Abdulaziz University, Jeddah, Saudi Arabia. His research interests include statistical signal processing applications, beamforming, discriminant analysis and random matrix theory.\nTARIG BALLAL (M\u201917) received the B.Sc. degree (Hons.) in electrical engineering from the University of Khartoum, Sudan, in 2001, the M.Sc. degree in telecommunications from the Blekinge Institute of Technology, Sweden, in 2005, and the Ph.D. degree from the School of Computer Science and Informatics, University College Dublin, Dublin, Ireland, in 2011. From 2011 to 2012, he was a Research Engineer with Resmed Sensor Technologies Ltd., and University\nCollege Dublin. During this period, he developed methods for non-contact breathing monitoring and classification from wireless biosensors data. Since 2012, he has been with the Electrical Engineering Department, King Abdullah University of Science and Technology, Saudi Arabia, where he is currently a Research Scientist. His current research focuses on regularization and robust estimation methods, image and signal processing, acoustic and electromagnetic sensing, and tracking and localization. His research interest also includes GNSS precise positioning and attitude determination, in addition to signal processing applications in wireless communications.\nMUHAMMAD MOINUDDIN received the B.S. degree from the Nadirshaw Edulji Dinshaw University of Engineering and Technology, Karachi, Pakistan, in 1998, and the M.S. and Ph.D. degrees from the King Fahd University of Petroleum and Minerals (KFUPM), Dharan, Saudi Arabia, in 2001 and 2006, respectively, all in electrical engineering. He was an Assistant Professor with the Department of Electrical Engineering, KFUPM, and the Hafr Al-Batin Community College, Hafr\nAl-Batin, Saudi Arabia, from 2007 to 2010. He was an Associate Professor with the Telecommunication Engineering Department, Iqra University, Karachi, from 2010 to 2012. He was an Assistant Professor with the Department of Electrical Engineering,(KAU), Jeddah, Saudi Arabia, from 2013 to 2015, where he is currently an Associate Professor and is also associated with the Center of Excellence in Intelligent Engineering Systems. His research interests include adaptive filtering, wireless communications, and signal processing for communications.(Based on document published on 27 September 2019).\nUBAID M. AL-SAGGAF received his double B.Sc. degrees with highest honors in Electrical Engineering and Mathematics in 1980 from King Fahd University of Petroleum and Minerals (KFUPM), Saudi Arabia. He received his M.Sc. degree in 1983 and the Ph.D. in 1986 in Electrical Engineering from Stanford University, USA. He joined KFUPM as an Assistant Professor in 1986. He was on leave from 1992 to 2010, working for the Ministry of Defense and Aviation as an\nExecutive Technical Advisor in the Research and Development Department. He joined King Abdulaziz University (KAU) in September 2010, where he is currently a Professor. He is also the Founder and Director of the Center of Excellence in Intelligent Engineering Systems (CEIES), and the Founder and Director of Innovation and Prototyping Center (IPC) both at KAU. His field of interests and specializations cover a broad spectrum from theoretical to practical aspects of engineering, including systems, control, communications, and signal processing.\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        }
    ],
    "title": "Portfolio Optimization Using a Consistent Vector-Based MSE Estimation Approach",
    "year": 2022
}