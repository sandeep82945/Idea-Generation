{
    "abstractText": "It is well known that the traditional Jensen inequality is proved by lower bounding the given convex function, f (x), by the tangential affine function that passes through the point (E{X}, f (E{X})), where E{X} is the expectation of the random variable X. While this tangential affine function yields the tightest lower bound among all lower bounds induced by affine functions that are tangential to f , it turns out that when the function f is just part of a more complicated expression whose expectation is to be bounded, the tightest lower bound might belong to a tangential affine function that passes through a point different than (E{X}, f (E{X})). In this paper, we take advantage of this observation by optimizing the point of tangency with regard to the specific given expression in a variety of cases and thereby derive several families of inequalities, henceforth referred to as \u201cJensen-like\u201d inequalities, which are new to the best knowledge of the author. The degree of tightness and the potential usefulness of these inequalities is demonstrated in several application examples related to information theory.",
    "authors": [
        {
            "affiliations": [],
            "name": "Neri Merhav"
        }
    ],
    "id": "SP:67bd673d6a3d92e53162bf52c00441a59261c2fd",
    "references": [
        {
            "authors": [
                "T.M. Cover",
                "J.A. Thomas"
            ],
            "title": "Elements of Information Theory, 2nd ed.",
            "year": 2006
        },
        {
            "authors": [
                "L. Xiao",
                "G. Lu"
            ],
            "title": "A new refinement of Jensen\u2019s inequality with applications in information theory",
            "venue": "Open Math. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Deng",
                "H. Ullah",
                "M.A. Khan",
                "S. Iqbal",
                "S. Wu"
            ],
            "title": "Refinements of Jensen\u2019s inequality via majorization results with applications in information theory",
            "venue": "Hindawi J. Math",
            "year": 2021
        },
        {
            "authors": [
                "S. Wu",
                "M.A. Khan",
                "T. Saeed",
                "Z.M.M.M. Sayed"
            ],
            "title": "A refined Jensen inequality connected to an arbitrary positive finite sequence",
            "year": 2022
        },
        {
            "authors": [
                "Y. Sayyari",
                "H. Barsam",
                "A.R. Sattarzadeh"
            ],
            "title": "On new refinement of the Jensen inequality using uniformly convex functions with applications",
            "year": 2023
        },
        {
            "authors": [
                "E. Jaafari",
                "M.S. Asgari",
                "M.S. Hosseini",
                "B. Moosavi"
            ],
            "title": "On the Jensen\u2019s inequality and its variants",
            "venue": "AIMS Math. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A. Matkovi\u0107",
                "J. Pec\u0306ari\u0107"
            ],
            "title": "A variant of Jensen\u2019s inequality for convex functions of several variables",
            "venue": "J. Math. 2007,",
            "year": 2007
        },
        {
            "authors": [
                "M.K. Bakula",
                "A. Matkovi\u0107",
                "J. Pec\u0306ari\u0107"
            ],
            "title": "On a variant of Jensen\u2019s inequality for functions of nondecreasing increments",
            "venue": "J. Korean Math. Soc",
            "year": 2008
        },
        {
            "authors": [
                "A. Seuret",
                "F. Gouaisbaut"
            ],
            "title": "Reducing the Gap of Jensen\u2019s Inequality by Using Wirtinger Inequality",
            "venue": "Preprint Submitted to Automatica",
            "year": 2012
        },
        {
            "authors": [
                "S.G. Walker"
            ],
            "title": "On a lower bound for the Jensen inequality",
            "venue": "SIAM J. Math. Anal",
            "year": 2014
        },
        {
            "authors": [
                "J.G. Liao",
                "A. Berg"
            ],
            "title": "Sharpening Jensen\u2019s inequality",
            "venue": "Am. Stat",
            "year": 2019
        },
        {
            "authors": [
                "T. Jebara",
                "A. Pentland"
            ],
            "title": "On reversing Jensen\u2019s inequality",
            "venue": "In Proceedings of the 13th International Conference on Neural Information Processing Systems (NIPS",
            "year": 2000
        },
        {
            "authors": [
                "I. Budimir",
                "S.S. Dragomir",
                "J. Pe\u010dari\u0107"
            ],
            "title": "Further reverse results for Jensen\u2019s discrete inequality and applications in information theory",
            "venue": "J. Inequalities Pure Appl. Math. 2001,",
            "year": 2001
        },
        {
            "authors": [
                "S. Simi\u0107"
            ],
            "title": "On an upper bound for Jensen\u2019s inequality",
            "venue": "J. Inequalities Pure Appl. Math. 2009,",
            "year": 2009
        },
        {
            "authors": [
                "S. Simi\u0107"
            ],
            "title": "On a new converse of Jensen\u2019s inequality",
            "venue": "Publ. L\u2019inst. Math",
            "year": 2009
        },
        {
            "authors": [
                "S.S. Dragomir"
            ],
            "title": "Some reverses of the Jensen inequality with applications",
            "venue": "Bull. Aust. Math. Soc",
            "year": 2013
        },
        {
            "authors": [
                "S.S. Dragomir"
            ],
            "title": "Some reverses of the Jensen inequality for functions of selfadjoint operators in Hilbert spaces",
            "venue": "J. Inequalities Appl",
            "year": 2010
        },
        {
            "authors": [
                "S. Khan",
                "M.A. Khan",
                "Y.-M. Chu"
            ],
            "title": "New converses of Jensen inequality via Green functions with applications",
            "venue": "Rev. Real Acad. Cienc. Exactas Fis",
            "year": 2020
        },
        {
            "authors": [
                "S. Khan",
                "M.A. Khan",
                "Y.-M. Chu"
            ],
            "title": "Converses of Jensen inequality derived from the Green functions with applications in information",
            "venue": "theory. Math. Methods Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "G. Wunder",
                "B. Gro\u00df",
                "R. Fritschek",
                "R.F. Schaefer"
            ],
            "title": "A reverse Jensen inequality result with application to mutual information estimation",
            "venue": "In Proceedings of the 2021 IEEE Information Theory Workshop (ITW 2021), Kanazawa, Japan,",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Ali",
                "H. Budak",
                "Z. Zhang"
            ],
            "title": "A new extension of quantum Simpson\u2019s and quantum Newton\u2019s inequalities for quantum differentiable convex functions",
            "venue": "Math. Methods Appl. Sci",
            "year": 2022
        },
        {
            "authors": [
                "H. Budak",
                "M.A. Ali",
                "M. Tarhanaci"
            ],
            "title": "Some new quantum Hermite-Hadamard like inequalities for co-ordinated convex functions",
            "venue": "J. Optim. Appl",
            "year": 2020
        },
        {
            "authors": [
                "S. Boyd",
                "L. Vandenberghe"
            ],
            "title": "Convex Optimization",
            "year": 2004
        },
        {
            "authors": [
                "R.E. Krichevsky",
                "V.K. Trofimov"
            ],
            "title": "The performance of universal encoding",
            "venue": "IEEE Trans. Inform. Theory",
            "year": 1981
        },
        {
            "authors": [
                "N. Merhav",
                "A. Cohen"
            ],
            "title": "Universal randomized guessing with application to asynchronous decentralized brute-force attacks",
            "venue": "IEEE Trans. Inform. Theory",
            "year": 2020
        },
        {
            "authors": [
                "A. Dong",
                "H. Zhang",
                "D. Wu",
                "D. Yuan"
            ],
            "title": "Logarithmic expectation of the sum of exponential random variables for wireless communication performance evaluation",
            "venue": "In Proceedings of the 2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall),",
            "year": 2015
        },
        {
            "authors": [
                "D. Tse",
                "P. Viswanath"
            ],
            "title": "Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Citation: Merhav, N. Some Families\nof Jensen-like Inequalities with\nApplication to Information Theory.\nEntropy 2023, 25, 752. https://\ndoi.org/10.3390/e25050752\nAcademic Editor: Ra\u00fal Alcaraz\nReceived: 4 April 2023\nRevised: 23 April 2023\nAccepted: 3 May 2023\nPublished: 4 May 2023\nCopyright: \u00a9 2023 by the author.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: Jensen\u2019s inequality; convex function; concave function; entropy; capacity; moment-generating function; cumulant-generating function\nIn memory of Jacob Ziv, a shining star in the sky of information theory,\nwhose legacy as a researcher will continue to inspire me and many others for years to come."
        },
        {
            "heading": "1. Introduction",
            "text": "As is well known, the Jensen inequality is one of the most fundamental and useful mathematical tools in a variety of fields, including information theory. Interestingly, it includes many other very well-known inequalities, which are important on their own, as special cases. Among many examples, we mention the Shwartz\u2013Cauchy inequality (which in turn supports uncertainty principles and the Cram\u00e9r\u2013Rao bound), the Lyapunov inequality, the H\u00f6lder inequality, and the inequalities among the harmonic, geometric and arithmetic means. In the field of information theory, the Jensen inequality stands at the basis of the information inequality (i.e., the non-negativity of the relative entropy), the data processing inequality (which in turn leads to the Fano inequality), and the inequality between conditional and unconditional entropies. Moreover, it plays a central role in support of the derivation of single-letter formulas in Shannon theory and in the theory of maximum entropy under moment constraints (see, for example, Chapter 12 of [1]). During the last two decades, there have been many research efforts around Jensen\u2019s inequality, which included refinements [2\u20135], variations [6\u20138], improvements [9\u201311], and extensions [12], just to name a few. There have also many derivations of reversed versions of the Jensen inequality. For a non-exhaustive list of works, see, e.g., ref. [13] for mixtures of exponential families, refs. [14\u201317] for global bounds on the difference between the two sides of Jensen\u2019s inequality, ref. [18] for functions of self-adjoint operators in Hibert spaces, refs. [19,20] for inequalities via Green functions, refs. [21,22] for inequalities via Chebychev\nEntropy 2023, 25, 752. https://doi.org/10.3390/e25050752 https://www.mdpi.com/journal/entropy\nand Chernoff bounds, ref. [23] for quantum Simpson\u2019s and quantum Newton\u2019s inequalities, and ref. [24] for new quantum Hermite\u2013Hadamard-like inequalities. In most of them, the derived inequalities are exemplified in many applications, for instance, useful relationships between arithmetic and geometric means, converse bounds on the entropy, the relative entropy, as well as the more general f -divergence, converse forms of the H\u00f6lder inequality, and so on. In many of these works, the main results are given in the form of an upper bound on the difference, E{ f (X)} \u2212 f (E{X}), where f is a convex function, E{\u00b7} is the expectation operator, and X is the random variable. However, those bounds depend mostly on global parameters associated with f , for example, its range and domain, but not particularly on the underlying probability function (probability density function in the continuous case, or probability mass function in the discrete case), of X. For one thing, a desirable property of a reverse Jensen inequality would be that it is tight when X is well concentrated in the vicinity of its mean, just like the same well-known property of the ordinary Jensen inequality. In [22], there is an attempt to address this issue. This paper revisits the Jensen inequality from a completely different angle. It is not meant to be another improvement of earlier bounds in an existing line of work. It is meant to propose a different approach for generating useful inequalities in the spirit of Jensen\u2019s inequality. It is based on the following simple observation, which is rooted in the proof of Jensen\u2019s inequality: The given convex function, f (x), is lower bounded by the tangential affine function, `(x) = f (a) + f \u2032(a)(x\u2212 a), where a is an arbitrary number in the domain of x and f \u2032(a) is the derivative of f at x = a (provided that f is differentiable at x = a). By selecting a = E{X} and taking expectations of both sides of the inequality, f (X) \u2265 `(X), the Jensen inequality is readily proved. The point to be remembered is that here, a\u2217 = E{X} is the optimal choice of a in the sense of maximizing E{`(X)} over all possible values of a, thus yielding the tightest lower bound within this class of lower bounds on E{ f (X)}. The optimal choice of a, however, might be different than E{X}when the function f (X) is only a part of a more complicated expression whose expectation is to be lower bounded. For example, one might be interested in lower bounding E{g[ f (X)]}, where g is a monotonically non-decreasing function, or E{ f (X)g(X)}, where g is a nonnegative and/or convex function, or a combination of both, etc. To demonstrate this fact, consider the example (to be treated in detail in Section 2) of lower bounding E{ f (X)g(X)}, where g is a non-negative function. In this case,\nE{ f (X)g(X)} \u2265 E{[ f (a) + f \u2032(a)(X\u2212 a)]g(X)}, (1)\nand by maximizing the right-hand side (r.h.s.) over a, we easily obtain that the optimal choice of a here is a\u2217 = E{Xg(X)}/E{g(X)}, yielding the inequality,\nE{ f (X)g(X)} \u2265 f ( E{Xg(X)} E{g(X)} ) \u00b7E{g(X)}, (2)\nwhich is useful as long as g is such that we can easily calculate both E{g(X)} and E{Xg(X)}. While this particular inequality could have been obtained also by applying the (ordinary) Jensen inequality, E{ f (X)} \u2265 f (E{X}), with respect to (with respect to) the density, p\u0303(x) = p(x)g(x)/ \u222b \u221e \u2212\u221e p(x\n\u2032)g(x\u2032)dx\u2032, we will see in the sequel also various examples of inequalities with no apparent simple interpretations such as this. We henceforth refer to these classes of inequalities as Jensen-like inequalities, since they are derived using the same general idea that underlies the proof the classical Jensen inequality. We will also demonstrate the usefulness of these inequalities in information theory.\nOur contributions, in this work, have the following features:\n1. In many cases (such as the one above), the optimal value of the parameter(s) (e.g., the parameter a in the above discussion) can be found in closed form. In other cases, the resulting expressions may not lend themselves to closed-form optimization, and then we have two possibilities: (i) carry out the optimization numerically, and (ii)\nselect an arbitrary choice of a and obtain a valid lower bound, bearing in mind that an educated guess can potentially result in a good bound.\n2. Our inequalities provide two types of bounds: (i) bounds that require the calculation of the first two moments (or equivalently, the first two cumulants) of X, and (ii) bounds that require the calculation of the moment-generating function (MGF) of X and its derivative, or equivalently, the cumulant-generating function (CGF) of X and its derivative. All these types of moments are often easily calculable in closed form, especially in situations where X is given by the sum of independent and identically distributed (i.i.d.) random variables, which is frequently encountered in information\u2013 theoretic applications. 3. Most of our derivations extend to convex functions of more than one variable. 4. The classes of Jensen-like inequalities that we consider allow enough flexibility to\nobtain derivations of lower bounds on functions that are not necessarily convex, and even for some concave functions, and thereby open the door for another route to reverse Jensen inequalities. This can be accomplished by representing the given function in one of the categories discussed (e.g., a product of a convex function and a non-negaive function, a product of two non-negative convex functions, a composition of a monotone function and a convex function, etc.).\n5. We demonstrate the utility of the Jensen-like inequalities in several examples of information\u2013theoretic relevance. We also display numerical results that exemplify the degree of tightness of these bounds. 6. Our Jensen-like inequalities have the desirable property of becoming tighter as X becomes more and more concentrated around its mean, just like the ordinary Jensen inequality. 7. Throughout the paper, we confine ourselves to lower bounds on expectations of expressions that include a convex function f , but it should be understood that they all continue to apply also if f is concave and the inequalities are reversed. 8. It should be understood that the classes of Jensen-like inequalities that we derive in this work are just examples that demonstrate the basic underlying idea of optimizing the point of tangency to the given convex function for the specific expression at hand. It is conceivable that the same idea can be applied to many more situations of theoretical and practical interest.\nIn all forthcoming derivations, it will be assumed that the convex functions involved are weakly convex and differentiable. In other words, we will rely on the well-known fact that a differentiable convex function, f (x), is nowhere below the supporting line, `(x) = f (a) + f \u2032(a)(x \u2212 a), for every value of the parameter a in the domain of the independent variable, x [25] (p. 69, eq. (3.2)). In order to show that the point of zeroderivative of the lower bound (w.r.t. a) indeed yields a maximum (and not a minimum, etc.) of the lower bound, we will need to further assume that f is twice differentiable, but such an assumption will not limit the applicability of the claimed lower bound, because the lower bound applies to any value of a, including the point of zero-derivative, even if this point cannot be proved to yield the maximum of the lower bound using the standard methods. Similar comments apply when the lower bound will depend on more than one parameter. In the remaining part of this article, each section is devoted to a different class of Jensenlike inequalities, which corresponds to a different form of an expression that includes the convex function, f ."
        },
        {
            "heading": "2. A Product of a Convex Function and a Non-Negative Function",
            "text": "In this section, we focus on lower bounding expressions of the form E{ f (X)g(X)}, where f is convex and g is non-negative. Indeed, let f : R\u2192 R be a convex function and let g : R\u2192 R+ be a non-negative function. Then, for any a \u2208 R,\nE{ f (X)g(X)} \u2265 E{[ f (a) + f \u2032(a)(X\u2212 a)]g(X)} (3) = [ f (a)\u2212 a f \u2032(a)]E{g(X)}+ f \u2032(a)E{Xg(X)}. (4)\nTo find the value of a that maximizes the r.h.s., we equate the derivative to zero and obtain:\n[ f \u2032(a)\u2212 f \u2032(a)\u2212 a f \u2032\u2032(a)]E{g(X)}+ f \u2032\u2032(a)E{Xg(X)} = 0 (5)\nor equivalently, f \u2032\u2032(a)[E{Xg(X)} \u2212 aE{g(X)}] = 0, (6)\nwhose solution is readily obtained as\na = a\u2217 4 = E{Xg(X)} E{g(X)} , (7)\nand it is easy to verify that the second derivative at a = a\u2217 is \u2212 f \u2032\u2032(a\u2217)E{g(X)} < 0, which means that it is a maximum (at least a local one). The resulting lower bound on E{ f (X)g(X)} is then given by\nE{ f (X)g(X)} \u2265 f ( E{Xg(X)} E{g(X)} ) \u00b7E{g(X)}. (8)\nThis result extends straightforwardly to the case where X is a vector provided that f is jointly convex and differentiable in all components of X. In particular, it extends to the case where f and g act as different random variables, X and Y, with a joint distribution:\nE{ f (X)g(Y)} \u2265 f ( E{Xg(Y)} E{g(Y)} ) \u00b7E{g(Y)}. (9)\nWe next consider several examples.\nExample 1. Let f (x) = \u2212 ln x and g(x) = x, x > 0. Applying Inequality (8),\nE{\u2212X ln X} \u2265 \u2212E{X} \u00b7 ln E{X 2} E{X} = \u2212E{X} \u00b7 ln(E{X})\u2212E{X} \u00b7 ln ( 1 + Var{X} [E{X}]2 ) . (10)\nNote that the function \u2212x ln x is concave, rather than convex, yet we have here a lower bound (rather than an upper bound) to its expectation, namely, a reversed Jensen inequality. The first term on the right-most side is the (ordinary) Jensen upper bound on E{\u2212X ln X}, and the second term is the gap, which depends not only on the expectation of X but also on its variance, which manifests the fluctuations around E{X}. Clearly, if Var{X} = 0, the second term vanishes, which makes sense, because when X is a degenerated random variable, Jensen\u2019s inequality is achieved with equality and there is no gap. This inequality has an immediate application for obtaining a lower bound to the expectation of the empirical entropy of a sequence drawn by a memoryless source, which is relevant in the context of universal source coding [26]. Each term of the empirical entropy is of the form \u2212X ln X, where X = N(u)/N, N(u) is the number of occurrences of a letter u in a randomly drawn N-tuple from a memoryless source, P, with a finite alphabet, U . Clearly, each N(u) is a binomial random variable with N trials and probability of success, P(u). In this case, E{X} = P(u) and Var{X} = P(u)[1\u2212 P(u)]/N. Thus, denoting the entropy and the empirical entropy, respectively, by\nH = \u2212 \u2211 u\u2208U P(u) ln P(u) (11)\nH\u0302 = \u2212 \u2211 u\u2208U\nN(u) N\nln (\nN(u) N\n) , (12)\nwith the convention that 0 ln 0 4 = 0, we have:\nE{H\u0302} \u2265 \u2212 \u2211 u\u2208U P(u) ln P(u)\u2212 \u2211 u\u2208U\nP(u) ln [ 1 + P(u)[1\u2212 P(u)]/N\nP2(u) ] = H \u2212 \u2211\nu\u2208U P(u) ln\n( 1 +\n1\u2212 P(u) NP(u) ) \u2265 H \u2212 \u2211\nu\u2208U P(u) \u00b7 1\u2212 P(u) NP(u)\n= H \u2212 1 N \u2211u\u2208U [1\u2212 P(u)] = H \u2212 |U| \u2212 1 N , (13)\nwhere |U | is the cardinality of U . The use of the ordinary Jensen inequality yields an upper bound rather than a lower bound, E{H\u0302} \u2264 H. We conclude that the expected empirical entropy, E{H\u0302}, is sandwiched between H and H \u2212 (|U | \u2212 1)/N, which is reasonable because the variance of the empirical probabilities, N(u)/N, decays at the rate of 1/N.\nExample 2. Let s and t be two real numbers whose difference, s\u2212 t, is either negative or larger than unity. Now, let g(x) = xt, and f (x) = xs\u2212t. Then,\nE{Xs} = E{XtXs\u2212t} \u2265 ( E{Xt+1} E{Xt} )s\u2212t \u00b7E{Xt}\n= (E{Xt+1})s\u2212t (E{Xt})s\u2212t\u22121 . (14)\nIn particular, for t = 1 and s /\u2208 (1, 2), this becomes\nE{Xs} \u2265 (E{X 2})s\u22121 (E{X})s\u22122 = [E{X}] s \u00b7 ( 1 + Var{X} [E{X}]2 )s\u22121 (15)\nwhich is, once again, a bound that depends only on the first two moments of X. For s \u2208 (0, 1), the function xs is concave, and so, this is a reversed version of the Jensen inequality. For s \u2264 0 and s \u2265 2, the function xs is convex, and so, this is an improved version of the Jensen inequality: While the first factor, [E{X}]s, corresponds to the ordinary Jensen inequality, the second factor expresses the improvement, which depends on the relative fluctuation term, Var{X}/[E{X}]2. The degree of improvement depends, of course, on the variance of X. If the variance vanishes, there is nothing to improve because the ordinary Jensen inequality becomes an equality. On the other hand, the larger the variance, the larger the gap between the ordinary Jensen bound, [E{X}]s, and the improved one. Accordingly, this also demonstrates the role of the optimization of the parameter a as opposed to the default choice of a = E{X} of the ordinary Jensen inequality. To particularize this example even further, consider the problem of randomized guessing under a distribution Q (see, e.g., [27] and many references therein). Then, the probability of a single success in guessing a discrete alphabet random variable, X, given that we know that X = x (but not the guesser), is Q(x). In sequential guessing until the first success, the number of guesses, G, is a geometric RV with parameter p = Q(x), whose mean and variance are 1/p and (1\u2212 p)/p2, respectively. For s \u2208 (1, 2),\nE{Gs} \u2265 (\n1 p\n)s \u00b7 ( 1 + (1\u2212 p)/p2\n1/p2\n)s\u22121 =\n(2\u2212 p)s\u22121 ps = [2\u2212Q(x)]s\u22121 [Q(x)]s . (16)\nExample 3. Let f be an arbitrary convex function and let g(x) = esx, where s is a given real number. Then, Inequality (8) becomes:\nE{ f (X)esX} \u2265 f (\u03c8\u2032(s)) \u00b7 e\u03c8(s) (17)\nwhere \u03c8(s) = lnE{esX} (18)\nis the CGF of X and \u03c8\u2032(s) is its derivative. This gives a lower bound in terms of the CGF of X and its derivative. The ordinary Jensen inequality is obtained as the special case of s = 0, where \u03c8(0) = 0 and \u03c8\u2032(0) = E{X}."
        },
        {
            "heading": "3. A Composition of a Monotone Function and a Convex Function",
            "text": "Another family of Jensen-like inequalities corresponds to the need to lower bound an expression of the form E{g[ f (X)]}, where f is convex as before and g is a monotonically non-decreasing function. The general idea is to carry out the optimization of the r.h.s. of the following inequality.\nE{g[ f (X)]} \u2265 sup a\nE{g[ f (a) + f \u2032(a)(X\u2212 a)]}. (19)\nIn the important special case where g(x) = ex, we have:\nE{e f (X)} \u2265 sup a\nE{e f (a)+ f \u2032(a)(X\u2212a)}\n= sup a\ne f (a)\u2212a f \u2032(a)E{eX f \u2032(a)}\n= exp {\nsup a { f (a)\u2212 a f \u2032(a) + \u03c8[ f \u2032(a)]}\n} , (20)\nwhere \u03c8(\u00b7) is again the CGF of X. The optimal value, a\u2217, of a, is the solution to the equation obtained by equating the derivative of the exponent to zero, i.e.,\n\u03c8\u2032[ f \u2032(a\u2217)] = a\u2217, provided that f \u2032\u2032(a\u2217)\u03c8\u2032\u2032[ f \u2032(a\u2217)] < 1, (21)\nwhere \u03c8\u2032(\u00b7) and \u03c8\u2032\u2032(\u00b7) are the first and the second derivatives of \u03c8(\u00b7), respectively.\nExample 4. Consider the case where f (x) = sx2/2 and X \u223c N (\u00b5, \u03c32), where \u03c32 < 1/s, as otherwise, E{esX2} = \u221e. In this case, the condition f \u2032\u2032(a\u2217)\u03c8\u2032\u2032[ f \u2032(a\u2217)] < 1 is equivalent to \u03c32 < 1/s, and we have f \u2032(a) = sa, \u03c8(t) = \u00b5t + \u03c32t2/2, and so, \u03c8\u2032(t) = \u00b5 + \u03c32t, which means that \u03c8\u2032[ f \u2032(a)] = \u00b5 + \u03c32sa. The equation for the optimal a becomes then\n\u00b5 + \u03c32sa = a, (22)\nwhose solution is a = a\u2217 4 =\n\u00b5\n1\u2212 \u03c32s , (23)\nwhich yields\nE { esX 2/2 } \u2265 exp { sa2\u2217/2\u2212 sa2\u2217 + \u00b5sa\u2217 + \u03c32s2a2\u2217/2 } = exp { \u00b52s\n2(1\u2212 \u03c32s)\n} . (24)\nThe ordinary Jensen inequality yields\nE { esX 2/2 } \u2265 exp { sE{X2}/2 } = es(\u00b5 2+\u03c32)/2, (25)\nwhich does not capture the singularity at s = 1/\u03c32. The exact calculation yields\nE { esX 2/2 } =\n1\u221a 1\u2212 \u03c32s\n\u00b7 exp {\n\u00b52s 2(1\u2212 \u03c32s)\n} , (26)\nnamely, the Jensen-like bound (24) gives the correct exponential term (along with the singularity at s = 1/\u03c32) and differs from the exact quantity only in the pre-exponential factor. Once again, this demonstrates the fact that optimizing the point of tangency, a, rather than using the default value, a = E{X}, can make a significant difference."
        },
        {
            "heading": "4. A Product of a Convex Function and a Monotone-Convex Composition",
            "text": "Yet another class of Jensen-like inequalities corresponds to lower bounding the expectation of the product of two functions, where one is convex and the other is a composition of a non-negative monotonically non-decreasing function and a convex function, i.e.,\nE{h[ f (X)]g(X)} \u2265 sup a,b\nE{h[ f (a) + f \u2032(a)(X\u2212 a)] \u00b7 [g(b) + g\u2032(b)(X\u2212 b)]}, (27)\nwhere f and g are convex and h is monotonically non-decreasing and non-negative. For the case where h(x) = ex, we end up with a bound that depends on the CGF of X and its derivative:\nE{e f (X)g(X)} \u2265 E { e f (a)+ f \u2032(a)(X\u2212a)[g(b) + g\u2032(b)(X\u2212 b)] } (28)\n= e f (a)\u2212a f \u2032(a)E { eX f \u2032(a)[g(b)\u2212 bg\u2032(b) + g\u2032(b)X] }\n(29)\n= exp{ f (a)\u2212 a f \u2032(a) + \u03c8[ f \u2032(a)]}{g(b) + g\u2032(b)(\u03c8\u2032[ f \u2032(a)]\u2212 b)}. (30)\nMaximizing with respect to b while a is kept fixed yields b\u2217 = \u03c8\u2032[ f \u2032(a)], and we obtain:\nE{e f (X)g(X)} \u2265 sup a exp{ f (a)\u2212 a f \u2032(a) + \u03c8[ f \u2032(a)]} \u00b7 g(\u03c8\u2032[ f \u2032(a)]). (31)\nExample 5. Considering the case where f (x) = \u2212 ln x and g(x) = x ln x, we may obtain a reversed Jensen-like inequality, namely, a lower bound to the expectation of the concave function ln X:\nE{ln X} = E { e\u2212 ln X \u00b7 X ln X }\n(32)\n\u2265 sup a\u22650\nexp{\u2212 ln a + 1 + \u03c8(\u22121/a)} \u00b7 \u03c8\u2032(\u22121/a) ln \u03c8\u2032(\u22121/a) (33)\n= sup \u03b1\u22650\nexp{ln \u03b1 + 1 + \u03c8(\u2212\u03b1)}\u03c8\u2032(\u2212\u03b1) ln \u03c8\u2032(\u2212\u03b1) (34)\n= e \u00b7 sup \u03b1\u22650\n\u03b1e\u03c8(\u2212\u03b1)\u03c8\u2032(\u2212\u03b1) ln \u03c8\u2032(\u2212\u03b1) (35)\n= e \u00b7 sup \u03b1\u22650\n\u03b1E{Xe\u2212\u03b1X} ln E{Xe \u2212\u03b1X}\nE{e\u2212\u03b1X} . (36)\nDefining the MGF \u03c6(s) = E{esX} = e\u03c8(s), we have:\nE{ln X} \u2265 e \u00b7 sup \u03b1\u22650\n\u03b1\u03c6\u2032(\u2212\u03b1) ln \u03c8\u2032(\u2212\u03b1) (37)\n= e \u00b7 sup \u03b1\u22650\n\u03b1\u03c6(\u2212\u03b1)\u03c8\u2032(\u2212\u03b1) ln \u03c8\u2032(\u2212\u03b1) (38)\n= e \u00b7 sup \u03b1\u22650\n\u03b1\u03c6\u2032(\u2212\u03b1) ln \u03c6 \u2032(\u2212\u03b1)\n\u03c6(\u2212\u03b1) . (39)\nWe obtained a lower bound in terms of the MGF and its derivative (or, equivalently, the CGF and its derivative), which is appealing in cases where X is the sum of i.i.d. random variables.\nAccordingly, we now particularize this example further by examining the case where X = 1 + \u2211ki=1 Y 2 i , with Yi \u223c N (0, \u03c32), i = 1, . . . , k, being independent random variables.\nThe motivation of assessing an expression of the form, E { ln (\n1 + \u2211ki=1 Y 2 i\n)} , is two-fold.\nThe first is that it is useful for bounding the ergodic capacity of the single-input, multipleoutput (SIMO) channel, where {Yi} designates random channel transfer coefficients (see, e.g., [22,28,29] and references therein). The second is that it is relevant for bounding the joint differential entropy associated with the multivariate Cauchy density. Here, (Y1, . . . , Yk) are not Gaussian as defined above, but their multivariate Cauchy density can be represented as a continuous mixture of i.i.d. zero-mean Gaussian random variables, where the mixture is taken over all possible variances\u2014see [22] (Example 6) for the details. In this case,\n\u03c6(s) = E { exp ( s [ 1 + k\n\u2211 i=1 Y2i\n])} (40)\n= es ( E{esY2} )k (41)\n= es\n(1\u2212 2s\u03c32)k/2 , s < 1 2\u03c32 . (42)\nThus,\n\u03c8(s) = s\u2212 k 2 ln(1\u2212 2s\u03c32), (43)\nand\n\u03c8\u2032(s) = 1 + k\u03c32\n1\u2212 2s\u03c32 . (44)\nIt follows that E { ln ( 1 + k\n\u2211 i=1 Y2i\n)} \u2265 e \u00b7 sup\n\u03b1\u22650\n{ \u03b1e\u2212\u03b1\n(1 + 2\u03b1\u03c32)k/2\n( 1 + k\u03c32\n1 + 2\u03b1\u03c32\n) ln ( 1 + k\u03c32\n1 + 2\u03b1\u03c32\n)} . (45)\nThe Jensen upper bound, ln(1 + k\u03c32), and the lower bound (45) are displayed in Figure 1 for \u03c32 = 1 and k = 1, 2, . . . , 100. As can be seen, the bounds are quite close. Interestingly, the choice \u03b1 = 1/(k\u03c32) yields results that are very close to those of the optimal \u03b1.\nAnother instance of this example is the circularly symmetric complex Gaussian channel whose signal-to-noise ratio (SNR), Z, is a random variable (e.g., due to fading), which is known to both the transmitter and the receiver. The capacity is given by C = E{ln(1+ gZ)}, where g is a certain deterministic gain factor and the expectation is with respect to the randomness of Z. For simplicity, let us assume that Z is distributed exponentially, i.e.,\np(z) = {\n\u03b8e\u2212\u03b8z z \u2265 0 0 z < 0\n(46)\nwhere the parameter \u03b8 > 0 is given. In this case,\n\u03c6(\u2212\u03b1) = \u03b8e \u2212\u03b1\n\u03b8 + g\u03b1 (47)\nand \u03c8(\u2212\u03b1) = ln \u03b8 \u2212 ln(\u03b8 + g\u03b1)\u2212 \u03b1, (48)\nand so,\nE{ln(1 + gZ)} \u2265 e\u03b8 \u00b7 sup \u03b1\u22650\n\u03b1e\u2212\u03b1 \u03b8 + g\u03b1 \u00b7 ( 1 + g g + \u03b8\u03b1 ) ln ( 1 + g g + \u03b8\u03b1 ) . (49)\nIn Figure 2, we plot this lower bound as a function of \u03b8 for g = 5 and compare it to the Jensen upper bound, ln(1 + g/\u03b8) (red curve) and to the lower bound of [22] (Sect. 4.1, Example 1). As can be seen, the lower bound proposed here is considerably tighter, especially for small \u03b8.\nExample 6. Yet another example of this family of Jensen-like inequalities applies to obtaining a lower bound to E{Xt}, where t is an arbitrary real. For a given t, let s \u2265 0 be either larger than 1\u2212 t or smaller than \u2212t, and consider the case where f (x) = xt+s, g(x) = \u2212s ln x and h(x) = ex. Then,\nE{Xt} = E{e\u2212s ln XXt+s} (50) \u2265 E { exp [ s ( \u2212 ln a\u2212 1\na (X\u2212 a)\n)] \u00b7 [ bt+s + (t + s)bt+s\u22121(X\u2212 b) ]} (51)\n= es[1\u2212ln a]\u03c6 ( \u2212 s\na\n)[ bt+s + (t + s)bt+s\u22121 ( \u03c8\u2032 ( \u2212 s\na\n) \u2212 b )] . (52)\nChoosing b = \u03c8\u2032(\u2212s/a), and changing the optimization variable a into \u03b1 = 1/a, we obtain\nE{Xt} \u2265 sup \u03b1\u22650 (\u03b1e)s\u03c6(\u2212\u03b1s)[\u03c8\u2032(\u2212\u03b1s)]t+s. (53)\nMore specifically, if X = \u2211ni=1 Yi, where {Yi} are Bernoulli i.i.d., with parameter p, then \u03c6(s) = (pes + q)n, where q = 1\u2212 p. We then obtain\nE{Xt} \u2265 sup \u03b1\u22650\n(\u03b1e)s(pe\u2212\u03b1s + q)n \u00b7 ( npe\u2212\u03b1s\npe\u2212\u03b1s + q\n)t+s . (54)\nSelecting \u03b1 = 1/(np), we obtain\nE{Xt} \u2265 (np)t \u00b7 e s(pe\u2212s/(np) + q)ne\u2212s(t+s)/(np)\n(pe\u2212s/(np) + q)t+s . (55)\nThe first factor is (EX)t. The second factor tends to unity as n grows, because pe\u2212s/np + q \u2248 p(1\u2212 s/(np)) + q = 1\u2212 s/n, and so, (pe\u2212s/np + q)n \u2248 (1\u2212 s/n)n \u2248 e\u2212s. For t \u2265 1 and t \u2264 0, the function f (x) = xt is convex, and so, (EX)t is the ordinary Jensen lower bound. In this case, the bound is valuable if the multiplicative factor,\nes(pe\u2212s/(np) + q)ne\u2212s(t+s)/(np)\n(pe\u2212s/(np) + q)t+s ,\nis larger than unity. If 0 < t < 1, the function f (x) = xt is concave, and then (EX)t is an upper bound. Of course, the parameter s can be optimized, too. Some numerical results for t = 0.5 are depicted in Figure 3. As can be seen, the upper and the lower bounds are fairly close.\nAnother application of this example is related to estimation theory. Let \u03b8 \u2208 R and let Y1, . . . , Yn be i.i.d., with mean \u03b8 and variance \u03c32. Consider the t-th moment of the estimation\nerror, E\u03b8 \u2223\u2223\u2223\u2223 1n \u2211ni=1 Yi \u2212 \u03b8|t. Defining X = ( 1n \u2211ni=1 Yi \u2212 \u03b8)2, we have\n\u03c6(s) = 1\u221a 1\u2212 2s\u03c32/n ; \u03c8(s) = \u22121 2 ln ( 1\u2212 2s\u03c3 2 n ) . (56)\nand so,\n\u03c6(\u2212\u03b1s) = 1\u221a 1 + 2\u03b1s\u03c32/n ; \u03c8\u2032(\u2212\u03b1s) = \u03c3 2/n 1 + 2\u03b1s\u03c32/n . (57)\nE\u03b8 \u2223\u2223\u2223\u2223 1n n\u2211i=1 Yi \u2212 \u03b8 \u2223\u2223\u2223\u2223t = E\u03b8Xt/2 (58) \u2265 (\u03b1e) s \u221a\n1 + 2\u03b1s\u03c32/n\n( \u03c32/n\n1 + 2\u03b1s\u03c32/n )t/2+s = ( \u03c32\nn\n)t/2+s \u00b7 (\u03b1e) s\n(1 + 2\u03b1s\u03c32/n)(t+1)/2+s . (59)\nwith either s \u2265 1\u2212 t/2 or s \u2264 \u2212t/2. For \u03b1 = \u03b6n/\u03c32 (\u03b6 > 0 being a constant), we have:\nE\u03b8 \u2223\u2223\u2223\u2223 1n n\u2211i=1 Yi \u2212 \u03b8 \u2223\u2223\u2223\u2223t \u2265 \u03c3tnt/2 \u00b7 sup\u03b6>0, s>1\u2212t/2 (\u03b6e) s (1 + 2\u03b6s)(t+1)/2+s (60)\nwhere for t \u2208 [0, 2], the first factor, \u03c3t/nt/2, is the Jensen upper bound. The second factor,\n\u00b5t = sup \u03b6>0, s>1\u2212t/2\n(\u03b6e)s\n(1 + 2\u03b6s)(t+1)/2+s , (61)\nis the gap between the Jensen upper bound and the proposed lower bound. In Figure 4, we display this factor. The result \u00b52 = 1 is expected, because for t = 2 and s = 0, the calculation is trivially exact. Note that the maximization over \u03b6, for a given s, can be carried out in closed form by equating to zero the partial derivative of ln[(\u03b6e)s/(1 + 2\u03b6s)(t+1)/2+s] with respect to \u03b6. The optimal \u03b6 turns out to be equal to 1/(t + 1) (independently of s), and so,\n\u00b5t = sup s>1\u2212t/2\n( t + 1\nt + 2s + 1\n)(t+1)/2 \u00b7 (\ne t + 2s + 1\n)s . (62)\nFinally, it should be pointed out that this family of Jensen-like bounds opens the door also to lower-bound calculations on the form E{ f (X)/g(X)}, where f is non-negative convex and g is non-negative and concave. Using the fact the identity 1/s = \u222b \u221e 0 e \u2212stdt, we have:\nE {\nf (X) g(X)\n} = E { f (X) \u00b7 \u222b \u221e 0 e\u2212tg(X)dt }\n(63)\n= \u222b \u221e 0 E { e\u2212tg(X) f (X) } dt (64)\nand we can apply the same ideas as before to the integrand, having the freedom to optimize the bound parameters with possible dependence on t."
        },
        {
            "heading": "5. A Product of Two Non-Negative Convex Functions",
            "text": "The last family of Jensen-like bounds that we present in this work is associated with the product of two non-negative convex functions. Let both f and g be non-negative convex functions of x \u2265 0. Then,\nE{ f (X)g(X)} \u2265 E{[ f (a) + f \u2032(a)(X\u2212 a)] \u00b7 g(X)} (65) = [ f (a)\u2212 a f \u2032(a)]E{g(X)}+ f \u2032(a)E{Xg(X))} (66) \u2265 [ f (a)\u2212 a f \u2032(a)]E{[g(b) + g\u2032(b)(X\u2212 b)]}+\nf \u2032(a)E{X[g(c) + g\u2032(c)(X\u2212 c)]} f (a) \u2265 a f \u2032(a) \u2265 0 (67) = [ f (a)\u2212 a f \u2032(a)] \u00b7 [g(b)\u2212 bg\u2032(b) + g\u2032(b)E{X}] +\nf \u2032(a)[(g(c)\u2212 cg\u2032(c))E{X}+ g\u2032(c)E{X2}}]. (68)\nThe optimal b and c are b\u2217 = E{X} and c\u2217 = E{X2}/E{X}, respectively. Thus,\nE{ f (X)g(X)} \u2265 [ f (a)\u2212 a f \u2032(a)] \u00b7 g(E{X}) + f \u2032(a)E{X} \u00b7 g ( E{X2} E{X} ) . (69)\nLet\na\u2217 = E{X} \u00b7 g(E{X2}/E{X})\ng(E{X}) (70)\nand assume that f (a\u2217) \u2265 a\u2217 f \u2032(a\u2217) \u2265 0. Then, a\u2217 is the optimal value of a, which yields\nE{ f (X)g(X)} \u2265 f ( E{X} \u00b7 g(E{X2}/E{X})\ng(E{X})\n) \u00b7 g(E{X}). (71)\nMore generally, when X and Y are two random variables with a joint distribution, the above derivation easily extends to\nE{ f (X)g(Y)} \u2265 f ( E{X} \u00b7 g(E{XY}/E{X})\ng(E{Y})\n) \u00b7 g(E{Y}). (72)\nIf f and g are both concave, rather than convex, then the inequalities are reversed.\nExample 7. Consider again the example of the capacity of the AWGN with a random SNR, c(Z) = ln(1 + gZ), and suppose that we wish to bound the variance of c(Z) in order to assess the fluctuations (e.g., for the purpose of bounding the outage probability). Then, obviously,\nVar{c(Z)} = E{c2(Z)} \u2212 [E{c(Z)}]2 = E{ln2(1 + gZ)} \u2212 [E{ln(1 + gZ)}]2. (73)\nTo upper bound Var{c(Z)}, we may derive an upper bound to E{ln2(1 + gZ)} and a lower bound to E{ln(1 + gZ)}. For the latter, a lower bound was already proposed earlier in Example 5. For the former, we may use the present inequality with the choice f (z) = g(z) = ln(1 + gz), which can easily be shown to satisfy the requirements. We then obtain the following upper bound, which depends merely on the first two moments of Z:\nE{ln2(1 + gZ)} \u2264 ln(1 + gE{Z}) \u00b7 ln ( 1 + gE{Z} ln(1 + gE{Z2}/E{Z})\nln(1 + gE{Z})\n) . (74)\nInterestingly, the function ln2(1 + gx) is neither convex nor concave, yet our approach offers an upper bound, which is fairly easy to calculate provided that one can compute the first two moments of Z."
        },
        {
            "heading": "6. Conclusions",
            "text": "In this work, we have revisited the Jensen inequality on the basis of taking advantage of the freedom to optimize the choice of the supporting line that is tangential to the given convex function. This optimal choice might be different than the ordinary one when the convex function does not stand alone, but it is rather only part of a more complicated expression. This more complicated expression can sometimes be created in an artificial manner, such as in Examples 2, 5 and 6. The resulting bounds depend on either the first two moments of the independent variable, X, or on its MGF and its derivative. Both types of moments often lend themselves to relatively easy calculations. The proposed methodology can be used both for improving on the ordinary Jensen inequality (such as in Examples 2 and 4), and for generating lower bounds to expectations of non-convex or even concave (rather than convex) functions (such as in Examples 1, 2, 5 and 7). Several families of Jensen-like inequalities have been derived along with a demonstration of numerical examples with application to information theory. The tightness of the inequalities obtained was also demonstrated in those examples.\nFunding: This research received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Not applicable.\nConflicts of Interest: The author declares no conflict of interest."
        }
    ],
    "title": "Some Families of Jensen-like Inequalities with Application to Information Theory",
    "year": 2023
}