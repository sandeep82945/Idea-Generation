{
    "abstractText": "Recent work suggests that interpolating between the weights of two specialized language models can transfer knowledge between tasks in a way that multi-task learning cannot. However, very few have explored interpolation between more than two models, where each has a distinct knowledge base. In this paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new few-sample task transfer approach for open-domain dialogue. Our framework creates a set of diverse expert language models trained using a predefined set of source tasks. Next, we finetune each of the expert models on the target task, approaching the target task from several distinct knowledge bases. Finally, we linearly interpolate between the model weights using a gradientfree-optimization algorithm, to efficiently find a good interpolation weighting. We demonstrate the effectiveness of the method on FETAFriends (Albalak et al., 2022) outperforming the standard pretrain-finetune approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dean Ninalga"
        }
    ],
    "id": "SP:2763a5a8df9960fbd4a588142cee01b666d2b9a7",
    "references": [
        {
            "authors": [
                "Alon Albalak",
                "Yi-Lin Tuan",
                "Pegah Jandaghi",
                "Connor Pryor",
                "Luke Yoffe",
                "Deepak Ramachandran",
                "Lise Getoor",
                "Jay Pujara",
                "William Yang Wang."
            ],
            "title": "FETA: A benchmark for few-sample task transfer in open-domain dialogue",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Zhao",
                "Yanping Huang",
                "Andrew Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language",
            "year": 2022
        },
        {
            "authors": [
                "Shachar Don-Yehiya",
                "Elad Venezian",
                "Colin Raffel",
                "Noam Slonim",
                "Yoav Katz",
                "Leshem Choshen."
            ],
            "title": "Cold fusion: Collaborative descent for distributed multitask finetuning",
            "venue": "ArXiv, abs/2212.01378.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Eaton",
                "Marie desJardins"
            ],
            "title": "Selective transfer between learning tasks using task-based boosting",
            "year": 2011
        },
        {
            "authors": [
                "Jean-Christophe Gagnon-Audet",
                "Ricardo Pio Monti",
                "David J. Schwab."
            ],
            "title": "AWE: Adaptive weightspace ensembling for few-shot fine-tuning",
            "venue": "ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models.",
            "year": 2023
        },
        {
            "authors": [
                "Fuchang Gao",
                "Lixing Han."
            ],
            "title": "Implementing the nelder-mead simplex algorithm with adaptive parameters",
            "venue": "Computational Optimization and Applications, 51:259\u2013277.",
            "year": 2012
        },
        {
            "authors": [
                "Almog Gueta",
                "Elad Venezian",
                "Colin Raffel",
                "Noam Slonim",
                "Yoav Katz",
                "Leshem Choshen."
            ],
            "title": "Knowledge is a region in weight space for fine-tuned language models",
            "venue": "ArXiv, abs/2302.04863.",
            "year": 2023
        },
        {
            "authors": [
                "Chengcheng Han",
                "Liqing Cui",
                "Renyu Zhu",
                "J. Wang",
                "Nuo Chen",
                "Qiushi Sun",
                "Xiang Li",
                "Ming Gao."
            ],
            "title": "When gradient descent meets derivative-free optimization: A match made in black-box scenario",
            "venue": "ArXiv, abs/2305.10013.",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Samir Yitzhak Gadre",
                "Shuran Song",
                "Hannaneh Hajishirzi",
                "Simon Kornblith",
                "Ali Farhadi",
                "Ludwig Schmidt."
            ],
            "title": "Patching open-vocabulary models by interpolating weights",
            "venue": "ArXiv, abs/2208.05592.",
            "year": 2022
        },
        {
            "authors": [
                "Joel Jang",
                "Seungone Kim",
                "Seonghyeon Ye",
                "Doyoung Kim",
                "Lajanugen Logeswaran",
                "Moontae Lee",
                "Kyungjae Lee",
                "Minjoon Seo."
            ],
            "title": "Exploring the benefits of training expert language models over instruction tuning",
            "venue": "ArXiv, abs/2302.03202.",
            "year": 2023
        },
        {
            "authors": [
                "John A. Nelder",
                "Roger Mead."
            ],
            "title": "A simplex method for function minimization",
            "venue": "Comput. J., 7:308\u2013313.",
            "year": 1965
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre Ram\u2019e",
                "Kartik Ahuja",
                "Jianyu Zhang",
                "Matthieu Cord",
                "L\u00e9on Bottou",
                "David LopezPaz"
            ],
            "title": "Recycling diverse models for out-ofdistribution generalization. ArXiv, abs/2212.10445",
            "year": 2022
        },
        {
            "authors": [
                "Maohao Shen",
                "Soumya Shubhra Ghosh",
                "Prasanna Sattigeri",
                "Subhro Das",
                "Yuheng Bu",
                "Gregory W. Wornell."
            ],
            "title": "Reliable gradient-free and likelihood-free prompt tuning",
            "venue": "Findings.",
            "year": 2023
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Zhengfu He",
                "Hong Qian",
                "Yunhua Zhou",
                "Xuanjing Huang",
                "Xipeng Qiu."
            ],
            "title": "Bbtv2: Towards a gradient-free future with large language models",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Hong Qian",
                "Xuanjing Huang",
                "Xipeng Qiu."
            ],
            "title": "Black-box tuning for language-model-as-a-service",
            "venue": "International Conference on Machine Learning.",
            "year": 2022
        },
        {
            "authors": [
                "Qi Wang",
                "Shengsheng Wang",
                "Bilin Wang."
            ],
            "title": "Class-rebalanced wasserstein distance for multisource domain adaptation",
            "venue": "Applied Intelligence, 53:8024\u20138038.",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Mike Li",
                "Jong Wook Kim",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong",
                "Ludwig Schmidt."
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2021
        },
        {
            "authors": [
                "Fuchao Yu",
                "Xianchao Xiu",
                "Yunhui Li."
            ],
            "title": "A survey on deep transfer learning and beyond",
            "venue": "Mathematics.",
            "year": 2022
        },
        {
            "authors": [
                "Wen Zhang",
                "Lingfei Deng",
                "Lei Zhang",
                "Dongrui Wu."
            ],
            "title": "A survey on negative transfer",
            "venue": "IEEE/CAA Journal of Automatica Sinica, 10:305\u2013329.",
            "year": 2020
        },
        {
            "authors": [
                "Jianyun Nie",
                "Ji rong Wen."
            ],
            "title": "A survey of large language models",
            "venue": "ArXiv, abs/2303.18223.",
            "year": 2023
        },
        {
            "authors": [
                "Yukun Zuo",
                "Hantao Yao",
                "Changsheng Xu."
            ],
            "title": "Attention-based multi-source domain adaptation",
            "venue": "IEEE Transactions on Image Processing, 30:3793\u2013 3803.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The growth in the number of open-vocabulary language models trained with multi-task learning has skyrocketed (as surveyed by Zhao et al. (2023)). They have had an integral part in the recent global success of large-scale conversational AI in recent years. Open-vocabulary models typically demonstrate strong zero-shot performance and are trained on a never-ending list of source tasks. For example, Flan-T5 (Chung et al., 2022) is trained with 1800+ tasks and can be shown to have decent performance on unseen tasks. However, recent work finds finetuning pre-trained models for specialized knowledge of a single informative task can yield better zero-shot performance than some of the latest zeroshot networks trained on hundreds or thousands of tasks (Jang et al., 2023). Hence, avoiding nongeneralizable knowledge contained in performing a\nset of source tasks, or the avoidance negative transfer, remains an area of concern in transfer learning research (Zhang et al., 2020; Yu et al., 2022).\nIn the few-shot learning setting, recent work has shown that interpolating between the weights of a zero-shot and finetuned models can give superior performance to finetuning (Gagnon-Audet et al., 2023). Indeed, recent studies suggest that even interpolating between two identical open-vocabulary language models that are trained on the same task can achieve even better performance (Gueta et al., 2023). Here, we attempt to expand on these recent findings by using interpolation to distill knowledge from several models finetuned on a single task. However, to the best of our knowledge, there is no known application of weight interpolation between more than two task-expert models. Hence, in order to optimize the interpolation of multiple model parameters - while maintaining a manageable computational cost - we propose derivativefree optimization to directly model the target task metric.\nTo diversify the weights from several models finetuned on a single task we adopt the tuning strategy of Model Ratatouille (Ram\u2019e et al., 2022). Namely, given a set of n auxiliary source tasks and a single target task, we first train n new models for each of the Subsequently, we finetune each of the s models on the target task. However, unlike Model Ratatouille (Ram\u2019e et al., 2022) we do not perform linear probing in this work to reduce implementation complexity.\nIn this paper, our contributions are the following:\n\u2022 We describe Derivative Free Weight-space Ensembling (DFWE) our framework towards few-shot transfer for language understanding in open-domain dialogue.\n\u2022 We provide results on FETA-Friends (Albalak et al., 2022) showing the effectiveness of our framework compared to standard approaches.\nar X\niv :2\n30 7.\n03 50\n6v 2\n[ cs\n.C L\n] 2\n6 Ju\nl 2 02\n3\n\u2022 We highlight limitations and directions for future improvements."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Gradient-Free Optimization of Language Models",
            "text": "Recently, several gradient-free optimization approaches have been proposed to tune extremely large language models that are prohibitively expensive to finetune with traditional differentiable network optimization (Sun et al., 2022b,a; Han et al., 2023). In broad terms, gradient-free approaches only use the forward pass of a network, making these approaches much more memory efficient and very attractive for tuning larger models. In particular, Shen et al. (2023) demonstrates a gradient-free optimization approach capable of tuning a large language model only using the classification metric as feedback."
        },
        {
            "heading": "2.2 Weight Interpolation",
            "text": "ColD Fusion (Don-Yehiya et al., 2022) shows that a cycle of finetuning and weight averaging on a set of tasks can gradually outperform the multitask learning approach on the same tasks. However, ColD Fusion requires constant retraining on all the available tasks which can be very inefficient. WiSE-FT (Wortsman et al., 2021) is a much more efficient framework that first proposed interpolating between the weights of a strong zero-shot model and the weights of fine-tuned models, leveraging the benefits of each approach. Where Ilharco et al. (2022) recently verify the effectiveness of WiSE-FT with CLIP (Radford et al., 2021). Re-\ncently, AWE (Gagnon-Audet et al., 2023) demonstrate WiSE-FT interpolation in the k-shot learning setting, where the interpolation parameter is determined using the data seen in the k-shots. Here, we will attempt to generalize WiSE-FT interpolation to more than two models by using weights from several fine-tuned models tuned on a single task."
        },
        {
            "heading": "3 Preliminaries",
            "text": "In this setting we are given a set of source tasks S = {s1, s2, ..., sn} and a target task t. Each task \u03c4 \u2208 {t}\u222aS has a task specific metric m\u03c4 (f (k)(x\u03c4 ), y\u03c4 ), for network predictions f (k)(x\u03c4 ) after k-shots, input data x\u03c4 and task labels y\u03c4 . Where the metric is not necessarily differentiable (e.g. F1, AUC, and, Acc). Here our goal is to increase performance on a target task t by using the knowledge obtained by performing the tasks in S. Namely, when measured on a test set the value of the metric mt using a model trained using S must be greater than the value of mt using the outputs of a model not trained with S."
        },
        {
            "heading": "4 Methodology",
            "text": "Here, we outline our method in greater detail. We provide an illustration overview of the framework in Figure 1. Our framework uses the T5-Flan-base1 model as it had better baseline performance over the T5 (Raffel et al., 2020) model in our preliminary experiments. Much of our framework builds upon the TLiDB 2 package, which is used to produce the baseline frameworks provided in Albalak\n1https://huggingface.co/google/flan-t5-base 2https://github.com/alon-albalak/TLiDB/tree/master\net al. (2022). Here, we train using the same preprocessing, prompts, and instructions used for tuning the T5 model in Albalak et al. (2022), since here, we train our T5-Flan-base models using the T5 training pipeline in the TLiDB package."
        },
        {
            "heading": "4.1 Baseline Model",
            "text": "We will compare our transfer strategy to the standard transfer learning approach. That is, we finetune the pre-trained T5-Flan-base model on the target task."
        },
        {
            "heading": "4.2 Our Approach",
            "text": "Our training framework consists of three training stages. The first training stage gathers source-taskspecific knowledge not present in the target task. Where the second and third stages are designed to distill the knowledge gained from the first step for transfer to the target task."
        },
        {
            "heading": "4.2.1 Training Stage",
            "text": "Instead of training a new model on each task in S, we instead construct\nS\u2217 := {t, {t} \u222a S, s1, s2, .., sn}.\nWhere we train a single model for each of the n+2 training sets in S\u2217. To be consistent with WiSE-FT (Wortsman et al., 2021), we include training on the set {t}\u222aS, which trains on all source tasks and the target task simultaneously. All models are trained until convergence on the metric for task t."
        },
        {
            "heading": "4.3 Finetuning Stage",
            "text": "Next, we take the resulting |S\u2217| = n + 2 models and finetune them on the target task t."
        },
        {
            "heading": "4.4 Interpolation Stage",
            "text": "Subsequently, we use the resulting parameters from each model \u0398 := {\u03b81, ..., \u03b8|S\u2217|} to construct the final model:\n\u03b8(\u0398, A) := |S\u2217|\u2211 i=1 \u03b1i\u03b8i (1)\nwhere for A := {\u03b1i|\u03b1i \u2265 0, \u2211\ni \u03b1i = 1}. Here we use the Nelder-Mead algorithm (Nelder and Mead, 1965; Gao and Han, 2012) a gradient-free optimizer, for optimizing A on the target task metric of the resulting model on the development data.\nFormally, we optimize:\nA\u2217 := argmax A mt(f (k)(xdevt |\u03b8(\u0398, A)), yt) (2)\nIn practice, we run the optimizer for 40 iterations to find an estimate for A\u2217 which we then use to parameterize our final model (see Equation 1) used for inference."
        },
        {
            "heading": "5 Experimental Setup",
            "text": "We study the transfer using our approach on the FETA-Friends (Albalak et al., 2022) dataset, where there are several annotated tasks. For each task t in the dataset, we use the remaining tasks to construct the set of source tasks S. We compare our results to the standard pretrain-finetune approach, where we finetune a pre-trained model on the target task.\nWe train all models using the hyper-parameters in Table 1. Training using our framework on all tasks in the FETA-Friends dataset, including baseline fine-tunings, takes about 3-5 days using a single T4 GPU."
        },
        {
            "heading": "6 Results and Discussion",
            "text": "Table 2 reports the test set results for each of the tasks in the FETA-Friends dataset. Our run achieves a uniform improvement on the test set. Where the score delta, the improvement over the baseline standard finetuning method, is 2.935 on average.\nHere, we do not perform any automated task selection and relied on hand-crafted source-task combinations. Moreover, simply including all tasks in the set of source tasks was not as performant as restricting the number of source tasks to three. Hence, future work may seek to incorporate an automated task-selection process that can regularize the number of included source tasks. A possible solution may include a source task weighting strategy, of which, there are several approaches designed for transfer learning setting (Eaton and desJardins, 2011; Zuo et al., 2021; Wang et al., 2022)."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we have presented Derivative Free Weight-space Ensembling (DFWE), a new fewsample task transfer approach. We showed how this approach can be effective in the open-domain dialogue setting. Where proposing a conjunction of a finetuning and weight interpolation approach capable of transferring knowledge between many tasks in a way that the baseline fine-tuning approach cannot. There are many ways to improve our approach, and we hope this serves to drive interest toward more efficient and effective transfer methods."
        }
    ],
    "title": "Derivative Free Weight-space Ensembling",
    "year": 2023
}