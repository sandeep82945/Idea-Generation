{
    "abstractText": "Web information extraction (WIE) is a fundamental problem in web document understanding, with a significant impact on various applications. Visual information plays a crucial role in WIE tasks as the nodes containing relevant information are often visually distinct, such as being in a larger font size or having a brighter color, from the other nodes. However, rendering visual information of a web page can be computationally expensive. Previous works have mainly focused on the Document Object Model (DOM) tree, which lacks visual information. To efficiently exploit visual information, we propose leveraging the render tree, which combines the DOM tree and Cascading Style Sheets Object Model (CSSOM) tree, and contains not only content and layout information but also rich visual information at a little additional acquisition cost compared to the DOM tree. In this paper, we present WIERT, a method that effectively utilizes the render tree of a web page based on a pretrained language model. We evaluate WIERT on the Klarna product page dataset, a manually labeled dataset of renderable e-commerce web pages, demonstrating its effectiveness and robustness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zimeng Li"
        },
        {
            "affiliations": [],
            "name": "Bo Shao"
        },
        {
            "affiliations": [],
            "name": "Linjun Shou"
        },
        {
            "affiliations": [],
            "name": "Ming Gong"
        },
        {
            "affiliations": [],
            "name": "Gen Li"
        },
        {
            "affiliations": [],
            "name": "Daxin Jiang"
        }
    ],
    "id": "SP:d54fde0234b5a4c1ac60417fb73d4aedd516d7e9",
    "references": [
        {
            "authors": [
                "J.L. Arjona Fern\u00e1ndez",
                "R. Corchuelo Gil",
                "D. Ruiz Cort\u00e9s",
                "M. Toro Bonilla"
            ],
            "title": "From Wrapping to Knowledge",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2007
        },
        {
            "authors": [
                "C.-H. Chang",
                "M. Kayed",
                "M.R. Girgis",
                "K.F. Shaalan"
            ],
            "title": "A survey of web information extraction systems",
            "venue": "IEEE transactions on knowledge and data engineering, 18(10): 1411\u20131428.",
            "year": 2006
        },
        {
            "authors": [
                "Q. Chen",
                "A. Lamoreaux",
                "X. Wang",
                "G. Durrett",
                "O. Bastani",
                "I. Dillig"
            ],
            "title": "Web question answering with neurosymbolic program synthesis",
            "venue": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, 328\u2013343.",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "Z. Zhao",
                "L. Chen",
                "J. Ji",
                "D. Zhang",
                "A. Luo",
                "Y. Xiong",
                "K. Yu"
            ],
            "title": "WebSRC: A Dataset for WebBased Structural Reading Comprehension",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 4173\u20134185. Associa-",
            "year": 2021
        },
        {
            "authors": [
                "X. Deng",
                "P. Shiralkar",
                "C. Lockard",
                "B. Huang",
                "H. Sun"
            ],
            "title": "DOM-LM: Learning Generalizable Representations for HTML Documents",
            "venue": "CoRR, abs/2201.10608.",
            "year": 2022
        },
        {
            "authors": [
                "D.G. Gregg",
                "S. Walczak"
            ],
            "title": "Adaptive web information extraction",
            "venue": "Commun. ACM, 49(5): 78\u201384.",
            "year": 2006
        },
        {
            "authors": [
                "W. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems.",
            "year": 2017
        },
        {
            "authors": [
                "Q. Hao",
                "R. Cai",
                "Y. Pang",
                "L. Zhang"
            ],
            "title": "From one tree to a forest: a unified solution for structured web data extraction",
            "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, 775\u2013784.",
            "year": 2011
        },
        {
            "authors": [
                "A. Hotti",
                "R.S. Risuleo",
                "S. Magureanu",
                "A. Moradi",
                "J. Lagergren"
            ],
            "title": "The Klarna Product Page Dataset: A Realistic Benchmark for Web Representation Learning",
            "venue": "arXiv preprint arXiv:2111.02168.",
            "year": 2021
        },
        {
            "authors": [
                "W. Hwang",
                "J. Yim",
                "S. Park",
                "S. Yang",
                "M. Seo"
            ],
            "title": "Spatial dependency parsing for semi-structured document information extraction",
            "venue": "arXiv preprint arXiv:2005.00642.",
            "year": 2020
        },
        {
            "authors": [
                "T. Iki",
                "A. Aizawa"
            ],
            "title": "Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs",
            "venue": "CoRR, abs/2203.07828.",
            "year": 2022
        },
        {
            "authors": [
                "S. Jia",
                "J. Kiros",
                "J. Ba"
            ],
            "title": "DOM-Q-NET: Grounded RL on Structured Language",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "A. Kumar",
                "K. Morabia",
                "J. Wang",
                "K.C. Chang",
                "A.G. Schwing"
            ],
            "title": "CoVA: Context-aware Visual Attention for Webpage Information Extraction",
            "venue": "CoRR, abs/2110.12320.",
            "year": 2021
        },
        {
            "authors": [
                "N. Kushmerick",
                "D.S. Weld",
                "R.B. Doorenbos"
            ],
            "title": "Wrapper Induction for Information Extraction",
            "venue": "Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI 97, Nagoya, Japan, August 23-29, 1997, 2 Volumes, 729\u2013737. Morgan Kaufmann.",
            "year": 1997
        },
        {
            "authors": [
                "A.H.F. Laender",
                "B.A. Ribeiro-Neto",
                "A.S. da Silva",
                "J.S. Teixeira"
            ],
            "title": "A Brief Survey of Web Data Extraction Tools",
            "venue": "SIGMOD Rec.,",
            "year": 2002
        },
        {
            "authors": [
                "V.A. Leksin",
                "S.I. Nikolenko"
            ],
            "title": "Semi-supervised tag extraction in a web recommender system",
            "venue": "International Conference on Similarity Search and Applications, 206\u2013212. Springer.",
            "year": 2013
        },
        {
            "authors": [
                "J. Li",
                "Y. Xu",
                "L. Cui",
                "F. Wei"
            ],
            "title": "MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding",
            "venue": "Muresan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "B.Y. Lin",
                "Y. Sheng",
                "N. Vo",
                "S. Tata"
            ],
            "title": "FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents",
            "venue": "The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1092\u20131102. ACM.",
            "year": 2020
        },
        {
            "authors": [
                "C. Lockard",
                "P. Shiralkar",
                "X.L. Dong"
            ],
            "title": "OpenCeres: When open information extraction meets the semi-structured web",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, 3047\u20133056.",
            "year": 2019
        },
        {
            "authors": [
                "J. Turmo",
                "A. Ageno",
                "N. Catal\u00e0"
            ],
            "title": "Adaptive information extraction",
            "venue": "ACM Comput. Surv., 38(2): 4.",
            "year": 2006
        },
        {
            "authors": [
                "Q. Wang",
                "Y. Fang",
                "A. Ravula",
                "F. Feng",
                "X. Quan",
                "D. Liu"
            ],
            "title": "WebFormer: The Web-page Transformer for Structure Information Extraction",
            "venue": "Proceedings of the ACM Web Conference 2022, 3124\u20133133.",
            "year": 2022
        },
        {
            "authors": [
                "C. Xie",
                "W. Huang",
                "J. Liang",
                "C. Huang",
                "Y. Xiao"
            ],
            "title": "WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2211\u20132220.",
            "year": 2021
        },
        {
            "authors": [
                "M. Zaheer",
                "G. Guruganesh",
                "K.A. Dubey",
                "J. Ainslie",
                "C. Alberti",
                "S. Onta\u00f1\u00f3n",
                "P. Pham",
                "A. Ravula",
                "Q. Wang",
                "L. Yang",
                "A. Ahmed"
            ],
            "title": "Big Bird: Transformers for Longer Sequences",
            "venue": "Advances in Neural Information Processing Systems, NeurIPS 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "P. Vines"
            ],
            "title": "Using the web for automated translation extraction in cross-language information retrieval",
            "venue": "Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, 162\u2013169.",
            "year": 2004
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "The rapid development of the internet has resulted in the creation of an enormous amount of information in the form of web pages. Extracting information from web pages is a long-standing problem in web document understanding(Hao et al. 2011; Lin et al. 2020; Wang et al. 2022; Hwang et al. 2020) and plays an important role in various fields such as knowledge base construction(Lockard, Shiralkar, and Dong 2019; Xie et al. 2021), retrieval systems(Zhang and Vines 2004), question answering(Chen et al. 2021a), recommender systems(Leksin and Nikolenko 2013) and self-driving web browsers(Iki and Aizawa 2022).\nA typical web page consists of various web documents, such as HTML, CSS, JavaScript, and resource files like images. Each of these documents has different formats and serves distinct roles in determining the overall semantics of the web page. CSS files contribute to the rendering process\n*Work done when the first author was an intern at Microsoft STCA.\n\u2020Corresponding author. Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nand appearance of a web page, and their absence can lead to difficulties in web page understanding.\nWeb page information extraction (WIE) involves extracting informative attributes from a given web page, which can be predetermined or unspecified. Attribute extraction, which involves extracting information such as university names, addresses, and phone numbers from university introduction pages, is a common form of WIE. Unlike information extraction from plain text documents, WIE involves understanding the complex layout and semantic information of web pages. This paper focuses on addressing the problem of attribute extraction from web pages.\nPrevious works have attempted to solve the WIE problem, with wrapper induction and its variants being classical methods for this task. However, these methods are difficult to generalize to unseen domains and are sensitive to page layout changes, limiting their performance on complex WIE tasks due to the small scale of annotated labeled data. Recent deep learning-based methods have significantly improved WIE performance, leveraging pre-trained language models from large scale unsupervised data to model DOM\ntree structure information(Li et al. 2022; Deng et al. 2022; Wang et al. 2022).\nNevertheless, several challenges still exist in WIE. HTML documents of web pages are too large to be fed into deep learning models due to computational resource limitations. Previous transformer-based works used BERT or RoBERTa as backbones, which have a maximum input length of 512 tokens(Li et al. 2022; Deng et al. 2022). Since the number of tokens in HTML documents typically exceeds ten thousand, previous works split the documents into segments, leading to a loss of global semantic modeling. Additionally, previous works have largely ignored other useful information in web pages such as CSS styles, and have also struggled with the problem of sparse labeling.\nTo address these challenges, we propose a new method for WIE called WIERT (Web Information Extraction via Render Tree). This approach utilizes the render tree of a web page based on a language model pre-trained on long documents. We leverage the render tree, which is the combination of the DOM tree and the Cascading Style Sheets Object Model (CSSOM) tree, to take advantage of visual information efficiently. The render tree contains content, layout information, and rich visual information with little additional acquisition cost compared to the DOM tree, making it an effective solution for WIE.\nIn summary, this paper makes the following contributions:\n1. We propose a novel transformer-based model, WIERT, for web page information extraction that effectively encodes the semantic and visual information of web documents via the render tree.\n2. To improve the representation of web pages, we use CSS style embedding to encode rich visual information in web pages and introduce three training strategies: informative node tagging, relationship with informative node prediction, and token type classification.\n3. Our experiments demonstrate the effectiveness and robustness of our method by achieving state-of-the-art results on the Klarna product information extraction task(Hotti et al. 2021)."
        },
        {
            "heading": "Related Work",
            "text": "With the development of Internet, web data is exploding, leading that web page information extraction is a hot issue now. Many works try to solve web page information extraction problems. Wrapper induction and its variants are the main classical methods(Arjona Ferna\u0301ndez et al. 2007; Chang et al. 2006; Kushmerick, Weld, and Doorenbos 1997; Laender et al. 2002; Turmo, Ageno, and Catala\u0300 2006; Gregg and Walczak 2006). They summarize a set of extraction rules by analyze the structure of DOM trees, then apply this set of rules on other web pages directly.\nClassical WIE methods focus on mining templates of web pages, which only involve shallow semantics. In order to conduct information extraction from the perspective of deep semantics, Modern WIE methods are based on deep learning techniques, which has been proven can understand patterns of big data. According to the backbone network architectures, modern WIE methods could be divided into two categories: graph neural network based methods(Hwang et al.\n2020; Kumar et al. 2021) and transformer based methods(Li et al. 2022; Deng et al. 2022; Wang et al. 2022).\nGraph neural network based methods regard the DOM tree as a graph(Hwang et al. 2020; Kumar et al. 2021). The DOM elements are nodes and the parent-child relationships are edges. Most works add edges between siblings nodes to indicate the tree structure. Then a graph neural network is used to generate the hidden representations for each element node, which are fed into a classifier to judge whether the node contains key information.\nTransformer-based methods first serialize a HTML document as a sequence, then use a transformer model to process it. To let models learn the DOM tree structure of web documents, transformer based methods usually design various modules to encode DOM trees. MarkupLM(Li et al. 2022) adds a xpath embedding layer at the bottom of language model to encode the DOM structure as fixed length vector. DOM-LM(Deng et al. 2022) utilizes multiple embedding layers (i.e., depth embedding) to represent the DOM structure. WebFormer(Wang et al. 2022) designs several DOM structure based attention mechanisms."
        },
        {
            "heading": "Preliminaries",
            "text": ""
        },
        {
            "heading": "DOM Tree, CSSOM Tree and Render Tree",
            "text": "A well-rendered webpage require a set of component files, including HTML, CSS, JS documents, and other resource files such as images. While receiving these raw files, a modern browser builds the DOM tree from HTML document and the CSSOM tree from CSS document. The latter encodes CSS rules into a tree structure. Then the browser combines them into a render tree to attach style sheets to the corresponding element nodes. The style sheets on render tree are computed, which means that these are the final styles for element nodes. This generative process is shown in Fig 2. After that, the browser computes coordinates for each node and\npaints pixel on the screen. It is worth noting that this final step is only directly related to the render tree.\nThe structure of the render tree is similar to that of the DOM tree, except that each element node in the render tree has a style sheet attached to it. The render tree provides more detailed visual information than the DOM tree, making it crucial for web information extraction tasks and closer to the essence of the web page. Additionally, the render tree only includes element nodes that are displayed on the web page, unlike the DOM tree, which may contain element nodes that are not displayed. This means that the render tree is smaller than the DOM tree, making it more efficient to encode. Because of these reasons, we have chosen to use the render tree for solving WIE problems."
        },
        {
            "heading": "WIE Problem Definition",
            "text": "In this study, we approach the problem of extracting information from web pages by framing it as a node tagging task for the render tree elements. First, we obtain the render tree of a web page, which we serialize to a sequence through a preorder traversal. This serialized render tree comprises text and element nodes, and we subsequently transform it into a token sequence that can be input into a pretrained language model that we have modified for this purpose. The process involves removing redundant spaces and tokenizing the content of text nodes and adding them to the token sequence. For element nodes, we add start and end tag tokens if the tag is not self-closing or a corresponding self-closing tag token if it is self-closing.\nThe resulting token sequence, denoted as [t1, t2, . . . , tL], comprises four types of tokens: text tokens, start tag tokens, end tag tokens, and self-closing tag tokens. We anticipate that the self-closing and start tag tokens will effectively summarize the information contained in their respective element nodes and serve as proxies for web page information extrac-\ntion after encoding by a language model."
        },
        {
            "heading": "Render Tree Encoder",
            "text": "Fig 3 shows the overall architecture of our proposed WIERT model, a render tree encoder initialized by a transformerbased pre-trained language model. First the render tree is serialized into a sequence composed of text tokens and tag tokens by preorder traversal. Then we feed the sequence into a transformer encoder and get the sequence of output vectors. On the other side, the style embedding module convert style sheets of element nodes into fixed length vectors. For each element node, we concatenate its corresponding encoder output and style embedding vector to generate the final render tree element node representation.\nIn this sector, we explain how to modify a pre-trained language model to a render tree encoder."
        },
        {
            "heading": "Tag Token Embedding",
            "text": "Since pretrained language models are trained on plain text documents, their word embedding layer cannot encode tag tokens. Here we add a new embedding layer to encode tag tokens. Because tag names are usually words with clear semantics, we can initialize the tag token embedding layer with word embeddings of tag names.\nFormally, the input embeddings of the token sequence [t1, t2, . . . , tL] is processed as\nwi = { ew(ti) ti is a text token et(ti) ti is a tag token\nwhere wi is the input embedding to the transformer encoder of the i-th token, ew is original word embedding layer of the pretrained model and et is our additional tag token embedding layer. Then the embedding sequence [w1, w2, . . . , wL] is fed into the encoder and the output sequence [h1, h2 . . . , hL] are generated."
        },
        {
            "heading": "Style Embedding",
            "text": "When accessing a web page, its HTML and CSS source code are not typically read. Rather, the visual content rendered from the web documents by a browser is examined. However, rendering images of web pages can be computationally intensive and resource-consuming. Consequently, we introduce a cheaper alternative to images of web pages, namely style sheets applied to the render tree.\nA style sheet comprises a set of style property-value pairs as shown in Figure 2. Each style property can impact the appearance of element nodes in various ways, such as text size, text weight, and background color.\nFor each element node in a render tree, its corresponding style sheet is represented as ki : vimi=1, where m is browserspecific. For each style property, a style embedding layer embeds vi into a vector. The resulting vectors are concatenated together to create the style embedding of the element node:\ns := [es1(v1) : . . . : esm(vm)]\nwhere s is an abbreviation for style embedding, : denotes the concatenation operator, and esi is the embedding mapping of the i-th style property for i = 1, . . . ,m. The style\nembedding and the encoder output of the i-th element node, namely si and hi, respectively, are concatenated to facilitate web page information extraction."
        },
        {
            "heading": "Model Training",
            "text": ""
        },
        {
            "heading": "Informative Node Tagging",
            "text": "Since we model the information extraction problem as render tree element node tagging problem, the output representations of start tag token and self-closing token are considered to represent the whole corresponding subtree. The index set I = {k1, k2, . . . , km} refers to indices of start tag tokens and self-closing tag tokens. We train the model using node-wise cross entropy loss:\nlINT = \u2211 i\u2208I ce(MLP([hi : si]), yi)\nwhere the function ce means cross entropy function, and yi is the label of the element node identified by the token i, taking value from {1, 2, . . . , C, C +1}. The positive integer C is the number of information types. We add a class called C + 1 to represent that a node doesn\u2019t express any valuable information of interest."
        },
        {
            "heading": "Relationship with Informative Node Prediction",
            "text": "The training objective INT focuses solely on candidate tokens, specifically start tag tokens and self-closing tag tokens. As a result, error signals cannot propagate through other tokens, which may hinder the efficiency of the training process. In other words, the main training objective fails to teach the model to learn the structure of information. To address this issue, the traditional solution involves adding a random conditional field on top of the backbone model.\nTo overcome this limitation, we have implemented a classifier for each attribute to determine the relationship between an element node and the node with the corresponding attribute. These relationships can be categorized into four types: ancestor, precedent, self, and other. The loss function is\nlRINP = \u2211 i\u2208I C\u2211 j=1 ce(MLP([hi : si], yij)\nwhere yij is the label of token i on label j, taking value from {self, ancestor, descendant, other}."
        },
        {
            "heading": "Token Type Classification",
            "text": "All input tokens could be divided into four classes, including text tokens, start tokens, end tokens and self-closing tokens. Tokens of different classes act different roles. Text tokens is responsible for the formation of text content in web page. HTML tag tokens are responsible for type setting, building non-textual content such as images and giving web pages more fruitful semantic information.\nThe token type classification task is viewed as four-class classification task. The loss functions of token type classification task is\nlTTC = L\u2211\ni=1\nce(MLP([hi : si]), yi)\nwhere yi is the token type of the i-th token, ce represents the cross entropy loss.\nFinally, we train the following loss l = \u03bb1lINT + \u03bb2lRINP + \u03bb3lTTC\nwhere positive numbers \u03bb1, \u03bb2 and \u03bb3 are weight factors to leverage the influence from three training objectives, which can vary as training process go.\nExperiments"
        },
        {
            "heading": "Dataset",
            "text": "Klarna product page dataset The Klarna product page dataset contains 51,701 manually labeled product pages from 8,175 real e-commerce websites(Hotti et al. 2021). In each web page, at most 5 kinds of information are annotated and each kind is only attached to at most one DOM node.\nThe Klarna dataset is multilingual. In order to utilize pretrained language models (e.g. BERT, BigBird), only English subsets are used in our experiments. Tab 2 show the scale of English part of the Klarna dataset. As we can see, the Klarna dataset provided a official train/test split. In our experiments, we keep the official test set to measure generalization performance and split the official train set into a new train set and a validation set without overlapping according to the ratio of 9 : 1."
        },
        {
            "heading": "Competitive Methods",
            "text": "We compare the results of our proposed WIERT and its variants with some strong baseline methods from (Hotti et al. 2021):\nFreeDOM-EXT FreeDOM-EXT adapts the state of the art FreeDOM architecture(Lin et al. 2020) by adding some style features. FreeDOM is a two-stage method where a neural network first computes node embeddings based on local and contextual information, and then uses embedding distances and semantic relatedness between node pairs in the second stage.\nTransformerEncoder consists of a single-layer multiheaded attention encoder stack fed with a sequence consisting of the features of the local node stacked with the features of its neighbors.\nGCN-mean is the best baseline model, which is a multilayer GCN model similar to GraphSage(Hamilton, Ying, and Leskovec 2017). GCN-mean models a DOM tree as a graph, each element is a node and the parent-son relationship on the tree is edge. GCN-mean apply average pooling to integrate information from neighbor nodes.\nGCN-GRU is inspired by the local embedding module in the DOM-Q-Net algorithm(Jia, Kiros, and Ba 2019). The encoding of node is computed by feeding a Gated Recurrent Unit (GRU) with the local features of the node and an average encoding of the neighborhood of the node.\nLSTM-BU A child-sum tree-LSTM model. For each element node, its hidden state is the sum of the hidden states of the children, and the cell state is computed based on the cell states of the children.\nBesides baselines from (Hotti et al. 2021), we introduce a strong baseline model which is well known and behaves well in various web document understanding tasks recently:\nMarkupLM encodes the Xpath of each DOM node and add the Xpath embeddings to the word embeddings, then fed into a pretrained language model. Because Markup LM is designed for modelling textual contents of HTML documents, we only use it to recognize two textual informative fields: name and price.\nLast we also compare our WIERT with a baseline model generated from itself:\nBaseline use the same model architecture as WIERT, but is only trained on informative node tagging task, i.e. \u03bb1 = 1 and \u03bb2 = \u03bb3 = 0, and doesn\u2019t contain style embedding layer at the top."
        },
        {
            "heading": "Implementation Detail",
            "text": "We use a pretrained Big Bird model to initialize our WIERT model, which was pretrained on a large corpus of long plain documents with a maximum input length of 4096(Zaheer et al. 2020). To handle discrete properties, we map each value to a one-hot vector. For continuous properties, we discretize the values into intervals of (\u2212\u221e, a1], . . . , (an,+\u221e) and treat them as discrete properties. For color properties, which are represented by four continuous values (i.e., red, green, blue, and alpha), we treat them as four continuous properties. We introduce only a few parameters in the embedding layers, and each style property is encoded by a style embedding layer. The details of the used style properties are listed in Table 1.\nUsually the token sequences generated from render trees are too long to be fed into model. To address this issue, we adopt a simple method of splitting an entire token sequence into several adjacent but disjoint segments with lengths less than model\u2019s limitation. During both the training and testing stages, we use this method to produce token sequences from the render tree structure. For all experiments, we set the batch size to 16 and use an initial learning rate of 5\u00d7 10\u22125, which decays to 85% after each epoch. Through coarse hyperparameter tuning, we set the weights of three losses as \u03bb1 = 1, \u03bb2 = 0.2, \u03bb = 0.1.\nAll experiments are conducted on eight V100 GPUs. When training WIERT and its variants, we first train the newly added parameters, which are randomly initialized, for up to two epochs. Then, we train all parameters for up to 30 epochs. The training results with the minimum error rate on the validation set are chosen to report."
        },
        {
            "heading": "Results and Discussion",
            "text": "Performance Comparison Table 3 shows the results of WIERT and competitive methods. We use predictive accuracy of each attribute and average of them as evaluation metrics. The predictive accuracy is the quotient of the number of correctly predicted pages over the number of all pages. As we can see, WIERT achieves the best performance among other baselines.\nCompare to the baseline, we observe that WIERT shows significantly superior, which illustrate the effectiveness to model render tree of our propose strategies. Compared to MarkupLM and other non-pretrained methods, such as FREEDOM-EXT, GCN-*, LSTM-BU, Our models achieve over 10% improvement, which shows the contribution of the\npretrained model and furthermore, our methods effectively adapt language model to web page information extraction task.\nAblation Study The ablation study was conducted to evaluate the effectiveness of different strategies in improving the performance of the proposed WIERT model. As shown in Table 4, we analyzed the contributions of style features and training objectives to the overall accuracy of the model.\nFirstly, we investigated the impact of CSS style features on the performance of the model by removing them from the input. The results showed a decrease in the average accuracy of 0.007, indicating that CSS information plays an important role in the identification of certain types of entities, such as \u201dbuy,\u201d \u201dimage,\u201d and \u201dprice,\u201d which contain rich visual information.\nSecondly, we examined the effect of the relationship with informative node prediction (RINP) task on the model\u2019s accuracy. The results showed a more significant decrease in the average accuracy of 0.013, suggesting that the structure of the DOM tree is crucial for identifying most of the attributes from the candidates within a page.\nFinally, we analyzed the impact of the token type classification (TTC) task on the model\u2019s accuracy by removing it\nfrom the training objectives. The results indicated a decrease in the average accuracy of 0.007, which demonstrates the effectiveness of the TTC task in improving the performance of the model.\nOverall, the ablation study highlights the importance of style features, the RINP task, and the TTC task in enhancing the performance of the WIERT model for web information extraction tasks.\nCase Study Key information on a web page often uses eye-catching style, i.e. larger font sizes, bold font weights, or bright font colors, to grab readers\u2019 attention. In this section, we have randomly selected a few web pages where key information is presented with significant style features. We will compare the performance of two models: one trained with style features and the other without.\nIn Figure 4, we compare the results of the same web page predicted by both models. We use a red bounding box to highlight the predicted product price attribute. The model trained without style features incorrectly marks an element node, while the model trained with style features correctly marks the intended node. It is evident that the product price is emphasized using a larger font size and brighter font color, which are captured by the style encoding module. Consequently, the model trained with style features is able to easily identify the correct element node."
        },
        {
            "heading": "Conclusion",
            "text": "Web information extraction is a crucial task in web document understanding with numerous commercial applications. In order to effectively understand web content, visual information is essential. However, previous approaches have\nmainly focused on modeling the DOM tree, which lacks significant visual information.\nIn this paper, we propose WIERT, an effective method for web information extraction that leverages the render tree of a web page. The render tree is a combination of the DOM tree and the CSSOM tree, and contains not only content and layout information but also rich visual information. This approach comes with only a small additional acquisition cost compared to DOM tree based methods, but provides substantial benefits in terms of efficiency and effectiveness.\nOur experiments demonstrate that the visual information\ncontained in render trees significantly improves the performance of web information extraction tasks. WIERT provides an innovative solution for utilizing the visual information of web pages and can be applied to a variety of web understanding tasks."
        }
    ],
    "title": "WIERT: Web Information Extraction via Render Tree",
    "year": 2023
}