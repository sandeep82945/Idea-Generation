{
    "abstractText": "Deep learning (DL)-based channel state information (CSI) feedback methods compressed the CSI matrix by exploiting its delay and angle features straightforwardly, while the measure in terms of information contained in the CSI matrix has rarely been considered. Based on this observation, we introduce self-information as an informative CSI representation from the perspective of information theory, which reflects the amount of information of the original CSI matrix in an explicit way. Then, a novel DL-based network is proposed for temporal CSI compression in the self-information domain, namely SD-CsiNet. The proposed SD-CsiNet projects the raw CSI onto a self-information matrix in the newly-defined self-information domain, extracts both temporal and spatial features of the selfinformation matrix, and then couples these two features for effective compression. Experimental results verify the effectiveness of the proposed SD-CsiNet by exploiting the self-information of CSI. Particularly for compression ratios 1/8 and 1/16, the SD-CsiNet respectively achieves 7.17 dB and 3.68 dB performance gains compared to state-of-the-art methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziqing Yin"
        },
        {
            "affiliations": [],
            "name": "Renjie Xie"
        },
        {
            "affiliations": [],
            "name": "Wei Xu"
        },
        {
            "affiliations": [],
            "name": "Zhaohui Yang"
        },
        {
            "affiliations": [],
            "name": "Xiaohu You"
        }
    ],
    "id": "SP:9f1e4afed6ae97d4818482d22d4c53fd8d8f901c",
    "references": [
        {
            "authors": [
                "W. Xu"
            ],
            "title": "Edge learning for B5G networks with distributed signal processing: Semantic communication, edge computing, and wireless sensing",
            "venue": "IEEE J. Sel. Topics Signal Process., vol. 17, no. 1, pp. 9\u201339, Jan. 2023. 12",
            "year": 2023
        },
        {
            "authors": [
                "W. Shen"
            ],
            "title": "Channel feedback based on AoD-adaptive subspace codebook in FDD massive MIMO systems",
            "venue": "IEEE Trans. Commun., vol. 66, no. 11, pp. 5235\u20135248, Nov. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Luo"
            ],
            "title": "Channel state information prediction for 5G wireless communications: A deep learning approach",
            "venue": "IEEE Trans. Netw. Sci. Eng., vol. 7, no. 1, pp. 227\u2013236, Mar. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Xu"
            ],
            "title": "Toward ubiquitous and intelligent 6G networks: From architecture to technology",
            "venue": "Sci. China Inf. Sci., vol. 66, no. 3, pp. 130300:1\u20132, Mar. 2023.",
            "year": 2023
        },
        {
            "authors": [
                "C.-K. Wen"
            ],
            "title": "Deep learning for massive MIMO CSI feedback",
            "venue": "IEEE Wireless Commun. Lett., vol. 7, no. 5, pp. 748\u2013751, Oct. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Sun"
            ],
            "title": "AnciNet: An efficient deep learning approach for feedback compression of estimated CSI in massive MIMO systems",
            "venue": "IEEE Wireless Commun. Lett., vol. 9, no. 12, pp. 2192\u20132196, Dec. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Song"
            ],
            "title": "SALDR: Joint self-attention learning and dense refine for massive MIMO CSI feedback with multiple compression ratio",
            "venue": "IEEE Wireless Commun. Lett., vol. 10, no. 9, pp. 1899\u20131903, Jun. 2021.",
            "year": 1899
        },
        {
            "authors": [
                "C. Lu"
            ],
            "title": "MIMO channel information feedback using deep recurrent network",
            "venue": "IEEE Commun. Lett., vol. 23, no. 1, pp. 188\u2013191, Jan. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Ji",
                "M. Li"
            ],
            "title": "CLNet: Complex input lightweight neural network designed for massive MIMO CSI feedback",
            "venue": "IEEE Commun. Lett., vol. 10, no. 10, pp. 2318\u20132322, Oct. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Yin"
            ],
            "title": "Deep CSI compression for massive MIMO: A self-information model-driven neural network",
            "venue": "IEEE Trans. Wireless Commun., vol. 21, no. 10, pp. 8872\u20138886, May. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Sayeed"
            ],
            "title": "Deconstructing multiantenna fading channels",
            "venue": "IEEE Trans. Signal Process., vol. 50, no. 10, pp. 2563\u20132579, Oct. 2002.",
            "year": 2002
        },
        {
            "authors": [
                "Y. Sun"
            ],
            "title": "A lightweight deep network for efficient CSI feedback in massive MIMO systems",
            "venue": "IEEE Commun. Lett., vol. 10, no. 8, pp. 1840\u20131844, May. 2021.",
            "year": 1840
        },
        {
            "authors": [
                "B. Shi"
            ],
            "title": "Informative dropout for robust representation learning: A shape-bias perspective",
            "venue": "Proc. 32nd Int. Conf. Mach. Learn., Jul. 2020, pp. 8828\u20138839.",
            "year": 2020
        },
        {
            "authors": [
                "K. Greff"
            ],
            "title": "LSTM: A search space odyssey",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 28, no. 10, pp. 2222\u20132232, Oct. 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Deep learning (DL)-based channel state information (CSI) feedback methods compressed the CSI matrix by exploiting its delay and angle features straightforwardly, while the measure in terms of information contained in the CSI matrix has rarely been considered. Based on this observation, we introduce self-information as an informative CSI representation from the perspective of information theory, which reflects the amount of information of the original CSI matrix in an explicit way. Then, a novel DL-based network is proposed for temporal CSI compression in the self-information domain, namely SD-CsiNet. The proposed SD-CsiNet projects the raw CSI onto a self-information matrix in the newly-defined self-information domain, extracts both temporal and spatial features of the selfinformation matrix, and then couples these two features for effective compression. Experimental results verify the effectiveness of the proposed SD-CsiNet by exploiting the self-information of CSI. Particularly for compression ratios 1/8 and 1/16, the SD-CsiNet respectively achieves 7.17 dB and 3.68 dB performance gains compared to state-of-the-art methods.\nIndex Terms\nDeep learning, self-information domain, massive MIMO, CSI feedback.\nI. INTRODUCTION\nMassive multiple-input multiple-output (mMIMO) has become a key technology for wireless communication networks. In order to fully exploit the advantages of mMIMO, the transmitter needs to obtain accurate channel state information (CSI). In order to reduce the overhead of feedback signaling, researches have been devoted to developing efficient CSI compression methods [1]. For instance, codebooks have been utilized for CSI feedback in various commercial systems, e.g., LTE/LTE-A, IEEE 802.11n/ac, and WiMAX. In [2], a codebook design based on angle-of-departure (AoD) was proposed for reducing the burden of CSI feedback. By utilizing slow-varying AoD information, the codebook was exploited to quantize the channel vector within\nar X\niv :2\n30 5.\n07 66\n2v 1\n[ cs\n.I T\n] 3\n0 A\npr 2\n02 3\n2 an angle coherence time period. However, both computational complexity and feedback overhead of the codebook-based CSI feedback methods increase sharply with the increase of the number of base station (BS) antennas [3], which limits the practicability in mMIMO systems.\nTo overcome the challenges of codebook-based feedback methods in mMIMO systems, deep learning (DL) has been considered to apply for communication systems [4], especially for CSI feedback. By representing the CSI matrix as an image, a DL-based network, namely CsiNet, was originated in [5] by using the architecture of auto-encoder in DL. Existing DL-based studies mainly focused on improving the auto-encoder structure and network lightweight to enhance the feedback performance. In [6], an improved auto-encoder structure, called AnciNet, was proposed by considering that the obtained CSI was always noisy in practice, which achieved effective reconstruction for noisy inputs of CSI. An alternative DL network was proposed in [7] to enhance the CSI feedback performance by introducing self-attention learning and dense refine (SALDR). Further in [8], a CSI compression network was proposed by exploiting the temporal correlation of wireless channels. In order to achieve a lightweight design of DL networks, a neural network with reduced size was proposed in [9] for CSI compression by introducing a complex-valued input layer and self-attention mechanism. By considering shape and texture features of CSI images, a lightweight DL network, namely IdasNet, was proposed in [10] to perform accurate CSI recovery.\nEssentially, from the perspective of information theory, the goal of CSI compression is to retain the amount of information of the channel matrix as much as possible under a limited feedback bandwidth. However, the original CSI image applied in [5]\u2013[10] mostly considered the intrinsic features of the channels, i.e., delay and angle features. These DL-based methods compressed the CSI image by exploiting the intrinsic features rather than directly from the perspective of information contained in the CSI. Therefore, it is natural to establish a direct representation of the information of an mMIMO CSI matrix and then compress it based on this informative representation for better reconstruction at the BS.\nIn this paper, we introduce self-information as the informative representation of CSI and compress the CSI in terms of the self-information. The definition of self-information is to measure and visually reflect the amount of block-wise information of the CSI matrix in an explicit way. We first project the original CSI matrix onto a self-information matrix in the newly-defined self-information domain. Then, a feature coupling encoder is proposed to extract both temporal and spatial features of the self-information representation. Note that the extracted temporal and\n3 spatial features are also correlated, thus we couple these two features for further compression. For recovery at the BS, a feature decoupling decoder is designed to recover the features and reconstruct the CSI matrix. Simulation results verify that the proposed method exhibits prominent performance gain compared to existing DL-based methods."
        },
        {
            "heading": "II. SYSTEM MODEL",
            "text": "We consider the downlink of a frequency division duplexed (FDD) mMIMO system where the BS has Nt antennas serving a user equipment (UE) with single antenna. Orthogonal frequency division multiplexing (OFDM) with Ns subcarriers is employed. The received signal at the nth subcarrier is expressed as\nyn = hHn vnxn + gn, (1)\nwhere hn \u2208 CNt\u00d71, \u2200n \u2208 {1, . . . , Ns}, denotes the channel vector at the nth subcarrier, vn \u2208 CNt\u00d71 is the precoding vector, xn \u2208 C is the transmitting signal, and gn \u2208 C represents the additive noise. The downlink channel matrix of all Ns subcarriers is then denoted by H = [h1, . . . ,hNs ]H \u2208 CNs\u00d7Nt . The number of complex-valued feedback parameters of H is therefore Ns\u00d7Nt, which is enormous in mMIMO under a limited feedback bandwidth. In order to reduce the feedback overhead, we transform the channel matrix H to the angular-delay domain, which explicitly presents possible channel sparsity [11]. By adopting the operation of two dimensionaldiscrete Fourier transform (2D-DFT), the angular-delay domain channel matrix is obtained as\nHa = FcHFd, (2)\nwhere Fc and Fd are the DFT matrices with proper dimensions. In the angular-delay domain, the channel Ha contains useful information only in the first Nc rows of multi-path delay with Nc being the number of multiple paths, while the other rows are made up of near-zero values due to large propagation delays. Without loss of generality, we retain the first Nc rows of Ha, denoted by Hf \u2208 CNc\u00d7Nt , for the CSI compression.\nGenerally for a DL-based CSI compression method, due to that typical DL network training does not support complex-valued inputs [12], we separate the real and imaginary parts of Hf as the input of the DL network, denoted by Hf \u2208 R2\u00d7Nc\u00d7Nt . Then an encoder deployed at the UE compresses the channel matrix Hf to a specific codeword, denoted by c. The encoder network is represented by\nc = fEN(Hf,\u0398E), (3)\n4\nwhere fEN(\u00b7) denotes the compression function of the encoder and \u0398E is the set of the training parameters. At the other side, a decoder network deployed at BS recovers the channel matrix based on the received codeword c, which is represented by\nH\u0302f = fDE(c,\u0398D), (4)\nwhere fDE(\u00b7) is the decompression function of the decoder and \u0398D is the set of corresponding training parameters. The desired channel matrix, denoted by H\u0302, can be finally acquired by adopting an inverse 2D-DFT to H\u0302f in (4)."
        },
        {
            "heading": "III. NEURAL CSI COMPRESSION WITH FEATURE COUPLING",
            "text": "In this section, we elaborate the architecture of the proposed neural CSI compression network with temporal and spatial features under the self-information domain, referred to as SD-CsiNet. The proposed network consists of a self-information transformation (SF) module, an encoder with feature coupling, and a corresponding decoder with feature decoupling. Without loss of generality, we assume that there exists T temporal CSI matrices within a coherent time. Therefore, it is necessary to introduce an additional dimension upon Hf to represent the number of temporal CSI matrices in a coherent time, which is denoted by Hc \u2208 RT\u00d72\u00d7Nc\u00d7Nt . Then the recovered CSI image at BS is denoted by H\u0302c \u2208 RT\u00d72\u00d7Nc\u00d7Nt . Specifically, the SF module transforms the CSI matrix from the angular-delay domain to the newly-defined self-information domain. The transformed matrix in the self-information domain successfully highlights the informative\n5 characteristic of the CSI. Subsequently, the feature coupling encoder extracts both temporal and spatial features of the transformed matrix and couples them into a codeword c. The decoder at the BS recovers the CSI matrix. The architecture details of each module in SD-CsiNet are elaborated in the following."
        },
        {
            "heading": "A. The SF Module",
            "text": "To better reflect the amount of information contained in the CSI matrix, the SF module transforms the CSI matrix from the angular-delay domain to the self-information domain. A concrete illustrative description of the SF module is depicted in Fig. 1. The module consists of a feature extraction layer, an index matrix calculation module, and a feature restoration layer. Note that the input of the SF module is a sequence of CSI matrices Hc. The output of the SF module is the self-information matrix of the CSI, denoted by He \u2208 RT\u00d72\u00d7Nc\u00d7Nt .\nThe upper part in Fig. 1 is the feature extraction layer, which transforms Hc to 64 feature maps, denoted by F \u2208 RT\u00d764\u00d7Nc\u00d7Nt , where each feature map characterizes a specific feature of Hc. Different from existing DL-based methods, e.g., [5]\u2013[10], we adopt a three-dimensional (3D) convolutional layer with a filter size of 64\u00d71\u00d73\u00d73, rather than conventional 2D convolutional layers, because Hc contains the time dimension. Here the dimension of 64 denotes the number of feature maps and 1\u00d7 3\u00d7 3 is the kernel size of the 3D convolutional layer.\nThe lower part is the index matrix calculation module, which is an essential component network for transforming Hc to the self-information domain. It analyzes the pixel-wise entropy of the CSI matrix, calculates the self-information of CSI matrix, Hc, in an pixel-wise manner and outputs an index matrix. The index matrix rules out pixels with marginal information in the feature matrices while retains pixels with large entropy in terms of the self-information. In the index matrix calculation module, we first separate Hc into T CSI matrices Hc,i \u2208 R2\u00d7Nc\u00d7Nt for i \u2208 {1, 2, . . . , T}. Then we divide each of R(Hc,i) and I(Hc,i) into Nc \u00d7Nt pixels by using a 1\u00d7 1 dividing grid. Each pixel is denoted by pj \u2208 R1\u00d71 for j \u2208 {1, 2, . . . , NcNt}. According to the definition of information theory, the probability needs to be acquired before calculating the self-information of pj . This probability relates to the surrounding pixels of pj . Intuitively, if pj is significantly different from surrounding pixels, it contains more essential information for CSI and corresponds to a low probability. Thus by checking on the surrounding pixels and inspired\n6 by [13], the self-information of pj is calculated as\nI\u0302j =\u2212log2 1\n9 9\u2211 r=1 1\u221a 2\u03c0 exp ( \u2212 \u2225\u2225pj \u2212 p\u2032j,r\u2225\u222522 /2) , (5)\nwhere I\u0302j denotes an estimate of self-information of pj , and p \u2032 j,r denotes the rth surrounding pixel of pj . Note that for a specific pj , considering the balance of the running time and the network performance, we pick out 9 pixels that are closet to the center pixel pj , including itself, to calculate the self-information. That is, we have r \u2208 {1, 2, . . . , 9}. Given the definition of self-information in (5), we now obtain a self-information matrix, denoted by Ii \u2208 R2\u00d7Nc\u00d7Nt , and the (2,m, n)th element of Ii is I\u0302j .\nAfter the SF calculation, it follows a mapping layer in Fig. 1. This layer maps the selfinformation matrix Ii to an informative feature matrix, denoted by Di \u2208 R64\u00d7Nc\u00d7Nt . Considering there is no time dimension in Ii, the mapping layer adopts a 2D convolution with a filter size of 64\u00d7 3\u00d7 3. Note that this mapping layer has non-trainable filter parameters and thus it is not involved for gradient update in network training. Note that the self-information I\u0302j is a numerical representation of the information contained in pj . The more information that pj contains, the larger I\u0302j is. Accordingly, we set a self-information threshold, denoted by Y , to rule out pixels with smaller self-information values in Di to ensure that only the most informative pixels in the CSI matrix retain in the compression. In particular, the elements in Di with their self-information smaller than Y are set to zeros, and the other elements are all ones. Mathematically, this operation is expressed as\nmj =  1, if dj \u2265 Y0, otherwise, (6) where dj denotes the jth element of Di and mj denotes the position of dj in Di. The corresponding index matrix Mi \u2208 R64\u00d7Nc\u00d7Nt contains all these {mj}\u2019s 0\u2019s and 1\u2019s. Since the index matrix calculation module only calculates the index matrix of a single CSI matrix Hc,i at each time, this module runs T times to get all the index matrices of Hc. The running times therefore corresponds to the first dimension of Hc. We then stack all the index matrices in M\u0304 \u2208 RT\u00d764\u00d7Nc\u00d7Nt as follows\nM\u0304 = [M1,M2, \u00b7 \u00b7 \u00b7,MT ]. (7)\nAfter acquiring M\u0304, the SF module uses operator F M\u0304 for removing the elements with trivial information in F, where denotes the Hadamard product. The last part in the SF module, i.e., the\n7\nfeature restoration layer with the filter size of 2\u00d7 1\u00d7 3\u00d7 3, completes the transformation from F to He in the self-information domain. The output of SF module highlights the informative characteristic of Hc compared to the original CSI matrix, which is conductive for extracting spatial and temporal features in the subsequent encoder network."
        },
        {
            "heading": "B. Feature Coupling Encoder",
            "text": "Given the self-information matrix He of the CSI, the feature coupling encoder extracts both temporal and spatial features of He and couples them to a codeword. This encoder consists of a spatial compression layer, a max-pooling layer, and a long short-term memory (LSTM) module as shown in the right part of Fig. 1. It outputs the feedback codeword c.\nThe self-information matrix He is first reshaped into a matrix V \u2208 RT\u00d72NcNt . The spatial compression layer adopts a 1D convolution with filter size of M\u00d72NcNt\u00d71, rather than a popular fully-connected (FC) layer, to extract the spatial feature of V. Note that M is the dimension of the feedback codeword, determined by M = \u03c3 \u00d7 2NcNt, where \u03c3 is the compression ratio and 2NcNt \u00d7 1 is the kernel size of the 1D convolutional layer. On the other hand, the max-pooling layer is used to reduce the dimension of V from T \u00d7 2NcNt to T \u00d7NcNt, then the single LSTM network [14] is used to extract the temporal feature of V."
        },
        {
            "heading": "C. Feature Decoupling Decoder",
            "text": "After receiving the codeword c at BS, the feature decoupling decoder in Fig. 2 is designed to recover the CSI matrix, which consists of the spatial decompression layer, two convolutional\n8\nneural network (CNN) blocks, and a normalization layer. The decoder inputs c and outputs the recovered CSI matrix, H\u0302c. The spatial decompression layer adopts a 1D convolutional layer with the filter size of 2NcNt \u00d7M \u00d7 1 to decompress the spatial features from c. The decompressed features are reshaped to an matrix, denoted by W \u2208 RT\u00d72\u00d7Nc\u00d7Nt , before forwarded to the CSI recovery module. The CSI recovery module contains two CNN blocks and a normalization layer. The parameters of these network layers are summarized in Table I. The value 0.3 of LReLU function is a constraint parameter, which allows negative values of a matrix to be mapped to corresponding tiny values rather than simply all zeros."
        },
        {
            "heading": "IV. EXPERIMENTAL RESULTS",
            "text": "In this section, we verify the effectiveness of the proposed SD-CsiNet in terms of normalized mean-squared error (NMSE). Then we compare the number of parameters of SD-CsiNet with state-of-the-art methods. For practical application, we also compare the performance when quantized feedback codeword is further considered. Further, we conducts ablation experiments to explore the impact of each module of SD-CsiNet on CSI recovery.\n1) Simulation Setting: We generate 150, 000 channel samples through the COST 2100 indoor channel model and the channel samples are split into 100, 000 for training sets, 30, 000 for validation sets and 20, 000 for testing sets. The BS with Nt = 32 antennas is deployed with a uniform linear array (ULA). The number of subcarriers of OFDM is Ns = 1024 and the first main-valued rows is Nc = 32. The time step of temporal CSI images is set to T = 5. The\ntrainable weights and bias of all convolutional layers are initialized randomly. The simulation is carried out in Pytorch on a GTX3090 GPU.\n2) NMSE Comparison: To valid the performance of the proposed SD-CsiNet on the CSI recovery, we compare the NMSE with existing methods such as RecCsiNet [8] and CLNet [9]. The NMSE is calculated by\nNMSE = E {\u2225\u2225\u2225Hc \u2212 H\u0302c\u2225\u2225\u22252\n2 / \u2016Hc\u201622\n} , (8)\nwhere notation E {\u00b7} is the expectation operator. We carry out the experimental simulations with various compression ratios of 1/8, 1/16, 1/32, and 1/64. The comparison results are shown in Fig. 3 and we can observe that the proposed SD-CsiNet performs best under all compression ratios. The performance gain of NMSE for SD-CsiNet is similar under other compression ratios.\n3) Parameter Comparison: As shown in Table II, we compare the network parameters of proposed SD-CsiNet with RecCsiNet [8] and CLNet [9]. We can observe that the proposed SD-\n10\nCsiNet has the least parameters under all compression ratios. The parameters of SD-CsiNet is 19 M less than RecCsiNet on average. The parameters of SD-CsiNet is 30.59%, 23.26%, 29.27%, and 28.57% than CLNet at the \u03c3 of 1/8, 1/16, 1/32, and 1/64.\n4) Comparison with Quantization Feedback: In this paper, we train the SD-CsiNet without considering the quantization on the offline training stage. For the online deployment, we adopt the Lloyd-Max algorithm to quantize the codeword c. The Lloyd-Max algorithm is a non-uniform quantization algorithm, which reduces the quantization interval when the probability density of the compressed codeword is large, and vice versa. In the simulation, each compressed codeword is quantized by 6 bits. The quantization results with \u03c3 = {1/8, 1/16} are compared in Table III. We observe that the proposed SD-CsiNet still outperforms RecCsiNet [8] and CLNet [9] when the codeword is quantized. In Table III, we also compare the online computational complexity, i.e., online running time, of these methods. It is observed that all these methods have similar online running time.\n11\n5) Ablation Experiment: To valid the effectiveness of each module of SD-CsiNet for CSI recovery, we conduct ablation experiment as shown in Table IV. The baseline is the CNN consists of naive convolutional layers and FC layers. The epoch of ablation experiment is set to 600. From Table IV we can observe that each module of SD-CsiNet has the essential impact on high-accuracy CSI recovery. Especially, the self-information image in the self-information domain explicitly represents the information contained in the CSI image. Therefore, the proposed SD-CsiNet can allocate more compressed resources to regions with large information in the self-information image, i.e., realize the \u201cdifference\u201d allocation of compressed resources according to the information amount, which results in the performance gain in CSI feedback and reconstruction."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we proposed a novel DL-based network, named SD-CsiNet, for the CSI compression and recovery. We exploited the self-information to represent the amount of information contained in the CSI. Then we extracted and coupled the spatial and temporal features of CSI from the perspective of self-information to achieve efficient compression. Further, the CSI matrix could be recovered accurately by decoupling these features. The experimental results showed that the proposed network could achieve the obvious NMSE performance gain compared to existing DL-based networks under all compression ratios."
        }
    ],
    "title": "Self-information Domain-based Neural CSI Compression with Feature Coupling",
    "year": 2023
}