{
    "abstractText": "Recovering detailed facial geometry from a set of calibrated multi-view images is valuable for its wide range of applications. Traditional multi-view stereo (MVS) methods adopt an optimization-based scheme to regularize the matching cost. Recently, learning-based methods integrate all these into an end-to-end neural network and show superiority of efficiency. In this paper, we propose a novel architecture to recover extremely detailed 3D faces within dozens of seconds. Unlike previous learning-based methods that regularize the cost volume via 3D CNN, we propose to learn an implicit function for regressing the matching cost. By fitting a 3D morphable model from multi-view images, the features of multiple images are extracted and aggregated in the mesh-attached UV space, which makes the implicit function more effective in recovering detailed facial shape. Our method outperforms SOTA learning-based MVS in accuracy by a large margin on the FaceScape dataset. The code and data are released in https://github.com/zhuhao-nju/mvfr.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yunze Xiao"
        },
        {
            "affiliations": [],
            "name": "Hao Zhu"
        },
        {
            "affiliations": [],
            "name": "Haotian Yang"
        },
        {
            "affiliations": [],
            "name": "Zhengyu Diao"
        },
        {
            "affiliations": [],
            "name": "Xiangju Lu"
        },
        {
            "affiliations": [],
            "name": "Xun Cao"
        }
    ],
    "id": "SP:2fa4aff6623a5d7fed99d373eedd7c6d7d2500d2",
    "references": [
        {
            "authors": [
                "H. Aan\u00e6s",
                "R.R. Jensen",
                "G. Vogiatzis",
                "E. Tola",
                "A.B. Dahl"
            ],
            "title": "Large-Scale Data for Multiple-View Stereopsis",
            "venue": "IJCV, 1\u201316.",
            "year": 2016
        },
        {
            "authors": [
                "T. Alldieck",
                "G. Pons-Moll",
                "C. Theobalt",
                "M. Magnor"
            ],
            "title": "Tex2shape: Detailed full human body geometry from a single image",
            "venue": "ICCV, 2293\u20132303.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Bai",
                "Z. Cui",
                "X. Liu",
                "P. Tan"
            ],
            "title": "Riggable 3D Face Reconstruction via In-Network Optimization",
            "venue": "arXiv preprint arXiv:2104.03493.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Bai",
                "Z. Cui",
                "J.A. Rahim",
                "X. Liu",
                "P. Tan"
            ],
            "title": "Deep Facial Non-Rigid Multi-View Stereo",
            "venue": "CVPR, 5850\u2013 5860.",
            "year": 2020
        },
        {
            "authors": [
                "T. Beeler",
                "B. Bickel",
                "A.P. Beardsley",
                "B. Sumner",
                "H.M. Gross"
            ],
            "title": "High-quality single-shot capture of facial geometry",
            "venue": "ToG, 1\u20139.",
            "year": 2010
        },
        {
            "authors": [
                "D. Bradley",
                "W. Heidrich",
                "T. Popa",
                "A. Sheffer"
            ],
            "title": "High resolution passive facial performance capture",
            "venue": "SIGGRAPH, 1\u201310.",
            "year": 2010
        },
        {
            "authors": [
                "A. Bulat",
                "G. Tzimiropoulos"
            ],
            "title": "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230, 000 3D facial landmarks)",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "D.F.N. Campbell",
                "G. Vogiatzis",
                "C. Hern\u00e1ndez",
                "R. Cipolla"
            ],
            "title": "Using Multiple Hypotheses to Improve Depth-Maps for Multi-View Stereo",
            "venue": "ECCV, 766\u2013779.",
            "year": 2008
        },
        {
            "authors": [
                "T. Chan",
                "W. Zhu"
            ],
            "title": "Level set based shape prior segmentation",
            "venue": "CVPR, volume 2, 1164\u20131170.",
            "year": 2005
        },
        {
            "authors": [
                "R. Chen",
                "S. Han",
                "J. Xu",
                "H. Su"
            ],
            "title": "Point-Based Multi-View Stereo Network",
            "venue": "ICCV, 1538\u20131547.",
            "year": 2019
        },
        {
            "authors": [
                "P. Dou",
                "I.A. Kakadiaris"
            ],
            "title": "Multi-view 3D face reconstruction with deep recurrent neural networks",
            "venue": "Image and Vision Computing, 80: 80\u201391.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Furukawa",
                "C. Hern\u00e1ndez"
            ],
            "title": "Multi-View Stereo: A Tutorial",
            "venue": "Foundations and Trends in Computer Graphics and Vision, 1\u2013148.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Furukawa",
                "J. Ponce"
            ],
            "title": "Accurate, dense, and robust multiview stereopsis",
            "venue": "PAMI, 1362\u20131376.",
            "year": 2010
        },
        {
            "authors": [
                "S. Galliani",
                "K. Lasinger",
                "K. Schindler"
            ],
            "title": "Massively Parallel Multiview Stereopsis by Surface Normal Diffusion",
            "venue": "ICCV.",
            "year": 2015
        },
        {
            "authors": [
                "A. Ghosh",
                "G. Fyffe",
                "B. Tunwattanapong",
                "J. Busch",
                "X. Yu",
                "P. Debevec"
            ],
            "title": "Multiview face capture using polarized spherical gradient illumination",
            "venue": "SIGGRAPH Asia, 1\u201310.",
            "year": 2011
        },
        {
            "authors": [
                "M. Glencross",
                "G.J. Ward",
                "F. Melendez",
                "C. Jay",
                "J. Liu",
                "R. Hubbold"
            ],
            "title": "A perceptually validated model for surface depth hallucination",
            "venue": "ACM Transactions on Graphics (TOG), 27(3): 1\u20138.",
            "year": 2008
        },
        {
            "authors": [
                "X. Gu",
                "Z. Fan",
                "S. Zhu",
                "Z. Dai",
                "F. Tan",
                "P. Tan"
            ],
            "title": "Cascade cost volume for high-resolution multi-view stereo and stereo matching",
            "venue": "CVPR, 2495\u20132504.",
            "year": 2020
        },
        {
            "authors": [
                "W. Hartmann",
                "S. Galliani",
                "M. Havlena",
                "K. Schindler",
                "V.L. Gool"
            ],
            "title": "Learned multi-patch similarity",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "P.-H. Huang",
                "K. Matzen",
                "J. Kopf",
                "N. Ahuja",
                "J.-B. Huang"
            ],
            "title": "DeepMVS: Learning Multi-view Stereopsis",
            "venue": "CVPR, 2821\u20132830.",
            "year": 2018
        },
        {
            "authors": [
                "S. Im",
                "H.-G. Jeon",
                "S. Lin",
                "S.I. Kweon"
            ],
            "title": "DPSNet: End-to-end Deep Plane Sweep Stereo",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "R. Jensen",
                "A. Dahl",
                "G. Vogiatzis",
                "E. Tola",
                "H. Aan\u00e6s"
            ],
            "title": "Large scale multi-view stereopsis evaluation",
            "venue": "CVPR, 406\u2013413.",
            "year": 2014
        },
        {
            "authors": [
                "M. Ji",
                "J. Gall",
                "H. Zheng",
                "Y. Liu",
                "L. Fang"
            ],
            "title": "SurfaceNet: An End-to-end 3D Neural Network for Multiview Stereopsis",
            "venue": "ICCV, 2326\u20132334.",
            "year": 2017
        },
        {
            "authors": [
                "A. Kar",
                "C. H\u00e4ne",
                "J. Malik"
            ],
            "title": "Learning a MultiView Stereo Machine",
            "venue": "NIPS, 364\u2013375.",
            "year": 2017
        },
        {
            "authors": [
                "A. Kendall",
                "H. Martirosyan",
                "S. Dasgupta",
                "P. Henry",
                "R. Kennedy",
                "A. Bachrach",
                "A. Bry"
            ],
            "title": "End-to-end learning of geometry and context for deep stereo regression",
            "venue": "ICCV, 66\u201375.",
            "year": 2017
        },
        {
            "authors": [
                "A. Knapitsch",
                "J. Park",
                "Q.-Y. Zhou",
                "V. Koltun"
            ],
            "title": "Tanks and temples: benchmarking large-scale scene reconstruction",
            "venue": "ToG.",
            "year": 2017
        },
        {
            "authors": [
                "N.K. Kutulakos",
                "M.S. Seitz"
            ],
            "title": "A Theory of Shape by Space Carving",
            "venue": "IJCV, 199\u2013218.",
            "year": 2000
        },
        {
            "authors": [
                "M. Lhuillier",
                "L. Quan"
            ],
            "title": "A quasi-dense approach to surface reconstruction from uncalibrated images",
            "venue": "PAMI, 418\u2013433.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Liu",
                "X. Cao",
                "Q. Dai",
                "W. Xu"
            ],
            "title": "Continuous depth estimation for multi-view stereo",
            "venue": "CVPR, 2121\u20132128.",
            "year": 2009
        },
        {
            "authors": [
                "K. Luo",
                "T. Guan",
                "L. Ju",
                "H. Huang",
                "Y. Luo"
            ],
            "title": "PMVSNet: Learning Patch-Wise Matching Confidence Aggregation for Multi-View Stereo",
            "venue": "ICCV, 10452\u201310461.",
            "year": 2019
        },
        {
            "authors": [
                "R. Malladi",
                "J. Sethian",
                "B. Vemuri"
            ],
            "title": "Shape Modeling with Front Propagation: A Level Set Approach",
            "venue": "PAMI, 17(2): 158\u2013175.",
            "year": 1995
        },
        {
            "authors": [
                "E. Ramon",
                "J. Escur",
                "X. Giro-i Nieto"
            ],
            "title": "Multi-view 3D face reconstruction in the wild using siamese networks",
            "venue": "ICCV Workshops.",
            "year": 2019
        },
        {
            "authors": [
                "G. Riegler",
                "O.A. Ulusoy",
                "A. Geiger"
            ],
            "title": "OctNet: Learning Deep 3D Representations at High Resolutions",
            "venue": "CVPR.",
            "year": 2017
        },
        {
            "authors": [
                "S. Saito",
                "Z. Huang",
                "R. Natsume",
                "S. Morishima",
                "A. Kanazawa",
                "H. Li"
            ],
            "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization",
            "venue": "ICCV, 2304\u20132314.",
            "year": 2019
        },
        {
            "authors": [
                "S. Saito",
                "T. Simon",
                "J. Saragih",
                "H. Joo"
            ],
            "title": "Pifuhd: Multi-level pixel-aligned implicit function for highresolution 3d human digitization",
            "venue": "CVPR, 84\u201393.",
            "year": 2020
        },
        {
            "authors": [
                "L.J. Sch\u00f6nberger",
                "E. Zheng",
                "J.-M. Frahm",
                "M. Pollefeys"
            ],
            "title": "Pixelwise View Selection for Unstructured MultiView Stereo",
            "venue": "ECCV, 501\u2013518.",
            "year": 2016
        },
        {
            "authors": [
                "M.S. Seitz",
                "B. Curless",
                "J. Diebel",
                "D. Scharstein",
                "R. Szeliski"
            ],
            "title": "A Comparison and Evaluation of MultiView Stereo Reconstruction Algorithms",
            "venue": "CVPR, 519\u2013528.",
            "year": 2006
        },
        {
            "authors": [
                "M.S. Seitz",
                "R.C. Dyer"
            ],
            "title": "Photorealistic Scene Reconstruction by Voxel Coloring",
            "venue": "IJCV, 151\u2013173.",
            "year": 1997
        },
        {
            "authors": [
                "E. Tola",
                "C. Strecha",
                "P. Fua"
            ],
            "title": "Efficient large-scale multi-view stereo for ultra high-resolution image sets",
            "venue": "Mach. Vis. Appl., 903\u2013920.",
            "year": 2012
        },
        {
            "authors": [
                "L. Valgaerts",
                "C. Wu",
                "A. Bruhn",
                "H.-P. Seidel",
                "C. Theobalt"
            ],
            "title": "Lightweight binocular facial performance capture under uncontrolled lighting",
            "venue": "ToG, 31(6): 187\u20131.",
            "year": 2012
        },
        {
            "authors": [
                "P.-S. Wang",
                "Y. Liu",
                "Y.-X. Guo",
                "C.-Y. Sun",
                "X. Tong"
            ],
            "title": "O-CNN: octree-based convolutional neural networks for 3D shape analysis",
            "venue": "ToG.",
            "year": 2017
        },
        {
            "authors": [
                "T.-C. Wang",
                "M.-Y. Liu",
                "J.-Y. Zhu",
                "A. Tao",
                "J. Kautz",
                "B. Catanzaro"
            ],
            "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
            "venue": "CVPR.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wu",
                "K. He"
            ],
            "title": "Group normalization",
            "venue": "ECCV, 3\u201319.",
            "year": 2018
        },
        {
            "authors": [
                "J. Yan",
                "Z. Wei",
                "H. Yi",
                "M. Ding",
                "R. Zhang",
                "Y. Chen",
                "G. Wang",
                "Y.-W. Tai"
            ],
            "title": "Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "H. Yang",
                "H. Zhu",
                "Y. Wang",
                "M. Huang",
                "Q. Shen",
                "R. Yang",
                "X. Cao"
            ],
            "title": "FaceScape: a Large-scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction",
            "venue": "CVPR.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yao",
                "Z. Luo",
                "S. Li",
                "T. Fang",
                "L. Quan"
            ],
            "title": "MVSNet: Depth Inference for Unstructured Multi-view Stereo",
            "venue": "ECCV.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Yao",
                "Z. Luo",
                "S. Li",
                "T. Shen",
                "T. Fang",
                "L. Quan"
            ],
            "title": "Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "L. Yariv",
                "J. Gu",
                "Y. Kasten",
                "Y. Lipman"
            ],
            "title": "Volume rendering of neural implicit surfaces",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        },
        {
            "authors": [
                "L. Yariv",
                "Y. Kasten",
                "D. Moran",
                "M. Galun",
                "M. Atzmon",
                "B. Ronen",
                "Y. Lipman"
            ],
            "title": "Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance",
            "venue": "NIPS, 33.",
            "year": 2020
        },
        {
            "authors": [
                "H. Yi",
                "Z. Wei",
                "M. Ding",
                "R. Zhang",
                "Y. Chen",
                "G. Wang",
                "Y.-W. Tai"
            ],
            "title": "Pyramid multi-view stereo net with self-adaptive view aggregation",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhu",
                "Y. Nie",
                "T. Yue",
                "X. Cao"
            ],
            "title": "The role of prior in image based 3d modeling: a survey",
            "venue": "Frontiers of Computer Science, 11(2): 175\u2013191.",
            "year": 2017
        },
        {
            "authors": [
                "H. Zhu",
                "H. Yang",
                "L. Guo",
                "Y. Zhang",
                "Y. Wang",
                "M. Huang",
                "Q. Shen",
                "R. Yang",
                "X. Cao"
            ],
            "title": "FaceScape: 3D Facial Dataset and Benchmark for Single-View 3D Face Reconstruction",
            "venue": "arXiv preprint arXiv:2111.01082.",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhu",
                "X. Zuo",
                "S. Wang",
                "X. Cao",
                "R. Yang"
            ],
            "title": "Detailed human shape estimation from a single image by hierarchical mesh deformation",
            "venue": "CVPR, 4491\u20134500.",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhu",
                "X. Zuo",
                "H. Yang",
                "S. Wang",
                "X. Cao",
                "R. Yang"
            ],
            "title": "Detailed avatar recovery from single image",
            "venue": "PAMI.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Recovering high-quality facial 3D models is of great value in many fields including the movie industry, facial analysis, and augmented reality. Multi-view stereo (MVS) focuses on reconstructing accurate shape from several calibrated images, which is the main way to obtain high-quality 3D face models. Traditional MVS pipeline mainly consists of matching cost computation, fusion, and refinement(Beeler et al. 2010; Furukawa and Ponce 2010; Galliani, Lasinger, and Schindler 2015), which can yield extremely fine facial geometry at the cost of tremendous calculations and running time. In recent years, learning-based MVS methods(Chen et al. 2019; Gu et al. 2020; Huang et al. 2018; Im et al. 2019; Ji et al. 2017; Kar, Ha\u0308ne, and Malik 2017; Luo et al. 2019; Yao et al. 2018, 2019) show superiority in accuracy and speed, which use a single-pass neural network instead of traditional iterative optimization. Though learning-based MVS outperforms traditional methods in certain scenes, there is still no learning-based MVS method that recovers fine-level facial geometry for industrial 3D modeling.\nCopyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. \u2217 These authors contributed equally to this work. \u2020 Xun Cao is the corresponding author.\nIn this paper, we propose a novel framework to efficiently recover detailed facial geometry from calibrated multi-view images. The key observation is that the previous state-ofthe-art learning-based methods(Yao et al. 2018, 2019; Chen et al. 2019; Gu et al. 2020; Yi et al. 2020; Yan et al. 2020) build and refine the cost volume in the camera frustum space, which is memory-consuming and limits the resolution. Though several works(Gu et al. 2020; Riegler, Ulusoy, and Geiger 2017; Wang et al. 2017) tried to improve the efficiency of the volume-based neural network, they didn\u2019t utilize the facial prior and fail to recover the detailed geometry. To overcome this problem, we propose to use an implicit function to regularize the matching cost to recover geometric details. To make the framework achievable, multi-view 3DMM estimation is utilized to transfer the cost space into UVD space (UV + displacement), which efficiently solves the rough shape, and supports the implicit function learning in detailed shape recovery. Compared to the previous learning-based MVS framework, our method doesn\u2019t require an additional fusion phase, which saves processing time and achieves higher accuracy.\nThe contribution can be summarized in three aspects:\n\u2022 A novel facial-specific multi-view reconstruction pipeline is proposed consisting of base model fitting, learned implicit function, and mesoscopic prediction.\n\u2022 We propose to tackle the problem of cost regression and multi-view fusion simultaneously by learning an implicit\nfunction in our network, which is proved to be more accurate and time-saving.\n\u2022 We propose to build the mesh-attached cost volume in UVD space, which greatly reduces the solution space for implicit function learning, achieving efficient and effective MVS for human faces."
        },
        {
            "heading": "Related Works",
            "text": "The related works can be divided into three categories: traditional MVS, learning-based MVS, and face-specific MVS.\nTraditional MVS. Traditional MVS recovers the 3D shape of the objects or scenes from a set of calibrated multi-view images. According to the representation of the 3D modeling, traditional MVS can be categorized into volumetric method(Kutulakos and Seitz 2000; Seitz and Dyer 1997), point-based method(Furukawa and Ponce 2010; Lhuillier and Quan 2005; Chen et al. 2019) and depthbased method(Campbell et al. 2008; Galliani, Lasinger, and Schindler 2015; Scho\u0308nberger et al. 2016; Tola, Strecha, and Fua 2012; Liu et al. 2009). Due to the space limitation, we recommend referring to the survey/benchmark(Seitz et al. 2006; Knapitsch et al. 2017; Zhu et al. 2017) and the tutorial(Furukawa and Herna\u0301ndez 2015) for the comprehensive review for traditional MVS. Here we focus on reviewing recently proposed learning-based MVS and face-specific MVS, which are more relevant to our work.\nLearning-based MVS. Learning-based MVS methods seek to improve the reconstructing performance by introducing deep neural networks. In early attempts, Hartmann et al.(Hartmann et al. 2017) proposed to use the encoder network with multiple Siamese input branches to measure the similarity of the image patches from the multi-view input images. Ji et al.(Ji et al. 2017) proposed to use a CNN to predict each voxel a binary attribute indicating whether the voxel is on the surface. Kar et al.(Kar, Ha\u0308ne, and Malik 2017) proposed an end-to-end learning MVS network by involving differentiable feature projection and inverseprojection along viewing rays. Based on this framework, Yao et al. (Yao et al. 2018, 2019) built the cost volume on the camera frustum, and adopts 3D CNN or RNN to regularize the matching cost. Huang et al. (Huang et al. 2018) proposed to use the CNN to estimate the disparity from the plane-sweep volumes generated from multi-view images. Sunghoon et al.(Im et al. 2019) introduced the cost fusion and aggregation network, and finally regress a depth map from the refined cost volume. Chen et al.(Chen et al. 2019) proposed to process the target scene as point clouds in a neural network, which predicts the depth in a coarseto-fine manner. Luo et al. (Luo et al. 2019) proposed the patch-based MVS network consisting of a patch-wise aggregation module to generate a matching confidence volume from extracted features, and a hybrid 3D CNN to infer a depth probability distribution. Gu et al.(Gu et al. 2020) extends the prior volume-based MVS network where cost volume is built upon a feature pyramid encoding geometry and context at gradually finer scales. Yan et al. (Yan et al. 2020) proposes a dense hybrid recurrent multi-view stereo net with dynamic consistency checking for accurate dense\npoint cloud reconstruction. Yi et al.(Yi et al. 2020) presents a pyramid multi-view stereo network with the self-adaptive view aggregation. Yariv et al.(Yariv et al. 2020) proposes an implicit differentiable renderer that learns 3D geometry, appearance, and cameras from masked 2D images and noisy camera initialization. Different from all the above methods, our method is the first to learn an implicit function for cost regularization in the learning-based framework.\nFace-specific MVS. Faces are special as they contain relatively fewer photometric features but plenty of geometric details. Based on the pyramid stereo matching framework, Beeler et al.(Beeler et al. 2010) uses smoothness constraint, ordering constraint, and uniqueness constraint to robustly recover the facial shape, then the shape is iteratively optimized to recover pore-level geometry. Bradley et al.(Bradley et al. 2010) extends the multi-view facial stereo to multiview videos that are captured by an array of video cameras. Ghosh et al.(Ghosh et al. 2011) proposes to capture high resolution diffuse and specular photometric information using a multi-view face capture system, then reconstruct detailed facial geometry. Valgaerts et al.(Valgaerts et al. 2012) proposed a lightweight passive facial performance capture approach that only requires a single pair of stereo cameras. Though generalized MVS methods are able to recover fine 3D models, they consume a lot of computing resources and time to recover faithful geometric details. In recent years, many works study how to recover non-rigid facial geometry from uncalibrated images or videos. Dou et al.(Dou and Kakadiaris 2018) and Ramon et al.(Ramon, Escur, and Giroi Nieto 2019) proposes to recover the face model from multiview images using a subspace representation of the 3D facial shape and a deep recurrent neural network to fuse the identity-related features. Bai et al.(Bai et al. 2020, 2021) propose to optimize the 3D face shape by explicitly enforcing multi-view appearance consistency, which makes it possible to recover shape details according to conventional multi-view stereo methods."
        },
        {
            "heading": "Methods",
            "text": "As shown in Figure 2, our method consists of three stages. In the first stage, the 2D landmarks of multi-view images are extracted, and a base mesh is obtained by fitting the 3DMM to these landmarks. In the second stage, the accurate 3D facial geometry is predicted by learning the implicit function, and the detailed shape is refined by the mesoscopic prediction network in the last stage. We will explain each module in detail in the following sections."
        },
        {
            "heading": "Base Mesh Fitting",
            "text": "Firstly, we use Bulat et al.\u2019s method (Bulat and Tzimiropoulos 2017) to obtain the 2D landmarks [l1, l2, ..., lM ] \u2208 RM\u00d72 for each image. Given the camera parameters, the 3D landmarks can be solved by minimizing the energy function below:\nE(Lj) = N\u2211 i=0 ||\u03a0i(Lj)\u2212 lj ||2 (1)\nwhere i is the index of the view, N is the total view number. \u03a0i is the perspective projection of the camera in ith view. Lj and lj are the jth 3D and 2D landmarks separately. The Lj in [L1,L2, ...,LM ] \u2208 RM\u00d73 is solved by least-squares respectively.\nAfter 3D landmarks are obtained, we fit the bilinear model generated by FaceScape(Yang et al. 2020) to the 3D landmarks and obtain the topologically-uniformed base mesh. The bilinear model of FaceScape can be formulated as V = B(id, exp), where B is a linear mapping from the parameter id and exp to the vertices position V of the facial mesh model. The model fitting is solved by minimizing the energy function below:\nE(id, exp, T ) = M\u2211 j=0 ||T (Kj(B(id, exp)))\u2212 Lj ||2 (2)\nwhere T () is the transformation between two coordinate systems. Kj(M) extracts the jth 3D landmarks from the input mesh M using the predefined indices. We use the bilinear model generated by Yang et al.(Yang et al. 2020) in our experiments, and this can be substituted by other 3D facial parametric models."
        },
        {
            "heading": "Mesh-attached Cost Space",
            "text": "Different from the previous method that builds the cost volume in the Euclidean space or camera frustum, our method builds the cost volume in the UVD space attaching to the fitted facial mesh. The UV map is the flat 2D representation of the surface of the 3D mesh, where the attributes that are attached to the surface can be efficiently stored. We extend the UV space with the third dimension \u2013 displacement along the surface normal (DaN), and regularize the matching cost in this mesh-attached UVD space. This strategy is similar to the idea of SDF(Malladi, Sethian, and Vemuri 1995;\nChan and Zhu 2005; Yariv et al. 2021) and has been used in many previous methods(Zhu et al. 2019, 2021b; Alldieck et al. 2019). We use the UV mapping defined by FaceScape, and the UV mapping is uniform in all fitted base meshes. The surface normal is obtained by sampling the vertex normal of the base mesh on the UV map, and the surface normal is smoothed to eliminate messy normal vectors caused by the high-frequency shape.\nThe establishment of mesh-attached volume brings three benefits to the facial MVS: 1) The mesh-attached volume connects the statistical facial model estimation and the volume-based depth regression method; 2) The solution space to represent geometry is reduced compared to the Euclidean volume(Kar, Ha\u0308ne, and Malik 2017) or camera frustum(Yao et al. 2018, 2019; Gu et al. 2020), thus the volume resolution can be enhanced; 3) The mesh-attached space can represent the complete face, while the camera frustum only sees the partial face and requires an additional fusion phase to produce complete facial geometry."
        },
        {
            "heading": "Implicit Function Learning",
            "text": "Implicit function. Inspired by the success of implicit function in single-view human shape recovery(Saito et al. 2019, 2020), we integrate implicit function learning into facial multi-view stereo. The overall network architecture is shown in Figure 3. Our implicit function F is formulated as:\nF(V (p), d(p)) = s, s \u2208 R (3) where d(p) is the displacement corresponding to the sampled point p. V (p) is the variance of the pixel-wise features extracted from multi-view images, formulated as:\nV (p) = 1\nN N\u2211 i=0 F ci (\u03a0i(p)) 2 \u2212 ( 1 N N\u2211 i=0 F ci (\u03a0i(p))) 2 (4)\nwhere F ci is the feature with c channels extracted from ith image using the feature extracting network; \u03a0i is the projection function of the ith camera; N is the number of the input images.\nWe use the trainable multi-layer perceptron (MLP) to learn this implicit function. The label s\u0302 for training is the distance-to-surface, which is the normalized probability to describe how close from the sampled point to the groundtruth surface. The label is formulated as:\ns\u0302(d) = 1\u221a 2\u03c0\u03c31 e \u2212 (d\u2212d\u0302)\n2\n2\u03c31 2 (5)\nwhere \u03c31 is set to 0.05 in our experiment. Our distance-tosurface label is different from the inside/outside label used in PIFu(Saito et al. 2019, 2020), and we will demonstrate the effectiveness in the ablation study section.\nFeature extractor. We use a convolutional neural network to extract the features F ci in Equation 3 from the input images. The architecture of our feature extractor is modified based on the feature extractor of MVSNet(Yao et al. 2018) and RMVSNet(Yao et al. 2019). In our feature extractor, the group normalization(Wu and He 2018) is integrated into each convolutional layer, and half of the striding operations are removed to enhance the resolution of the generated feature maps. The feature extractor and the implicit function are trained in an end-to-end manner.\nSurface-aware sampling. Compared to the CNN-based regularization network in the previous works(Yao et al. 2018, 2019; Gu et al. 2020; Yi et al. 2020; Yan et al. 2020), the implicit function benefits from the surface-aware sampling strategy, which uses more points close to the surface for training. This strategy has been used for human shape recovery in the 3D volumetric space(Saito et al. 2019, 2020), while we extend it to samples in the UVD space with a Gaussian distribution. Specifically, the probability distribution to sample in the UVD space follows Gaussian distribution in\ndisplacement dimension, and uniform distribution in U and V dimensions. The probability is formulated as:\nP (u, v, d) = 1\u221a 2\u03c0A e \u2212 (d\u2212d\u0302)\n2\n2\u03c32 2 (6)\nwhere u, v, d are the coordinates along U, V, and displacement axes. d\u0302 is the ground-truth displacement. A is the constant to normalize the integral value of probability to 1. In our experiments, we set \u03c32 = 0.1 and the range of d is normalized to [\u22121, 1]. In order to sample the points far away from the surface, an additional uniform sampling in meshattached UVD space is adopted. We combine surface-aware sampled points and uniformly sampled points with a ratio of 16 : 1.\nPost-regularization. In the testing phase, the points in the complete UVD space are fed to the feature extractor and the implicit function, obtaining the cost volume in UVD space. A 3D convolutional neural network is used to further refine the cost volume, and the displacement map is extracted from the regularized cost volume using softargmax(Kendall et al. 2017) along the displacement dimension. Here we use softargmax as it is the differentiable substitution of argmax. The predicted displacement map is applied to the base mesh, generating the final mesh."
        },
        {
            "heading": "Mesoscopic Prediction",
            "text": "The method in the last Section can recover fine middle-scale geometry, but still lacks the micro-scale geometry such as skin pores and minor wrinkles, because the disparity change across such a feature is too small to detect. Inspired by the traditional MVS method(Beeler et al. 2010), we notice the fact that light reflected by a diffuse surface is related to the integral of the incoming light, where the small concavities may block part of the incoming light and appear darker. This fact has been adopted in prior works(Beeler et al. 2010; Glencross et al. 2008) to synthesize the bump based on the\nrough mesh. Here we seek the potential to recover a displacement map representing the mesoscopic geometry from the multi-view images.\nTo be specific, we firstly use the resulting mesh to transfer the texture from the multiple images into the UV space, forming the partial UV textures. For each pixel in the UV texture, we compute the cosine of the angle between the normal direction and the camera direction as the weight, then blend these multi-view UV textures into a complete UV texture using the weighting average. The blended UV texture is fed to the mesoscopic network to predict the mesoscopic displacement map. The mesoscopic network uses the Pix2PixHD(Wang et al. 2018) network as the backbone and is trained using the displacement maps provided by FaceScape dataset.\nMethod CD-mean/mm CD-rms/mm Comp./% MVSNet 0.378 / 0.305 0.333 / 0.308 74.8 / 87.2 CasMVS 0.381 / 0.319 0.323 / 0.282 80.8 / 90.7 PVA 0.409 / 0.323 0.347 / 0.295 77.4 / 92.2 D2HC 0.382 0.324 83.5 Ours 0.175 0.202 100.0\n* Numbers before and after \u2018/\u2019 represent the original trained model and the fine-tuned model respectively.\nTable 1: Comparison with Model-Generalized Methods"
        },
        {
            "heading": "Experiments",
            "text": ""
        },
        {
            "heading": "Implementation Details",
            "text": "Data Preparation. We use FaceScape(Yang et al. 2020; Zhu et al. 2021a) dataset to train and validate our method. The\nFaceScape dataset contains 7120 multi-view images and corresponding 3D models, each of which consists of roughly 50 images, corresponding calibrations, and the 3D mesh model. These data are captured from 359 different subjects and each in 20 expressions. We manually filter out the tuples with blur or calibration problems, then selected 80% of the remaining data as the training set and the other 20% as the testing set. For each tuple, we select 10 images from the frontal views as input. These images are scaled to 864\u00d7 1296 for training and testing. The authorization of the data is described in the section of Ethics Statement.\nIn the base mesh fitting phase, we use the bilinear model generated by FaceScape dataset. The officially provided bilinear model is generated from 847 subjects, which contains the 359 subjects in our training/testing data. To fairly evaluate our methods, we regenerated the bilinear model with data that excluded the 359 subjects in our training/test set.\nTraining Details. The feature extractor and the implicit function are trained in an end-to-end manner, while the postregularizer is trained after the other parameters are fixed. MSE loss is used to train the feature extractor + implicit function, and also the post-regularizer. We train our network using Adam optimizer, with the learning rate as 10\u22123, and our model is trained in 200 epochs. The batch size is set to 1. We trained the model using Nvidia RTX 3090 for about 100 hours.\nMetrics. We use chamfer distance (CD) between the predicted mesh and ground-truth mesh to measure the accuracy of the methods. We only measure the accuracy in the facial area, excluding eyes and inner mouth where the ground-truth geometry is hard to obtain. Considering the result meshes of generalized MVS methods(Yao et al. 2018; Gu et al. 2020; Yi et al. 2020; Yan et al. 2020) tend to extend out of the facial area, we mask out the outer mesh using the groundtruth mask of the frontal view. We run the experiments of comparison and ablation study on our testing sets, which contains 1341 tuples of data. For each tuple, there are images in 10 views, calibration parameters, and the groundtruth facial models. For all 1341 tuples of results, we report the mean of the chamfer distance (CD-mean) and the root mean square of chamfer distance (CD-rms). We also report the completeness (Comp.) for other methods, which is the area on the ground-truth mesh with error distance < 2mm dividing the area of the full ground-truth mesh. When computing the chamfer distance, the area of the predicted mesh with error distance > 2mm is filtered out. Considering our method recovers the complete face, the completeness of our method is labeled as 100%. All our mesh are counted when computing the chamfer distance."
        },
        {
            "heading": "Comparison with SOTA",
            "text": "We compare our method with previous methods quantitatively and qualitatively. The baselines are MVSNet(Yao et al. 2018), CasMVSNet(Gu et al. 2020), D2HCRMVSNet(Yan et al. 2020), and PVA-MVSNET(Yi et al. 2020), which are state-of-the-art generalized MVS methods. We evaluate two models for these methods: \u2018ori\u2019 means the originally trained model provided by the authors, which were trained on DTU dataset(Aan\u00e6s et al. 2016; Jensen\net al. 2014); \u2018ft\u2019 means the model fine-tuned using our training data generated from FaceScape dataset. The fine-tuned model is trained with epochs that are equivalent to our training settings. The scores of CD-mean, CD-rms, and completeness are reported in Table 1. The rendered results, as well as heat maps of error distance from predicted mesh to ground-truth mesh, are shown in Figure 4, and these results of previous methods are from the fine-tuned models.\nFrom the quantitative comparison in Table 1, we can see that our method outperforms previous methods in CDmean, CD-rms, and completeness for facial reconstruction. Among previous methods, though MVSNet(Yao et al. 2018) is slightly better in CD-mean and CD-rms than the other three methods, the completeness of MVSNet is worse, which means more bad areas are not counted in chamfer distance. We consider the reason is that our method benefits from the mesh-attached UVD cost space and the implicit function learning framework. The enhancement for each proposed module is discussed in the ablation study.\nBesides, we compare our method with IDR(Yariv et al. 2020), which is a model-specific MVS method that also uses implicit learning for 3D reconstruction. IDR requires to be trained for each object, and the learned parameters are specific to the certain object. By contrast, our method and the methods mentioned above are model-generalized, which means they do not require to be trained for different input images. Since IDR takes an extremely long time to process (about 400 times slower than our method), we only compare the first 5 tuples of multi-view images, and the quantitative results are shown in Table 2. We can see that our method outperforms IDR by a large margin."
        },
        {
            "heading": "Ablation Study",
            "text": "We evaluate the performance with different components as defined below. For all the settings, the feature extractor and the base mesh fitting phase remain the same.\n\u2022 (a) Base. The base mesh generated by bilinear model fitting.\n\u2022 (b) Base+3DCNN. The implicit function prediction is replaced by 3D CNN regularizer. This strategy is adopted by prior works(Yao et al. 2018, 2019; Gu et al. 2020).\n\u2022 (c) Base+IF. Post-regularize module and mesoscopic prediction are removed.\n\u2022 (d) Base+IF(I/O)+Reg. Mesoscopic prediction is removed, and the implicit function here predicts the inside/outside labels, as used in (Saito et al. 2019).\n\u2022 (e) Base+IF+Reg. Only mesoscopic prediction is removed.\n\u2022 (f) Base+IF+Reg+Meso. Our complete method.\nWe visualize the results with different components in Figure 5, and report the quantitative evaluations of ablation study in Table 3. We can see that the base mesh in (a) is quite inaccurate, with chamfer distance > 2mm. Comparing (b) with (c), we can see that the implicit function-based framework is more accurate than 3D-CNN-based framework, and the performance of (c) can be further improved by adding a post-regularizer, as shown in (e). Comparing (d) and (e), we find that distance-to-surface labels are more effective than inside/outside labels. Comparing (f) with (e), though mesoscopic prediction doesn\u2019t enhance the accuracy quantitatively, it improves the visual effects by synthesizing mesoscopic geometry. Comparing (b) with MVSNet in Table 1, we can see that the cost volume in UVD space outperforms that in camera-frustum, since the much redundant space is ignored in the UVD space.\nThe performance for different view numbers are shown in Table 4. The mean chamfer distance from 10 views to 6 views fluctuates relatively little, but decrease severely when view points are fewer than 5. We believe that this is normal for the multi-view reconstruction method, because fewer\nviewpoints provide less matching information, resulting in unstable reconstruction."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, a novel architecture that leverages parametric model fitting and builds cost volume based on the attached UVD space to recover extremely detailed 3D faces. We propose to learn an implicit function for regularization in the task of multi-view stereo, which is proved to be more efficient and effective in recovering geometric details.\nLimitation. There are still unresolved problems with our proposed method. Firstly, our method is only designed for rigid 3D facial reconstruction from well-calibrated and highquality images. We found that the performance will degrade for the in-the-wild images, and the reason is that the resolution and signal-to-noise ratio of in-the-wild images are commonly too low for extremely fine 3D reconstruction. Secondly, our method can only recover a rigid 3D face, while cannot work for the image sequences with a dynamic face. An expression-identity disentangling module may be further added to solve this problem. Thirdly, it is hard to extend our method to arbitrary objects other than human faces, as our implicit function is built on the base of a parametric model."
        },
        {
            "heading": "Ethics Statement",
            "text": "The proposed method will promote the development of 3D facial modeling technology, which can be useful for applications requiring 3D face models with mesoscopic details, such as movies, games, and other entertainment industries. With the advancement of this technology, people may conveniently recover high-resolution and high-accuracy 3D face models from multi-view images through specialist devices and efficient algorithms. However, the efficient acquisition of detailed 3D facial models may cause severer privacy violation problems than 2D images. Therefore, we suggest that policymakers should establish an efficient monitoring platform to regulate the illegal spread of 3D face models with private information that may cause ethical problems.\nMany images used in this paper are provided by FaceScape dataset(Zhu et al. 2021a; Yang et al. 2020), and the corresponding subjects have been known and authorized to use their photos for academic research. We have contacted the author of the FaceScape to confirm that the images in this article are authorized for publication. To protect the identity information of the subjects, we anonymized the face images by mosaicing the eyes. Please note that we used non-mosaic images for training and testing in the experiment, and only mosaic the images in the figures of this paper."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the NSFC grant 62025108, 62001213, and a gift funding from iQIYI Inc."
        }
    ],
    "title": "Detailed Facial Geometry Recovery from Multi-View Images by Learning an Implicit Function",
    "year": 2022
}