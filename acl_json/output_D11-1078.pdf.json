{
    "abstractText": "An A-C bilingual dictionary can be inferred by merging A-B and B-C dictionaries using B as pivot. However, polysemous pivot words often produce wrong translation candidates. This paper analyzes two methods for pruning wrong candidates: one based on exploiting the structure of the source dictionaries, and the other based on distributional similarity computed from comparable corpora. As both methods depend exclusively on easily available resources, they are well suited to less resourced languages. We studied whether these two techniques complement each other given that they are based on different paradigms. We also researched combining them by looking for the best adequacy depending on various application scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xabier Saralegi"
        },
        {
            "affiliations": [],
            "name": "Iker Manterola"
        },
        {
            "affiliations": [],
            "name": "I\u00f1aki San Vicente"
        }
    ],
    "id": "SP:39ae6b317d7d7f9527beba3296416c2eee7169b2",
    "references": [
        {
            "authors": [
                "Francis Bond",
                "Kentaro Ogura."
            ],
            "title": "Combining linguistic resources to create a machine-tractable Japanese-Malay dictionary",
            "venue": "Language Resources and Evaluation, 42(2):127\u2013136.",
            "year": 2007
        },
        {
            "authors": [
                "Maike Erdmann",
                "Kotaro Nakayama",
                "Takahiro Hara",
                "Shojiro Nishio."
            ],
            "title": "An approach for extracting bilingual terminology from wikipedia",
            "venue": "Proceedings of the 13th international conference on Database systems for advanced applications, DASFAA\u201908,",
            "year": 2008
        },
        {
            "authors": [
                "Pascale Fung."
            ],
            "title": "Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus",
            "venue": "David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 173\u2013183, Somerset, New Jersey. Association for",
            "year": 1995
        },
        {
            "authors": [
                "Pablo Gamallo",
                "Jos\u00e9 Pichel."
            ],
            "title": "Automatic generation of bilingual dictionaries using intermediary languages and comparable corpora",
            "venue": "Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, 11th International",
            "year": 2010
        },
        {
            "authors": [
                "Conference",
                "CICLing"
            ],
            "title": "Proceedings, volume 6008 of Lecture Notes in Computer Science, pages 473\u2013483",
            "venue": "Springer.",
            "year": 2010
        },
        {
            "authors": [
                "Varga Istv\u00e1n",
                "Yokoyama Shoichi."
            ],
            "title": "Bilingual dictionary generation for low-resourced language pairs",
            "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP \u201909, pages 862\u2013870,",
            "year": 2009
        },
        {
            "authors": [
                "Hiroyuki Kaji",
                "Shin\u2019ichi Tamamura",
                "Dashtseren Erdenebat"
            ],
            "title": "Automatic construction of a Japanese-Chinese dictionary via English",
            "venue": "In Proceedings of the Sixth International Language Resources and Evaluation",
            "year": 2008
        },
        {
            "authors": [
                "Mausam",
                "Stephen Soderland",
                "Oren Etzioni",
                "Daniel S Weld",
                "Michael Skinner",
                "Jeff Bilmes."
            ],
            "title": "Compiling a massive, multilingual dictionary via probabilistic inference",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL",
            "year": 2009
        },
        {
            "authors": [
                "Kyonghee Paik",
                "Satoshi Shirai",
                "Hiromi Nakaiwa."
            ],
            "title": "Automatic construction of a transfer dictionary considering directionality",
            "venue": "Proceedings of the Workshop on Multilingual Linguistic Ressources, MLR \u201904, pages 31\u201338, Stroudsburg, PA, USA.",
            "year": 2004
        },
        {
            "authors": [
                "R. Rapp."
            ],
            "title": "Automatic identification of word translations from unrelated English and German corpora",
            "venue": "Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pages 519\u2013526, College Park, USA. ACL.",
            "year": 1999
        },
        {
            "authors": [
                "Daphna Shezaf",
                "Ari Rappoport."
            ],
            "title": "Bilingual lexicon generation using non-aligned signatures",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, page 98107, Stroudsburg, PA, USA. Association for",
            "year": 2010
        },
        {
            "authors": [
                "S. Shirai",
                "K. Yamamoto."
            ],
            "title": "Linking english words in two bilingual dictionaries to generate another language pair dictionary",
            "venue": "Proceedings of ICCPOL, pages 174\u2013179.",
            "year": 2001
        },
        {
            "authors": [
                "J. Sj\u00f6bergh."
            ],
            "title": "Creating a free digital Japanese-Swedish lexicon",
            "venue": "Proceedings of PACLING 2005.",
            "year": 2005
        },
        {
            "authors": [
                "Kumiko Tanaka",
                "Kyoji Umemura."
            ],
            "title": "Construction of a bilingual dictionary intermediated by a third language",
            "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING\u201994), pages 297\u2013303.",
            "year": 1994
        },
        {
            "authors": [
                "Takashi Tsunakawa",
                "Naoaki Okazaki",
                "Jun\u2019ichi Tsujii"
            ],
            "title": "Building bilingual lexicons using lexical translation probabilities via pivot languages. Proceedings of the Sixth International Language Resources and Evaluation (LREC\u201908)",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 846\u2013856, Edinburgh, Scotland, UK, July 27\u201331, 2011. c\u00a92011 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Nobody doubts the usefulness and multiple applications of bilingual dictionaries: as the final product in lexicography, translation, language learning, etc. or as a basic resource in several fields such as Natural Language Processing (NLP) or Information Retrieval (IR), too. Unfortunately, only major languages have many bilingual dictionaries. Furthermore, construction by hand is a very tedious job. Therefore, less resourced languages (as well as less-common language pairs) could benefit from a method to reduce the costs of constructing bilingual dictionaries. With the growth of the web, resources like Wikipedia seem to be a good option to extract new bilingual lexicon (Erdmann et al., 2008), but the reality is that a dictionary is quite different from\nan encyclopedia. Wiktionary1 is a promising asset more oriented towards lexicography. However, the presence of less resourced languages in these kinds of resources is still relative -in Wikipedia, too-.\nAnother way to create bilingual dictionaries is by using the most widespread languages (e.g., English, Spanish, French...) as a bridge between less resourced languages, since most languages have some bilingual dictionary to/from a major language. These pivot techniques allow new bilingual dictionaries to be built automatically. However, as the next section will show, it is no small task because translation between words is not a transitive relation at all. The presence of polysemous or ambiguous words in any of the dictionaries involved may produce wrong translation pairs. Several techniques have been proposed to deal with these ambiguity cases (Tanaka and Umemura, 1994; Shirai and Yamamoto, 2001; Bond et al., 2001; Paik et al., 2004; Kaji et al., 2008; Shezaf and Rappoport, 2010). However, each technique has different performance and properties producing dictionaries of certain characteristics, such as different levels of coverage of entries and/or translations. The importance of these characteristics depends on the context of use of the dictionary. For example, a small dictionary containing the most basic vocabulary and the corresponding most frequent translations can be adequate for some IR and NLP tasks, tourism, or initial stages of language learning. Alternatively, a dictionary which maximizes the vocabulary coverage is more oriented towards advanced users or translation services.\nThis paper addresses the problem of pruning 1http://www.wiktionary.org/\n846\nwrong translations when building bilingual dictionaries by means of pivot techniques. We aimed to come up with a method suitable for less resourced languages. We analyzed two of the approaches proposed in the literature which are not very demanding on resources: Inverse Consultation (IC) (Tanaka and Umemura, 1994) and Distributional Similarity (DS) (Kaji et al., 2008), their strong points and weaknesses, and proposed that these two paradigms be combined. For this purpose, we studied the effect the attributes of the source dictionaries have on the performance of IC and DS-based methods, as well as the characteristics of the dictionaries produced. This could allow us to predict the performance of each method just by looking at the characteristics of the source dictionaries. Finally, we tried to provide the best combination adapted to various application scenarios which can be extrapolated to other languages.\nThe basis of the pivot technique is dealt with in the next section, and the state of the art in pivot techniques is reviewed in the third section. After that, the analysis of the aforementioned approaches and experiments carried out for that purpose are presented, and a proposal for combining both paradigms is included. The paper ends by drawing some conclusions from the results."
        },
        {
            "heading": "2 Pivot Technique",
            "text": "The basic pivot-oriented construction method is based on assuming the transitive relation of the translation of a word between two languages. Thus:\nif p (pivot word) is a translation of s (source word) in the A-B dictionary and t (target word) is a translation of p in the B-C dictionary, we can say that t is therefore a translation of s, or translationA,B(s) = p and translationB,C(p) = t\u2192 translationA,C(s) = t\nThis simplification is incorrect because it does not take into account word senses. Translations correspond to certain senses of the source words. If we look at figure 1, t (case of t1 and t2) can be the translation of p (p2) for a sense c (c3) different from the sense for which p (p2) is the equivalent of s (c1). This can happen when p pivot word is polysemous.\nIt could be thought that these causalities are\nnot frequent, and that the performance of this basic approach could be acceptable. Let us analyze a real case. We merged a Basque-English dictionary composed of 17,672 entries and 43,021 pairs with an English-Spanish one composed of 16,326 entries and 38,128 pairs, and obtained a noised Basque-Spanish dictionary comprising 14,000 entries and 104,165 pairs. 10,000 (99,844 pairs) among all the entries have more than one translation. An automatic evaluation shows that 80.32% of these ambiguous entries contain incorrect translation equivalents (80,200 pairs out of 99,844). These results show that a basic pivot-oriented method is very sensitive to the ambiguity level of the source dictionaries. The conclusion is that the transitive relation between words across languages can not be assumed, because of the large number of ambiguous entries that dictionaries actually have. A more precise statement for the transitive property in the translation process would be:\nif p (pivot word) is a translation of s with respect to a sense c and t is a translation of p with respect to the same sense c we can say that t is a translation of s, or translationA,B(sc1) = p and translationB,C(pc2) = t and c1 = c2 \u2192 translationA,C(s) = t\nUnfortunately, most dictionaries lack comparable information about senses in their entries. So it is not possible to map entries and translation equivalents according to their corresponding senses. As an alternative, most papers try to guide this mapping according to semantic distances extracted from the dictionaries themselves or from external resources\nsuch as corpora. Another problem inherent in pivot-based techniques consists of missing translations. This consists of pairs of equivalents not identified in the pivot process because there is no pivot word, or else one of the equivalents is not present. We will not be dealing with this issue in this work so that we can focus on the translation ambiguity problem."
        },
        {
            "heading": "3 State of the Art",
            "text": "In order to reject wrong translation pairs, Tanaka et al. (1994) worked with the structure of the source dictionaries and introduced the IC method which measures the semantic distance between two words according to the number of pivot-words they share. This method was extended by using additional information from dictionaries, such as semantic classes and POS information in (Bond et al., 2001; Bond and Ogura, 2007). Sjo\u0308bergh (2005) compared full definitions in order to detect words corresponding to the same sense. However, not all the dictionaries provide this kind of information. Therefore, external knowledge needs to be used in order to guide mapping according to sense. Istva\u0301n et al. (2009) proposed using WordNet, only for the pivot language (for English in their case), to take advantage of all the semantic information that WordNet can provide. Mausam et. al. (2009) researched the use of multiple languages as pivots, on the hypothesis that the more languages used, the more evidences will be found to find translation equivalents. They used Wiktionary for building a multilingual lexicon. Tsunakawa et al. (2008) used parallel corpora to estimate translation probabilities between possible translation pairs. Those reaching a minimum threshold are accepted as correct translations to be included in the target dictionary. However, even if this strategy achieves the best results in the terminology extraction field, it is not adequate when less resourced languages are involved because parallel corpora are very scarce.\nAs an alternative, (Kaji et al., 2008; Gamallo and Pichel, 2010) proposed methods to eliminate spurious translations using cross-lingual context or distributional similarity calculated from comparable corpora. In this line of work, (Shezaf and Rappoport, 2010) propose a variant of DS, and show\nhow it outperforms the IC method. In comparison, our work focuses on analyzing the strong and weak points of each technique and aims to combine the benefits of each of them.\nOther characteristics of the merged dictionaries like directionality (Paik et al., 2004) also influence the results."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "This work focuses on adequate approaches for less resourced languages. Thus, the assumption for the experimentation is that few resources are available for both source and target languages. The resources for building the new dictionary are two basic (no definitions, no senses) bilingual dictionaries (A-B, B-C) including source (A), target (C) and a pivot language (B), as well as a comparable corpus for the source-target (A-C) language pair. We explored the IC (Tanaka and Umemura, 1994) and DS (Kaji et al., 2008; Gamallo and Pichel, 2010) approaches. In our experiments, the source and target languages are Basque and Spanish, respectively, and English is used for pivot purposes. In any case, the experiments could be conducted with any other language set, so long the required resources are available.\nIt must be noted that the proposed task is not a real problem because there is a Basque-Spanish dictionary already available. Resources like parallel corpora for that language pair are also available. These dictionaries and pivot language were selected in order to be able to evaluate the results automatically. During the evaluation we also used frequency information extracted from a parallel corpus, but then again, this corpus was not used during the dictionary building process, and therefore, it would not be used in a real application environment."
        },
        {
            "heading": "4.1 Resources",
            "text": "In order to carry out the experiments we used three dictionaries. The two dictionaries mentioned in the previous section (Basque-English Deu\u2192en and English-Spanish Den\u2192es) were used to produce a new Basque-Spanish Deu\u2192en\u2192es dictionary. In addition, we used a Basque-Spanish Deu\u2192es dictionary for evaluation purposes. Its broad coverage is indicative of its suitability as a reference\ndictionary. Table 1 shows the main characteristics of the dictionaries. We can observe that the ambiguity level of the entries (average number of translations per source word) is significant. This produces more noise in the pivot process, but it also benefits IC due to the increase in pivot words. As for the directions of source dictionaries, English is taken as target. Like Paik et al. (2004) we obtained the best coverage of pairs in that way.\nSince we were aiming to merge two general dictionaries, the most adequate strategy was to use open domain corpora to compute DS. The domain of journalism is considered to be close to the open domain, and so we constructed a Basque-Spanish comparable corpus composed of news articles (see Table 2). The articles were gathered from the newspaper Diario Vasco (Hereinafter DV) for the Spanish part and from the Berria newspaper for the Basque part. Both publications focus on the Basque Country. In order to achieve a higher comparability degree, some constraints were applied:\n\u2022 News in both languages corresponded to the same time span, 2006-2010.\n\u2022 News corresponding to unrelated categories between newspapers were discarded.\nIn addition, as mentioned above, we extracted the frequencies of translation pairs from a Basque-Spanish parallel corpus. The corpus had 295,026 bilingual segments (4 Mw in Basque and 4.7 Mw in Spanish) from the domain of journalism."
        },
        {
            "heading": "5 Pruning Methods",
            "text": "IC and DS a priori suffer different weak points. IC depends on the structure of the source dictionaries. On the other hand, DS depends on a good comparable corpus and translation process. DS is measured more precisely between frequent words because context representation is richer.\nThe conditions for good performance of both IC and DS are analyzed below. These conditions will then be linked to the required characteristics for the initial dictionaries. In addition, we will measure how divergent the entries solved for each method are."
        },
        {
            "heading": "5.1 Inverse consultation",
            "text": "IC uses the structure of the Da\u2212b and Db\u2212c source dictionaries to measure the similarity of the meanings between source word and translation candidate. The description provided by Tanaka et al. (1994) is summarized as follows. To find suitable equivalents for a given entry, all target language translations of each pivot translation are looked up (e.g., Db\u2192c(Da\u2192b(s))). This way, all the \u201cequivalence candidates\u201d (ECs) are obtained. Then, each one is looked up in the inverse direction (following the previous example, Dc\u2192b(t)) to create a set of words called \u201cselection area\u201d (SA). The number of common elements of the same language between SA and the translations or equivalences (E) obtained in the original direction (Da\u2192b(s)) is used to measure the semantic distance between entries and corresponding translations. The more matches there are, the better the candidate is. If only one inverse dictionary is consulted, the method is called \u201cone time inverse consultation\u201d or IC1. If n inverse dictionaries are consulted, the method is called \u201cn time inverse consultation\u201d. As there is no significant difference in performance, we simply implemented IC1. Assuming that each element (x) of these two sets (SA,E) has a weight that is determined by the number of times it appears in the set that belongs (X), this weight is denoted as \u03b4(X,x). In the same way, the number of common elements between SA and E is denoted as follows:\n\u03b4(E,SA) = \u2211\nx\u2208SA \u03b4(E, x) (1)\nIC asks for more than one pivot word between source word s and translation candidate t. In our example:\n\u03b4(Da\u2192b(s), Dc\u2192b(t)) > 1 (2)\nIn general, this condition guarantees that pivot words belong to the same sense of the source word (e.g. iturri\u2192tap\u2192grifo, iturri\u2192faucet\u2192grifo). Consequently, source word and target word also belong to the same sense.\nConceptually, the IC method is based on the confluence of two evidences. Let us take our dictionaries as examples. If two or more pivot words share a translation t in the Des\u2192en dictionary (|tr(tc, Des\u2192en| > 1) (e.g. grifo\u2192tap, grifo\u2192faucet) we could hypothesize that they are lexical variants belonging to a unique sense c. If an entry s includes those translations (|tr(sc, Deu\u2192en)| > 1) (e.g. iturri\u2192tap, iturri\u2192faucet)) in the Deu\u2192en dictionary, we could also hypothesize the same. We can conclude that entry s and candidate t are mutual translations because the hypothesis that \u201cfaucet\u201d and \u201ctap\u201d are lexical variants of the same sense c is contrasted against two evidences. This makes IC highly dependant on the number of lexical variants. Specifically, IC needs several lexical variants in the pivot language per each entry sense in both dictionaries. Assuming that wrong pairs cannot fulfill this requirement (see Formula 2) we can estimate the probabilities of the conditions for solving an ambiguous pair (s, t) where s and t \u2208 c, as follows:\n(a) p(|tr(sc, Da\u2192b)| > 1): Estimated by computing the average coverage of lexical variants in the pivot language for each entry in Da\u2192b.\n(b) p(|tr(tc, Dc\u2192b)| > 1): Estimated by computing the average coverage of lexical variants in the pivot language for each entry in Dc\u2192b.\n(c) p(|tr(sc, Da\u2192b) \u22c2 tr(tc, Dc\u2192b)| > 1):\nConvergence degree between translations of s and t in Da\u2192b and Dc\u2192b corresponding to c.\nSo, in order to obtain a good performance with IC, the dictionaries used need to provide a high coverage of lexical variants per sense in the pivot language. If we assume that variants of a sense do not vary considerably between dictionaries, performance of IC in terms of recall would be estimated as follows:\nR = p(|tr(sc, Da\u2192b)| > 1) \u2217 p(|tr(tc, Dc\u2192b)| > 1) (3)\nWe estimated the adequacy of the different dictionaries in the experimental setup according to estimations (a) and (b). Average coverage of lexical variants in the pivot language was calculated for both dictionaries. It was possible because lexical variants in the target language were grouped according to senses in both dictionaries. Only ambiguous entries were analyzed because they are the set of entries which IC must solve. In the Deu\u2192en dictionary more than 75% of senses have more than one lexical variant in the pivot language. So, p(|tr(sc, Deu\u2192en)| > 1) = 0.75. In Des\u2192en this percentage (23%) is much lower. So, p(|tr(tc, Des\u2192en)| > 1) = 0.23. Therefore, Deu\u2192en dictionary is more suited to the IC method than Des\u2192en. As the conditions must be met in the maximum of both dictionaries, performance according to Formula 3 would be: 0.75 \u2217 0.23 = 0.17. This means that IC alone could solve about 17% of ambiguous entries."
        },
        {
            "heading": "5.2 Distributional Similarity",
            "text": "DS has been used successfully for extracting bilingual terminology from comparable corpora. The underlying idea is to identify as translation equivalents those words which show similar distributions or contexts across two corpora of different languages, assuming that this similarity is proportional to the semantic distance. In other words, establishing an equivalence between cross lingual semantic distance and translation probability. This technique can be used for pruning wrong translations produced in a pivot-based dictionary building process (Kaji et al., 2008; Gamallo and Pichel, 2010).\nWe used the traditional approach to compute DS (Fung, 1995; Rapp, 1999). Following the \u201cbag-of-words\u201d paradigm, the contexts of a word w\nare represented by weighted collections of words. Those words are delimited by a window (\u00b15 words around w) and punctuation marks. The context words are weighted with regard to w according to the Log-likelihood ratio measure, and the context vector ofw is formed. After representing word contexts in both languages, the algorithm computes for each source word the similarity between its context vector and all the context vectors corresponding to words in the target language by means of the cosine measure. To be able to compute the cross-lingual similarity, the context vectors are put in the same space by translating the vectors of the source words into the target language. This is done by using a seed bilingual dictionary. The problem is that we do not have that bilingual dictionary, since that is precisely the one we are trying to build. We propose that dictionaries extracted from our noisy dictionary (Deu\u2192en\u2192es) be used:\n\u2022 Including the unambiguous entries only\n\u2022 Including unambiguous entries and selecting the most frequent candidates according to the target language corpus for ambiguous entries\n\u2022 The dictionary produced by the IC1 method\nThe second method performed better in the tests we carried out. So, that is the method implemented for the experiments in the next section.\nDS calls for several conditions in order to perform well. For solving an ambiguous translation t of a source word s, both context representations must be accurate. The higher their frequency in the comparable corpus, the richer their context representation will be. In addition to context representation, the translation quality of contexts is also a critical factor for the performance of DS. Factors can be formulated as follows if we assume big and highly comparable corpora:\n(a) Precision of context representation: this can be estimated by computing the frequency of the words\n(b) Precision of translation process: this can be estimated by computing the quality of the seed dictionary"
        },
        {
            "heading": "6 Results",
            "text": "In order to evaluate the performance of each pruning method, the quality of the translations was measured according to the average precision and recall of translations per entry with respect to the reference dictionary. As we were not interested in dealing with missing translations, the reference for calculating recall was drawn up with respect to the intersection between the merged dictionary (Deu\u2192en\u2192es) and the reference dictionary (Deu\u2192es). F-score is the metric that combines both precision and recall.\nWe also introduced the frequency of use of both entry and pair as an aspect to take into account in the analysis of the results. It is better to deal effectively with frequent words and frequent translations than rare ones. Frequency of use of Basque words and frequency of source-target translation equivalent pairs were extracted respectively from the open domain monolingual corpus and the parallel corpus described in the previous section. Corpora were lemmatized and POS tagged in both cases in order to extract the frequency information of the lemmas."
        },
        {
            "heading": "6.1 Inverse Consultation",
            "text": "Results show that IC precision is about 0.6 (See Figure 2). This means that many wrong pairs fulfill IC conditions. After analyzing the wrong pairs by hand, we observed that some of them corresponded to correct pairs not included in the reference dictionary. They are not included in\nthe reference because not all synonyms -or lexical variants- are included in it, only the most common ones. This is an inherent problem in automatic evaluation, and affects all the experiments presented throughout section 6 equally. Other wrong pairs comprise translation equivalents which have the same stem but different gramatical categories (e.g., \u2019aldakuntza\u2019 (noun) (change, shift) \u2192 \u2019cambiar\u2019 (verb) (to change, to shift)). These wrong cases could be filtered if POS information would be available in the source dictionaries.\nPrecision is slightly better when dealing with frequent words, a maximum of 0.62 is reached when minimum frequency is between 150 and 2,000. Precision starts to decline significantly when dealing\nwith those entries over a minimum frequency of 10,000. However, only very few entries (234) reach that minimum frequency.\nRecall is about 0.2 (See Figure 3), close to the estimation computed in section 5.1. It presents a more marked variability according to the frequency of entries, improving the performance as the frecuency increases. This could be due to the fact that frequent entries tend to have more translation variants (See Table 3). The fact that there are too many candidates to solve would explain why the recall starts to decline when dealing with very frequent entries.\nGlobal performance according to F-score reflects the variability depending on frequency (See Figure 4).\nRecall according to frequency of pairs provides information about whether IC selects rare translations or the most probable ones (See Figure 5). It must be noted that this recall is calculated with respect to the translation pairs of the merged dictionary Deu\u2192en\u2192es which appear in the parallel corpus (see section 4.1). Results (See Figure 5) show that IC deals much better with frequent translation pairs. However, recall for pairs whose frequency is higher than 100 only reaches 0.5. Even if the maximum recall is achieved for pairs whose frequency is above 40,000, it is not significant because they suppose a minimum number (3 pairs). In short, we can conclude that IC often does not find the most probable translation\n(e.g. \u2019usain\u2019\u2192\u2019olor\u2019 (smell), \u2019zulo\u2019\u2192\u2019agujero\u2019 (hole),...)."
        },
        {
            "heading": "6.2 Distributional Similarity",
            "text": "DS provides an idea of semantic distance. However, in order to determine whether a candidate is a correct translation, a minimum threshold must be established. It is very difficult to establish a threshold manually because its performance depends on the characteristics of the corpora and the seed dictionaries. The threshold can be applied at a global level, by establishing a numeric threshold for all candidates, or at local level by selecting certain top ranked candidates for each entry. The dictionary created by IC or unambiguous pairs can be used as a reference for tuning the threshold in a robust way with respect to the evaluation score such as F-score. In our experiments, thresholds estimated against the dictionary created by IC are very close to those calculated with respect to the whole reference dictionary (see Figure 6).\nThere is not much variation in performance between local and global thresholds. Precision increases from 0.4 to 0.5 depending on the strictness level of the threshold (See Figure 2), the stricter the better. In all cases, precision is slightly better when dealing with frequent words (frequency > 20). This improvement is more marked with the strictest thresholds (TOP1, 0.1). However, if global thresholds are used, performance starts to decline significantly when dealing with words whose frequency is above 1,000. So, it seems that local thresholds (TOP3) perform more consistendly with respect to the high frequencies of entries.\nRecall (See Figure 3) goes from 0.5 to 0.7 depending on the strictness level of the threshold. It starts declining when frequency is above 50\ndepending on the type of threshold. In this case, global thresholds seem to perform better because the most frequent entries are handled better. These entries tend to have many translations. Therefore thresholds based on top ranks are too rigid.\nThere is no significant difference between global and local thresholds in terms of F-Score (See Figure 4). Each threshold type is more stable in precision or recall. So the F-Score is similar for both. Variability of F-Score according to frequency is lower than in precision and recall. As performance peaks on both measures at different points of frequency, the variability is mitigated when measures are combined by F-Score.\nWe have plotted the recall according to the frequency of pairs calculated from a parallel corpus in order to analyze the performance of DS when dealing with frequent translation pairs (See Figure 5). The performance decreases when dealing with pairs whose frequency is higher than 100. This means that DSs performance is worse when dealing with the most common translation pairs. So it is clear that it is very difficult to represent the contexts of very frequent words correctly.\nThe results show that DS rankings are worse when dealing with some words above a certain frequency threshold (e.g. \u2019on\u2019 \u2019good\u2019, \u2019berriz\u2019 \u2019again\u2019, \u2019buru\u2019 \u2019head\u2019, \u2019orain\u2019 \u2019now\u2019...). Although context representation of frequent words is based on many evidences, high polysemy level related to high frequency leads to a poorer representation. Alternatively we found that some of those frequent words are not very polysemous. Those words do not have strong collocates, that is, they tend to appear freely in contexts, which also leads to poor representation. This low quality representation hampers an accurate computation of semantic distance."
        },
        {
            "heading": "6.3 Comparison between IC and DS",
            "text": "As for average precision, IC provides better results than DS if all entries are taken into account. However, DS tips the scales in its favor if only entries with frequencies above 50 are considered and strict thresholds are used (TOP1, 0.1).\nDS clearly outperforms IC in terms of average recall of translations. Even if strict thresholds are used, DS outperforms IC for all entries whose\nfrequency is lower than 640. If average precision and recall are evaluated together by means of F-score, DS outperforms IC (Figure 4). Only when dealing with very frequent entries (frequency > 8, 000) is ICs performance close to DSs, but these entries make up a very small group (234 entries).\nIn order to compare the recall with respect to the frequency of translation pairs under the same conditions, we have to select a threshold that provides a similar precision to IC. TOP1 is the most similar one (see figure 2). As Figure 5 shows, again DS is better than IC. Even if IC\u2019s recall clearly surpasses DS\u2019s when dealing with frequent translation pairs (frequency > 2, 560), it only represents a minimal number of pairs (39)."
        },
        {
            "heading": "6.4 Combining IC and DS according to different scenarios",
            "text": "In order to see how the methods can complement each other, we calculated the performance for solving ambiguous entries obtained by combining the results of both methods using various alternatives:\n\u2022 Union: IC \u222a DS: Pairs obtained by both methods are merged. Duplicated pairs are cleaned.\n\u2022 Lineal combination (Lcomb): IC \u2217 k + DS \u2217 (1 \u2212 k). Each method provides a value representing the translation score. For IC that value is the number of pivot words (see Formula 1), and the context similarity score in the case of DS. Those values are linearly combined and applied over the noised dictionary.\nAs mentioned in the first section, one of the goals of the paper was to analyze which method and which combination was best depending on the use case. We have selected some measures which are a good indicator of good performance for different use cases:\n\u2022 AvgF : Average F-score per entry.\n\u2022 wAvgF : Average F-score per entry weighted by the frequency of the entry. Higher frequency increases the weight.\n\u2022 AvgF2: Average F-score per entry where recall is weighted higher.\n\u2022 AvgF0.5: Average F-score per entry where precision is weighted higher.\nFor the use cases presented in section 1, some measures will provide richer information than others. On the one hand, if we aim to build small, accurate dictionaries, AvgF0.5 would be a better indicator since it attaches more importance to high precision. In addition, if we want the dictionaries to cover the most common entries (e.g., in a basic dictionary for language learners) it is also interesting to look at wAvgF values because greater value is given to finding translations for the most frequent words. On the other hand, if our objective is to build big dictionaries with a high recall, it would be better to look at AvgF2 measure which attaches importance to recall.\nTable 3 shows the results for the different combinations. The parameters of all methods are optimized for each metric (as explained in section 6.2, see figure 6). In all cases, the combinations surpass the results of both methods separately. There is a reasonable improvement over DS (10.6% for AvgF ), and an even more startling one over IC (52.9% for AvgF ). IC only gets anywhere near the other methods when precision is given priority (AvgF0.5). There is no significant difference in terms of performance between the two combinations, although Lcomb is slightly better. wAvgF measure is stricter than the others since it takes frequency of entries into account. This is emphasised more in the case of IC where results decrease notably compared with AvgF ."
        },
        {
            "heading": "7 Conclusions",
            "text": "This paper has analyzed IC and DS, for the task of pruning wrong translations from bilingual dictionaries built by means of pivot techniques. After analyzing their strong and weak points we have showed that IC requires high ambiguity level dictionaries with several lexical variants per entry sense. With an average ambiguity close to 2 translation candidates DS obtains better results. IC is a high precision method, but contrary to our expectations, it seems that it is not much more precise than DS. In addition, DS offers much better recall of translations and entries. As a result, DS performs the best if both precision and recall are taken into account by F-score.\nBoth methods prune most probable translations for a significant number of frequent entries. DS encounters a problem when dealing with very frequent words due to the difficulty in representing their context. The main reason behind this is the high polysemy level of those words.\nOur initial beliefs were that the translations found by each method would diverge to a certain extent. The results obtained when combining the two methods show that although the performance does not increase as much as expected (10.6% improvement over DS), there is in fact some divergence. As for the different use cases proposed, combinations offer the best performance in all cases. IC is indeed the poorer method, although it presents competitive results when precision is given priority.\nFuture experiments include contrasting these results with other dictionaries and language pairs."
        },
        {
            "heading": "8 Aknowledgments",
            "text": "This work has been partially founded by the Industry Department of the Basque Government under grants IE09-262 (Berbatek project) and SA-2010/00245 (Pibolex+ project)."
        }
    ],
    "title": "Analyzing Methods for Improving Precision of Pivot Based Bilingual Dictionaries",
    "year": 2011
}