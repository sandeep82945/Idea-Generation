{
    "abstractText": "Background noise considerably reduces the accuracy and reliability of speaker verification (SV) systems. These challenges can be addressed using a speech enhancement system as a front-end module. Recently, diffusion probabilistic models (DPMs) have exhibited remarkable noisecompensation capabilities in the speech enhancement domain. Building on this success, we propose Diff-SV, a noise-robust SV framework that leverages DPM. Diff-SV unifies a DPM-based speech enhancement system with a speaker embedding extractor, and yields a discriminative and noise-tolerable speaker representation through a hierarchical structure. The proposed model was evaluated under both in-domain and out-of-domain noisy conditions using the VoxCeleb1 test set, an external noise source, and the VOiCES corpus. The obtained experimental results demonstrate that Diff-SV achieves state-of-the-art performance, outperforming recently proposed noise-robust SV systems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ju-ho Kim"
        },
        {
            "affiliations": [],
            "name": "Jungwoo Heo"
        },
        {
            "affiliations": [],
            "name": "Hyun-seo Shin"
        },
        {
            "affiliations": [],
            "name": "Chan-yeong Lim"
        },
        {
            "affiliations": [],
            "name": "Ha-Jin Yu"
        }
    ],
    "id": "SP:110b4c89cdf4b7a567a48ba7312043cebe1fca44",
    "references": [
        {
            "authors": [
                "Ehsan Variani",
                "Xin Lei",
                "Erik McDermott",
                "Ignacio Lopez Moreno",
                "Javier Gonzalez-Dominguez"
            ],
            "title": "Deep neural networks for small footprint text-dependent speaker verification",
            "venue": "2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014, pp. 4052\u20134056.",
            "year": 2014
        },
        {
            "authors": [
                "David Snyder",
                "Daniel Garcia-Romero",
                "Gregory Sell",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "X-vectors: Robust dnn embeddings for speaker recognition",
            "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "Brecht Desplanques",
                "Jenthe Thienpondt",
                "Kris Demuynck"
            ],
            "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
            "venue": "arXiv preprint arXiv:2005.07143, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "Ju-ho Kim",
                "Hye-jin Shim",
                "Jungwoo Heo",
                "Ha-Jin Yu"
            ],
            "title": "Rawnext: Speaker verification system for variable-duration utterances with deep layer aggregation and extended dynamic scaling policies",
            "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7647\u20137651.",
            "year": 2022
        },
        {
            "authors": [
                "Matthias W\u00f6lfel",
                "John McDonough"
            ],
            "title": "Distant speech recognition",
            "year": 2009
        },
        {
            "authors": [
                "Danwei Cai",
                "Weicheng Cai",
                "Ming Li"
            ],
            "title": "Within-sample variability-invariant loss for robust speaker recognition under noisy environments",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6469\u20136473.",
            "year": 2020
        },
        {
            "authors": [
                "Oldrich Plchot",
                "Lukas Burget",
                "Hagai Aronowitz",
                "Pavel Matejka"
            ],
            "title": "Audio enhancing with dnn autoencoder for speaker recognition",
            "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5090\u20135094.",
            "year": 2016
        },
        {
            "authors": [
                "Ond\u0159ej Novotn\u1ef3",
                "Old\u0159ich Plchot",
                "Ond\u0159ej Glembek",
                "Luk\u00e1\u0161 Burget"
            ],
            "title": "Analysis of dnn speech signal enhancement for robust speaker recognition",
            "venue": "Computer Speech & Language. 2019, vol. 58, pp. 403\u2013421, Elsevier.",
            "year": 2019
        },
        {
            "authors": [
                "Ju-ho Kim",
                "Jungwoo Heo",
                "Hye-jin Shim",
                "Ha-Jin Yu"
            ],
            "title": "Extended u-net for speaker verification in noisy environments",
            "venue": "INTERSPEECH, 2022, pp. 590\u2013594.",
            "year": 2022
        },
        {
            "authors": [
                "Simon Leglaive",
                "Xavier Alameda-Pineda",
                "Laurent Girin",
                "Radu Horaud"
            ],
            "title": "A recurrent variational autoencoder for speech enhancement",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 371\u2013375.",
            "year": 2020
        },
        {
            "authors": [
                "Meet H Soni",
                "Neil Shah",
                "Hemant A Patil"
            ],
            "title": "Time-frequency masking-based speech enhancement using generative adversarial network",
            "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5039\u20135043.",
            "year": 2018
        },
        {
            "authors": [
                "Martin Strauss",
                "Bernd Edler"
            ],
            "title": "A flow-based neural network for time domain speech enhancement",
            "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5754\u20135758.",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "ICML. PMLR, 2015, pp. 2256\u20132265.",
            "year": 2015
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems, 2021, vol. 34, pp. 8780\u20138794.",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems, 2020, vol. 33, pp. 6840\u20136851.",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Ju Lu",
                "Yu Tsao",
                "Shinji Watanabe"
            ],
            "title": "A study on speech enhancement based on diffusion probabilistic model",
            "venue": "2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2021, pp. 659\u2013666.",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Ju Lu",
                "Zhong-Qiu Wang",
                "Shinji Watanabe",
                "Alexander Richard",
                "Cheng Yu",
                "Yu Tsao"
            ],
            "title": "Conditional diffusion probabilistic model for speech enhancement",
            "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7402\u20137406.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "ICLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Arsha Nagrani",
                "Joon Son Chung",
                "Andrew Zisserman"
            ],
            "title": "Voxceleb: A large-scale speaker identification dataset",
            "venue": "INTERSPEECH. 2017, pp. 2616\u20132620, ISCA.",
            "year": 2017
        },
        {
            "authors": [
                "David Snyder",
                "Guoguo Chen",
                "Daniel Povey"
            ],
            "title": "Musan: A music, speech, and noise corpus",
            "venue": "arXiv preprint arXiv:1510.08484, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Yibo Wu",
                "Longbiao Wang",
                "Kong Aik Lee",
                "Meng Liu",
                "Jianwu Dang"
            ],
            "title": "Joint feature enhancement and speaker recognition with multi-objective task-oriented network",
            "venue": "INTERSPEECH, 2021, pp. 1089\u20131093.",
            "year": 2021
        },
        {
            "authors": [
                "Suwon Shon",
                "Hao Tang",
                "James Glass"
            ],
            "title": "Voiceid loss: Speech enhancement for speaker verification",
            "venue": "INTERSPEECH, 2019, pp. 2888\u20132892.",
            "year": 2019
        },
        {
            "authors": [
                "Yao Sun",
                "Hanyi Zhang",
                "Longbiao Wang",
                "Kong Aik Lee",
                "Meng Liu",
                "Jianwu Dang"
            ],
            "title": "Noise-disentanglement metric learning for robust speaker verification",
            "venue": "2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.",
            "year": 2023
        },
        {
            "authors": [
                "Vadim Popov",
                "Ivan Vovk",
                "Vladimir Gogoryan",
                "Tasnima Sadekova",
                "Mikhail Kudinov"
            ],
            "title": "Grad-tts: A diffusion probabilistic model for text-to-speech",
            "venue": "ICML. PMLR, 2021, pp. 8599\u20138608.",
            "year": 2021
        },
        {
            "authors": [
                "Diganta Misra"
            ],
            "title": "Mish: A self regularized non-monotonic activation function",
            "venue": "BMVC. 2020, BMVA Press.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 2017, vol. 30.",
            "year": 2017
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "CVPR, 2019, pp. 4690\u20134699.",
            "year": 2019
        },
        {
            "authors": [
                "Guoning Hu",
                "DeLiang Wang"
            ],
            "title": "A tandem algorithm for pitch estimation and voiced speech segregation",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing. 2010, vol. 18, pp. 2067\u20132079, IEEE.",
            "year": 2010
        },
        {
            "authors": [
                "Colleen Richey",
                "Maria A. Barrios",
                "Zeb Armstrong",
                "Chris Bartels",
                "Horacio Franco",
                "Martin Graciarena",
                "Aaron Lawson",
                "Mahesh Kumar Nandwana",
                "Allen Stauffer",
                "Julien van Hout",
                "Paul Gamble",
                "Jeffrey Hetherly",
                "Cory Stephenson",
                "Karl Ni"
            ],
            "title": "Voices Obscured in Complex Environmental Settings (VOiCES) Corpus",
            "venue": "INTERSPEECH, 2018, pp. 1566\u20131570.",
            "year": 2018
        },
        {
            "authors": [
                "Sashank J Reddi",
                "Satyen Kale",
                "Sanjiv Kumar"
            ],
            "title": "On the convergence of adam and beyond",
            "venue": "ICLR, 2018.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 speaker verification, noisy environment, feature enhancement, diffusion probabilistic models"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Speaker verification (SV) involves determining whether the speaker of a given utterance matches an authorized identity. Although recent advances in deep learning have yielded highly accurate deep neural network-based SV systems in clean and controlled environments [1\u20134], the performances of these systems degrade significantly under noisy, real-world conditions. Background noise diminishes speech intelligibility and quality, consequently hindering the extraction of accurate speaker representations [5,6]. Prior research has incorporated speech enhancement models as front-end modules in SV systems to mitigate the negative effects of noise [7\u20139].\nIn the speech enhancement field, generative models such as variational autoencoders [10], generative adversarial networks [11], and flowbased models [12] are widely used to generate clean speech from noisy speech. Recently, diffusion-based generative models have exhibited outstanding generation capabilities with respect to traditional generative models [13, 14]. As a representative approach, the denoising diffusion probabilistic model (DDPM) learns to predict Gaussian noise added to original data through a series of steps (i.e., forward diffusion process) and generates data by denoising random Gaussian noise iteratively via the Markov chain property using the trained model (i.e., reverse diffusion process) [15]. Leveraging the DDPM\u2019s potent data generation capabilities, researchers have proposed speech enhancement systems that adopt diffusion principles. For instance, Lu et al. [16] recovered clean speech from noisy speech using the DDPM\u2019s Markov chain process. Furthermore, CDiffuSE [17] was developed for non-Gaussian real-world noise adaptation by incorporating noisy speech into the forward and reverse processes of DDPM.\n\u2020Corresponding author. This work was supported by the National Research Foundation of Korea(NRF)\ngrant funded by the Korea government. (MSIT) (2023R1A2C1005744)\nDespite its exceptional noisy-compensation capabilities, the direct application of a DDPM-based speech enhancement model may not be ideal for SV. Due to the stochastic generation process, DDPMs tend to produce low-consistency data (i.e., different data from the same input) [15,18]. Consequently, speaker embedding extractors may yield representations with increased and decreased intra- and inter-class variances, respectively, ultimately degrading SV performance. Furthermore, the numerous reverse steps owing to the Markov chain entail a considerable computational cost [18], complicating the joint learning process with the SV system. These challenges can be alleviated using a score-based generation model [19] (refer to the Section 2 for details). In comparison with the stepwise prediction process of the DDPM, the score-based diffusion probabilistic model (DPM) generates data using fewer steps by numerically solving differential equations in continuous-condition sampling procedures. In addition, by employing an ordinary differential equation (ODE) solver instead of a stochastic differential equation (SDE) solver, deterministic data sampling can be achieved with minimal distribution perturbation.\nIn this study, we propose a unified noise-robust SV framework named Diff-SV, which applies a score-based DPM as a front-end module of a speaker embedding extractor. Diff-SV uses not only a score-based DPM (denoiser) solely, but also an auxiliary enhancement system (enhancer) to remove noise from the input. Therefore, the denoiser only needs to remove residual noise contained in the features derived from the enhancer, allowing for highly stable and effective data sampling. Furthermore, the proposed framework combines the original noisy spectrogram, the enhanced feature from the enhancer, and the denoised feature from the denoiser to extract speaker embeddings. By simultaneously leveraging features with different characteristics, this hierarchical approach encourages embedding extraction that is robust to distributional perturbation and informative.\nVoxCeleb1 [20] was used for model training, and this dataset was augmented with the MUSAN dataset [21]. We evaluated the generalizability of the model under in- and out-of-domain noise conditions using external data. Our proposed Diff-SV outperformed recent SV systems, demonstrating the efficacy of the framework through ablation experiments and visualizations."
        },
        {
            "heading": "2. SCORE-BASED DIFFUSION PROBABILISTIC MODELS",
            "text": "DPMs [13] have demonstrated promising results in the field of generative modeling by training the model to reverse data from noise. Recently, Song et al. [19] introduced a generalized framework for DPMs, employing SDEs and score, the gradient of the probability density function (p) for the data z. The score can be simply expressed as follows:\n\u2207zlogp(z). (1)\nConsidering z0\u223cp0 and zT \u223cpT as the data and prior probability distributions, respectively, the forward diffusion process with a continuous time variable t\u2208 [0,T ] is formulated as a solution to the following SDE:\ndzt=f(zt,t)dt+g(t)dwt, (2)\nar X\niv :2\n30 9.\n08 32\n0v 2\n[ ee\nss .A\nS] 1\n4 D\nec 2\n02 3\nwhere wt represents the standard Wiener process, also known as Brownian motion. The drift coefficient for zt and t is denoted as f(zt,t), and g(t) signifies the diffusion coefficient. The infinitesimal timesteps close to 0 are designated as dt. From zT , the sample z0 can be recovered via the reverse diffusion process (i.e., data sampling or data generation) as follows:\ndzt= [ f(zt,t)\u2212g(t)2\u2207zt logpt(zt) ] dt+g(t)dw\u0304, (3)\nIn this equation, w\u0304 indicates a standard Wiener process, with time changing from T to 0. As accurately computing \u2207zt logpt(zt) is difficult during the reverse process, score-based DPMs train a score estimator s\u03b8(zt,t) to predict the score (i.e., score matching) for data sampling."
        },
        {
            "heading": "3. PROPOSED FRAMEWORK",
            "text": "In this section, we introduce Diff-SV, the proposed noise-robust SV framework that uses a score-based DPM, and the overview is illustrated in Fig. 1 (a). The architecture of Diff-SV includes three primary components, namely, an enhancer, a denoiser, and an extractor. Initially, the enhancer pre-processes the original Mel-spectrogram by reducing noise. Subsequently, the DPM-based denoiser further refines the enhanced feature. Lastly, the extractor produces speaker embeddings by hierarchically processing features obtained from previous stages. All modules are trained in a unified approach."
        },
        {
            "heading": "3.1. Enhancer",
            "text": "Lu et al. [16] employed a DPM-based speech enhancement system that uses the original noisy speech as the initial value in the reverse diffusion process to produce a denoised speech. However, we hypothesized that preliminary enhanced features can closely resemble to clean speech, and thus, allowing the DPM to sample denoised features more reliably compared to using the original noisy input. This assumption was similarly applied to the speech synthesis domain using DPM, which demonstrated impressive naturalness of the generated speech [25]. Therefore, Diff-SV utilizes an enhancer to obtain enhanced features for initializing the sampling process conducted in the denoiser, a DPM-based speech enhancement system.\nAs depicted in Fig. 1 (b), the enhancer (fenh) comprises a linear block containing two fully connected layers, a Mish activation [26], a dropout layer, and transformer blocks [27]. The goal of the enhancer is to extract the enhanced features (x\u0302) from the noisy Mel-spectrogram (x) as follows:\nx\u0302=fenh(x), x, x\u0302\u2208RL\u00d7F\u00d71\u00d7B, (4)\nwhere L, F , and B represent the feature length, feature frequency, and batch size, respectively. The enhancer is optimized to minimize the L2 distance between the output and the clean Mel-spectrogram (y).\nLenh= 1\nB B\u2211 i=1 ||yi\u2212x\u0302i||22. (5)"
        },
        {
            "heading": "3.2. Denoiser",
            "text": "The denoiser, which comprises a score-based DPM, removes any residual noise from the enhanced features. We designed the denoiser by transforming all data distributions of infinite-time-horizon forward diffusion to N(x\u0302,I), rather than N(0,I). This concept is inspired by the methods reported in [25], which generalizes the data distribution of diffusion processes. Therefore, the terminal condition zT can be considered as data sampled from a normal distribution with the enhancer\u2019s output (x\u0302) as the mean. We redefined the standard forward diffusion process (Equation (2)) as the following SDE:\ndzt= 1\n2 (x\u0302\u2212zt)\u03b2tdt+\n\u221a \u03b2tdwt, (6)\nwhere \u03b2t is the noise scheduling function [19]. Consequently, the denoiser can yield the denoised feature from the enhanced feature using the reverse diffusion process. To generate high-fidelity features for SV tasks, we employed ODEs with the random Wiener terms removed and formulated the sampling process as follows:\ndzt= 1\n2 (x\u0302\u2212zt\u2212s\u03b8(zt, t, x\u0302))\u03b2tdt, (7)\nAs \u2207zt logpt(zt) cannot be computed accurately in the reverse process, we estimated the score using s\u03b8. As shown in Fig. 1 (c), the structure of the score estimator is based on U-Net as reported in [25] and takes zt, t, and x\u0302 as input to predict the score corresponding to each time point t. Thus, the feature z0, which is deterministically derived from zT , can be used in the SV framework owing to less perturbation of the speaker distribution and fast sampling.\nTo train s\u03b8, we calculated the expectation by marginalizing over a tractable transition kernel. Given that p(zt|z0) follows a Gaussian distribution, the loss function for score estimation is as follows:\nLdif=Et\u223cU(0,T)Ez0\u223cp0E\u03f5t\u223cN(0,I)\u2225s\u03b8(zt, t, x\u0302)\u2212\u03c3 \u22121 t \u03f5t\u222522, (8) where \u03c3t= \u221a 1\u2212e\u2212 \u222b t 0\u03b2(s)ds."
        },
        {
            "heading": "3.3. Extractor",
            "text": "In the proposed framework, the ResNet-based extractor (fext) derives embedding (v) (Fig. 1 (d)). To configure a unified noise-robust SV framework, the original, enhanced, and denoised features are jointly fed to the extractor. However, we empirically observed that the output of Diff-SV failed to converge, possibly due to gradient exploding or vanishing problems caused by repeated Gaussian denoising operations during the denoiser\u2019s reverse process. Therefore, we delivered z0 after the stop gradient calculation operation (sg) to prevent the exploding or vanishing gradient of the denoiser from occurring by backpropagation through the extractor\u2019s objective function as follows:\nv=fext(x\u0303),\nx\u0303=[x, x\u0302, sg(z0)], x\u0303\u2208RL\u00d7F\u00d73\u00d7B, (9)\nwhere [\u00b7] denotes the concatenation of each element on the channel axis. Thus, using a combination of (i) noisy but non-destructive original features x, (ii) enhanced features x\u0302 considering the target task, and (iii) independently denoised features z0, which are almost similar to clean features, the extractor can enrich the information from different perspectives while mitigating the distortion of speaker information via speech enhancement. The embeddings were optimized to classify the speakers included in the training data using an additional linear layer (W ), and we employed the additive angular margin (AAM)-Softmax function [28].\nLspk=\u2212 1\nB B\u2211 i=1 log es\u00b7(cos(\u03b8ci,i+m)) es\u00b7(cos(\u03b8ci,i+m))+ \u2211 j=\u0338yi es\u00b7(cos(\u03b8j,i)) , (10)\nwhere \u03b8j,i is the angle between the j-th weight vector Wj and i-th embedding vector vi, and ci denotes the speaker label of vi. Additionally, the scaling factor (s) and margin (m) are set to 0.3 and 30, respectively.\nFinally, Diff-SV was trained to optimize the following three losses:\nL=Lenh+Ldif+Lspk. (11)"
        },
        {
            "heading": "4. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1. Datasets",
            "text": "The models were trained using VoxCeleb1 [20] training data, consisting of 1,211 speakers. We employed MUSAN corpus [21] to generate noise\ndata, which we divided into non-overlapping training and test subsets. Noisy data for training was constructed using the MUSAN training subset with a randomly selected signal-to-noise ratio (SNR) between 0\u201320, further augmented with room impulse response reverberation, pitch shift, and gain variations. We constructed three test conditions to evaluate the noise robustness of our proposed model from different perspectives. First, in-domain speech evaluation with in-domain noisy data was conducted by augmenting the VoxCeleb1 test set with the SNR values {0, 5, 10, 15, and 20} for each noise type in the MUSAN test subset. Although the noise sources are separated, the data distributions within the same corpus could be similar. Therefore, we organized an in-domain speech evaluation with out-of-domain noisy data using a separate noise source. We used the Nonspeech100 dataset [29] as the out-of-domain noise source and evaluated the system with the same configurations as those corresponding to the in-domain noise evaluation conditions. Finally, to verify the generalizability of the system, we conducted out-of-domain speech evaluation with out-of-domain noisy data. To achieve this, we used the VOiCES development and evaluation dataset. The VOiCES dataset [30] was collected at different distances and under various acoustic conditions using array microphones in rooms of different sizes. We evaluated the models based on the parameter that achieved the best performance in the clean VoxCeleb1 test scenario."
        },
        {
            "heading": "4.2. Implementation details",
            "text": "We inputted an 80-dimensional log Mel-spectrogram extracted by conducting a 512-point fast Fourier transform with a Hamming-window width of 25 ms and 10-ms hopping. Diff-SV was trained using the AMSGrad optimizer [31] with a mini-batch of 160. The initial learning rate (LR) was 1e-3, which was decreased to 1e-7 over four cycles for 320 epochs using a cosine LR scheduler. The models were compared based on their equal error rates (EER) obtained using the cosine similarity score and the minimum detection cost function (Cmindet ). The enhancer in the proposed framework consists of four transformer blocks with a hidden size of 80 dimensions, and the architecture of our extractor is identical to ResNet structures reported in [9], respectively. The baseline had the same structure as that of the extractor, except that the channel size of the first convolution layer was changed from 3 to 1 using only the original features as the input. Additional details are present under the experimental code at https://github.com/wngh1187/Diff-SV."
        },
        {
            "heading": "5. RESULTS AND DISCUSSION",
            "text": "Table 1 lists the in-domain evaluation results using the VoxCeleb1 test set and the MUSAN evaluation partition. The baseline surpasses the average results obtained from noise-robust SV systems developed in recent years This improvement can be attributed to the optimization of the advanced objective function (AAM-Softmax) and the application of various data augmentation techniques during training. Nevertheless, the baseline performance deteriorates under considerably noisy conditions (e.g., under babble and noise with an SNR of 0 dB), indicating the need for specialized noise-reduction approaches. Our proposed method, Diff-SV, demonstrates enhanced noise robustness with respect to that of the baseline across all evaluation scenarios. Diff-SV achieves a relative error reduction (RER) that is 14.29% higher while using fewer parameters than ExU-Net, the top-performing noise-robust system (4.55% vs. 3.9%).\nTable 2 presents the results of each model under out-of-domain noise evaluation with in-domain speech conditions. In contrast to in-domain noise evaluation results, ExU-Net showed superior performance with respect to the baseline, indicating the effectiveness of the noise compensation. Moreover, Diff-SV outperforms all models, achieving an average EER and Cmindet of 4.65% and 0.247, respectively. These results reveal the noise robustness of the proposed model under in- and out-of-domain noise scenarios.\nTable 3 displays the results obtained from each model using the VOiCES development and evaluation datasets. Diff-SV achieves RERs of 15.38% and 24.98% for each dataset with respect to the baseline due to the proposed noise-compensation modules. Based on the evaluation results obtained from the out-of-domain speech and noise datasets, we confirmed the outstanding generalizability of our proposed framework.\nAn ablation study was performed to evaluate the efficacy of each component of the proposed framework. The models were evaluated based on their corresponding EER and Cmindet under the same evaluation scheme of Table 1. The results are displayed in Table 4, in which system #1 corresponds to Diff-SV. System #2 signifies an architecture that employed a pre-trained enhancer and denoiser to train an extractor, rather than applying unified training. This approach is sub-optimal because the enhancer and denoiser are not specifically designed for SV tasks, making early stopping challenging. In addition, feeding only the denoised features into the extractor without applying the sg operation (system #3), instated of following the hierarchical input structure, leads to non-convergence. We suspect that this stems from the gradient vanishing or exploding in the repeated score matching operation during the reverse process of the denoiser. Furthermore, the results of systems #4 and #5 reveal that excluding the denoiser and enhancement loss significantly degrades the generaliza-\ntion performance. Therefore, these results emphasize the importance of integrating the components of Diff-SV with a hierarchical input structure.\nWe visualized the internal spectrograms of the Diff-SV framework to verify the denoising effectiveness. Fig. 2 presents the original spectrogram (a) of a randomly chosen speech from the VoxCeleb1 evaluation set, a noisy spectrogram (d) synthesized by adding music at an SNR of 0 dB, and the output features of the enhancer (b), (e) and denoiser (c), (f). As noise is introduced, the original utterance becomes contaminated with signals of varying shapes and frequency bands ((a) vs. (d)). Although an enhancer trained to map input to clean speech can effectively eliminate noise, simultaneous optimization for the target task results in over-smoothing (blurring) or collapse (horizontal lines) of low- and high-frequency information ((a) and (d) vs. (b) and (e)). Due to the difficulty of accurately removing Gaussian noise during the inversion process, the output of the denoiser retains more noise but exhibits better pitch information owing to the superior generalizability of DPM ((b) and (e) vs. (c) and (f)). Therefore, by simultaneously supplying features with complementary properties, Diff-SV is capable of producing speaker embeddings that are highly discriminative and noise-tolerant."
        },
        {
            "heading": "6. CONCLUSION",
            "text": "Based on the impressive generative capabilities of DPM, we propose a unified noise-robust SV framework that uses a DPM-based speech enhancement system. Using the ODE solver of the score-based DPM, DiffSV generates denoised features from the enhancer\u2019s output in an efficient and deterministic manner. Furthermore, the complementary hierarchical inputs improve the extractor\u2019s ability to derive discriminative and noiserobust speaker embeddings. Our proposed model outperforms recently reported models, including baselines, under both in- and out-of-domain scenarios. Additionally, we demonstrate the effectiveness of Diff-SV and the importance of each component through visualization and ablation analyses. We intend to explore the incorporation of other generative approaches to improve noise compensation capabilities in future studies."
        },
        {
            "heading": "7. REFERENCES",
            "text": "[1] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier Gonzalez-Dominguez, \u201cDeep neural networks for small footprint text-dependent speaker verification,\u201d in 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014, pp. 4052\u20134056.\n[2] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur, \u201cX-vectors: Robust dnn embeddings for speaker recognition,\u201d in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5329\u20135333.\n[3] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, \u201cEcapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\u201d in arXiv preprint arXiv:2005.07143, 2020.\n[4] Ju-ho Kim, Hye-jin Shim, Jungwoo Heo, and Ha-Jin Yu, \u201cRawnext: Speaker verification system for variable-duration utterances with deep layer aggregation and extended dynamic scaling policies,\u201d in 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7647\u20137651.\n[5] Matthias Wo\u0308lfel and John McDonough, Distant speech recognition, John Wiley & Sons, 2009.\n[6] Danwei Cai, Weicheng Cai, and Ming Li, \u201cWithin-sample variability-invariant loss for robust speaker recognition under noisy environments,\u201d in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6469\u20136473.\n[7] Oldrich Plchot, Lukas Burget, Hagai Aronowitz, and Pavel Matejka, \u201cAudio enhancing with dnn autoencoder for speaker recognition,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5090\u20135094.\n[8] Ondr\u030cej Novotny\u0300, Oldr\u030cich Plchot, Ondr\u030cej Glembek, Luka\u0301s\u030c Burget, et al., \u201cAnalysis of dnn speech signal enhancement for robust speaker recognition,\u201d in Computer Speech & Language. 2019, vol. 58, pp. 403\u2013421, Elsevier.\n[9] Ju-ho Kim, Jungwoo Heo, Hye-jin Shim, and Ha-Jin Yu, \u201cExtended u-net for speaker verification in noisy environments,\u201d in INTERSPEECH, 2022, pp. 590\u2013594.\n[10] Simon Leglaive, Xavier Alameda-Pineda, Laurent Girin, and Radu Horaud, \u201cA recurrent variational autoencoder for speech enhancement,\u201d in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 371\u2013375.\n[11] Meet H Soni, Neil Shah, and Hemant A Patil, \u201cTime-frequency masking-based speech enhancement using generative adversarial network,\u201d in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5039\u20135043.\n[12] Martin Strauss and Bernd Edler, \u201cA flow-based neural network for time domain speech enhancement,\u201d in 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5754\u20135758.\n[13] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli, \u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d in ICML. PMLR, 2015, pp. 2256\u20132265.\n[14] Prafulla Dhariwal and Alexander Nichol, \u201cDiffusion models beat gans on image synthesis,\u201d in Advances in neural information processing systems, 2021, vol. 34, pp. 8780\u20138794.\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel, \u201cDenoising diffusion probabilistic models,\u201d in Advances in neural information processing systems, 2020, vol. 33, pp. 6840\u20136851.\n[16] Yen-Ju Lu, Yu Tsao, and Shinji Watanabe, \u201cA study on speech enhancement based on diffusion probabilistic model,\u201d in 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2021, pp. 659\u2013666.\n[17] Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao, \u201cConditional diffusion probabilistic model for speech enhancement,\u201d in 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7402\u20137406.\n[18] Jiaming Song, Chenlin Meng, and Stefano Ermon, \u201cDenoising diffusion implicit models,\u201d in ICLR, 2021.\n[19] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole, \u201cScore-based generative modeling through stochastic differential equations,\u201d in ICLR, 2020.\n[20] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman, \u201cVoxceleb: A large-scale speaker identification dataset,\u201d in INTERSPEECH. 2017, pp. 2616\u20132620, ISCA.\n[21] David Snyder, Guoguo Chen, and Daniel Povey, \u201cMusan: A music, speech, and noise corpus,\u201d in arXiv preprint arXiv:1510.08484, 2015.\n[22] Yibo Wu, Longbiao Wang, Kong Aik Lee, Meng Liu, and Jianwu Dang, \u201cJoint feature enhancement and speaker recognition with multi-objective task-oriented network,\u201d in INTERSPEECH, 2021, pp. 1089\u20131093.\n[23] Suwon Shon, Hao Tang, and James Glass, \u201cVoiceid loss: Speech enhancement for speaker verification,\u201d in INTERSPEECH, 2019, pp. 2888\u20132892.\n[24] Yao Sun, Hanyi Zhang, Longbiao Wang, Kong Aik Lee, Meng Liu, and Jianwu Dang, \u201cNoise-disentanglement metric learning for robust speaker verification,\u201d in 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\n[25] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov, \u201cGrad-tts: A diffusion probabilistic model for text-to-speech,\u201d in ICML. PMLR, 2021, pp. 8599\u20138608.\n[26] Diganta Misra, \u201cMish: A self regularized non-monotonic activation function,\u201d in BMVC. 2020, BMVA Press.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, \u201cAttention is all you need,\u201d in Advances in neural information processing systems, 2017, vol. 30.\n[28] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in CVPR, 2019, pp. 4690\u20134699.\n[29] Guoning Hu and DeLiang Wang, \u201cA tandem algorithm for pitch estimation and voiced speech segregation,\u201d in IEEE Transactions on Audio, Speech, and Language Processing. 2010, vol. 18, pp. 2067\u20132079, IEEE.\n[30] Colleen Richey, Maria A. Barrios, Zeb Armstrong, Chris Bartels, Horacio Franco, Martin Graciarena, Aaron Lawson, Mahesh Kumar Nandwana, Allen Stauffer, Julien van Hout, Paul Gamble, Jeffrey Hetherly, Cory Stephenson, and Karl Ni, \u201cVoices Obscured in Complex Environmental Settings (VOiCES) Corpus,\u201d in INTERSPEECH, 2018, pp. 1566\u20131570.\n[31] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar, \u201cOn the convergence of adam and beyond,\u201d in ICLR, 2018."
        }
    ],
    "title": "DIFF-SV: A UNIFIED HIERARCHICAL FRAMEWORK FOR NOISE-ROBUST SPEAKER VERIFICATION USING SCORE-BASED DIFFUSION PROBABILISTIC MODELS",
    "year": 2023
}