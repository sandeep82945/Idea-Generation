Imagine you are a research scientist, read the following paper and generate top 5 possible future research ideas after brainstorming:  
``` To this date, the efficacy of the scientific publishing enterprise fundamentally rests on the strength of the peer review process. The journal editor or the conference chair primarily relies on the expert reviewers’ assessment, identify points of agreement and disagreement and try to reach a consensus to make a fair and informed decision on whether to accept or reject a paper. However, with the escalating number of submissions requiring review, especially in toptier Artificial Intelligence (AI) conferences, the editor/chair, among many other works, invests a significant, sometimes stressful effort to mitigate reviewer disagreements. Here in this work, we introduce a novel task of automatically identifying contradictions among reviewers on a given article. To this end, we introduce ContraSciView, a comprehensive review-pair contradiction dataset on around 8.5k papers (with around 28k review pairs containing nearly 50k review pair comments) from the open reviewbased ICLR and NeurIPS conferences. We further propose a baseline model that detects contradictory statements from the review pairs. To the best of our knowledge, we make the first attempt to identify disagreements among peer reviewers automatically. We make our dataset and code public for further investigations1. 1 introduction Despite being the widely accepted standard for validating scholarly research, the peer-review process has faced substantial criticism. Its perceived lack of transparency (Wicherts, 2016; Parker et al., 2018), sometimes being biased (Stelmakh et al., 2021, 2019), arbitrariness (Brezis and Birukou, 2020), inconsistency (Shah et al., 2018; Langford and Guzdial, 2015), being regarded as a poorly defined task (Rogers and Augenstein, 2020a) and failure to recognize influential papers (Freyne et al., 2010) have
1https://github.com/sandeep82945/ Contradiction-in-Peer-Review and https://www.iitp. ac.in/~ai-nlp-ml/resources.html#ContraSciView
all been subjects of concern within the scholarly community. The rapid increase in research article submissions across different venues has caused the peer review system to undergo a huge amount of stress (Kelly et al., 2014; Gropp et al., 2017). The role of an editor in scholarly publishing is crucial (Hames, 2012). Typically, editors or chairs have to manage a multitude of responsibilities. These include finding expert reviewers, assigning review tasks, mediating disagreements among reviewers, ensuring reviews are received on time, stepping in when reviewers are not responsive, making informed decisions, and maintaining communication with authors, among other duties. Simultaneously, they must ensure the validity and quality of the reviews to make an informed decision. However, the complexity of scholarly discourse and inherent subjectivity within research interpretation sometimes leads to conflicting views among peer reviewers (Bornmann and Daniel, 2009; Borcherds and Editor, 2017). For instance, in Figure 1, Reviewer 1 regards the evidence as both strong and sufficient, reinforcing the paper’s theoretical soundness. Conversely, Reviewer 2 remains skeptical of this evidence, underscoring their differing perspectives on the soundness of the paper. Feedback between authors and reviewers can help improve the peer review system (Rogers and Augenstein, 2020b). However, reviewer disagreement can create confusion for authors and editors as they try to address reviewer feedback. There are various guidelines or suggestions that editors consider in order to resolve the conflicting reviews (Borcherds and Editor, 2017). Given this volume, it
is challenging for editors to identify contradictions manually. Our current investigation can assist in pinpointing these discrepancies and help editors make informed decisions. It is to be noted that this AI-based system aims to aid editors in identifying potential contradictions in reviewer comments. While it provides valuable insights, it is not infallible. Editors should use it as a supplementary tool, understanding that not all contradictions may be captured and manual review remains crucial. They should make decision with careful analysis beyond the system’s recommendations. Our contributions are three-fold: 1) We introduce a novel task: identifying contradictions/disagreement within peer reviews. 2) To address the task, we create a novel labeled dataset of around 8.5k papers and 25k reviews. 3) We establish a baseline method as a reference point for further research on this topic. 2 related work Artificial Intelligence (AI) has been applied in recent years to the realm of scientific peer review with the goal of enhancing its efficacy and quality (Checco et al., 2021; Ghosal et al., 2022). These applications span a diverse range of tasks, including decision prediction (Kumar et al., 2022; Ghosal et al., 2019), rating prediction (Li et al., 2020; Kang et al., 2018a), sentiment analysis (Kumar et al., 2021; Chakraborty et al., 2020; Kang et al., 2018b), argument mining (Hua et al., 2019; Cheng et al., 2020), and review summarization (Xiong, 2013). In this work, we aim to broaden the scope of AI’s usefulness by assisting the editor in determining the disagreement between reviewers. Contradiction detection is not new. Alamri and Stevenson (2015) use linguistic features and support vector machines (SVMs) to detect contradictions in scientific claims, while Badache et al. (2018b) employ review polarity to predict contradiction intensity in reviews. Li et al. (2018) combine sentiment analysis with contradiction detection for Amazon reviews. Meanwhile, Lendvai and Reichel (2016) employ textual similarity features to classify contradictions in rumors, and Li et al. (2017) use contradiction-specific word embeddings for sentence pairs. Tan et al. (2019) propose a dual attention-based gated recurrent unit for conflict detection. Recent advances in Natural Language Inference (NLI) have brought forth a category of textual entailment, categorizing
text pairs into entailment, neutral, or contradiction (Chen et al., 2017; Gong et al., 2018). Well-known transformer-based models, such as BERT (Devlin et al., 2019) and its variants leading to state-of-theart performance. As far as we know, contradiction detection in peer review has never been studied. Contradiction detection in peer reviews is complex and requires domain knowledge of the subject. It is not straightforward to detect contradictions in peer reviews because reviewers often have different writing styles and approaches to commenting. We believe that our work can significantly contribute to the peer review process. 3 dataset  3.1 dataset collection We utilize a subset of 8,582 out of 8,877 papers from the extensive ASAP-Review dataset (Yuan et al., 2021). The dataset comprises reviews from the ICLR (2017-2020) and NeurIPS (2016-2019) conferences. Each review is labeled with eight aspect categories: (Motivation, Clarity, Soundness, Substance, Originality, Meaningful Comparison, Replicability, and Summary) along with their sentiment (Positive/Negative) except Summary. 3.2 dataset pre-processing We define review is as a collection of comments/sentences written by one reviewer. Formally, we can represent it as a list:
R = {cmt1, cmt2, . . .} A review pair takes two such lists, one from Reviewer1 and one from Reviewer2. It can be represented as:
RP = {R1, R2}
Lastly, a review pair comment selects one comment from each reviewer and forms a pair. It is a set of such pairs:
RPC = {(cmt from R1, cmt from R2), . . .} To make it easier to annotate, we first create pairs of reviews of papers. Suppose, if there are n number of reviews in a paper then we create ( n 2 ) pairs resulting in a total of around 28k pairs. Detailed statistics regarding this dataset can be found in Table 1. We follow the contradiction definition of Badache et al. (2018a). According to this definition, a contradiction exists between two review pairs when any review pair comments, denoted as ra1 and ra2, contain a common aspect category but convey opposite sentiments. Therefore, we categorize those review pairs as no contradiction if none of their comments share the same aspect with opposing sentiments. This labeling process resulted in 17,095 pairs of reviews. For the remaining review pairs, we compile a list of review pair comments that share the same aspect but express opposing sentiments, and we designate these for human annotation. Finally, we annotate a total of 50,303 pair of review pair comments. 4 annotating for contradiction  4.1 annotation guidelines We follow the contradiction definition by De Marneffe et al. (2008) for our annotation process where they define contradiction as: “ask yourself the following question: If I were shown two contemporaneous documents, one containing each of these passages, would I regard it as very unlikely that both passages could be true at the same time? If so, the two contradict each other”. We offer multiple examples from different aspect categories that may contain conflicting reviews to assist and direct the annotators. Figure 4 illustrates the flowchart that represents our annotation procedure. For the detailed annotation guidelines, please refer to Appendix A. 4.2 annotator training Given the complexity of the reviews and their frequent use of technical terminology, we had six doctoral students, each with four years of experience in scientific research publishing. To facilitate their training, two experts with more than ten years of experience in scientific publishing annotated 1,500 review pairs from a selection of random papers, following our guidelines. Our experts convened to discuss and reconcile any discrepancies in their annotations. The initial dataset comprises 227 pairs with contradictions and 1273 pairs without contradiction comments. We randomly selected 100
review pairs from this more extensive set to train our annotators, ensuring both classes are equally represented. Upon completion of this round of annotation, we reviewed and corrected any misinterpretations with the annotators, further refining their training and enhancing the clarity of the annotation guidelines. To evaluate the effectiveness of the initial training round, we compiled another 80 review pairs from both classes drawn from the remaining review pairs. From the second round on wards, most annotators demonstrated increased proficiency, accurately annotating at least 70% of the contradictory cases. 4.3 annotation process We regularly monitored the annotated data, placing emphasis on identifying and rectifying inconsistencies and cases of confusion. We also implemented an iterative feedback system that continuously aimed to refine and improve the annotation process. In cases of conflict or confusion, we consulted experts to make the final decision. Following the annotation phase, we obtained an average interannotator agreement score of 0.62 using Cohen’s kappa (Cohen, 1960), indicating a substantial consensus among the annotators. 4.4 annotator’s pay We compensated each annotator based on standard salaries in India, calculated by the hours they worked. The appointment and salaries are governed by the standard practices of our university. We chose not to pay per review pair because the time needed to understand reviews varies due to their complexity, technical terms, and the annotator’s familiarity with the topic. Some reviews are also extensive, requiring more time to comprehend. Hence, basing pay on review pairs could have compromised annotation quality. To ensure accuracy and prevent fatigue, we set a daily limit of 6 hours for annotators. 4.5 final dataset Figure 2 displays the annotation statistics. We observed that the majority of disagreements among reviewers pertain to the paper’s clarity. Such disagreements could stem from differences in reviewers’ expertise, domain knowledge (a reviewer unfamiliar with the domain might find it hard to grasp the content), language proficiency (some reviewers might struggle with English, while others are fluent), or interest level (a disinterested reader might
find the paper difficult to engage with). Discrepancies regarding replicability and meaningful comparison are notably fewer, likely because these topics are less frequently commented on. 5 baseline system description We describe the flow of our proposed baseline setup through the flowchart in Figure 3. The initial input involves a pair of reviews for a given paper, which are subjected to the Aspect Sentiment Model. This model classifies the aspect and sentiment of each review comment/sentence within the reviews. Subsequently, we identify pairs of comments that, while sharing the same aspects, exhibit differing sentiments; we term it as Sentiment Disparity Aspect Pair (SDAP). As a final step, the SDAPs are then passed to the Contradiction Classifier in order to classify whether these pairs of review comments are contradictory or not. We describe the Aspect Sentiment Model and Contradiction Detection Model in details as follows:
Aspect Sentiment Model: Aspect and sentiment in peer review have been studied as a multi-
task model (Kumar et al., 2021). A review pair comment can have different sentiments corresponding to multiple aspect categories. So, we utilize MultiInstance Multi-Label Learning Network (MIMLLN) (Li et al., 2020) which uniquely identifies sentences as bags and words as instances. It functions as a multi-task model, which performs both Aspect Category Detection (ACD) and Aspect Category Sentiment Analysis (ACSA). Given a sentence and its aspect categories, it predicts sentiment by identifying key instances, and aggregating these to obtain the sentence’s sentiment towards each category. We discuss MIMLLN in more detail in Appendix B. We trained MIMLLN on the humanannotated ASAP-Review dataset for our task. Reviewer Disagreement Classifier We use techniques from Natural Language Inference (NLI) sentence pair classification to identify reviewer disagreement, particularly contradictions from Sentence Dependency Pairs (SDPs). Unlike traditional NLI tasks that provide a three-category output of "entailment", "contradiction", and "neutral", we have adjusted the model to a two-category output system: "contradiction" and "non-contradiction". The latter category combines "entailment" and "neutral" labels, as our primary focus is on contradiction detection. 6 results and analysis We discuss the implementation details in Appendix C. Table 2 reports the performance of the models when trained on our dataset. The results are of the Reviewer Disagreement Classifier, which is the final step in our proposed approach. We report macro average P, R, and F1 scores as the rarity of the contradiction class is of particular interest for this task (Gowda et al., 2021). RoBERTa large outperforms XLNet large by 0.7 points, RoBERTa base by 4.1 points, XLNet by 4.5 points, and DistilBERT by 7.5 points with respect to F1 score. In order to analyze how models perform when trained on natural language inference datasets, we trained the models on the ANLI+ALL dataset and evaluated them on our test set. It was found that the models trained by combining datasets, i.e., SNLI, MNLI, FEVER, and ANLI A1+A2+A3, perform the best (Nie et al., 2020). We obtain the highest F1 score of 71.14 F1 with RoBERTa Large. The results show that all the models trained on our dataset outperform those trained on the existing datasets. This large increase in performance can
be attributed to the differences in reviews and the content of the existing datasets. Scientific peer review comments are written differently from typical human-written English sentences and Wikipedia entries. They contain a more technical style of writing, which is challenging for models to parse when trained on current datasets. Our findings, therefore, highlight the pressing need for innovative datasets in this specific domain. We discuss the results of our proposed intermediate aspect sentiment model in Section D.
Next, we evaluate the performance of RoBERTa Large across the entire process (in evaluation mode). We obtain an accuracy of 88.60% from the Aspect Sentiment Classifier (in determining whether a pair of reviews has any SDAP or not) and a 74.25 F1 score for the Reviewer Disagreement Classifier. We compare our findings with those achieved by the zero-shot Large Language Model, ChatGPT. On the test set, ChatGPT scored an F1 of 64.67 which is 9 points lower than the baseline model, likely due to its lack of explicit training for this specific downstream task. We discuss the prompts and outputs in details in the Appendix E.
We also discuss where our proposed baseline fails (Error Analysis) in the Appendix F. We also utilized the BARD API 2 for our evaluation. We provided identical prompts to both Google BARD and CHATGPT to ensure a fair comparison. We compared our findings with those achieved by Google BARD. BARD scored an F1 of 61.35 on the test set, 12 points lower than the baseline model. This is likely due to its BARD’s requirement for more specialized training for this specific task. We
2https://github.com/dsdanielpark/Bard-API
found that Google BARD registered a higher number of false positives compared to CHATGPT. 7 conclusion and future work In this work, we introduce a novel task to automatically identify contradictions in reviewers’ comments. We develop ContraSciView, a reviewpair contradiction dataset. We designed a baseline model that combines the MIMLLN framework and NLI model to detect contradictory review pair comments. We found that RoBERTa large performed best for this task with F1 score of 71.14. Our proposed framework doesn’t consider the full review context when predicting contradictions. Limitations
Our study mainly focuses on identifying “explicit” contradictions. Explicit Contradictions: These are clear, direct, and unmistakable contradictions that can be easily identified. For example: The author claims in the introduction that “X has been proven to be beneficial,” but in the discussion section, they state that “X has not been shown to have any benefit.” One reviewer says, “Figure 2 clearly shows the relationship between A and B,” while another reviewer comments, “Figure 2 does not show any clear relationship between A and B.”
We do not delve into “implicit” contradictions, which can be hard to detect and can be subjective, making them a topic of debate. Implicit Contradictions: These are more subtle and may require deeper understanding or closer examination to identify. It may require the annotators to read the paper and also learn many related works or things to annotate. They are not directly stated but can be inferred from the context or how information is presented. For example: Review 1: “the method lacks algorithmic novelty and the exposition of the method severely inhibits the reader from understanding the proposed idea .” Review 2: “the work presented is novel but there are some notable omissions - there are no specific numbers presented to back up the improvement claims graphs are presented but not specific numeric results - there is limited discussion of the computational cost of the framework presented - there is no comparison to a baseline in which the additional learning cycles used for learning the embedding are used for training the student model .” For instance, Review1 mentions that the method lacks algorithmic novelty, and Review2 acknowledges the work as a novel but points out some notable omissions. This difference in perception is not a direct contradiction, as one reviewer finds the method lacking in novelty, while the other recognizes it as novel but with some omissions. Additionally, we do not incorporate information from the papers being reviewed, as they cover a wide range of topics and would require many experts from various domains. ```
5 possible future research ideas from the paper are: 