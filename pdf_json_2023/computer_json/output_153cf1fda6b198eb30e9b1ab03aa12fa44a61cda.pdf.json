{
    "abstractText": "With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUMgenerated fixes are compared with developers\u2019 fixes on opensource projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Linyi Li"
        },
        {
            "affiliations": [],
            "name": "Yuhao Zhang"
        },
        {
            "affiliations": [],
            "name": "Luyao Ren"
        },
        {
            "affiliations": [],
            "name": "Yingfei Xiong"
        },
        {
            "affiliations": [],
            "name": "Tao Xie"
        }
    ],
    "id": "SP:27cbe11039217c61d7965ee65821f48cf5791490",
    "references": [
        {
            "authors": [
                "M. Abadi",
                "A. Agarwal",
                "P. Barham",
                "E. Brevdo",
                "Z. Chen",
                "C. Citro",
                "G.S. Corrado",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "I.J. Goodfellow",
                "A. Harp",
                "G. Irving",
                "M. Isard",
                "Y. Jia",
                "R. J\u00f3zefowicz",
                "L. Kaiser",
                "M. Kudlur",
                "J. Levenberg",
                "D. Man\u00e9",
                "R. Monga",
                "S. Moore",
                "D.G. Murray",
                "C. Olah",
                "M. Schuster",
                "J. Shlens",
                "B. Steiner",
                "I. Sutskever",
                "K. Talwar",
                "P.A. Tucker",
                "V. Vanhoucke",
                "V. Vasudevan",
                "F.B. Vi\u00e9gas",
                "O. Vinyals",
                "P. Warden",
                "M. Wattenberg",
                "M. Wicke",
                "Y. Yu",
                "X. Zheng"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems",
            "venue": "CoRR, vol. abs/1603.04467, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Albarghouthi"
            ],
            "title": "Introduction to neural network verification",
            "venue": "Foundations and Trends in Programming Languages, vol. 7, no. 1-2, pp. 1\u2013157, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Bengio",
                "N. L\u00e9onard",
                "A.C. Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "CoRR, vol. abs/1308.3432, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "N.D.Q. Bui",
                "Y. Yu",
                "L. Jiang"
            ],
            "title": "InferCode: Self-supervised learning of code representations by predicting subtrees",
            "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE. IEEE, 2021, pp. 1186\u20131197.",
            "year": 2021
        },
        {
            "authors": [
                "P. Cousot",
                "R. Cousot"
            ],
            "title": "Static determination of dynamic properties of generalized type unions",
            "venue": "ACM Conference on Language Design for Reliable Software, LDRS. ACM, 1977, pp. 77\u201394.",
            "year": 1977
        },
        {
            "authors": [
                "T. Gehr",
                "M. Mirman",
                "D. Drachsler-Cohen",
                "P. Tsankov",
                "S. Chaudhuri",
                "M. Vechev"
            ],
            "title": "AI2: safety and robustness certification of neural networks with abstract interpretation",
            "venue": "39th IEEE Symposium on Security and Privacy, SP. IEEE, 2018, pp. 3\u201318.",
            "year": 2018
        },
        {
            "authors": [
                "X. Glorot",
                "A. Bordes",
                "Y. Bengio"
            ],
            "title": "Deep sparse rectifier neural networks",
            "venue": "14th International Conference on Artificial Intelligence and Statistics, AISTATS. JMLR.org, 2011, pp. 315\u2013 323.",
            "year": 2011
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "3rd International Conference on Learning Representations, ICLR. OpenReview.net, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Guerriero",
                "R. Pietrantuono",
                "S. Russo"
            ],
            "title": "Operation is the hardest teacher: estimating DNN accuracy looking for mispredictions",
            "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE. IEEE, 2021, pp. 348\u2013358.",
            "year": 2021
        },
        {
            "authors": [
                "A. Gurfinkel",
                "T. Kahsai",
                "A. Komuravelli",
                "J.A. Navas"
            ],
            "title": "The SeaHorn verification framework",
            "venue": "27th International Conference on Computer Aided Verification, CAV. Springer, 2015, pp. 343\u2013361.",
            "year": 2015
        },
        {
            "authors": [
                "M. Hattori",
                "S. Sawada",
                "S. Hamaji",
                "M. Sakai",
                "S. Shimizu"
            ],
            "title": "Semi-static type, shape, and symbolic shape inference for dynamic computation graphs",
            "venue": "4th ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, 2020, pp. 11\u201319.",
            "year": 2020
        },
        {
            "authors": [
                "N. Humbatova",
                "G. Jahangirova",
                "G. Bavota",
                "V. Riccio",
                "A. Stocco",
                "P. Tonella"
            ],
            "title": "Taxonomy of real faults in deep learning systems",
            "venue": "42nd ACM/IEEE International Conference on Software Engineering, ICSE. IEEE, 2020, pp. 1110\u20131121.",
            "year": 2020
        },
        {
            "authors": [
                "N. Jay",
                "N. Rotman",
                "B. Godfrey",
                "M. Schapira",
                "A. Tamar"
            ],
            "title": "A deep reinforcement learning perspective on Internet congestion control",
            "venue": "36th International Conference on Machine Learning, ICML. PMLR, 2019, pp. 3050\u20133059.",
            "year": 2019
        },
        {
            "authors": [
                "E. Kloberdanz",
                "K.G. Kloberdanz",
                "W. Le"
            ],
            "title": "DeepStability: A study of unstable numerical methods and their solutions in deep learning",
            "venue": "44th International Conference on Software Engineering, ICSE. ACM, 2022, pp. 586\u2013597.",
            "year": 2022
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems 25, NIPS, 2012, pp. 1106\u20131114.",
            "year": 2012
        },
        {
            "authors": [
                "L. Li",
                "Z. Zhong",
                "B. Li",
                "T. Xie"
            ],
            "title": "Robustra: Training provable robust neural networks over reference adversarial space",
            "venue": "28th International Joint Conference on Artificial Intelligence, IJCAI. ijcai.org, 2019, pp. 4711\u20134717.",
            "year": 2019
        },
        {
            "authors": [
                "L. Li",
                "T. Xie",
                "B. Li"
            ],
            "title": "SoK: Certified robustness for deep neural networks",
            "venue": "44th IEEE Symposium on Security and Privacy, SP. IEEE, 2023, pp. 94\u2013115.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Li",
                "Y. Yuan"
            ],
            "title": "Convergence analysis of two-layer neural networks with ReLU activation",
            "venue": "Advances in Neural Information Processing Systems 30, NIPS, 2017, pp. 597\u2013607.",
            "year": 2017
        },
        {
            "authors": [
                "C. Liu",
                "J. Lu",
                "G. Li",
                "T. Yuan",
                "L. Li",
                "F. Tan",
                "J. Yang",
                "L. You",
                "J. Xue"
            ],
            "title": "Detecting TensorFlow program bugs in realworld industrial environment",
            "venue": "36th IEEE/ACM International Conference on Automated Software Engineering, ASE. IEEE, 2021, pp. 55\u201366.",
            "year": 2021
        },
        {
            "authors": [
                "F. Liu",
                "G. Li",
                "B. Wei",
                "X. Xia",
                "Z. Fu",
                "Z. Jin"
            ],
            "title": "A unified multi-task learning model for AST-level and token-level code completion",
            "venue": "Empirical Software Engineering, vol. 27, no. 4:91, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "P. Luo",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "2015 IEEE International Conference on Computer Vision, ICCV. IEEE, 2015, pp. 3730\u20133738.",
            "year": 2015
        },
        {
            "authors": [
                "L. Ma",
                "F. Juefei-Xu",
                "F. Zhang",
                "J. Sun",
                "M. Xue",
                "B. Li",
                "C. Chen",
                "T. Su",
                "L. Li",
                "Y. Liu",
                "J. Zhao",
                "Y. Wang"
            ],
            "title": "DeepGauge: Multigranularity testing criteria for deep learning systems",
            "venue": "33rd ACM/IEEE International Conference on Automated Software Engineering, ASE. ACM, 2018, pp. 120\u2013131.",
            "year": 2018
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt",
                "D. Tsipras",
                "A. Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "6th International Conference on Learning Representations, ICLR. OpenReview.net, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P. McCausland"
            ],
            "title": "Self-driving uber car that hit and killed woman did not recognize that pedestrians jaywalk",
            "venue": "2022. [Online]. Available: https://www.nbcnews.com/tech-news/n1079281",
            "year": 2022
        },
        {
            "authors": [
                "M. Mirman",
                "T. Gehr",
                "M.T. Vechev"
            ],
            "title": "Differentiable abstract interpretation for provably robust neural networks",
            "venue": "35th International Conference on Machine Learning, ICML. PMLR, 2018, pp. 3575\u20133583.",
            "year": 2018
        },
        {
            "authors": [
                "M.N. M\u00fcller",
                "G. Makarchuk",
                "G. Singh",
                "M. P\u00fcschel",
                "M. Vechev"
            ],
            "title": "PRIMA: general and precise neural network certification via scalable convex hull approximations",
            "venue": "the ACM on Programming Languages, vol. 6, no. POPL, pp. 1\u201333, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Odena",
                "C. Olsson",
                "D. Andersen",
                "I. Goodfellow"
            ],
            "title": "TensorFuzz: Debugging neural networks with coverage-guided fuzzing",
            "venue": "36th International Conference on Machine Learning, ICML. PMLR, 2019, pp. 4901\u20134911.",
            "year": 2019
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. K\u00f6pf",
                "E.Z. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "PyTorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in Neural Information Processing Systems 32, NeurIPS, 2019, pp. 8024\u20138035.",
            "year": 2019
        },
        {
            "authors": [
                "B. Paulsen",
                "C. Wang"
            ],
            "title": "LinSyn: Synthesizing tight linear bounds for arbitrary neural network activation functions",
            "venue": "28th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS. Springer, 2022, pp. 357\u2013376.",
            "year": 2022
        },
        {
            "authors": [
                "B. Paulsen",
                "J. Wang",
                "J. Wang",
                "C. Wang"
            ],
            "title": "NeuroDiff: scalable differential verification of neural networks using finegrained approximation",
            "venue": "35th IEEE/ACM International Conference on Automated Software Engineering, ASE. IEEE, 2020, pp. 784\u2013796.",
            "year": 2020
        },
        {
            "authors": [
                "K. Pei",
                "Y. Cao",
                "J. Yang",
                "S. Jana"
            ],
            "title": "DeepXplore: automated whitebox testing of deep learning systems",
            "venue": "Communications of the ACM, vol. 62, no. 11, pp. 137\u2013145, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H.V. Pham",
                "S. Qian",
                "J. Wang",
                "T. Lutellier",
                "J. Rosenthal",
                "L. Tan",
                "12 Y. Yu",
                "N. Nagappan"
            ],
            "title": "Problems and opportunities in training deep learning software systems: An analysis of variance",
            "venue": "35th IEEE/ACM International Conference on Automated Software Engineering, ASE. IEEE, 2020, pp. 771\u2013783.",
            "year": 2020
        },
        {
            "authors": [
                "L. Powell"
            ],
            "title": "The problem with artificial intelligence in security",
            "venue": "2022. [Online]. Available: https://darkreading.com/threat-intelligence/ the-problem-with-artificial-intelligence-in-security",
            "year": 2022
        },
        {
            "authors": [
                "H. Qi",
                "Z. Wang",
                "Q. Guo",
                "J. Chen",
                "F. Juefei-Xu",
                "L. Ma",
                "J. Zhao"
            ],
            "title": "ArchRepair: Block-level architecture-oriented repairing for deep neural networks",
            "venue": "CoRR, vol. abs/2111.13330, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Serra",
                "C. Tjandraatmadja",
                "S. Ramalingam"
            ],
            "title": "Bounding and counting linear regions of deep neural networks",
            "venue": "35th International Conference on Machine Learning, ICML. PMLR, 2018, pp. 4565\u20134573.",
            "year": 2018
        },
        {
            "authors": [
                "D.K. Sharma",
                "S.K. Dhurandher",
                "I. Woungang",
                "R.K. Srivastava",
                "A. Mohananey",
                "J.J. Rodrigues"
            ],
            "title": "A machine learning-based protocol for efficient routing in opportunistic networks",
            "venue": "IEEE Systems Journal, vol. 12, no. 3, pp. 2207\u20132213, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Q. Shen",
                "H. Ma",
                "J. Chen",
                "Y. Tian",
                "S. Cheung",
                "X. Chen"
            ],
            "title": "A comprehensive study of deep learning compiler bugs",
            "venue": "29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE. ACM, 2021, pp. 968\u2013980.",
            "year": 2021
        },
        {
            "authors": [
                "D. Silver",
                "J. Schrittwieser",
                "K. Simonyan",
                "I. Antonoglou",
                "A. Huang",
                "A. Guez",
                "T. Hubert",
                "L. Baker",
                "M. Lai",
                "A. Bolton",
                "Y. Chen",
                "T.P. Lillicrap",
                "F. Hui",
                "L. Sifre",
                "G. van den Driessche",
                "T. Graepel",
                "D. Hassabis"
            ],
            "title": "Mastering the game of Go without human knowledge",
            "venue": "Nature, vol. 550, no. 7676, pp. 354\u2013359, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Singh",
                "M. P\u00fcschel",
                "M.T. Vechev"
            ],
            "title": "Fast polyhedra abstract domain",
            "venue": "44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL. ACM, 2017, pp. 46\u201359.",
            "year": 2017
        },
        {
            "authors": [
                "G. Singh",
                "T. Gehr",
                "M. P\u00fcschel",
                "M. Vechev"
            ],
            "title": "An abstract domain for certifying neural networks",
            "venue": "The ACM on Programming Languages, vol. 3, no. POPL, pp. 1\u201330, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Sotoudeh",
                "A.V. Thakur"
            ],
            "title": "Provable repair of deep neural networks",
            "venue": "42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI. ACM, 2021, pp. 588\u2013603.",
            "year": 2021
        },
        {
            "authors": [
                "F. Tambon",
                "A. Nikanjam",
                "L. An",
                "F. Khomh",
                "G. Antoniol"
            ],
            "title": "Silent bugs in deep learning frameworks: An empirical study of Keras and TensorFlow",
            "venue": "CoRR, vol. abs/2112.13314, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "The Linux Foundation"
            ],
            "title": "ONNX home",
            "venue": "https://onnx.ai/, 2022, accessed: 2023-02-01.",
            "year": 2022
        },
        {
            "authors": [
                "S. Tizpaz-Niari",
                "P. Cern\u00fd",
                "A. Trivedi"
            ],
            "title": "Detecting and understanding real-world differential performance bugs in machine learning libraries",
            "venue": "29th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA. ACM, 2020, pp. 189\u2013199.",
            "year": 2020
        },
        {
            "authors": [
                "F. Tram\u00e8r",
                "N. Carlini",
                "W. Brendel",
                "A. Madry"
            ],
            "title": "On adaptive attacks to adversarial example defenses",
            "venue": "Advances in Neural Information Processing Systems 33, NeurIPS, 2020, pp. 1633\u2013 1645.",
            "year": 2020
        },
        {
            "authors": [
                "C. Wan",
                "S. Liu",
                "H. Hoffmann",
                "M. Maire",
                "S. Lu"
            ],
            "title": "Are machine learning cloud APIs used correctly?",
            "venue": "IEEE/ACM International Conference on Software Engineering, ICSE. IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "C. Wan",
                "S. Liu",
                "S. Xie",
                "Y. Liu",
                "H. Hoffmann",
                "M. Maire",
                "S. Lu"
            ],
            "title": "Automated testing of software that uses machine learning APIs",
            "venue": "44th IEEE/ACM International Conference on Software Engineering, ICSE. ACM, 2022, pp. 212\u2013224.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wang",
                "J. Chen",
                "Y. Sun",
                "X. Ma",
                "D. Wang",
                "J. Sun",
                "P. Cheng"
            ],
            "title": "RobOT: Robustness-oriented testing for deep learning systems",
            "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE. IEEE, 2021, pp. 300\u2013311.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "M. Yan",
                "J. Chen",
                "S. Liu",
                "D. Zhang"
            ],
            "title": "Deep learning library testing via effective model generation",
            "venue": "28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE. ACM, 2020, pp. 788\u2013799.",
            "year": 2020
        },
        {
            "authors": [
                "M. Wardat",
                "W. Le",
                "H. Rajan"
            ],
            "title": "DeepLocalize: Fault localization for deep neural networks",
            "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE. IEEE, 2021, pp. 251\u2013262.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xiao",
                "I. Beschastnikh",
                "D.S. Rosenblum",
                "C. Sun",
                "S. Elbaum",
                "Y. Lin",
                "J.S. Dong"
            ],
            "title": "Self-checking deep neural networks in deployment",
            "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE. IEEE, 2021, pp. 372\u2013384.",
            "year": 2021
        },
        {
            "authors": [
                "D. Xie",
                "Y. Li",
                "M. Kim",
                "H.V. Pham",
                "L. Tan",
                "X. Zhang",
                "M.W. Godfrey"
            ],
            "title": "Leveraging documentation to test deep learning library functions",
            "venue": "CoRR, vol. abs/2109.01002, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xiong",
                "Y. Tian",
                "Y. Liu",
                "S. Cheung"
            ],
            "title": "Toward actionable testing of deep learning models",
            "venue": "Science China, Information Sciences, 2022, online first: 2022-09-26.",
            "year": 2022
        },
        {
            "authors": [
                "M. Yan",
                "J. Chen",
                "X. Zhang",
                "L. Tan",
                "G. Wang",
                "Z. Wang"
            ],
            "title": "Exposing numerical bugs in deep learning via gradient backpropagation",
            "venue": "29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE. ACM, 2021, pp. 627\u2013638.",
            "year": 2021
        },
        {
            "authors": [
                "J.M. Zhang",
                "M. Harman",
                "L. Ma",
                "Y. Liu"
            ],
            "title": "Machine learning testing: Survey, landscapes and horizons",
            "venue": "IEEE Transactions on Software Engineering, vol. 48, no. 2, pp. 1\u201336, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Zhang",
                "Z. Zhang",
                "Z. Li",
                "Y. Qiao"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "P. Zhang",
                "J. Wang",
                "J. Sun",
                "G. Dong",
                "X. Wang",
                "X. Wang",
                "J.S. Dong",
                "T. Dai"
            ],
            "title": "White-box fairness testing through adversarial sampling",
            "venue": "42nd ACM/IEEE International Conference on Software Engineering, ICSE. ACM, 2020, pp. 949\u2013960.",
            "year": 2020
        },
        {
            "authors": [
                "T. Zhang",
                "C. Gao",
                "L. Ma",
                "M.R. Lyu",
                "M. Kim"
            ],
            "title": "An empirical study of common challenges in developing deep learning applications",
            "venue": "30th IEEE International Symposium on Software Reliability Engineering, ISSRE. IEEE, 2019, pp. 104\u2013115.",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhang",
                "J. Zhai",
                "S. Ma",
                "C. Shen"
            ],
            "title": "AutoTrainer: An automatic DNN training problem detection and repair system",
            "venue": "43rd IEEE/ACM International Conference on Software Engineering, ICSE. IEEE, 2021, pp. 359\u2013371.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "Y. Chen",
                "S. Cheung",
                "Y. Xiong",
                "L. Zhang"
            ],
            "title": "An empirical study on TensorFlow program bugs",
            "venue": "27th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA. ACM, 2018, pp. 129\u2013140.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhang",
                "L. Ren",
                "L. Chen",
                "Y. Xiong",
                "S. Cheung",
                "T. Xie"
            ],
            "title": "Detecting numerical bugs in neural network architectures",
            "venue": "28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE. ACM, 2020, pp. 826\u2013837.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "A. Albarghouthi",
                "L. D\u2019Antoni"
            ],
            "title": "Certified robustness to programmable transformations in LSTMs",
            "venue": "2021 Conference on Empirical Methods in Natural Language Processing, EMNLP. Association for Computational Linguistics, 2021, pp. 1068\u20131083.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014neural network, numerical defect, testing, fix\nI. INTRODUCTION\nDeep Neural Networks (DNNs) are successfully deployed and show remarkable performance in many challenging applications, including facial recognition [21, 56], game playing [38], and code completion [20, 4]. To develop and deploy DNNs, one needs to attain a DNN architecture, which is usually encoded by program code as the example shown in Figure 2. First, for training, the user executes the program with the architecture on the given training/validation data, attains the model weights, and stores them in a weight file. The architecture along with the weights is named a model. Then, for inference, the user loads the weight file to CPU/GPU memory or AI chips, executes the same program with the given inference sample and weights as arguments, and gets the model prediction result as the program output. With the wide deployment of DNN models (resulted from training DNN architectures), reliability issues of DNN-based systems have become a serious concern, where malfunctioning DNN-based systems have led to serious consequences such as fatal traffic accidents [24].\nTo assure the reliability of DNN-based systems, it is highly critical to detect and fix numerical defects for two main reasons. First, numerical defects widely exist in DNN-based systems. For example, in the DeepStability database [14], over\n\u00b6Corresponding author.\n250 defects are identified in deep learning (DL) algorithms where over 60% of them are numerical defects. Moreover, since numerical defects exist at the architecture level, any model using the architecture naturally inherits these defects. Second, numerical defects can result in serious consequences. Once numerical defects (such as divide-by-zero) are exposed, the faulty DNN model will output NaN or INF instead of producing any meaningful prediction, resulting in numerical failures and system crashes [60, 58]. Thus, numerical defects hinder the application of DNNs in scenarios with high reliability and availability requirements such as threat monitoring in cybersecurity [33] and cloud system controlling [36, 13].\nTo address numerical defects in DNN architectures in an actionable manner [53], in this paper, we propose a workflow of reliability assurance (as shown in Figure 1), consisting of three tasks: potential-defect detection, feasibility confirmation, and fix suggestion, along with our proposed approach to support all these three tasks.\nPotential-Defect Detection. In this task, we detect all potential numerical defects in a DNN architecture, with a focus on operators with numerical defects (in short as defective operators) that potentially exhibit inference-phase numerical failures for two main reasons, following the literature [61, 54]. First, these defective operators can be exposed after the model is deployed and thus are more devastating than those that potentially exhibit training-phase numerical failures [27, 61]. Second, a defective operator that potentially exhibits trainingphase numerical failures can usually be triggered to exhibit inference-phase numerical failures, thus also being detected by\nar X\niv :2\n30 2.\n06 08\n6v 3\n[ cs\n.S E\n] 2\n3 A\npr 2\n02 3\nour task. For example, the type of training-phase NaN gradient failures is caused by an operator\u2019s input that leads to invalid derivatives, and this input also triggers failures in the inference phase [54].\nFeasibility Confirmation. In this task, we confirm the feasibility of these potential numerical defects by generating failure-exhibiting system tests. As shown in Figure 1, a system test is a tuple of training example1 xtrain and inference example x such that after the training example is used to train the architecture under consideration, applying the resulting model on the inference example exhibits a numerical failure.\nFix Suggestion. In this task, we fix a feasible numerical defect. To determine the fix form, we have inspected the developers\u2019 fixes of the numerical defects collected by Zhang et al. [60] by looking at follow-up Stack Overflow posts or GitHub commits. Among the 13 numerical defects whose fixes can be located, 12 fixes can be viewed as explicitly or implicitly imposing interval preconditions on different locations, such as after inputs or weights are loaded and before defective operators are invoked. Thus, imposing an interval precondition, e.g., by clipping (i.e., chopping off the input parts that exceed the specified input range) the input for defective operator(s), is an effective and common strategy for fixing a numerical defect. Given a location (i.e., one related to an operator, input, or weight where users prefer to impose a fix), we suggest a fix for the numerical defect under consideration.\nTo support all the three tasks of the reliability assurance process against DNN numerical defects, we propose the RANUM approach in this paper.\nFor task 1\u00a9 and task 2\u00a9a, which are already supported by two existing tools (DEBAR [61] and GRIST [54]), RANUM introduces novel extensions and optimizations that substantially improve the effectiveness and efficiency. (1) DEBAR [61] is the state-of-the-art tool for potential-defect detection; however, DEBAR can handle only static computational graphs and does not support widely used dynamic graphs in PyTorch programs [28]. RANUM supports dynamic graphs thanks to our novel technique of backward fine-grained node labeling. (2) GRIST [54] is the state-of-the-art tool for generating failure-exhibiting unit tests to confirm potential-defect feasibility; however, GRIST conducts gradient back-propagation by using the original inference input and weights as the starting point. Recent studies [23, 8] on DNN adversarial attacks suggest that using a randomized input as the starting point leads to stronger attacks than using the original input. Taking this observation, we combine gradient back-propagation with random initialization in RANUM.\nFor task 2\u00a9 and task 3\u00a9, which are not supported by any existing tool, RANUM is the first automatic approach for them.\nFor feasibility confirmation, RANUM is the first approach that generates failure-exhibiting system tests that contain train-\n1In real settings, multiple training examples are used to train an architecture, but generating a single training example to exhibit failures (targeted by our work) is desirable for ease of debugging while being more challenging than generating multiple training examples to exhibit failures.\ning examples. Doing so is a major step further from the existing GRIST tool, which generates failure-exhibiting unit tests ignoring the practicality of generated model weights. Given that in practice model weights are determined by training examples, we propose the technique of two-step generation for this task. First, we generate a failure-exhibiting unit test. Second, we generate a training example that leads to the model weights in the unit test when used for training. For the second step, we extend the deep-leakage-from-gradient (DLG) attack [63] by incorporating the straight-through gradient estimator [3].\nFor fix suggestion, RANUM is the first automatic approach. RANUM is based on the novel technique of abstraction optimization. We observe that a defect fix in practice is typically imposing interval clipping on some operators such that each later-executed operator (including those defective ones) can never exhibit numerical failures. Therefore, we propose the novel technique of abstraction optimization to \u201cdeviate away\u201d the input range of a defective operator from the invalid range, falling in which can cause numerical failures.\nFor RANUM, we implement a tool2 and evaluate it on the benchmarks [54] of 63 real-world DNN architectures containing 79 real numerical defects; these benchmarks are the largest benchmarks of DNN numerical defects to the best of our knowledge. The evaluation results show that RANUM is both effective and efficient in all the three tasks for DNN reliability assurance. (1) For potential-defect detection, RANUM detects >60% more true defects than the state-of-the-art DEBAR approach. (2) For feasibility confirmation, RANUM generates failure-exhibiting unit tests to confirm potential numerical defects in the benchmarks with 100% success rate; in contrast, with the much higher time cost (17.32X), the state-of-theart GRIST approach generates unit tests to confirm defects with 96.96% success rate. More importantly, for the first time, RANUM generates failure-exhibiting system tests that confirm defects (with 92.78% success rate). (3) For fix suggestion, RANUM proposes fix suggestions for numerical defects with 100% success rate. In addition, when the RANUM-generated fixes are compared with developers\u2019 fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.\nThis paper makes the following main contributions: \u2022 We formulate the reliability assurance problem for DNN\narchitectures against numerical defects and elaborate on three important tasks for this problem. \u2022 We propose RANUM\u2014the first automatic approach that solves all these three tasks. RANUM includes three novel techniques (backward fine-grained node labeling, two-step test generation, and abstraction optimization) and solves system test generation and fix suggestion for the first time. \u2022 We implement RANUM and apply it on 63 real-world DNN architectures, showing the high effectiveness and efficiency of RANUM compared to both the state-of-theart approaches and developers\u2019 fixes.\n2Open source at https://github.com/llylly/RANUM."
        },
        {
            "heading": "II. BACKGROUND AND APPROACH OVERVIEW",
            "text": "In this section, we introduce the background of DNN numerical defects and failures, and then give an overview of the RANUM approach with a running example."
        },
        {
            "heading": "A. Background",
            "text": "DL developers define the DNN architecture with code using modern DL libraries such as PyTorch [28] and TensorFlow [1]. The DNN architecture can be expressed by a computational graph. Figures 2 and 3 depict a real-world example. Specifically, the DNN architecture in a DL program can be automatically converted to an ONNX-format computational graph [43].\nThe computational graph can be viewed as a Directed Acyclic Graph (DAG): G = \u3008V, E\u3009, where V and E are sets of nodes and edges, respectively. We call nodes with zero indegree as initial nodes, which correspond to input, weight, or constant nodes. Initial nodes provide concrete data for the DNN models resulted from training the DNN architecture. The data from each node is formatted as a tensor, i.e., a multidimensional array, with a specified data type and array shape annotated alongside the node definition. We call nodes with positive in-degree as internal nodes, which correspond to concrete operators, such as matrix multiplication (MatMul) and addition (Add). During model training, the model weights, i.e., data from weight nodes, are generated by the training algorithm. Then, in the deployment phase (i.e., model inference), with these trained weights and a user-specified input named inference example, the output of each operator is computed in the topological order. The output of some specific node is used as the prediction result.\nWe let x and w denote the concatenation of data from all input nodes and data from all weight nodes, respectively.3 For example, in Figure 3, x concatenates data from nodes 1 and 11; and w concatenates data from nodes 2 and 4. Given specific x and w, the input and output for each node are deterministic.4 We use f inn (x;w) and f out n (x;w) to express input and output data of node n, respectively, given x and w. Numerical Defects in DNN Architecture. We focus on inference-phase numerical defects. These defects lead to numerical failures when specific operators receive inputs within invalid ranges so that the operators output NaN or INF.\n3A bolded alphabet stands for a vector or tensor throughout the paper. 4An architecture may contain stochastic nodes. We view these nodes as nodes with randomly sampled data, so the architecture itself is deterministic.\nDefinition 1. For the given computational graph G = \u3008V, E\u3009, if there is a node n0 \u2208 V , such that there exists a valid input and valid weights that can let the input of node n0 fall within the invalid range, we say there is a numerical defect at node n0. Formally,\u2203x0 \u2208 Xvalid,w0 \u2208 Wvalid, f inn0(x0;w0) \u2208 In0,invalid\n=\u21d2\u2203 numerical defect at node n0.\nIn the definition, Xvalid and Wvalid are valid input range and weight range, respectively, which are clear given the deployed scenario. For example, ImageNet Resnet50 models have valid input range Xvalid = [0, 1]3\u00d7224\u00d7224 since image pixel intensities are within [0, 1], and valid weight range Wvalid = [\u22121, 1]p where p is the number of parameters since weights of welltrained Resnet50 models are typically within [\u22121, 1]. The invalid range In0,invalid is determined by n0\u2019s operator type with detailed definitions in Suppl. B. For example, for the Log operator, the invalid range In0,invalid = (\u2212\u221e, Umin) where Umin is the smallest positive number of a tensor\u2019s data type."
        },
        {
            "heading": "B. Approach Overview",
            "text": "In Figure 4, we show the overview structure of the RANUM approach. RANUM takes a DNN architecture as the input. Note that although RANUM is mainly designed and illustrated for a DNN architecture, the RANUM approach can also be directly applied to general neural network architectures since they can also be expressed by computational graphs. First, the DNN static analysis framework (task 1\u00a9 in Figure 1) in\nRANUM detects all potential numerical defects in the architecture. Second, the two-step test generation component (task 2\u00a9 in Figure 1), including unit test generation and training example generation, confirms the feasibility of these potential numerical defects. Third, the abstraction optimization component (task 3\u00a9 in Figure 1) takes the input/output abstractions produced by the DNN static analysis framework along with the user-specified fix locations, and produces preconditions to fix the confirmed defects.\nWe next go through the whole process in detail taking the DNN architecture shown in Figure 3 as a running example.\nTask 1\u00a9: Potential-Defect Detection via Static Analysis. The DNN static analysis framework within RANUM first computes the numerical intervals of possible inputs and outputs for all nodes within the given DNN architecture, and then flags any nodes whose input intervals overlap with their invalid ranges as nodes with potential numerical defects.\nIn Figure 3, suppose that the user-specified input x-input (node 1) is within (elementwise, same below) range [(\u221210,\u221210)T, (10, 10)T]; weights (node 2) are within range [ [ \u221210 \u221210 \u221210 \u221210 ] , [ 10 10 10 10 ] ]; and biases (node 4) are within range [(\u221210,\u221210)T, (10, 10)T]. Our DNN static analysis framework computes these interval abstractions for node inputs:\n1) Node 5 (after MatMul): [(\u2212200,\u2212200)T, (200, 200)T]; 2) Node 6 (after Add): [(\u2212210,\u2212210)T, (210, 210)T]; 3) Node 8 (after Softmax in float32): [(0, 0)T, (1, 1)T]; 4) Node 9 (after Sub of [1, 1] and node 8), 10: [(0, 0)T, (1, 1)T].\nSince nodes 9 and 10 use the Log operator whose invalid input range (\u2212\u221e, Umin) overlaps with their input range [(0, 0)T, (1, 1)T], we flag nodes 9 and 10 as potential numerical defects.\nThis static analysis process follows the state-of-the-art DEBAR tool [61]. However, we extend DEBAR with a novel technique named backward fine-grained node labeling. This technique detects all nodes that require fine-grained abstractions, e.g., nodes that determine the control flow in a dynamic graph. For these nodes, we apply interval abstractions with the finest granularity to reduce control flow ambiguity. For other nodes, we let some neighboring elements share the same interval abstraction to improve efficiency while preserving tightness. As a result, the static analysis in RANUM has high efficiency and supports much more DNN operators including dynamic control-flow operators like Loop than DEBAR does.\nTask 2\u00a9: Feasibility Confirmation via Two-Step Test Generation. Given nodes that contain potential numerical defects (nodes 9 and 10 in our example), we generate failureexhibiting system tests to confirm their feasibility. A failureexhibiting system test is a tuple \u3008xtrain,xinfer\u3009, such that after training the architecture with the training example xtrain,5 with the trained model weights winfer, the inference input\n5In particular, if our generation technique outputs xtrain, the numerical failure can be triggered if the training dataset contains only xtrain or only multiple copies of xtrain and the inference-time input is xinfer. Our technique can also be applied for generating a batch of training examples by packing the batch as a single example: xtrain = (xtrain1,xtrain2, . . . ,xtrainB).\nxinfer triggers a numerical failure. The name \u201csystem test\u201d is inspired by traditional software testing, where we test the method sequence (m = train(xtrain); m.infer(xinfer)). In contrast, GRIST [54] generates model weights winfer along with inference input xinfer that tests only the inference method m.infer(), and the weights may be infeasible from training. Hence, we view the GRIST-generated tuple \u3008winfer,xinfer\u3009 as a \u201cunit test\u201d.\nWe propose a two-step test generation technique to generate failure-exhibiting system tests.\nStep a: Generate failure-exhibiting unit test \u3008winfer,xinfer\u3009. The state-of-the-art GRIST tool supports this step. However, GRIST solely relies on gradient back-propagation, which is relatively inefficient. In RANUM, we augment GRIST by combining its gradient back-propagation with random initialization inspired by recent research on DNN adversarial attacks [8, 23]. As a result, RANUM achieves 17.32X speedup with 100% success rate. Back to the running example in Figure 3, RANUM can generate [ 5 \u22125 \u22125 5 ] for node 2 and (0.9,\u22120.9)T for node 4 as model weights winfer; and (10,\u221210)T for node 1 and (1, 0)T for node 11 as the inference input xinfer. Such winfer and xinfer induce input (0, 1)T and (1, 0)T for nodes 9 and 10, respectively. Since both nodes 9 and 10 use the log operator and log 0 is undefined, both nodes 9 and 10 trigger numerical failures.\nStep b: Generate training example xtrain that achieves model weights winfer. To the best of our knowledge, there is no automatic approach for this task yet. RANUM provides support for this task based on our extension of DLG attack [63]. The DLG attack is originally designed for recovering the training data from training-phase gradient leakage. Here, we figure out the required training gradients to trigger the numerical failure at the inference phase and then leverage the DLG attack to generate xtrain that leads to such training gradients. Specifically, many DNN architectures contain operators (such as ReLU) on which DLG attack is hard to operate [35]. We combine straight-through estimator [3] to provide proxy gradients and bypass this barrier. Back to the running example in Figure 3, supposing that the initial weights are [ \u22120.1 0.1 0.1 \u22120.1\n] for node 2 and (0, 0)T for node 4, RANUM can generate training example xtrain composed of (5.635,\u22125.635)T for node 1 and (1, 0)T for node 11, such that one-step training with learning rate 1 on this example leads to winfer. Combining xtrain from this step with xinfer from step a, we obtain a failureexhibiting system test that confirms the feasibility of potential defects in nodes 9 and 10.\nTask 3\u00a9: Fix Suggestion via Abstract Optimization. In this task, we suggest fixes for the confirmed numerical defects. RANUM is the first approach for this task to our knowledge.\nThe user may prefer different fix locations, which correspond to a user-specified set of nodes Vfix \u2286 V to impose the fix. For example, if the fix method is clipping the inference input, Vfix are input nodes (e.g., nodes 1, 11 in Figure 3); if the fix method is clipping the model weights during training, Vfix are weight nodes (e.g., nodes 2, 4 in Figure 3); if the fix\nmethod is clipping before the defective operator, Vfix are nodes with numerical defects (e.g., nodes 9, 10 in Figure 3).\nAccording to the empirical study of developers\u2019 fixes in Section I, 12 out of 13 defects are fixed by imposing interval preconditions for clipping the inputs of Vfix. Hence, we suggest interval precondition, which is interval constraint ln \u2264 f inn (x;w) \u2264 un for nodes n \u2208 Vfix, as the defect fix in this paper. A fix should satisfy that, when these constraints\u2227 n\u2208Vfix(ln \u2264 f in n (x;w) \u2264 un) are imposed, the input of any node in the computational graph should always be valid, i.e., f inn0(x;w) /\u2208 In0,invalid,\u2200n0 \u2208 V .\nIn RANUM, we formulate the fix suggestion task as a constrained optimization problem, taking the endpoints of interval abstractions for nodes in Vfix as optimizable variables. We then propose the novel technique of abstraction optimization to solve this constrained optimization problem. Back to the Figure 3 example, if users plan to impose a fix on inference input, RANUM can suggest the fix \u22121 \u2264 x-input \u2264 1; if users plan to impose a fix on nodes with numerical defects, RANUM can suggest the fix 10\u221238 \u2264 node 9 & node 10.input \u2264 +\u221e."
        },
        {
            "heading": "III. THE RANUM APPROACH",
            "text": "In this section, we introduce the three novel techniques in RANUM: backward fine-grained node labeling in Section III-A; two-step test generation in Section III-B; and abstraction optimization in Section III-C."
        },
        {
            "heading": "A. DNN Static Analysis Framework with Backward FineGrained Node Labeling for Potential-Defect Detection",
            "text": "RANUM contains a static analysis framework to enable potential-defect detection and support downstream tasks as shown in Figure 4. Given a DNN architecture and valid ranges for input and weight nodes, the static analysis framework computes interval abstractions for possible inputs and outputs of each node. As a result, we can check whether an overlap exists between the interval abstraction and invalid input ranges for all nodes in the graph to detect potential numerical defects. Then, the defective nodes are fed into the two-step test generation component to confirm the feasibility of potential defects; and the differentiable abstractions are fed into the abstract optimization component to produce fixes.\nFormally, for given valid ranges of inference input and model weights, namely X and W , for each node n \u2208 V , our framework computes sound input interval abstraction [ln,un] := {x : ln \u2264 x \u2264 un} such that [ln,un] always captures all possible inputs of the node: [ln,un] \u2287 {f inn (x,w) : x \u2208 X ,w \u2208 W}. We also compute output interval abstractions similarly.\nCompared with traditional analysis tools for numerical software [10, 39], RANUM\u2019s static analysis framework designs abstractions for DNN primitives operating on multidimensional tensors that are not supported by traditional tools. Compared with the state-of-the-art DEBAR tool [61], RANUM uses the same abstraction domain (interval domain with tensor partitioning), but incorporates a novel tech-\nnique (backward fine-grained node labeling) to improve abstraction precision and support a wider range of DNN architectures.\nAbstract Domain: Interval with Tensor Partitioning. Following DEBAR\u2019s design, we use the interval with tensor partitioning [61] as the abstraction domain. This abstraction domain partitions a tensor into multiple subblocks and shares the interval abstractions at the block level instead of imposing abstractions at the element level. Therefore, we can compute the abstraction of a smaller size than the original tensor to improve efficiency.\nOur Technique: Backward Fine-Grained Node Labeling. The interval domain with tensor partitioning provides a degree of freedom in terms of the partition granularity, i.e., we can choose the subblock size for each node\u2019s abstraction. When the finest granularity, i.e., elementwise abstraction, is chosen, the abstraction interval is the most concrete. When the coarsest granularity (i.e., one scalar to summarize the node tensor) is chosen, the abstraction saves the most space and computational cost but loses much precision.\nExample. Suppose that the possible input range of a node is ([\u22121, 0], [0, 1], [1, 2], [\u22121, 0]), where each interval [l, u] specifies the range of corresponding elements in the fourdimensional vector. If we choose the finest granularity, we use [ln,un] = [(\u22121, 0, 1,\u22121), (0, 1, 2, 0)] as the input range abstraction. If we choose the coarsest granularity, we use [ln,un] = [\u22121, 2] as the abstraction where the same interval is shared for all elements. As we can see, finer granularity provides tighter abstraction at the expense of larger computational and space costs.\nIn DEBAR, the coarsest granularity is used by default for most operators. However, we find that using the finest instead of the coarsest granularity for some nodes is more beneficial for overall abstraction preciseness. For example, the control-flow operators (e.g., Loop) benefit from concrete execution to determine the exact control flow in the dynamic graph, and the indexing operators (e.g., Slice) and shaping operators (e.g., Reshape) benefit from explicit indexers and shapes to precisely infer the output range. Hence, we propose to use the finest granularity for some nodes (namely finegrained requiring operators) while the coarsest granularity for other nodes during static analysis.\nTo benefit from the finest granularity abstraction for required nodes, typically, all of their preceding nodes also need the finest granularity. Otherwise, the over-approximated intervals from preceding nodes will be propagated to the required nodes, making the finest abstraction for the required nodes useless. To solve this problem, in RANUM, we back-propagate \u201cfinegrained\u201d labels from these fine-grained requiring nodes to initial nodes by topologically sorting the graph with inverted edges, and then apply the finest granularity abstractions on all labeled nodes. In practice, we find that this strategy eliminates the control-flow ambiguity and indexing ambiguity with little\nloss of efficiency6. As a result, RANUM supports all dynamic graphs (which are not supported by DEBAR) that comprise 39.2% of the benchmarks proposed by Yan et al. [54].\nFurthermore, when preceding nodes use finer-grain abstraction granularity, the subsequent nodes should preserve such fine granularity to preserve the analysis preciseness. Principally, the choice of abstraction granularity should satisfy both tightness (bearing no precision loss compared to elementwise interval abstraction) and minimality (using the minimum number of partitions for high efficiency). To realize these principles, we dynamically determine a node\u2019s abstraction granularity based on the granularity of preceding nodes. The abstraction design for some operators is non-trivial. Omitted details (formulation, illustration, and proofs) about the static analysis framework are in Suppl. C.\nIn summary, the whole static analysis process consists of three steps. (1) Determine the tensor partition granularity of all initial nodes by our technique of backward finegrained node labeling. (2) Sort all nodes in the graph in the topological order. (3) Apply corresponding abstraction computation algorithms for each node based on the preceding node\u2019s abstractions. The key insight behind the design of our static analysis framework is the strategic granularity selection for tensor abstraction, maintaining both high efficiency (by selecting the coarse granularity for data-intensive nodes) and high precision (by selecting the fine granularity for some critical nodes, such as nodes with control-flow, indexing, and shaping operators)."
        },
        {
            "heading": "B. Two-Step Test Generation for Feasibility Confirmation",
            "text": "RANUM generates failure-exhibiting system tests for the given DNN to confirm the feasibility of potential numerical defects. Here, we take the DNN architecture as the input. From the static analysis framework, we obtain a list of nodes that have potential numerical defects. For each node n0 within the list, we apply our technique of two-step test generation to produce a failure-exhibiting system test tsys = \u3008xtrain,xinfer\u3009 as the output. According to Section II-B, the test should satisfy that after the architecture is trained with xtrain, entering xinfer in the inference phase results in a numerical failure.\nWe propose the novel technique of two-step test generation: first, generate failure-exhibiting unit test \u3008winfer,xinfer\u3009; then, generate training example xtrain that leads model weights to be close to winfer after training.\nStep a: Unit Test Generation. As sketched in Section II-B, we strengthen the state-of-the-art unit test generation approach, GRIST [54], by combining it with random initialization to complete this step. Specifically, GRIST leverages the gradients of the defective node\u2019s input with respect to the inference input and weights to iteratively update the inference input and weights to generate failure-exhibiting unit tests. However,\n6Theoretically, using the finest granularity for tensor partitioning cannot fully eliminate the ambiguity, since interval abstraction is intrinsically an overapproximation. Nevertheless, in our evaluation (Section IV), we find that this technique eliminates control-flow and indexing ambiguities on all 63 programs in the benchmarks.\nGRIST always conducts updates from the existing inference input and weights, suffering from local minima problem [23]. Instead, motivated by DNN adversarial attack literature [23, 45], a sufficient number of random starts help find global minima effectively. Hence, in RANUM, we first conduct uniform sampling 100 times for both the inference input and weights to trigger the numerical failure. If no failure is triggered, we use the sample that induces the smallest loss as the start point for gradient optimization. As Section IV-A shows, this strategy substantially boosts the efficiency, achieving 17.32X speedup.\nStep b: Training Example Generation. For this step, RANUM takes the following inputs: (1) the DNN architecture, (2) the failure-exhibiting unit test tunit = \u3008winfer,xinfer\u3009, and (3) the randomly initialized weights w0. Our goal is to generate a legal training example xtrain, such that the model trained with xtrain will contain weights close to winfer.\nDNNs are typically trained with gradient-descent-based algorithms such as stochastic gradient descent (SGD). In SGD, in each step t, we sample a mini-batch of samples from the training dataset to compute their gradients on model weights and use these gradients to update the weights. We focus on one-step SGD training with a single training example, since generating a single one-step training example to exhibit a failure is more desirable for debugging because, in one-step training, the model weights are updated strictly following the direction of the gradients. Therefore, developers can inspect inappropriate weights, easily trace back to nodes with inappropriate gradients, and then fix these nodes. In contrast, in multi-step training, from inappropriate weights, developers cannot trace back to inappropriate gradients because weights are updated iteratively and interactions between gradients and weights are complex (even theoretically intractable [18]).\nIn this one-step training case, after training, the model weights winfer satisfy winfer = w0 \u2212 \u03b3\u2207wL(xtrain;w0), (1) where \u03b3 \u2208 R+ is a predefined learning rate, and L is the predefined loss function in the DNN architecture. Hence, our goal becomes finding xtrain that satisfies\n\u2207wL(xtrain;w0) = (w0 \u2212winfer)/\u03b3. (2) The DLG attack [63] is a technique for generating input data that induce specific weight gradients. The attack is originally designed for recovering training samples from monitored gradient updates. Since the right-hand side (RHS) of Equation (2) is known, our goal here is also to generate input example xtrain that induces specific weight gradients. Therefore, we leverage the DLG attack to generate training example xtrain.\nExtending DLG Attack with Straight-Through Estimator. Directly using DLG attack suffers from an optimization challenge in our scenario. Specifically, in DLG attack, suppose that the target weight gradients are \u2206wtarg, we use gradient descent over the squared error \u2016\u2207wL(x;w0) \u2212\u2206wtarg\u201622 to generate x. In this process, we need meaningful gradient information of this squared error loss to perform the optimization. However, the gradient of this loss involves secondorder derivatives of L(x;w0), which could be zero. For\nexample, DNNs with ReLU as activation function are piecewise linear and have zero second-order derivatives almost everywhere [35]. This optimization challenge is partly addressed in DLG attack by replacing ReLU with Sigmoid, but it changes the DNN architecture (i.e., the system under test) and hence is unsuitable.\nWe leverage the straight-through estimator to mitigate the optimization challenge. Specifically, for a certain operator, such as ReLU, we do not change its forward computation but change its backward gradient computation to provide secondorder derivatives within the DLG attack process. For example, for ReLU, in backward computation we use the gradient of Softplus function, namely 1\u2212 11+exp(x) , because Softplus is an approximation of ReLU [7] with non-zero second-order derivatives. Note that we modify the computed gradients only within the DLG attack process. After such xtrain is generated by the attack, we evaluate whether it triggers a numerical failure using the original architecture and gradients in Equation (1).\nSuppl. E lists hyperparameters used by our implementation."
        },
        {
            "heading": "C. Abstraction Optimization for Fix Suggestion",
            "text": "In this task, we aim to generate the precondition fix given imposing locations. The inputs are the DNN architecture, the node n0 with numerical defects, and a node set Vfix to impose the fix. We would like to generate interval preconditions for Vfix node inputs so that after these preconditions are imposed, the defect on n0 is fixed.\nFormally, our task is to find \u3008ln, un\u3009 for each n \u2208 Vfix (ln and un are scalars so the same interval bound applied to all elements of n\u2019s tensor), such that for any x,w satisfying f inn (x;w) \u2208 [ln, un], \u2200n \u2208 Vfix, for the defective node n0, we have f inn0(x;w) /\u2208 In0,invalid, where the full list of invalid input ranges In0,invalid is in Suppl. B. There is an infinite number of possible \u3008ln, un\u3009 interval candidates since ln and un are floating numbers. Hence, we need an effective technique to find a valid solution from the exceedingly large search space that incurs a relatively small model utility loss. To achieve so, we formulate a surrogate optimization problem for this task.\nmaximize ln,un:n\u2208Vfix\ns s.t. un \u2265 ln + s(uvalidn \u2212 lvalidn ),\u2200n \u2208 Vfix, (3)\nlvalidn \u2264 ln \u2264 un \u2264 uvalidn , \u2200n \u2208 Vfix, (4) Lprecondn0 ({ln, un}n\u2208Vfix) < 0. (5)\nHere, lvalidn and u valid n are the valid ranges (of the node\u2019s input\nn), which are fixed and determined by the valid ranges of input and weights. Lprecondn0 is the node-specific precondition generation loss that is the distance between the furthest endpoint of defective node n0\u2019s interval abstraction and n0\u2019s valid input range. Hence, when Lprecondn0 ({ln, un}n\u2208Vfix) becomes negative, the solution {ln, un}n\u2208Vfix is a valid precondition. The optimization variables are the precondition interval endpoints ln and un and the objective is the relative span of these intervals. The larger the span is, the looser the precondition constraints are, and the less hurt they are for the model\u2019s utility. Equation (3) enforces the interval span requirement. Equation (4) assures that the precondition interval is in the valid range.\nAlgorithm 1 Abstraction Optimization (Section III-C) Input: DNN architecture G = \u3008V, E\u3009, defective node n0 \u2208 V , nodes\nto impose fix Vfix \u2286 V 1: s\u2190 1, \u03b3s \u2190 0.9, \u03b3c \u2190 0.1,minstep\u2190 0.1,maxiter\u2190 1000 2: cn \u2190 (lvalidn + uvalidn )/2, ln \u2190 lvalidn , un \u2190 uvalidn , \u2200n \u2208 Vfix 3: for i = 1 to maxiter do 4: for n \u2208 Vfix do 5: loss \u2190 Lprecondn0 ({ln\u2032 , un\u2032}n\u2032\u2208Vfix) 6: cn \u2190 cn \u2212 \u03b3c max{|cn|,minstep}sgn(\u2207cn loss) 7: (ln, un)\u2190 (cn \u2212 s(u valid n \u2212l valid n )\n2 , cn +\ns(uvalidn \u2212l valid n )\n2 )\n8: (ln, un)\u2190 (max{ln, lvalidn },min{un, uvalidn }) 9: end for\n10: if Lprecondn0 ({ln, un}n\u2208Vfix) < 0 then 11: return {ln, un}n\u2208Vfix // Find precondition fix 12: end if 13: s\u2190 \u03b3s \u00b7 s 14: end for 15: return \u201cfailed\u201d // Failed to find precondition fix\nEquation (5) guarantees the validity of the precondition as a fix.\nFor any {ln, un}n\u2208Vfix , thanks to RANUM\u2019s static analysis framework, we can compute induced intervals of defective node n0, and thus compute the loss value Lprecondn0 .\nAs shown in Algorithm 1, we propose the technique of abstraction optimization to effectively and approximately solve this optimization. Our technique works iteratively. In the first iteration, we set span s = 1, and in the subsequent iterations, we reduce the span s exponentially as shown in Line 13 where hyperparameter \u03b3s = 0.9. Inside each iteration, for each node to impose precondition n \u2208 Vfix, we use the interval center cn = (ln + un)/2 as the optimizable variable and compute the sign of its gradient: sgn(\u2207cn loss). We use this gradient sign to update each cn toward reducing the loss value in Line 6. Then, we use cn and the span s to recover the actual interval in Line 7 and clip ln and un by the valid range [lvalidn , u valid n ] in Line 8. At the end of this iteration, for updated ln and un, we compute Lprecondn0 ({ln, un}n\u2208Vfix) to check whether the precondition is a fix. If so, we terminate; otherwise, we proceed to the next iteration. We note that if the algorithm finds a precondition, the precondition is guaranteed to be a valid fix by the soundness nature of our static analysis framework and the definition of Lprecondn0 . When no feasible precondition is found within maxiter = 1000 iterations, we terminate the algorithm and report \u201cfailed to find the fix\u201d.\nRemark. The key ingredient in the technique is the gradientsign-based update rule (shown in Line 6), which is much more effective than normal gradient descent for two reasons. (1) Our update rule can get rid of gradient explosion and vanishing problems. For early optimization iterations, the span s is large and interval bounds are generally coarse, resulting in too large or too small gradient magnitude. For example, the input range for Log could be [1, 1010] where gradient can be 10\u221210, resulting in almost negligible gradient updates. In contrast, our update rule leverages the gradient sign, which always points to the correct gradient direction. The update step size in our rule is the maximum of current magnitude |cn| and minstep to avoid stagnation. (2) Our update rule mitigates the gradient\nmagnitude discrepancy of different cn. At different locations, the nodes in DNNs can have diverse value magnitudes that are not aligned with their gradient magnitudes, making gradient optimization challenging. Therefore, we use this update rule to solve the challenge, where the update magnitude depends on the value magnitude (|cn|) instead of gradient magnitude (\u2207cn loss). We empirically compare our technique with standard gradient descent in Section IV-C."
        },
        {
            "heading": "IV. EXPERIMENTAL EVALUATION",
            "text": "We conduct a systematic experimental evaluation to answer the following research questions. RQ1 For tasks already supported by existing state-of-the-\nart (SOTA) tools (tasks 1\u00a9 and 2\u00a9a), how much more effective and efficient is RANUM compared to these SOTA tools?\nRQ2 For feasibility confirmation via generating failureexhibiting system tests (task 2\u00a9), how much more effectively and efficiently can RANUM confirm potential numerical defects compared to baseline approaches? RQ3 For suggesting fixes (task 3\u00a9), how much more efficient and effective is RANUM in terms of guarding against numerical failures compared to baseline approaches and developers\u2019 fixes, respectively?\nFor RQ1, we compare RANUM with all SOTA tools. For RQ2 and RQ3, RANUM is the first approach to the best of our knowledge, so we compare RANUM with baseline approaches (constructed by leaving our novel techniques out of RANUM) and developers\u2019 fixes. We conduct the evaluation on the GRIST benchmarks [54], being the largest dataset of real-world DNN numerical defects to our knowledge. The benchmarks contain 63 real-world DL programs with numerical defects collected from previous studies and GitHub. Each program contains a DNN architecture, and each architecture has one or more numerical defects. There are 79 real numerical defects in total.\nWe perform our evaluation on a Linux workstation with a 24-core Xeon E5-2650 CPU running at 2.20 GHz. Throughout the evaluation, we stop the execution after reaching 30 min limit by following the evaluation setup by the most recent related work [54]."
        },
        {
            "heading": "A. RQ1: Comparison with SOTA Tools",
            "text": "For two tasks, existing tools can provide automatic support: potential-defect detection (task 1\u00a9) where the SOTA tool is DEBAR [61], and failure-exhibiting unit test generation (task 2\u00a9a) where the SOTA tool is GRIST [54]. We compare RANUM with these tools on their supported tasks, respectively.\nComparison with DEBAR. RANUM successfully detects all 79 true defects and DEBAR detects only 48 true defects according to both our evaluation and the literature [54]. Hence, RANUM detects 64.58% more true defects than DEBAR. In terms of efficiency, DEBAR and RANUM have similar running time, and both finish in 3 s per case.\nWe manually inspect the cases where DEBAR fails but RANUM succeeds. They correspond to DL programs written\nwith the PyTorch library, which generates dynamic computational graphs that DEBAR cannot handle. In contrast, RANUM provides effective static analysis support for dynamic computational graphs thanks to our backward fine-grained node labeling technique (Section III-A) that is capable of disambiguating the control flow within dynamic graphs.\nComparison with GRIST. Results are shown in Table I. Since both RANUM and GRIST have a randomness component where RANUM uses random initialization and GRIST relies on DNN\u2019s randomly initialized weights, we repeat both approaches for 10 runs, record the total number of times where a failure-exhibiting unit test is generated, and the average execution time per run. RANUM succeeds in all cases and all repeated runs, and GRIST fails to generate such unit test in 24 out of 790 runs (i.e., 96.96% success rate). RANUM has 6.66 s average execution time and is 17.32X faster than GRIST.\nThe superior effectiveness and efficiency of RANUM are largely due to the existence of random initialization as introduced in Section III-B. We observe that since GRIST always takes initial model weights and inference input as the starting point to update from, the generated unit test is just slightly changed from initial weights and input, being hard to expose the target numerical defect. In contrast, RANUM uses random initialization to explore a much larger space and combines gradient-based optimization to locate the failure-exhibiting instances from the large space. We also evaluate the pure random strategy that uses only random initialization without gradient-based optimization, and such strategy fails in 30 runs, being inferior to both RANUM and GRIST, implying that both random initialization and gradient-based optimization are important. Among all the 79 cases, RANUM is slower than GRIST on only one case (28a). For this case, we find the default inference input loaded by the DNN program (used by GRIST) is not far from a failure-exhibiting one, but a randomly sampled inference input (used by RANUM) is usually far from that. Hence, RANUM takes more iterations to find a failure-exhibiting inference input by the nature of gradientbased optimization."
        },
        {
            "heading": "B. RQ2: Feasibility Confirmation via System Test Generation",
            "text": "In task 2\u00a9, RANUM confirms the feasibility of potential numerical defects by generating failure-exhibiting system tests.\nBaseline. Since RANUM is the first approach for this task, we do not compare with existing literature and propose one random-based approach (named \u201cRandom\u201d hereinafter) as the baseline. In \u201cRandom\u201d, we first generate a failure-exhibiting unit test with random sampling. If there is any sample that triggers a failure, we stop and keep the inference example part as the desired xinfer. Then, we generate xtrain again by random sampling. If any sample, when used for training, could induce model weights w that cause a numerical failure when using xinfer as the inference input, we keep that sample as xtrain and terminate. If and only if both xinfer and xtrain are found, we count this run as a \u201csuccess\u201d one for \u201cRandom\u201d.\nFor each defect, due to the randomness of the model\u2019s initial weights, we repeat both RANUM and \u201cRandom\u201d for 10 runs. Both approaches use the same set of random seeds.\nEvaluation Result. Results are in Table II. We observe that RANUM succeeds in 733/(79 \u00d7 10) = 92.78% runs and the baseline \u201cRandom\u201d succeeds in 649/(79\u00d710) = 82.15% runs. Moreover, RANUM spends only 17.31 s time on average per run, which is a 19.30X speedup compared to \u201cRandom\u201d. We also observe that RANUM is more reliable across repeated runs. There are only 6 cases with unsuccessful repeated runs in RANUM, but there are 19 such cases in \u201cRandom\u201d. Hence, RANUM is substantially more effective, efficient, and reliable for generating system tests than the baseline.\nDiscussion. The high effectiveness of RANUM mainly comes from the advantage of gradient-guided search compared with random search. As described in Section III-B, RANUM leverages both first-order gradients (in step a) and secondorder derivatives (in step b) to guide the search of system tests. In contrast, \u201cRandom\u201d uses random sampling hoping that failure-exhibiting training examples can emerge after sufficient sampling. Hence, when such training examples are relatively rare in the whole valid input space, \u201cRandom\u201d is less effective. We conduct an ablation study (in Suppl. F) for showing that RANUM improves over \u201cRandom\u201d in both steps inside RANUM.\nFailing-Case Analysis. We study all six defects where RANUM may fail and have the following findings. (1) For four defects (Case IDs 1, 15, 37, and 38), the architecture is\nchallenging for gradient-based optimization, e.g., due to the Min/Max/Softmax operators that provide little or no gradient information. We leave it as future work to solve these cases, likely in need of dynamically detecting operators with vanishing gradients and reconstructing the gradient flow. (2) Two defects (Case IDs 28a and 28b) correspond to those caused by Div operators where only a close-to-zero divisor can trigger a numerical failure. Hence, for operators with narrow invalid ranges, RANUM may fail to generate failure-exhibiting system tests."
        },
        {
            "heading": "C. RQ3: Fix Suggestion",
            "text": "In task 3\u00a9, RANUM suggests fixes for numerical defects. We compare RANUM with fixes generated by baseline approaches and developers\u2019 fixes.\n1) Comparison between RANUM and Baselines: RANUM is the first approach for this task, and we propose two baseline approaches to compare with. (1) RANUM-E: this approach changes the abstraction domain of RANUM from interval with tensor partitioning to standard interval.\nTo some degree, RANUM-E represents the methodology of conventional static analysis tools that use standard interval domain for abstraction and search of effective fixes. (2) GD: this approach uses standard gradient descent for optimization instead of the abstraction optimization technique in RANUM.\nEvaluation Protocol. We evaluate whether each approach can generate fixes that eliminate all numerical defects for the DNN architecture under analysis given imposing locations. We consider three types of locations: on both weight and input nodes, on only weight nodes, and on only input nodes. In practice, model providers can impose fixes on weight nodes by clipping weights after a model is trained; and users can impose fixes on input nodes by clipping their inputs before loading them into the model. Since all approaches are deterministic, for each case we run only once. We say that the fix eliminates all numerical defects if and only if (1) the RANUM static analysis framework cannot detect any defects from the fixed architecture; and (2) 1,000 random samples cannot trigger any numerical failures after imposing the fix.\nEvaluation Result. We report the statistics, including the number of successful cases among all the 79 cases and the total running time, in Table III. From the table, we observe that on all the three imposing location settings, RANUM always succeeds in most cases and spends much less time. For example, when fixes can be imposed on both weights and input nodes, RANUM succeeds on all cases with a total running time 54.23 s. In contrast, RANUM-E requires > 10\u00d7 time, and GD succeeds in only 72.15% cases. Hence, RANUM is substantially more effective and efficient for suggesting fixes compared to baseline approaches.\nSince RANUM is based on iterative refinement (see Algorithm 1), we study the number of iterations needed for finding the fix. When fixes can be imposed on both weight and input nodes, where RANUM succeeds on all the 79 cases, the average number of iterations is 29.80, the standard deviation is 14.33, the maximum is 53, and the minimum is 2. Hence, when RANUM can find the fix, the number of iterations is small, coinciding with the small total running time 54.23 s.\nDiscussion. The two baseline approaches can be viewed as ablated versions of RANUM. Comparing RANUM and GD, we conclude that the technique of abstraction optimization substantially improves the effectiveness and also improves the efficiency. Comparing RANUM and RANUM-E, we conclude that the interval abstraction with tensor partitioning as the abstraction domain substantially improves the efficiency and also improves the effectiveness.\nFrom Table III, it is much easier to find the fix when imposing locations are weight nodes compared to input nodes. Since model providers can impose fixes on weights and users impose on inputs, this finding implies that fixing numerical defects on the providers\u2019 side may be more effective than on the users\u2019 side.\n2) Comparison between RANUM and Developers\u2019 Fixes: We conduct an empirical study to compare the fixes generated by RANUM and by the developers.\nEvaluation Protocol. We manually locate GitHub repositories from which the GRIST benchmarks are constructed. Among the 79 cases, we find the repositories for 53 cases on GitHub and we study these cases. We locate the developers\u2019 fixes of the numerical defects by looking at issues and followup pull requests. Since RANUM suggests different fixes for different imposing locations, for each case we first determine the imposing locations from the developer\u2019s fix, and then compare with RANUM\u2019s fix for these locations.\nRANUM fixes are on the computational graph and developers\u2019 fixes are in the source code, so we determine to conduct code-centered comparison: RANUM fixes are considered feasible only when the fixes can be easily implemented by code (within 10 lines of code) given that developers\u2019 fixes are typically small, usually in 3-5 lines of code. In particular, our comparison is based on two criteria: (1) which fix is sound on any valid input; (2) if both are sound, which fix hurts less to model performance and utility (based on the span of imposed precondition, the larger span the less hurt). Two authors independently classify the comparison results for each case and discuss the results to reach a consensus.\nResults. We categorize the comparison results as below. A (30 cases) Better than developers\u2019 fixes or no available\ndeveloper\u2019s fix. Developers either propose no fixes or use heuristic fixes, such as reducing the learning rate or using the mean value to reduce the variance. These fixes may work in practice but are unsound, i.e., cannot rigorously guarantee the elimination of the numerical defect for any training or inference data. In contrast, RANUM generates better fixes since these fixes rigorously eliminate the defect. B (7 cases) Equivalent to developers\u2019 fixes. Developers and RANUM suggest equivalent or highly similar fixes. C (13 cases) No need to fix. For these cases, there is no need to fix the numerical defect in the given architecture. There are mainly three reasons. (1) The DNN is used in the whole project with fixed weights or test inputs. As a result, although the architecture contains defects, no system failure can be caused. (2) The architecture is injected a defect as a test case for automatic tools, such as a test architecture in the TensorFuzz [27] repository. (3) The defect can be hardly exposed in practice. For example, the defect is in a Div operator where the divisor needs to be very close to zero to trigger a divide-by-zero failure, but such situation hardly happens in practice since the divisor is randomly initialized. D (3 cases) Inferior than developers\u2019 fixes or RANUMgenerated fixes are impractical. In two cases, RANUMgenerated fixes are inferior to developers\u2019 fixes. Human developers observe that the defective operator is Log, and its input is non-negative. So they propose to add 10\u22126\nto the input of Log as the fix. In contrast, RANUM can generate only a clipping-based fix, e.g., clipping the input if it is less than 10\u22126. When the input is small, RANUM\u2019s fix interrupts the gradient flow from output to input while the human\u2019s fix maintains it. As a result, the human\u2019s fix does less hurt to the model\u2019s trainability and is better than\nRANUM\u2019s fix. In another case, the RANUM-generated fix imposes a small span for some model weights (less than 0.1 for each component of that weight node). Such a small weight span strongly limits the model\u2019s expressiveness and utility. We leave it as the future work to solve these limitations.\nFrom the comparison results, we can conclude that for the 40 cases where numerical defects are needed to be fixed (excluding case C), RANUM suggests equivalent or better fixes than human developers in 37 cases. Therefore, RANUM is comparably effective as human developers in terms of suggesting numerical-defect fixes, and is much more efficient since RANUM is an automatic approach.\nGuidelines for Users. We discuss two practical questions for RANUM users. (1) Does RANUM hurt model utility, e.g., inference accuracy? If no training or test data ever exposes a numerical defect, RANUM does not confirm a defect and hence no fix is generated and there is no hurt to the utility. If RANUM confirms numerical defects, whether the fix affects the utility depends on the precondition-imposing locations. If imposing locations can be freely selected, RANUM tends to impose the fix right before the vulnerable operator, and hence the fix does not reduce inference performance. The reason is that the fix changes (by clipping) the input only when the input falls in the invalid range of the vulnerable operator. In practice, if the imposing locations cannot be freely selected and need to follow developers\u2019 requirements, our preceding empirical study shows that, in only 3 out of 40 cases, compared with developers\u2019 fixes, our fixes incur larger hurt to the inference or training performance of the architecture. (2) Should we always apply RANUM to fix any architecture? We can always apply RANUM to fix any architecture since RANUM fixes do not visibly alter the utility in most cases. Nonetheless, in deployment, we recommend first using RANUM to confirm defect feasibility. If there is no such failure-exhibiting system test, we may not need to fix the architecture; otherwise, we use RANUM to generate fixes."
        },
        {
            "heading": "V. RELATED WORK",
            "text": "Understanding and Detecting Defects in DNNs. Discovering and mitigating defects and failures in DNN based systems is an important research topic [60, 32, 14]. Following the taxonomy in previous work [12], DNN defects are at four levels from bottom to top. (1) Platform-level defects. Defects can exist in real-world DL compilers and libraries. Approaches exist for understanding, detecting, and testing against these defects [49, 37, 42, 52]. (2) Architecture-level defects. Our work focuses on numerical defects, being one type of architecture-level defects. Automatic detection and localization approaches [50, 19] exist for other architecturelevel defects such as suboptimal structure, activation function, and initialization and shape mismatch [11]. (3) Model-level defects. Once a model is trained, its defects can be viewed as violations of desired properties as discussed by Zhang et al. [55]. Some example defects are correctness [44, 9], robustness [48], and fairness [57] defects. (4) Interface-level\ndefects. DNN-based systems, when deployed as services, expose interaction interfaces to users where defects may exist, as shown by empirical studies on real-world systems [12, 46, 47].\nTesting and Debugging for DNNs. A rich body of work exists for testing and debugging DNN defects [55]. Some representatives are DeepXplore [31] and DeepGauge [22]. Recent work enables automatic model debugging and repair via consistency checking [51], log checking [59], spectrum analysis [34], or analyzer-guided synthesis [41].\nDNN Static Analysis. Another solution for eliminating DNN defects is conducting static analysis to rigorously guarantee the non-existence of defects [17, 2]. Although DNNs essentially conduct numerical computations, traditional tools of numerical analysis [10, 39] are inefficient for DNN analysis due to lack of support for multi-dimensional tensor computations. Recently, static analysis tools customized for DNNs are emerging, mainly focusing on proposing tighter abstractions [6, 26, 29] or incorporating abstractions into training to improve robustness [16, 25, 62]. Besides robustness, static analysis has also been applied to rigorously bound model difference [30]. Our approach includes a static analysis framework customized for numerical-defect detection and fixing.\nDetecting and Exposing Numerical Defects in DNNs. Despite the widespread existence of numerical defects in realworld DNN-based systems [60, 12, 14], only a few automatic approaches exist for detecting and exposing these defects. To the best of our knowledge, DEBAR [61] and GRIST [54] are the only two approaches. We discuss and compare RANUM with both approaches extensively in Sections III and IV."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this paper, we have presented a novel automatic approach named RANUM for reliability assurance of DNNs against numerical defects. RANUM supports detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. RANUM includes multiple novel extensions and optimizations upon existing tools, and includes three novel techniques. Our extensive evaluation on real-world DNN architectures has demonstrated high effectiveness and efficiency of RANUM compared to both the stateof-the-art approaches and developers\u2019 fixes.\nData Availability. All artifacts including the tool source code and experiment logs are available and actively maintained at https://github.com/llylly/RANUM."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work is sponsored by the National Natural Science Foundation of China under Grant No. 62161146003, the National Key Research and Development Program of China under Grant No. 2019YFE0198100, the Innovation and Technology Commission of HKSAR under Grant No. MHP/055/19, and the Tencent Foundation/XPLORER PRIZE."
        },
        {
            "heading": "A. List of Supported Operators",
            "text": "In this section, we provide a list of the 84 supported operators in the RANUM static analysis framework. These operators cover common operators used in DNNs. To the best of our knowledge, RANUM provides abstractions for the largest number of operator types among existing DNN abstraction frameworks. We believe that the framework can be further extended to support other types of analysis, testing, and debugging for general DNN architectures or models such as robustness analysis and fairness testing.\nIn particular, when compared to the state-of-the-art tool DEBAR [61], the underlined 17 operators are newly supported by RANUM. Sub Add Mul Div Pow MatMul Gemm MaxPool GlobalMaxPool GlobalAveragePoolCos Sin AveragePool Conv ConvTranspose Pad Reciprocal Sqrt Tanh Relu Softplus LeakyRelu Softsign Sigmoid Neg Exp Log Softmax LogSoftmax Abs Ceil Floor Sign Reshape Flatten Transpose Shape Cast Slice Gather GatherND Squeeze Unsqueeze ScatterElements Expand Concat Split Identity ConstantOfShape RandomNormalLike RandomUniformLike RandomNormal RandomUniform Range Constant Less LessOrEqual Greater GreaterOrEqual Equal Not Or Min Max Clip Sum ReduceMin ReduceMax ReduceSum ReduceMean ArgMax ArgMin Tile NegativeLogLikelihoodLoss Loop SequenceInsert BatchNormalization OneHot NonZero Resize ReduceProd ReduceSumSquare IsInf IsNaN"
        },
        {
            "heading": "B. List of Operators with Potential Numerical Defects",
            "text": "In this section, we provide a full list of DNN operators that may contain numerical defects, along with their invalid ranges In0,invalid (see definition of the numerical defect in Definition 1), respectively. In the table, Umin and Umax stand for the minimum and maximum positive number of the input tensor\u2019s data type, respectively.\nOp. Type In0,invalid Pow [\u2212Umin, Umin]\u00d7 (\u2212\u221e,\u2212Umin] Div R\u00d7 [\u2212Umin, Umin]\nReciprocal [\u2212Umin, Umin] Sqrt (\u2212\u221e, Umin] Exp [lnUmax,\u221e) Log (\u2212\u221e, Umin] Range R\u00d7 R\u00d7 [\u2212Umin, Umin]\nNegativeLogLikelihoodLoss [0, 0] for number of non-zero cells using mean reduction"
        },
        {
            "heading": "C. Detail Description of RANUM Static Analysis Framework",
            "text": "We present the omitted details in Section III-A.\n1) Abstraction Domain and Characteristics: We first formally define our abstraction domain: interval with tensor partitioning. Following the notation in abstract interpretation literature [5], suppose the tensor has m dimensions with shape n1 \u00d7 n2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nm, we define the abstract domain as such:\nA := {\u3008l,u, (Si)mi=1\u3009 : l,u \u2208 Rn \u2032 1\u00d7\u00b7\u00b7\u00b7\u00d7n \u2032 m\n|Si| = n\u2032i,\u2200s \u2208 Si, s \u2208 N, 0 \u2264 s < ni}. (6)\nEach element in A is a triplet a = \u3008l,u, (Si)mi=1\u3009, where the first two elements are subblock-wise interval lower bound and upper bound, respectively, and the last element (S1, S2, . . . , Sm) contains m sets, where each set Si corresponds to the 0-indexed split indices for the i-th dimension to form subblocks. We use Si[j] \u2208 N to represent the jth element of split index set Si sorted in ascending order and define Si[|Si|] = ni. Figure 5 illustrates this abstraction domain.\nTo define the correspondence between the abstract domain and concrete domain C := 2Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nm , we form Galois connection \u3008C,\u2286\u3009\n\u03b1 \u03b3 \u3008A,v\u3009, where an abstraction function\n\u03b1 : C \u2192 A and the concretization function \u03b3 : A \u2192 C are defined as follows:\n\u03b1(X ) = \u3008l,u, (Si)mi=1\u3009 where li1,i2,...,im = min\nx\u2208X min\n1\u2264k\u2264m Sk[ik]\u2264jk<Sk[ik+1]\nxj1,j2,...,jm ,\nui1,i2,...,im = max x\u2208X\nmax 1\u2264k\u2264m\nSk[ik]\u2264jk<Sk[ik+1]\nxj1,j2,...,jm ,\n(7a)\n\u03b3(\u3008l,u, (Si)mi=1\u3009) = {x :li1,i2,\u00b7\u00b7\u00b7 ,im \u2264 xj1,j2,\u00b7\u00b7\u00b7 ,jm \u2264 ui1,i2,\u00b7\u00b7\u00b7 ,im , Sk[ik] \u2264 jk < Sk[ik + 1], 1 \u2264 k \u2264 m}. (7b) In Equation (7a), the split indices (Si)mi=1 can be arbitrarily chosen but need to be kept consistent with those in Equation (7b). Take Figure 5 as the example, to abstract a set of tensors X with shape 12 \u00d7 9 \u00d7 4, we define split indices for each dimension, respectively, then impose interval constraints on each subblock of the tensor. For example, [l0,0,0,u0,0,0] constrain any element in x0:2,0:2,0:1, [l3,2,1,u3,2,1] constrain any element in x9:11,6:8,2:3.\nAbstraction Characteristics. There are multiple ways to compute the abstractions, and we design our particular computational algorithms to achieve soundness, tightness, and differentiability. (1) Soundness: We guarantee that all abstractions are sound. Formally, suppose ax and aw are the abstractions for input and weights, respectively, we guarantee f\u2217n(\u03b3(ax); \u03b3(aw)) \u2286 \u03b3(T \u2217n(ax; aw)) where \u2217 \u2208 {in, out}, n is any node in the architecture, and T \u2217n(ax; aw) is our computed abstract domain for input or output of node n. The soundness property theoretically guarantees that our detection approach has no false negatives, i.e., flag all potential numerical defects, and the validity of generated preconditions (see Section II-B). (2) Tightness: For most operators in DNN, given abstractions for its inputs, we compute the tightest possible interval abstraction for the output. Formally, for any atomic DNN operator op and any abstract domain i \u2208 A of op\u2019s input, if op(\u03b3(i)) \u2286 [l,u], i.e., [l,u] is an interval abstraction of op\u2019s output, then \u03b3(Top(i)) \u2286 [l,u], i.e., our generated abstract domain Top(i) is always tighter or equal to any interval abstraction [l,u]. Such tightness can reduce false positives for detecting potential numerical defects and increase the span of generated failure-exhibiting intervals and fix intervals, which improves the quality of generated tests and preconditions. (3) Differentiability: We compute differentiable abstractions. Concretely, if the input of node n2 is deferentially dependent on output of node n1, we can compute out gradients \u2207io for any i \u2208 {ln1 ,un1} and any o \u2208 {ln2 ,un2}, where \u3008ln1 ,un1 , (S n1 i ) dim(n1.out) i=1 \u3009 and \u3008ln2 ,un2 , (S n2 i ) dim(n2.in) i=1 \u3009 are abstractions of foutn1 (\u00b7; \u00b7) and f in n2(\u00b7; \u00b7), respectively. When tightness and differentiability cannot be achieved at the same time (e.g., for floor operator, tight abstraction \u3008l,u\u3009 7\u2192 \u3008blc, buc\u3009 is not generally differentiable, and differentiable abstraction \u3008l,u\u3009 7\u2192 \u3008l\u22121,u\u3009 is not tight), we implement two abstract algorithms to achieve tightness and differentiability, respectively.\nThe existing approach DEBAR [61] also proposes a static analysis framework for DNN architectures with tensor partitioning. In contrast to our framework, the abstract domain in DEBAR contains affine equalities besides interval domains. Therefore, DEBAR can be tighter than ours in some cases and can produce fewer positives, but due to the additional complexity of affine equalities, DEBAR tends to use the coarsest abstraction (i.e., tensor partitioning) granularity and supports fewer operators than ours. At the same time, our algorithms produce the tightest interval abstractions while DEBAR has no tightness guarantee. As a result, RANUM detects more true numerical defects and has a comparable number of false positives (see Section IV-A), and we can leverage the feasibility confirmation support in RANUM to filter out false positives.\n2) Initial Abstraction Construction with Backward FineGrained Node Labeling: We construct abstract domains for initial nodes in two steps: First, we determine the tensor partitions, i.e., the split index sets (Si)mi=1 (see Equation (6)), which determine the tightness and efficiency of our static analysis framework, because the tensor partitions of all other\nnodes will be solely dependent on the partitions of initial nodes as we will show in Suppl. C3. Second, we compute the interval bounds l and u.\nWe use the following principle to decide the tensor partitions: For nodes that are connected to operators requiring fine-grained abstractions (e.g., the shape input for operator Reshape) with valid paths (which we will specify later), we construct tensor partitions with the finest granularity, i.e., Si = {0, 1, . . . , ni}. We call these nodes fine-grained initial nodes and starting from the next paragraph we introduce our novel technique of backward fine-grained node labeling to find them out. For other nodes, we rely on downstream task requirements and user specifications to determine the partitions. For example, for fix generation (see Section III-C), we use the coarsest granularity by default.\nBackward Fine-Grained Node Labeling. The fine-grained initial nodes are those starting a valid path in DNN computational graph G, where a path is valid if and only if the path does not traverse through fine-grained stopping operators and terminates at a fine-grained requiring operator with some specific input indices. As discussed in Section III-A, the finegrained requiring operators fall into three categories: controlflow operators (e.g., Loop), indexing operators (e.g., Slice), and shaping operators (e.g., Reshape). Fine-grained stopping operators are those whose output is independent of input abstraction granularity (so the granularity of preceding nodes does not matter). Detail lists of fine-grained stopping operators and fine-grained requiring operators are provided in Suppl. C4.\nTo find out fine-grained initial nodes, we propose backward fine-grained node labeling, which is similar to data dependency analysis in traditional programs: First, we invert all edges of the given computational graph G = \u3008V, E\u3009 to get G\u2032 = \u3008V, E \u2032\u3009. Second, we attach a boolean label for each node n \u2208 V and initialize them with False. Third, we do topology sorting on G\u2032. When encountering a node n with True, if the node does not contain a fine-grained stopping operator, we propagate this label to all its subsequent nodes; otherwise, we propagate False. When encountering a node n with False, if the node is a fine-grained requiring operator, we propagate True to some subsequent nodes corresponding to specific input indices and False to others; otherwise, propagate False to all subsequent nodes. Lastly, we collect all labels for initial nodes. The nodes with True label are fine-grained initial nodes.\nTo this point, we have determined the tensor partitions (Si) m i=1 for all initial nodes, and we now compute l and u and thus finish the abstraction domain construction for the valid ranges of initial nodes. Initial nodes are further divided into input, weight, and constant nodes. In practice, most weight nodes and all constant nodes have their initial values stored in the ONNX file, and we directly use Equation (7a) with these initial values to compute l and u. Otherwise, we rely on user specifications and built-in heuristics to determine l and u for initial nodes.\n3) Internal Abstraction with Dynamic Partitioning: For all supported operator types (listed in Suppl. A), we propose concrete algorithms to compute abstract domains for output\nwith dynamic tensor partitioning. Formally, for each operator op, we construct the computable function Top : A \u2192 A that satisfies soundness and tentatively satisfies tightness and differentiability, where A is the abstract domain defined in Equation (6). Therefore, following the computational procedure introduced at the end of Section III-A, we can compute end-toend abstractions for all nodes in the given DNN architecture. As the showcase of our abstraction algorithm, we describe the algorithms for four representative operators: MatMul, Conv, Softmax, and Loop. MatMul. The MatMul operator computes the matrix multiplication of two operands. This operator is widely used in DNNs to express fully-connected layers. To simplify the narration, we focus on the two-dimensional case where we compute op(A,B) = C = AB with A \u2208 Rn\u00d7m and B \u2208 Rm\u00d7l. Extensions to other dimensions by abstracting the broadcasting mechanism can be found in our open-source implementation.\nWe denote the input abstractions of op by a = \u3008La,Ua, (S1A, S2A)\u3009 and b = \u3008Lb,Ub, (S1B , S2B)\u3009, respectively. First, we compute the union U = S2A \u222a S1B . Second, we dynamically partition both a and b with split points (S1A, U) and (U, S2B), respectively, and get a\n\u2032 = \u3008L\u2032a,U \u2032a, (S1A, U)\u3009 and b\u2032 = \u3008L\u2032b,U \u2032b, (U, S2B)\u3009. Note that a and a\u2032 (or b and b\u2032) correspond to the same concrete domain, but a\u2032 and b\u2032 have finer or equal partition granularity than a and b. Third, we compute output abstraction Top(a, b) = c = \u3008Lc,Uc, (S1A, S2B)\u3009\nwhere (Lc)ij = |U|\u2211 k=1 vk min A\u2208{L\u2032a,U \u2032a},B\u2208{L\u2032b,U \u2032 b } AikBkj ,\n(Uc)ij = |U|\u2211 k=1 vk max A\u2208{L\u2032a,U \u2032a},B\u2208{L\u2032b,U \u2032 b } AikBkj ,\nvk = U [k]\u2212 U [k \u2212 1].\n(8)\nThis formulation can guarantee tightness but is not efficient for tensor computation due to the inner minimum and maximum. Therefore, we also implement a fast-mode abstraction trading tightness for efficiency: T \u2032op(a, b) = c\n\u2032 = \u3008L\u2032c,U \u2032c, (S1A, S2B)\u3009 where\nL \u2032 c =U \u2032\u2212 a vU \u2032\u2212 b +U \u20320 a vL \u2032\u2212 b +U \u2032+ a vL \u2032\u2212 b +\nL \u2032\u2212 a vU \u20320 b +U \u20320 a vL \u20320 b +L \u20320 a vU \u20320 b +U \u2032+ a vL \u20320 b + L \u2032\u2212 a vU \u2032+ b +L \u20320 a vU \u2032+ b +L \u2032+ a vL \u2032+ b ,\nU \u2032 c =L \u2032\u2212 a vL \u2032\u2212 b +L \u20320 a vL \u2032\u2212 b +L \u2032+ a vU \u2032\u2212 b +\nL \u2032\u2212 a vL \u20320 b +L \u20320 a vL \u20320 b +U \u20320 a vU \u20320 b +U \u2032+ a vU \u20320 b + U \u2032\u2212 a vL \u2032+ b +U \u20320 a vU \u2032+ b +U \u2032+ a vU \u2032+ b .\n(9)\nIn the above equation, for any \u2217 \u2208 {a, b}, let \u25e6 be the elementwise (Hadamard) product,\nL \u2032\u2212 \u2217 =L \u2032 \u2217 \u25e6 I[U \u2032 \u2217 < 0],U \u2032\u2212 \u2217 = U \u2032 \u2217 \u25e6 I[U \u2032 \u2217 < 0],\nL \u20320 \u2217 =L \u2032 \u2217 \u25e6 I[L \u2032 \u2217 \u2264 0,U \u2032 \u2217 \u2265 0],U \u20320 \u2217 = U \u2032 \u2217 \u25e6 I[L \u2032 \u2217 \u2264 0,U \u2032 \u2217 \u2265 0], L \u2032+ \u2217 =L \u2032 \u2217 \u25e6 I[L \u2032 \u2217 > 0],U \u2032+ \u2217 = U \u2032 \u2217 \u25e6 I[L \u2032 \u2217 > 0].\n(10) From Equation (9), we can observe that the abstraction can be easily implemented with tensor computations. In Suppl. C5, we prove the soundness and tightness of these abstractions.\nConv. The Conv operator computes the discrete convolution of two operands. This operator is widely used in convolutional neural networks (CNNs, [15]). To simplify the narration, we focus on the single-channel single-stride two-dimensional case where we compute op(A,W ) = C with A being the input matrix and W being the convolution kernel. Extensions to general cases are provided in our open-source implementation.\nWe first dynamically split the kernel abstraction to the finest granularity. Second, we compute the receptive field, which is the sub-region of A that decides each output position, of each output position. Third, we inspect the alignment between receptive fields and A\u2019s partitions. If neighboring output positions have their receptive fields partitioned in the same sub-block of A, it means that these positions can be abstracted by a single interval, i.e., these positions can be partitioned together. Fourth, using this principle, we derive the partitions of the output tensor, and repeat the input tensors accordingly and efficiently so that the abstract computation can be written as convolutional operations. Last, we modify the abstraction computation equations in Equation (9) by replacing matrix multiplication with convolution to compute the abstraction of output C. Softmax. The Softmax operator computes normalized exponential values for the given input. Softmax is widely deployed for classification tasks to yield normalized confidence. In one-dimensional case, for input x \u2208 Rn, a Softmax operator outputs op(x) = exp(x)\u2211n\ni=1 exp(x)i . The output\nabstraction of Softmax operator op can be thus computed: Top(\u3008l,u, (S)\u3009) = \u3008lo,uo, (S)\u3009 where\nloi = exp(li)\nexp(u)Tv \u2212 exp(ui) + exp(li) ,uoi =\nexp(ui)\nexp(l)Tv \u2212 exp(li) + exp(ui) ,\nvk = S[k]\u2212 S[k \u2212 1]. (11)\nThe output abstraction\u2019s partition is dynamically decided by the input abstraction\u2019s partition. We prove the soundness and tightness of the abstraction in Suppl. C5. Loop. The node with a Loop operator contains a subcomputational graph with dynamic controlling counters and conditions to represent a runtime loop. The Loop operator can be used to represent recurrent neural networks (RNNs) and handle the input sequence of variable length. We require the controlling counter and loop termination condition to have the finest abstraction granularity. Then, we recursively apply our static analysis framework to the sub-graph representing the loop body and update the interval of the loop counter iteratively. When the abstraction interval of the loop counter can explicitly decide whether to terminate the loop, we continue or terminate the loop iterations, respectively; otherwise, we merge the current abstraction interval with the interval obtained after another iteration. We repeat this process until termination. Theoretically, this execution process cannot guarantee the termination, i.e., the loop body may execute for infinite times. However, in practice, on our dataset, our analysis framework already suffices to guarantee the termination in all cases. In the future, we can apply a widening operation if termination cannot be tightly abstracted.\nRemark. As we can see, the novel dynamic partition technique is incorporated into the computation process of each operator\u2019s abstraction. The soundness and tightness of our designed abstractions are immediately achieved by design, and the differentiability of our designed abstractions can be implemented via the auto-differentiation functionality of popular DL libraries like PyTorch [28] and Tensorflow [1] where we use PyTorch for implementation. The abstractions for DNNs are also implemented for other applications, e.g., for robustness verification [40, 25]. However, our abstractions are tailored for the tensor-partitioned interval domain that is particularly suitable for testing and debugging the numerical defects. To the best of our knowledge, these abstractions are the first that achieve soundness, tightness, and differentiability.\n4) List of Fine-Grained Requiring and Stopping Operators: We use fine-grained requiring and fine-grained stopping operators in Suppl. C2. Among all supported operators, the finegrained requiring operators are\nReshape (input #2), Slice (input #2, #3, #4, #5), Squeeze (input #2), Unsqueeze (input #2), Tile (input #2, #3), Loop (input #1, #2), SequenceInsert (input #3), ConstantOfShape (input #1), Gather (input #2), GatherND (input #2), ReduceSum (input #2), ScatterElements (input #2), Expand (input #2), Split (input #2), Pad (input #2, #3), NegativeLogLikelihoodLoss (input #2), Clip (input #2, #3), OneHot (input #2), Resize (input #2, #3, #4).\nThe fine-grained stopping operators are\nShape, RandomNormalLike, RandomUniformLike.\n5) Proofs: Here we present the omitted proofs in Suppl. C3. MatMul.\nTheorem 1 (Tightness of Abstraction by Equation (8) for MatMul). Suppose op is the MatMul operator and Top is as defined in Equation (8), then if op(\u03b3(a), \u03b3(b)) \u2286 [L,U ] for a, b \u2208 A, \u03b3(Top(a, b)) \u2286 [L,U ]. Proof. We use La and Ua to denote the element-wise interval lower and upper bounds of \u03b3(a); and use Lb and Ub to denote these bounds of \u03b3(b), respectively, where a formal definition is in Equation (7b). Then, there exist A and B with La \u2264 A \u2264 Ua and Lb \u2264 B \u2264 Ub, such that\n(AB)ij = l\u2211 k=1 min{La,ikLb,kj ,La,ikUb,kj ,\nUa,ikLb,kj ,Ua,ikUb,kj}, (12)\nor (AB)ij = l\u2211\nk=1\nmax{La,ikLb,kj ,La,ikUb,kj ,\nUa,ikLb,kj ,Ua,ikUb,kj}. (13)\nBy definition, we have\nLij \u2264 Equation (12),Uij \u2265 Equation (13). (14)\nWe let L\u2032c and U \u2032 c to denote the element-wise interval lower and upper bounds for \u03b3(Top(a, b)), let S1A[i \u2032] \u2264 i\u22121 < S1A[i\u2032+\n1] and S2B [j \u2032] \u2264 j \u2212 1 < S2B [j\u2032 + 1], then from Equation (8),\n(L\u2032c)ij = (Lc)i\u2032j\u2032\n= |U|\u2211 k=1 U [k]\u2211 k\u2032=U [k\u22121]+1 min{La,ik\u2032Lb,k\u2032j ,La,ik\u2032Ub,k\u2032j ,\nUa,ik\u2032Lb,k\u2032j ,Ua,ik\u2032Ub,k\u2032j}\n= l\u2211 k=1 min{La,ikLb,kj ,La,ikUb,kj ,Ua,ikLb,kj ,Ua,ikUb,kj} =Equation (12) \u2265 Lij .\n(15)\nSimilarly, (U \u2032c)ij \u2264 Uij . Thus, \u03b3(Top(a, b)) \u2286 [L,U ].\nTheorem 2 (Soundness of Abstraction by Equation (8) for MatMul). Suppose op is the MatMul opeartor and Top is as defined in Equation (8), then op(\u03b3(a), \u03b3(b)) \u2286 \u03b3(Top(a, b)).\nProof. We use La and Ua to denote the element-wise interval lower and upper bounds of \u03b3(a); and use Lb and Ub to denote these bounds of \u03b3(b), respectively, where a formal definition is in Equation (7b). For any A and B such that La \u2264 A \u2264 Ua and Lb \u2264 B \u2264 Ub,\n(AB)ij = l\u2211 k=1 AikBkj\n\u2265 min{La,ikLb,kj ,La,ikUb,kj ,Ua,ikLb,kj ,Ua,ikUb,kj} =: (L\u2032ab)ij\n(16) and (AB)ij \u2264 max{La,ikLb,kj ,La,ikUb,kj ,Ua,ikLb,kj ,Ua,ikUb,kj}\n=: (U \u2032ab)ij . (17)\nThus, op(\u03b3(a), \u03b3(b)) \u2286 [L\u2032ab,U \u2032ab]. On the other hand, \u03b3(Top(a, b)) = [L \u2032 ab,U \u2032 ab] as seen from Equation (15). Therefore, op(\u03b3(a), \u03b3(b)) \u2286 \u03b3(Top(a, b)).\nTheorem 3 (Soundness of Abstraction by Equation (9) for MatMul). Suppose op is the MatMul operator and T \u2032op is as defined in Equations (9) and (10), then op(\u03b3(a), \u03b3(b)) \u2286 \u03b3(T \u2032op(a, b)).\nProof. We use La and Ua to denote the element-wise interval lower and upper bounds of \u03b3(a); and use Lb and Ub to denote these bounds of \u03b3(b), respectively, where a formal definition is in Equation (7b). We define L\u2032ab and U \u2032 ab by Equations (16) and (17). From the proof of Theorem 2, we have op(\u03b3(a), \u03b3(b)) \u2286 [L\u2032ab,U \u2032ab]. We now only need to show that [L\u2032ab,U \u2032 ab] \u2286 \u03b3(T \u2032op(a, b)).\n\u03b3(T \u2032op(a, b)) imposes independent interval abstractions element-wise, therefore, we study each element independently. For the element (i, j), from Equations (9) and (10), the interval lower bound of \u03b3(T \u2032op(a, b)), namely (Lc)ij , satisfies\n(Lc)ij = l\u2211 k=1 (U\u2212a )ik(U \u2212 b )kj + (U 0 a )ik(L \u2212 b )kj + (U + a )ik(L \u2212 b )kj+ (L\u2212a )ik(U 0 b )kj + (U 0 a )ik(L 0 b)kj + (L 0 a)ik(U 0 b )kj + (U + a )ik(L 0 b)kj+ (L\u2212a )ik(U + b )kj + (L 0 a)ik(U + b )kj + (L + a )ik(L + b )kj . (18) By Equation (10),\n\u2022 when La,ik \u2264 Ua,ik < 0, (L\u2212a )ik = La,ik, (U \u2212 a )ik = Ua,ik, (L 0 a)ik = 0,\n(U0a )ik = 0, (L + a )ik = 0, (U + a )ik = 0;\n\u2022 when La,ik \u2264 0 \u2264 Ua,ik, (L\u2212a )ik = 0, (U \u2212 a )ik = 0, (L 0 a)ik = La,ik,\n(U0a )ik = Ua,ik, (L + a )ik = 0, (U + a )ik = 0;\n\u2022 when 0 < La,ik \u2264 Ua,ik, (L\u2212a )ik = 0, (U \u2212 a )ik = 0, (L 0 a)ik = 0,\n(U0a )ik = 0, (L + a )ik = La,ik, (U + a )ik = Ua,ik.\nSimilarly for (L\u2212b )kj , (U \u2212 b )kj , (L 0 b)kj , (U 0 b )kj , (L + b )kj and (U+b )kj . Thus, by enumerating all cases, we have\nEquation (18) \u2264 min{La,ikLb,kj ,La,ikUb,kj , Ua,ikLb,kj ,Ua,ikUb,kj} = (L\u2032ab)ij . (19)\nSimilarly, the interval lower bound of \u03b3(T \u2032op(a, b)), namely (Uc)ij ,\n(Uc)ij \u2265 max{La,ikLb,kj ,La,ikUb,kj , Ua,ikLb,kj ,Ua,ikUb,kj} = (U \u2032ab)ij .\n(20)\nThus, [L\u2032ab,U \u2032 ab] \u2286 \u03b3(T \u2032op(a, b)).\nSoftmax.\nTheorem 4 (Tightness of Abstraction for Softmax). Suppose op : Rn \u2192 Rn is the Softmax operator and Top is as defined in Equation (11), if op(\u03b3(\u3008l,u, (S)\u3009)) \u2286 [lr,ur], then \u03b3(Top(\u3008l,u, (S)\u3009)) \u2286 [lr,ur].\nProof. We use l\u2032 and u\u2032 to denote the element-wise interval lower and upper bounds of \u03b3(\u3008l,u, (S)\u3009). Formally, l\u2032i = li\u2032 and u\u2032i = ui\u2032 where S[i\n\u2032] \u2264 i\u2212 1 < S[i\u2032 + 1]. Then, for each i, by setting xi = li and xj = uj for all j 6= i, we get\nlri \u2264 exp(l\u2032i) exp(l\u2032i) + \u2211n k=1,k 6=i exp(u \u2032 k)\n= exp(li\u2032) exp(li\u2032) + \u2211n k=1 exp(u \u2032 k)\u2212 exp(u\u2032i) = exp(li\u2032)\nexp(li\u2032) + vT exp(uk)\u2212 exp(ui\u2032) = loi\u2032 .\n(21)\nIn addition, by setting xi = ui and xj = lj for all j 6= i, we get uri \u2265 uoi\u2032 . Here, lo and uo are as defined in Equation (11). Combining these two arguments, we get\n\u03b3(Top(\u3008l,u, (S)\u3009)) \u2286 [lr,ur].\nTheorem 5 (Soundness of Abstraction for Softmax). Suppose op : Rn \u2192 Rn is the Softmax operator and Top is as defined in Equation (11), then op(\u03b3(\u3008l,u, (S)\u3009)) \u2286 \u03b3(Top(\u3008l,u, (S)\u3009)).\nProof. Leveraging the fact that the softmax function is monotonically increasing, i.e., dop(x)idxj > 0, we have \u03b3(\u3008l,u, (S)\u3009)i \u2208 [loi\u2032 ,uoi\u2032 ], where S[i\u2032] \u2264 i \u2212 1 < S[i\u2032 + 1]. Since [loi\u2032 ,u o i\u2032 ] = \u03b3(Top(\u3008l,u, (S)\u3009))i by our definition of Top, op(\u03b3(\u3008l,u, (S)\u3009)) \u2286 \u03b3(Top(\u3008l,u, (S)\u3009)) follows.\nD. Implementation\nWe have implemented a tool for RANUM in roughly 10k lines of Python code based on PyTorch. Our tool leverages existing modules (tf2onnx for PyTorch and torch.onnx.export for Tensorflow and Keras) to extract ONNX-format DNN architectures from DL programs to take as the input, and automatically generates detection results, system tests, and defect fixes (given imposing locations)."
        },
        {
            "heading": "E. Hyperparameters",
            "text": "In this section, we listed hyperparameters in our two-step system test generation technique: For the unit test generation, we use the Adam optimizer where the learning rate is 1 and the maximum iteration number is 100. For the training example generation, we target for training example under learning rate \u03b3 = 1 and the approach has similar performance under other learning rates. We follow the convention in the DLG attack, where we use the L-BFGS method as the optimizer for gradient-based minimization. We terminate the method and return \u201cfailed\u201d if either the running time exceeds 1800 s (universal execution time limit for all experimental evaluations), or a failure-exhibiting example training is not found after 300 iterations of L-BFGS optimization."
        },
        {
            "heading": "F. Ablation Study of System Test Generation",
            "text": "RANUM uses the two-step test generation technique to produce failure-exhibiting system tests: it first generates failureexhibiting unit tests with gradient back-propagation, then generates failure-exhibiting training examples via the extended DLG attack. To isolate the impact of RANUM at each step, we replace either step with random sampling: \u201cRandom + RANUM\u201d, which first generates failure-exhibiting unit tests via random sampling, then generates training example via RANUM; \u201cRANUM + Random\u201d, which first generates failureexhibiting unit tests via RANUM, then generates training example via random sampling. We follow the same evaluation protocol as in Section IV-B in this ablation study. We find that \u201cRANUM + Random\u201d takes 9.38X running time than RANUM and fails for 68 runs (RANUM only fails for 57 runs); and \u201cRandom + RANUM\u201d fails for 113 runs (roughly 2X failed runs compared to RANUM). This study implies that RANUM\u2019s technique helps to improve effectiveness and efficiency at both steps of failure-exhibiting system test generation compared to the pure random baseline. The improvement for the first step is mainly from the effectiveness perspective, and the improvement for the second step is mainly from the efficiency perspective."
        },
        {
            "heading": "G. Threats to Validity",
            "text": "An external threat to validity is the evaluation subjects, which are the GRIST benchmarks [54], used to evaluate RANUM and baseline approaches. Although the subjects contain 63 real-world DL programs and are the largest to our knowledge, they still may not be representative enough. Another external threat is the approach randomness. To mitigate this threat, we repeat all randomized approaches for 10 runs. To mitigate bias in the empirical study, two authors independently conduct the study and discuss all cases to reach\na consensus. A major internal threat to validity comes from the approach implementation. We reduce this threat by conducting code review and log checking extensively. An author independently checks the correctness of code implementation. We also verify the soundness of our static analysis framework with over 50 carefully designed unit tests."
        }
    ],
    "title": "Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects",
    "year": 2023
}