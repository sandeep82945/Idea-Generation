{
    "abstractText": "Online toxic attacks, such as harassment, trolling, and hate speech have been linked to an increase in offline violence and negative psychological effects on victims. In this paper, we studied the impact of toxicity on users\u2019 online behavior. We collected a sample of 79.8k Twitter conversations. Then, through a longitudinal study, for nine weeks, we tracked and compared the behavioral reactions of authors, who were toxicity victims, with those who were not. We found that toxicity victims show a combination of the following behavioral reactions: avoidance, revenge, countermeasures, and negotiation. We performed statistical tests to understand the significance of the contribution of toxic replies toward user behaviors while considering confounding factors, such as the structure of conversations and the user accounts\u2019 visibility, identifiability, and activity level. Interestingly, we found that compared to other random authors, victims are more likely to engage in conversations, reply in a toxic way, and unfollow toxicity instigators. Even if the toxicity is directed at other participants, the root authors are more likely to engage in the conversations and reply in a toxic way. However, victims who have verified accounts are less likely to participate in conversations or respond by posting toxic comments. In addition, replies are more likely to be removed in conversations with a larger percentage of toxic nested replies and toxic replies directed at other users. Our results can assist further studies in developing more effective detection and intervention methods for reducing the negative consequences of toxicity on social media.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ana Aleksandric"
        },
        {
            "affiliations": [],
            "name": "Sayak Saha Roy"
        },
        {
            "affiliations": [],
            "name": "Shirin Nilizadeh"
        }
    ],
    "id": "SP:102646a287dd96132b4b3a4c358b6971ea507050",
    "references": [
        {
            "authors": [
                "Shazia Akhtar",
                "Catriona M. Morrison"
            ],
            "title": "The prevalence and impact of online trolling of UK members of parliament",
            "venue": "Computers in Human Behavior",
            "year": 2019
        },
        {
            "authors": [
                "Mohammed Alhajji",
                "Sarah Bass",
                "Ting Dai"
            ],
            "title": "Cyberbullying, Mental Health, and Violence in Adolescents and Associations With Sex and Race: Data From the 2015 Youth Risk Behavior Survey",
            "venue": "Global Pediatric Health",
            "year": 2019
        },
        {
            "authors": [
                "Ahmed Arafa",
                "Shaimaa Senosy"
            ],
            "title": "Pattern and correlates of cyberbullying victimization among Egyptian university students in Beni-Suef, Egypt",
            "venue": "Journal of Egyptian Public Health Association 92,",
            "year": 2017
        },
        {
            "authors": [
                "Nicole A Beres",
                "Julian Frommel",
                "Elizabeth Reid",
                "Regan L Mandryk",
                "Madison Klarkowski"
            ],
            "title": "Don\u2019t you know that you\u2019re toxic: Normalization of toxicity in online gaming",
            "venue": "In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Kerryn Brack",
                "Nerina Caltabiano"
            ],
            "title": "Cyberbullying and self-esteem in Australian adults",
            "venue": "Cyberpsychology: Journal of Psychosocial Research on Cyberspace",
            "year": 2014
        },
        {
            "authors": [
                "Caitlin Ring Carlson",
                "Hayley Rousselle"
            ],
            "title": "Report and repeat: Investigating Facebook\u2019s hate speech removal process",
            "venue": "First Monday",
            "year": 2020
        },
        {
            "authors": [
                "Despoina Chatzakou",
                "Nicolas Kourtellis",
                "Jeremy Blackburn",
                "Emiliano De Cristofaro",
                "Gianluca Stringhini",
                "Athena Vakali"
            ],
            "title": "Mean birds: Detecting aggression and bullying on twitter",
            "venue": "In ACM Web Science Conference",
            "year": 2017
        },
        {
            "authors": [
                "Despoina Chatzakou",
                "Ilias Leontiadis",
                "Jeremy Blackburn",
                "Emiliano De Cristofaro",
                "Gianluca Stringhini",
                "Athena Vakali",
                "Nicolas Kourtellis"
            ],
            "title": "Detecting cyberbullying and cyberaggression in social media",
            "venue": "ACM Transactions on the Web (TWEB) 13,",
            "year": 2019
        },
        {
            "authors": [
                "Mudit Chaudhary",
                "Chandni Saxena",
                "Helen Meng"
            ],
            "title": "Countering online hate speech: An nlp perspective",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Miroslav Chr\u00e1ska",
                "Kamil Kopeck\u1ef3",
                "Veronika Krejc\u00ed",
                "Ren\u00e9 Szotkowski"
            ],
            "title": "Is a Victim also an Attacker? Research of Cyberbullying at Czech Pupils and Students in the Whole Czech Republic I",
            "venue": "Journal of Technology and Information Education",
            "year": 2012
        },
        {
            "authors": [
                "Denzil Correa",
                "Leandro Ara\u00fajo Silva",
                "Mainack Mondal",
                "Fabr\u00edcio Benevenuto",
                "Krishna P Gummadi"
            ],
            "title": "The many shades of anonymity: Characterizing anonymous social media content",
            "venue": "In Ninth International AAAI Conference on Web and Social Media",
            "year": 2015
        },
        {
            "authors": [
                "Periwinkle Doerfler",
                "Andrea Forte",
                "Emiliano De Cristofaro",
                "Gianluca Stringhini",
                "Jeremy Blackburn",
                "DamonMcCoy"
            ],
            "title": " I\u2019m a Professor, which isn\u2019t usually a dangerous job\": Internet-facilitated Harassment and Its Impact on Researchers",
            "venue": "Proceedings of the ACM on Human-Computer Interaction",
            "year": 2021
        },
        {
            "authors": [
                "Brooke Erin Duffy",
                "Emily Hund"
            ],
            "title": "Gendered Visibility on Social Media: Navigating Instagram\u2019s",
            "venue": "Authenticity Bind. International Journal of Communication (19328036)",
            "year": 2019
        },
        {
            "authors": [
                "Mai ElSherief",
                "Vivek Kulkarni",
                "Dana Nguyen",
                "William Yang Wang",
                "Elizabeth Belding"
            ],
            "title": "Hate lingo: A target-based linguistic analysis of hate speech in social media",
            "venue": "In Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2018
        },
        {
            "authors": [
                "Mai ElSherief",
                "Shirin Nilizadeh",
                "Dana Nguyen",
                "Giovanni Vigna",
                "Elizabeth Belding"
            ],
            "title": "Peer to peer hate: Hate speech instigators and their targets",
            "venue": "In Proceedings of the International AAAI Conference onWeb and Social Media,",
            "year": 2018
        },
        {
            "authors": [
                "Bahad\u0131r Eri\u015fti",
                "Yavuz Akbulut"
            ],
            "title": "Reactions to cyberbullying among high school and university students",
            "venue": "The Social Science Journal 56,",
            "year": 2019
        },
        {
            "authors": [
                "Karmen Erjavec",
                "Melita Poler Kova\u010di\u010d"
            ],
            "title": "You Don\u2019t Understand, This is a New War!",
            "venue": "Analysis of Hate Speech in News Web Sites\u2019 Comments. Mass Communication and Society 15,",
            "year": 2012
        },
        {
            "authors": [
                "Murat Eyuboglu",
                "Damla Eyuboglu",
                "Seval Caliskan Pala",
                "Didem Oktar",
                "Zeynep Demirtas",
                "Didem Arslantas",
                "Alaettin Unsal"
            ],
            "title": "Traditional school bullying and cyberbullying: Prevalence, the effect on mental health problems and selfharm",
            "venue": "behavior. Psychiatry research",
            "year": 2021
        },
        {
            "authors": [
                "Bronwyn Fredericks",
                "Abraham Bradfield"
            ],
            "title": "Waiting with Bated Breath\u2019: Navigating the Monstrous World of Online Racism",
            "venue": "M/C Journal 24,",
            "year": 2021
        },
        {
            "authors": [
                "R Stuart Geiger"
            ],
            "title": "Bot-based collective blocklists in Twitter: the counterpublic moderation of harassment in a networked public space",
            "venue": "Information, Communication & Society 19,",
            "year": 2016
        },
        {
            "authors": [
                "Stefan Gei\u00df",
                "Svenja Sch\u00e4fer"
            ],
            "title": "Any publicity or good publicity? A competitive test of visibility-and tonality-based media effects on voting behavior",
            "venue": "Political Communication 34,",
            "year": 2017
        },
        {
            "authors": [
                "Ysabel Gerrard"
            ],
            "title": "Beyond the hashtag: Circumventing content moderation on social media",
            "venue": "New Media & Society 20,",
            "year": 2018
        },
        {
            "authors": [
                "Gary W Giumetti",
                "Andrea L Hatfield",
                "Jenna L Scisco",
                "Amber N Schroeder",
                "Eric R Muth",
                "Robin M Kowalski"
            ],
            "title": "What a rude e-mail! Examining the differential effects of incivility versus support on mood, energy, engagement, and performance in an online context",
            "venue": "Journal of Occupational Health Psychology",
            "year": 2013
        },
        {
            "authors": [
                "Jason Hannan"
            ],
            "title": "Trolling ourselves to death? Social media and post-truth politics",
            "venue": "European Journal of Communication 33,",
            "year": 2018
        },
        {
            "authors": [
                "Summer Harlow"
            ],
            "title": "Story-chatterers stirring up hate: Racist discourse in reader comments on US newspaper websites",
            "venue": "Howard Journal of Communications 26,",
            "year": 2015
        },
        {
            "authors": [
                "Karin Hellfeldt",
                "Laura L\u00f3pez-Romero",
                "Henrik Andershed"
            ],
            "title": "Cyberbullying and Psychological Well-being in Young Adolescence: The Potential Protective Mediation Effects of Social Support from Family, Friends, and Teachers",
            "venue": "International Journal of Environmental Research and Public Health 17,",
            "year": 2020
        },
        {
            "authors": [
                "Sameer Hinduja",
                "Justin W Patchin"
            ],
            "title": "Offline consequences of online victimization: School violence and delinquency",
            "venue": "Journal of school violence 6,",
            "year": 2007
        },
        {
            "authors": [
                "Gabriel Emile Hine",
                "Jeremiah Onaolapo",
                "Emiliano De Cristofaro",
                "Nicolas Kourtellis",
                "Ilias Leontiadis",
                "Riginos Samaras",
                "Gianluca Stringhini",
                "Jeremy Blackburn"
            ],
            "title": "Kek, Cucks, and God Emperor Trump: A Measurement Study of 4chan\u2019s Politically Incorrect Forum and its Effects on the Web. ICWSM",
            "year": 2017
        },
        {
            "authors": [
                "Gabriel Emile Hine",
                "Jeremiah Onaolapo",
                "Emiliano De Cristofaro",
                "Nicolas Kourtellis",
                "Ilias Leontiadis",
                "Riginos Samaras",
                "Gianluca Stringhini",
                "Jeremy Blackburn"
            ],
            "title": "Kek, cucks, and god emperor trump: A measurement study of 4chan\u2019s politically incorrect forum and its effects on the web",
            "venue": "In Eleventh International AAAI Conference on Web and Social Media",
            "year": 2017
        },
        {
            "authors": [
                "Jane Im",
                "Eshwar Chandrasekharan",
                "Jackson Sargent",
                "Paige Lighthammer",
                "Taylor Denby",
                "Ankit Bhargava",
                "Libby Hemphill",
                "David Jurgens",
                "Eric Gilbert"
            ],
            "title": "Still out there: Modeling and identifying russian troll accounts on twitter",
            "venue": "In 12th ACM Conference on Web Science",
            "year": 2020
        },
        {
            "authors": [
                "Cristina Jenaro",
                "Noelia Flores",
                "Cinthia Patricia Fr\u00edas"
            ],
            "title": "Systematic review of empirical studies on cyberbullying in adults: What we know and what we should investigate",
            "venue": "Aggression and Violent Behavior",
            "year": 2018
        },
        {
            "authors": [
                "Shagun Jhaver",
                "Sucheta Ghoshal",
                "Amy Bruckman",
                "Eric Gilbert"
            ],
            "title": "Online harassment and content moderation: The case of blocklists",
            "venue": "ACM Transactions on Computer-Human Interaction (TOCHI) 25,",
            "year": 2018
        },
        {
            "authors": [
                "Robin M Kowalski",
                "Gary W Giumetti",
                "Amber N Schroeder",
                "Micah R Lattanner"
            ],
            "title": "Bullying in the digital age: A critical review and meta-analysis of cyberbullying research among youth",
            "venue": "Psychological bulletin 140,",
            "year": 2014
        },
        {
            "authors": [
                "Amanda Lenhart",
                "Michele Ybarra",
                "Kathryn"
            ],
            "title": "Zickuhr, and Myeshia Price-Feeney",
            "venue": "Data & Society Research Institute. Center for Innovative Public Health Research. Retrieved from: https://datasociety. net/pubs/oh/Online_Harassment_2016",
            "year": 2016
        },
        {
            "authors": [
                "Sharon Levy",
                "Robert E. Kraut",
                "Jane A. Yu",
                "Kristen M. Altenburger",
                "Yi-Chia Wang"
            ],
            "title": "Understanding Conflicts in Online Conversations",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Karla Mantilla"
            ],
            "title": "Gendertrolling: Misogyny adapts to new media",
            "venue": "Feminist studies 39,",
            "year": 2013
        },
        {
            "authors": [
                "Mar\u00eda Carmen Mart\u00ednez-Monteagudo",
                "Beatriz Delgado",
                "\u00c1ngela D\u00edaz-Herrero",
                "Jos\u00e9 Manuel Garc\u00eda-Fern\u00e1ndez"
            ],
            "title": "Relationship between suicidal thinking, anxiety, depression and stress in university students who are victims of cyberbullying",
            "venue": "Psychiatry Research",
            "year": 2020
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Georgi Georgiev",
                "Preslav Nakov"
            ],
            "title": "Finding opinion manipulation trolls in news community forums",
            "venue": "In Proceedings of the nineteenth conference on computational natural language learning",
            "year": 2015
        },
        {
            "authors": [
                "Mainack Mondal",
                "Johnnatan Messias",
                "Saptarshi Ghosh",
                "Krishna P Gummadi",
                "Aniket Kate"
            ],
            "title": "Forgetting in social media: Understanding and controlling longitudinal exposure of socially shared data",
            "venue": "In Twelfth Symposium on Usable Privacy and Security (SOUPS",
            "year": 2016
        },
        {
            "authors": [
                "Timo T Ojanen",
                "Pimpawun Boonmongkon",
                "Ronnapoom Samakkeekarom",
                "Nattharat Samoh",
                "Mudjalin Cholratana",
                "Anusorn Payakkakom",
                "Thomas E Guadamuz"
            ],
            "title": "Investigating online harassment and offline violence among young people in Thailand: methodological approaches, lessons learned",
            "venue": "Culture, health & sexuality 16,",
            "year": 2014
        },
        {
            "authors": [
                "Alexandra Olteanu",
                "Carlos Castillo",
                "Jeremy Boy",
                "Kush Varshney"
            ],
            "title": "The effect of extremist violence on hateful speech online",
            "venue": "In Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2018
        },
        {
            "authors": [
                "Marco A Palomino",
                "Aditya Padmanabhan Varma"
            ],
            "title": "Any Publicity is Good Publicity: Positive, Negative and Neutral Tweets Can All Become Trends",
            "venue": "In 2020 39th International Conference of the Chilean Computer Science Society (SCCC). IEEE,",
            "year": 2020
        },
        {
            "authors": [
                "Mike C Parent",
                "Teresa D Gobble",
                "Aaron Rochlen"
            ],
            "title": "Social media behavior, toxic masculinity, and depression",
            "venue": "Psychology of Men & Masculinities 20,",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Ratkiewicz",
                "Michael Conover",
                "Mark Meiss",
                "Bruno Gon\u00e7alves",
                "Snehal Patil",
                "Alessandro Flammini",
                "Filippo Menczer"
            ],
            "title": "Truthy: mapping the spread of astroturf in microblog streams",
            "venue": "In Proceedings of the 20th international conference companion on World wide web",
            "year": 2011
        },
        {
            "authors": [
                "Manoel Ribeiro",
                "Pedro Calais",
                "Yuri Santos",
                "Virg\u00edlio Almeida",
                "Wagner Meira Jr."
            ],
            "title": "Characterizing and detecting hateful users on twitter",
            "venue": "In Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2018
        },
        {
            "authors": [
                "Ari Schlesinger",
                "Eshwar Chandrasekharan",
                "Christina A Masden",
                "Amy S Bruckman",
                "WKeith Edwards",
                "Rebecca E Grinter"
            ],
            "title": "Situated anonymity: Impacts of anonymity, ephemerality, and hyper-locality on social media",
            "venue": "In Proceedings of the 2017 CHI conference on human factors in computing systems",
            "year": 2017
        },
        {
            "authors": [
                "Leandro Silva",
                "Mainack Mondal",
                "Denzil Correa",
                "Fabr\u00edcio Benevenuto",
                "Ingmar Weber"
            ],
            "title": "Analyzing the targets of hate in online social media",
            "venue": "In Tenth International AAAI Conference on Web and Social Media",
            "year": 2016
        },
        {
            "authors": [
                "Rosalynd Southern",
                "Emily Harmer"
            ],
            "title": "Othering political women: Online misogyny, racism and ableism towards women in public life",
            "venue": "In Online Othering",
            "year": 2019
        },
        {
            "authors": [
                "Frithjof Staude-M\u00fcller",
                "Britta Hansen",
                "Melanie Voss"
            ],
            "title": "How stressful is online victimization? Effects of victim\u2019s personality and properties of the incident",
            "venue": "European Journal of Developmental Psychology 9,",
            "year": 2012
        },
        {
            "authors": [
                "Qiusi Sun",
                "Cuihua Shen"
            ],
            "title": "Who would respond to A troll? A social network analysis of reactions to trolls in online communities",
            "venue": "Computers in Human Behavior",
            "year": 2021
        },
        {
            "authors": [
                "Fatemeh Tahmasbi",
                "Leonard Schild",
                "Chen Ling",
                "Jeremy Blackburn",
                "Gianluca Stringhini",
                "Yang Zhang",
                "Savvas Zannettou"
            ],
            "title": "Go eat a bat, Chang!\u201d: On the Emergence of Sinophobic Behavior on Web Communities in the Face of COVID-19",
            "venue": "In Proceedings of the web conference",
            "year": 2021
        },
        {
            "authors": [
                "Marcia Layton Turner"
            ],
            "title": "Like, Love, Delete: Social Media\u2019s Influence on College Choice",
            "venue": "Journal of College Admission",
            "year": 2017
        },
        {
            "authors": [
                "Kris Varjas",
                "Jasmaine Talley",
                "Joel Meyers",
                "Leandra Parris",
                "Hayley Cutts"
            ],
            "title": "High school students\u2019 perceptions of motivations for cyberbullying: An exploratory study",
            "venue": "Western Journal of Emergency Medicine 11,",
            "year": 2010
        },
        {
            "authors": [
                "George Veletsianos",
                "Shandell Houlden",
                "Jaigris Hodson",
                "Chandell Gosse"
            ],
            "title": "Women scholars\u2019 experiences with online harassment and abuse: Self-protection, resistance, acceptance, and self-blame",
            "venue": "New Media & Society 20,",
            "year": 2018
        },
        {
            "authors": [
                "Meng-Jie Wang",
                "Kumar Yogeeswaran",
                "Nadia P Andrews",
                "Diala R Hawi",
                "Chris G Sibley"
            ],
            "title": "How common is cyberbullying among adults? Exploring gender, ethnic, and age differences in the prevalence of cyberbullying",
            "venue": "Cyberpsychology, Behavior, and Social Networking 22,",
            "year": 2019
        },
        {
            "authors": [
                "Matthew L Williams",
                "Pete Burnap"
            ],
            "title": "Cyberhate on social media in the aftermath of Woolwich: A case study in computational criminology and big data",
            "venue": "British Journal of Criminology 56,",
            "year": 2015
        },
        {
            "authors": [
                "Aimei Yang",
                "Michael Kent"
            ],
            "title": "Social media and organizational visibility: A sample of Fortune 500 corporations",
            "venue": "Public relations review 40,",
            "year": 2014
        },
        {
            "authors": [
                "Xueping Yang",
                "Hua Jonathan Ye",
                "Xinwei Wang"
            ],
            "title": "Social media use and work efficiency: Insights from the theory of communication visibility",
            "venue": "Information & Management 58,",
            "year": 2021
        },
        {
            "authors": [
                "Savvas Zannettou",
                "Barry Bradlyn",
                "Emiliano De Cristofaro",
                "Haewoon Kwak",
                "Michael Sirivianos",
                "Gianluca Stringini",
                "Jeremy Blackburn"
            ],
            "title": "What is Gab: A Bastion of Free Speech or an Alt-Right Echo Chamber",
            "venue": "In WWW Companion",
            "year": 2018
        },
        {
            "authors": [
                "Savvas Zannettou",
                "Tristan Caulfield",
                "Jeremy Blackburn",
                "Emiliano De Cristofaro",
                "Michael Sirivianos",
                "Gianluca Stringhini",
                "Guillermo Suarez-Tangil"
            ],
            "title": "On the Origins of Memes by Means of Fringe Web Communities",
            "venue": "In IMC",
            "year": 2018
        },
        {
            "authors": [
                "Savvas Zannettou",
                "Tristan Caulfield",
                "Emiliano De Cristofaro",
                "Michael Sirivianos",
                "Gianluca Stringhini",
                "Jeremy Blackburn"
            ],
            "title": "Disinformation warfare: Understanding state-sponsored trolls on Twitter and their influence on the web",
            "venue": "In Companion proceedings of the 2019 world wide web conference",
            "year": 2019
        },
        {
            "authors": [
                "Savvas Zannettou",
                "Mai ElSherief",
                "Elizabeth Belding",
                "Shirin Nilizadeh",
                "Gianluca Stringhini"
            ],
            "title": "Measuring and characterizing hate speech on news Twitter Users\u2019 Behavioral Response to Toxic Replies The Web Conference (WWW) \u201923",
            "venue": "April 30\u2013May",
            "year": 2020
        },
        {
            "authors": [
                "Savvas Zannettou",
                "Joel Finkelstein",
                "Barry Bradlyn",
                "Jeremy Blackburn"
            ],
            "title": "A quantitative approach to understanding online antisemitism",
            "venue": "In Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Social and professional topics\u2192 User characteristics.\nKEYWORDS social media, toxicity attacks, user online behavior, longitudinal study\nACM Reference Format: Ana Aleksandric, Sayak Saha Roy, and Shirin Nilizadeh. 2023. Twitter Users\u2019 Behavioral Response to Toxic Replies. In Proceedings of The Web Conference(WWW) \u201923 (The Web Conference (WWW) \u201923). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. The Web Conference (WWW) \u201923, April 30\u2013May 4, 2023, Austin, Texas \u00a9 2023 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "These days, social media is rampant with toxic attacks such as offensive language, trolling, and using hate speech. The primary goal of these attacks is to silence, insult or demoralize people, especially those belonging to alreadymarginalized groups [1, 21, 40, 56]. These attacks are usually targeted, e.g., as part of a smear campaign to damage or call into question someone\u2019s reputation [26, 32, 41, 48, 69]. They can also be coordinated using other communication mediums and implemented by many users [8, 12, 56, 70].\nPsychological research has studied the negative effects of online harassment, cyberbullying, and trolling on individuals\u2019 psychological states and well-being [25, 29, 35], showing that they often cause overwhelming and stressful situations for the victims [1, 3, 36, 43, 54]. These studies found that victims are more prone to show self-harming behaviors as well as suffer from depression and anxiety [2, 18, 28]. To counter online toxic attacks, many social media platforms have started implementing content moderation mechanisms [19, 60] that block accounts [6, 22] and remove content [9]. However, it is debatable whether they provide sufficient mitigation to the potential psychological damage caused to the victim as a result of the online toxicity [24, 34].\nTo the best of our knowledge, no work has conducted a longitudinal data-driven study to examine the impact of toxic content on victims\u2019 online behavior. In this paper, we study how the victims respond to toxic content in terms of their behavioral actions on Twitter. Our goal is to identify key factors which accurately represent the scale at which toxic replies impact victims. A recent study on online cyberbullying [16] targeting undergraduate and high school students found that victims primarily engaged in four types of behavioral reactions towards such attacks: avoidance, revenge, employ countermeasures, and negotiation. We use this as a framework for creating meaningful groupings of behavioral responses to toxic content. For example, we examine if the victims try to avoid further encounters with toxic content, e.g., by removing their posts or even deleting their accounts (which are also signs of being silenced), or if they tend to negotiate by posting comments in conversations, and even take revenge by responding in a toxic way, or employ countermeasures by ignoring such content but unfollowing the toxicity instigators. In our analysis, we consider factors that might have an impact on users\u2019 social media behavior, including the number of toxic replies in conversations, the structure of conversations, the location of the toxic content in the conversations tree, and the social relationship between conversations\u2019 participants (i.e., if they follow each other). We also explore the effects of account-specific attributes on users\u2019 decisions, including their online visibility, identifiability, and activity level.\nFor our analysis, we collected a random sample of 79.8k Twitter conversations from August 14th to September 28th, 2021. We used this data to identify users involved in these conversations, identify conversations with toxic replies, and finally collect longitudinal\nar X\niv :2\n21 0.\n13 42\n0v 1\n[ cs\n.S I]\n2 4\nO ct\n2 02\n2\ndata on users\u2019 online behavior. We represented the structure of a conversation using a reply tree, which encodes the relationships between tweets, where two tweets are connected if one is a reply to the other. We used Google Perspective [47], a natural language-based AI tool used to identify toxicity in text, to detect conversations that received toxic replies. Finally, we analyzed and characterized the behavioral reactions of root authors receiving toxic replies, who we call toxicity victims, and compared their behavior with those of root authors receiving no toxic replies, who we call random authors and are our control group. We formulated the following hypotheses and performed appropriate statistical tests to understand the significance of the contribution of toxic replies toward user behaviors: H1: Toxicity victims are more likely to deactivate their accounts compared to random authors. H2: Toxicity victims are more likely to switch their accounts to private mode. H3: Toxicity victims are more likely to engage in conversations. H4: Toxicity victims are more likely to engage in conversations if receiving toxic replies from a larger number of toxicity instigators. H5: Toxicity victims are more likely to respond back in a toxic way.H6: Toxicity victims are more likely to respond back in a toxic way if receiving toxic replies from a larger number of toxicity instigators. H7: Toxicity victims are more likely to delete their original posts compared to random authors.H8: Replies in conversations with toxic replies are more likely to be deleted compared to conversations without toxic replies. H9: Toxicity victims are more likely to unfollow toxicity instigators compared to random authors.\nOur study yields multiple important findings. We observed that different users respond differently to toxic content and identified groups of users who show similar reaction patterns. For example, in terms of disregard and avoidance, we demonstrate that 30.96% of victims ignored toxic content and did not show any reactions. In terms of negotiation, we found toxicity victims compared to others are more likely to engage in the conversations (60.3% vs. 47.4%), and 12.4% of victims employed countermeasures by unfollowing toxicity instigators. Analyzing the contributing factors to users\u2019 behavior, our results suggest that users who receive toxic direct replies are less likely to engage in the conversation, while users who receive nested toxic replies engage more in conversations with toxic replies and tend to have toxic responses. These show that the location of toxic content in the conversation can affect victim\u2019s reactions. Interestingly, verified accounts are less likely to engage in conversations with toxic replies, indicating that the social status of users can have an impact on how they perceive toxic content and react to them. Finally, identifiable accounts are more likely to engage in conversations by responding in a toxic way."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Generalized online abuse encompasses several unhealthy online behaviors, including attacks of racism against minority communities [21, 40, 56], misogynistic hatred [38, 46] and toxic masculinity [53], which are aimed at groups of vulnerable individuals. In recent years, online abuse have received a lot of attention. Here, we focus on studies that characterize online abuse and investigate its psychological impact of them on humans. To the best of our knowledge, there is no work that has studied the consequences of receiving toxic replies on users\u2019 online behavior.\nPrevalence of Social Media Victimization. Since abuse and toxicity are often carried out with the intention to humiliate or manipulate targeted individuals [10, 17, 46, 61], social media is often considered to be the chief outlet for housing such attacks due to high visibility [13, 65, 66] and more opportunities to remain anonymous [11, 51, 72]. Online abuse and toxicity can be influenced by several socioeconomic factors. Prior work has examined the association between abuse and on-the-ground \u201ctrigger\u201d events, e.g., terrorist attacks, and political events [27, 31, 44, 64]. The prevalence and characteristics of hate speech have also been studied on specific web communities, such as r/Gab [67, 68], 4chan\u2019s Politically Incorrect board (/pol/) [30], Twitter [7, 14, 71], and Whisper [52]. Some works have shown that online abuse and toxic comments are normalized in several communities [4, 38, 46]. There have also been a few efforts to understand the characteristic differences between hate targets and hate instigators [14, 15, 37, 50].\nThe Psychological Impacts of Online Abuse. Established literature on the psychological impacts of online abuse is mostly focused on cyberbullying. A study determined that middle and high school students who had been cyber-bullied were more prone to exhibit self-harming behavior as well as suffer from depression, anxiety, and lower self esteem [18]. Similar studies [2, 5, 33, 39] found the same patterns of psychological distress in young adolescents [28] and adults [63], due to cyberbullying attacks. Victims of online harassment might also respond by acceptance and selfblame [62], with those having lower psychological endurance being more vulnerable to emotional outbursts. Some studies explored the effects of trolling on victims [1], and identified the circumstances when victims are more likely to respond to trolls [55]."
        },
        {
            "heading": "3 DATA COLLECTION",
            "text": "Figure 1 shows the pipeline used for collecting and processing our datasets. Broadly, we aimed to (a) obtain a random set of conversations, and (b) track the twitter activities of root authors.\nDaily Collection of a Random Sample of Twitter Conversations: We used the Twitter API [59] to collect a 1% random sample of tweets for 46 consecutive days, starting from August 14th till September 28th, 2021, and extracted English tweets which are not retweets or replies belonging to other conversations. For each initial tweet, we waited at least two days before collecting the replies of the conversation. For example, if we collected a random sample of tweets on September 1st, we would start collecting replies for each of these tweets on September 3rd. This is to give enough time so that the initial tweet can turn into a conversation. However, we discarded many tweets that did not receive any comment or had been deleted by the time we attempted to collect their replies. We also removed the conversations where the replies for the tweets were all posted by the author of the initial tweet.\nConversations, ReplyTrees and SomeDefinitions:Weused Twarc [58], a Python wrapper for the official Twitter API to obtain the entire conversation for each initial tweet, including its direct replies and nested replies (replies to replies). As it is shown in Figure 2, we represent each conversation as a reply tree, where one tweet is child of another tweet when one is a reply to the other. The initial tweets represent the roots of reply trees. We call the author of the root tweet as root author. We also define direct replies as the\nfirst level replies in a conversation, which is the set of replies to the root tweet. Nested replies refer to other levels of replies in a conversation other than the first layer, i.e., replies to replies.\nAs it is shown in Figure 2, each conversation as a reply tree has the following properties: Size, which indicates the number of tweets in the conversation; Depth, which is the depth of the conversation\u2019s deepest node;Width, which is the maximum number of nodes at any depth in the tree.\nPre-processing and Filtering: Getting the replies for tweets of each day can take a long time, up to couple of hours. Therefore, the first tweets collected for the certain day would have much less time to receive any comments than last tweets obtained from that day. In order to solve this problem, we only kept replies sent in the first 48 hours after the root tweet was posted. Also, some tweets contained only links, images, and videos instead of text. Since our approach of detecting toxic tweets is text-based, such conversations were removed from the dataset. Moreover, certain reply trees were missing some replies due to the errors received during their data collection. Since these errors can have multiple reasons including, replies being deleted or hidden by their authors, or root authors, we removed trees containing such errors from our dataset. Furthermore, we noticed that not all the root authors in our sample are unique, therefore to avoid duplication in our statistical analysis, we randomly selected a single conversation from each root author. Our Conversation Dataset: Finally, our dataset consists of 79,799 conversations with 528,041 tweets, posted by 328,390 unique users, out of which 79,799 are root authors.\nTracking activity of root authors: For each root author and root tweet, we kept track of the following activities: account deactivation, account privatization, and tweet deletion, as well as root authors\u2019 lists of followers and friends. In detail, as it is shown in Figure 1, from the second day of our data collection, we checked on a daily basis if the root tweets and authors still exist using Twitter API. Similarly, from the fourth day of our data collection, after collecting the replies to a root tweet, every day we were checking if the replies still exist. When a certain tweet or account is not accessible, API returns helpful error codes, e.g., Sorry, you are not authorized to see this status, User not found, and No status found with that ID [49]. These error codes indicate that the user account has become private, been deactivated and the tweet has been deleted, respectively. Note that we cannot certainly identify who deleted\na particular tweet. For instance, the root tweet can be deleted by the root author, or in case the deleted tweet is a comment, it can be deleted by the user who posted that specific comment or by the root author. Even though this process was repeated daily during the entire process of data collection, the data collection failed on some days. More precisely, in the analysis, we focus on the presence of the tweets and accounts three days after the corresponding root tweets were collected, but the data about their presence is missing in 7.6% of the conversations, with very similar distributions in conversations with and without toxic replies. Therefore, in the statistical models related to tweet deletion, account deactivation, and account privatization, we omit these conversations.\nAs illustrated in Figure 1, we collected the followers and friends lists of all root authors every day, once the daily random sample is obtained. Thus, the lists collected at this time represent a snapshot of the authors\u2019 friends/followers lists before their tweets turned into a conversation. Furthermore, around 49 days after the first day when root tweets were collected, we again collected the list of friends and followers for all root authors. This is to analyze the impact of toxicity on users\u2019 online relationships. Also, note that in the analysis we report the distribution of the percent of toxicity instigators that the victims unfollowed. As toxicity instigators do not exist in other conversations, comparing the unfollowing ratio between victims and other random users was not applicable. We could not collect the relationships, i.e., follower and friends\u2019 lists, right after obtaining the conversations, because of the number of API calls we could issue every day. This delay in collecting relationships imposes some limitations as users might end followership and friendships due to other events during this time. However, in our analysis, we compare these variables of toxicity victims and random authors and seeing a difference can be an indicator of the impact of toxicity on Twitter relationships. From our unfriend analysis, we had to discard 2,488 conversations, where 403 and 2,085 are conversations with and without toxic replies respectively because we were not able to obtain the friends\u2019 list for their root authors due to the authors either making their accounts private or deactivating them.\nIdentifyingConversationswithToxicReplies:WeusedGoogle\u2019s Perspective API [47] to detect toxic replies in our dataset. This AIbased tool investigates whether a provided text contains language indicating abusive or inappropriate attributes such as Severe toxicity, Profanity, Sexually expletives, Threats, Insults and assigns a\nscore between 0 to 1, with a higher score indicating more severity for a particular attribute. In this paper, we are only considering scores for the Severe Toxicity attribute since Google\u2019s Perspective API defines a text having this attribute to as rude, disrespectful, or unreasonable comment which meets the general standards for a comment which might be considered to be hateful or toxic. In our analysis, for finding the number of toxic comments, we needed to create a binary variable that labels a tweet as toxic or not toxic. We labeled a tweet as toxic, if its severe toxicity score is higher than 0.5. To determine this score threshold, two independent coders labelled 200 random tweets with an inter-rater agreement of 94% and a Cohen\u2019s kappa of 0.69. The inter-rater reliability showed substantial agreement for severe toxicity between the manual coders and the Perspective API for scores around 0.5.\nFrom 528,041 tweets in our dataset, 34,208 (6.5%) tweets were labelled as toxic, where about 52% of toxic tweets were directed to the root authors, 38.4% of toxic tweets were written by the root authors. Only 9.6% of toxic tweets were posted by other users and directed to other conversation participants. From all conversations, 10,953 conversations (13.7%) have received toxic replies posted by other users and directed to the root author. We call such conversations as Conversations of Toxicity Victims (CTV), and the rest of conversations is named Conversations of Other Authors (COA), which might include toxic replies that are written by the root author or are directed to others. However, our focus is to investigate the way root authors respond to toxic replies that are directed to them only."
        },
        {
            "heading": "4 VARIABLES AND STATISTICAL MODELS",
            "text": "We use multivariate regression to study the associations between toxicity and Twitter users\u2019 behaviors. To provide a larger context for interpretation of our analyses, we compare the users\u2019 behaviors of conversations with and without toxic replies. Below, we explain our dependent, independent, and control variables in detail."
        },
        {
            "heading": "4.1 Dependent Variables",
            "text": "These variables represent our understudy user behaviors, including: (1) deactivated: a binary variable indicating whether the account is deactivated. Account deactivation can be considered an extreme reaction to toxicity, when someone decides to leave the platform temporarily or permanently, and can be considered as an avoidance behavior. Although we collected the data about presence of the accounts for nine weeks, we decided to use the data obtained on\nthe third day after collection of the root tweets, because most of the root authors deactivated their accounts on that date (See Figure 5a in Appendix A). (2) private: a binary variable showing whether the account is gone to the private mode (i.e., only the root author\u2019s followers can observe and interact with their tweets). This can be considered an severe reaction to toxicity, when someone decides to make their account private and limit their visibility. Similarly, this can be considered as an avoidance behavior. We assigned values to this variable using the data from day three for the analysis, because most of the accounts went to the private mode on that date (See Figure 5b in Appendix A). (3) root_tweet_deleted: a binary variable indicating whether the root tweet is deleted. Some users might delete their initial posts once they receive toxic replies to avoid further toxicity. To assign values to this variable, we used the data collected three days after the collection of root tweets, because most of the root tweets were deleted on that date (See Figure 5c in Appendix A). (4) #deleted_replies: a numeric variable showing the number of deleted replies. Some users might delete the replies within the conversation to avoid the situation. The information about replies\u2019 presence used in the analysis was based on the data collected three days after the collection of initial tweets. (5) #root_author_replies: a numeric variable indicating the number of root authors\u2019 replies. In contrary to deleting tweets and replies, some users might decide to negotiate about their point of view and participate in the conversation. This variable can help us capture this behavior. (6) #root_author_toxic_replies: the number of root authors\u2019 toxic replies per conversation. This can capture the revenge behavior, when the victim responses to the toxicity in a toxic way. (7) unfollowing: a binary variable that shows if the user unfollows another user. This can be considered as either avoidance or revenge behavior, as the victim tries to avoid further communication with the toxicity instigator."
        },
        {
            "heading": "4.2 Independent Variables",
            "text": "Our independent variables are computed based on toxicity scores obtained for replies in the conversations. We could define the overall toxicity as the ratio of number of toxic replies to the size of the conversation. However, not all toxic replies are directed at the root author and sometimes they are directed at other users posting in the conversations. Also, the location of toxic replies in the tree structure can affect the victims\u2019 reactions. For example, as it is shown in Figure 2, direct replies might be more visible compared to nested replies. Therefore, we considered three independent variables considering such factors: (1) direct_toxicity, a numeric variable defined as the ratio of the toxic direct replies, targeting the root author, to the total number of direct replies. (2) nested_toxicity, a numeric variable defined as the ratio of the number of toxic nested replies, targeting the root author, to the total number of nested replies in the conversation. (3) toxicity_to_others, a numeric variable defined as the ratio of the toxic replies, directed at the other conversation participants, to the total number of toxic replies in a conversation."
        },
        {
            "heading": "4.3 Control Variables",
            "text": "When comparing the behavior of root authors in conversations with and without toxic replies, i.e., victims and other authors, we controlled for features that might have influenced their behavior\nin conversations (i.e., confounding factors). In particular, we controlled for features that capture the structure of conversations, the relationship between root author and toxicity instigator, and users\u2019 activity, visibility and identifiability on Twitter.\nOnline activity includes num_friends, num_tweets and account_age (in years) as numeric variables. If a user posts many tweets then they might also tend to engage in conversations more, even if they receive toxic replies, or if a user is more established, i.e., been using Twitter for more years, then they might perceive toxicity and react to it differently. Online visibility includes num_followers, and listed_counts as numeric variables and verified as a binary variable. While other works [15] have shown that online visibility and receiving hate are related, they might also influence users\u2019 reactions towards toxicity. For example, verified users or a user with more followers might get less negatively affected by receiving a few toxic replies, compared to a user who is not verified or has a few followers. Identifiability represent features from the user profiles which can help identify a user, including description_length (in chars) as a numeric variable, and has_URL and has_location as binary variables. We also captured has_image but noticed that all accounts are with profile images and therefore discarded this variable. We argue that it might be that more identifiable users are more likely to get negatively affected when they are under attack by others compare to anonymous users. Also, other works have shown that anonymous accounts tend to show abusive behavior more than others [11, 51, 72]. Conversation structure includes size, width, and depth as numeric variables. These features can play a role in how victims respond to toxicity, e.g., a toxic reply buried in a nested conversation might not have the same negativity level as having one in a short conversation. In our analysis, we also controlled for root_toxicity, which indicates if the conversations starts with a toxic tweet. Such authors probably could expect receiving toxic replies and they might react differently."
        },
        {
            "heading": "5 VICTIM\u2019S BEHAVIORAL RESPONSES",
            "text": "We provide the descriptive statistics about conversations, author accounts and victim\u2019s behavioral reactions.\nConversations\u2019 Characteristics: Table 1 compares the characteristics of CTV and COA. Since the distribution of features is not normal, we present min, mean, median, and max values. It shows that the prevalence of toxic direct replies to the root author (\ud835\udc40\ud835\udc52\ud835\udc4e\ud835\udc5b = 0.06) is higher compared to nested replies (\ud835\udc40\ud835\udc52\ud835\udc4e\ud835\udc5b = 0.008,). The Mean number of toxicity instigators is 0.21, while the Mean in CTV is 1.5, and the maximum number of instigators in our dataset is\n113. The min, mean, median, and max values for conversation size in our dataset are 2, 3, 6.6, and 1,842, respectively. These results show that most conversations are small, however, the size of 8,872 (11.1%), 291 (0.4%), and 25 (0.03%) of conversations is more than 10, 100, and 500. We also see that size, width and depth values are higher for CTV, e.g., the median and mean of size for CTV are about 6 and 16, while those for COA are about 3 and 5. The number of unique users who participated in the conversations is related to the conversation size because a conversation with more users has more posts. Similarly, we observe on average more users participate in CTV (Mean is about 11) compared to COA (Mean is about 3). We ran Mann-Whitney U tests for all the variables and found that there are significant differences between these characteristics in CTV and COA. For brevity, we do not present these results.\nVictim\u2019s reactions: Table 2 shows the statistics about the victim\u2019s reactions and identifies the more prevalent reactions. As illustrated in Figure 3b, percentage of victims who deactivated accounts or went private are 1% and 0.6%, respectively. Interestingly, Table 2 shows that the maximum #root_author_replies is higher in COA compared to CTV (504 vs. 313), but the mean of #root_author_replies in CTV is approximately twice times the mean of #root_author_replies in COA (1.009 vs. 2.13). As shown in Figure 3a, toxicity victims are engaged in a larger percentage of conversations compared to other users (60.3% vs. 47.4%). Moreover, Figure 3d reveals that the percent of CTV where the toxicity victims responded with at least one toxic reply is 14.5%, while in COA, that number is 4.4%. Furthermore, the percentage of all root authors\u2019 toxic replies that were immediate toxic replies to toxic comments is 3.1% (the toxic children of the toxic node in the conversation tree). Finally, the percent of all children nodes where the root author directly responded to a toxic tweet which is toxic in nature is 13.4%.\nThe percentage of victims that removed the root tweet is 1.8% while the percentage of other users who deleted a root post is 2.4% (Figure 3b). Also, the mean of #deleted_replies in CTV is 0.02, while in COA this number corresponds to 0.004, indicating that there might be a significant correlation between toxicity of the conversation and number of deleted comments. The percentage of CTV where there is at least one removed comment belonging\nto the root author is 0.07% while comment deletion occurred only in 0.05% of COA (presented in Figure 3c). We focused on deleted replies posted by the root author because authors can only delete their own tweets/replies. Interestingly, in 8 out of 137 CTV with at least one deleted reply, conversations contained at least one reply posted by the root author. In case of COA, in 35 out of 201 cases, conversations contained at least one removed reply belonging to the root author. This indicates that in most of the conversations, removed replies belong to other participants. Additionally, in our dataset, in 12.4% of CTV, root authors unfollowed at least one user who posted toxic comments on their posts. Finally, in 30.96% of CTV, the root author did not respond in any way.\nRootAuthors\u2019 AccountCharacteristics: Interestingly, Table 2 shows that our dataset contains a higher percentage of verified accounts among the toxicity victims (0.07) compared to other users (0.038), consistent with the prior studies [15]. In addition, the Mean values of followers, friends, tweets, and listed counts of toxicity victims are higher compared to these characteristics of other random root authors, again consistent with prior studies [15], indicating that toxicity victims might be more active and visible accounts compared to other users. Moreover, a higher percentage of victims have URL and location specified on their profiles, and the victims Mean account age might indicate that their accounts are younger compared to other users. We ran a Mann-Whitney test to compare the distributions of all account characteristics, and we found that there is a statistically significant difference between all account characteristics among victims and other users.\nClusteringVictims by their Reactions: Victimsmight show a combination of behavioral reactions, and such combinations might also be common among some groups of victims. Figure 4 shows the histogram of the number of behaviors a victim has shown in our dataset. The victim can have up to seven reactions, corresponding to the number of dependent variables described in Section 4.1. As you can see, a huge number of victims (30.96%) decided to ignore the toxic replies and did not respond, while 39.7% of victims showed only one of our studied behavioral reactions. It is interesting that\nabout 30% of the victims showed a combination of these behaviors. In particular, 18.6% and 2.9% of victims showed a combination of two and three different behaviors, respectively. Only ten victims showed more than three different reactions in our dataset, specifically 9, 0, 1, and 0 showed four, five, six, and seven behaviors, respectively.\nWe further employed k-means clustering to group victims using the seven behavioral reactions as features. While features such as deactivated, private, deleted are binary variables, the rest are numeric variables. For this analysis, we discarded 868 (7.9%) of CTVs, as for these conversations we failed to collect data for account and tweet existence three days after initial tweets were collected.\nSince we could assign any \ud835\udc58 for clustering, we examined clustering with various k values, starting from 4 clusters. We increased the number of clusters by one, as long as we would see that the results create a meaningful cluster that is based on certain reactions. However, testing \ud835\udc58 = 8 or more clusters produced very small clusters of victims but did not create a new grouping of reactions. Consequently, we set the number of clusters to seven. As shown in Table 3, each cluster is a combination of one or more users\u2019 behavioral reactions. The values in the table show the percentage of victims that have those reactions. Interestingly, 96% of victims in cluster one (\ud835\udc41 = 1, 042) unfollowed toxicity instigators and engaged in the conversation positively because 21% of victims in this cluster replied and only 0.3% of them replied in a toxic way. Also, victims in cluster four (\ud835\udc41 = 234) unfollowed toxicity instigators and engaged in the conversation negatively because 37% of victims in this cluster have replied and 48% of those replied in a toxic way. Interestingly, users who are grouped in cluster seven were the ones who ignored toxic comments on their tweets, and this represents the largest cluster in the dataset (\ud835\udc41 = 4, 337). The second largest\ncluster, cluster two (\ud835\udc41 = 3, 158), represents victims who only engaged in conversations positively. The smallest cluster, cluster three (\ud835\udc41 = 60), includes victims who all went private, about 14% of them also engaged in conversations, and 32% of them unfollowed toxicity instigators. Finally, all victims in cluster five have deleted the root tweet and 19% of them unfollowed toxicity instigators, suggesting that these two behaviors might coexist. Cluster six includes 1,072 root authors who responded in a toxic way.\nIn summary, this analysis confirms that victims can be grouped based on their behavioral reactions, and that some combinations of behaviors tend to appear together. For example, we observed that unfollowing is more likely to be used along with other reactions, as it was seen with user engagement (both positive and negative) in clusters 1 and 4, deleting in cluster 5, and going private in cluster 3."
        },
        {
            "heading": "6 HYPOTHESES TESTING",
            "text": "We used multivariate regression to examine the associations between toxicity and users\u2019 behaviors and to compare the behaviors of victims and other random users while considering the confounding factors. For numeric dependent variables we used Poisson multivariate regression models to test our hypotheses and for binary dependent variables, we employed logistic regressions. In all the models, we added all independent and control variables.\nH1: Toxicity victims are more likely to deactivate their accounts compared to random authors. Victims might be so negatively affected that they decide to deactivate their accounts. Even though one toxicity attack might not trigger such behavior, we examine whether users tend to deactivate their social media accounts after receiving toxic comments on their posts. We only found 51 toxicity victims had deactivated their accounts, only contributing to 1% of all CTV in our dataset. While this seems like a rare phenomenon, considering such drastic action is still alarming. We ran a logistic regression model having deactivated as the dependent variable. The results presented in Table 4 (column H1) suggested that there is no statistical difference in account deactivation among toxicity victims and other users, and there is no evidence to support H1.\nH2: Toxicity victims are more likely to switch their accounts to private mode. We examined switching to private mode as a countermeasure activity because in this mode, only the user\u2019s followers can reply under their tweets. Twitter recently added circles as a mechanism that allows limiting audience using more finegrained policies, however, this option was not available during our data collection. We found that switching accounts to the private mode occurred in only 0.6% of CTV, and the logistic regression analysis presented in Table 4 (column H2) revealed that there is no association between becoming private and any type of toxicity, rejecting H2. This can have various reasons, e.g., users might go private for other reasons, like when they post about something sensitive or private.\nH3: Toxicity victims are more likely to engage in conversations. Prior study [16] showed that victims might negotiate by communicating with the instigator to stop the aggressive behavior. H3 aims to test whether toxicity victims tend to negotiate their\npoints of view by engaging in conversations. We measured engagement using #root_author_replies as the dependent variable. The results of the Poisson regression model are shown in Table 4 (column H3). They indicate that there is a positive significant association between the nested_toxicity and the #root_author_replies (\ud835\udc5d < 0.001), as well as between the toxicity_to_others and the #root_author_replies (\ud835\udc5d < 0.001). However, the relationship between direct_toxicity and #root_author_replies is negatively significant (\ud835\udc5d < 0.001). In other words, toxicity victims are more likely to engage in the conversation with more toxic nested and toxic replies to others, compared to random authors. This might indicate that root authors engage more in cases when discussion develops within the conversation, while they do not tend to respond to direct toxicity.\nInterestingly, verified accounts tend to engage less in conversations compared to other users (\ud835\udc5d < 0.001), which can be due to the great number of replies they receive on their posts, or that they believe any attention is good attention [23, 45]. Moreover, young and more identifiable accounts who provide location and URL (\ud835\udc5d < 0.001), and have larger description length tend to engage more in toxic conversations compared to other users, which might indicate that more identifiable users care more about their account reputability. Finally, there is a significant negative correlation between the #root_author_replies in the conversation and root_toxicity (\ud835\udc5d < 0.001). This shows that victims who started the conversation with a toxic tweet engage less in conversations compared to victims whose conversations did not start with the toxic tweet. In conclusion, this analysis supports H3 as it shows that toxicity victims are more likely to engage in the conversation when receiving more toxicity.\nH4: Toxicity victims are more likely to engage in conversations if receiving toxic replies froma larger number of toxicity instigators. This hypothesis examines whether victims tend to engage more in CTV if toxicity they receive is coming from a larger percentage of users involved in the conversation. Users might not react the same way if receiving one or many toxic replies from a single user vs many users. To test this hypothesis, we defined our independent variable, toxicity_instigators, as the percentage of unique users who are posting toxic replies to the total number of unique users in the conversation. We avoided using this variable together with our independent variables in other models due to multicollinearity, as number of toxicity instigators is correlated with the number of toxic replies in the conversation. The results of employing the Poisson regression model suggest that users are more likely to engage in the conversation if toxicity is coming from a larger number of toxicity instigators compared to other users (\ud835\udc5d < 0.001), and therefore H4 is supported.\nH5: Toxicity victims are more likely to respond back in a toxic way. Some victims might try to get revenge by responding to toxic comments in a toxic way. The selected dependent variable of the Poisson regression model is #root_author_toxic_replies in the conversation, while predictors are the same as in the previous models. The results of regression analysis, illustrated in Table 4 (column H5), show that there is a positive significant correlation between the number of root authors\u2019 toxic replies and all three toxicity variables (\ud835\udc5d < 0.001). That is, the more toxic replies posted by other users are involved in the conversation, toxicity victims are more likely to respond back in a toxic way compared to random\nroot authors, and therefore H5 is supported. Furthermore, root authors are more likely to engage in a conversation in a toxic way if the conversation root is toxic compared to other users (\ud835\udc5d < 0.001). Similarly as in H3, we found that young (\ud835\udc5d < 0.001) and identifiable accounts which provide location (\ud835\udc5d < 0.001) and URL (\ud835\udc5d < 0.01) are more likely to respond in a toxic way. This can be explained as users whose identities are known to their social network might argue back about their point of view instead of letting others openly attack them and degrade their reputation.\nH6: Toxicity victims are more likely to respond back in a toxic way if receiving toxic replies from a larger number of toxicity instigators. Toxicity victims might react differently depending on the number of unique toxicity instigators. We used toxicity_instigators as the independent variable and ran the Poisson regression model. The results suggest that users are more likely to respond back in a toxic way if toxicity is coming from more toxicity instigators compared to other users (\ud835\udc5d < 0.001), supporting H6.\nH7: Toxicity victims are more likely to delete their original posts compared to random authors. Receiving toxic comments on a tweet, the users might regret sharing their thoughts and decide to delete their post (signs of getting silenced). We ran logistic regression model having root_tweet_deleted as a binary dependent variable. As it is shown in Table 4 (column H7), the test did not yield any significant evidence that root tweet is more likely to get deleted in CTV compared to COA, rejecting H7. This can have some explanation as users might decide to delete their tweets for other reasons than receiving toxic replies, e.g., when the tweet is not relevant anymore, or for privacy and cleaning the traces of activities from the social media [42, 57]. However, we found that root tweets that are toxic itself, showed by the control variable root_toxicity, are more likely to be deleted compared to non-toxic root tweets (\ud835\udc5d < 0.001). It could be that authors of toxic roots decide to remove their posts because they realize that the content was not appropriate, or they might not receive support from other users as they expected.\nH8: Replies in conversations with toxic replies are more likely to be deleted compared to conversationswithout toxic replies. This hypothesis examines whether replies that belonged to the root author are being removed more in CTV compared to COA. We ran a Poisson model with #deleted_replies by the root author as the dependent variable in the model. The results obtained from the regression analysis (presented in Table 4 in column H8) suggest that there is a statistically significant positive relationship between #deleted_replies that belonged to the root author and nested_toxicity (\ud835\udc5d < 0.001), supporting H8 partially. This indicates that the CTV with the higher percentage of toxic nested replies are more likely to experience a higher number of deleted comments that belonged to the root author compared to COA.\nH9: Toxicity victims are more likely to unfollow toxicity instigators compared to random authors. Unfollowing can be an example of employing a countermeasure, as it can reduce the probability of receiving additional toxic comments on future posts. To further test whether root authors tend to unfollow the accounts who post toxic comments on their posts that are directed to them, we averaged the toxicity scores of all comments belonging to each replier within a conversation. Afterwards, we ran a logistic regression model where the binary dependent variable indicates whether the root author unfollowed the replier while independent variable\nis the repliers\u2019 average toxicity scores. Additional control variables:We used two sets of control variables in this model, including a set of control variables related to the root author and control variables describing the replier. Victims might perceive toxic replies differently depending on who the replies are. For example, victims might be more likely to unfollow instigators who are not identifiable accounts. The results in Table 5 showed that there is a statistically significant positive correlation between unfollowing the replier and replier\u2019s average toxicity score within a conversation (\ud835\udc5d < 0.001), supporting H9. Furthermore, significance of control variables suggest that root authors are more likely to unfollow identifiable repliers with the lower number of followers (\ud835\udc5d < 0.05)."
        },
        {
            "heading": "7 DISCUSSIONS AND LIMITATIONS",
            "text": "This study presents interesting findings that demonstrate the association between toxicity and users\u2019 behavioral responses. Prior work has shown that toxicity has negative effects on users\u2019 mentality and well-being, and with this work, we showed that toxicity affects victims\u2019 online behaviors. Even though approximately one third of the victims just ignored toxicity on their posts, which is also considered as a (lack of) reaction, others performed one or more reactions that are not necessarily considered as positive. For example, we found that victims are more likely to respond back in a toxic way. We also found that he location of toxic replies plays an important role when it comes to behavioral reactions of the victims. For example, victims tend to not remove their initial posts and conversation comments if receiving toxic direct comments, while they are more likely to remove them receiving toxic nested replies. Also, this example shows that some victims tend to avoid further argument by deleting their comments, which indicates that toxicity has an impact on users\u2019 behaviors. Furthermore, we showed that similar victims can be clustered into groups based on their behavioral reactions and we also observed that some reactions tent to appear together. For example, we noticed that some victims unfollowed toxicity instigators and engaged in the conversation in a toxic way, while victims in the other cluster deleted the root tweet and unfollowed instigators. This implies that some users might share some psychological traits triggering the similar reactions of these users, which can be investigated in a future work.\nLimitations: We solely relied on the Perspective API model to identify toxic tweets, and our data thus inherits its shortcomings or biases. Additionally, we only considered tweets in our dataset which were written in English, and thus our analysis does not account for toxic behaviour in other languages. Studying the timelines of the accounts and establish a variable of the quantity of toxicity received could help overall understanding of social media users behaviors.\nBroader impacts: The primary aims of moderation and intervention techniques are to detect and remove toxic content and decrease the impact of toxic behavior online [16, 20]. We argue that our study can help with both aims. First, toxicity can be defined by context and culture; by analyzing the content that evokes victims\u2019 reactions, social media platforms might be able to implement more effective detection and moderation methods. We also argue that such analysis can help social media platforms to implement a targeted intervention method, since it can help identify vulnerable\nindividuals, who might need more support from the community or the platform."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "One of the main contributions of this study is the analysis of victims\u2019 behavioral reactions against toxic replies on social media, in particular Twitter, in terms of avoidance, countermeasures, negotiation and revenge. In conclusion, our results contribute to the general understanding of the impact of toxic replies on social media users\u2019 online behaviors. This study can help develop efficient intervention mechanisms to help mitigate the negative consequences of receiving toxic replies on social media."
        },
        {
            "heading": "A LONGITUDINAL ANALYSIS OF ACCOUNT DEACTIVATION, SWITCHING TO PRIVATE MODE, AND DELETING ROOT TWEETS",
            "text": "Figure 5 illustrated the distribution of daily percentages of newly deactivated accounts, private accounts, and deleted tweets for nine weeks. Interestingly, all figures demonstrate spikes of these three reactions on day three, for both victims and other authors, while the\ndistribution on other days is mostly uniform. Even though the results show that there is no significant evidence that toxicity impacts account deactivation, the percent of victims who deactivated their accounts is around 1%, which is higher compared to other authors who deactivated their accounts (0.4%) on day three (as illustrated in Figure 5a). In case of switching the accounts to the private mode, Figure 5b shows that, on day three, a higher percentage of other authors went private compared to victims (0.6% vs. 0.8%). There can be many reasons behind such occurrence, e.g., some users share a new posts and make their profiles public in order to receive a greater attention to that specific post. After couple of days, they might go back to being private. In terms of root tweet deletion shown in Figure 5c, more root tweets belonging to COA are getting deleted on day three compared to roots of CTV. The reason behind this might be that deletion of the root tweet does not really remove the conversation comments, so there is really nothing that victims can do it terms of toxic comments to their root. It is also interesting that root tweets that initiated COA get deleted at a higher rate. It might happen due to authors not getting desired amount of feedback, i.e., comments, likes or retweets, or for privacy reasons they decide to remove their activities from the social media."
        },
        {
            "heading": "B THE RESULTS FOR H9",
            "text": "H9: Toxicity victims are more likely to unfollow toxicity instigators compared to random authors."
        }
    ],
    "title": "Twitter Users\u2019 Behavioral Response to Toxic Replies",
    "year": 2022
}