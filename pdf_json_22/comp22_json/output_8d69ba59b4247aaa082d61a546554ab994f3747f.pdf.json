{
    "abstractText": "Achieving the capability of adapting to everchanging environments is a critical step towards building fully autonomous robots that operate safely in complicated scenarios. In multiagent competitive scenarios, agents may have to adapt to new opponents with previously unseen behaviors by learning from the interaction experiences between the ego-agent and the opponent. However, this adaptation is susceptible to opponent exploitation. As the ego-agent updates its own behavior to exploit the opponent, its own behavior could become more exploitable as a result of overfitting to this specific opponent\u2019s behavior. To overcome this difficulty, we developed a safe adaptation approach in which the ego-agent is trained against a regularized opponent model, which effectively avoids overfitting and consequently improves the robustness of the ego-agent\u2019s policy. We evaluated our approach in the Mujoco domain with two competing agents. The experiment results suggest that our approach effectively achieves both adaptation to the specific opponent that the ego-agent is interacting with and maintaining low exploitability to other possible opponent exploitation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Macheng Shen"
        },
        {
            "affiliations": [],
            "name": "Jonathan P. How"
        }
    ],
    "id": "SP:e8f66e2c05b099636f78afd038c4d9ff908efb14",
    "references": [
        {
            "authors": [
                "Y. Duan",
                "J. Schulman",
                "X. Chen",
                "P.L. Bartlett",
                "I. Sutskever",
                "P. Abbeel"
            ],
            "title": "RL2: Fast reinforcement learning via slow reinforcement learning",
            "venue": "arXiv preprint arXiv:1611.02779, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "International conference on machine learning. PMLR, 2017, pp. 1126\u20131135.",
            "year": 2017
        },
        {
            "authors": [
                "A. Nagabandi",
                "C. Finn",
                "S. Levine"
            ],
            "title": "Deep online learning via metalearning: Continual adaptation for model-based rl",
            "venue": "arXiv preprint arXiv:1812.07671, 2018.",
            "year": 1812
        },
        {
            "authors": [
                "A. Nagabandi",
                "I. Clavera",
                "S. Liu",
                "R.S. Fearing",
                "P. Abbeel",
                "S. Levine",
                "C. Finn"
            ],
            "title": "Learning to adapt in dynamic, real-world environments through meta-reinforcement learning",
            "venue": "arXiv preprint arXiv:1803.11347, 2018.",
            "year": 1803
        },
        {
            "authors": [
                "A. Nichol",
                "J. Schulman"
            ],
            "title": "Reptile: a scalable metalearning algorithm",
            "venue": "arXiv preprint arXiv:1803.02999, vol. 2, no. 3, p. 4, 2018.",
            "year": 1803
        },
        {
            "authors": [
                "M. Al-Shedivat",
                "T. Bansal",
                "Y. Burda",
                "I. Sutskever",
                "I. Mordatch",
                "P. Abbeel"
            ],
            "title": "Continuous adaptation via meta-learning in nonstationary and competitive environments",
            "venue": "arXiv preprint arXiv:1710.03641, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.N. Foerster",
                "R.Y. Chen",
                "M. Al-Shedivat",
                "S. Whiteson",
                "P. Abbeel",
                "I. Mordatch"
            ],
            "title": "Learning with opponent-learning awareness",
            "venue": "arXiv preprint arXiv:1709.04326, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D.K. Kim",
                "M. Liu",
                "M.D. Riemer",
                "C. Sun",
                "M. Abdulhai",
                "G. Habibi",
                "S. Lopez-Cot",
                "G. Tesauro",
                "J. How"
            ],
            "title": "A policy gradient algorithm for learning to learn in multiagent reinforcement learning",
            "venue": "International Conference on Machine Learning. PMLR, 2021, pp. 5541\u2013 5550.",
            "year": 2021
        },
        {
            "authors": [
                "K. Khetarpal",
                "M. Riemer",
                "I. Rish",
                "D. Precup"
            ],
            "title": "Towards continual reinforcement learning: A review and perspectives",
            "venue": "arXiv preprint arXiv:2012.13490, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Towards continual reinforcement learning: A review and perspectives",
            "venue": "arXiv preprint arXiv:2012.13490, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "C.A. Holt",
                "A.E. Roth"
            ],
            "title": "The nash equilibrium: A perspective",
            "venue": "Proceedings of the National Academy of Sciences, vol. 101, no. 12, pp. 3999\u20134002, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "S. Ganzfried",
                "T. Sandholm"
            ],
            "title": "Safe opponent exploitation",
            "venue": "ACM Transactions on Economics and Computation (TEAC), vol. 3, no. 2, pp. 1\u201328, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Liu",
                "C. Wu",
                "Q. Liu",
                "Y. Jing",
                "J. Yang",
                "P. Tang",
                "C. Zhang"
            ],
            "title": "Safe opponent-exploitation subgame refinement",
            "venue": "2022. [Online]. Available: https://openreview.net/forum?id=VwSHZgruNEc",
            "year": 2022
        },
        {
            "authors": [
                "N. Brown",
                "T. Sandholm"
            ],
            "title": "Safe and nested subgame solving for imperfect-information games",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Safe and nested subgame solving for imperfect-information games",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M.L. Littman"
            ],
            "title": "Markov games as a framework for multi-agent reinforcement learning",
            "venue": "Machine learning proceedings 1994. Elsevier, 1994, pp. 157\u2013163.",
            "year": 1994
        },
        {
            "authors": [
                "R. Lowe",
                "Y.I. Wu",
                "A. Tamar",
                "J. Harb",
                "O. Pieter Abbeel",
                "I. Mordatch"
            ],
            "title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Silver",
                "A. Huang",
                "C.J. Maddison",
                "A. Guez",
                "L. Sifre",
                "G. Van Den Driessche",
                "J. Schrittwieser",
                "I. Antonoglou",
                "V. Panneershelvam",
                "M. Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree search",
            "venue": "nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M. Jaderberg",
                "W. Czarnecki",
                "I. Dunning",
                "L. Marris",
                "G. Lever",
                "A. Castaneda",
                "C. Beattie",
                "N. Rabinowitz",
                "A. Morcos",
                "A. Ruderman"
            ],
            "title": "Human-level performance in 3D multiplayer games with populationbased reinforcement learning",
            "venue": "Science, vol. 364, no. 6443, pp. 859\u2013 865, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "O. Vinyals",
                "I. Babuschkin",
                "W.M. Czarnecki",
                "M. Mathieu",
                "A. Dudzik",
                "J. Chung",
                "D.H. Choi",
                "R. Powell",
                "T. Ewalds",
                "P. Georgiev"
            ],
            "title": "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
            "venue": "Nature, vol. 575, no. 7782, pp. 350\u2013354, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Lanctot",
                "V. Zambaldi",
                "A. Gruslys",
                "A. Lazaridou",
                "K. Tuyls",
                "J. P\u00e9rolat",
                "D. Silver",
                "T. Graepel"
            ],
            "title": "A unified game-theoretic approach to multiagent reinforcement learning",
            "venue": "arXiv preprint arXiv:1711.00832, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Bansal",
                "J. Pachocki",
                "S. Sidor",
                "I. Sutskever",
                "I. Mordatch"
            ],
            "title": "Emergent complexity via multi-agent competition",
            "venue": "arXiv preprint arXiv:1710.03748, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "E. Vinitsky",
                "Y. Du",
                "K. Parvate",
                "K. Jang",
                "P. Abbeel",
                "A. Bayen"
            ],
            "title": "Robust reinforcement learning using adversarial populations",
            "venue": "arXiv preprint arXiv:2008.01825, 2020.",
            "year": 2008
        },
        {
            "authors": [
                "W.M. Czarnecki",
                "G. Gidel",
                "B. Tracey",
                "K. Tuyls",
                "S. Omidshafiei",
                "D. Balduzzi",
                "M. Jaderberg"
            ],
            "title": "Real world games look like spinning tops",
            "venue": "arXiv preprint arXiv:2004.09468, 2020.",
            "year": 2004
        },
        {
            "authors": [
                "J.F. Nash"
            ],
            "title": "Equilibrium points in n-person games",
            "venue": "Proceedings of the national academy of sciences, vol. 36, no. 1, pp. 48\u201349, 1950.",
            "year": 1950
        },
        {
            "authors": [
                "A. Gleave",
                "M. Dennis",
                "C. Wild",
                "N. Kant",
                "S. Levine",
                "S. Russell"
            ],
            "title": "Adversarial policies: Attacking deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1905.10615, 2019.",
            "year": 1905
        },
        {
            "authors": [
                "H. He",
                "J. Boyd-Graber",
                "K. Kwok",
                "H. Daum\u00e9 III"
            ],
            "title": "Opponent modeling in deep reinforcement learning",
            "venue": "International conference on machine learning. PMLR, 2016, pp. 1804\u20131813.",
            "year": 2016
        },
        {
            "authors": [
                "R. Raileanu",
                "E. Denton",
                "A. Szlam",
                "R. Fergus"
            ],
            "title": "Modeling others using oneself in multi-agent reinforcement learning",
            "venue": "International conference on machine learning. PMLR, 2018, pp. 4257\u20134266.",
            "year": 2018
        },
        {
            "authors": [
                "A. Grover",
                "M. Al-Shedivat",
                "J. Gupta",
                "Y. Burda",
                "H. Edwards"
            ],
            "title": "Learning policy representations in multiagent systems",
            "venue": "International conference on machine learning. PMLR, 2018, pp. 1802\u20131811.",
            "year": 2018
        },
        {
            "authors": [
                "G. Papoudakis",
                "S.V. Albrecht"
            ],
            "title": "Variational autoencoders for opponent modeling in multi-agent systems",
            "venue": "arXiv preprint arXiv:2001.10829, 2020.",
            "year": 2001
        },
        {
            "authors": [
                "S. Li",
                "Y. Wu",
                "X. Cui",
                "H. Dong",
                "F. Fang",
                "S. Russell"
            ],
            "title": "Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 4213\u20134220.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhang",
                "Y. Li",
                "J. Li"
            ],
            "title": "Policy search by target distribution learning for continuous control",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, 2020, pp. 6770\u20136777.",
            "year": 2020
        },
        {
            "authors": [
                "J. Queeney",
                "I. Paschalidis",
                "C. Cassandras"
            ],
            "title": "Generalized proximal policy optimization with sample reuse",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Masood",
                "F. Doshi-Velez"
            ],
            "title": "Diversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse policies",
            "venue": "arXiv preprint arXiv:1906.00088, 2019.",
            "year": 1906
        },
        {
            "authors": [
                "J. Ho",
                "S. Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in neural information processing systems, vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "B. Peng",
                "T. Rashid",
                "C. Schroeder de Witt",
                "P.-A. Kamienny",
                "P. Torr",
                "W. B\u00f6hmer",
                "S. Whiteson"
            ],
            "title": "Facmac: Factored multi-agent centralised policy gradients",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, vol. 27, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "International conference on machine learning. PMLR, 2018, pp. 1861\u20131870.",
            "year": 2018
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nOne critical step towards building fully autonomous intelligent robots is to enable the capability of continual adaptation to new environments. In multiagent scenarios, besides the changing environment dynamics, agents must also adapt to the novel/evolving behaviors of other agents, which may not have been seen during training. There has been a lot of recent progress on fast adaptation to new tasks via metalearning in both single-agent reinforcement learning (RL) [1]\u2013[5], and multiagent reinforcement learning (MARL) [6]\u2013 [8]. During the meta-training phase, the agent meta-learns from tasks sampled from a task distribution how to quickly learn from a new task, which enables fast learning during the meta-testing phase. One important assumption within the meta-learning framework is that the task distribution is stationary [9]. As a result, the tasks encountered during the meta-testing phase are sampled from the same distribution as those encountered during the meta-training phase. However, in multiagent competitive settings, assuming access to the task distribution (sampling from the unknown opponent agent\u2019s policy distribution) is often unrealistic. Furthermore, as the ego-agent adapts to the opponent, the opponent may also adapt to the ego-agent concurrently, leading to non-stationarity from the ego-agent\u2019s perspective [8], [10]. Achieving effective adaptation requires knowledge about the\n*This work is supported by ARL-DCIST (Cooperative Agreement Number W911NF-17- 2-0181).\n1Macheng Shen is with the Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139, USA macshen@mit.edu\n2 Jonathan P. How is with the Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA jhow@mit.edu\nopponent\u2019s learning dynamics [7], [8], which is a very strong assumption in competitive scenarios.\nIn addition to the lack of knowledge about the opponent, another challenge of adaptation in competitive scenarios is to avoid being exploited by the opponent. In two-player zero-sum games, to exploit the opponent, the ego-agent has to deviate from the Nash-Equilibrium [11], leading to increased exploitability. Previous works demonstrated searchbased safe exploitation in extensive-form games [12], where the ego-agent updates its strategy via subgame-resolving leveraging a model of the opponent\u2019s strategy without substantially increasing the exploitability [13]\u2013[16]. However, these approaches are specialized for extensive-form games with knowledge of the whole game tree, so it is unclear how to extend these approaches to handle more general multiagent settings, potentially with continuous dynamics, within the model-free reinforcement learning setting.\nThis paper focuses on safe adaptation in two-player zerosum scenarios, where the ego-agent needs to update its policy based on the limited amount of interaction experience with an opponent agent. The goal of adaptation is to achieve high competitiveness (measured by cumulative reward) against this specific opponent that the ego-agent is interacting with, while the safety requirement is that the ego-agent must also maintain high competitiveness against any other possible opponent (with either a stationary or evolving policy) during the whole adaptation phase. We investigate this problem under the framework of Markov game [17] and multiagent reinforcement learning in a model-free setting without explicit assumptions on the state or action space. As such, the main contributions of this paper are:\n1) We present a novel Bayesian formulation of the safe adaptation problem within the MARL framework, which bridges the connection between robust MARL and safe adaptation. 2) We proposed an optimization objective for modeling the opponent, with a behavior cloning term for adaptation and a novel ensemble-regularization term to achieve low exploitability, which is derived from the Bayesian formulation. 3) We demonstrated that our approach achieves adaptation by learning from a limited amount of interaction experience with the opponent while maintaining low exploitability against a second opponent that actively co-adapts to exploit the ego-agent.\nar X\niv :2\n20 3.\n07 56\n2v 1\n[ cs\n.L G\n] 1\n4 M\nar 2\n02 2"
        },
        {
            "heading": "II. PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "A. Markov Games",
            "text": "A Markov game for N agents is defined by a set of states S describing the possible configurations of all agents, a set of actions A1, . . . ,AN , and a set of observations O1, . . . ,ON for each agent. Each agent has a stochastic policy \u03c0i : Oi\u00d7 Ai 7\u2192 [0, 1], and a reward function ri : S \u00d7Ai 7\u2192 R."
        },
        {
            "heading": "B. Multiagent reinforcement learning",
            "text": "The objective of each agent is to maximize its own cumulative reward Ri = \u2211T t=0 \u03b3\ntrti with discount factor \u03b3 and time horizon T [18]. As a result, the learning problem is formulated as finding a joint policy \u03c0 = {\u03c0i}i=1:N , where each policy maximizes its own reward,\nJi = Es\u223cp\u03c0,ai\u223c\u03c0i,a\u2212i\u223c\u03c0\u2212i [Ri(s,a)] , (1)\nwith p\u03c0 being the the transition dynamics induced by the joint policy \u03c0 and the subscript \u2212i denotes the set {j|j 6= i, j = 1, 2, . . . , N}."
        },
        {
            "heading": "C. MARL with policy distribution",
            "text": "Various empirical studies [18]\u2013[24] and a recent theoretical study [25] suggest a more general formulation of MARL in which each agent samples its policy from a policy distribution. Therefore, we consider the following objective function that learns a distribution of policies for each agent,\nJi = Es\u223cp\u03c0,a\u223c\u03c0,\u03c0\u223cP(\u03a0), [Ri(s,a)] , (2)\nwhere P is a joint distribution over the joint policy space \u03a0 = \u03a01 \u00d7 \u03a02 . . . \u00d7 \u03a0N . Each agent is learning its own policy distribution \u03a0i to optimize its objective Ji subject to the joint distribution \u03a0.\nNote that the feasibility set of Eq. 2 contains that of Eq. 1, which is analogous to the relationship between a mixed-strategy Nash Equilibrium and a pure-strategy Nash Equilibrium [26]. This relationship also suggests that Eq. 2 is a more appropriate learning objective than Eq. 1."
        },
        {
            "heading": "III. OUR APPROACH",
            "text": "As we focus on two-player zero-sum games, we rewrite Eq. 2 from the ego-agent\u2019s perspective as:\nJego = E\u03c0ego\u223cp(\u03a0ego),\u03c0oppo\u223cp(\u03a0oppo) [Es\u223cp\u03c0,a\u223c\u03c0 [Rego(s,a)]] , (3) where the ego-agent optimizes its policy distribution \u03a0ego, subject to a given opponent policy distribution \u03a0oppo. Since the optimal \u03a0ego with respect to Eq. 3 depends on the opponent policy distribution \u03a0oppo, determining \u03a0oppo is critical. Here, we discuss some common choices for the opponent policy distribution \u03a0oppo:\n1) Oracle policy distribution: Suppose we know the true policy distribution of the opponent, we can optimize the ego-agent policy distribution against the opponent policy distribution to obtain \u03a0oracleego . However, there are two problems with this approach: 1) Feasibility: In competitive scenarios, it is unlikely to get access to\nthe policy of the opponent. 2) Robustness: As the egoagent over-fits its policy to the opponent, the resulting \u03a0oracleego may not perform well (or even poorly as shown in [27]) against an adversarial opponent that is trained against \u03a0oracleego to exploit its weakness. 2) Learned opponent policy distribution [28]\u2013[31]: As the ego-agent interacts with the opponent, the egoagent can learn an internal model of the opponent policy \u03a0modeloppo distribution from the interaction experience as an approximation of the true opponent policy distribution. Many previous works [28]\u2013[30], assume access to the opponent\u2019s observation and action for this model learning, which is not a strong assumption in robotic domains with full-observability over the state space since the opponent\u2019s observation and action can be deducted from the state observation. Besides, [31] also demonstrates the possibility of learning an opponent model from the ego-agent\u2019s observation alone via variational inference over a hidden space that models the opponent\u2019s private information. However, this opponent modeling approach also suffers from the robustness problem mentioned earlier. 3) Nash Equilibrium policy distribution: Another way to model the opponent is to solve for the Nash Equilibrium policy distribution \u03a0nashoppo (or equivalently, the minimax solution [32] in two-player zero-sum games [26]), with no prior knowledge of the opponent\u2019s policy distribution. The corresponding optimal policy distribution for the ego-agent is also the Nash Equilibrium \u03a0nashego , which is the least exploitable policy. However, this approach does not attempt to adapt to the opponent that the ego-agent is interacting with, leading to sub-optimal performance against an opponent with exploitability.\nWe argue that a better approach to modeling the opponent\u2019s policy distribution should leverage the available interaction experience for adaptation as well as stay close to the equilibrium distribution for robustness against adversarial exploitation. We derive this approach by rephrasing the subproblem within Eq. 3 of modeling the opponent policy distribution \u03c0oppo \u223c p (\u03a0oppo) into a Bayesian inference problem over the space of policy distributions given the interaction experience D between the ego-agent and the opponent:\n\u03c0oppo \u223c p (\u03a0oppo|D) \u221d p (\u03a0oppo|\u2205)\u00d7 p (D|\u03a0oppo) , = pNE (\u03a0oppo)\u00d7 p (D|\u03a0oppo) , (4)\nwhere \u2205 denotes the empty set, so pprior (\u03a0oppo) = p (\u03a0oppo|\u2205) is the prior distribution over the opponent\u2019s policy space before obtaining any interaction experience. We argue that the Nash Equilibrium policy distribution is a sensible choice for this prior, i.e. pprior (\u03a0oppo) = pNE (\u03a0oppo), since with no information about the opponent, the best choice is to minimize the ego-agent\u2019s exploitability. As the ego-agent receives more interaction experience with the opponent, the posterior distribution p (\u03a0oppo|D) is updated\nthrough the likelihood term p (D|\u03a0oppo) while regularized by the prior term, which ensures adaptation to the opponent while maintaining low exploitability.\nHowever, the posterior inference problem Eq. 4 is challenging for two reasons: 1) Solving for the Nash Equilibrium policy distribution pNE (\u03a0oppo) is a challenging problem; 2) representing and parameterizing policy distribution is challenging. Therefore, we apply the following two approximations,\n1) We approximate the Nash Equilibrium policy distribution via an ensemble of policies generated via Alg. 1, which has been shown in [18], [23], [27] to produce robust agent behaviors that are much less exploitable than policies generated without ensembling. 2) Instead of modeling the posterior distribution, we seek for a single opponent policy that approximates the maximum a posteriori probability (MAP) estimate of Eq. 4.\nWith these two approximations, we propose an alternative formulation for the estimated opponent policy as the optimization problem in Eq. 5.\n\u03c0\u0302oppo = arg min \u03c0\n[D(\u03a0ensembleoppo | \u03c0) + \u03bb1Llikelihood(D | \u03c0)\n+ \u03bb2LRL(\u03a0ensembleego , \u03c0)], (5)\nwhere \u03bb1 and \u03bb2 are hyper-parameters. The first term D(\u03a0ensembleoppo | \u03c0) denotes a distance metric between the opponent policy ensemble generated via Alg. 1 and the estimated opponent model, which regularizes the opponent policy to stay close to the robust ensemble policy distribution. This term corresponds to the prior term pNE (\u03a0oppo) in Eq. 4. The second term Llikelihood(D | \u03c0) is the loglikelihood of observing the interaction experience D given the opponent policy, which corresponds to the likelihood term p (D|\u03a0oppo) in Eq. 4. The third term LRL(\u03a0ensembleego , \u03c0)] is the reinforcement learning loss that optimizes the opponent policy against the ego-agent\u2019s policy ensemble,\nLRL(\u03a0ensembleego , \u03c0) = \u2212E\u03c0ego\u223cp(\u03a0ensembleego ),\u03c0oppo=\u03c0 [Es\u223cp\u03c0,a\u223c\u03c0 [Roppo(s,a)]] , (6)\nwhere the minus sign ensures that minimizing this loss results in maximization of the reward. This term does not correspond to any term in Eq. 4. Intuitively, this term enables the opponent policy to continually evolve, as we update the ego-agent\u2019s policy ensemble to adapt to the opponent via optimizing its reinforcement learning objective in Eq. 7,\nJego = E\u03c0ego\u223cp(\u03a0ensembleego ),\u03c0oppo=\u03c0\u0302oppo [Es\u223cp\u03c0,a\u223c\u03c0 [Rego(s,a)]] . (7) We include LRL because we found that although this term does not make much difference when adapting to a stationary opponent, it could be critical for achieving high competitiveness against an evolving opponent as we show later in the experiment section.\nNow we discuss our choice for the first two terms in Eq. 5. The first term D(\u03a0ensembleoppo | \u03c0) measures the discrepancy\nAlgorithm 1 Ensemble training Require: Ensemble size N , number of training iterations K\n1: Randomly initialize policy ensembles: \u03a0ensembleego = {\u03c0iego}i=1:N , \u03a0ensembleoppo = {\u03c0ioppo}i=1:N 2: for k = 1:K do 3: Randomly sample policy index: j \u223c {1, . . . , N}, l \u223c {1, . . . , N} 4: Environment rollout(\u03c0jego, \u03c0loppo) 5: Update \u03c0jego and \u03c0loppo to optimize objective Eq. 2 6: end for 7: return \u03a0ensembleego and \u03a0ensembleoppo\nAlgorithm 2 Safe adaptation\nRequire: Policy ensembles \u03a0ensembleego and \u03a0ensembleoppo , interaction experience D, number of iterations K\n1: freeze \u03a0ensembleoppo 2: Initialize \u03c0\u03020oppo as random policy 3: \u03a0ensemble,0oppo \u2190 \u03a0ensembleoppo 4: for k = 1:K do 5: \u03c0\u0302koppo \u2190 update opponent(\u03c0\u0302k\u22121oppo ,\u03a0ensembleoppo , 6: \u03a0ensemble,k\u22121ego ,D) . one gradient step of Eq. 5 7: \u03a0ensemble,kego \u2190 update ego agent(\u03c0\u0302koppo,\u03a0 ensemble,k\u22121 ego )\n. one gradient step of Eq. 7 8: end for 9: return \u03a0ensemble,Kego\nbetween the policy ensemble \u03a0ensembleoppo and our estimated opponent policy \u03c0. There are several closed-form metrics to measure the discrepancy between two policies, including KL-divergence discrepancy [33], total variation distance [34] and maximum mean discrepancy [35]. However, it is unclear how to select one metric over another given a specific application domain, and whether the selected metric can optimally discriminate between two policies. To resolve this ambiguity and achieving optimal discriminative power, we choose to learn the discrepancy metric via adversarial learning following the paradigm of generative adversarial imitation learning (GAIL) [36], where we train a discriminator Dw(o, a) : Ooppo\u00d7Aoppo \u2192 [0, 1] to minimize the following discrimination loss,\nE\u03c4\u03c0 [logDw(o, a)] + E\u03c4\u03a0ensembleoppo [log(1\u2212Dw(o, a))], (8)\nsuch that\nD(\u03a0ensembleoppo | \u03c0) = \u2212E\u03c4\u03c0 [logDw(o, a)], (9)\nwhere the shorthand notation \u03c4(\u00b7) denotes the trajectory distribution when the ego-agent follows policy \u03c0ego \u223c p (\u03a0ego), while the opponent follows policy (\u00b7). This loss function is minimized by maximizing an imitation reward rimit = logDw(o, a).\nThe second term in Eq. 5 is the log-likelihood of observing the experience D given the opponent policy \u03c0. Since the opponent policy can only affect the probability of the\nopponent\u2019s taken action, this term can be reduced to behavior cloning loss,\np (D | \u03c0) = \u2211\n(a,o)\u2208D\nlog \u03c0(a | o). (10)\nIn practice, we use mini-batch to calculate the gradient of this loss."
        },
        {
            "heading": "IV. RESULTS AND DISCUSSION",
            "text": ""
        },
        {
            "heading": "A. Experiment setting",
            "text": "We evaluate our safe adaptation approach on the Multiagent Mujoco domain [37], where each robot is decomposed into parts that are controlled by individual agents as illustrated in Fig. 1. We use a zero-sum reward where the egoagent tries to maximize the reward for moving forward and the opponent agent tries to minimize this reward.\nTo evaluate the capability of safe adaptation to a previously unseen opponent, we describe the following procedure to set up the evaluation:\nOff-line training phase: 1) Alternating for K1 iterations, between one-step (K =\n1) policy ensemble training of size N = 5 for both agents, \u03a0ensembleego ,\u03a0 ensemble oppo via Alg. 1 and one-step\ntraining of exploiter opponent \u03c0expoppo via Alg. 3. 2) Freeze \u03a0ensembleego ,\u03a0 ensemble oppo , and training exploiter op-\nponent \u03c0expoppo against \u03a0ensembleego for an additional K2 iterations. On-line adaptation phase (for \u03a0ensembleego to adapt to \u03c0 exp oppo): 1) Freeze \u03c0expoppo, \u03a0ensembleoppo , and \u03a0 ensemble ego . 2) Collect interaction experience D between \u03a0ensembleego and \u03c0expoppo. 3) Unfreeze \u03a0ensembleego . 4) Initialize second exploiter opponent \u03c0\u2217expoppo from \u03c0 exp oppo. 5) Alternating for K3 iterations, between one-step safe adaptation for \u03a0ensembleego to adapt to \u03c0 exp oppo via Alg. 2\nand one-step training of the second exploiter opponent \u03c0\u2217expoppo to exploit \u03a0 ensemble ego via Alg. 3.\nAlgorithm 3 Train exploiter opponent\nRequire: Ego-agent ensemble \u03a0ensembleego , exploiter opponent policy \u03c0expoppo, number of training iterations K\n1: freeze \u03a0ensembleego 2: for k = 1:K do 3: Train \u03c0expoppo against \u03a0ensembleego by gradient decent on\nLRL(\u03a0ensembleego , \u03c0 exp oppo)\n4: end for 5: Unfreeze \u03a0ensembleego 6: return \u03c0expoppo\nDuring the off-line training phase, we alternate between ensemble training \u03a0ensembleego and \u03a0 ensemble oppo , and training of exploiter opponent \u03c0expoppo to mitigate the well-known problem of training imbalance [23], [38] in competitive/adversarial training, so that the exploiter opponent can always catch up with the ego-agent before the ego-agent becomes too strong. The additional training of the exploiter opponent ensures that the exploiter is sufficiently trained to exploit the ego-agent, which motivates the ego-agent to adapt to this exploiter in the adaptation phase. During the whole off-line training phase, the ego-agent does not collect experience against the exploiter opponent \u03c0expoppo. As a result, this exploiter is a previously unseen opponent in the ego-agent\u2019s perspective.\nDuring the on-line adaptation phase, the ego-agent policy ensemble \u03a0ensembleego adapts to the exploiter opponent \u03c0 exp oppo given a fixed size interaction experience D, with regularization from \u03a0ensembleoppo . Concurrently, the second exploiter \u03c0\u2217expoppo is trained to exploit the \u03a0 ensemble ego . As a result, the reward against the first exploiter opponent \u03c0expoppo during the adaptation phase measures the capability of adaptation against a stationary opponent, while the reward against the second exploiter opponent \u03c0\u2217expoppo measures robustness/safety against an evolving adversarial exploiter.\nWe use stochastic policy with Gaussian distribution, and two fully-connected hidden layers, each with 128 hidden units followed by ReLU activate layer, as the policy and critic network architecture. We use a pytorch implementation1 of Soft Actor-Critic [39] (SAC) with dual critic networks and automatic tuning of the entropy parameter to train the ensemble networks and the two exploiter opponents in both the off-line training phase and the on-line adaptation phase. We use a replay buffer size of one million for the off-line training phase but reduce that to half a million for the online adaptation phase to save memory. Our adaptation implementation is modified from the GAIL implementation in PyTorch-RL2 with Proximal Policy Optimization [40] (PPO) for training \u03c0\u0302oppo, with PPO rollout batch size of 1000, minibatch size of 128, and 10 gradient updates per PPO batch. We use a learning rate 0.001 for both training and adaptation. In both environments, the number of steps per episode is fixed at 500. Each agent can observe the joints/bodies position and\n1https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithmswith-PyTorch\n2https://github.com/Khrylx/PyTorch-RL\nvelocity of its own and its opponent\u2019s, so the agents have full observability.\nIn the off-line training phase, we use K1 = 10000 iterations (episodes), and K2 = 5000 iterations the swimmer environment and K2 = 2000 for the ant environment. In the on-line adaptation phase, we use different K3 for the two environments until the average rewards become steady. We collect 10 episodes of interaction experience for adaptation, which corresponds to |D| = 5000 environment steps. We tune the hyper-parameters \u03bb1 and \u03bb2 independently for each environment. We select from the following values: {0.1, 0.5, 1.0, 5.0}, and manually choose the best one by looking at the adaptation rewards against both of the two exploiter opponents. The selected hyper-parameters are: \u03bb1 = 1.0, \u03bb2 = 1.0 for the swimmer environment, and \u03bb1 = 0.1, \u03bb2 = 1.0 for the ant environment."
        },
        {
            "heading": "B. Results",
            "text": "We show the exploiter opponent rewards during the adaptation phase in Fig. 2, which includes the following settings: 1) oracle opponent, where the ego-agent is trained against the first exploiter directly. This setting is unrealistic since the first exploiter\u2019s policy is unknown to the ego-agent; 2) ensemble opponent, where the ego-agent is trained against the opponent ensemble policy generated from the off-line training phase; 3) reg bc rl: our proposed approach including all the three terms in Eq. 5; 4) bc rl: ablation of our approach without the ensemble regularization term; 5) reg rl: ablation of our approach without the behavior cloning term; 6) reg bc: ablation of our approach without the RL term.\nFig. 2a shows that the oracle opponent setting (reference) achieves the best adaptation against the first exploiter opponent, while all the settings with the behavior cloning loss achieve comparable adaptation performance as the reference. For all the settings, the opponent reward decreases due to\nthe fact that the opponent policy is fixed but the ego-agent policy is updating. However, there remains a gap between those settings without interaction experience with the first exploiter opponent (ensemble opponent and reg rl) and the other settings.\nFig. 2b shows that those settings without the ensemble regularization term (oracle opponent and bc rl) are unable to achieve robustness against the second exploiter opponent which actively exploits the ego-agent\u2019s policy. Combining Fig. 2a and 2b, we can conclude that our approach (reg bc rl) strikes a better trade-off between adaptation and safety, compared with the two reference approaches (oracle opponent: good adaptation but poor robustness; ensemble opponent: good robustness but poor adaptation).\nFig. 2c and 2d show consistent results as those in Fig. 2a and 2b: the ego-agent\u2019s policies that are adapting to the first exploiter opponent are also more susceptible to be exploited by the second exploiter opponent. Besides, Fig. 2d shows that the setting without the RL loss (reg bc) is also exploited after the second exploiter is sufficiently trained. Our conjectured reason for this observation is that the RL term enables the estimated opponent policy \u03c0\u0302oppo to discover the weakness of the ego-agent\u2019s policy, which also helps reduce the egoagent\u2019s exploitability because the ego-agent is trained against the estimated opponent policy.\nTo quantitatively measure the adaptation and robustness of different settings, we calculate the area under curve (AUC) metrics of both the first exploiter\u2019s (measures adaptation) and the second exploiter\u2019s (measures robustness) reward curves normalized by the two reference settings (oracle opponent and ensemble opponent), as shown in Table I. We calculate the AUC using reward curves from steps 1e7 to 2e7 for the swimmer environment and from steps 1e7 to 3.5e7 for the ant environment. These results are consistent with our hypothesis that learning from the interaction experience with the opponent enables adaptation, but without regularization from the ensemble policy, this adaptation could be highly exploitable. The regularization term is effective for achieving safe adaptation. As a result, our approach achieves the best overall metric which combines adaptation and robustness.\nFrom Table I, we can also see that the adaptation metric and the robustness metric tend to be negatively correlated. To further analyze the relationship between adaptation (exploitation) and robustness (exploitability), we show the normalized area between curves (ABC) in Table II, which is the gap between the reward against the second exploiter opponent and the first exploiter opponent. Lower ABC indicates less sensitivity to opponent exploitation. This result, together with the result shown in Table I, verifies the well-known trade-off between exploitation and exploitability [13]: the settings with both the ensemble regularization term and the behavior cloning terms (reg bc rl and reg bc) are slightly more exploitable than their counterpart without the behavior cloning term (reg rl), which is an inevitable consequence of exploiting the interaction experience against the first exploiter opponent."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "This paper investigates safe adaptation which is an important problem in competitive MARL. In contrast to the widely-studied fast adaptation problem, our focus is on maintaining low exploitability during the adaptation. Our key innovation is the derivation of a novel ensemble regularization term from a Bayesian formulation of the MARL objective function. We show empirically that our proposed approach is effective both at adaptation to a previously unseen opponent given experience from a few interaction episodes and at maintaining low exploitability against an adversarial opponent that actively exploits the weakness of the ego-agent. Our ablation study and analysis reveal the effect of each term in our proposed loss function, as well as verify the well-known trade-off between exploitation and exploitability. Our work contributes to an important step towards building reliable intelligent robots that are able to operate safely in competitive multiagent scenarios against ever-changing adversarial opponents."
        }
    ],
    "title": "Safe adaptation in multiagent competition",
    "year": 2022
}