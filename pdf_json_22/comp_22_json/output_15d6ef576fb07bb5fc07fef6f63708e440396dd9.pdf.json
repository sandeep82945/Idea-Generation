{
    "abstractText": "Image aesthetic quality assessment (AQA) aims to assign numerical aesthetic ratings to images whilst image aesthetic captioning (IAC) aims to generate textual descriptions of the aesthetic aspects of images. In this paper, we study image AQA and IAC together and present a new IAC method termed Aesthetically Relevant Image Captioning (ARIC). Based on the observation that most textual comments of an image are about objects and their interactions rather than aspects of aesthetics, we first introduce the concept of Aesthetic Relevance Score (ARS) of a sentence and have developed a model to automatically label a sentence with its ARS. We then use the ARS to design the ARIC model which includes an ARS weighted IAC loss function and an ARS based diverse aesthetic caption selector (DACS). We present extensive experimental results to show the soundness of the ARS concept and the effectiveness of the ARIC model by demonstrating that texts with higher ARS\u2019s can predict the aesthetic ratings more accurately and that the new ARIC model can generate more accurate, aesthetically more relevant and more diverse image captions. Furthermore, a large new research database containing 510K images with over 5 million comments and 350K aesthetic scores, and code for implementing ARIC are available at https://github.com/PengZai/ARIC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhipeng Zhong"
        },
        {
            "affiliations": [],
            "name": "Fei Zhou"
        },
        {
            "affiliations": [],
            "name": "Guoping Qiu"
        }
    ],
    "id": "SP:d2ae65dcf4e61a95db0cbbc6090de881d8fd0946",
    "references": [
        {
            "authors": [
                "P. Anderson",
                "B. Fernando",
                "M. Johnson",
                "S. Gould"
            ],
            "title": "Spice: Semantic propositional image caption evaluation",
            "venue": "European conference on computer vision, 382\u2013398. Springer.",
            "year": 2016
        },
        {
            "authors": [
                "P. Anderson",
                "X. He",
                "C. Buehler",
                "D. Teney",
                "M. Johnson",
                "S. Gould",
                "L. Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 6077\u20136086.",
            "year": 2018
        },
        {
            "authors": [
                "S. Banerjee",
                "A. Lavie"
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 65\u201372.",
            "year": 2005
        },
        {
            "authors": [
                "K.-Y. Chang",
                "K.-H. Lu",
                "C.-S. Chen"
            ],
            "title": "Aesthetic critiques generation for photos",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 3514\u20133523.",
            "year": 2017
        },
        {
            "authors": [
                "J. Chen",
                "H. Guo",
                "K. Yi",
                "B. Li",
                "M. Elhoseiny"
            ],
            "title": "VisualGPT: Data-Efficient Adaptation of Pretrained Language Models for Image Captioning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 18030\u201318040.",
            "year": 2022
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2020
        },
        {
            "authors": [
                "K. Ghosal",
                "A. Rana",
                "A. Smolic"
            ],
            "title": "Aesthetic image captioning from weakly-labelled photographs",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "M.Z. Hossain",
                "F. Sohel",
                "M.F. Shiratuddin",
                "H. Laga"
            ],
            "title": "A Comprehensive Survey of Deep Learning for Image Captioning",
            "venue": "ACM Comput. Surv., 51(6).",
            "year": 2019
        },
        {
            "authors": [
                "X. Jin",
                "L. Wu",
                "G. Zhao",
                "X. Li",
                "X. Zhang",
                "S. Ge",
                "D. Zou",
                "B. Zhou",
                "X. Zhou"
            ],
            "title": "Aesthetic attributes assessment of images",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia, 311\u2013319.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Kim"
            ],
            "title": "Convolutional Neural Networks for Sentence Classification",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1746\u20131751. Doha, Qatar: Association for Computational Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "S. Lai",
                "L. Xu",
                "K. Liu",
                "J. Zhao"
            ],
            "title": "Recurrent Convolutional Neural Networks for Text Classification",
            "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI\u201915, 2267\u20132273. AAAI Press. ISBN 0262511290.",
            "year": 2015
        },
        {
            "authors": [
                "Lin",
                "C.-Y."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "N. Murray",
                "L. Marchesotti",
                "F. Perronnin"
            ],
            "title": "AVA: A large-scale database for aesthetic visual analysis",
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, 2408\u20132415. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "D.V. Nieto",
                "L. Celona",
                "C. Fernandez-Labrador"
            ],
            "title": "Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment",
            "year": 2022
        },
        {
            "authors": [
                "J. Olive",
                "C. Christianson",
                "J. McCary"
            ],
            "title": "Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation",
            "venue": "Springer Publishing Company, Incorporated, 1st edition. ISBN 1441977120.",
            "year": 2011
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "W.-J. Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "J.M. P\u00e9rez",
                "J.C. Giudici",
                "F. Luque"
            ],
            "title": "pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks",
            "venue": "arXiv preprint arXiv:2106.09462.",
            "year": 2021
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "A. Radford",
                "J. Wu",
                "R. Child",
                "D. Luan",
                "D. Amodei",
                "I Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems, 28: 91\u201399.",
            "year": 2015
        },
        {
            "authors": [
                "K. Schwarz",
                "P. Wieschollek",
                "H.P. Lensch"
            ],
            "title": "Will people like your image? learning the aesthetic space",
            "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 2048\u20132057. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "G. Valenzise",
                "C. Kang",
                "F. Dufaux"
            ],
            "title": "Advances and challenges in computational image aesthetics",
            "venue": "Human Perception of Visual Information, 133\u2013181.",
            "year": 2022
        },
        {
            "authors": [
                "R. Vedantam",
                "C. Lawrence Zitnick",
                "D. Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 4566\u20134575.",
            "year": 2015
        },
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and Tell: A Neural Image Caption Generator",
            "year": 2014
        },
        {
            "authors": [
                "X. Zhang",
                "X. Gao",
                "L. He",
                "W. Lu"
            ],
            "title": "MSCAN: Multimodal Self-and-Collaborative Attention Network for image aesthetic prediction tasks",
            "venue": "Neurocomputing, 430: 14\u2013",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "X. Gao",
                "W. Lu",
                "L. He",
                "J. Li"
            ],
            "title": "Beyond vision: A multimodal recurrent attention convolutional neural network for unified image aesthetic prediction tasks",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhao",
                "X. Yao",
                "J. Yang",
                "G. Jia",
                "G. Ding",
                "T.-S. Chua",
                "B.W. Schuller",
                "K. Keutzer"
            ],
            "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u20131.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhou",
                "X. Lu",
                "J. Zhang",
                "J.Z. Wang"
            ],
            "title": "Joint Image and Text Representation for Aesthetics Analysis",
            "venue": "Proceedings of the 24th ACM International Conference on Multimedia, MM \u201916, 262\u2013266. New York, NY, USA: Association for Computing Machinery. ISBN 9781450336031.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Image aesthetic quality assessment (AQA) aims to automatically score the aesthetic values of images. This is very challenging because aesthetics is a highly subjective concept. AQA models are either regressors or classifiers that extract image features and output the aesthetic scores or classes (Zhao et al. 2021). Many images, especially those on photography competition websites contain both aesthetic scores and textual comments. It has been shown that including both the visual and textual information can improve AQA performances (Zhou et al. 2016; Zhang et al. 2021).\nUnlike visual contents which are rather abstract, texts are much easier for human to comprehend. Image captioning, which aims to automatically generate textual descriptions of images has been extensively researched, and much progress has been achieved in recent years with the help of deep learning technology (Hossain et al. 2019). Whilst the vast major-\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nity of researchers have focused on image captions about objects and their relations and interactions, the more abstract and arguably more challenging problem of image aesthetic captioning (IAC) (Chang, Lu, and Chen 2017), which aims to generate comments about the aesthetic aspects of images, has received much less attention.\nIn the existing literature, image AQA and IAC are studied independent from each other. However, these are closely related areas of aesthetic visual computing, we therefore believe that jointly study them is beneficial. For example, currently IAC performances are evaluated either subjectively or based on metrics such as SPICE (Anderson et al. 2016) and BLEU (Papineni et al. 2002) which evaluate the similarity between the generated and reference (ground truth) sentences rather than the aesthetic relevance of the texts. It will be very useful if we can directly measure the aesthetic relevance of the IAC results, for example, how accurate the generated caption can predict the images aesthetic scores.\nIn this paper, we first contribute a large research database called DPC2022 which contains 510K images, over 5 million comments and 350K aesthetic ratings. We then present a new IAC method called Aesthetically Relevant Image Captioning (ARIC). Based on the observation that most image comments are general descriptions of image contents and not about their aesthetics, we first introduce the concept of Aesthetic Relevance Score (ARS) of a sentence. ARS consists of 5 components including scores related to aesthetic words, the length of the sentence, words describing objects, the sentiments of the sentence, and term frequency-inverse document frequency (tf-idf). A list of aesthetic and object words has been manually constructed from DPC2022. After (automatically) labelling the comments in DPC2022 with the ARS scores, we then construct an ARS predictor based on the Bidirectional Encoder Representations (BERT) language representation model (Devlin et al. 2018).\nThe introduction of ARS and its predictor have enabled the design of the ARIC model which includes an ARS weighted IAC loss function and an ARS based diverse aesthetic caption selector (DACS). Unlike methods in the literature that simply learned a direct mapping between the images and their comments in the database, regardless of the aesthetic relevance of the comments (in fact many of the texts have nothing to do with aesthetics), the ARIC model is constructed based on ARS weighted loss function which\nar X\niv :2\n21 1.\n15 37\n8v 1\n[ cs\n.C V\n] 2\n5 N\nov 2\n02 2\nensures that it learns aesthetically relevant information. Furthermore, unlike traditional methods that pick the output sentences based on the generator\u2019s confidence which is not directly based on aesthetic relevance, the introduction of the ARS has enabled the design of DACS which can output a diverse set of aesthetically highly relevant sentences. In addition, we have fine tuned the powerful pre-trained image and text matching model CLIP (Radford et al. 2021) using DPC2022 as an alternative to ARS for selecting aesthetically relevant captions.\nWe have performed extensive experiments. As DPC2022 is by far one of the largest AQA databases, we first provide baseline AQA results. We then present ARIC\u2019s image aesthetic captioning performances to demonstrate its effectiveness. In summary, the contributions of this paper are 1. A large image database, DPC2022, is constructed for re-\nsearching image aesthetic captioning and image aesthetic quality assessment. DPC2022 is the largest dataset containing both aesthetic comments and scores.\n2. A new concept, aesthetic relevance score (ARS), is introduced to measure the aesthetic relevance of sentences. Lists of key words and other statistical information for constructing the ARS model are made available.\n3. Based on ARS, we have developed the new aesthetically relevant image captioning (ARIC) system capable of producing not only aesthetically relevant but also diverse image captions."
        },
        {
            "heading": "Related Work",
            "text": "Image aesthetic quality assessment (IAQA). One of the main challenges in AQA is the lack of large scale high quality annotated datasets. The aesthetic visual analysis (AVA) dataset (Murray, Marchesotti, and Perronnin 2012) contains 250K images, each has an aesthetic score, and other labels. This is still one of the most widely used databases in aesthetic quality assessment. The aesthetic ratings from online data (AROD) dataset (Schwarz, Wieschollek, and Lensch 2018) contains 380K images from a photo sharing website. The score of an image is based on the number of viewers who liked it, which can be an unreliable indicator. Recent AQA systems are mostly based on deep learning neural networks and supervised learning that take the images or their textual descriptions or both as input to predict the aesthetic scores (Valenzise, Kang, and Dufaux 2022; Zhang et al. 2020).\nImage aesthetic captioning (IAC). Aesthetic image captioning was first proposed in (Chang, Lu, and Chen 2017) where the authors also presented the photo critique captioning dataset (PCCD) which contains pair-wise image comment data from professional photographers. It contains 4235 images and more than sixty thousands captions. The AVACaptions dataset (Ghosal, Rana, and Smolic 2019) was obtained by using a probabilistic caption filtering method to clean the noise of the original AVA captions. It has about 230K images with roughly 5 captions per image. The DPCCaptions dataset (Jin et al. 2019) contains over 150K images and nearly 2.5 million comments where each comment was automatically annotated with one of the 5 aesthetic attributes\nof the PCCD through knowledge transfer. Very recently, the Reddit Photo Critique Dataset (RPCD) was published by (Nieto, Celona, and Fernandez-Labrador 2022). This dataset contains tuples of image and photo critiques which has 74K images and 220K comments. Image aesthetic captioning is an under explored area and existing works mostly used LSTM model to generate aesthetic captions.\nImage captioning. Image captioning aims to generate syntactically and semantically correct sentences to describe images. This is a complex and challenging task in which many deep learning-based techniques have been developed in recent years (Hossain et al. 2019). Training deep models requires large amount of annotated data which are very difficult to obtain. VisualGPT (Chen et al. 2022) leverages the linguistic knowledge from a large pre-trained language model GPT-2 (Radford et al. 2019) and quickly adapt it to the new domain of image captioning. It has been shown that this Transformer (Vaswani et al. 2017) based technique has superior performances to LSTM based methods used in previous IAC works (Chang, Lu, and Chen 2017)(Ghosal, Rana, and Smolic 2019)(Jin et al. 2019). We adopt this method for IAC to provide benchmark performances for the newly established DPC2022 dataset."
        },
        {
            "heading": "The DPC2022 Dataset",
            "text": "The AVA dataset (Murray, Marchesotti, and Perronnin 2012) which was constructed a decade ago remains to be one of the largest and most widely used in image AQA. As discussed above, datasets available for IAC are either small or constructed from the original AVA dataset. In the past 10 years, the source website of the AVA dataset (www.dpchallenge.com) has been continuously organising more photography competitions and has accumulated a lot more photos and comments. We therefore believe it is a good time to make full use of the data currently available from the website to construct a new dataset to advance research in image aesthetic computing. We first crawled the website and grabbed all currently available images and their comments. Initially we obtain a total of 780K images and their comments. We then used an industrial strength natural language processing tool spaCy (https://spacy.io/) to clean the data by removing items such as emoji and other strange spellings, symbols and punctuation marks. At the end, we have kept 510K images with good quality and clean comments. Within these 510K images, there are 350K with both comments and aesthetic scores ranging between 1 and 10. Figure 1 shows the statistics of the DPC2022 dataset. On average, each image contains roughly 10 comments, each comments on average contains 21 sentences, and the average sentence length is 19 words."
        },
        {
            "heading": "Image Aesthetic Relevance of Text",
            "text": "The dictionary definition of aesthetic is \u201cconcerned with beauty or the appreciation of beauty\u201d, this is a vague and subjective concept. Photographers often use visual characteristics such as lighting, composition, subject matter and colour schemes to create aesthetic photos. Comments associated with images, especially those on the internet web-\nsites, can refer to a variety of topics, not all words are relevant to the aesthetics of images, and some words may be more relevant than others. To the best of the authors\u2019 knowledge, there exist no definitions of image aesthetic relevant words and phrases in the computational aesthetic literature. And yet when inspecting the comments from the DPC2022 dataset, it is clear that many have nothing to do with the images\u2019 aesthetic qualities. It is therefore appropriate to distinguish words that refer to the aesthetic quality and those that are not relevant. We have developed the Aesthetic Relevant Score (ARS) to quantify a comment\u2019s aesthetic relevance. It is based on a mixture of subjective judgement and statistics from the dataset. Before describing the ARS in detail, it is appropriate to note that this is by no means the only way to define such a quantity, however, we will demonstrate its usefulness through application in image aesthetic captioning and multi-modal image aesthetic quality assessment."
        },
        {
            "heading": "Labelling a Sentence with its ARS",
            "text": "The ARS of a sentence t is defined as:\nARS(t) = A(t) + L(t) +O(t) + S(t) + Tfidf (t) (1)\nwhere A(t) is related to aesthetic words, L(t) is related to the length of t, O(t) is related to object words, S(t) is the sentiment score of t, and Tfidf (t) is related to term frequency\u2013inverse document frequency.\nThe components in (1) are computed based on statistics of the DPC2022 database and details of the computational procedures are in Appendix I in the supplementary materials. For computing A(t), we manually selected 1022 most frequently appeared image aesthetics related words such as shot, color, composition, light, focus, background, subject, detail, contrast, etc., the full list of these words, {AWlist}, can be found in Appendix II. For computing O(t), we manually selected 2146 words related to objects such as eye, sky, face, ribbon, water, tree, flower, expression, hand, bird, glass, dog, hair, cat, smile, sun, window, car, etc, the full\nlist of these words, {OWlist}, can be found in Appendix III. The sentiment score S(t) is calculated based on the BerTweet model (Pe\u0301rez, Giudici, and Luque 2021). Figure 1(d) shows the distribution of ARS of the DPC2022 dataset. It is seen that many sentences contain no aesthetic relevant information, and the majority of the comments contain very low aesthetic relevant information. Informally inspecting the data shows that this is reasonable and expected."
        },
        {
            "heading": "Automatically Predicting the ARS",
            "text": "For the ARS to be useful, we need to be able to predict any given text\u2019s aesthetic relevant score. With the labelled data described above, we adopt the pre-trained Bidirectional Encoder Representations (BERT) language representation model (Devlin et al. 2018) for this purpose. A 768\u00d7 1 fully connected layer is cascaded to the output of BERT as shown in Figure 2. We train the model with the mean squared error (MSE) loss function. Table 1 shows the ARS prediction performance. It is seen that the Spearman\u2019s rank-order correlation (SRCC) and the Pearson linear correlation coefficient (PLCC) are both above 0.95, indicating excellent prediction accuracy. It is therefore possible to predict a sentence\u2019s ARS with an architecture shown in Figure 2."
        },
        {
            "heading": "Aesthetically Relevant Image Captioning",
            "text": ""
        },
        {
            "heading": "Image Captioning Model",
            "text": "For image captioning, many models based on LSTM (Vinyals et al. 2014) and Transformer (Vaswani et al. 2017) have been developed. Given an image, we first extract visual features using an encoder structure, then use a decoder to generate image captions as shown in Figure 3. To obtain image embedding, we follow the bottom-up-attention model (Anderson et al. 2018) and use ResNet-101 (He et al. 2016) as the backbone network of the Faster R-CNN (Ren et al. 2015) to extract image features. The bottom-up attention model uses a region proposal network (RPN) and a region-of-interest (ROI) pooling strategy for object detection. In this paper, we retain 50 most interesting regions and pass them to the encoder for image caption generation. Each region is represented by a 2048-dimensional feature vector.\nFor the decoder, we use VisualGPT (Chen et al. 2022) to generate image aesthetic captions. VisualGPT is an encoderdecoder transformer structure based on GPT-2 (Radford et al. 2019) which is a powerful language model but does\nnot have the ability to understand images. VisualGPT uses a self-resurrecting activation unit to encode visual information into the language model, and balances the visual encoder module and the language deocder module."
        },
        {
            "heading": "Aesthetically Relevant Loss Function",
            "text": "Unlike general image captioning which in most cases is about revealing the content and semantics of the image, e.g., objects in the image and their relations, image aesthetic captioning (IAC) should focus on learning aesthetically relevant aspects of the image. Arguably, IAC is much more challenging than generic image captioning. The very few existing IAC related literature, e.g., (Chang, Lu, and Chen 2017), (Jin et al. 2019) and (Ghosal, Rana, and Smolic 2019), simply treat the raw image comments from the training data as the IAC ground truth. However, as discussed previously, many of the texts contain no or very low aesthetic relevant information. The ARS(t) quantitatively measures the aesthetic relevance of a piece of text t, a higher ARS(t) indicates that t contains high aesthetic relevant information and a low ARS(t) indicates otherwise. We therefore use ARS(t) to define the aesthetically relevant loss function to construct the IAC model.\nGiven training set {T (k)} = {I, y\u2217(tk)} which contains pairs of input image I and one of its corresponding ground-truth caption sentences y\u2217(tk) = (y\u22171(tk), y \u2217 2(tk), ..., y \u2217 Nk\n(tk)) consisting of words y\u2217i (tk), i = 1, 2, ..., Nk, we generate a caption sentence y(tk) = (y1(tk), y2(tk), ..., yNk(tk)) that maximises the following ARS(t) weighted cross-entropy loss\nLAR(\u03b8) = \u2212 k=|T |\u2211 k=1 ARS(tk) Nk\u2211 i=1 log p (yi(tk) = y \u2217 i (tk)|\u03b8)\n(2) where tk is the kth training sentence (note one image I can have multiple sentences), |T | is the training set size (in terms of sentences), Nk is the number of words in the kth training sentence, and \u03b8 is the model parameters."
        },
        {
            "heading": "Diverse Aesthetic Caption Selector (DACS)",
            "text": "Traditional IAC models such as the one described in Figure 3 output sentences based on the generator\u2019s confidence. All existing methods in the literature (Chang, Lu, and Chen 2017), (Jin et al. 2019) and (Ghosal, Rana, and Smolic 2019) adopt this approach. However, the generator\u2019s confidence is not directly based on aesthetic relevance. The new ARS concept introduced in this paper has provided a tool to measure the aesthetic value of a sentence, which in turn can help selecting aesthetically more relevant and more diverse captions from the generator.\nBased on the ARS score defined in (1) for a piece of text t, we design a sentence selector method that enables the IAC model to generate diverse and aesthetically relevant sentences. Given a picture, we use the IAC model and beam search (Olive, Christianson, and McCary 2011) to generate N most confident sentences. Then we use a sentence transformer1 to extract the features of the sentences, and then calculate the cosine similarity among the N sentences. We group sentences whose cosine similarity is higher than 0.7 into the same group such that each group contains many similar sentences. Then, we use the ARS predictor in Figure 2 to estimate the ARS of the sentences in each group. If the average ARS of a group\u2019s sentences is below the mean ARS of the training data (2.1787), then the group is discarded because their aesthetic values are low. From the remaining groups, we then pick the sentence with the highest ARS in each group as the output. With this method, which we call the Diverse Aesthetic Caption Selector (DACS), we generate aesthetically highly relevant and diverse image captions. In the experiment section, we will show that OpenAI\u2019s CLIP (Radford et al. 2021), a powerful image and text embedding model that can be used to find the text snippet best represents a given image, can be fine tuned to play the role of ARS for picking the best sentence in a group as output."
        },
        {
            "heading": "Multi-modal Aesthetic Quality Assessment",
            "text": "The purposes of performing multi-modal AQA are two folds. Firstly, the newly established DPC2022 is one of the largest publicly available datasets and quite unique in the sense that it contains both comments and aesthetic scores. We want to provide some AQA performance baselines. Secondly, we make the reasonable assumption that the more accurate a piece of text can predict an image\u2019s aesthetic rating, the more aesthetically relevant is the text to the image.\nThe image AQA model is shown in Figure 4. There are 3 separate paths. The first takes the image as input and output\n1https://huggingface.co/models, all-miniLM-L6-v2\nthe image\u2019s aesthetic rating. A backbone neural network is used for feature extraction. The features are then fed to an MLP to regress the aesthete rating. The second path takes the textual description of the image as input and output the image\u2019s aesthetic score. Again, a neural network backbone is used for textual feature extraction. The features are fed to an MLP network to output the aesthetic score. The third path is used to implement multi-modal AQA where visual and textual features are concatenated and fed to a MLP to output the aesthetic score."
        },
        {
            "heading": "Experimental Results",
            "text": "We perform experiments based on the newly constructed DPC2022 dataset. It has a total of 510,000 photos where each photo also contains a review text. 350,000 out of the 510,000 photos also have aesthetic ratings. We call the dataset containing all images setA, and the subset containing images with both reviews and aesthetic ratings SetB. We use SetA for photo aesthetic captioning experiment and SetB for multi-modal aesthetic quality assessment. We divide SetB into a test set containing 106,971 images and a validation set containing 10,698 images, and the remaining 232,331 images are used as the training set. In photo aesthetic captioning, we use the same test set and validation set as those used in multi-modal aesthetic quality assessment, and the remaining SetA data (392,331 images) is used for training."
        },
        {
            "heading": "Evaluation Criteria",
            "text": "Similar to those in the literature, we use 5 metrics to measure the performance of image AQA including Binary Accuracy (ACC), Spearman rank order correlation coefficient (SRCC), Pearson linear correlation coefficient (PLCC), root mean square error (RMSE), and mean absolute error(MAE). For image captioning, we use 4 metrics widely used in related literature including CIDER (Vedantam, Lawrence Zitnick, and Parikh 2015), METETOR (Banerjee and Lavie 2005), ROUGE (Lin 2004), and SPICE (Anderson et al. 2016). SPICE is a word-based semantic similarity measure for scene graphs, and the others are all based on n-gram."
        },
        {
            "heading": "Implementation Details",
            "text": "For image based AQA, we have used VGG-16, RESNET18, DESNET121, RESNEXT50, and ViT (Dosovitskiy et al.\n2020) as the backbone network and used their pre-trained models rather than training from scratch. The images are resized to 224x224, randomly flipped horizontally, and normalised to have a mean of 0.5 and a variance 0.5. For text based AQA, we have used the TEXTCNN (Kim 2014), TEXTRNN (Lai et al. 2015), BERT (Devlin et al. 2018) and ROBERTA (Liu et al. 2019)) as the backbone network. For BERT and ROBERTA, we use their pre-trained models. Finally, we limit the number of tokens for each image\u2019s comment to 512. For IAC, we used the pre-trained GPT2 model and set token size to 64. All experiments were performed on a machine with 4 NVIDIA A100 GPUs. Adam optimizer with a learning rate of 2e\u22125 without weight decay was used. The AQA models were trained for 16 epochs and the IAC models were trained for 32 epochs. Codes will be made publicly available."
        },
        {
            "heading": "Image AQA Baseline Results",
            "text": "In the first set of experiments, we experimented various network architectures in order to obtain some baseline image AQA results. Table 2 lists the baseline results of image based, text based, and mutlti-modal AQA results when the backbone used different network architectures. These results show that the textual reviews of the images are aesthetically highly relevant. In two out of 5 metrics, using the review text only gives the best performances. It is also seen that in 3 other metrics, multi-modal AQA has the best performances."
        },
        {
            "heading": "Verification of ARS",
            "text": "To verify the soundness of the newly introduced ARS, we divide the DPC2022 validation set into two groups according to the ARS. If the ARS model is sound, then we would expect a review text with a higher ARS should be able to predict the image\u2019s aesthetic rating more accurately. Conversely, a review text that has a lower ARS would produce a less accurate prediction of the image\u2019s aesthetic rating. Table 3 shows the multi-modal AQA performances for two groups of testing samples. Low ARS (A,O,L, S, Tfidf ) represent the group where test samples have a ARS (A,O,L, S, Tfidf ) lower than their respective average values. High ARS\n(A,O,L, S, Tfidf ) represent the group where test samples have a ARS (A,O,L, S, Tfidf ) higher than their respective average values. Table 4 shows that as the ARS increases, so does the AQA performances. These results clearly show that comments with high ARS can consistently predict the images aesthetic rating more accurately than those with low ARS. This means that ARS and its components can indeed measure the aesthetic relevance of textual comments."
        },
        {
            "heading": "ARS versus CLIP for Ranking Text Relevance",
            "text": "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs (Radford et al. 2021). It is a powerful model that can match natural language descriptions with visual contents. In this experiment, we first fine tune the pre-trained CLIP model with the training set of the DPC2022 data such that the images and their corresponding comments are matched. With such a fine tuned model, we can rank the sentences in the comments of the images. Let CLIP(I, t) be the matching score between an image I and a sentence t. Suppose we have two sentences ti\nand tj , if CLIP(I, ti) > CLIP(I, tj) then ti and I is a better match. Because CLIP has been fine tuned on DPC2022, it is reasonable to assume that if a text and an image has a better CLIP matching score, then the text can describe the image more accurately. Similar to ARS, we can use CLIP to rank the sentences according to their CLIP matching scores. To compare the CLIP and ARS in the selection of texts for image AQA, we rank the sentences of the images in the test set and perform text based AQA for the sentences in different ranks. The SRCC and MAE performances of those ranked by ARS and CLIP are shown in Figure 5. It is seen that the higher ranking sentences by both methods can predict the aesthetic rating more accurately, and that both seem to perform very similarly. These results show that the properly fine tuned CLIP model can also be used to select aesthetically relevant text for image AQA. It is worth noting that ARS is a very simple scheme while CLIP is a much more complex but powerful model. Figures 6 shows visual examples of how the sentences are ranked by ARS and CLIP."
        },
        {
            "heading": "Aesthetic Captioning Performances",
            "text": "Table 5 shows the aesthetic image captioning performances of our new aesthetically relevant model (ARIC) based on the loss function LAR(\u03b8) as defined in (2). For comparison, we have implemented a baseline model in which standard cross entropy loss function is used, i.e., setting ARS(tk) = 1 in (2). It is seen that our new model consistently outperforms the baseline model. These results demonstrate the usefulness and effectiveness of introducing the aesthetically relevant score (ARS) for aesthetics image captioning. Even though these metrics do not directly measure aesthetic relevance, the better performances of the new method nevertheless demonstrate the soundness of the new algorithm design.\nAs described in the main method, with the introduction of ARS, we can use ARS based diverse aesthetic caption selector (DACS) to generate a diverse set of image captions rather than being restricted to output only one single sentence as in previous methods (Chang, Lu, and Chen 2017). Figures 6 shows examples of aesthetic captions generated with ARS based and CLIP based DACS. More examples are available in the Supplementary materials."
        },
        {
            "heading": "Image AQA based on Generated Captions",
            "text": "In this experiment, we evaluate image AQA performances based on the generated image captions and results are shown\nin Table 6. We can observe that in text only AQA, the generated texts perform worse than the true captions, and also worse than image only AQA. It is also seen that using captions selected by the new diverse aesthetic caption selector (DACS) either based on ARS or CLIP performs better than using captions without selection. It is interesting to observe that in multi-modal AQA, including the generated texts has started to slightly exceed image only AQA, but is still quite far from those using ground truth. For example, using ground truth text, the ACC of multi-modal AQA is 0.8407 whilst that using generated captions is 0.8203. Would a future image captioning model be able to generate captions to close the gap? This would be a very interesting question and a goal for future research."
        },
        {
            "heading": "Concluding Remarks",
            "text": "In this paper, we have attempted to study two closely related subjects of aesthetic visual computing, image aesthetic quality assessment (AQA) and image aesthetic captioning (IAC). We first introduce the concept of aesthetic relevance\nscore (ARS) and use it to design the aesthetically relevant image captioning (ARIC) model through an ARS weighted loss function and an ARS based diverse aesthetic caption selector (DACS). We have presented extensive experimental results which demonstrate the soundness of the ARS concept and the effectiveness of the ARIC model. We have also contributed a large research database DPC2022 that contains images with both comments and aesthetic ratings."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Natural Science Foundation of China under grants U22B2035 and 62271323, in part by Guangdong Basic and Applied Basic Research Foundation under grant 2021A1515011584, and in part by the Shenzhen Research and Development Program under grants JCYJ20220531102408020 and JCYJ20200109105008228)."
        }
    ],
    "title": "Aesthetically Relevant Image Captioning",
    "year": 2022
}