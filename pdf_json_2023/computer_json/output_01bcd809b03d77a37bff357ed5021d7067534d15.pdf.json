{
    "abstractText": "Since adaptive learning comes in many shapes and sizes, it is crucial to find out which adaptions can be meaningful for which areas of learning. Our work presents the result of an experiment conducted on an online platform for the acquisition of German spelling skills. We compared the traditional online learning platform to three different adaptive versions of the platform that implement machine learning-based student-facing interventions that show the personalized solution probability. We evaluate the different interventions with regards to the error rate, the number of early dropouts, and the users\u2019 competency. Our results show that the number of mistakes decreased in comparison to the control group. Additionally, an increasing number of dropouts was found. We did not find any significant effects on the users\u2019 competency. We conclude that student-facing adaptive learning environments are effective in improving a person\u2019s error rate and should be chosen wisely to have a motivating impact.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nathalie Rzepka"
        },
        {
            "affiliations": [],
            "name": "Katharina Simbeck"
        },
        {
            "affiliations": [],
            "name": "Hans-Georg M\u00fcller"
        },
        {
            "affiliations": [],
            "name": "Marlene B\u00fcltemann"
        },
        {
            "affiliations": [],
            "name": "Niels Pinkwart"
        }
    ],
    "id": "SP:2ef2ec746a8882415c49fc333dcae1db906b627b",
    "references": [
        {
            "authors": [
                "K.E. Arnold",
                "M.D. Pistilli"
            ],
            "title": "Course signals at Purdue",
            "venue": "Proceedings of the 2nd International Conference on Learning Analytics and Knowledge. ACM,",
            "year": 2012
        },
        {
            "authors": [
                "Beheshitha",
                "S. S"
            ],
            "title": "The role of achievement goal orientations when studying effect of learning analytics visualizations",
            "venue": "Proceedings of the Sixth International Conference on Learning Analytics & Knowledge. ACM,",
            "year": 2016
        },
        {
            "authors": [
                "R. Bodily",
                "K. Verbert"
            ],
            "title": "Review of Research on Student-Facing Learning Analytics Dashboards and Educational Recommender Systems",
            "venue": "IEEE Transactions on Learning Technologies",
            "year": 2017
        },
        {
            "authors": [
                "G. Chen",
                "C. Chang",
                "C. Wang"
            ],
            "title": "Ubiquitous learning website: Scaffold learners by mobile devices with information-aware techniques",
            "venue": "Computers & Education",
            "year": 2008
        },
        {
            "authors": [
                "N. Cliff"
            ],
            "title": "Dominance statistics: Ordinal analyses to answer ordinal questions",
            "venue": "Psychological Bulletin",
            "year": 1993
        },
        {
            "authors": [
                "T. De14 Denley"
            ],
            "title": "How Predictive Analytics and Choice Architecture Can Improve Student Success",
            "venue": "Research & Practice in Assessment",
            "year": 2014
        },
        {
            "authors": [
                "B. Dodge",
                "J. Whitmer",
                "J.P. Frazee"
            ],
            "title": "Improving undergraduate student achievement in large blended courses through data-driven interventions",
            "venue": "In (Baron, J. et al. Eds.): Proceedings of the Fifth International Conference on Learning Analytics And Knowledge. ACM,",
            "year": 2015
        },
        {
            "authors": [
                "J. Grann",
                "Bushway",
                "D.: Competency map. In (Pistilli"
            ],
            "title": "Eds.): Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK '14",
            "year": 2014
        },
        {
            "authors": [
                "Hu09 Huang",
                "Y.-M"
            ],
            "title": "A Markov-based Recommendation Model for Exploring the Transfer of Learning on the Web",
            "venue": "Journal of Educational Technology & Society",
            "year": 2009
        },
        {
            "authors": [
                "J. Kim",
                "I.-H. Jo",
                "Y. Park"
            ],
            "title": "Effects of learning analytics dashboard: analyzing the relations among dashboard utilization, satisfaction, and learning achievement",
            "venue": "Asia Pacific Education Review 1/17,",
            "year": 2016
        },
        {
            "authors": [
                "C Ott"
            ],
            "title": "Illustrating performance indicators and course characteristics to support students\u2019 self-regulated learning in CS1",
            "venue": "Computer Science Education",
            "year": 2015
        },
        {
            "authors": [
                "Y. PJ15 Park",
                "I. Jo"
            ],
            "title": "Development of the learning analytics dashboard to support students\u2019 learning performance",
            "venue": "Journal of Universal Computer Science 1/21,",
            "year": 2015
        },
        {
            "authors": [
                "A. PL03 Paramythis",
                "S. Loidl-Reisinger"
            ],
            "title": "Adaptive learning environments and elearning standards",
            "venue": "European Conference on eLearning. Glasgow Caledonian University,",
            "year": 2003
        },
        {
            "authors": [
                "N Rzepka"
            ],
            "title": "An Online Controlled Experiment Design to Support the Transformation of Digital Learning towards Adaptive Learning Platforms",
            "venue": "Proceedings of the 14th International Conference on Computer Supported Education. SCITEPRESS - Science and Technology Publications,",
            "year": 2022
        },
        {
            "authors": [
                "O.C. Santos",
                "J.G. Boticario",
                "D. P\u00e9rez-Mar\u00edn"
            ],
            "title": "Extending web-based educational systems with personalised support through User Centred Designed recommendations along the e-learning life cycle",
            "venue": "Science of Computer Programming",
            "year": 2014
        },
        {
            "authors": [
                "C. SW14 Saul",
                "H.D. Wuttke"
            ],
            "title": "Turning Learners into Effective Better Learners: The Use of the askMe! System for Learning Analytics",
            "year": 2014
        },
        {
            "authors": [
                "A. Vargha",
                "H.D. Delaney"
            ],
            "title": "A Critique and Improvement of the \"CL\" Common Language Effect Size Statistics of McGraw and Wong",
            "venue": "Journal of Educational and Behavioral Statistics 2/25,",
            "year": 2000
        },
        {
            "authors": [
                "B Ve13 Vesin"
            ],
            "title": "Applying Recommender Systems and Adaptive Hypermedia for e-Learning Personalizatio",
            "venue": "COMPUTING AND INFORMATICS",
            "year": 2013
        },
        {
            "authors": [
                "F.M. van der Kleij",
                "R.C.W. Feskens",
                "T.J.H.M. Eggen"
            ],
            "title": "Effects of Feedback in a Computer-Based Learning Environment on Students\u2019 Learning Outcomes",
            "venue": "Review of Educational Research",
            "year": 2015
        },
        {
            "authors": [
                "F. Wa08 Wang"
            ],
            "title": "Content recommendation based on education-contextualized browsing events for web-based personalized",
            "year": 2008
        },
        {
            "authors": [
                "M. Wa84 Wang"
            ],
            "title": "The Adaptive Learning Environments Model: Design, Implementation, and Effects",
            "year": 1984
        },
        {
            "authors": [
                "B.T. WL18 Wong",
                "K.C. Li"
            ],
            "title": "Learning Analytics Intervention: A Review of Case Studies",
            "venue": "In (Wang, F. L. Ed.): 2018 International Symposium on Educational Technology. ISET 2018",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "adaptions can be meaningful for which areas of learning. Our work presents the result of an experiment conducted on an online platform for the acquisition of German spelling skills. We compared the traditional online learning platform to three different adaptive versions of the platform that implement machine learning-based student-facing interventions that show the personalized solution probability. We evaluate the different interventions with regards to the error rate, the number of early dropouts, and the users\u2019 competency. Our results show that the number of mistakes decreased in comparison to the control group. Additionally, an increasing number of dropouts was found. We did not find any significant effects on the users\u2019 competency. We conclude that student-facing adaptive learning environments are effective in improving a person\u2019s error rate and should be chosen wisely to have a motivating impact.\nKeywords: Adaptive Learning, Adaptive Intervention, Learning Analytics."
        },
        {
            "heading": "1 Introduction",
            "text": "Adaptive learning environments have been the subject of research for years and are also increasingly used in practice. Being broadly defined and having a large number of possibilities, it is important to examine the effectiveness of different interventions in adaptive learning environments. In this contribution, we specifically investigate adaptive learning interventions for acquisition of spelling skills in German. Our interventions can all be grouped under the term student-facing interventions, i.e., information is displayed to the user in the UI (User Interface). In this case, the student-facing interventions are displays that show the personalized solution probability. Not exclusively, yet especially in the case of student-facing interventions, it should be noted that the influence of the intervention on motivation plays a major role and can influence learning success.\nOur article is structured as follows: first, section two explains the theoretical foundations of underlying motivation theory, adaptive learning, and summarizes previous research on\n1 HTW Berlin, Treskowallee 8, 10318 Berlin, {rzepka@,simbeck@,marlene.bueltemann@student.}htw-ber-\nlin.de 2 Universit\u00e4t Potsdam, Am neuen Palais 10, 14469 Potsdam, hgmuelle@uni-potsdam.de, 3 Humboldt-Universit\u00e4t, Unter den Linden 6, 10117 Berlin, pinkwart@hu-berlin.de\nstudent-facing interventions. This is followed by an explanation of the experimental design methodology, the underlying predictive model, the three interventions, and the hypotheses for evaluation. The results and a discussion of the findings follow. In the end, the summary, the limitation of the work as well as implications for further research are given."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Adaptive Learning",
            "text": "Adaptive learning is a concept that aims to optimize the learning success for students through the modification and adaptation of learning content and environments [Wa84]. It is based on the premise that students learn in different ways, which requires different instructional methods, learning paths, or learning characteristics [Wa84]. To meet the students\u2019 individual needs, different interventions can be used [Wa84]. Paramythis and LoidlReisinger define a learning environment as adaptive \u201cif it is capable of: monitoring the activities of its users; interpreting these on the basis of domain-specific models; inferring user requirements and preferences out of the interpreted activities, appropriately representing these in associated models; and, finally, acting upon the available knowledge on its users and the subject matter at hand, to dynamically facilitate the learning process.\u201d [PL03].\n[PL03] differentiate between various categories of adaptation in learning environments These are adaptive interaction, adaptive course delivery, content discovery and assembly and adaptive collaboration support. Adaptive interaction describes the adaptation to the user interface of an app or a learning environment. Adaptive course delivery adjusts the content of the course or exercise. Content discovery and assembly uses diverse sources to assemble material and adaption collaboration support focuses on learning processes that involve collaboration and communication. In their review, Wong and Li categorized intervention methods into four categories [WL18]. The first category, direct message, describes all interventions in which a student or a tutor is contacted through messaging., For instance, when they are identified as being at risk or to provide them with additional learning resources. Actionable feedback, the second category, describes all interventions that provide insights and dashboards about the users\u2019 learning performance, as well as recommendations to improve ones\u2019 learning progress. Categorization of students summarizes interventions that are grouping students based on learning analytics results, such as at-risk predictions. The last category, course redesign aims to adapt the content of a course to the users\u2019 need.\nOne intervention in the category of actionable feedback is student-facing intervention."
        },
        {
            "heading": "2.2 Student-facing interventions",
            "text": "One possibility to implement an intervention in an adaptive learning environment may be to show the student his or her performance data [WL18]. In their review, Bodily and Verbert review student-facing learning analytics reporting systems, that directly show students\u2019 performance data [BV17]. With 29% and 27% the functionalities \u201cenhanced visualization\u201d and \u201cdata mining recommender system\u201d were the most prevalent systems. Enhanced visualization was defined here as the visualization of student data including a class comparison or interactivity feature [BV17]. In their review, they found 14 articles that measured the effects of student-facing reports on student achievements. Of these, eight articles showed significant improvement in student achievements [AP12, CCW08, De14, HHWH09, KJP16, SW14, VKIB13, Wa08] while five had no significant results [DWF15, GB14, ORH15, PJ15, SBP14]. One study had positive and negative effects, depending on the visualization [BHGJ16].\nArnold and Pistilli\u2019s research showed how information on students\u2019 performance prediction provided directly back to students can improve student retention [AP12]. They proposed an early intervention in which a performance prediction is generated, and the results are sent to the students via e-mail. Additionally, the e-mail contains a traffic light signal indicating how well the student is progressing. Results showed higher retention rates in all years, with higher retention rates the earlier students were exposed to the system [AP12].\nKim et al. validated the impact of a learning analytics dashboard that displays online behavior patterns in college students [KJP16]. They found that student\u2019s scores improved in the intervention group compared to the control group. Furthermore, they analyzed the usage frequency and the satisfaction with the dashboard and found that the frequency did not have a significant impact. However, the satisfaction with the dashboard is highest among students who open it infrequently. Users with a high academic score, in contrast, are less satisfied with the dashboard. An earlier study with the same dashboard and fewer participants showed no significant improvement in learning achievement [PJ15].\nChen et al. were able to improve academic performance, task completion rates, and learning goal achievement rates in an experiment that used a ubiquitous learning environment that implemented learning status awareness, schedule reminders, and mentor recommendations [CCW08]. An example with a huge sample size of 50,000 students is the research of [De14]. Here, a course recommendation system is evaluated, and improved the passing grade rate of its users is compared to students who did not use the system [De14]. The results are particularly high for low-income and minority students. Furthermore, Huang et al. evaluated a recommender system based on the Markov chain model to provide learning paths for students [HHWH09]. Users in the treatment group outperformed users in the control group in terms of knowledge acquisition and integration.\nThe knowledge dashboard in Sauls and Wuttkes\u2019 study provided learner insights into the number and scores of questions and tests, strengths and weaknesses, as well as the status\nof the learning goals [SW14]. Users of the system had higher average grades and lower failure rates [SW14].\nAnother possibility to intervene in digital learning environments is to display adaptive feedback. Van der Kleij et al. conducted a meta-analysis to compare the effectiveness of different feedback methods on learning outcomes in computer-based learning [vFE15]. They distinguished between three forms of feedback: correctness of the answer, providing the correct answer, and elaborate feedback, which for example provides an explanation. They found that elaborate feedback was more effective than the other two forms of feedback [vFE15]. The results showed that the effects of elaborate feedback on higher-order learning outcomes were greater than on lower-order learning outcomes [vFE15].\nWhile there is already much research about different kinds of student-facing interventions, research lacks student-facing interventions in online language learning. This paper, therefore, presents an online-controlled experiment that compares the effectiveness of adaptive learning in three different student-facing interventions on a German spelling learning platform to the control group. For this purpose, we transformed a learning platform into an adaptive learning platform and implemented a machine learning-based prediction model on which the interventions are based. We evaluate the interventions based on three different research questions:\nRQ 1: How effective are student-facing interventions on an online spelling platform in terms of the error rates?\nRQ 2: How effective are student-facing interventions on an online spelling platform concerning the number of early dropouts?\nRQ 3: How effective are student-facing interventions on an online spelling platform concerning the users\u2019 competency?\nRQ 4: How do effects differ with regards to different implementations of student-facing interventions?"
        },
        {
            "heading": "3 Methodology",
            "text": "The platform Orthografietrainer.net is a platform to support the acquisition of German spelling and grammar skills. It is a free, web-based platform that currently has more than one million registered users from Germany, Austria, and Switzerland. The platform offers spelling exercises on various orthographic areas, e.g., comma formation, capitalization, hyphenation, and sounds and letters. The exercises are suitable for students from fifth grade onwards, as well as adults, university students, or older school children are among the users. Typically, the platform is used in blended classroom scenarios. Thus, the students are registered by their teachers and receive the login credentials during class. The teacher can assign homework their students, which is then displayed to the students as\npending. These exercise sets usually consist of ten exercise sentences with increasing difficulty. After each sentence, the user receives immediate feedback as to whether the solution was correct or not. If not, the task is repeated, and the user must solve more similar sentences before moving on to the more difficult sentences. At the end, the user receives an overview of his or her results. A teacher is provided with a dashboard that shows the progress and results of all the students.\nWe conducted an online-controlled experiment that was carried out from the 21st of June to the 31st of October in 2022. During this time, all users in the student user group who performed capitalization tasks were randomly assigned to the control group or one of three intervention groups. All three intervention groups adapt to the user based on the prediction of the users\u2019 performance. Tab. 1 shows the distribution of users across the different groups. The experiment was pre-registered at the OSF4 and as an architectural setup described in [Rz22].\nIntervention group\nNumber of users Number of sessions Number of answered\nsentences\nControl 2,447 8,049 225,426\nIntervention 1 1,835 5,950 148,625\nIntervention 2 1,929 6,222 159,677\nIntervention 3 1,910 6,072 153,658\nTab. 1: Experiment metrics per intervention group\nThe adaptive learning environment includes a prediction model, which predicts the probability of correctly processing the next sentence by the user. It was trained using a dataset from the Orthografietrainer.net platform and contained 200.000 records of capitalization performed by students from grades five through twelve. After feature engineering, the dataset contains 1078 features: 6 features that refer to demographic data of the user, such as gender or grade level; 17 features that describe the upcoming sentence, including its difficulty as well as information on former attempts of the user to solve this sentence. The other 1055 features represent the users\u2019 learning history on the platform. Each feature indicates for an exercise sentence whether the user has already processed this sentence and if so, whether his or her attempt was correct or not. Using this dataset, we trained a decision tree classifier and were able to predict a user\u2019s probability of correctly solving the sentence with an accuracy of 97,04% (Recall: 96,31%; Precision: 97,75%). The decision tree model was chosen over other model implementations because of inferior accuracy or performance.\nIn our experiment, we compared the control group and three interventions (Tab. 2). The control group does not receive any adaptive interventions on top of the established platform. Interventions 1 and 2 are student-facing interventions where users are shown their\n4 10.17605/OSF.IO/3R5Y7\nprediction results. In Intervention 1, the prediction result is shown verbally (see Tab. 3 and Fig. 1, left), in intervention 2 it is shown as a percentage (see Fig. 1, right). Intervention 3 does not show the prediction results. Instead, for users whose prediction result is below 50%, the suitable spelling rule is displayed (see Fig. 2).\nGroup Description\nControl No adaptive interventions.\nIntervention 1 Prediction results are shown to the user verbally.\nIntervention 2 Prediction results are shown to the user as a percentage.\nIntervention 3 If the prediction is <50% to solve that sentence correctly, the ortho-\ngraphical rule is shown to the user.\nTab. 2: Overview adaptive interventions.\nP Verbal display\nP<0.2 Attention! This sentence is especially difficult for you.\nP<0.4 Beware! This sentence is difficult for you.\nP<0.6 Think carefully! This sentence is moderately difficult for you.\nP<0.8 Relax! This sentence is easy for you.\nP>0.8 No problem! This sentence is especially easy for you.\nTab. 3: Verbal prediction results (translated from German into English).\ncentage (\u201cYou are 92.09% likely to get this sentence right.\u201d)\nEvaluation.\nThe effects of adaptive learning interventions are evaluated using three hypotheses. In hypothesis 1 we expect a change in the relative number of incorrect answers. We calculate the relative number of incorrect answers by dividing all incorrect answers per user by all given answers per user. In hypothesis 2, we expect a change in the number of early dropouts. As stated before, each exercise set consists of ten sentences. A session is defined as dropped out if there are more than 45 minutes between two processed sentences. We set 45 minutes as the threshold because, after 45 minutes of inactivity, the user is logged out automatically. If the user leaves the platform before he or she finished the whole exercise set, it is possible to continue later in time. However, this will count as a new session. In hypothesis 3 we expect a change in the users\u2019 competency. We calculate the competency of the users with the Rasch model, an implementation of the item response theory (IRT). Here, not only the users\u2019 responses are included, but also the item difficulty. In the statistical analysis, we first test the assumptions of homogeneity of variance and normal distribution. Since the assumptions were not met in all three hypotheses, we continue with nonparametric tests. For each hypothesis, we perform a Kruskal-Wallis-Test. In case of significant results, we proceed with a Wilcoxon-Mann-Whitney-Test. As multiple tests are performed, we carry out a Bonferroni correction. After that, the level of significance is set at 0.017. Effect sizes are calculated with Cliff\u2019s Delta [Cl93]. We set the threshold for a small effect at .11 and at .28 for a medium effect, as in [VDV00]."
        },
        {
            "heading": "4 Results",
            "text": ""
        },
        {
            "heading": "4.1 H1 \u2013 Incorrect answers",
            "text": "With regards to the relative number of incorrect answers we found significantly fewer mistakes in all interventions compared to the control group. Here, interventions 1 and 2\nresult in an effect size of 0.11 and 0.12, while intervention 3 produces an effect size of 0.09 and is therefore negligible (Tab. 4). The mean percentage of errors in the control group is 16.98% and which is higher as the mean percentage of errors in all other groups (Intv 1: 13.91%, Intv 2: 14.32%, Intv 3: 14.77%).\nTab. 4: Results of H1- incorrect answers. Left: p-value, significant results in bold font. Right: ef-\nfect sizes, results with small effect in bold font; medium effects in italic font.\nInterestingly, the results for intervention group 3 differ for boys and girls. While boys only have a median of relative mistakes of 16.05% (effect size: 0.07), for girls it results in 13.48% (effect size: 0.12). Hence, girls appear to benefit most from the display of the rule, while for boys, this is the least effective intervention."
        },
        {
            "heading": "4.2 H2 \u2013 Dropout",
            "text": "An analysis of the number of dropouts (H2) showed significantly higher dropouts in comparison to the control group in all intervention groups. The highest mean number of dropouts per user was found in intervention 2 with 15.51% (control group: 12.33%). Intervention 1 had a mean number of dropouts of 14.06%; intervention 3 of 14.25%. However, the effect sizes are negligible.\nTab. 5: Results of H2- Number of dropouts. Left: p-value, significant results in bold font. Right:\neffect sizes, results with small effect in bold font; medium effects in italic font."
        },
        {
            "heading": "4.3 H3 \u2013 User competency",
            "text": "The last hypothesis H3 compared the mean competency that is calculated by the Rasch model. As seen in Tab. 6 differences between groups are not significant.\np-value Control Intv 1 Intv 2 Effect\nsizes\nControl Intv 1 Intv 2\nIntv 1 3.99e-12 Intv 1 0.1237\nIntv 2 9.43e-12 0.8788 Intv 2 0.1198 -0.0029\nIntv 3 2.91e-08 0.1846 0.2588 Intv 3 0.0978 -0.0250 -0.0210\np-value Control Intv 1 Intv 2 Effect\nsizes\nControl Intv 1 Intv 2\nIntv 1 0.0027 Intv 1 -0.0173\nIntv 2 4.70e-08 0.0251 Intv 2 -0.0317 -0.0144\nIntv 3 0.0009 0.7790 0.0491 Intv 3 -0.0191 -0.0018 -0.0126\nTab. 6: Results of H3: User competency. Left: p-value, significant results in bold font. Right: ef-\nfect sizes, results with small effect in bold font; medium effects in italic font."
        },
        {
            "heading": "5 Discussion",
            "text": "In our article, we present the results of an online-controlled experiment that compares a traditional online learning platform for German spelling skills to three adaptive versions of the platform implementing student-facing interventions. We evaluate the experiment results concerning the error rate, the number of dropouts, and the users\u2019 competency.\nWe found that error rates decreased significantly for all users in the intervention groups in comparison to the users that were in the control group. Hence, the student-facing interventions had positive effects on that metric. Users in intervention 3 received the spelling rule as a hint if the solving probability is below 50%. Here, we found that this is particularly effective for girls, but the least effective intervention for boys. One could interpret that it is more likely for girls to read the spelling rule carefully and use it to solve the exercise while boys are not using the additional information that is displayed.\nConcerning the dropout rate, we found that dropouts increased in all intervention groups in comparison to the control group. This could mean, that users get demotivated when they receive low solving probabilities and thus leave the session. Furthermore, leaving a session after receiving a very good prediction could have two reasons: first, users might get bored or think that they don\u2019t need to practice anymore. Secondly, users could become demotivated if they receive a very high prediction score and still fail. This also goes hand in hand with the theoretical foundations of motivation theory. In flow theory, users are most persistent when the tasks are neither too easy nor too difficult. It should be discussed if dropping out can only be seen as a negative consequence. It is clear that a dropout can be out of frustration or demotivation. However, there are many other reasons why a user does not to finish the exercise set that are neither positive nor negative. This could be because the school lesson is over, or homework is interrupted for private reasons. Furthermore, a dropout can be a positive effect of student-facing interventions. For example, if the student feels that it is too difficult and the student-facing intervention shows him or her the same, then the student\u2019s emotions are validated, and one could decide to do some simpler sets first or to take a break. From a pedagogical point of view, that would be a positive effect of student-facing interventions that leads to a dropout.\np-value Control Intv 1 Intv 2 Effect\nsizes\nControl Intv 1 Intv 2\nIntv 1 0.4901 Intv 1 0.0124\nIntv 2 0.3242 0.8124 Intv 2 0.0175 0.0045\nIntv 3 0.0569 0.2761 0.3751 Intv 3 0.0340 0.0208 0.0167\nWe did not find any significant intervention groups with regard to the users\u2019 competence. As spelling competence does not develop quickly, it seems that the experimental period was not long enough to find significant differences. Furthermore, we did not do a pre-and post-test of the users to detect changes in time. Instead, we used all exercises that were solved during the experimental period to calculate the person\u2019s ability estimates."
        },
        {
            "heading": "6 Conclusion",
            "text": "Our article presents the results of an online-controlled experiment that was carried out on an online platform for the acquisition of German spelling skills from June to October 2022. For that, we implemented a machine learning-based prediction model that calculated the personalized solving probability for a user and an exercise at hand. We further implemented three different student-facing interventions that all used the prediction results. Our results showed that all three interventions led to a decreasing error rate for the users in comparison to the control group. As dropout numbers increased, we discussed the meaning of a dropout and found a connection between dropouts and the prediction received by the user. The calculation of the users\u2019 competencies did not show significant results. In summary, we found that student-facing machine learning-based interventions lead to fewer errors in German spelling learning environments. However, it can also demotivate users leading to more dropouts. An experimental period of four months is not enough to conclude the long-term development of competencies.\nOur study comes with limitations that are discussed in the following. First, our experimental period did not cover a whole school year, but only four months. Moreover, it was conducted during summertime, including the German summer school break. Furthermore, concerning the competency calculation, we did not conduct pre-and post-tests to compare the users\u2019 developments.\nFurther research should therefore include standardized pre- and post-test to be able to conclude the users\u2019 competency. Additionally, one could also focus on more orthographical areas and other languages. Especially a comparison of the effects on first-language and second-language learning would be of interest."
        }
    ],
    "title": "Show me the numbers! - Student-facing Interventions in Adaptive Learning Environments for German Spelling",
    "year": 2023
}