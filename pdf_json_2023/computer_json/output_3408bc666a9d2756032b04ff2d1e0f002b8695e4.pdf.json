{
    "abstractText": "In recent years, significant progress has been made in visual-linguistic multi-modality research, leading to advancements in visual comprehension and its applications in computer vision tasks. One fundamental task in visual-linguistic understanding is image captioning, which involves generating human-understandable textual descriptions given an input image. This paper introduces a referring expression image captioning model that incorporates the supervision of interesting objects. Our model utilizes user-specified object keywords as a prefix to generate specific captions that are relevant to the target object. The model consists of three modules including: (i) visual grounding, (ii) referring object selection, and (iii) image captioning modules. To evaluate its performance, we conducted experiments on the RefCOCO and COCO captioning datasets. The experimental results demonstrate that our proposed method effectively generates meaningful captions aligned with users\u2019 specific interests.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seokmok Park"
        },
        {
            "affiliations": [],
            "name": "Joonki Paik"
        }
    ],
    "id": "SP:243a3795609195ba220613cfbcd62f9a6e80c3d8",
    "references": [
        {
            "authors": [
                "A Radford"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "J. Johnson",
                "A. Karpathy",
                "L. Fei-Fei"
            ],
            "title": "Densecap: Fully convolutional localization networks for dense captioning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Lin",
                "T.-Y",
                "A. RoyChowdhury",
                "S. Maji"
            ],
            "title": "Bilinear convolutional neural networks for fine-grained visual recognition",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2017
        },
        {
            "authors": [
                "A Fukui"
            ],
            "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
            "venue": "arXiv preprint arXiv: 1606",
            "year": 2016
        },
        {
            "authors": [
                "M. Hodosh",
                "P. Young",
                "J. Hockenmaier"
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation",
            "venue": "metrics. J. Artif. Intell. Res",
            "year": 2013
        },
        {
            "authors": [
                "Z Gan"
            ],
            "title": "Semantic compositional networks for visual captioning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Tu",
                "C. Zhou",
                "J. Guo",
                "S. Gao",
                "Z. Yu"
            ],
            "title": "Enhancing the alignment between target words and corresponding frames for video captioning",
            "venue": "Pattern Recogn",
            "year": 2021
        },
        {
            "authors": [
                "Y. Tu",
                "L. Li",
                "C. Yan",
                "S. Gao",
                "Z. Yu"
            ],
            "title": "R3Net:relation-embedded representation reconstruction network for change captioning",
            "venue": "arXiv preprint arXiv: 2110",
            "year": 2021
        },
        {
            "authors": [
                "K Xu"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2057
        },
        {
            "authors": [
                "J. Lu",
                "C. Xiong",
                "D. Parikh",
                "R. Socher"
            ],
            "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "J. Lu",
                "J. Yang",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Hierarchical question-image co-attention for visual question answering",
            "venue": "Adv. Neural Inf. Process",
            "year": 2016
        },
        {
            "authors": [
                "A.H.B. Ron Mokady",
                "Amir Hertz"
            ],
            "title": "Clipcap: Clip prefix for image captioning",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "R. Ramos",
                "B. Martins",
                "D. Elliott",
                "Y. Kementchedjhieva"
            ],
            "title": "Smallcap: lightweight image captioning prompted with retrieval augmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "S. Kazemzadeh",
                "V. Ordonez",
                "M. Matten",
                "T. Berg"
            ],
            "title": "Referitgame: Referring to objects in photographs of natural scenes",
            "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2014
        },
        {
            "authors": [
                "J Mao"
            ],
            "title": "Generation and comprehension of unambiguous object descriptions",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "R. Hu",
                "A. Rohrbach",
                "T. Darrell",
                "K. Saenko"
            ],
            "title": "Language-conditioned graph networks for relational reasoning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "R Krishna"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "Int. J. Comput. Vis. 123,",
            "year": 2017
        },
        {
            "authors": [
                "X. Xiao",
                "Z. Sun",
                "T. Li",
                "Y. Yu"
            ],
            "title": "Relational graph reasoning transformer for image captioning",
            "venue": "IEEE International Conference on Multimedia and Expo (ICME),",
            "year": 2022
        },
        {
            "authors": [
                "Y. Cong",
                "M.Y. Yang",
                "B. Rosenhahn"
            ],
            "title": "Reltr: Relation transformer for scene graph generation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2023
        },
        {
            "authors": [
                "X Yang"
            ],
            "title": "Transforming visual scene graphs to image captions",
            "venue": "arXiv preprint arXiv: 2305",
            "year": 2023
        },
        {
            "authors": [
                "H Rezatofighi"
            ],
            "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "A Vaswani"
            ],
            "title": "Attention is all you need",
            "venue": "Adv. Neural Inf. Process",
            "year": 2017
        },
        {
            "authors": [
                "S.J. Rennie",
                "E. Marcheret",
                "Y. Mroueh",
                "J. Ross",
                "V. Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks. Adv",
            "venue": "Neural Inf. Process",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Niu",
                "Chang",
                "S.-F"
            ],
            "title": "Grounding referring expressions in images by variational context",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "L Yu"
            ],
            "title": "Mattnet: Modular attention network for referring expression comprehension",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "X Chen"
            ],
            "title": "Real-time referring expression comprehension by single-stage grounding network",
            "venue": "arXiv preprint arXiv: 1812",
            "year": 2018
        },
        {
            "authors": [
                "Y Liao"
            ],
            "title": "A real-time cross-modality correlation filtering method for referring expression comprehension",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "L. Yu",
                "P. Poirson",
                "S. Yang",
                "A.C. Berg",
                "T.L. Berg"
            ],
            "title": "Modeling context in referring expressions",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "M. Cornia",
                "L. Baraldi",
                "R. Cucchiara"
            ],
            "title": "Show, control and tell: A framework for generating controllable and grounded captions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "J. Yang",
                "J. Lu",
                "S. Lee",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Graph r-cnn for scene graph generation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "Zhu",
                "W.-J"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "S. Banerjee",
                "A. Lavie"
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,",
            "year": 2005
        },
        {
            "authors": [
                "Lin",
                "C.-Y"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text Summarization Branches Out,",
            "year": 2004
        },
        {
            "authors": [
                "R. Vedantam",
                "C. Lawrence Zitnick",
                "D. Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "P Anderson"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6\nwww.nature.com/scientificreports"
        },
        {
            "heading": "RefCap: image captioning",
            "text": "with referent objects attributes Seokmok Park 1 & Joonki Paik 1,2*\nIn recent years, significant progress has been made in visual-linguistic multi-modality research, leading to advancements in visual comprehension and its applications in computer vision tasks. One fundamental task in visual-linguistic understanding is image captioning, which involves generating human-understandable textual descriptions given an input image. This paper introduces a referring expression image captioning model that incorporates the supervision of interesting objects. Our model utilizes user-specified object keywords as a prefix to generate specific captions that are relevant to the target object. The model consists of three modules including: (i) visual grounding, (ii) referring object selection, and (iii) image captioning modules. To evaluate its performance, we conducted experiments on the RefCOCO and COCO captioning datasets. The experimental results demonstrate that our proposed method effectively generates meaningful captions aligned with users\u2019 specific interests.\nVisual understanding, which is analogous to human visual perception, is a major research topic in computer vision research. An essential aspect of this understanding involves resolving the intricate relationships between visual information and textual associations. Image captioning is a fundamental task in visual understanding, where human-comprehensible textual information is derived from analyzing visual data. Current image captioning research primarily focuses on attention mechanisms, commonly employing region proposal networks to obtain region features for attention modeling. In addition, the field of visual-linguistic multi-modality has exhibited significant advancements in image captioning, with notable contributions such as Contrastive Language-Image Pre-Training (CLIP)1. CLIP is designed to learn contrastive loss from extensive datasets, understanding visual features and textual descriptions, thereby establishing meaningful and correlated visual-text representations.\nDense captioning, a subcategory of image captioning, predicts diverse captions from a given input image, instead of being limited to specific caption outcomes2,3. By selecting various objects in the image, dense captioning can capture aspects of multiple situations. However, even though there are many dense captions available, the difficulty lies in choosing truly meaningful captions. In this paper, we introduce a novel approach called RefCap (Image Captioning with Referent Objects Attributes) for generating meaningful captions that align with user preferences in referring expression image captioning. This approach enhances visual understanding by incorporating object relation descriptors. In the proposed RefCap model, after a region proposal network detects various objects, our approach selectively focuses on the related objects based on user input prompts. This enables RefCap to predict specific caption outcomes that correspond to the objects and their referring expressions provided by the user, thus providing a more targeted caption generation instead of a range of possible outcomes.\nVisual grounding (VG), also referred to as referring expression comprehension, is an intriguing research area in the field of computer vision4. VG methods aim to identify objects in an image that correspond to a user\u2019s query. For example, if the user inputs a query such as \u201cthe man on the right,\u201d the visual grounding module identifies and highlights the specified referent, and then the captioning module generates the corresponding expression. In our RefCap model, we utilize the localized objects obtained from the VG task\u2019s results and establish their relationships.\nFigure \u00a02 illustrates the pipeline of our RefCap model, which combines four main computer vision algorithms: object detection, visual grounding, scene graph generation, and image caption generation. A description of each pipeline network\u2019s role is provided in the \u201cProposed Method\u201d section. Compared to other tasks of visual understanding, image captioning, and dense captioning, our RefCap model has the following differences, as shown in Figure\u00a01. The image captioning task derives the most descriptive single textual information about a given image. Compared to image captioning, the dense captioning task can express more diverse information in the image.\nOPEN\n1Department of Image, Chung-Ang University, 84 Heukseok-ro, Seoul 06974, Republic of South Korea. 2Department of Artificial Intelligence, Chung-Ang University, 84 Heukseok-ro, Seoul 06974, Republic of South Korea. *email: paikj@cau.ac.kr\n2 Vol:.(1234567890) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6\nHowever our RefCap model, the user first selects the object of interest in a given image and then derives a corresponding representation of that object. The RefCap model has the following steps:\n1. The word-level encoder encodes the user prompt, which is then passed to the Transformer along with the output of the object detection task. 2. The VG task provides localization information about the target query, which is then used to construct objectlevel relations with a given object.\nFigure\u00a01. Comparison between RefCap and other approaches: (a)\u00a0image captioning, (b)\u00a0dense captioning, and (c)\u00a0RefCap. The given image is the sampled from the COCO2014 train dataset. The # means the image index of the dataset.\nFigure\u00a02. The RefCap model generates a caption for the object specified by the user prefix in the same image as shown in Figure \u00a01.\n3 Vol.:(0123456789) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6\n3. Finally, we perform image captioning (IC) using the constructed relations to derive textual information about the object relationships.\nCompared to other visual understanding tasks, such as image captioning and dense captioning, our RefCap model has the following key differences:\n\u2022 Image captioning generates a single textual description of a given image. \u2022 Dense captioning generates multiple textual descriptions of different objects in a given image. \u2022 RefCap generates a textual description of a user-specified object in a given image."
        },
        {
            "heading": "Related work",
            "text": "In the past few years, the field of image captioning and visual-linguistic multi-modality research has exhibited significant advancement. In this section, we briefly review related literature for image captioning, visual-linguistic model, and referring expression with discussion about the limitations of existing approaches."
        },
        {
            "heading": "Image captioning",
            "text": "In the early stages of image captioning, approaches primarily relied on pre-defined caption templates or matching image regions to textual descriptions. Hodosh\u00a0et al. introduced methods such as template matching and retrieval-based image captioning5. Gan et\u00a0al. used the object detection results as input to the LSTM to generate the caption6. However, these methods faced limitations in terms of flexibility and the ability to generate diverse, contextually relevant captions.\nWith the advent of deep learning, the focus shifted towards using neural networks for image captioning. A representative work is the Show and Tell model by Vinyals\u00a0et al. , which introduced an encoder-decoder framework using a convolutional neural network (CNN) as an image encoder and a recurrent neural network (RNN) as a caption generator7. This approach significantly improved the quality of generated captions by learning rich visual representations and capturing temporal dependencies in language.\nRecently, researchers have also studied temporal information-based caption generation approaches. For example, Tu et\u00a0al. investigated how to derive a representative caption that expresses a video\u2019s temporal causal relationship and how to find changes in multiple images8,9."
        },
        {
            "heading": "Visual-linguistic models",
            "text": "To enhance the visual-linguistic understanding, recent research efforts have explored incorporating attention mechanisms into image captioning models. Xu et\u00a0al. proposed the attention mechanism, allowing the model to selectively focus on different image regions while generating captions10. Similarly, Lu et\u00a0al. 11 introduced the adaptive attention model, which dynamically adjusted the attention weights based on the input image and generated caption. These attention-based models achieved better alignment between the generated words and visual content, leading to improved caption quality11,12.\nSome studies integrate the extensive prior knowledge of visual-linguistic models, such as ClipCap13 and SMALLCAP14, which combine CLIP and GPT-2 with vast amounts of pre-trained models to present lightweighttraining models with only minimal fine-tuning for image captioning."
        },
        {
            "heading": "Referring expression generation",
            "text": "In the field of visual understanding research, the generation of referring expressions has gained increasing attention. In this context, Kazemzadeh et\u00a0al. presented the RefCOCO dataset, which contains referring expressions for objects in images15. Mao et\u00a0al. proposed a language-guided attention model to generate referring expressions, which makes the model attend to relevant image regions described in the expression16. More recently, referring expressions image captioning has become a crucial research topic in the visual-linguistic multi-modality research field. Hu et\u00a0al. proposed an end-to-end referring expression image captioning model that incorporated explicit supervision of referred objects17. In their model, user-specified object keywords are used as a prefix to generate specific captions focused on the target object to improve alignment between the referred object and the generated captions.\nVisual scene graph generation The goal of the visual scene graph generation (SGG) task is to detect the relationships between objects. The representative dataset is Visual Genome which is presented by Ranjay et\u00a0al. 18. The Visual Genome dataset provides localized bounding boxes for objects, along with their attributes such as color, size, location, and more. Furthermore, objects are cross-connected through the relationships. Understanding contextual relationships between objects in the image and its corresponding language is useful for downstream visual-linguistic understanding tasks. In the SGG task, the encoder-decoder method is generally utilized. Xiao et\u00a0al. generated the graph using both spatial and semantic attention modules and its fusion19. Cong et\u00a0al. proposed a one-stage approach that can predict the relations directly using a triplet decoder, and they also provide abundant demonstrations20.\nThere is also a study that has attempted to image captioning through the SGG task. Yang et\u00a0al. 21 presented a method for image captioning through the SGG task using a homogeneous network to convert a scene graph into a caption21. They used a scene graph to represent objects in the image and generated a caption via an encoderdecoder structure.\n4 Vol:.(1234567890) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6"
        },
        {
            "heading": "Proposed method",
            "text": "In this section, we present an image captioning model called RefCap using referent object relationships. As shown in Figure\u00a02, RefCap requires user prompts to initiate the captioning process. Subsequently, it employs Visual Selection (VS) and Image Captioning (IC) tasks to derive a textual description of the selected object and its corresponding referent. To gain a better understanding of RefCap\u2019s functionality, we provide a detailed explanation in the following subsections."
        },
        {
            "heading": "Visual grounding",
            "text": "For the visual grounding task, both visual and linguistic features are used to compute embedding vectors as input. Given an image as input, the visual branch is composed of a stack of 6-encoder layers. Each encoder layer of the visual branch includes a multi-head self-attention layer and feed-forward network (FFN). Positional encoding is then added at each encoder layer. Meanwhile, the linguistic branch utilizes a 12-encoder layer with a pre-trained BERT model. A [CLS] token is appended at the beginning, and a [SEP] token is appended at the end of each token. Subsequently, each token is used as input for the linguistic transformer. To merge these visual-linguistic tokens, a linear projection is applied to them with the same dimension, and a learnable regression token [REG] is added for bounding box prediction. The visual-linguistic fusion tokens are then fed into a visual-linguistic transformer with six encoder layers. To configure a loss function, we sum the differences between predicted and ground-truth boxes across all stages to calculate the total loss.\nwhere \u03b11 and \u03b1g are hyper-parameters. L1 is the \u21131 loss and Lgiou is the generalized intersection over union (GIoU) loss proposed by Rezatofighi et\u00a0al. 22. As a result, the bounding box corresponding to the user prefix is predicted."
        },
        {
            "heading": "Referent objects selection",
            "text": "Following the Visual Grounding (VG) task, our model establishes relationships between referent objects. To construct these relationships, we require additional object locations. By utilizing a CNN-based object detection algorithm, we can easily obtain localized objects. The image features are then linearized to form the input vector, and a subset of these features is fed into the transformer encoder layer.\nFor every possible relationship between the objects, we compute the probability of their relationships. Considering that there are m(m\u2212 1) potential relationships among m objects, we generate m(m\u2212 1) pairwise features. Thus, the relationships between the i-th subject and the j-th object can be denoted as Ri\u2192j \u2208 R , where i = j . These relationships form a directed graph represented by a collection of subject-predicate-object triplets.\nThe triplet prediction can be expressed as:\nwhere each subject and object contain class and bounding box labels denoted as y\u0302 =< c\u0302, b\u0302 > . Using the ground truth triplet R =< ysub, cpred, yobj > , the triplet cost is computed using the cost function cm introduced in RelTR20.\nGiven the triplet cost ctri , we can get the triple loss Ltri as:\nwhere L is the cross-entropy loss between the predicted class and the ground truth class. As following steps from RelTR, we can get pruned relationships such as <Obj-relation-BG> and <Obj-no relation-Obj>. These dense object relationships obtain abundant descriptions between subject-object relations. However, unnecessary relations still follow. Therefore we need to prune some edges for remaining meaningful relations. For remaining referent objects relationships, we apply some criteria:\n1. The initial object yinit from the VG task is root node. 2. The duplicated relationship must not be included. 3. The subject which is predicted from subject yinit or yobj as object previous step, can have another relationship. 4. The object which is predicted from object yinit or ysub as subject previous step, can have another relationship. 5. Both subject and object can each have multiple relationships, unless the above paragraph is contradicted.\nFinally, we perform image captioning (IC) using the constructed relations to derive textual information about the object relationships."
        },
        {
            "heading": "Image captioning",
            "text": "The final step of our model is to generate the target caption result by incorporating the structure derived from the selected referent objects. Before entering the caption step, we concatenate the output of SGG and VG tasks. However predicted triplet embeddings R\u0302 =< y\u0302sub, c\u0302pred , y\u0302obj > and visual features contain different information and lengths. Therefore, we need to unify both triplet embeddings and visual features as the same dimension features. These concatenated features enter the encoder of the transformer network23. The Encoder layer is composed of a stack of 6-encoder layers, and each encoder layer includes a multi-head self-attention layer and FFN similar to our VG task. As following the transformer23, the attention can be calculated by:\n(1)L = \u03b11L1 + \u03b1gLgiou,\n(2)R\u0302 =< y\u0302sub, c\u0302pred, y\u0302obj >,\n(3)ctri = cm(y\u0302sub, ysub)+ cm(c\u0302pred , cpred)+ cm(y\u0302obj , yobj).\n(4)Ltri = Lsub + Lobj + Lpred ,\n5 Vol.:(0123456789) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6\nwhere Q,K,V is query, key, value of attention module, dK is dimension of model. And the multi-headed attention is also calculated by:\nwhere hi = Attention(QWqi ,KWki ,VWvi ) and the projections W is parameter matrices of each head. The encoded features are passed to the linguistic decoder for captioning. The linguistic decoder is also composed of a stack of 6-decoder layers. By decoding the features, we finally obtain a caption that corresponds to the user input object.\nThe objective function for image captioning consists of two terms: cross-entropy loss and Self-Critical Sequence Training (SCST) loss24. The cross-entropy loss is defined as follows:\nwhere y\u2217t is the ground-truth target for all sequences, and \u03b8 is the model parameters. The SCST loss minimizes the negative expectation of the CIDEr score, which is a metric for evaluating the quality of image captions:\nwhere r denotes the CIDEr fraction function. The gradient of the SCST loss can be approximated as:\nwhere r(y1:T ) is the CIDEr score of the sampled caption and r(y\u03021:T ) is the CIDEr score of the greedy decoding of the model."
        },
        {
            "heading": "Experimental results",
            "text": ""
        },
        {
            "heading": "Implementation details",
            "text": "In our experimental results, we present a comprehensive evaluation of each module, incorporating both quantitative and qualitative assessments. Our RefCap model consists of four main modules, namely:\n(i) Object detection: This module enables the system to search for relevant visual content based on user queries.\n(ii) Visual grounding: The visual grounding module aims to establish a connection between textual queries and specific objects or regions in the visual content.\n(iii) Scene graph generation: This module generates a structured representation of the relationships between objects in the scene, capturing their interactions and contextual information. (iv) Image captioning: The image captioning module generates descriptive captions that accurately convey the content and context of the visual input.\nEach of these modules plays a crucial role in our RefCap model, and we provide detailed descriptions and evaluations for each module in the following sections."
        },
        {
            "heading": "Object detection",
            "text": "For object detection, we utilize a Faster R-CNN model25 pretrained on the ImageNet dataset, with ResNet-10126 as the backbone architecture. This model is then fine-tuned on the Visual Genome dataset to perform visual grounding and scene graph generation tasks.\nTo reduce the dimensionality of the object features, which initially yield a 2048-dimensional feature vector, we apply dimensionality reduction to obtain a dimension of dK = 512 . This reduction is followed by a ReLU activation and a dropout layer to enhance the model\u2019s performance. During training, we employ the SGD optimizer with an initial learning rate of 1\u00d7 10\u22122."
        },
        {
            "heading": "Visual grounding",
            "text": "To evaluate the Visual Grounding task, we conduct experiments on two datasets: ReferItGame15 and RefCOCO31. These datasets consist of images that contain objects referred to in the referring expressions. Each object may have one or multiple referring expressions associated with it.\nWe split each dataset into three subsets: a 70% training set, a 20% test set, and a 10% validation set. The input image size is standardized to 640\u00d7 640 , and the maximum expression length is set to 10 tokens, including the [CLS] and [SEP] tokens. The shorter maximum expression length is chosen because the inference process only requires the keywords related to the target object.\nThese evaluation setups allow us to assess the performance of the Visual Grounding task on different datasets and validate the effectiveness of our approach.\n(5)Attention(Q,K,V) = softmax ( QKT\u221a dK ) V,\n(6)Multihead(Q,K,V) = Concat(h1, h2, ..., hn)Wo,\n(7)LCE = \u2212 T \u2211\ni\nlog(p\u03b8 (t \u2217 t |y\u22171 , ..., y\u2217t\u22121)),\n(8)LSCST = \u2212Ey1:T\u223cp\u03b8 [r(y1:T )],\n(9)\u2207\u03b8LSCST \u2248 \u2212(r(y1:T )\u2212 r(y\u03021:T ))\u2207\u03b8 log p\u03b8 (y1:T ),\n6 Vol:.(1234567890) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6"
        },
        {
            "heading": "Scene graph generation",
            "text": "The Visual Genome dataset is used for evaluating Scene Graph Generation. The Visual Genome dataset consists of 108K images with 34K object categories, 68K attribute categories, and 42K relationship categories. We select the most frequent 150 object categories and 50 relationship categories. The attribute categories are omitted by merging with relationship categories. Thus, each image has object and relationship (with attributes) categories in the scene graph. During inference, the criteria are applied aforementioned in the referent object selection section for pruning the unrelated relationships."
        },
        {
            "heading": "Image captioning",
            "text": "For the image captioning task, the commonly used COCO Entities dataset32 was used. The dataset contains diverse caption annotations with an abundant combination of objects and their bounding boxes. Thus employing these datasets by RefCap which builds a sub-graph makes sense."
        },
        {
            "heading": "Quantitative evaluation",
            "text": "We first evaluate the visual grounding task of RefCap on the ReferItGame15, RefCOCO31, and RefCOCO+31 datasets, comparing it to other state-of-the-art methods including Maximum Mutual Information (MMI)16, Variational Context (VC)27, Modular Attention Network (MAttNet)28, Single-Stage Grounding (SSG)29, and Real-time Cross-modality Correlation Filtering (RCCF)30. Note that the accuracy of the RefCOCO and RefCOCO+ datasets is based on TestA only. Table\u00a01 shows that our RefCap model is competitive with other state-of-the-art methods.\nWe also evaluate our visual scene graph generation of RefCap with grounded objects on the Visual Genome dataset. Our desired output of the scene graph is the sub-graph of the entire graph. This means the result doesn\u2019t need to include the entire relationship. Thus we aim that how the sub-graph represents well about target object and the output includes ground truth. We adopt the four metrics (PredCls, PhrCls, SGGen, SGGen+) for evaluating our scene graph generation which is presented by Yang et\u00a0al. 33 with our insight. We modified the ground truth data to reference the target object. Each image contains multiple target objects and its referent relationship. Thus we just compare the relation to the target object with modified ground truth. The performance of generating sub-graph by RefCap is shown in Table\u00a02.\nWe finally evaluate our image captioning task. We employ conventional metrics (BLEU34, METEOR35, ROUGE36, and CIDEr37) to measure the quality of the predicted captions on the COCO Entities dataset. Table\u00a03 shows the results of evaluating the predicted caption on COCO Entities."
        },
        {
            "heading": "Qualitative evaluation",
            "text": "Figure\u00a03. shows the examples of our entire RefCap model. The few keywords are typed as input by the user, RefCap detects the corresponding object, builds a relationship with related objects, and draws its caption result. Unlike traditional caption methods, RefCap shows the caption results for the user\u2019s desired target. Our RefCap can provide caption results, not only in images with a single object but also in images with multiple objects.\n7 Vol.:(0123456789) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6"
        },
        {
            "heading": "Ablation study",
            "text": "In this section, we analyze the impact of each hyperparameter on the model for each module. First, we explored the effect of the prefix length on the visual grounding (VG) task. As summarized in Fig.\u00a04, the performance tends to improve as the prefix length increases. However, continually increasing the prefix length slows down processing due to the increased parameters of the model. Therefore, RefCap uses a prefix length of 15, which balances performance and processing time.\nModels BLEU-4 METEOR ROUGE CIDEr\nSCST24 25.3 25.7 50.1 131.4\nUp-Down38 25.5 26.8 53.2 137.1\nClipCap13 33.5 30.4 \u2013 124.1\nGRIT39 38.2 30.3 55.7 142.9\nRefCap 33.2 29.7 56.2 143.7\nFigure\u00a03. Some examples of RefCap model. The given images are selected from the COCO2014 test dataset. The # means the image index of the dataset. Incorrect caption results are highlighted in red.\n1 5 10 15 20 25 30\n4\n10\n16\n22\n28\n34\nPrefix length\nB L E U -4\ntest\ntrain\nFigure\u00a04. Effect of prefix length on the image captioning performance of RefCap.\n8 Vol:.(1234567890) Scientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6\nIn a separate experiment, we examined how the scene graph generator (SGG) affects the performance of caption generation. Table\u00a04 shows the accuracy of different combinations of subject, predicate, and object. The results indicate that using all three components achieved the best outcome. As you can see, the combination of object and predicate is superior to subject and object alone, because it is difficult to represent the target\u2019s properties with class alone."
        },
        {
            "heading": "Research plan",
            "text": "In this paper, we introduce a novel image captioning model, RefCap, which leverages referent object attributes to generate more specific and tailored captions. However, our model has several limitations. First, it consists of a combination of several pipeline networks, which makes it complex and sensitive to the performance of each individual network. To address this, we plan to develop an end-to-end model in our future work. We look forward to sharing our progress in future publications."
        },
        {
            "heading": "Discussion and conclusion",
            "text": "The main idea of the paper is to predict a meaningful caption from a selected user prefix. By exploring object relationships for image captioning, our method can more accurately and concretely predict the caption results. As a result, the user of our method can get the more satisfying result that corresponds to his prefix. We also demonstrated quantitative evaluation and qualitative evaluation. As a quantitative evaluation, we experiment with various datasets for each module. Both quantitative and qualitative evaluations yielded gratifying results. Moreover, our RefCap can provide multiple caption results from a single image based on user input. We hope the utilization of this convergence of the object detection and image captioning tasks, would provide insight into the future of computer vision and multimodality research."
        },
        {
            "heading": "Data availability",
            "text": "All data generated or analyzed in this study are included in this published article. The training and testing datasets used in this study are publicly available and have been cited in accordance with research rules. Detailed descriptions of the datasets and their citations can be found in the \u201cExperimental results\u201d section of the paper. For instance, the ReferItGame, RefCOCO, and RefCOCO+ dataset\u2019s training set can be downloaded from https:// github. com/ liche ngunc/ refer. Furthermore, The COCO2014 dataset and Visual Genome dataset\u2019s training set can be accessed via https:// cocod ataset. org, https:// homes. cs. washi ngton. edu/ \u00a0ranjay/ visua lgeno me/ index. html, respectively. The testing set of the COCO Entities dataset can be downloaded from https:// github. com/ aimag elab/ show- contr ol- and- tell, respectively.\nReceived: 13 July 2023; Accepted: 1 December 2023"
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by Institute for Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) [No. 2014-0-00077, Development of global multi-target tracking and event prediction techniques based on real-time large-scale video analysis], and by Field-oriented Technology Development Project for Customs Administration through National Research Foundation (NRF) of Korea funded by the Ministry of Science & ICT and Korea Customs Service [2021M3I1A1097911], and by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (2021-0-01341, Artificial Intelligence Graduate School Program(Chung-Ang University))."
        },
        {
            "heading": "Author contributions",
            "text": "S.P. designed and developed the algorithm and performed the experiment. S.P. and J.P. prepared training data and analyzed the experiment. J.P. guided the project and wrote the original draft. All authors reviewed the manuscript."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Correspondence and requests for materials should be addressed to J.P.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n10\nVol:.(1234567890)\nScientific Reports | (2023) 13:21577 | https://doi.org/10.1038/s41598-023-48916-6\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2023"
        }
    ],
    "title": "RefCap: image captioning with referent objects attributes",
    "year": 2023
}