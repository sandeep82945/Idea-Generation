{
    "abstractText": "While numerous methods exist to solve classification problems within curated datasets, these solutions often fall short in biomedical applications due to the biased or ambiguous nature of the data. These difficulties are particularly evident when inferring height reduction from vertebral data, a key component of the clinically-recognized Genant score. Although strategies such as semi-supervised learning, proposal usage, and class blending may provide some resolution, a clear and superior solution remains elusive. This paper introduces a flowchart of general strategy to address these issues. We demonstrate the application of this strategy by constructing a vertebral fracture dataset with over 300,000 annotations. This work facilitates the transition of the classification problem into clinically meaningful scores and enriches our understanding of vertebral height reduction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lars Schmarje"
        },
        {
            "affiliations": [],
            "name": "Vasco Grossmann"
        },
        {
            "affiliations": [],
            "name": "Claudius Zelenka"
        },
        {
            "affiliations": [],
            "name": "Reinhard Koch"
        }
    ],
    "id": "SP:ba8eabcf8fc67368241b5401476bfd64a52a92ba",
    "references": [
        {
            "authors": [
                "V. Basile",
                "M. Fell",
                "T. Fornaciari",
                "D. Hovy",
                "S. Paun",
                "B. Plank",
                "M. Poesio",
                "A. Uma"
            ],
            "title": "We Need to Consider Disagreement in Evaluation",
            "venue": "BPPF",
            "year": 2021
        },
        {
            "authors": [
                "D. Berthelot",
                "N. Carlini",
                "I. Goodfellow",
                "N. Papernot",
                "A. Oliver",
                "C.A. Raffel"
            ],
            "title": "Mixmatch: A holistic approach to semi-supervised learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 1905
        },
        {
            "authors": [
                "L. Beyer",
                "O.J. H\u00e9naff",
                "A. Kolesnikov",
                "X. Zhai",
                "A. van den Oord"
            ],
            "title": "Are we done with ImageNet",
            "venue": "arXiv preprint arXiv:2006.07159 (jun 2020),",
            "year": 2006
        },
        {
            "authors": [
                "J. Br\u00fcnger",
                "S. Dippel",
                "R. Koch",
                "C. Veit"
            ],
            "title": "Tailception\u2019: using neural networks for assessing tail lesions on pictures of pig carcasses",
            "venue": "Animal 13(5), 1030\u20131036",
            "year": 2019
        },
        {
            "authors": [
                "J.C. Chang",
                "S. Amershi",
                "E. Kamar"
            ],
            "title": "Revolt: Collaborative crowdsourcing for labeling machine learning datasets",
            "venue": "Conference on Human Factors in Computing Systems - Proceedings 2017-May, 2334\u20132346",
            "year": 2017
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "K. Swersky",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
            "venue": "Advances in Neural Information Processing Systems 33 pre-proceedings",
            "year": 2020
        },
        {
            "authors": [
                "A.M. Davani",
                "M. D\u0131\u0301az",
                "V. Prabhakaran"
            ],
            "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "M. Desmond",
                "E. Duesterwald",
                "K. Brimijoin",
                "M. Brachman",
                "Q. Pan"
            ],
            "title": "Semiautomated data labeling",
            "venue": "NeurIPS 2020 Competition and Demonstration Track. pp. 156\u2013169. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "M. Desmond",
                "M. Muller",
                "Z. Ashktorab",
                "C. Dugan",
                "E. Duesterwald",
                "K. Brimijoin",
                "C. Finegan-Dollak",
                "M. Brachman",
                "A. Sharma",
                "N.N. Joshi",
                "Q. Pan"
            ],
            "title": "Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface",
            "venue": "26th International Conference on Intelligent User Interfaces. pp. 392\u2013401. IUI \u201921, Association for Computing Machinery, New York, NY, USA",
            "year": 2021
        },
        {
            "authors": [
                "J.M. Durden",
                "B.J. Bett",
                "T. Schoening",
                "K.J. Morris",
                "T.W. Nattkemper",
                "H.A. Ruhl"
            ],
            "title": "Comparison of image annotation data generated by multiple investigators for benthic ecology",
            "venue": "Marine Ecology Progress Series 552, 61\u201370",
            "year": 2016
        },
        {
            "authors": [
                "H.K. Genant",
                "M. Jergas",
                "L. Palermo",
                "M. Nevitt",
                "R.S. Valentin",
                "D. Black",
                "S.R. Cummings"
            ],
            "title": "Comparison of semiquantitative visual and quantitative morphometric assessment of prevalent and incident vertebral fractures in osteoporosis",
            "venue": "Journal of Bone and Mineral Research 11(7), 984\u2013 996",
            "year": 1996
        },
        {
            "authors": [
                "R. Girdhar",
                "A. El-Nouby",
                "M. Singh",
                "K.V. Alwala",
                "A. Joulin",
                "I. Misra"
            ],
            "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos. arXiv preprint arXiv:2206.08356 (jun 2022)",
            "year": 2022
        },
        {
            "authors": [
                "V. Grossmann",
                "L. Schmarje",
                "R. Koch"
            ],
            "title": "Beyond Hard Labels: Investigating data label distributions. ICML 2022 Workshop DataPerf: Benchmarking Data for DataCentric AI (jul 2022)",
            "year": 2022
        },
        {
            "authors": [
                "A. Guermazi",
                "C. Tannoury",
                "A.J. Kompel",
                "A.M. Murakami",
                "A. Ducarouge",
                "A. Gillibert",
                "X. Li",
                "A. Tournier",
                "Y. Lahoud",
                "M. Jarraya",
                "E. Lacave",
                "H. Rahimi",
                "A. Pourchot",
                "R.L. Parisien",
                "A.C. Merritt",
                "D. Comeau",
                "N.E. Regnard",
                "D. Hayashi"
            ],
            "title": "Improving Radiographic Fracture Recognition Performance and Efficiency Using Artificial Intelligence",
            "venue": "Radiology 302(3), 627\u2013636",
            "year": 2022
        },
        {
            "authors": [
                "J. Haczynski",
                "A. Jakimiuk"
            ],
            "title": "Vertebral fractures: a hidden problem of osteoporosis",
            "venue": "Medical Science Monitor: International Medical Journal of Experimental and Clinical Research 7(5), 1108\u20131117",
            "year": 2001
        },
        {
            "authors": [
                "K. He",
                "X. Chen",
                "S. Xie",
                "Y. Li",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Masked Autoencoders Are Scalable Vision Learners (nov 2021)",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Identity Mappings in Deep Residual Networks",
            "venue": "Computer Vision \u2013 ECCV 2016. pp. 630\u2013645",
            "year": 2016
        },
        {
            "authors": [
                "V. Hemming",
                "M.A. Burgman",
                "A.M. Hanea",
                "M.F. McBride",
                "B.C. Wintle"
            ],
            "title": "A practical guide to structured expert elicitation using the IDEA protocol",
            "venue": "Methods in Ecology and Evolution 9(1), 169\u2013180",
            "year": 2018
        },
        {
            "authors": [
                "J.M. Jachimowicz",
                "S. Duncan",
                "E.U. Weber",
                "E.J. Johnson"
            ],
            "title": "When and why defaults influence decisions: A meta-analysis of default effects",
            "venue": "Behavioural Public Policy 3(2), 159\u2013186",
            "year": 2019
        },
        {
            "authors": [
                "M.H. Jarrahi",
                "A. Memariani",
                "S. Guha"
            ],
            "title": "The Principles of Data-Centric AI (DCAI) pp",
            "year": 2022
        },
        {
            "authors": [
                "J.A. Jiang",
                "M.K. Scheuerman",
                "C. Fiesler",
                "J.R. Brubaker"
            ],
            "title": "Understanding international perceptions of the severity of harmful content online",
            "venue": "PLOS ONE 16(8), e0256762",
            "year": 2021
        },
        {
            "authors": [
                "D. Karimi",
                "G. Nir",
                "L. Fazli",
                "P.C. Black",
                "L. Goldenberg",
                "S.E. Salcudean"
            ],
            "title": "Deep Learning-Based Gleason Grading of Prostate Cancer From Histopathology Images\u2014Role of Multiscale Decision Aggregation and Data Augmentation",
            "venue": "IEEE Journal of Biomedical and Health Informatics 24(5), 1413\u20131426",
            "year": 2020
        },
        {
            "authors": [
                "K.M. Knausg\u030aard",
                "A. Wiklund",
                "T.K. S\u00f8rdalen",
                "K.T. Halvorsen",
                "A.R. Kleiven",
                "L. Jiao",
                "M. Goodwin"
            ],
            "title": "Temperate fish detection and classification: a deep learning based approach",
            "venue": "Applied Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems. vol. 60, pp. 1097\u20131105. Association for Computing Machinery",
            "year": 2012
        },
        {
            "authors": [
                "U. Krothapalli",
                "A.L. Abbott"
            ],
            "title": "Adaptive label smoothing",
            "venue": "arXiv preprint arXiv:2009.06432",
            "year": 2020
        },
        {
            "authors": [
                "S. Kullback",
                "R.A. Leibler"
            ],
            "title": "On Information and Sufficiency",
            "venue": "Ann. Math. Statist. 22(1), 79\u201386",
            "year": 1951
        },
        {
            "authors": [
                "S. Laine",
                "T. Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "In: International Conference on Learning Representations (2017),",
            "year": 2017
        },
        {
            "authors": [
                "D.H. Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
            "venue": "Workshop on challenges in representation learning, ICML. vol. 3, p. 2",
            "year": 2013
        },
        {
            "authors": [
                "J. Li",
                "R. Socher",
                "Hoi"
            ],
            "title": "S.C.H.: DivideMix: Learning with Noisy Labels as Semisupervised Learning",
            "venue": "In: International Conference on Learning Representations. pp",
            "year": 2002
        },
        {
            "authors": [
                "Y.H. Liao",
                "A. Kar",
                "S. Fidler"
            ],
            "title": "Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets",
            "venue": "CVPR pp",
            "year": 2021
        },
        {
            "authors": [
                "H. Liu",
                "K. Son",
                "J. Yang",
                "C. Liu",
                "J. Gao",
                "Y.J. Lee",
                "C. Li"
            ],
            "title": "Learning Customized Visual Models with Retrieval-Augmented Knowledge (jan 2023)",
            "year": 2023
        },
        {
            "authors": [
                "L. Liu",
                "T. Zhou",
                "G. Long",
                "J. Jiang",
                "X. Dong",
                "C. Zhang"
            ],
            "title": "Isometric Propagation Network for Generalized Zero-shot Learning",
            "venue": "International Conference on Learning Representations (feb 2021),",
            "year": 2038
        },
        {
            "authors": [
                "Z.Y.C. Liu",
                "S. Roychowdhury",
                "S. Tarlow",
                "A. Nair",
                "S. Badhe",
                "T. Shah"
            ],
            "title": "AutoDC: Automated data-centric processing (NeurIPS)",
            "year": 2021
        },
        {
            "authors": [
                "M.T. L\u00f6ffler",
                "A. Sekuboyina",
                "A. Jacob",
                "A.L. Grau",
                "A. Scharr",
                "M.E. Husseini",
                "M. Kallweit",
                "C. Zimmer",
                "T. Baum",
                "J.S. Kirschke"
            ],
            "title": "A vertebral segmentation dataset with fracture grading",
            "venue": "Radiology: Artificial Intelligence 2(4), 1\u20136",
            "year": 2020
        },
        {
            "authors": [
                "M. Lukasik",
                "S. Bhojanapalli",
                "A.K. Menon",
                "S. Kumar"
            ],
            "title": "Does label smoothing mitigate label noise",
            "venue": "International Conference on Machine Learning (PMLR),",
            "year": 2003
        },
        {
            "authors": [
                "O.R. Lyman"
            ],
            "title": "An Introduction to Statistical Methods and Data Analysis",
            "year": 1993
        },
        {
            "authors": [
                "D.P. Papadopoulos",
                "E. Weber",
                "A. Torralba"
            ],
            "title": "Scaling up instance annotation via label propagation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15364\u201315373",
            "year": 2021
        },
        {
            "authors": [
                "K. Park",
                "H. Chung"
            ],
            "title": "Uncertainty Guided Pseudo-Labeling: Estimating Uncertainty on Ambiguous Data for Escalating Image Recognition Performance",
            "venue": "Proceedings of the 14th International Conference on Agents and Artificial Intelligence. vol. 2, pp. 541\u2013551. SCITEPRESS - Science and Technology Publications",
            "year": 2022
        },
        {
            "authors": [
                "C. Sager",
                "C. Janiesch",
                "P. Zschech"
            ],
            "title": "A survey of image labelling for computer vision applications",
            "venue": "Journal of Business Analytics 4(2), 91\u2013 110",
            "year": 2021
        },
        {
            "authors": [
                "A. Saleh",
                "I.H. Laradji",
                "D.A. Konovalov",
                "M. Bradley",
                "D. Vazquez",
                "M. Sheaves"
            ],
            "title": "A realistic fish-habitat dataset to evaluate algorithms for underwater visual analysis",
            "venue": "Scientific Reports 10(1), 1\u201310",
            "year": 2020
        },
        {
            "authors": [
                "A.S. Sambyal",
                "N.C. Krishnan",
                "D.R. Bathula"
            ],
            "title": "Towards Reducing Aleatoric Uncertainty for Medical Imaging Tasks",
            "year": 2021
        },
        {
            "authors": [
                "L. Schmarje",
                "J. Br\u00fcnger",
                "M. Santarossa",
                "S.M. Schr\u00f6der",
                "R. Kiko",
                "R. Koch"
            ],
            "title": "Fuzzy Overclustering: Semi-supervised classification of fuzzy labels with overclustering and inverse cross-entropy",
            "venue": "Sensors 21(19), 6661",
            "year": 2021
        },
        {
            "authors": [
                "L. Schmarje",
                "V. Grossmann",
                "T. Michels",
                "J. Nazarenus",
                "M. Santarossa",
                "C. Zelenka",
                "R. Koch"
            ],
            "title": "Label Smarter, Not Harder: CleverLabel for Faster Annotation of Ambiguous Image Classification with Higher Quality (may 2023), http://arxiv",
            "year": 2023
        },
        {
            "authors": [
                "L. Schmarje",
                "V. Grossmann",
                "C. Zelenka",
                "S. Dippel",
                "R. Kiko",
                "M. Oszust",
                "M. Pastell",
                "J. Stracke",
                "A. Valros",
                "N. Volkmann",
                "R.R. Koch"
            ],
            "title": "Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation",
            "year": 2022
        },
        {
            "authors": [
                "L. Schmarje",
                "M. Santarossa",
                "S.M. Schr\u00f6der",
                "C. Zelenka",
                "R. Kiko",
                "J. Stracke",
                "N. Volkmann",
                "R. Koch"
            ],
            "title": "A data-centric approach for improving ambiguous labels with combined semi-supervised classification and clustering",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "year": 2022
        },
        {
            "authors": [
                "L. Schmarje",
                "C. Zelenka",
                "U. Geisen",
                "C.C. Gl\u00fcer",
                "R. Koch"
            ],
            "title": "2D and 3D Segmentation of uncertain local collagen fiber orientations in SHG microscopy",
            "venue": "DAGM German Conference of Pattern Regocnition,",
            "year": 1907
        },
        {
            "authors": [
                "S.M. Schr\u00f6der",
                "R. Kiko",
                "R. Koch"
            ],
            "title": "MorphoCluster: Efficient Annotation of Plankton images by Clustering",
            "year": 2005
        },
        {
            "authors": [
                "A. Sekuboyina",
                "M.E. Husseini",
                "A. Bayat",
                "M. L\u00f6ffler",
                "H. Liebl",
                "H. Li",
                "G. Tetteh",
                "J. Kuka\u010dka",
                "C. Payer",
                "D. \u0160tern",
                "M. Urschler",
                "M. Chen",
                "D. Cheng",
                "N. Lessmann",
                "Y. Hu",
                "T. Wang",
                "D. Yang",
                "D. Xu",
                "F. Ambellan",
                "T. Amiranashvili",
                "M. Ehlke",
                "H. Lamecker",
                "S. Lehnert",
                "M. Lirio",
                "N.P. de Olaguer",
                "H. Ramm",
                "M. Sahu",
                "A. Tack",
                "S. Zachow",
                "T. Jiang",
                "X. Ma",
                "C. Angerman",
                "X. Wang",
                "K. Brown",
                "A. Kirszenberg",
                "\u00c9. Puybareau",
                "D. Chen",
                "Y. Bai",
                "B.H. Rapazzo",
                "T. Yeah",
                "A. Zhang",
                "S. Xu",
                "F. Hou",
                "Z. He",
                "C. Zeng",
                "Z. Xiangshang",
                "X. Liming",
                "T.J. Netherton",
                "R.P. Mumme",
                "L.E. Court",
                "Z. Huang",
                "C. He",
                "L.W. Wang",
                "S.H. Ling",
                "L.D. Huynh",
                "N. Boutry",
                "R. Jakubicek",
                "J. Chmelik",
                "S. Mulay",
                "M. Sivaprakasam",
                "J.C. Paetzold",
                "S. Shit",
                "I. Ezhov",
                "B. Wiestler",
                "B. Glocker",
                "A. Valentinitsch",
                "M. Rempfler",
                "B.H. Menze",
                "J.S. Kirschke"
            ],
            "title": "VERSE: A Vertebrae labelling and segmentation benchmark for multi-detector CT images",
            "venue": "Medical Image Analysis 73",
            "year": 2021
        },
        {
            "authors": [
                "V. Sharmanska",
                "D. Hernandez-Lobato",
                "J.M. Hernandez-Lobato",
                "N. Quadrianto"
            ],
            "title": "Ambiguity Helps: Classification with Disagreements in Crowdsourced Annotations",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016-Decem, 2194\u20132202",
            "year": 2016
        },
        {
            "authors": [
                "K. Sohn",
                "D. Berthelot",
                "C.L. Li",
                "Z. Zhang",
                "N. Carlini",
                "E.D. Cubuk",
                "A. Kurakin",
                "H. Zhang",
                "C. Raffel"
            ],
            "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
            "venue": "Advances in Neural Information Processing Systems 33 pre-proceedings",
            "year": 2020
        },
        {
            "authors": [
                "P. Tarling",
                "M. Cantor",
                "A. Clap\u00e9s",
                "S. Escalera"
            ],
            "title": "Deep learning with self-supervision and uncertainty regularization to count fish in underwater images pp",
            "venue": "Annotating Ambiguous Images: General Strategy and Validation",
            "year": 2021
        },
        {
            "authors": [
                "A. Tarvainen",
                "H. Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "year": 2017
        },
        {
            "authors": [
                "A.N. Uma",
                "T. Fornaciari",
                "D. Hovy",
                "S. Paun",
                "B. Plank",
                "M. Poesio"
            ],
            "title": "Learning from Disagreement: A Survey",
            "venue": "Journal of Artificial Intelligence Research 72, 1385\u20131470",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention Is All You Need",
            "venue": "Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "N. Volkmann",
                "J. Br\u00fcnger",
                "J. Stracke",
                "C. Zelenka",
                "R. Koch",
                "N. Kemper",
                "B. Spindler"
            ],
            "title": "Learn to train: Improving training data for a neural network to detect pecking injuries in turkeys",
            "venue": "Animals 2021 11, 1\u201313",
            "year": 2021
        },
        {
            "authors": [
                "W. Wang",
                "V.W. Zheng",
                "H. Yu",
                "C. Miao"
            ],
            "title": "A survey of zero-shot learning: Settings, methods, and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 10(2), 1\u201337",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wang",
                "Q. Yao",
                "J.T. Kwok",
                "L.M. Ni"
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "ACM Computing Surveys (CSUR) 53(3), 1\u201334",
            "year": 2020
        },
        {
            "authors": [
                "X. Wei",
                "H. Cong",
                "Z. Zhang",
                "J. Peng",
                "G. Chen",
                "J. Li"
            ],
            "title": "Faint Features Tell: Automatic Vertebrae Fracture Screening Assisted by Contrastive Learning (aug 2022)",
            "year": 2022
        },
        {
            "authors": [
                "S.E. Whang",
                "Y. Roh",
                "H. Song",
                "J.G. Lee"
            ],
            "title": "Data collection and quality challenges in deep learning: a data-centric AI perspective",
            "venue": "VLDB Journal",
            "year": 2023
        },
        {
            "authors": [
                "S. Yun",
                "S.J. Oh",
                "B. Heo",
                "D. Han",
                "J. Choe",
                "S. Chun"
            ],
            "title": "Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized Labels",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2340\u20132350",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Zheng",
                "Y. Shi"
            ],
            "title": "A Soft Label Method for Medical Image Segmentation with Multirater Annotations",
            "venue": "Computational Intelligence and Neuroscience 2023, 1\u201311",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Keywords: Biomedical application \u00b7 annotation strategy\u00b7 data-centric \u00b7 Vertebral fracture"
        },
        {
            "heading": "1 Introduction",
            "text": "The diagnosis of vertebral fractures is a critical field in the biomedical community. [15] states that the relative risk of death after a vertebral fracture is almost 9 times higher and that almost 20% of women will experience another fracture within 1 year after a vertebral fracture. The classification of vertebral fractures is primarily based on the height reduction of the vertebrae. Due to degenerative deformities, it is not easy to determine fractures on the height loss alone. This issue leads to inconsistent / ambiguous annotations of the data, which leads to inferior training of a neural network. In this paper, we present a strategy to produce high quality data for this task, independent of the network architecture used.\nOur primary focus is on the application of vertebral fracture diagnosis. However, we will also present a general strategy for annotating and generating highquality data for ambiguous classification tasks. Ultimately, we will circle back to the vertebral fracture diagnosis application to validate the effectiveness of this general approach.\nar X\niv :2\n30 6.\n12 18\n9v 1\n[ cs\n.C V\n] 2\n1 Ju\nIn general, deep learning based methods yield state-of-the-art results in image classification [17]. However, their success is founded in large amounts of high quality data [3,61]. While the amount of labeled data may be reduced with selfor semi-supervision for image classification, some high quality labeled data is still needed especially for domain-specific tasks [32]. However, a key challenge in annotating data is the reliability and consistency of annotators and thus their annotations. Annotations are typically created by humans and as such they may have different opinions or interpretations for difficult image classification tasks or when the images are ambiguous in nature. The literature concludes that a single annotation is not sufficient to the ensure high quality of labels in these cases [7,13,1,50]. To address this challenge, soft labels as an average across multiple annotations have been proposed as a possible solution to this problem [45].\nDespite the benefits of multiple annotations per image, annotation is costly and time-consuming. Thus, this approach is often not feasible for large datasets or even for smaller datasets where high-quality domain experts are required [25]. [44] proposes the use of proposal-guided annotations to overcome this issue. Proposal-guided annotation means that a preliminary trained network provides a rough estimate of the class for each image, which serves the annotators as guidance to the annotators in their decision making [8]. This guidance can help reduce the annotation time and increase the consistency and thus the overall quality [43,46]\nAll of these challenges, low data quality and difficult labeling apply to vertebral fractures, but there are no well-established guidelines for precessing the data. Moreover, medical experts are in high-demand and cannot be burdened with annotating thousands of images multiple times. Our work makes several contributions to the field of data-centric deep learning for the application of vertebral fracture diagnosis and ambiguous image classification in general.\nOur first core contributions are that we propose to unify the individual learning\u2019s from the literature to a practical guide of how to annotate data, especially ambiguous data. Although previous work has explored the use of individual techniques,we provide a step-by-step strategy that covers all aspects of the annotation process. The goal of this guide is applicable to any image classification task to ensure high quality data.\nOur second contribution is we verify the effectiveness of the proposed strategy by applying in on a dataset of vertebral fracture classification [35,49]. The guidelines were defined before they were tested of the dataset, thus we can reflect and discuss the benefits of the proposed guidelines.\nImportantly, we note that this is an application paper that aims to reflect and improve upon the existing literature, rather than introduce a novel method. By emphasizing the practical applications of our approach, we hope to facilitate the adoption of da ata-centric perspective in more research and applications, while contributing to the ongoing conversation about best practices in deep learning research."
        },
        {
            "heading": "2 Practical Guidelines: How to annotate ambiguous data?",
            "text": "Before outlining our practical approach, we will address the rationale for its necessity and explain the concept of ambiguous data to which these guidelines should be applied. Common labeling guidelines [40,5] use hard-encoded labels (one correct class) and do not consider the issue that arises from the fact that people may disagree about the class for a given image. This can lead to the presence of ambiguous labels or, more generally, ambiguous data \u2013 an issue that is common in many real-world applications [52,42,23,4,22,62]. The literature provides several reasons why one annotation per image is insufficient to capture the ambiguity of the data [54,45,44,7,13,1,50]. The hard label probability distribution for an image x can be described as P (Lx = \u00b7) \u2208 {0, 1}K with K classes.\nDavani et al. showed that these multiple annotations should then be averaged to generate a soft label as in [45,19], as opposed to using a majority vote, which is not optimal for ambiguous cases [7]. In addition, label smoothing, which involves smoothing hard-coded labels with a constant factor, is an established method for improving the performance of networks [55,26,36]. A soft-label probability distribution for x can be described as P (Lx = \u00b7) \u2208 [0, 1]K .\nIt is important to note that ambiguous data is not just an error [39], but a data property. The data is inherently uncertain or ambiguous and thus must be treated as such. Thus, in general, the soft ground truth distribution P (Lx = \u00b7) is unknown and can only be approximated for the verification of the predicted results. This research is in line with a growing movement of data-centric deep learning [34,21,60,13], where the focus is on data rather than model building.\nWe give an overview about the practical guidelines in a flowchart in Figure 1. It is divided into five main steps: Defining the specifics of what, who, and how the annotation process will be performed, the process itself, and its post-processing."
        },
        {
            "heading": "2.1 Definition - What?",
            "text": "The first step is to precisely define the classification task. In particular, the K classes must be defined for the given problem and data. Additionally, raw unlabeled data must be collected, downloaded or generated based on the use case. The complete unlabeled image data is called Xr.\nThe next step is to determine the number of images to label. We looked at use cases with several thousand to several tens of thousands of images because we believe this is a common use case in real-world applications such as [4,43,56,10,41]. When hundreds of thousands or even millions of images are available, self-supervised techniques may be applicable [6,12]. However, even in these cases, performance evaluation requires a smaller labeled dataset that can be labeled with our strategy. We define Xu \u2286 Xr as the subset containing only the samples that should be labeled.\nThe research we focus on builds on common supervised or semi-supervised deep learning [51,53,29]. Thus, we need enough samples per class to be available\nafter annotation. The formulation \u201denough samples\u201d depends on the use cases, but our recommendation based on previously reported results would be that each class represents at least 1% of the complete data and that there are several dozen images per class, so that enough samples are available for evaluation [45]. If too few images are available, additional considerations about few or zero-shot learning approaches have to be taken into account [58,57,33]. However, this is beyond the scope of this paper.\nFor each image x in a small labeled dataset x \u2208 Xl \u2286 Xu at least one hardencoded ground truth label is needed. This data is used for annotator evaluation (see subsection 2.2) as well as potential training data of a proposal generation network (see subsection 2.3). An initial size of about 20% of the original dataset size was beneficial in [44], but it can be smaller or larger depending on the use-case."
        },
        {
            "heading": "2.2 Definition - Who?",
            "text": "A central question for the annotation process is who will annotate the image. Various concepts have been suggested in the literature, including the use of a single user, the collaboration of several workers, or even crowd-sourcing [40]. Depending on the background of the annotators, they need to be trained or selected for the given task [31]. To evaluate the quality of annoations, two elements are required in addition to the small labeled dataset for evaluation Xl: a threshold of acceptable performance \u00b5 and a suitable annotation platform.\nThe threshold \u00b5 depends on the evaluation metrics used. Macro accuracy or F1-score are commonly used and can show a high correlation [28,16,2,45,24,30]. The results in [46] indicate that a threshold \u00b5 of 60% to 80% is often reasonable enough if the annotations are aggregated in the end. When choosing the threshold, one should keep in mind that the quality of the labels in Xl affects the maximum achievable scores. For example, due to intra- or inter-observer variability, a score higher than 80% may not be possible [47].\nBased on the task definition in subsection 2.1, a few classes with many occurrences are expected, so Sagar et al. recommend using a browsing environment for the annotation process, where multiple images can be viewed side by side [40]. However, other annotation platforms such as [48] are possible. These platforms may introduce an additional overhead during the setup, but can dramatically increase the speed of annotation.\nThe annotators should be trained until they exceed the desired performance \u00b5 on the labeled data Xl with the chosen platform."
        },
        {
            "heading": "2.3 Definition - How?",
            "text": "The key question in this section is whether or not to use proposals in the annotation process. This question is central due to the fact that proposals can reduce the cost of annotation by a multitude, but may introduce unwanted side effects [20,44]. This side-effect / bias can be introduced by to the fact that annotators are more likely to accept a proposed class, resulting in a skewed label distribution P (Lx = \u00b7) towards the proposed class. For this decision process, several parameters need to be approximated or at least their way of approximating them later should be defined. These parameters are motivated by [44] and are the class confusion matrix c and the expected speedup S. We will first describe how these variables are used and refer to the supplement for possible approximation techniques. The most important part is to understand, when specific variables are needed, how they could be approximated at this step or how they will be approximated if they are only needed later.\nThe decision of whether to use proposals is guided by two main questions from [44]. The first question concerns the acceptability of introducing a bias. Although introducing a bias may seem undesirable, it is a bias that has been approved by human annotators. In recommendation systems [8,9,14], this bias is often inherently accepted, and even when humans reach a consensus [35], they\nmay influence each other. If the conditions allow for an acceptable bias, proposals should be utilized. If not, the expected speedup must be evaluated.\nThe benefit of proposals is greater with a higher speedup (S), as a higher speedup enables easier reversal of the introduced bias with more annotations. However, there is a trade-off point where the speedup is too small to justify the introduced bias, and it becomes unlikely that the bias can be recovered with an equal or smaller budget. In [44], this tradeoff point is defined to be around 3. If the speedup falls below this threshold, the recommendation is to avoid using proposals and resort to normal annotations. Conversely, if the speedup exceeds the threshold, proposals can be employed.\nThe class confusion matrix c is used in post-processing (see subsection 2.5) to improve the final distribution and is not needed until then."
        },
        {
            "heading": "2.4 Annotation Process",
            "text": "This step involves the most work because it concentrates on creating the annotations. Several preparatory steps need to be taken, depending on whether the utilization of proposals is intended or not. Previous literature has shown that proposals improve the consistency of annotations [38,8], especially the use of overclustering seems to be beneficial with ambiguous data [43]. Therefore, we recommend using DC3 proposals [46], which have been shown to perform better on real-world ambiguous data than common network predictions and work with many semi-supervised methods [51,30,53]. The initial labeled data Xl (defined in subsection 2.1) can be used as training data for such an arbitrary neural network. The network should be trained until it reaches the same quality threshold \u00b5 as the annotators. Before starting the annotation process, two variables must be defined based on the decision in subsection 2.3: the number of annotations until consensus (Ac) and the total number of annotations (A\nc). These variables play a crucial role in determining the trade-off point between achieving the best approximation of the underlying distribution P (Lx = \u00b7) and minimizing the annotation effort and associated costs.\nAc represents the general number of annotations per image, reflecting the quality of the approximation. While some data in Xu may exhibit ambiguity and result in disagreement among annotators, there is a portion of data where all annotators reach a consensus. Although the specific proportion of complete consensus (pc) has not been reported, [46] reports the proportion of at least 95% consistent annotations (p\u0302c) ranging from about 50% to 80% across four different datasets. To reduce the total annotation costs, we propose annotating only those images that do not achieve complete consensus after Ac annotations. This approach aims to identify images where complete consensus can be achieved with fewer annotations than Ac.\nThe values chosen for Ac and A c depend on the specific classification problem and previous design choices. In the supplementary section, we discuss plausible numbers of annotations per hour ( ah ) and potential 95% confidence guarantees for different combinations of Ac and A c.\nFor the case where Ac = 10 and A c = 50, two main conclusions can be drawn. First, based on the previously reported ah in [46], it is likely that all annotations can be completed within 10 days by a single annotator. This workload seems manageable, even for smaller projects, especially when shared among multiple annotators. Second, considering the approximation of P (Lx = \u00b7) and its unknown true value, if we want to achieve an approximation within a margin of 0.15 around the unknown true value with at least 95% confidence, using Ac = 10 and A\nc = 50 is a reasonable choice."
        },
        {
            "heading": "2.5 Post-Processing",
            "text": "As discussed in subsection 2.3, the use of proposals during the annotation process introduces a bias due to the default effect [20]. It is important to note that while this speeds up the annotation process and produces more consistent annotations, it may also introduce a bias that may not be acceptable. Schmarje et al. propose in [44] with CleverLabel a combination of classblending and bias correction to improve the approximated distribution, which should be applied if proposals were used during the annotation process. If no proposals were used to create the annotations, it is still recommended to improve the distribution P (Lx = \u00b7) by blending with the class distribution c."
        },
        {
            "heading": "3 Evaluation",
            "text": "The goal of this paper is to solve real-world application annotation tasks such as the medical task of vertrebral fracture classification. For this purpose, we will describe how the previously defined strategy can be applied to this task on the publicly available datasets Verse2019[35] and Verse2020[49]. An analysis of the benefits or shortfalls of the decisions is given in subsection 3.2."
        },
        {
            "heading": "3.1 Applying the Strategy",
            "text": "We use the Verse datasets for better reproducibility of our osteoporotic vertrebral fracture classification. As in [35], we consider only at thoracolumbar vertebrae because osteoporotic fractures occur mainly in this region of the spine, as shown in Figure 2. For the classification of the fractures, we use an adaptation of the most commonly used score, the semi-quantitative Genant score [11]. As shown in Figure 2, the score defines four classes, which are mainly guided by the height reduction of the vertebrae compared to its neighbors or unfractured vertebrae. However, degenerative deformities (e.g. short vertebral height) of up to 20% are ignored for the osteoporotic fracture classification. Partially, these degenerative cases can not be assessed in the dataset because the resolution is lower than in normal radiographs. As shown in Table 1, this results in 3,761 individual vertebrae with a high class imbalance towards class zero. Overall, we have at least 1.1% samples per class and can apply common semi- or supervised techniques.\nUnlike to the original author of the Verse2019 [35] dataset, we will not use medical experts for the annotation and will use 2D projections of the original 3D CT data.\nOur decision to use non-medical experts has two main reasons. First, we want to show that non-experts can successfully annotate data with the help of proposals. Second, we plan to generate tens of thousands of annotations, which would not be feasible with medical experts, who are in high demand. While we will not use medical experts for the majority of the annotations, we have worked closely with the radiologist at XXXX (censored for review). We discussed our design choices with them, particularly regarding the definition of the task and the representation of the vertebrae used. As mentioned above, we trained the hired workers on this specific task and ensured that they achieved a macro accuracy and F1 score of at least \u00b5 = 0.6 on the complete dataset with and without proposals for at least two scores before they could annotate the final scores.\nWe do not use the original 3D CT data (see Figure 3a) for the annotation but rather a 2D projection of the central slices for each vertebrae (see Figure 3b) and its segmentation (see Figure 3c). Additionally, we show the vertebrae above and below the target vertebrae to allow an evaluation of the height reduction. A blended example is shown in Figure 3d. There are three major benefits/reasons of using 2D projections over 3D CT data. First, the Genant scores were defined\non 2D X-Ray and 2D projections of CT data are visually very similar to X-Ray and thus are better comparable. Second, we want to use the reported methods from the literature for the proposal generation [46], which were only tested on 2D data. While it should be theoretically possible to extend the used networks to 3D, such major structural changes could lead to an inferior result which would dilute the results, such experiments may be future work. Lastly, when it comes to diagnostics, radiologists mainly look at one central slice which we include and the major 3D structures can be detected on this projection.\nWe hired 5 workers to annotate vertebrae in a web-based environment developed for this evaluation. Public annotation platforms do not support the combination of clusters and classes that we want to evaluate in this paper. Details about the platform are given in the supplementary. As Xl we used the partially available labels from [35].\nIn a medical consensus process in an expert panel, the proposals of different people are unified, but influences between people due to social dynamics cannot be excluded. Therefore, we argue that a bias that may be introduced due to human approved annotations is comparable to a consensus process and therefore acceptable. This means that we will use proposals during the annotation and c and S only need to be approximated after the annotation process based on the defined strategy. To allow a validation of our decisions, we will also raise annotations without proposals and thus we can approximate the class distributions c and speedup S based on these annotations. Additional approaches to approximate S as well as the approximation of a data-specific offset \u03b4 (see [44]) are discussed in the supplement.\nBased on the training annotations, we estimated the ah to be about 2,000 and 3,000 without and with proposals, respectively. These results are similar values previously reported in [46]. For Ac = 10, A\nc = 50 and estimated pc = 0.5, this would result in about 250,000 annotations with and without proposals. Furthermore, based on the estimated ah of about 2,500, this resulted in a workload of about 20 hours per annotator which was within our given budget. So we chose the number of annotations at the recommended values of Ac = 10, A\nc = 50. Additionally, we planned to annotate the 3D data to evaluate the quality difference between 2D and 3D. We limited these annotations to the test set of the original Verse2019 dataset [35]."
        },
        {
            "heading": "3.2 Analysis",
            "text": "The annotators completed more than 100 iterations of annotation, half of them with proposals and half without. However, one annotator did not meet the acceptance threshold \u00b5 and was excluded along with the training iterations, leaving 80 valid iterations or a total of about 250,000 annotations. Additionally, 11 iterations were performed using 3D data from the test dataset for comparison. The average annotation rate was calculated to be 3,259.36 ah , which means that it takes about one hour to complete the 3,761 annotations with the additional technical overhead and pauses. The comparison of the resulting dataset-specific variables with previously reported values [46] is given in Table 1. We see that the\ndataset offset \u03b4 and the rate of near-consensus rate p\u0302c are similar to previously reported results. It is important to note that pc = 0.27, which is significantly smaller, resulting in more data with Ac number of annotations. It should be evaluated in the future, if instead of total consensus a high number of agreeing votes is sufficient to reduce the cost of the annotation process. The largest class is greater than in the previous reports and the speedup S is smaller in Table 1. This relationship is theoretically justified because a proposal is less important since the majority of images already belongs to a class and thus defines some kind of a proposal.\nA comparison of the annotator and network performance on the Verse2019 [35] test set is given in Table 2. When comparing the macro F1 scores in 2D between annotations performed with and without proposals and those with proposalbased guidance achieved a significantly higher F1 score of about 2%. This means that proposal-based guidance led to better annotations by the annotators. The average macro F1 score for 3D annotations shows no significant difference com-\npared to the 2D annotations, meaning that the modality difference was of no to little importance to the annotators.\nThe marginal class distribution on all vertebrae from the test set between the ground truth and the majority vote for 2D annotations was quite similar, with percentages of 80.33, 10.38, 6.01, and 3.28 versus 80.33, 10.93, 5.74, and 3.01, respectively. In addition, only 17% of the majority votes deviated from the ground truth. Comparing the average uncertainty based on P (Lx = \u00b7) for concurring and dissenting majority votes with the ground truth, we find a higher annotator uncertainty of 24.20% for dissenting scores than for concurring scores at 7.78%. These results indicate that the annotators produced annotations of similar quality to the original test data, with higher uncertainties for potentially difficult or incorrect majority votes.\nThe best performing network on the validation data for the proposal generation did not surpass the set target threshold \u00b5 = 0.6 with a macro F1 of 0.57. However, it surpassed \u00b5 on the validation and training data and it was decided that the performance was close enough. The proposal network was a semi-supervised Mean-Teacher model [53] with overclustering and default hyperparameters from [46]. The performance of the model was significantly better when trained only on the mask (see Figure 3c) with only the desired vertebra visible instead of all other 2D modalities. We attribute this difference to the fact that the model could be more easily confused with vertebrae that are not currently of interest and to the more complicated representation of information e.g. in the form of blended images. In terms of the performance of the network trained on our new annotated data, but evaluated on the original test ground truth, we found that more annotations sampled from the annotated data generally led to better performance. This sampling from the human annotation could achieve random peaks up to about 0.7 F1-score. A standard ResNet50 model [18] with default hyperparameters (see supplementary for details) and cross-entropy as loss function could achieve a score of 0.63, which is higher than the more complex proposal network (trained on the original training data) and previously reported values by Wei et al. for non-specialized networks [59]. While they show that they can achieve higher scores with more problem-specific loss functions, with our data-centric approach, we can achieve an improvement without changing the model or training purely from the input data. Future potential improvement to also use special loss functions and better backend-models nonwithstanding.\nIn Table 3, we compare the network predictions with the approximated soft ground truth distribution (P (Lx = \u00b7)) using the Kullback-Leibler-divergence [27]. Comparing the use of DC3 proposals with and without blending or correction, we see that the results are better with both improvement methods proposed in [44]. Thus, we can confirm that the application of CleverLabel (the combination of both improvements) should be done in the post-processing step. We can also confirm that applying balanced blending to annotations without proposals improves the results. Using only the class blending based on the majority class seems to work particularly well for this task. However, as the number of annota-\nmethod number of annotations\nproposal blending correction 1 3 5 10 DC3 balanced yes 0.4481 \u00b1 0.1957 (0.00%) 0.2425 \u00b1 0.0741 (0.00%) 0.1832 \u00b1 0.0095 (0.00%) 0.1615 \u00b1 0.0129 (0.00%) DC3 balanced no 1.0309 \u00b1 0.1750 (130.07%) 0.2704 \u00b1 0.0760 (11.53%) 0.2105 \u00b1 0.0256 (14.85%) 0.4546 \u00b1 0.4343 (181.49%) DC3 no yes 1.7382 \u00b1 0.1553 (287.91%) 0.2499 \u00b1 0.0374 (3.08%) 0.2081 \u00b1 0.0171 (13.57%) 0.2798 \u00b1 0.1325 (73.25%) no only blends no 0.2311 \u00b1 0.0224 (-48.42%) 0.2394 \u00b1 0.0930 (-1.27%) 0.1904 \u00b1 0.0209 (3.88%) 0.1644 \u00b1 0.0062 (1.79%) no balanced no 0.8565 \u00b1 0.4466 (91.14%) 0.1966 \u00b1 0.0245 (-18.90%) 0.1678 \u00b1 0.0224 (-8.44%) 0.1568 \u00b1 0.0165 (-2.92%) no no no 0.5578 \u00b1 0.1259 (24.49%) 0.2451 \u00b1 0.0127 (1.10%) 0.5435 \u00b1 0.4285 (196.61%) 0.1898 \u00b1 0.0380 (17.52%)\nTable 3: Resulting KL across different methods and number of annotations \u2013 The first method is the recommendation based on our defined guidelines. The next two methods use proposals and the last three methods do not use proposals which means their annotation time is also slower in comparison to the recommendation. Thus, one should not compare with the same number of annotations but higher number of annotations for DC3 proposals because the higher number can be achieved potentially in the same or less time. The results are given as KL \u00b1 STD ( Relative Change in % with regard to recommended method)\ntions increases, the benefit decreases and from 5 annotations onward, it becomes worse than the recommended method of our defined strategy. In our description of how to apply the proposed annotation strategy (see subsection 3.1), we stated that the introduction of a bias is not a major concern. Nevertheless, we can validate that the strategy leads to the best results even if a bias would not be acceptable. Based on the speedup of about 1.2, the recommendation would have been to not annotate with proposals and to use only balanced blending, which is the best method for multiple annotations. Theoretically, if we had a high speedup, we could compare a higher number of annotations with proposals to fewer annotations without proposals. For example, a speedup at the decision boundary of 3, 5 annotations with proposal and 3 annotations without proposal, we have a lower annotation cost based on the calculations form [44] (5 \u00b7 0.33 vs. 3) and have a lower KL score. Thus, even in the case of higher speedups, our strategy suggests the correct choice."
        },
        {
            "heading": "4 Conclusion",
            "text": "In conclusion, this study presents a general strategy for annotating and processing ambiguous images with verification in vertebral fracture diagnosis. Overall, the human performance analysis demonstrated the effectiveness of proposalguided annotation, resulting in improved F1 scores and reduced training time. A neural network trained on newly annotated data achieved a higher classification performance on the original test set and can potentially more than halve the KL score on our new baseline. Based on the results for all possible scenarios, we were able to verify the effectiveness of our strategy based on the literature for the given use case. By providing practical guidance and verifying its application on a real-world vertebral fracture dataset, this research contributes to the creation of high-quality datasets and advances the field of data-centric deep learning."
        },
        {
            "heading": "Supplementary Material",
            "text": ""
        },
        {
            "heading": "Approximation of workload",
            "text": "We look at the expected number of annotations per hour done by one annotator as ah . Depending on the expected portion of complete consensus of the dataset pc and the total dataset size N = |Xu|, we can calculate the expected annotation time in hours with AN = N \u00b7 (pc \u00b7Ac + (1\u2212 pc) \u00b7Ac)/ ah . In Figure 4 the required annotation time AN in days for N = 10, 000, Ac = 10, A\nc = 50 with varying pc is given as well as values reported in [46]. The reported values for a h are between 1,000 and 4,500 without proposals or 3,000 and 10,000 with proposals. Thus, even in the worst-case the expected workdays with proposals are less than 10 days, or two weeks, which should be feasible in most cases especially when multiple annotators are available. These values are examples and have of course to be adopted to the specific use-case."
        },
        {
            "heading": "Approximation of confidences",
            "text": "Considering the binary case of the annotation task for class k for each image x, i.e. whether class k is annotated for x or not, and having a sufficient number of annotations A, the 95%-confidence interval can be calculated using the Wald confidence interval [37]. The interval is given by\nP (Lx = k)\u00b1 Z95% \u00b7 \u221a\npk(1\u2212 pk) A\n(1)\nwith 95%-confidence value, which is 1.96, and pk is the unknown true GT probability for class k. The confidence interval is the largest for p = 0.5. The formula can be rearranged to calculate the required annotations A if a 95% confidence interval around the approximation P (Lx = k) of width W and margin W/2 is to be guaranteed.\nW\n2 = Z95% \u00b7\n\u221a pk(1\u2212 pk)\nA\nW 2\n4 = Z295% \u00b7 pk(1\u2212 pk) A\nA = 4 \u00b7 Z295% \u00b7 pk(1\u2212 pk)\nW 2\n(2)\nFigure 5 visualizes the required annotations A for various p and W . These calculations can guarantee with 95% confidence that the confidence interval has a width of W \u2248 1 for A = 3, W \u2248 0.62 for A = 10 and W \u2248 0.28 for A = 50. However, a high number of A is indirectly assumed because the Wald confidence interval approximates the original binomial distribution with a normal distribution. This approximation is especially inaccurate for small numbers of A and p near 0 or 1, as pointed out by Lyman et al. [37]. They suggest to approximate the 95% confidence intervals with ((0.25) 1 A , 1) when p is close to one. This would\ngive a lower bound of the interval of 0.63 for A = 3, 0.87 for A = 10 and 0.97 for A = 50. All these calculations assume an idealized version of the annotators, which must be taken into account in their interpretation. For this reason, it will not be investigated further how the confidence intervals can be approximated more precisely, but conclude that a consensus decision is uncertain for A = 3 annotations, but reasonable for ten or more annotations. Similarly, approximations of p with a confidence interval width less than 0.3 are only reasonable for about 50 annotations.\nAdditional minor considerations for Annotation Strategy\n\u2013 All decisions should typically be supported by domain experts or the end users which will use the labeled data in the end.\n\u2013 Constraints like collaboration modes and support for proposals should be considered. For example, if crowd-sourcing should be used for annotation, this restricts the appropriate platforms. Due to the fact if proposals will be used is decided later in subsection 2.3, it is advisable to ensure that these options are supported to avoid unnecessary backtracking.\n\u2013 If the training of the annotators is not successful it might be necessary to step back and rethink previous decision to allow a successful training. For example, special training with experts could be held, more detailed description of the task could be given and general feedback of annotators could be considered.\n\u2013 Proposals \u03c1x with a low ground truth probability (P (L x = \u03c1x) = 0) should\nbe avoided and [46] indicates only up to 10% of all proposals should be of such poor quality to allow successful training. The distribution P (Lx = \u00b7) might not be known at this point but if available this should be checked in addition to the threshold \u00b5.\n\u2013 The variables c, \u03b4 and S may be approximated on various ways. One possible approach is defined in [44] by annotating about 100 images at least 10 times for the estimation of c, \u03b4 and P (Lx = \u00b7). The speedup S can for example be measured on a small subset with and without proposals during the training of the annotators. However, if the variables are not required during the decision process described above, they could also be approximated on the real annotation data or ignored entirely. For example, c, can easily be approximated on the complete dataset if no proposals are used during the annotation process and an approximation of S is not required if a small introduction of a bias is acceptable. The variables could also be approximated with an educated guess either based on previous knowledge or experiments. \u2013 The question is how Ac, A c should be selected. We concentrate on two major\naspects, what is a feasible workload for the annotators and what guarantees can be given. See for more details the approximations in section 4 and section 4.\n\u2013 The blending step may not be necessary if the number of annotations is high (e.g. above 50), since the distribution is already approximated quite accurately."
        },
        {
            "heading": "Approximation of \u03b4",
            "text": "We used annotations created during the training of the annotators to estimate \u03b4. We randomly picked 20 examples without consensus from the annotations without proposals. Based on these samples, the proposal per image px, the prob-\nabilities P (Lx = \u03c1x) with and without proposal and the formula for simulated proposal acceptance of [44], we calculated \u03b4 \u2248 0.11."
        },
        {
            "heading": "Used hyperparameters",
            "text": "The used hyperparameters are defined in Table 4. For more detail see the respective modified source code."
        },
        {
            "heading": "Additional results of the learning effect",
            "text": "Regarding the learning effect of the annotators, the training iterations showed an increase in F1 scores over time. The improvement in F1 scores across all annotators between the training and the last three iterations was 6.79 without proposals and 9.19 with proposals. When comparing the benefit of DC3 proposals versus no proposals, the average improvement in F1 scores was 3.27 for the first three iterations and 0.41 for the last three iterations. The proposal still has in the end a benefit but the annotators can more successfully annotate event without them. Moreover, the introduction of proposal-based annotations resulted in a reduction in training time which also diminished over time. The first three iterations saw a reduction of 13.49 minutes, while the last three iterations experienced a decrease of 4.55 minutes. Overall, we can conclude that the proposals helped to improve the quality and reduce the annotation time. However, this effect was diminished overtime while the performance with and without proposals increase. Thus, we see a learning effect of the annotators."
        },
        {
            "heading": "Annotation platform",
            "text": "Figure 6 displays the used annotation website. It is important to note, that all images are displayed to the user in a grid. In the case without proposal (see Figure 6a), the shown classes are mixed and all have to be annotated manually\nby the user. In the second case with proposal (see Figure 6b), most of the images are of the same class and can be annotated simultaneously, while only the errors need to be corrected."
        }
    ],
    "title": "Annotating Ambiguous Images: General Annotation Strategy for Image Classification with Real-World Biomedical Validation on Vertebral Fracture Diagnosis",
    "year": 2023
}