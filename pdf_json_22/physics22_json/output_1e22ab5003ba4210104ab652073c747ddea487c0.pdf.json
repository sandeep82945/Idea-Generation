{
    "abstractText": "Type Ia supernovae (SNe Ia), standardizable candles that allow tracing the expansion history of the Universe, are instrumental in constraining cosmological parameters, particularly dark energy. State-of-the-art likelihood-based analyses scale poorly to future large data sets, are limited to simplified probabilistic descriptions, and must explicitly sample a high-dimensional latent posterior to infer the few parameters of interest, which makes them inefficient. Marginal likelihood-free inference, on the other hand, is based on forward simulations of data, and thus can fully account for complicated redshift uncertainties, contamination from non-SN Ia sources, selection effects, and a realistic instrumental model. All latent parameters, including instrumental and surv e y-related ones, per object and population-level properties, are implicitly marginalized, while the cosmological parameters of interest are inferred directly. As a proof of concept, we apply truncated marginal neural ratio estimation (TMNRE), a form of marginal likelihood-free inference, to BAHAMAS , a Bayesian hierarchical model for SALT parameters. We verify that TMNRE produces unbiased and precise posteriors for cosmological parameters from up to 100 000 SNe Ia. With minimal additional effort, we train a network to infer simultaneously the \u223c100 000 latent parameters of the supernovae (e.g. absolute brightnesses). In addition, we describe and apply a procedure that utilizes local amortization of the inference to convert the approximate Bayesian posteriors into frequentist confidence regions with exact coverage. Finally, we discuss the planned impro v ements to the model that are enabled by using a likelihood-free inference framework, like selection effects and non-Ia contamination. Key w ords: (cosmolo gy:) cosmological parameters \u2013 methods: statistical.",
    "authors": [
        {
            "affiliations": [],
            "name": "Roberto Trotta"
        },
        {
            "affiliations": [],
            "name": "Christoph Weniger"
        }
    ],
    "id": "SP:c4eb0a7a341439f4a0b91abe1b35638d68ca242a",
    "references": [],
    "sections": [
        {
            "text": "MNRAS 520, 1056\u20131072 (2023) https://doi.org/10.1093/mnras/stac3785 Advance Access publication 2022 December 30\nSICRET: Superno v a Ia Cosmology with truncated mar ginal neural Ratio EsTimation\nKonstantin Karchev , 1 \u2039 Roberto Trotta 1 , 2 and Christoph Weniger 3 1 Theoretical and Scientific Data Science Group, Scuola Internazionale Superiore di Studi Avanzati (SISSA), via Bonomea 265, I-34136 Trieste, Italy 2 Astr ophysics Gr oup, Department of Physics, Black ett Lab, Imperial Colleg e London, Prince Consort Road, London SW7 2AZ, UK 3 Gravitation Astroparticle Physics Amsterdam (GRAPPA), University of Amsterdam, Science Park 904, NL-1098 XH Amsterdam, the Netherlands\nAccepted 2022 December 20. Received 2022 December 20; in original form 2022 September 20\nA B S T R A C T Type Ia supernovae (SNe Ia), standardizable candles that allow tracing the expansion history of the Universe, are instrumental in constraining cosmological parameters, particularly dark energy. State-of-the-art likelihood-based analyses scale poorly to future large data sets, are limited to simplified probabilistic descriptions, and must explicitly sample a high-dimensional latent posterior to infer the few parameters of interest, which makes them inefficient. Marginal likelihood-free inference, on the other hand, is based on forward simulations of data, and thus can fully account for complicated redshift uncertainties, contamination from non-SN Ia sources, selection effects, and a realistic instrumental model. All latent parameters, including instrumental and surv e y-related ones, per object and population-level properties, are implicitly marginalized, while the cosmological parameters of interest are inferred directly. As a proof of concept, we apply truncated marginal neural ratio estimation (TMNRE), a form of marginal likelihood-free inference, to BAHAMAS , a Bayesian hierarchical model for SALT parameters. We verify that TMNRE produces unbiased and precise posteriors for cosmological parameters from up to 100 000 SNe Ia. With minimal additional effort, we train a network to infer simultaneously the \u223c100 000 latent parameters of the supernovae (e.g. absolute brightnesses). In addition, we describe and apply a procedure that utilizes local amortization of the inference to convert the approximate Bayesian posteriors into frequentist confidence regions with exact coverage. Finally, we discuss the planned impro v ements to the model that are enabled by using a likelihood-free inference framework, like selection effects and non-Ia contamination.\nKey w ords: (cosmolo gy:) cosmological parameters \u2013 methods: statistical.\n1\nT u t c d p a c w d\nt r 4 2 C t ( ( t\nS o f y C\nt f p e S i r\no n a t l e d 2\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023 I N T RO D U C T I O N he momentous disco v ery of the accelerated e xpansion of the niverse (Riess et al. 1998 ; Perlmutter et al. 1999 ) demonstrated hat type Ia supernovae (SNe Ia) can be used as standardizable andles to measure extragalactic distances, which, together with a etermination of redshift, lead to constraints on the cosmological arameters. SNe Ia are also useful to extend the distance ladder, thus llowing, when properly calibrated, to measure the Hubble\u2013Lema \u0302 \u0131tre onstant, H 0 \u2013 a measurement famously and mysteriously in tension ith the result obtained from cosmic microwave background (CMB) ata at high redshift (Di Valentino et al. 2021 ). While the initial disco v ery of cosmic acceleration relied on a otal of 58 high-redshift SNe Ia (backed by a similar-sized lowedshift anchor sample), several dedicated SN Ia campaigns: CfA 1\u2013 (Perlmutter et al. 1999 ; Jha et al. 2006 ; Hicken et al. 2009 , 012 ), BSNIP (Silverman et al. 2012 ), SNLS (Guy et al. 2010 ), SP I\u2013II (Krisciunas et al. 2017 ; Phillips et al. 2019 ), general ransient searches: Pan-STARRS 1 (Scolnic et al. 2018 ), Foundation F ole y et al. 2018 ), and e xtensiv e cosmological surv e ys: SDSS Sako et al. 2018 ), DES (Brout et al. 2019 ), have since enlarged he spectroscopically confirmed cosmological sample to about 2000\nE-mail: kkarchev@sissa.it\nw f S\nPub\nNe Ia (Scolnic et al. 2022 ). Similarly to the Universe, the rate f SN Ia disco v ery is accelerating e xponentially, and in the near uture, the Vera Rubin Observatory will observe \u223c10 5 SNe Ia per ear in its Le gac y Surv e y of Space and Time (LSST; LSST Science ollaboration 2009 ; Ivezi \u0301c et al. 2019 ). This massive increase in the quantity of the data has spurred he development of increasingly sophisticated statistical methods or their analysis, in order to accurately model the data collection rocedure in all of its complexity, to capture important systematic ffects, and to describe the underlying population variability of both Ne Ia and host-galaxy properties. These efforts have become central n ensuring more precise and accurate constraints on the dark energy edshift evolution from future large samples of SNe Ia.\nTraditionally, cosmological inference with type Ia SNe has relied n standardization : the process of correcting their observed brightesses so that when the dimming effect of distance is taken into ccount, the SN Ia population is as homogeneous as possible. Iniially, this was done using hand-crafted descriptions of the observed ight curv es (Psko vskii 1967 , 1977 , 1984 ; Phillips 1993 ; Perlmutter t al. 1997 ), which were later replaced by summaries derived from ata. By far the most popular such model is SALT (Guy et al. 2005 , 007 ; Betoule et al. 2014 ; Kenworthy et al. 2021 ; Taylor et al. 2021 ), hich decomposes the SN Ia spectral energy distribution (SED) using unctional principal component analysis: a methodology extended in NEMO (Saunders et al. 2018 ) to up to 15 components. A particular\n\u00a9 2022 The Author(s) lished by Oxford University Press on behalf of Royal Astronomical Society\nS c f S\nS G h ( ( s d fi v N o w s\nh s f f G p w u i M 1 \u223c b a\na i ( d t o i n p e d r c i\nv t t s 2 2 2 e ( K B 2 f\nr W e o a\nd ( n b B I r n s e a e a p\nf c m s l p\nM n 2 i a s t ( t p M b r a t c\nt a w t i h d S s u l m ( p a p i\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nN Ia is then described by the relative contributions of the principal omponents and by a colour parameter, which are subsequently used or standardization and state-of-the-art cosmological inference from Ne Ia (e.g. Abbott et al. 2019 ; Brout et al. 2022 ). Following the realization that the observed excess variability of Ne Ia after standardization is not simply described by additional aussian noise/scatter, in the last decade, sophisticated Bayesian ierarchical models (BHMs) were developed. Some, like BAHAMAS March et al. 2011 ; Shariff et al. 2016b ; Rahman et al. 2022 ), UNITY Rubin et al. 2015 ), the model of Ma, Corasaniti & Bassett ( 2016 ), imple-BayeSN (Mandel et al. 2017 ), and Steve (Hinton et al. 2019 ), escribe SNe Ia using summary parameters obtained from SALT ts but fundamentally distinguish observational noise from intrinsic ariability. In contrast, Mandel et al. ( 2009 , 2022 ) and Mandel, arayan & Kirshner ( 2011 ) presented a fully Bayesian treatment f the underlying SN Ia SED template in a model called BayeSN, hich was recently used for a cosmological analysis of the RAISIN ample (Jones et al. 2022 ). Hierarchical Bayesian modelling, ho we ver, comes at the cost of aving to infer individual parameters for each observed SN Ia, and o the computational burden scales with the data set size. While or current surv e ys and compilations of N \u223c 1000 SNe Ia this is easible with methods like Hamiltonian Monte Carlo (HMC) and ibbs sampling (provided the conditional independence of global arameters is not negated by e.g. selection effects), such methods ill not be applicable to larger data sets, e.g. due to systematic ncertainties which necessitate the inversion of large dense matrices, ncurring an O ( N 3 ) penalty even in the case of a Gaussian likelihood.\noreo v er, whereas light-curv e-summarizing techniques pro vide 1\u2013 0 parameters per object, modelling SEDs as in BayeSN requires 1000 parameters for each supernova, and so joint inference quickly ecomes impossible (e.g. Jones et al. 2022 and Mandel et al. 2022 nalyse only 79 SNe Ia).\nEven if computation were not an issue, all likelihood-based nalyses share one fundamental limitation: they need to be explicit n their probabilistic description of the data, including \u2018physical\u2019 intrinsic) properties of the supernovae and the environment (e.g. ust), their population-level distributions (which possibly evolve hrough time), instrumental effects and surv e y cross-calibration, the bservational selection process, and purity of the sample. This results n necessarily simplified descriptions, consisting predominantly of ormal distributions (out of computational convenience), and, where robabilities are intractable (or hard to compute, e.g. for selection ffects), in non-principled or ad hoc de-biasing procedures. As we emonstrate in Section 4 , in some cases, simplifications required to ender the model tractable (e.g. propagating photometric redshift unertainties linearly on to magnitudes) lead to biases in cosmological nference that only become apparent with large data samples.\nOn the other hand, the major modelling challenges \u2013 intrinsic ariability, dust properties, redshift uncertainty from photo-z, selecion effects, and contamination \u2013 are all relatively straightforward o simulate. Indeed, some recent analyses already employ forward imulations with the comprehensive SN AN A package (Kessler et al. 009a ) for parts of their workflow, e.g. for calibration (Burke et al. 018 ), inferring the distribution of dust properties (Popovic et al. 021a ) and possible correlations with features of the host (Popovic t al. 2021b ), and, most prominently, correcting for Malmquist bias Malmquist 1922 , 1925 ) in the derived distance moduli (see e.g. essler & Scolnic 2017 ). These studies use forms of approximate ayesian computation (ABC; see e.g. Sisson, Fan & Beaumont 018 ), an established technique belonging to the class of likelihoodree simulation-based inference (LF-SBI), which was also used di-\nectly for cosmological inference from SNe Ia by Weyant, Schafer & ood-Vasey ( 2013 ) and Jennings, Wolf & Sako ( 2016 ). ABC, howver, requires hand-crafted distance measures comparing simulation utput to the data or to (hand-crafted) summary representations and n abundance of simulations to produce accurate results.\nTogether with advances in computer science like automatic ifferentiation and more straightforward graphics processing unit GPU) utilization, which enabled the widespread adoption of neural etworks (NNs), recent years have seen the introduction of new NNased likelihood-free Bayesian analysis techniques (see Cranmer, rehmer & Louppe 2020 ; Lueckmann et al. 2021 , for re vie ws). n these methods, a stochastic simulator is used as an implicit epresentation of the likelihood, whose numerical e v aluation is o longer necessary. This simplifies the inference procedure and hifts the focus to constructing a realistic simulator: an often much asier feat than reverse-solving a Bayesian model. Importantly, it llows the simultaneous inclusion of all rele v ant processes influncing the data, including those like selection effects, for which probabilistic description is either intractable or computationally rohibitive. This work is meant as a first proof of concept that a likelihoodree methodology can deliver accurate and precise posteriors for osmological parameters from up to \u223c10 5 SNe Ia. We only use sumary statistics deri v able with SALT in the context of an intentionally imple BHM resembling BAHAMAS so as to enable verification of the ikelihood-free posteriors on mock data. We discuss our immediate lans towards a realistic simulator in Section 2.6 . We employ truncated marginal neural ratio estimation (TMNRE; iller et al. 2020 , 2022 ), a sequential implementation of the general eural ratio estimation (NRE) technique (Hermans, Begy & Louppe 020 ) that composes well with marginalization. It converts posterior nference into a classification problem and then solves it by training neural network on simulated data. The key to making the analysis calable even in the presence of numerous parameters describing he individual SNe Ia is that TMNRE only targets low-dimensional one- or two-dimensional in this work) marginal posteriors for he parameters of interest, rather than the joint , high-dimensional osterior as necessary in likelihood-based inference [including with arkov chain Monte Carlo (MCMC) and variational simulationased inference (SBI) techniques]. In addition, TMNRE iteratively efines the regions in parameter space from which training examples re drawn based on a given target observation, so as to maximize he simulator efficiency and utilization of the network\u2019s learning apacity.\nThis paper, a first example of Supernova Ia Cosmology with runcated marginal neural Ratio EsTimation (SICRET), is structured s follows. We present in Section 2 the Bayesian hierarchical model e use in this proof-of-concept paper and discuss the ways we plan o impro v e it in Section 2.6 . In Section 3 , we elaborate TMNRE, ncluding the truncation scheme used to zoom into regions of igh posterior density, and the network architecture we adopt. We emonstrate the inference procedure on mock data in Section 4 . In ection 4.1 , we infer marginally the cosmological parameters and how that TMNRE can derive accurate posteriors from 10 5 SNe Ia tilizing the full model complexity, whereas model simplifications ike linear uncertainty propagation, necessary for MCMC sampling, ay introduce a significant bias. In Section 4.2 , we show calibrated i.e. with e xact co v erage) confidence re gions for the cosmological arameters, derived using a procedure presented in Section 3.4 nd Appendix A . We also verify, in Section 4.3 , that the TMNRE osteriors are precise, i.e. the inference procedure is able to combine nformation from the full data set. Lastly, in Section 4.4 , we perform\nMNRAS 520, 1056\u20131072 (2023)\nM\ns s S\n2\nI a d S t I t\na T\n2\nW t s\nw \u2018 m ( o i e o\nH n\nw r I t h r t\nz\nw r s\n2\nT p w f\nM\nw a\n2\nT c\nm\nw m W p o d m\n2\nW l M d w g s a p h w\ni\n2\nT r\n1 The term \u2018correction\u2019 comes from the tradition of SN Ia standardization, which is a technique of reverse modelling. From a forward-modelling perspective, M s 0 is a \u2018noisy\u2019 realization (i.e. with scatter) of the brightness of the standard SN Ia ( x 1 = 0, c = 0). 2 This equation, strictly, applies only to bolometric magnitudes; otherwise, one needs to account for the redshifting of light in and out of the observed band, which is usually achieved through so-called K-corrections . Indeed, SN Ia magnitudes ( m ) are usually reported in the rest-frame B -band, and all the K -correction has already been applied during the analysis of light curves. 3 Regardless of the particular cosmological model, the luminosity distance, to which the distance modulus is directly related, can be expanded in a Taylor series, whose coefficients can be regarded as the \u2018cosmological parameters of interest\u2019. To second order, the expansion is (Weinberg 2008 , equation 1.4.9) d L = H \u22121 0 [ z + 1 2 (1 \u2212 q 0 ) z 2 + O ( z 3 )] , where H 0 is the Hubble\u2013Lema \u0302 \u0131tre constant, and q 0 is the deceleration parameter . Evidently, H \u22121 0 simply sets the absolute distance scale (i.e. the distance units) and is therefore degenerate with the absolute brightness scale of the SNe Ia (i.e. the luminosity units). Thus, the \u2018first-order\u2019 effect that can be derived from SN Ia data is that of q 0 . Indeed, in the posteriors presented in Section 4 , the best constrained direction is that corresponding to varying q 0 = m0 /2 \u2212 0 in CDM (Peebles 1993 , equation 13.7). 4 The interpretation of the so-called residual scatter , \u03c3res , is multifold: it could be taken to represent a scatter of the absolute magnitudes of SNe Ia or equi v alently an additional unaccounted-for observ ational noise (hence the label: residual after standardization).\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nimultaneous marginal inference of 10 5 latent parameters of the upernovae. We discuss our results and present our conclusions in ection 5 .\nM O D E L : BAH AMAS\nn this section, we present in detail the forward model used for our nalysis. It is implemented as a simulator that generates mock SN Ia ata, which is then used to train an inference network, as described in ection 3 . The simulator consists of layers: global parameters control he distributions of latent variables, which describe individual SNe a; finally, an instrument model produces observables ( data ) from he latent parameters.\nIn this proof-of-concept paper, we make a number of simplifying ssumptions, which will be relaxed in future works in order to apply MNRE to real data. We discuss them in Section 2.6 .\n.1 Obser v ables\ne focus on inference from processed multiband SN Ia light curves hat are summarized in the observed SALT parameters for each upernova s :\nd s = [ \u0302 m s , \u0302 xs 1 , \u0302 cs ] , (1) here x 1 and c are parameters describing the light-curve shape (its\nstretch\u2019 and \u2018colour\u2019, respectively), and m is the peak apparent agnitude of the superno va. F or N observed SNe Ia this is a lengthN \u00d7 3) vector, d , for which analysis pipelines usually provide an bservational covariance matrix \u02c6 of size 3 N \u00d7 3 N , which could nclude correlations between different SNe Ia due to systematic ffects and uncertainties in the SALT model. The sampling distribution f the data is, therefore, assumed Gaussian:\nd \u223c N ([ m , x 1 , c ] , \u02c6 ) . (2) ere m s , x s 1 , c\ns are the latent parameter values of a SN Ia, of which oisy estimates \u02c6 m s , \u02c6 xs 1 , \u02c6 c\ns are observed. Finally, each supernova has a latent cosmological redshift z s , of\nhich a measurement \u02c6 zs is made. We will not forward-simulate the edshift measurements; for that we will need a prior, related to SN a rates, as we discuss in Section 2.6.3 . Instead, we will assume that he observations from which \u0302 zs has been determined (e.g. multiband ost photometry) are independent of the SN Ia light-curve data and esult in a posterior for the latent redshift, whose form follows the oy model in Roberts et al. ( 2017 ): s | \u0302 zs \u223c N (\u02c6 zs , (1 + \u0302 zs ) 2 \u03c3 2 z ) , (3) here \u03c3 z is a global parameter (to be inferred) controlling the o v erall edshift variance. We will use this posterior as a prior and forwardimulate the latent redshift from it.\n.2 Superno v a model\nhe \u2018physical\u2019 model of SN Ia latent variables includes two comonents. Firstly, the light-curve parameters are linearly correlated ith the supernova\u2019s intrinsic magnitude, M s , via the so-called Tripp ormula (Phillips 1993 ; Tripp 1997 , 1998 ):\ns = M s 0 \u2212 \u03b1x s 1 + \u03b2c s , (4)\nNRAS 520, 1056\u20131072 (2023)\nhere \u03b1 and \u03b2 are the correction coefficients, global to all SNe Ia, nd M s 0 is the supernova\u2019s absolute magnitude post-corrections . 1\n.3 Cosmology\nhe second part of the model is the influence of cosmology, which onverts between absolute and apparent magnitudes: 2\ns = M s + \u03bc( C , z s ) (5) ith \u03bcs \u2261 \u03bc( C , z s ) the distance modulus of a supernova with cosological redshift z s under a cosmological model labelled with C . e will assume radiationless Lambda cold dark matter ( CDM), arametrized by C \u2261 { m 0 , 0 } : the present-day relative densities f matter and dark energy, respecti vely; ho we ver, our method is irectly applicable to any cosmological model for which the distance odulus can be calculated. 3\n.4 Prior model\ne build a Bayesian hierarchical model for supernova cosmology a\u0300 a BAHAMAS (Shariff et al. 2016a , b ) and assume normal priors on\ns 0 , x s 1 , and c s . The respective prior means ( M\u0304 0 , x\u0304 1 , c\u0304 ) and standard eviations 4 ( \u03c3res , R x 1 , R c ) are in turn global hyperparameters for hich (hyper)priors are assumed: normal for the means, an inverseamma distribution for the residual scatter, and log-normal for the tretch and colour variances. For the redshift uncertainty, \u03c3 2 z , we lso take an appropriate inverse-gamma prior. All SN Ia-related arameters, along with their priors, their hyperparameters, and their yperpriors are listed in Table 1 . Finally, for the parameters of CDM e use a uniform prior o v er [0; 2] \u22972 in m0 \u2013 0 space and use m0 = 0.3 and 0 = 0.7 for generating mock data. The full model s depicted as a directed acyclic graph in Fig. 1 .\n.5 Simulator configuration: mimicking Pantheon\nhe simulator requires two fixed inputs: the vector of observed edshifts, \u02c6 z, and the observational covariance, \u02c6 . We build these\nb t a t\n5\nt L V f o p\n( c i a\nt r m t f u c o c w C b i p c i\n2\nI i c t r a p O s t o\n2\nR e c b a t o t d t M w e b\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nased on the Pantheon compilation (Scolnic et al. 2018 ), so that he SNe Ia we simulate have a realistic distribution of redshifts 5 nd reasonably sized observational uncertainties, appropriate for he supernova\u2019s redshift. We select N supernovae from Pantheon\nWe note that future large surv e ys will have a different distribution of \u02c6 z han Pantheon, since their selection probabilities will be different (see e.g. SST Science Collaboration 2009 , fig. 11 .1, for the expectations for the era Rubin Observatory). And since the statistical power of a SN Ia sample or cosmological parameter inference depends strongly on this distribution, ur posteriors from 10 5 SNe Ia should be taken as an indication rather than rediction of possible future constraints.\no i\np s\n6\nt n\nwith replacement when N is larger than the compilation size) 6 and oncatenate their reported redshifts into \u02c6 z and stack diagonally their ndividual parameter covariance matrices (each of size 3 \u00d7 3) to form block-diagonal \u02c6 .\nCosmological analyses usually consider additionally dense staistical and systematic covariances (see e.g. Conley et al. 2011 ), epresenting, respectively, uncertainties in the underlying SN Ia odel ( SALT ) and the effect of e.g. instrument calibration. While he latter can be efficiently and rigorously replaced in an SBI ramew ork by forw ard-simulating the rele v ant systematics, model ncertainties can be propagated only by performing fits to light urves. Furthermore, having a dense \u02c6 would affect the performance f the simulator, in its current form, by introducing an O ( N 2\n) omputational complexity \u2013 and a corresponding memory footprint \u2013 hen sampling N mock SNe Ia from latent variables. The e xpensiv e holesky decomposition which this requires, ho we ver, only needs to e performed once at the beginning of the analysis if \u02c6 is fixed, i.e. nstrumental and SALT model parameters are not being varied. In this roof of concept we choose not to go beyond the block-diagonal data ovariance and discuss in Section 2.6.1 how we plan to address these ssues more naturally as part of future realistic forward simulators.\n.6 Room for impro v ement\nmpro v ements to the model which need to be addressed before t can be used for analysis of real data fall in several broad ate gories: re garding the nature of the analysed data, the effect of he SN Ia environment (properties of the host galaxy), measuring its edshift, object classification, and sample selection. Most of these re unwieldy in a likelihood-based framework either due to the resence of complicated distributions or their outright intractability. n the other hand, as long as they can be included in a forward imulator, none of these modifications impose fundamental changes o a likelihood-free inference (LFI) procedure. Specifically, TMNRE f the cosmological parameters is not affected in any way.\n.6.1 Intrinsic SN Ia model and observed data\naw SN Ia data is (exclusively, for the vast majority of SNe Ia xpected from large future surv e ys) in the form of light curves: a ollection of flux observations at different times and in different road spectral bands, and, to a lesser extent, spectra. To properly nalyse them, a model needs a representation of the intrinsic SED of he supernova. On the other hand, SALT parameters only summarize bserved properties of the light curves (and spectra), and it is clear hat describing the intrinsic SED with just the latent SALT parameters oes not capture its full complexity and the observed variations within he SN Ia population (Saunders et al. 2018 ; Mandel et al. 2022 ).\noreo v er, it has been found that the SN Ia population itself evolves ith cosmic time and that this can bias cosmological inference (Lee\nt al. 2020 ; Nicolas et al. 2021 ). This effect can be included in a BHM y allowing the global parameters ( M\u0304 0 , \u03c3 2 res , etc.) to be functions f redshift, represented e.g. as splines whose parameters are also nferred (see e.g. Rubin et al. 2015 ).\nLikelihood-based analyses rely on simple distributions and emirical prescriptions to capture these effects, whereas a forward imulator can more explicitly describe the underlying SEDs, while\nMNRAS 520, 1056\u20131072 (2023)\nNominally, the Pantheon data release contains 1048 SNe Ia, but for two of hem (\u2018 16232 \u2019 and \u2018 PTF10bjs \u2019) the reported parameter covariances are ot positive definite... so we only use 1046.\nM\na p F p o f b t a\n2\nA s m a h S f g 2 t i i p T b c\na p a T u g\n2\nA a e m t o e s A r o\ns o v C o a (\n7\ne (\nk H i I a\nt S W ( t t t L ( t\n2\nW t o h t c\nr H o c i s t b m 2 e v o\n2\nF p M S i f p n s d\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nv oiding the need to calculate a covariance matrix for the summary arameters and instead fully accounting for the SED uncertainties. inally, shifting the analysis towards raw flux measurements allows rincipled modelling of all instrumental effects: Poisson-distributed bserved photon counts, background and instrumental noise, light rom the host, and zero-point calibration, all of which are currently eing summarized in a simple Gaussian systematic uncertainty on he corrected magnitudes. In contrast, a forward simulation of data cquisition can be arbitrarily complex.\n.6.2 Environment\nwell-kno wn ef fect that is dif ficult to include on the level of ummary parameters, but easily applied to SED-based light-curve odels, is that of the SN Ia environment, commonly referred to s \u2018dust extinction\u2019, which is dictated by other properties of the ost galaxy like its mass and star-formation rate (SFR; Brout & colnic 2021 ). In that respect, the model also needs to account or variations in the extinction properties among and within host alaxies. Extinction within the Milky Way (Schlafly & Finkbeiner 011 ), which depends on the location of the SN Ia on the sky, needs o be taken into account as well. It should be noted that extinction s not the only \u2018effect\u2019 that host properties have on a supernovae: t is possible that type Ia SNe that come from different stellar opulations have different intrinsic properties (Childress et al. 2013 ). he locations of SNe Ia within their hosts have also been shown to e important (Hill et al. 2018 ), although there is little hope in having onstraints for them in large surv e ys.\nA forward simulator enables correlations between arbitrary host nd SN Ia properties to be included under the control of specialized arameters, in a manner similar to the Tripp formula, while still llowing for additional effects to be included as \u2018residual scatter\u2019. ogether with a forward model for host observations based on nderlying parameters like mass, SFR, etc., this opens the door to eneral simultaneous analysis of supernovae and their hosts.\n.6.3 Redshift\ns we will show in Section 4 , proper treatment of the redshift of SN Ia is crucial for obtaining unbiased cosmological posteriors, specially in the era of large-scale surv e ys that will disco v er far too any SNe Ia for spectroscopic follow-up. One will need to rely, hen, on estimates derived from multiband host photometry, which ften exhibit departures from normality and even multimodality (see .g. Leistedt, Mortlock & Peiris 2016 ; Linder & Mitra 2019 ), and o the Gaussian approximation in equation ( 3 ) will not be adequate. part from using a complicated likelihood, one must also place a edshift prior based on SN Ia rates, which come with their own set f parameters and are related to the cosmological model. Moreo v er, in this work, we assume that the cosmological redhift (which enters in the distance modulus calculation) is directly bservable. In reality, the observed light is affected also by a ariety of peculiar motions: of the host galaxy with respect to the MB frame, of the SN Ia itself inside the galaxy, and, similarly, f the Milky Way and Solar System (and Earth). 7 Whereas there re stringent constraints on the latter two from CMB anisotropies Planck Collaboration I 2020 ), the motions of f ar-aw ay galaxies are\nNRAS 520, 1056\u20131072 (2023)\nPapers in the literature have even considered the velocity of ejecta in a SN Ia xplosion (F ole y 2012 ) and a gravitational redshift due to density fluctuations Davis et al. 2011 ; Wojtak, Davis & Wiis 2015 ; Calcino & Davis 2017 ).\nr t t n c\nnown only on average, e.g. from bulk galaxy-flow models (Boruah, udson & Lavaux 2020 , 2021 ), whose intrinsic scatter must be taken nto account. Similarly to its position, the peculiar velocity of a SN a within its host is hardly observable and, therefore, a source of dditional peculiar-velocity scatter.\nFailing to model peculiar velocities has been shown to cause a bias o cosmological inference when the sample contains low-redshift Ne Ia (Wojtak et al. 2015 ; Linder & Mitra 2019 ; Huterer 2020 ). hile early studies, therefore, discarded SNe Ia with e.g. z 0.02 Kessler et al. 2009b ), more recent analyses usually correct redshifts o the CMB reference frame and add the various spreads in quadrature o form an uncertainty , or resort to high-dimensional Monte Carlo echniques (e.g. Gibbs or Hamiltonian MC; Rahman et al. 2022 ). F-SBI pro vides an e xplicit way to include all sources of redshift and the correlations they induce between different SNe Ia) without he need to explicitly infer O ( N ) additional parameters.\n.6.4 Contamination\nithout spectroscopic data for the supernova, one must consider he possibility of two types of contamination, namely, the presence f non-SN Ia events in the sample, and misidentification of the ost, from which the redshift is derived. Unaccounted for, any of hese effects can introduce a significant bias to the inference of osmological parameters (Roberts et al. 2017 ).\nContamination can be dealt with by explicitly calculating the odds atio of an event being a SN Ia or a contaminant (Kunz, Bassett & lozek 2007 ). A likelihood-free framework learns this implicitly by bserving that in simulations certain measurements (interpreted as ontamination) do not contribute information to the cosmological nference task, or, if they do, in what way they differ from type Ia upernovae. While SBI still relies on having an explicit model for he possible contaminants, these can be arbitrarily complex. Going eyond the simple assumption of a Gaussian dispersion of absolute agnitudes used in past analyses (Hlozek et al. 2012 ; Jones et al. 017 ), one can, e.g. use any non-Ia template from SN AN A (Kessler t al. 2009a ) or sophisticated data-driven models (Revsbech, Trotta & an Dyk 2018 ; Boone 2019 ), setting a priori probabilities on each bject\u2019s class during the forward run.\n.6.5 Selection effects\ninally, an assumption implicit in our hierarchical model is that the opulation-level distributions (i.e. the priors on the latent variables 0 , x 1 , c , and z) are appropriate for observed , as opposed to all Ne Ia in the Universe. That is, all probabilities in our model are mplicitly conditional on the SN having been observed and selected or inclusion in the data set. If one assumes the converse \u2013 that opulation distributions indeed reflect the whole population \u2013 one eeds to deal with selection effects , which in general modify the hape and normalization of downstream distributions in ways that epend on the cosmological and other global parameters and usually equire numerical or Monte Carlo integration in order to calculate he total selection probability at each step of a sampling chain. On he other hand, a simulator used for likelihood-free inference only eeds to faithfully recreate the surv e y strate gy and various quality uts applied during the acquisition and reduction of the real data.\n3 N\n3\nW h\np\nm w t d a a c e\nt c t p ( a b\no p m\nr\nw ( v s\na ( s n o s a a (\nr\nr s T t a r p S\nr l\n8\na I t p\n3\nI H t d r n t\nL\nw\na t p\n2\nw t l\n3\nI o p W w o e i\np { o o h\n3\nL r f a e a e\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nINFERENCE: TRUNCATED M A R G I NA L E U R A L RATIO ESTIMATION\n.1 Marginal ratio estimation\ne aim to infer from data d a subset of the parameters of a ierarchical Bayesian model, i.e. derive the posterior\n( | d ) = \u222b\np ( d | , Z) p ( , Z) d Z p ( d )\n(6)\narginalized o v er the nuisance 8 parameters Z . Often, the integral, hich describes the marginal likelihood of the data given only he parameters of interest, cannot be calculated due to the high imensionality of Z or because some of the distributions involved re inherently unkno wn. Ho we ver, if the model can be implemented s a forward simulator, i.e. if samples from the distributions involved an be drawn, the posterior of interest can be obtained via (neural) stimation of the likelihood-to-evidence ratio, as we now describe.\nRatio estimation reformulates Bayesian inference as a classificaion task, underpinned by two realizations. First, that a Bayes optimal lassifier (i.e. which minimizes the Bayesian risk of misclassificaion) trained to distinguish samples from two distributions p 1 ( x ) and 2 ( x ) must consider the ratio of their probability densities, p 1 ( x )/ p 2 ( x ) Devroye, Gy \u0308orfi & Lugosi 1996 ). Thus, even if the densities p 1 ( x ) nd p 2 ( x ) themselves are not known, one can approximate p 1 ( x )/ p 2 ( x ) y training a classifier on samples x 1, i \u223c p 1 ( x ) and x 2, i \u223c p 2 ( x ). The second realization is that Bayes\u2019 theorem equates three ratios f probability densities: the likelihood p ( d | ) to evidence p ( d ), the osterior p ( | d ) to prior p ( ), and the joint p ( , d ) to product of arginals p ( ) p ( d ):\n( , d ) \u2261 p( , d ) p( ) p( d ) = p( d | ) p( d ) = p( | d ) p( ) , (7)\nhich means that one can gain access to the posterior-to-prior ratio and subsequently use it for inference) by training a classifier of joint ersus marginal pairs, which are easily obtainable with a forward imulator.\nConcretely, joint pairs ( i , d i ) \u223c p ( , d ) \u2261 p 1 ( , d ) are generted by first sampling all parameters from the (hierarchical) prior , Z \u223c p( , Z )) and then generating mock observations with the imulator conditioned on the sampled values. Ignoring the values of uisance parameters from this point forward is the SBI equi v alent f the marginalization in equation ( 6 ). On the other hand, the ets of samples { i } and { d i } , taken separately, are distributed ccording to the respective mar ginals: the (mar ginal) prior p ( ) nd the evidence p ( d ), and so random pairing results in samples i , d j ) \u223c p ( ) p ( d ) \u2261 p 2 ( , d ).\nAs the last equality in equation ( 7 ) suggests, if the prior is tractable, ( , d ) gives direct access to the posterior density: p( | d ) = ( , d ) p( ). Alternatively, one can use r( , d ) to weight prior amples, so that they are re-interpreted as such from the posterior. he priors used in this work are simple and analytic, and we will, herefore, be e v aluating and analysing the marginal posteriors (which re at most 2D) on regular grids spanning the prior support in the espective parameter spaces, with the notable exception of the latent arameters, for which we use re-weighted samples, as discussed in ection 4.4 . Ratio estimation as a Bayesian inference technique does not equire actual classification of pairs ( , d ), i.e. assigning concrete abels to examples. Instead, the classifier is constructed in such a way\nSignifying, here, any hierarchic parameters that are deemed not of interest.\nl n o\ns to make an estimate of r( , d ) explicitly calculable (see below). n a typical classification task, the ratio will then be used to calculate he class probabilities: r /(1 + r ) = p 1 /( p 1 + p 2 ) and 1/(1 + r ) = 2 /( p 1 + p 2 ), based on which an example will be classified.\n.2 Neural ratio estimation\nn the practice of neural ratio estimation (NRE), as proposed by ermans et al. ( 2020 ), the classifier is a neural network (NN), dubbed he inference network , that takes as input a parameters-of-interest\u2013 ata pair and produces an estimate of the joint-to-marginal ratio: \u02c6 ( , d ), or, for numerical reasons, its logarithm ln \u0302 r ( , d ). The etwork parameters are optimized via stochastic gradient descent o minimize the binary cross-entropy:\n( ) = E p( , d) [ \u2212 ln \u02c6 r ( , d ) 1 + \u0302 r ( , d ) ]\n+ E p ( ) p ( d) [ \u2212 ln 1 1 + \u0302 r ( , d ) ] , (8)\nhich one can verify indeed results in \u02c6 r ( , d ) \u2192 r( , d ). During training, we estimate the loss using two joint pairs, ( 1 , d 1 )\nnd ( 2 , d 2 ), sampled independently of one another, intermingling hem to produce marginal pairs ( 1 , d 2 ) and ( 2 , d 1 ). Then, we erform the gradient descent on the loss\nl( ) \u2248 \u2212 [ ln \u03c3 ( ln \u0302 r ( 1 , d 1 )) + ln \u03c3 ( \u2212 ln \u0302 r ( 2 , d 1 )) ] \u2212 [ ln \u03c3 ( ln \u0302 r ( 2 , d 2 )) + ln \u03c3 ( \u2212 ln \u0302 r ( 1 , d 2 )) ] , (9)\nhere \u03c3 ( x ) \u2261 [1 + exp ( \u2212 x )] \u22121 is the sigmoid function. We exploit he simulator to continually produce training samples for online earning, i.e. we do not have fixed training, validation, etc. sets.\n.2.1 Parameter groups\nn scientific applications, one is often interested in a number of 1- r 2-dimensional marginal posteriors, so we might have multiple arameter subsets (groups) that we want to consider separately. e will differentiate groups of global parameters, labelling them ith \u03b8 i , from those consisting of a single object\u2019s latent parameter(s) f interest: \u03d1 s i . Usually, each parameter of interest will figure in xactly one group, but this is not required. The posteriors that we are nterested in are, then, the set { p ( \u03b8 i | d ) } i \u222a { p ( \u03d1 s i | d ) } i,s .\nConcretely, when we are inferring all global arameters, we will have 10 \u2018groups\u2019: { \u03b8 i } \u2261 [ m 0 , 0 ] , \u03c3z , \u03b1, \u03b2, M\u0304 0 , \u03c3res , \u0304x 1 , log 10 R x 1 , \u0304c , log 10 R c } , one f which is 2-dimensional, and when we are inferring cosmology nly, we will have only \u03b81 \u2261 [ m 0 , 0 ]. Finally, we will eventually ave \u03d1 s 1 \u2261 M s 0 .\n.2.2 Network ar chitectur e\nearning the parameter groups marginally means having separate atio estimators particular to each of them. This moti v ates a structure or the inference network split into a data pre-processor ( head ) nd multiple tails , containing a parameter pre-processor and a ratio stimator. At each step of training, we pre-processed the data once nd pass it as input to all tails. Then we e v aluate equation ( 9 ) with ach parameter group independently and combine the obtained osses before the gradient descent step. This means that the head etwork is updated based on the performance across all parameters f interest, so it needs to extract rele v ant information for all of them.\nMNRAS 520, 1056\u20131072 (2023)\nM\nn d t l\ne\nd\nw d\nS\nT s i o u\ni\nt c\ne a o ( a\nN p a p s\n3\nI p d v e r c b t w c n c c t\n9\no\na\na\nb e\ng b a f r\nS S a p i t e u c m o S l p b t\n3\nW l a p r m d n a o t s t\na b b l a s h S\n10 Thus, in Table 2 , we write the s -specific ratio estimator s s\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nBelow, we elaborate on the different components of the inference etwork. We list the particular implementations in Table 2 and epict the network graphically in Fig. 2 . We will omit for clarity he subscripts indicating dependence on network parameters, jointly abelled before.\nThe head network operates independently on the observations from ach supernova and produces a number of non-linear features for it:\ns = DataHead ( d s ) , (10) hich it then combines into a number of summary statistics that escribe the data set as a whole:\n= Summariser ([ d s ] N s= 1 ) . (11) he summariser encodes information on the statistics of the data et as a whole, which relate to the global model parameters. It s, therefore, important to preserv e an y ordering of the data, in ur case stemming from the measured redshifts and observational ncertainties, while simulating training data (see also Section 3.2.3 ). Each tail network first featurizes the parameter(s) of interest that t is responsible for:\nglobal: \u03b8 i = ParamHead \u03b8 i ( \u03b8 i ) , latent: \u03d1 s i = ParamHead \u03d1 i ( \u03d1 s i ) ,\n(12)\nhus possibly accounting for obvious degeneracies or (non-linear) ombinations that simplify the problem, e.g. q 0 .\nFinally, the (log-)ratio estimator combines the featurized paramter(s) with the output of the data head. Since global parameters re informed only by summary statistics, the respective network nly considers S , while for latent variables of a particular SN, the featurized) observational data relating to that object is also passed s input:\nglobal: ln \u0302 r( \u03b8 i , d ) = RatioEstimator \u03b8 i ( \u03b8 i , S ) , latent: ln \u0302 r( \u03d1 s i , d ) = RatioEstimator \u03d1 s i ( \u03d1 s i , S , d s ) .\n(13)\notice that the latent-variable ratio estimator needs to be arametrized by s , i.e. it needs to know which object\u2019s parameters re being inferred. Below, we discuss how we implement this arametrization so that we can infer all latent parameters with a ingle network.\n.2.3 Auxiliary input for latent-variable inference\nn our current model, the data [ d s ] N s= 1 are not invariant under ermutation of the supernovae because each has a different sampling istribution controlled by \u02c6 s and \u02c6 zs . Thus, for marginal latentariable ratio estimation, we either need to train a separate ratio stimator for each SN Ia or parametrize the s -dependence of the atio with an auxiliary label , a s , to identify which object is being onsidered. In principle, a s could be as simple as the index s itself, ut since we know what the ratio estimator is supposed to learn: he observ ational cov ariance and observed redshift of the supernova, e simplify its task by directly setting a s to a concatenation of the omponents of \u02c6 s\nand \u02c6 zs into a 3 \u00d7 3 + 1-dimensional vector. 9 We ote that \u02c6 and \u02c6 z are, in reality, derived from the observed light urves and spectra, just like \u02c6 m , \u02c6 x1 , and \u02c6 c. In our simplified model, onsidering them as part of the data, even though they are fixed in he simulator, makes each supernova fully identifiable.\nNRAS 520, 1056\u20131072 (2023)\nIn fact, for convenience of implementation, we provide the Cholesky factor f \u02c6 s , which has three constant null \u2013 instead of duplicated \u2013 entries.\nR a 1\nt i\nF or added e xpressivity, we first featurize the labels with an uxiliary head network similar to the data pre-processor:\ns = AuxiliaryHead ( a s ) , (14) efore appending them to the featurized data as input to the ratio stimator. 10\nFinally, we note that the marginal posterior of latent variables of a iven object depends, in general, on the observed data on all objects ecause of the possibility of correlations introduced by the likelihood nd/or a priori in the forward model. As mentioned, this is accounted or in the inference network by S being an input also to latent-variable atio estimators.\nIn our particular model, ho we ver, the data d s on each individual N Ia are independent since we take \u02c6 to be block-diagonal (see ection 2.5 ), and so a posteriori correlations between latent variables rise only because of the hierarchical structure: conditioned on articular values for the global parameters, the latent variables are ndeed a priori uncorrelated. Similarly, when in the last stages of runcation (described in Section 3.3 ) the priors of the global paramters are tightly constrained, the latent variables are approximately ncorrelated. In fact, truncating the prior ranges of global parameters an be viewed as an approximate way of accounting for the effect that easuring N \u2212 1 other SNe Ia has on inferring the parameters of each ne SN Ia \u2013 which is precisely the purpose of the data set summary, . Hence, in our experiments, including the connections from S to the atent-variable ratio estimator makes little difference to the inferred osteriors. We, instead, present results where these connections have een expunged, which simplifies greatly the network and hastens its raining.\n.2.4 NN implementation\ne implement all components of the inference network with multiayer perceptrons (MLPs), each of which contains a number of layers, s detailed in Table 2 . Each layer consists of a fully connected linear art whose output is whitened online, i.e. shifted by the mean and escaled by the standard deviation of hitherto-seen examples. 11 The ean and standard deviation of each layer\u2019s outputs o v er the training ata are, thus, learnt parameters updated at every training step (albeit ot with gradient descent) and then fixed during inference. Finally, rectified linear unit (ReLU) non-linearity is applied (except for the utput layer). We found that the choice of non-linearity did not affect he results and chose the other network hyperparameters (number and ize of layers) manually based on the final loss value and how fast he network learns in the initial stages of training.\nUsing only MLPs imposes restrictions on the simulator and the nalysed data. In particular, the data on individual objects must e fixed in size and the same across all of them so that it can e fed to a fully connected linear layer, whereas the cadence of ight curves typically varies from object to object. Similarly, using fully connected summarizing component requires the size of the imulated sample to be exactly the same throughout training and, ence, that the simulator be conditioned on the number of observed Ne Ia, but selection effects and differing levels of contamination\natioEstimator \u03d1 s i ( \u03d1 i , S , d ) from equation ( 13 ) as s -agnostic but with n auxiliary input: RatioEstimator \u03d1 i ( \u03d1 s i , S , d\ns , a s ). 1 This is a special case of \u2018batch normalization\u2019 (Ioffe & Szegedy 2015 ) from he NN literature with momentum set so as to calculate cumulative averages n an online-learning setting.\nTable 2. Details about the components of the inference network: their input and output dimensions and particular implementation in this work. For all components we use multilayer perceptrons (MLPs), for which we list the number and size of the hidden layers. Each hidden layer consists of a fully-connected layer, an online whitening step, and a rectified linear unit (ReLU) non-linearity. Inputs are also whitened online. The dimension of global parameter groups is denoted with m = 1, or 2 for \u03b81 \u2261 C . Note that in this work we did not provide the summary to the latent-variable ratio estimator, as motivated in Section 3.2.3 . The inference network is also depicted in Fig. 2 .\nComponent Inputs \u2208 Space \u2192 Output \u2208 Space Implementation DataHead d s \u2208 R 3 \u2192 d s \u2208 R 32 MLP(3 \u00d7 128) Summariser [ d s ] N s= 1 \u2208 R 32 \u00d7N \u2192 S \u2208 R 256 MLP(2 \u00d7 256) ParamHead \u03b8 i \u03b8 i \u2208 R m \u2192 \u03b8 i \u2208 R 256 MLP(2 \u00d7 256) RatioEstimator \u03b8 i \u03b8 i , S \u2208 R 256 + 256 \u2192 ln \u0302 r( \u03b8 i , d ) \u2208 R 1 MLP(3 \u00d7 256) ParamHead M 0 M s 0 \u2208 R 1 \u2192 M s 0 \u2208 R 1 Identity AuxiliaryHead \u02c6 s , \u0302 zs \u2208 R 9 + 1 \u2192 a s \u2208 R 16 MLP(3 \u00d7 128) RatioEstimator M 0 M s 0 , d s , a s \u2208 R 1 + 32 + 16 \u2192 ln \u0302 r( M s 0 , d ) \u2208 R 1 MLP(3 \u00d7 256)\nFigure 2. Inference network architecture as used for final inference, i.e. after constraining the non-cosmological global parameters. The solid lines represent linear connections, which inside the MLPs are followed by online whitening and a ReLU non-linearity. Inputs are also whitened. Dashed lines are an identity operation, duplicating a layer for presentation purposes. Note that in this work we expunge the connections from the summary to the latent-variable ratio estimator, as moti v ated in Section 3.2.3 , and do not pre-process (featurize) the latent M s 0 . Full details on the specific components, inputs, outputs, and intermediate variables are given in Table 2 .\nl n p\nt d t\n3\nT i p t t\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nead to a variable N in the simulator. Another consequence is that the etwork size scales with the data set, reaching about a billion trainable arameters and a memory footprint of \u223c10 GB for 10 5 SNe Ia. We have explored alternatives to MLPs that do not suffer from hese limitations; for example, 1D convolution along the SN Ia index imension, recurrent and set-based architectures, whose application o realistic data will be presented in future work.\ni\n.3 Truncated marginal ratio estimation\nhe key to practical ratio estimation is the ability of NNs to nterpolate the function they are trained to approximate. Thus, their erformance depends, on the one hand, on the amount of training he y receiv e, i.e. on how densely the parameter space is sampled o produce training examples, and on the other, on the network\u2019s ntrinsic capacity to represent the full complexity of the target. To\nMNRAS 520, 1056\u20131072 (2023)\nM\na m t t f m s D n s\ni p p ( m f r c\n3\nI i p v f h s s w\nl a i r l t t i g h\n3\nN c q l r p o c b\n1\n( b t u\nt t t s T c r\n3\nW o t a a d\n\u03b3\nw p\nS t w\n> f s\nf b q i o a\no c\ni e H t t t e o a t a o\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nlleviate the burden, Miller et al. ( 2020 , 2022 ) proposed truncated arginal neural ratio estimation (TMNRE), which focuses the raining on to regions of parameter space compatible with a specific arget data set by successively (in stages) restricting the prior range rom which training examples are generated. Unlike other sequential ethods that use a posterior estimate as proposal for generating ubsequent training data (Durkan, Papamakarios & Murray 2018 ; urkan, Murray & Papamakarios 2020 ), this truncation scheme does ot modify the shape of the prior distrib ution b ut merely restricts its upport (and hence introduces to it only a uniform re-normalization).\nAt each stage, the truncated (e xcluded) re gion \u2013 determined ndependently for each parameter group \u2013 is that in which the osterior density is deemed negligible. Specifically, we restrict the rior to a rectangular box enclosing the highest probability density HPD) region that contains 99.99 per cent (i.e. 1 \u2212 10 \u22124 ) probability ass from the current approximate posterior, q( | d ) (e v aluated or the target data). 12 After each truncation, a new network is andomly initialized and trained on samples generated with the newly onstrained priors.\n.3.1 Truncation and latent variables\nn principle, the priors of latent variables can also be truncated teratively, which would tailor the simulator output to the concrete roperties of the observed SNe Ia. Ho we ver, the effective priors of ariables in intermediate layers of a hierarchical model (resulting rom marginalizing the upstream model) are typically intractable or ard to sample from directly \u2013 this is precisely why the forward imulator is implemented in layers. Hence, sampling from a contrained latent-variable prior usually requires a rejection strategy, hose wastefulness increases the more the prior is truncated. In this work, we do not explicitly restrict the prior ranges of atent variables. Instead, we only train a ratio estimator for them fter we have constrained the global parameters as much as possible, .e. after the last truncation stage. This results in simulations which esemble the targeted data set only in terms of the distribution of atent variables rather than particular values. Naturally, this modifies he range and shape of the ef fecti ve latent-v ariable prior. Ho we ver, he modifications are restricted to regions where the joint posterior s negligible (and hence so are all the marginals): restricting the lobal parameters can be viewed as a way to enact constraints in igh dimensions.\n.4 Validation and calibration of amortized inference\neural ratio estimation is an amortized technique: after the upfront ost of training the ratio estimator, inference can be performed uickly with any number of data sets. Put otherwise, in contrast to ikelihood-based methods that learn the posterior for a given data set, atio estimation learns the procedure to derive a posterior, and this rocedure can be validated (in a Bayesian sense) with collections f simulated data and/or calibrated so as to produce (frequentist) onfidence regions with guaranteed exact coverage, as we explain elow.\nNRAS 520, 1056\u20131072 (2023)\n2 In one dimension, the box, i.e. a line segment, fits the HPD region exactly except for multimodal posteriors, which we do not encounter in this work), ut in higher dimensions, e.g. for the cosmological parameters C , which we reat as one 2-dimensional parameter, this means the new constrained region sually contains even more probability mass than required.\ni p\n1\n1\nt\nIn contrast to other sequential methods, inference using the runcation scheme described abo v e is still locally amortized since he target data (and the posterior associated with it) is used during raining only to determine the sequence of constraints in parameter pace, while training data is al w ays sampled according to the prior. herefore, the inference network used in each stage of truncation an, and must, be validated and/or calibrated within the constrained e gion o v er which it has been trained.\n.4.1 Bayesian and frequentist co vera g e (P\u2013P plots 13 )\ne wish to examine the empirical (i.e. determined from analyses f simulated data d ) co v erage properties of an inference procedure hat derives approximate posteriors q( | d ). To do this, we first ssociate with each parameter value 0 a credibility \u03b3 ( 0 , d ) as the pproximate posterior probability enclosed by the highest probability ensity (HPD) region which has 0 on its boundary:\n( 0 , d ) \u2261 \u222b\n( 0 , d ) q( | d ) d , (15)\nhere the integration is over the region where the approximate osterior density is higher than at 0 :\n( 0 , d ) \u2261 { : q( | d ) > q( 0 | d ) } . (16) ee also the top panel of Fig. 3 for an illustration. Taking 0 to be the\nrue parameters used to generate d , we then plot the frequency F ( \u03b3 ) ith which HPD regions of different credibility \u03b3 include (cover) 0 . If this empirical co v erage is larger than the credibility ( F ( \u03b3 ) \u03b3 ), the approximation is said to be conserv ati v e: it co v ers more requently than its credibility; if F ( \u03b3 ) < \u03b3 , on the other hand, it is aid to be underco v ering.\nIn practice, such a plot is built by repeatedly simulating data d rom parameters 0 , deriving q( | d ) with the inference procedure eing validated, determining from it the region ( 0 , d ) where ( | d ) > q( 0 | d ), and integrating q( | d ) o v er it, as illustrated n the top panel of Fig. 3 . The cumulative distribution of the \u03b3 ( 0 , d ) btained in this way gives the empirical coverage F ( \u03b3 ). Full details re presented in Appendix A .\nThere are two possible options for choosing the distribution f parameters 14 from which to simulate data in the first place, orresponding to Bayesian and frequentist approaches.\nBayesian validation through the empirical distribution of credbilities was discussed by Cook, Gelman & Rubin ( 2006 ), Talts t al. ( 2020 ), and more recently, in the context of LFI and NRE, by ermans et al. ( 2022 ). In this setting, the so-called self-consistency of he prior and data-averaged posterior (see Appendix A1 ) means that he empirical co v erage frequenc y of a set of credible regions matches heir credibility calculated with the exact posterior. Therefore, by xamining deviations from the diagonal line in a Bayesian P\u2013P plot, ne can determine whether the NRE posteriors q( | d ) are good pproximations to the exact posterior p ( | d ) and, if not, whether he inference procedure is conserv ati v e or underco v ering, on aver a g e cross the prior range. Still, F B ( \u03b3 ) = \u03b3 (i.e. a diagonal P\u2013P plot) is nly a necessary but not sufficient condition for q( | d ) \u2192 p ( | d ); ndeed, even q( | d ) = p ( ) has proper Bayesian coverage (i.e. the rior is self-consistent with the data-averaged prior ).\n3 See e.g. Gibbons & Chakraborti ( 2010 , section 4.8). 4 This concerns the parameters of interest. The rest are al w ays drawn from heir (conditional) priors.\nFigure 3. Procedure for obtaining frequentist confidence regions with exact co v erage from an approximate amortized Bayesian posterior. Top panel: the credibility \u03b3 ( 0 , d ) associated with a parameter value 0 is the credibility of the HPD region bounded by 0 for a given (approximate) posterior q , conditioned on data d . Middle panel: by repeated draws of d at fixed 0 , we obtain samples for \u03b3 ( 0 , d ), from which we build its empirical cumulative distribution, F f ( \u03b3 | 0 ) (red lines). The credibilities \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) required for regions to cover 0 with a given frequency \u02dc \u03b3 are indicated with green dots and determined as the \u02dc \u03b3 th quantiles of F f . We obtain the green line in the bottom panel by repeating this procedure on a grid of 0 . Bottom panel: C 0 ( d , \u02dc \u03b3 ), the region with confidence level \u02dc \u03b3 , is that in which \u03b3 ( 0 , d ) (red line) is lower than \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) (green line). For comparison, if q is equal to the true posterior, and a uniform prior is used, \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) is constant across all 0 and equal to the target confidence level (purple line). The credible region,\n0\n(\n\u03b3 \u22121 ( \u0303 \u03b3 , d ) , d\n)\n, then coincides with the confidence region.\ni i c n\ni a ( b P u i O h\nu o t o \u03b3 p e i A\n4\nI s i p m S f m s c a p d b w a l r d t w b w N u\ni a o a t d p\n4\nW 1 i s o s s s\n15 In the future, we plan to modify the network structure so as to accept arbitrary-size permutation-invariant data sets, which will make the inference network completely general.\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nOn the other hand, if data are simulated at fixed parameter values, .e. from p ( d | ), one is working in a frequentist setting. If the prior s uniform, credible regions of the marginal Bayesian posterior again o v er with frequency equal to their credibility, but this property is ot guaranteed a priori. In an y case, irrespectiv e of the prior used and even from an approxmation to the posterior, one can still derive exact confidence regions, s demonstrated by Dalmasso, Izbicki & Lee ( 2020 ), Dalmasso et al. 2022 ), and Masserano et al. ( 2022 ). While traditionally the focus has een on likelihood ratios as a test statistic, moti v ated by the Neyman\u2013 earson lemma, we consider \u03b3 ( 0 , d ) instead. If the prior in is niform, the two choices are equi v alent since \u03b3 ( 0 , d ) is monotonic n the posterior probability density and thence, in the likelihood. therwise, our procedure takes into account the effect that the prior as on posterior credibility and corrects for it implicitly.\nBuilding confidence sets from credibilities is then similar to the sual Neyman construction (Neyman & Jeffreys 1937 ): first, a series f \u2018frequentist\u2019 validation plots for different fixed parameters across he support of the prior are built; from them, one can derive a map f \u2018threshold credibilities\u2019, \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ), for any given confidence level \u02dc ; finally, for a particular data set d , the confidence region includes arameter values that attain in q( | d ) a lower credibility (i.e. a less xtreme test statistic) than the threshold. The procedure is illustrated n the middle and bottom panels of Fig. 3 and fully detailed in ppendix A2 .\nEXPERI MENTS A N D RESULTS\nn this section, we apply our inference procedure to mock data imulated by the forward model described in Section 2 and depicted n Fig. 1 . Even though the likelihood in this case is known, obtaining osteriors for the cosmological parameters of interest with traditional ethods would in general require explicitly sampling all individual N Ia parameters, which is impractical for the \u223c 10 5 SNe Ia expected rom future surv e ys. Instead, one might attempt to analytically arginalize the latent variables in order to reduce the parameter pace to just the O (10) global parameters. This is possible for our urrent model, in which the latent-variable priors and the likelihood re normal, with the only necessary approximation being a linear ropagation of the redshift uncertainties to the distance modulus, as etailed in Appendix B . Ho we ver, this simplification can significantly ias inference of the cosmological parameters (see Fig. 4 ). The bias, hich increases in severity for larger sample sizes, arises due to bundance in the sample of supernovae at low redshift, where the noninearity of the distance modulus is most pronounced: using an incorect likelihood then fa v ours a different shape of \u03bc( C , z) and, hence, ifferent cosmological parameters. Furthermore, propagating uncerainties only linearly artificially enlarges the expected variability, hich, on the other hand, leads to o v erconfident results (evidenced y the tightly constraining, yet biased, MCMC posteriors in Fig. 4 ) hen these variations are not present in the data. In contrast, marginal RE places no restrictions on the model and, therefore, produces nbiased posteriors with correct uncertainties, as shown in Fig. 4 . In the following, we first elaborate on the application of TMNRE to nfer global model parameters, validate the approximate posteriors, nd produce calibrated frequentist confidence regions in the space f cosmological parameters. We then validate the NRE posteriors gainst MCMC results for mock data generated with the simplified ractable model, which, ho we ver, is a poor description of real ata. Finally, we demonstrate high-dimensional NRE on the latent arameters, specifically, on the intrinsic magnitudes M s 0 .\n.1 Inferring global parameters with iterati v e truncation\ne apply TMNRE to mock data sets containing between 10 3 and 0 5 SNe Ia, all sharing the same global parameter values listed n Table 1 but with different latent parameters. For each sample ize we also generate a collection of observed redshifts, \u02c6 z, and bserv ational cov ariance \u02c6 as described in Section 2.5 . For each imulator configuration (uniquely defined by { N, \u0302 z, \u02c6 } ), we train eparate inference networks. 15 The analysis proceeds in stages , tarting with the priors listed in Table 1 and truncating them as\nMNRAS 520, 1056\u20131072 (2023)\nM\nFigur e 4. Mar ginal posteriors for cosmological parameters (90 per cent credible regions) for increasing SN Ia sample size from TMNRE (with the full generative model) in green and from MCMC (with a linearized model, required to make the problem tractable for sampling methods) in red. A star marks the values used to simulate mock data.\nFigure 5. Approximate posterior density and 68 per cent and 95 per cent HPD contours for the cosmological parameters of interest at sequential stages of truncation in the analysis of 10 5 SNe Ia. The rectangular boxes delineate the selected range for the following stage. The prior used is al w ays flat across the depicted range, and green lines denote the values used to produce the mock data.\nd t f\ni s r s ( t 1 A t d\nS a d p i p p l e c\nfi c d\n4\nW a 5 m p t T p w a\na t F 6 p a a p t a c t a\n4\nI\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nescribed in Section 3.3 , targeting the mock observation. We stop runcating when none of the parameter ranges shrink by more than a actor of 2. For training we use the Adam optimizer (Kingma & Ba 2017 ) with nitial learning rate of 10 \u22124 , decreasing it by a factor of 2 every 2000 teps for a total of 10 000 steps. We do not use a fixed training set but ather produce new examples on-the-fly in mini-batches of size 64, o that the total number of simulations used to train a given network i.e. the estimator for a given sta g e of truncation ) is 640 000. New raining examples are generated at each stage. Training in the case of 0 5 SNe Ia took \u22482 h per stage (around 12 h in total) on an NVIDIA -100 GPU. The run time is dominated by the simulator, whereas he memory footprint is largely due to the network, specifically, the ense summarizing layer. The results are illustrated in Figs 5 and 6 for the data set with 10 5\nNe Ia. Often, at first not all parameters are constrained: e.g. \u03b1 and \u03b2 re learnt appreciably only after two truncations, when the training ata variance is reduced enough by constraining the rest of the arameters. On the other hand, the obvious approximate de generac y n the cosmological parameters ( q 0 = m0 /2 \u2212 0 ) is immediately icked up by the network, as indicated by the narrow strip in the first anel of Fig. 5 . Note that the posteriors in the initial stages are much arger than later ones even though training has largely converged (as videnced by the loss stabilizing): this is due to the limited network apacity and demonstrates the need and utility of truncation.\nThe final cosmological posterior we obtain by training with the nal truncated priors for all global parameters but only inferring the osmological parameters in order to utilize the full flexibility of the ata pre-processor network. This same procedure was used for all the NRE posteriors in Fig. 4 .\nNRAS 520, 1056\u20131072 (2023)\n\u2018\n.2 Validation and calibration\ne validate the posteriors from the last truncation stage in the nalysis of 10 5 SNe Ia, i.e. those depicted in the last panels of Figs and 6 , using a Bayesian P\u2013P plot shown in Fig. 7 . We use 4096 ock data sets, which the trained NN analysed in \u22481 min. For all arameter groups the empirical co v erage is close to the credibility of he posterior approximation, as indicated by the nearly diagonal lines. he biggest deviations, of up to 5 per cent, are in the cosmological arameters, which is to be expected since it is the only 2D posterior e are inferring. This provides further moti v ation for including an dditional training phase targeting e xclusiv ely cosmology. We also validate the final cosmological posterior approximation t fixed parameter values. The P\u2013P plots from a grid across the final runcated prior (the box in the last panel of Fig. 5 ) are shown in ig. A1 , and the full map of the credibility required to co v er with 8.4 per cent confidence is shown in Fig. 8 b. Using this map and the rocedure described in Section 3.4.1 and Appendix A2 we derive nd show in Fig. 8 a confidence regions with exact coverage from the pproximate posteriors for three different data sets generated with arameters randomly drawn from the (truncated) priors. We notice hat, even though the approximation is in some regions conserv ati ve nd in others underco v ering by a few tens per cent (for 68 per cent o v erage), this is actually the result of small inaccuracies in the size of he inferred posterior, which would not affect scientific conclusions nd can in any case be calibrated away.\n.3 Validation of NRE posteriors on a tractable example\nn this subsection, we test the NRE inference procedure against ground-truth\u2019 posteriors obtained with MCMC. In order to make\nFigure 6. Approximate posteriors (blue solid lines) for the non-cosmological global parameters at sequential stages of truncation (each row is a stage) in the analysis of 10 5 SNe Ia. The dashed lines show the prior density (the same across stages), which gets truncated to the unshaded region for the following stage. In all plots the green vertical line denotes the value used to produce the target mock observations (from Table 1 ), and the approximate posteriors and priors are independently arbitrarily normalized to aid presentation.\nFigure 7. Bayesian P\u2013P plots for the 10 marginal posteriors inferred from 10 5 SNe Ia in the last stage of truncation. (The posteriors themselves e v aluated on the target mock observations are shown in Figs 5 and 6 .) Note that the axes are switched with respect to other works (e.g. Hermans et al. 2022 ).\nt o A o 1 m s p\nm t p\n( ( w\nt o t 5 i e\no d s p d t s e s B s a\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nhe latter numerically tractable, we marginalize out the latent layer f BAHAMAS and linearize redshift error propagation, as described in ppendix B . From this simplified model, we generate a number f data sets, varying the number of SNe Ia between N = 10 3 \u2013 0 5 , spanning the range from current to near-future surv e ys. We imic spectroscopic observations by setting \u03c3 z = 0 (labelled pecz ) in addition to data sets with photometric redshift errors ropagated linearly ( \u03c3 z = 0.04, a variable to be inferred, labelled\nphotoz ). In each case, we generate 10 data realizations with he same global parameters (from Table 1 ) but different latent arameters. We perform the MCMC analysis using the EMCEE package F oreman-Macke y et al. 2013 ), sampling all 11 global parameters 10 for specz ) for 1000 steps (discarding the first 200 as burn-in) ith 50 chains. For each sample size N and data type ( specz or mphotoz ) we hen train a neural ratio estimator for the cosmological parameters nly. We imitate the last stage of iterative truncation by constraining he prior of each global parameter so that it contains (to within \u03c3 ) the MCMC posteriors from all 10 analysed data sets. Once the nference network is trained, the 10 NRE posteriors can easily be v aluated.\nWe present a selection of posteriors from 10 5 SNe Ia in Fig. 9 . We bserve that NRE posteriors are slightly larger than and slightly isplaced from the MCMC \u2018ground truth\u2019, although there is no ystematic bias. The displacement appears to be larger when the true arameters fall in the tail of the true posterior, i.e. for more \u2018unlikely\u2019 ata, of which the inference network has seen fewer examples during raining. Still, examining the Bayesian P\u2013P plots, which are very imilar to Fig. 7 , reveals a slight underco v erage, which means that ven though the approximate posteriors are slightly larger, they are cattered around the true values. We also remind the reader that ayesian P\u2013P plots indicate performance across the whole parameter pace, while in Fig. 9 we only show posteriors on data generated from single set of parameter values.\nMNRAS 520, 1056\u20131072 (2023)\nM\ncredibility (approximate) confidence (exact)\nFigure 8. (a) Calibrated constraints from 10 5 SNe Ia with different input cosmological parameters (indicated by a star). Red contours delineate the 68 per cent and 95 per cent (approximate) credible regions from the last stage of TMNRE (whose constrained prior range is the extent of the plots), while the respective calibrated (exact) confidence regions are shown in green. (b) Credibility of the approximate posterior required to co v er the true value for 68.4 per cent of data realizations (in a frequentist sense, i.e. at fixed values of the cosmological parameters according to the location of the pixel).\nFigure 9. Comparison of posteriors for the cosmological parameters (68 per cent and 95 per cent HPD credible regions) from NRE (blue contours) and MCMC (green filled areas) for 10 5 SNe Ia simulated with the linearized marginal BAHAMAS model (see Appendix B ), for which the MCMC solution is taken as the ground truth. On the top row are five different mock data realizations with spectroscopic redshifts ( \u03c3z = 0), and on the bottom with \u03c3z = 0.04. Note the different scales for the two rows. A star shows the true parameters used to generate the mock data.\no W M S p\n1\nc t E c t s\ns i s o p n\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nFinally, in Fig. 10 , we examine how the precision and accuracy 16\nf NRE posteriors behave across the range of SN Ia sample sizes. e confirm that NRE posteriors are consistently larger than their CMC counterparts by up to a few tens per cent per parameter.\ntill, both MCMC and NRE posterior sizes clearly scale as 1 / \u221a N er parameter, while the offset of the mean is proportional to the\nNRAS 520, 1056\u20131072 (2023)\n6 We measure precision through the determinant of the approximate-posterior o variance: size \u2261 \u221a |\u3008 C C T \u3009| , and accurac y with the offset of the mean from he ground-truth: bias 2 \u2261 |\u3008 C \u3009 \u2212 C true | 2 , where av erages are o v er q( C | d ). ven though these two quantities have the same dimensions for our 2D osmological posteriors, we caution against comparing them directly since he former represents a volume in parameter space, while the latter is the quare of a length.\n4\nT t a p a\ntandard deviation for both MCMC and NRE analyses, as expected n a purely Gaussian model. This means that the inference network ucceeds in combining the information from the large number of bserv ed objects. Ov erall, NRE achiev es comparable accurac y and recision to MCMC across the sample sizes considered, and we do ot observe signs of bias.\n.4 Inference for individual SNe Ia\no infer latent parameters with NRE, one simply needs to designate hem as parameters of interest and feed them in the ratio estimator, s described in Sections 3.2.2 and 3.2.3 . Ho we ver, since the ef fecti ve rior of latent variables is only defined implicitly through the hierrchical model, it is intractable, and so the posterior density cannot\nb ( u b\no t t v\nN \u03c3 c p c t l F N\nv f t d\n1\nt\nh i\n5\nW w m s s p r a t s d s s\nb u f S a i a c d s o i\nnloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\ne directly e v aluated numerically. Instead, we use simulator samples the ones used in training) to represent the prior and re-weight them sing the neural ratio estimate. We can then calculate moments or uild histograms to represent the marginal NRE posteriors. We do not iteratively constrain the latent parameters and instead nly infer them after the priors of the global parameters have been runcated. As discussed in Section 3.2.3 , this allows us to simplify he inference network and not feed data set summaries into the latentariable ratio estimator.\nWe present marginal results for the intrinsic magnitudes, M s 0 , of = 10 5 SNe Ia simulated with spectroscopic redshifts (i.e. setting z = 0) since data with photometric redshift uncertainty is too weakly onstraining in the parameters of an individual SN Ia: the marginal osteriors nearly coincide with the constrained ef fecti ve prior. In this ase, as described in Appendices B and B1 , we can also perform he global inference with MCMC and then use the samples to obtain atent-variable posteriors, which we consider as ground truth. In ig. 11 , we compare their moments 17 to moments of the marginal RE posteriors. NRE learns to correctly identify the location and size of the latentariable posteriors. There are small deviations from the ground truth or extreme values: where the posterior (and thence, the data) falls in he tail of the prior, and so the network has seen few similar examples uring training, and when the posterior is tightly constraining (i.e.\n7 The true posteriors in this case are Gaussian, and we have verified visually hat the NRE posteriors resemble Gaussians.\nf\nt n\nas a small standard deviation). In the latter case, the NRE estimate s again conserv ati ve.\nDI SCUSSI ON A N D C O N C L U S I O N S\ne have presented a proof of concept for Supernova Ia Cosmology ith truncated marginal neural Ratio EsTimation (SICRET). The ain advantages of this modern likelihood-free method are that it\ncales to very large data sets (in this paper we analysed up to 10 5 imulated SNe Ia), and that it allows seamless inclusion of arbitrary hysical, statistical, and observ ational ef fects in the analysis. By eformulating Bayesian inference as a classification task, TMNRE llows a neural network trained on simulated data to implicitly take hese effects into account and directly produce results for the limited et of parameters of interest: in our case, the matter and dark-energy ensities. Meanwhile, an iterative truncation procedure tailors the imulator so that training data resembles the analysed (target) data et, which impro v es the network\u2019s learning.\nAfter demonstrating the systematic bias that can be introduced y o v ersimplifying aspects of the data-generation process, e.g. ncertainties in measured redshift, we have presented posteriors or cosmological parameters derived with TMNRE from 10 5 mock Ne Ia simulated from the BAHAMAS model. Exploiting the local mortization of TMNRE inference, we have validated the approxmate posteriors for appropriate Bayesian co v erage and presented procedure to derive confidence regions with exact frequentist o v erage. Furthermore, we hav e v erified the TMNRE posteriors irectly by comparing them with the \u2018ground-truth\u2019 obtained with tandard MCMC on a simplified model (with spectroscopic redshifts r redshift uncertainty propagated linearly). We have shown that the nference network is able to extract all the rele v ant information even rom large numbers of SNe Ia.\nMoreo v er, we hav e presented a method to simultaneously infer he latent parameters of all 10 5 SNe Ia with a single small neural etwork that takes advantage of the truncation of global-parameter\nMNRAS 520, 1056\u20131072 (2023)\nM\np p\nw s s m d a t p i s\nh H p\nA\nW G r s w P a d\nD\nT r\nR\nA B B B B B B B B B C C C C C D\nD\nD D\n1\n1\n2\nD D D\nF F F\nG\nG\nG G G H\nH\nH H H H H H I\nI J J J J K K K K K K K L L L L L\nM M M M\nM M\nM\nM\nM\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nrior ranges. We have verified the results for the intrinsic magnitude arameter in the simplified case against MCMC. The likelihood-free simulator-based framework we have described ill allow us to impro v e the simulator in the future, so that it is uitable for real data, without modifying the inference procedure, and cale it to the output of upcoming surv e ys like LSST. This will include odelling full SN Ia light curves using an underlying spectral energy istribution and a realistic instrumental model (e.g. calibration), ccounting for selection effects, the influence of the environment hrough dust extinction, and the possibility of correlations with host roperties, considering all contributions to the observed redshift and ts uncertainty and scatter, and finally, non-SN Ia contaminants in the ample.\nFuture developments of the technique presented here might also elp in shedding light on the mysterious tension in the value of the ubble\u2013Lema \u0302 \u0131tre constant obtained from high- versus low-redshift robes.\nC K N OW L E D G E M E N T S\ne would like to thank David van Dyk, Davide Piras, and Sebastian oldt for useful comments and discussions and the anonymous eferee for a careful read and insightful comments. The following oftware were used in this research: CLIPPPY , 18 a probabilistic frameork based on PYRO (Bingham et al. 2019 ), for the generative model; HYTORCH 19 for cosmological distance calculations (with gradients); nd PYTORCH (Paszke et al. 2019 ) and PYTORCH LIGHTNING 20 for efining and training the neural networks.\nATA AVAILABILITY\nhe mock data underlying this article will be shared on reasonable equest to the corresponding author.\nE FERENCES\nbbott T. M. C. et al., 2019, ApJ , 872, L30 etoule M. et al., 2014, A&A , 568, A22 ingham E. et al., 2019, J. Mach. Learn. Res., 20, 973 oone K., 2019, AJ , 158, 257 oruah S. S., Hudson M. J., Lavaux G., 2020, MNRAS , 498, 2703 oruah S. S., Hudson M. J., Lavaux G., 2021, MNRAS , 507, 2697 rout D., Scolnic D., 2021, ApJ , 909, 26 rout D. et al., 2019, ApJ , 874, 106 rout D. et al., 2022, ApJ , 938, 110 urke D. L. et al., 2018, AJ , 155, 41 alcino J., Davis T., 2017, J. Cosmol. Astropart. Phys., 2017, 038 hildress M. et al., 2013, ApJ , 770, 108 onley A. et al., 2011, ApJS , 192, 1 ook S. R., Gelman A., Rubin D. B., 2006, J. Comput. Graph. Stat., 15, 675 ranmer K., Brehmer J., Louppe G., 2020, Proc. Natl. Acad. Sci. , 117, 30055 almasso N., Izbicki R., Lee A., 2020, Proc. 37th Int. Conf. Mach.\nLearn., Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting. PMLR, Cambridge, MA, p. 2323 almasso N., Masserano L., Zhao D., Izbicki R., Lee A. B., 2022, preprint ( arXiv:2107.03920 ) avis T. M. et al., 2011, ApJ , 741, 67 evroye L., Gy \u0308orfi L., Lugosi G., 1996, A Probabilistic Theory of Pattern\nRecognition, corrected edition. Springer, New York\nNRAS 520, 1056\u20131072 (2023)\n8 https:// github.com/kosiokarchev/ clipppy 9 https:// github.com/kosiokarchev/ phytorch 0 ht tps://www.pytorchlight ning.ai/\nM\nM\ni Valentino E. et al., 2021, Class. Quantum Gravity , 38, 153001 urkan C., Papamakarios G., Murray I., 2018, preprint ( arXiv:1811.08723 ) urkan C., Murray I., Papamakarios G., 2020, Proc. 37th Int. Conf. Mach.\nLearn., On Contrastive Learning for Likelihood-Free Inference. PMLR, Cambridge, MA, p. 2771 ole y R. J., 2012, ApJ , 748, 127 ole y R. J. et al., 2018, MNRAS , 475, 193 oreman-Macke y D., Hogg D. W., Lang D., Goodman J., 2013, PASP , 125,\n306 ardner J. R., Pleiss G., Bindel D., Weinberger K. Q., Wilson A. G., 2021,\npreprint ( arXiv:1809.11165 ) ibbons J. D., Chakraborti S., 2010, Nonparametric Statistical Inference, 5th\nedn. Chapman and Hall/CRC, Boca Raton uy J., Astier P., Nobili S., Regnault N., Pain R., 2005, A&A , 443, 781 uy J. et al., 2007, A&A , 466, 11 uy J. et al., 2010, A&A , 523, A7 ermans J., Begy V., Louppe G., 2020, Proc. 37th Int. Conf. Mach. Learn.,\nICML\u201920, Likelihood-free MCMC with amortized approximate ratio estimators. JMLR, Lille, France, p. 4239 ermans J., Delaunoy A., Rozet F ., W ehenkel A., Begy V., Louppe G., 2022, Trans. Mach. Learn. Res. icken M. et al., 2009, ApJ , 700, 331 icken M. et al., 2012, ApJS , 200, 12 ill R. et al., 2018, MNRAS , 481, 2766 inton S. R. et al., 2019, ApJ , 876, 15 lozek R. et al., 2012, ApJ , 752, 79 uterer D., 2020, ApJ , 904, L28 offe S., Szegedy C., 2015, Proc. 32nd Int. Conf. Int. Conf. Mach. Learn. Vol. 37, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. JMLR, Lille, France, p. 448 vezi \u0301c Z\u030c. et al., 2019, ApJ , 873, 111 ennings E., Wolf R., Sako M., 2016, preprint ( arXiv:1611.03087 ) ha S. et al., 2006, AJ , 131, 527 ones D. O. et al., 2017, ApJ , 843, 6 ones D. O. et al., 2022, ApJ , 933, 172 enworthy W. D. et al., 2021, ApJ , 923, 265 essler R., Scolnic D., 2017, ApJ , 836, 56 essler R. et al., 2009a, PASP , 121, 1028 essler R. et al., 2009b, ApJS , 185, 32 ingma D. P., Ba J., 2017, preprint ( arXiv:1412.6980 ) risciunas K. et al., 2017, AJ , 154, 211 unz M., Bassett B. A., Hlozek R. A., 2007, Phys. Rev. D , 75, 103508 ee Y.-W., Chung C., Kang Y., Jee M. J., 2020, ApJ , 903, 22 eistedt B., Mortlock D. J., Peiris H. V., 2016, MNRAS , 460, 4258 inder E. V., Mitra A., 2019, Phys. Rev. D , 100, 043542 SST Science Collaboration, 2009, preprint ( arXiv:0912.0201 ) ueckmann J.-M., Boelts J., Greenberg D., Goncalves P., Macke J., 2021,\nProc. 24th Int. Conf. Artif. Intell. Stat., Benchmarking Simulation-Based Inference. PMLR, Cambridge, MA, p. 343 a C., Corasaniti P.-S., Bassett B. A., 2016, MNRAS , 463, 1651 almquist K. G., 1922, Medd. Fran Lunds Astron. Obs. Ser. I, 100, 1 almquist K. G., 1925, Medd. Fran Lunds Astron. Obs. Ser. I, 106, 1 andel K. S., Wood-Vasey W. M., Friedman A. S., Kirshner R. P., 2009, ApJ ,\n704, 629 andel K. S., Narayan G., Kirshner R. P., 2011, ApJ , 731, 120 andel K. S., Scolnic D. M., Shariff H., F ole y R. J., Kirshner R. P., 2017,\nApJ , 842, 93 andel K. S., Thorp S., Narayan G., Friedman A. S., Avelino A., 2022,\nMNRAS , 510, 3939 arch M. C., Trotta R., Berkes P., Starkman G. D., Vaudre v ange P. M., 2011,\nMNRAS , 418, 2308 asserano L., Dorigo T., Izbicki R., Kuusela M., Lee A. B., 2022, preprint\n( arXiv:2205.15680 ) iller B. K., Cole A., Louppe G., Weniger C., 2020, preprint\n( arXiv:2011.13951 ) iller B. K., Cole A., Forr \u0301e P., Louppe G., Weniger C., 2022, J. Open Source\nSoftw., 7, 4205\nN\nN P\nP\nP P P P P P\nP P P P R\nR R R\nR S S S S S S\nS S S\nT\nT\nT T W\nW W\nA\nA \u03b3 t o p \u2018 s w a a r\nA\nI t\nFigure A1. Frequentist P\u2013P plots; i.e. at fixed cosmology (cf. the middle panel of Fig. 3 ), created with the final cosmology-only posterior approximation for 10 5 SNe Ia. Each panel corresponds to a pixel in the map of Fig. 8 b with cosmological parameter values indicated at the top and right edges (only some pixels are shown). The colour values in Fig. 8 b correspond to the credibilities indicated with a horizontal dashed line: those that achieve an empirical co v erage of 68.4 per cent (vertical dashed line).\n(\np\na i\nF\nw b c o a\nF\nw\nc F e w t o\n21 Note that this arises only in the so-called Bayesian optimal setting, in which the posterior is derived using the model used to generate the analysed data, and it is crucial to average the credibilities over the exact joint model.\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\ne yman J., Jeffre ys H., 1937, Philos. Trans. R. Soc. Lond. Ser. Math. Phys. Sci. , 236, 333 icolas N. et al., 2021, A&A , 649, A74 aszke A. et al., 2019, in Wallach H., Larochelle H., Beygelzimer A.,\nd\u2019Alch \u0301e-Buc F., Fox E., Garnett R., eds, Advances in Neural Information Processing Systems 32. Curran Associates, Inc., Canada, p. 8024 eebles P. J. E., 1993, Principles of Physical Cosmology. Princeton Univ. Press, Princeton, NJ erlmutter S. et al., 1997, ApJ , 483, 565 erlmutter S. et al., 1999, ApJ , 517, 565 hillips M. M., 1993, ApJ , 413, L105 hillips M. M. et al., 2019, PASP , 131, 014001 lanck Collaboration I, 2020, A&A , 641, A1 opovic B., Brout D., Kessler R., Scolnic D., 2021a, preprint\n( arXiv:2112.04456 ) opovic B., Brout D., Kessler R., Scolnic D., Lu L., 2021b, ApJ , 913, 49 skovskii Y. P., 1967, Sov. Astron., 11, 63 skovskii I. P., 1977, Sov. Astron., 21, 675 skovskii Y. P., 1984, Sov. Astron., 28, 658 ahman W., Trotta R., Boruah S. S., Hudson M. J., van Dyk D. A., 2022,\nMNRAS , 514, 139 evsbech E. A., Trotta R., van Dyk D. A., 2018, MNRAS , 473, 3969 iess A. G. et al., 1998, AJ , 116, 1009 oberts E., Lochner M., Fonseca J., Bassett B. A., Lablanche P.-Y., Agarwal\nS., 2017, J. Cosmol. Astropart. Phys., 2017, 036 ubin D. et al., 2015, ApJ , 813, 137 ako M. et al., 2018, PASP , 130, 064002 aunders C. et al., 2018, ApJ , 869, 167 chlafly E. F., Finkbeiner D. P., 2011, ApJ , 737, 103 colnic D. M. et al., 2018, ApJ , 859, 101 colnic D. et al., 2022, ApJ , 938, 113 hariff H., Dhawan S., Jiao X., Leibundgut B., Trotta R., van Dyk D. A.,\n2016a, MNRAS , 463, 4311 hariff H., Jiao X., Trotta R., van Dyk D. A., 2016b, ApJ , 827, 1 ilverman J. M. et al., 2012, MNRAS , 425, 1789 isson S., Fan Y., Beaumont M., 2018, Handbook of Approximate Bayesian\nComputation, 1st edn. Chapman and Hall/CRC alts S., Betancourt M., Simpson D., Vehtari A., Gelman A., 2020, preprint\n( arXiv:1804.06788 ) aylor G., Lidman C., Tucker B. E., Brout D., Hinton S. R., Kessler R., 2021,\nMNRAS , 504, 4111 ripp R., 1997, A&A, 325, 871 ripp R., 1998, A&A, 331, 815 einberg S., 2008, Cosmology, illustrated edition. Oxford Univ. Press,\nOxford eyant A., Schafer C., Wood-Vasey W. M., 2013, ApJ , 764, 116 ojtak R., Davis T. M., Wiis J., 2015, J. Cosmol. Astropart. Phys., 2015, 025\nPPENDIX A : P\u2013P PLOTS\nP\u2013P plot is a diagnostic tool which compares credibilities ( 0 , d ), as defined in equation ( 15 ), with their cumulative distribu-\nion, F ( \u03b3 ) \u2261 \u222b \u03b30 p( \u03b3 \u2032 ) d \u03b3 \u2032 , where p ( \u03b3 \u2032 ) is implied by a distribution f 0 and d . The interpretation of F( \u03b3 ) is the frequency with which arameters are co v ered by credible regions and is hence called the empirical co v erage\u2019. In practice, it is constructed by dra wing a ample for 0 and d , e v aluating the approximate posterior across the hole parameter space, determining ( 0 , d ) (from equation 16 ), nd inte grating o v er it, to determine the approximate credibility ssociated with 0 , as illustrated in the top panel of Fig. 3 . This esults in a sample from p ( \u03b3 ).\n1 Bayesian coverage\nn the LFI literature, notably Hermans et al. ( 2022 ), 0 , d are taken o follow the joint distribution p ( 0 , d ) defined by a forward model\nsimulator). In this set-up, the probability density of \u03b3 is\nB ( \u0303 \u03b3 ) = \u201c \u03b4( \u0303 \u03b3 \u2212 \u03b3 ( 0 , d )) p( 0 , d ) d 0 d d , (A1)\nnd so its cumulative distribution is, after switching the order of ntegration,\nB ( \u0303 \u03b3 ) = \u201c [\u222b \u02dc \u03b3\n0 \u03b4( \u03b3 \u2032 \u2212 \u03b3 ( 0 , d )) d \u03b3 \u2032\n] p( 0 , d ) d 0 d d\n= \u201c\n{ 0 , d : \u02dc \u03b3>\u03b3 ( 0 , d ) } p( 0 , d ) d 0 d d (A2)\nhere the step resulting from integrating the delta function has een written as an appropriate region of integration. This expression orresponds to the probability mass, averaged over all possible data, f the true posterior in the regions that have credibility \u02dc \u03b3 under the pproximate posterior :\nB ( \u0303 \u03b3 ) = E p( d ) [ \u222b\n0 ( \u03b3 \u22121 ( \u0303 \u03b3 , d ) , d ) p( 0 | d ) d 0\n] , (A3)\nith \u03b3 \u22121 ( \u0303 \u03b3 , d ) an inversion of \u03b3 ( 0 , d ) for a fixed d . Thus, we see that when q( | d ) \u2192 p( | d ), the integral will be\nonstant across all data sets and equal to the credibility, \u02dc \u03b3 , and so B ( \u0303 \u03b3 ) \u2192 \u02dc \u03b3 . In other words, if the estimator q is well calibrated, the mpirical co v erage will match the credibility, and so the P\u2013P plot ill be a diagonal line. 21 On the other hand, when q is \u2018wider\u2019 than he true posterior, i.e. the ratio estimator is conserv ati ve, the area f integration will be larger, and so F B ( \u0303 \u03b3 ) > \u02dc \u03b3 . An example of a\nMNRAS 520, 1056\u20131072 (2023)\nM\nB a t\nf t b t\nA\nI f t\nF\nU t t F g i i c\nc c\nF\nT t p\nw a t\ni\nC\nT e u A\nA B\nT i t w o t b w\n2\nn\nc t\nw\nm\nN i p\nc t\nw r\nc\nM q o t c i ( a\nl M a\nB\nE m i\n)\nw l m g o e p\n2\nc\nT\nD ow nloaded from https://academ ic.oup.com /m nras/article/520/1/1056/6965837 by SISSA user on 13 M arch 2023\nayesian P\u2013P plot is Fig. 7 , where the empirical co v erage has been v eraged o v er the entire training domain of the inference network hat is being validated.\nHo we ver, a diagonal Bayesian P\u2013P plot is not a sufficient condition or establishing convergence of q on to the true posterior because of he data-averaging in equation ( A3 ): one can imagine q conspiring to e conserv ati ve for some data and o v erconfident for others in ways hat exactly cancel.\n2 Frequentist co v erage testing and calibration\nn frequentist statistics, the parameters are fixed, 22 and the data ollows the sampling distribution, p ( d | ). A frequentist P\u2013P plot, herefore, depicts\nf ( \u0303 \u03b3 | 0 ) = \u222b [\u222b \u02dc \u03b3\n0 \u03b4( \u03b3 \u2032 \u2212 \u03b3 ( 0 , d )) d \u03b3 \u2032\n] p( d | 0 ) d d . (A4)\nnlike F B ( \u0303 \u03b3 ), the frequentist empirical co v erage F f ( \u0303 \u03b3 | 0 ) needs o be determined by sampling even for the exact posterior due to he effect of the prior, which is integrated out in equation ( A2 ). urthermore, while the credible regions of the true posterior are uaranteed to have perfect coverage when avera g ed o ver the joint nference model , this is not necessarily true for fixed 0 if the prior n is not uniform. Still, we can use the approximate Bayesian redibilities to construct confidence regions with exact coverage.\nTo this end, we define the \u2018required credibility\u2019 \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) as the redibility of approximate credible regions which have frequentist o v erage with frequency \u02dc \u03b3 , i.e. as the solution of f ( \u0302 \u03b3 | 0 ) = \u222b \u02c6 \u03b3\n0 p( \u03b3 \u2032 | 0 ) d \u03b3 \u2032 = \u02dc \u03b3 . (A5)\nhus, \u02c6 \u03b3 is simply the \u02dc \u03b3 th quantile in the frequentist P\u2013P plot: see he middle panel of Fig. 3 and the concrete results for cosmological arameter inference with TMNRE in Fig. A1 . Now we can trivially construct frequentist confidence regions\nith exact coverage using \u02c6 \u03b3 . For a given data set, we e v aluate the pproximate posterior and from it \u03b3 ( 0 , d ), which we now treat as a est statistic. If the credibility associated with a given parameter value 0 is lower than the required credibility, the parameter is included n the confidence region:\n0 ( d , \u02dc \u03b3 ) \u2261 { 0 : \u03b3 ( 0 , d ) \u2264 \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) } . (A6) hus, if the approximate posterior already has exact frequentist covrage ( \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) = \u02dc \u03b3 ), no modification is necessary, but otherwise, nderco v ered parameters ( \u02c6 \u03b3 ( 0 , \u02dc \u03b3 ) > \u02dc \u03b3 ) are included more often. demonstration is shown in the bottom panel of Fig. 3 .\nPPENDIX B: LINEARIZED M A R G I NA L AH AMAS\nhe BAHAMAS model, as described in Table 1 and depicted in Fig. 1 , s non-Gaussian only in the priors of some global parameters. Furhermore, the only non-linearity is present in the distance modulus, hich has a complicated dependence on the redshift. If the redshift f each supernova is fixed (e.g. with spectroscopic observations), or he uncertainties \u03c3 s z \u2261 (1 + \u0302 zs ) \u03c3z are small enough so that they can e propagated at linear order, the latent model becomes Gaussian as ell, so that it can be analytically marginalized; i.e. the SN Ia plate\n2 In this conte xt, fix ed are only the parameters of interest, while all other uisance and latent variables are still marginalized.\nNRAS 520, 1056\u20131072 (2023)\nan be collapsed to a single normal distribution giving the prior of he latent parameters, conditioned on the global parameters, namely \u23a1 \u23a3 m s x s 1\nc s\n\u23a4 \u23a6 \u223c N \u239b \u239d \u0304d s p \u2261 \u23a1 \u23a3 m\u0304 s x\u0304 1\nc\u0304\n\u23a4 \u23a6 , s p \u2261 \u23a1 \u23a3 ( R s m ) 2 \u2212\u03b1R 2 x 1 \u03b2R 2 c \u2212\u03b1R 2 x 1 R 2 x 1 0\n\u03b2R 2 c 0 R 2 c\n\u23a4 \u23a6 \u239e \u23a0 (B1)\nith\n\u00af s = M\u0304 0 \u2212 \u03b1x\u0304 1 + \u03b2c\u0304 + \u03bc ( C , \u0302 zs ) , (B2) ( R s m )2 = \u03c3 2 res + ( \u03b1R x 1 ) 2 + ( \u03b2R c ) 2 + ( \u2202 \u03bc\n\u2202 z \u2223\u2223\u2223\u2223 C , \u0302 zs \u03c3 s z )2 . (B3)\note that even though \u2202 \u03bc/ \u2202 z is evaluated at a fixed set of redshifts, t has to be recalculated at each new draw of the cosmological arameters. The prior covariance from equation ( B1 ) is added to the data ovariance, \u02c6 , to obtain the marginal covariance of the data, given he global parameters:\nd | \u223c N ( \u0304d p , p + \u02c6 ) , (B4) here \u2261 { C , \u03c3z , \u03b1, \u03b2, M\u0304 0 , \u03c3res , \u0304x 1 , R x 1 , \u0304c , R c } are the global pa-\nameters of the model, which the marginal prior mean, d\u0304 p \u2261 oncat s ( d\u0304 s p ) , and covariance, p \u2261 diag s ( s p ) , are functions of. As\narch et al. ( 2011 , equation C19) sho w, e v aluating this likelihood uickly, i.e. without inverting the large matrix p + \u02c6 , is possible nly up a normalization factor that depends on and therefore needs o be recomputed at every MCMC step, incurring a determinant alculation of O ( (3 N ) 3 ) complexity. 23 In the simplified case of ndependent likelihoods for each SN Ia, i.e. a block-diagonal \u02c6 in addition to p ), which we consider, ho we v er, the comple xity is nyway reduced to O ( 3 3 \u00d7 N ).\nThus, in its linearized marginal version and assuming independent ikelihoods for each SN Ia, BAHAMAS can easily be analysed with\nCMC in O (10) dimensions by inverting a block-diagonal matrix, s was done for the comparison in Fig. 9 .\n1 The latent parameters a posteriori\nven though the latent parameters do not appear explicitly in arginal BAHAMAS , one can derive posteriors for them, considering,\nnitially, the conditional posterior m , x 1 , c | , d \u223c N ( \u02c6 \u03bcd\u0304 = ( \u02c6 \u22121 + \u22121 p )\u22121 ( \u22121 p d\u0304 p + \u02c6 \u22121 d ) ,\n\u02c6 V d\u0304 = ( \u02c6 \u22121 + \u22121 p )\u22121 ) , (B5\nhich is the posterior of an a priori Gaussian variable under Gaussian ikelihood. The marginal posterior for the latent variables is then the arginalization of this expression over the marginal posterior of the lobal parameters, which can be obtained in a post-processing step f the global MCMC run, in which the means and variances from quation ( B5 ), e v aluated for each sampled , are averaged. This rocedure was used to calculate the values on the abscissa of Fig. 11 .\n3 A stochastic estimate based on an augmented conjugate gradient technique an be obtained in linear time, as detailed by Gardner et al. ( 2021 ).\nhis paper has been typeset from a T E X/L A T E X file prepared by the author."
        }
    ],
    "title": "SICRET: Supernova Ia Cosmology with truncated marginal neural Ratio EsTimation",
    "year": 2023
}