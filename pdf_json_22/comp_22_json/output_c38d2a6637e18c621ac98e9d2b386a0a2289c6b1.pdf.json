{
    "abstractText": "Crowd-sourcing deals with solving problems by assigning them to a large number of non-experts called crowd using their spare time. In these systems, the final answer to the question is determined by summing up the votes obtained from the community. The popularity of using these systems has increased by facilitation of access to community members through mobile phones and the Internet. One of the issues raised in crowd-sourcing is how to choose people and how to collect answers. Usually, the separation of users is done based on their performance in a pre-test. Designing the pre-test for performance calculation is challenging; The pre-test questions should be chosen in a way that they test the characteristics in people related to the main questions. One of the ways to increase the accuracy of crowd-sourcing systems is to pay attention to people\u2019s cognitive characteristics and decision-making model to form a crowd and improve the estimation of the accuracy of their answers to questions. People can estimate the correctness of their responses while making a decision. The accuracy of this estimate is determined by a quantity called metacognition ability. Metacoginition is referred to the case where the confidence level is considered along with the answer to increase the accuracy of the solution1. In this paper, by both mathematical and experimental analysis, we would answer the following question: Is it possible to improve the performance of the crowd-sourcing system by knowing the metacognition of individuals and recording and using the users\u2019 confidence in their answers?",
    "authors": [
        {
            "affiliations": [],
            "name": "Samin Nili Ahmadabadi"
        },
        {
            "affiliations": [],
            "name": "Maryam Haghifam"
        },
        {
            "affiliations": [],
            "name": "Vahid Shah-Mansouri"
        },
        {
            "affiliations": [],
            "name": "Sara Ershadmanesh"
        }
    ],
    "id": "SP:035c9dc8f0333bd2411a87bb4d4309bc30b63424",
    "references": [
        {
            "authors": [
                "B. Maniscalco",
                "Lau",
                "H. Signal detection theory analysis of type 1",
                "type 2 data"
            ],
            "title": "meta-d, response-specific meta-d, and the unequal variance sdt model",
            "venue": "In The cognitive neuroscience of metacognition, 25\u201366",
            "year": 2014
        },
        {
            "authors": [
                "J. Howe"
            ],
            "title": "The rise of crowdsourcing",
            "venue": "Wired magazine 14,",
            "year": 2006
        },
        {
            "authors": [
                "Fleming",
                "S.M. Hmeta-d"
            ],
            "title": "hierarchical bayesian estimation of metacognitive efficiency from confidence ratings",
            "venue": "Neurosci. consciousness nix007",
            "year": 2017
        },
        {
            "authors": [
                "M.V. Veenman",
                "J.J. Elshout",
                "J. Meijer"
            ],
            "title": "The generality vs domain-specificity of metacognitive skills in novice learning across domains",
            "venue": "Learn. instruction",
            "year": 1997
        },
        {
            "authors": [
                "A. Mazancieux",
                "S.M. Fleming",
                "C. Souchay",
                "C. Moulin"
            ],
            "title": "Retrospective confidence judgments across tasks: domaingeneral processes underlying metacognitive accuracy (2018)",
            "year": 2018
        },
        {
            "authors": [
                "X. Sheng",
                "J. Tang",
                "X. Xiao",
                "Xue",
                "G. Sensing as a service"
            ],
            "title": "Challenges, solutions and future directions",
            "venue": "IEEE Sensors journal 13, 3733\u20133741",
            "year": 2013
        },
        {
            "authors": [
                "A. Faggiani",
                "E. Gregori",
                "L. Lenzini",
                "V. Luconi",
                "Vecchio",
                "A. Smartphone-based crowdsourcing for network monitoring"
            ],
            "title": "opportunities, challenges, and a case study",
            "venue": "IEEE Commun. Mag. 52, 106\u2013113",
            "year": 2014
        },
        {
            "authors": [
                "Y Liu"
            ],
            "title": "A crowdsourcing based mobile image translation and knowledge sharing service",
            "venue": "In 9th International Conference on Mobile and Ubiquitous Multimedia,",
            "year": 2010
        },
        {
            "authors": [
                "K. Ntalianis",
                "N. Tsapatsoulis",
                "A. Doulamis",
                "N. Matsatsinis"
            ],
            "title": "Automatic annotation of image databases based on implicit crowdsourcing, visual concept modeling and evolution",
            "venue": "Multimed. Tools Appl",
            "year": 2014
        },
        {
            "authors": [
                "L. Von Ahn",
                "L. Dabbish"
            ],
            "title": "Labeling images with a computer game",
            "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems,",
            "year": 2004
        },
        {
            "authors": [
                "F. Ribeiro",
                "D. Florencio",
                "V. Nascimento"
            ],
            "title": "Crowdsourcing subjective image quality evaluation",
            "venue": "In 2011 18th IEEE International Conference on Image Processing (Brussels, Belgium,",
            "year": 2011
        },
        {
            "authors": [
                "D. Ghadiyaram",
                "A.C. Bovik"
            ],
            "title": "Massive online crowdsourced study of subjective and objective picture quality",
            "venue": "IEEE Transactions on Image Process",
            "year": 2015
        },
        {
            "authors": [
                "H. Shin",
                "Y. Chon",
                "Y. Kim",
                "H. Cha"
            ],
            "title": "A participatory service platform for indoor location-based services",
            "venue": "IEEE Pervasive Comput",
            "year": 2015
        },
        {
            "authors": [
                "S. Chen",
                "M. Li",
                "K. Ren",
                "Qiao",
                "C. Crowd map"
            ],
            "title": "Accurate reconstruction of indoor floor plans from crowdsourced sensor-rich videos",
            "venue": "In 2015 IEEE 35th International conference on distributed computing systems",
            "year": 2015
        },
        {
            "authors": [
                "D. McDuff",
                "R. El Kaliouby",
                "R.W. Picard"
            ],
            "title": "Crowdsourcing facial responses to online videos",
            "venue": "IEEE Transactions on Affect. Comput",
            "year": 2012
        },
        {
            "authors": [
                "R. Gatautis",
                "E. Vitkauskaite"
            ],
            "title": "Crowdsourcing application in marketing activities",
            "venue": "Procedia-Social Behav. Sci. 110,",
            "year": 2014
        },
        {
            "authors": [
                "Zhang",
                "X. et al. Free market of crowdsourcing"
            ],
            "title": "Incentive mechanism design for mobile sensing",
            "venue": "IEEE transactions on parallel distributed systems 25, 3190\u20133200",
            "year": 2014
        },
        {
            "authors": [
                "W. Liu",
                "Y. Yang",
                "E. Wang",
                "Z. Han",
                "X. Wang"
            ],
            "title": "Prediction based user selection in time-sensitive mobile crowdsensing",
            "venue": "In 14th Annual IEEE International Conference on Sensing,",
            "year": 2017
        },
        {
            "authors": [
                "L. Pu",
                "X. Chen",
                "J. Xu",
                "Fu",
                "X. Crowdlet"
            ],
            "title": "Optimal worker recruitment for self-organized mobile crowdsourcing",
            "venue": "In IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications",
            "year": 2016
        },
        {
            "authors": [
                "Liu",
                "X. et al. Cdas"
            ],
            "title": "a crowdsourcing data analytics system",
            "venue": "Proc. VLDB Endow. 5, 1040\u20131051",
            "year": 2012
        },
        {
            "authors": [
                "A.T. Nguyen",
                "M. Lease",
                "B.C. Wallace"
            ],
            "title": "Explainable modeling of annotations in crowdsourcing",
            "venue": "IUI",
            "year": 2019
        },
        {
            "authors": [
                "K. Yadav",
                "P. Kumaraguru",
                "A. Goyal",
                "A. Gupta",
                "Naik",
                "V. Smsassassin"
            ],
            "title": "crowdsourcing driven mobile-based system for sms spam filtering",
            "venue": "In Proceedings of the 12th Workshop on Mobile Computing Systems and Applications",
            "year": 2011
        },
        {
            "authors": [
                "E. Kamar",
                "S. Hacker",
                "E. Horvitz"
            ],
            "title": "Combining human and machine intelligence in large-scale crowdsourcing",
            "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems,",
            "year": 2012
        },
        {
            "authors": [
                "J. Yang",
                "T. Drake",
                "A. Damianou",
                "Maarek",
                "Y. Leveraging crowdsourcing data for deep active learning an application"
            ],
            "title": "Learning intents in alexa",
            "venue": "In World Wide Web Conference, Lyon, France, 23\u201332",
            "year": 2018
        },
        {
            "authors": [
                "J.D. Abernethy",
                "R.M. Frongillo"
            ],
            "title": "A collaborative mechanism for crowdsourcing prediction problems",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2011
        },
        {
            "authors": [
                "U. Gadiraju",
                "B. Fetahu",
                "R. Kawase",
                "P. Siehndel",
                "S. Dietze"
            ],
            "title": "Using worker self-assessments for competence-based pre-selection in crowdsourcing microtasks",
            "venue": "ACM Transactions on Comput. Interact. (TOCHI) 24,",
            "year": 2017
        },
        {
            "authors": [
                "D. Zhao",
                "Li",
                "X.-Y.",
                "Ma",
                "H. How to crowdsource tasks truthfully without sacrificing utility"
            ],
            "title": "Online incentive mechanisms with budget constraint",
            "venue": "In IEEE INFOCOM 2014-IEEE Conference on Computer Communications",
            "year": 2014
        },
        {
            "authors": [
                "R. Bi",
                "X. Zheng",
                "G. Tan"
            ],
            "title": "Optimal assignment for deadline aware tasks in the crowdsourcing. In IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom)(BDCloud-SocialCom-SustainCom",
            "year": 2016
        },
        {
            "authors": [
                "M. Mazor",
                "L. Charles",
                "R.M. Mor",
                "S. Fleming"
            ],
            "title": "Paradoxical evidence weighting in confidence judgments for detection and discrimination (2022)",
            "year": 2022
        },
        {
            "authors": [
                "J.A. Livingston"
            ],
            "title": "Metacognition: An overview",
            "year": 2003
        },
        {
            "authors": [
                "S.M. Fleming",
                "Daw",
                "N.D. Self-evaluation of decision-making"
            ],
            "title": "A general bayesian framework for metacognitive computation",
            "venue": "Psychol. review 124, 91",
            "year": 2017
        },
        {
            "authors": [
                "B. Maniscalco",
                "H. Lau"
            ],
            "title": "A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings",
            "venue": "Conscious. cognition 21,",
            "year": 2012
        },
        {
            "authors": [
                "S.M. Fleming",
                "H.C. Lau"
            ],
            "title": "How to measure metacognition",
            "venue": "Front. human neuroscience 8,",
            "year": 2014
        },
        {
            "authors": [
                "B. Baird",
                "J. Smallwood",
                "K.J. Gorgolewski",
                "D.S. Margulies"
            ],
            "title": "Medial and lateral networks in anterior prefrontal cortex support metacognitive ability for memory and perception",
            "venue": "J. Neurosci",
            "year": 2013
        },
        {
            "authors": [
                "S. Sadeghi",
                "H. Ekhtiari",
                "B. Bahrami",
                "M.N. Ahmadabadi"
            ],
            "title": "Metacognitive deficiency in a perceptual but not a memory task in methadone maintenance patients",
            "venue": "Sci. reports 7,",
            "year": 2017
        },
        {
            "authors": [
                "K. Fiedler",
                "R. Ackerman",
                "Scarampi",
                "C. Metacognition"
            ],
            "title": "Monitoring and controlling one\u2019s own knowledge, reasoning and decisions",
            "venue": "The psychology human thought: An introduction 89\u2013111",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Crowd-sourcing deals with solving problems by assigning them to a large number of non-experts called crowd using their spare time. In these systems, the final answer to the question is determined by summing up the votes obtained from the community. The popularity of using these systems has increased by facilitation of access to community members through mobile phones and the Internet. One of the issues raised in crowd-sourcing is how to choose people and how to collect answers. Usually, the separation of users is done based on their performance in a pre-test. Designing the pre-test for performance calculation is challenging; The pre-test questions should be chosen in a way that they test the characteristics in people related to the main questions. One of the ways to increase the accuracy of crowd-sourcing systems is to pay attention to people\u2019s cognitive characteristics and decision-making model to form a crowd and improve the estimation of the accuracy of their answers to questions. People can estimate the correctness of their responses while making a decision. The accuracy of this estimate is determined by a quantity called metacognition ability. Metacoginition is referred to the case where the confidence level is considered along with the answer to increase the accuracy of the solution1. In this paper, by both mathematical and experimental analysis, we would answer the following question: Is it possible to improve the performance of the crowd-sourcing system by knowing the metacognition of individuals and recording and using the users\u2019 confidence in their answers?"
        },
        {
            "heading": "1 Introduction",
            "text": "The goal of crowd-sourcing is to entrust answering questions, called tasks, to non-experts and using their idle time at a low cost. In these systems, cost is defined as the time consumed to decide and the payment to the users. Crowd-sourcing tries to use the extra processing power of millions of human brains. The experience with open-source software proved that a network of enthusiastic volunteers and highly paid developers at Microsoft and other companies could develop software. Wikipedia showed that this model could be used to create a surprisingly distributed online encyclopedia2.\nGiven that most crowd-sourcing systems are online, it usually dose not matter where people are located. These systems help companies, industries, research, etc., find the solution to their problems or the required data in a more affordable and faster way. Also, crowd-sourcing systems can be an easy source of income for people in their spare time. Furthermore, people with disabilities can easily participate in these systems.\nMany discussions in the field of crowd-sourcing systems deal with solutions to avoid and minimize errors. In this regard, this research deals with the use of the decision-making model of people in such systems. Knowing how people make decisions makes it possible to predict the probability of the person answering the question correctly. One unsolved question in this area is that is it possible to improve the system\u2019s performance by asking people\u2019s confidence in their answers and how this improvement can be done. In literature, metacognition ability is defined based on users\u2019 responses and confidence in order to see how accurately they can assess their performance. Thus, Metacognition ability is a predictor of how well users can respond and how well they are reporting their confidence3.\nIn typical crowd-sourcing systems, people are separated based on efficiency, and the majority vote algorithm is used to aggregate votes. In order to investigate the impact of metacognition and people\u2019s decision-making model in crowd-sourcing systems, in this paper, we present a new method for screening people relying on metacognition and aggregation of votes with\nar X\niv :2\n21 2.\n05 77\n7v 2\n[ cs\n.C E\n] 1\n5 Fe\nb 20\nconfidence and measuring their performance against a conventional system. Metacognition ability-based crowd-sourcing systems can be useful in two main cases:\n\u2022 Metacognition ability is the same among different tasks4, 5. Therefore, this parameter can be measured once and recruit users for several tasks according to their metacognition ability.\n\u2022 Sometimes, people choose an answer that are not completely sure about its correctness. Asking how confident they are may help the system to change the weight of their decision in the final aggregation.\nIn this research, our contributions are as follows:\n\u2022 We propose a novel crowd-sourcing system based on the users\u2019 reported confidence and their metacognition ability.\n\u2022 With the help of simulation, we check the superiority of our proposed system for different communities. With the help of the results, we have identified the characteristics of the communities in which our system works better.\n\u2022 By designing and implementing a real-world task, we prove our hypothesis that our system outperforms the typical crowd-sourcing systems in certain communities. We ask people to identify the gender of the author of several tweets. This experiment\u2019s results show that using the proposed crowd-sourcing system reduces the error.\nIn Section 2, we introduce the components of crowd-sourcing systems and examine the solutions for various problems in these systems. Then we will introduce and review several practical and commercial examples of this field. In the following, we will introduce the concept of metacognition ability by examining and modeling the decision-making process of humans. Then, we will describe the use of metacognition in crowd-sourcing systems. The proposed model of the crowd-sourcing system, considering the first and second types of decision-making, is introduced in Section 3. Through mathematical and experimental analysis, we will examine our proposed system. According to our findings, the crowd-sourcing system error can be decreased by asking people how confident they are and khowing their metacognition ability. Furthermore, in this manner, we can avoid repetitive pre-tests and recruit users based on their metacognition ability which will be tested at the beginning of their enrollment."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Crowd-sourcing Systems",
            "text": "Crowd-sourcing is a method that relies on collective wisdom to answer questions. Usually, this method is used for questions whose answer is unavailable or there is not enough confidence in the available response. To obtain the answer, the desired question is asked from several people, and based on the crowd\u2019s response, the final answer is extracted. There are different types of crowd-sourcing systems, some of which are listed below:\n\u2022 In some crowd-sourcing systems, only one person provides a solution for each problem; For example, in InnoCentive and kaggle, companies describe their issues and questions. The people in this network offer a solution after studying the problem. At the end, the crowd answer will be extracted by an answer aggregation algorithm, which will be explained later. The users whose answers are the same as the crowd answer will be rewarded.\n\u2022 Another group of crowd-sourcing systems is designed to use data sent from mobile phones or computers6. In this type of system, there is no need for people to answer questions. One example of this category is traffic estimation by Google Map. This software receives the speed of cars from the sensors of smartphones and using this information and data obtained in the previous days and hours with the same method, it estimates the traffic and announces the fastest route to destination. In this type of systems, people are not given cash rewards, but they can use the data obtained by these softwares (for example, routing according to traffic). Another example of this area is finding a network map and classifying points in terms of antenna quality. For this purpose, a software is installed on the smartphone, and this software sends information about the network to a server in each region7.\n\u2022 In another type of crowd-sourcing system, a problem arises that is difficult to find an answer to or requires an expert. Since it is expensive to get help from an expert, crowd-sourcing suggests that we leave this task to a large number of non-experts and then use their answer set. In the rest of this paper, a crowd-sourcing system is a system that uses the method of assigning a task to a large number of people.\nOne of the most famous crowd-sourcing system is Amazon Mechanical Turk (AMT). This site is a web marketplace that helps companies find people to solve problems that computers can\u2019t solve; For example, identifying and categorizing photos and\n2/17\nvideos and annotating them, converting images into documents and books8, implementing the text of podcasts, etc. These tasks are called HITs. Each HIT is designed in such a way that it is not necessary to spend a lot of time doing them.\nOne of the uses of crowd-sourcing is categorizing and annotating images and videos. Due to the large number of images available on the web, database, etc., this work is not possible manually because it is time consuming. On the other hand, for obtaining the best result, experts are needed and recruiting them is usually expensive. Sometimes techniques can be used that automatically annotate multimedia content using algorithms. The main disadvantage of these methods is that they only support limited vocabulary for annotation, and in complex cases, they lead to high errors9. For annotation complex cases, we can use an expert which is expensive. Therefore, one of the standard solutions is to use crowd-sourcing in this field. For example,10 has been able to label many images by designing a game. The paper predicts that if the game is played continuously by 5,000 people, all the photos on Google (425,000,000 photos in 2005) will be tagged within 31 days. In this game, participants are paid cash, and people play for fun only. Finally, a label that a certain number of people agree on is selected for each photo. It is also possible to classify images by quality using crowd-sourcing11 or improve their quality12.\nAnother common application of these systems is to obtain a complete and accurate map inside the buildings13. CrowdMap is a software designed for finding indoor maps which can help a variaty of jobs such as interior designers14. This software asks the user to film the space inside the building. Then, a map inside the building is obtained using the people\u2019s geographic location and the video sent.\nOther uses of crowd-sourcing include data collection for problem analysis. For example,15 shows different ads to people using crowd-sourcing software. Then by analyzing their facial expressions and the questions , the system tries to provide a way to determine the impact of ads on people.\nCrowd-sourcing systems have made it possible for different people to participate in business activities. Commercial companies can conduct market research, development, and testing of new products, pricing, etc., through these systems16."
        },
        {
            "heading": "2.1.1 Steps of a crowd-sourcing system",
            "text": "In a crowd-sourcing system, first, a task is defined. Then the system announces the task\u2019s information to all the crowd-sourcing network\u2019s people. The task information is its topic and category, description, the time required to do it, etc. Then the users who are enthusiastic about doing this task announce their willingness. In some systems, users propose an amount they want to be paid in exchange for performing the desired task. But sometimes, the system calculates and announces the payment amount. Next, the system holds a pre-test. Based on predefined criteria (such as performance in the pre-test section), the system decides which users to recruit to contribute to the task. After this step, the task will be fully announced to the selected users. After completing the task, the system collects the answers, and the crowd\u2019s response is calculated using an answer aggregation method. Rewards will be paid to the people whose answers are recorded in the assigned time and whose answers are the same as the crowd\u2019s answer.\nUser Recruitment Selecting users is done in two main ways: online and offline17. In the offline method, the system waits to find the required number of users and then announces the task to everyone. For this mode, it is assumed that there are a number of simultaneous users in the system. One of the crucial things in this selection is predicting the location of people so that we can estimate how many people will be present in the situation where we want the task to be done and adjust the filters for selecting people18, 19. Since offline recruiting is not easy, the online method is usually used. In this method, any user can announce his willingness in a period of time, and the system should decide whether to recruit the user or not.\nAnswer Aggregation Algorithms. In order to extract the answer from the crowd, different algorithms can be used. The two main categories of them are mentioned below:\n\u2022 Majority voting (MV): In this method, an answer is announced as the final answer that the majority of the crowd has agreed on20. This method (as well as weighted majority voting) is the most common in these systems. That is because this method is easy to implement and the calculation overhead is much less compared to the other methods.\n\u2022 Algorithms based on machine learning and pattern recognition: these algorithms are one of the main answer aggregation algorithms in the field of annotation. For example,21 found the final answer through clustering. Also, in22, it detects spam text messages using SVM and Bayesian filters. Naive Bayes23 or deep Bayesian24 can be used to find suitable photo labels.\nIncentive mechanisms. Users\u2019 rewards can be cash or non-cash. Some crowd-sourcing systems are designed in such a way that participation in them is enjoyable (such as systems developed in the form of a game) or ultimately provide information and services to the users. But considering that doing the task will be costly for the person (use of mobile phone charging, internet, time, etc.), in some systems, a cash bonus is paid to the users. There are different solutions for determining cash rewards. as an example, paying the winner is one of the incentive methods.\n3/17\nThis method puts people into a competitive phase and prevents them from cooperating with each other. Although the noncooperation of users causes different responses and comments to be registered, it may reduce the number of users willing to do the task because they are no different in this second and last place structure. Therefore, except for people who are very interested in competition, other people will not have the motivation to participate25. Another method is to pay a fixed amount to all the users21, 26, which reduces the number of calculations to find the payment amount and gives everyone an incentive to participate in the task. Also, this method does not incentivize people to cheat for more money.\nAuction is also used in these systems. In this case, the user submits his bid. Based on the bid price and budget, the system decides which users to use17, 27.\nAlso, payment based on the required time and the difficulty of the question can be utilized28."
        },
        {
            "heading": "2.2 Meta-cognitive ability",
            "text": "Decision-making requires gathering, organizing, synthesizing, and evaluating information. Due to each person\u2019s difference in understanding and intuition of the information needed to make a decision, people will not make the same decision in the same situations. Decision-making in people can be divided into two types: the first and the second type. The first type of decision-making refers to the individual\u2019s choice between the available options, while the second type refers to the degree of confidence that the decision has been made correctly29. The level of a person\u2019s ability to correctly estimate his level of confidence is called metacognition.\nMetacognition refers to higher-order thinking that deals with the ability to influence and monitor one\u2019s cognitive processes, such as perception, memory, and decision-making. Metacognition usually deals with determining whether a person\u2019s certainty of answering can be a predictor of the correctness of their decision3. Metacognition is more simply defined as \"thinking about thinking.\" Metacognitive strategies are sequential processes that a person uses to control cognitive activities and ensure the achievement of a cognitive goal (for example, understanding a text). These processes help regulate and monitor learning and include planning and monitoring cognitive activities as well as examining the results of those activities. For example, after reading the paragraph in the text, each person can ask themselves about the concepts discussed in the paragraph. Their cognitive goal was to understand the text. Self-questioning is a metacognitive monitoring strategy30.\nAs mentioned, metacognition plays a vital role in learning. People are often aware of their mistakes and report a level of confidence in their choices that is related to their performance. These metacognitive assessments of decision quality are essential for guiding behavior and subsequent decisions, especially when external feedback is absent31.\nSuppose we consider a simple two-choice decision-making task. In that case, a person with high metacognition is assigned to one who declares his correct answers with high confidence and his incorrect answers with low confidence.\nIn the presented models for decision-making, it is assumed that a decision variable is produced in the brain for each question. This variable is compared to the individual\u2019s decision threshold, and being greater or lower determines which option they choose. The difference between the decision threshold and decision variable will show one\u2019s level of confidence32. If a person is asked to declare their confidence in a leveled and discrete manner, they will report this value according to the thresholds specified in their mind to declare confidence levels. The parameters involved in the production of the decision variable, the decision threshold, and the choice of confidence level are different from person to person.\nThere are various methods to study the first and the second type decision-makings. The most common method is using Signal Detection Theory (SDT) and using Receiver Operating Characteristic (ROC)1, which are explained below.\nThe answer to any question that a person submits, based on the correct answer to the question, can have four states which are shown in Table 1.\nHit rate (HR) and false alarm rate (FAR) are used to examine the individual\u2019s behavior to answer the questions. If we draw a graph whose horizontal axis is FAR, its vertical axis is HR, and its points are the pairs (FAR, HR) and fit it, the obtained curve will be the first type ROC. If the person responds completely randomly, their ROC curve will be the line FAR=HR. Therefore, the area above this line shows a person\u2019s ability and expertise. The area under the first type ROC diagram (AUROC1) is directly related to performance (the rate of answering questions correctly).\n4/17\nTo analyze Type II decision, assuming that there are only two levels of confidence (high and low), we would have four states for Type II decision-making, shown in Table 2.\nSimilarly, we use HR 2 and FAR 2 to draw the second type ROC. The area below this diagram (AUROC2) represents a person\u2019s metacognition level. This method can be used for more confidence levels."
        },
        {
            "heading": "2.3 Metacognition in Crowd-sourcing Systems",
            "text": "If we ask people for their confidence when asking crowd-sourcing questions, we can calculate their metacognition and performance. Their confidence can also be used to aggregate votes. Existing examples of crowd-sourcing systems do not typically use metacognition, and there are only a few examples in this field. One of the examples close to these systems using metacognition is a crowd-sourcing system using self-assessment introduced in26.\nIt is stated in26 that self-assessment can help attention and accuracy in answering questions in users. In the model presented by this article, after running a pre-test, it asks people what percentage of questions they have answered correctly according to their opinion. In addition to high efficiency, people will be recruited based on a relatively good performance estimation.\nThe Dunning-Kruger effect is a cognitive bias in which people assume their competency is low when they have high performance and vice versa. This effect and its influence on crowd-sourcing systems are studied in26. This paper proposes a mathematical model for performance and confidence based on the DK effect. By assuming MV, and WMV aggregation algorithms, they have studied the impact of confidence reporting in crowd-sourcing systems. They have concluded that WMV causes more errors than MV. According to our experiment, this finding is invalid, and the proposed model in this paper cannot be used in all cases.\nIn the next section, we will introduce and analyze our proposed crowd-sourcing system. In this system, we will answer whether asking about the confidence level will improve the system\u2019s performance. To check this point, we will compare one of the common crowd-sourcing systems with our proposed system."
        },
        {
            "heading": "3 Decision-Making Model",
            "text": "As we mentioned in Section 2, there are two types of decisions: Type I and Type II. A Type I decision is a choice between the possible options. Type II decision represents the confidence level of the decision. In this section, we are going to discuss the mathematical model for these types based on what32 has presented. For the sake of simplicity, we will model the decision when humans face two-choice questions."
        },
        {
            "heading": "3.1 Type I Decision",
            "text": "When a question is presented, a decision variable is generated corresponding to the evidence in favor of each option, A = 1, and A = 2. You will decide among the options based on the value of this variable and your decision-making threshold. A Gaussian model often models this random variable. The distribution\u2019s mean and variance vary from person to person depending on their performance and bias. The distribution of the decision variable, x, is as follows:\nx|A = i\u223cN (\u00b5i,\u03c3i), i \u2208 {1,2} (1)\n5/17\nImagine you are to answer a question whose actual answer is A = 1. In your mind, a random variable is generated that comes from a Gaussian model, N (\u00b51,\u03c31). If the generated variable is lower than your Type I decision-making threshold, c1, you will choose A = 1, otherwise A = 2.\nWith the help of this model, the probability of answering correctly for users can be calculated, also known as performance. The uth user\u2019s performance, shown by Per fu, is as follows:\nPer fu =Pr(A = 1)\u00d7Pr(x\u2264 c1|A = 1) +Pr(A = 2)\u00d7Pr(x > c1|A = 2). (2)\nDue to (1), both Pr(x\u2264 c1|A = 1) and Pr(x > c2|A = 2) can be written as Q(x) = 1\u221a 2\u03c0 \u222b \u221e x exp(\u2212 u 2 2 )du. For defining this\nfunction, assume Y is a Gaussian random variable with mean \u00b5 and variance \u03c32, then V = Y\u2212\u00b5\u03c3 is standard normal and\nPr(Y > y) = Pr(V > v) = Q(v) = 1\u221a 2\u03c0 \u222b \u221e v exp(\u2212u 2 2 )du\nTherefor, The Per fu is calculated as\nPer fu =Pr(A = 1)\u00d7 [1\u2212Q( c1\u2212\u00b51\n\u03c31 )]\n+Pr(A = 2)\u00d7Q(c1\u2212\u00b52 \u03c32 ). (3)"
        },
        {
            "heading": "3.2 Type II Decision",
            "text": "After choosing your answer to the question, you will decide how confident you are about the correctness of your decision; This is called Type II decision. This decision is similar to Type I, according to the decision variable, x, and Type II decision-making threshold. By assuming cmax is the maximum level of confidence that can be reported, this threshold is c2 = j|A = i i \u2208 {1,2}, j \u2208 {1, ...,cmax}.\n6/17\nAssuming we have just two confidence levels (low and high), the levels are divided by one threshold. In the previous example, if your choice was A = 1 and x is greater than c2|A = 1, your confidence is low; otherwise high.\nThe meta-cognitive ability that was discussed in section 1 is how efficiently the user\u2019s confidence ratings discriminate between their own correct and incorrect stimulus classifications32. Several methods measures meta-cognition ability, but in this study, we use the area under Type II ROC curve, AUROC233. The y-axis of this curve is Pr( confidence | correct) and the x-axis is Pr(confidence|incorrect). Due to its definition, the greater AUROC2 means the user is more meta-cognitive.\nAll the parameters defined in this section differ among users and can vary over time. They all can be obtained by fitting this model to their decision-making behavior. In the next section, we will use this model to make decisions in a crowd-sourcing system."
        },
        {
            "heading": "4 Method",
            "text": "In any crowd-sourcing system, after broadcasting the task information and collecting volunteers\u2019 requests, who desire to do the task, the system selects the most proper subset of volunteers. This selection is either on the features or skills each person claims or according to the pre-test results. The selected volunteers, also known as users, are recruited and will participate in the test phase. In this phase, the main questions are presented to the users, and their answers are collected. The system extracts the crowd\u2019s answer based on an aggregation method and announces it as the correct answer. Users receive incentives at the end of the task due to the system\u2019s payment policy. In this paper, we assume incentives for all the users are the same. In this section, we first discuss about a conventional crowd-sourcing system, Response Based Crowdsourcing System (ReBaCS), and then our proposed system,Confidence-Based Crowd-Sourcing system (CoBaCS) will be introduced."
        },
        {
            "heading": "4.1 Response Based Crowd-Sourcing System (ReBaCS)",
            "text": "One of the popular and conventional types of crowd-sourcing systems are just based on users\u2019 responses, and the answer aggregation is done by the Majority Voting (MV) algorithm. As mentioned earlier, each crowd-sourcing system has two phases: the first, a pre-test phase that examines the volunteers\u2019 capabilities, and the second, the test phase, that the main questions are presented to the recruited users. In the pre-test phase, the users submit their answers to the questions while the system knows the correct answers. Users are selected due to their performance at the end of the pre-test phase; by performance, we mean how correctly their answers were. To have proper user selection, the pre-test questions should be in the same context as the main crowd-sourcing task. The recruited users and volunteers\u2019 performance reached a minimum threshold in the test phase and must answer the main crowd-sourcing questions. The crowd\u2019s answer to each question is the one that more than half of the crowd has consensus on. Users will be paid at the end of the task. Now on, we will refer to these systems as Response Based Crowdsourcing System (ReBaCS)."
        },
        {
            "heading": "4.1.1 Error Calculation",
            "text": "For the sake of simplicity, we assume that all the questions in both the pre-test and test phases are two-choice. The answer of the nth user to question q can be written as:\nrn,q = { 0 if the nth user has chosen first option 1 if the nth user has chosen second option\n(4)\nBy assuming Nu users have passed the pre-requisite test and are presented in the system, the crowd\u2019s answer for question q, RMVq , is driven by MV as\nRMVq =\n{ 0, \u2211Nn=1 rn,q < Nu 2 ,\n1, o.w, (5)\nIf we denote the correct answer of the qth question by Aq, the system\u2019s performance is the proportion of the questions in which their actual response, Aq, matches the crowd response, RMVq . If we assume NQ questions are available in the system, the system\u2019s accuracy is defined as the percentage of the correct answers as\naccReBaCS = \u2211\nNQ q=1 \u03b4 (RMVq ,Aq)\nNQ , (6)\n\u03b4 (x,y) = { 1, if x = y, 0, o.w.,\n7/17\nBy the above definition, the error of the system will be errReBaCS = 1\u2212accReBaCS. With the help of the total probability theorem, the expectation of error can be rewritten as follows:\nENQ [errReBaCS] =Pr(A 6= RMV) (7) =Pr(A = 0)\u00d7Pr(RMV = 1|A = 0) +Pr(A = 1)\u00d7Pr(RMV = 0|A = 1)\n=Pr(A = 0)\u00d7Pr\n( N\n\u2211 n=1 rn \u2265 Nu 2 |A = 0\n)\n+Pr(A = 1)\u00d7Pr\n( N\n\u2211 n=1 rn < Nu 2 |A = 1\n) .\nIn these equations, A shows the actual answer to a question, and RMV refers to the crowds\u2019 response based on the MV algorithm. According to the decision-making model described in Section 3, each user decides between the first and the second option in every question based on a Bernoulli random variable as:\nPr(rn = r|A = i) = Q ( cn\u2212\u00b5i,n \u03c3i,n ) r = 1 1\u2212Q (\ncn\u2212\u00b5i,n \u03c3i,n\n) r = 0 , i \u2208 {0,1} (8)\nwhere Q(x) is 1\u221a 2\u03c0 \u222b \u221e x exp(\u2212 u 2\n2 )du, cn , \u00b5i,n and \u03c3i,n are the decision making parameters for the nth user, respectively. Each user\u2019s decision-making characteristics are encoded in how far the \u00b5i,ns are from cn and the difference between \u00b5i,ns. Therefore, without loss of generality, we can assume cn = 0 and \u03c30,n = \u03c31,n = 1 for all users and let the value of \u00b5i,n show their decision making policies. By these assumptions, the equation in (8) becomes less complex:\nPr(rn = r|A = i) = { Q(\u00b5i,n) r = 1 1\u2212Q(\u00b5i,n) r = 0\n(9)\nThe summation of several independent Bernoulli random variables with the same distribution parameters is a Binomial random variable. Therefore, by considering that all \u00b5i,n are the same and equal to E[\u00b5i,n] = \u00b5i, the random variables defined as \u03b3i = \u2211Nun=1 rn| ( A = i ) , i \u2208 {0,1} are binomial random variables. If we suppose NQ is large enough, \u03b3i follows a normal distribution as\n\u03b3i \u223cN (mi,r,si,r) (10) mi,r = Nu\u00d7Q(\u2212\u00b5i)\nsi,r = \u221a Nu\u00d7Q(\u2212\u00b5i)\u00d7 ( 1\u2212Q(\u2212\u00b5i) ) With the equation in (10), E[EReBaCS] is calculated as:\nE[errReBaCS] =Pr(A = 0)\u00d7Pr (\nNu 2 \u2264 \u03b30 \u2264 Nu ) +Pr(A = 1)\u00d7Pr ( 0\u2264 \u03b31 <\nNu 2 ) =Pr(A = 0)\u00d7 ( Q ( Nu 2 \u2212m0,r\ns0,r\n) \u2212Q ( Nu\u2212m0,r\ns0,r )) +Pr(A = 1)\u00d7 ( Q ( \u2212m1,r\ns1,r\n) \u2212Q ( Nu 2 \u2212m1,r\ns1,r )) (11)\nIn conclusion, by knowing the expectation of all users decision variables and the number of users, expectation of ReBaCS error can be determined.\n8/17"
        },
        {
            "heading": "4.2 CoBaCS",
            "text": "As we discussed earlier, sometimes users are not 100% sure about the correctness of their answers, so they choose the option they feel more confident about. By knowing the meta-cognitive ability of each user, we can estimate how accurately their confidence leads them. In other words, we can answer this question: \"Is the user\u2019s confidence a proper estimator of his/her accuracy?\". If we select volunteers based on their meta-cognitive ability and, in every question, ask them to submit their confidence as well as their answers, we can improve system performance. So, in the Confidence-Based Crowd-Sourcing system, CoBaCS, first we show the volunteers pre-test questions. As we mentioned earlier, both their response and confidence are recorded. At the end of this phase, users are selected due to their both performance and meta-cognition ability. For measuring performance, the context and the complexity of the pre-test and test phase questions must be the same. Still, according to4, 5 meta-cognitive ability is similar across domains. So, the developed tasks in the literature can be applied, and there is no need to re-design them.\nIn the test phase, the same as the pre-test, users report their confidence in being correct in each question. After collecting users\u2019 answers and confidence, the system states the crowd answer by Weighted Majority Voting (WMV) algorithm. This algorithm works similarly to MV with one difference; answers are multiplied with their corresponding confidence. Using WMV, a user with low confidence has a lower contribution to the crowd\u2019s answer."
        },
        {
            "heading": "4.2.1 Error Calculation",
            "text": "In CoBaCS, we assume rn,q will be either 1 or -1 as\nrn,q = { \u22121 if the nth user has chosen first option 1 if the nth user has chosen second option\n(12)\nUsers are asked to report their confidence as discrete levels from 1 (i.e., lowest confidence) to cmax (i.e., highest confidence). We denote the reported confidence of user n to the qth question by cn,q. Then, we have\nRWMVq = { \u22121 if \u2211Nn=1 rn,q\u00d7 cn,q < 0 1 o.w.\n(13)\nSimilar to the error calculation of ReBaCS, CoBaCS performance is the fraction of questions that the crowds\u2019 consensus leads the system to the correct answer. The expectation of error in this system is as follows:\nENQ [errCoBaCS] =Pr(A =\u22121)\u00d7Pr(RWMV = 1|A =\u22121) +Pr(A = 1)\u00d7Pr(RWMV =\u22121|A = 1)\n=Pr(A =\u22121)\u00d7Pr\n( N\n\u2211 n=1 rn\u00d7 cn \u2265 0 |A =\u22121\n)\n+Pr(A = 1)\u00d7Pr\n( N\n\u2211 n=1 rn\u00d7 cn < 0 |A = 1\n)\nDue to the independency of rn\u00d7 cn across n \u2208 {1, ...,Nu} and their limited variance, the sum of these variables can be approximated with a normal random distribution as\n\u03bbi = Nu\n\u2211 n=1\n( rn\u00d7 cn ) | ( A = i ) , i \u2208 {\u22121,1} (14)\n\u03bbi \u223cN (mi,c,si,c) mi,c = Nu\u00d7E[r\u00d7 c|A = i]\nsi,c = \u221a Nu\u00d7Var[r\u00d7 c|A = i]\nFor determining the error of CoBaCS, the confidence level threshold of users is needed as well as their first-order decision variables and the number of users. By having these parameters, the expected error of CoBaCS can be written as follows.\n9/17\nE[errCoBaCS] =Pr(A =\u22121)\u00d7Pr(\u03bb1 >= 0) (15) +Pr(A = 1)\u00d7Pr(\u03bb2 < 0)\n=Pr(A =\u22121)\u00d7Q( \u2212m1,c\ns1,c )\n+Pr(A = 1)\u00d7 (1\u2212Q( \u2212m2,c\ns2,c ))"
        },
        {
            "heading": "4.3 Evaluations Environment",
            "text": "In this section, we evaluate the performance of these systems and compare them in terms of error. To do so, we simulate the recruited population of these cases. For computing error of ReBaCS, we consider that the recruited population is made of users with various performances. We choose the performance of each user randomly. With having users\u2019 performances, the parameters of Type I decision making described in Section 3 can be computed (\u00b5i,n i \u2208 {0,1},n \u2208 {1, ..,N}). Then, the expected parameters of decision-making will be obtained (\u00b5i,r i \u2208 {0,1}).\nWe assume each user has either low, medium or the high meta-cognitive ability for meta-cognition ability without loss of generality. A user with low meta-cognitive ability is one who rates his or her confidence low in the time of being correct and high when the answer is wrong (AUROC2 \u2248 0). The high meta-cognitive user acts the contrary: low confidence in wrong answers and high confidence in correct answers (AUROC2 \u2248 1). The medium meta-cognitive user is one who consistently rates confidence at the medium level (AUROC2 = 0.5). The users\u2019 meta-cognition ability is chosen randomly.\nWe assumed that 200 questions and 100 users were participating in both ReBaCS and CoBaCS systems. We have chosen these numbers to have enough questions to calculate the system\u2019s error correctly. Also, We aimed to have a reasonable number of users remain in the system after filtering. Considering 100,000 samples of the population with different performances and meta-cognitive ability, Figure 2 is obtained. As you can see in this figure, without having any filters on performance and meta-cognitive ability, based on performance, CoBaCS is in 52.38 % of viable populations better the ReBaCS. Although in some populations, CoBaCS outperforms ReBaCS, in general, without any pre-testing and filtering, there is no significant difference between ReBaCS and CoBaCS systems.\nIn figure 3, we have demonstrated the effect of different filters in the pretest phase on systems\u2019 performance. Users are filtered according to their performance in the horizontal direction and their meta-cognition ability in the vertical direction. According to this figure, in the populations where the experts (users with high performance) are present, it is wiser to use ReBaCS if we have no hard meta-cognition filter. But, when the users are not experts and have low performance, we can obtain better performance by filtering very low metacognitive users using CoBaCS. In the first column of figure 3, there are no criteria\n10/17\nfor users\u2019 performance, and with existing users, even with performance near 10%, the CoBaCS is much better when we filter very low metacognitive users.\nIn conclusion, according to the simulations\u2019 results, when the CoBaCS is used, and the population does not include experts, it is important to filter people with low meta-cognition ability. It is noteworthy to mention that for filtering users based on their performance, we should take pre-tests in the context of the main task, which is challenging. On the other side, the metacognition ability is not task dependent4, 5; we can compute users\u2019 metacognition first, and then assign different taks to the ones with no low metacognition ability. Also, in tasks with high difficulty, finding users who are expert is hard and time-consuming. But, if they are available with hard metacognition filters, CoBaCS has a lower error. Therefore, CoBaCS can increase performance and reduce the cost in the time of difficulty. In Section 5, we will examine our proposed system with a real-world task."
        },
        {
            "heading": "5 Experimental Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Tasks and Procedures",
            "text": "We consider three tasks to study the role of meta-cognitive ability as a filter and the effect of confidence reporting in crowdsourcing systems. The first task is a tweet task in which the participants are asked to guess the gender of the Twitter account owner by seeing a few tweets of him/her. The tweet task is designed to test the performance of a confidence-based crowdsourcing system, CoBaCS. Before the tweet task, we ran a pre-tweet task designed to evaluate the gender bias in different topics to rank the questions in the tweet task based on their difficulty. The other task is a word recognition memory task which was used to measure the meta-cognitive ability of subjects34, 35. The pre-tweet task was an online test using Google Forms. The memory and tweet tasks were computerized and programmed in MATLAB (Mathworks), using COGENT 2000 toolbox 1.\nSubjects: Eighty-six participants completed the pre-tweet task. Eighty-six participants (42 women, 44 men) with a mean age of 24.24 (range = 17 to 37) were instructed to do the other two tasks. In this study, there is no excluded subject. We paid subjects a fixed amount of money (15 thousand Tomans), so the incentive for all the subjects was the same. In addition, we ordered the participants according to their performance in the Memory task plus twice their performance in the tweet task. We paid a bonus (100 thousand Tomans) to the top three participants who were higher in our ordering.\nMemory task: The memory task was a Persian word recognition memory test based on34 and adapted from35. The study had two phases: encoding and recall. During the encoding phase, 100 words from a set containing 200 words were presented on the screen in a random order, separately and sequentially. In the recall phase, immediately after the first phase, all 200 words were displayed sequentially. Subjects decided whether each word was shown in the encoding phase or not by pressing 1 (yes) or 2 (no) on the keypad using their left hand. Next, they reported their confidence in being correct on a scale of 1 (low confidence) to 5 (high confidence) using a number pad with their right hand. The procedure is shown in Figure 4. Individuals did not receive any feedback on the correctness of their responses.\nIn the memory task, average memory performance of our subjects was 67.53% (standard deviation 7.42%, range 53% -86.5% ), comparable to the results reported in34 (mean accuracy 71%, standard deviation 9%, range 57-91%) and the results reported in35 (mean accuracy 71%, standard deviation 1%, range 68 - 73 %).\nThe average meta-cognitive ability of subjects was 0.62 (standard deviation 0.06, range 0.508- 0.784). Reported results on meta-cognitive accuracy were measured by area under ROC Type II curve, AUROC24, 32, 33.\nPre-tweet task. The goal of this task was to investigate how difficult it is to guess the gender of a Twitter account based on the topics that have been discussed in their tweets. We designed an online questionnaire using Google Forms, including 17 titles. Sixty eight participants answered the questioner; they were instructed to report male or female for each of the contents. There was not any time limit to answer the questionnaire.\nWe define the gender of a tweet based on the gender that more than 60% of the participants picked. A simple tweet is one in that the gender of the tweet is the same as the actual gender of its Twitter account. By the results of this task, we categorized selected tweets for the next task, the tweet task, into four groups based on the actual gender of the writer of tweets (female or male) and the difficulty of gender recognition according to the discussed topic (simple or complicated).\nTweet task. We collected a set of thousands of Twitter accounts whose tweets were in Persian. Then, one hundred accounts were selected, with more than one hundred followers and one hundred followings, and also had tweeted more than five hundred tweets. The last condition is to make sure that the accounts\u2019 owners are familiar with twitting and that we have enough tweets from them to choose for our task. We omitted the accounts that had a massive number of tweets, but they have joined Twitter in recent months from our selections to avoid bots. Half of the accounts\u2019 owners were women, and the other half were men. We\n1http://www.vislab.ucl.ac.uk/cogent.php\n11/17\n12/17\nchose three of the tweets of these one hundred accounts sudo-random; thus, we had three hundred tweets in the Persian language. The selection of accounts and tweets was that every four categories of Twitter accounts (female/male and difficult/simple) occurred exactly 25 times (a quarter of the total number of accounts). We call an account simple, and most of its selected tweets are simple (based on the definition in the pre-tweet task).\nAt first, all the participants were asked to submit their gender and age. Participants of the tweet task saw the three tweets of each account on the same page on the monitor, and they decided whether the account belonged to a male or female by pressing the key \"1\" or \"2\". Then, they reported their confidence of correctness by pressing keys 1 (i.e., lowest confidence) to 5 (i.e., highest confidence) on the number pad. There was no time limit to report the decision and confidence. At the end of the task, participants reported their estimation of the number of correct answers; then, they saw their actual performance.\nIn the tweet task, the average performance of our subjects was 61.79% (standard deviation 4.56%, range 49%-72% ). The mean of meta-cognitive ability was 0.58 (standard deviation 0.07, range 0.445- 0.777). In the literature, There is no similar task to our tweet task, so a comparison of results is impossible."
        },
        {
            "heading": "5.2 Behavioral Results",
            "text": "Data Analysis: We would like to study the influence of the performance filter and Metacognition filter on the error of two systems; ReBaCS and CoBaCS. Thus we needed two scores for each participant regarding performance and metacognitive ability. We saw that participants\u2019 performance in the first thirty trials of the tweet task was significantly correlated (Pearson correlation = 0.59, p < 0.05) with their performance in the remaining questions, the last seventy, of the tweet task. Therefore, we used the performance in the first thirty questions (including 15 questions about men tweet applicants and 15 questions about women tweet applicants) of the tweet task as each participant\u2019s performance score. For the score of metacognitive ability, we used the AUROC2 for each participant in the memory task. It is noticeable that the first thirty trials of the tweet task were not enough to measure metacognitive ability in the tweet task to use it as the score of metacognitive ability; thus, we measured this ability in the memory task.\nThen, we randomly sampled 60% of participants, and we used four filters for performance ability (0.4,0.5,0.6,0.7) and three filters for metacognitive ability (0.5,0.6,0.7). Thus we applied twelve coupled filters on the 60 sampled participants. For example, for the coupled filter (performance 0.4, metacognition 0.5), we excluded the participants whose performances were\n13/17\nlower than 0.4 and whose metacognitive abilities were lower than 0.5. To achieve the performance of the ReBaCS system, we saw the performance of the remaining people in the society after filtering in the last seventy questions in the tweet task according to the majority voting; society\u2019s answer to each question was the answer of the majority. Similarly, to achieve the performance of the CoBaCS system, we applied the WMV algorithm to measure the performance of the filtered society. The difference in society performance between the two MW/WMV algorithms and the 100% performance showed the error of two crowd-sourcing systems, ReBaCS and CoBaCS. In this way, we estimated the error of two crowd-sourcing systems with twelve filters for a randomly sampled society with 60 participants. We repeated the above process 100 times, randomly sampled a new society with 60 percent, and measured the error of two systems after implementing the twelve filters. The results of this analysis is shown in Table ?? and Table ??. After obtaining the results, we ran a one-tailed T-test to check if the ReBaCS error was significantly greater than the CoBaCS error. The p-value for the first meta filter (meta>0.5) and performance greater than 40% was 0.03. For the same meta condition and performance greater than 70%, the p-value was 0.01. For the rest, this value was below 2e\u22125. Therefore, for all conditions, the error of CoBaCs is significantly less than ReBaCS in this experiment. To test our hypothesis with the filter effect, we used a one-tailed T-test, and the result showed that regardless of the filter, the error of ReBaCS is significantly greater than the CoBaCS (p-value = 2e\u2212100).\nDiscussion. As you can see in tables ?? and ??, the mean of error of RebaCS is greater than the other system in almost every case, Even in the first filter (meta>0.5, performance>40%) which is equal to now having any filter (all the population meet these criteria). It is noteworthy to mention that this experiment tests our proposed system in one of the populations simulated in the previous section. Based on our simulation and the results shown in 3, we did not expect to obtain a better performance of CoBaCS in all the cases. Still, we wanted to test if the performance of CobaCS significantly exceeds ReBaCS.\nIn36, it is mentioned that the subjects\u2019 confidence is related to their response time; greater response time leads to less confidence. If this is true, we can measure the response time and not ask the users to rate their confidence. During our task, we measured the users\u2019 response time. According to the gathered data, in the memory task, this hypothesis is true (correlation coefficient is negative, p-value<0.05 for 71 users). On the other hand, in the tweet task, the correlation coefficient for just 42 users out of 86 is negative. This can be because the length of tweets is different, and each person has a different reading speed. Therefore, the findings in36 cannot be applied to all the tasks.\nAs we mentioned in 2, the DK effect (Dunning-Kruger effect) refers to the cognitive bias in which worthy person rates her or his performance low, and the person with actual low performance reports that she or he had done the task properly. Also, in26 is mentioned that using weighted majority voting in a population where the DK effect exists leads to more error compared with a majority voting-based system. To test whether this finding can be applied to our system, we asked users to rate their performance at the end of tweet tasks. As Figure 6 shows, in our population, the DK effect exists, but the CoBaCS performance was significantly better than the ReBaCS."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this paper, we first studied crowd-sourcing systems. Then, we proposed our method, CoBaCS, which asks the users for not only their responses but also their confidence. Furthermore, this system did not include heavy mathematical calculations and can be implicated and run easily in online and offline crowd-sourcing systems. Based on the research done in the field of meta-cognition, we measured the meta-cognition ability of the users and then used this measurement to select proper users. According to our findings, both mathematically and via a real-world experiment, our proposed system outperform the most popular crowd-sourcing system (ReBaCS). As a result of our simulation, when the task is hard, and there are not enough experts present in the system, using CoBaCS is a wiser choice. Also, because the meta-cognition ability is not task-dependent and can be measured once a user is asked a variety type of questions, one can use CoBaCS and filter users with low meta-cognition."
        }
    ],
    "title": "Design and Evaluation of Crowd-sourcing Platforms Based on Users\u2019 Confidence Judgments",
    "year": 2023
}