{
    "abstractText": "Naked eye recognition of age is usually based on comparison with the age of others. However, this idea is ignored by computer tasks because it is difficult to obtain representative contrast images of each age. Inspired by the transfer learning, we designed the Delta Age AdaIN (DAA) operation to obtain the feature difference with each age, which obtains the style map of each age through the learned values representing the mean and standard deviation. We let the input of transfer learning as the binary code of age natural number to obtain continuous age feature information. The learned two groups of values in Binary code mapping are corresponding to the mean and standard deviation of the comparison ages. In summary, our method consists of four parts: FaceEncoder, DAA operation, Binary code mapping, and AgeDecoder modules. After getting the delta age via AgeDecoder, we take the average value of all comparison ages and delta ages as the predicted age. Compared with state-of-the-art methods, our method achieves better performance with fewer parameters on multiple facial age datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ping Chen"
        },
        {
            "affiliations": [],
            "name": "Xingpeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Ye Li"
        },
        {
            "affiliations": [],
            "name": "Ju Tao"
        },
        {
            "affiliations": [],
            "name": "Bin Xiao"
        },
        {
            "affiliations": [],
            "name": "Bing Wang"
        },
        {
            "affiliations": [],
            "name": "Zongjie Jiang"
        }
    ],
    "id": "SP:d5388289be276c1c2ac7975350a0ae46a7818e7f",
    "references": [
        {
            "authors": [
                "Yuval Alaluf",
                "Or Patashnik",
                "Daniel Cohen-Or"
            ],
            "title": "Only a matter of style: age transformation using a style-based regression model",
            "venue": "ACM Trans. Graph,",
            "year": 2021
        },
        {
            "authors": [
                "Raphael Angulu",
                "Jules-Raymond Tapamo",
                "Aderemi O. Adewumi"
            ],
            "title": "A survey of aging and weathering phenomena in computer graphics",
            "venue": "EURASIP J. Image Vide.,",
            "year": 2018
        },
        {
            "authors": [
                "Kuang-Yu Chang",
                "Chu-Song Chen"
            ],
            "title": "A learning framework for age rank estimation based on face images with scattering transform",
            "venue": "IEEE TIP,",
            "year": 2015
        },
        {
            "authors": [
                "Kuang-Yu Chang",
                "Chu-Song Chen",
                "Yi-Ping Hung"
            ],
            "title": "Ordinal hyperplanes ranker with cost sensitivities for age estimation",
            "venue": "In CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "Shixing Chen",
                "Caojin Zhang",
                "Ming Dong",
                "Jialiang Le",
                "Mike Rao"
            ],
            "title": "Using ranking-cnn for age estimation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "LiJia Li",
                "Kai Li",
                "FeiFei Li"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Zongyong Deng",
                "Hao Liu",
                "Yaoxing Wang",
                "Chenyang Wang",
                "Zekuan Yu",
                "Xuehong Sun"
            ],
            "title": "Pml: Progressive margin loss for long-tailed age classification",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Dumoulin",
                "Jonathon Shlens",
                "Manjunath Kudlur"
            ],
            "title": "A learned representation for artistic style",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Nickolaos F. Fragopanagos",
                "John G. Taylor"
            ],
            "title": "Emotion recognition in human-computer interaction",
            "venue": "Neural Networks,",
            "year": 2005
        },
        {
            "authors": [
                "Bin-Bin Gao",
                "Chao Xing",
                "Chen-Wei Xie",
                "Jianxin Wu",
                "Xin Geng"
            ],
            "title": "Deep label distribution learning with label ambiguity",
            "venue": "IEEE TIP,",
            "year": 2017
        },
        {
            "authors": [
                "Leon A. Gatys",
                "Alexander S. Ecker",
                "Matthias Bethge"
            ],
            "title": "Image style transfer using convolutional neural networks",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Xin Geng",
                "Kate Smith-Miles",
                "Zhi-Hua Zhou"
            ],
            "title": "Facial age estimation by learning from label distributions",
            "venue": "In AAAI,",
            "year": 2010
        },
        {
            "authors": [
                "Xin Geng",
                "Chao Yin",
                "Zhi-Hua Zhou"
            ],
            "title": "Deep differentiable random forests for age estimation",
            "venue": "IEEE TPAMI,",
            "year": 2021
        },
        {
            "authors": [
                "Guodong Guo",
                "Yun Fu",
                "Charles R. Dyer",
                "Thomas S. Huang"
            ],
            "title": "Image-based human age estimation by manifold learning and locally adjusted robust regression",
            "venue": "IEEE TIP,",
            "year": 2008
        },
        {
            "authors": [
                "Guodong Guo",
                "Guowang Mu"
            ],
            "title": "Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression",
            "venue": "In CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "Guodong Guo",
                "Guowang Mu",
                "Yun Fu",
                "Thomas S. Huang"
            ],
            "title": "Human age estimation using bio-inspired features",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Hu Han",
                "Charles Otto",
                "Xiaoming Liu",
                "Anil K. Jain"
            ],
            "title": "Demographic estimation from face images: Human vs. machine performance",
            "venue": "IEEE TPAMI,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Zhouzhou He",
                "Xi Li",
                "Zhongfei Zhang",
                "Fei Wu",
                "Xin Geng",
                "Yaqing Zhang",
                "Ming-Hsuan Yang",
                "Yueting Zhuang"
            ],
            "title": "Data-dependent label distribution learning for age estimation",
            "venue": "IEEE TIP,",
            "year": 2017
        },
        {
            "authors": [
                "Dong Huang",
                "Longfei Han",
                "Fernando De la Torre"
            ],
            "title": "Softmargin mixture of regressions",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Xun Huang",
                "Serge J. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Peipei Li",
                "Yibo Hu",
                "Xiang Wu",
                "Ran He",
                "Zhenan Sun"
            ],
            "title": "Deep label refinement for age",
            "venue": "estimation. PR,",
            "year": 2020
        },
        {
            "authors": [
                "Wanhua Li",
                "Jiwen Lu",
                "Jianjiang Feng",
                "Chunjing Xu",
                "Jie Zhou",
                "Qi Tian"
            ],
            "title": "Bridgenet: A continuity-aware probabilistic network for age estimation",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "RFarkhod Makhmudkhujaev",
                "Sungeun Hong",
                "In Kyu Park"
            ],
            "title": "Re-aging gan: Toward personalized face age transformation",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Stephane Merillou",
                "Djamchid Ghazanfarpour"
            ],
            "title": "A survey of aging and weathering phenomena in computer graphics",
            "venue": "Computers & Graphics,",
            "year": 2008
        },
        {
            "authors": [
                "Zhenxing Niu",
                "Mo Zhou",
                "Le Wang",
                "Xinbo Gao",
                "Gang Hua"
            ],
            "title": "Ordinal regression with multiple output cnn for age estimation",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Roy OrEl",
                "Soumyadip Sengupta",
                "Ohad Fried",
                "Eli Shechtman",
                "Ira Kemelmacher-Shlizerman"
            ],
            "title": "Lifespan age transformation synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Sveinn Palsson",
                "Eirikur Agustsson",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Generative adversarial style transfer networks for face aging",
            "venue": "In CVPR workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Hongyu Pan",
                "Hu Han",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "title": "Mean-variance loss for deep age estimation from a face",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Gabriel Panis",
                "Andreas Lanitis",
                "Nicolas Tsapatsoulis",
                "Timothy F. Cootes"
            ],
            "title": "Overview of research on facial ageing using the fg-net ageing database",
            "venue": "IET Biometrics,",
            "year": 2016
        },
        {
            "authors": [
                "Karl Ricanek",
                "Tamirat Tesafaye"
            ],
            "title": "Morph: A longitudinal image database of normal adult age-progression",
            "venue": "In FG,",
            "year": 2006
        },
        {
            "authors": [
                "Rasmus Rothe",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Dex: Deep expectation of apparent age from a single image",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Rasmus Rothe",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Deep expectation of real and apparent age from a single image without facial landmarks",
            "year": 2018
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew G. Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Wei Shen",
                "Yilu Guo",
                "Yan Wang",
                "Kai Zhao",
                "Bo Wang",
                "Alan L. Yuille"
            ],
            "title": "Deep differentiable random forests for age estimation",
            "venue": "IEEE TPAMI,",
            "year": 2021
        },
        {
            "authors": [
                "Xiangbo Shu",
                "Jinhui Tang",
                "Zechao Li",
                "Hanjiang Lai",
                "Liyan Zhang",
                "Shuicheng Yan"
            ],
            "title": "Personalized age progression with bi-level aging dictionary learning",
            "venue": "IEEE TPAMI,",
            "year": 2018
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Vadim Lebedev",
                "Andrea Vedaldi",
                "Victor S. Lempitsky"
            ],
            "title": "Texture networks: Feed-forward synthesis of textures and stylized images",
            "venue": "In ICML,",
            "year": 2016
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor S. Lempitsky"
            ],
            "title": "Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Xin Wen",
                "Biying Li",
                "Haiyun Guo",
                "Zhiwei Liu",
                "Guosheng Hu",
                "Ming Tang",
                "Jinqiao Wang"
            ],
            "title": "Adaptive variance based label distribution learning for facial age estimation",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Tsun-Yi Yang",
                "Yi-Hsuan Huang",
                "Yen-Yu Lin",
                "Pi-Cheng Hsiu",
                "Yung-Yu Chuang"
            ],
            "title": "Ssr-net: A compact soft stagewise regression network for age estimation",
            "venue": "In IJCAI,",
            "year": 2018
        },
        {
            "authors": [
                "Chao Zhang",
                "Shuaicheng Liu",
                "Xun Xu",
                "Ce Zhu"
            ],
            "title": "C3ae: Exploring the limits of compact model for age estimation",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Yunxuan Zhang",
                "Li Liu",
                "Cheng Li",
                "Chen Change Loy"
            ],
            "title": "Quantifying facial age by posterior of age comparisons",
            "venue": "In BMVC,",
            "year": 2017
        },
        {
            "authors": [
                "Qilu Zhao",
                "Junyu Dong",
                "Hui Yu",
                "Sheng Chen"
            ],
            "title": "Distilling ordinal relation and dark knowledge for facial age",
            "venue": "estimation. TNNLS,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Facial age estimation has been an active research topic in the computer version, for its important role in humancomputer interaction [9,40], facial attribute analysis [2,29], market analysis [2], and so on. After the rise of deep learning, many deep structures, such as VGG [41], ResNet [18], MobileNet [38], have been used as feature learning methods to solve the problem of facial age estimation [7, 45, 46].\nIn general, the methods for facial age estimation can be grouped into three categories: regression methods, classification methods, and ranking methods [27, 33]. The age regression methods consider labels as continuous numerical values [15, 30]. Except for the universal regression, researchers also proposed hierarchical models [17] and the\nsoft-margin mixture of regression [21] to handle the heterogeneous data. Facial age classification approaches usually regard different ages or age groups as independent category labels [16], which can be divided into single-label learning and label distribution learning methods [7]. The single label learning [16,37] treats each age independently, ignoring the fact that facial images of similar ages are very similar. Label distribution learning methods [12, 13, 20, 39] learn a label distribution that represents the relative importance of each label when describing an instance. This method is to compare the distance or similarity between the distribution predicted by the model and the actual distribution [7]. Nevertheless, acquiring distributional labels for thousands of face images itself is a non-trivial task. The ranking approaches treat the age value as rank-ordered data and use multiple binary classifiers to determine the rank of the age in a facial image [3\u20135].\nAlthough the above methods study the problem of facial age estimation from different emphases, they all belong to the perspective of computer vision, which can be summarized as feature extraction and modeling to predict age. This is different from the mechanism of the naked human eye recognizing age, which is obtained by comparing the current experience information with most humans. Because it is difficult to get representative age images of different races, computer tasks often ignore the idea of comparative learning. The style image can also be a contrast in style transfer learning. [23, 24]. Inspired by this, we propose a Delta Age Adaptive Instance Normalization operation (DAA) to obtain representative results of each age through transfer learning. We want to transfer the current image into a style map of each comparative age. And then learn the feature difference between the current age and all the comparative ages. Finally, the predicted age is obtained based on the comparative age difference. Style images\u2019 mean and standard deviation are the keys to style transfer, and the random value cannot reflect the process of aging. We convert all ages into unique 8-bit binary codes and then learn comparative ages\u2019 mean and standard deviation vecar X iv :2 30 3.\n07 92\n9v 1\n[ cs\n.C V\n] 1\n4 M\nar 2\ntors through the fully connected layer. The experiment results on four challenging age datasets demonstrate that our approach outperforms state-of-the-art methods.\nThe main contributions of this paper are as follows:\n\u2022 We designed the Delta Age AdaIN (DAA) operation based on the idea of human eye contrast learning. \u2022 To ensure that the delta age after transfer reflects continuity, we convert the natural number of ages into binary code. Finally, 100 delta ages feature maps will be generated for each content feature map. \u2022 We designed a network based on age transfer learning to realize robust age estimation, achieving excellent performance on four datasets."
        },
        {
            "heading": "2. Related works",
            "text": ""
        },
        {
            "heading": "2.1. Facial age estimation",
            "text": "From the perspective of machine learning, facial age estimation can be regarded as two steps: feature extraction and modeling. The regression methods [15, 30], classification methods [16, 37], and ranking methods [3\u20135] for age estimation are paid more attention to put forward different research methods according to label information. Regression methods regard labels as continuous values, and classification regards labels as independent values. While the ranking approach treats labels as rank-order data. These methods gradually consider aging a slow and continuous process, which means processing label information is essential.\nBesides, some researchers learn label distribution to represent the relative importance of each label [12, 13, 20, 39], which can also be seen as a special facial age classification method. Label distribution is a hot research direction at present, but acquiring distributional labels for thousands of face images itself is a non-trivial task.\nThe main role of the deep learning method in age estimation is feature extraction. In Ranking-CNN [5], DEX [36], AP [47], DLDL [10], and other papers, the model usually adopts the deep structures such as AlexNet, VGG, and ResNet as the feature extraction module. In addition to feature extraction and modeling methods, some scholars also focus on the objective optimization function [7, 33]. MLloss [33] proposed mean-variance loss for robust age estimation via distribution learning. Deng et al. [7] proposed progressive margin loss (PML) for long-tailed age classification, aiming to adaptively refine the age label pattern by enforcing a couple of margins.\nThe direct use of deep structure will cause huge model parameters, so many scholars try to compress the deep model structure for age estimation. Some lightweight structures are introduced into facial age estimation, such as OCRNN [30], MRCNN [30], MobileNet [38], and so on. Besides, Yang et al [45] proposed a compact soft stagewise regression network (SSR-Net), which reduced the parameters to 40Kb. And Zhang et. al [46] propose an extremely compact yet efficient cascade context-based age estimation model(C3AE)."
        },
        {
            "heading": "2.2. Style transfer and adaptive instance normalization",
            "text": "Style transfer is a fascinating work; the ideas contained in it are worth thinking about deeply. In 2016, Gatys et al. [11] realized style transfer by calculating two images\u2019 content and style distance. Ulyanov et al. [42] proposed a trained generator, which applies batch normalization (BN). This research found that replacing BN with instance normalization (IN) can significantly improve the convergence speed [43]. Dumoulin et al. [8] found that images with different styles can be generated by using different scales and displacements during IN operation, also known as conditional instance normalization (CIN). Huang et al. [22] proposed that the artistic style of the image is the cross-spatial statistical information of each feature channel of the feature graph, such as mean and variance. Style transfer can be realized by transferring each channel\u2019s mean and standard deviation. This operation is named adaptive instance normalization (AdaIN), which ensures transferring any styles to the feature maps.\nStyleGAN [23] draws on the idea of AdaIN style transfer, removes the traditional input, and proposes a stylebased generator, which takes a learnable constant as input. It mainly controls the visual features represented by each level by modifying the input of each level separately without affecting other levels. And StyleGAN2 [24] adjusted the use of AdaIN to avoid water droplet artifacts effectively. With the advantage of GAN network, some scholars try to generate high-quality facial age images. LATS [31] presented a method for synthesizing lifespan age transformations. RAGAN [28] introduces a personalized self-guidance scheme that enables transforming the input face across various target age groups while preserving identity."
        },
        {
            "heading": "3. Proposed Approach",
            "text": "Fig.1 shows the overall architecture of our proposed approaches, containing four components: FaceEncoder module, DAA operation, Binary code mapping module, and AgeDecoder module.\nOur designed network needs two modal inputs. One is the facial age images X , fed into the FaceEncoder module. The other is the 8-bit binary code z of age natural number, entered into the binary code mapping module aiming to learn a set of data reflecting each age characteristic."
        },
        {
            "heading": "3.1. FaceEncoder Module",
            "text": "The FaceEncoder module aims to do feature extraction via deep learning models. Let the input image of our approach as X \u2208 R3\u00d7H\u00d7W , where 3, H,W denotes the channel, height, and width, respectively. After the feature extraction structure, the output is as follows.\nE = fE(X) (1)\nwhere E = [E0, E1, \u00b7 \u00b7 \u00b7 , EC\u22121] \u2208 RC\u00d7h\u00d7w. Except for the ResNet18, we also apply the C3AE [46], which is a famous lightweight network for age estimation.\nIn the next subsection, we will do a transfer operation. Followed by AdaIN and StyleGAN, we need to calculate the mean and standard deviation value of Ec \u2208 R1\u00d7h\u00d7w, and c = {0, 1, \u00b7 \u00b7 \u00b7 , C \u2212 1}."
        },
        {
            "heading": "3.2. Delta Age AdaIN operation",
            "text": "Delta Age AdaIN (DAA) operation is the most essential operation in our method. As mentioned earlier, we hope to estimate the age by comparing the current image with the most representative images of all ages. However, usual characteristic information of all ages is usually challenging to obtain. In style transfer learning [22\u201324], the mean and standard deviation are considered to be the most representative of the image style. Inspired by this, we hope that the representative information of each age can be obtained via the transfer learning. Then, the feature difference was obtained by comparing the features of input age with transferred age. This is our proposed DAA operation.\nLet E(x) = {E0(x), \u00b7 \u00b7 \u00b7 , EC\u22121(x)} be the learned feature E of age x, \u00b5(x) = (\u00b50(x), \u00b51(x), \u00b7 \u00b7 \u00b7 , \u00b5C\u22121(x)) and \u03c3(x) = (\u03c30(x), \u03c31(x), \u00b7 \u00b7 \u00b7 , \u03c3C\u22121(x)) are the mean and standard deviation of feature E(x), calculated as follows.\n\u00b5c(x) = 1\nh\u00d7 w h\u22121\u2211 i=0 w\u22121\u2211 j=0 Ec(x)\n\u03c3c(x) = \u221a\u221a\u221a\u221a 1 h\u00d7 w h\u22121\u2211 i=0 w\u22121\u2211 j=0 (Ec(x)\u2212 \u00b5c(x))2 + \u03f5\nInspired by literature [1, 32] based on style transfer to complete face aging, we use AdaIN [22] for age estimation. The AdaIN has the following formula.\nAdaIN(E(x), E(y)) = \u03c3(y) E(x)\u2212 \u00b5(x)\n\u03c3(x) + \u00b5(y) (2)\nwhere x is the age label of input X , and y is the style age from 0 to 99, E(x), E(y) \u2208 RC\u00d7h\u00d7w can be seen as the feature of the content image and style image, respectively. This process is also shown in Fig. 2 (a).\nWe do not need to decode the style feature into the style image but combine it with the content feature to get the feature difference for estimating the age difference.Let AdaIN(E(x), E(y)) denote the style feature of age x to y, and AdaIN(E(x), E(x)) denotes the content feature of age x. Following the facial aging process, we can get the feature difference of age difference by AdaIN(E(x), E(y)) \u2212 AdaIN(E(x), E(x)). This process can be described with DAA operations as Eq.(3).\n\u03b4(x, y) = (\u03c3(y)\u2212\u03c3(x))E(x)\u2212 \u00b5(x) \u03c3(x) +\u00b5(y)\u2212\u00b5(x) (3)\nwhere \u03b4(x, y) \u2208 RC\u00d7h\u00d7w denote the feature difference between age x and y. Eq. (3) is a single channel transfer strategy with each feature map having its own mean and standard deviation. This process is shown in Fig 2 (b).\nAccording to StyleGAN [23], and SAM [1], one channel of the deep feature can represent an attribute, so we consider all channel information simultaneously to obtain more representative comparative age transfer results. And the mean and standard deviation of the style feature of age y can be recalculated as follows.\n\u00b5y = 1\nh\u00d7 w \u00d7 C h\u22121\u2211 i=0 w\u22121\u2211 j=0 C\u22121\u2211 c=0 E(y)\n\u03c3y = \u221a\u221a\u221a\u221a 1 h\u00d7 w \u00d7 C h\u22121\u2211 i=0 w\u22121\u2211 j=0 C\u22121\u2211 c=0 (E(y)\u2212 \u00b5y)2 + \u03f5 (4)\nThen, the DAA operation can be rewritten as Eq.(5)\n\u03b4c(x, y) = (\u03c3y\u2212\u03c3c(x)) Ec(x)\u2212 \u00b5c(x)\n\u03c3c(x) +\u00b5y\u2212\u00b5c(x) (5)\nwhere \u03b4(x, y) = [\u03b40(x, y), \u00b7 \u00b7 \u00b7 , \u03b4C\u22121(x, y)]. Eq.(5) is a multi-channel transfer strategy with all feature maps sharing a mean and standard deviation. Let fD(\u03b4) be the nonlinear function that decodes feature difference into age difference, denoted as y \u2212 x = fD(\u03b4(x, y)).\nAssuming that there are template images representing a history of 0 to 99 years, it is easy to obtain 100 sets of feature differences for each age image through DAA operation, shown as Fig. 2 (c). Then, a more robust age estimation method can be obtained through multiple feature dif-\nferences and their corresponding template age labels.\nx \u2032 =\n1\n100 99\u2211 y=0 (y \u2212 fD(\u03b4(x, y))) (6)\nwhere x \u2032\nis the predicted age for input X . However, there are also enormous differences between images of the same age due to the influence of living environment, race, etc. Therefore, it is difficult to find an adaptive template for all ages. Consequently, we further design a binary code mapping module to replace the age templates to realize the robust age estimation by DAA operation."
        },
        {
            "heading": "3.3. Binary code mapping",
            "text": "In the DAA operation, the age is estimated through different age image templates, similar to the naked eye estimation. However, it is difficult to obtain representative contrast images. Therefore, we hope to use random input latent code [23] to learn the mean (\u00b5y) and standard deviation (\u03c3y) of style feature of age y. And the phenomenon that age is a gradual aging process also corresponds to the representative image of each age, i.e., it has the characteristics of continuity. Obviously, the random input cannot meet this requirement. Therefore, we replace the random latent code with an 8-bit binary code for all-natural age values based on the age range and the characteristics of binary coding. The uniqueness and continuity of binary coding are why we adopt this operation.\nzy = bin(y + 1)\nwhere bin is a function that converts a decimal to binary. And y \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , 99}, z = [z0, \u00b7 \u00b7 \u00b7 , z99] \u2208 R100\u00d78, where 100 corresponds to the age ranging from 0 to 99, and 8 denotes the bit. Then, z is normalized and got Z0.\nSimilar to style transfer, we learn two 100-dimensional values, which correspond to the mean and standard deviation of the style features, respectively. After three fully connected layers, the Z0 is learned to two values.\nZi = f B i (wi \u2217 Zi\u22121 + bi)\nwhere i = 1, 2, 3 denotes the three FC layers, fB1 (\u00b7) and fB2 (\u00b7) apply the ReLU activation, fB3 (\u00b7) applies identify function, wi and bi denotes the weight and bias of the ith FC layer. For lightweight purposes, the number of nodes in three FC layers is {16, 32, 2}. We find that the reasoning speed of the binary module is very fast. Sometimes it only needs one training to get a better transfer value.\nZ3 can be expressed as two vectors Z3 = {S, T}, where S = [s0, s1, \u00b7 \u00b7 \u00b7 , s99] and T = [t0, t1, \u00b7 \u00b7 \u00b7 , t99].\nAfter learning S, T corresponding to each style age, the DAA operation Eq. (5) can be rewritten as Eq. (7)\n\u03b4c(x, y) = (sy \u2212 \u03c3c(x)) Ec(x)\u2212 \u00b5c(x)\n\u03c3c(x) + ty \u2212 \u00b5c(x) (7)\nAnd E(\u03b4) = [\u03b4(x, 0), \u00b7 \u00b7 \u00b7 , \u03b4(x, 99)] is the continuous feature differences between each age from 0 to 99 and age x, and E(\u03b4) \u2208 R100\u00d7C\u00d7h\u00d7w.\nIt is worth noting that Z0 is only used in training, and in the test phase, we only need to use Z3 for DAA operations."
        },
        {
            "heading": "3.4. AgeDecoder",
            "text": "After the Binary code mapping module and DAA operation, we can get the continuous feature difference between ages 0 to 99. And we use AgeDecoder module to learn the nonlinear function fD(\u03b4)) mentioned in Eq.(6). Our AgeDecoder module contains a nonlinear module and a linear regression module. Then, we get the age difference via the regression module.\n\u2206 = fD(E(\u03b4))\n= Regression(GAP (Conv(E(\u03b4)))) (8)\nwhere \u2206 = [\u22060, \u00b7 \u00b7 \u00b7 ,\u220699], and \u2206y \u2208 R1 is the delta age between age x and the style age y from 0 to 99. And Regression is a fully-connected layer, Conv denotes the convolution operation with a kernel size of 3, and its output channel is 64 in our experiments. GAP is the global average pooling.\nThen the Eq.(6) can be rewritten as Eq.(9).\nx \u2032 =\n1\n100 99\u2211 y=0 (y \u2212\u2206y) (9)\nThe regression loss can be written as Eq.(10).\nLoss = SL1(x\u2212 x \u2032 ) (10)\nwhere SL1(\u00b7) denotes the smooth l1 loss. During the training stage, we performed DAA operations for each age group to ensure continuity. In the test stage, we perform the DAA operation by selecting the age at equal intervals to speed up reasoning, which ensures continuity and robustness. For more details, see ablation experiments."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Datasets and Metrics",
            "text": "Datasets. Morph [35] is the most popular dataset for facial age estimation, consisting of 55134 face images from 13617 subjects, and age ranges from 16 to 77 years. In our experiments, we follow the setting in C3AE [46], where the dataset was randomly divided into the training part (80%) and the testing part (20%).\nFG-Net [34] contains 1002 facial images from 82 subjects, where the age ranges from 0 to 69. In experiment, we adopt the setup of paper [7, 14, 27, 33], which uses leaveone person-out (LOPO) cross-validation. Hence, the average performance over 82 splits is reported, which makes the time of each training longer, and the MAEs fluctuate greatly in each split.\nIMDB-Wiki [37] consists of 523051 facial images of celebrities, crawled from IMDB and Wikipedia, and age ranges from 0 to 100. Since there is much noise in this dataset, we selected about 300,000 images for training, where all non-face and severely occluded images were removed.\nMegaAge-Asian [47] is newly facial age dataset consisting of 40,000 Asian faces with ages ranging from 0 to 70 years old. It helps to increase the diversity of human races and improve the generalization ability of the model. Followed by the setting of [45,47], 3,945 images were reserved for testing, and the remains are treated as the training set. This dataset applies the cumulative accuracy (CA) [47] as the evaluation metric.\nMetrics. We take the Mean Absolute Error (MAE) in the experiments to calculate the discrepancy between the estimated age and the ground truth. Obviously, the lower the MAE value, the better performance it achieves.\nFor the Mega-Age dataset, we also choose cumulative accuracy (CA) as the evaluation metric, which is defined as\nCA(n) = Kn K \u00d7 100\nin which K is the total number of testing images and Kn represents the number of testing images whose absolute errors are smaller than n."
        },
        {
            "heading": "4.2. Implementation Details",
            "text": "The input images are cropped to 3 \u00d7 128 \u00d7 128. We randomly augmented all images in the training stage with horizontal flipping, scaling, rotation, and translation.\nTo reflect the universality and further highlight the performance of the proposed DAA method, we choose the classical network structure ResNet18 [19] with fewer parameters instead of ResNet34 and the lightweight network C3AE [46] for experiments on the Morph dataset. From the perspective of training time and video card resources, we choose ResNet18 with fewer parameters than ResNet34 as the backbone. And in the C3AE network, we do not use multi-scale but use the C3AE(plain) network. Limited by the length of the article, only the ResNet18 network is used for experiments on other datasets. When applying ResNet18 and C3AE as the frame of the FaceEncoder module, we also do a few changes to satisfy the age image. The kernel size of the first convolution layer is changed to 3, and the first maximum pooling layer is discarded in ResNet18. Besides, the output E of the FaceEncoder module using ResNet18 is the output of the last stage with the dimension of 512 \u00d7 8 \u00d7 8. When using C3AE, the dimension of E is 32\u00d7 8\u00d7 8.\nTo have high performance and make a fair comparison, we also pretrained on the IMDB-Wiki dataset, which is similar to DEX [36] and MV [33] methods.\nFor all experiments, we employed the Adam optimizer [25], where the weight decay and the momentum were set to 0.0005 and 0.9, respectively. Our DAA network is trained for 200 epochs with a batch size of 128. The initial learning rate was set to 0.001 and changed according to cosine learning rate decay. We trained our model with PyTorch on an RTX 3080 GPU."
        },
        {
            "heading": "4.3. Experiments Results and Analysis",
            "text": "The DAA operation we designed can be transplanted to different feature extraction networks for age estimation. Our methods are expressed as Resnet18+DAA and C3AE+DAA. C3AE [46] is a lightweight network with a multi-scale image input, aiming to show that the designed DAA is also a lightweight operation. Following the design in C3AE [46], we also give the \u201dplain\u201d configuration, applying single-scale input.\nComparisons on Morph dataset. As is shown in Table 1, the upper part of the table is the method with large parameters, while the lower part is the lightweight network. Compared with Ranking-CNN, AP, MV, and PML, our proposed method achieved the best MAEs value of 2.06 with the pre-trained operation. At the same time, the parameters of our approach are significantly reduced compared with other methods. Compared with \u201dVGG+Distillation\u201d norm version, our DAA is lower but with much fewer parameters.\nOur proposed method can also achieve excellent performance when applying C3AE (plain) [46] as the FaceEncoder module. Compared with ORCNN, MRCNN, and MobileNet, the MAEs value of our C3AE+DAA reduced by more than 0.62. Compared with only 40K lightweight net-\nwork SSR and C3AE, our method has a similar amount of parameters, but the MAEs value is further reduced. Even compared with the C3AE network of multi-scale image input, the C3AE of single input using DAA operation is still reduced by 0.1. Compared with \u201dVGG+Distillation\u201d small version, our DAA can achieve better performance with much fewer parameters.\nComparisions on FG-Net. As shown in Table 2, we compared our model with the state-of-the-art models on FG-Net. Our DAA network achieves 2.19 on the FG-Net dataset, which is only a little higher than the PML method in performance. It is better than the other pretrained model, such as MV-loss.\nTable 2 also shows the relationship between MAEs and parameters of different models. Compared with MV models, our DAA model reduced the MAEs from 2.68 to 2.19\nand decreased the parameters from 138M to 11M. Although the parameter of our method is higher than the C3AE model, the MAEs value decrease by 0.65.\nComparision on MegaAge-Asian dataset. The experiment result of the Asian facial dataset is shown in Table 3. As seen from the table, the previous methods used pre-training operations on the IMDB-Wiki dataset. Compared with the pre-trained Posterior method, our approach increases 6.74, 4.46, and 2.28 on each metric. On CA(3) and CA(5), our DAA model is 13.92 and 10.75 higher than SSR-Net. Compared with the distillation method which is pre-trained on ImageNet [6], IMDB-WIKI, and AFAD [30], our DAA method also achieves higher performance.\nComparision on IMDB-clean dataset. The experiment shown in Table 4 indicates that our approach can achieve a better MAEs value with much fewer parameters. Similar to the results on FG-Net, the parameter is higher than the C3AE model, and the MAEs value decrease by 5.17."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "In the ablation experiment, we apply ResNet18 as the frame of the FaceEncoder module. Three experiments to illustrate the effectiveness of DAA operation, visualization of S, T value learned via Binary code mapping module, and comparison of reasoning time\nExperiment1: Effectiveness of DAA operation. We designed three baseline models to show the effectiveness of the DAA operation and binary code mapping. Base-\nline 1 is a network without the DAA operation. Baselines 2 and 3 adopt DAA operation using age template, corresponding to Eq. (3) and (5), respectively. Because the age template is difficult to obtain, we adopted the average value of 10 random experiments. We randomly selected an image from each age as a template and continued to use these templates in subsequent training and testing. The DAA operation with the binary transfer follows Eq. (7).\nAs shown in Table 5, the performance of the three models using DAA has been improved. Single channel and multi-channel represent DAA operation using equations Eq. (3) and (5), respectively. The experimental results show that the DAA operation using the mean of all channel information is more consistent with the style transfer of age. The DAA operation applying binary code mapping improved the Morph containing different races and the MegaAge dataset with fewer racial differences. This shows that the age templates that are difficult to obtain can be perfectly replaced by binary code mapping, thus reflecting the effectiveness of binary code mapping.\nExperiment 2: Learned two values in the Binary code mapping module.\nThe effectiveness of binary code mapping has been demonstrated in Table 5, and we further analyzed it by visualization, as shown in Fig. 3. We can draw interesting conclusions from the meaning of the normalized value S and T . Each value on the S and T curve reflects the dataset\u2019s standard deviation and mean of the current age.\nThe T curve shows that the mean value of the age image increases with age. This is consistent with the changing process of face images in reality. As age grows older, more and more wrinkles and spots will appear. This increases the complexity of the facial features, resulting in an increase in the average. Although the mean value increases, the difference between different images of the same age is getting smaller and smaller, which makes the standard deviation show a downward trend. The S value we learned just fits this situation. The two values in Morph dataset is higher than that in the MegaAge-Asian dataset. This is because Morph contains facial pictures of multiple races, and the amount is less.\nExperiment 3: Reasoning time analysis. According to the curve in Figure 3, we believe that the transfer process of DAA from a feature difference to an age difference is continuous and approximately linear. Then, we further verify this phenomenon by selecting \u201dS\u201d and \u201dT\u201d in different parts, whose results are shown in Table 6 and 7. During the experiment, we use all ages during training and partial \u201dS\u201d and \u201dT\u201d values in the test.\nTo illustrate their continuity, we analyze the learned S and T with different intervals. Our method is plug and play, so the reasoning time here does not include FaceEncoder. Table 6 shows the running time of our approach with different intervals. The GPU is an RTX3080, and the CPU is Intel i5-8265U. In this table, \u201dinterval\u201d represents the 100 S and T are grouped by the interval and then selected\nby sequence numbers. This process can be described as sampling, i.e. S100/interval = [si\u2217interval] 100/interval\u22121 i=0 , T100/interval = [ti\u2217interval] 100/interval\u22121 i=0 . For example, when interval = 5, S20 = [si\u22175]19i=0. As is exhibited in Table 6, the running time of different intervals under different settings is almost the same. Experiment 4: Continuity of learned two values. Table 6 and Table 7 jointly show that the learned \u201dS\u201d and \u201dT\u201d have continuity, and sampling with intervals from 100 values can still have similar performance. The smaller the number of samples, the performance will be slightly reduced, but the reasoning time will be faster."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this work, we have proposed a Delta Age AdaIN operation (DAA) to obtain representative results of each age through transfer learning. The proposed DAA is a lightweight and efficient feature learning network. Our DAA will transfer each content map into 100 delta age maps via learned S and T corresponding to the style of each age. We set the input of transfer learning to binary code form to obtain continuous image feature information. With the characteristic of the uniqueness and continuity of binary coding, we make the fused feature information continuous through DAA operation. The designed module transfers the learned values in the binary code mapping module to feature maps learned by FaceEncoder module. Experiments on four datasets demonstrate the effectiveness of our approach. In future works, we will focus on the constraints in the training process to further improve the potential transfer effect. We will also discuss the probability distribution and corresponding interpretation of binary transfer."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported by Natural Science starting project of SWPU (No.2022QHZ023) and Sichuan Scientific Innovation Fund (No. 2022JDRC0009)."
        }
    ],
    "title": "DAA: A Delta Age AdaIN operation for age estimation via binary code transformer",
    "year": 2023
}