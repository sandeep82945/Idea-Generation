{
    "abstractText": "We study the graph alignment problem over two independent Erd\u0151s-R\u00e9nyi graphs on n vertices, with edge density p falling into two regimes separated by the critical window around pc = \u221a logn/n. Our result reveals an algorithmic phase transition for this random optimization problem: polynomial-time approximation schemes exist in the sparse regime, while statistical-computational gap emerges in the dense regime. Additionally, we establish a sharp transition on the performance of online algorithms for this problem when p lies in the dense regime, resulting in a \u221a 8/9 multiplicative constant factor gap between achievable and optimal solutions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hang Du"
        },
        {
            "affiliations": [],
            "name": "Shuyang Gong"
        }
    ],
    "id": "SP:f59758470e7af6828e21f99321b583324266a846",
    "references": [
        {
            "authors": [
                "N. Alon",
                "J.H. Spencer"
            ],
            "title": "The probabilistic method",
            "venue": "Wiley Publishing, 4th edition",
            "year": 2016
        },
        {
            "authors": [
                "R. Arratia",
                "L. Gordon"
            ],
            "title": "Tutorial on large deviations for the binomial distribution",
            "venue": "Bulletin of mathematical biology, 51(1):125\u2013131",
            "year": 1989
        },
        {
            "authors": [
                "A. Berg",
                "T. Berg",
                "J. Malik"
            ],
            "title": "Shape matching and object recognition using low distortion correspondences",
            "venue": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), volume 1, pages 26\u201333 vol. 1",
            "year": 2005
        },
        {
            "authors": [
                "B. Barak",
                "C.-N. Chou",
                "Z. Lei",
                "T. Schramm",
                "Y. Sheng"
            ],
            "title": "Nearly) efficient algorithms for the graph matching problem on correlated random graphs",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "R.E. Burkard",
                "E. Cela",
                "P.M. Pardalos",
                "L.S. Pitsoulis"
            ],
            "title": "The quadratic assignment problem",
            "venue": "Handbook of combinatorial optimization, pages 1713\u20131809. Springer",
            "year": 1998
        },
        {
            "authors": [
                "A.S. Bandeira",
                "A. Perry",
                "A.S. Wein"
            ],
            "title": "Notes on computational-to-statistical gaps: predictions using statistical physics",
            "venue": "Port. Math., 75(2):159\u2013186",
            "year": 2018
        },
        {
            "authors": [
                "M. Bozorg",
                "S. Salehkaleybar",
                "M. Hashemi"
            ],
            "title": "Seedless graph matching via tail of degree distribution for correlated erdos-renyi graphs. Preprint, arXiv:1907.06334",
            "year": 1907
        },
        {
            "authors": [
                "D. Cullina",
                "N. Kiyavash"
            ],
            "title": "Improved achievability and converse bounds for erdos-renyi graph matching",
            "venue": "Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science, SIGMETRICS \u201916, page 63\u201372, New York, NY, USA",
            "year": 2016
        },
        {
            "authors": [
                "D. Cullina",
                "N. Kiyavash",
                "P. Mittal",
                "H.V. Poor"
            ],
            "title": "Partial recovery of erdos-r\u00e9nyi graph alignment via k-core alignment",
            "venue": "SIGMETRICS \u201920, pages 99\u2013100, New York, NY, USA",
            "year": 2020
        },
        {
            "authors": [
                "T. Cour",
                "P. Srinivasan",
                "J. Shi"
            ],
            "title": "Balanced graph matching",
            "venue": "B. Sch\u00f6lkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19. MIT Press",
            "year": 2006
        },
        {
            "authors": [
                "O.E. Dai",
                "D. Cullina",
                "N. Kiyavash",
                "M. Grossglauser"
            ],
            "title": "Analysis of a canonical labeling algorithm for the alignment of correlated erdos-r\u00e9nyi graphs",
            "venue": "Proc. ACM Meas. Anal. Comput. Syst.,",
            "year": 2019
        },
        {
            "authors": [
                "J. Ding",
                "Z. Ma",
                "Y. Wu",
                "J. Xu"
            ],
            "title": "Efficient random graph matching via degree profiles",
            "venue": "Probab. Theory Relat. Fields, 179(1-2):29\u2013115",
            "year": 2021
        },
        {
            "authors": [
                "Z. Fan",
                "C. Mao",
                "Y. Wu",
                "J. Xu"
            ],
            "title": "Spectral graph matching and regularized quadratic relaxations: Algorithm and theory",
            "venue": "Foundations of Computational Mathematics",
            "year": 2022
        },
        {
            "authors": [
                "Z. Fan",
                "C. Mao",
                "Y. Wu",
                "J. Xu"
            ],
            "title": "Spectral graph matching and regularized quadratic relaxations II: Erdos-r\u00e9nyi graphs and universality",
            "venue": "Foundations of Computational Mathematics",
            "year": 2022
        },
        {
            "authors": [
                "D. Gamarnik"
            ],
            "title": "The overlap gap property: A topological barrier to optimizing over random structures",
            "venue": "Proceedings of the National Academy of Sciences, 118(41):e2108492118",
            "year": 2021
        },
        {
            "authors": [
                "L. Ganassali",
                "L. Massouli\u00e9"
            ],
            "title": "From tree matching to sparse graph alignment",
            "venue": "Proceedings of Thirty Third Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "D. Gamarnik",
                "M. Sudan"
            ],
            "title": "Limits of local algorithms over sparse random graphs",
            "venue": "Proceedings of the 5th conference on Innovations in theoretical computer science, pages 369\u2013376. ACM",
            "year": 2014
        },
        {
            "authors": [
                "D. Gamarnik",
                "I. Zadik"
            ],
            "title": "The landscape of the planted clique problem: Dense subgraphs and the overlap gap property",
            "year": 1904
        },
        {
            "authors": [
                "G. Hall",
                "L. Massouli\u00e9"
            ],
            "title": "Partial recovery in the graph alignment problem",
            "year": 2007
        },
        {
            "authors": [
                "A. Haghighi",
                "A. Ng",
                "C. Manning"
            ],
            "title": "Robust textual inference via graph matching",
            "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,",
            "year": 2005
        },
        {
            "authors": [
                "B. Huang",
                "M. Sellke"
            ],
            "title": "Tight Lipschitz Hardness for optimizing Mean Field Spin Glasses",
            "venue": "2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), Denver, CO, USA",
            "year": 2022
        },
        {
            "authors": [
                "E. Kazemi",
                "S.H. Hassani",
                "M. Grossglauser"
            ],
            "title": "Growing a graph matching from a handful of seeds",
            "venue": "Proc. VLDB Endow., 8(10):1010\u20131021,",
            "year": 2015
        },
        {
            "authors": [
                "M. El-Kebir",
                "J. Heringa",
                "G.W. Klau"
            ],
            "title": "Lagrangian relaxation applied to sparse global network alignment",
            "venue": "Pattern Recognition in Bioinformatics. Springer",
            "year": 2011
        },
        {
            "authors": [
                "O. Kuchaiev",
                "N. Pr\u017eulj"
            ],
            "title": "Integrative network alignment reveals large regions of global network similarity in yeast and human",
            "venue": "Bioinformatics 27.10 ",
            "year": 2011
        },
        {
            "authors": [
                "V. Lyzinski",
                "D.E. Fishkind",
                "C.E. Priebe"
            ],
            "title": "Seeded graph matching for correlated Erdos- R\u00e9nyi graphs",
            "venue": "J. Mach. Learn. Res., 15:3513\u20133540",
            "year": 2014
        },
        {
            "authors": [
                "K. Makarychev",
                "R. Manokaran",
                "M. Sviridenko"
            ],
            "title": "Maximum quadratic assignment problem: Reduction from maximum label cover and lp-based approximation algorithm",
            "venue": "Automata, Languages and Programming, pages 594\u2013604",
            "year": 2010
        },
        {
            "authors": [
                "A. Montanari"
            ],
            "title": "Optimization of the sherrington-kirkpatrick hamiltonian",
            "venue": "2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS), pages 1417\u20131433",
            "year": 2019
        },
        {
            "authors": [
                "E. Mossel",
                "J. Xu"
            ],
            "title": "Seeded graph matching via large neighborhood statistics",
            "venue": "Random Structures Algorithms, 57(3):570\u2013611",
            "year": 2020
        },
        {
            "authors": [
                "A. Narayanan",
                "V. Shmatikov"
            ],
            "title": "Robust de-anonymization of large sparse datasets",
            "venue": "2008 IEEE Symposium on Security and Privacy ",
            "year": 2008
        },
        {
            "authors": [
                "A. Narayanan",
                "V. Shmatikov"
            ],
            "title": "De-anonymizing social networks",
            "venue": "2009 30th IEEE Symposium on Security and Privacy, pages 173\u2013187",
            "year": 2009
        },
        {
            "authors": [
                "P. Pedarsani",
                "M. Grossglauser"
            ],
            "title": "On the privacy of anonymized networks",
            "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201911, pages 1235\u20131243, New York, NY, USA",
            "year": 2011
        },
        {
            "authors": [
                "P.M. Pardalos",
                "F. Rendl",
                "H. Wolkowicz"
            ],
            "title": "The quadratic assignment problem: A survey and recent developments",
            "venue": "In Proceedings of the DIMACS Workshop on Quadratic Assignment Problems, volume 16 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 1\u201342. American Mathematical Society",
            "year": 1994
        },
        {
            "authors": [
                "P. Raghavendra",
                "T. Schramm",
                "D. Steurer"
            ],
            "title": "High dimensional estimation via sum-of-squares proofs",
            "venue": "Proceedings of the International Congress of Mathematicians\u2014Rio de Janeiro 2018. Vol. IV. Invited lectures, pages 3389\u20133423. World Sci. Publ., Hackensack, NJ",
            "year": 2018
        },
        {
            "authors": [
                "M. Rahman",
                "B. Vir\u00e1g"
            ],
            "title": "Local algorithms for independent sets are half-optimal",
            "venue": "Ann. Probab., 45(3):1543\u20131577",
            "year": 2017
        },
        {
            "authors": [
                "F. Shirani",
                "S. Garg",
                "E. Erkip"
            ],
            "title": "Seeded graph matching: Efficient algorithms and theoretical guarantees",
            "venue": "2017 51st Asilomar Conference on Signals, Systems, and Computers, pages 253\u2013257",
            "year": 2017
        },
        {
            "authors": [
                "E.V. Slud"
            ],
            "title": "Distribution inequalities for the binomial law",
            "venue": "Ann. Probab., 5(3):404\u2013412",
            "year": 1977
        },
        {
            "authors": [
                "E. Subag"
            ],
            "title": "Following the ground states of full-RSB spherical spin glasses",
            "venue": "Comm. Pure Appl. Math., 74(5):1021\u20131044",
            "year": 2021
        },
        {
            "authors": [
                "R. Singh",
                "J. Xu",
                "B. Berger"
            ],
            "title": "Global alignment of multiple protein interaction networks with application to functional orthology detection",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America, 105:12763\u20138,",
            "year": 2008
        },
        {
            "authors": [
                "J.T. Vogelstein",
                "J.M. Conroy",
                "V. Lyzinski",
                "L.J. Podrazik",
                "S.G. Kratzer",
                "E.T. Harley",
                "D.E. Fishkind",
                "R.J. Vogelstein",
                "C.E. Priebe"
            ],
            "title": "Fast approximate quadratic programming for graph matching",
            "venue": "PLOS ONE, 10(4):1\u201317,",
            "year": 2015
        },
        {
            "authors": [
                "A.S. Wein"
            ],
            "title": "Optimal low-degree hardness of maximum independent set",
            "venue": "Mathematical Statistics and Learning",
            "year": 2022
        },
        {
            "authors": [
                "Y. Wu",
                "J. Xu"
            ],
            "title": "Statistical Problems with Planted Structures: Information-Theoretical and Computational Limits",
            "venue": "page 383\u2013424. Cambridge University Press",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wu",
                "J. Xu",
                "S.H. Yu"
            ],
            "title": "Settling the Sharp Reconstruction Thresholds of Random Graph Matching",
            "venue": "2021 IEEE International Symposium on Information Theory (ISIT), Melbourne, Australia",
            "year": 2021
        },
        {
            "authors": [
                "L. Yartseva",
                "M. Grossglauser"
            ],
            "title": "On the performance of percolation graph matching",
            "venue": "Proceedings of the First ACM Conference on Online Social Networks, COSN \u201913, pages 119\u2013 130, New York, NY, USA",
            "year": 2013
        },
        {
            "authors": [
                "L. Zdeborov\u00e1",
                "F. Krzakala"
            ],
            "title": "Statistical physics of inference: thresholds and algorithms",
            "venue": "Advances in Physics, 65(5):453\u2013552",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 7.\n06 59\n0v 1\n[ m\nat h.\nPR ]\n1 3\n\u221a logn/n. Our result reveals an algorithmic phase transition for\nthis random optimization problem: polynomial-time approximation schemes exist in the sparse regime, while statistical-computational gap emerges in the dense regime. Additionally, we establish a sharp transition on the performance of online algorithms for this problem when p lies in the dense regime, resulting in a \u221a 8/9 multiplicative constant factor gap between achievable and optimal solutions."
        },
        {
            "heading": "1 Introduction and main results",
            "text": "The Graph Alignment Problem (GAP) for two simple graphs with the same number of vertices involves finding a vertex bijection which maximizes the size of overlap of these two graphs. Specifically, given G(V,E) and G(V,E) with |V | = |V|, the goal is to find a one-to-one mapping \u03c0 : V \u2192 V that maximizes the expression\n\u2211\nvi 6=vj 1(vi,vj)\u2208E1(\u03c0(vi),\u03c0(vj))\u2208E .\nClosely related to the Maximum Common Subgraph Problem (MCS), GAP is an important but challenging combinatorial optimization problem, which plays essential roles in various applied fields, such as computational biology [53, 54], social networking [44, 45], computer vision [3, 12] and natural language processing [32]. The study of efficient algorithms for solving GAP exactly or approximately has been conducted extensively through the past decades.\nUnfortunately, as a special case of the Quadratic Assignment Problem (QAP) [47, 5], the exact solution of GAP is known to be NP-hard, which means no known algorithm can solve it in polynomial time for all instances. Moreover, it was shown in [38] that\napproximating QAP within a factor 2log 1\u2212\u03b5(n) for any \u03b5 > 0 is also NP-hard, so finding near-optimal solutions for GAP efficiently also seems to be out of reach in general. Due to the worst-case hardness result, analyses of GAP in existing literature are typically restricted to specific classes of instances, such as sparse graphs (e.g. [35, 36]) or correlated Erdo\u030bs-Re\u0301nyi graphs (e.g. [46, 59, 37, 34, 22, 50, 4, 19, 20, 21, 7, 11, 13, 43, 27, 40, 28, 42, 18], which we will discuss in details in Section 1.1). In this paper we consider GAP over typical instances for a pair of independent Erdo\u030bs-Re\u0301nyi graphs with the same edge density, which we refer to as the random GAP. Our primary objective is to find near-optimal solutions within a multiplicative constant factor.\nFormally we fix n \u2208 N, p \u2208 (0, 1), denote V = {v1, . . . , vn},V = {v1, . . . , vn} and let G,G be independent samples of Erdo\u030bs-Re\u0301nyi graphs on V,V with edge density p (whose distribution we denote as G(n, p)), respectively. For 1 \u2264 i 6= j \u2264 n, we let\nGi,j = Gj,i = 1(vi,vj)\u2208E , Gi,j = Gj,i = 1(vi,vj)\u2208E,\nthen {Gi,j}1\u2264i<j\u2264n, {Gi,j}1\u2264i<j\u2264n are independent Bernoulli variables with parameter p by definition. For any permutation \u03c0 \u2208 Sn, define\nO(\u03c0) = \u2211\n1\u2264i<j\u2264n Gi,jG\u03c0(i),\u03c0(j)\nas the overlap of G,G along with the vertex correspondence vi 7\u2192 v\u03c0(i). We will study the optimization problem of O(\u03c0) under typical realizations of the random inputs G and G. we note that for any \u03c0 \u2208 Sn, the expectation of O(\u03c0) is given by En,p = ( n 2 ) p2. We will see later that when p is large enough, En,p is indeed the leading order term of max\u03c0\u2208Sn O(\u03c0), so it would be reasonable to consider the centered version of the family of variables\n{O\u0303(\u03c0)}\u03c0\u2208Sn def = {O(\u03c0)\u2212 En,p}\u03c0\u2208Sn . (1.1)\nFor positive functions f, g : N \u2192 R+, we use standard notations like f = o(g), O(g),\u2126(g) to mean that f/g is converging to 0, bounded above from \u221e, bounded below from 0, respectively. Additionally, we use the notation f \u226a g (resp. f \u226b g) to indicate that f/g tends to 0 (resp. \u221e) as n \u2192 \u221e. We also use f \u224d g to denote that cf \u2264 g \u2264 Cf for some universal constants 0 < c < C. In this paper, we focus on two regimes of the parameter p that are separated by the critical window around pc = \u221a log n/n. Formally, we introduce the following conventions:\n\u2022 we say p in the sparse regime, if log n/n \u2264 p \u226a pc;\n\u2022 we say p in the dense regime, if pc \u226a p \u226a 1.\nOur first result establishes the asymptotics of the maximum centered overlap.\nTheorem 1.1. Let (G,G) \u223c G(n, p)\u22972, then the following hold.\n\u2022 When p lies in the sparse regime, for Sn,p= n logn log ( logn/np2 ) , we have\nmax \u03c0\u2208Sn\nO\u0303(\u03c0) / Sn,p in probability\u2192 1 , as n \u2192 \u221e . (1.2)\n\u2022 If p lies in the dense regime, then for Dn,p = \u221a n3p2 log n, we have\nmax \u03c0\u2208Sn\nO\u0303(\u03c0) / Dn,p in probability\u2192 1 , as n \u2192 \u221e . (1.3)\nRemark 1.1. A simple computation reveals that\np \u226a pc \u21d2 Sn,p \u226b En,p and p \u226b pc \u21d2 Dn,p \u226a En,p . From these relations, it follows that the typical asymptotic behavior of max\u03c0\u2208Sn O(\u03c0) is governed by Sn,p in the sparse regime. While in the dense regime, it is captured by the expectation En,p. Moreover, a straightforward application of concentration inequalities suggests that in the dense regime, all the O(\u03c0) values concentrate around En,p. This explains why we focus on the centered version O\u0303(\u03c0). Otherwise, the problem would become trivial in the dense regime, both in terms of informational and computational aspects.\nWe now shift our attention to the computational aspect of the problem. To investigate the power of algorithms for random GAP, formally we make the following definition.\nDefinition 1.1 (Graph alignment algorithms). Denote Gn as the collection of all simple graphs on n vertices. A graph alignment algorithm is a potentially randomized algorithm denoted by A = A(\u2126, f,PG,G). Here, \u2126 is an abstract sample space, f is a deterministic function from Gn\u00d7Gn\u00d7\u2126 to Sn, and PG,G is a family of probability measures on \u2126 indexed by Gn\u00d7Gn. Given an input (G,G), the algorithm first samples \u03c9 from PG,G and then outputs A(G,G) = f(G,G, \u03c9) \u2208 Sn.\nWe denote the set of graph alignment algorithms as GAA.\nWe aim to determine the power limit of efficient algorithms in GAA for typical instances of (G,G) \u223c G(n, p)\u22972. For G,G independently sampled from G(n, p), let S\u03b2(G,G) be the set of permutations \u03c0 which satisfy O\u0303(\u03c0) \u2265 \u03b2Sn,p (resp. O\u0303(\u03c0) \u2265 \u03b2Dn,p) for p in the sparse regime (resp. the dense regime). Namely, S\u03b2(G,G) is the set of asymptotically \u03b2-optimal solutions to the random GAP. Our next result states that for p in the sparse regime, polynomial-time algorithms may successfully find solutions in S\u03b2(G,G) with high probability for \u03b2 arbitrarily close to 1.\nTheorem 1.2. When p lies in the sparse regime, there is a polynomial-time algorithm A \u2208 GAA (depending on n, p, \u03b5) such that as n \u2192 \u221e,\nP[A(G,G) \u2208 S1\u2212\u03b5(G,G)] = 1\u2212 o(1) , where P is taken over (G,G) \u223c G(n, p)\u22972 as well as the internal randomness of A.\nAccording to Theorem 1.2, no statistical-computational gap exists in the sparse regime. However, in the dense regime, algorithmic barrier seems to emerge. We present an evidence for such a claim by establishing the so-called overlap-gap property in the dense regime, and illustrate the failure of stable algorithms via this property. We state the result informally as below, and the rigorous version will be given in Section 4.1.\nTheorem 1.3 (informal). There exists a universal constant \u03b20 < 1 such that for any p in the dense regime, the solution space S\u03b20(G,G) satisfies 2-OGP with high probability (with respect to (G,G) \u223c G(n, p)\u22972). As a result, no stable algorithm in GAA can find solutions in S\u03b20(G,G) with (sufficiently) high probability in the dense regime.\nBuilding upon the existence of the statistical-computational gap, our research aims to determine the precise threshold of optimal computationally achievable solutions, commonly referred to as the computational threshold. While lower bounds on this threshold can be obtained by constructing and analyzing specific efficient algorithms, establishing a nontrivial upper bound for the entire class of polynomial-time algorithms poses a significant challenge. However, there are strong indications and widely-used approaches that demonstrate algorithmic limitations beyond certain thresholds within specific sub-classes. In this paper, we narrow our focus to a specific class of algorithms called online algorithms, which are defined as follows:\nDefinition 1.2 (Online algorithms). An algorithm A \u2208 GAA is considered as an online algorithm if A outputs a permutation \u03c0\u2217 following the rules that\n\u2022 The values of \u03c0\u2217(1), \u03c0\u2217(2), . . . , \u03c0\u2217(n) are determined sequentially.\n\u2022 For each 1 \u2264 k \u2264 n, once \u03c0\u2217(1), . . . , \u03c0\u2217(k \u2212 1) are determined, \u03c0\u2217(k) is a random variable (with internal randomness of the algorithm itself) that takes a value in [n] \\ {\u03c0\u2217(1), . . . , \u03c0\u2217(k \u2212 1)} with a distribution Pk determined by {Gij}1\u2264i<j\u2264k, {Gij}1\u2264i<j\u2264n, and \u03c0\u2217(1), . . . , \u03c0\u2217(k \u2212 1).\nWe denote the set of online algorithms as OGAA.\nIntuitively, one might think of G as a graph that is constructed online, vertex by vertex, while G is a pre-specified offline graph. From this perspective, the above definition implies that an online algorithm must determine \u03c0\u2217(k) using only the information before the k-th update of the vertices in G.\nWe focus on the power limit of online algorithms and denote \u03b2c = \u221a\n8/9. We establish a sharp transition on the performance of online algorithms in the dense regime.\nTheorem 1.4. For p in the dense regime and any \u03b5 > 0, the following hold. (i) Assume further that p \u2264 1/(log n)4, then there exists A\u2217 \u2208 OGGA which runs in O(n3) times, such that as n \u2192 \u221e,\nP [ A\u2217(G,G) \u2208 S\u03b2c\u2212\u03b5(G,G) ] = 1\u2212 o(1) ,\ni.e. A\u2217 finds (\u03b2c \u2212 \u03b5)-optimal solutions for typical instances of (G,G) \u223c G(n, p)\u22972. (ii) There exists c = c(\u03b5) > 0 such that for any A \u2208 OGAA,\nP [ Q [ A(G,G) \u2208 S\u03b2c+\u03b5(G,G) ] \u2265 exp(\u2212cn log n) ] = o(1) ,\nwhere Q denotes the internal randomness of the algorithm A. In other words, no online algorithm can find (\u03b2c + \u03b5)-optimal solutions with probability exceeding exp ( \u2212\u2126(n log n) ) under typical instances of (G,G) \u223c G(n, p)\u22972.\nRemark 1.2. Several remarks about Theorem 1.4 are provided below. (i) The assumption p \u2264 1/(log n)4 in (i) of Theorem 1.4 is made for technical reasons. Indeed, it is expected that a similar result holds for any p in the dense regime (see Remark 3.1 for more discussions). (ii) The upper bound of success probability exp ( \u2212\u2126(n log n) ) is exponentially tight up to a multiplicative constant. This is because even a trivial guessing would find solutions in S\u03b2c+\u03b5(G,G) with probability at least 1/n! > exp(\u2212n log n) under typical instances. (iii) The definition of online algorithms can be generalized to allow for the matching of any o(n) pairs of vertices in each round. With this modification, the hardness result can still be obtained with only minor changes in the wording. Moreover , our framework for establishing the hardness result is robust enough to accommodate algorithms that extend slightly beyond the online case, such as any algorithm that follows a \u201clocal greedy iteration\u201d approach. In this context, \u201clocal\u201d signifies that the algorithm only involves a vanishing fraction of coordinates in each step, while \u201cgreedy\u201d implies that the algorithm consistently exploits immediate advantages in the current step, which may have negative effects on future iterations. For instance, the algorithms constructed in [16] do not fall under the category of online algorithms, but they can still be categorized as \u201clocal greedy\u201d algorithms. Our argument can be extended to demonstrate the limitations of these algorithms beyond the threshold \u03b2c in the dense regime. (iv) It is anticipated that the framework introduced in [33] can be applied to exclude the possibility of any sufficiently stable algorithm in GAA finding solutions beyond the threshold \u03b2c with non-vanishing probability. Determining the precise threshold for stable algorithms remains an intriguing avenue for future research.\nBased on the aforementioned theorems, we establish sharp informational thresholds in both regimes, as well as an algorithmic phase transition as p traverses between them. To provide a comprehensive overview, let us briefly discuss the problem when p falls into other regimes. Firstly, the case p < log n/n may not be particularly interesting for considering the random GAP, as both graphs are likely to be disconnected, reducing the problem to matching between their components. Additionally, it would be intriguing to consider the regime when p = \u2126(1) and we believe this regime shares a more or less similar picture with the dense regime. However, our work does not aim to cover this specific regime as it does differ in specific details from the dense regime, both in terms of informational\nand computational perspectives. Finally, arguably the most intriguing regime is the critical window p \u224d pc, where multiple phase transitions occur. Unfortunately, our current methods are tailored separately for the sparse and dense regimes, and they do not establish a sharp threshold for p within the critical window. Further insights beyond the scope of this paper are necessary to fully understand how these phase transitions take place in this regime."
        },
        {
            "heading": "1.1 Backgrounds and related works",
            "text": "Our motivation for considering the random GAP is twofold. On one hand, the GAP for a pair of independent Erdo\u030bs-Re\u0301nyi graphs arises naturally in the study of correlated random graph models, which is an extensively studied topic in the field of combinatorial statistics in recent years. A deeper understanding of either the informational threshold or the computational threshold for random GAP may also shed lights on the correlated model itself. On the other hand, there is inherent intrigue in gaining insights into the computational complexity of such a natural and important problem. Although the computational intractability for the worst-case scenario is already known (primarily due to certain highly structured \u201cbad\u201d instances), the situation for the average-case setting remains unclear.\nWe now delve into these two aspects that motivate our current work in greater detail. Correlated random graph model. The correlated random graph model refers to a pair of correlated Erdo\u030bs-Re\u0301nyi random graphs with the same number of vertices, where the correlation between them is determined by a latent vertex bijection. The study of the correlated model primarily focuses on recovering the hidden correspondence based solely on the topological structures of these two graphs, as well as the closely related correlation detection problem. Significant progress has been made in recent years, including informationtheoretic analysis [10, 9, 31, 57, 58, 14, 15] and proposals for various efficient algorithms [46, 59, 37, 34, 22, 50, 4, 19, 20, 21, 7, 11, 13, 43, 27, 40, 41, 28, 42, 18], etc. Currently, the community has achieved a comprehensive understanding of the informational thresholds for the correlated random graph model. However, a substantial statistical-computational gap remains and the precise computational threshold has yet to be determined.\nThe initial exploration of the GAP over two independent instances of Erdo\u030bs-Re\u0301nyi graphs stemmed from the study of correlation detection for a pair of random graphs. This can be formulated as a hypothesis testing problem, where, under the null hypothesis, the two graphs are independently sampled, while under the alternative hypothesis, the pair is sampled from the correlated law. The authors of [57] were the first to investigate this problem, and they introduced the maximal overlap of these two graphs as the testing statistic, which is where the concept of random GAP arises. However, the authors only derived an informational upper bound using a straightforward first moment method, which was sufficient for their purposes. Subsequently, in [24, Section 7, Open question Q1], the authors formally formulated the random GAP problem and sought to determine the exact informational thresholds for different regimes of p. One of the main objectives of this paper is to provide an answer to this question.\nComputational complexity of random optimization problems. Random optimization problems often refer to solving optimization problems when the input instances are generated randomly. These problems arise in various domains, including computer science, operations research, and statistical physics. Due to the difficulty caused by non-convexity and high\u2013dimensionality, the computational complexity of random optimization problems is currently an active but challenging research area. As mentioned earlier, while worst-case hardness results for these problems are well-established, the picture of average-case complexity (captured by the computational difficulty associated with solving the problem over randomly generated data) can be much more intricate.\nIn general for random optimization problems, a statistical-computational gap may exist, indicating that efficient algorithms encounter barriers below the optimal threshold. Alternatively, there might be a polynomial-time approximation scheme that finds near-optimal solutions for typical instances of the random input. The presence of such statisticalcomputational gaps is quite common, and notable examples include the hidden clique problem and the closely related problem of densest submatrix detection (for an overview, see [56]). To achieve the latter case, the optimization algorithms must exploit specific properties of the random instances, as the optimization problem becomes NP-hard in the worst case. A successful example of designing a polynomial-time approximation scheme is given in [52], where the author constructed a greedy algorithm that finds near ground states of certain Gaussian processes on the n-dimensional sphere with high probability. Building on this work, [39] presented an efficient approximation scheme for finding near ground state of the Sherrington-Kirkpatrick model (as well as a broader class of spin glass models), and the algorithms can also be tailored to find near-optimal solutions for the max-cut problem in dense Erdo\u030bs-Re\u0301nyi graphs. It is worth noting that all the efficient approximation algorithms mentioned above rely on a specific property of the underlying stochastic models called full replica symmetry breaking (FRSB), and in fact it is tempting to conjecture that FRSB indicates low computational complexity [25].\nOver the past few decades, various frameworks have been proposed to provide insights into the computational hardness of optimization problems with random inputs (for surveys, see [60, 6, 48, 23]). Notably, the overlap-gap property, initially introduced in [29], serves as a geometric barrier that provides solid evidence for the failure of stable algorithms to find solutions beyond a certain threshold [23]. In the last ten years, several generations of the initial overlap-gap property have been discovered, such as the multi-OGP [49] and the ladder-OGP [55], which establish computational hardness results close to the believed computational thresholds. Recently, a seminal work [33] introduced yet another variant of the overlap-gap property, called the branching-OGP, which was used to provide tight and satisfactory hardness results for stable algorithms in mean-field spin glasses. The current work is also significantly inspired by the branching-OGP framework. Prior works. In the paper [16], the authors developed a polynomial-time approximation scheme for the random GAP when the parameter p satisfies p = n\u2212\u03b1+o(1) for some constant \u03b1 \u2208 (1/2, 1] and p \u2265 log n/n. Despite their sophisticated forms, these algorithms are rooted\nin a fairly simple idea. Consider a naive greedy algorithm, where \u03c0(1), \u03c0(2), . . . , \u03c0(n) are determined sequentially, and for each k, \u03c0(k) is chosen from the remaining unmatched vertices to maximize \u2211 i<k Gi,kG\u03c0(i),\u03c0(k). Interestingly, the authors of [16] observed that for certain values of \u03b1, this simple algorithm produces near-optimal solutions with high probability. This indicates that the greedy matching algorithm somehow captures the essence of the random GAP problem. Consequently, it is not surprising that all the algorithms presented in [16] and the algorithms that will be discussed in this paper are essentially variants of this simple greedy algorithm."
        },
        {
            "heading": "1.2 Proof overview",
            "text": "The proof of the main results can be divided into four parts: informational upper/lower bounds and computational upper/lower bounds.\nWe obtain the informational upper bounds in Section 2.1 for both the sparse and dense regimes, via a simple union bound (a.k.a. the first moment method). For the informational lower bound in the dense regime, we employ a truncated second-moment method argument in Section 2.2. It is important to note that a straightforward application of the PaleyZygmund inequality only yields a vanishing lower bound of probability. However, inspired by [8], we enhance the lower bound to 1 \u2212 o(1) by combining the use of Talagrand\u2019s concentration inequality.\nIn the case where p lies in the sparse regime, surprisingly, we have not discovered any non-constructive proof of the informational lower bound. Instead, in Section 3, we construct specific algorithms to find near-optimal solutions for the random GAP. By analyzing these algorithms in Section 3.2, we establish an algorithmic lower bound in the sparse regime, which corresponds to the informational upper bound. This completes the proof of Theorem 1.1 and thereby concludes Theorem 1.2. Additionally, in Section 3.3, we analyze the performance of the same algorithms in the dense regime and show that these algorithms can find near \u03b2c-optimal solutions, leading to the first item in Theorem 1.4.\nIn Section 4, we focus on the hardness results in the dense regime and derive Theorem 1.3 as well as the second item in Theorem 1.4. We establish the existence of algorithmic barriers by taking advantage of various types of the overlap-gap property. While the argument for the failure of stable algorithms via 2-OGP in Section 4.1 is rather standard, our proof of the failure of online algorithms in Section 4.2 is a novel combination of the branching-OGP framework presented by [33] and a resampling technique inspired by [26].\nAcknowledgements. We would like to express our sincere gratitude to Jian Ding for his heartfelt guidance and support during the early stages of this project. We are also grateful to Nike Sun for introducing us to the crucial literature reference [33]. Furthermore, we thank Brice Huang, Zhangsong Li, and Mark Sellke for their engaging discussions on the algorithmic aspects of this problem."
        },
        {
            "heading": "2 The informational thresholds",
            "text": "This section is devoted to the proof of Theorem 1.1, where the upper and lower bounds are established separately in the next two subsections."
        },
        {
            "heading": "2.1 Informational upper bounds",
            "text": "In this short subsection we prove the upper bounds for Theorem 1.1. First we recall the Chernoff bound for binomial variables.\nProposition 2.1 (Chernoff bound). For any N \u2208 N, P \u2208 (0, 1) and \u03b4 > 0, let X \u223c B(N,P ), it holds that\nP[X \u2265 (1 + \u03b4)NP ] \u2264 exp ( \u2212NP ( (1 + \u03b4) log(1 + \u03b4)\u2212 \u03b4 )) , (2.1)\nand\nP[X \u2264 (1\u2212 \u03b4)NP ] \u2264 exp ( \u2212\u03b4 2NP\n2\n) . (2.2)\nIn particular, we have for any K \u2265 0,\nP[|X \u2212NP | \u2265 K] \u2264 2 exp ( \u2212 K 2\n2(NP +K)\n) . (2.3)\nThe proof of (2.1) and (2.2) can be found in, for example, [57, Appendix C, Lemma 11]. Subsequently, (2.3) follows from the easy-checked fact that\n(1 + \u03b4) log(1 + \u03b4)\u2212 \u03b4 \u2265 \u03b4 2\n2(1 + \u03b4) .\nThe remaining proof is just a straightforward application of the union bound.\nProof of the upper bound for Theorem 1.1. For any fixed permutation \u03c0 \u2208 Sn, it is evident that O(\u03c0) \u223c B( (n 2 ) , p2). We will show that for any fixed \u03b5 > 0, the probability of such a binomial variable deviating from its expectation by more than (1+ \u03b5)Sn,p or (1+ \u03b5)Dn,p is much less than 1/n!. This accomplishes our goal through a simple union bound.\nWhen p lies in the sparse regime, we have |E|p < n2p2 \u226a Sn,p. Hence from (2.1),\nP [ B ((n\n2\n) , p2 ) \u2265 (1 + \u03b5)Sn,p + En,p ]\n\u2264 exp ( \u2212 ( En,p + (1 + \u03b5)Sn,p ) log ( 1 +\n(1 + \u03b5)Sn,p n2p2\n) + (1 + \u03b5)Sn,p + En,p )\n\u2264 exp ( \u2212 ( 1 + \u03b5+ o(1) ) Sn,p log\nSn,p n2p2\n) = exp ( \u2212Sn,p \u00b7 (1 + \u03b5+ o(1) log ( log n\nnp2\n))\n=exp ( \u2212 ( 1 + \u03b5+ o(1) ) n log n ) ,\nwhich is much less than 1/n!, as desired. When p lies in the dense regime, we get from (2.3) that\nP [ B ((n\n2\n) , p2 ) \u2265 (1 + \u03b5)Dn,p +En,p ] \u2264 2 exp ( \u2212 ( 1 + \u03b5+ o(1) )2 D2n,p\n2En,p + 2(1 + \u03b5)Dn,p\n) ,\nwhich equals to exp ( \u2212 [ (1 + \u03b5)2 + o(1) ] n log n ) \u226a 1/n!, as desired."
        },
        {
            "heading": "2.2 Informational lower bounds",
            "text": "We now focus on the lower bounds stated in Theorem 1.1. The result of the sparse regime will be proved in Section 3.2, and this subsection is dedicated to proving the informational lower bound for p in the dense regime. So throughout this subsection, we always assume that pc \u226a p \u226a 1.\nA frequently employed strategy in this paper involves conditioning on a suitable realization of either graph G or G, while relying solely on the randomness of the other graph. In simpler terms, we will treat either G or G as a deterministic graph possessing a desired property. To this end, we will introduce a concept named as admissibility, which is related to the regularity of a graph across various aspects. This property will play a crucial role in establishing the informational lower bounds and will also contribute to the computational hardness analysis discussed in Section 4.\nBefore introducing the definition of admissibility, we emphasize that this property is specifically designed for Erdo\u030bs-Re\u0301nyi graphs G(n, p) when p falls into the dense regime. We begin with several notations and observations. Denote U as the set of unordered pairs {(i, j) : i, j \u2208 [n], i 6= j}. Fix G with edge set E \u2282 U. For any \u03c0 \u2208 Sn, consider the set\nOL(G,\u03c0) = {(i, j) \u2208 U : Gi,j = G\u03c0(i),\u03c0(j) = 1} .\nthen for any \u03c01, \u03c02 \u2208 Sn, let \u03c01,2 = \u03c0\u221211 \u25e6 \u03c02, we claim that it holds\nCov ( O\u0303(\u03c01), O\u0303(\u03c02) ) = p(1\u2212 p)\u00d7 |OL(G,\u03c01,2)| ,\nwhere the covariance is taken with respect to G. This is because we can express the covariance Cov ( O\u0303(\u03c01), O\u0303(\u03c02) ) = Cov ( O(\u03c01),O(\u03c02) ) as\n\u2211\n(i1,j1)\u2208E (i2,j2)\u2208E\nCov ( G\u03c01(i1),\u03c01(j1),G\u03c02(i2),\u03c02(j2) ) = \u2211\n(i1,j1)\u2208E (i2,j2)\u2208E\np(1\u2212 p)1(\u03c01(i1),\u03c01(j1))=(\u03c02(i2),\u03c02(j2))\nand the number of pairs (i1, j1), (i2, j2) such that (\u03c0 \u22121 1 (i1), \u03c0 \u22121 1 (j1)) = (\u03c0 \u22121 2 (i2), \u03c0 \u22121 2 (j2)) is exactly |OL(G,\u03c01,2)|. For each permutation \u03c0 \u2208 Sn, let F (\u03c0) denote the number of fixed points and T (\u03c0) denote the number of transpositions in \u03c0. It is worth noting that in the case of G \u223c G(n, p),\nan unordered pair (i, j) \u2208 U appears in OL(G,\u03c0) with a probability of either p or p2, depending on whether (i, j) = (\u03c0(i), \u03c0(j)) or not. In the former case, both i and j are fixed points, or (i, j) represents a transposition. This leads to the following relation\nE|OL(G,\u03c0)| = [( F (\u03c0)\n2\n) + T (\u03c0) ] (p\u2212 p2) + ( n\n2\n) p2 . (2.4)\nDefinition 2.1. For any p in the dense regime, we say a graph G on n vertices is padmissible (we simply call it admissible when the parameter p is clear in the context), if the following items hold:\n\u2022 The total number of edges in G satisfies \u2223\u2223\u2223\u2223|E(G)| \u2212 ( n\n2\n) p \u2223\u2223\u2223\u2223 \u2264 2 \u221a n2p log n . (2.5)\n\u2022 For any induced subgraph H of G with k vertices, it holds that\n\u2223\u2223\u2223\u2223|E(H)| \u2212 ( k\n2\n) p \u2223\u2223\u2223\u2223 \u2264 n2p\nlog n1/4 . (2.6)\n\u2022 for any \u03c0 \u2208 Sn it holds \u2223\u2223|OL(G,\u03c0)| \u2212 E|OL(G,\u03c0)| \u2223\u2223 \u2264 2 \u221a |F |np log n+ 3 \u221a 2n3p2 log n . (2.7)\nIn particular, |OL(G,\u03c0)|/E|OL(G,\u03c0)| = 1 + o(1) uniformly for all \u03c0 \u2208 Sn.\nLemma 2.2. For any p in the dense regime, an Erdo\u030bs-Re\u0301nyi graph G \u223c G(n, p) is padmissible with probability 1\u2212 o(1/n2).\nThe lemma can be essentially derived using a union bound, and we will provide its proof in the appendix. From this point forward in this section, we will consider a fixed value of p within the dense regime, along with a p-admissible graph G. Therefore, throughout the remaining portion of this section, the notation P will refer to the probability distribution of a single graph G drawn from G(n, p).\nThe following proposition establishes a concentration result for the maximum of O\u0303(\u03c0), which will serve as a key component in proving the lower bound stated in Theorem 1.1.\nProposition 2.3. For any constant \u03b5 > 0, there exists a constant \u03b3 = \u03b3(\u03b5) > 0, such that for any admissible graph G,\nP [\u2223\u2223max \u03c0\u2208Sn O\u0303(\u03c0)\u2212 Emax \u03c0\u2208Sn O\u0303(\u03c0) \u2223\u2223 \u2265 \u03b5Dn,p ] \u2264 exp (\u2212\u03b3n log n) ,\nwhere the probability and expectation is taken over G \u223c G(n, p).\nThe result stated in Proposition 2.3 can be obtained through a standard application of Talagrand\u2019s concentration inequality. The crucial observation is that the maximum max\u03c0\u2208Sn O(\u03c0) is Lipschitz and s-certifiable, with its median asymptotically equalling to n2p2/2. In order to provide a comprehensive explanation, we will include a summary of the relevant terminology and the results of Talagrand\u2019s concentration inequality in the appendix. Additionally, we will present a complete proof of Proposition 2.3 therein.\nFor any \u03b5 > 0, we define X\u03b5 as the number of permutations \u03c0 \u2208 Sn such that O\u0303(\u03c0) \u2265 (1\u2212 \u03b5)Dn,p. Another main ingredient for the proof is the following estimate.\nProposition 2.4. For any admissible graph G and any constant \u03b5 \u2208 (0, 1), it holds that\nEX2\u03b5 = exp ( o(n log n) ) (EX\u03b5) 2 ,\nwhere the expectation is taken with respect to G \u223c G(n, p).\nWith Proposition 2.4 in hand, we can establish the lower bound of Theorem 1.1 by combining the Paley-Zygmund inequality with the aforementioned concentration result.\nCorollary 2.5. For any \u03b5 > 0 we have as n \u2192 \u221e,\nP [ max \u03c0\u2208Sn O\u0303(\u03c0) \u2265 (1\u2212 3\u03b5)Dn,p ] = 1\u2212 o(1) ,\nwhere the probability is taken over (G,G) \u223c G(n, p)\u22972.\nProof. Since P[G is admissible] = 1 \u2212 o(1) we may condition on the realization of G and assume it is admissible. From Proposition 2.4 and the Paley-Zygmund inequality, we have\nP [ max \u03c0\u2208Sn O\u0303(\u03c0) \u2265 (1\u2212 \u03b5)Dn,p ] = P[X\u03b5 > 0] \u2265 (EX\u03b5) 2 EX2\u03b5 = exp ( \u2212 o(n log n) ) .\nCombining with Proposition 2.3, this implies Emax\u03c0\u2208Sn O\u0303(\u03c0) \u2265 (1 \u2212 2\u03b5)Dn,p. Then by applying the result of Proposition 2.3 again we reach the conclusion.\nNow we turn to the proof of Proposition 2.4. We start by the following two-point estimation.\nProposition 2.6. For any admissible graph G, any \u03b5 \u2208 (0, 1) and any \u03c01, \u03c02 \u2208 Sn, write \u03c01,2 = \u03c0 \u22121 1 \u25e6 \u03c02, then it holds\nP [ O\u0303(\u03c01) \u2265 (1\u2212 \u03b5)Dn,p, O\u0303(\u03c02) \u2265 (1\u2212 \u03b5)Dn,p ]\n\u2264 exp ( \u2212 2(1\u2212 \u03b5) 2n log n\n1 + (F (\u03c01,2)/n)2 + o(n log n)\n) .\n(2.8)\nProof. Recall that U is the set of unordered pairs and OL(G,\u03c01,2) is the subset of U that\n{(i, j) \u2208 U : Gi,j = G\u03c01,2(i),\u03c01,2(j) = 1} = {(i, j) \u2208 U : G\u03c0\u221211 (i),\u03c0\u221211 (j) = G\u03c0\u221212 (i),\u03c0\u221212 (j) = 1} .\nWe write L = |OL(G,\u03c01,2)| for simplicity, and we denote\nS0 = \u2211\n(i,j)\u2208OL(G,\u03c01,2) G\u03c0\u221211 (i),\u03c0 \u22121 1 (j) Gi,j,\nS1 = \u2211\n(i,j)\u2208U \\OL(G,\u03c01,2) G\u03c0\u221211 (i),\u03c0 \u22121 1 (j) Gi,j,\nS2 = \u2211\n(i,j)\u2208U \\OL(G,\u03c01,2) G\u03c0\u221212 (i),\u03c0 \u22121 2 (j) Gi,j.\nIt is clear that S0 \u223c B(L, p) and S1, S2 \u223c B(E \u2212L, p) are independent binomial variables, and\nO(\u03c01) = \u2211\n(i,j)\u2208U G\u03c0\u221211 (i),\u03c0 \u22121 1 (j) Gi,j = S0 + S1,\nO(\u03c02) = \u2211\n(i,j)\u2208U G\u03c0\u221212 (i),\u03c0 \u22121 2 (j) Gi,j = S0 + S2 .\nDenote M = \u2308(1 \u2212 \u03b5)Dn,p\u2309, then the probability P [ O\u0303(\u03c01) \u2265 M, O\u0303(\u03c02) \u2265 M ] equals to\nP [ S0 + S1 \u2265 Ep+M,S0 + S2 \u2265 Ep+M ]\n= \u2211\nk\nP[S0 = Lp+ k] \u00b7 ( P[S1 \u2265 (E \u2212 L)p+M \u2212 k)] )2\n\u2264 P[S0 \u2265 Lp+M ] + ( P[S1 \u2265 (E \u2212 L)p+M ] )2\n+ (M + 1) max 0\u2264k\u2264M\nP[S0 \u2265 Lp+ k] \u00b7 ( P[S1 \u2265 (E \u2212 L)p +M \u2212 k] )2 .\n(2.9)\nWe consider the three terms in (2.9) separately and claim that each one of them is bounded by the right-hand side of (2.8). By the third item in admissibility, we have L/E = (F (\u03c01,2)/n)\n2 + o(1). Applying the Chernoff bound (2.3), we see that the first term of the expression is bounded by\nexp ( \u2212\n(1\u2212 \u03b5)2D2n,p 2(Lp+Dn,p)\n) \u2264 { exp ( \u22122(1\u2212 \u03b5)2n log n+ o(n log n) ) , if F (\u03c01,2)/n \u2264 1/2 ,\nexp ( \u2212 (1\u2212\u03b5)2n logn\n(F (\u03c01,2)/n)2 + o(n log n)\n) , if F (\u03c01,2)/n > 1/2 ,\nwhich is always bounded by exp ( \u2212 2(1\u2212\u03b5)2n logn\n1+(F (\u03c01,2)/n)2 + o(n log n)\n) , as desired. Similar argu-\nments suggest that this is true for the second term in (2.9). Regarding the last term, by\napplying (2.3) again, we see that it is bounded by M + 1 = exp ( o(n log n) ) times\nmax 0\u2264k\u2264M exp\n( \u2212 k 2\n2(Lp+ k) \u2212 (M \u2212 k) 2 (E \u2212 L)p+M \u2212 k\n) \u2264 max\n0\u2264k\u2264M exp\n( \u2212 M 2\nEp+ Lp+M + k\n)\n= exp ( \u22122(1\u2212 \u03b5) 2n3p2 log n\nEp+ Lp+ 2M\n) = exp ( \u22122(1\u2212 \u03b5) 2n log n\n1 + L/E + o(1)\n)\n= exp ( \u2212 2(1 \u2212 \u03b5) 2n log n\n1 + (F (\u03c01,2)/n)2 + o(n log n)\n) ,\nwhere in the first inequality we used Cauchy-Schwarz inequality. This completes the proof of the claim and thus (2.8) follows.\nProof of Proposition 2.4. By definition we have\nEX2\u03b5 = \u2211 \u03c01,\u03c02\u2208Sn P [ O\u0303(\u03c01) \u2265 (1\u2212 \u03b5)Dn,p, O\u0303(\u03c02) \u2265 (1\u2212 \u03b5)Dn,p ] .\nFrom Proposition 2.6 we get this is bounded by\nexp ( o(n log n) ) \u00d7 \u2211\n\u03c01,\u03c02\u2208Sn exp\n( \u2212 2(1 \u2212 \u03b5) 2n log n\n1 + (F (\u03c01,2)/n)2\n)\n= exp ( [1 + o(1)]n log n ) \u2211\n\u03c0\u2208Sn exp\n( \u22122(1 \u2212 \u03b5) 2n log n\n1 + (F (\u03c0)/n)2)\n)\n= exp ( [1 + o(1)]n log n ) \u00d7 n\u2211\nk=0\nexp ( \u22122(1\u2212 \u03b5) 2n log n\n1 + (k/n)2\n) \u00d7 |{\u03c0 \u2208 Sn : F (\u03c0) = k}|\n\u2264 exp ( [2 + o(1)]n log n ) \u00d7 n\u2211\nk=0\nexp ( \u2212 [ 2(1\u2212 \u03b5)2 1 + (k/n)2 + k/n ] n log n )\n\u2264 exp ( [2\u2212 2(1\u2212 \u03b5)2 + o(1)]n log n ) ,\nwhere in the first inequality we used a well-known fact that |{\u03c0 \u2208 Sn : F (\u03c0) = k}| is bounded by exp ( (n \u2212 k) log n + o(n log n) ) and the second inequality follows from the observation that for any \u03b3 \u2208 [0, 1], it holds\n\u03b3[1\u2212 2(1\u2212 \u03b5)2\u03b3 + \u03b32] \u2265 0 \u21d0\u21d2 2(1 \u2212 \u03b5) 2\n1 + \u03b32 + \u03b3 \u2265 2(1 \u2212 \u03b5)2 .\nFinally, we conclude the proof of Proposition 2.4 by noting that from Lemma A.2,\nEX\u03b5 = n!\u00d7 P[O\u0303(\u03c0) \u2265 (1\u2212 \u03b5)Dn,p] \u2265 exp ( [1\u2212 (1\u2212 \u03b5)2 + o(1)]n log n ) ."
        },
        {
            "heading": "3 Algorithmic lower bounds via greedy algorithms",
            "text": "In this section, we shift our attention to the algorithmic aspect of the random GAP and develop matching algorithms that possess the desired properties stated in Theorem 1.2 and (i) in Theorem 1.4. It is important to note that when p = n\u2212\u03b1+o(1), where 1/2 < \u03b1 < 1 is a constant, the value of Sn,p is asymptotically n/(2\u03b1\u2212 1), and a polynomial-time approximation scheme has already been presented in [16]. Therefore, our focus will be on the remaining cases, specifically when p = n\u22121/2+o(1) with p \u226a pc, and pc \u226a p \u2264 1/(log n)4.\nAs we will see below, the algorithms designed for the remaining regimes are relatively straightforward. However, the analysis of these algorithms differs in essential ways between the sparse and dense regimes. Consequently, in Section 3.1, we provide a general description of the algorithms and outline the framework for their analyses. Then, in Section 3.2 and Section 3.3, we narrow our focus to the specific case when p = n\u22121/2+o(1) with p \u226a pc, and pc \u226a p \u2264 1/(log n)4, where we complete the proof of Theorem 1.2 and (i) in Theorem 1.4, respectively."
        },
        {
            "heading": "3.1 Framework of the algorithm",
            "text": "Fix a constant \u03b7 \u2208 (0, 1/2). We present the following natural greedy algorithm A\u03b7 \u2208 GAA.\nAlgorithm 1 Greedy Matching Algorithm A\u03b7 1: Input: G,G \u2208 Gn with adjacency matrices {Gi,j}, {Gi,j}. 2: Initialize: \u03c0\u2217(i) = i for 1 \u2264 i \u2264 \u03b7n, R\u230a\u03b7n\u230b = {\u230a\u03b7n\u230b+ 1, . . . , n}. 3: for \u230a\u03b7n\u230b+ 1 \u2264 s \u2264 \u230a(1\u2212 \u03b7)n\u230b do 4: Uniformly sample \u03c0\u2217(s) from argmaxr\u2208Rs\u22121 \u2211 j<sGj,sG\u03c0\u2217(j),r.\n5: Set Rs = Rs\u22121 \\ {\u03c0\u2217(s)}. 6: end for 7: Complete \u03c0\u2217 to a permutation such that \u03c0\u2217(\u230a(1 \u2212 \u03b7)n\u230b+ 1) < \u00b7 \u00b7 \u00b7 < \u03c0\u2217(n). 8: Output: \u03c0\u2217.\nIt is clear that A\u03b7 is an online algorithm which runs in O(n3) times for any \u03b7 > 0. The primary goal of this section is to show that for sufficiently small constant \u03b7 > 0, the algorithm A\u03b7 indeed outputs near-optimal solutions in the case p = n\u22121/2+o(1) with p \u226a pc, and near \u03b2c-optimal solutions in the case pc \u226a p \u2264 1/(log n)4.\nBefore starting the analyses of the greedy algorithms, we introduce an equivalent form of A\u03b7 that decouples the correlation posed by the uniform sampling in the algorithm. We sample i.i.d. perturbations Xi,j = Xj,i \u223c U(0, 1/n2) and define G\u2217i,j = Gi,j +Xi,j,\u22001 \u2264 i < j \u2264 n. We make the following observation: since the total sum of Xi,j\u2019s is no more than 1, by symmetry, uniformly sampling \u03c0\u2217(s) from argmaxr\u2208Rs\u22121 \u2211 j<sGj,sG\u03c0\u2217(j),r is equiv-\nalent to setting \u03c0\u2217(s) = argmaxr\u2208Rs\u22121 \u2211 j<sGj,sG \u2217 \u03c0\u2217(j),r, which is almost surely uniquely determined. Henceforth we will work on this version of A\u03b7.\nWe proceed with the analysis of the algorithm and introduce some notations. First, let us keep in mind that throughout the analysis in this section, we always fix some realization of G and P denotes the probability measure on the sole graph G \u223c G(n, p) as well as the random perturbations Xi,j , 1 \u2264 i < j \u2264 n. We will work with appropriate realizations of G which possess some nice properties incorporated in the event G0 (which has different meanings for the two regimes, see Definition 3.1 and Definition 3.2, respectively). Given G \u2208 G0, let \u03b7 > 0 be a small constant, we write a\u03b7 = \u230a\u03b7n\u230b and b\u03b7 = \u230a(1\u2212\u03b7)n\u230b for simplicity. For a\u03b7 < s \u2264 b\u03b7+1, we use Fs\u22121 to denote the \u03c3-field generated by the entire graph G, the matching indices \u03c0\u2217(i) for i < s, the matched parts of G before the s-th step (G\u03c0\u2217(i),\u03c0\u2217(j) for i < j < s), and the corresponding perturbations Xi,j for i < j < s. Moreover, we define Ns = {j < s : Gj,s = 1} for a\u03b7 < s \u2264 b\u03b7.\nRecall that G\u2217i,j = Gi,j +Xi,j. For each 1 \u2264 s \u2264 n we let\nOs = \u2211\nj<s\nGj,sG \u2217 \u03c0\u2217(j),\u03c0\u2217(s) =\n\u2211\nj\u2208Ns G \u2217 \u03c0\u2217(j),\u03c0\u2217(s).\nAdditionally, for a\u03b7 < s \u2264 b\u03b7 and r \u2208 Rs\u22121, let\nEr,s = \u2211\nj<s\nGj,sG \u2217 \u03c0\u2217(j),r =\n\u2211\nj\u2208Ns G \u2217 \u03c0\u2217(j),r.\nFurthermore, conditioned on any realization of Fs\u22121, we denote Ns as the \u03c3-field generated by the random variables Gi,j\u2019s and Xi,j \u2019s, where at least one of i or j belongs to Rs\u22121.\nNow we fix an index s with a\u03b7 < s \u2264 b\u03b7 +1 and investigate the impact of the s-th step posed by conditioning on Fs\u22121. For any r \u2208 Rs\u22121, it must hold that Er,k < Ok for any a\u03b7 < k < s, since \u03c0 \u2217(k) is chosen to maximize E\u03c0\u2217(k),k = Ok. Define the events\nDr,s = \u22c2\na\u03b7<k<s\n{Er,k < Ok} ,\u2200r \u2208 Rs\u22121 , and Ds = \u22c2\nr\u2208Rs\u22121 Dr,s .\nThe crucial observation is that given Fs\u22121 (where the sets Nk, a\u03b7 < k < s and Rs\u22121 are deterministic), for any event measurable with respect to Ns, conditioning on Fs\u22121 is equivalent to conditioning on Ds. Moreover, each event Dr,s is measurable with respect to the variables G\u2217r,\u03c0\u2217(j), j < s, and these events are conditionally independent for different indices r \u2208 Rs\u22121. Given Fs\u22121, we define Fs(\u00b7) as the distribution function of Er,s conditioned on Fs\u22121, i.e.,\nFs(x) = P[Er,s \u2264 x | Fs\u22121] = P[Er,s \u2264 x | Ds] = P[Er,s \u2264 x | Dr,s] . (3.1)\nThe last equality holds because the random variable Er,s and the event Dr,s are both independent of the events Dr\u2032,s for r\u2032 \u2208 Rs\u22121 \\{r}. Here and henceforth in this section whenever Fs\u22121 is given, with a slight abuse of notation we use P to denote the product measure on B(1, p) variables Gi,j\u2019s and U[0, 1/n 2] variables Xi,j\u2019s with either of i or j in\nRs\u22121. We emphasize that under such conventions, the sets like Nk, a\u03b7 < k \u2264 b\u03b7 and Rs\u22121 are considered to be deterministic and known.\nIt is evident that the distribution function Fs does not depend on the specific choice of r \u2208 Rs\u22121. In the s-th step, the quantity Os is sampled as the maximum of |Rs\u22121| = n\u2212s+1 i.i.d random variables with the distribution function Fs.\nLet F \u2217s (\u00b7) denote the distribution function of Er,s without conditioning on Dr,s, i.e. the distribution function of the sum of a binomial variable B(|Ns |, p) and |Ns | independent U[0, 1/n2] variables. Our goal is to show that under typical realizations of Fs\u22121, Fs is approximately equal to F \u2217s . In other words, conditioning on the event Dr,s does not significantly alter the distribution of Er,s, indicating that Os behaves similarly to the maximum of n\u2212 s+ 1 i.i.d binomial variables B(|Ns |, p)."
        },
        {
            "heading": "3.2 Analysis of greedy algorithm: the sparse regime",
            "text": "Assume p = n\u22121/2+o(1) and p \u226a pc, this subsection is devoted to proving Theorem 1.2 for this case. To this end, we will show the following proposition:\nProposition 3.1. Under the assumption of p, for any sufficiently small constant \u03b7 > 0, it holds that as n \u2192 \u221e,\nP[A\u03b7(G,G) \u2208 S1\u22124\u03b7(G,G)] = 1\u2212 o(1) ,\nwhere P is taken over (G,G) \u223c G(n, p) together with the internal randomness of A\u03b7.\ndenote\nM\u03b7 = (1 \u2212 \u03b7) log n\nlog ( log n/np2 ) .\nThe assumption of p implies that 1 \u226a M\u03b7 \u226a log n. We start by defining several good events which we will work with. Recall that\nNs = {j < s : Gj,s = 1},\u2200a\u03b7 < s \u2264 b\u03b7 .\nDefinition 3.1 (Good events in the sparse regime). Denote G0 for the event that\n\u2022 (1\u2212 \u03b7)sp \u2264 |Ns | \u2264 (1 + \u03b7)sp,\u2200a\u03b7 < s \u2264 b\u03b7;\n\u2022 |Nr \u2229Ns | \u2264 (log n)3 ,\u2200a\u03b7 < r < s \u2264 b\u03b7.\nIn addition, for each a\u03b7 < s \u2264 b\u03b7, we denote Gs as the event that\nOk \u2265 M\u03b7,\u2200a\u03b7 < k \u2264 s . (3.2)\nNote that G0 is measurable with respect to G, and for any a\u03b7 < s \u2264 b\u03b7, Gs is measurable with respect to Fs. We first show that G0 is a typical event.\nLemma 3.2. For G \u223c G(n, p), P[G \u2208 G0] = 1\u2212 o(1).\nProof. Note that for each a\u03b7 < s \u2264 b\u03b7, |Ns | is distributed as a binomial variable B(s, p), and for any a\u03b7 < r < s \u2264 b\u03b7, the distribution of |Nr \u2229Ns | is given by B(r, p2). Using the Chernoff bound (2.3), we can show that for any a\u03b7 < s \u2264 b\u03b7,\nP[||Ns | \u2212 sp| \u2265 \u03b7sp] \u2264 2 exp ( \u2212 (\u03b7sp) 2\n2(1 + \u03b7)\u03b7sp\n) = o(1/n),\nand (recall that np2 \u226a log n since we are in the sparse regime)\nP[B(r, p2) \u2265 (log n)3] \u2264 exp ( \u2212 (log n) 6\n2 ( (1 + \u03b7)rp2 + (log n)3\n) ) = o ( 1/n2 ) ,\nThus the lemma follows from a simple union bound.\nWe claim that now it remains to show the following proposition.\nProposition 3.3. For each a\u03b7 < s \u2264 b\u03b7 and any realization of Fs\u22121 that satisfies G0\u2229Gs\u22121, it holds that for any r \u2208 Rs\u22121\nP[Er,s \u2265 M\u03b7 | Fs\u22121,G0 \u2229 Gs\u22121] = 1\u2212 Fs(M\u03b7) \u226b log n/n . (3.3)\nProof of Proposition 3.1. Write Ga\u03b7 for the trivial event. Note that for each a\u03b7 < s \u2264 b\u03b7, we have Os = maxr\u2208Rs\u22121 Er,s. Thus\nP[Gs\u22121 \u2229 G0]\u2212 P[Gs \u2229 G0] = P[Os \u2264 M\u03b7,G0,Gs\u22121] \u2264 P[Os \u2264 M\u03b7 | G0 \u2229 Gs\u22121] = E [ Er,s \u2264 M\u03b7,\u2200r \u2208 Rs\u22121 | Fs\u22121,G0 \u2229 Gs\u22121 ]\n= E [ Fs(M\u03b7) n\u2212s+1 | Fs\u22121,G0 \u2229 Gs\u22121 ]\n= o(1/n) ,\nwhere the second-to-last equality is due to conditional independence, and the last relation follows from (3.3) and the fact that n\u2212 s+ 1 \u2265 \u03b7n for any s \u2264 b\u03b7. This implies that\nP[Gb\u03b7 \u2229 G0] = P[G0] + \u2211\na\u03b7<s\u2264b\u03b7\n( P[Gs \u2229 G0]\u2212 P[Gs\u22121 \u2229 G0] ) \u2265 1\u2212 o(1) ,\nand in particular, P[Gb\u03b7 ] = 1\u2212 o(1). Finally we note that under this event, we have\nO(\u03c0\u2217) = n\u2211\ns=1\nOs \u2212 \u2211\ni<j\nGi,jX\u03c0\u2217(i),\u03c0\u2217(j) \u2265 b\u03b7\u2211\ns=a\u03b7+1\nOs \u2212 1 \u2265 (1\u2212 2\u03b7)nM\u03b7 \u2212 1 ,\nwhich implies O\u0303(\u03c0\u2217) \u2265 (1\u2212 4\u03b7)Sn,p, and thus \u03c0\u2217 \u2208 S1\u22124\u03b7(G,G), completing the proof.\nThe remaining of this section is devoted to the proof of Proposition 3.3. We fix a realization of G that satisfies G0 and then a realization of Fs\u22121 that satisfy Gs\u22121. Note that we may apply the following relaxation:\nEr,s = \u2211\nj\u2208Ns G \u2217 r,\u03c0\u2217(j) \u2265\n\u2211\nj\u2208Ns Gr,\u03c0\u2217(j)\ndef = E\u2032r,s ,\nand E\u2032r,s is measurable with respect to Gr,\u03c0\u2217(j), j \u2208 Ns. According to previous discussions, it holds that\nP[Er,s \u2265 M\u03b7 | Dr,s] \u2265 P[M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 | Dr,s] = P[{M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7} \u2229 Dr,s]\nP[Dr,s]\n= P[M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 ]\u00d7 P[Dr,s |M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 ]\nP[Dr,s] .\nIt is clear that E\u2032r,s \u223c B(|Ns |, p), so we can conclude from Lemma A.1 that\nP[M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7] = P[E\u2032r,s \u2265 M\u03b7]\u2212 P[E\u2032r,s \u2265 6M\u03b7 ] \u226b log n/n ,\nhence it suffices to show that\nP[Dr,s | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 ] \u2265 [1\u2212 o(1)]P[Dr,s]. (3.4)\nTo obtain (3.4), for each r \u2208 Rs\u22121 we define for \u2113 \u2208 N the random set of indices\nR\u2113s = { a\u03b7 < k < s : \u2211\nj\u2208Nk \u2229Ns Gr,\u03c0\u2217(j) = \u2113\n} ,\nnamely, the set of step indices k in which r connect exactly \u2113 edges in G to the set \u03c0\u2217(Nk)\u2229 \u03c0\u2217(Ns). Let Rs denote the collection (R\u2113s, \u2113 \u2208 N). We refer to a collection Rs as typical if it satisfies the conditions |R1s | + |R2s | < n0.6 and R\u2113s = \u2205 for all \u2113 \u2265 3. The following lemma justifies the use of the term \u201ctypical\u201d for such collections.\nLemma 3.4. Whenever Fs\u22121 satisfies Gs\u22121 \u2229 G0, we have\nP[Rs is typical |M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7] \u2265 1\u2212 o(1). (3.5)\nProof. We fix an arbitrary integer K in the range [M\u03b7, 6M\u03b7 ]. We observe that conditioned on the event {E\u2032r,s = K}, the set of indices i \u2208 Ns such that Gr,\u03c0\u2217(i) = 1 forms a uniform subset of Ns with K elements. Consequently, for each K, the quantity \u2211 j\u2208Nk \u2229Ns Gr,\u03c0\u2217(j) follows a hypergeometric distribution HG(|Ns |, |Nk \u2229Ns |,K), which represents the size of the intersection of Nk \u2229Ns with a uniform subset of Ns of cardinality K. Recall that under the good event G0, we have (1 \u2212 \u03b7)sp \u2264 |Ns | \u2264 (1 + \u03b7)sp and |Nk \u2229Ns | \u2264 (log n)3 for each \u230a\u03b7n\u230b \u2264 k \u2264 s\u2212 1.\nFor any triple (N,M,K) with (1 \u2212 \u03b7)sp \u2264 N \u2264 (1 + \u03b7)sp, 0 \u2264 M \u2264 (log n)3 and M\u03b7 \u2264 K \u2264 6M\u03b7 \u226a log n, we have\nP [ HG(N,M,K) \u2265 1 ] = \u2211\nk\u22651\n(M k )(N\u2212M K\u2212k ) (N K ) \u2264 \u2211 k\u22651 ( M k ) \u00b7 N K\u2212k (K \u2212 k)! \u00b7 K! (N \u2212K)K\n\u2264 \u2211\nk\u22651 Mk\n( 1\u2212 K\nN\n)\u2212K+k \u00b7 K k\nk!(N \u2212K)k\n\u2264 exp ( K2\nN\n)\u2211\nk\u22651\n1\nk!\n( KM\nN \u2212K\n)k \u2264 eKM\nN \u2212K \u2264 (log n)4 np .\nAs a result,\nE\n[\u2211\n\u2113\u22651 |R\u2113s | | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7\n] \u2264 C(log n)4/p \u226a n0.6 , (3.6)\nwhere the last inequality follows since we assume p = n\u22121/2+o(1). Similarly we have\nP[HG(N,M,K) \u2265 3] = \u2211\nk\u22653\n(M k )(N\u2212M K\u2212k ) (N K ) \u2264 \u2211 k\u22653 ( M k )( 1\u2212 K N )\u2212K+k ( K N \u2212K )k\n\u2264 ( 1\u2212 K\nN\n)\u2212K\u2211\nk\u22653\n1\nk!\n( MK\nN \u2212K\n)k \u2264 (KM) 3\n(N \u2212K)3 \u2264 (log n)12 (np)3 ,\nand hence it follows E [\u2211\n\u2113\u22653 |R\u2113s | | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7\n] = o(1) . (3.7)\nTherefore, we conclude from Markov inequality that |R1s | + |R2s | \u2264 n0.6 and R\u2113s = \u2205 for any \u2113 \u2265 3 with high probability, which completes the proof of the lemma.\nRecall that\nDr,s = \u22c2\na\u03b7<k<s\n{Er,k \u2264 Ok} = \u22c2\na\u03b7<k<s\n{ \u2211\nj\u2208Nk G \u2217 r,\u03c0\u2217(j) \u2264 Ok} .\nBy introducing the random sets R\u2113s, \u2113 \u2208 N, we obtain that P[Dr,s | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 ] equals to\nP [ Er,k \u2212 \u2211\nj\u2208Nk \u2229Ns Gr,\u03c0\u2217(j) \u2264 Ok \u2212 \u2113,\u2200\u2113 \u2265 0,k \u2208 R\u2113s |M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7\n] .\nFrom the total probability formula, we can express P[Dr,s | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7] as \u2211\n(R0s ,R 1 s ,... )\nP[Rs = (R0s, R1s , . . . ) | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 ]\u00d7\nP [ Er,k \u2212 \u2211\nj\u2208Nk \u2229Ns Gr,\u03c0\u2217(j) \u2264 Ok \u2212 \u2113,\u2200\u2113 \u2265 0,r \u2208 R\u2113s | M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7 ,Rs = (R0s , R1s, . . . )\n] ,\nwhere the summation is taken over all possible realizations of R = (R0s ,R1s , . . . ). Note that for any r \u2208 Rs\u22121,\nEr,k \u2212 \u2211\nj\u2208Nk \u2229Ns Gr,\u03c0\u2217(j) =\n\u2211\nj\u2208Nk \\Ns Gr,\u03c0\u2217(j) +\n\u2211\nj\u2208Nk Xr,\u03c0\u2217(j) ,\nwhich is independent with the random variables Gr,\u03c0\u2217(j), j \u2208 Ns. Conversely, the event {M\u03b7 \u2264 E\u2032r,s \u2264 6M\u03b7} together with the random sets R\u2113s, \u2113 \u2208 N are all measurable with respect to these variables, suggesting that the conditioning in the second probability term can be removed. By combining this with Lemma 3.4, it suffices to show that for any realization of Rs = (R0s,R1s, . . . ) that is typical (denoted as (R0s , R1s, . . . )) the following inequality holds:\nP [ Er,k \u2212 \u2211\nj\u2208Nk \u2229Ns Gr,\u03c0\u2217(j) \u2264 Ok \u2212 \u2113,\u2200\u2113 \u2265 0, k \u2208 R\u2113s\n] \u2265 [1\u2212 o(1)]P[Dr,s] .\nTo tackle this relation, a trivial lower bound simplifies our goal to show\nP [ Er,k \u2264 Ok \u2212 \u2113,\u2200\u2113 \u2265 0, k \u2208 R\u2113s | Dr,s ] \u2265 1\u2212 o(1) ,\nor equivalently (recall that R\u2113s = \u2205 for \u2113 \u2265 3 since (R0s, R1s , . . . ) is typical),\nP [ \u2203\u2113 \u2208 {1, 2}, k \u2208 R\u2113s s.t.Er,k > Ok \u2212 \u2113 | Dr,s ] \u2264 o(1). (3.8)\nSince the event in the probability is increasing and Dr,s is decreasing, the FKG inequality and a simple union bound imply that the left hand side of (3.8) is bounded by\nP [ \u2203\u2113 \u2208 {1, 2}, k \u2208 R\u2113s s.t. Er,k > Ok \u2212 \u2113 ]\n\u2264 ( |R1s|+ |R2s| ) max\na\u03b7<k<s P[E\u2032r,k > Ok \u2212 3]\n\u2264 n0.6 \u00d7 P[B(2np, p) \u2265 M\u03b7 \u2212 3] ,\nwhere in the second inequality we used the facts that |R1s |+ |R2s| \u2264 n0.6 by the assumption that (R0s , R 1 s, . . . ) is typical, |Nk | \u2264 2np by G0 and Ok \u2265 M\u03b7 by Gs\u22121. From Lemma A.1 we see the probability term is n\u22121+\u03b7+o(1), thus (3.8) holds for sufficiently small \u03b7. This concludes Proposition 3.3 and completes the analysis for p in the sparse regime."
        },
        {
            "heading": "3.3 Analysis of greedy algorithm: the dense regime",
            "text": "Now we focus on the regime pc \u226a p \u2264 1/(log n)4. The main result of this subsection is the following, which concludes the first item in Theorem 1.4.\nProposition 3.5. When p satisfies pc \u226a p \u2264 1/(log n)4, for any sufficiently small constant \u03b7 > 0, it holds that as n \u2192 \u221e,\nP[A\u03b7(G,G) \u2208 S\u03b2c\u221220\u03b7(G,G)] = 1\u2212 o(1) ,\nwhere P is taken over (G,G) \u223c G(n, p) and the internal randomness of A\u03b7.\nTo prove Proposition 3.5, we will continue to work with the notations and conventions developed in Section 3.1. Note that\nO(\u03c0\u2217) = n\u2211\ns=1\nOs \u2212 \u2211\ni<j\nGi,jX\u03c0\u2217(i),\u03c0\u2217(j) \u2265 n\u2211\ns=1\nOs \u2212 1 ,\nWe claim that it is sufficient to establish the following lower bounds with high probability.\nProposition 3.6. Denote Gfirst for the event that a\u03b7\u2211\ns=1\nOs \u2265 \u2211\n1\u2264s\u2264a\u03b7 sp2 \u2212 \u03b7Dn,p , (3.9)\nGmiddle for the event that\nOs \u2265 sp2 + \u221a 2(1\u2212 2\u03b7)sp2 log n,\u2200a\u03b7 < s \u2264 b\u03b7 , (3.10)\nand Glast for the event that\nOs \u2265 sp2 \u2212 \u221a 10np2 log n,\u2200b\u03b7 < s \u2264 n . (3.11)\nThen P[Gfirst \u2229 Gmiddle \u2229 Glast] = 1\u2212 o(1).\nWe claim that these lower bounds are adequate for proving Proposition 3.5.\nProof of Proposition 3.5 assuming Proposition 3.6. Recall that Dn,p = \u221a n3p2 log n. Under Gfirst \u2229 Gmiddle \u2229 Glast, we have O(\u03c0\u2217) + 1 is bounded below by \u2211\ns\u2264a\u03b7 sp2 \u2212 \u03b7Dn,p +\n\u2211\na\u03b7<s\u2264b\u03b7\n( sp2 + \u221a 2(1 \u2212 2\u03b7)sp2 log n ) + \u2211\nb\u03b7<s\u2264n\n( sp2 \u2212 \u221a 10np2 log n )\n\u2265 \u2211\ns\u2264n sp2 \u2212 \u03b7Dn,p + (\u03b2c \u2212 2\u03b7)Dn,p \u2212 \u03b7n \u00b7\n\u221a 10np2 log n \u2265\n( n\n2\n) p2 + (\u03b2c \u2212 20\u03b7)Dn,p + 1 ,\nwhere the first inequality follows from the fact that\n\u2211\na\u03b7<s\u2264b\u03b7\n\u221a 2s \u2265\n\u222b b\u03b7\na\u03b7\n\u221a 2xdx =\n\u221a 8x3\n9\n\u2223\u2223\u2223\u2223 b\u03b7\na\u03b7\n\u2265 (\u03b2c \u2212 \u03b7)n \u221a n .\nThis suggests that under Gfirst \u2229 Gmiddle \u2229 Glast it holds \u03c0\u2217 \u2208 S\u03b2c\u221220\u03b7(G,G). Since Gfirst \u2229 Gmiddle \u2229 Glast occurs with high probability, the proof is completed.\nThe remainder of this section is dedicated to proving Proposition 3.6. It is straightforward to deal with the first event, since\n\u2211\n1\u2264s\u2264a\u03b7 Os \u2265\n\u2211\n1\u2264i<j\u2264a\u03b7 Gi,jGi,j ,\nwhich is a binomial variable B ((a\u03b7\n2\n) , p2 ) and P[Gfirst] = 1\u2212 o(1) follows from (2.2).\nThe proof demonstrating that both Gmiddle and Glast occur with high probability is much more involved. We provide the detailed explanation in the following two sub-subsections, respectively. However, before delving into the details of these two events, we introduce the good event G0 in the dense regime concerning G, which will serve as the basis for our analysis.\nDefinition 3.2 (Good event in the dense regime). Denote G0 for the event that G satisfies \u2022 \u2223\u2223|Ns | \u2212 sp \u2223\u2223 \u2264 \u221a4sp log n,\u2200a\u03b7 < s \u2264 b\u03b7;\n\u2022 |Nr \u2229Ns | \u2264 2np2,\u2200a\u03b7 < r < s \u2264 b\u03b7. Similar to Lemma 3.2, we can show, using a simple union bound, that when p falls within the dense regime, it holds P[G \u2208 G0] = 1 \u2212 o(1). Hence, it suffices to prove that under any realization of G that satisfies G0, both Gmiddle and Glast are typical events. For the remainder of this section, we will fix G \u2208 G0 and treat it as a deterministic graph."
        },
        {
            "heading": "3.3.1 Controlling the last steps",
            "text": "For the convenience of analysis, we use another way to encode the random variables {Os}a\u03b7<s\u2264b\u03b7 . Recall that by definition, for any a\u03b7 < s \u2264 b\u03b7 and any r \u2208 Rs\u22121, Fs(x) = P[Er,s \u2264 x | Fs\u22121] = P[Er,s \u2264 x | Dr,s] = P[Er,s \u2264 x | Er,k \u2264 Ok,\u2200a\u03b7 < k < s] .\nWe define a sequence of random variables {cs}a\u03b7<s\u2264b\u03b7 such that\n1\u2212 Fs(Os) = cs\nn\u2212 s+ 1 ,\u2200a\u03b7 < s \u2264 b\u03b7 . (3.12)\nIt is evident that for any a\u03b7 < s \u2264 b\u03b7+1, there is a one-to-one correspondence between the two sequences {Ok}a\u03b7<k<s and {ck}a\u03b7<k<s. Hence all the ck\u2019s with k < s are measurable with respect to Fs\u22121. We provide further properties of the sequence {cs}a\u03b7<s\u2264b\u03b7 in the following lemma.\nLemma 3.7. For any a\u03b7 < s \u2264 b\u03b7 + 1, given the sequence {Ok}a\u03b7<k<s (and hence also {ck}a\u03b7<k<s), the following identity holds for any r \u2208 Rs\u22121,\nP[Dr,s] = \u220f\na\u03b7<k<s\n( 1\u2212 ck\nn\u2212 k + 1\n) . (3.13)\nFurthermore, given any realization of {Ok}a\u03b7<k<s, {ck}a\u03b7<k<s, the conditional distribution of cs is stochastically dominated by an exponential variable with rate 1.\nProof. By the multiplicative rule and the definition of Fk, a\u03b7 < k \u2264 b\u03b7 we have\nP[Dr,s] = P[Er,k \u2264 Ok,\u2200a\u03b7 < k < s] = \u220f\na\u03b7<k<s\nP[Er,k \u2264 Ok | Er,l \u2264 Ol.\u2200a\u03b7 < l < k]\n= \u220f\na\u03b7<k<s\nFk(Ok) = \u220f\na\u03b7<k<s\n( 1\u2212 ck\nn\u2212 k + 1\n) ,\nconcluding (3.13). Furthermore, given any realization of Fs\u22121 (including the realization of {Ok}a\u03b7<k<s, {ck}a\u03b7<k<s), we see for any x \u2265 0, it holds that\nP[cs \u2265 x | Fs\u22121] = P [ 1\u2212 Fs(Os) \u2265\nx n\u2212 s+ 1 | Fs\u22121 ]\n= P [ Fs ( max\nr\u2208Rs\u22121 Er,s\n) \u2264 1\u2212 x n\u2212 s+ 1 | Fs\u22121 ]\n= P [ Er,s \u2264 F\u22121s ( 1\u2212 x\nn\u2212 s+ 1\n) ,\u2200r \u2208 Rs\u22121 | Fs\u22121 ]\n= \u220f\nr\u2208Rs\u22121 P\n[ Er,s \u2264 F\u22121s ( 1\u2212 x\nn\u2212 s+ 1\n) | Fs\u22121 ]\n= ( 1\u2212 x\nn\u2212 s+ 1\n)n\u2212s+1 \u2264 e\u2212x ,\nwhere the second-to-last equality follows from conditional independence, and the last equality arises from the definition of Fs. This indicates that for any a\u03b7 < s \u2264 b\u03b7, the conditional distribution of cs given {ck}a\u03b7<k<s is always dominated by an exponential variable with a rate of 1, concluding the lemma.\nWith Lemma 3.7 in hand, we now prove P[Glast] = 1\u2212o(1). Define G\u2217 as the event that b\u03b7\u2211\ns=a\u03b7+1\ncs n\u2212 s+ 1 \u2264 2/\u03b7 .\nNote that G\u2217 is measurable with respect to Fb\u03b7 . We will show that P[G\u2217] = 1\u2212 o(1), and for any realization of Fb\u03b7 that satisfies G\u2217, it holds for any b\u03b7 + 1 \u2264 s \u2264 n,\nP[Os \u2264 sp2 \u2212 \u221a 10np2 log n | Fs\u22121,G\u2217] = o(1/n) . (3.14)\nOnce this is established, by applying a union bound we obtain\nP [ \u2203b\u03b7 < s \u2264 n,Os \u2264 |Ns |p\u2212 \u221a 10np2 log n ]\n\u2264 P[(G\u2217)c] + n\u2211\ns=b\u03b7+1\nP [ Os \u2264 |Ns |p\u2212 \u221a 10np2 log n | G\u2217 ] = o(1) ,\nand the main result follows. From Lemma 3.7, we observe that the sequence {cs}a\u03b7<s\u2264b\u03b7 is stochastically dominated by i.i.d. exponential variables with mean 1. Therefore, by the law of large numbers, it follows that with high probability,\nb\u03b7\u2211\ns=a\u03b7+1\ncs n\u2212 s+ 1 \u2264 1 \u03b7n\nb\u03b7\u2211\ns=a\u03b7+1\ncs \u2264 2/\u03b7 .\nThis implies that P[G\u2217] = 1 \u2212 o(1). The main purpose of G\u2217 is as follows: whenever Fb\u03b7 satisfies G\u2217, we get from equation (3.13) that for any r \u2208 Rb\u03b7 ,\nP[Dr,b\u03b7 ] = b\u03b7\u220f\ns=a\u03b7+1\n( 1\u2212 cs\nn\u2212 s+ 1\n) \u2265 exp ( \u2212 b\u03b7\u2211\ns=a\u03b7+1\n2cs n\u2212 s+ 1\n) \u2265 exp(\u22124/\u03b7) . (3.15)\nNow we fix a realization of Fb\u03b7 that satisfies G\u2217. Recall that in the last n \u2212 b\u03b7 steps, A\u03b7 completes \u03c0\u2217 to a permutation such that \u03c0\u2217(b\u03b7 +1) < \u00b7 \u00b7 \u00b7 < \u03c0\u2217(n) and hence the values of \u03c0\u2217(b\u03b7 + 1), \u00b7 \u00b7 \u00b7 , \u03c0\u2217(n) are measurable with respect to Fb\u03b7 . Conditioned on Fb\u03b7 , for each b\u03b7 + 1 \u2264 s \u2264 n, we have Os = \u2211 j<sGj,sG \u2217 \u03c0\u2217(j),\u03c0\u2217(s) which is measurable with respect to G \u2217 j,\u03c0\u2217(s), 1 \u2264 j \u2264 n. Therefore, Os is conditionally independent with the events Dr,b\u03b7 for r \u2208 Rb\u03b7 \\ {\u03c0\u2217(s)}. By combining this observation with (3.15) (let us denote the event {Os \u2264 sp2 \u2212 \u221a 10np2 log n} as As), we can conclude that\nP[As | Fb\u03b7 ] = P[As | Db\u03b7 ] = P[As | D\u03c0\u2217(s),b\u03b7 ] \u2264 P[As]\nP[D\u03c0\u2217(s),b\u03b7 ] \u2264 exp(4/\u03b7)P[As] .\nFinally, Note that\nOs \u2265 \u2211\nj<s\nGj,sG\u03c0\u2217(j),\u03c0\u2217(s) = \u2211\nj\u2208Ns G\u03c0\u2217(j),\u03c0\u2217(s) ,\nwhich is a binomial variable B(|Ns |, p). And from the definition of G0 we see\nsp2 \u2212 \u221a 10np2 log n \u2264 |Ns |p\u2212 \u221a 4|Ns |p log n .\nTherefore, by applying (2.2) we get\nP[As] \u2264 P [ B(|Ns |, p) \u2264 |Ns |p\u2212 \u221a 4|Ns |p log n ] \u2264 exp(\u22122 log n) .\nAs a result, P[As | Fb\u03b7 ] \u2264 exp(4/\u03b7 \u2212 2 log n) = o(1/n), which establishes (3.14) and thus completes the proof of P[Glast] = 1\u2212 o(1)."
        },
        {
            "heading": "3.3.2 Controlling the middle steps",
            "text": "Finally, we prove that P[Gmiddle] = 1\u2212 o(1). Similar as before, we begin by defining some good events Gs, a\u03b7 < s \u2264 b\u03b7 in the dense regime.\nDefinition 3.3. For each a\u03b7 < s \u2264 b\u03b7, define Gs as the event that\n\u2022 ck \u2264 2 log n,\u2200a\u03b7 < k \u2264 s and \u2211 a\u03b7<k\u2264s ck \u2264 2n;\n\u2022 \u221a 2(1 \u2212 \u03b7)|Nk |p log n \u2264 Ok \u2212 |Nk |p \u2264 \u221a 10np2 log n,\u2200a\u03b7 \u2264 k \u2264 s;\n\u2022 For each a\u03b7 < k \u2264 s, it holds that\n1\u2212 Fk(Ok) \u2265 ( 1\u2212 o(1) )( 1\u2212 F \u2217k (Ok) ) , (3.16)\n(recall that F \u2217k is the distribution function of Er,k without conditioning on Dr,k) where the o(1) terms are uniform for each a\u03b7 < k \u2264 s.\nNote that by the first item in G0 (Definition 3.2) it holds\n|Ns |p+ \u221a 2(1 \u2212 \u03b7)|Ns |p log n \u2265 sp2 + \u221a 2(1 \u2212 2\u03b7)sp2 log n ,\nso Gb\u03b7 \u2282 Gmiddle. Now it remains to show that under any realization of G \u2208 G0, Gb\u03b7 holds with high probability.\nWrite Ga\u03b7 for the trivial event. Our goal is to show that for any a\u03b7 < s \u2264 b\u03b7, it holds the following inequality\nP[Gs\u22121]\u2212 P[Gs] \u2264 P[Gcs \u2229 Gs\u22121] = o(1/n) .\nTherefore, for the rest part we fix a\u03b7 < s \u2264 b\u03b7 and focus on establishing this relation. It can be deduced from Lemma 3.7 that the events cs \u2264 2 log n and \u2211 a\u03b7<s\u2264s cs \u2264 2n fail with probability o(1/n). Additionally, utilizing the FKG inequality, we can show that\n(note that {Os \u2265 |Ns |p+ \u221a 10|Ns |p log n } is an increasing event while Ds is a decreasing event)\nP [ Os \u2265 |Ns |p+ \u221a 10|Ns |p log n | Fs\u22121 ] = P [ Os \u2265 |Ns |p+ \u221a 10|Ns |p log n | Ds ]\n\u2264 P [ Os \u2265 |Ns |p+ \u221a 10|Ns |p log n ] \u2264 nP [ B(|Ns |, p) \u2265 |Ns |p+ \u221a 10|Ns |p log n\u2212 1 ] ,\nwhich is o(1/n) by (2.3). Now we claim that the events {cs \u2264 2 log n}, { \u2211\na\u03b7<k\u2264s ck \u2264 2n} together with Gs\u22121 indeed imply that Os \u2265 |Ns |p+ \u221a 2(1 \u2212 \u03b7)|Ns |p log n as well as (3.16) holding for k = s. This would complete the proof. To verify this claim, we take any r\u2217 \u2208 Rs\u22121 and define \u2126s = \u21261s \u00d7 \u21262s = {0, 1}|Ns | \u00d7 [0, 1/n2]|Ns |, denoted by { (gj , . . . , xj , . . . )j\u2208Ns : gj \u2208 {0, 1}, 0 \u2264 xj \u2264 1/n2,\u2200j \u2208 Ns } ,\nas the set of realizations of (Gr\u2217,\u03c0\u2217(j))j\u2208Ns and (Xr\u2217,\u03c0\u2217(j))j\u2208Ns . We see P restricts as a product measure of |Ns | Bernoulli variables with parameter p and |Ns | uniform variables in [0, 1/n2]. Denote\nTk =    |Nk \u2229Ns |p+ \u221a 10|Nk \u2229Ns |p log n , p \u2264 1/(log n)4 and np3 \u2265 (log n)3 , 2(log n)3 , n\u22120.1 \u2264 np3 < (log n)3 , 20 , p \u226b pc and np3 < n\u22120.1 . (3.17)\nwe define \u2126\u2217s \u2282 \u21261s as the set of coordinates \u03c9 = (gj , . . . )j\u2208Ns that satisfies \u2211\nj\u2208Nk \u2229Ns gj \u2264 Tk,\u2200a\u03b7 < k < s . (3.18)\nFurthermore, denote Is for the interval [ |Ns |p+ \u221a 2(1\u2212 \u03b7)|Ns |p log n, |Ns |p+ \u221a 10|Ns |p log n ]\nand for any x \u2208 Is we define Ps,x as the conditional distribution on \u2126s given that\nEr\u2217,s = \u2211\nj\u2208Ns G \u2217 r\u2217,\u03c0\u2217(j) =\n\u2211\nj\u2208Ns (Gr\u2217,\u03c0\u2217(j) +Xr\u2217,\u03c0\u2217(j)) \u2265 x .\nWe will utilize the following two lemmas, the proofs of which have been postponed to the appendix.\nLemma 3.8. For any x \u2208 Is it holds Ps,x[\u03c91 \u2208 \u2126\u2217s] = 1\u2212 o(1/n). Lemma 3.9. Denote \u03b4(n) = 2/ \u221a log n, then when pc \u226a p \u2264 1/(log n)4, for each a\u03b7 < k < s it holds\nP\n[ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 Tk \u2212 1\n] \u2265 F \u2217k (Ok)\u2212 2\u03b4(n)(ck + 1)\nn\u2212 k + 1 . (3.19)\nWe will first show that (3.16) holds when replacing Os with any x \u2208 Is. For any x \u2208 Is, by definition we have\n1\u2212 Fs(x) = P[Er\u2217,s \u2265 x | Dr\u2217,s] = P[Er\u2217,s \u2265 x,Dr\u2217,s]\nP[Dr\u2217,s] .\nFor the numerator, it can be rewritten as\nP[Er\u2217,s \u2265 x,Dr\u2217,s] = P[Er\u2217,s \u2265 x] \u00b7 Ps,x[Dr\u2217,s] = ( 1\u2212 F \u2217s (x) ) Ps,x[Dr\u2217,s] .\nWe expand Ps,x[Dr\u2217,s] according to the realization of \u03c91 = (gj , . . . )j\u2208Ns :\nPs,x[Dr\u2217,s] = \u2211\n\u03c91\u2208{0,1}|N |s Ps,x[\u03c91] \u00b7 Ps,x[Dr\u2217,s | \u03c91] \u2265\n\u2211\n\u03c91\u2208\u2126\u2217s\nPs,x[\u03c91] \u00b7 Ps,x[Dr\u2217,s | \u03c91] . (3.20)\nFor any realization \u03c91 \u2208 \u2126\u2217s, we have\nPs,x[Dr\u2217,s | \u03c91] = P[Er\u2217,k \u2264 Ok,\u2200a\u03b7 < k < s | \u03c91, Er\u2217,s \u2265 x] \u2265 P [ \u2211\nj\u2208Nk Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 1,\u2200a\u03b7 < k < s\n\u2223\u2223\u03c91, Er\u2217,s \u2265 x ]\n= P [ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 1\u2212\n\u2211 j\u2208Ns \u2229Nk gj \u2223\u2223\u03c91, \u2211 j\u2208Ns Xr\u2217,\u03c0\u2217(j) \u2265 x\u2212 \u2211 j\u2208Ns gj\n]\n= P [ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 1\u2212\n\u2211\nj\u2208Nk \u2229Ns gj ,\u2200a\u03b7 < k < s\n] ,\nwhere the last equality follows from independence. Note that \u03c91 \u2208 \u2126\u2217s means \u2211\nj\u2208Nk \u2229Ns gj \u2264 Tk,\u2200a\u03b7 < k < s ,\nHence, the above expression is lower-bounded by\nP\n[ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 Tk \u2212 1,\u2200a\u03b7 < k < s\n] ,\nwhich is bounded below by\n\u220f\na\u03b7<k<s\nP\n[ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 Tk \u2212 1\n]\nfrom FKG inequality since all these events are decreasing. From Lemma 3.9 and the third item in Gs\u22121, we see for any a\u03b7 < k < s,\nP\n[ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 Tk \u2212 1\n] \u2265 F \u2217k (Ok)\u2212 2\u03b4(n)(ck + 1)\n(n\u2212 k + 1) \u2265 1\u2212 ck + o(ck + 1) n\u2212 k + 1 ,\nwhere the o(\u00b7) terms are uniform for all a\u03b7 < k < s. Plugging this into the above estimation, we see for any \u03c91 \u2208 \u2126\u2217s, Ps,x[Dr\u2217,s | \u03c91] is bounded from below by \u220f\na\u03b7<k<s\n( 1\u2212 ck + o(ck + 1)\nn\u2212 k + 1\n) \u2265 ( 1\u2212 o(1) ) \u220f\na\u03b7<k<s\n( 1\u2212 ck\nn\u2212 k + 1\n) = ( 1\u2212 o(1) ) P[Dr\u2217,s] ,\nwhere in the first inequality we used the fact that \u2211\na\u03b7<s\u2264b\u03b7 cs \u2264 2n and the second equality follows from (3.13). As a result, we see from (3.20) that\nPs,x[Dr\u2217,s] \u2265 Ps,x[\u03c91 \u2208 \u2126\u2217s]\u00d7 ( 1\u2212 o(1) ) P[Dr\u2217,s] \u2265 ( 1\u2212 o(1) ) P[Dr,s] ,\nwhere the last inequality follows from Lemma 3.8. Therefore, we get for any x \u2208 Is, 1\u2212 Fs(x) \u2265 ( 1\u2212 o(1) ) (1\u2212 F \u2217s (x)) , (3.21) which is equivalent to (3.16) with Ok replaced by x \u2208 Is. It is also worth noting that the o(1) term can be taken uniformly for a\u03b7 < s \u2264 b\u03b7.\nFinally, it can be deduced from Lemma A.2 that\n1\u2212 F \u2217s ( |Ns |p+ \u221a 2(1 \u2212 \u03b7)|Ns |p log n ) = n\u22121+\u03b7+o(1) \u226b log n\nn\u2212 s+ 1 ,\nand thus cs \u2264 2 log n together with (3.21) implies Os \u2265 |Ns |p + \u221a 2(1 \u2212 \u03b7)|Ns |p log n. In particular, we conclude that (3.16) holds for k = s, and this completes the proof.\nRemark 3.1. As mentioned in Remark 1.2, we anticipate that the assumption p \u2264 1/(log n)4 is primarily for technical reasons and can potentially be removed. The proof presented above relies on the property that under this assumption, one has\n1\u2212 Fs(Os) \u2265 ( 1\u2212 o(1) ) (1\u2212 F \u2217s (Os))\nwith overwhelming probability. Therefore, a simple application of the FKG inequality is sufficient for our purposes. However, we believe that for any p = o(1), it should hold that\n1\u2212 Fs(Os) \u2265 n\u2212o(1)(1\u2212 F \u2217s (Os)) , which would also be enough to establish Os \u2265 sp2 + \u221a 2(1\u2212 \u03b7)sp2 log n. Unfortunately, deriving such estimates seems challenging, as the FKG inequality is not particularly tight when p exceeds some poly-log factor threshold. Detailed calculations involving multiple binomial variables are required to obtain precise results."
        },
        {
            "heading": "4 Hardness results via the overlap-gap property",
            "text": "In this section, we present evidence supporting the existence of statistical-computational gap in the dense regime based on various types of the overlap-gap property. Recall the property of admissibility defined as in Definition 2.1. We fix an arbitrary p in the dense regime, and in most part of this section, we will work with some specific realization of G which is p-admissible."
        },
        {
            "heading": "4.1 Failure of stable algorithms via 2-OGP",
            "text": "This subsection is dedicated to proving the failure of stable algorithms in finding nearoptimal solutions. To formally state Theorem 1.3, we introduce several definitions and notations. Specifically, we use d(\u00b7) to denote the Hamming distance on the symmetric group. We also define stable algorithms in GAA as follows.\nDefinition 4.1 (stable algorithms). We say an algorithm A \u2208 GAA is (\u2206, \u03ba)\u2212stable, if\nd(A(G,G),A(G\u2032,G)) \u2264 \u03ba (4.1)\nholds for G,G\u2032,G \u2208 Gn whenever G and G\u2032 differ at most \u2206 edges. Furthermore, we say A \u2208 GAA is almost (\u2206, \u03ba)-stable, if (4.1) holds with probability 1\u2212o(1/n2) for G,G\u2032,G \u2208 Gn, where (G,G) is sampled from G(n, p)\u22972 and G\u2032 is any graph that differs from G at most \u2206 edges.\nWrite \u03c10 = 1/3. Fix any \u03b20 \u2208 ( \u221a 25/27, 1) and take \u03b7 > 0 such that\n2\u2212 \u03c10 + \u03b7 \u2212 2\u03b220\n1 + (\u03c10 + \u03b7)2 < 0 . (4.2)\nNote that this can be accomplished since (4.2) holds when \u03b7 = 0. The following theorem presents our main evidence of algorithmic barriers in the dense regime.\nTheorem 4.1. When p lies in the dense regime, for any almost (1, \u03b7n)-stable algorithm A \u2208 GAA, it holds P[A(G,G) \u2208 S\u03b20(G,G)] \u2264 1\u2212 1/n2 , (4.3) where the probability is taken over (G,G) \u223c G(n, p)\u22972.\nWe offer several comments on Theorem 4.1. Our result excludes the possibility of achieving near-optimal solutions with high enough probability for a broad class of algorithms. Moreover, although Theorem 4.1 states only for deterministic algorithms, similar hardness result can be obtained for randomized stable algorithms with verbal changes. While an upper bound of 1\u2212 1/n2 for the success probability may seem less than ideal, we still consider it as an indication of computational barriers for the following reasons: (i) Efficient algorithms that can find near-optimal solutions (like the algorithms constructed for Theorem 1.2) typically achieve this with extremely high probability, often super-polynomially close to 1. (ii) Additionally, through further analysis, we can demonstrate that for sufficiently stable algorithms (e.g., (\u2206, \u03ba)-stable algorithms with \u03ba2 \u226a \u2206), none of them can find nearoptimal solutions with any non-vanishing probability. Based on these considerations, Theorem 4.1 provides convincing evidence for the computational hardness in the dense regime.\nTo prove Theorem 4.1, we will make use of the overlap-gap property (2-OGP). Denote N = (n 2 ) . Recall that U represents the set of unordered pairs. We label the elements in U as e1, . . . , eN according to lexicographical order. For illustration, e1 = (1, 2), e2 = (1, 3), e3 = (2, 3), and so on, up to eN = (n \u2212 1, n). This labeling possesses a specific property: for any \u03b1 \u2208 [0, 1], the set of pairs E\u03b1 = {ek : k \u2264 \u03b1N} is contained within the first(\u221a\n2\u03b1 + o(1) ) n integers.\nWe begin by introducing the definition of the (2, \u03b1)-correlated instance.\nDefinition 4.2 ((2, \u03b1)-correlated instance). For any \u03b1 \u2208 [0, 1], we call a pair of graphs (G1, G2) is a (2, \u03b1)-correlated instance, if\nG1ek = G 2 ek , 1 \u2264 k \u2264 \u230a\u03b1N\u230b, Giek , \u230a\u03b1N\u230b + 1 \u2264 k \u2264 N, i = 1, 2 ,\nare independent B(1, p) variables.\nRecall the property of admissibility in Definition 2.1. For any admissible graph G, we provide a useful estimate for (2, \u03b1)-correlated instances analogous to Proposition 2.6.\nProposition 4.2. For any admissible graph G, any \u03b1, \u03c1 \u2208 [0, 1] and \u03c01, \u03c02 \u2208 Sn with overlap(\u03c01, \u03c02) = \u03c1n, it holds that\nP [ O\u0303(\u03c01) \u2265 \u03b20Dn,p , O\u0303(\u03c02) \u2265 \u03b20Dn,p ] \u2264 exp ( \u22122\u03b2 2 0n log n 1 + \u03c12 \u2227 \u03b1 + o(n log n) ) , (4.4)\nwhere O\u0303(\u03c0i) is the centered overlap of (G i,G) through \u03c0i, i = 1, 2, and P is taken over (2, \u03b1)-correlated instance (G1, G2).\nProof. The proof of Proposition 4.2 is very similar to that of Proposition 2.6 so we only give a sketch. Since G is a deterministic admissible graph, we have\nO(\u03c01) = \u2211\n(i,j)\u2208U G1i,jG\u03c01(i),\u03c01(j) = S0 + S1 , O(\u03c02) =\n\u2211\n(i,j)\u2208U G2i,jG\u03c02(i),\u03c02(j) = S0 + S2 ,\nwhere S0, S1, S2 are independent binomial variables given by\nS0 = \u2211\n(i,j)\u2208E\u03b1 G1i,j1G\u03c01(i),\u03c01(j)=G\u03c02(i),\u03c02(j)=1 , S1 = O(\u03c01)\u2212 S0 , S2 = O(\u03c02)\u2212 S0.\nAs in the proof of Proposition 2.6, the core lies in controlling the variance of S0. On the one hand, we have\nVar(S0) = p(1\u2212 p) \u2211\n(i,j)\u2208E\u03b1 1G\u03c01(i),\u03c01(j)=G\u03c02(i),\u03c02(j)=1\n\u2264 p \u2211\n(i,j)\u2208U 1G\u03c01(i),\u03c01(j)=G\u03c02(i),\u03c02(j)=1\n= p|OL(G, \u03c01 \u25e6 \u03c0\u221212 )| ,\nwhich is bounded by ( \u03c12 + o(1) ) Np2 from the third item in admissibility. On the other hand, considering the specific property enjoyed by the labelling, we have\nVar(S0) \u2264 p \u2211\n(i,j)\u2208E\u03b1 1G\u03c01(i),\u03c01(j)=1\n\u2264 \u2211\ni<j<( \u221a 2\u03b1+o(1))n\nG\u03c01(i),\u03c01(j) ,\nwhich is bounded by ( \u03b1 + o(1) ) Np2 from the second item of admissibility. As a result, Var(S0) is bounded by ( \u03c12 \u2227 \u03b1 + o(1) ) Np2 and this gives rise to the factor \u03c12 \u2227 \u03b1 in the expression. We omit the detailed calculations.\nThe main input for proving Theorem 4.1 is the following geometric property of \u03b20optimal solution spaces concerning the (2, \u03b1)-correlated instance. This property suggests that certain forbidden structures are highly unlikely to exist.\nProposition 4.3. For a couple of graph pairs (G1,G), (G2,G), denote\nEFS = EFS ( (G1,G), (G2,G) )\nas the event that there exists \u03c0i \u2208 S\u03b20(Gi,G), i = 1, 2 such that overlap(\u03c01, \u03c02) \u2208 ( (\u03c10 \u2212 \u03b7)n, (\u03c10 + \u03b7)n ) . (4.5)\nThen for any \u03b1 \u2208 [0, 1], it holds that P[EFS happens for (G1,G), (G2,G)] = o(1/n2) , (4.6)\nwhere the probability is taken over a (2, \u03b1)-correlated instance (G1, G2) together with another independent G \u223c G(n, p). Proof. Since P[G is admissible] = 1\u2212 o(1/n2) by Lemma 2.2, we can fix a realization of G which is admissible and work with this specific graph. The next step is to prove the result by applying a union bound.\nOn the one hand, we consider the enumeration of permutations pairs (\u03c01, \u03c02) that satisfy equation (4.5). It is easy to see that there are at most\n\u2211\n\u03c1\u2208(\u03c1\u2212\u03b7,\u03c1+\u03b7),\u03c1n\u2208N n!\u00d7\n( n\n\u03c1n\n) ((1\u2212 \u03c1)n)! \u2264 exp ((2\u2212 \u03c10 + \u03b7)n log n+O(n))\nmany of such pairs. On the other hand, according to Proposition 4.2, for each of these pairs and any \u03b1 \u2208 [0, 1],\nP [ \u03c01 \u2208 S\u03b20(G1,G), \u03c02 \u2208 S\u03b20(G2,G) ] \u2264 exp ( \u2212 2\u03b2 2 0n log n\n1 + (\u03c10 + \u03b7)2 + o(n log n)\n) .\nAs a result, the probability of EFS conditioned on G is bounded by\nexp ([ 2\u2212 \u03c10 + \u03b7 \u2212\n2\u03b220 1 + (\u03c10 + \u03b7)2\n+ o(1) ] n log n ) (4.2) = exp ( \u2212 \u2126(n log n) ) = o(1/n2) ,\ncompleting the proof.\nNow we proceed with the proof of Theorem 4.1. We begin by sampling three graphs G, G\u2032 and G, independently from G(n, p). We will construct an interpolation path between G and G\u2032 as follows: for each 0 \u2264 k \u2264 N , let Gk be the graph defined by\nGkei = Gei for 1 \u2264 i \u2264 k and Gkei = G\u2032ei for i = k + 1, . . . , N.\nThen it is straightforward to verify that (G0, Gk) forms a (2, k/N)-correlated instance. Define\nEOGP = N\u22c2\nk=0\nEcFS ( (G0,G), (Gk,G) ) (4.7)\nas the event that the forbidden structure does not exist on the entire interpolation path. By combining Proposition 4.3 with a union bound, we can conclude that P[EOGP] = 1\u2212 o(1).\nNow take any almost (1, \u03b7n)-stable algorithm A \u2208 GAA, we define the following three events\nEsuc = {A(Gk,G) \u2208 S\u03b20(Gk,G),\u22000 \u2264 k \u2264 N} , (4.8) Estable = {d ( A(Gk,G),A(Gk+1,G) ) \u2264 \u03b7n,\u22000 \u2264 k \u2264 N \u2212 1} , (4.9)\nEends = {overlap ( A(G0,G),A(GN ,G) ) \u2264 (\u03c1\u2212 \u03b7)n} . (4.10)\nProposition 4.4. The intersection of EOGP, Esuc, Estable and Eends is empty. Proof. We denote Ok = overlap ( A(G0,G),A(Gk,G) ) for 0 \u2264 k \u2264 N . Based on Estable and the triangle inequality, we obtain that |Ok+1 \u2212 Ok| \u2264 2\u03b7n holds for any 0 \u2264 k \u2264 N \u2212 1. Considering that O0 = n and ON \u2264 (\u03c1 \u2212 \u03b7)n based on Eends, we can conclude that there exists 1 \u2264 k \u2264 N \u2212 1 such that Ok \u2208 ( (\u03c1 \u2212 \u03b7)n, (\u03c1 + \u03b7)n ) . Assuming that Esuc also holds, we have A(G0,G) \u2208 S\u03b20(G0,G) and A(Gk,G) \u2208 S\u03b20(Gk,G). However, this contradicts with EOGP, which means the intersection of the four events is empty.\nWe are now ready to prove the main result of this section.\nProof of Theorem 4.1. We will argue by contradiction. Assume that for some A it holds\nP[A(G,G) \u2208 S\u03b20(G,G)] \u2265 1\u2212 1/n2 .\nThen it holds P[Esuc] \u2265 1/2 from a union bound. Similarly, from Proposition 4.3 and the definition of almost (1, \u03b7n)-stable algorithms, we can conclude that P[EOGP] = 1\u2212o(1) and P[Estable] = 1\u2212 o(1).\nFurthermore, we note that (G0,G) and (GN ,G) form a pair of (2, 0)-correlated instance. Therefore, by utilizing Lemma 2.2, Proposition 4.2, and a similar argument as in the proof of Proposition 4.3, we get the probability P[Ecends] is bounded by\nP[G is not admissible] + P[Ecends | G is admissible] \u2264 o(1) + exp ( [2\u2212 \u03c10 + \u03b7 \u2212 2\u03b220 + o(1)]n log n ) = o(1) .\nThis shows P[Eends] = 1\u2212 o(1) and thus\nP[EOGP \u2229 Esuc \u2229 Estable \u2229 Eends] \u2265 1/2 \u2212 o(1)\u2212 o(1)\u2212 o(1) > 0 ,\ncontradicting Proposition 4.4. This concludes the desired result."
        },
        {
            "heading": "4.2 Failure of online algorithms via branching-OGP",
            "text": "In this subsection, we establish the hardness result for online algorithms, which corresponds to the second item in Theorem 1.4. The proof enjoys a similar flavor with the proof of failure of stable algorithms. We begin by demonstrating that a particular forbidden structure is highly unlikely to exist. Subsequently, we show that it is possible to construct such a structure by leveraging online algorithms. The desired result then follows from a combination of these two considerations."
        },
        {
            "heading": "4.2.1 The branching overlap-gap property",
            "text": "Let \u03b5 > 0 be fixed. The objective of this subsection is to design a specific forbidden structure and demonstrate its highly improbable existence. By the definition of the Riemann integral, we have that for a sequence 0 = \u03b10 < \u03b11 < \u00b7 \u00b7 \u00b7 < \u03b1N = 1 with \u2206 = max1\u2264i\u2264n(\u03b1i \u2212 \u03b1i\u22121),\nlim \u2206\u21920\nN\u2211\ni=1\n(\u03b1i \u2212 \u03b1i\u22121) \u221a \u03b1i + \u03b1i\u22121 =\n\u222b 1\n0\n\u221a 2xdx =\n2 \u221a 2x3\n3\n\u2223\u2223\u2223 1\n0 = \u03b2c.\nTherefore, we can pick some integer N together with \u2212\u2192\u03b1 = (\u03b10, . . . , \u03b1N ), 0 = \u03b10 < \u03b11 < \u00b7 \u00b7 \u00b7 < \u03b1N = 1 such that\nN\u2211\ni=1\n(\u03b1i \u2212 \u03b1i\u22121) \u221a \u03b1i + \u03b1i\u22121 < \u03b2c + \u03b5/3 . (4.11)\nDenote \u03b2c+2\u03b5/3\u03b2c+\u03b5/3 as 1 + \u03b4. Fix a large integer D satisfying\nD > max 1\u2264k\u2264N \u03b1k\u22121 2\u03b4(\u03b1k \u2212 \u03b1k\u22121) . (4.12)\nConsider a D-regular tree T with (N + 1) generations. We denote the set of leaves in T by L, which has size L = |L| = DN . For a vertex v of T , we use |v| to denote its depth in T . If v \u2208 L, we use v(k) to denote the ancestor of v with depth k. For two vertices v,w \u2208 L, we define v \u2227w as the common ancestor of v and w with the largest depth.\nWe now define the (T ,\u2212\u2192\u03b1 )-correlated instance as follows:\nDefinition 4.3 ((T ,\u2212\u2192\u03b1 )-correlated instance). For each vertex v of T , we assign it with independent B(1, p) variables {Evi,j}1\u2264i<j\u2264\u03b1|v|n. A (T ,\u2212\u2192\u03b1 )-correlated instance is a set of graphs {Gv}v\u2208L indexed by L, where for each v \u2208 L, the adjacency matrix of Gv is defined by\nGvi,j = E v(k) i,j , if i < j and \u03b1k\u22121n < j \u2264 \u03b1kn .\nNow we are able to state the forbidden structure concerning (T ,\u2212\u2192\u03b1 )-correlated instance.\nDefinition 4.4 (Forbidden structure for correlated instance). For a (T ,\u2212\u2192\u03b1 )-correlated instance {Gv}v\u2208L as well as a graph G \u2208 Gn, we say a set of permutations {\u03c0v}v\u2208L has the forbidden structure, if\n(i) For each v \u2208 L, \u03c0v \u2208 S\u03b2c+\u03b5(Gv,G).\n(ii) For each pair of u, v \u2208 L, it holds that \u03c0u(i) = \u03c0v(i) for any 1 \u2264 i \u2264 \u03b1|u\u2227v|n.\nIn the next proposition, we will show that for any admissible graph G, the forbidden structure exists with a probability no greater than exp ( \u2212 \u2126(n log n) ) . This result will serve as a crucial ingredient for deriving the hardness results for online algorithms.\nProposition 4.5. There exists c > 0 such that for any admissible graph G,\nP[the forbidden structure exists] \u2264 exp(\u2212cn log n) , (4.13)\nwhere the probability is taken over (T ,\u2212\u2192\u03b1 )-correlated instance.\nProof. Keep in mind that G is a deterministic graph which is admissible. Let us assume the existence of a forbidden structure {\u03c0v}v\u2208L. We define a set of embeddings {\u03c3v}v\u2208T indexed by the vertices of T as follows: for each vertex v of T , \u03c3v is an embedding from {i : \u03b1|v|\u22121n < i \u2264 \u03b1|v|n} to [n] with \u03c3v(i) = \u03c0v\u2032(i) for each i, where v\u2032 is any descendant of v in L. Note that condition (ii) guarantees the well-definedness of {\u03c3v}v\u2208T , and the mapping {\u03c0v}v\u2208L 7\u2192 {\u03c3v}v\u2208T is clearly injective.\nFor each v \u2208 T , denote \u03b3\u2217v as the random variable such that \u2211\ni<j, 1\u2264i\u2264\u03b1|v|n,\n\u03b1|v|\u22121n<j\u2264\u03b1|v|n\nGvi,jG\u03c3v(i),\u03c3v(j) = \u2211\ni<j, 1\u2264i\u2264\u03b1|v|n,\n\u03b1|v|\u22121n<j\u2264\u03b1|v|n\nG\u03c3v(i),\u03c3v(j)p+ \u03b3 \u2217 vDn,p . (4.14)\nFor any v \u2208 L, summing (4.14) along the path connecting from the root to v, we get\n\u2211\ni<j\nGvi,jG\u03c0v(i),\u03c0v(j) = |E(G)|p + N\u2211\nk=0\n\u03b3\u2217v(k)Dn,p . (4.15)\nCombining with condition (i), we have that the left hand side of (4.15) is lower bounded by (n 2 ) p2 + (\u03b2c + \u03b5)Dn,p, which implies\nN\u2211\nk=0\n\u03b3\u2217v(k) \u2265 \u03b2c + \u03b5\u2212 | (n 2 ) p\u2212 |E(G)||p Dn,p (2.5) \u2265 \u03b2c + 2\u03b5/3, \u2200 v \u2208 L .\nSumming over the above inequality for all v \u2208 L, we get N\u2211\nk=0\n1\nDk\n\u2211\n|v|=k \u03b3\u2217v \u2265 \u03b2c + 2\u03b5/3 .\nCombining with (4.11), we see there must exists some 0 \u2264 k \u2264 N such that 1\nDk\n\u2211\n|v|=k \u03b3\u2217v \u2265\n\u03b2c + 2\u03b5/3\n\u03b2c + \u03b5/3 \u00b7(\u03b1k\u2212\u03b1k\u22121)\n\u221a \u03b1k + \u03b1k\u22121 = (1+\u03b4)(\u03b1k\u2212\u03b1k\u22121) \u221a \u03b1k + \u03b1k\u22121 . (4.16)\nWe denote the corresponding events as Ek, k = 0, 1, . . . , N , and thus conclude that\nP[forbidden structure exists] \u2264 N\u2211\nk=0\nP[Ek] .\nNow, we need to bound the probability of Ek, and we will do this using the first moment method. Note that Ek only depends on those \u03c3v with |v| \u2264 k. Hence, we can bound P[Ek] by considering the number of possible realizations of {\u03c3v}|v|\u2264k as well as the probability that (4.16) occurs for each fixed {\u03c3}|v|\u2264k. It is not hard to see that the number of possible realizations of such {\u03c3v}|v|\u2264k is bounded by\nexp\n( k\u2211\ni=1\nDi\u22121(\u03b1i \u2212 \u03b1i\u22121)n log n+O(n) ) .\nIn order to control the probability term, for each realization {\u03c3v}|v|\u2264k, we note that by our constructions, the family of random variables\nX\u03c3v def =\n\u2211\ni<j, 1\u2264i\u2264\u03b1|v|n,\n\u03b1|v|\u22121n<j\u2264\u03b1|v|n\nGvi,jG\u03c3v(i),\u03c3v(j), v \u2208 T\nare mutually independent with each other (and so do the variables \u03b3\u2217v , v \u2208 T ), while each X\u03c3v has the distribution of B(N \u03c3 v , p), where\nN\u03c3v def =\n\u2211\ni<j, 1\u2264i\u2264\u03b1|v|n,\n\u03b1|v|\u22121n<j\u2264\u03b1|v|n\nG\u03c3v(i),\u03c3v(j) .\nFrom (2.6) in admissibility we have for any v \u2208 T\nN\u03c3v = \u03b12|v| \u2212 \u03b12|v|\u22121\n2 n2p+ o(n2p) .\nHence, by (2.3) we see for each vertex v \u2208 T and any \u03b3 such that |\u03b3|Dn,p \u226a n2p2, the probability P[\u03b3\u2217v \u2265 \u03b3] = P[X\u03c3v \u2265 N\u03c3v p+ \u03b3Dn,p] is bounded by\nexp ( \u2212 (\u03b3 \u2228 0) 2n3p2 log n\n2(N\u03c3v p+ (\u03b3 \u2228 0)Dn,p)\n) = exp ( \u2212 (\u03b3 \u2228 0) 2\n\u03b12|v| \u2212 \u03b12|v|\u22121 n log n+ o(n log n)\n) .\nWrite M = \u221a Dk\u22121(1 + \u03b4)2(\u03b1k \u2212 \u03b1k\u22121), and we denote\n\u0393k =   (\u03b3v)|v|\u2264k : \u2211\n|v|=k \u03b3v \u2265 (1 + \u03b4)(\u03b1k \u2212 \u03b1k\u22121)\n\u221a \u03b1k + \u03b1k\u22121, \u03b3v \u2264 M,\u2200|v| \u2264 k    .\nFirstly, based on the previous estimation, we obtain that the probability that X\u03c3v \u2265 N\u03c3v + MDn,p for some v \u2208 T is at most exp ( \u2212M2n log n+o(n logn) ) . Therefore, by considering the two cases that whether there exists v with |v| \u2264 k such that \u03b3\u2217v \u2265 M , we conclude that the probability term can be bounded by exp ( \u2212M2n log n+ o(n log n) ) plus\nexp ( O(log n) ) \u00d7 sup\n(\u03b3v)|v|\u2264k\u2208\u0393k P[\u03b3\u2217v \u2265 \u03b3v,\u2200v s.t. |v| \u2264 k] ,\nwhere the first factor represents the number of possible realizations of \u03b3\u2217 in \u0393k. As a result of independence, the supremum of the probability term given above is bounded by\nsup (\u03b3v)|v|\u2264k\u2208\u0393k exp\n \u2212 k\u2211\ni=1\n\u2211\n|v|=i\n(\u03b3v \u2228 0)2 \u03b12i \u2212 \u03b12i\u22121 n log n+ o(n log n)\n \n\u2264 sup (\u03b3v)|v|\u2264k\u2208\u0393k exp\n( \u2212 k\u2211\ni=1\nDi\u22121 ( \u2211\n|v|=i \u03b3v \u2228 0)2 \u03b12i \u2212 \u03b12i\u22121 n log n+ o(n log n)\n)\n\u2264 sup (\u03b3v)|v|\u2264k\u2208\u0393k exp\n( \u2212Dk\u22121 ( \u2211\n|v|=k \u03b3v \u2228 0)2 \u03b12k \u2212 \u03b12k\u22121 n log n+ o(n log n)\n)\n\u2264 exp ( \u2212Dk\u22121(1 + \u03b4)2(\u03b1k \u2212 \u03b1k\u22121)n log n+ o(n log n) ) .\nHere, the first inequality follows from Cauchy-Schwartz inequality, and the last inequality follows from the definition of \u0393k. With these estimates in hand, we obtain that P[Ek] is\nbounded by\nexp\n( k\u2211\ni=1\nDi\u22121(\u03b1i \u2212 \u03b1i\u22121)n log n+ o(n log n) ) \u00d7\n[ exp ( \u2212M2n log n+ o(n log n) ) + exp ( \u2212Dk\u22121(1 + \u03b4)2(\u03b1k \u2212 \u03b1k\u22121)n log n+ o(n log n )]\n\u2264 2 exp ( [Dk\u22122\u03b1k\u22121 \u2212 2\u03b4Dk\u22121(\u03b1k \u2212 \u03b1k\u22121)]n log n+ o(n log n) ) = exp(\u2212cn log n) ,\nfor some constant c > 0 (which follows from the choice of D in (4.12)), as desired."
        },
        {
            "heading": "4.2.2 Computational hardness of online algorithms",
            "text": "Let A be an arbitrary online algorithm. According to Definition 1.2, we can assume that there exist probability spaces \u21261, . . . ,\u2126n and deterministic functions\nfk : {Gi,j}1\u2264i<j\u2264k \u00d7 {Gi,j}1\u2264i<j\u2264n \u00d7 \u2126k \u2192 [n] ,\nsuch that A operates as follows: for each 1 \u2264 k \u2264 n, assuming that \u03c0\u2217(1), . . . , \u03c0\u2217(k \u2212 1) are determined, A samples \u03c9k \u2208 \u2126k according to some probability measure Pk determined by {Gij}1\u2264i<j\u2264k, {Gij}1\u2264i<j\u2264n and \u03c0\u2217(1), . . . , \u03c0\u2217(k \u2212 1), and sets\n\u03c0\u2217(k) = fk({Gi,j}1\u2264i<j\u2264k, {Gi,j}1\u2264i<j\u2264n, \u03c9k) .\nThe mechanism should also ensure that the final output \u03c0\u2217 is always a permutation in Sn. We fix an arbitrary admissible graph G. For a (T ,\u2212\u2192\u03b1 )-correlated instance of graphs {Gv}v\u2208L as defined in Definition 4.3, we simultaneously run A on the pairs (Gv,G) for all v \u2208 L with outputs {\u03c0\u2217v}v\u2208L. The sampling procedure follows the following rule: for any two vertices u, v \u2208 L, the internal randomness \u03c9k used for determining \u03c0\u2217u(k) and \u03c0\u2217v(k) are sampled identically if k \u2264 \u03b1|u\u2227v|n, while they are independently sampled if k > \u03b1|u\u2227v|n. It is important to note that due to the construction of (T ,\u2212\u2192\u03b1 )-correlated instance and the definition of online algorithms, one can prove by induction that the law Pk of \u03c0 \u2217 u(k) and \u03c0\u2217v(k) are the same as long as 1 \u2264 k \u2264 \u03b1|u\u2227v|n. This demonstrates that the sampling rule mentioned above is self-consistent, and the running procedure is valid.\nWe have the following lower bound for the probability of \u03c0\u2217v \u2208 S\u03b2c+\u03b5(G,G) for all v \u2208 L.\nProposition 4.6. Fix an admissible graph G. Assume that the output \u03c0\u2217 of A satisfies\nP[\u03c0\u2217 \u2208 S\u03b2c+\u03b5(G,G)] = EG\u223cG(n,p)Q [\u03c0\u2217 \u2208 S\u03b2c+\u03b5(G,G)] \u2265 psuc ,\nwhere Q is the internal randomness of A. Then through the aforementioned procedure, we have\nP[\u03c0\u2217v \u2208 S\u03b2c+\u03b5(Gv ,G),\u2200v \u2208 L] \u2265 pLsuc , (4.17)\nProof. Let Sv denote the event that \u03c0 \u2217 v \u2208 S\u03b2c+\u03b5(Gv,G). For i = 0, 1, . . . , N , we define Fi as the \u03c3-field generated by {Gvi,j}1\u2264i<j\u2264\u03b1in,\u2200v \u2208 L and \u03c0\u2217(k), 1 \u2264 k \u2264 \u03b1in. For each v \u2208 T , we use L(v) to denote the offspring of v in L, and S(v) represents the event that \u03c0\u2217u \u2208 S\u03b2c+\u03b5(Gu,G) for each u \u2208 L(v). Note that we are interested in the event that P[S(vo)] for the root vo, and we will show by induction that\nP[S(vo)] \u2265 \u220f\n|v|=i P[S(v)] ,\u22000 \u2264 i \u2264 N . (4.18)\nThe case i = 0 trivially holds. Assuming that (4.18) holds for some 0 \u2264 i \u2264 N \u2212 1. For each v with |v| = i, we denote its D descendants as v1, . . . , vD, then it follows that S(v) = \u22c2Dk=1 S(vk), and the events S(vk), k = 1, 2, . . . ,D are conditionally independent given Fi. By conditioning on Fi and utilizing the iterated expectation formula, we deduce that\nP[S(v)] = E [ D\u220f\nk=1\nP[S(vk) | Fi] ] = E [ P[S(v1) | Fi]D ]\n\u2265 ( E [ P[S(v1) | Fi] ])D = ( P[S(v1)] )D = D\u220f\nk=1\nP[S(vk)] ,\nwhere we employed the symmetry between v1, . . . , vD twice, and the inequality follows directly from Jensen\u2019s inequality. Consequently, by considering the product of all v with |v| = i, we observe that (4.18) holds for i + 1. This establishes the validity of (4.18) for each 0 \u2264 i \u2264 N . In particular, for i = N , we can deduce that\nP[S(vo)] \u2265 \u220f\n|v|=N P[S(v)] = (P[\u03c0\u2217 \u2208 S\u03b2+c(G,G)]\n)L \u2265 pLsuc ,\ncompleting the proof.\nNote that if \u03c0\u2217v \u2208 S\u03b2c+\u03b5(Gv ,G) for each v \u2208 L, then (\u03c0\u2217v)v\u2208L constitutes the forbidden structure. By combining this observation with Proposition 4.5, we conclude that whenever G is admissible, the following holds for any A \u2208 OGAA:\nP[A(G,G) \u2208 S\u03b2c+\u03b5(G,G)] \u2264 exp(\u2212cn log n/L) = exp(\u2212c0n log n) .\nSo, by the Markov inequality, we see\nP [ G : Q[A(G,G) \u2208 S\u03b2c+\u03b5(G,G) \u2265 exp(\u2212c0n log n/2)] ] \u2264 exp(\u2212c0n log n/2) .\nNote that the above statement holds for any admissible graph G and any online algorithm A \u2208 OGAA. Since G \u223c G(n, p) is admissible with high probability by Lemma 2.2, the proof of the second item in Theorem 1.4 is now completed."
        },
        {
            "heading": "A Tail estimates",
            "text": "In this section, we provide various tail estimates for binomial variables that are useful for the proof. These estimates are categorized into two types based on the range of the parameter p, and the two regimes correspond to Poisson approximation and normal approximation, respectively. For simplicity, we denote the binomial distribution with parameters N and p as B(N, p), which also represents a random variable following this distribution. Lemma A.1. When p = n\u22121/2+o(1) and p \u226a pc, write M0 = log n/ log ( log n/np2 ) , then for any integer N \u224d np and any constant \u03b1 > 0,\nP [ B(N, p) \u2265 \u03b1M0 ] = n\u2212\u03b1+o(1).\nProof. For the upper bound, by applying (2.1), we have the deviation probability is bounded by\nexp ( \u2212Np [ \u03b1M0 Np log ( \u03b1M0 Np ) \u2212 \u03b1M0 Np + 1 ]) .\nNow by the assumption we have M0 log (\u03b1M0/Np) = log n + o(log n) and M0 = o(log n), thus the expression above reduces to exp ( \u2212 \u03b1 log n+ o(log n) ) = n\u2212\u03b1+o(1), as desired.\nFor the lower bound, it suffices to derive a one-point estimation. Write k = \u2308\u03b1M0\u2309 for short. Note that by our assumption M0 \u226b 1 so k = ( \u03b1+ o(1) ) M0. As a result, we have\nP [B(N, p) = k] =\n( N\nk\n) pk(1\u2212 p)N\u2212k \u2265 (Np) k\nk! exp\n( \u2212k 2\nN\n) (1\u2212 p)N\n= ( 1 + o(1) ) (Np)k k! exp(\u2212Np) = ( 1 + o(1) ) exp ( k log(Np)\u2212 k log k \u2212 k + log( \u221a 2\u03c0k)\u2212Np )\n= ( 1 + o(1) ) exp ( \u2212k log ( k\nNp\n) + o(log n) )\n= ( 1 + o(1) ) exp ( \u2212 \u03b1 log n+ o(log n) ) = n\u2212\u03b1+o(1) ,\nwhich completes the proof.\nLemma A.2. When p lies in the dense regime, for any integer N = \u2126(np) and any constant \u03b1 > 0, it holds that\nP [ B(N, p) \u2265 Np+ \u221a 2\u03b1Np log n ] = n\u2212\u03b1+o(1) .\nProof. Under the given assumptions, we have Np \u226b \u221aNp log n, which allows us to obtain the upper bound using (2.3). On the other hand, we recall a well-known result presented in\n[51, Theorem 2.1]. This theorem states that for a binomial variable B(N, p) with p \u2264 1/4 and any k with Np \u2264 k \u2264 N(1\u2212 p), it always holds that\nP[B(N, p) \u2265 k] \u2265 1\u2212 \u03a6 (\nk \u2212Np\u221a Np(1\u2212 p)\n) ,\nwhere \u03a6 is the distribution function of a standard normal variable. Since p = o(1) based on our assumption, the lower bound can be derived from this result and classic estimations for normal tails readily."
        },
        {
            "heading": "B Complimentary proof",
            "text": "B.1 Proof of Lemma 2.2\nProof. It suffices to prove that all the items are true with probability 1\u2212 o(1/n2). For the first one, we have |E(G)| \u223c B( (n 2 ) , p) and thus by (2.3) we get\nP [\u2223\u2223\u2223|E(G)| \u2212 ( n\n2\n) p \u2223\u2223\u2223 \u2265 2 \u221a n2p log n ] \u2264 2 exp ( \u2212 4n 2p log n\nn2p+ 4 \u221a n2p log n\n) = o(1/n2) .\nTo establish the second item, which requires (2.6) to be true for every graph induced subgraph H, we observe that for each induced subgraph H with k vertices, |E(H)| \u223c B( (k 2 ) , p). By applying (2.3) again we obtain that for each induced subgraph H,\nP [\u2223\u2223\u2223|E(H)| \u2212 ( k\n2\n) p \u2223\u2223\u2223 \u2265 n 2p\nlog n1/4\n] \u2264 2 exp ( \u2212 n 4p2/ log n1/2\nk2p \u2228 (2n2p/ log n1/4)\n) \u2264 exp ( \u2212 n 3/2\nlog n1/4\n) ,\nwhere the last inequality follows from p \u226b pc = \u221a\nlog n/n. Therefore, the result follows by applying a union bound since there are 2n = exp(O(n)) induced subgraphs H in total.\nFor the third item, we recall that U represents the set of unordered pairs. Given a fixed permutation \u03c0 \u2208 Sn, we define the mapping \u03a0 : U \u2192 U as \u03a0(i, j) = (\u03c0(i), \u03c0(j)). It is evident that \u03a0 is a bijection on U. Now consider a pair (i, j) \u2208 U and let Xi,j = Gi,jG\u03c0(i),\u03c0(j). We will make use of the following observations: for each (i, j) \u2208 U,\n(i) Xi,j \u223c B(1, p) if \u03a0(i, j) = (i, j), and Xi,j \u223c B(1, p2) otherwise.\n(ii) Xi,j is independent with the variables Xk,l, (k, l) \u2208 U \\{\u03a0(i, j),\u03a0\u22121(i, j)}.\nMotivated by these observations, we can partition U into the disjoint union of four sets: F\u2294A\u2294B\u2294C. Here, F represents the set of fixed points of \u03a0, while A, B, and C are chosen in such a way that no pair (i, j) \u2208 U exists where both (i, j) and \u03a0(i, j) belong to the same set. The existence of such partitioning can be proven by partitioning each orbit in\nthe cycle decomposition of \u03a0 into three parts, ensuring that no two edges within the same part are connected.\nDenote \u03c3(\u22c6) = \u2211\n(i,j)\u2208\u22c6 Xi,j, \u2200\u22c6 \u2208 {F,A,B,C} ,\nthen OL(G,\u03c0) = \u2211\n\u22c6\u2208{F,A,B,C} \u03c3(\u22c6). Additionally, by combining the previous observations with the choices we made for F, A, B, and C, we obtain the following result:\n\u03c3(F) \u223c B(|F |, p), and \u03c3(\u22c6) \u223c B(| \u22c6 |, p2),\u2200 \u22c6 \u2208 {A,B,C} .\nAs a result, the probability that |OL(G,\u03c0)| deviates from its expectation by at least 2 \u221a |F |np log n+ 3 \u221a 2n3p2 log n is bounded by\nP [ |\u03c3(F)\u2212 E\u03c3(F)| \u2265 2 \u221a |F |np log n ] +\n\u2211\n\u22c6\u2208{A,B,C} P\n[ |\u03c3(\u22c6) \u2212 E\u03c3(\u22c6)| \u2265 \u221a 2n3p2 log n ]\n(2.3) \u2264 exp ( \u2212 4|F |np log n 2|F |p+ 4 \u221a |F |np log n ) +\n\u2211\n\u22c6\u2208{A,B,C} exp\n( \u2212 2n 3p2 log n\n2| \u22c6 |p2 + 2 \u221a 2n3p2 log n\n) .\nNote that it trivially holds | \u22c6 | \u2264 n2/2 for all \u22c6 \u2208 A,B,C. Consequently, the expression mentioned above is much smaller than exp(\u2212n log n). As a result, (2.7) holds for all permutation \u03c0 \u2208 Sn with probability of 1\u2212 o(1/n2), as indicated by a union bound.\nAdditionally, since E|OL(G,\u03c0)| = |F |(p\u2212 p2)+ ( n 2 ) p2 \u2265 (|F |p\u2228n2p2)/3, and it can be\neasily checked that under the assumption p \u226b pc, \u221a n3p2 log n \u226a n2p2 and \u221a\n|F |np log n \u226a |F |p\u2228 n2p2. Therefore, (2.7) implies |OL(G,\u03c0)|/E|OL(G,\u03c0)| = 1+ o(1), which concludes the final statement in the third item. The proof is completed.\nB.2 Proof of Proposition 2.3\nHere in this subsection we provide a complete proof of Proposition 2.3. Let us begin by recalling some definitions and notations concerning to Talagrand\u2019s concentration inequality. Definition B.1. Let \u2126 = \u220fN\ni=1 \u2126i be a product space. For a function h : \u2126 \u2192 R, we call h to be Lipschitz, if |h(x) \u2212 h(y)| \u2264 1 whenever x, y differ in at most one coordinate. Furthermore, let f : R\u22650 \u2192 R\u22650 be an increasing function, we say that h is f -certifiable if, whenever h(x) \u2265 s, there exists I \u2282 {1, ..., (n 2 ) } with |I| \u2264 f(s) so that all y \u2208 \u2126 that agree with x on the coordinates in I have h(y) \u2265 s. A special case of Talagrand\u2019s concentration inequality can be stated as follow.\nProposition B.2. For any product probability measure P on \u2126 and any Lipschitz function h that is f -certifiable, let X = h(\u03c9), we have for any b, t > 0:\nP[X \u2264 b\u2212 t \u221a f(b)] \u00b7 P [X \u2265 b] \u2264 e\u2212t2/4 .\nThe proof of this powerful inequality can be found in, e.g. [1, Section 7.7]. Now we are ready to give the proof of Proposition 2.3. Recall the definition of admissibility in Definition 2.1.\nProof of Proposition 2.3. Fix an admissible graph G. For our purposes, we consider \u2126 = {0, 1}(n2) and P be the product measure of ( n 2 ) Bernoulli variables with parameter p. Denote\nX = h(G) , max \u03c0\u2208Sn O(\u03c0) ,\nwhere we view G as a vector in {0, 1}(n2). It is trivial to verify that h is Lipschitz. Furthermore, we claim that the function h defined above is f -certifiable, where f(s) = \u2308s\u2309. To see this, suppose h(x) \u2265 s for some x = (Gi,j)(i,j)\u2208U \u2208 \u2126. Then, there exists a permutation \u03c0\u2217 and \u2308s\u2309 unordered pairs (ik, jk) \u2208 U, 1 \u2264 k \u2264 \u2308s\u2309, such that Gik ,jk = 1 and G\u03c0\u2217(ik),\u03c0\u2217(jk) = 1 for each k. We can select I as the set (\u03c0\n\u2217(ik), \u03c0\u2217(jk)), 1 \u2264 k \u2264 \u2308s\u2309, which contains \u2308s\u2309 edges. For any G\u2032 that includes all the edges in I, it holds that h(G\u2032) \u2265 O(\u03c0\u2217) \u2265 s.\nAs a result, Proposition B.2 yields that for any b, t \u2265 0, P[X \u2264 b\u2212 t \u221a \u2308b\u2309] \u00b7 P[X \u2265 b] \u2264 e\u2212t2/4 . (B.1)\nTaking b = Median(X) in (B.1) and by an easy transformation, we get for any t \u2265 0,\nP [X \u2212Median(X) \u2264 \u2212t] \u2264 2 exp ( \u2212 t 2\n4\u2308Median(X)\u2309\n) .\nIn addition, for any r \u2265 0, taking b = Median(X) + r, t = r/ \u221a b in (B.1) yields:\nP [X \u2212Median(X) \u2265 r] \u2264 2 exp ( \u2212 r 2\n4(\u2308Median(X) + r\u2309)\n)\n\u2264 2 exp ( \u2212 r 2\n8\u2308Median(X)\u2309\n) + 2exp ( \u2212 r 2\n8\u2308r\u2309\n) .\nFrom (2.5) in admissibility and the proof of the upper bounds in Section 2.1, we can deduce that for any admissible graph G, the quantity Median(X) is asymptotically given by EO(\u03c0) \u223c n2p2/2. In particular, we have \u2308Median(X)\u2309 \u2264 n2p2. Now, we claim that |EX \u2212Median(X)| = o(Dn,p). To see this, we can use the aforementioned tail estimates, which imply that\n|EX \u2212Median(X)| \u2264 E[|X \u2212Median(X)|]\n=\n\u222b +\u221e\n0 P (|X \u2212Median(X)| \u2265 t) dt\n\u2264 \u222b +\u221e\n0\n[ 2 exp ( \u2212 t 2\n4n2p2\n) + 2exp ( \u2212 t 2\n8n2p2\n) + 2exp ( \u2212 t 2\n8\u2308t\u2309\n)] dt\n= O(np) = o(Dn,p) .\n(B.2)\nAs a result, we see for n large enough,\nP [|X \u2212 EX| \u2265 \u03b5Dn,p] \u2264 P [|X \u2212Median(X)| \u2265 \u03b5Dn,p/2]\n\u2264 2 exp ( \u2212 \u03b5\n2D2n,p 16n2p2\n) + 2exp ( \u2212 \u03b5\n2D2n,p 32n2p2)\n) + 2exp ( \u2212\u03b5Dn,p\n16\n)\n= exp ( \u2212 \u2126(n log n) ) .\nNote that X \u2212 EX = max\u03c0\u2208Sn O\u0303(\u03c0)\u2212 Emax\u03c0\u2208Sn O\u0303(\u03c0), the desired result follows.\nB.3 Proof of Lemma 3.8\nProof. Recall that\nIs = [ |Ns |p+ \u221a 2(1\u2212 \u03b7)|Ns |p log n, |Ns |p+ \u221a 10|Ns |p log n ] .\nWe denote p\u2217 = \u221a 100p log n/|Ns | \u2264 \u221a\n100/\u03b7 \u00b7 pc \u226a p, and P\u2217 as the product measure of |Ns | independent Bernoulli variables B(1, p + p\u2217) on \u21261. For any x \u2208 Is, we claim that there exists a coupling {(\u03c9, \u03c9\u2032) : \u03c9 \u223c Ps,x, \u03c9\u2032 \u223c P\u2217} such that with probability 1\u2212 o(1/n), it holds \u03c9j \u2264 \u03c9\u2032j for all j \u2208 Ns.\nTo prove the existence of such a coupling, we start with the following observation: for \u03c9 = (gj , . . . )j\u2208Ns sampled from Ps,x (resp. P \u2217), given that \u2211\nj\u2208Ns gj = K, the conditional distribution of \u03c9 can be characterized by uniformly sampling a subset A of Ns with |A| = K and setting gj = 1j\u2208A. It is easy to see that for anyK1 \u2264 K2 \u2264 |Ns |, there exists a coupling between two uniformly sampled subsets A1, A2 \u2282 Ns with |Ai| = Ki for i = 1, 2, such that A1 \u2282 A2 holds with probability one.\nBased on these observations, it suffices to show the existence of a coupling (\u03c9, \u03c9\u2032) between Ps,x and P \u2217 such that with probability 1 \u2212 o(1/n), \u2211j\u2208Ns \u03c9j \u2264 \u2211 j\u2208Ns \u03c9 \u2032 j. This\ncan be verified as follows: with y = |Ns |p+ \u221a 16|Ns |p log n, it holds\nPs,x[Er,s \u2265 y] \u2264 P[Er,s \u2265 y] P[Er,s \u2265 x] \u2264 exp(\u2212(8 + o(1)) log n) exp(\u2212(5 + o(1)) log n = o(1/n) ,\nand (since EP\u2217[Er,s] \u2265 |Ns |p+ 10 \u221a |Ns |p log n \u2265 y + 6 \u221a |Ns |p log n)\nP\u2217[Er,s \u2264 y] \u2264 exp(\u2212(18 + o(1) log n) = o(1/n) .\nHence, the desired coupling exists. Note that the event {\u03c91 /\u2208 \u2126\u2217s} is an increasing event. Therefore, with the aforementioned coupling, it suffices to show that P\u2217[\u03c91 /\u2208 \u2126\u2217s] = o(1/n). It is clear that under P\u2217,\u2211 j\u2208Nk \u2229Ns gj \u223c B(|Nk \u2229Ns |, p+ p\n\u2217). Recall that under G0, we have |Nk \u2229Ns | \u2264 2np2 for all a\u03b7 < k < s. The remaining proof is just simple applications of binomial tail estimates and then taking union bounds, which we omit the detailed calculations.\nB.4 Proof of Lemma 3.9\nProof. Since by definition,\nF \u2217k (Ok) = P [ \u2211\nj\u2208Nk G\u2217r\u2217,\u03c0\u2217(j) \u2264 Ok\n] \u2264 P [ \u2211\nj\u2208Nk Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 1\n] ,\nit suffices to show\nP\n[ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 Tk \u2212 1\n] \u2265 P [ \u2211\nj\u2208Nk Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 1\n] \u2212 2\u03b4(n)(ck + 1)\nn\u2212 k + 1 .\nIt is clear that \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j)\ndef = Sk,s \u223c B(|Nk \\Ns |, p) ,\n\u2211\nj\u2208Nk Gr\u2217,\u03c0\u2217(j)\ndef = Pk \u223c B(|Nk |, p) .\nWe introduce an auxiliary random variable Y \u223c B(|Nk \u2229Ns |, p) which is independent of everything before, then\nP\n[ \u2211\nj\u2208Nk \\Ns Gr\u2217,\u03c0\u2217(j) \u2264 Ok \u2212 Tk \u2212 1\n] = P [Sk,s + Y \u2264 Ok \u2212 Tk \u2212 1 + Y ] . (B.3)\nThe analysis of (B.3) varies in details for p lies in different regimes. We take Sk to be\nSk =\n{ |Nk \u2229Ns |p\u2212 \u221a 10np3 log n , np3 \u2265 (log n)3 and p \u226a 1/(log n)4 ,\n0 , np3 \u226a (log n)3 and p \u226b pc . (B.4)\nFrom (2.2) and the fact that |Nk \u2229Ns | \u2264 2np2 we see P[Y \u2265 Sk] = 1 \u2212 o(1/n2). Noting that Sk,s + Y \u223c B(|Nk |, p), thus the left hand side of (B.3) is lower bounded by\nP[B(|Nk |, p) \u2264 Ok \u2212 Tk \u2212 1 + Sk]\u2212 o(1/n2) . (B.5)\nRecall that \u03b4(n) = 2/ \u221a log n, in order to prove the claim, it suffices to show that\nP [Ok \u2212 Tk \u2212 1 + Sk \u2264 B(|Nk |, p) \u2264 Ok \u2212 2] P [B(|Nk |, p) \u2265 Ok \u2212 1] \u2264 \u03b4(n) , (B.6)\nsince this together with (B.5) would imply that\nP[Sk,s \u2264 Ok \u2212 Tk \u2212 1]\u2212 P[Pk \u2264 Ok \u2212 1] \u2265\u2212 P[Ok \u2212 Tk \u2212 1 + Sk \u2264 B(|Nk |, p) \u2264 Ok \u2212 2]\u2212 o(1/n2) \u2265\u2212 \u03b4(n)P[B(|Nk |, p) \u2265 Ok \u2212 1]\u2212 o(1/n2)\n\u2265\u2212 2\u03b4(n)(ck + 1) n\u2212 k + 1 ,\nleading to the desired result. Here the last inequality exploits the fact that from the assumption in Gs\u22121, it holds\nP[Pk \u2265 Ok \u2212 1] \u2265 P[Er,k \u2265 Ok] \u2265 ( 1 + o(1) ) ck\nn\u2212 k + 1 .\nTo prove (B.6), write Ok = |Nk |p + Mk, we compute the binomial one-point density for each t = |Nk |p+Mk + rk for |rk| \u2264 Rk, where\nRk =    \u221a np2.5 log n , np3 \u2265 (log n)3 and p \u226a 1/(log n)4 ,\n(log n)4 , n\u22120.1 \u2264 np3 \u2264 (log n)3 ,\u221a log n , p \u226b pc and np3 \u2264 n\u22120.1 .\nNote that Gs\u22121 implies \u221a 2(1 \u2212 \u03b7)|Nk |p log n \u2264 Mk \u2264 \u221a\n10np2 log n. Write N = |Nk |, by Stirling\u2019s formula, we have P[B(N, p) = t] = (N t ) pt(1\u2212 p)N\u2212t equals to 1 + o(1) times\n\u221a 2\u03c0NNNe\u2212N\u221a\n2\u03c0ttte\u2212t \u221a 2\u03c0(N \u2212 t)(N \u2212 t)N\u2212te\u2212(N\u2212t) pt(1\u2212 p)N\u2212t\n= 1\u221a 2\u03c0 exp\n{ N logN \u2212 t log t\u2212 (N \u2212 t) log(N \u2212 t)\u2212 1\n2 log t\n} pt(1\u2212 p)N\u2212t\n= 1 + o(1)\u221a 2\u03c0Np exp {N logN \u2212 t log t\u2212 (N \u2212 t) log(N \u2212 t)} pt(1\u2212 p)N\u2212t\n= 1 + o(1)\u221a 2\u03c0Np exp\n{ \u2212t log ( t\nNp\n) \u2212 (N \u2212 t) log ( N \u2212 t\nN(1\u2212 p)\n)}\n= 1 + o(1)\u221a 2\u03c0Np exp\n{ \u2212 (Np+Mk) log ( 1 +\nMk + rk Np\n) \u2212 (N(1\u2212 p)\u2212Mk) log ( 1\u2212 Mk + rk\nN(1\u2212 p)\n)}\n= 1 + o(1)\u221a 2\u03c0Np exp\n{ \u2212 (Np+Mk) log ( 1 +\nMk Np\n) \u2212 (N(1 \u2212 p)\u2212Mk) log ( 1\u2212 Mk\nN(1\u2212 p)\n)} ,\nwhere the approximation in the last line is uniform for rk with |rk| \u2264 Rk. Let\nE(k, p) = (Np+Mk) log ( 1 +\nMk Np\n) + (N(1\u2212 p)\u2212Mk) log ( 1\u2212 Mk\nN(1\u2212 p)\n) ,\nwith the aforementioned one-point estimates, we can conclude by only considering the first Rk terms in P[B(N, p) \u2265 Ok \u2212 1] = \u2211\u221e t=\u22121 P[B(N, p) = Ok + t] that the denominator in (B.6) is lower bounded by 1 + o(1) times\nRk\u221a 2\u03c0Np exp (\u2212E(k, p)) .\nIn addition, from the definition of Tk, Sk in (3.17) and (B.4), we see that for any t \u2208 [Ok \u2212 Tk\u22121+Sk, Ok\u22122], it holds |t\u2212Ok| \u2264 Rk. Therefore, the one-point estimate above can be\napplied to each term in P[Ok\u2212Tk\u22121+Sk \u2264 B(N, p) \u2264 Ok\u22122] = \u2211\u22122\nt=\u2212Tk\u22121+Sk P[B(N, p) = Ok + t], which yields that the numerator in (B.6) is upper bounded by 1 + o(1) times\nTk \u2212 Sk\u221a 2\u03c0Np exp (\u2212E(k, p)) .\nTherefore, uniformly in the three regimes (where we essentially make use of the assumption that p \u2264 1/(log n)4), the upper bound of (B.6) is given by:\n( 1 + o(1) ) \u00b7 Tk \u2212 Sk\nRk \u2264 1 + o(1)\u221a log n \u2264 \u03b4(n) ,\ncompleting the proof."
        }
    ],
    "title": "The Algorithmic Phase Transition of Random Graph Alignment Problem",
    "year": 2023
}