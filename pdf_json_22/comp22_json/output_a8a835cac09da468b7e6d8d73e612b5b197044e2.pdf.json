{
    "abstractText": "Pairwise re-rankingmodels predict which of two documents is more relevant to a query and then aggregate a final ranking from such preferences. This is often more effective than pointwise re-ranking models that directly predict a relevance value for each document. However, the high inference overhead of pairwise models limits their practical application: usually, for a set of k documents to be re-ranked, preferences for all k2\u2212k comparison pairs excluding selfcomparisons are aggregated. We investigate whether the efficiency of pairwise re-ranking can be improved by sampling from all pairs. In an exploratory study, we evaluate three sampling methods and five preference aggregation methods. The best combination allows for an order of magnitude fewer comparisons at an acceptable loss of retrieval effectiveness, while competitive effectiveness is already achieved with about one third of the comparisons.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lukas Gienapp"
        },
        {
            "affiliations": [],
            "name": "Maik Fr\u00f6be"
        },
        {
            "affiliations": [],
            "name": "Matthias Hagen"
        },
        {
            "affiliations": [],
            "name": "Martin Potthast"
        }
    ],
    "id": "SP:af070d44272fd9191cbf0ce2ff048f3651dd1e07",
    "references": [
        {
            "authors": [
                "Alan Agresti",
                "Maria Kateri"
            ],
            "title": "Categorical Data Analysis",
            "venue": "In International Encyclopedia of Statistical Science",
            "year": 2011
        },
        {
            "authors": [
                "Nir Ailon",
                "Moses Charikar",
                "Alantha Newman"
            ],
            "title": "Aggregating Onconsistent Information: Ranking and Clustering",
            "venue": "J. ACM 55,",
            "year": 2008
        },
        {
            "authors": [
                "Juan A. Aledo",
                "Jos\u00e9 A. G\u00e1mez",
                "Alejandro Rosete"
            ],
            "title": "A Highly Scalable Algorithm for Weak Rankings Aggregation",
            "venue": "Inf. Sci",
            "year": 2021
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry"
            ],
            "title": "Rank Analysis of Incomplete Block Designs: The Method of Paired Comparisons",
            "venue": "Biometrika 39,",
            "year": 1952
        },
        {
            "authors": [
                "St\u00e9phan Cl\u00e9men\u00e7on",
                "G\u00e1bor Lugosi",
                "Nicolas Vayatis"
            ],
            "title": "Ranking and Empirical Minimization of U-Statistics",
            "venue": "The Annals of Statistics 36,",
            "year": 2008
        },
        {
            "authors": [
                "William W. Cohen",
                "Robert E. Schapire",
                "Yoram Singer"
            ],
            "title": "Learning to Order Things",
            "venue": "J. Artif. Intell. Res",
            "year": 1999
        },
        {
            "authors": [
                "Nick Craswell",
                "Bhaskar Mitra",
                "Emine Yilmaz",
                "Daniel Campos"
            ],
            "title": "Overview of the TREC 2020",
            "venue": "Deep Learning Track. CoRR",
            "year": 2021
        },
        {
            "authors": [
                "Nick Craswell",
                "Bhaskar Mitra",
                "Emine Yilmaz",
                "Daniel Campos",
                "Ellen M. Voorhees"
            ],
            "title": "Overview of the TREC 2019 Deep Learning Track",
            "venue": "CoRR abs/2003.07820",
            "year": 2020
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Jamie Callan"
            ],
            "title": "Context-Aware Document Term Weighting for Ad-Hoc Search",
            "venue": "In Proc. of WWW 2020. ACM /",
            "year": 2020
        },
        {
            "authors": [
                "Roger Fletcher"
            ],
            "title": "Practical Methods of Optimization (2 ed.)",
            "year": 1987
        },
        {
            "authors": [
                "Norbert Fuhr"
            ],
            "title": "Optimum Polynomial Retrieval Functions Based on the Probability Ranking Principle",
            "venue": "ACM Trans. Inf. Syst. 7,",
            "year": 1989
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Jamie Callan"
            ],
            "title": "Understanding BERT Rankers Under Distillation",
            "venue": "In Proc. of ICTIR 2020",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Hofst\u00e4tter",
                "Sophia Althammer",
                "Michael Schr\u00f6der",
                "Mete Sertkan",
                "Allan Hanbury"
            ],
            "title": "Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Hofst\u00e4tter",
                "Allan Hanbury"
            ],
            "title": "Let\u2019s Measure Run Time! Extending the IR Replicability Infrastructure to Include Performance Aspects",
            "venue": "In Proc. of OSIRRC@SIGIR 2019 (CEUR,",
            "year": 2019
        },
        {
            "authors": [
                "Kalervo J\u00e4rvelin",
                "Jaana Kek\u00e4l\u00e4inen"
            ],
            "title": "Cumulated Gain-Based Evaluation of IR Techniques",
            "venue": "ACM Trans. Inf. Syst. 20,",
            "year": 2002
        },
        {
            "authors": [
                "Marcin Kaszkiel",
                "Justin Zobel"
            ],
            "title": "Passage Retrieval Revisited",
            "venue": "In Proc. of SIGIR",
            "year": 1997
        },
        {
            "authors": [
                "Yanyan Lan",
                "Jiafeng Guo",
                "Xueqi Cheng",
                "Tie-Yan Liu"
            ],
            "title": "Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space",
            "venue": "In Proc. of NeurIPS",
            "year": 2012
        },
        {
            "authors": [
                "Jimmy Lin"
            ],
            "title": "The Neural Hype, Justified! A Recantation",
            "venue": "SIGIR Forum 53,",
            "year": 2019
        },
        {
            "authors": [
                "Jimmy Lin",
                "Rodrigo Nogueira",
                "Andrew Yates"
            ],
            "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
            "venue": "Synthesis Lectures on Human Language Technologies 14,",
            "year": 2021
        },
        {
            "authors": [
                "Tie-Yan Liu"
            ],
            "title": "Learning to Rank for Information Retrieval",
            "year": 2011
        },
        {
            "authors": [
                "Craig Macdonald",
                "Nicola Tonellotto"
            ],
            "title": "Declarative Experimentation in Information Retrieval using PyTerrier",
            "venue": "In Proc. of ICTIR 2020",
            "year": 2020
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau"
            ],
            "title": "TextRank: Bringing Order into Text",
            "venue": "In Proc. of EMNLP",
            "year": 2004
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng"
            ],
            "title": "2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
            "year": 2016
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Zhiying Jiang",
                "Ronak Pradeep",
                "Jimmy Lin"
            ],
            "title": "Document Ranking with a Pretrained Sequence-to-Sequence Model",
            "venue": "In Proc. of Findings EMNLP 2020",
            "year": 2020
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Wei Yang",
                "Kyunghyun Cho",
                "Jimmy Lin"
            ],
            "title": "Multi-Stage Document Ranking with BERT",
            "venue": "CoRR abs/1910.14424",
            "year": 2019
        },
        {
            "authors": [
                "Lawrence Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd"
            ],
            "title": "The PageRank Citation Ranking: Bringing Order to the Web",
            "year": 1999
        },
        {
            "authors": [
                "Ronak Pradeep",
                "Rodrigo Nogueira",
                "Jimmy Lin"
            ],
            "title": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models",
            "venue": "CoRR abs/2101.05667",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Qin",
                "Le Yan",
                "Honglei Zhuang",
                "Yi Tay",
                "Rama Kumar Pasumarthi",
                "Xuanhui Wang",
                "Michael Bendersky",
                "Marc Najork"
            ],
            "title": "Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees",
            "venue": "In Proc. ICLR",
            "year": 2021
        },
        {
            "authors": [
                "Leonardo Rigutini",
                "Tiziano Papini",
                "Marco Maggini",
                "Franco Scarselli"
            ],
            "title": "SortNet: Learning to Rank by a Neural Preference Function",
            "venue": "IEEE Trans. Neural Networks 22,",
            "year": 2011
        },
        {
            "authors": [
                "Tetsuya Sakai"
            ],
            "title": "Alternatives to Bpref",
            "venue": "In Proc. of SIGIR",
            "year": 2007
        },
        {
            "authors": [
                "Hal Stern"
            ],
            "title": "Are All Linear Paired Comparison Models Empirically Equivalent",
            "venue": "Mathematical Social Sciences 23,",
            "year": 1992
        },
        {
            "authors": [
                "Hongyin Tang",
                "Xingwu Sun",
                "Beihong Jin",
                "Jingang Wang",
                "Fuzheng Zhang",
                "Wei Wu"
            ],
            "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval",
            "venue": "In Proc. of ACL 2021",
            "year": 2021
        },
        {
            "authors": [
                "Louis Thurstone"
            ],
            "title": "The Method of Paired Comparisons for Social Values",
            "venue": "The Journal of Abnormal and Social Psychology 21,",
            "year": 1927
        },
        {
            "authors": [
                "Michael V\u00f6lske",
                "Alexander Bondarenko",
                "Maik Fr\u00f6be",
                "Benno Stein",
                "Jaspreet Singh",
                "Matthias Hagen",
                "Avishek Anand"
            ],
            "title": "Towards Axiomatic Explanations for Neural Ranking Models",
            "venue": "In Proc. of ICTIR 2021",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wu",
                "Tao Jin",
                "Hao Lou",
                "Pan Xu",
                "Farzad Farnoud",
                "Quanquan Gu"
            ],
            "title": "Adaptive Sampling for Heterogeneous Rank Aggregation from Noisy Pairwise Comparisons",
            "year": 2021
        },
        {
            "authors": [
                "Yu Xiao",
                "Hongzhong Deng",
                "Xin Lu",
                "Jun Wu"
            ],
            "title": "Graph-Based Rank Aggregation Method for High-Dimensional and Partial Rankings",
            "venue": "J. Oper. Res. Soc. 72,",
            "year": 2021
        },
        {
            "authors": [
                "Ji Xin",
                "Rodrigo Nogueira",
                "Yaoliang Yu",
                "Jimmy Lin"
            ],
            "title": "Early Exiting BERT for Efficient Document Ranking",
            "venue": "In Proc. of SustaiNLP",
            "year": 2020
        },
        {
            "authors": [
                "Yue Zhang",
                "ChengCheng Hu",
                "Yuqi Liu",
                "Hui Fang",
                "Jimmy Lin"
            ],
            "title": "Learning to Rank in the Age of Muppets: Effectiveness\u2013Efficiency Tradeoffs in Multi-Stage Ranking",
            "venue": "In Proc. of SustaiNLP",
            "year": 2021
        },
        {
            "authors": [
                "Ke Zhou",
                "Gui-Rong Xue",
                "Hongyuan Zha",
                "Yong Yu"
            ],
            "title": "Learning To Rank With Ties",
            "venue": "In Proc. of SIGIR 2008. ACM,",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Information systems \u2192 Learning to rank; Rank aggregation; Retrieval effectiveness; Retrieval efficiency.\nKEYWORDS Pairwise re-ranking; Sampling; Efficiency; Pre-trained transformers ACM Reference Format: Lukas Gienapp, Maik Fr\u00f6be, Matthias Hagen, and Martin Potthast. 2022. Sparse Pairwise Re-ranking with Pre-trained Transformers. In Proceedings of the 2022 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR \u201922), July 11\u201312, 2022, Madrid, Spain. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3539813.3545140"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Pre-trained transformers have ushered in a new era in information retrieval: with a sufficient amount of training data, transformerbased re-ranking models can significantly outperform traditional retrieval models [24]. Two classes of re-rankers are implemented using pre-trained transformers [25]: (1) pointwise re-rankers that predict the relevance of a document \ud835\udc51 to a query \ud835\udc5e, and (2) pairwise re-rankers that predict which of two documents (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) is more relevant to \ud835\udc5e. To further maximize re-ranking effectiveness, the monoduo design pattern [34] shown in Figure 1 applies both sequentially. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICTIR \u201922, July 11\u201312, 2022, Madrid, Spain \u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9412-3/22/07. . . $15.00 https://doi.org/10.1145/3539813.3545140\nGiven a query \ud835\udc5e, a document set \ud835\udc37 , and a ranking of \ud835\udc37 produced by a traditional retrieval model like BM25, the top-\ud835\udc58 \u2032 documents\ud835\udc37\ud835\udc58\u2032 = {\ud835\udc511, . . . , \ud835\udc51\ud835\udc58\u2032} \u2282 \ud835\udc37 are re-ranked according to their pointwise relevance to \ud835\udc5e. Then, the top-\ud835\udc58 documents \ud835\udc37\ud835\udc58 \u2282 \ud835\udc37\ud835\udc58\u2032 , \ud835\udc58 \u226a \ud835\udc58 \u2032, are re-ranked based on pairwise comparisons in three steps. First, pairs of documents (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) are sampled, where \ud835\udc56, \ud835\udc57 \u2208 [1, \ud835\udc58] and \ud835\udc56 \u2260 \ud835\udc57 . Second, each pair (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) is passed to a transformer model to predict a probability \ud835\udc5d\ud835\udc56 \ud835\udc57 indicating whether the document \ud835\udc51\ud835\udc56 (\ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5) or the document \ud835\udc51 \ud835\udc57 (\ud835\udc5d\ud835\udc56 \ud835\udc57 < 0.5) is more relevant to \ud835\udc5e. Third, for each document \ud835\udc51\ud835\udc56 , a relevance score \ud835\udc60\ud835\udc56 is aggregated from all probabilities of comparisons including \ud835\udc51\ud835\udc56 , and these scores are then used to derive the final pairwise re-ranking.\nEmpirical evidence suggests that pairwise re-rankers are more effective than pointwise re-rankers since their relevance scores take the relative relevance differences between documents into account, rather than making independent relevance predictions [34]. To maximize the potential effectiveness gains, previous work has relied on exhaustive comparisons of all \ud835\udc582 \u2212 \ud835\udc58 pairs of the top-\ud835\udc58 documents \ud835\udc37\ud835\udc58 to be re-ranked. Given the high run time overhead of transformer inferences, this quadratic step led to the recommendation that the re-ranking depth should be limited to \ud835\udc58 \u2264 50.\nHowever, many of the estimated comparison probabilities may be redundant in that they can be predicted from those of other comparisons. A theoretical lower bound on the run time complexity is \ud835\udc42 (\ud835\udc58 log\ud835\udc58) using a suitable sorting algorithm if the estimated\nar X\niv :2\n20 7.\n04 47\n0v 1\n[ cs\n.I R\n] 1\n0 Ju\nl 2 02\n2\ncomparisons were \u201cconsistent\u201d (i.e., \ud835\udc5d\ud835\udc56 \ud835\udc57 = 1 \u2212 \ud835\udc5d \ud835\udc57\ud835\udc56 ) and transitive. We investigate for the first time if the efficiency of pairwise rerankers can be increased without a significant loss of effectiveness by sampling from and thus sparsifying the comparison set.\nThe two components of the mono-duo re-ranking pipeline that we study in this paper are highlighted in Figure 1: We introduce a sampling step before the pairwise inference to draw a subset of the \ud835\udc582 \u2212 \ud835\udc58 possible comparisons, and we revisit the aggregation step since its effectiveness directly depends on the kind of sample it receives (Section 3). To investigate the effect of sparsification on the retrieval effectiveness, we study three sampling methods (global random, exhaustive window, skip-window) and five aggregation methods (sorting, summation, regression, greedy, and graph-based) on three datasets (ClueWeb09, ClueWeb12, MS MARCO) using the pointwise monoT5 and the pairwise duoT5 models [34]. Our results show that skip-window sampling with greedy aggregation allows for an order of magnitude fewer comparisons at an acceptable loss of effectiveness, while competitive effectiveness to the allpairs approach can already be achieved with only one third of the comparisons (Section 5). All code and data underlying our experiments are publicly available.1"
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "We briefly give some background on the history of learning-to-rank retrieval models before detailing the nature of pairwise learningto-rank models and reviewing rank aggregation approaches, which we employ to aggregate pairwise preferences into a final ranking. Finally, we describe some related efforts at making transformerbased learning to rank more efficient.\nLearning to Rank. Since decades, machine learning has been applied to improve retrieval effectiveness [17, 25]. Traditional featurebased learning-to-rank models evolved from pointwise over pairwise to listwise approaches [26]. While feature-based models are still successful [35], the recent promising retrieval effectiveness results of pre-trained transformer models [25] has shifted the community\u2019s focus away from feature-based learning to rank. But history appears to repeat itself as the aforementioned evolution from pointwise approaches like monoBERT [32] and monoT5 [31] to pairwise approaches [25] can been observed as well.\nPairwise Learning to Rank. Pairwise learning-to-rank approaches predict which document in a pair is probably more relevant to a query and should be ranked higher [26]. In feature-based as well as transformer-based learning to rank, pairwise approaches usually outperform pointwise ones that score documents independently of each other [26]. Yet, when the inference step of a pairwise model compares all pairs of documents, the run time requirement is quadratic in the number of documents to be ranked. In an effort to reduce the comparison count, extensive studies of theoretical properties of feature-based pairwise approaches [9, 23] have led to suggestions for better run time characteristics. For example, SortNet [36] uses a learned preference function that is guaranteed to output symmetric preferences, allowing to skip half of the comparisons. Yet, recent pairwise transformer-based models like duoBERT [34] and the more effective duoT5 [34] lack the desirable symmetry property of models like SortNet. Additionally, the theoretical analysis of 1https://github.com/webis-de/ICTIR-22\nduoBERT and duoT5 is still in its infancy; previous work even found such models difficult to be interpreted [27, 41]. The effectiveness of duoBERT or duoT5 relies on computing preferences for all pairs of documents, at the expense of their efficiency [45] limiting their applicability in search scenarios with run time constraints.\nRank Aggregation. Rank aggregation [26] uses pairwise relevance preference probabilities to derive a ranking\u2014often by computing a score for each individual document. Finding an optimal aggregation with arbitrarily-sized inputs is an NP-hard problem [10], but many approaches are known to work well in practice. While dynamic aggregation methods decide which documents to compare next based on all previous comparisons, static aggregation methods assume that the required pairwise comparisons are conducted before the actual aggregation starts [42].\nIn our study, we employ the following five aggregation methods (details in Section 3.2): (1) Sorting via the KwikSort method [2], which assumes that the comparisons are consistent and form a total order, (2) additive aggregation [34], where the rank of a document is indicated by the sum of the document\u2019s comparison probabilities (potentially transforming the probabilities before summation), (3) regression-based aggregation [4, 38, 40, 46], where latent scores for documents are learned so that they optimally correspond to a given set of pairwise comparisons, (4) greedy aggregation [3, 10], in which a heuristic iteratively selects and removes the best document from a given set and then proceeds with the rest, and (5) graphbased aggregation [43], where comparisons are interpreted as directed edges between document nodes and a measure of graph centrality is used to derive a ranking score.\nEfficiency Improvements for Transformer-based Re-Rankers. The high computational cost of re-ranking documents with pre-trained transformers has recently received attention [20]. Even for pointwise approaches, the inference overhead can be prohibitive for practical applications [45]. There are two ideas to improve the efficiency of neural re-rankers: (1) improving the efficiency of the ranking model, and (2) reducing the required number of inferences.\nApproaches to the former include early-exiting from inference by intermediate between-layer classification in BERT-like models [44], model distillation [18, 19], or improved dense representations [39]. For the latter, Zhang et al. [45] propose to introduce filtering steps inmulti-stage re-ranking pipelines. They utilize feature-based learning to rank to compute a set of candidate documents that is then re-ranked using a BERT-like neural model, increasing efficiency by a factor of up to 18 compared to an unfiltered baseline at the same effectiveness. But while document filtering has been studied for pointwise re-ranking, to our knowledge, filtering approaches for pairwise re-ranking have not been addressed to date."
        },
        {
            "heading": "3 SPARSIFIED PAIRWISE RE-RANKING",
            "text": "In this section, we describe the steps we adapted in the monoduo re-ranking pipeline (Figure 1): sampling methods to select the to-be-compared document pairs (three methods, Section 3.1), and aggregation methods that derive a ranking from the ranking preferences (five methods, Section 3.2). For completeness, we also briefly detail the steps adopted from the literature: initial retrieval, as well as pointwise and pairwise re-ranking (Section 3.3)."
        },
        {
            "heading": "3.1 Sampling",
            "text": "Based on the top-\ud835\udc58 results \ud835\udc37\ud835\udc58 of the pointwise re-ranking step of the mono-duo paradigm, we propose to sparsify the set \ud835\udc36all of all \ud835\udc582 \u2212 \ud835\udc58 comparisons (no self-comparisons) and to use a sampled comparison set \ud835\udc36 \u2282 \ud835\udc36all as input for the pairwise re-ranking step. The goal is to minimize the size of\ud835\udc36 and thus the effort of pairwise re-ranking without compromising the quality of the final ranking.\nWe distinguish random from structured sampling, with the main difference being their (non-)determinism. Independent random samples from a given \ud835\udc36all very likely contain different comparisons, but structured samples always choose the same comparisons. To be compatible with a variety of aggregation methods, a sampling must meet two requirements: (1) each document is part of at least one comparison, (2) each comparison is sampled at most once. Figure 2 illustrates the three sampling methods introduced below for a document set \ud835\udc37\ud835\udc58 of size \ud835\udc58 = 20 at two sampling rates, one per line.\nGlobal random sampling (G-Random). For each of the top-\ud835\udc58 documents \ud835\udc51 \u2208 \ud835\udc37\ud835\udc58 from the pointwise ranking, a fraction \ud835\udc5f \u2208 (0...1] of the remaining \ud835\udc58 \u2212 1 documents is randomly selected for the comparison set \ud835\udc36 that then has the size |\ud835\udc36 | = \u230a\ud835\udc5f \u00b7 (\ud835\udc582 \u2212 \ud835\udc58)\u230b.\nNeighborhood window sampling (N-Window). A sliding window of size \ud835\udc5a \u2264 \ud835\udc58 \u2212 1 is moved over the top-\ud835\udc58 documents \ud835\udc37\ud835\udc58 from the pointwise ranking. For document \ud835\udc51\ud835\udc56 \u2208 \ud835\udc37\ud835\udc58 , its\ud835\udc5a direct successors in the ranking are sampled for comparison. But since the last \ud835\udc5a documents in the ranking of \ud835\udc37\ud835\udc58 have less than\ud835\udc5a successors, we let the window \u201cwrap around\u201d to the top-ranked documents. For document \ud835\udc51\ud835\udc56 , the\ud835\udc5a sampled comparisons (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36 thus fulfill \ud835\udc57 = 1 + (\ud835\udc4e mod \ud835\udc58) for \ud835\udc4e \u2208 {\ud835\udc56, . . . , \ud835\udc56 +\ud835\udc5a \u2212 1}. The size of the sampled comparison set \ud835\udc36 is \ud835\udc58 \u00b7\ud835\udc5a and each document is the first entry of \ud835\udc5a comparisons and the second entry of another\ud835\udc5a comparisons.\nAn assumption underlying the neighborhood sampling is that the \u201cglobal\u201d pointwise ranking is sensible but that \u201clocal\u201d re-ranking leads to an improved effectiveness. If true, however, it would be plausible to stop the window for \ud835\udc56 > \ud835\udc58 \u2212\ud835\udc5a rather than to wrap it around. However, pilot experiments have shown that this leads to poorer effectiveness, possibly because fewer comparisons are sampled at both the beginning and the end of a top-\ud835\udc58 ranking.\nSkip-window sampling (S-Window). N-Window samples from the \u201clocal\u201d neighborhood in a ranking. To enable more \u201cglobal\u201d comparisons, we introduce a skip size \ud835\udf06 \u2208 N+. For \ud835\udc51\ud835\udc56 \u2208 \ud835\udc37\ud835\udc58 , comparisons (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) to\ud835\udc5a successors are sampled so that \ud835\udc57 = 1+ (\ud835\udc4e mod \ud835\udc58) for \ud835\udc4e \u2208 {\ud835\udc56+\ud835\udf06\u22121, \ud835\udc56+2\ud835\udf06\u22121, . . . , \ud835\udc56+\ud835\udc5a\ud835\udf06\u22121}; when \ud835\udc57 = \ud835\udc56 for some \ud835\udc4e, that comparison is not included in the sample. For \ud835\udf06 = 1, S-Window corresponds to N-Window, and for \ud835\udf06 = 3, for example, each document is compared to every third of its successors. The \ud835\udf06-skip deterministically controls the \u201cglobality\u201d of a sample without increasing the amount |\ud835\udc36 | of sampled comparisons compared to N-Window."
        },
        {
            "heading": "3.2 Aggregation",
            "text": "For each comparison (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36 , a pairwise model computes a preference probability \ud835\udc5d\ud835\udc56 \ud835\udc57 , which indicates how likely \ud835\udc51\ud835\udc56 should be ranked above (\ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5) or below \ud835\udc51 \ud835\udc57 (\ud835\udc5d\ud835\udc56 \ud835\udc57 < 0.5). From the probabilities computed for\ud835\udc36 , an aggregation method derives a relevance value \ud835\udc60\ud835\udc56 for each document \ud835\udc51\ud835\udc56 . We study five paradigmatically different aggregation methods.\nKwikSort. As a baseline, we use the KwikSort method [2]. It is an extension of the Quicksort algorithm for data with preferences and, in our case of top-\ud835\udc58 ranking, has an expected number of comparisons in \ud835\udc42 (\ud835\udc58 log\ud835\udc58). First, a random document \ud835\udc51\ud835\udc56 is chosen to be the pivot. Then, all other documents are compared to the pivot placing the ones to be lower-ranked than \ud835\udc51\ud835\udc56 and the ones to be higher-ranked in separate subsets. These subsets are recursively ranked until a final ranking is obtained. KwikSort does not rely on a preceding sampling step; that the expected number of comparisons is in \ud835\udc42 (\ud835\udc58 log\ud835\udc58) is a feature of the dynamic aggregation itself.\nAdditive Aggregation. Pradeep et al. [34] propose four different aggregation techniques based on preference probability summation. They find the symmetric sum of preference probabilities to yield the best effectiveness:\n\ud835\udc60\ud835\udc56 = \u2211\ufe01\n\ud835\udc57 \u22081...\ud835\udc58 (\ud835\udc5d\ud835\udc56 \ud835\udc57 + (1 \u2212 \ud835\udc5d \ud835\udc57\ud835\udc56 )) .\nHowever, in our samples, not all comparisons are present so that we replace missing summands \ud835\udc5d\ud835\udc56 \ud835\udc57 or (1 \u2212 \ud835\udc5d \ud835\udc57\ud835\udc56 ) by 0.\nBradley-Terry Aggregation. The Bradley-Terry model [4] infers a latent score \ud835\udc60\ud835\udc56 \u2208 \ud835\udc46 for each document \ud835\udc51\ud835\udc56 \u2208 \ud835\udc37\ud835\udc58 based on the preferences expressed in the sampled comparison set \ud835\udc36 using maximumlikelihood estimation. In its original form, exponential score functions were used, which corresponds to a logistic regression on pairwise data [1] and can be expressed as:\nL(\ud835\udc46,\ud835\udc36) = \u2211\ufe01\n\ud835\udc51\ud835\udc56 \u227b\ud835\udc51 \ud835\udc57 log \ud835\udc52\n\ud835\udc60\ud835\udc56 \ud835\udc52\ud835\udc60\ud835\udc56 + \ud835\udc52\ud835\udc60 \ud835\udc57 + \u2211\ufe01\n\ud835\udc51\ud835\udc56 \u227a\ud835\udc51 \ud835\udc57 log \ud835\udc52\n\ud835\udc60 \ud835\udc57\n\ud835\udc52\ud835\udc60\ud835\udc56 + \ud835\udc52\ud835\udc60 \ud835\udc57 .\nHere, \ud835\udc51\ud835\udc56 \u227b \ud835\udc51 \ud835\udc57 denotes all comparisons (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36 with \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5 and \ud835\udc51\ud835\udc56 \u227a \ud835\udc51 \ud835\udc57 denotes all comparisons (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36 with \ud835\udc5d\ud835\udc56 \ud835\udc57 < 0.5. The unknown latent score set \ud835\udc46 is usually found via BFGS optimization [16] so that a ranking according to the \ud835\udc60\ud835\udc56 violates as few of the preferences from the comparison sample \ud835\udc36 as possible.\nInput: Document set \ud835\udc37\ud835\udc58 , preference probabilities \ud835\udc5d\ud835\udc56 \ud835\udc57 Output: Score \ud835\udc60 for each \ud835\udc51 \u2208 \ud835\udc37\ud835\udc58 foreach \ud835\udc51\ud835\udc56 \u2208 \ud835\udc37\ud835\udc58 do \ud835\udc61\ud835\udc56 \u2190 \u2211 \ud835\udc51 \ud835\udc57 \u2208\ud835\udc37\ud835\udc58 \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2212 \u2211 \ud835\udc51 \ud835\udc57 \u2208\ud835\udc37\ud835\udc58 \ud835\udc5d \ud835\udc57\ud835\udc56 ; while \ud835\udc37\ud835\udc58 \u2260 \u2205 do \ud835\udc51 \ud835\udc57 \u2190 arg max\ud835\udc51\ud835\udc56 \u2208\ud835\udc37\ud835\udc58 \ud835\udc61\ud835\udc56 ; \ud835\udc60 \ud835\udc57 \u2190 |\ud835\udc37\ud835\udc58 |; \ud835\udc37\ud835\udc58 \u2190 \ud835\udc37\ud835\udc58 \\ {\ud835\udc51 \ud835\udc57 }; foreach \ud835\udc51\ud835\udc56 \u2208 \ud835\udc37\ud835\udc58 do \ud835\udc61\ud835\udc56 \u2190 \ud835\udc61\ud835\udc56 \u2212 \ud835\udc5d\ud835\udc56 \ud835\udc57 + \ud835\udc5d \ud835\udc57\ud835\udc56 ; end Algorithm 1: Greedy aggregation of preferences [10].\nGreedy Aggregation. Cohen et al. [10] propose a greedy ordering algorithm that is proven to closely approximate the best total order in terms of the number of violated preferences. Algorithm 1 shows its pseudocode. In every iteration, the document \ud835\udc51 \ud835\udc57 with the highest \u201cpotential\u201d \ud835\udc61 \ud835\udc57 2 is appended to the re-ranking on the highest still unoccupied rank by setting score \ud835\udc60 \ud835\udc57 accordingly. The potentials of the remaining documents are updated by canceling out the respective terms that include \ud835\udc51 \ud835\udc57 . With sampling, the comparison set is incomplete; missing probabilities \ud835\udc5d\ud835\udc56 \ud835\udc57 are set to zero.\nPageRank Aggregation. A comparison set \ud835\udc36 induces a directed graphwith\ud835\udc37\ud835\udc58 as nodes and comparisons as directed edgesweighted with preference probabilities. We introduce a new aggregation method that computes the graph centrality measure PageRank [33], extended for weighted graphs [29], to rank the documents. The fundamental principle of PageRank is that nodes with incoming edges from nodes with high PageRank scores should also receive high PageRank scores. The respective PageRank-style aggregation of a score \ud835\udc60\ud835\udc56 for a document \ud835\udc51\ud835\udc56 then is\n\ud835\udc60\ud835\udc56 = \ud835\udefe \u00b7 1 |\ud835\udc37\ud835\udc58 |\n+ (1 \u2212 \ud835\udefe) \u00b7 \u2211\ufe01\n(\ud835\udc51 \ud835\udc57 ,\ud835\udc51\ud835\udc56 ) \u2208\ud835\udc36\n\ud835\udc5d \ud835\udc57\ud835\udc56\u2211 \ud835\udc59 \u2208[1,\ud835\udc58 ] \ud835\udc5d \ud835\udc57\ud835\udc59 \u00b7 \ud835\udc60 \ud835\udc57 ,\nwhere using the components with the damping factor \ud835\udefc \u2208 [0, 1] ensure convergence when computing the PageRank scores iteratively."
        },
        {
            "heading": "3.3 Initial Retrieval and Re-ranking",
            "text": "Following the experimental setup of Pradeep et al. [34] closely (see Figure 1), for each query, we first obtain an initial ranking using BM25 (PyTerrier implementation [28], default configuration). The top-1000 BM25 results are then re-ranked using monoT5 [31] in the pointwise re-ranking step. For the top-50 monoT5 results, duoT5 [34] infers preference probabilities in the second step of pairwise re-ranking, after sampling. For both, monoT5 and duoT5, we apply the largest available pre-trained version.3 We use T5 instead of BERT variants, as T5 has been shown to be more effective [25]. To avoid repeated inferences in our experiments, all \ud835\udc582 \u2212\ud835\udc58 pairwise preference probabilities are cached once for each query.\nThe maximum input length of transformer models is limited, so that a representative passage has to be chosen from each document for inference. Following the method of Dai and Callan [15], we split each document into fixed-length non-overlapping passages of 2The \u201cpotential\u201d basically tallies \ud835\udc51\ud835\udc56 \u2019s \u201cwins\u201d against other documents compared to its \u201closses\u201d in terms of preference probabilities. 3monoT5: https://huggingface.co/castorini/monot5-3b-msmarco duoT5: https://huggingface.co/castorini/duot5-3b-msmarco\nabout 250 words (using the TREC CAsT Y4 tools;4 splits at sentence boundaries). Fixed-length passages have been shown to be more effective than variable-length passages [22]. In our pilot experiments, using the first passage was the most effective heuristic, so that we use them for preference probability inference."
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": "In this section, we introduce our evaluation measures, detailing in particular measures for consistency, complementarity, and transitivity of the aggregated relevance scores, and recap the used datasets."
        },
        {
            "heading": "4.1 Evaluation Measures",
            "text": "We follow similar studies of the mono/duoT5 models [34] and use nDCG@10 [21] to evaluate the retrieval effectiveness. When reranking the top-\ud835\udc58 results of a pointwise model, the \ud835\udc582 \u2212 \ud835\udc58 comparisons \ud835\udc36all usually performed by a pairwise model may result in inconsistent preference probabilities (1) at the level of a document pair and (2) at the level of document triples. We further examine these potential inconsistencies, as they can \u201ccomplicate\u201d the aggregation step and affect the retrieval effectiveness. At the document pair level, one would expect \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2248 1 \u2212 \ud835\udc5d \ud835\udc57\ud835\udc56 but pairwise models do not guarantees this and may predict both \ud835\udc51\ud835\udc56 \u227b \ud835\udc51 \ud835\udc57 for the input pair (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) and \ud835\udc51 \ud835\udc57 \u227b \ud835\udc51\ud835\udc56 for the input pair (\ud835\udc51 \ud835\udc57 , \ud835\udc51\ud835\udc56 ), or vice versa, where \ud835\udc51\ud835\udc56 \u227b \ud835\udc51 \ud835\udc57 denotes a ranking preference of the left document \ud835\udc51\ud835\udc56 over the right one \ud835\udc51 \ud835\udc57 . At the document triple level, transitivity may be violated as a model may predict \ud835\udc51\ud835\udc56 \u227b \ud835\udc51 \ud835\udc57 and \ud835\udc51 \ud835\udc57 \u227b \ud835\udc51\ud835\udc59 but \ud835\udc51\ud835\udc59 \u227b \ud835\udc51\ud835\udc56 .\nThe consistency of an all-(\ud835\udc582 \u2212 \ud835\udc58)-pairs comparison set \ud835\udc36all at the level of document pairs is the fraction of pairs (\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36all for which \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5 and \ud835\udc5d \ud835\udc57\ud835\udc56 < 0.5:\nconsistency(\ud835\udc36all) = |{(\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36all : \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5 and \ud835\udc5d \ud835\udc57\ud835\udc56 < 0.5}|\n|\ud835\udc36all | .\nWhile consistency captures the comparison direction, also the numerical complementarity of how close \ud835\udc5d\ud835\udc56 \ud835\udc57 + \ud835\udc5d \ud835\udc57\ud835\udc56 is to the \u201cideal\u201d 1 can be interesting. Thus, we also measure the \ud835\udf00-complementarity with respect to a margin of error \ud835\udf00 as:\n\ud835\udf00-complementarity(\ud835\udc36all) = |{(\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 ) \u2208 \ud835\udc36all : |\ud835\udc5d\ud835\udc56 \ud835\udc57 + \ud835\udc5d \ud835\udc57\ud835\udc56 \u2212 1| < \ud835\udf00}|\n|\ud835\udc36all | .\nFinally, the transitivity of \ud835\udc36all measures the fraction of document triples for which the pairwise comparisons are transitive:\ntransitivity(\ud835\udc36all) = |\ud835\udc47 | |\ud835\udc47 | + |\ud835\udc3c | , where\n\ud835\udc47 = {(\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 , \ud835\udc51\ud835\udc59 ) : \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5, \ud835\udc5d \ud835\udc57\ud835\udc59 \u2265 0.5, and \ud835\udc5d\ud835\udc56\ud835\udc59 \u2265 0.5} \u222a {(\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 , \ud835\udc51\ud835\udc59 ) : \ud835\udc5d\ud835\udc56 \ud835\udc57 < 0.5, \ud835\udc5d \ud835\udc57\ud835\udc59 < 0.5, and \ud835\udc5d\ud835\udc56\ud835\udc59 < 0.5} and\n\ud835\udc3c = {(\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 , \ud835\udc51\ud835\udc59 ) : \ud835\udc5d\ud835\udc56 \ud835\udc57 \u2265 0.5, \ud835\udc5d \ud835\udc57\ud835\udc59 \u2265 0.5, but \ud835\udc5d\ud835\udc56\ud835\udc59 < 0.5} \u222a {(\ud835\udc51\ud835\udc56 , \ud835\udc51 \ud835\udc57 , \ud835\udc51\ud835\udc59 ) : \ud835\udc5d\ud835\udc56 \ud835\udc57 < 0.5, \ud835\udc5d \ud835\udc57\ud835\udc59 < 0.5, but \ud835\udc5d\ud835\udc56\ud835\udc59 \u2265 0.5} .\nThe more the \ud835\udf00-complementarity for some small \ud835\udf00 and the more the transitivity of some \ud835\udc36all approach 1, the more a total order between the documents is implied that probably can also be derived from some smaller comparison sample \ud835\udc36 \u2282 \ud835\udc36all . 4https://github.com/grill-lab/trec-cast-tools\ncomplementarity over \ud835\udf00. In the plots, the corpora are color-coded as ClueWeb09 , ClueWeb12 , andMSMARCO (Passage)\n."
        },
        {
            "heading": "4.2 Evaluation Data",
            "text": "We employ three standard retrieval corpora in our experiments: the ClueWeb09, the ClueWeb12, and the MS MARCO passage corpus.\nClueWeb09. The ClueWeb09 corpus5 consists of 1 billion documents crawled between January and February 2009. It was used for the ad-hoc search tasks of the TREC Web tracks 2009\u20132012 [5\u20138], where 70,575 graded relevance judgments were collected on a 4-point scale for 200 topics (avg. 356 judgments per topic).\nClueWeb12. The ClueWeb12 corpus6 consists of 733 million documents crawled between April and May 2012. It was used for the ad-hoc search tasks of the TRECWeb tracks 2013/14 [11, 12], where 28,116 graded relevance judgments were collected on a 4-point scale for 100 topics (avg. 281 judgments per topic).\nMS MARCO (Passage). The MS MARCO passage corpus [30] consists of 8.8 million passages extracted from Bing search engine results. It was used for the passage ranking task of the TREC Deep Learning tracks 2019/20 [13, 14], where 20,646 graded relevance judgments were collected on a 4-point scale for 97 topics (avg. 213 judgments per topic). With this corpus, we replicate the experimental setup of Pradeep et al. [34].\nRemark. In our evaluation of re-ranking results, we only consider judged documents. Evaluation scores calculated excluding unjudged documents correlate well with evaluations including them [37]."
        },
        {
            "heading": "5 EVALUATION RESULTS",
            "text": "We conduct two experiments to evaluate the suitability of sampling and aggregation methods for efficient pairwise re-ranking. The first experiment (Section 5.1) explores the properties of the comparison sets inferred for each of the three corpora. This supplies context to the ranking effectiveness evaluation in the second experiment (Section 5.2), in which we analyze rankings for different combinations of samplers and aggregators. 5http://lemurproject.org/clueweb09.php/ 6http://lemurproject.org/clueweb12.php/"
        },
        {
            "heading": "5.1 Evaluation of Pairwise Prediction Properties",
            "text": "For each topic from each corpus, we derive the duoT5 preference probabilities for the set \ud835\udc36all of all \ud835\udc582 \u2212 \ud835\udc58 pairwise comparisons for the top-50 results of the pointwise re-ranking and compute the statistics and measures per corpus shown in Figure 3.\nThe preference probabilities are highly skewed towards the extremes of the scale (cf. Figure 3a): for the majority of document pairs, the preference probability is approximately zero or one. This effect is stronger for the MS MARCO passage corpus (on which the model was trained) than for the ClueWeb corpora. Interestingly, the score distributions are not symmetric, but are slightly skewed towards 1.0 for all three corpora. Since the comparison set\ud835\udc36all contains both comparison directions for every pair, the observed skew directly suggests to further inspect how consistent, transitive, and complementary the preferences are for document pairs or triples.\nIndeed, on average, only between half (MS MARCO) and a third of the comparisons (ClueWeb09) are consistent in their direction (cf. Figure 3b). Some variation across topics exists, yet the consistency is rather low in the majority of the topic-wise comparison sets. Further, also the cumulative \ud835\udf16-complementarity (cf. Figure 3d) confirms that the preferences of the pairwise duoT5 model are not that complementary (i.e., \ud835\udc5d\ud835\udc56 \ud835\udc57 + \ud835\udc5d \ud835\udc57\ud835\udc56 \u2260 1) for a document pair\u2019s two possible input orders. Only for a rather large value of \ud835\udf00 = 0.4 all probabilities for pairs in all corpora are \ud835\udf00-complementary (i.e., |\ud835\udc5d\ud835\udc56 \ud835\udc57 + \ud835\udc5d \ud835\udc57\ud835\udc56 \u2212 1| \u2260 0.4), and more than half of the pairs require an \ud835\udf16-value between 0.3 (MS MARCO) and 0.2 (ClueWeb09). For all corpora, almost no comparison pairs reach a complementarity of \ud835\udf16 < 0.1. Also the transitivity rates are consistently between 0.7 and 0.8 across all corpora with very little variation per topic. These observations (consistency and transitivity not that high) suggest that the comparison-based KwikSort aggregationwill not output the most effective re-ranking and that very likely more than\ud835\udc42 (\ud835\udc58 log\ud835\udc58) comparison pairs are needed in a sample for the other aggregation methods to \u201cwork around\u201d the low consistency and lacking complementarity using more information.\nand the two samplers G-Random and N-Window with different sampling rates. Dotted: pointwise re-ranking, dashed: unsampled \ud835\udc36all ."
        },
        {
            "heading": "5.2 Evaluation of Ranking Effectiveness",
            "text": "To evaluate the effectiveness of different combinations of sampling and aggregation methods on all three corpora, we simulate runs on sparsified comparison sets at sample rates ranging from 0.05 to 0.95 in steps of 0.05. Each simulation is repeated ten times and the effectiveness is averaged to account for random variation in both the sampling and the aggregation step. In addition, baseline runs for each aggregator use the full comparison sets \ud835\udc36all per topic without any sampling. In total, 4950 runs are simulated. The pointwise ranking to be re-ranked by a pairwise model achieves nDCG@10 scores of 0.38 (ClueWeb09), 0.49 (ClueWeb12) and 0.50 (MS MARCO).\nEffectiveness of KwikSort. From the observations in Section 5.1 (low consistency and low transitivity), it is clear that KwikSort with its pivot-based dynamic sampling of a rather \u201cfew\u201d \ud835\udc42 (\ud835\udc58 log\ud835\udc58) comparisons will not be able to achieve a really good effectiveness. Indeed, the nDCG@10 scores of KwikSort of 0.34 (ClueWeb09), 0.39 (ClueWeb12), and 0.42 (MS MARCO) are even lower than the pointwise effectiveness. We tested different pivot selection methods\nbut all resulted in a similarly bad effectiveness. Using KwikSort for pairwise rank aggregation can thus not be recommended in settings with inconsistent and intransitive comparisons.\nEffectiveness on the full comparison set \ud835\udc36all . Figure 4 shows the nDCG@10 of the non-KwikSort aggregation methods on each corpus. The dotted and dashed horizontal lines indicate the baseline effectiveness of the pointwise ranking and the pairwise re-ranking, respectively, when using the respective aggregation method on the complete comparison set \ud835\udc36all without sampling. Since the pointwise and the \ud835\udc36all-aggregation effectiveness are thus independent of the sampling rate, they are depicted as horizontal lines.\nUsing the full comparison set\ud835\udc36all without any sampling, greedy aggregation yields the most effective re-rankings for the ClueWeb09 (Subplot 4c) and the MS MARCO passage corpus (Subplot 4g) while additive aggregation is the most effective on the ClueWeb12 (Subplot 4i). On MS MARCO (first row of Figure 4), all aggregation methods are approximately equally effective when using the full comparison set\ud835\udc36all (dashed lines show about the same nDCG@10).\nN-Window . Dotted line: pointwise ranking, dashed line: effectiveness on unsampled \ud835\udc36all .\nOn the ClueWeb corpora, though, PageRank aggregation is less effective than the pointwise ranking (Subplots 4h and l; dashed line below dotted line) and also Bradley-Terry aggregation struggles on the ClueWeb12 (Subplot 4j). Only additive and greedy aggregation always improve upon the pointwise ranking when using the full comparison set \ud835\udc36all (Subplots 4e, g, i, and k) but the improvement is smaller on the ClueWeb corpora. That the effectiveness and the improvement over the pointwise ranking are the highest on the MS MARCO passage corpus is not surprising since the duoT5 reranking model was trained on MS MARCO, and since the TREC Web track relevance judgments used to evaluate the effectiveness on the ClueWeb corpora are at the document level, while we only rank one passage per document due to input length limitations.\nEffectiveness with G-Random and N-Window sampling. The colorcoded curves in the plots of Figure 4 show the effectiveness of the different aggregation methods with G-Random or N-Window sampling at different sampling rates from the full comparison set \ud835\udc36all . Four trends are apparent across all corpora.\nFirst, N-Window sampling results in less effective re-rankings than G-Random sampling in nearly all cases, especially at smaller sampling rates. This effect is particularly noticeable for additive aggregation (Subplots 4a, e, and i). One reason probably is that the more \u201clocal\u201d comparisons of N-Window are likely to yield less extreme comparison probability differences that decrease the overall separability of document pairs in sparse sampling setups. Also, inconsistencies in pairwise judgments are more likely for \u201clocal\u201d pairs. Overall, this indicates that the global context of documents (which is better represented by G-Random) is important to obtain effective re-rankings via aggregation.\nSecond, greedy aggregation is the most effective aggregation method for both, G-Random and N-Window sampling (comparing Subplots 4c, g, and k to the rest). It is also the only aggregation method for which the effectiveness on the full \ud835\udc36all is reached by some sparsified comparison sets.\nThird, the effectiveness degradation is not linear with respect to the sample rate (all subplots), but drops sharply below 15\u201320%. This suggests a lower bound of comparisons needed to derive good rankings from the pairwise comparisons of duoT5 which lack in consistency and transitivity.\nFourth, Bradley-Terry- and PageRank-aggregated re-rankings often are the least effective (Subplots 4b, f, and j, as well as d, h, and l). A possible reason for Bradley-Terry is similar to the bad effectiveness of KwikSort-aggregated re-rankings: Bradley-Terry only takes the direction of a comparison into account but not the magnitude of the respective probability. With the inconsistent probabilities of duoT5 that lead to inconsistent comparison directions, BradleyTerry cannot derive good final re-rankings\u2014just like KwikSort. In case of PageRank aggregation, also the inconsistent probabilities that are used as edge weights, might \u201cconfuse\u201d the actual derivation of the PageRank scores.\nEffectiveness with S-Window sampling. To find a good value for \ud835\udf06 in S-Window sampling, we run a grid search over \ud835\udf06 = 2 . . . 15, separately for all sample rates (0.05 to 0.95 in steps of 0.05). We use fivefold cross validation to determine the best choice on theMSMARCO corpus, as the overall effectiveness gains on the ClueWeb corpora were too small to meaningfully distinguish between setups. Figure 5 shows the nDCG@10 effectiveness on MS MARCO of the run with the optimal \ud835\udf06-value for each sample rate. The best runs for G-Random and N-Window are also shown for reference.\nRe-rankings aggregated from S-Window samples are more effective by a margin for each of the aggregation methods at all sampling rates. The combination of S-Window sampling with greedy aggregation allows for a rather stable effectiveness down to sampling only 30% of the comparisons. Even when using an order of magnitude fewer comparisons (i.e., \u2248 10% of \ud835\udc36all ), a competitive effectiveness is achieved (nDCG@10 only 0.04 less).\nThe best values for \ud835\udf06 are between 7 and 10 in most cases; they are not correlated with the window size (Pearson\u2019s \ud835\udf0c = 0.04). Already the better effectiveness of aggregated rankings using G-Random sampling over N-Window sampling suggests that the global context is important when sampling comparisons. Also the rather large bestworking \ud835\udf06-values for S-Window sampling corroborate this since even for small sample sizes\ud835\udc5a they ensure that the sampled comparisons cover a pretty \u201cglobal\u201d context. For large sample sizes\ud835\udc5a, \ud835\udf06 is not as important as the sample then already covers a larger amount of the full comparison set \ud835\udc36all .\nTable 1: Effectiveness on the MS MARCO corpus as nDCG@10 for the full comparison set \ud835\udc36all and the lowest similarly effective sampling rate (non-significant nDCG@10 difference; delta in brackets) per sampling method and aggregator. Bonferroni-correction for all (incl. hidden) tests per row.\nAggregator nDCG@10 Lowest Similarly Effect. Sampl. Rate\nUnsampled\ud835\udc36all S-Window G-Random N-Window\nAdditive 0.691 0.35 (-0.014) 0.85 (-0.019) 0.95 (-0.004) Bradley-Terry 0.691 0.50 (-0.012) 1.00 (-0.000) 0.90 (-0.008) Greedy 0.707 0.30 (-0.013) 0.85 (-0.006) 0.50 (-0.010) PageRank 0.695 0.30 (-0.016) 0.65 (-0.012) 0.95 (-0.004)\nMinimal sampling rates. Table 1 shows the minimum attainable sampling rates on the MS MARCO corpus for which each combination of sampling and aggregation method is not significantly less effective in terms of nDCG@10 than the respective aggregation on the full comparison set \ud835\udc36all . Per aggregator, the difference of the runs for each of the 19 sampling rates is tested against the run that aggregates a ranking from the full comparison set \ud835\udc36all using a paired Student\u2019s t-test with an \ud835\udefc-level of 0.05 and Bonferroni correction for the multiple tests. For G-Random with potentially different effectiveness scores for the 10 runs per sampling rate, we use the least effective run per sampling rate in terms of nDCG@10 to increase the overall confidence in case of observed differences.\nAmong the sampling strategies, S-Window achieves the by far lowest sampling rates per aggregator without hurting the retrieval effectiveness too much. About the same effectiveness is possible with S-Window for additive, greedy, and PageRank aggregation with just one third of the usually used unsampled comparisons. G-Random and N-Window need more comparisons with any aggregation method to achieve the same re-ranking effectiveness and, in fact, only lead to some substantial savings compared to the unsampled \ud835\udc36all for PageRank aggregation (G-Random) or greedy aggregation (N-Window).\nAmong the aggregation strategies, additive and Bradley-Terry aggregation are the least effective and all sampling methods need larger sample rates for Bradley-Terry than for the other aggregators. Greedy aggregation leads to the best effectiveness and G-Random and N-Window achieve their lowest sampling rates without effectiveness loss for greedy aggregation.\nOverall, the best combination in terms of effectiveness and sampling rate is greedy aggregation with S-Window sampling: with about one third of the usual \ud835\udc582 \u2212 \ud835\udc58 comparisons, the best effectiveness can be reached."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we analyze several methods to substantially reduce the quadratic number of document comparisons usually conducted in pairwise re-ranking with transformers. To this end, we introduce a sampling step at the beginning of the pairwise re-ranking and adapt the aggregation step to derive relevance scores for a re-ranking from smaller samples of the pairwise comparisons. By\ncomparing combinations of three sampling methods and five aggregation methods, we show that only one third of the comparisons are needed to achieve competitive re-ranking effectiveness and that also an order of magnitude less comparisons still can yield only a very slightly decreased effectiveness.\nCompared to the usually applied additive rank aggregation without sampling, our new combination of skip-window sampling with greedy aggregation achieves an even better effectiveness at only about one third of the comparisons. When tolerating a very slight loss in effectiveness, even an order of magnitude fewer comparisons suffice. The more local exhaustive window sampling method leads to less effective rankings for which larger samples are needed than for skip-window or a global random sample. This suggests that a good sample of pairwise comparisons should not just sample from a very local environment per rank in the pointwise ranking.\nSparsification in pairwise re-ranking opens up new areas of research. Of the sampling paradigms evaluated (random vs. structured and local vs. global), the global structured sampling works better than the random one. Still, the samples are static in the sense that the sample is pre-computed before aggregation. Dynamic sampling techniques, in which new comparisons could be selected even during aggregation could merit further analyses. Sparsification could also be used to increase the depth of the pairwise re-ranking rather than its efficiency. Instead of minimizing the comparison budget for a fixed depth \ud835\udc58 , a fixed comparison budget can be used to maximize \ud835\udc58 . For example, the usually recommended depth \ud835\udc58 = 50 requires 2,450 comparisons (\ud835\udc582\u2212\ud835\udc58) for traditional pairwise re-ranking. A sampling rate of 30% (or 10%) now allows a re-ranking depth of \ud835\udc58 = 90 (\ud835\udc58 = 157) for a budget of about 2,450 comparisons. This may have a strong effect for recall-intensive retrieval tasks."
        }
    ],
    "title": "Sparse Pairwise Re-ranking with Pre-trained Transformers",
    "year": 2022
}