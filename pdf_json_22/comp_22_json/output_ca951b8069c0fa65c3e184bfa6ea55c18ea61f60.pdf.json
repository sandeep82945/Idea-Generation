{
    "abstractText": "Exploiting label correlations is important to multi-label classification. Previous methods capture the high-order label correlations mainly by transforming the label matrix to a latent label space with low-rank matrix factorization. However, the label matrix is generally a full-rank or approximate full-rank matrix, making the low-rank factorization inappropriate. Besides, in the latent space, the label correlations will become implicit. To this end, we propose a simple yet effective method to depict the high-order label correlations explicitly, and at the same time maintain the high-rank of the label matrix. Moreover, we estimate the label correlations and infer model parameters simultaneously via the local geometric structure of the input to achieve mutual enhancement. Comparative studies over twelve benchmark data sets validate the effectiveness of the proposed algorithm in multi-label classification. The exploited high-order label correlations are consistent with common sense empirically. Our code is publicly available at https://github.com/Chongjie-Si/HOMI.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chongjie Si"
        },
        {
            "affiliations": [],
            "name": "Yuheng Jia"
        },
        {
            "affiliations": [],
            "name": "Yanghe Feng"
        },
        {
            "affiliations": [],
            "name": "Chongxiao Qu"
        }
    ],
    "id": "SP:119742b2651a939e11fa7ec9bbed6fd632f75c3a",
    "references": [
        {
            "authors": [
                "T.N. Rubin",
                "A. Chambers",
                "P. Smyth",
                "M. Steyvers"
            ],
            "title": "Statistical topic models for multi-label document classification",
            "venue": "Machine learning, vol. 88, no. 1-2, pp. 157\u2013208, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "F. Sun",
                "J. Tang",
                "H. Li",
                "G.-J. Qi",
                "T.S. Huang"
            ],
            "title": "Multi-label image categorization with sparse factor representation",
            "venue": "IEEE Transactions on Image Processing, vol. 23, no. 3, pp. 1028\u20131037, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Wang",
                "G. Sukthankar"
            ],
            "title": "Multi-label relational neighbor classification using social context features",
            "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 464\u2013472.",
            "year": 2013
        },
        {
            "authors": [
                "R. Wang",
                "S. Kwong",
                "X. Wang",
                "Y. Jia"
            ],
            "title": "Active k-labelsets ensemble for multi-label classification",
            "venue": "Pattern Recognition, vol. 109, p. 107583, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.R. Boutell",
                "J. Luo",
                "X. Shen",
                "C.M. Brown"
            ],
            "title": "Learning multilabel scene classification",
            "venue": "Pattern recognition, vol. 37, no. 9, pp. 1757\u20131771, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "M.-L. Zhang",
                "L. Wu"
            ],
            "title": "Lift: Multi-label learning with labelspecific features",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 1, pp. 107\u2013120, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Read",
                "B. Pfahringer",
                "G. Holmes",
                "E. Frank"
            ],
            "title": "Classifier chains for multi-label classification",
            "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "G. Tsoumakas",
                "I. Katakis",
                "I. Vlahavas"
            ],
            "title": "Random k-labelsets for multilabel classification",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 23, no. 7, pp. 1079\u20131089, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "K. Punera",
                "S. Rajan",
                "J. Ghosh"
            ],
            "title": "Automatically learning document taxonomies for hierarchical classification",
            "venue": "Special interest tracks and posters of the 14th international conference on World Wide Web, 2005, pp. 1010\u20131011.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Zhu",
                "J.T. Kwok",
                "Z.-H. Zhou"
            ],
            "title": "Multi-label learning with global and local label correlation",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 6, pp. 1081\u20131094, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Feng",
                "B. An",
                "S. He"
            ],
            "title": "Collaboration based multi-label learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, 2019, pp. 3550\u20133557.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Jia",
                "H. Liu",
                "J. Hou",
                "S. Kwong",
                "Q. Zhang"
            ],
            "title": "Multi-view spectral clustering tailored tensor low-rank representation",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 12, pp. 4784\u20134797, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.K. Valadi",
                "P.T. Ovhal",
                "K.J. Rathore"
            ],
            "title": "A simple method of solution for multi-label feature selection",
            "venue": "2019 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT), 2019, pp. 1\u20134.",
            "year": 2019
        },
        {
            "authors": [
                "Z.-H. Zhou",
                "M.-L. Zhang"
            ],
            "title": "Multi-instance multi-label learning with application to scene classification",
            "venue": "Advances in neural information processing systems, 2006, pp. 1609\u20131616.",
            "year": 2006
        },
        {
            "authors": [
                "L. Yong",
                "T. Liu",
                "D. Tao",
                "X. Chao"
            ],
            "title": "Multi-view matrix completion for multi-label image classification",
            "venue": "IEEE Transactions on Image Processing, vol. 24, no. 8, pp. 2355\u20132368, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Wang",
                "X. Zhou",
                "T.S. Chua"
            ],
            "title": "Automatic image annotation via local multi-label classification",
            "venue": "ACM, 2008, p. 17.",
            "year": 2008
        },
        {
            "authors": [
                "C. Wang",
                "S. Yan",
                "Z. Lei",
                "H.J. Zhang"
            ],
            "title": "Multi-label sparse coding for automatic image annotation",
            "venue": "IEEE Computer Society Conference on Computer Vision Pattern Recognition, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "W. Fei",
                "Y. Han",
                "T. Qi",
                "Y. Zhuang"
            ],
            "title": "Multi-label boosting for image annotation by structural grouping sparsity",
            "venue": "Proceedings of the 18th International Conference on Multimedea 2010, Firenze, Italy, October 25-29, 2010, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S. Feng",
                "D. Xu"
            ],
            "title": "Transductive multi-instance multi-label learning algorithm with application to automatic image annotation",
            "venue": "Expert Systems with Applications, vol. 37, no. 1, pp. 661\u2013670, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "X. Tao",
                "Y. Li",
                "R. Lau",
                "W. Hua"
            ],
            "title": "Unsupervised multi-label text classification using a world knowledge ontology",
            "venue": "Proceedings of the 16th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining - Volume Part I, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "K.M. Ozonat",
                "D. Young"
            ],
            "title": "Towards a universal marketplace over the web: Statistical multi-label classification of service provider forms with simulated annealing.",
            "year": 2009
        },
        {
            "authors": [
                "B. Parlak",
                "A.K. Uysal"
            ],
            "title": "Classification of medical documents according to diseases",
            "venue": "2015 23nd Signal Processing and Communications Applications Conference (SIU), 2015, pp. 1635\u20131638.",
            "year": 2015
        },
        {
            "authors": [
                "B. Wu",
                "E. Zhong",
                "A. Horner",
                "Q. Yang"
            ],
            "title": "Music emotion recognition by multi-label multi-layer multi-instance multi-view learning",
            "venue": "Proceedings of the 22nd ACM international conference on Multimedia, 2014, pp. 117\u2013126.",
            "year": 2014
        },
        {
            "authors": [
                "H.-Y. Lo",
                "J.-C. Wang",
                "H.-M. Wang",
                "S.-D. Lin"
            ],
            "title": "Cost-sensitive stacking for audio tag annotation and retrieval",
            "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2011, pp. 2308\u20132311.",
            "year": 2011
        },
        {
            "authors": [
                "F. Pachet",
                "P. Roy"
            ],
            "title": "Improving multilabel analysis of music titles: A large-scale validation of the correction approach",
            "venue": "IEEE Transactions on Audio Speech & Language Processing, vol. 17, no. 2, pp. 335\u2013343, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Wieczorkowska",
                "P. Synak",
                "Z.W. Ra"
            ],
            "title": "Multi-label classification of emotions in music",
            "venue": "Intelligent Information Processing and Web Mining, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Y. Yang",
                "S. Gopal"
            ],
            "title": "Multilabel classification with meta-level features in a learning-to-rank framework",
            "venue": "Machine Learning, vol. 88, no. 1-2, pp. 47\u201368, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "H. Elghazel",
                "A. Aussem",
                "O. Gharroudi",
                "W. Saadaoui"
            ],
            "title": "Ensemble multi-label text categorization based on rotation forest and latent semantic indexing",
            "venue": "Expert Systems With Applications, vol. 57, no. Sep., pp. 1\u201311, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "M.L. Zhang",
                "K. Zhang"
            ],
            "title": "Multi-label learning by exploiting label dependency",
            "venue": "ACM, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M.-L. Zhang",
                "Z.-H. Zhou"
            ],
            "title": "Ml-knn: A lazy learning approach to multi-label learning",
            "venue": "Pattern recognition, vol. 40, no. 7, pp. 2038\u2013 2048, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. F\u00fcrnkranz",
                "E. H\u00fcllermeier",
                "E. Loza?Menc\u0131\u0301a",
                "K. Brinker"
            ],
            "title": "Multilabel classification via calibrated label ranking",
            "venue": "Machine Learning, vol. 73, no. 2, pp. 133\u2013153, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Jia",
                "H. Liu",
                "J. Hou",
                "S. Kwong"
            ],
            "title": "Pairwise constraint propagation with dual adversarial manifold regularization",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 12, pp. 5575\u20135587, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Wang",
                "J. Xie",
                "L. Yu",
                "X. Tao"
            ],
            "title": "Ml-lrc: Low-rank-constraintbased multi-label learning with label noise",
            "venue": "2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC), vol. 1, 2020, pp. 129\u2013136.",
            "year": 2020
        },
        {
            "authors": [
                "L. Xu",
                "Z. Wang",
                "Z. Shen",
                "Y. Wang",
                "E. Chen"
            ],
            "title": "Learning lowrank label correlations for multi-label classification with missing labels",
            "venue": "2014 IEEE international conference on data mining. IEEE, 2014, pp. 1067\u20131072.",
            "year": 2014
        },
        {
            "authors": [
                "H.-F. Yu",
                "P. Jain",
                "P. Kar",
                "I. Dhillon"
            ],
            "title": "Large-scale multi-label learning with missing labels",
            "venue": "International conference on machine learning. PMLR, 2014, pp. 593\u2013601.",
            "year": 2014
        },
        {
            "authors": [
                "K.H. Huang",
                "H.T. Lin"
            ],
            "title": "Cost-sensitive label embedding for multi-label classification",
            "venue": "Machine Learning, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.-K. Yeh",
                "W.-C. Wu",
                "W.-J. Ko",
                "Y.-C.F. Wang"
            ],
            "title": "Learning deep latent space for multi-label classification",
            "venue": "Thirty-first AAAI conference on artificial intelligence, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Ciss\u00e9",
                "M. Al-Shedivat",
                "S. Bengio"
            ],
            "title": "Adios: Architectures deep in output space",
            "venue": "International Conference on Machine Learning. PMLR, 2016, pp. 2770\u20132779.",
            "year": 2016
        },
        {
            "authors": [
                "J. Wang",
                "Y. Yang",
                "J. Mao",
                "Z. Huang",
                "C. Huang",
                "W. Xu"
            ],
            "title": "Cnnrnn: A unified framework for multi-label image classification",
            "venue": "JOURNAL OF LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13 in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2285\u20132294.",
            "year": 2015
        },
        {
            "authors": [
                "J. Nam",
                "E. Loza Menc\u0131\u0301a",
                "H.J. Kim",
                "J. F\u00fcrnkranz"
            ],
            "title": "Maximizing subset accuracy with recurrent neural networks in multi-label classification",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "E. Elhamifar"
            ],
            "title": "High-rank matrix completion and clustering under self-expressive models",
            "venue": "Advances in Neural Information Processing Systems, vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "G. Golub",
                "S. Nash",
                "C. Van Loan"
            ],
            "title": "A hessenberg-schur method for the problem ax + xb= c",
            "venue": "IEEE Transactions on Automatic Control, vol. 24, no. 6, pp. 909\u2013913, 1979.",
            "year": 1979
        },
        {
            "authors": [
                "Z.-B. Yu",
                "M.-L. Zhang"
            ],
            "title": "Multi-label classification with labelspecific feature generation: A wrapped approach",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Sun",
                "M. Kudo",
                "K. Kimura"
            ],
            "title": "Multi-label classification with meta-label-specific features",
            "venue": "2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2016, pp. 1612\u20131617.",
            "year": 2016
        },
        {
            "authors": [
                "J. Ma",
                "H. Zhang",
                "T.W.S. Chow"
            ],
            "title": "Multilabel classification with label-specific features and classifiers: A coarse- and fine-tuned framework",
            "venue": "IEEE Transactions on Cybernetics, vol. 51, no. 2, pp. 1028\u20131042, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.-L. Zhang",
                "J.-P. Fang",
                "Y.-B. Wang"
            ],
            "title": "Bilabel-specific features for multi-label classification",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 16, no. 1, pp. 1\u201323, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "P. Li",
                "X. Hu",
                "K. Yu"
            ],
            "title": "Learning common and labelspecific features for multi-label classification with correlation information",
            "venue": "Pattern Recognition, vol. 121, p. 108259, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Demiar",
                "D. Schuurmans"
            ],
            "title": "Statistical comparisons of classifiers over multiple data sets",
            "venue": "Journal of Machine Learning Research, vol. 7, no. 1, pp. 1\u201330, 2006.",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014High-rank matrix approximation, high-order label correlations, multi-label classification.\n\u2726\n1 INTRODUCTION\nR ECENTLY, multi-label classification has attracted a lotof attention, aiming to solve real-world tasks with rich semantics [1], [2], [3], [4]. Specifically, in multi-label classification, one instance may be associated with several labels. For example, an image may be associated with a set of tags [5], and a piece of news may belong to several topics. Different from the traditional single-label classification problem which can be regarded as a degenerated version of multilabel classification, the overwhelming size of output space makes multi-label classification a much more challenging task.\n\u2022 This work was supported in part by the National Natural Science Foundation of China under Grant 62106044, 62176160 and 62225602, in part by the Natural Science Foundation of Jiangsu Province under Grant BK20210221, in part by the Guangdong Basic and Applied Basic Research Foundation (Grant 2022A1515010791), in part by ZhiShan Youth Scholar Program from Southeast University under Grant 2242022R40015, and in part by the Hong Kong UGC under grant UGC/FDS11/E02/22. (Corresponding author: Yuheng Jia.) \u2022 C. Si is with the Chien-Shiung Wu College, Southeast University, Nanjing 210096, China, and also with the MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai 200240, China. E-mail: chongjiesi@sjtu.edu.cn. \u2022 Y. Jia is with the School of Computer Science and Engineering, Southeast University, Nanjing 210096, and with the Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, and also with School of Computing & Information Sciences, Caritas Institute of Higher Education, Hong Kong. E-mail: yhjia@seu.edu.cn. \u2022 M. Zhang is with the School of Computer Science and Engineering, Southeast University, Nanjing 210096, China, and also with the Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China e-mail: zhangml@seu.edu.cn. \u2022 R. Wang is with the School of Mathematical Science, Shenzhen University, Shenzhen 518060, China, and also with Shenzhen Key Laboratory of Advanced Machine Learning and Applications, Shenzhen University, Shenzhen 518060, China. E-mail: wangran@szu.edu.cn. \u2022 Y. Feng is with the College of Systems Engineering, National University of Defense Technology.E-mail: fengyanghe@nudt.edu.cn. \u2022 C. Qu is with the 52nd Research Institute of China Electronics Technology Group.E-mail:quchongxiao@163.com. Exploiting label correlations is of great importance, as label correlations can provide valuable semantic relationship for the output of multi-label classification. For instance, if two labels, \u201crainforest\u201d and \u201csoccer\u201d are assigned to a sample, then the label \u201cBrazil\u201d may be also assigned to it. Similarly, if \u201cteacher\u201d and \u201cblackboard\u201d are present, it is very likely that label \u201cclassroom\u201d will also be present. Based on how to explore the label correlations, multi-label classification methods can be roughly divided into three families: first-order, second-order and high-order. For firstorder methods, the label correlations are not considered. For example, binary relevance (BR) [5] transformed multi-label classification into a set of independent binary classification problems and solved them separately. The second-order methods take the pairwise relationship of labels into considerations. For example, multi-label classification with Label specIfic FeaTures (LIFT) [6] employed clustering techniques to find second-order correlations between labels. However, in real-world scenarios, the label correlations may be much more complex than first-order and second-order relations. To this end, many high-order label correlations exploiting methods were proposed. For example, classifier chains (CC) [7] transformed the multi-label classification into a chain of binary classification problems. Random-k-labelsets (RAKEL) [8] converted multi-label classification into a set of multi-class classification problems over k randomly-chosen class labels. Some approaches assumed the labels were correlated in a hierarchical structure [9]. All the mentioned approaches specify the high-order correlations of labels manually, which will depress the classification performance if the manual setting is inappropriate. Recently, some high-order approaches were proposed to automatically explore the high-order label correlations [10], [11]. They generally decomposed the label matrix to a latent space by low-rank matrix factorization [12], and then assumed the latent labels may capture the higher level ar X iv :2 20 7. 04 19 7v 2 [ cs\n.L G\n] 6\nN ov\n2 02\n3\nsemantic concepts [13]. However, as can be seen from Table 1, the rank of the label matrix usually equals to or approximately equals to the number of labels, which means the label matrix is full-rank or approximate full-rank, making the low-rank matrix assumption inappropriate. Besides, in the latent space, the label correlations become indirect and semantically unclear.\nTo solve these issues, in this paper, we propose a novel approach called HOMI, with High-Rank and High-Order MultI-Label learning. Specifically, we argue that if a label is highly correlated to a set of other labels, it can be easily reconstructed by the linear combination of that set of labels. Therefore, we propose to use self-representation to exploit the high-order label correlations for multi-label classification. Note that it can keep the rank of the label matrix unchanged and indicate the high-order correlations among labels explicitly. Moreover, the local geometric structure is also beneficial to multi-label classification, as if two samples are similar to each other in the feature space, they are likely to share similar labels. Here, we adopt an snearest-neighbors (SNN) graph to depict the local geometric structure of the input samples and incorporate the local geometric structure by a graph Laplacian regularization. Besides, the proposed approach naturally unifies high-order label correlations learning and multi-label prediction into a joint model via the graph Laplacian regularization, such that those two separate processes can be well interacted with each other to achieve mutual enhancement. Comprehensive experiments substantiate that HOMI outperforms the state-of-the-art multi-label classification methods significantly, and reasonable high-order label correlations can be constructed by HOMI.\nThe rest of the paper is organized as follows. We first review some related works in exploiting label correlations and explain why the label matrix should be full-rank or approximate full-rank in multi-label classification in Section 2. Then, we introduce the proposed approach as well as the numerical solution in Section 3, and present the experimental results and analysis in Section 4. Finally, conclusion is given in Section 5.\n2 RELATED WORK\n2.1 Exploiting Label Correlations in Multi-label Classification\nIn multi-label classification, an instance is associated with a set of labels. In recent years, this new machine learning paradigm has made great progress and has been widely used in image classification [14], [15], [16], automatic annotation [17], [18], [19], web mining [20], [21], [22], audio recognition [23], [24], [25], [26] and information retrieval [27], [28], etc.\nHowever, the task of inducing multi-label classification functions is challenging, as the classifier\u2019s output space is exponential in size to the number of possible class labels, i.e., |2Y |, where Y denotes the number of possible labels. A useful way to cope with this issue is to exploit label correlations to simplify the learning procedure. Based on the degree of label correlations used, the algorithms of multilabel classification can be divided into three categories [29]: first-order, second-order and high-order.\nFirst-order methods do not take label correlations into consideration and assume that all the labels are independent. BR [5] is a prevailing first-order approach, transforming the original multi-label classification into a set of independent binary classification tasks. ML-KNN [30] is also a popular first-order algorithm based on k-nearest-neighbour classification. The major advantage of first-order approaches is the conceptual simplicity and high efficiency, for they are easy to understand and operate. Nevertheless, they ignore the label correlations, which results in performance degeneration.\nSecond-order approaches focus on pairwise label relations. For instance, calibrated label ranking (CLR) [31] and LIFT [6] are two representative approaches, transforming original multi-label classification into pairwise ranking problems. Second-order approaches are relatively more effective than first-order ones in exploiting label correlations. However, in real-world applications, the relationship of labels may be quite complex and sophisticated, such that the pairwise relationship cannot describe the real-world label correlations very well.\nHigh-order approaches aim to dig the high-order label correlations. CC [7], for instance, converted the multilabel task into a chain of independent binary classification problems, with the ground-truth labels decoded into the features each time. RAKEL [8] reformulated the multi-label classification into several sets of multi-class classification tasks.\nRecently, some low-rank based approaches proposed to learn high-order label correlations based on the assumption that the label matrix is low-rank, for there are correlations among labels in multi-label classification. For example, Zhu et al. [10] used low-rank decomposition in multi-label learning, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds [32]. Wang et al. [33] controlled the sparsity of the coefficient matrix to filter out labelspecific features and applied low-rank constraints to the label matrix to mine the local correlations of class labels. Xu et al. [34] proposed an integrated framework that learns the correlations among labels while training the multi-label model simultaneously, and specifically adopted a low-rank structure to capture the complex correlations among labels. Moreover, Yu et al. [35] proposed an approach learning a linear instance-to-label mapping with low-rank structure, and implicitly taking advantage of global label correlations.\nThe general strategy of the above mentioned methods to capture label semantics is to decompose the label matrix to a latent label space by low-rank matrix factorization, [36], [37]. Specifically, denote the label matrix Y \u2208 Rn\u00d7l, with n and l being the number of samples and labels, respectively. The low-rank based approaches usually decompose Y into two smaller matrices U and V, i.e., Y can be written as the product\nmin UV\n\u2225Y \u2212UV\u22252F , (1)\nwhere V \u2208 Rh\u00d7l is the latent label matrix exploiting higher level label semantics, and U \u2208 Rn\u00d7h is a basis matrix relating the original labels to the latent labels. As h is smaller\nthan l and n, the rank of UV is smaller than Y, i.e., Eq. (1) approximates the label matrix by low-rank factorization.\n2.2 Why is the label matrix full-rank?\nHowever, we believe that the label matrix is full-rank or approximately full-rank, and accordingly the low-rank matrix assumption is not the best choice for multi-label learning. The reasons are as follows. First, the size of the label matrix Y is n \u00d7 l, as l \u226a n in general, the rank of Y is very likely to be or close to l. Second, although there are some connections among labels, the connections are usually not determinated. For example, if label A is related to label B, in other words, if A appears on a sample, we can infer that B has high possibility to be also appeared on that sample. But we cannot conclude that B will absolutely appear, as there will always be samples that only have label A or B. Therefore, those connections cannot reduce the rank of the label matrix, and likewise cannot result in a low-rank label matrix. Last but not least, as shown in the Table 1, the commonly used multi-label data sets are always full-rank or approximately full-rank, which further empirically validates that the label matrix of multi-label classification should be high-rank rather than low-rank.\nAs a summary, the label matrix in multi-label learning is usually a full-rank matrix, which cannot be well approximated by low-rank decomposition. Besides, in the latent space, the label correlations become indirect and semantically unclear. To solve these issues, in the next section, a new approach named HOMI is proposed, which can keep the rank of the label matrix unchanged, and indicate label correlations directly in the label space.\n2.3 Deep-learning Based Multi-label Classification\nDue to its robust learning capability, deep learning has emerged as an important technique for achieving multilabel classification. In those methods, effectively leveraging deep learning is critical for capturing intricate label dependencies. To exploit the underlying intricate label structure, Cisse et al. (2016) proposed ADIOS [38], which employs a novel deep architecture that partitions the labels into a Markov blanket chain, capitalizing on this partition to enhance classification performance. Wang et al. (2016) introduced CNN-RNN [39], which leverages recurrent neural networks (RNNs) to better model higher-order label dependencies. CNN-RNN learns a unified image-label embedding that encapsulates both semantic label dependencies and image-label relevance. Notably, this approach enables endto-end training from scratch. Moreover, Nam et al. proposed an alternative technique to the traditional classifier chain method [40]. Their approach employs RNNs to convert the MLC problem into a sequential prediction task, with initially arbitrary label ordering. This method offers the advantage\nof focusing on predicting positive labels exclusively, significantly reducing the prediction space compared to the complete set of labels.\nThese methodologies exemplify diverse strategies for harnessing deep learning in MLC, each addressing label dependencies through distinctive means. However, different from these methods, HOMI explicitly reveals the label dependency based on the full-rank assumption, which further improves the MLC problem.\n3 THE PROPOSED APPROACH Motivated by the fact that the label matrix is generally fullrank, in this section, we introduce HOMI to exploit the highorder label correlations for multi-label classification. Prior to that, we first briefly summarize the notations used in this paper. Formally, let X = Rm denote the m-dimensional feature space and Y = {c1, c2, .., cl} denote the label space of l labels, where ci \u2208 {0, 1} stands for the ith label, multi-label classification learns a function f : X \u2192 2Y from the training data set D = {(xi,yi)|1 \u2264 i \u2264 n}, where xi stands for the ith instance and yi stands for the corresponding label set, and n is the number of instances. Let X = [x1,x2, ...,xn]T \u2208 Rn\u00d7m denote the instance matrix and Y = [y1,y2, ...,yn]T \u2208 {0, 1}n\u00d7l denote the label matrix with l labels. Note that the value of the labels is binary, and we have yi = [yi1, yi2, ..., yil] where yij = 1 (resp. yij = 0) if the sample xi has (resp. does not have) the jth label.\n3.1 Basic Model\nFirst, HOMI uses a weight matrix W = [w1,w2, ...,wl] \u2208 Rm\u00d7l to map the instance to the labels by minimizing the following least squares loss:\nmin W,z\n\u2225Y \u2212XW \u2212 1nzT\u22252F + \u03bb(\u2225W\u22252F + \u2225z\u222522), (2)\nwhere \u2225 \u00b7 \u2225F and \u2225 \u00b7 \u22252 stand for the Frobenius norm and \u21132 norm of a matrix and a vector, respectively. z = [z1, z2, ..., zl]\nT \u2208 Rl is the bias term and 1n \u2208 Rn is an all one vector. The first term in Eq. (2) measures the predictive error of the model, and the second term is the regularization of the weight matrix W and the bias z, trying to control the complexity of the whole model, and \u03bb \u2265 0 is a hyperparameter to balance those two terms.\n3.2 Exploiting High-Order Label Information\nAs mentioned earlier, if a label is highly correlated with other labels, it can be easily reconstructed by those labels. Thus, we propose to adopt the self-representation strategy to dig the high-order correlations between labels, which can be mathematically formulated as\nmin B,t\n\u2225Y \u2212YB\u2212 1ntT\u22252F + \u03bb(\u2225B\u22252F + \u2225t\u222522), (3)\nwhere B \u2208 Rl\u00d7l records the high-order label correlations, and similar to Eq. (2), t \u2208 Rl is the bias term for better selfregression. We also introduce an additional penalty term on B and t to avoid the trivial solution (i.e., B = I with I being an identity matrix) and over-fitting.\nThe previous methods use low-rank matrix factorization to decompose the label matrix Y to a latent space to exploit the high-order correlations, however, the label matrix is usually a full-rank matrix. Technically, this is because although correlations exist in labels, the correlated labels also have a chance to exist alone on some samples. Table 1 also empirically verifies this observation. The full-rank property of the label matrix makes the low-rank factorization-based methods unreasonable. Differently, the adopted self-representation approach can keep the rank of label matrix unchanged [41]. Moreover, in the latent space, the label correlations can only be captured implicitly, while on the contrary, the elements in B can explicitly indicate the correlations between two labels.\n3.3 Incorporating Local Geometric Structure\nHOMI also takes the local geometric structure of instances into consideration, i.e., if xi is similar to xj , the predictive label sets of them may have some labels in common. Specifically, we first calculate the Pearson correlation coefficient of samples, i.e.,\nRij = Cov(xi,xj)\n\u03c3xi\u03c3xj ,\nwhere Cov(xi,xj) is the covariance of xi and xj , and \u03c3xi is the standard deviation of xi. Afterwards, we construct an s-nearest-neighbors graph to capture the local geometric structure of the input, i.e.,\nSij = { Rij if(i, j) \u2208 Nsi 0 otherwise , (4)\nwhere Nsi is the set of s nearest instances of the ith instance, and we choose the ones with the top s values of Rij of the ith sample as its neighbors. Then, the local geometric structure is incorporated by minimizing\u2211\ni,j\nSij\u2225g(xi)\u2212 g(xj)\u2225 2 2 = tr(K TLK), (5)\nwhere L = Q \u2212 S is the graph Laplacian matrix with Q being the diagonal degree matrix of S. In order to learn the local structural information more reasonably, we make L symmetric, i.e., L = 12 (L + L\nT). g is the discriminative function, i.e. g(xi) = (xiW + z\nT)B + tT, which takes the high-order label correlations into account. K = [g(x1),g(x2), ...,g(xn)]\nT \u2208 Rn\u00d7l (i.e. K = (XW + 1nz T)B + 1nt T). If Eq. (5) is minimized, two similar instances will have similar predictive label sets.\n3.4 Model Formulation Based on the above discussion, the objective function of HOMI is finally formulated as\nmin W,B,z,t\n\u2225Y \u2212XW \u2212 1nzT\u22252F + \u03b3tr(KTLK)\n+ \u03b2\u2225Y \u2212YB\u2212 1ntT\u22252F + \u03bb(\u2225W\u22252F + \u2225z\u222522 + \u2225B\u22252F + \u2225t\u222522),\n(6)\nwhere \u03b3, \u03b2, and \u03bb denote different hyper-parameters to balance different terms. As will be illustrated in the experiments, HOMI is quite robust to those hyper-parameters. It is also worth pointing out that HOMI integrates high-order correlations exploitation and model prediction into a joint model via the graph Laplacian regularization term. It is able to simultaneously enhance those two processes via the joint learning.\n3.5 Prediction For an unseen instance x, the discriminative function g of HOMI is obtained by\ng(x) = (xW + zT)B+ tT, (7)\nand the predictive label set is obtained by\nypre = f(x) = {ci|gi(x) > 0.5, 1 \u2264 i \u2264 l}, (8)\nwhere gi(x) is the ith element of g(x).\n3.6 Numerical Solution to Eq. (6) The problem in Eq. (6) is not convex in all the variables together, but it is convex to each variable with the remaining variables fixed. Therefore, we solved it by the following alternating optimization procedure. Update W With fixed B, z and t, Eq. (6) becomes:\nmin W\n\u2225Y \u2212XW \u2212 1nzT\u22252F + \u03b3tr(KTLK) + \u03bb\u2225W\u22252F . (9)\nTaking the gradient of Eq. (9) w.r.t. W, we have\n\u2207W = XT(XW + 1nzT \u2212Y) + \u03bbW +\u03b3XTL(XWB+ 1nz TB+ 1nt T)BT.\n(10)\nThe optimal solution of Eq. (9) is achieved when \u2207W = 0, and accordingly we have the following Sylvester equation\nAW +WE = Q (11)\nwhere A = 1\u03b3 (X TLX)\u22121(XTX + \u03bbI), E = BBT,\nQ = 1\u03b3 (X TLX)\u22121(XTY \u2212 XT1nzT \u2212 \u03b3XTL1ntTBT \u2212\n\u03b3XTL1nz TBBT), which can be efficiently solved according\nto [42]. Update B Fixing W,z and t, the B-subproblem becomes\nmin B\n\u03b2\u2225Y \u2212YB\u2212 1ntT\u22252F + \u03b3tr(KTLK) + \u03bb\u2225B\u22252F ,\nwhich is a quadratic optimization problem, and the solution is obtained by setting its derivative to 0, i.e.,\nB =(\u03b2YTY + \u03bbI+ \u03b3(WTXT + z1n T)L(XW + 1nz T))\u22121\n(\u03b2YTY \u2212 \u03b2YT1ntT \u2212 \u03b3(WTXT + z1nT)L1ntT). (12)\nUpdate z With fixed W, B and t, the z-subproblem is rewritten as\nmin z\n\u2225Y \u2212XW \u2212 1nzT\u22252F + \u03b3tr(KTLK) + \u03bb\u2225z\u222522,\nwhich is also a quadratic optimization problem, and the optimal solution is achieved when the derivative approaches zero, i.e.,\nz = ((n+ \u03bb)I+ \u03b31n TL1nBB T)\u22121(YT1n \u2212WTXT1n \u2212 \u03b3B(BTWTXT + t1nT)L1n).\n(13)\nUpdate t With other variables fixed, the t-subproblem is reformulated as\nmin t\n\u03b2\u2225Y \u2212YB\u2212 1ntT\u22252F + \u03b3tr(KTLK) + \u03bb\u2225t\u222522,\nwhich is also a quadratic optimization problem, and the solution of it is achieved when \u2207t = 0, i.e.,\nt = 1\n(\u03b2n+ \u03b31nTL1n + \u03bb) (\u03b2YT1n\n\u2212 \u03b2BTYT1n \u2212 \u03b3BT(WTXT + z1nT)L1n). (14)\nIn summary, HOMI first randomly initialize W,B, z, t, and then iteratively and alternatively update these four variables. The iteration stops when the difference between two consecutive loss is less than 0.001. Finally the label set ypre for an unseen x is predicted according to Eq. (8). The whole schedule is concluded in algorithm 1.\nAlgorithm 1 HOMI.\nRequire: Training data set D; s; \u03b2; \u03b3; \u03bb, max iteration number iter; an unseen instance x.\nEnsure: Predicted label set ypre for x. 1: Initialize W,B, z, t to 0; 2: Calculate L by Eqs. (4)-(5); 3: Repeat: 4: Update W according to Eq. (11); 5: Update B according to Eq. (12); 6: Update z according to Eq. (13); 7: Update t according to Eq. (14); 8: If number of iteration \u2265 iter: break 9: Until Convergence;\n10: return ypre according to Eq. (8).\n3.7 Complexity Analysis\nHOMI iteratively solves four optimization problems. To solve W, HOMI needs to solve a Sylvester equation, which can be computed in O(max(m3, l3,mn2,m2n,mnl)); the rest three optimization problems are all quadratic optimization problems with the complexity of O(max(l2n, lmn, l3)), O(max(l3, lmn, n2)) and O(max(l3, lmn, n2)). In summary, the overall complexity of HOMI is O(max(m3, l3,mn2,m2n,mnl)) in one iteration.\n3.8 Convergence Analysis\nThe proposed numerical solution in Algorithm 1 is guaranteed to converge theoretically. Specifically, define the optimal function in Eq. (6) as F , the problem in Eq. (6) is not convex in all the variables together, but it is convex to each variable with the remaining variables fixed. Therefore, we solved it by an alternating optimization procedure. Specifically, we transform the original problem into four subproblems, where each subproblems can be solved efficiently. We need to minimize F (W,B, z, t) with four variables W,B, z and t. We transform the original problem into four subproblems minW F (W,B, z, t), minB F (W,B, z, t), minz F (W,B, z, t) and mint F (W,B, z, t) and solve them alternatively and iteratively. When solving the Wsubproblem minW F (Wk\u22121,Bk\u22121, zk\u22121, tk\u22121) at the kth iteration, the variables Bk\u22121, zk\u22121, tk\u22121 are fixed, and we try to find optimal Wk to minimize the corresponding function value. It is obvious that F (Wk\u22121,Bk\u22121, zk\u22121, tk\u22121) \u2265 F (Wk,Bk\u22121, zk\u22121, tk\u22121). Similarly, when solving the B-subproblem minB F (Wk,Bk\u22121, zk\u22121, tk\u22121) at the kth iteration, the variables Wk, zk\u22121, tk\u22121 are fixed, and F (Wk,Bk\u22121, zk\u22121, tk\u22121) \u2265 F (Wk,Bk, zk\u22121, tk\u22121), and the same to zk and tk. Therefore, we can get F (Wk\u22121,Bk\u22121, zk\u22121, tk\u22121) \u2265 F (Wk,Bk, zk, tk), i.e., in each iteration, the value of the loss function is decreased. As the loss function has a lower bound (F \u2265 0), the above alternating algorithm will surely be converged.\n4 EXPERIMENTS\n4.1 Data Sets\nIn this section, comparative studies were conducted on twelve commonly used benchmark multi-label data sets. Table 2 summarizes the detailed characteristics of each data set D, with the number of examples (n), the dimension of features (m), the number of class labels (l), label cardinality, i.e., average number of relevant labels per example (LCard(D)), label density, i.e. label cardinality over the number of class labels (LDen(D)), and number of distinct label sets (DL(D)) in D. Those data sets are publicly available at https://mulan.sourceforge.net/datasets-mlc.html\n4.2 Compared Methods\nWe compared HOMI with the following ten state-of-the-art multi-label classification approaches.\n\u2022 ECC (Ensemble of classifier chains) [7] is an ensemble-based multi-label classification approach, building an ensemble of N classifier chains to solve multi-label classification. [hyper-parameter configuration: N = 5];\n\u2022 BR [5] is a classical algorithm in multi-label classification, trying to decompose the original multi-label classification task into a set of binary classification tasks. [hyper-parameter C = 1];\n\u2022 ML-KNN [30] is a popular first-order multi-label learning algorithm based on k-nearest-neighbour classification. [hyper-parameter configuration: k=10];\nlabel classification. [hyper-parameter configuration: step size = 1, \u03bb = 0.1, \u03b1 = 0.9, d = \u03b1min(m, l)];\n\u2022 MLSF [44] generates label-specific features by analyzing local and global feature-to-label correlations. [hyper-parameter configuration: K = l/10, \u03f5 = 0.01, \u03b1 = 0.8, \u03b3 = 0.01];\n\u2022 LFLC [45] generates label tailored features by analyzing local and global feature-to-label correlations. [hyper-parameter configuration: grid search for \u03bb \u2208 {1, 3, ..., 19} with step-size 2, \u03b7 \u2208 {1e\u221210, ..., 1e\u22125} with a multiple of e at each step, \u03b2 = 104];\n\u2022 BILAS [46] generates a group of tailored features for a pair of class labels with heuristic prototype selection and embedding. [hyper-parameter configuration: t=0.1, ratio=0.5];\n\u2022 GLOCAL [10] uses low-rank factorization to dig the global and local label correlations at the same time, through learning a latent label representation and optimizing label manifolds. [hyper-parameter configuration: \u03bb = 1];\n\u2022 ML-LRC [33] is a low-rank approach applying low-rank constraints to the label matrix to mine the local correlations of class labels. [hyperparameter configuration: grid search for \u03b1, \u03b2 \u2208 {2\u221210, 2\u22129, 2\u22128, ..., 210}, \u03b3 = 0.1 and \u03c4 = 0.5];\n\u2022 CLML [47] is a approach that learns common and label-specific features based on the correlation information from labels and instances. [hyper-parameter configuration: grid search for \u03b1, \u03b2, \u03bb1, \u03bb2 \u2208 {2\u221210, 2\u22129, 2\u22128, ..., 210} with step 22].\nIn brief, BR [5] and ML-KNN [30] belong to first-order approaches, BILAS [46] is a second-order approach, and WRAP [43], ECC [7], GLOCAL [10] and CLML [47] are high-order approaches. Particularly, GLOCAL and ML-LRC [33] are two low-rank based approaches. MLSF [44] and LFLC [45] are two label-specific approaches. The hyper-parameter configurations for different methods are suggested by their original papers.\n4.3 Experimental Settings\nThe hyper parameters of HOMI are set as follows: \u03b2 = 2, \u03b3 = 1, \u03bb = 1, iter = 100 and s = 101. Following [43], fivefold cross-validation is performed on each data set, with mean metric and standard deviation recorded.\n4.4 Evaluation Metrics\nLet C+i , C \u2212 i be the sets of positive and negative labels corresponding to the ith instance, and T+i , T \u2212 i be the sets of positive and negative instances corresponding to the ith label, p is the number of the test instances. We chose the following four popular metrics to evaluate the performance of the proposed method and the methods under comparison.\n\u2022 Hamming loss (Hloss) evaluates the rate of the mistook labels. Hloss = 1p \u2211 i f(xi)\u2206yi, \u2206 stands for the\nsymmetric difference between two sets, i.e., a\u2206b = 1 (resp. 0) if a = b (resp. a \u0338= b).\n1. For emotions and bibtex, s is set to be 2.\n\u2022 Ranking loss (Rloss) calculates the fraction that a negative label is ranked higher than a positive label. Specifically, for instance i, suppose Mi = {(j\u2032 , j\u2032\u2032)|gj\u2032 (xi) \u2264 gj\u2032\u2032 (xi), (j \u2032 , j \u2032\u2032 ) \u2208 C+i \u00d7 C \u2212 i },\nRloss = 1p \u2211p i=1 |Mi|\n|C+i \u2225C \u2212 i | . \u2022 One-error evaluates the fraction of examples whose\ntop-ranked label is not in the relevant label set. Oneerror = 1p \u2211p i=1[cargmaxjfj(xi) /\u2208 yi]. \u2022 Average Area Under the ROC Curve (Macroaveraging AUC) denotes the fraction that a positive instance is ranked higher than a negative instance averaged over all labels. Suppose Ni = {(i\u2032 , i\u2032\u2032)|gj(xi\u2032 ) \u2265 gj(xi\u2032\u2032 ), (xi\u2032 , xj\u2032\u2032 \u2208 T + j \u00d7 T \u2212 j )},\nMacro-averaging AUC = 1l \u2211l j=1 |Ni|\n|T+i \u2225T \u2212 i |\n.\nFor hamming loss, ranking loss and one-error, the lower the better, while for macro-averaging AUC, the higher the better. All the metrics lie in the range of [0, 1].\n4.5 Experimental Analysis\nTables 3-6 show the experimental results of the proposed method and the compared baselines on twelve data sets with respect to four metrics. Additionally, the widely-used Friedman test [48] is used for statistical analysis of the performance among all the methods on the benchmark data sets. Suppose k denotes the number of comparing algorithms, N denotes the number of data sets and rji denotes the rank of the jth approach on the ith data set. Suppose Rj = 1N \u2211N i=1 r j i denotes the average rank of the jth method on all the data sets. The Friedman statistic FF , which is distributed according to the F -distribution with (k \u2212 1) numerator degrees of freedom and (k \u2212 1)(N \u2212 1) denominator degrees of freedom, is defined as\nFF = (N \u2212 1)X 2F\nN(k \u2212 1)\u2212X 2F ,\nwhere\nX 2F = 12N\nk(k + 1) ( k\u2211 j=1 R2j \u2212 k(k + 1)2 4 ).\nTable 7 reports the detailed statistics over all evaluation metrics as well as the related critical value at 0.05 significance level for HOMI (k = 11, N = 12). We can observe that the FF value is larger than the critical value w.r.t. all evaluation metrics. Therefore, the null hypothesis of equal performance among comparing approaches is clearly rejected.\nIn order to verify whether HOMI significantly outperforms other algorithms, we employ Holm\u2019s procedure [48] as\nthe post-hoc test by treating HOMI as the control approach. Without loss of generality, we take HOMI as the first comparing approach A1, and for the other k\u2212 1 approaches, we let Aj (2 \u2264 j \u2264 k) denote the one with the (j \u2212 1)th largest average rank. Then, the test statistic for comparing A1 and Aj is defined as follows:\nzj = (R1 \u2212Rj)/\n\u221a k(k + 1)\n6N (2 \u2264 j \u2264 k),\nAccordingly, let pj denote the p-value of zj under normal distribution, and the Holm\u2019s procedure sequentially checks whether pj is below \u03b1/(k \u2212 j + 1) in ascending order of j at significance level \u03b1. Specifically, the Holm\u2019s procedure is supposed to terminate at j\u2217 where j\u2217 is the first j that satisfying pj \u2265 \u03b1/(k \u2212 j + 1)2. Then HOMI is deemed to perform significantly different compared with Aj where j \u2208 {2, ..., j\u2217 \u2212 1}.\nTable 8 reports the statistics by taking Holm\u2019s procedure as post-hoc test at 0.05 significance level, where HOMI is treated as the control approach. We can have the following observations based on the experimental results:\n\u2022 HOMI performs better than all the first-order and second-order methods. For example, HOMI performs significantly better than BR according to Table 8. It performs nearly 2 times better than BR on bibtex in average with respect to hamming loss, 1.6 times better than BILAS with respect to macro-averaging AUC on CAL500, and more than 6 times better than ML-KNN on bibtex with respect to one-error, which validates the importance of involving highorder information.\n\u2022 HOMI also significantly outperforms all the highorder methods including the low-rank based approaches. Specifically, the improvements of HOMI over GLOCAL and ML-LRC are significant according to Table 8, which are two state-of-the-art multilabel classification methods based on low-rank factorization. For example, HOMI performs nearly 2 times better than GLOCAL on corel16k001 regarding macro-averaging AUC and nearly 4 times better w.r.t. ranking loss on bibtex. It outperforms ML-LRC more than 2 times on CAL500 w.r.t. ranking loss. This observation verifies the rationality of our basic assumption that the label matrix of multi-label classification should be high-rank rather than low-rank.\n\u2022 LFLC and BILAS perform well on scene, while they are not apt to deal with enron and bibtex. Besides, LFLC performs well on emotions, but does not on mediamill. However, HOMI is adept in almost all kinds of data set especially on text and image data sets, which validates the robustness of HOMI to different types of data sets.\n\u2022 HOMI is also robust to different evaluation metrics compared with the other approaches. HOMI achieves especially outstanding performance on macro-averaging AUC, for it enables positive labels to rank higher than negative ones effectively.\n2. If pj < \u03b1/(k \u2212 j + 1) holds for all j, j\u2217 is set to be k + 1.\n\u2022 The performance of WRAP and LFLC seems comparable to HOME with respect to the Hamming Loss and the One-error, while HOMI is obvious superior to the other methods with respect to the Macro-averaging AUC. The reason is that HOMI uses self-representation to exploit the high-order label correlations while keeping the label matrix fullrank, leading to more effective label-wise discrimination and aggregation. Therefore, HOMI outperforms the other two methods significantly w.r.t. Macroaveraging AUC.\n\u2022 In general, HOMI performs superior or at least comparable to the other algorithms in 85%, 71.7%, 65.8%, 90.8% cases in terms of hamming loss, ranking loss, one-error and macro-averaging AUC which validates that HOMI is a promising approach in multi-label classification.\nThe success of HOMI is partially credited to the information of high-order label correlations exploited from label space, and partially credited to the incorporation of the local geometric structure of instances to achieve joint learning of high-order label correlations and model prediction.\n4.6 Further Analysis 4.6.1 High-order correlations exploited by HOMI HOMI exploits high-order information during the training process. In order to know what information HOMI has learned, we recorded the high-order correlation matrix B\ntrained on emotions. Emotions is a multi-label data set that describes the emotions of different music. It contains six common emotions, such as happy, relaxing and angry.\nFig. 1(a) shows the matrix B that HOMI learns. The value of each column represents the contribution of the corresponding row label to the column label. In order to better display the learned high-order label correlations, the matrix is normalized, and the relationships among labels extracted from B is visually represented in Fig. 1(b). It is obvious that the diagonal element of B is large, which means in prediction, a label should be dom-\ninated by itself. Moreover, it should also be influenced by correlated labels. We can clearly observe that label \u201crelaxing/calm\u201d and \u201cquiet/still\u201d are positively related, while \u201camazed/surprised\u201d and \u201csad/lonely\u201d are negatively correlated, and \u201chappy/pleased\u201d and \u201csad/lonely\u201d are negatively correlated, too. Additionally, label \u201cangry/aggressive\u201d has negative contribution to all the other labels except \u201camazed/surprised\u201d. Those label correlations learned by HOMI are consistent with common sense.\nWe can further verify the correctness and validity of the excavated higher-order information based on the Russell\u2019s emotion circumplex theory [49]. Russell thought that emotions can be measured in two dimensions, i.e., pleasure\u2013displeasure and degree-of-arousal. We show a simplified Russell\u2019s emotion circumplex in Fig. 1(c). We as-\nsume that the correlations between two emotions can be determined by their cosine similarity, i.e, if the similarity is larger than 0, the two emotions are positively connected and a larger cosine similarity means more correlation; ; otherwise, they are negatively correlated. From Fig. 1(c), it can be observed that \u201chappy/pleased\u201d and \u201csad/lonely\u201dare negatively related, while \u201cangry\u201d and \u201crelaxing/calm\u201d are negatively correlated, which is also learned by HOMI.\nWe can conclude that the correlations exploited by HOMI are reasonable as they are confirmed by both common sense and the Russell\u2019s emotion circumplex theory.\n4.6.2 Usefulness of High-order Information\nIn order to validate the effectiveness of using high-order information, we compared HOMI with its degenerated ver-\nsion that makes predictions without considering the highorder label correlations, i.e., g(x) = xW + 1nzT on all the twelve data sets with respect to hamming loss, ranking loss, one-error and macro-averaging AUC. The detailed comparisons are shown in Fig. 2.\nFrom Fig. 2 we can observe that the performance of HOMI with high-order information outperforms its degenerated version in 83.3%, with total 48 cases (12 data sets \u00d7 4 evaluation metrics). Especially on the data set bibtex with respect to the one-error, HOMI performs 5 times better than its degenerated version. As the matrix B captures the highorder correlations among labels, it is effective in improving prediction accuracy. We thus can conclude that considering high-order label information when making predictions enable HOMI to perform better and more stable in general.\n4.6.3 Effectiveness of Joint Learning\nHOMI exploits the high-order label correlations and make predictions in a joint manner. To show the effectiveness of joint learning, we also designed a compared method with a step-wise learning fashion that first learns high-order information by minimizing \u2225Y \u2212 YB \u2212 1ntT\u22252F and then makes predictions by (xW + 1nzT)B+ 1ntT.\nThe comparison between joint learning and step-wise learning is shown in Fig. 3, where we can observe that the joint learning manner outperforms the step-wise learning significantly. Specifically, the performance of joint learning model is relatively superior to step-wise learning in 85.4%, with total 48 cases (12 data sets \u00d7 4 evaluation metrics). Especially on the data set mediamill, the Ranking Loss of joint learning manner is 20 times lower than that of the stepwise one.\n4.6.4 Usefulness of Local Geometric Structure\nAiming to validate the effectiveness of local geometric structure, we evaluated the performance of HOMI without local geometric structure (i.e., \u03b3 = 0) in Fig. 4, where we can find that the local geometric structure is important to the proposed approach. For example, HOMI performs 13 times better than its simplified version on bibtex with respect to ranking loss. Generally, the one with local geometric structure significantly outperforms the one without that in 95.9% cases.\n4.6.5 Sensitivity Analysis\nIn Fig. 5, we investigate the sensitivity of HOMI with respect to \u03b2, \u03b3, \u03bb and the number of nearest instances (s) on emotions. It is evident that the performance of HOMI is\nrelatively stable and excellent as the value of the parameters change within a reasonable wide range, validating the robustness of HOMI to the hyper-parameters, which serves as a desirable property in practice.\n4.6.6 Convergence Analysis\nFig. 6 shows the convergence property of the proposed approach on twelve data sets where we can observe that the objective function decreases significantly and converges in about 50 iterations on nearly all the data sets.\n4.6.7 Running time\nWe also compare the running time of each algorithm on each data set, which is shown in Fig. 7. It is obvious that HOMI is usually faster than BILAS, WRAP and ECC. Additionally, the running speed of HOMI is also comparable to other methods, which is also a promising property in practice.\n5 CONCLUSION\nIn this paper, we have presented an effective multi-label classification approach. Different from the traditional lowrank based methods, we argue that the label matrix of multilabel classification is full-rank or approximately full-rank, and thus we propose to keep the rank of label matrix unchanged by self-representation. Moreover, by incorporating the local geometric structure of the input, the proposed method can simultaneously make predictions and exploit high-order label correlations. Extensive experiments validate the effectiveness of the proposed approach in learning high-order label correlations, and in incorporating the local geometric structure. As the proposed model can explicitly indicate the high-order label correlations, we have verified HOMI can learn meaningful label correlations. Besides, HOMI also significantly outperforms the state-of-the-art multi-label classification approaches, serving as a promising solution for multi-label classification. In the future, it is interesting to investigate how to extend HOMI to a nonlinear version.\nAPPENDIX A THE MATRIX B HOMI HAS LEARNED\nIn section 4.6.1, we have recorded the normalized matrix B trained by HOMI on emotions. The original value of the matrix B is shown in Fig. 8.\nREFERENCES\n[1] T. N. Rubin, A. Chambers, P. Smyth, and M. Steyvers, \u201cStatistical topic models for multi-label document classification,\u201d Machine learning, vol. 88, no. 1-2, pp. 157\u2013208, 2012. [2] F. Sun, J. Tang, H. Li, G.-J. Qi, and T. S. Huang, \u201cMulti-label image categorization with sparse factor representation,\u201d IEEE Transactions on Image Processing, vol. 23, no. 3, pp. 1028\u20131037, 2014. [3] X. Wang and G. Sukthankar, \u201cMulti-label relational neighbor classification using social context features,\u201d in Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 464\u2013472. [4] R. Wang, S. Kwong, X. Wang, and Y. Jia, \u201cActive k-labelsets ensemble for multi-label classification,\u201d Pattern Recognition, vol. 109, p. 107583, 2021. [5] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown, \u201cLearning multilabel scene classification,\u201d Pattern recognition, vol. 37, no. 9, pp. 1757\u20131771, 2004. [6] M.-L. Zhang and L. Wu, \u201cLift: Multi-label learning with labelspecific features,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 1, pp. 107\u2013120, 2015. [7] J. Read, B. Pfahringer, G. Holmes, and E. Frank, \u201cClassifier chains for multi-label classification,\u201d in Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, 2009. [8] G. Tsoumakas, I. Katakis, and I. Vlahavas, \u201cRandom k-labelsets for multilabel classification,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 23, no. 7, pp. 1079\u20131089, 2011. [9] K. Punera, S. Rajan, and J. Ghosh, \u201cAutomatically learning document taxonomies for hierarchical classification,\u201d in Special interest tracks and posters of the 14th international conference on World Wide Web, 2005, pp. 1010\u20131011. [10] Y. Zhu, J. T. Kwok, and Z.-H. Zhou, \u201cMulti-label learning with global and local label correlation,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 6, pp. 1081\u20131094, 2018. [11] L. Feng, B. An, and S. He, \u201cCollaboration based multi-label learning,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, 2019, pp. 3550\u20133557. [12] Y. Jia, H. Liu, J. Hou, S. Kwong, and Q. Zhang, \u201cMulti-view spectral clustering tailored tensor low-rank representation,\u201d IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 12, pp. 4784\u20134797, 2021. [13] J. K. Valadi, P. T. Ovhal, and K. J. Rathore, \u201cA simple method of solution for multi-label feature selection,\u201d in 2019 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT), 2019, pp. 1\u20134. [14] Z.-H. Zhou and M.-L. Zhang, \u201cMulti-instance multi-label learning with application to scene classification,\u201d in Advances in neural information processing systems, 2006, pp. 1609\u20131616. [15] L. Yong, T. Liu, D. Tao, and X. Chao, \u201cMulti-view matrix completion for multi-label image classification,\u201d IEEE Transactions on Image Processing, vol. 24, no. 8, pp. 2355\u20132368, 2015.\n[16] M. Wang, X. Zhou, and T. S. Chua, \u201cAutomatic image annotation via local multi-label classification,\u201d in ACM, 2008, p. 17. [17] C. Wang, S. Yan, Z. Lei, and H. J. Zhang, \u201cMulti-label sparse coding for automatic image annotation,\u201d in IEEE Computer Society Conference on Computer Vision Pattern Recognition, 2009. [18] W. Fei, Y. Han, T. Qi, and Y. Zhuang, \u201cMulti-label boosting for image annotation by structural grouping sparsity,\u201d in Proceedings of the 18th International Conference on Multimedea 2010, Firenze, Italy, October 25-29, 2010, 2010. [19] S. Feng and D. Xu, \u201cTransductive multi-instance multi-label learning algorithm with application to automatic image annotation,\u201d Expert Systems with Applications, vol. 37, no. 1, pp. 661\u2013670, 2010. [20] X. Tao, Y. Li, R. Lau, and W. Hua, \u201cUnsupervised multi-label text classification using a world knowledge ontology,\u201d in Proceedings of the 16th Pacific-Asia conference on Advances in Knowledge Discovery and Data Mining - Volume Part I, 2012. [21] K. M. Ozonat and D. Young, \u201cTowards a universal marketplace over the web: Statistical multi-label classification of service provider forms with simulated annealing.\u201d ACM, 2009. [22] B. Parlak and A. K. Uysal, \u201cClassification of medical documents according to diseases,\u201d in 2015 23nd Signal Processing and Communications Applications Conference (SIU), 2015, pp. 1635\u20131638. [23] B. Wu, E. Zhong, A. Horner, and Q. Yang, \u201cMusic emotion recognition by multi-label multi-layer multi-instance multi-view learning,\u201d in Proceedings of the 22nd ACM international conference on Multimedia, 2014, pp. 117\u2013126. [24] H.-Y. Lo, J.-C. Wang, H.-M. Wang, and S.-D. Lin, \u201cCost-sensitive stacking for audio tag annotation and retrieval,\u201d in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2011, pp. 2308\u20132311. [25] F. Pachet and P. Roy, \u201cImproving multilabel analysis of music titles: A large-scale validation of the correction approach,\u201d IEEE Transactions on Audio Speech & Language Processing, vol. 17, no. 2, pp. 335\u2013343, 2009. [26] A. Wieczorkowska, P. Synak, and Z. W. Ra, \u201cMulti-label classification of emotions in music,\u201d Intelligent Information Processing and Web Mining, 2006. [27] Y. Yang and S. Gopal, \u201cMultilabel classification with meta-level features in a learning-to-rank framework,\u201d Machine Learning, vol. 88, no. 1-2, pp. 47\u201368, 2012. [28] H. Elghazel, A. Aussem, O. Gharroudi, and W. Saadaoui, \u201cEnsemble multi-label text categorization based on rotation forest and latent semantic indexing,\u201d Expert Systems With Applications, vol. 57, no. Sep., pp. 1\u201311, 2016. [29] M. L. Zhang and K. Zhang, \u201cMulti-label learning by exploiting label dependency,\u201d ACM, 2010. [30] M.-L. Zhang and Z.-H. Zhou, \u201cMl-knn: A lazy learning approach to multi-label learning,\u201d Pattern recognition, vol. 40, no. 7, pp. 2038\u2013 2048, 2007. [31] J. Fu\u0308rnkranz, E. Hu\u0308llermeier, E. Loza?Menc\u0131\u0301a, and K. Brinker, \u201cMultilabel classification via calibrated label ranking,\u201d Machine Learning, vol. 73, no. 2, pp. 133\u2013153, 2008. [32] Y. Jia, H. Liu, J. Hou, and S. Kwong, \u201cPairwise constraint propagation with dual adversarial manifold regularization,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 12, pp. 5575\u20135587, 2020. [33] X. Wang, J. Xie, L. Yu, and X. Tao, \u201cMl-lrc: Low-rank-constraintbased multi-label learning with label noise,\u201d in 2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC), vol. 1, 2020, pp. 129\u2013136. [34] L. Xu, Z. Wang, Z. Shen, Y. Wang, and E. Chen, \u201cLearning lowrank label correlations for multi-label classification with missing labels,\u201d in 2014 IEEE international conference on data mining. IEEE, 2014, pp. 1067\u20131072. [35] H.-F. Yu, P. Jain, P. Kar, and I. Dhillon, \u201cLarge-scale multi-label learning with missing labels,\u201d in International conference on machine learning. PMLR, 2014, pp. 593\u2013601. [36] K. H. Huang and H. T. Lin, \u201cCost-sensitive label embedding for multi-label classification,\u201d Machine Learning, 2017. [37] C.-K. Yeh, W.-C. Wu, W.-J. Ko, and Y.-C. F. Wang, \u201cLearning deep latent space for multi-label classification,\u201d in Thirty-first AAAI conference on artificial intelligence, 2017. [38] M. Cisse\u0301, M. Al-Shedivat, and S. Bengio, \u201cAdios: Architectures deep in output space,\u201d in International Conference on Machine Learning. PMLR, 2016, pp. 2770\u20132779. [39] J. Wang, Y. Yang, J. Mao, Z. Huang, C. Huang, and W. Xu, \u201cCnnrnn: A unified framework for multi-label image classification,\u201d\nin Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2285\u20132294. [40] J. Nam, E. Loza Menc\u0131\u0301a, H. J. Kim, and J. Fu\u0308rnkranz, \u201cMaximizing subset accuracy with recurrent neural networks in multi-label classification,\u201d Advances in neural information processing systems, vol. 30, 2017. [41] E. Elhamifar, \u201cHigh-rank matrix completion and clustering under self-expressive models,\u201d Advances in Neural Information Processing Systems, vol. 29, 2016. [42] G. Golub, S. Nash, and C. Van Loan, \u201cA hessenberg-schur method for the problem ax + xb= c,\u201d IEEE Transactions on Automatic Control, vol. 24, no. 6, pp. 909\u2013913, 1979. [43] Z.-B. Yu and M.-L. Zhang, \u201cMulti-label classification with labelspecific feature generation: A wrapped approach,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [44] L. Sun, M. Kudo, and K. Kimura, \u201cMulti-label classification with meta-label-specific features,\u201d in 2016 23rd International Conference on Pattern Recognition (ICPR). IEEE, 2016, pp. 1612\u20131617. [45] J. Ma, H. Zhang, and T. W. S. Chow, \u201cMultilabel classification with label-specific features and classifiers: A coarse- and fine-tuned framework,\u201d IEEE Transactions on Cybernetics, vol. 51, no. 2, pp. 1028\u20131042, 2021. [46] M.-L. Zhang, J.-P. Fang, and Y.-B. Wang, \u201cBilabel-specific features for multi-label classification,\u201d ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 16, no. 1, pp. 1\u201323, 2021. [47] J. Li, P. Li, X. Hu, and K. Yu, \u201cLearning common and labelspecific features for multi-label classification with correlation information,\u201d Pattern Recognition, vol. 121, p. 108259, 2022. [48] J. Demiar and D. Schuurmans, \u201cStatistical comparisons of classifiers over multiple data sets,\u201d Journal of Machine Learning Research, vol. 7, no. 1, pp. 1\u201330, 2006. [49] J. A. Russell, \u201cA circumplex model of affect.\u201d Journal of personality and social psychology, vol. 39, no. 6, p. 1161, 1980.\nChongjie Si received the B.S. degree in artificial intelligence from Chien-Shiung Wu College, Southeast University, Nanjing, China, in 2022. He is currently a Ph.D. student at the Artificial Intelligence Institute, Shanghai Jiao Tong University, Shanghai, China. His current research interests lie in machine learning, incremental learning and semantic segmentation.\nYuheng Jia (Member, IEEE) received the B.S. degree in automation and the M.S. degree in control theory and engineering from Zhengzhou University, Zhengzhou, China, in 2012 and 2015, respectively, and the Ph.D. degree in computer science from the City University of Hong Kong, Hong Kong, China, in 2019. He is currently an Associate Professor with the School of Computer Science and Engineering, Southeast University, Nanjing, China. His research interests broadly include topics in machine learning\nand data representation, such as weakly-supervised learning, highdimensional data modeling and analysis, and low-rank tensor/matrix approximation and factorization.\nRan Wang (Senior Member, IEEE) received her B.Eng. degree in computer science from the College of Information Science and Technology, Beijing Forestry University, Beijing, China, in 2009, and the Ph.D. degree from the Department of Computer Science, City University of Hong Kong, China, in 2014. From 2014 to 2016, she was a Postdoctoral Researcher at the Department of Computer Science, City University of Hong Kong. She is currently an Associate Professor at the School of Mathematical Science,\nShenzhen University, China. Her current research interests include machine learning, pattern recognition, uncertainty modeling, fuzzy sets and fuzzy logic, and their related applications.\nMin-Ling Zhang (Senior Member, IEEE) received the BSc, MSc, and PhD degrees in computer science from Nanjing University, China, in 2001, 2004 and 2007, respectively. Currently, he is a Professor at the School of Computer Science and Engineering, Southeast University, China. His main research interests include machine learning and data mining. In recent years, Dr. Zhang has served as the General Co-Chairs of ACML\u201918, Program Co-Chairs of PAKDD\u201919, CCF-ICAI\u201919, ACML\u201917, CCFAI\u201917, PRICAI\u201916,\nSenior PC member or Area Chair of AAAI 2022-2024, IJCAI 2017- 2023, KDD 2021-2023, ICDM 2015-2022, etc. He is also on the editorial board of IEEE Transactions on Pattern Analysis and Machine Intelligence, ACM Transactions on Intelligent Systems and Technology, Science China Information Sciences, Frontiers of Computer Science, Machine Intelligence Research, etc. Dr. Zhang is the Steering Committee Member of ACML and PAKDD, Vice Chair of the CAAI Machine Learning Society. He is a Distinguished Member of CCF, CAAI, and Senior Member of AAAI, ACM, IEEE.\nYanghe Feng received the master\u2019s and Ph.D. degrees from the Information System and Engineering Laboratory, National University of Defense Technology. He is currently an Associate Professor with the National University of Defense Technology. Before joining the National University of Defense Technology, he was a Visiting Scholar and a Research Assistant Professor with The University of Iowa and Havard University. His Ph.D. research focused on building \u201cThe plan online and learn offline\u201d framework to\nenable computers with the ability to analyze, recognize, and predict real-world uncertainty. His primary research interests include casual discovery and inference, active learning, and reinforcement learning. He has published a lot of influential papers in top-tier journals and conferences, e.g., IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Cybernetics, IEEE Transactions on Communications, and so on. He was awarded the young distinguished research scientist of China Command and Control Society. He has been serving as the vice secretary of the Intelligent Computing Subcommittee of the Operational Research Society of China and (co-)editing several international journals.\nChongxiao Qu is with the 52nd Research Institute of China Electronics Technology Group Corporation, Hangzhou, P.R. China. His research interests lie in deep learning (artificial intelligence), learning (artificial intelligence), autonomous aerial vehicles, computer based training, computer vision, cooperative systems, decision making, image classification, image segmentation, military computing, multi-agent systems, multirobot systems, etc."
        }
    ],
    "title": "Multi-label Classification with High-rank and High-order Label Correlations",
    "year": 2023
}