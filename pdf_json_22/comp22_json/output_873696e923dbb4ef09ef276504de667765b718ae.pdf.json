{
    "abstractText": "We present an elementary yet general proof of duality for Wasserstein distributionally robust optimization. The duality holds for any arbitrary Kantorovich transport cost, measurable loss function, and nominal probability distribution, provided that an interchangeability principle holds, which is equivalent to certain measurability conditions. To illustrate the broader applicability of our approach, we provide a rigorous treatment of duality results in distributionally robust Markov decision processes and distributionally robust multistage stochastic programming. Furthermore, we extend the result to other problems including infinity-Wasserstein distributionally robust optimization, risk-averse optimization, and globalized distributionally robust counterpart.",
    "authors": [
        {
            "affiliations": [],
            "name": "Luhao Zhang"
        },
        {
            "affiliations": [],
            "name": "Jincheng Yang"
        },
        {
            "affiliations": [],
            "name": "Rui Gao"
        }
    ],
    "id": "SP:c2f89241b56d54550ab09667c957a4ce7819e08c",
    "references": [
        {
            "authors": [
                "CD Aliprantis",
                "K Border"
            ],
            "title": "Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide (Springer Science & Business Media)",
            "year": 2006
        },
        {
            "authors": [
                "L Ambrosio",
                "N Fusco",
                "D Pallara"
            ],
            "title": "Functions of bounded variation and free discontinuity problems (Oxford",
            "year": 2000
        },
        {
            "authors": [
                "JP Aubin",
                "H Frankowska"
            ],
            "title": "Set-valued analysis (Springer Science & Business Media)",
            "year": 2009
        },
        {
            "authors": [
                "DP Bertsekas",
                "SE Shreve"
            ],
            "title": "Stochastic optimal control: the discrete-time case, volume 5 (Athena Scientific)",
            "year": 1996
        },
        {
            "authors": [
                "J Blanchet",
                "K Murthy"
            ],
            "title": "Quantifying distributional model risk via optimal transport",
            "venue": "Mathematics of Operations Research",
            "year": 2019
        },
        {
            "authors": [
                "J Blanchet",
                "K Murthy",
                "VA Nguyen"
            ],
            "title": "Statistical analysis of Wasserstein distributionally robust estimators. Tutorials in Operations Research: Emerging OptimizationMethods andModeling Techniques with Applications, 227\u2013254 (INFORMS)",
            "year": 2021
        },
        {
            "authors": [
                "C Castaing",
                "M Valadier"
            ],
            "title": "Measurable multifunctions. Convex Analysis and Measurable Multifunctions",
            "year": 1977
        },
        {
            "authors": [
                "Z Chen",
                "D Kuhn",
                "W Wiesemann"
            ],
            "title": "On approximations of data-driven chance constrained programs over Wasserstein balls",
            "venue": "Operations Research Letters 51(3):226\u2013233",
            "year": 2023
        },
        {
            "authors": [
                "H F\u00f6llmer",
                "A Schied"
            ],
            "title": "Convex and coherent risk measures. Encyclopedia of Quantitative Finance 355\u2013363",
            "year": 2010
        },
        {
            "authors": [
                "R Gao",
                "A Kleywegt"
            ],
            "title": "Distributionally robust stochastic optimization with wasserstein distance. Mathematics of Operations Research 48(2):603\u2013655",
            "year": 2023
        },
        {
            "authors": [
                "O Kallenberg"
            ],
            "title": "Foundations of modern probability, volume 2 (Springer)",
            "year": 1997
        },
        {
            "authors": [
                "D Kuhn",
                "PM Esfahani",
                "VA Nguyen",
                "S Shafieezadeh-Abadeh"
            ],
            "title": "Wasserstein distributionally robust optimization: Theory and applications in machine learning",
            "venue": "Operations Research & Management Science in the Age of Analytics,",
            "year": 2019
        },
        {
            "authors": [
                "F Liu",
                "Z Chen",
                "S Wang"
            ],
            "title": "Globalized distributionally robust counterpart",
            "venue": "INFORMS Journal on Computing",
            "year": 2023
        },
        {
            "authors": [
                "P Mohajerin Esfahani",
                "D Kuhn"
            ],
            "title": "Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations",
            "venue": "Mathematical Programming",
            "year": 2018
        },
        {
            "authors": [
                "RT Rockafellar"
            ],
            "title": "Convex analysis. Number 28 in Princeton Mathematical Series (Princeton university press)",
            "year": 1970
        },
        {
            "authors": [
                "RT Rockafellar",
                "S Uryasev"
            ],
            "title": "Optimization of conditional value-at-risk",
            "venue": "Journal of risk",
            "year": 2000
        },
        {
            "authors": [
                "A Shapiro"
            ],
            "title": "On duality theory of conic linear problems",
            "venue": "Semi-infinite programming,",
            "year": 2001
        },
        {
            "authors": [
                "A Shapiro"
            ],
            "title": "Interchangeability principle and dynamic equations in risk averse stochastic programming. Operations Research Letters 45(4):377\u2013381",
            "year": 2017
        },
        {
            "authors": [
                "A Shapiro",
                "D Dentcheva",
                "Ruszczynski"
            ],
            "title": "A (2021) Lectures on stochastic programming: modeling and theory (SIAM)",
            "year": 2021
        },
        {
            "authors": [
                "SE Shreve",
                "DP Bertsekas"
            ],
            "title": "Universally measurable policies in dynamic programming.Mathematics of Operations Research 4(1):15\u201330",
            "year": 1979
        },
        {
            "authors": [
                "A Sinha",
                "H Namkoong",
                "J Duchi"
            ],
            "title": "Certifying some distributional robustness with principled adversarial training",
            "venue": "International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "J Wang",
                "R Gao",
                "H Zha"
            ],
            "title": "Reliable off-policy evaluation for reinforcement learning.Operations Research Forthcoming",
            "year": 2022
        },
        {
            "authors": [
                "W Xie"
            ],
            "title": "On distributionally robust chance constrained programs with Wasserstein distance",
            "venue": "Mathematical Programming",
            "year": 2021
        },
        {
            "authors": [
                "I Yang"
            ],
            "title": "A convex optimization approach to distributionally robust markov decision processes with Wasserstein distance. IEEE control systems letters 1(1):164\u2013169",
            "year": 2017
        },
        {
            "authors": [
                "Z Yang",
                "R Gao"
            ],
            "title": "Wasserstein Regularization for 0-1 Loss. Optimization Online preprint",
            "year": 2022
        },
        {
            "authors": [
                "C Zhao",
                "Y Guan"
            ],
            "title": "Data-driven risk-averse stochastic optimization with Wasserstein metric. Operations Research Letters 46(2):262\u2013267",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 5.\n00 36\n2v 3\n[ m\nat h.\nO C\n] 3\n1 D\nec 2\n02 3\nA Simple and General Duality Proof for\nWasserstein Distributionally Robust Optimization\nLuhao Zhang Department of Industrial Engineering and Operations Research, Columbia University in the City of New York\nlz2487@columbia.edu\nJincheng Yang Department of Mathematics, University of Chicago\njincheng@uchicago.edu\nRui Gao Department of Information, Risk and Operations Management, The Unversity of Texas at Austin\nrui.gao@mccombs.utexas.edu\nWe present an elementary yet general proof of duality for Wasserstein distributionally robust optimization. The duality holds for any arbitrary Kantorovich transport cost, measurable loss function, and nominal probability distribution, provided that an interchangeability principle holds, which is equivalent to certain measurability conditions. To illustrate the broader applicability of our approach, we provide a rigorous treatment of duality results in distributionally robust Markov decision processes and distributionally robust multistage stochastic programming. Furthermore, we extend the result to other problems including infinity-Wasserstein distributionally robust optimization, risk-averse optimization, and globalized distributionally robust counterpart.\nKey words: Wasserstein metric, distributionally robust optimization, duality\n1. Introduction\nIn this paper, we consider the following problem\nL(d) := sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) \u2264 d } , (P)\nwhere d \u2208 [0,\u221e), P (X ) is the set of all probability distributions on a data space X , 5 : X \u2192 \u211d is a loss function, - is a random variable on X having a nominal distribution \u2119\u0302, and K2 denotes the Kantorovich transport cost, defined as\nK2 (\u2119\u0302,\u2119) = inf W\u2208\u0393(\u2119\u0302,\u2119) E (-\u0302,-)\u223cW\n[ 2( -\u0302, -) ] , (1)\nwhere \u0393(\u2119\u0302,\u2119) denotes the set of all probability distributions on X \u00d7X with marginals \u2119\u0302 and \u2119, and 2 :X \u00d7X \u2192 [0,\u221e] is a transport cost function. Note that K2 is a distance on P (X ) when 2 is a metric. The function L represents the robust loss hedging against deviations of data within d-neighborhood of the nominal distribution \u2119\u0302. When 2 = 3 ?, where 3 is a metric on X and ? \u2208 [1,\u221e), problem (P) is the inner worst-case problem in ?-Wasserstein distributionally robust optimization, which has raised much interest recently; see [12, 6] for tutorials.\nA central question of interest is to establish the dual problem for (P). Existing duality proofs have relied on advanced conic duality of the problem of moments [19] and imposed unnecessary technical conditions [15, 28, 29], or have involved lengthy analysis for the sake of generality [5, 10]. In this paper, we present a novel proof that is both elementary and concise, while yielding results that are evenmore general than those found in existing literature. A detailed comparison can be found in Table 1, along with a discussion after Theorem 1. Furthermore, we provide examples that demonstrate the\n1\n2\nbroader applicability of our approach, not only recovering existing results but also offering valuable implications in other distributionally robust problems.\nThe remainder of this paper is structured as follows. In Section 2, we present our main proof. The key technique of our proof is to view the robust loss as a function of the radius of the uncertainty set and then apply the Legendre transform to it twice. The concavity of the robust loss enables us to establish strong duality directly through the Legendre transformation. This is valid when (and only when) a certain interchangeability principle is satisfied. In Section 3, we delve into this condition and provide an equivalent condition, which has a strong connection to the measurable projection theorem and the measurable selection theorem. In Section 5, we extend our results to other problems, including infinity-Wasserstein distributionally robust optimization, risk-averse optimization, and the globalized distributionally robust counterparts. Finally, we conclude the paper in Section 6.\n2. Model Formulation and Main Result\nIn this section, we state and prove our main duality results under general assumptions. Proofs of auxiliary lemmas are provided in Appendix A.\n2.1. Main Assumptions\nNotations. We denote by \u211d\u0304 :=\u211d\u222a {\u00b1\u221e} the extended reals and adopt the convention that 0 \u00b7 \u221e =\u221e. For a probability space (X ,\u2131, \u2119\u0302), we say an extended real-valued function on (X ,\u2131) is measurable if it is (\u2131,\u212c(\u211d\u0304))-measurable, where \u212c(\u211d\u0304) is the Borel f-algebra on \u211d\u0304, and we say an extended real-valued function on X is \u2119\u0302-measurable if it is measurable with respect to the completion of \u2131 under the measure \u2119\u0302 [2, Definition 1.11]. We allow the expectation to take values in \u211d\u0304 \u2013 recall that the integration of a measurable function under a measure is well-defined whenever the positive part or the negative part of the integrand has a finite integral. When X is a metric space, we denote by 3 :X \u00d7X \u2192 [0,\u221e) its metric. Let P\u0304 (X ) denote the set of probability measures \u2119 on (X ,\u2131) satisfying K2 (\u2119\u0302,\u2119) < \u221e. For a function \u210e : \u211d \u2192 \u211d \u222a {+\u221e}, we denote by \u210e\n\u2217 : \u211d \u2192 \u211d \u222a {+\u221e} its Legendre transform \u210e\u2217 (_) := supd\u2208\u211d{_d \u2212 \u210e(d)}. If \u210e :\u211d\u2192 \u211d\u0304 attains \u2212\u221e somewhere, then \u210e \u2217 \u2261 +\u221e.\nIn addition to the problem (P), we also study its soft-penalty counterpart\nsup \u2119\u2208P\u0304\n{ E-\u223c\u2119 [ 5 (-)] \u2212_K2 (\u2119\u0302,\u2119) } , (P-soft)\nwhere _ \u2208 [0,\u221e).\nWe assume the following situation.\nAssumption 1. Let (X ,\u2131, \u2119\u0302) be a probability space, 5 :X \u2192\u211d be a measurable function with E \u2119\u0302 [ 5 ] > \u2212\u221e, and 2 :X \u00d7X \u2192 [0,\u221e] be a measurable transport cost function satisfying 2(G, G) = 0 for all G \u2208X .\nThe following lemma shows some useful properties of the worst-case loss L(\u00b7) defined in (P).\nLemma 1. Assume Assumption 1 holds. Then L(\u00b7) is lower bounded by E \u2119\u0302 [ 5 ], monotonically increasing, and concave on [0,\u221e).\n3 As we will see, the interchangeability principle discussed next is essential for strong duality, being both a necessary and sufficient condition. This principle is often mentioned in stochastic programming literature, as seen in references like [21, Section 9.3.4] and [20]. It facilitates the swapping of expectation and minimization operators in our analysis.\nInterchangability Principle (IP) We say an (\u2131 \u2297\u2131)-measurable function q : X \u00d7X \u2192\u211d\u222a {\u2212\u221e} satisfies the interchangeability principle if the function G\u0302 \u21a6\u2192 supG\u2208X q(G\u0302, G) is \u2119\u0302-measurable and it holds that\nE -\u0302\u223c\u2119\u0302 [ sup G\u2208X q( -\u0302, G) ] = sup W\u2208\u0393\n\u2119\u0302\nE (-\u0302,-)\u223cW [q( -\u0302, -)],\nwhere \u0393 \u2119\u0302 is the set of probability distributions on (X \u00d7X ,\u2131 \u2297\u2131) with first marginal \u2119\u0302.\n2.2. Main Result and its Proof\nUsing Lemma 1, we derive the dual of (P) as follows.\nTheorem 1. Assume Assumption 1 holds. Let _, d > 0. Then (P-soft) is equivalent to\n(\u2212L)\u2217(\u2212_) = sup W\u2208\u0393\n\u2119\u0302\nE (-\u0302,-)\u223cW\n[ 5 (-) \u2212_2( -\u0302, -) ] ,\nand (P) is equivalent to\nL(d) =min _\u22650\n{ _d + sup\nW\u2208\u0393 \u2119\u0302\n{ E (-\u0302,-)\u223cW [ 5 (-) \u2212_2( -\u0302 , -) ]}} .\nIn addition, for _ > 0, if and only if the function q_(G\u0302, G) := 5 (G) \u2212_2(G\u0302, G) satisfies (IP), it holds that\n(\u2212L)\u2217(\u2212_) = E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212_2( -\u0302, G) }] ,\nand if and only if q_ satisfies (IP) for every _ > 0, it holds that\nL(d) =min _\u22650\n{ _d +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212_2( -\u0302, G) }]} , \u2200d > 0. (D)\nRemark 1 (?-Wasserstein distance). Recall that the ?-Wasserstein distance W? (\u2119\u0302,\u2119), ? \u2208 [1,\u221e), is defined by\nW? (\u2119\u0302,\u2119) = inf W\u2208\u0393(\u2119\u0302,\u2119) \u20163\u2016!? (X \u00d7X ;W) = inf W\u2208\u0393(\u2119\u0302,\u2119) E (-\u0302,-)\u223cW\n[3 ( -\u0302, -) ?] 1 ? .\nBy setting 2(G\u0302, G) = 3 (G\u0302, G) ? we have K2 (\u2119\u0302,\u2119) = W ? ? (\u2119\u0302,\u2119). Thereby (D) corresponds to the dual formulation of the ?-Wasserstein DRO\nL(d?) =min _\u22650\n{ _d? +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (-) \u2212_3 ( -\u0302, G) ? }]} .\nWe will handle the case ? =\u221e separately in Section 5.1. \u2666\nRemark 2 (Necessity of (IP)). The second part of Theorem 1 discusses the necessity of the interchangeability principle (IP). As will be elaborated on in the next section, it ensures the measurability of the supremum function in (D) and the existence of approximately worst-case distributions. To our knowledge, (IP) is weaker than the assumptions in all existing results that enable the expression (D). \u2666\n4 Remark 3 (Continuity at d = 0). In general, (D) does not hold at d = 0. Indeed, the right-hand side of (D) is continuous in d \u2208 [0,\u221e), but L(d) may be not right-continuous at 0. For instance, if X =\u211d, 2(G\u0302, G) = |G\u0302 \u2212 G |, 5 (G) = 1{G \u2260 0}, and \u2119\u0302 = 0, the Dirac measure at 0, then L(d) = 1 for any d > 0 and L(0) = 0. A sufficient condition ensuring the right-continuity of L(d) at 0 is the following: there exists a continuous concave function i : [0,\u221e) \u2192 [0,\u221e) with i(0) = 0 such that 5 (G) \u2212 5 (G\u0302) \u2264 i \u25e6 2(G\u0302, G) for all G \u2208 X and \u2119\u0302-a.e. G\u0302 \u2208 X with 2(G\u0302, G) <\u221e. Indeed, under this condition, for any n > 0 and \u2119 \u2208 P (X ) with K2 (\u2119\u0302,\u2119) \u2264 n , there exists a W \u2208 \u0393\u2119\u0302 such that EW [2( -\u0302 , -)] \u2264 2n , hence EW [ 5 (-) \u2212 5 ( -\u0302)] \u2264 EW [i \u25e6 2] \u2264 i(EW [2]) by Jensen\u2019s inequality. Therefore, L(n) \u2264 E\u2119\u0302[ 5 ] + i(EW [2]) \u2264 L(0) + i(EW [2]), which converges to L(0) as n \u2192 0. When 2 = 3 ?, where 3 is a metric on X , this condition is related to the growth condition imposed in [10] and the upper semi-continuity of 5 . \u2666\nProof of Theorem 1. Fix _ > 0. By definition, L(d) = \u2212\u221e for d < 0. Taking the Legendre transform of \u2212L(\u00b7) gives that\n(\u2212L)\u2217(\u2212_) = sup d\u22650 {(\u2212_)d \u2212 (\u2212L(d))}\n= sup d\u22650 {L(d) \u2212_d}\n= sup d\u22650 sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] \u2212_d : K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X ) sup d\u22650\n{ E-\u223c\u2119 [ 5 (-)] \u2212_d : K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P\u0304\n{ E-\u223c\u2119 [ 5 (-)] \u2212_K2 (\u2119\u0302,\u2119) } ,\nwhich gives (P-soft). Using the definition of the Kantorovich transport cost (1), it follows that\n(\u2212L)\u2217(\u2212_) = sup \u2119\u2208P\u0304\n{ E-\u223c\u2119 [ 5 (-)] \u2212_K2 (\u2119\u0302,\u2119) }\n= sup \u2119\u2208P\u0304\n{ E-\u223c\u2119 [ 5 (-)] \u2212_ inf\nW\u2208\u0393(\u2119\u0302,\u2119)\nE (-\u0302,-)\u223cW [2( -\u0302 , -)]\n}\n= sup \u2119\u2208P\u0304 ,W\u2208\u0393(\u2119\u0302,\u2119)\n{ E-\u223c\u2119 [ 5 (-)] \u2212_E(-\u0302,-)\u223cW [2( -\u0302, -)] : EW [2] <\u221e }\n= sup W\u2208\u0393\n\u2119\u0302\nE (-\u0302,-)\u223cW\n[ 5 (-) \u2212_2( -\u0302, -) ] .\nObserve that (\u2212L)\u2217(\u2212_) \u2265 supd\u22650{(\u2212_)d +L(0)} = +\u221e for _ < 0. From Lemma 1 we have that L(\u00b7) is bounded from below, increasing and concave in [0,\u221e), so either L(d) < +\u221e for every d \u2265 0 or L(d) = +\u221e for every d > 0. In the former case, by the involution property of Legendre transform (see, e.g., [16, Theorem 12.2]), (\u2212L)\u2217\u2217 equals to the lower semi-continuous convex envelope of \u2212L. Since L(\u00b7) is concave in [0,\u221e), \u2212L(d) = \u2212L\u2217\u2217 (d) on the interior of the set {d \u2208 \u211d : \u2212L(d) < +\u221e}, which is (0,\u221e). Hence, for every d > 0,\nL(d) =\u2212(\u2212L)\u2217\u2217(d) =\u2212max _\u22650 {(\u2212_)d \u2212 (\u2212L)\u2217(\u2212_)}\n=min _\u22650\n{_d + (\u2212L)\u2217(\u2212_)}\n=min _\u22650\n{ _d + sup\nW\u2208\u0393 \u2119\u0302\n{ E (-\u0302,-)\u223cW [ 5 (-) \u2212_2( -\u0302, -) ]}} .\nNote that we switched from supremum to maximum because (\u2212L)\u2217(\u2212_) is lower semi-continuous and bounded from below for _ \u2265 0 by Lemma 2. Therefore, (\u2212_)d\u2212 (\u2212L)\u2217(\u2212_) can be arbitrarily small as\n5 _\u2192+\u221e, hence the maximum is attainable. In the latter case, (\u2212L)\u2217(\u2212_) = +\u221e for any _ \u2208 \u211d, so the above is also true. This proves the first part of the theorem. For the second part, note that for _ > 0, q_ = 5 \u2212_2 satisfies (IP) means\n(\u2212L)\u2217(\u2212_) = sup W\u2208\u0393\n\u2119\u0302\n{ E (-\u0302,-)\u223cW [ 5 (-) \u2212_2( -\u0302 , -)] } = E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212_2( -\u0302 , G) }] =: G (_),\nwhere G (_) is defined as above for _ \u2265 0 and G (_) := (\u2212L)\u2217(\u2212_) = +\u221e for _ < 0. By Lemma 2 in Appendix A, (\u2212L)\u2217(\u2212_) and G (_) are lower bounded by E\n\u2119\u0302 [ 5 ], monotonically decreasing, convex,\nand lower semi-continuous on [0,\u221e), implying the right continuity at _ = 0. Hence, the following equivalence holds:\nq_ satisfies (IP) for all _ \u2208 (0,\u221e)\n\u21d0\u21d2 (\u2212L)\u2217(\u2212_) = G (_) for all _ \u2208 (0,\u221e) \u21d0\u21d2 (\u2212L)\u2217(\u2212_) = G (_) for all _ \u2208 [0,\u221e) \u21d0\u21d2 (\u2212L)\u2217(_) = G (\u2212_) for all _ \u2208\u211d\nBelow we separately consider the cases L(d) < +\u221e and L(d) \u2261 +\u221e for d \u2265 0. If L(d) < +\u221e for every d \u2265 0, then (\u2212L)\u2217 . +\u221e by Lemma 3, so by the involution property of Legendre transform, we have\n(\u2212L)\u2217(_) = G (\u2212_) for all _ \u2208\u211d\n\u21d0\u21d2 (\u2212L)\u2217\u2217(d) = (G (\u2212\u00b7))\u2217(d) = G\u2217 (\u2212d) for all d \u2208\u211d \u21d0\u21d2 (\u2212L)\u2217\u2217(d) = G\u2217 (\u2212d) for all d \u2208 [0,\u221e) \u21d0\u21d2 (\u2212L)\u2217\u2217(d) = G\u2217 (\u2212d) for all d \u2208 (0,\u221e) \u21d0\u21d2L(d) = G\u2217 (\u2212d) = inf _\u2208\u211d {_d +G (_)} =min _\u22650 {_d +G (_)} for all d \u2208 (0,\u221e)\nHere we have used the proporties that (\u2212L)\u2217\u2217 and G\u2217 (\u2212\u00b7) are both right continuous at 0 and are both +\u221e on (\u2212\u221e,0), thanks to Lemma 3. In the last step, the minimum is attainable because _d +G (_) is bounded from below and lower semi-continuous, and becomes arbitrarily large as _\u2192\u221e. IfL(d) \u2261 +\u221e for d > 0, then (\u2212L)\u2217(\u2212_) = +\u221e for all _ \u2208\u211d, so\n(\u2212L)\u2217(_) = G (\u2212_) for all _ \u2208\u211d\n\u21d0\u21d2 G (_) = +\u221e for all _ \u2208\u211d\n\u21d0\u21d2 G (_) = +\u221e for all _ \u2265 0\n\u21d0\u21d2 min _\u22650 {_d +G (_)} = +\u221e for all d \u2208 (0,\u221e)\n\u21d0\u21d2 min _\u22650 {_d +G (_)} =L(d) for all d \u2208 (0,\u221e)\nIn conclusion, q_ satisfies (IP) for all _ \u2208 (0,\u221e) if and only if L(d) = min_\u22650 {_d +G (_)} for all d \u2208 (0,\u221e).\nLet us compare our proof technique with existing duality results in the literature. Proofs in [15, 5, 28, 23, 29] rely on advanced convex duality theory. More specifically, Mohajerin Esfahani and Kuhn [15], Zhao and Guan [28], Zhen et al. [29] exploit advanced conic duality [19] for the problem of moments. This approach requires the nominal distribution \u2119\u0302 to be finitely supported and the space X to be convex, along with additional assumptions on the transport cost 2 and the loss function 5 . Blanchet and Murthy [5] uses an approximation argument that represents the Polish space X as an increasing sequence of compact subsets. This enables duality for any Borel distribution \u2119\u0302, based on the Fenchel conjugate on vector spaces [14], under semi-continuity assumptions on the transport cost function 2 and loss function 5 . Using the same infinite-dimensional convex duality, Sinha et al. [23,\n6 Theorem 5] streamlines the analysis by assuming the function ( -\u0302, -) \u21a6\u2192 _2( -\u0302 , -) \u2212 5 (-) is a normal integrand [18]. Compared with these non-constructive duality proofs, our (non-constructive) proof employs only the Legendre transform, namely, the convex duality for univariate real-valued functions. The constructive proof developed by Gao and Kleywegt [10] achieves a similar level of generality as [5], but without relying on convex duality theory. They construct an approximately worst-case distribution using the first-order optimality condition of the weak dual problem. Although both their approach and ours avoid using advanced minimax theorems, our analysis is notably shorter and more elementary.\n3. Discussion on the Interchangeability Principle\nIn this section, we first show that the interchangeability principle (IP) in Section 2 is intricately linked to the measurable projection and measurable selection conditions, and then prove (IP) holds in the Wasserstein DRO setting.\nNote that in (D), the function 5 belongs to a family of loss functions, and the transport cost function 2 is chosen contingent on the specific application. Consequently, from a pragmatic standpoint, we aim for (IP) to be applicable to functions within the family\n{ q :X \u00d7X \u2192\u211d\u222a {\u2212\u221e}\nq(G\u0302, G) = 5 (G) \u2212_2(G\u0302, G), 5 :X \u2192\u211d measurable, _ \u2265 0 2 :X \u00d7X \u2192 [0,\u221e] measurable, 2(G, G) = 0 for all G \u2208X\n}\nObserve that the non-negativity and reflexivity of 2 imply the following diagonally dominant property of functions in this family (see Lemma 4 in Appendix A).\nDefinition 1. We say a (\u2131 \u2297\u2131)-measurable function q :X \u00d7X \u2192\u211d\u222a {\u2212\u221e} is diagonally dominant if q(G\u0302, G) \u2264 q(G, G) for every G\u0302, G \u2208 X . We say a (\u2131 \u2297 \u2131)-measurable set \u2282 X \u00d7 X is diagonally dominant if for every (G\u0302, G) \u2208 , it holds that (G, G) \u2208 . We say a set function : X \u2192\u2131 \\ {\u2205} with (\u2131 \u2297\u2131)-measurable graph is diagonally dominant if G \u2208 (G\u0302) implies G \u2208 (G). \u2666\nWith this definition, we show that (IP) is equivalent to the measurable projection and the weak measurable selection conditions. We denote by \u2131\u0302\n\u2119 the completion of \u2131 under \u2119\u0302 [11, Page 13].\nProposition 1. (IP) holds for all (\u2131 \u2297 \u2131)-measurable diagonally dominant functions q : X \u00d7 X \u2192 \u211d\u222a {\u2212\u221e} if and only if (X ,\u2131, \u2119\u0302) satisfies the following two conditions: (Proj) [Measurable Projection] For any diagonally dominant set \u2208\u2131 \u2297\u2131,\nProjG\u0302 ( ) := {G\u0302 \u2208X : (G\u0302, G) \u2208 for some G \u2208X } \u2208 \u2131\u0302\u2119.\n(Sel*) [Weak Measurable Selection] For any diagonally dominant set-valued function :X \u2192\u2131 \\ {\u2205} with a measurable graph\nGraph( ) := {(G\u0302, G) \u2208X \u00d7X : G \u2208 (G\u0302)} \u2208\u2131 \u2297\u2131,\nthere exists a probability measure W \u2208 \u0393 \u2119\u0302 , such that suppW \u2282 Graph( ).\nRemark 4. Measurable projection and measurable selection are often seen in the literature on stochastic control (e.g., [18, 4]) and stochastic programming (e.g., [20]). (Proj) states that the projection operator ProjG\u0302 : (G\u0302, G) \u21a6\u2192 G\u0302 maps measurable sets in \u2131 \u2297\u2131 to \u2119\u0302-measurable sets. We refer (Sel*) to as the weak measurable selection, because it is weaker than the measurable selection in the literature [18, 21]. The measurable selection therein involves a deterministic selection which, in our context, reads as (Sel) [Measurable Selection] For any set-valued function : X \u2192\u2131 \\ {\u2205} with a measurable graph,\nthere exists an (\u2131\u0302 \u2119 ,\u2131)-measurable map ) :X \u2192X such that ) (G\u0302) \u2208 (G\u0302), \u2200G\u0302 \u2208X .\n7 Rockafellar and Wets [18, Theorem 14.60] (see also [21, Section 9.3.4]) shows that measurable projection and measurable selection together imply the following\nE -\u0302\u223c\u2119\u0302 [ sup G\u2208X q( -\u0302, G) ] = sup )\u2208T E -\u0302\u223c\u2119\u0302 [ q( -\u0302,) ( -\u0302)) ] ,\nwhere T denotes the set of (\u2131\u0302 \u2119 ,\u2131)-measurable maps. Note that (Id\u2297) )#\u2119\u0302 \u2208 \u0393\u2119\u0302, so the above is stronger than (IP). Comparatively, the weak measurable selection condition (Sel*) allows a random selection, represented by the conditional distribution of WG | G\u0302 supported on (G\u0302). This indicates that (Sel*) is weaker than (Sel). By Proposition 1, (IP) is equivalent to (Sel*) and (Proj). \u2666\nThe next result shows that (IP) holds when the transport cost function corresponds to the ?- Wasserstein DRO, even if 5 is not\u2131-measurable butmerely \u2119\u0302-measurable. This will be used in Example 4.\nProposition 2. Let (X , 3) be a metric space equipped with Borel f-algebra\u2131, \u2119\u0302 be a tight measure, 5 be \u2119\u0302-measurable with E \u2119\u0302 [ 5 ] > \u2212\u221e. Let ? \u2208 [1,\u221e), _ \u2265 0. Then the function q(G\u0302, G) = 5 (G) \u2212_3 (G\u0302, G) ? is (\u2131 \u2297 \u2131\u0302 \u2119 )-measurable and satisfies (IP).\n4. Examples\nIn this section, we offer several examples that demonstrate how our findings not only align with existing research but also are useful for important applications in the area of distributionally robust sequential decision-making. The following two examples illustrate that existing results in the literature are based on assumptions that are strictly stronger than (Proj) and (Sel*). Consequently, our results strictly generalize existing findings in the literature.\nExample 1 (Empirical distribution). If (X , \u2131\u0302 \u2119 ) is a discrete measurable space, that is, \u2131\u0302 \u2119 = 2X , then (Proj) and (Sel) always holds, because every subset of X is \u2131\u0302 \u2119 -measurable, and every map ) : X \u2192 X is \u2119\u0302-measurable. For instance, when X is a measurable space equipped with a Borel f-algebra and \u2119\u0302 is finitely supported, then \u2131\u0302\n\u2119 = 2X is the collection of all subsets of X , which is the\ndiscrete f-algebra on X . Thus our result covers the results in [15, 28, 29], which studies the case where X is a convex subset of \u211d3. \u2663\nExample 2 (Polish space). If X is a Polish (complete and separable metric) space and \u2131 is its Borel f-algebra, then (Proj) holds due to [3, Theorem 8.3.2], and (Sel) holds due to [1, Theorem 18.26]. Thereby our result covers the results in [5, 10]. \u2663\nExample 2 can be generalized as follows.\nExample 3 (Suslin space). A Hausdorff topological space is Suslin (also known as analytic) if it is the continuous image of a Borel set in a Polish space. IfX is a Suslin space and\u2131 is its Borel f-algebra, then (Proj) holds due to [7, Theorem III.23], and (Sel) holds due to [7, Theorem III.22]. \u2663\nThe following examples showcase the broad applicability of our results.\nExample 4 (Distributionally robust Markov decision process). Consider a finite-horizon Markov decision process. The standard working horse [4, 22] involves the Borel space (i.e., a Borel subset of a complete and separable metric space). To avoid measurability issues, existing literature often assumes a finite or countable state space. Nonetheless, for general Borel state space, we can still verify (IP) by Proposition 2. Let the state space S and the action space A be non-empty Borel spaces. Let {A(B)}B\u2208S \u2282A be a family of non-empty feasible action sets such that the corresponding constraint set K = {(B, 0) : B \u2208 S , 0 \u2208 A(B)} is an analytic subset of S \u00d7A. Let the one-stage cost 6C be a lower semi-analytic function on K bounded from below. Suppose we have a nominal transition\n8 kernel {\u2119\u0302(\u00b7 | B, 0)} (B,0) \u2208K that is a Borel measurable stochastic kernel on S given (B, 0). Consider the following uncertainty sets defined for every state-action pair (B, 0)\nM (B, 0) = { \u2119 \u2208 P (S) :K2 (\u2119\u0302(\u00b7 | B, 0),\u2119) \u2264 d(B, 0) } , (B, 0) \u2208 K,\nwhere the positive radius function d is lower semi-analytic on K, and the transport cost 2 associated with K2 is 3\n?, ? \u2208 [1,\u221e) where 3 is the metric on S. The distributionally robust counterpart of the value iteration [26, 24] is given by +)+1 \u2261 0, and for C = 1, . . . , ) ,\n+C (B) = inf 0\u2208A(B)\n{ 6C (B, 0) + sup\n\u2119\u2208M (B,0)\nEB\u2032\u223c\u2119 [+C+1 (B \u2032)] } .\nThen by induction, we can prove the duality\n+C (B) = inf 0\u2208A(B) _\u22650\n{ 6C (B, 0) +_d(B, 0) +EB\u0302\u2032\u223c\u2119\u0302( \u00b7 |B,0) [ sup B\u2032\u2208S { +C+1 (B \u2032) \u2212_2( B\u0302\u2032, B\u2032) }]} . (2)\nIn the inductive step, +C+1 is \u2119\u0302-measurable and lower semi-analytic. The strong duality holds thanks to Proposition 2. Since {\u2119\u0302(\u00b7 | B, 0)}(B,0) \u2208K is a Borel measurable kernel, the robust loss is also lower semi-analytic. Taking the infimum yields a lower semi-analytic value function +C , which completes the induction. We refer to Appendix EC.3 for details. \u2663\nExample 5 (Data-driven robust multi-stage stochastic programming). Consider a multistage stochastic programming problem [21]. Let (G1, . . . , G)) be a ) -stage random data process, which is assumed to be stagewise independent, namely, {GC } ) C=1\nare mutually independent. At each stage, after the current-stage uncertainty GC is realized, the decision maker seeks a non-anticipative decision DC from a feasible set UC (DC\u22121, GC ) \u2282 \u211d\n3C dependent on the previous-stage decision DC\u22121 and the current-stage uncertainty GC , where UC is a measurable multifunction (i.e., set-valued function). The (random) cost of taking a decision DC at stage C is 5C (DC , GC ) and the goal is to minimize the cumulative cost. Suppose the uncertainty GC has an unknown distribution on a Suslin space XC equipped with the Borel f-algebra, and one formulates an uncertainty set based on a nominal Borel distribution \u2119\u0302C with a radius dC > 0:\nMC = { \u2119 \u2208 P (XC ) :K2 (\u2119\u0302C ,\u2119) \u2264 dC } , C = 1, . . . , ) .\nFollowing the convention in stochastic programming, we assume there is no uncertainty in the first stage, i.e., d1 = 0 and \u2119\u03021 is a Dirac measure \u2119\u03021 = G1, where G1 \u2208 X1, and we set D0 = \u2205. It would be natural to consider the following distributionally robust counterpart of Bellman recursion: &)+1 \u2261 0, and for C = 1, . . . , ) ,\n&C (DC\u22121, GC ) = inf D\u2208UC (DC\u22121 ,GC )\n{ 5C (D, GC) + sup\n\u2119\u2208MC+1\nE\u2119 [ &C+1 (D, GC+1) ] } .\nUnder this setting, (Proj) and (Sel) follow from Example 3. For every GC \u2208 XC , assume that 5C (\u00b7, \u00b7) is random lower semi-continuous and bounded from below, and the multifunction UC (\u00b7, GC) is non-empty, measurable and closed valued; assume further that there exists a bounded set containing UC (DC\u22121, GC ) for all DC\u22121 and GC . Then by induction (see Appendix EC.3 for details), we have the duality\n&C (DC\u22121, GC ) = inf D\u2208UC (DC\u22121 ,GC )\n_\u22650\n{ 5C (D, GC ) +_dC +E\u2119\u0302C+1 [ sup G\u2208XC+1 { &C+1 (D, G) \u2212_2(G\u0302C+1, G) } ]} . \u2663\n9 Example 6 (Chance constraint). Let (X , 3) be a metric space and let \u2131 be its Borel f-algebra. Consider a distributionally robust chance constraint [8, 25, 27]\ninf \u2119\u2208P (X )\n{ \u2119(S) :W? (\u2119\u0302,\u2119) \u2264 d } \u2265 1\u2212 V, (3)\nwhich ensures that an event S \u2208 \u2131 happens with probability higher than 1 \u2212 V, V \u2208 (0,1), with respect to every distribution within the uncertainty set. Denote by S2 the complement of the set S. In Appendix EC.3, we show that if q(G\u0302, G) = 1S2 (G) \u2212 _3 (G\u0302, G)\n? satisfies (IP) for every _ > 0, then using Theorem 1, the distributionally robust chance constraint (3) is equivalent to a worst-case conditional value-at-risk constraint\nd? \u2264 \u2212V\u2102V@R\u2119\u0302V (\u22123 ( -\u0302,S 2) ?).\nwhere \u2102V@R\u2119\u0302V (\u00b7) represents the conditional value-at-risk at risk level V. Note that the constraint\nis infeasible if d \u2265 E -\u0302\u223c\u2119\u0302\n[3 ( -\u0302 ,S2) ?] 1 ? . Similar results have been obtained by [8, 27, 25] but with\nless generality. In [8], the ambient space X is Euclidean, and \u2119\u0302 is empirical. In [27], the ambient space is a subset of a Euclidean space and \u2119\u0302 is a Borel probability measure. In [25], X is a totally bounded Polish space. Both [8, 25] consider only 1-Wasserstein distance. In comparison, the result above requires (X , 3) only to be a metric space satisfying (IP) and has no further requirement on the nominal probability distribution \u2119\u0302 nor the set S beyond measurability. \u2663\n5. Extensions\nIn this section, we extend our results to several other problems. The proofs are based on similar techniques that we developed in Section 2.\n5.1. Infinity-Wasserstein DRO and Maximum Transport Cost\nRecall the \u221e-Wasserstein distance\nW\u221e (\u2119\u0302,\u2119) = inf W\u2208\u0393(\u2119\u0302,\u2119) \u20163\u2016!\u221e (X \u00d7X ;W) = inf W\u2208\u0393(\u2119\u0302,\u2119) W- ess sup G\u0302,G\u2208X 3 (G\u0302, G),\nThe result in Section 2 only covers the Wasserstein distance of a finite order. To study the case ? =\u221e, we introduce the maximum transport cost as\nK2 (\u2119\u0302,\u2119) := inf W\u2208\u0393(\u2119\u0302,\u2119) W- ess sup G\u0302,G\u2208X 2(G\u0302, G),\nwhere 2 is a transport cost function satisfying Assumption 1. We define the maximum transport cost robust loss by\nL(d) := sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) \u2264 d } .\nSimilar to the results in Section 2, the soft-constrained counterpart can be viewed as the Legendre transform of negative hard-constrained robust loss\n(\u2212L)\u2217(\u2212_) = sup d\u22650\n{ (\u2212_)d\u2212 (\u2212L(d)) } = sup\n\u2119\u2208P (X )\n{ E-\u223c\u2119[ 5 (-)] \u2212_K2 (\u2119\u0302,\u2119) } .\nWith this definition, we cover the \u221e-Wasserstein DRO by setting 2 = 3. Below, we first establish a duality result when the constraint of the uncertainty set is a strict inequality.\n10\nProposition 3. Let 5 and 2 satisfy Assumption 1. Define\nL \u25e6 (d) := sup\n\u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) < d } .\nSuppose the function kd (G\u0302, G) := 5 (G) \u2212\u221e1{2(G\u0302, G) \u2265 d} satisfies (IP) for some d > 0. Then\nL \u25e6 (d) = E\n-\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) < d }] .\nSuppose the kd satisfies (IP) for every d > 0. Then\n(\u2212L)\u2217(\u2212_) = sup d>0\n{ E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) < d } \u2212_d ]} .\nRemark 5. Unlike the results in Section 2, we have to distinguish two cases: L \u25e6 (d) which involves the strict inequality constraint, and L(d) which involves the non-strict inequality constraint. Indeed, for problem (P) in Section 2, the value of L(d) in (P) would not be affected if we replace the non-strict inequality by the strict inequality thanks to the concavity, and thus continuity, of L with respect to d \u2208 (0,\u221e). However, for the problem with the maximum transport cost, neither L \u25e6 nor L is necessarily continuous. Without additional assumptions, the duality does not hold for the equality constraint, even when the cost function 2 is also a metric. For instance, considerX = [0,1] \u00d7 [0,1], \u2119\u0302 is a uniform distribution on {0} \u00d7 [0,1], and 5 (G1, G2) = 1{G1 = 1}. We introduce the following cost function:\n2((G1, G2), (H1, H2)) = { 0, G1 = H1, G2 = H2, 101+ |G1 \u2212 H1 |, G1 \u2260 H1, G2 = H2, 100+ |G1 \u2212 H1 | + |G2 \u2212 H2 |, G2 \u2260 H2.\nThen 2(G, H) = 0 iff G = H, and 2 is symmetric. Triangular inequality holds because the distance between any two distinct points is between 100 and 102. If \u2119 is a uniform distribution on {1} \u00d7 [0,1], then K2 (\u2119\u0302,\u2119) = 101, thus\nL(101) = sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) \u2264 101 } = 1.\nHowever, because 3 ((0, C), (1, B)) > 101 for any C, B \u2208 [0,1], we would have\nE -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) \u2264 101 }] = 0 \u2260 1. \u2666\nThe following result shows that, with additional assumptions on the space and the transport cost function, we can obtain the duality result. The detailed proofs for Proposition 3 and Theorem 2 can be found in Appendix EC.4.\nTheorem 2. Suppose X is a Polish space and 2 :X \u00d7X \u2192 [0,\u221e) is continuous. Let 5 satisfy Assumption 1. Then\nL(d) = E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) \u2264 d }] ,\n(\u2212L)\u2217(\u2212_) = sup d\u22650\n{ E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) \u2264 d ] \u2212_d }} .\nExample 7 (Chance constraint (continued)). Consider the chance constraint introduced in Example 6 but with \u221e-Wasserstein distance. If X is complete and separable, then the robust chance constraint becomes \u2119\u0302(3 ( -\u0302,S2) \u2264 d) \u2264 V. In particular, it is infeasible if d \u2265 3 (supp \u2119\u0302,S2). Compared with [27] which assumes X is a normed space, we only require (X , 3) to be Polish. \u2663\n11\n5.2. Risk-averse Optimization\nRecall (X ,\u2131, \u2119\u0302) is a probability space. Consider a concave risk measure :P (X ) \u2192 \u211d\u0304 of the following form:\n(\u2119) := inf U\u2208 E-\u223c\u2119 [ 5U (-)], (4)\nwhere 5U : X \u2192 \u211d are a family of measurable functions indexed by U \u2208 , a subset of a linear topological space. Given a nominal distribution \u2119\u0302, define\nLJ(d) := sup \u2119\u2208P (X )\n{ (\u2119) :K2 (\u2119\u0302,\u2119) \u2264 d } ,\nLJ (d) := sup \u2119\u2208P (X )\n{ (\u2119) :K2 (\u2119\u0302,\u2119) \u2264 d } .\nWe assume that there exists a compact set \u2032 \u2282 such that for all distributions \u2119 in the distributional uncertainty set, it holds that\ninf U\u2208 E\u2119[ 5U] = min U\u2208 \u2032 E\u2119[ 5U]. (5)\nThis enables the exchange of sup over \u2119 and inf over U using Sion\u2019s minimax theorem. We have the following result.\nTheorem 3. Let 5U and 2 satisfy Assumption 1, and let U \u21a6\u2192 5U (G) be lower semi-continuous and convex for each G. Assume for every compact subset \u2032 \u2282 , infU\u2208 \u2032,G\u2208X 5U (G) > \u2212\u221e. (I) Suppose q_,U (G\u0302, G) = 5U (G) \u2212_2(G\u0302, G) satisfies (IP) for every _ > 0 and U \u2208 . Assume for any d > 0,\nthere exists a compact subset \u2032 \u2282 such that (5) holds for every \u2119 satisfying K2 (\u2119\u0302,\u2119) \u2264 d. Then\nLJ (d) = inf _\u22650,U\u2208\n{ _d +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }]} . (6)\n(II) Suppose X is a Polish space, and 2 : X \u00d7X \u2192 [0,\u221e) is continuous. Assume there exists a compact subset \u2032 \u2282 such that (5) holds for every \u2119 satisfying K2 (\u2119\u0302,\u2119) \u2264 d. Then\nLJ (d) = inf U\u2208 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) : 2( -\u0302, G) \u2264 d }] . (7)\nBelow we list several examples where - \u2208\u211d represents a random loss and =\u211d. \u2022 Conditional value-at-risk \u2102V@R\u2119V (-) at risk level V \u2208 (0,1): 5U (G) = U + 1 V (G \u2212 U)+. \u2022 Variance Var\u2119(-): 5U (G) = (G \u2212 U) 2. \u2022 Mean absolute deviation (around median) MAD\u2119(-): 5U (G) = |G \u2212 U |. \u2022 Entropic risk measure Ent\u2119\n\\ (-) = 1 \\ logE[4\\-] with risk-aversion parameter \\ > 0: 5U (G) = U +\n1 \\\n( 4\\ (G\u2212U) \u2212 1 ) .\nIn Appendix EC.5, we can verify (5) and other assumptions of Theorem 3 hold for all these risk measures.\nExample 8. Let / \u2208 (\u211d3, \u2016 \u00b7 \u2016) be the vector of random loss of 3 assets with nominal distribution \u211a\u0302, and 1 \u2208 \u211d3 be a portfolio weight vector on these assets. The portfolio loss is thus 1\u22a4/. We have the following results on the robust risk of portfolio loss under various risk measures. Their proofs are given in Appendix EC.5.\n\u2022 Conditional value-at-risk: for ? \u2208 [1,\u221e]:\nsup \u211a\u2208P (Z )\n{ \u2102V@R\u211a\nV (1\u22a4/) :W? (\u211a\u0302,\u211a) \u2264 d\n} =\u2102V@R\u211a\u0302\nV (1\u22a4 /\u0302) + (1\u2212 V) \u2212 1 ? \u20161\u2016\u2217d.\n12\n\u2022 Variance: for ? = 2,\nsup \u211a\u2208P (Z )\n{ Var\u211a (1\u22a4/) :W2(\u211a\u0302,\u211a) \u2264 d } = ( Var\u211a\u0302 (1\u22a4 /\u0302) 1 2 + \u20161\u2016\u2217d )2 .\nFor ? =\u221e,\nsup \u211a\u2208P (Z )\n{ Var\u211a (1\u22a4/) :W\u221e (\u211a\u0302,\u211a) \u2264 d } =min U\u2208\u211d E /\u0302\u223c\u211a\u0302 [ ( | /\u0302 \u2212 U | + \u20161\u2016\u2217d) 2 ] .\nFor 1 \u2264 ? < 2, the robust loss is positive infinity. \u2022 Mean average deviation: for 1 \u2264 ? \u2264\u221e,\nsup \u211a\u2208P (Z )\n{ MAD\u211a (1\u22a4/) :W? (\u211a\u0302,\u211a) \u2264 d } =MAD\u211a\u0302 (1\u22a4 /\u0302) + \u20161\u2016\u2217d.\n\u2022 Entropic risk measure: for ? =\u221e,\nsup \u211a\u2208P (Z )\n{ Ent\u211a\n\\ (1\u22a4/)] :W\u221e(\u211a\u0302,\u211a) \u2264 d\n} = Ent\u211a\u0302\n\\ (1\u22a4 /\u0302)] + \u20161\u2016\u2217d.\nFor 1 \u2264 ? <\u221e, the robust loss is positive infinity. \u2663\n5.3. Globalized Distributionally Robust Counterpart\nThe globalized distributionally robust counterpart [13] studies the following problem\nsup \u2119,\u2119\u0303\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] \u2212_K2 (\u2119\u0303,\u2119) :K2\u0303 (\u2119\u0302, \u2119\u0303) \u2264 \\ } . (G)\nWe also consider its hard- and soft-constrained variants\nLG(d, \\) := sup \u2119,\u2119\u0303\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0303,\u2119) \u2264 d,K2\u0303 (\u2119\u0302, \u2119\u0303) \u2264 \\ } , (G-hard)\nsup \u2119,\u2119\u0303\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] \u2212_K2 (\u2119\u0303,\u2119) \u2212 `K2\u0303 (\u2119\u0302, \u2119\u0303) } . (G-soft)\nThe following result extends the work of [13], which is based on the assumption that X is a subset of Euclidean space and the transport cost is defined by the 1-Wasserstein distance. The proof can be found in Appendix EC.6.\nProposition 4. Let the loss function 5 and two cost functions 2, 2\u0303 satisfy Assumption 1. For d, \\ > 0, _, ` \u2265 0, if (G\u0303, G) \u21a6\u2192 5 (G) \u2212 _2(G\u0303, G) satisfies (IP) for every _ \u2265 0, and (G\u0302, G) \u21a6\u2192 supG\u0303\u2208X 5 (G) \u2212 _2(G\u0303, G) \u2212 `2\u0303(G\u0302, G\u0303) satisfies (IP) for every _, ` \u2265 0, then for every _, `, d, \\ > 0, (G-hard) is equivalent to\nmin _,`\u22650\n{ _d + `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_2(G\u0303, G) \u2212 `2\u0303( -\u0302, G\u0303) }]} ,\n(G) is equivalent to\n(\u2212LG(\u00b7, \\)) \u2217(\u2212_) =min\n`\u22650\n{ `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_2(G\u0303, G) \u2212 `2\u0303( -\u0302 , G\u0303) }]} ,\nand (G-soft) is equivalent to\nL \u2217 \u2217 G (\u2212_,\u2212`) = E -\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_2(G\u0303, G) \u2212 `2\u0303( -\u0302 , G\u0303) }] .\n13\nHere L \u2217 \u2217 G (\u2212_, \u00b7) is the dual of the mapping \\ \u21a6\u2192 \u2212(\u2212LG(\u00b7, \\))\n\u2217(\u2212_). Moreover, if 2 = 2\u0303 = 3, then (G) is further equivalent to\nmin 0\u2264`\u2264_\n{ `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212 `3 ( -\u0302, G) }]} .\n6. Concluding Remarks\nWe develop a new and general duality result for Wasserstein distributionally robust optimization, which is based on applying the Legendre transform twice to the worst-case loss as a function of the Wasserstein radius. Our proof is elementary, concise, and general. The proof technique may apply to other choices of statistical distance in distributionally robust optimization.\nAppendix A: Auxiliary Results\nLemma 2. Assume Assumption 1 holds. Recall for _ \u2265 0,\n(\u2212L)\u2217(\u2212_) = sup \u2119\u2208P\u0304\n{ E-\u223c\u2119 [ 5 (-)] \u2212_K2 (\u2119\u0302,\u2119) } , G (_) = E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212_2( -\u0302, G) }] .\nThen (\u2212L)\u2217(\u2212\u00b7) and G (\u00b7) are lower bounded by E \u2119\u0302 [ 5 ], monotonically decreasing, convex, and lower semi-continuous on [0,\u221e).\nLemma 3. If 5 :\u211d\u2192\u211d\u222a {+\u221e} is a monotonically decreasing convex function with 5 (d) = +\u221e for d < 0 and 5 . +\u221e, then 5 \u2217 (\u2212_) = supd\u2208\u211d{\u2212_d \u2212 5 (d)} is a lower semi-continuous, monotonically decreasing convex function of _ with 5 \u2217 (\u2212_) = +\u221e for _ < 0 and 5 \u2217 . +\u221e.\nLemma 4. q :X \u00d7X \u2192\u211d\u222a{\u2212\u221e} is diagonally dominant if and only if there exists a measurable function 5 : X \u2192 \u211d \u222a {\u2212\u221e}, a constant _ \u2265 0, and a measurable function 2 : X \u00d7 X \u2192 [0,\u221e] vanishing on diagonal such that q(G\u0302, G) = 5 (G) \u2212_2(G\u0302, G).\nReferences\n[1] Aliprantis CD, Border K (2006) Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide (Springer Science & Business Media).\n[2] Ambrosio L, Fusco N, Pallara D (2000) Functions of bounded variation and free discontinuity problems (Oxford University Press).\n[3] Aubin JP, Frankowska H (2009) Set-valued analysis (Springer Science & Business Media).\n[4] Bertsekas DP, Shreve SE (1996) Stochastic optimal control: the discrete-time case, volume 5 (Athena Scientific).\n[5] Blanchet J, Murthy K (2019) Quantifying distributional model risk via optimal transport. Mathematics of Operations Research 44(2):565\u2013600.\n[6] Blanchet J, Murthy K, Nguyen VA (2021) Statistical analysis of Wasserstein distributionally robust estimators. Tutorials in Operations Research: Emerging OptimizationMethods andModeling Techniques with Applications, 227\u2013254 (INFORMS).\n[7] Castaing C, Valadier M (1977) Measurable multifunctions. Convex Analysis and Measurable Multifunctions, 59\u201390 (Springer).\n[8] Chen Z, Kuhn D, Wiesemann W (2023) On approximations of data-driven chance constrained programs over Wasserstein balls. Operations Research Letters 51(3):226\u2013233.\n14\n[9] F\u00f6llmer H, Schied A (2010) Convex and coherent risk measures. Encyclopedia of Quantitative Finance 355\u2013363.\n[10] Gao R, Kleywegt A (2023) Distributionally robust stochastic optimization with wasserstein distance. Mathematics of Operations Research 48(2):603\u2013655.\n[11] Kallenberg O (1997) Foundations of modern probability, volume 2 (Springer).\n[12] Kuhn D, Esfahani PM, Nguyen VA, Shafieezadeh-Abadeh S (2019) Wasserstein distributionally robust optimization: Theory and applications in machine learning. Operations Research & Management Science in the Age of Analytics, 130\u2013166 (INFORMS).\n[13] Liu F, Chen Z, Wang S (2023) Globalized distributionally robust counterpart. INFORMS Journal on Computing 35(5):1120\u20131142.\n[14] Luenberger DG (1997) Optimization by vector space methods (John Wiley & Sons).\n[15] Mohajerin Esfahani P, Kuhn D (2018) Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming 171(1-2):115\u2013166.\n[16] Rockafellar RT (1970) Convex analysis. Number 28 in Princeton Mathematical Series (Princeton university press).\n[17] Rockafellar RT, Uryasev S, et al. (2000) Optimization of conditional value-at-risk. Journal of risk 2:21\u201342.\n[18] Rockafellar RT, Wets RJB (2009) Variational analysis, volume 317 (Springer Science & Business Media).\n[19] Shapiro A (2001) On duality theory of conic linear problems. Semi-infinite programming, 135\u2013165 (Springer).\n[20] Shapiro A (2017) Interchangeability principle and dynamic equations in risk averse stochastic programming. Operations Research Letters 45(4):377\u2013381.\n[21] Shapiro A, Dentcheva D, Ruszczynski A (2021) Lectures on stochastic programming: modeling and theory (SIAM).\n[22] Shreve SE, Bertsekas DP (1979) Universally measurable policies in dynamic programming.Mathematics of Operations Research 4(1):15\u201330.\n[23] Sinha A, Namkoong H, Duchi J (2018) Certifying some distributional robustness with principled adversarial training. International Conference on Learning Representations.\n[24] Wang J, Gao R, Zha H (2022) Reliable off-policy evaluation for reinforcement learning.Operations Research Forthcoming.\n[25] Xie W (2021) On distributionally robust chance constrained programs with Wasserstein distance. Mathematical Programming 186(1):115\u2013155.\n[26] Yang I (2017) A convex optimization approach to distributionally robust markov decision processes with Wasserstein distance. IEEE control systems letters 1(1):164\u2013169.\n[27] Yang Z, Gao R (2022) Wasserstein Regularization for 0-1 Loss. Optimization Online preprint.\n[28] Zhao C, Guan Y (2018) Data-driven risk-averse stochastic optimization with Wasserstein metric. Operations Research Letters 46(2):262\u2013267.\n[29] Zhen J, Kuhn D, Wiesemann W (2023) A unified theory of robust and distributionally robust optimization via the primal-worst-equals-dual-best principle. Operations Research Forthcoming.\nec1\nAdditional Proofs\nAppendix EC.1: Proofs for Appendix A\nProof of Lemma 1. The monotonicity of L(d) can be seen from the definition. Moreover, since K2 (\u2119\u0302, \u2119\u0302) = 0,\nL(d) \u2265 L(0) \u2265 E -\u223c\u2119\u0302 [ 5 (-)] > \u2212\u221e.\nTherefore for all d \u2265 0, L(d) is bounded from below. To verify the concavity, fix d0, d1 \u2265 0. For any C \u2208 [0,1] and \u21190,\u21191 \u2208 P (X ) satisfying K2 (\u2119\u0302,\u21190) \u2264 d0, K2 (\u2119\u0302,\u21191) \u2264 d1, denote \u2119C = (1 \u2212 C)\u21190 + C\u21191. For arbitrary n > 0, we can find transport plans W0 \u2208 \u0393(\u2119\u0302,\u21190), W1 \u2208 \u0393(\u2119\u0302,\u21191) such that EW0 [2] \u2264 K2 (\u2119\u0302,\u21190) + n , EW1 [2] \u2264K2 (\u2119\u0302,\u21191) + n . Define WC = (1\u2212 C)W0 + CW1, then WC \u2208 \u0393(\u2119\u0302,\u2119C ) and\nK2 (\u2119\u0302,\u2119C) \u2264 EWC [2] = (1\u2212 C)EW0 [2] + CEW1 [2] \u2264 (1\u2212 C) (K2 (\u2119\u0302,\u21190) + n) + C (K2 (\u2119\u0302,\u21191) + n)\n\u2264 (1\u2212 C)K2 (\u2119\u0302,\u21190) + CK2 (\u2119\u0302,\u21191) + n \u2264 (1\u2212 C)d0 + C d1 + n .\nSince it is true for any n , we know K2 (\u2119\u0302,\u2119C) \u2264 (1 \u2212 C)d0 + C d1, hence \u2119C is a feasible solution to (P) with d = (1\u2212 C)d0 + C d1 and\nL((1\u2212 C)d0 + C d1) \u2265 E-\u223c\u2119C [ 5 (-)] = (1\u2212 C)E-\u223c\u21190 [ 5 (-)] + CE-\u223c\u21191 [ 5 (-)].\nTaking the supremum over \u21190 and \u21191, we have\nL((1\u2212 C)d0 + C d1) \u2265 (1\u2212 C)L(d0) + CL(d1),\nwhich completes the proof.\nProof of Lemma 3. Since 5 (d) = +\u221e for d < 0, we have\n5 \u2217 (\u2212_) = sup d\u2208\u211d {\u2212_d \u2212 5 (d)} = sup d\u22650 {\u2212_d \u2212 5 (d)}.\nFor each fixed d \u2265 0, \u2212_d\u2212 5 (d) is a monotonically decreasing, lower semi-continuous convex function of _, so the supremum over d is also monotonically decreasing, lower semi-continuous, and convex. Suppose 5 (d0) < +\u221e at some d0 \u2265 0. For each _ < 0,\n5 \u2217 (\u2212_) = sup d\u22650 {\u2212_d \u2212 5 (d)} \u2265 sup d\u2265d0 {\u2212_d \u2212 5 (d)} \u2265 sup d\u2265d1 {\u2212_d \u2212 5 (d0)} = +\u221e.\nPick d1 > d0. For each _ \u2265 0,\n5 \u2217 (\u2212_) = sup d\u22650 {\u2212_d \u2212 5 (d)} = sup 0\u2264d\u2264d1 {\u2212_d \u2212 5 (d)} \u2228 sup d\u2265d1 {\u2212_d \u2212 5 (d)}.\nWhen 0 \u2264 d \u2264 d1, \u2212_d\u2212 5 (d) \u2264 \u2212 5 (d1) < +\u221e. When d \u2265 d1, by convexity we have 5 (d) \u2265 5 (d1) + (d\u2212 d1) 5 (d1 )\u2212 5 (d0 ) d1\u2212d0 , so if _ \u2265 5 (d0 )\u2212 5 (d1 ) d1\u2212d0 we must have\n\u2212_d \u2212 5 (d) \u2264 \u2212_d \u2212 5 (d1) \u2212 (d \u2212 d1) 5 (d1) \u2212 5 (d0)\nd1 \u2212 d0 \u2264 \u2212 5 (d1) \u2212 d1\n5 (d0) \u2212 5 (d1)\nd1 \u2212 d0 < \u2212 5 (d1) < +\u221e.\nHence 5 \u2217 (_) \u2264 \u2212 5 (d1) < +\u221e, so 5 \u2217 . +\u221e.\nec2\nProof of Lemma 2. The lower bound follows by setting G = -\u0302: for any _ \u2208 [0,\u221e), we have\n(\u2212L)\u2217(\u2212_) \u2265 E -\u0302\u223c\u2119\u0302 [ 5 ( -\u0302)] \u2212_K2 (\u2119\u0302, \u2119\u0302) = E\u2119\u0302[ 5 ], G \u2217 (_) \u2265 E -\u0302\u223c\u2119\u0302\n[ 5 ( -\u0302) \u2212_2( -\u0302, -\u0302) ] = E\n\u2119\u0302 [ 5 ].\nHere we used K2 (\u2119\u0302, \u2119\u0302) = 0 because 2(G, G) = 0 for every G \u2208X . If L(d) < +\u221e for all d > 0, then (\u2212L)\u2217(\u2212\u00b7) is decreasing, convex, and lower semi-continuous because of Lemma 3. If L(d) = +\u221e for all d > 0, then (\u2212L)\u2217 \u2261 +\u221e. Since 5 (G) \u2212 _2(G\u0302, G) is decreasing and affine in _, \u03a6(_; G\u0302) := supG\u2208X { 5 (G) \u2212_2(G\u0302, G)} is also a decreasing, convex, and lower semi-continuous function of _. Recall that when _ = 0 and 2(G\u0302, G) = +\u221e, we use the convention 0 \u00b7 \u221e =\u221e. We now verify that (a) G\u2217 is monotonically decreasing:\n_1 \u2264 _2 =\u21d2 \u03a6(_1; G\u0302) \u2265\u03a6(_2; G\u0302) for all G\u0302 \u2208X =\u21d2 E\u2119\u0302[\u03a6(_1; -\u0302)] \u2265 E\u2119\u0302[\u03a6(_2; -\u0302)].\n(b) G\u2217 is convex:\n_\\ = (1\u2212 \\)_1 + \\_2 =\u21d2 \u03a6(_\\ ; G\u0302) \u2264 (1\u2212 \\)\u03a6(_0; G\u0302) + \\\u03a6(_1; G\u0302) for all G\u0302 \u2208X\n=\u21d2 E \u2119\u0302 [\u03a6(_\\ ; -\u0302)] \u2264 (1\u2212 \\)E\u2119\u0302[\u03a6(_0; -\u0302)] + \\E\u2119\u0302 [\u03a6(_1; -\u0302)].\n(c) G\u2217 is lower semi-continuous: note that \u03a6(_; G\u0302) \u2265 5 (G\u0302) and E \u2119\u0302 [ 5 ] > \u2212\u221e. Taking _= \u2192 _ where\n_=, _ \u2208 [0,\u221e), then by Fatou\u2019s lemma, we have\nlim inf _=\u2192_ E \u2119\u0302 [\u03a6(_=; -\u0302)] \u2265 E\u2119\u0302 [ lim inf _=\u2192_ \u03a6(_=; -\u0302) ] \u2265 E \u2119\u0302 [\u03a6(_)].\nWith this we complete the proof.\nProof of Lemma 4. 5 (G) \u2212_2(G\u0302, G) \u2264 5 (G) \u22120 = 5 (G) \u2212_2(G, G), so 5 \u2212_2 is diagonally dominant. If q is diagonally dominant, we define _ := 1, 5 (G) := q(G, G), and 2(G\u0302, G) := 5 (G) \u2212q(G\u0302, G) when 5 (G) > \u2212\u221e, 2(G\u0302, G) = 0 when 5 (G) =\u2212\u221e. Then 2(G\u0302, G) \u2265 0 and 2(G, G) = 0.\nAppendix EC.2: Proof of Proposition 1\nBefore proving Proposition 1, we make a simple observation.\nLemma EC.1. \u2282 X \u00d7 X is a diagonally dominant set if and only if its indicator function 1 is a diagonally dominant function. q : X \u00d7X \u2192\u211d\u222a {\u2212\u221e} is a diagonally dominant function if and only if its superlevel set {q > U} is a diagonally dominant set for any U \u2208 \u211d. : X \u2192\u2131 \\ {\u2205} is a diagonally dominant set-valued function if and only if its graph is a diagonally dominant set.\nProof of Proposition 1. We first prove the sufficiency. Let q be a diagonally dominant (\u2131 \u2297 \u2131)measurable function. Define \u03a6(G\u0302) = supG\u2208X q(G\u0302, G). For any U \u2208 \u211d, the superlevel set of \u03a6 can be regarded as\n{G\u0302 :\u03a6(G\u0302) > U} = {G\u0302 : \u2203G, q(G\u0302, G) > U} = ProjG\u0302 ({(G\u0302, G) : q(G\u0302, G) > U}).\nThe superlevel set {q > U} is diagonally dominant. By assumption (Proj), ProjG\u0302 maps (\u2131 \u2297 \u2131)measurable diagonally dominant sets to \u2131\u0302\n\u2119 -measurable sets, thus the superlevel set of \u03a6 is \u2131\u0302 \u2119 -\nmeasurable. Therefore, \u03a6 is \u2119\u0302-measurable, and it remains to show that\nE \u2119\u0302 [\u03a6] = sup\nW\u2208\u0393 \u2119\u0302\nEW [q].\nec3\nSince for any G \u2208X , q(G\u0302, G) \u2264\u03a6(G\u0302), it is clear that for any W \u2208 \u0393 \u2119\u0302 ,\nE -\u0302\u223c\u2119\u0302\n[ \u03a6( -\u0302) ] \u2265 E\n(-\u0302,-)\u223cW\n[ q( -\u0302, -) ] .\nTo see the other direction, we may assume E -\u0302\u223c\u2119\u0302 [\u03a6( -\u0302)] > \u2212\u221e, otherwise the conclusion holds trivially. Then {\u03a6 =\u2212\u221e} is a \u2119\u0302-nullset. We fix n, \" > 0, and below we construct a near optimal W \u2208 \u0393 \u2119\u0302 .\nDefine (= = {G\u0302 \u2208 X : =n < \u03a6(G\u0302) \u2264 (= + 1)n} for = \u2208 \u2124. (= is \u2131\u0302\u2119-measurable, so we can find = \u2282 (= which is \u2131-measurable and \u2119\u0302((= \\ =) = 0. Define set-valued function = :X \u2192\u2131 \\ {\u2205} by\n= (G\u0302) = { X \\ = G\u0302 \u2209 = {G \u2208X : q(G\u0302, G) > =n} G\u0302 \u2208 = .\nGraph( =) = (X \\ =) \u00d7 (X \\ =) \u222a (( = \u00d7 X ) \u2229 {q > =n}) is (\u2131 \u2297 \u2131)-measurable. We claim = is diagonally dominant. That is, G \u2208 = (G\u0302) implies G \u2208 = (G). To see this, note that if G \u2209 = then G \u2208 X \\ = = = (G); if G\u0302 \u2209 = and G \u2208 = (G\u0302) = X \\ = then G \u2209 = so G \u2208 = (G). Now suppose G\u0302, G \u2208 = and G \u2208 = (G\u0302), then q(G, G) \u2265 q(G\u0302, G) > =n , so again we have G \u2208 = (G). This finishes the proof of the claim. By assumption (Sel*) we can find a measurable transport plan W= \u2208 \u0393\u2119\u0302 supported in Graph( =). Define (\u221e = {G\u0302 \u2208 X : \u03a6(G\u0302) = \u221e}. (\u221e is \u2131\u0302\u2119-measurable, so we can find \u221e \u2282 (\u221e which is \u2131measurable and \u2119\u0302((\u221e \\ \u221e) = 0. For some \" > 0 to be determined, define set-valued function \u221e : X \u2192\u2131 \\ {\u2205} by\n\u221e(G\u0302) = { X \\ \u221e G\u0302 \u2209 \u221e {G \u2208X : q(G\u0302, G) > \"} G\u0302 \u2208 \u221e .\nSame as before, \u221e is diagonally dominant, so we can find a measurable transport plan W\u221e \u2208 \u0393\u2119\u0302 supported in Graph( \u221e). Now we define measure W \u2208 P (X \u00d7X ,\u2131 \u2297\u2131) by\nW( ) = \u2211\n=\u2208\u2124\u222a{+\u221e}\nW= ( \u2229 ( = \u00d7X )).\nThen for any ( \u2282 X ,\nW(( \u00d7X ) = \u2211\n=\u2208\u2124\u222a{+\u221e}\nW= (((\u2229 =) \u00d7X ) = \u2211\n=\u2208\u2124\u222a{+\u221e}\n\u2119\u0302((\u2229 =) = \u2119\u0302((),\nso W \u2208 \u0393 \u2119\u0302 . In the last equality, we used that = \u2282 (= are pairwise disjoint and \u2119\u0302(X \\\n\u22c3 =\u2208\u2124\u222a{+\u221e} =) = 0.\nMoreover, W is supported in\n{(G\u0302, G) \u2208X \u00d7X : q(G\u0302, G) >\u03a6(G\u0302) \u2212 n if \u03a6(G\u0302) <\u221e, q(G\u0302, G) > \" if \u03a6(G\u0302) =\u221e}.\nTherefore, EW [q] \u2265 E\u2119\u0302[(\u03a6 \u2212 n)1{\u03a6 < +\u221e}] + \"\u2119\u0302(\u03a6 = +\u221e). By making n arbitrarily small and \" arbitrarily large, we have supW\u2208\u0393\n\u2119\u0302\nEW [q] \u2265 E\u2119\u0302[\u03a6]. This proves that (Proj) and (Sel*) combined imply\n(IP) holds for all diagonally dominant functions. Next, we prove the necessity. Suppose (IP) holds for all diagonally dominant functions. Given \u2208 \u2131 \u2297\u2131 diagonally dominant, let q be the indicator of the set : q(G\u0302, G) = 1 if (G\u0302, G) \u2208 and 0 otherwise. Then q is \u2131 \u2297\u2131-measurable, diagonally dominant, and by (IP) the function \u03a6(G\u0302) = supG\u2208X q(G\u0302, G) is \u2119\u0302-measurable. Observe that\nProjG\u0302 ( ) = {G\u0302 \u2208X :\u03a6(G\u0302) \u2265 1},\nwhich is the upper level set of \u03a6, and thus belongs to \u2131\u0302 \u2119 . Therefore (IP) implies (Proj). Lastly, given a diagonally dominant set function :X \u2192\u2131 \\ {\u2205}, let\nq(G\u0302, G) = { 0 G \u2208 (G\u0302) \u2212\u221e G \u2209 (G\u0302) .\nec4\nThat is, q =\u2212\u221e \u00b71X \\Graph( ) . To see it is diagonally dominant, note that\nq(G\u0302, G) = 0 =\u21d2 G \u2208 (G\u0302) =\u21d2 G \u2208 (G) =\u21d2 q(G, G) = 0.\nSo q(G\u0302, G) \u2264 q(G, G). Since (G\u0302) \u2260\u2205, \u03a6(G\u0302) := supG\u2208X q(G\u0302, G) = 0. By (IP),\n0 = E -\u0302\u223c\u2119\u0302\n[ \u03a6( -\u0302, G) ] = sup W\u2208\u0393\n\u2119\u0302\n{ E (-\u0302,-)\u223cW [q( -\u0302, -)] } .\nNote that\nEW [q] = { 0, suppW \u2282 Graph( ), \u2212\u221e, otherwise.\nHence, there exists some W \u2208 \u0393 \u2119\u0302 supported in . Therefore (IP) implies (Sel*).\nProof of Proposition 2. Note that q is continuous in G\u0302, so\u03a6(G\u0302) = supG\u2208X q(G\u0302, G) is lower semicontinuous, and thus Borel measurable. Therefore, for any W \u2208 \u0393\n\u2119\u0302 , it holds that EW [q] \u2264 E\u2119\u0302[\u03a6]. It remains\nto construct an n-optimizer W. First, we assume E\n\u2119\u0302 [\u03a6] < +\u221e, and fix n > 0. Since \u2119\u0302 is tight, there exists a compact subset  \u0302 \u2282\u2282X\nsufficiently large such that E \u2119\u0302 [| 5 |1  \u03022 ] < n and E \u2119\u0302 [|\u03a6|1  \u03022 ] < n . Moreover, fix some G\u03020 \u2208X , we define\n= =  \u0302 \u222a = (G\u03020) =  \u0302 \u222a {G \u2208X : 3 (G\u03020, G) \u2264 =},\nand define\nq= (G\u0302, G) = q(G\u0302, G) \u2212\u221e1{G \u2209 =}, \u03a6= (G\u0302) = sup G\u2208 = q(G\u0302, G) = sup G\u2208X q= (G\u0302, G).\nThen \u03a6=\u2192\u03a6 pointwise as =\u2192\u221e. Since 5 \u2264\u03a6= \u2264\u03a6 in  \u0302, From dominant convergence theorem, we have\nE \u2119\u0302 [\u03a61  \u0302 ] \u2212E \u2119\u0302 [\u03a6=1 \u0302 ] < n\nfor = sufficiently large. We fix = from now on. Note that q(G\u0302, G) is uniformly continuous in G\u0302 in  \u0302 \u00d7 =: there exists X > 0 such that if G\u03021, G\u03022 \u2208  \u0302 and 3 (G\u03021, G\u03022) < X, we must have |q(G\u03021, G) \u2212q(G\u03022, G) | \u2264 n for all G \u2208 =, and consequently |\u03a6= (G\u03021) \u2212 \u03a6= (G\u03022) | \u2264 n . Since  \u0302 is compact, there exists a X-net X\u0302 = {G\u03028} = 8=1 \u2282  \u0302. Define *8 =  \u0302 \u2229 X (G\u03028) \\ \u22c3 9<8 X (G\u0302 9), then {*8} = 8=1\nforms a partition of  \u0302. For each G\u03028, we can find G8 such that\nq(G\u03028, G8) >\u03a6(G\u03028) \u2212 n .\nNow we construct a Borel-measurable selection mapping\n) (G\u0302) = { G8 G\u0302 \u2208*8 G G\u0302 \u2208  \u03022 , G\u0302 \u2208X .\nThis induces a measure W = (Id\u2297) )#\u2119\u0302. Under this selection, we have\nEW [q] = E\u2119\u0302\n[ q( -\u0302,) ( -\u0302)) ] = E\n\u2119\u0302\n[ q( -\u0302, -\u0302)1{-\u0302 \u2208  \u03022} ] + =\u2211\n8=1\nE \u2119\u0302\n[ q( -\u0302, G8)1{-\u0302 \u2208*8} ] .\nThe first term is\nE \u2119\u0302\n[ q( -\u0302, -\u0302)1{-\u0302 \u2208  \u03022} ] = E\n\u2119\u0302 [ 5 1  \u03022 ] \u2265 \u2212n .\nec5\nFor the second term, note that |G\u0302 \u2212 G\u03028 | < X for G\u0302 \u2208*8, so by uniform continuity we have\nq(G\u0302, G8) \u2265 q= (G\u0302, G8) \u2265 q= (G\u03028, G8) \u2212 n \u2265\u03a6= (G\u03028) \u2212 2n \u2265\u03a6= (G\u0302) \u2212 3n .\nHence we have\nEW [q] \u2265 \u22124n + =\u2211 8=1 E \u2119\u0302 [ \u03a6= ( -\u0302)1{-\u0302 \u2208*8} ] = \u22124n +E \u2119\u0302 [\u03a6=1 \u0302 ] > \u22125n +E\u2119\u0302 [\u03a61 \u0302 ] > \u22126n +E\u2119\u0302 [\u03a6].\nSince n can be chosen arbitrarily small, we proved interchangeability for q. The case E \u2119\u0302 [\u03a6] = +\u221e is similar and we omit the proof.\nRemark EC.1. It can be seen from the proof that beyond the Wasserstein setting, we need 2 to be continuous in G\u0302 uniformly in  \u0302 \u00d7 = for any compact  \u0302 and some sequence of = \u2283  \u0302 such that \u22c3 = = = X . In particular, this would hold if 2 is continuous and X is a f-compact metrizable topological space.\nAppendix EC.3: Proofs for Section 4\nIn this section, we provide additional details of the proof in Example 4, Example 5, and Example 6. Proof of Example 4. We start with C =) . We have\n+) (B) = inf 0\u2208A(B) 6) (B, 0),\nwhich is lower semi-analytic [4, Proposition 7.47], and in particular, \u2119\u0302-measurable [4, Corollary 7.42.1]. Since 6) is bounded from below by our assumption, +) is also bounded from below. In a Borel space or Polish space, any measure is tight. Thus by Proposition 2 we have\n+)\u22121(B) = inf 0\u2208A(B)\n{ 6)\u22121(B, 0) + sup\n\u2119\u2208M (B,0)\nEB\u2032\u223c\u2119( \u00b7 |B,0) [+) (B \u2032)]\n}\n= inf 0\u2208A(B) _\u22650\n{ 6)\u22121(B, 0) +_d(B, 0) +EB\u0302\u2032\u223c\u2119\u0302( \u00b7 |B,0) [ sup B\u2032\u2208S { +) (B \u2032) \u2212_2( B\u0302\u2032, B\u2032) }]} ,\nwhich is also bounded from below since 6)\u22121 is bounded from below by assumption and d > 0. Suppose we have shown +C+1 (\u00b7) is lower semi-analytic, +C is bounded from below and obtain the reformulation for +C . Now let us show that +C (\u00b7) is lower semi-analytic and derive the expression for +C\u22121 and show it is bounded from below. By the continuity of 2, the function B\u0302 \u2032 \u21a6\u2192 supB\u2032\u2208S { +C+1 (B \u2032) \u2212\n_2( B\u0302\u2032, B\u2032) } is lower semi-continuous, which is Borel measurable and thus lower semi-analytic. Hence the function (B, 0) \u21a6\u2192 E B\u0302\u2032\u223c\u2119\u0302( \u00b7 |B,0) [ supB\u2032\u2208S { +C+1 (B \u2032) \u2212_2( B\u0302\u2032, B\u2032) }] is lower semi-analytic [4, Proposition 7.48]. Since 6C\u22121 and d are lower semi-analytic due to our assumptions, +C is also lower semi-analytic [4, Proposition 7.47]. Then using Proposition 2 again we have\n+C\u22121(B) = inf 0\u2208A(B)\n{ 6C\u22121(B, 0) + sup\n\u2119\u2208M (B,0)\nEB\u2032\u223c\u2119( \u00b7 |B,0) [+C (B \u2032)]\n}\n= inf 0\u2208A(B) _\u22650\n{ 6C\u22121(B, 0) +_d(B, 0) +EB\u0302\u2032\u223c\u2119\u0302( \u00b7 |B,0) [ sup B\u2032\u2208S { +C (B \u2032) \u2212_2( B\u0302\u2032, B\u2032) }]} ,\nwhich is bounded from below since 6C\u22121, d,+C are all bounded from below. Therefore the proof is completed.\nec6\nProof of Example 5. We start with C =) . In this case,\n&) (D)\u22121, G) ) = inf D\u2208U) (D)\u22121 ,G) ) 5) (D, G)).\nSince 5) is random lower semi-continuous and U) is uniformly bounded, &) (\u00b7, \u00b7) is random lower semi-continuous [21, Theorem 9.50], and particularly, &) (D)\u22121, \u00b7) is measurable. Since 5) is bounded from below by our assumption, &) is also bounded from below. Thus Assumption 1 holds and using Example 3 we have\n&)\u22121(D)\u22122, G)\u22121) = inf D\u2208U)\u22121 (D)\u22122 ,G)\u22121 )\n{ 5)\u22121(D, G)\u22121) + sup\n\u2119\u2208M)\nE\u2119 [ &) (D, G) )\n] }\n= inf D\u2208U)\u22121 (D)\u22122 ,G)\u22121 )\n_\u22650\n{ 5)\u22121(D, G)\u22121) +_d) +E\u2119\u0302) [ sup G\u2208X) { &) (D, G) \u2212_2(G\u0302) , G) } ]} ,\nwhich is also bounded from below since 5)\u22121, d) , &) are all bounded from below. Suppose we have shown &C+1 (\u00b7, \u00b7) is random lower semi-continuous and obtain the reformulation for &C that is bounded from below. Now let us show that &C (\u00b7, \u00b7) is random lower semi-continuous and derive the expression for&C\u22121 and show it is bounded from below. Since&C+1 (\u00b7, \u00b7) is random lower semicontinuous, D \u21a6\u2192&C+1 (D, G) is lower semi-continuous for every G \u2208 XC+1. Therefore, for every G, G\u0302C+1 \u2208 XC+1, the function D \u21a6\u2192 &C+1 (D, G) \u2212 _2(G\u0302C+1, G) is lower semi-continuous, and thus their supremum D \u21a6\u2192 supG\u2208XC+1 {&C+1 (D, G) \u2212_2(G\u0302C+1, G)} is lower semi-continuous, and taking the expectation with respect to \u2119\u0302C+1, the function E\u2119\u0302C+1 [ supG\u2208XC+1 { &C+1 (D, G) \u2212_2(G\u0302C+1, G) }] is lower semi-continuous in D. It follows from [21, Theorem 9.50] that &C (\u00b7, \u00b7) is random lower semi-continuous. Then using Example 3 we have\n&C\u22121(DC\u22122, GC\u22121) = inf D\u2208UC\u22121 (DC\u22122 ,GC\u22121 )\n{ 5C\u22121(D, GC\u22121) + sup\n\u2119\u2208MC\nE\u2119 [ &C (D, GC)\n] }\n= inf D\u2208UC\u22121 (DC\u22122 ,GC\u22121 )\n_\u22650\n{ 5C\u22121(D, GC\u22121) +_dC +E\u2119\u0302C [ sup G\u2208XC { &C (D, G) \u2212_2(G\u0302C , G) } ]} ,\nwhich is again bounded from below, and thereby we complete the induction.\nProof of Example 6. Define 5 = 1S2 , 2(G\u0302, G) = 3 (G\u0302, G) ?. Then (3) is equivalent to\nL(d?) = sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) \u2264 d ? } \u2264 V.\nFor ? \u2208 [1,\u221e), we observe the following for each G\u0302 \u2208X :\nsup G\u2208X { 5 (G) \u2212_2(G\u0302, G)} = sup G\u2208S2 {1\u2212_3 (G\u0302, G) ?} \u2228 sup G\u2208S {0\u2212_3 (G\u0302, G) ?}\n= (1\u2212_3 (G\u0302,S2) ?) \u2228 (\u2212_3 (G\u0302,S) ?)\n=\n{ 1, G\u0302 \u2208 S2\n(1\u2212_3 (G\u0302,S2) ?)+, G\u0302 \u2208 S\n= (1\u2212_3 (G\u0302,S2) ?)+.\nBy Theorem 1, if 5 \u2212 _2 satisfies (IP) for every _ > 0, then the dual problem can be calculated as the following:\n(\u2212L)\u2217(\u2212_) = E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212_2( -\u0302, G) }] = E -\u0302\u223c\u2119\u0302 [ (1\u2212_3 ( -\u0302,S2) ?)+ ] .\nec7\nTherefore, for every d > 0, we have\nL(d?) =min _\u22650 {_d? + (\u2212L)\u2217(\u2212_)} =min _\u22650\n{ _d? +E\n-\u0302\u223c\u2119\u0302\n[ (1\u2212_3 ( -\u0302,S2) ?)+ ]} .\nFor V \u2208 (0,1), the chance constraint can be written as\nL(d?) \u2264 V \u21d0\u21d2 _d? +E -\u0302\u223c\u2119\u0302 [ (1\u2212_3 ( -\u0302,S2) ?)+ ] \u2264 V for some _ \u2265 0\n\u21d0\u21d2 _d? +E -\u0302\u223c\u2119\u0302 [ (1\u2212_3 ( -\u0302,S2) ?)+ ] \u2264 V for some _ > 0 \u21d0\u21d2 d?\nV + 1 V E -\u0302\u223c\u2119\u0302\n[( 1\n_ \u2212 3 ( -\u0302,S2) ?\n)\n+\n] \u2264 1\n_ for some _ > 0\n\u21d0\u21d2 U + 1\nV E -\u0302\u223c\u2119\u0302\n[( \u22123 ( -\u0302,S2) ? \u2212 U ) + ] \u2264 \u2212 d? V for some U < 0\n\u21d0\u21d2 U + 1\nV E -\u0302\u223c\u2119\u0302\n[( \u22123 ( -\u0302,S2) ? \u2212 U ) + ] \u2264 \u2212 d? V for some U \u2208\u211d\n\u21d0\u21d2 \u2102V@R\u2119\u0302V (\u22123 ( -\u0302,S 2) ?) =min\nU\u2208\u211d\n{ U + 1\nV E -\u0302\u223c\u2119\u0302\n[( \u22123 ( -\u0302,S2) ? \u2212 U ) + ]} \u2264 \u2212 d? V .\nAppendix EC.4: Proofs for Section 5.1\nProof of Proposition 3. We compute L \u25e6 (d) as follows.\nL \u25e6 (d) = sup\n\u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) < d }\n= sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] : inf\nW\u2208\u0393(\u2119\u0302,\u2119) W- ess sup G\u0302,G\u2208X 2(G\u0302, G) < d\n}\n= sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] : W- ess sup\nG\u0302,G\u2208X\n2(G\u0302, G) < d for some W \u2208 \u0393(\u2119\u0302,\u2119)\n}\n= sup W\u2208\u0393\n\u2119\u0302\n{ E (-\u0302,-)\u223cW\n[ 5 (-)] : W-ess sup G\u0302,G\u2208X 2(G\u0302, G) < d\n}\n= sup W\u2208\u0393\n\u2119\u0302\n{ E (-\u0302,-)\u223cW [ 5 (-) \u2212\u221e1{2( -\u0302, -) \u2265 d} ]}\n= E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) \u2212\u221e1{2( -\u0302, G) \u2265 d} }] .\n= E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) < d }] .\nIn the second to the last line, we need (IP) on the function q(G\u0302, G) = 5 (G) \u2212\u221e1{2(G\u0302, G) \u2265 d}. Next, we compute the dual\n(\u2212L \u25e6 )\u2217(\u2212_) = sup\nd\u22650\n{ L \u25e6 (d) \u2212_d } = sup d\u22650 sup \u2119\u2208P (X ) { E-\u223c\u2119 [ 5 (-)] \u2212_d :W\u221e(\u2119\u0302,\u2119) < d }\n= sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] \u2212_W\u221e (\u2119\u0302,\u2119) }\n= sup d\u22650 sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] \u2212_d :W\u221e(\u2119\u0302,\u2119) \u2264 d }\n= sup d\u22650\n{ L(d) \u2212_d } = (\u2212L)\u2217(\u2212_).\nec8\nIt follows that both (\u2212L \u25e6 )\u2217 (\u2212_) and (\u2212L)\u2217 (\u2212_) equal the soft-constrained robust loss. Thus\n(\u2212L)\u2217(\u2212_) = (\u2212L \u25e6 )\u2217(\u2212_) = sup\nd\u22650\n{ E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) < d ] \u2212_d }} .\nThis completes the proof of the proposition. We remark that L \u25e6 is no longer necessarily concave, so (\u2212L)\u2217\u2217 (d) may differ from \u2212L \u25e6 (d).\nProof of Theorem 2. Similarly to the above proof of Propositions 3, we have\nL(d) = sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] :K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] : inf\nW\u2208\u0393(\u2119\u0302,\u2119) W- ess sup G\u0302,G\u2208X 2(G\u0302, G) \u2264 d\n}\n\u2264 sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] : W-ess sup\nG\u0302,G\u2208X\n2(G\u0302, G) \u2264 d for some W \u2208 \u0393(\u2119\u0302,\u2119)\n}\n= sup W\u2208\u0393\n\u2119\u0302\n{ E (-\u0302,-)\u223cW\n[ 5 (-)] : W- ess sup G\u0302,G\u2208X 2(G\u0302, G) \u2264 d\n}\n= sup W\u2208\u0393\n\u2119\u0302\n{ E (-\u0302,-)\u223cW [ 5 (-) \u2212\u221e1{2( -\u0302, -) > d} ] }\n= E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) \u2212\u221e1{2( -\u0302, G) > d} : 2( -\u0302, G) \u2264 d }] .\n= E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) \u2264 d }] .\nHere we used (IP) as it holds for all measurable functions according to Example 2. Inequality becomes equality if\ninf W\u2208\u0393(\u2119\u0302,\u2119) W-ess sup G\u0302,G\u2208X 2(G\u0302, G) \u2264 d (EC.1)\ncan be achieved at some W \u2208 \u0393(\u2119\u0302,\u2119). Now we use the additional information that X is Polish. If (EC.1) holds, we first find W= \u2208 \u0393(\u2119\u0302,\u2119) with suppW= \u2282 {2 \u2264 d + 1 = }. For any n > 0, we can find compact sets  \u0302, \u2282 X with \u2119\u0302[ \u0302] > 1 \u2212 n and \u2119[ ] > 1 \u2212 n , because \u2119\u0302 and \u2119 are probability measures on a Polish space, which are tight. Then W= [ \u0302 \u00d7 ] > 1\u22122n for each =. This shows {W=}= is a tight sequence. Since X is complete and separable, by Prokhorov theorem, there exists a weakly converging subsequence W=: \u2192 W \u2208 P (X \u00d7X ). Marginals of W= also converge weakly to the marginals of W, so W \u2208 \u0393(\u2119\u0302,\u2119). To show that W is supported in {2 \u2264 d}, define 6 :X \u00d7X \u2192\u211d by\n6(G\u0302, G) = 1\u2227 (2(G\u0302, G) \u2212 d)+.\n6 is continuous and bounded on X \u00d7X . Moreover, EW= [6] \u2264 1 = . By weak convergence, EW [6] = 0, so 2(G\u0302, G) \u2264 d for W-a.e. (G\u0302, G) \u2208X \u00d7X . That is, W-ess sup2 \u2264 d. Next, we compute the dual.\n(\u2212L)\u2217(\u2212_) = sup d\u22650\n{ L(d) \u2212_d } = sup d\u22650 sup \u2119\u2208P (X ) { E-\u223c\u2119 [ 5 (-)] \u2212_d :W\u221e (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X )\n{ E-\u223c\u2119 [ 5 (-)] \u2212_W\u221e (\u2119\u0302,\u2119) } .\nec9\nThus\n(\u2212L)\u2217(\u2212_) = sup d\u22650\n{ E -\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 2( -\u0302, G) \u2264 d ] \u2212_d }} .\nThis completes the proof of the theorem.\nProof of Example 7. By Theorem 2, we have\nL(d) = sup \u2119\u2208P (X ) {E-\u223c\u2119 [ 5 (-)] :W\u221e(\u2119\u0302,\u2119) \u2264 d} = E-\u0302\u223c\u2119\u0302 [ sup G { 5 (G) : 3 ( -\u0302, G) \u2264 d }] = \u2119\u0302(3 ( -\u0302,S2) \u2264 d).\nIn particular, L(d) = 1 if d \u2265 3 (supp \u2119\u0302,S2). We remark that now the corresponding soft robust problem is\n(\u2212L)\u2217(\u2212_) = sup d\u22650\n{ \u2119\u0302(3 ( -\u0302,S2) \u2264 d) \u2212_d } .\nAppendix EC.5: Proofs for Section 5.2\nProof of Theorem 3. First, we consider the maximum transport cost K2. Similar as Theorem 2, we have\nLJ (d) = sup \u2119\u2208P (X ) { inf U\u2208 E-\u223c\u2119 [ 5U (-)] :K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X ) { inf U\u2208 \u2032 E-\u223c\u2119 [ 5U (-)] :K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X ) { inf U\u2208 \u2032 E-\u223c\u2119[ 5U (-)] : inf W\u2208\u0393(\u2119\u0302,\u2119) W- ess sup G\u0302,G\u2208X 2(G\u0302, G) \u2264 d }\n= sup \u2119\u2208P (X ) { inf U\u2208 \u2032 E-\u223c\u2119[ 5U (-)] : W- ess sup G\u0302 ,G\u2208X 2(G\u0302, G) \u2264 d for some W \u2208 \u0393(\u2119\u0302,\u2119) }\n= sup W\u2208\u0393\n\u2119\u0302\n{ inf U\u2208 \u2032 E-\u223c\u2119 [ 5U (-)] : W- ess sup G\u0302,G\u2208X 2(G\u0302, G) \u2264 d }\n= sup W\u2208\u0393\n\u2119\u0302\n{ inf U\u2208 \u2032 E (-\u0302,-)\u223cW [ 5U (-) \u2212\u221e1{2( -\u0302, -) > d} ] } .\nWe claim that for each W \u2208 \u0393 \u2119\u0302 , \u2032 \u220b U \u21a6\u2192 EW [ 5U (-)] has the following properties:\n(a) Convexity: given U0, U1 \u2208 \u211d, \\ \u2208 (0,1), define U\\ = (1 \u2212 \\)U0 + \\U1. Due to convexity of 5U in U we have 5U\\ (G) \u2264 (1\u2212 \\) 5U0 (G) + \\ 5U1 (G) for every G \u2208X . So\nEW [ 5U\\ (-)] \u2264 (1\u2212 \\)EW [ 5U0 (-)] + \\EW [ 5U1 (-)].\n(b) Lower semi-continuity: let U= \u2192 U as =\u2192\u221e. Due to lower semi-continuity of 5U in U we have 5U (G) \u2264 lim inf=\u2192\u221e 5U= (G) for every G \u2208X . So\nEW [ 5U (-)] \u2264 EW [ lim inf =\u2192\u221e 5U= (-) ] \u2264 lim inf =\u2192\u221e EW [ 5U= (-)].\nThe second inequality is due to Fatou\u2019s lemma thanks to { 5U}U\u2208 \u2032 being uniformly bounded from below.\nec10\nSince \u2032 is compact, by Sion\u2019s minimax theorem and interchangeability,\nLJ (d) = inf U\u2208 \u2032 { sup W\u2208\u0393\n\u2119\u0302\nE (-\u0302,-)\u223cW\n[ 5U (-) \u2212\u221e1{2( -\u0302 , -) > d}\n]}\n= inf U\u2208 \u2032 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) \u2212\u221e1{2( -\u0302, G) > d} : 2( -\u0302, G) \u2264 d }] .\n= inf U\u2208 \u2032 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) : 2( -\u0302 , G) \u2264 d }] =: inf U\u2208 \u2032 \u2113U.\nTo complete the proof, we need to enlarge \u2032 to again. Let U= be a minimizing sequence:\nlim =\u2192\u221e \u2113U= = inf U\u2208 \u2113U.\nDenote \u2032= = \u2032 \u222a {U=}. \u2032 = is still compact, so the same argument on \u2032 instead of gives LJ (d) = infU\u2208 \u2032= \u2113U. This holds for every =, so\nLJ (d) = inf U\u2208 \u2113U = inf U\u2208 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) : 2( -\u0302 , G) \u2264 d }] .\nNext, we consider the Kantorovich costK2. It is easy to see that is concave in \u2119. Similar as Lemma 1, it is a simple exercise to show LJ (\u00b7) is lower bounded by supU E\u2119\u0302[ 5U], monotonically increasing, and concave on [0,\u221e). Now we take the dual of \u2212LJ:\n(\u2212LJ) \u2217 (\u2212_) := sup\nd\u22650 {LJ (d) \u2212_d} = sup d\u22650,\u2119\u2208P (X )\n{ (\u2119) \u2212_d :K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X )\n{ (\u2119) \u2212_K2 (\u2119\u0302,\u2119) : - \u223c\u2119 }\n= sup W\u2208\u0393\n\u2119\u0302\ninf U\u2208\n{ EW [ 5U (-) \u2212_2( -\u0302, -)] } .\nSince (\u2212LJ) \u2217(\u2212_) is convex, lower semi-continuous and decreasing in _, we know that there exists _ \u2265 0, such that (\u2212LJ) \u2217(\u2212_) =\u221e for _ < _, and (\u2212LJ)\n\u2217(\u2212_) <\u221e for _ \u2265 _. For each fixed _ > _, note that\n(\u2212LJ) \u2217(\u2212_) = sup\nd\u22650 {LJ (d) \u2212_d} = sup d\u22650 {LJ (d) \u2212_d \u2212 (_ \u2212_)d}\nis achieved beneath a finite value of d (depending on _). Hence we can make a restriction without changing the value of (\u2212LJ) \u2217(\u2212_):\n(\u2212LJ) \u2217 (\u2212_) = sup\n0\u2264d\u2264d {LJ (d) \u2212_d} = sup 0\u2264d\u2264d,\u2119\u2208P (X )\n{ (\u2119) \u2212_d :K2 (\u2119\u0302,\u2119) \u2264 d }\n= sup \u2119\u2208P (X )\n{ (\u2119) \u2212_K2 (\u2119\u0302,\u2119) :K2 (\u2119\u0302,\u2119) \u2264 d } .\nWith this restriction, we can further restrict the optimization over U to a compact set \u2032 (depending on _):\n(\u2212LJ) \u2217(\u2212_) = sup\nW\u2208\u0393 \u2119\u0302\ninf U\u2208 \u2032\n{ EW [ 5U (-) \u2212_2( -\u0302, -)] } .\nec11\nWe have shown that U \u21a6\u2192 EW [ 5U (-)] is lower semi-continuous and convex in the first half of the proof. By Sion\u2019s minimax theorem we can exchange sup and inf, so for every _ > _, it holds that\n(\u2212LJ) \u2217(\u2212_) = inf\nU\u2208 \u2032 sup W\u2208\u0393\n\u2119\u0302\n{ EW [ 5U (-) \u2212_2( -\u0302 , -)] } = inf U\u2208 \u2032 E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302 , G) }] .\nHere we used (IP) on q_,U = 5U \u2212_2. By enlarging \u2032 to as in the maximal cost case, we have\n(\u2212LJ) \u2217(\u2212_) = inf\nU\u2208 E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }] .\nFor each _ \u2264 _, we can still replace \u201c=\u201d with \u201c\u2264\u201d since sup inf is always bounded by inf sup. Therefore\n(\u2212LJ) \u2217(\u2212_) \u2264 inf\nU\u2208 E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }] .\nBut for each _ < _, (\u2212LJ) \u2217(\u2212_) = +\u221e, so the above is still equality. The only place where the inequality could be strict is at _ = _. We can compute it by (\u2212LJ) \u2217(\u2212_) = lim_\u2192_+ (\u2212LJ) \u2217(\u2212_) because (\u2212LJ) \u2217(\u2212_) is right continuous by Lemma 3. Therefore,\nLJ(d) = inf _\u22650,U\u2208\n{_d + (\u2212LJ) \u2217(\u2212_)} = inf\n_\u22650,_\u2260_,U\u2208 {_d + (\u2212LJ)\n\u2217(\u2212_)}\n= inf _\u22650,_\u2260_,U\u2208\n{ _d +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302 , G) }]}\n= inf _\u22650,U\u2208\n{ _d +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }]} .\nTo apply Theorem 3 on Example 8, we need to verify that the infimum is achieved in a finite interval dependent only on the transport cost. The following lemma confirms this property for \u2102V@R and MAD.\nLemma EC.2. Suppose X = \u211d. Let \u2119\u0302,\u2119 \u2208 P (X ), V \u2208 (0,1). Let -\u0302 \u223c \u2119\u0302, - \u223c \u2119. If \u2119(- \u2265 U) \u2265 V and \u2119(- \u2264 U) \u2265 1\u2212 V, then\nU \u2208 [ \u2212\u2102V@R\u2119\u03021\u2212V (\u2212-\u0302) \u2212 1\n1\u2212 V W1(\u2119\u0302,\u2119),\u2102V@R\n\u2119\u0302 V ( -\u0302) + 1\nV W1(\u2119\u0302,\u2119)\n] .\nProof. Given any W \u2208 \u0393(\u2119\u0302,\u2119), we have\nE (-\u0302,-)\u223cW\n[ \u2016 -\u0302 \u2212 - \u2016 ] \u2265 E\n(-\u0302,-)\u223cW\n[ \u2016- \u2212 -\u0302 \u20161{- \u2265 U} ]\n\u2265 E (-\u0302,-)\u223cW\n[ (U\u2212 -\u0302)1{- \u2265 U} ]\n= U\u2119(- \u2265 U) \u2212E -\u0302\u223c\u2119\u0302\n[ -\u0302E-\u223cW\n-|-\u0302\n[ 1{- \u2265 U} | -\u0302 ] ]\n=\u2119(- \u2265 U) ( U\u2212E\n-\u0302\u223c\u2119\u0302\n[ -\u0302 \u00b7 \u2119(- \u2265 U | -\u0302)\n\u2119(- \u2265 U)\n])\nNote that E -\u0302\u223c\u2119\u0302\n[ \u2119(- \u2265 U | -\u0302)\n\u2119(- \u2265 U)\n] = 1, and \u2119(- \u2265 U | -\u0302)\n\u2119(- \u2265 U) \u2264 1 V . Therefore\nE -\u0302\u223c\u2119\u0302\n[ -\u0302 \u00b7 \u2119(- \u2265 U | -\u0302)\n\u2119(- \u2265 U)\n] \u2264 sup\n\u211a\u0302\u2208P (X )\n{ E -\u0302\u223c\u211a\u0302 [-\u0302] : \u211a\u0302\u226a \u2119\u0302, 3\u211a\u0302\n3\u2119\u0302 \u2264\n1\nV\n} =\u2102V@R\u2119\u0302V ( -\u0302).\nec12\nHere we used the dual formulation for CVaR in [9]. Hence, we have shown that\nU\u2212\u2102V@R\u2119\u0302V ( -\u0302) \u2264 1\n\u2119(- \u2265 U) E (-\u0302,-)\u223cW\n[ \u2016 -\u0302 \u2212 - \u2016 ] \u2264 1\nV E (-\u0302,-)\u223cW\n[ \u2016 -\u0302 \u2212 - \u2016 ] .\nThe proof of the upper bound is completed by taking infimum over W \u2208 \u0393(\u2119\u0302,\u2119). The proof of the lower bound is similar:\nE (-\u0302,-)\u223cW\n[ \u2016 -\u0302 \u2212 - \u2016 ] \u2265 E\n(-\u0302,-)\u223cW\n[ \u2016 -\u0302 \u2212 - \u20161{- \u2264 U} ]\n\u2265 E (-\u0302,-)\u223cW\n[ ( -\u0302 \u2212 U)1{- \u2264 U} ]\n= E -\u0302\u223c\u2119\u0302\n[ -\u0302E-\u223cW\n-|-\u0302\n[ 1{- \u2264 U} | -\u0302 ] ] \u2212 U\u2119(- \u2264 U)\n=\u2119(- \u2264 U) ( E -\u0302\u223c\u2119\u0302 [ -\u0302 \u00b7 \u2119(- \u2264 U | -\u0302)\n\u2119(- \u2264 U)\n] \u2212 U )\n\u2265 \u2119(- \u2264 U) ( \u2212\u2102V@R\u2119\u03021\u2212V (\u2212-\u0302) \u2212 U ) .\nProof of Example 8 (\u2102V@R). Given /\u0302 \u223c \u211a\u0302 and / \u223c\u211a, define -\u0302 = 1\u22a4 /\u0302 and - = 1\u22a4/, and let \u2119\u0302, \u2119 denote the law of -\u0302 and - respectively. We observe the following\n(a) \u2102V@R\u211a V (1\u22a4/) =\u2102V@R\u2119V (-) and \u2102V@R \u211a\u0302 V (1\u22a4 /\u0302) =\u2102V@R\u2119\u0302V ( -\u0302). (b) For any \u211a \u2208 P (Z), W? (\u2119\u0302,\u2119) \u2264 \u20161\u2016\u2217W? (\u211a\u0302,\u211a).\n(c) For any \u2119\u2032 \u2208 P (\u211d), we can find \u211a \u2208 P (Z) such that \u2119 =\u2119\u2032, and W? (\u2119\u0302,\u2119) \u2265 \u20161\u2016\u2217W? (\u211a\u0302,\u211a).\nIf all these claims are true, then\nsup \u211a\u2208P (Z )\n{ \u2102V@R\u211a\nV (1\u22a4/) :W? (\u211a\u0302,\u211a) \u2264 d\n} = sup\n\u2119\u2208P (\u211d)\n{ \u2102V@R\u2119V (-) :W? (\u2119\u0302,\u2119) \u2264 \u20161\u2016\u2217d } .\nThe first two claims are direct: - = 1\u22a4/, -\u0302 = 1\u22a4 /\u0302, and |- \u2212 -\u0302 | \u2264 \u20161\u2016\u2217\u2016/ \u2212 /\u0302 \u2016. For the third claim, we prove it as follows. Let 1\u2217 \u2208 Z be the unit dual of 1\u22a4, i.e. 1\u22a41\u2217 = \u20161\u22a4\u2016\u2217 and \u20161 \u2217\u2016 = 1. Given /\u0302 \u223c \u2119\u0302, -\u0302 = 1\u22a4- \u223c \u211a\u0302, - \u2032 \u223c \u2119\u2032, define / = /\u0302 + (- \u2032 \u2212 -\u0302)1\u2217/\u20161\u22a4\u2016\u2217. Then - = 1 \u22a4/ = -, \u2119\u2032 = \u2119, and \u2016 /\u0302 \u2212 / \u2016 = \u20161\u22a4\u2016\u22121\u2217 \u2016 -\u0302 \u2212 - \u2016, thus W? (\u211a\u0302,\u211a) \u2264 \u20161 \u22a4\u2016\u22121\u2217 W? (\u2119\u0302,\u2119). We have transformed the problem to the following form: with ? <\u221e, 2(G\u0302, G) = |G\u0302 \u2212 G |?,\nsup \u211a\u2208P (Z )\n{ \u2102V@R\u211a\nV (1\u22a4/) :W? (\u211a\u0302,\u211a) \u2264 d\n} = sup\n\u2119\u2208P (X )\n{ (\u2119) :K2 (\u2119\u0302,\u2119) \u2264 (\u20161 \u22a4\u2016\u2217d) ? } =LJ ((\u20161 \u22a4\u2016\u2217d) ?),\nand with ? =\u221e, 2(G\u0302, G) = |G\u0302 \u2212 G |?,\nsup \u211a\u2208P (Z )\n{ \u2102V@R\u211a\nV (1\u22a4/) :W? (\u211a\u0302,\u211a) \u2264 d\n} = sup\n\u2119\u2208P (X )\n{ (\u2119) :K2 (\u2119\u0302,\u2119) \u2264 \u20161 \u22a4\u2016\u2217d } =LJ (\u20161 \u22a4\u2016\u2217d).\nFor simplicity, assume \u20161\u2217\u2016 = 1 from now on. To apply Theorem 3, we verify the following prerequisites: \u2022 5U satisfies Assumption 1: 5U \u2265 U so E\u2119\u0302[ 5U] \u2265 U > \u2212\u221e. \u2022 5U is lower semi-continuous and convex in U: this is obvious since 5U =max{U, 1 V G + (1\u2212 1 V )U} is\nthe maximum of two affine functions.\n\u2022 infU\u2208 \u2032 ,G\u2208X 5U (G) > \u2212\u221e for compact \u2032 \u2282 \u211d: infU\u2208 \u2032 ,G\u2208X 5U (G) = infU\u2208 \u2032 U = minU\u2208 \u2032 U > \u2212\u221e,\nsince \u2032 is compact and bounded.\n\u2022 5U \u2212_2 satisfies (IP): this is because X is a Euclidean space, which is complete and separable.\nec13\n\u2022 (5) holds for \u2119 in Wasserstein ball: from [17] we know that for CVaR problem, the minimum of (4) is attained on a nonempty closed bounded interval U \u2208 [U, U] (possibly a singleton). This interval contains U such that \u2119(- \u2265 U) \u2265 V and \u2119(- \u2264 U) \u2265 1\u2212 V. By Lemma EC.2, this interval is contained in\n\u2032 = [ \u2212\u2102V@R\u2119\u03021\u2212V (\u2212-\u0302) \u2212 1\n1\u2212 V W1(\u2119\u0302,\u2119),\u2102V@R\n\u2119\u0302 V ( -\u0302) + 1\nV W1(\u2119\u0302,\u2119)\n] .\nSince W1(\u2119\u0302,\u2119) \u2264W? (\u2119\u0302,\u2119) for ? \u2208 [1,\u221e], we have verified all the prerequisites of Theorem 3. When ? =\u221e, by Theorem 3, we conclude\nLJ (d) = sup \u2119\u2208P\n{ \u2102V@R\u2119V (-) :W\u221e(\u2119\u0302,\u2119) \u2264 d } = inf U\u2208 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) : | -\u0302 \u2212 G | \u2264 d }] ,\nwhere\nsup G\n{ 5U (G) : | -\u0302 \u2212 G | \u2264 d } = sup\nG\n{ U + 1\n1\u2212 V (G \u2212 U)+ : | -\u0302 \u2212 G | \u2264 d\n} = U + 1\n1\u2212 V ( -\u0302 + d \u2212 U)+.\nWe thus conclude that\nLJ (d) = inf U\u2208\n{ U + 1\n1\u2212 V E -\u0302\u223c\u2119\u0302 [( -\u0302 + d \u2212 U)+]\n} =\u2102V@R\u2119\u0302V ( -\u0302) + d.\nWhen ? = 1,\nsup G\u2208X { 5U (G) \u2212_2(G\u0302, G)} = sup G\u2208X\n{ U + 1\n1\u2212 V (G \u2212 U)+ \u2212_ |G\u0302 \u2212 G |\n} =   U + 1 1\u2212 V (G\u0302 \u2212 U)+ _ \u2265 1 1\u2212V \u221e _ < 11\u2212V .\nTherefore for _ \u2265 1 1\u2212V ,\ninf U\u2208\n{ E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }]} = inf U\u2208 { E -\u0302\u223c\u2119\u0302 [ U + 1 1\u2212 V ( -\u0302 \u2212 U)+ ]} =\u2102V@R\u2119\u0302V ( -\u0302).\nThus\nLJ (d) = inf U\u2208 ,_\u22650\n{ _d +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302 , G) }]} =\u2102V@R\u2119\u0302V ( -\u0302) + d 1\u2212 V .\nWhen ? > 1,\nsup G\u2208X\n{ 5U (G) \u2212_ |G\u0302 \u2212 G | ?} = sup\nG\u2208X\n{ U + 1\n1\u2212 V (G \u2212 U)+ \u2212_ |G\u0302 \u2212 G |\n?\n}\n= sup G\u2208X\n{ U + 1\n1\u2212 V (G \u2212 U) \u2212_ |G\u0302 \u2212 G |? } \u2228 sup G\u2208X {U\u2212_ |G\u0302 \u2212 G |?}\n= ( U + 1\n1\u2212 V (G\u0302 \u2212 U) + sup\nC\u2208\u211d\n{ C\n1\u2212 V \u2212_ |C |?\n}) \u2228U\n= U +\n( 1\n1\u2212 V (G\u0302 \u2212 U) + _\n\u2212 1?\u22121\n)\n+\n= U +\n( 1\n1\u2212 V\n( G\u0302 \u2212 (U\u2212 (1\u2212 V)_ \u2212 1 ?\u22121\n))\n+\n,\nec14\nwhere = (? \u2212 1) (?(1\u2212 V))\u2212 ? ?\u22121 . Thus\ninf U\u2208\n{ E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }]} =\u2102V@R\u2119\u0302V ( -\u0302) + (1\u2212 V)_ \u2212 1 ?\u22121 .\nTherefore\nLJ (d ?) =\u2102V@R\u2119\u0302V ( -\u0302) +min\n_\u22650\n{ _d? + (1\u2212 V)_ \u2212 1 ?\u22121 } =\u2102V@R\u2119\u0302V ( -\u0302) + d(1\u2212 V) \u2212 1 ? .\nIn conclusion, for ? \u2208 [1,\u221e], it holds that\nsup \u2119\u2208P\n{ \u2102V@R\u2119V (-) :W? (\u2119\u0302,\u2119) \u2264 d } =\u2102V@R\u2119\u0302V ( -\u0302) + d(1\u2212 V) \u2212 1 ? .\nProof of Example 8 (Var). Same as Example 8 (\u2102V@R), we can reduce to a one-dimensional problem and assume without loss of generality that \u20161\u22a4\u2016\u2217 = 1. It is well-known that the optimal U is the expectation:\nVar\u2119(-) =min U E[(- \u2212 U)2] = E[(- \u2212E[-])2].\nGiven \u2119\u0302,\u2119 and a transport plan W \u2208 \u0393(\u2119\u0302,\u2119), the transport cost\nEW [\u2016 -\u0302 \u2212 - \u2016] \u2265 |EW [-\u0302 \u2212 -] | \u2265 |E\u2119\u0302[-\u0302] \u2212E\u2119 [-] |.\nMinimizing over all W \u2208 \u0393(\u2119\u0302,\u2119) gives\nE-\u223c\u2119 [-] \u2264 [ E -\u0302\u223c\u2119\u0302 [-\u0302] \u2212W1 ( -\u0302, -),E-\u0302\u223c\u2119\u0302 [-\u0302] +W1 ( -\u0302, -) ] .\nSame as before, we can verify that 5U (G) = (G \u2212 U) 2 meets all the prerequisites of Theorem 3.\nWhen ? =\u221e, by Theorem 3, we conclude\nLJ (d) = sup \u2119\u2208P\n{ Var\u2119 (-) :W\u221e (\u2119\u0302,\u2119) \u2264 d } = inf U\u2208 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) : | -\u0302 \u2212 G | \u2264 d }] ,\nwhere\nsup G\n{ 5U (G) : | -\u0302 \u2212 G | \u2264 d } = sup\nG\n{ (G \u2212 U)2 : | -\u0302 \u2212 G | \u2264 d } = ( | -\u0302 \u2212 U | + d)2.\nWe thus conclude that\nLJ (d) = inf U\u2208\n{ E -\u0302\u223c\u2119\u0302 [ ( | -\u0302 \u2212 U | + d)2 ]} .\nWhen 1 \u2264 ? < 2, for any _ \u2265 0, it holds that\nsup G\u2208X { 5U (G) \u2212_2(G\u0302, G)} = sup G\u2208X\n{ (G \u2212 U)2 \u2212_ |G\u0302 \u2212 G |? } = +\u221e.\nThus for any d > 0 we must have\nLJ(d ?) = +\u221e.\nWhen ? = 2,\nsup G\u2208X\n{ 5U (G) \u2212_ |G\u0302 \u2212 G | 2 } = sup G\u2208X { (G \u2212 U)2 \u2212_(G\u0302 \u2212 G)2 }\n=   +\u221e 0 \u2264 _ < 1, or _ = 1, G\u0302 \u2260 U, or 0 _ = 1, G\u0302 = U _ _\u22121 (G\u0302 \u2212 U) 2 _ > 1.\nec15\nThus\ninf U\u2208\n{ E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302 , G) }]} =   +\u221e 0 \u2264 _ < 1, or _ = 1, \u2119\u0302\u2260 G\u0302 for any G\u0302 \u2208X 0 _ = 1, \u2119\u0302 = G\u0302 for some G\u0302 \u2208X _ _\u22121Var \u2119\u0302 ( -\u0302) _ > 1.\nWe now conclude\nLJ(d 2) = inf\n_>1\n{ _d2 + _\n_ \u2212 1 Var\u2119\u0302( -\u0302)\n} = (Var\u2119\u0302( -\u0302) 1 2 + d)2.\nProof of Example 8 (MAD). Same as Example 8 (\u2102V@R), we can reduce to a one-dimensional problem and assume without loss of generality that \u20161\u22a4\u2016\u2217 = 1. It is well-known that the optimal U is the median. By Lemma EC.2, the median of \u2119 inside the Wasserstein uncertainty set is attained in\nU \u2208 [ \u2212\u2102V@R\u2119\u03021\n2 (\u2212-\u0302) \u2212 2W1(\u2119\u0302,\u2119),\u2102V@R \u2119\u0302 1 2\n( -\u0302) +2W1(\u2119\u0302,\u2119) ] .\nSame as in Example 8 (\u2102V@R), we can verify that 5U (G) = |G \u2212 U | meets all the prerequisites of Theorem 3. When ? =\u221e, by Theorem 3, we conclude\nLJ (d) = sup \u2119\u2208P\n{ MAD\u2119(-) :W\u221e (\u2119\u0302,\u2119) \u2264 d } = inf U\u2208 E -\u0302\u223c\u2119\u0302 [ sup G { 5U (G) : | -\u0302 \u2212 G | \u2264 d }] ,\nwhere\nsup G\n{ 5U (G) : | -\u0302 \u2212 G | \u2264 d } = sup\nG\n{ |G \u2212 U | : | -\u0302 \u2212 G | \u2264 d } = | -\u0302 \u2212 U | + d.\nWe thus conclude that\nLJ (d) = inf U\u2208\n{ E -\u0302\u223c\u2119\u0302 [| -\u0302 \u2212 U |] + d } =MAD\u2119\u0302( -\u0302) + d.\nWhen ? = 1,\nsup G\u2208X { 5U (G) \u2212_2(G\u0302, G)} = sup G\u2208X {|G \u2212 U | \u2212_ |G\u0302 \u2212 G |} =\n{ |G\u0302 \u2212 U | _ \u2265 1 \u221e _ < 1 .\nTherefore for _ \u2265 1,\ninf U\u2208\n{ E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302 , G) }]} = inf U\u2208 { E -\u0302\u223c\u2119\u0302 [ | -\u0302 \u2212 U | ]} =MAD\u2119\u0302 ( -\u0302).\nThus\nLJ (d) = inf U\u2208 ,_\u22650\n{ _d +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5U (G) \u2212_2( -\u0302, G) }]} =MAD\u2119\u0302( -\u0302) + d.\nRobust loss for ? = 1 and ? =\u221e are both MAD\u2119\u0302( -\u0302) + d. As W1(\u2119\u0302,\u2119) \u2264W? (\u2119\u0302,\u2119) \u2264W\u221e (\u2119\u0302,\u2119), we have for any 1 \u2264 ? \u2264\u221e:\nsup \u2119\u2208P\n{ \u2102V@R\u2119V (-) :W? (\u2119\u0302,\u2119) \u2264 d } =MAD\u2119\u0302( -\u0302) + d.\nec16\nProof of Example 8 (Ent). Same as Example 8 (\u2102V@R), we can reduce to a one-dimensional problem and assume without loss of generality that \u20161\u22a4\u2016\u2217 = 1. U \u21a6\u2192 E[ 5U (-)] = U + 1 \\ ( E[4\\ (-\u2212U) ] \u2212 1 ) is convex, and limU\u2192\u00b1\u221eE[ 5U (-)] = +\u221e, so the minimizer U\u2217 satisfies\n0 = 3\n3U\nU=U\u2217\nE[ 5U (-)] = 1\u2212E[4 \\ (-\u2212U\u2217 ) ].\nTherefore, 4\\U \u2217 = E[4\\-]. Suppose U\u0302\u2217 is the minimizer to E[ 5U ( -\u0302)], and W- ess supG\u0302,G \u2016G\u0302\u2212G\u2016 \u2264 d, then\n4\\U \u2217 = E[4\\-] \u2264 E[4\\ (-\u0302+d) ] = E[4\\-\u0302]4\\d = 4\\ ( U\u0302 \u2217+d) .\nHence U\u2217 \u2264 U\u0302\u2217 + d. Similarly, U\u2217 \u2265 U\u0302\u2217 \u2212 d. Therefore,\nU \u2208 [U\u0302\u2217 \u2212W\u221e(\u2119\u0302,\u2119), U\u0302 \u2217 +W\u221e (\u2119\u0302,\u2119)].\nBy Theorem 3, we conclude for ? =\u221e:\nLJ (d) = inf U\u2208\u211d E -\u0302\u223c\u2119\u0302 [ sup G\u2208\u211d { U + 1 \\ ( 4\\ (G\u2212U) \u2212 1 ) : | -\u0302 \u2212 G | \u2264 d }]\n= inf U\u2208\u211d E -\u0302\u223c\u2119\u0302\n[ U + 1\n\\\n( 4\\ (-\u0302+d\u2212U) \u2212 1 ) \u2212 U\n]\n= 1\n\\ log(E\n-\u0302\u223c\u2119\u0302 [4\\-\u0302 ]) + d = Ent\u2119\u0302\\ ( -\u0302) + d.\nFor ? < \u221e, we verify LJ (d) = +\u221e directly. We define \u2119n = (1 \u2212 n)\u2119\u0302 + n\u2119\u0302\"n , where \u2119\u0302\" = (G \u21a6\u2192\nG +\")#\u2119\u0302 is right-translation of \u2119\u0302 by \", and \"n = dn \u22121/?. Then W? (\u2119\u0302,\u2119) \u2264 d. However,\nE\u2119n [4 \\-] = (1\u2212 n)E[4\\-\u0302] + nE[4\\ (-\u0302+\"n ) ] = (1\u2212 n + n4\\\"n )E[4\\-\u0302],\nand accordingly\nEnt \u2119n\n\\ (-) =\n1 \\ log(E-\u223c\u2119n [4 \\- ]) = 1 \\ log(E -\u0302\u223c\u2119\u0302 [4\\-\u0302]) + 1 \\ log(1+ n (4\\ n \u22121/? \u2212 1)),\nwhich tends to infinity as n \u2192 0.\nAppendix EC.6: Proofs for Section 5.3\nProof of Proposition 4. For a fixed \\, first we apply Theorem 1 to \u2212LG(\u00b7, \\) by taking the Fenchel conjugate\n(\u2212LG(\u00b7, \\)) \u2217(\u2212_) = sup\n\u2119,\u2119\u0303\u2208P (X )\n{ E-\u223c\u2119[ 5 (-)] \u2212_K2 (\u2119\u0303,\u2119) :K2\u0303 (\u2119\u0302, \u2119\u0303) \u2264 \\ }\n= sup \u2119\u0303\u2208P (X )\n{ E -\u0303\u223c\u2119\u0303 [ sup G\u2208X 5 (G) \u2212_2( -\u0303, G) ] :K2\u0303 (\u2119\u0302, \u2119\u0303) \u2264 \\ } .\nDenote 5\u0303 (G\u0303) = supG\u2208X 5 (G) \u2212 _2(G\u0303, G). Then we apply Theorem 1 to 5\u0303 , 2\u0303 and \u2212(\u2212LG(\u00b7, \\)) \u2217(\u2212_) by taking Fenchel conjugate \\\u2192\u2212`:\nL \u2217 \u2217 G (\u2212_,\u2212`) = sup\n\u2119\u0303\u2208P (X )\n{ E -\u0303\u223c\u2119 [ sup G\u2208X { 5 (G) \u2212_2( -\u0303, G) }] \u2212 `K2\u0303 (\u2119\u0302, \u2119\u0303) }\n= E -\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_2(G\u0303, G) \u2212 `2\u0303( -\u0302, G\u0303) }] .\nec17\nSince (\u2212LG(\u00b7, \\)) \u2217(\u2212_) is concave in \\, we recover it by\n(\u2212LG(\u00b7, \\)) \u2217(\u2212_) =min\n`\u22650\n{ `\\ + (\u2212LG) \u2217 \u2217 (\u2212_,\u2212`) }\n=min `\u22650\n{ `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_2(G\u0303, G) \u2212 `2\u0303( -\u0302 , G\u0303) }]} .\nSince LG(d, \\) is concave in d, we recover it by\nLG(d, \\) = min _,`\u22650\n{ _d + `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_2(G\u0303, G) \u2212 `2\u0303( -\u0302, G\u0303) }]} .\nIn particular, if 2(G1, G2) = 2\u0303(G1, G2) = 3 (G1, G2) are the same metric, then\nL \u2217 \u2217 G (\u2212_,\u2212`) = E -\u0302\u223c\u2119\u0302 [ sup G,G\u0303\u2208X { 5 (G) \u2212_3 (G\u0303, G) \u2212 `3 ( -\u0302, G\u0303) }]\n= E -\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212 (_\u2227 `)3 ( -\u0302, G) }]\nby taking G\u0303 = G when _ \u2265 ` and G\u0303 = -\u0302 when _ \u2264 `. Correspondingly,\n(\u2212LG) \u2217(\u2212_, \\) =min\n`\u22650\n{ `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212 (_\u2227 `)3 ( -\u0302, G) }]}\n= min `\u2208[0,_]\n{ `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212 `3 ( -\u0302, G) }]} ,\nand\nLG(d, \\) = min 0\u2264`\u2264_\n{ _d + `\\ +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212 `3 ( -\u0302, G) }]}\n=min `\u22650\n{ `(d + \\) +E\n-\u0302\u223c\u2119\u0302 [ sup G\u2208X { 5 (G) \u2212 `3 ( -\u0302, G) }]} ."
        }
    ],
    "title": "A Simple and General Duality Proof for Wasserstein Distributionally Robust Optimization",
    "year": 2024
}