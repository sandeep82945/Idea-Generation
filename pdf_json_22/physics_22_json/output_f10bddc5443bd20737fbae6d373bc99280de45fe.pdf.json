{
    "authors": [
        {
            "affiliations": [],
            "name": "Bas de Bruijne"
        },
        {
            "affiliations": [],
            "name": "Gleb Vdovin"
        },
        {
            "affiliations": [],
            "name": "Oleg Soloviev"
        }
    ],
    "id": "SP:49636df50c680510f9b572bcd1067a745463199b",
    "references": [
        {
            "authors": [
                "D.R. Neal",
                "J. Copland",
                "D.A. Neal"
            ],
            "title": "Shack-Hartmann wavefront sensor precision and accuracy,",
            "venue": "Proc. SPIE 4779,",
            "year": 2002
        },
        {
            "authors": [
                "M. Viegers",
                "E. Brunner",
                "O. Soloviev",
                "C.C. de Visser",
                "M. Verhaegen"
            ],
            "title": "Nonlinear spline wavefront reconstruction through moment-based Shack-Hartmann sensor measurements,",
            "venue": "Opt. Express",
            "year": 2017
        },
        {
            "authors": [
                "H. Guo",
                "N. Korablinova",
                "Q. Ren",
                "J. Bille"
            ],
            "title": "Wavefront reconstruction with artificial neural networks,",
            "venue": "Opt. Express 14,",
            "year": 2006
        },
        {
            "authors": [
                "R. Swanson",
                "M. Lamb",
                "C. Correia",
                "S. Sivanandam",
                "K. Kutulakos"
            ],
            "title": "Wavefront reconstruction and prediction with convolutional neural networks,",
            "venue": "Proc. SPIE 10703,",
            "year": 2018
        },
        {
            "authors": [
                "S.L. Su\u00e1rez G\u00f3mez",
                "C. Gonz\u00e1lez-Guti\u00e9rrez",
                "E. D\u00edez Alonso",
                "J.D. Santos Rodr\u00edguez",
                "M.L. S\u00e1nchez Rodr\u00edguez",
                "J. Carballido Landeira",
                "A. Basden",
                "J. Osborn"
            ],
            "title": "Improving adaptive optics reconstructions with a deep learning approach,",
            "venue": "Hybrid Artificial Intelligent Systems,",
            "year": 2018
        },
        {
            "authors": [
                "L. Hu",
                "S. Hu",
                "W. Gong",
                "K. Si"
            ],
            "title": "Learning-based Shack-Hartmann wavefront sensor for high-order aberration detection,",
            "venue": "Opt. Express",
            "year": 2019
        },
        {
            "authors": [
                "L. Hu",
                "S. Hu",
                "W. Gong",
                "K. Si"
            ],
            "title": "Deep learning assisted Shack\u2013 Hartmann wavefront sensor for direct wavefront detection,",
            "venue": "Opt. Lett. 45,",
            "year": 2020
        },
        {
            "authors": [
                "J. Bekendam"
            ],
            "title": "Deep learning wavefront sensing via raw ShackHartmann images,",
            "venue": "Master\u2019s thesis (Delft University of Technology,",
            "year": 2020
        },
        {
            "authors": [
                "F. S\u00e1nchez-Lasheras",
                "C. Ord\u00f3\u00f1ez",
                "J. Roca",
                "F. de Cos Juez"
            ],
            "title": "Real-time tomographic reconstructor based on convolutional neural networks for solar observation,",
            "venue": "Math. Methods Appl. Sci",
            "year": 2019
        },
        {
            "authors": [
                "D. Wilding",
                "O. Soloviev",
                "P. Pozzi",
                "G. Vdovin",
                "andM. Verhaegen"
            ],
            "title": "Blind multi-frame deconvolution by tangential iterative projections (tip),",
            "venue": "Opt. Express",
            "year": 2017
        },
        {
            "authors": [
                "G.R. Ayers",
                "J.C. Dainty"
            ],
            "title": "Iterative blind deconvolution method and its applications,",
            "venue": "Opt. Lett. 13,",
            "year": 1988
        },
        {
            "authors": [
                "L.P. Yaroslavsky",
                "H.J. Caulfield"
            ],
            "title": "Deconvolution of multiple images of the same object,",
            "venue": "Appl. Opt",
            "year": 1994
        },
        {
            "authors": [
                "F. Sroubek",
                "P. Milanfar"
            ],
            "title": "Robust multichannel blind deconvolution via fast alternating minimization,",
            "venue": "IEEE Trans. Image Process",
            "year": 2012
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation,",
            "year": 2015
        },
        {
            "authors": [
                "V. de Bruijne"
            ],
            "title": "Extended scene deep learning wavefront sensing for real time image deconvolution,",
            "venue": "Master\u2019s thesis (Delft University of Technology,",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhou",
                "L. Zhang",
                "L. Zhu",
                "H. Bao",
                "Y. Guo",
                "X. Rao",
                "L. Zhong",
                "C. Rao"
            ],
            "title": "Comparison of correlation algorithms with correlating Shack-Hartmann wave-front images,",
            "venue": "Proc. SPIE 10026,",
            "year": 2016
        },
        {
            "authors": [
                "V. de Bruijne"
            ],
            "title": "Extended scene deep learning wavefront sensing for real time image deconvolution, code repository,",
            "venue": "Master\u2019s thesis (Delft University of Technology,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Delft University of Technology\nExtended scene deep learning wavefront sensing\nde Bruijne, Bas; Vdovin, Gleb; Soloviev, Oleg\nDOI 10.1364/JOSAA.443436 Publication date 2022 Document Version Final published version Published in Journal of the Optical Society of America A: Optics and Image Science, and Vision\nCitation (APA) de Bruijne, B., Vdovin, G., & Soloviev, O. (2022). Extended scene deep learning wavefront sensing. Journal of the Optical Society of America A: Optics and Image Science, and Vision, 39(4), 621-627. https://doi.org/10.1364/JOSAA.443436\nImportant note To cite this publication, please use the final published version (if applicable). Please check the document version above.\nCopyright Other than for strictly personal use, it is not permitted to download, forward or distribute the text or part of it, without the consent of the author(s) and/or copyright holder(s), unless the work is under an open content license such as Creative Commons.\nTakedown policy Please contact us and provide details if you believe this document breaches copyrights. We will remove access to the work immediately and investigate your claim.\nThis work is downloaded from Delft University of Technology. For technical reasons the number of authors shown on this cover page is limited to a maximum of 10.\nGreen Open Access added to TU Delft Institutional Repository\n'You share, we take care!' - Taverne project\nhttps://www.openaccess.nl/en/you-share-we-take-care\nOtherwise as indicated in the copyright section: the publisher is the copyright holder of this work and the author uses the Dutch legislation to make this work public.\nExtended scene deep learning wavefront sensing Bas de Bruijne,1,* Gleb Vdovin,1,2 AND Oleg Soloviev1,2 1Delft Center for Systems andControl, TUDelft, Mekelweg 2, 2628CDDelft, The Netherlands 2Flexible Optical B.V., Polakweg 10-11, 2288GGRijswijk, The Netherlands *Corresponding author: BasdBruijne@gmail.com\nReceived 5 October 2021; revised 19 February 2022; accepted 21 February 2022; posted 24 February 2022; published 17 March 2022\nWe have applied a combination of blind deconvolution and deep learning to the processing of Shack\u2013Hartmann images. By using the intensity information contained in spot positions, and the fine structure of the separate images created by the lenslets, we have increased the sensitivity and resolution of the sensor over the limit defined by standard processing of spot displacements only. We also have demonstrated the applicability of the method to wavefront sensing using extended objects as a reference. \u00a92022Optica PublishingGroup\nhttps://doi.org/10.1364/JOSAA.443436"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "The Shack-Hartmann (SH) sensor is commonly applied in a wide range of imaging applications. The SH sensor consists of two parts: a microlens array (MLA) and imaging sensor, usually based on CCD or CMOS technology. The MLA, usually positioned in the system pupil, divides the light into M images, each formed in a separate subaperture (SA). Conventionally, only the tip\u2013tilt modes are reconstructed for each SA by measuring the x and y shifts of the image from its reference position. For this calculation, the found wavefront slopes can be either fit to a number of Zernike polynomials (modal wavefront reconstruction) or integrated into an array of wavefront values connected to the SAs (zonal wavefront reconstruction) [1].\nUsing this method, a SH sensor with M SAs can be used to retrieve a wavefront with maximum 2M modes, and the wavefront reconstruction is limited to the value of r0 > D\u221aM .\nDifferent techniques have been proposed to attempt to elevate these limitations, e.g., by using the second moment information of the SAs [2]. The most promising of these techniques in terms of wavefront reconstruction accuracy is the use of machine learning. Earlier research showed that artificial neural networks (ANNs) can be used to accurately reconstruct the wavefront from SH slope measurements, by using either Zernike polynomials [3] or a convolutional neural network (CNN) to reconstruct the wavefront directly [4]. Su\u00e1rez G\u00f3mezet al. [5] showed that CNNs can be used to reconstruct wavefront slopes from SH images. Recently, it was shown that CNNs could be used not only for wavefront slope sensing and reconstruction separately, but that one CNN could be used to fulfill both functions [6\u20138].\nDeep learning techniques are superior to conventional methods for wavefront sensing, as they do not solely rely on the shifts of SA images but can also analyze their shapes. The shapes of\nSA images provide information about the higher order wavefront aberrations that are neglected by conventional wavefront sensing methods.\nThe above mentioned papers share a major simplification that prevents their methods from being applied in certain imaging applications: they take only point-source observations into account.\nWhile S\u00e1nchez-Lasheras et al. [9] showed that it is possible to train an ANN to reconstruct the wavefront from an extended scene SH image directly, their methods were applicable only to solar spots, a very specific type of scene. To the authors\u2019 best knowledge, no deep learning wavefront sensing (DLWS) technique has been proposed that can be applied to extended scene observations in general.\nIn this paper, a method is proposed that is aimed to extend DLWS methods to extended scene imaging applications. This method makes use of a semi-blind image deconvolution algorithm that pre-processes the SH image by removing the influence of the extended scene, and returns an estimated pointsource SH image. This image is then used in a CNN that is designed to reconstruct the wavefront given uncertainties in the SH image introduced by the pre-processing step. These methods are tested using numerical simulations of a deconvolution from wavefront sensing (DFWS) system as shown in Fig. 6.\nFirst, Section 2 will discuss the mentioned pre-processing step. Second, Section 3 will introduce the modified DLWS methods. Section 4 will discuss how the newly proposed methods compare to traditional extended scene wavefront sensing systems as well as earlier proposed DLWS architectures."
        },
        {
            "heading": "2. BLIND SHACK\u2013HARTMANN IMAGE DECONVOLUTION",
            "text": "This section discusses the pre-processing of SH images to remove the dependency of the object on the SH image.\n1084-7529/22/040621-07 Journal \u00a9 2022Optica PublishingGroup\nFirst, consider the mathematical model dictating how the SH image is constructed:\nish = o \u2217 ksh + n. (1)\nHere, \u2217 refers to the convolution operator, o is the undistorted object, ksh is the SH point spread function (PSF), ish is the SH image, and n represents a noise component. In the case where a point source is observed, o is a delta function, and ish = ksh + n.\nksh is related to the wavefront\u03c6 by ksh \u221d \u2223\u2223F {P ei(\u03c6+\u03c6sh)}\u2223\u22232. (2)\nHere, P is the pupil function, and\u03c6sh is a piece-wise linear phase component that is added to simulate the influence of the MLA, as described by Soloviev et al. [10].\nIn this paper, blind deconvolution refers to the retrieval of ksh without the knowledge of o . However, more information is assumed to be available than usual in a blind image deconvolution setting.\nIn many adaptive optics (AO) applications, a second imaging sensor is used to capture high-resolution images with a higher diffraction limit. The construction of this image (i ) is equivalent to the construction of ish as described above, but with a different pupil function and \u03c6sh = E0. The down-sampling of i is used as an initial estimate of the object in the blind deconvolution problem. Compared to using one individual SA image or a shifted averaging of all SA images, this initial estimate is distorted in an unbiased manner compared to the individual SA images while requiring minimal processing.\nThe blind image deconvolution algorithm used in this paper is an adaptation of the tangential iterative projections (TIP) algorithm as introduced by Wilding et al. [11]. TIP is representative of a broad class of blind deconvolution techniques that work by alternating between estimating the PSF and the object from the image and the latest estimates of the PSF and object (see [12\u201314] for the most related work and [15] for a general overview), and differs from other algorithms by using minimal a priori assumptions, namely, only the support size of the PSFs and the nonnegativity of the PSFs and the object. In TIP, one iteration looks as follows. Given the latest estimate of the SH PSF (K\u0302 sh) and the SH image (Ish), an estimate of the object (O\u0302) can be calculated as\nO\u0302 =PO arg min O\u2208CM\u00d7M \u2225\u2225\u2225Ish \u2212 O K\u0302 sh\u2225\u2225\u22252, (3) where the capitalization of a variable refers to its Fourier transform. Given the updated estimation of the object, the estimate of the SH PSF follows from\nK\u0302 sh =PK Ish\nO\u0302 . (4)\nHere, the projection operatorsPX project the result of the leastsquares deconvolution onto a set of valid objects or PSFs X . In general, this valid set consists of positive and normalized images. Additional constraints can be added to this set to guide the convergence of the algorithm.\nFor this application, the TIP algorithm is altered. Rather than using the projection operator to enforce a constraint on the results from the deconvolution, the projection operators\nare used to over-constrain the images to direct the TIP algorithm towards the right solution. These projection operators are relaxed as the iteration continues. For construction of the projection operator, some prior information of the SH image is used. Consider the two possibilities describing the relation between the latest estimate of the object and the SA images (ignoring the odd case where the SA image is equal to the estimated object).\n(1) The SA image is more blurred than the estimated object. This will result in a large PSF (e.g., SA c in Fig. 1). (2) The SA image is sharper than the estimated object. In this latter case, the PSF does not represent a blurring function, but instead a sharpening function. For the PSF to perform a sharpening action, it must contain negative values as well as large positive values. This effect is highlighted in Fig. 1. SA a is sharper than the estimated object and contains negative values.\nTo improve the estimate of the object, the algorithm must focus on SA PSFs in the second category. While convolution with these PSFs results in sharper images, deconvolution (which is applied in the next step) with these PSFs will again result in a more blurred estimate of the object. To preserve the sharpness of the images in the deconvolution step, the negative component of the PSFs must be removed. By increasing the contrast of the image, the influence of SA images in the first category is reduced, as these PSFs have lower intensities. This way the TIP algorithm can be directed to a sharper estimate of the object.\nThe pseudocode shown in Fig. 2 shows a snippet of the exact implementation of the TIP algorithm.\nLines 2\u20134 perform a least-squares deconvolution and extract the positive real values from the estimated PSF. Note that the\nFig. 2. Snippet of pseudocode showing the exact implementation of the modified TIP algorithm. Note that for the clarity of the reader, the code shown is not optimized for speed. The object o is referred to as ob.\nregularization parameter is added only to the frequencies with a lower absolute value than the regularization parameter in order not to distort valid frequencies. Line 4 removes the negative values in the PSF. Lines 6\u20137 normalize the estimated PSF between zero and one. Line 8 increases the contrast of the estimated PSF by raising it to the power of f (nmax, n). f (nmax, n) must be selected according to the number of iterations (nmax), noise levels, and scenery and is constrained to f (nmax, nmax \u2212 1)= 1 and f > 1 for n < nmax \u2212 1 (note that 0\u2264 n < nmax). In the final implementation of this algorithm, f (nmax, n)= nmax \u2212 n, and a value of nmax = 3 was used as a balance between evaluation speed and accuracy.\nLines 10\u201312 normalize the PSF such that the sum is equal to one and calculate the Fourier transform of the PSF. Lines 14\u201320 calculate the estimate of the object and normalizes it. In line 17, fov refers to a binary array that assures that o does not exceed the field of view of each SA (a finite support constraint for the object estimate).\nUnlike in the implementation of Wilding et al., the estimated object is the absolute value of the inverse Fourier transform rather than its real component (see line 16). This choice of operator introduces slight discrepancies in the estimated object, which helps avoid a trivial solution where one single SA-PSF reduces to a delta function.\nFigure 3 shows how the implementation of the TIP algorithm performs on a simulation of the optical system. It can be seen that the estimated SH pattern is nearly identical to the true SH pattern except for a stronger noise component, which is apparent on the PSFs in log scale.\nIt must be noted that the pre-processing algorithm does not reliably conserve the tip and tilt modes of the global wavefront. It is observed that the pre-processing algorithm introduces shifts in both the estimated SH pattern and estimated object. As long as the shifts are equal and in the same direction, these images provide a valid solution to the blind deconvolution problem. These shifts, however, corrupt the information of the tip and tilt modes of the wavefront. As a result, the estimated object can be shifted from its true position, but this does not have an inference on its resulting sharpness."
        },
        {
            "heading": "3. DEEP LEARNING WAVEFRONT SENSING",
            "text": "The CNN architecture used to reconstruct the wavefront given the processed SH image is, like [7,8], based on the U-net [16]."
        },
        {
            "heading": "A. Network Architecture",
            "text": "There are several key differences in SH patterns used as input for the CNN between literature [7,8] and this research. Most notably, previous research has used the true SH pattern, whereas the SH pattern used in this research is an estimate of the true SH pattern found through a blind-deconvolution step. The blind-deconvolution step not only introduces discrepancies in the shape of the individual PSFs, but also results in a lower signal-to-noise ratio.\nThe architecture used by Hu et al. [7] is designed such that information can be preserved throughout the network without being filtered by introducing a bypassing layer in the residual block. This can be very useful when there is limited noise present in the SH PSF, but limits the architecture\u2019s ability to extract information from the pre-processed images used in this paper. For the same reasons, additional convolution layers are placed between the concatenations of input to output layers compared to the original U-net architecture, as shown in Fig. 4.\nFigure 5 shows the residual block used for this proposed architecture. This residual block has features based on the architecture of Hu et al. [7], where different sizes of convolutional kernels are used. The amount of larger size kernels is, however, significantly reduced and does not depend on N. As a result of this reduction, the size of the network is much lower than the architecture of Hu et al. Additionally, there is another concatenation action introduced in the new residual block, located between the two convolutional layers. This concatenation action mixes the information of the differently sized convolutional layers and allows information to pass through two different sizes of kernels within the residual block."
        },
        {
            "heading": "B. Training",
            "text": "For the training of the CNN, a numerical simulation of a DFWS system was constructed in Python. 2 \u2217 105 Kolmogorov phase\nscreens were simulated with a turbulence strength ranging from D/r0 = 0 to D/r0 = 18 with removed piston, tip, and tilt modes. Details on the practical implementation of this process are described in [17].\nThe training of the CNN was done on a desktop PC with a 12 core Intel Xeon E5-2630 DUAL CPU with 64 GB of memory and a NVIDIA GeForce GTX 970 GPU with 4 GB of memory. The Python package Keras was used with a Tensorflow backend. The used loss function is mean absolute error (MAE) with the Adam optimizer and a batch size of 32. MAE was chosen rather than the more standard mean squared error (MSE) because of its better observed performance in this use case. The reason for this is likely the relatively lower penalty MAE puts on large errors, which occur mostly in higher-level wavefront modes, which the CNN cannot reconstruct due to sensor noise, normalization errors, and physical sensor limitations."
        },
        {
            "heading": "4. RESULTS",
            "text": "To testing the wavefront sensing capabilities of the newly developed method, the developed SH image pre-processing combined with three different DLWS networks is tested on a turbulence strength ranging from D/r0 = 0 to D/r0 = 18 in 19 discrete steps. The three different DLWS architectures are the architecture proposed in Section 3, the architecture proposed by Hu et al. [7], and the U-net based architecture used by Bekendam [8]. For each step in turbulence strength, 100 unique wavefronts were generated that were not included in the training data set. The tests were performed using the software simulation of the optical setup for easier calculation of the wavefront reconstruction performance.\nTo compare the developed methods to the conventional way of extended scene wavefront sensing, zonal and modal methods were implemented, too. For conventional methods, slope detection was done in accordance with Zhou et al. [18], using the absolute difference function as a correlation function combined with sub-pixel interpolation to retrieve SA shifts with an accuracy higher than one pixel. The modal method represents the estimated wavefront using the first 24 Zernike modes, equal to the number of effective SAs.\nThe experiment was conducted on a software simulation of the optical system as shown in Fig. 6, with a MLA of 6\u00d7 6 SAs. The atmospheric turbulence follows a Kolmogorov spectrum, and it is approximated as a single phase screen located at the entrance pupil.\nFigure 7 [19] shows the performance of the different methods. In this figure, the 1 rad RMS wavefront estimation error is highlighted, indicating the minimum required performance.\nThe RMS wavefront error is calculated by\neRMS = \u221a\u221a\u221a\u221a X\u2211 x=1 Y\u2211 y=1 ( \u03c6(x , y )\u2212 \u03c6\u0303(x , y ) )2 , (5)\nwhere \u03c6\u0303 is the reconstructed wavefront, and \u03c6 is the true wavefront of size [X , Y ].\nAs expected, the zonal method manages to sufficiently estimate the wavefront up to a turbulence strength of roughly D/r0 = 6, which is equal to the linear amount of SAs in the SH sensor. Since the wavefront is generated using Kolmogorov statistics rather than Zernike polynomials, the performance of the modal method is lower than that of the zonal method. The performance of the tested CNNs is very similar, all estimating wavefront sufficiently up to a turbulence strength of D/r0 = 14, which is a very significant improvement compared to the modal and zonal methods.\nUp to D/r0 = 12, the newly proposed architecture performs slightly, but not significantly, better than Hu\u2019s architecture, while both perform roughly 4% better than Bekendam\u2019s architecture. At turbulence strengths of D/r0 > 12, Hu\u2019s architecture performs roughly 2% better than the proposed architecture.\nTable 1 shows the computational efficiency of the evaluated architectures. Even though the proposed architecture reconstructs the wavefront with accuracy similar to Hu\u2019s architecture, the evaluation time and memory requirements are much lower, which is beneficial for real-time integration of the DLWS techniques.\nFigure 8 shows an example of the wavefront reconstruction performance of the tested CNN architectures. It can be seen that the DLWS method is able to reconstruct the global shape of\naThe evaluation times depend greatly on the amount of background processes active. These tests were performed while the other parts of the DFWS system were also running.\nthe wavefront accurately, but fails to retrieve the pixel-to-pixel fluctuations in the turbulent wavefront.\nFigure 8 also shows the PSFs of reconstructed wavefronts, as well as the deconvolution of the true PSF with that of reconstructed wavefronts. This \u201cdeconvoluted\u201d PSF depends on the used deconvolution technique but gives a rough idea of the quality of the corrected image in a DFWS setting. The noise in these PSFs makes the calculation of the Strehl ratio unreliable. Instead, the Strehl ratios of the PSFs resulting from the residual wavefronts are shown.\nInterestingly, the DLWS methods show perfect phase unwrapping behavior. The training data for the CNNs consisted of continuous wavefronts (i.e., no large steps in phase delay), which led to the DLWS reconstructing only continuous wavefronts. This effect is highlighted in Fig. 9, where a jump in phase is manually added to the true wavefront, but the DLWS method reconstructs a wavefront with a phase wrap that corresponds to the turbulent wavefront without the added jump in phase.\nFigure 10 [17,20] shows an example of the performance of the proposed system integrated in a DFWS setup. The simulated turbulence has a strength of D/r0 = 15, which is on the limit of what the proposed system is capable of correcting using a SH sensor with a MLA of 6\u00d7 6 SAs.\nAs part of the experiment, the developed method was implemented in an optical laboratory setup, as shown in Fig. 11. The physical experiment was used to verify the real-world performance of the system, as well as test its resilience to anisoplanatic wavefront aberrations. The turbulence simulator (custom-made\nby Lexitek, Inc.) in the setup introduces known aberrations in the wavefront.\nFigure 12 showcases the real-world performance of the system. As the turbulence strength is D/r0 \u2248 18, a diffraction limited correction is not expected, and the estimated image contains high-frequency error component."
        },
        {
            "heading": "5. CONCLUSION AND DISCUSSION",
            "text": "In this paper, it is shown that blind deconvolution methods have the potential to open up DLWS methods to extended-scene observations. The proposed system outperforms conventional extended-scene wavefront sensing methods significantly in terms of wavefront reconstruction resolution. Using this approach, a 6\u00d7 6 microlens SH sensor was shown to be able to reconstruct the wavefront up to a turbulence strength of D/r0 = 14. Using a conventional wavefront sensing method, a 14\u00d7 14 microlens SH sensor would be necessary to achieve this accuracy. As a result of this reduction in microlenses, more than five times the amount of light becomes available per SA, allowing for the wavefront correction in low-light scenes.\nThe proposed wavefront sensing method is still in early development and currently has a number of limitations and questions.\nIt was observed that the TIPs algorithm used for preprocessing the SH image occasionally introduces shifts in the estimated SH pattern. As a result of this, the proposed DFWS system was not able to correct the tip and tilt modes of the wavefront. Additional constraints on the PSF and object could be added in\nthe TIP implementation to eliminate the occurrence of these shifts.\nThe developed system is currently limited to extended objects surrounded by a dark background, which limits it employment in, for example, satellite or surveillance imaging. It is expected that using a field stop to introduce a dark background into the SA images can elevate this limitation, but this needs to be verified in future research.\nThis paper investigated the performance of the system using a 6\u00d7 6 MLA. Bekendam [8] showed promising wavefront reconstruction capabilities for larger MLA sizes using point-source DLWS techniques. This trend is believed to be true for the proposed system as well, but future research will have to verify this assumption.\nMore research is needed to explore the potential of the proposed wavefront sensing method in other areas of AO. For example, extended-scene DLWS can be extended to anisoplanatic imaging or combined with the frozen flow hypothesis to utilize temporal correlations of turbulence for more accurate wavefront reconstruction.\nDisclosures. The authors declare no conflicts of interest.\nData availability. Data underlying the results presented in this paper are not publicly available but can be generated using the resources available in [19]."
        }
    ],
    "title": "Extended scene deep learning wavefront sensing"
}