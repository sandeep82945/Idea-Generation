{
    "abstractText": "A hallmark of chaotic dynamics is the loss of information with time. Although information loss is often expressed through a connection to Lyapunov exponents\u2014 valid in the limit of high information about the system state\u2014this picture misses the rich spectrum of information decay across different levels of granularity. Here we show how machine learning presents new opportunities for the study of information loss in chaotic dynamics, with a double pendulum serving as a model system. We use the Information Bottleneck as a training objective for a neural network to extract information from the state of the system that is optimally predictive of the future state after a prescribed time horizon. We then decompose the optimally predictive information by distributing a bottleneck to each state variable, recovering the relative importance of the variables in determining future evolution. The framework we develop is broadly applicable to chaotic systems and pragmatic to apply, leveraging data and machine learning to monitor the limits of predictability and map out the loss of information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kieran A. Murphy"
        },
        {
            "affiliations": [],
            "name": "Dani S. Bassett"
        }
    ],
    "id": "SP:560e822a4ad6cafe21b009e624451bf34ce3a6ab",
    "references": [
        {
            "authors": [
                "I\u00f1aki Estella Aguerri",
                "Abdellatif Zaidi"
            ],
            "title": "Distributed variational representation learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander A. Alemi",
                "Ian Fischer",
                "Joshua V. Dillon",
                "Kevin Murphy"
            ],
            "title": "Deep variational information bottleneck",
            "venue": "arXiv preprint arXiv:1612.00410,",
            "year": 2016
        },
        {
            "authors": [
                "Shahab Asoodeh",
                "Flavio P. Calmon"
            ],
            "title": "Bottleneck problems: An information and estimation-theoretic",
            "venue": "view. Entropy,",
            "year": 2020
        },
        {
            "authors": [
                "Arjun Berera",
                "Daniel Clark"
            ],
            "title": "Information production in homogeneous isotropic turbulence",
            "venue": "Physical Review E,",
            "year": 2019
        },
        {
            "authors": [
                "Guido Boffetta",
                "Massimo Cencini",
                "Massimo Falcioni",
                "Angelo Vulpiani"
            ],
            "title": "Predictability: a way to characterize complexity",
            "venue": "Physics reports,",
            "year": 2002
        },
        {
            "authors": [
                "Felix Creutzig",
                "Amir Globerson",
                "Naftali Tishby"
            ],
            "title": "Past-future information bottleneck in dynamical systems",
            "venue": "Physical Review E,",
            "year": 1925
        },
        {
            "authors": [
                "I\u00f1aki Estella Aguerri",
                "Abdellatif Zaidi"
            ],
            "title": "Distributed information bottleneck method for discrete and gaussian sources",
            "venue": "In International Zurich Seminar on Information and Communication (IZS 2018)",
            "year": 2018
        },
        {
            "authors": [
                "J. Doyne Farmer"
            ],
            "title": "Information dimension and the probabilistic structure of chaos",
            "venue": "Zeitschrift fu\u0308r Naturforschung A,",
            "year": 1982
        },
        {
            "authors": [
                "Pierre Gaspard",
                "Xiao-Jing Wang"
            ],
            "title": "Noise, chaos, and ( , \u03c4 )-entropy per unit time",
            "venue": "Physics Reports,",
            "year": 1993
        },
        {
            "authors": [
                "Irina Higgins",
                "Loic Matthey",
                "Arka Pal",
                "Christopher Burgess",
                "Xavier Glorot",
                "Matthew Botvinick",
                "Shakir Mohamed",
                "Alexander Lerchner"
            ],
            "title": "\u03b2-vae: Learning basic visual concepts with a constrained variational framework",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Ryan G. James",
                "Korana Burke",
                "James P. Crutchfield"
            ],
            "title": "Chaos forgets and remembers: Measuring information creation, destruction, and storage",
            "venue": "Physics Letters A,",
            "year": 2014
        },
        {
            "authors": [
                "Thomas Kailath"
            ],
            "title": "The divergence and Bhattacharyya distance measures in signal selection",
            "venue": "IEEE transactions on communication technology,",
            "year": 1967
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational Bayes",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "David McAllester",
                "Karl Stratos"
            ],
            "title": "Formal limitations on the measurement of mutual information",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Kieran A. Murphy",
                "Dani S. Bassett"
            ],
            "title": "The distributed information bottleneck reveals the explanatory structure of complex systems",
            "venue": "arXiv preprint arXiv:2204.07576,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Ben Poole",
                "Sherjil Ozair",
                "Aaron Van Den Oord",
                "Alex Alemi",
                "George Tucker"
            ],
            "title": "On variational bounds of mutual information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Rudin"
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Rudin",
                "Chaofan Chen",
                "Zhi Chen",
                "Haiyang Huang",
                "Lesia Semenova",
                "Chudi Zhong"
            ],
            "title": "Interpretable machine learning: Fundamental principles and 10 grand challenges",
            "venue": "Statistics Surveys,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew M. Saxe",
                "Yamini Bansal",
                "Joel Dapello",
                "Madhu Advani",
                "Artemy Kolchinsky",
                "Brendan D. Tracey",
                "David D. Cox"
            ],
            "title": "On the information bottleneck theory of deep learning",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2019
        },
        {
            "authors": [
                "Claude Elwood Shannon"
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell System Technical Journal,",
            "year": 1948
        },
        {
            "authors": [
                "Robert Shaw"
            ],
            "title": "Strange attractors, chaotic behavior, and information flow",
            "venue": "Zeitschrift fu\u0308r Naturforschung A,",
            "year": 1981
        },
        {
            "authors": [
                "Troy Shinbrot",
                "Celso Grebogi",
                "Jack Wisdom",
                "James A Yorke"
            ],
            "title": "Chaos in a double pendulum",
            "venue": "American Journal of Physics,",
            "year": 1992
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Naftali Tishby"
            ],
            "title": "Opening the black box of deep neural networks via information",
            "venue": "arXiv preprint arXiv:1703.00810,",
            "year": 2017
        },
        {
            "authors": [
                "Naftali Tishby",
                "Fernando C. Pereira",
                "William Bialek"
            ],
            "title": "The information bottleneck method",
            "venue": "arXiv preprint physics/0004057,",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A fundamental aspect of chaos is the loss of information over time: for any measurement of a chaotic system with finite resolution, there is a finite time horizon beyond which the measurement bears no predictive power [22, 8, 26, 5, 11, 4]. The premise of this work is simple: to find the optimally predictive information in a chaotic system at different levels of granularity, and to study how the predictive power of this information erodes with the passage of time.\nInformation loss is intimately connected to the distortion of regions in phase space by chaotic dynamics, and thus to Lyapunov exponents: the sum of the positive Lyapunov exponents gives the Kolmogorov-Sinai (KS) entropy, the average rate of information loss[5]. However, these quantities are valid in the limit of maximal information\u2014where infinitesimally-separated trajectories are discernible\u2014and are thus somewhat removed from reality [5]. The ( , \u03c4)-entropy generalizes the KS entropy, describing the loss of predictive power for different amounts of information about the system state [9]. It is defined by way of a rate-distortion objective that minimizes the rate of information needed to predict the system state better than a threshold value of some chosen measure of distortion.\nThe Information Bottleneck (IB) is a rate-distortion problem where the measure of distortion is based on mutual information; it extracts the information from one variable that is most shared with a second variable [25]. We can use the IB to find optimally predictive information from one state of a chaotic system about a future state, and at the same time measure the loss of predictive power [6]. In this work we develop a framework that uses the IB for analyzing chaotic dynamics with machine learning. We\nar X\niv :2\n21 0.\n14 22\n0v 1\n[ cs\n.L G\n] 2\nuse an interpretable variant of the IB, the Distributed IB [15], to decompose the optimally predictive information in terms of a system\u2019s state variables."
        },
        {
            "heading": "2 Approach",
            "text": "Our testbed is a double pendulum (Fig. 1a): one of the simplest physical systems to exhibit chaotic behavior [23]. We simulated 10,000 trajectories at constant energy (details in the Appendix).\nGiven a random variable St \u223c p(st) for the system state at time t, a measurement Ut is any (possibly stochastic) function of the system state: Ut \u223c p(ut|st). A measurement process shares mutual information I(Ut;St) with the underlying state, defined as the reduction in Shannon\u2019s entropy [21] about St onceUt is known. For a continuous variable St and without infinite measurement capabilities, the outcome of Ut can only narrow down the possible values of St, which we visualize in Fig. 1b as a region of possible pendulum states. We can similarly examine the reduction in entropy about a future state after time \u2206 has elapsed, given the same measurement Ut. Then I(Ut;St+\u2206) serves as an upper bound of the predictive capabilities of any forecasting device given the outcome of the measurement Ut. The passage of time invariably expands our uncertainty about the system state (Fig. 1c).\nImportantly, different measurements of the state\u2014that is, different measurement processes U , not different outcomes u of the same measurement\u2014can have identical I(Ut;St) but vary in the information shared with the future state. We seek the measurement Ut that is most predictive of the future dynamics, for a given allowance of information about the present. To find this optimally predictive information, we use the Information Bottleneck [25, 3, 6] and optimize over the space of possible measurements. The following objective maximizes information about the future state and the information about the current state:\nLIB = \u03b2I(Ut;St)\u2212 I(Ut;St+\u2206). (1) The parameter \u03b2 controls the bottleneck, restricting the information preserved about the present state.\nAs the bottleneck strength \u03b2 varies, a trajectory in the \u201cinformation plane\u201d [24] is traced (Fig. 2a), which shows the exchange rate of information preserved about the present state versus information predictive of the future state, for different time horizons \u2206. Ut can have no more information about St+\u2206 than it does about St, so no trajectory can exist above the line with slope 1. Optimal Ut are as close to this line as possible, and for a chaotic system longer time horizons are further displaced.\nMeasuring mutual information from data is notoriously tricky [20, 14, 17]. To be compatible with machine learning, the Variational Information Bottleneck (VIB) [2] replaces the mutual information terms of Eqn. 1 with bounds in a framework nearly identical to that of Variational Autoencoders [13, 10]. The bottleneck of Eqn. 1 is replaced with an upper bound on the mutual information,\nI(Ut;St) \u2264 DKL(p(ut|st)||r(u)). (2)\nThe Kullback-Leibler (KL) divergence\u2014DKL(w(x)||z(x)) = Ex\u223cw(x)[\u2212log (z(x)/w(x))]\u2014 quantifies the difference between the encoded distribution p(ut|st) and a prior distribution r(ut) = N (0, 1) [13]. As the KL divergence tends to zero, all representations become indistinguishable from the prior and from each other, and therefore uninformative. We will use the KL divergence as a proxy for the amount of information contained in the measurement Ut.\nTo estimate the information with the future state, we employ the Noise Contrastive Estimation (InfoNCE) loss used in representation learning as a lower bound for the mutual information in Eqn. 1, I(Ut;St+\u2206) \u2264 LInfoNCE [16, 17] (the loss is reproduced in the Appendix). Instead of decoding the representation Ut to a distribution over St+\u2206, which would require discretization of a four-dimensional space, we learn an encoder for the future state and compare Ut to Vt+\u2206 in a shared representation space. The degree to which Ut can be matched with its future counterpart in the midst of distractor representations quantifies the amount of information they share."
        },
        {
            "heading": "3 Results and Discussion",
            "text": "As the time horizon increases, we find the expected trend in information plane trajectories (Fig. 2b,c): for a given amount of information about the present, less information is recoverable about the future. What are the optimal measurements Ut for different information allowances and time horizons? A measurement encodes a particular present state s into a distribution p(u|s) in embedding space; we visualize other states s\u2032 that have similar distributions and would therefore be difficult to distinguish given a particular measurement outcome u. We display in Fig. 2d-g scatter plots of the positions of the second pendulum mass for all states that are co-embedded at different information rates DKL. Specifically, as all states are embedded to multidimensional Gaussian distributions, we use the Bhattacharyya coefficient [12]\u20140 for perfectly disjoint and 1 for perfectly matching distributions\u2014to characterize the similarity between distributions, displaying all states with value greater than 0.5. The measurement process is therefore a clustering of states with similar fates after time \u2206.\nVisualizing the information content of the optimal measurements leaves much to be desired. We necessarily exclude several dimensions of the state, and can do little more than visual inspection. Instead, we seek a general method to interrogate the optimal information. We employ the Distributed\nInformation Bottleneck [15, 7, 1], which modifies the optimization objective in 1 by distributing bottlenecks to each of the state variables (Fig. 1e). While maximizing the predictability of the future state as before, the variables are encoded independently and the sum of information is minimized. We acquire a degree of interpretability in the form of information allocation across the measurements, in exchange for less optimal measurements than can be found with the IB.\nIn Fig. 3, the information plane trajectories are decomposed in terms of the KL-divergence contribution from each of the state variables. Intuitively, a variable is encoded with a higher KL-divergence if it is more predictive of the future state, thereby granting insight into the relative role each variable plays in the dynamics. By changing the ratio of the arm lengths in the double pendulum, the variables pertaining to the longer arm become more informative about the future dynamics.\nWe have developed a framework to analyze the dynamics of a chaotic system through the lens of information theory using machine learning and data. The Information Bottleneck finds the optimally predictive information in a system state, and the Distributed IB confers interpretability about the role that each variable plays in the dynamics. The bounds on mutual information facilitate integration with off-the-shelf machine learning models and allow per-sample inspection of information allocation. The bounds are also the method\u2019s primary limitation: careful characterization of the mutual information bounds is necessary for any absolute measurements to be made (e.g., estimates of the KolmogorovSinai or ( , \u03c4) entropy). Nevertheless, even with only relative measurements of information there is a rich source of insight into chaotic dynamics.\nImpact statement\nMachine learning struggles with interpretability [18, 19], complicating its application in the natural sciences and many other domains. In this work we have introduced a machine learning framework whose emphasis is on understanding a dynamical system. The Distributed Information Bottleneck offers a degree of interpretability in the form of information allocation across components of an input, thereby enhancing the utility of machine learning for the sciences."
        }
    ],
    "title": "Characterizing information loss in a chaotic double pendulum with the Information Bottleneck",
    "year": 2022
}