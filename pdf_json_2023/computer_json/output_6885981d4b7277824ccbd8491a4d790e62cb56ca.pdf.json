{
    "abstractText": "This article was posted on the Archives Web site as a New Article. New Articles have been peer reviewed, copyedited, and reviewed by the authors. Additional changes or corrections may appear in these articles when they appear in a future print issue of the Archives. New Articles are citable by using the Digital Object Identifier (DOI), a unique number given to every article. The DOI will typically appear at the end of the abstract.",
    "authors": [
        {
            "affiliations": [],
            "name": "Matthew G. Hanna"
        },
        {
            "affiliations": [],
            "name": "Niels H. Olson"
        },
        {
            "affiliations": [],
            "name": "Rajesh C. Dash"
        },
        {
            "affiliations": [],
            "name": "Markus D. Herrmann"
        },
        {
            "affiliations": [],
            "name": "Larissa V. Furtado"
        },
        {
            "affiliations": [],
            "name": "Michelle N. Stram"
        },
        {
            "affiliations": [],
            "name": "Patricia M. Raciti"
        },
        {
            "affiliations": [],
            "name": "Liron Pantanowitz"
        },
        {
            "affiliations": [],
            "name": "Savitri Krishnamurthy"
        }
    ],
    "id": "SP:e5dc25a78135dc82295bce1314f5ca92b09c600f",
    "references": [
        {
            "authors": [
                "Wians FH Jr.",
                "Gill GW"
            ],
            "title": "Clinical and anatomic pathology test volume by specialty and subspecialty among high-complexity CLIA-certified laboratories",
            "venue": "Lab Med",
            "year": 2011
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration",
            "venue": "FDA authorizes software that can help identify prostate cancer. https://www.fda.gov/news-events/press-announcements/ fda-authorizes-software-can-help-identify-prostate-cancer, Accessed November 9,",
            "year": 2021
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration",
            "venue": "510(k) Premarket notification. X100 with full field peripheral blood smear (PBS) Application. https://www.accessdata. fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID1\u20444K201301. Accessed November 9,",
            "year": 2021
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration",
            "venue": "510(k) Premarket notification. CellaVision. https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?start_ search1\u204441&productcode1\u20444JOY&knumber1\u20444&applicant1\u20444CELLAVISION%20AB. Accessed November 9,",
            "year": 2021
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration",
            "venue": "510(k) Premarket Notification. APAS independence with urine analysis module. https://www.accessdata.fda.gov/scripts/cdrh/ cfdocs/cfpmn/pmn.cfm?ID1\u20444K183648. Accessed December 30,",
            "year": 2021
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration",
            "venue": "Premarket approval (PMA). ThinPrep integrated imager. https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpma/pma. cfm?id1\u20444P950039S036. Accessed November 9,",
            "year": 2021
        },
        {
            "authors": [
                "F Chabrun",
                "X Dieu",
                "M Ferre"
            ],
            "title": "Achieving expert-level interpretation of serum protein electrophoresis through deep learning driven by human reasoning",
            "venue": "Clin Chem. 2021;67(10):1406\u20131414",
            "year": 2021
        },
        {
            "authors": [
                "R Punchoo",
                "S Bhoora",
                "N. Pillay"
            ],
            "title": "Applications of machine learning in the chemical pathology laboratory",
            "venue": "J Clin Pathol. 2021;74(7):435\u2013442",
            "year": 2021
        },
        {
            "authors": [
                "JM Baron",
                "CH Mermel",
                "KB Lewandrowski",
                "AS. Dighe"
            ],
            "title": "Detection of preanalytic laboratory testing errors using a statistically guided protocol",
            "venue": "Am J Clin Pathol",
            "year": 2012
        },
        {
            "authors": [
                "Rosenbaum MW",
                "Baron JM"
            ],
            "title": "Using machine learning-based multianalyte delta checks to detect wrong blood in tube errors",
            "venue": "Am J Clin Pathol",
            "year": 2018
        },
        {
            "authors": [
                "CJL Farrell",
                "J. Giannoutsos"
            ],
            "title": "Machine learning models outperform manual result review for the identification of wrong blood in tube errors in complete blood count results",
            "venue": "Int J Lab Hematol",
            "year": 2022
        },
        {
            "authors": [
                "Y Luo",
                "P Szolovits",
                "AS Dighe",
                "JM. Baron"
            ],
            "title": "Using machine learning to predict laboratory test results",
            "venue": "Am J Clin Pathol",
            "year": 2016
        },
        {
            "authors": [
                "S Poole",
                "LF Schroeder",
                "N. Shah"
            ],
            "title": "Personnel training and competency evaluation planning should be included for all personnel involved in the patient testing process in an end-to-end fashion Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al",
            "venue": "J Biomed Inform",
            "year": 2015
        },
        {
            "authors": [
                "Lee ES",
                "Durant TJS"
            ],
            "title": "Supervised machine learning in the mass spectrometry laboratory: a tutorial",
            "venue": "J Mass Spectrom Adv Clin Lab. 2021;23:1\u20136",
            "year": 2021
        },
        {
            "authors": [
                "M Yu",
                "LAL Bazydlo",
                "DE Bruns",
                "Jr. Harrison JH"
            ],
            "title": "Streamlining quality review of mass spectrometry data in the clinical laboratory by use of machine learning",
            "venue": "Arch Pathol Lab Med",
            "year": 2018
        },
        {
            "authors": [
                "F Demirci",
                "P Akan",
                "T Kume",
                "AR Sisman",
                "Z Erbayraktar",
                "S. Sevinc"
            ],
            "title": "Artificial neural network approach in laboratory test reporting: learning algorithms",
            "venue": "Am J Clin Pathol",
            "year": 2016
        },
        {
            "authors": [
                "J Lipkova",
                "RJ Chen",
                "B Chen"
            ],
            "title": "Artificial intelligence for multimodal data integration in oncology",
            "venue": "Cancer Cell. 2022;40(10):1095\u20131110",
            "year": 2022
        },
        {
            "authors": [
                "A Wong",
                "E Otles",
                "JP Donnelly"
            ],
            "title": "External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients",
            "venue": "JAMA Intern Med",
            "year": 2021
        },
        {
            "authors": [
                "N Rank",
                "B Pfahringer",
                "J Kempfert"
            ],
            "title": "Deep-learning-based real-time prediction of acute kidney injury outperforms human predictive performance",
            "venue": "Npj Digit Med",
            "year": 2020
        },
        {
            "authors": [
                "MA Abd-Elrazek",
                "AA Eltahawi",
                "MH Abd Elaziz",
                "MN. Abd-Elwhab"
            ],
            "title": "Predicting length of stay in hospitals intensive care unit using general admission features",
            "venue": "Ain Shams Eng J. 2021;12(4):3691\u20133702",
            "year": 2021
        },
        {
            "authors": [
                "R Ashmore",
                "R Calinescu",
                "C. Paterson"
            ],
            "title": "Assuring the machine learning lifecycle: desiderata, methods, and challenges",
            "venue": "ACM Comput Surv. 2021;54(5):1\u201339",
            "year": 2021
        },
        {
            "authors": [
                "T Schaffter",
                "DSM Buist",
                "CI Lee"
            ],
            "title": "Evaluation of combined artificial intelligence and radiologist assessment to interpret screening mammograms",
            "venue": "JAMA Netw Open",
            "year": 2020
        },
        {
            "authors": [
                "LM da Silva",
                "EM Pereira",
                "PG Salles"
            ],
            "title": "Independent real-world application of a clinical-grade automated prostate cancer detection system",
            "venue": "J Pathol",
            "year": 2021
        },
        {
            "authors": [
                "D Capper",
                "DTW Jones",
                "M Sill"
            ],
            "title": "DNA methylation-based classification of central nervous system tumours",
            "year": 2018
        },
        {
            "authors": [
                "Aikins JS"
            ],
            "title": "Prototypes and production rules: an approach to knowledge representation for hypothesis formation",
            "venue": "In: International Joint Conference on Artificial Intelligence;",
            "year": 1979
        },
        {
            "authors": [
                "JS Aikins",
                "JC Kunz",
                "EH Shortliffe",
                "RJ. Fallat"
            ],
            "title": "PUFF: an expert system for interpretation of pulmonary function data",
            "venue": "Comput Biomed Res Int J",
            "year": 1983
        },
        {
            "authors": [
                "Aikins JS"
            ],
            "title": "Prototypical knowledge for expert systems: a retrospective analysis",
            "venue": "Artif Intell. 1993;59(1):207\u2013211",
            "year": 1993
        },
        {
            "authors": [
                "Perry CA"
            ],
            "title": "Knowledge bases in medicine: a review",
            "venue": "Bull Med Libr Assoc",
            "year": 1990
        },
        {
            "authors": [
                "AJ Evans",
                "RW Brown",
                "MM Bui"
            ],
            "title": "Validating whole slide imaging systems for diagnostic purposes in pathology: guideline update from the College of American Pathologists in collaboration with the American Society for Clinical Pathology and the Association for Pathology Informatics",
            "venue": "Arch Pathol Lab Med",
            "year": 2020
        },
        {
            "authors": [
                "MM Bui",
                "MW Riben",
                "KH Allison"
            ],
            "title": "quantitative image analysis of human epidermal growth factor receptor 2 immunohistochemistry for breast cancer: guideline from the College of American Pathologists",
            "venue": "Arch Pathol Lab Med",
            "year": 2018
        },
        {
            "authors": [
                "N Aziz",
                "Q Zhao",
                "L Bry"
            ],
            "title": "College of American pathologists\u2019 laboratory standards for next-generation sequencing clinical tests",
            "venue": "Arch Pathol Lab Med",
            "year": 2014
        },
        {
            "authors": [
                "Pressman NJ"
            ],
            "title": "Markovian analysis of cervical cell images",
            "venue": "J Histochem Cytochem. 1976;24(1):138\u2013144",
            "year": 1976
        },
        {
            "authors": [
                "GM Levine",
                "P Brousseau",
                "DJ O\u2019Shaughnessy",
                "GJ. Losos"
            ],
            "title": "Quantitative immunocytochemistry by digital image analysis: application to toxicologic pathology",
            "venue": "Toxicol Pathol",
            "year": 1987
        },
        {
            "authors": [
                "Cornish TC"
            ],
            "title": "Clinical application of image analysis in pathology",
            "venue": "Adv Anat Pathol. 2020;27(4):227\u2013235",
            "year": 2020
        },
        {
            "authors": [
                "Gil J",
                "Wu HS"
            ],
            "title": "Applications of image analysis to anatomic pathology: realities and promises",
            "venue": "Cancer Invest",
            "year": 2509
        },
        {
            "authors": [
                "Webster JD",
                "Dunstan RW"
            ],
            "title": "Whole-slide imaging and automated image analysis: considerations and opportunities in the practice of pathology",
            "venue": "Vet Pathol",
            "year": 2014
        },
        {
            "authors": [
                "AB Tosun",
                "F Pullara",
                "MJ Becich",
                "DL Taylor",
                "JL Fine",
                "SC. Chennubhotla"
            ],
            "title": "Explainable AI (xAI) for anatomic pathology",
            "venue": "Adv Anat Pathol",
            "year": 2020
        },
        {
            "authors": [
                "PHC Chen",
                "Y Liu",
                "L. Peng"
            ],
            "title": "How to develop machine learning models for healthcare.Nat Mater",
            "year": 2019
        },
        {
            "authors": [
                "JH Harrison",
                "JR Gilbertson",
                "MG Hanna"
            ],
            "title": "Introduction to artificial intelligence and machine learning for pathology",
            "venue": "Arch Pathol Lab Med",
            "year": 2020
        },
        {
            "authors": [
                "L Pantanowitz",
                "D Hartman",
                "Y Qi"
            ],
            "title": "Accuracy and efficiency of an artificial intelligence tool when counting breast mitoses",
            "venue": "Diagn Pathol",
            "year": 2020
        },
        {
            "authors": [
                "J Sandbank",
                "A Nudelman",
                "I Krasnitsky"
            ],
            "title": "Implementation of an AI solution for breast cancer diagnosis and reporting in clinical practice",
            "venue": "USCAP",
            "year": 2022
        },
        {
            "authors": [
                "J Sandbank",
                "G Sebag",
                "A Arad"
            ],
            "title": "Validation and clinical deployment of an AI-based solution for detection of gastric adenocarcinoma and Helicobacter pylori in gastric biopsies",
            "venue": "USCAP",
            "year": 2022
        },
        {
            "authors": [
                "B Ehteshami Bejnordi",
                "M Veta",
                "P Johannes van Diest"
            ],
            "title": "diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer",
            "venue": "JAMA. 2017;318(22):2199\u20132210",
            "year": 2017
        },
        {
            "authors": [
                "S Perincheri",
                "AW Levi",
                "R Celli"
            ],
            "title": "An independent assessment of an artificial intelligence system for prostate cancer detection shows strong diagnostic accuracy.Mod Pathol",
            "year": 2021
        },
        {
            "authors": [
                "W Bulten",
                "K Kartasalo",
                "PHC Chen"
            ],
            "title": "Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge",
            "venue": "Nat Med",
            "year": 2022
        },
        {
            "authors": [
                "DF Steiner",
                "R MacDonald",
                "Y Liu"
            ],
            "title": "Impact of deep learning assistance on the histopathologic review of lymph nodes for metastatic breast cancer",
            "venue": "Am J Surg Pathol",
            "year": 2018
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration",
            "venue": "Proposed regulatory framework for modifications to artificial intelligence/machine learning (AI/ML)-based software as a medical device (SAMD). US FDA Artificial Intelligence and Machine Learning Discussion Paper. US https://www.fda.gov/media/122535/download. Accessed December 30,",
            "year": 2021
        },
        {
            "authors": [
                "US Foo"
            ],
            "title": "and Drug Administration Software as a medical device (SAMD): clinical evaluation\u2014guidance for industry and Food and Drug Administration staff",
            "venue": "https://www.fda.gov/media/100714/download. Accessed November 10,",
            "year": 2022
        },
        {
            "authors": [
                "MD Wilkinson",
                "M Dumontier",
                "IJJ Aalbersberg"
            ],
            "title": "The FAIR guiding principles for scientific data management and stewardship",
            "venue": "Sci Data",
            "year": 2016
        },
        {
            "authors": [
                "RD Kush",
                "D Warzel",
                "MA Kush"
            ],
            "title": "FAIR data sharing: the roles of common data elements and harmonization",
            "venue": "J Biomed Inform",
            "year": 2020
        },
        {
            "authors": [
                "S Barocas",
                "M Hardt",
                "A. Narayanan"
            ],
            "title": "Fairness and machine learning",
            "venue": "http:// www.fairmlbook.org. Accessed April",
            "year": 2022
        },
        {
            "authors": [
                "MW Sjoding",
                "RP Dickson",
                "TJ Iwashyna",
                "SE Gay",
                "TS. Valley"
            ],
            "title": "Racial bias in pulse oximetry measurement",
            "venue": "N Engl J Med. 2020;383(25):2477\u20132478",
            "year": 2029
        },
        {
            "authors": [
                "J Buolamwini",
                "T. Gebru"
            ],
            "title": "Gender shades: intersectional accuracy disparities in commercial gender classification",
            "venue": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency. Proc. Machine Learning Res. 2018;81:77\u201391. https://proceedings.mlr.press/v81/buolamwini18a.html. Accessed April",
            "year": 2018
        },
        {
            "authors": [
                "Z Obermeyer",
                "B Powers",
                "C Vogeli",
                "S. Mullainathan"
            ],
            "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
            "year": 2019
        },
        {
            "authors": [
                "FM Howard",
                "J Dolezal",
                "S Kochanny"
            ],
            "title": "The impact of site-specific digital histology signatures on deep learning model accuracy and bias",
            "venue": "Nat Commun",
            "year": 2021
        },
        {
            "authors": [
                "P Leo",
                "G Lee",
                "NNC Shih",
                "R Elliott",
                "MD Feldman",
                "A. Madabhushi"
            ],
            "title": "Evaluating stability of histomorphometric features across scanner and staining variations: prostate cancer diagnosis from whole slide images",
            "venue": "J Med Imaging",
            "year": 2016
        },
        {
            "authors": [
                "T Panch",
                "H Mattie",
                "R. Atun"
            ],
            "title": "Artificial intelligence and algorithmic bias: implications for health systems",
            "venue": "J Glob Health. 2019;9(2):020318",
            "year": 2031
        },
        {
            "authors": [
                "A Jobin",
                "M Ienca",
                "E. Vayena"
            ],
            "title": "The global landscape of AI ethics guidelines",
            "venue": "Nat Machine Intell. 2019;1(9):389\u2013399",
            "year": 2019
        },
        {
            "authors": [
                "BR Jackson",
                "Y Ye",
                "JM Crawford"
            ],
            "title": "The ethics of artificial intelligence in pathology and laboratory medicine: principles and practice",
            "venue": "Acad Pathol. 2021;8:2374289521990784",
            "year": 1990
        },
        {
            "authors": [
                "D Howerton",
                "N Anderson",
                "D Bosse",
                "S Granade",
                "G. Westbrook"
            ],
            "title": "Good laboratory practices for waived testing sites: survey findings from testing sites holding a certificate of waiver under the clinical laboratory improvement amendments of 1988 and recommendations for promoting quality testing",
            "venue": "MMWR Recomm Rep. 2005;54(RR-13):1\u201325; quiz CE1\u20134",
            "year": 2005
        },
        {
            "authors": [
                "J Ezzelle",
                "IR Rodriguez-Chavez",
                "JM Darden"
            ],
            "title": "Guidelines on good clinical laboratory practice",
            "venue": "J Pharm Biomed Anal. 2008;46(1):18\u201329",
            "year": 2007
        },
        {
            "authors": [
                "JA Tworek",
                "MR Henry",
                "B Blond",
                "BA. Jones"
            ],
            "title": "College of American Pathologists Gynecologic Cytopathology Quality Consensus Conference on good laboratory practices in gynecologic cytology: background, rationale, and organization",
            "venue": "Arch Pathol Lab Med",
            "year": 2012
        },
        {
            "authors": [
                "DA Gutman",
                "J Cobb",
                "D Somanna"
            ],
            "title": "Cancer digital slide archive: an informatics resource to support integrated in silico analysis of TCGA pathology data",
            "venue": "J Am Med Inform Assoc",
            "year": 2012
        },
        {
            "authors": [
                "A Fedorov",
                "WJR Longabaugh",
                "D Pot"
            ],
            "title": "NCI imaging data commons",
            "venue": "Cancer Res",
            "year": 2021
        },
        {
            "authors": [
                "JH Choi",
                "SE Hong",
                "HG. Woo"
            ],
            "title": "Pan-cancer analysis of systematic batch effects on somatic sequence variations",
            "venue": "BMC Bioinformatics",
            "year": 2017
        },
        {
            "authors": [
                "WWB Goh",
                "W Wang",
                "L. Wong"
            ],
            "title": "Why batch effects matter in omics data, and how to avoid them",
            "venue": "Trends Biotechnol. 2017;35(6):498\u2013507",
            "year": 2017
        },
        {
            "authors": [
                "S Kothari",
                "JH Phan",
                "TH Stokes",
                "AO Osunkoya",
                "AN Young",
                "MD. Wang"
            ],
            "title": "Removing batch effects from histopathological images for enhanced cancer diagnosis",
            "venue": "IEEE J Biomed Health Inform",
            "year": 2013
        },
        {
            "authors": [
                "JA Tom",
                "J Reeder",
                "WF Forrest"
            ],
            "title": "Identifying and mitigating batch effects in whole genome sequencing data",
            "venue": "BMC Bioinformatics. 2017;18(1):351",
            "year": 2017
        },
        {
            "authors": [
                "F Aeffner",
                "K Wilson",
                "NT Martin"
            ],
            "title": "the gold standard paradox in digital image analysis: manual versus automated scoring as ground truth",
            "venue": "Arch Pathol Lab Med",
            "year": 2016
        },
        {
            "authors": [
                "G St\u00e5lhammar",
                "N Fuentes Martinez",
                "M Lippert"
            ],
            "title": "Digital image analysis outperforms manual biomarker assessment in breast cancer",
            "venue": "Mod Pathol",
            "year": 2016
        },
        {
            "authors": [
                "TO Nielsen",
                "SCY Leung",
                "DL Rimm"
            ],
            "title": "Assessment of Ki67 in breast cancer: updated recommendations from the International Ki67 in Breast Cancer Working Group",
            "venue": "J Natl Cancer Inst. 2021;113(7):808\u2013819",
            "year": 2021
        },
        {
            "authors": [
                "M Dolan",
                "D. Snover"
            ],
            "title": "Comparison of immunohistochemical and fluorescence in situ hybridization assessment of HER-2 status in routine practice",
            "venue": "Am J Clin Pathol",
            "year": 2005
        },
        {
            "authors": [
                "M Singer",
                "CS Deutschman",
                "CW Seymour"
            ],
            "title": "The third international consensus definitions for sepsis and septic shock (Sepsis-3)",
            "year": 2016
        },
        {
            "authors": [
                "KH Goh",
                "L Wang",
                "AYK Yeow"
            ],
            "title": "Artificial intelligence in sepsis early prediction and diagnosis using unstructured data in healthcare",
            "venue": "Nat Commun",
            "year": 2091
        },
        {
            "authors": [
                "JG Elmore",
                "GM Longton",
                "PA Carney"
            ],
            "title": "Diagnostic concordance among pathologists interpreting breast biopsy specimens",
            "venue": "JAMA. 2015;313(11):1122\u20131132",
            "year": 2015
        },
        {
            "authors": [
                "K Viswanathan",
                "A Patel",
                "M Abdelsayed"
            ],
            "title": "Interobserver variability between cytopathologists and cytotechnologists upon application and characterization of the indeterminate category in the Milan system for reporting salivary gland cytopathology",
            "venue": "Cancer Cytopathol",
            "year": 2020
        },
        {
            "authors": [
                "P Tummers",
                "K Gerestein",
                "JW Mens",
                "H Verstraelen",
                "H. van Doorn"
            ],
            "title": "Interobserver variability of the International Federation of Gynecology and Obstetrics staging in cervical cancer",
            "venue": "Int J Gynecol Cancer. 2013;23(5):890\u2013894",
            "year": 2013
        },
        {
            "authors": [
                "S Thomas",
                "Y Hussein",
                "S Bandyopadhyay"
            ],
            "title": "Interobserver variability in the diagnosis of uterine high-grade endometrioid carcinoma",
            "venue": "Arch Pathol Lab Med",
            "year": 2015
        },
        {
            "authors": [
                "M Pentenero",
                "D Todaro",
                "R Marino",
                "S. Gandolfo"
            ],
            "title": "Interobserver and intraobserver variability affecting the assessment of loss of autofluorescence of oral mucosal lesions",
            "venue": "Photodiagn Photodyn Ther. 2019;28:338\u2013342",
            "year": 2019
        },
        {
            "authors": [
                "N Ortonne",
                "SL Carroll",
                "FJ Rodriguez"
            ],
            "title": "Assessing interobserver variability and accuracy in the histological diagnosis and classification of cutaneous neurofibromass.Neuro-Oncol Adv. 2020;2(Suppl 1):i117\u2013i123",
            "year": 2020
        },
        {
            "authors": [
                "HA Kwak",
                "X Liu",
                "DS Allende",
                "RK Pai",
                "J Hart",
                "SY. Xiao"
            ],
            "title": "Interobserver variability in intraductal papillary mucinous neoplasm subtypes and application of their mucin immunoprofiles",
            "venue": "Mod Pathol. 2016;29(9):977\u2013984",
            "year": 2016
        },
        {
            "authors": [
                "CEL Klaver",
                "N Bulkmans",
                "P Drillenburg"
            ],
            "title": "Interobserver, intraobserver, and interlaboratory variability in reporting pT4a colon cancer",
            "venue": "Virchows Arch Int J Pathol",
            "year": 2020
        },
        {
            "authors": [
                "HJ Kang",
                "SY Kwon",
                "A Kim"
            ],
            "title": "A multicenter study of interobserver variability in pathologic diagnosis of papillary breast lesions on core needle biopsy with WHO classification",
            "venue": "J Pathol Transl Med. 2021;55(6):380\u2013387",
            "year": 2021
        },
        {
            "authors": [
                "B Horvath",
                "D Allende",
                "H Xie"
            ],
            "title": "Interobserver variability in scoring liver biopsies with a diagnosis of alcoholic hepatitis",
            "venue": "Alcohol Clin Exp Res",
            "year": 2017
        },
        {
            "authors": [
                "M Burchardt",
                "R Engers",
                "M M\u20ac uller"
            ],
            "title": "Interobserver reproducibility of Gleason grading: evaluation using prostate cancer tissue microarrays",
            "venue": "J Cancer Res Clin Oncol",
            "year": 2008
        },
        {
            "authors": [
                "S Bektas",
                "B Bahadir",
                "NO Kandemir",
                "F Barut",
                "AE Gul",
                "SO. Ozdamar"
            ],
            "title": "Intraobserver and interobserver variability of Fuhrman and modified Fuhrman grading systems for conventional renal cell carcinoma",
            "venue": "Kaohsiung J Med Sci",
            "year": 2009
        },
        {
            "authors": [
                "FD Allard",
                "JD Goldsmith",
                "G Ayata"
            ],
            "title": "Intraobserver and interobserver variability in the assessment of dysplasia in ampullary mucosal biopsies",
            "venue": "Am J Surg Pathol",
            "year": 2018
        },
        {
            "authors": [
                "FJ Rodriguez",
                "C. Giannini"
            ],
            "title": "Oligodendroglial tumors: diagnostic and molecular pathology",
            "venue": "Semin Diagn Pathol",
            "year": 2010
        },
        {
            "authors": [
                "E Samorodnitsky",
                "J Datta",
                "BM Jewell"
            ],
            "title": "Comparison of custom capture for targeted next-generation DNA sequencing",
            "venue": "J Mol Diagn",
            "year": 2015
        },
        {
            "authors": [
                "G Campanella",
                "MG Hanna",
                "L Geneslaw"
            ],
            "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
            "venue": "Nat Med",
            "year": 2019
        },
        {
            "authors": [
                "ME Shipe",
                "SA Deppen",
                "F Farjah",
                "EL. Grogan"
            ],
            "title": "Developing prediction models for clinical use using logistic regression: an overview",
            "venue": "J Thorac Dis. 2019; 11(Suppl 4):S574\u2013S584",
            "year": 2019
        },
        {
            "authors": [
                "SH Park",
                "K. Han"
            ],
            "title": "Methodologic guide for evaluating clinical performance and effect of artificial intelligence technology for medical diagnosis and prediction",
            "year": 2018
        },
        {
            "authors": [
                "KG Moons",
                "AP Kengne",
                "DE Grobbee"
            ],
            "title": "Risk prediction models: II. External validation, model updating, and impact assessment",
            "year": 2012
        },
        {
            "authors": [
                "TPA Debray",
                "Y Vergouwe",
                "H Koffijberg",
                "D Nieboer",
                "EW Steyerberg",
                "KGM. Moons"
            ],
            "title": "A new framework to enhance the interpretation of external validation studies of clinical prediction models",
            "venue": "J Clin Epidemiol",
            "year": 2014
        },
        {
            "authors": [
                "L Wu",
                "J Zhang",
                "W Zhou"
            ],
            "title": "Randomised controlled trial of WISENSE, a real-time quality improving system for monitoring blind spots during esophagogastroduodenoscopy.Gut",
            "year": 2019
        },
        {
            "authors": [
                "P Wang",
                "X Liu",
                "TM Berzin"
            ],
            "title": "Effect of a deep-learning computer-aided detection system on adenoma detection during colonoscopy (CADe-DB trial): a double-blind randomised study",
            "venue": "Lancet Gastroenterol Hepatol",
            "year": 2020
        },
        {
            "authors": [
                "A Repici",
                "M Badalamenti",
                "R Maselli"
            ],
            "title": "Efficacy of real-time computer-aided detection of colorectal neoplasia in a randomized trial",
            "venue": "Gastroenterology. 2020;159(2):512\u2013520.e7",
            "year": 2020
        },
        {
            "authors": [
                "M Wijnberge",
                "BF Geerts",
                "L Hol"
            ],
            "title": "Effect of a machine learning-derived early warning system for intraoperative hypotension vs standard care on depth and duration of intraoperative hypotension during elective noncardiac surgery: the HYPE randomized clinical trial",
            "venue": "JAMA",
            "year": 2020
        },
        {
            "authors": [
                "P Wang",
                "TM Berzin",
                "JR Glissen Brown"
            ],
            "title": "Real-time automatic detection system increases colonoscopic polyp and adenoma detection rates: a prospective randomised controlled study",
            "venue": "Gut. 2019;68(10):1813\u20131819",
            "year": 2018
        },
        {
            "authors": [
                "INFANT Collaborative Group"
            ],
            "title": "Computerised interpretation of fetal heart rate during labour (INFANT): a randomised controlled trial",
            "venue": "Lancet.",
            "year": 2017
        },
        {
            "authors": [
                "B Van Calster",
                "DJ McLernon",
                "M van Smeden"
            ],
            "title": "Calibration: the Achilles heel of predictive analytics",
            "venue": "BMC Med",
            "year": 2019
        },
        {
            "authors": [
                "K Van Hoorde",
                "S Van Huffel",
                "D Timmerman",
                "T Bourne",
                "B. Van Calster"
            ],
            "title": "A spline-based tool to assess and visualize the calibration of multiclass risk predictions",
            "venue": "J Biomed Inform. 2015;54:283\u2013293",
            "year": 2014
        },
        {
            "authors": [
                "T van der Ploeg",
                "D Nieboer",
                "EW. Steyerberg"
            ],
            "title": "Modern modeling techniques had limited external validity in predicting mortality from traumatic brain injury",
            "venue": "J Clin Epidemiol",
            "year": 2016
        },
        {
            "authors": [
                "L Pantanowitz",
                "GM Quiroga-Garza",
                "L Bien"
            ],
            "title": "An artificial intelligence algorithm for prostate cancer diagnosis in whole slide images of core needle Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 25 biopsies: a blinded clinical validation and deployment study",
            "venue": "Lancet Digit Health",
            "year": 2020
        },
        {
            "authors": [
                "SE Davis",
                "RA Greevy",
                "C Fonnesbeck",
                "TA Lasko",
                "CG Walsh",
                "ME. Matheny"
            ],
            "title": "A nonparametric updating method to correct clinical prediction model drift",
            "venue": "J Am Med Inform Assoc",
            "year": 2019
        },
        {
            "authors": [
                "JI Epstein",
                "MJ Zelefsky",
                "DD Sjoberg"
            ],
            "title": "A contemporary prostate cancer grading system: a validated alternative to the Gleason score",
            "venue": "Eur Urol",
            "year": 2015
        },
        {
            "authors": [
                "EM Hattab",
                "MO Koch",
                "JN Eble",
                "H Lin",
                "L. Cheng"
            ],
            "title": "Tertiary Gleason pattern 5 is a powerful predictor of biochemical relapse in patients with Gleason score 7 prostatic adenocarcinoma",
            "venue": "J Urol",
            "year": 2006
        },
        {
            "authors": [
                "V Garc\u0131\u0301a",
                "RA Mollineda",
                "JS. S\u00e1nchez"
            ],
            "title": "Index of balanced accuracy: a performance measure for skewed class distributions",
            "venue": "Pattern Recognition and Image Analysis. Lecture Notes in Computer Science",
            "year": 2009
        },
        {
            "authors": [
                "Delgado R",
                "Tibau XA"
            ],
            "title": "Why Cohen\u2019s kappa should be avoided as performance measure in classification",
            "venue": "PloS One",
            "year": 2019
        },
        {
            "authors": [
                "A. Ben-David"
            ],
            "title": "Comparison of classification accuracy using Cohen\u2019s weighted kappa",
            "venue": "Expert Syst Appl. 2008;34(2):825\u2013832",
            "year": 2006
        },
        {
            "authors": [
                "D Chicco",
                "G. Jurman"
            ],
            "title": "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation",
            "venue": "BMC Genomics",
            "year": 2020
        },
        {
            "authors": [
                "J. Gorodkin"
            ],
            "title": "Comparing two K-category assignments by a K-category correlation coefficient",
            "venue": "Comput Biol Chem. 2004;28(5):367\u2013374",
            "year": 2004
        },
        {
            "authors": [
                "P Baldi",
                "S Brunak",
                "Y Chauvin",
                "CA Andersen",
                "H. Nielsen"
            ],
            "title": "Assessing the accuracy of prediction algorithms for classification: an overview",
            "year": 2000
        },
        {
            "authors": [
                "Moskowitz CS"
            ],
            "title": "Using free-response receiver operating characteristic curves to assess the accuracy of machine diagnosis of cancer",
            "venue": "JAMA. 2017;318(22):2250\u2013",
            "year": 2017
        },
        {
            "authors": [
                "SH Park",
                "J Choi",
                "JS. Byeon"
            ],
            "title": "Key principles of clinical validation, device approval, and insurance coverage decisions of artificial intelligence",
            "venue": "Korean J Radiol",
            "year": 2021
        },
        {
            "authors": [
                "QD Vu",
                "S Graham",
                "T Kurc"
            ],
            "title": "Methods for segmentation and classification of digital microscopy tissue",
            "venue": "images. Front Bioeng Biotechnol",
            "year": 2019
        },
        {
            "authors": [
                "R D\u2019Agostino",
                "B. Nam"
            ],
            "title": "Evaluation of the performance of survival analysis models: discrimination and calibration measures. Amsterdam, The Netherlands: Elsevier; 2003:1\u201325",
            "year": 2003
        },
        {
            "authors": [
                "DW Hosmer",
                "S. Lemeshow"
            ],
            "title": "Assessing the fit of the model",
            "year": 2000
        },
        {
            "authors": [
                "US Departmen"
            ],
            "title": "of Health and Human Services, Food and Drug Administration, Center for Devices and Radiological Health",
            "venue": "Statistical guidance on reporting results from studies evaluating diagnostic tests\u2014guidance for industry and FDA staff. US Food and Drug Administration Web site. https://www.fda.gov/ regulatory-information/search-fda-guidance-documents/statistical-guidancereporting-results-studies-evaluating-diagnostic-tests-guidance-industry-and-fda. Accessed September 5,",
            "year": 2022
        },
        {
            "authors": [
                "S. Morgenthaler"
            ],
            "title": "Exploratory data analysis",
            "venue": "WIREs Comput Stat",
            "year": 2009
        },
        {
            "authors": [
                "I. Ben-Gal"
            ],
            "title": "Outlier detection",
            "year": 2005
        },
        {
            "authors": [
                "Bland JM",
                "Altman DG"
            ],
            "title": "Statistical methods for assessing agreement between two methods of clinical measurement",
            "year": 1986
        },
        {
            "authors": [
                "Bland JM",
                "Altman DR"
            ],
            "title": "Statistical methods for assessing agreement between measurements",
            "year": 1986
        },
        {
            "authors": [
                "PH Petersen",
                "D St\u00f6ckl",
                "O Blaabjerg"
            ],
            "title": "Graphical interpretation of analytical data from comparison of a field method with reference method by use of difference plots",
            "venue": "Clin Chem",
            "year": 1997
        },
        {
            "authors": [
                "S. Hollis"
            ],
            "title": "Analysis of method comparison studies",
            "venue": "Ann Clin Biochem. 1996;33(1):1\u20134",
            "year": 1996
        },
        {
            "authors": [
                "D. St\u00f6ckl"
            ],
            "title": "Beyond the myths of difference plots",
            "venue": "Ann Clin Biochem",
            "year": 1996
        },
        {
            "authors": [
                "PJ Cornbleet",
                "N. Gochman"
            ],
            "title": "Incorrect least-squares regression coefficients in method-comparison analysis",
            "venue": "Clin Chem",
            "year": 1979
        },
        {
            "authors": [
                "RJ McEnroe",
                "AP Durham",
                "MD Goldford"
            ],
            "title": "Evaluation of Precision of Quantitative Measurement Procedures; Approved Guideline",
            "venue": "3rd ed. CLSI document EP05-A3. Wayne, PA: Clinical and Laboratory Standards Institute;",
            "year": 2014
        },
        {
            "authors": [
                "RN Carey",
                "AP Durham",
                "WW Hauck"
            ],
            "title": "User Verification of Precision and Dstimation of Bias; Approved Guideline",
            "venue": "3rd ed. CLSI document EP15-A3. Wayne, PA: Clinical and Laboratory Standards Institute;",
            "year": 2014
        },
        {
            "authors": [
                "Berte LM"
            ],
            "title": "Process Management. Wayne, PA: Clinical and Laboratory Standards Institute",
            "year": 2015
        },
        {
            "authors": [
                "DA Jenkins",
                "M Sperrin",
                "GP Martin",
                "N. Peek"
            ],
            "title": "Dynamic models to predict health outcomes: current status and methodological challenges",
            "venue": "Diagn Progn Res",
            "year": 2018
        },
        {
            "authors": [
                "DB Toll",
                "KJM Janssen",
                "Y Vergouwe",
                "KGM. Moons"
            ],
            "title": "Validation, updating and impact of clinical prediction rules: a review",
            "venue": "J Clin Epidemiol",
            "year": 2008
        },
        {
            "authors": [
                "TH Kappen",
                "Y Vergouwe",
                "WA van Klei",
                "L van Wolfswinkel",
                "CJ Kalkman",
                "KGM. Moons"
            ],
            "title": "Adaptation of clinical prediction models for application in local settings.Med Decis Making. 2012;32(3):E1\u2013E10",
            "year": 2012
        },
        {
            "authors": [
                "SE Davis",
                "TA Lasko",
                "G Chen",
                "ED Siew",
                "ME. Matheny"
            ],
            "title": "Calibration drift in regression and machine learning models for acute kidney injury",
            "venue": "J Am Med Inform Assoc",
            "year": 2017
        },
        {
            "authors": [
                "Diamond GA"
            ],
            "title": "What price perfection?: calibration and discrimination of clinical prediction models",
            "venue": "J Clin Epidemiol",
            "year": 1992
        },
        {
            "authors": [
                "SE Davis",
                "TA Lasko",
                "G Chen",
                "ME. Matheny"
            ],
            "title": "Calibration drift among regression and machine learning models for hospital mortality",
            "venue": "AMIA Annu Symp Proc. 2018;2017:625\u2013634",
            "year": 2017
        },
        {
            "authors": [
                "Sinard JH"
            ],
            "title": "An analysis of the effect of the COVID-19 pandemic on case volumes in an academic subspecialty-based anatomic pathology practice",
            "year": 2095
        },
        {
            "authors": [
                "DM Mann",
                "J Chen",
                "R Chunara",
                "PA Testa",
                "O. Nov"
            ],
            "title": "COVID-19 transforms health care through telemedicine: evidence from the field",
            "venue": "J Am Med Inform Assoc",
            "year": 2020
        },
        {
            "authors": [
                "F Calabrese",
                "F Pezzuto",
                "F Fortarezza"
            ],
            "title": "Pulmonary pathology and COVID-19: lessons from autopsy. The experience of European pulmonary pathologists",
            "venue": "Virchows Arch",
            "year": 2002
        },
        {
            "authors": [
                "F Di Toro",
                "M Gjoka",
                "G Di Lorenzo"
            ],
            "title": "Impact of COVID-19 on maternal and neonatal outcomes: a systematic review and meta-analysis",
            "venue": "Clin Microbiol Infect",
            "year": 2020
        },
        {
            "authors": [
                "MG Hanna",
                "VE Reuter",
                "O Ardon"
            ],
            "title": "Validation of a digital pathology system including remote review during the COVID-19 pandemic",
            "venue": "Mod Pathol",
            "year": 2020
        },
        {
            "authors": [
                "E Vigliar",
                "R Cepurnaite",
                "E Alcaraz-Mateos"
            ],
            "title": "Global impact of the COVID-19 pandemic on cytopathology practice: results from an international survey of laboratories in 23 countries",
            "venue": "Cancer Cytopathol",
            "year": 2020
        },
        {
            "authors": [
                "YW Tang",
                "JE Schmitz",
                "DH Persing",
                "CW. Stratton"
            ],
            "title": "Laboratory diagnosis of COVID-19: current issues and challenges",
            "venue": "J Clin Microbiol",
            "year": 2020
        },
        {
            "authors": [
                "SE Davis",
                "RA Greevy",
                "TA Lasko",
                "CG Walsh",
                "ME. Matheny"
            ],
            "title": "Detection of calibration drift in clinical prediction models to inform model updating",
            "venue": "J Biomed Inform. 2020;112:103611",
            "year": 2020
        },
        {
            "authors": [
                "SG Finlayson",
                "JD Bowers",
                "J Ito",
                "JL Zittrain",
                "AL Beam",
                "IS. Kohane"
            ],
            "title": "Adversarial attacks on medical machine learning",
            "year": 2019
        },
        {
            "authors": [
                "J Allyn",
                "N Allou",
                "C Vidal",
                "A Renou",
                "C. Ferdynus"
            ],
            "title": "Adversarial attack on deep learning-based dermatoscopic image recognition systems: risk of misdiagnosis due to undetectable image perturbations. Medicine (Baltimore)",
            "year": 2020
        },
        {
            "authors": [
                "NG Laleh",
                "D Truhn",
                "GP Veldhuizen"
            ],
            "title": "Adversarial attacks and adversarial robustness in computational pathology",
            "venue": "Nat Commun. 2022;13(1):5711",
            "year": 2022
        },
        {
            "authors": [
                "G Bortsova",
                "C Gonz\u00e1lez-Gonzalo",
                "SC Wetstein"
            ],
            "title": "Adversarial attack vulnerability of medical image analysis systems: unexplored factors",
            "venue": "Med Image Anal. 2021;73:102141",
            "year": 2021
        },
        {
            "authors": [
                "P Raciti",
                "J Sue",
                "R Ceballos"
            ],
            "title": "Novel artificial intelligence system increases the detection of prostate cancer in whole slide images of core needle biopsies.Mod Pathol",
            "year": 2066
        },
        {
            "authors": [
                "Nishikawa RM",
                "Bae KT"
            ],
            "title": "Importance of better human-computer interaction in the era of deep learning: mammography computer-aided diagnosis as a use case",
            "venue": "J Am Coll Radiol",
            "year": 2018
        },
        {
            "authors": [
                "JK Burgoon",
                "JA Bonito",
                "B Bengtsson",
                "C Cederberg",
                "M Lundeberg",
                "L. Allspach"
            ],
            "title": "Interactivity in human-computer interaction: a study of credibility, understanding, and influence",
            "venue": "Comput Hum Behav",
            "year": 2000
        },
        {
            "authors": [
                "M Jensen",
                "T Meservy",
                "J Burgoon",
                "J. Nunamaker"
            ],
            "title": "Automatic, multimodal evaluation of human interaction",
            "venue": "Group Decis Negot. 2010;19:367\u2013389",
            "year": 2010
        },
        {
            "authors": [
                "Lee EJ"
            ],
            "title": "Factors That Enhance Consumer Trust in Human-Computer Interaction: An Examination of Interface Factors and Moderating Influences [dissertation",
            "venue": "Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al Knoxville: University of Tennessee. https://trace.tennessee.edu/utk_graddiss/2148. Accessed April",
            "year": 2022
        },
        {
            "authors": [
                "Szalma JL",
                "Hancock PA"
            ],
            "title": "Noise effects on human performance: a meta-analytic synthesis",
            "year": 2011
        },
        {
            "authors": [
                "C. Rudin"
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nat Machine Intell",
            "year": 2019
        },
        {
            "authors": [
                "I Banerjee",
                "AR Bhimireddy",
                "JL Burns"
            ],
            "title": "Reading race: AI recognises patient\u2019s racial identity in medical images. ArXiv.2107.10356 [Cs.Eess]. [Preprint",
            "venue": "Posted online July",
            "year": 2021
        },
        {
            "authors": [
                "B Sch\u00f6mig-Markiefka",
                "A Pryalukhin",
                "W Hulla"
            ],
            "title": "Quality control stress test for deep learning-based diagnostic model in digital pathology",
            "venue": "Mod Pathol",
            "year": 2021
        },
        {
            "authors": [
                "Shapley LS"
            ],
            "title": "A value for n-person games",
            "venue": "eds. Contributions to the Theory of Games (AM-28),",
            "year": 1953
        },
        {
            "authors": [
                "Lundberg S",
                "Lee SI"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "arXiv. [Preprint. Posted online Nov 25,",
            "year": 2017
        },
        {
            "authors": [
                "C. Molnar"
            ],
            "title": "Interpretable Machine Learning. https://christophm.github.io/ interpretable-ml-book",
            "venue": "Accessed May",
            "year": 2022
        },
        {
            "authors": [
                "B Kim",
                "M Wattenberg",
                "J Gilmer"
            ],
            "title": "Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCAV)",
            "venue": "arXiv. [Preprint. Posted online June",
            "year": 2018
        },
        {
            "authors": [
                "MT Ribeiro",
                "S Singh",
                "C. Guestrin"
            ],
            "title": "Why should I trust you?\u201d: explaining the predictions of any classifier",
            "venue": "arXiv. [Preprint. Posted online February",
            "year": 2016
        },
        {
            "authors": [
                "T Evans",
                "CO Retzlaff",
                "C Geibler"
            ],
            "title": "The explainability paradox: challenges for xAI in digital pathology",
            "venue": "Future Gener Comput Syst",
            "year": 2022
        },
        {
            "authors": [
                "P Linardatos",
                "V Papastefanopoulos",
                "S. Kotsiantis"
            ],
            "title": "Explainable AI: a review of machine learning interpretability methods. Entropy Basel Switz",
            "year": 2020
        },
        {
            "authors": [
                "A Sears",
                "JA Jacko",
                "eds"
            ],
            "title": "Human-Computer Interaction Fundamentals",
            "venue": "Boca Raton, FL: CRC Press;",
            "year": 2010
        },
        {
            "authors": [
                "PL Fitzgibbons",
                "LA Bradley",
                "LA Fatheree"
            ],
            "title": "Principles of analytic validation of immunohistochemical assays: Guideline from the College of American Pathologists Pathology and Laboratory Quality Center",
            "venue": "Arch Pathol Lab Med",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "NEW ARTICLE This article was posted on the Archives Web site as a New Article. New Articles have been peer reviewed, copyedited, and reviewed by the authors. Additional changes or corrections may appear in these articles when they appear in a future print issue of the Archives. New Articles are citable by using the Digital Object Identifier (DOI), a unique number given to every article. The DOI will typically appear at the end of the abstract.\nThe DOI for this manuscript is doi: 10.5858/arpa.2023-0042-CP\nThe print version of this manuscript will replace the New Article version at the above DOI once it is available.\n\u00a9 College of American Pathologists2023\nCAP Laboratory Improvement Programs\nRecommendations for Performance Evaluation of Machine Learning in Pathology\nA Concept Paper From the College of American Pathologists\nMatthew G. Hanna, MD; Niels H. Olson, MD; Mark Zarella, PhD; Rajesh C. Dash, MD; Markus D. Herrmann, MD, PhD; Larissa V. Furtado, MD; Michelle N. Stram, MD; Patricia M. Raciti, MD; Lewis Hassell, MD; Alex Mays, MD;\nLiron Pantanowitz, MD, PhD, MHA; Joseph S. Sirintrapun, MD; Savitri Krishnamurthy, MD; Anil Parwani, MD, PhD, MBA; Giovanni Lujan, MD; Andrew Evans, MD; Eric F. Glassy, MD; Marilyn M. Bui, MD, PhD; Rajendra Singh, MD;\nRhona J. Souers, MS; Monica E. de Baca, MD; Jansen N. Seheult, MD\nContext.\u2014Machine learning applications in the pathology clinical domain are emerging rapidly. As decision support systems continue to mature, laboratories will increasingly need guidance to evaluate their performance in clinical practice. Currently there are no formal guidelines to assist pathology laboratories in verification and/or validation of such systems. These recommendations are being proposed for the evaluation of machine learning systems in the clinical practice of pathology.\nObjective.\u2014To propose recommendations for performance evaluation of in vitro diagnostic tests on patient samples that incorporate machine learning as part of the preanalytical, analytical, or postanalytical phases of the laboratory workflow. Topics described include considerations for machine learning model evaluation including risk assessment, predeployment requirements, data sourcing and curation, verification and\nvalidation, change control management, human-computer interaction, practitioner training, and competency evaluation.\nData Sources.\u2014An expert panel performed a review of the literature, Clinical and Laboratory Standards Institute guidance, and laboratory and government regulatory frameworks.\nConclusions.\u2014Review of the literature and existing documents enabled the development of proposed recommendations. This white paper pertains to performance evaluation of machine learning systems intended to be implemented for clinical patient testing. Further studies with real-world clinical data are encouraged to support these proposed recommendations. Performance evaluation of machine learning models is critical to verification and/or validation of in vitro diagnostic tests using machine learning intended for clinical practice.\n(Arch Pathol Lab Med. doi: 10.5858/arpa.2023-0042-CP)\nInstitution of machine learning (ML) in the pathology clinicaldomain has gained momentum and is rapidly advancing. As ML-based clinical decision support (CDS) systems continue to\nmature, laboratories will increasingly need support to evaluate their performance in clinical practice. The medical domain of pathology has traditionally contributed vast nonimaging data\nAccepted for publication September 11, 2023. From the Department of Pathology and Laboratory Medicine, Memorial Sloan Kettering Cancer Center, New York, New York (Hanna, Sirintrapun); The Defense Innovation Unit, Mountain View, California (Olson); The Department of Pathology, Uniformed Services University, Bethesda, Maryland (Olson); Department of Laboratory Medicine and Pathology, Mayo Clinic, Rochester, Minnesota (Zarella, Seheult); Department of Pathology, Duke University Health System, Durham, North Carolina (Dash); Department of Pathology, Massachusetts General Hospital and Harvard Medical School, Boston (Herrmann); Department of Pathology, St. Jude Children\u2019s Research Hospital, Memphis, Tennessee (Furtado); The Department of Forensic Medicine, New York University, and Office of Chief Medical Examiner, New York (Stram); Paige.AI, New York, New York (Raciti); Department of Pathology, Oklahoma University Health Sciences Center, Oklahoma City (Hassell); The MITRE Corporation, McLean, Virginia (Mays); Department of Pathology & Clinical Labs, University of Michigan, Ann Arbor (Pantanowitz); Department of Pathology, MD Anderson Cancer Center, Houston, Texas (Krishnamurthy); Department of Pathology, The Ohio State University Wexner Medical Center, Columbus (Parwani, Lujan); Laboratory Medicine, Mackenzie Health, Toronto, Ontario, Canada (Evans); Affiliated Pathologists Medical Group, Rancho Dominguez, California (Glassy); Departments of Pathology and Machine Learning, Moffitt Cancer Center, Tampa, Florida (Bui); Department of Dermatopathology, Summit Health, Summit Woodland Park, New Jersey (Singh); Department of Biostatistics, College of American Pathologists, Northfield, Illinois (Souers); Pacific Pathology Partners, Seattle, Washington (de Baca).\nAll authors are members from the Machine Learning Workgroup, Informatics Committee, Digital and Computational Pathology Committee, and Council on Informatics and Pathology Innovation of the College of American Pathologists, except Souers, who is an employee of the College of American Pathologists. Hanna is a consultant for PaigeAI, PathPresenter, and VolastraTX. Krishnamurthy is a consultant on the breast pathology faculty advisory board for Daiichi Sankyo, Inc. and AstraZeneca and serves as a scientific advisory board member for AstraZeneca. Krishnamurthy received an investigator-initiated sponsored research award from PathomIQ Research, sponsored research funding from IBEX Research, and an investigator-initiated sponsored research award from Caliber Inc. Raciti has stock options at Paige (,1%) and has employment and stock compensation at Janssen. Mays\u2019 affiliation with the MITRE Corporation is provided for identification purposes only, and is not intended to convey or imply MITRE\u2019s concurrence with, or support for, the positions, opinions, or viewpoints expressed by the author. The other authors have no relevant financial interest in the products or companies described in this article.\nCorresponding author: Matthew G. Hanna, MD, Department of Pathology and Laboratory Medicine, Memorial Sloan Kettering Cancer Center, 1275 York Avenue, New York, New York 10065 (email: hannam@mskcc.org).\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 1\nto clinical information systems (eg, chemistry analyzers, hematology results, microbiology species, bioinformatic molecular pipelines, mass spectrometry). With pathology continuing to undergo digital transformation, a myriad of digital imaging data (eg, whole slide imaging, digital peripheral blood smears, digital plate reading, imaging mass spectrometry) are being generated by devices in pathology laboratories and used for clinical decision making.1 Image-based CDS systems in pathology have emerged for a variety of use cases, including white blood cell classification in peripheral blood smears, screening of gynecologic liquid-based cytology specimens, serum protein electrophoresis interpretation, microbiology automated culture plate evaluation, parasite classification in stool samples, and detection of prostatic carcinoma in whole slide images of tissue section specimens.2\u20137 However, MLbased CDS systems are not limited to imaging; they also include the analysis of nonimaging data. Use cases include evaluation of preanalytic testing errors,8,9 patient misidentification via wrong blood in tube errors,10,11 redundant analyte testing,12,13 establishing reference intervals,14 amino acid profiles/ mass spectrometry,15,16 and autoverification.17,18 Additionally, electronic health records can leverage laboratory instrument data and other patient information, including multimodal data,19 to train ML models for prediction of specific patient outcomes (eg, sepsis prediction,20 acute kidney injury,21 patient length of stay,22 etc). The vision for clinical use of ML-based CDS systems is apparent; however, evidence that these systems are safe and efficacious for widespread clinical use is lacking. Patient care is a safety-critical application, where errors may have significant negative consequences. ML-based systems intended to support or guide clinical decisions require a level of assurance beyond those needed for research or other nonclinical applications.23 The use of CDS in pathology must be assured with sufficient evidence to be fit for purpose (ie, diagnostic testing), have defined performance characteristics, be integrated into clinical workflows (eg, with verification and/ or validation), and vigilantly require ongoing monitoring.\nIn principle, CDS using ML-based systems can either be designed for use with pathologists in the loop as a humanmanaged workflow, or autonomously to directly influence an intended use. Autonomous ML-based systems in pathology can be defined as any artificial intelligence (AI)\u2013based methods that operate to perform any critical part of a pathologist\u2019s clinical duties (eg, diagnostic reporting) without agreement and oversight of a qualified pathology healthcare professional. Evidence to support clinical use of autonomous ML-based systems is lacking. Furthermore, professional societies in the field of radiology have recommended the US Food and Drug Administration (FDA) refrain from authorizing autonomous ML-based software as a medical device (SaMD) until sufficient data are available to advocate for these devices to be safe for patient care.24 As pathologists practice medicine using MLbased systems, the combined competencies of the human and computational model infers a term called \u201caugmented intelligence.\u201d25,26 The American Medical Association (AMA) has propagated this term to represent the use of ML-based systems that supplement, rather than substitute, human healthcare providers. The concept of augmented intelligence is based upon achieving an enhanced performance when the ML-based model is used in conjunction with the trained healthcare professional, compared to their individual performance alone. Examples of augmented intelligence in pathology include assistive tools that may impact pathologists\u2019 diagnostic capabilities, such as identifying suspicious regions of interest on\na slide for review, probabilistic-based protein electrophoresis interpretation, or tumor methylation\u2013based classification to support clinical decision making.7,27,28\nDevelopment of formal guidelines to deploy ML-based models in medical practice is challenging. Medical practice has arguably been founded on knowledge-based expert systems. Human-derived knowledge has been encoded as rules and relies on information available from experts in a specified domain. Decisions made through these rule-based systems and the decision-making process underlying the algorithm can be easily traced. Rule-based expert systems use human expert knowledge to solve real-world problems that would normally necessitate human intelligence. Rulebased expert systems encoded in knowledge bases have been developed since the 1970s.29\u201332 The advancement in technology and computing resources coupled with a digital transformation of the pathology field has demonstrated a need to provide further guidance for systems that incorporate emerging technologies such as ML-based CDS. Typically, when organizations put forth formal validation guidelines, they are based on extensive meta-analysis and expert review. The College of American Pathologists (CAP) Pathology and Laboratory Quality Center for Evidence-Based Guidelines has proffered guidelines for clinical implementation of whole slide imaging, digital quantitative image analysis of human epidermal growth factor receptor 2 (HER2) immunohistochemical stains, and molecular testing.33\u201335 The final recommended guidelines encompass substantial evidence-based data including systematic literature review, strength of evidence framework, open comment feedback, expert panel consensus, and advisory panel review. Due to the relative fewML-based applications currently used in clinical settings, the evidence available to formulate guidelines is lacking. Additionally, ML systems differ fundamentally from rule-based expert systems, conventional digital image processing or computer vision techniques, or other traditional software or hardware implementations in the laboratory. Conventional digital image processing (ie, image analysis) uses relatively defined image properties that are explainable and reproducible; relying on manually engineered features (eg, cell size, contours, entropy, skewness, brightness, contrast, and other Markovian features).36\u201340 In contrast, ML-based systems \u201clearn\u201d features or patterns from the training data, with the resultant ML model having potential inherent biases. ML models are generally tested on data not used in the training of the model (ie, unseen held-out test data) to estimate the generalization performance.41 In addition, explainable ML models may aid in performance evaluation, safeguard against bias, facilitate regulatory evaluation, and offer insights into the augmented intelligence decision-making process.42,43\nThe increasing utilization of ML-based approaches has facilitated the development of ML models relevant for many applications pertinent to pathology practice. Implementing ML-based CDS will transform pathology practice. ML-based systems may be authorized for an intended use by regulatory bodies for clinical applications, or they may be deployed clinically by a laboratory (eg, local deployment site), as a laboratory-developed test (LDT). There are commercially available systems, including some authorized by the FDA. Pathologists need to know how to evaluate the analytical and clinical performance of ML models and understand how ML-based CDS systems integrate with the patient care paradigm. Currently data on real-world clinical use is insufficient to develop formal evidence-based guidelines, as clinically deployed benchmarks\n2 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nof ML model performance evaluations are still emerging; thus, no published guidelines exist for clinical evaluation of MLbased systems in the pathology and laboratory medicine domain. This paper provides proposed recommendations relating to performance evaluation of a ML-based system intended for clinical use.\nScope of Guidance\nThis guidance document intends to (1) provide information on performance evaluation of systems that utilize ML derived from laboratory-generated data and that is intended to guide pathology clinical reporting of patient samples; (2) discuss aspects pertinent to evaluating the performance of a pretrained, ML model intended to be verified and/or validated at a deployment laboratory site; (3) discuss ML models, either procured commercially or established as LDTs, that include ML in any part of the test and are used in reporting of patient samples; and (4) provide a proposed framework to inform performance evaluation of ML-based software devices with a focus on verification and validation (eg, analytical, clinical) of a given ML model implemented at a laboratory using realworld data from the local deployment site.\nThese ML models may be used in conjunction with imaging-based microscopy in surgical pathology or laboratory medicine or use clinical nonimaging data. The goal of performance evaluation is to assess the model\u2019s ability to generalize the features learned from the training data and render predictions from the local deployment site laboratory data. To improve the generalizability of the ML model, additional training (eg, recalibration) using data retrieved from the deployment-site laboratory can be performed prior to verification and/or validation, and ultimately before use in a clinical setting as an LDT.\nThis guidance document provides recommendations on performance evaluation of clinical tests that utilize ML in any part of the preanalytic, analytic, or postanalytic workflow. Outside of the scope of this document are (1) ML models deployed outside the clinical laboratory and pathology, even if they use laboratory data (eg, electronic medical record); (2) development of an ML-based model; (3) providing guidance on study design for model training and testing;44 (4) guidance on local site retuning of parameters or retraining using deployment-site local data; and (5) software that solely retrieves information, organizes, or optimizes processes. ML basics will not be reviewed in detail; readers may reference a prior publication introducing ML in pathology.45 The authors encourage publication of additional literature to generate data to strengthen these recommendations and future formal guideline creation. The generation of evidence-based data will enable model credibility and build trust surrounding the predictive capability of a given model.\nEVALUATION CONSIDERATIONS\nPathology and laboratory medicine healthcare professionals are familiar with incorporating new diagnostic testing procedures and technologies into clinical practice. This includes new laboratory instrumentation, deployment of digital pathology systems, onboarding and optimization of new immunohistochemistry clones, and the required verification and/or validation of new procedures and technologies before clinical testing on patient samples. If certain clinical tests are not commercially available, laboratories have conventionally generated their own tests and testing protocols, referred to as an LDT, to support\nemergent clinical needs and clinical decision making. ML models are similarly available commercially through industry entities or can also be deployed as LDTs if being used for patient testing.\nAs laboratories historically have evaluated, and continue to evaluate, performance of new tests being offered clinically, laboratories should pursue performance evaluation of ML models based on intended use cases; the evaluation should be carried out in the same fashion as the test will be clinically implemented. Performance evaluation should include inputs related to the intended use case and assessed similarly to clinical practice (eg, imaging modalities, tissue type, procedure, stain, task, etc). Each component of the ML-based system should be evaluated in the assessment as the system will be clinically deployed. At the completion of any change to the evaluated system, reevaluation should be conducted, to ensure the system is working as intended and performance characteristics are defined.\nTerminologies and definitions relating to ML performance evaluation, such as validation, vary based on the respective domains (eg, clinical healthcare, regulatory, ML science). Table 1 provides a glossary of terms as they are defined and used in this document. Laboratory evaluation of an unmodified FDA-authorized (eg, cleared, approved) ML model, which has already undergone external validation, may only require limited verification showing that the deployment laboratory-use case fits those defined in the approved use and that performance matches that predicted in the regulatory review. Using a modified FDA-authorized ML model not in compliance with the stipulations of the intended use requires additional development of an LDT, assertion of the performance characteristics, and approval from the laboratory medical director. The Clinical Laboratory Improvement Act of 1988 (CLIA)46 and Amendment regulations are federal standards to provide certification and oversight to laboratories performing clinical testing in the United States. CLIA regulations require LDTs to be developed in a certified clinical laboratory and to evaluate the performance characteristics prior to reporting patient test results.47 The laboratory should continue to follow mandated regulatory standards for maintaining quality systems for nonwaived testing46 (Figure 1).\nIn evaluating the performance characteristics of a test, clinical verification and validation procedures are commonplace in the laboratory. Table 2 reflects the definitions for verification and validation based on various laboratory, standards, and regulatory organizations. LDTs currently do not require FDA authorization; however, the CLIA requires the laboratory to establish performance characteristics relating to the analytical validity prior to reporting test results from patient samples. The LDT is limited to the deploying CLIA-certified laboratory and may not be meaningful, or used clinically, by other nonaffiliated laboratories. In the pathology literature, analytical validation studies using ML-based models predominate; however, relatively few have included comprehensive true clinical validation of an end-to-end system with a pathology healthcare professional using these tools and reporting results compared to a reference standard.27,48\u201354"
        },
        {
            "heading": "Risk Assessment",
            "text": "The CAP supports the regulatory framework that proposes a risk-based stratification of ML models.55 The scope of the performance evaluation should be guided by a comprehensive risk assessment to evaluate both potential sources of variability and error and the potential for patient harm. The risk assessment must include failure mode effects analysis, a\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 3\nAbbreviations: FDA, US Food and Drug Administration; ML, machine learning.\n4 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nprocess to identify the sources, frequency, and impact of potential failures and errors for a testing process. Aspects of the risk assessment include the following:\n1. Preanalytic, analytic, and postanalytic phases of testing (see Predeployment Considerations section); 2. Intended medical uses of the software and impact if inaccurate results are reported (clinical risk); 3. System components (environment, specimen, instrumentation, reagents, and testing personnel); and 4. Manufacturer/developer instructions and recommendations for the intended use (if applicable).\nThe laboratory director must consider the laboratory\u2019s clinical, ethical, and legal responsibilities for providing accurate and reliable patient test results. Published data and information may be used to supplement the risk assessment but are not substitutes for the laboratory\u2019s own studies and evaluation. Representative testing personnel from the laboratory should be involved in the risk assessment and performance evaluation.56\nThere are different risk-stratification approaches of ML models. Risk classification schema from the SaMD risk categorization framework is based on the significance of the information provided by SaMD toward healthcare decisions (eg, inform or drive clinical management, diagnose, or treat) and the severity of the healthcare situation or condition itself (eg, nonserious, serious, critical).55,57 A risk-stratification approach put forth by the American Society of Mechanical Engineers (ASME), the risk assessment matrix of ASME V&V40-2018, compares the combination of the model influence and the consequence the output decision might infer.58 The model\u2019s influence can be defined as the measure of contribution to the resultant decision compared to other available information, or power of the model output in the decision-making process. The risk categorization of the ML-based CDS software may also depend on the characteristics of the underlying ML model (eg, static model versus adaptive/continuous-learning model) (Figure 2).\nPredeployment Considerations\nPrior to clinical laboratory deployment of ML-based CDS software, pathology personnel should be familiar with the laboratory integration points and be aware that preanalytic variables may have a downstream impact on the model outputs. Furthermore, laboratory processes can impact the performance of the software system and an integrated clinical\nworkflow design is needed for the deployment site. This ensures receipt of data and output by the model are based on the intended clinical workflow."
        },
        {
            "heading": "Model Development and Properties",
            "text": "Understanding the model training and testing environment is helpful to evaluate the model for the intended deployment context. The intended use of a model will facilitate scoping the performance evaluation in the accepted and expected patient sample cohort.59 ML models range from detection tasks (eg, localization, segmentation) to classification, quantification, regression (eg, continuous variable), or clustering, amongst others. Dataset curation is one of the earliest steps of model development, a portion of which becomes the training data from which the selected model extracts features and on which it bases the outputs. Developers should provide the medical rationale (eg, clinical utility) of the model. The sample size and distribution of data used for model training, validation, selection, and the testing set should be described, including the composition of the training, validation, and test sets as well as the data sampling strategies and other weighting factors (eg, class weights used in the loss function). In addition, the description of the data should also cover any inclusion and exclusion criteria that were applied during curation of the datasets, or how missing data may have been handled during the model development process. If data were annotated, understanding how and by whom those annotations were provided can provide insights into model development and affect subsequent performance evaluation design. Furthermore, comprehending the reference standard used for initial model training and testing can provide confidence that the model is appropriate for evaluation in the clinical setting and fits the proposed use case for laboratories. The laboratory should be provided with the stand-alone performance of the model for each included sample type as it was tested; this includes accuracy, precision (eg, positive predictive value), recall (eg, sensitivity), specificity, and any additional information related to the reportable range or values."
        },
        {
            "heading": "Data Characteristics (Case Mix and Data Compatibility)",
            "text": "The difference in data distributions between the data samples that are used to train the model versus those used for performance evaluation at a deployment laboratory site can be analyzed through data compatibility. The case cohorts and data compatibility are essential to estimate generalizable\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 5\nT ab\nle 2 .\nV ar ia ti o n o f V er ifi ca ti o n an\nd V al id at io n T er m in o lo gi es\nb y D if fe re n t D o m ai n s\nP ro ce d u re\nC A P\nC LI A\nC LS I\nFD A\nIS O\nV er if ic at io n\nT h e p ro ce ss\nb y w h ic h a\nla b o ra to ry\nd et er m in es\nth at\nan u n m o d if ie d FD A -a u th o ri ze d te st p er fo rm s ac co rd in g to th e sp ec if ic at io n s se t fo rt h b y th e m an u fa ct u re r w h en u se d as d ir ec te d\nEa ch\nla b o ra to ry\nth at\nin tr o d u ce s an\nu n m o d if ie d FD\nA -a u th o ri ze d te st\nsy st em\n(n o n w ai ve d ) m u st d o th e\nfo ll o w in g b ef o re\nre p o rt in g p at ie n t\nte st re su lt s: 1 . D em\no n st ra te\nth at\nit ca n o b ta in\np er fo rm\nan ce\nsp ec if ic at io n s co\nm -\np ar ab\nle to\nth o se\nes ta b li sh ed\nb y\nth e m an\nu fa ct u re r fo r th e fo ll o w -\nin g p er fo rm\nan ce\nch ar ac te ri st ic s:\n(A ) A cc u ra cy (B ) P re ci si o n (C ) R ep o rt ab\nle ra n ge\no f te st re su lt s fo r th e te st sy st em\n2 .\nV er if y th at\nth e m an\nu fa ct u re r\u2019 s\nre fe re n ce\nin te rv al s (n o rm\nal va l-\nu es ) ar e ap\np ro p ri at e fo r th e la b -\no ra to ry \u2019s p at ie n t p o p u la ti o n\nT h e co\nn fi rm\nat io n th ro u gh\nte st in g th at\nth e p er fo r-\nm an\nce ch\nar ac te ri st ic s\no f th e m ea su re m en t p ro ce d u re ar e co\nn si s-\nte n t w it h es ta b li sh ed la b el cl ai m s. V er if ic at io n is n o rm\nal ly\np er fo rm\ned b y th e en d u se r b ef o re m ea su re - m en t im p le m en ta ti o n fo r ro u ti n e u se in th e la b o ra to ry\nA n al yt ic al\nva li d at io n m ea su re s th e ab\nil it y\no f a Sa M D\nto ac cu\nra te ly , re li ab\nly , an d p re ci se ly ge n er at e th e in te n d ed te ch n ic al o u tp u t fr o m th e in p u t d at a \u2022 C o n fi rm s an d p ro vi d es o b je ct iv e ev id en ce th at th e so ft w ar e w as co rr ec tl y co n st ru ct ed \u2013 n am el y, co rr ec tl y an d re li ab ly p ro ce ss es in p u t d at a an d ge n er at es o u tp u t d at a w it h th e ap p ro p ri at e le ve l o f ac cu ra cy , an d re p ea ta b il it y an d re p ro d u ci b il it y (i e, p re ci si o n ); an d \u2022 D em o n st ra te s th at th e so ft w ar e m ee ts it s sp ec if ic at io n s, an d th e so ft w ar e sp ec if ic at io n s co n fo rm to u se r n ee d s an d in te n d ed u se s\nR ec o m m en\nd at io n V\nEx am\nin at io n p ro ce d u re s th at , b as ed\no n av ai la b le\nd o cu\nm en\nta ti o n , d o\nn o t n ec es sa ri ly\nh av e to\nb e\nva li d at ed\nsh al l at\nle as t b e ve ri fi ed\nfo r th e re le va n t p er fo rm\nan ce\nch ar ac te ri st ic s\nR ec o m m en\nd at io n V I\nIf ev id en ce\nfr o m\na va lid at io n p er fo rm ed el se w h er e is in co m p le te ,v er ifi ca tio n is in su ff ic ie n t an d a su p p le m en ta l va li d at io n in o n e\u2019 s o w n la b o ra to ry sh al l b e n ec es sa ry\nR ec o m m en\nd at io n V II\nW h en\nu si n g m o re\nth an\no n e an\nal yz er\nfo r th e sa m e m ea su re , an ap p ro p ri at e ve ri fi ca ti o n o f ea ch in d iv id u al d ev ic e sh al l b e p er fo rm ed ap p ly in g th e ap p ro p ri at e ac ce p ta n ce cr ite ri a\nV al id at io n\nT h e p ro ce ss\nu se d to\nco n fi rm\nw it h o b je ct iv e\nev id en\nce th at a la b o ra to ry -d ev el o p ed te st o r m o d if ie d FD\nA - au th o ri ze d te st m et h o d o r in st ru m en t sy st em d el iv er s ac cu ra te an d re li ab le re su lt s fo r th e in te n d ed ap p li ca ti o n\nEa ch\nla b o ra to ry\nth at\nm o d if ie s an\nFD A -a u th o ri ze d te st sy st em\n, o r\nin tr o d u ce s a te st sy st em\nn o t su b je ct\nto FD\nA cl ea ra n ce\no r ap\np ro va l (i n cl u d in g m et h o d s d ev el o p ed in -h o u se /l ab o ra to ry -d ev el o p ed\nte st\nan d st an\nd ar d iz ed\nm et h o d s su ch as te xt b o o k p ro ce d u re s) , o r u se s a te st sy st em in w h ic h p er fo rm an ce sp ec if ic at io n s ar e n o t p ro vi d ed b y th e m an u fa ct u re r m u st , b ef o re re p o rt in g p at ie n t te st re su lt s, es ta b li sh fo r ea ch te st sy st em th e p er fo rm an ce sp ec if ic at io n s fo r th e fo ll o w in g p er fo rm an ce ch ar ac te ri st ic s, as ap p li ca b le : 1 . A cc u ra cy 2 . P re ci si o n 3 . A n al yt ic al se n si ti vi ty 4 . A n al yt ic al sp ec if ic it y to in cl u d e in te rf er in g su b st an ce s 5 . R ep o rt ab le ra n ge o f te st re su lt s fo r th e te st sy st em 6 . R ef er en ce in te rv al s (n o rm al va lu es ) 7 . A n y o th er p er fo rm an ce ch ar ac te ri st ic re q u ir ed fo r te st p er fo rm an ce .\nT h e es ta b li sh m en\nt an\nd /o r\nco n fi rm\nat io n th ro u gh\nex te n si ve\nte st in g,\no f th e\nan al yt ic al\nan d /o r\ncl in ic al\np er fo rm\nan ce ch ar ac te ri st ic s o f th e m ea su re m en t p ro ce d u re\nC li n ic al\nva li d at io n m ea su re s th e ab\nil it y o f\na Sa M D\nto yi el d a cl in ic al ly\nm ea n in g-\nfu l o u tp u t as so ci at ed\nw it h th e ta rg et\nu se\no f Sa M D\no u tp u t in\nth e ta rg et\nh ea lt h ca re\nsi tu at io n o r co\nn d it io n\nid en\nti fi ed\nin th e Sa M D\nd ef in it io n\nst at em\nen t. C li n ic al ly\nm ea n in gf u l\nm ea n s th e p o si ti ve\nim p ac t o f a Sa M D\no n th e h ea lt h o f an\nin d iv id u al\no r p o p u -\nla ti o n , to\nb e sp ec if ie d as\nm ea n in gf u l, m ea su ra b le , p at ie n tre le va n t cl in ic al o u tc o m e( s) , in cl u d in g o u tc o m e( s) re la te d to th e fu n ct io n o f th e Sa M D (e g, d ia gn o si s, tr ea tm en t, p re d ic ti o n o f ri sk , p re d ic ti o n o f tr ea tm en t re sp o n se ), o r a p o si ti ve im p ac t o n in d iv id u al o r p u b li c h ea lt h . C li n ic al va li d it y is ev al u at ed an d d et er m in ed b y th e m an u fa ct u re r d u ri n g th e d ev el o p m en t o f a Sa M D b ef o re it is d is tr ib u te d fo r u se (p re m ar ke t) an d af te r d is tr ib u ti o n w h il e th e Sa M D is in u se (p o st m ar ke t) . C li n ic al va li d at io n o f a Sa M D ca n al so b e vi ew ed as th e re la ti o n sh ip b et w ee n th e ve ri fi ca ti o n an d va li d at io n re su lt s o f th e Sa M D al go ri th m an d th e cl in ic al co n d it io n s o f in te re st . C li n ic al va li d ati o n is a n ec es sa ry co m p o n en t o f cl in ic al ev al u at io n fo r al l Sa M D an d ca n b e d em o n st ra te d b y ei th er :\nR ec o m m en\nd at io n II\nT h e la b o ra to ry\nsh al l va li d at e ex am in at io n p ro ce d u re s d er iv ed fr o m th e fo ll o w in g so u rc es : \u2022 n o n st an d ar d ex am in at io n p ro ce d u re s \u2022 la b o ra to ry -d es ig n ed o r d ev el o p ed ex am in at io n p ro ce d u re s \u2022 va li d at ed ex am in at io n p ro ce d u re s u se d o u ts id e th ei r in te n d ed u se \u2022 va li d at ed ex am in at io n p ro ce d u re s su b se q u en tl y m o d if ie d\nR ec o m m en\nd at io n II fu rt h er\nex p la in ed : T h e n o n st an d ar d p ro ce d u re n am ed u n d er (a ) is n am ed as su ch in IS O 1 5 1 8 9 :2 0 1 2 b u t in p ra ct ic e al w ay s re fe rs to th e p ro ce d u re h er e li st ed u n d er (b ) o r (c ). In (c ), a \u201cv al id at ed ex am in at io n p ro ce - d u re \u201d is co n si d er ed eq u iv al en t to th e te rm in o lo gy \u201cs ta n d ar d m et h o d \u201d n am ed in IS O 1 5 1 8 9 :2 0 1 2 . P ar t o f th e va li d at io n p ro c e ss is th e o b je c ti v e e st a b - li sh m e n t (t h ro u g h m e a su re m e n t) o f th e re le v a n t p e rf o rm a n c e ch ar ac te ri st ic s. In th e af o re m en - ti o n ed si tu at io n s, ei th er th e p er fo rm an ce ch ar ac te ri st ic s ar e\n6 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nT ab\nle 2 .\nC o n ti n u ed\nP ro ce d u re\nC A P\nC LI A\nC LS I\nFD A\nIS O\n\u2022 R ef er en\nci n g ex is ti n g d at a fr o m\nst u d -\nie s co\nn d u ct ed\nfo r th e sa m e in te n d ed\nu se \u2022 R ef er en\nci n g ex is ti n g d at a fr o m\nst u d ie s\nco n d u ct ed\nfo r a d if fe re n t in te n d ed\nu se , w h er e ex tr ap\no la ti o n o f su ch\nd at a\nca n b e ju st if ie d , o r \u2022 G e n e ra ti n g n e w\nc li n ic a l d a ta\nfo r a\nsp e c if ic\nin te n d e d u se\nn o t es ta b li sh ed\nac co\nrd in g to\nIS O\n1 5 1 8 9 :2 0 1 2 , o r th e ex am\nin at io n\np ro ce d u re\nh as\nb ee n m o d if ie d in\nsu ch\na w ay\nth at\nth es e p er fo rm\nan ce\nch ar ac te ri st ic s ar e,\nac co\nrd in g to\nIS O\n1 5 1 8 9 :2 0 1 2 , n o t au\nto m at i-\nca ll y va li d fo r th e m o d if ie d ex am in at io n p ro ce d u re . If an ex am in at io n p ro ce d u re is m o d if ie d af te r va li d at io n o r ve ri fi ca ti o n , th e ef fe ct o f su ch a m o d if ic at io n sh al l b e ta ke n in to co n si d er at io n an d ev al u at ed (d o cu m en te d m o ti va ti o n ). A p ro fe ss io n al ev al u at io n , fo r ex am p le o f d ec re as in g th e sa m p le vo lu m e in a te st to m ak e it su it ab le fo r p ed ia tr ic sa m p le s, w o u ld ta ke in to ac co u n t w h et h er th is d ec re as e co u ld ch an ge th e re ac ti o n m ix tu re co n d it io n s si gn if ic an tl y an d as a co n se q u en ce w h ic h p er fo rm an ce ch ar ac te ri st ic s m ig h t b e af fe ct ed b y su ch a ch an ge\nR ec o m m en\nd at io n II\nIf an\nex am\nin at io n p ro ce d u re\nis m o d i-\nfi ed\n, it sh al l b e ta ke n in to\nco n si d -\ner at io n an\nd d o cu\nm en\nte d if th er e ar e p o te n ti al /r el ev an t co n se q u en ce s fo r th e p er fo rm\nan ce\nch ar ac te ri st ic s an\nd , if so , w h at\nth es e co\nn se q u en\nce s ar e\nR ec o m m en\nd at io n IV\nW h en\nan ex am\nin at io n p ro ce d u re\nis m o d if ie d w it h re sp ec t to\nth e\nd o cu\nm en\nte d m o ti va ti o n , a\n(s u p p le m en\nta l) va li d at io n o r ve ri fi ca ti o n sh al l b e p er fo rm\ned\nA b b re vi at io n s: C A P , C o ll eg e o f A m er ic an\nP at h o lo gi st s; C LI A , C li n ic al\nLa b o ra to ry\nIm p ro ve m en\nt A m en\nd m en\nts ; C LS I, C li n ic al\nan d La b o ra to ry\nSt an\nd ar d s In st it u te ; FD\nA , U S Fo\no d an\nd D ru g\nA d m in is tr at io n ; IS O , In te rn at io n al\nO rg an\niz at io n fo r St an\nd ar d iz at io n ; Sa M D , so ft w ar e as\na m ed\nic al\nd ev ic e.\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 7\nperformance of the pretrained model to the deployment laboratory. As ML models are trained, specific data are used and should be explicitly stated in the operating procedures. The deployment site laboratory can infer cohort compatibility based on appropriate measures related to the intended patient and specimen characteristics. These should include the data properties from both clinical and technical standpoints. Clinical parameters include but are not limited to patient demographics, specimen type, specimen collection body site, specimen collection method (eg, biopsy, resection), stains and other additives (fixatives, embedding media), analyte, variant, additive, container (eg, tube type), and the diagnosis. Technical parameters include data type, file extension, instrumentation hardware and software versions, relevant data acquisition parameters, imaging resolution, units, output value set (eg, classes, reportable range), and visualizations. Including a thorough predeployment evaluation of the clinical and technical parameters will ensure the ML model will work as intended.\nEthics and Ethical Bias\nBias is inherent in almost all aspects of medical practice, including ML-based systems. Different types of bias can affect the model outputs and performance characterization and 2 predominant forms of bias are described. Algorithmic bias refers to systematic and unfair discrimination that may be present in a model. FAIR (findability, accessibility, interoperability, and reusability) guiding principles were described to support meaningful data management and stewardship, and can help mitigate bias.60\u201362 Representation bias (eg, evaluation bias, sample bias) refers to an ML model that omits a specific subgroup or characteristic within the training data compared to the eventual data the model will be used on clinically. Studies showing underrepresented populations in medical data are emerging and may impact model performance in various subclasses.63\u201365 Even within open-access medical databases, predictive modeling shows the ability to generate site-specific signatures from\nhistopathology features and accurately predict the respective institution that submitted the data.66 Measurement bias refers to the model being trained on proxy data or features rather than the ideal intended target; for instance, a model trained on data from a single instrument, and planned for deployment at a site with a similar instrument, but from a different vendor.67 Measurement bias can also be related to the data annotation process, where inconsistently labeled data may cause bias in the model outputs. A definitive reference standard should be used and qualified during the ML model performance evaluation (see Reference Standard section). Bias should be considered in the evaluation process and include equal and equitable review of the inputs and outputs. Algorithmic bias in ML-based models has been shown to have demographic inequities.68 Equality ensures identical assets are provided for each group regardless of differences; equity acknowledges differences exist between the groups, and respectfully allocates weights to each group based on their needs. Evaluation of ethics across multiple domains has engendered the need for describing transparency, justice and fairness, nonmaleficence, responsibility, and privacy as part of ML model development and deployment.69,70\nLaboratory Influences\nDue to the variation in laboratory protocols and practices, inherent biases and respective influences exist, which the preanalytic processes may embed within the laboratory\u2019s data. These variables can impact the downstream model development and deployment. To ensure appropriate model performance, the ML model should ideally be trained across the range of all possible patient samples to which the model will eventually be exposed in clinical practice. While this is not always possible, when evaluating model performance in deployment sites, careful attention should be given to patient sample subgroups\u2014especially those with clinical significance. Variations in sample collection, preparation, processing, and handling may affect\n8 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nmodel outputs if the model has not been exposed to the observed laboratory data variation during model training.\nWhile it is recommended to ensure good laboratory practices71\u201373 in order to sustain appropriate patient care, as well as maintain consistent data inputs for model predictions, there are inherent intralaboratory and interlaboratory variations that need to be taken into account. Laboratories should implement approved procedures to support a robust quality control (QC) process to mitigate and minimize defects. QC defects may impact ML models and it is arguably more important to remediate ML models compared to human evaluation since humans may be less sensitive to such artifacts. Contrarily, the QC process itself may introduce bias by removing artifacts commonly encountered in specimen preparation, testing, and result reporting. Furthermore, it is likely that QC protocols and operating procedures vary considerably across laboratories, leading to the potential for additional generalizability challenges. Pathologists, and ultimately the laboratory directors, are responsible for assessments in the predeployment phase and through ongoing monitoring of ML models implemented for clinical testing. Representative examples of preanalytic variables and their potential impact on model outcomes are listed in Table 3.\nEvaluation Data Sourcing\nQuality CDS systems in pathology result from thoughtful design in the developmental stages and careful attention to components of clinical workflow both upstream and downstream. The laboratory must first implement, verify, and validate the digital workflow including additional hardware and\nsoftware, and be in accordance with pertinent regulatory guidelines.\nComprehensive development of the ML model with an inclusive and diverse training dataset is critical to the performance of the deployed tool. Proper verification or establishment of performance specifications using a deployment site\u2019s real-world laboratory data as a substrate is required. The deployment site\u2019s data (eg, laboratory instrument data, imaging data, sequencing data, etc) should reflect the spectrum of sample types and characteristics typically seen during clinical patient testing at the deployment site. The data should be readily accessible to complete the needed performance characterization of the ML model. All samples used in verification and/or validation of the pipeline should be documented as part of the clinical performance evaluation dataset. The evaluation data used for performance characterization should be further quarantined; it should not be included in any training or tuning of the ML model."
        },
        {
            "heading": "Using Real-World Data",
            "text": "Laboratories intending to use ML-based models in the clinical workflow will need to perform evaluation studies on their deployment site data. Based on the laboratory blueprint and intended uses, intralaboratory and interlaboratory variables need to be assessed. This may require clinical informaticists or technical staff (eg, information technology staff) involvement to oversee the evaluation. Laboratory information systems can be queried for patient specimens to identify appropriate retrospective data for determination of performance specifications. Prospective data can be used for evaluation if the\nAbbreviations: ABL, Abelson; BCR, Breakpoint cluster region; HLA, human leukocyte antigen.\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 9\nMLmodel results are not clinically reported prior to verification and/or validation. It is imperative that all clinically meaningful variations are included in the diverse dataset for performance evaluation so that the model may be exposed to and evaluate performance in respective subgroups.\nSome ML-based models in pathology have been developed using open-access databases. National Institutes of Health databases, such as The Cancer Genome Atlas, have pioneered data sharing and governance for supporting research through a data commons.74,75 However, literature shows open databases are not without challenges. Open datasets have limitations (in batch effects relating to local laboratory idiosyncrasies) that introduce biases in ML models.66,76\u201379 Private data used for model training may also suffer from local site biases and transparency, and may not generalize well to deployment site data if the local sample characteristics differ from those trained in the model. Generalizability of the learned features in the model to the deployment laboratory data will vary depending on the similarity of the represented data distribution.\nIntended Use and Indications for Use\nFor each clinically deployed ML model, a clinical utility that can be described through clinical needs and use cases should exist. Once the intended use of a pretrained model has been defined, the clinical pipeline from sample collection to clinical reporting should be determined for clinical verification and/or validation. Regulatory terms are helpful in understanding \u201cwhat for\u201d and \u201chow\u201d (eg, intended use) versus \u201cwho,\u201d \u201cwhere,\u201d and \u201cwhen\u201d (eg, indications for use) an ML model is used in the clinical setting. The clinical utility determines the \u201cwhy.\u201d The intended use is essentially the claimed model purpose, whereas the indications for use are the specific reasons or situations the laboratory will use the ML model for testing. The model performance evaluation should, therefore, be limited to the intended use and evaluated based on the indications for use. The clinical verification and/or validation procedures should be applicable to both of these terms, including the time point of use (eg, primary diagnosis, consultative, or quality assurance) and using a similar distribution of specimen types to be examined with enrichment of variant subtypes that are expected to be reproducibly and accurately detected in the deployment site\u2019s clinical practice. These samples should be selected to evaluate the performance of the model using appropriate evaluation metrics. While not required, evaluating the ML model with out-of-domain samples (eg, those not indicated as part of the intended use or indications of use) may be appropriate to further characterize model performance and establish model credibility and trust. Out-of-domain testing can support establishing \u201cclassification ranges\u201d to the validated model input data (eg, a model intended to detect prostate cancer in prostate biopsies is tested on a prostate biopsy with lymphoma and identifies a focus \u201csuspicious for cancer\u201d).\nReference Standard\nTo appropriately evaluate the performance of a given ML model, a ground truth is needed to compare against the outputs of the ML model. The medical domain may contain noisy data, such that data trained on archival specimens may have different diagnostic criteria, patient populations, disease prevalence, or treatments compared to current practice. Pathology data include diseases that may have been\nreclassified, or diagnostic criteria that may have changed over time. This temporal data heterogeneity is also inherent within pathology reports, electronic medical records, and other information systems in relation to data quality. These factors should be considered when using historical data inputs for verification and/or validation if they do not accurately and precisely reflect current pathology practice. Patients who have the same disease can demonstrate varying characteristics (eg, age, sex, demographics, comorbidities) that differ based on clinical setting. Prevalence of disease may also differ between geographic areas. Furthermore, certain disease states are challenging even to expert interpretation, and cause label noise during the evaluation of the model. Traditional histopathology diagnosis or laboratory value reporting have been regarded as ground truth; however, based on the contextual use of the model, these may only represent a proxy for the ideal target of the model. For imaging data, the diagnosis may also only exist in the form of narrative free-text, which may contain nuances and may not be readily comparable with model outputs to establish whether the predictions are accurate.\nAccuracy in certain settings may be applicable as concordance. The performance evaluation may need to evaluate the reference data to ensure \u201cinaccurate\u201d model predictions are not in fact errors in ground truth through discordance analysis. A paradox exists where pathologists and laboratory data are clinically reported and fundamentally provide the basis of medical decision making; however, with any assessment there may be inherent human biases to the results.80 This is especially true when correlated with visual imaging quantification.81\u201383 Regardless, pathologists\u2019 diagnostic analyses have been utilized as the prime training labels for ML-based models. Furthermore, there exists a spectrum of confidence for the ground truth of a given test. For instance, predicting HER2 amplification from the morphology of a hematoxylin-eosin slide can be directly compared to in situ hybridization results of the same tissue block. Tumor heterogeneity aside, there can be relative high confidence of comparing the model output and HER2 probe signals. Similarly, a regression model predicting sepsis from laboratory instrument inputs can be correlated to sepsis diagnostic criteria.84\u201386 However, for challenging diagnoses or those with high interobserver or interinstrument variability,87\u2013101 a consensus or adjudication review panel may be appropriate for establishing the reference standard for a given evaluation cohort. The reference standard may differ for various models depending on their intended use, output, and interaction in the pathology workflow. Furthermore, the reference standard requires comparable labels for comparison to the model outputs. Weakly supervised techniques may obviate manual human expert curation by knowing the presence of a tumor or not from synoptic reporting data elements linked directly to the digital data. However, large numbers of data elements linked to their respective metadata may be required.102 Some reference data, such as diagnostic free-text, may not be structured and easily exportable. It may require manual curation to identify referential data points to which to compare. Additionally, development of a true reference standard will require effort to ensure the labels are accurate. For example, in surgical pathology, the diagnostic report is conventionally described at the specimen part level\u2014inclusive of findings across all slides of the surgical specimens. The digital slides that are used for evaluation of the model thus may need to be carefully selected, otherwise they may not be representative of the features being\n10 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nevaluated in the ML model. The final utilized reference standard and testing protocol are recommended to be accepted and approved by the laboratory director prior to starting the performance evaluation process, ensuring mitigation of potential errors with use of the CDS tool.\nPERFORMANCE EVALUATION\nPerformance evaluation of any test deployed in a clinical laboratory for patient testing is an accreditation and regulatory requirement under CLIA 1988.46 Pathology departments follow manufacturer instructions, regulatory requirements, guidelines, and established best practices to perform, develop, or provide clinical testing. Following regulatory requirements is necessary prior to patient testing, and performance characteristics should be specified for a given clinical test. Generalizability of an ML model is tested through performance evaluation, to ensure the model renders accurate predictions given the deployment site\u2019s data, compared to the data that were used to train the model. Differences in data representation (eg, units, class distribution, interlaboratory differences) may bias the trained model and performance at the deployment site may be low (eg, underfit) due to the data mismatch. While there is literature supporting the hypothesis that models trained using data from multiple sites may have higher performance, it is the responsibility of the deployment site laboratory to verify and/or validate the model using the real-world data expected to represent the case mix and distribution of the implementing laboratory (eg, single site versus receiving samples from other sites).103\u2013105\nSoftware systems can be evaluated using white-box or black-box testing. In white-box testing, evaluation is performed through inspection of the internal workings or structures of the system and the details of the model design. In black-box testing, the evaluation is entirely based on the examination of the systems\u2019 functionality without any knowledge of design details. Evaluation is performed by passing an input to the system and comparing the returned output with the established reference standard. The system performance is then characterized using evaluation metrics that quantify the difference between the returned value (eg model output) and the expected value (eg reference ground truth label) aggregated following multiple samples. In most cases, evaluation of ML models for clinical practice relies on black-box testing, because the internal workings of the evaluated models cannot be inspected (eg in cases of proprietary software) or are highly complex and difficult to interpret (eg, deep neural network models with innumerable parameters and many nonlinearities). The goal of ML model evaluation is to derive an estimate of the generalization error, which represents a measure of the ability of a model trained on a specific set of samples in the training set to generalize and make accurate predictions for unseen data in the validation or test set. It is important to note that performance evaluation only provides an estimate of the generalization error: the true generalization error remains unknown, as the total data to test the model is infinite. The performance estimation depends on the size and composition of the evaluation set and how closely samples in the training data resemble the patient or specimen population for which the model will be used in clinical practice. Additionally, the performance characterization will be impacted by how closely the clinical test procedure resembles the intended use.\nClinical Verification and Validation\nPerformance evaluation in a laboratory consists at least of 2 concepts: verification and validation. Verification contains the root word \u201cverus\u201d (eg, true), whereas the root word of validation is \u201cvalidus\u201d (eg, strong). Clinical verification is the process of evaluating whether the ML model is trained correctly; clinical validation evaluates the ML model using laboratory real-world data to ensure it performs as expected. This is not to be confused with the technical ML concept of validation that is intended to find data anomalies or detect differences between existing training and new data, then subsequently adjusting hyperparameters of the model to optimize performance (eg, tuning). While beyond the scope of this document, satisfactory software code verification should be performed by the model developers prior to clinical verification or validation. Clinical validation confirms the performance characteristics of the ML model using the deployment laboratory\u2019s real-world data. Validation is needed for those ML models that have been internally developed by the laboratory (i.e, LDT) or for ML models that have been modified from published documentation by the original manufacturer or process developer (eg, modified FDA authorized ML model) or an unmodified ML model used beyond the manufacturer\u2019s statement of intended use.\nClinical Verification.\u2014Clinical verification focuses on development methodologies (eg, technical requirements). Performance of the software system may be tested in silico using a dedicated testing environment and a curated test dataset. This is similar to the regulatory analytical validity, where ML scientists work in conjunction with subject matter experts to evaluate performance of a trained model on a specified dataset. Clinical verification may not specifically involve prospective clinical patient results reporting; instead, it enables the implementing laboratory visibility of the model performance by analyzing the model outputs compared to the known specified reference standard. Curation and labeling of the dataset should be performed in close collaboration with, and under the supervision of, pathologists to ensure that the dataset is representative and ground truth labels are correct. The verification process may be used for an unmodified FDA-authorized ML model where the specifications of the manufacturer are used as directed. Verification of an unmodified FDA-authorized ML model should demonstrate that the performance characteristics (eg, accuracy, precision, reportable range, and reference intervals/range) match those of the manufacturer.\nBy predetermining the dataset, the prevalence of the medical condition in the evaluation dataset is artificially determined compared to the real-world clinical setting. To avert selection bias, it is imperative to select datasets for clinical verification performance evaluation that reflect the degree of variation of the intended condition(s) seen by the deployment site laboratory and compare the performance characteristics of the ML model established by the manufacturer or developer. Quantifying the extent of relatedness to the verification samples is intended to mitigate selection bias during dataset selection to evaluate the performance of the ML model. Selected specimens can range from being \u201cnear identical\u201d to being completely unrelated. Determining the relatedness of the ML model\u2019s manufacturer dataset compared to the deployment laboratory is helpful to deduce the results of the performance evaluation and can infer the generalizability of the ML model.106 Distribution of the sample characteristics include preanalytic and predictor factors such as\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 11\npatient characteristics, laboratory processes, and hardware used to process the samples (eg, stainers, whole slide scanners, chemistry analyzers, genomic sequencers).\nClinical Validation.\u2014Clinical validation focuses on clinical requirements to evaluate the MLmodel in a clinical production environment. Clinical validation requires end-to-end testing including each component of the complete integrated clinical workflow, using noncurated, real-world data. Clinical validation intends to demonstrate the acceptability of the ML model being used in a clinical setting. In the case of a CDS system, clinical validation generally compares pathologist test resulting with and without access to the model outputs. Clinical validation is akin to the regulatory concept of clinical validity, or the ML concept of evaluating the performance of an external \u201cvalidation\u201d dataset. The ML concept for internal (technical) validation using k-fold cross validation or split-sample validation are out of scope of this paper as they are generally used for model selection during model development. Prior to, or as part of, the clinical validation, a controlled reader study may be performed where all components of the model are tested with pathologists in the loop. Clinical validation is used to confirm with objective evidence that a modified FDA-authorized ML model or LDT delivers reliable results for the intended application. The validating laboratory (eg, deployment site) is required to establish the performance characteristics that best captures the clinical requirements, which may include accuracy, precision, analytical sensitivity, analytical specificity, reportable range, reference interval, and any other performance characteristic deemed necessary for appropriate evaluation of the test system. Clinical validation can be performed as a clinical diagnostic cohort study where consecutive prospective patients are being evaluated in the appropriate medical domain of the intended use of the ML model (eg, satisfying eligibility criteria based on intended use/indications of use). This ensures realworld clinical data from the deployment laboratory are included in the performance characterization and the natural spectrum and prevalence of the medical condition will be observed. Establishing a \u201climit of detection\u201d for quantitative and qualitative assessment can help provide validation across appropriate clinically meaningful subgroups (eg, stratifying tumor detection by size, flow cytometry gating thresholds). The requirement to define reportable ranges as part of the test results can be exemplified for quantitative results (ie 0%\u2013100% for prediction of estrogen receptor nuclear-positive tumor cells) or specified values for output classification (ie, Gleason patterns for prostatic acinar carcinoma classification). Reportable range definitions\nshould be included in the standard operating and guide validation procedures. Defining the reportable range will facilitate analysis of evaluation metrics and resultant performance characteristics of the ML model. Clinical utility demonstration and validation can be further analyzed where performance characteristics prove the clinical or diagnostic outcome enhances patient outcomes.107\u2013112\nClinical verification and/or validation are required prior to clinical patient testing for all nonwaived tests, methods, or instrument system prior to use in patient testing.113,114 Furthermore, due to the diversity of diagnostic tests\u2014with FDA-authorized tests, modified FDA-authorized tests, or LDTs\u2014it is not possible to recommend a single experimental design for all verification and/or validation of all ML models. However, the guidance in this document attempts to provide recommendations for evaluation procedures and metrics to support verification and validation of clinical tests that use ML models."
        },
        {
            "heading": "Evaluation Metrics",
            "text": "Laboratories have routinely evaluated performance of instruments, methodologies, and moderate-complexity to high-complexity tests. A variety of performance evaluation metrics are available to laboratorians based on the appropriate model (Table 4). While these evaluation metrics will be briefly discussed here, a more in-depth explanation of these metrics was previously published.45 The ideal metrics used for evaluating the performance of an ML model are guided by the data type of the generated output (eg, continuous values of a quantitative measurement generated by a regression model versus categorical values of a qualitative evaluation generated by a classification model). For outputs on an interval or ratio scale, such as proportion (percentage) of tumor-infiltrating lymphocytes, quantitative evaluation aims to estimate systematic and proportionate bias between the test method and a reference method (eg, human reader). Qualitative evaluation applies to outputs on a nominal or categorical scale (eg, presence or absence of tumor), as well as to clinical diagnostic thresholds or cutoffs applied to interval or ratio outputs (eg, probabilistic output of a logistic regression model). The data outputs of an ML model can be either nominal or ordinal in a discriminative classification task, where the output is a specified label. In a discriminative regression task, the outputs can be either an interval or a ratio, and the output value is a number that\nAbbreviations: ANOVA, analysis of variance; AUFROC curve, area under free response receiver operating characteristic curve; AUROC curve, area under the receiver operating characteristic curve.\n12 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\ncan be a continuous variable (eg, time to recurrence) or ratio (eg, serum free light chains [k/k]). The appropriate evaluation metrics should be selected based upon the specified model intended to be clinically deployed and the variables (independent or dependent) defining the output results. For classification models, discrimination and calibration are used to evaluate the performance.\nDiscrimination is the ability of a predictive model to separate the data into respective classes (eg, with disease, without disease); calibration evaluates the closeness of agreement between the model\u2019s observed output and predicted output probability. Discrimination is typically quantified using the area under receiver operating characteristic (AUROC), or concordance statistic (C-statistic). In isolation, discrimination is inadequate to evaluate the performance of an ML model. For instance, a model may accurately discriminate the relative quantification of Ki67 nuclear stain positivity of specimen A to be \u201cdouble\u201d that of specimen B (ie, discriminatory relative quantification could be 2% and 1%, respectively); however, their true, absolute Ki67 nuclear stain positivity could be 30% and 15%. Alternatively, a model showing excellent calibration can still misrepresent the relative risks (eg, discrimination). For example, a model predicting glomerular filtration rate in 2 patients shows a 50% absolute increase in creatinine between 2 patients, but not be able to discriminate the risk of patient A\u2019s creatinine rising from 0.4 mg/dL to 0.6 mg/dL and patient B\u2019s 1.2 mg/dL to 1.8 mg/dL. This model is arguably of no use as patient A is within normal limits, and patient B should be evaluated for being at risk to develop acute kidney injury. This model would be considered to have poor discrimination.\nCalibration measures how similar the predicted outputs are to the true, observed (eg, absolute), output classes. It is intended to fine tune the model performance based on the probability of the actual output and the expected output. For a given set of input data, deployment site calibration can be used to change the model parameters and optimize the model output data to match the reference standard more closely. While calibration is used for performance evaluation, guidance on how to perform local site calibration or retraining of the entire ML model is out of the scope for this proposed framework as the comparative performance of the calibrated model may deviate from the outputs of the original model. However, as a performance metric, calibration should be analyzed for the final iterated model intended to be clinically deployed. Calibration documentation of performance should be assessed as part of the verification and/or validation prior to clinical deployment. If the ML model is to be updated, using local site calibration, the tuning of hyperparameters of the ML model should be reflected as a new version of the model, and should consider the final trained model as the model intended for clinical deployment. Calibration is evaluated as a graphical representation called a reliability diagram. Reliability diagrams compare the model\u2019s observed predictions to the expected reference standard. Calibration can also be evaluated for logistic regression models using the Pearson v2 statistic, or the Hosmer-Lemeshow test.\nPerformance evaluation of ML models may show good discrimination and poor calibration, or vice versa. A performant ML model should show high levels of discrimination and calibration across all classes. It is also possible that the model shows poor calibration amongst a subset of classes (eg, shows high calibration for Gleason pattern 3, but poor\ncalibration for Gleason pattern 5). Calibration, or goodness of fit, has been termed the Achilles heel of predictive modeling: while critically important in evaluation, it is commonly overlooked or misunderstood, potentially leading to poor model performance.115 Poorly calibrated models can either underestimate or overestimate the intended outcome. If model calibration is poor, it may render the CDS tool ineffective or clinically harmful.115\u2013117 The deployment site laboratory should not report clinical (patient) samples until performance characterization has been completed; ML model updating should be considered in case of poor discrimination or calibration, including retraining of the model with sufficient data, if appropriate. While it is the responsibility of the ML model developer to ensure appropriate discrimination and calibration, laboratories may perform local site calibration, model updating, and subsequent verification and/or validation to ensure patient testing is accurate.118,119\nMost literature supports analyzing accuracy as a performance metric to show the ratio between the number of correctly classified samples and the overall number of samples. When datasets are imbalanced (eg, increased number of samples in one class compared to the other classes), accuracy as defined above may not be considered the most reliable measure, as it can overestimate the performance of the classifier\u2019s ability to discriminate between the majority class and the less prevalent class (eg, minority class). A prime example is prostate adenocarcinoma grading in surgical pathology. Gleason pattern 5 is less prevalent than Gleason pattern 3 and may be less represented in the verification and/or validation cases. Gleason pattern 5, while less common, has significant prognostic implications for patients, and a model intended to grade prostate adenocarcinoma can appear to have excellent performance if the validation cases are imbalanced and do not include samples representing the higher Gleason pattern.120,121 For highly imbalanced datasets, where one class is disproportionately underrepresented, maximizing the accuracy of a prediction tool favors optimization of specificity over sensitivity; where false positives are minimized at the expense of false negatives (accuracy paradox). This has led some investigators to recommend use of alternative metrics. The index of balanced accuracy (IBA), a performance metric for skewed class distributions that favors classifiers with better results for the positive (eg, tumor present), and generally most important class. IBA represents a trade-off between a global performance measure and an index that reflects how balanced the individual accuracies are: high values of the IBA are obtained when the accuracies of both classes are high and balanced.122 Sampling techniques for verification dataset curation can be considered where oversampling of the minority class or undersampling of the majority class can provide a better distribution for evaluation. While the Cohen kappa, originally developed to test interrater reliability, has been used for assessing classifier accuracy in multiclass classification, there are some notable limitations that limit its utility as a performance metric, especially when there is class imbalance.123,124 The Matthews correlation coefficient, introduced in the binary setting and later generalized to the multiclass case, is now recognized as a reference performance measure, especially for unbalanced datasets.125\u2013127 Designing an appropriate validation study should include a diverse and inclusive representation of real-world data that the model could be exposed to at the deployment laboratory. It is the responsibility of the laboratory to ensure sufficient samples\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 13\nare included for verification and/or validation of a given model and that the appropriate evaluation metrics are used.\nEvaluation Metrics for Imaging\nIn evaluating models for ML-based CDS software for computer-assisted diagnosis, free-response receiver operating characteristic curves are an appropriate evaluation metric. The model output is deemed correct when the detection and localization of the appropriate label is accurate. For instance, a model intended to detect parasites in peripheral blood smear slides can miss a region that indeed has red blood cells with Plasmodium falciparum, and separately label a different region of normal red blood cells incorrectly as being positive for parasitic infection\u2014the model has produced both a false positive and a false negative. If the validating laboratory considered only the slide label output, it would appear to show as if the slide-level output of the model identified the slide as having a parasitic infection (eg, true positive), however in reality the model should be penalized for the inaccuracies. Model evaluation should allow multiple false negative or false positive labels in a given sample.51,128 Another evaluation metric for ML models that perform image segmentation is the Dice similarity coefficient.129 Segmentation is the pixel-wise classification in an image of a given label or class. For instance, if a model is intended to identify and segment mitoses in an image, given a verification dataset that has an expert pathologist\u2019s manual annotation of mitoses, the model\u2019s performance can be evaluated by measuring the intersection over union (eg, degree of overlap) between the model segmentation output and the human expert manual annotation.130\nThe ideal metrics used for evaluating the performance of an ML model are guided by the data type of the generated output (eg, continuous values of a quantitative measurement generated by a regression model versus categorical values of a qualitative evaluation generated by a classification model). For outputs on an interval or ratio scale, such as proportion (percentage) of tumor-infiltrating lymphocytes, quantitative evaluation aims to estimate systematic and proportionate bias between the test method and a reference method (eg, human reader). Qualitative evaluation applies to outputs on a nominal or categorical scale (eg, presence or absence of tumor), as well as to clinical diagnostic thresholds or cutoffs applied to interval or ratio outputs (eg, probabilistic output of a logistic regression model). The data outputs of an ML\nmodel can be either nominal or ordinal in a discriminative classification task, where the output is a specified label. In a discriminative regression task, the outputs can be either an interval or a ratio, and the output value is a number that can be a continuous variable (eg, time to recurrence) or ratio (eg, serum free light chains [j/k])."
        },
        {
            "heading": "Classification Test Statistics",
            "text": "To evaluate ML models with classification outputs, arguably the best-known evaluation metric is the confusion matrix for binary classification outputs (disease versus no disease) where sensitivity, specificity, accuracy, positive predictive value, negative predictive value, and receiver operating characteristic (ROC) curve calculations can be computed (Figure 3). While for ML-based classification models the final outputs may be represented as a binary output (eg, tumor versus no tumor), a preceding continuous variable (eg, probability range from 0 to 1) with cut-point threshold is applied to convert it into a binary result. The sensitivity and specificity of the model vary depending on where the operating point is set (eg, higher threshold \u00bc decreased sensitivity and increased specificity; lower threshold \u00bc increased sensitivity and decreased specificity). Sensitivity (also referred to as \u201ctrue positive rate\u201d or \u201crecall\u201d) is the ratio of true positive predictions to all positive cases and represents a method\u2019s ability to correctly identify positive cases. Specificity is the ratio of true negative predictions to all negative cases and represents a method\u2019s ability to correctly identify negative cases. Positive predictive value and negative predictive value are directly related to prevalence. Positive predictive value (also referred to as precision) is the probability that, following a positive test result, an individual will truly have the disease or condition, and is the ratio of true positive predictions to all positive predictions. Negative predictive value is the probability that, following a negative test result, an individual will truly not have the disease/condition, and is the ratio of true negative predictions to all negative predictions. The overall or naive accuracy represents the proportion of all cases that were correctly identified. ROC curves are graphical representations plotting the true positive rate (sensitivity) and false positive rate (1 specificity) at different classification thresholds. The ROC curve is plotted to illustrate the diagnostic ability of a binary classifier system as its discrimination threshold or operating point is varied. The AUROC curve is a single scalar measure of the overall performance of\n14 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\na binary classifier across all possible thresholds, ranging from 0 (no accuracy) to 1 (perfect accuracy). A model with an AUROC curve of 1 (perfect accuracy) does not necessarily portend high performance of the ML model in clinical practice; the AUROC curve weights sensitivity and specificity equally. If a specific intended use of a model is for screening, this may have a lower AUROC value and still maintain high sensitivity despite having lower specificity. Given that a certain operating point is needed when iterating and versioning an ML model, the sensitivity and specificity values for the selected operating-point threshold should be considered when measuring the model\u2019s performance. The F-score (eg F1-score) is a combined single metric (eg, harmonic mean) of the sensitivity (eg, recall) and positive predictive value (eg, precision) of a model. For multiclass or multilabel classification problems, pairwise AUROC analyses are needed since the single AUROC curves are intended for evaluating binary outputs. For multiclass instances, one method allows an arbitrary class to be compared against the collective combination of all other classes at the same time. For instance, one class is considered the positive class, and all other classes are considered in the negative class. This enables a \u201cbinary\u201d classification using AUROC by using the \u201cone versus rest\u201d technique. Another evaluation metric for multiclass models termed \u201cone versus one\u201d, analyzes the discrimination of each individual class against each other, with all permutations of individual classes compared against each other. In a 3-class output (eg, tumor grading of low, moderate, and high), there would be 6 separate one-versus-one comparisons. In both one versus rest and one versus one, the average AUC for each class would result in the final model accuracy performance. Generally accepted values of AUROC less than 0.6 represent poor discrimination, where 0.5 is due to chance (eg random assignment). Discrimination AUROC of 0.6 to 0.75 can be classified as helpful discrimination, and values greater than 0.75 as clearly useful discrimination.131,132\nPositive percent agreement and negative percent agreement are the preferred terms for sensitivity and specificity, respectively, when a nonreference standard\u2014such as subjective expert assessment\u2014is used to determine the \u201cground truth.\u201d In other words, this approach measures how many positives/negatives a test identifies that are in agreement with another method or rater used on the same samples.133\nStandalone verification performance of the model should undergo analysis of discordant cases (false negatives, false positives) to ensure the model outputs are checked against the reference standard. Discordance analysis should be performed for all false negatives and all false positives. Additionally, a minimum sufficient number of true positives and true negatives should also be evaluated to mitigate evidence of selection bias. For clustering models, these include evaluating the longest centroid-dataset pairs, and a sufficient number of centrally clustered data points. Discordance analysis enables adjudication of false negatives and false positives that may be labeled inappropriately during the ground truth process, or where true positives predicted in the positive class are, in fact, false positives. The discordance analysis will result in a final true positive, true negative, false positive, and false negative rate for verification and/ or validation documentation. Discordance analysis should be performed, documented, and available for review.\nSample Size Estimations for Binary Classification\nThe size of the accuracy evaluation dataset is determined by the scope of the performance evaluation (eg, validation\nversus verification). The sample size calculation for verification of the accuracy (or sensitivity or specificity) of a method relies on knowledge of the predetermined accuracy (or sensitivity or specificity) from the package insert (denoted as P0). In comparing the method\u2019s accuracy to P0, the null and alternative hypotheses are represented by the following:\nH0: Accuracy\u00bc P0 versus H1: Accuracy, P0 \u00f0orAccuracy\u00bc P1\u00de\nwhere P1 is the value of accuracy (or sensitivity or specificity) under the alternative hypothesis. A power-based sample size formula for comparison of a proportion to a fixed value can be applied for evaluation of a single diagnostic method using the normal approximation of the binomial distribution. With (1 a)% confidence level and (1 b)% power for detecting an effect of P1 \u2013 P0, the required sample size for cases is obtained from the following:\nn\u00bc Z1 a3\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P0 1 P0\u00f0 \u00de p \u00fe Z1 b 3 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P1 1 P1\u00f0 \u00de\nph i2\nP1 P0\u00f0 \u00de2 (1)\nFor example, if the laboratory wishes to compare the locally determined accuracy of a software or algorithm to the vendor\u2019s claim of 95%, the sample size required to have 95% confidence and 80% power to detect a difference of 5% or more lower than the claimed accuracy of 95% would be calculated as follows:\nn\u00bc 1:6453\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 0:95 1 0:95\u00f0 \u00de p \u00fe 0:843 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 0:900 1 0:900\u00f0 \u00de\nph i2\n0:900 0:95\u00f0 \u00de2\n\u00bc 150 (2)\nSince the laboratory is solely interested in knowing whether the locally determined accuracy is significantly lower than the manufacturer\u2019s claimed accuracy, a one-sided Za score of 1.645 is used instead of the 2-sided Za/2 of 1.96 at an a level of 0.05, and Zb \u00bc 0.84 at b \u00bc 0.20. Due to the properties of the binomial distribution, as the accuracy claim decreases, the sample size increases for a given tolerance level or deviation from the claimed accuracy.\nAlthough sample size calculations for accuracy do not depend on the prevalence of the entity of interest in the population sampled, sample sizes for verifying sensitivity and specificity do. Sample sizes for sensitivity and specificity can also be derived using Equation 1; for sensitivity, the calculated estimate represents the total number of positive samples and for specificity, the calculated estimate represents the total number of negative samples. For example, using Equation 2 above and targeting a prevalence of 50% in the accuracy evaluation cohort, one would require at least 75 positive samples and at least 75 negative samples to have 95% confidence and 80% power to detect a difference of 7.5% or more lower than claimed sensitivity and specificity of 97.5%: that is, 94 cases in total. An imbalanced sample with prevalence of 20% (20% positive, 80% negative) would require at least 92 positive cases and a total sample size of 460 to verify the sensitivity claim; the sample size for verification of the specificity claim would be 115 to ensure at least 92 negative cases.\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 15\nAssuming an a level of 0.05 and b \u00bc 0.20, a sample size of at least 150 (75 positive and 75 negative cases) should be considered sufficient for verification of binary classification tasks, when the accuracy, sensitivity, and/or specificity claim is at least 95% or higher.\nWhen establishing an accuracy claim (P) for a classification model, a precision-based sample size calculation is applicable using a 2-sided Za score of 1.96 is recommended (a \u00bc 0.05) and a prespecified confidence interval width (e) for the accuracy metric:\nn\u00bc Z2a=2 3 P3 1 P\u00f0 \u00de\ne2 (3)\nFor example, during development, a binary classification model for prostate cancer identification in hematoxylineosin whole slide images demonstrated an accuracy of 94.2%. If the laboratory wishes to establish an accuracy claim for the model of 95% 6 2.5%, then the required sample size would be:\nn\u00bc 1:96 2 3 0:953 1 0:95\u00f0 \u00de\n0:0252 \u00bc 292 (4)\nIt should be noted that these are na\u0131\u0308ve assumptions and the actual number needed may depend on additional considerations for determining the estimated sample size for establishing diagnostic accuracy. These include, but are not limited to, study design (single reader versus multiple readers), the number of diagnostic methods being evaluated (single versus 2 or more), how cases will be identified (retrospective versus prospective), the estimated prevalence of the diagnosis of interest in the population being sampled (prospective design) or the actual prevalence in the study cohort (retrospective design), the summary measure of accuracy that will be employed, the conjectured accuracy of the diagnostic method(s), and regulatory clearance of the model.\nRegression Model Test Statistics\nFor regression models, the outputs are measured on an interval or ratio scale rather than categorically. The purpose of the quantitative accuracy study is to determine whether there\nare systematic differences between the test method and reference method. This type of evaluation typically uses a form of regression analysis to estimate the slope and intercept with their associated error(s), and the coefficient of determination (R2), where applicable. Regression analysis is a technique that can measure the relation between 2 or more variables with the goal of estimating the value of 1 variable as a function of 1 or more other variables. Data analysis proceeds in a stepwise fashion, first using exploratory data analysis techniques, such as scatterplots, histograms, and difference plots, to visually identify any systematic errors or outliers.134 Specific tests for outliers such as a model-based, distance-based test, or influence-based test may also be performed.135\nThe purpose of the method comparison study for quantitative methods is to estimate constant and/or proportional systematic errors. There are 2 recognized approaches for method comparison of quantitative methods: Bland-Altman analysis and regression analysis. In Bland-Altman analysis, the absolute or relative difference between the test and comparator methods is plotted on the y-axis versus the average of the results by the 2 methods on the x-axis.136,137 Since the true value of a sample is unknown in many cases, except when a gold standard or reference method exists, using the average of the test and comparator results as the estimate of the true value is usually recommended. While visual assessment of the difference plot on its own does not provide sufficient information about the systematic error of the test method, t test statistics can be utilized to make a quantitative estimate of systematic error; however, this bias estimate is reliable only at the mean of the data if there is any proportional error present.138\u2013140\nRegression techniques are now generally preferred over t test statistics for calculating the systematic error at any decision level, as well as getting estimates of the proportional and constant components. However, it is important to note that the slope and intercept estimates derived from regression can be affected by lack of linearity, presence of outliers, and a narrow range of test results; choosing the right regression method and ensuring that basic assumptions are met remain paramount. If there is a constant standard deviation across the measuring interval, ordinary least squares (OLS) regression and/or constant standard deviation Deming regression can be used. If instead, the data exhibit proportional difference variability, then the assumptions for either OLS or\n16 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nconstant standard deviation Deming regression are not met and instead, a constant coefficient of variation Deming regression should be performed. If there is mixed variability (standard deviation and coefficient of variation), then Passing-Bablok regression, a robust nonparametric method that is insensitive to the error distribution and data outliers, should be used. An assumption in OLS regression is that the reference method values are measured without error and that any difference between reference method and test method values is assignable to error in the test method; this assumption is seldom valid for clinical laboratory data unless a defined reference method or standard exists and, therefore, OLS regression is not appropriate in most cases for performance evaluation of quantitative outputs. In Deming regression, the errors between methods are assigned to both reference and test methods in proportion to the variances of the methods141 (Figure 4, A through C). Two special situations arise when the measurand is either a count variable (eg, inpatient length of stay in days) or a percentage bounded by the open interval (0, 1). For the former, Poisson or negative binomial regression techniques should be used depending on the dispersion of the data and for the latter, b regression techniques should be considered. Statistical techniques are also available for evaluation of models comparing ordinal to continuous variables, such as ordinal regression, but these are beyond the scope of this paper.\nPrecision: Repeatability and Reproducibility\nPrecision refers to how closely test results obtained by replicate measurements on the same or similar conditions agree.142 Precision studies for ML models can be conducted similarly to test methodology evaluations in clinical laboratories; approaches for precision evaluation have been extensively described by the Clinical and Laboratory Standards Institute (CLSI). Repeatability reflects the variability among replicate measurements of a sample under experimental conditions held as constant as possible, that is, using the same operator(s), same instrument, same operating conditions, and same location over a short period of time (eg, typically considered within a day or a single run). Within-laboratory precision (an \u201cintermediate\u201d precision type) incorporates run-to-run and day-to-day sources of variation using a single instrument; these precision types may also include operator-to-operator variability and calibration cycleto-cycle variability for procedures that require frequent calibration. As such, repeatability and within-laboratory precision are evaluated via a single-site study. Sometimes confused with \u201cwithin-laboratory precision,\u201d reproducibility (or betweenlaboratory reproducibility) refers to the precision between the results obtained at different laboratories, such as multiple laboratories operating under the same CLIA license. Reproducibility, while not always needed for single-lab precision evaluation, may be beneficial when a model is being deployed at more than one site.\nEvaluation of repeatability and within-laboratory precision and reproducibility can be conducted as separate experiments (\u201csimple precision\u201d design). For the simple repeatability study, samples are tested in multiple replicates within a single day by 1 operator and 1 instrument under controlled conditions. For simple within-laboratory precision studies, the same samples are run once a day for several days within the same site, varying the instrument and/or operator as appropriate and per standard operating procedure. In a complex precision design, a single experiment is performed with replicate measurements for several days and across several instruments or operators,\nwhere applicable. Detailed statistical techniques for analyzing complex precision studies are described elsewhere (for quantitative studies using nested repeated-measured ANOVA [regression outputs], see EP05\u2013Evaluation of Precision of Quantitative Measurement Procedures; for qualitative studies [classification outputs], see ISO 16140).143,144 These techniques provide estimates for repeatability and between-run and between-day precision, as well as within-laboratory precision or reproducibility depending on the study design. They allow a laboratory to rigorously establish the precision profile of a modified FDA-approved or laboratory-developed model or verify the precision claims of a manufacturer for an unmodified FDA-approved model.\nFor an unmodified FDA-authorized model, the objective is to verify the manufacturer\u2019s precision claim, such as the proportion of correct replicates and/or the proportion of samples that have 100% concordance across replicates or days in the manufacturer\u2019s precision experiment. For example, in a repeatability experiment, the manufacturer processes 1 whole slide image each from 35 tumor biopsies and 36 benign biopsies in 3 replicates using 1 scanner or operator in the same analytical run. For biopsies containing tumor, 99.0% (104 of 105) (95% CI: 94.8%\u201399.8%) of all scans and 97.1% (34 of 35) of all slides produced correct results, while for benign biopsies, 94.4% (102 of 108) (95% CI: 88.4%\u201397.4%) of all scans and 88.9% (32 of 36) of all slides produced correct results. An acceptable number of replicates for the repeatability study can be derived from Equation 1, but 60 replicates per model class should be sufficient in most cases allowing for a 10% tolerance below the manufacturer\u2019s stated precision claim. Therefore, a user can verify the manufacturer\u2019s stated repeatability claim by analyzing a set of slides from tumor biopsies and benign biopsies (n \u00bc 10 each) obtained at their local institution in replicates (n \u00bc 6) using 1 scanner or operator in the same analytical run.\nIf the observed precision profile meets or exceeds the manufacturer\u2019s claims, the precision study can be accepted. For precision studies where most of the ML model outputs pass, but some fail, the medical director should review the study design and results to evaluate potential confounding causes of the imprecision. Failure to meet the stated claims does not mean that the observed precision is necessarily worse than the manufacturer\u2019s claim, but it should require a 1-sample or 2-sample test of proportions to compare the observed precision profile to that described by the manufacturer. Alternatively, the laboratory may choose to qualitatively evaluate the precision profile of the model. The precision study may still be accepted if the laboratory director deems it appropriate; written justification and rationale for accepting the precision performance will be needed. Precision study results where all samples show 100% concordance across replicates, or where no more than 1 sample shows discordance, should be acceptable for most use cases. When discordance across replicates, runs, days, instruments, or operators is observed, the medical director should review the study design and results to evaluate confounding variables as a potential cause of the imprecision and may decide to repeat samples or runs, or the entire precision study if appropriate. In this scenario, close ongoing monitoring of the ML model is advised. For precision studies where many results failed or showed a high level of imprecision, the deployment site may reject the ML model for use as a clinical test, troubleshoot\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 17\nlaboratory influences affecting the output results, or design a new study to better evaluate precision of the model.144,145\nCHANGE CONTROL AND CHANGE MANAGEMENT\nChange control for ML-based testing in the laboratory is defined as the process of how changes to requirements for laboratory processes are managed. Change management is closely related and defined as the process for adjusting to changes for a new implementation. The goal of change management is to control the life cycle of any modifications. It is the fourth pillar of process management as defined by the CLSI, which describes the pillars as (1) analyzing and documenting laboratory processes, (2) verifying or validating a laboratory process, (3) implementing controls for monitoring process performance, and (4) change control and change\nmanagement to approved laboratory processes.146 Change control encompasses alterations to preanalytical, analytical, and postanalytical variables. In the context of ML models in pathology, most alterations can be divided into changes in physical components (eg, reagents such as stains, specimen collection and transportation parameters, fixation times, specimen types, or instruments such as whole slide scanners, nucleic acid sequencers, chemistry and coagulation analyzers) and digital dependencies (eg, data inputs, algorithm code, file formats, resolution, software interoperability). Laboratory management should have a structured process for assessing changes in laboratory processes that involve or impact ML models deployed in the laboratory. Change management can become necessary for a variety of reasons including identification of a problem in the quality control process, an external or internal need to update instruments, equipment, or reagents, or to respond to the changing needs of the end user (Table 5).\nThe purpose of creating a change management system is to ensure that changes to the system do not adversely affect the safety and quality of laboratory testing. For laboratory tests that employ ML systems, the ML model performance has a critical impact on the overall test performance and patient safety. Any changes to a previously verified and validated model should require performance characterization of the updated model. In addition to evaluating how changes may affect clinical functionality or performance, model changes can also be interpreted through the lens of risk assessment, with proposed changes stratified by risk. Modifications should be evaluated with particular attention to the potential for changes that may introduce a new risk or modify an existing risk that could result in significant harm, with controls in place to prevent harm. In addition, changes to the model or processes that the model is dependent on need to be transparent and communicated to all personnel and stakeholders. A successful change management strategy should reduce the failed changes and disruptions to service, meet regulatory requirements through auditable evidence of change management, and identify opportunities for improvement in the change process. Documentation for change control depends on the type of change, the scope and anticipated impact of the change, and the risk associated with the change. The documentation and verification should be completed before the change is clinically implemented (examples of documentation to support the change management process are listed in Table 6. CLIA regulations for proficiency testing require periodic reevaluation (eg, twice annually), even if there are no changes, to verify tests or assess accuracy.\nOne of the main challenges with ML-based systems is that they can have many dependencies that may not be readily apparent to everyone in the organization. It is recommended to identify all possible dependencies in the\nTable 6. Examples of Documentation That Should Be Prepared Prior to Implementation of a Change\nPrevious verification or validation report(s) including discordance analysis\nRisk analysis\nProvision of a change schedule\nSummarized laboratory test data (eg, inputs and results)\nRevised verification or validation report, including discordance analysis\nStandard operating procedures (revised as necessary) to include clinical utility, hardware, software, versions\nEvidence of personnel training and competency\n18 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\ninitial implementation and to update this documentation when process changes are made. Having the process team participate in the preparation and proposed modifications of a flow diagram can help ensure inclusion of the steps in the upstream and downstream workflows, especially when data inputs or processes are supplied by different personnel or different divisions in a laboratory. Implementation of ML-based CDS systems with dependencies across the laboratory will mean that the wider laboratory (eg, the histology laboratory, molecular pathology laboratory, chemistry laboratory, etc) should consider whether a new or revised process will affect any other processes including any deployed ML-based CDS systems. If so, a timeline needs to be established for the wider systemic review and revision of the associated process flows. Team members from each of the impacted processes for a new or changed process should be included and verify the updated flow chart. Once all documents associated with the revised process are prepared, validation and/or verification is needed to determine the performance characteristics of the revised test. With any revision of the model, the model version should be documented for auditing and tracking of performance. Verification is also performed when a laboratory intends to use a validated process in a similar manner for a different application. For example, consider a scenario where a laboratory has a validated ML-based CDS system on a specific whole slide scanning device for primary diagnosis. The laboratory now intends to install a new whole slide scanner from a different vendor and wants to use the same ML model with the new scanners. Reassessing the performance characteristics of the ML-based system is recommended after any model parameters have been changed to iterate a priori and assess the performance as the final model would be clinically deployed. Recent draft guidance from the FDA entitled, \u201cMarketing Submission Recommendations for a Predetermined Change Control Plan for Artificial Intelligence/Machine Learning (AI/ML)-Enabled Device Software Functions\u201d speaks to these concepts.147 Ideally, predetermined change control plans should delineate manufacturer versus local medical director responsibilities relative to validation and verification. For the medical director, the predetermined change control plan should include both the verification process and a set of target verification metrics that have a clear relationship to clinical performance and are tailored to what is practical for local sites including data volume and number of cases.\nDeployment Site Ongoing Monitoring\nAfter a model has been duly verified and/or validated and appropriate instruction for use (eg, standard operating procedure) is in place, it is the responsibility of the laboratory to ensure the model performance is monitored for stability and reliability over time. Changes in the input data may result in an alteration of the model performance. The time it takes for the model to deteriorate in performance is determined by factors pertaining to the upstream and downstream processes or data that correspond to the model. Model performance may shift (eg, sudden loss of performance), drift (eg, gradual loss of performance), or show cyclical or recurring change depending on these influences. Shift may be obvious to the laboratory and can be exemplified by an update in the laboratory information system where the units of a specific input value have changed, and the ML model using that numerical data for predictions\nis now significantly different and shows inferior performance. In this scenario, the results of any test including an ML system with shift should not be reported until the updated system undergoes verification as part of change management with updating laboratory documentation as necessary. Certain changes causing shift or drift may require retraining or calibration of a new ML model and subsequent verification and/or validation of the updated model. Drift can be more challenging to identify, especially for very small incremental changes over time; it usually refers to changes in the data distribution that the model was originally trained with and validated for. Drift may occur due to changes in patient demographics, updated practice patterns, or new workflows105,112,148\u2013150 The loss in performance may show a loss of discrimination or calibration; however, depending on the change of the data, there may be loss of calibration without a significant change in discrimination, or vice versa.151\u2013153 Changes in healthcare can introduce transformed patient populations, specimen collection, data inputs, and clinical workflows.154\u2013160 Vigilance in laboratory testing and performance monitoring is crucial to patient safety, medical decision making, and good laboratory practice. When the performance of a verified and/or validated model has deteriorated (eg, shift, drift), clinical testing using the ML system should be halted until the performance characterizations have been remediated. Performance monitoring techniques such as calibration drift detection have been suggested and will likely play an important role for ongoing monitoring of clinically deployed ML models.161 Additional ongoing monitoring of model stability includes recognition of adversarial attacks on ML models, predominantly for image classification.162 These attacks are adversarial images (eg, perturbations, skew vectors) that can be applied to the medical image that are indiscernible to the human eye, but may cause drastic changes in model performance.163 With appropriately diverse training data, decreasing the disparity between the development and target data, as well as the model architecture, has been shown to minimize the effect of adversarial attacks on the ML model performance.164,165"
        },
        {
            "heading": "HUMAN-AI (COMPUTER) INTERACTION",
            "text": "Often the primary method for evaluating the performance of ML-based software is a standalone or in silico evaluation of the ML-based CDS system\u2019s output compared to a reference standard (eg, ground truth) on a series of samples that are not part of the system\u2019s training or tuning set (ie, on which the model was not trained). As described above, these evaluations can provide useful overall statistical metrics (ie, sensitivity, specificity, positive predictive value, correlation, and mean square error analysis). Validation performance evaluations educate the user about the augmented workflow of the pathologist\u2019s use of the ML model during clinical routine workflow. While analytical validity is critical to establishing benchmark model performances, as augmented intelligence, a human interpreter (eg, pathologist) is still required to finalize and authenticate a patient\u2019s report. Standalone (eg, in silico) evaluations fall short of capturing how the human ingests, interprets, and incorporates the results of the ML model when evaluating a sample. As there are disruptive clinical distractions, the ML-based CDS system may also have disruptive, time-consuming visualizations, workflows, or false results that may undermine the mental demand of such systems on users in the medical\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 19\ndomain. The effect of this on patient care has been shown to be significant in several studies.166,167 Moreover, user certainty (eg, digital confidence), uncertainty (eg, digital hesitancy) and comfort with the interface may likewise have a profound effect on issuing a timely and accurate result.168\u2013170 Implicit in this evaluation is the impact of the stress-related tasks related to the human-computer interaction.171 Presentation interaction complexity, display features, explainability of the model outputs, and information density may influence user-experienced stress and, hence, also reliability and adoption.\nStudies have examined the human-computer interaction, capturing various metrics, such as improvement in accuracy, patient outcomes, and productivity. In surgical pathology, this has been most studied for ML-based CDS systems related to prostate cancer diagnosis (detection, grading, or assessment of perineural invasion). The use of these systems has resulted in changing various aspects for the diagnostic interpretation, although few studies assess CDS utility in a setting that mimics diagnostic practice.52,118,164 Generally, improvements are seen with the use of these highly accurate CDS systems. Optimization of the human-computer interaction is essential to ensuring that the user and the ML model show optimal performance when used together and exceed the performance than either the ML model or user alone. Optimal performance of the human-AI interface can be evaluated in terms of accuracy and productivity. Central to this optimization are themes including trust, understanding the model, explainability, and usability (eg, user interface, visualizations).\nTrust\nRigorous evaluation of ML-based systems is needed to build trust or provide evidence not to trust the evaluated system. Regardless, ML systems can be met by skepticism from users.172 Distrust may stem from a lack of understanding, feeling threatened by the technology, or perhaps feeling that automation may negatively impact quality of care. In contrast, excess trust in CDS systems can lead to overreliance on the technology and user complacency. User complacency might result in a convergence of the human\u2019s performance characteristics with that of the ML system. This consequence is arguably desirable if an ML model outperforms the human for a given task; however, it is rightfully worrisome until proven otherwise. Instead, an approach of vigilance is recommended to support assistive workflows. The user (eg, pathologist) can correct or adjust the model output if incorrect or accept the output if deemed correct. For a model with high performance, this supervision may result in the most optimal result for the patient in that the combined benefits of human and machine will be incorporated into the final diagnostic assessment. Trust and, ultimately, confidence will result from a high-performance system (eg, minimal errors). The key to achieving this level of human vigilance relies on the user understanding the strengths and weaknesses of the system.\nUnderstanding the Model\nBoth diagnosticians and the CDS systems designed to aid them are imperfect. Thoughtfully designed standalone studies of ML models, with involvement of the diagnosticians intended to use them, can provide invaluable information as to how human weaknesses or strengths can be informed by the tool. For example, an ML model designed to detect invasive prostatic acinar carcinoma might show excellent standalone overall performance, but when tested against mimics of\ncarcinoma, mimics of benign prostatic tissue, or other diagnostically challenging scenarios, the standalone performance might deteriorate. These situations may be less frequent, however represent the true diversity of conditions in which a human pathologist could mitigate potential error introduced by the ML model. Outside of biologically relevant situations that pose a challenge for a pathologist, the tool should be evaluated in scenarios that expose possible bias or errors, such as in a variety of ages, races, and ethnicities, and on a sample set with a range of quality.173,174 Additionally, the model developer may have selected an operating point to support a screening task workflow, and thus the output predictions may have a perceived high false positive rate. Knowing the model\u2019s operating point and intended use can allow the user to further understand the strengths and weaknesses of the ML system."
        },
        {
            "heading": "Explainability",
            "text": "The complexity of an ML model can impact the ability of humans to understand how it works and hinder the ability to predict the conditions in which the model may fail (eg, vulnerabilities to preanalytical variables or artifacts). The complexity of the model should be considered in terms of the number of parameters of the model and the extent to which nonlinear relationships are being modeled. For models that have highly nonlinear or multivariate properties, it can be challenging to relate the model\u2019s behavior to the corresponding attributes that humans have studied in the medical domain. Explainability for ML systems is critical to the safety, approval, and acceptance of CDS systems.\nMany ML models learn low-level features and the interpretability of those models using high-level interrogative processes can provide insights that humans can understand regarding how the model output was produced. Model transparency does not necessarily mean knowing the data or what features were used to train the model, but rather visibility into the model itself and relationships between the model and the outputs. ML systems may be considered a \u201cblack box\u201d where the model\u2019s internal workings are not directly interpretable and cannot be communicated to the pathologist in an understandable way. For example, a model that uses a deep convolutional neural network to estimate percentage of nuclear immunohistochemical DAB stain positivity may be too complex for the pathologist to thoroughly understand the mechanism by which it predicts an image to be \u201cpositive\u201d but such a model may also perform better than an image analysis-based tool that explicitly segments cells and measures pixel intensity on a cell-bycell basis. Failures of the image analysis software due to poor staining or crush artifacts can be more easily understood and remediated, whereas failures of the deep network may have a less obvious solution. Explainability for ML models include (1) returning exemplars to the pathologist to evaluate visually (eg, saliency map) or (2) quantifying image attributes (eg, number of cells in an image patch) and mathematically relating such measurements to the output of the model. For fair interpretability, Shapley values, adapted from coalition game theory, provide insights to the contribution of a feature compared to the difference between the actual prediction and the average prediction, given all features learned in the model. Newer tools to estimate Shapley values, such as Shapley additive explanation (SHAP) values are model agnostic (eg, classification and regression), openly supported, and easily interrogate an existing pretrained model. SHAP is a\n20 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\ntool that can provide local accuracy (eg, differentiating between the expected ML model output and the output of a given instance), \u201cmissingness\u201d (eg, missing features can be attributed), and consistency (eg, if an ML model changes and the weight of a given feature is modified, it should be directly proportional to the corresponding Shapley value). Shapley values can provide meaningful information showing the effects of the ML model output for the prediction of that instance175,176 (Figure 5, A and B). Other approaches for model explainability such as individual conditional expectation, local interpretable model-agnostic explanations, testing with concept activation vectors, or counterfactual explanations offering varying fidelity based on the use case.177\u2013179\nThere is also controversy surrounding the definitive need to make ML models humanly interpretable, as it is conceivable that some image features utilized by the model are intrinsically not interpretable by humans, and if they were, there would not be a need to use complex models in the first place. These analyses can be useful companions to ML performance metrics as they can help a pathologist surmise how the model works. However, these methods are not without limitations. Providing examples to the pathologist relies on a qualitative human assessment that makes assumptions from observed correlations but does not establish causality. Further work in explainable AI is needed to help bridge the gap between human interpretation and model behavior.43,180,181 In addition, user interfaces and visualization methods designed to present the results of such analyses to pathologists in an intuitive fashion must continue to be developed.\nUser Interface and Model Output Visualization\nMost end users will not directly interact with the ML model in isolation, but rather with a graphical user interface or output visualization. An intuitive user interface is integral to evaluate an ML model output effectively and efficiently. Interface design is critical to minimize improper use of the CDS system.182 Careful consideration of user-required inputs and methods ensures both ergonomic and psychometric appropriateness. User interface and data presentation are also crucial components to efficient evaluation of an ML model outputs. For instance, for detection of low-grade squamous\nintraepithelial lesion in a liquid-based cytology specimen it may be more helpful to have a gallery of suspicious cells for ease of review than a binary slide-level label that does not provide the user with pixel-wise awareness of where the cells of interest are within the image. Similarly, tumor purity quantification of a molecular extraction specimen could provide numerical counts for each cell class, or highlight (eg, segment) the cells of each class for the user to view and assess. There are various visualizations to direct the user\u2019s attention to specific representations of the model outputs (eg, arrows, crosshairs, heat maps). Additional studies reviewing user interface/user experience in using such tools are important to distinguish user perception from design principles. For instance, a large blinking red arrow around a region of interest may influence users to overcall a diagnosis compared to a static neutral-colored outline. The visualization type, if applicable, should be documented in the operating procedures. Not all model outputs will rely on specified visualization tools; clinical verification and/or validation should document whether human-AI interaction will be required to interpret the results. Ideally, the ML developer will have studied the tool in a setting that examines the human-AI interaction, and through user-centered design principles, provide the most optimal user interface and visualization to support the use case of the model (Figure 6, A through F)."
        },
        {
            "heading": "PERSONNEL TRAINING AND COMPETENCY EVALUATION",
            "text": "The training plan for an ML-based CDS system should include all professionals and supporting personnel who will interact with the system. This may include administrative, technical, and professional personnel. The scope of training provided should be tailored to the level of responsibility, delegated tasks, and amount of interaction anticipated for a given trainee. In general, closest attention should be given to those responsible for case selection, dataset or input field selection, and decision support to use or reject the model outputs. Clerical or supporting technical personnel should be aware of inputs that may impact model outputs (eg, specimen source driving analysis by a specific ML model),\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 21\nparticularly if such data are not automatically transferred from the laboratory information systems.\nVerification and/or validation datasets should include sufficient variation to ensure acceptable performance across a range of possible inputs or classes to the model. Additionally, personnel training materials should include case materials that fall close to critical decision thresholds and also include cases to illustrate the model failure modes (eg, image digitization artifacts, insufficient data, improper input data, case selection, use of model output, reporting). For those responsible for reporting results, a clinical competency assessment to use the tool should be demonstrated by successful completion of training cases and satisfactory concordance on a representative spectrum of competency cases. Competency assessment should replicate the procedures planned for patient cases and include cases that should be rejected for evaluation.183\nTraining and competency materials can be prepared and provided by the manufacturer or derived from laboratoryvalidated cases from the user institution files. If manufacturer-provided training materials are used, the deployment laboratory should ensure that their in-house procedures and outputs are comparable to the materials provided by the manufacturer by tangible quality metrics.184,185 Good laboratory practices dictate that a tool should not be placed into use prior to completion of a written, lab director\u2013approved procedure. Additionally, the training exercises and documentation should clearly delineate how to revert to downtime procedures in the event of model drift, shift, or other error. If any part of the model is consistently producing errors, the part of the testing process where the ML model is being used should cease immediately. The performance evaluation should include scenarios present within the intended clinical environment. This should also confirm that safety elements work properly. Confirmation of acceptable failure behavior in the clinical environment should be established, with plans\nfor failsafe procedures in the event the model cannot be used. Definitive understanding of the failure modes including inappropriate use of inputs or errors in the model outputs should be remediated prior to clinical use. Training and competency assessment records should be retained in a retrievable manner for the full period of employment, or use of the tool. Standard operating procedures should reflect retraining triggers clearly; retraining and education should be performed whenever procedures change significantly, such as an expansion of case selection criteria. Documentation of training and competency assessment of appropriate users should be retained similarly to other laboratory procedures as per good laboratory practice.78,186"
        },
        {
            "heading": "CONCLUSIONS",
            "text": "ML models have enabled newfound functionality and workflows in pathology. Significant numbers of ML models are commercially available and organizations with computational pathology resources may also develop ML models that can be introduced into clinical practice. ML-based models in pathology include imaging-based and non\u2013 image-based methodologies. These proposed recommendations are based on the available evidence and literature, and the authors hope to encourage additional peerreviewed literature to support adoption of these novel technologies in clinical practice (Table 7). The combination of pathologists or others and ML-based systems creates a paradigm of augmented intelligence, where the pathologist is assisted by ML from patient testing and reporting to enhance cognitive performance and clinical decision making. High-performance ML models will facilitate specific tasks for pathologists in an assistive fashion; the role of the ML model is limited to providing competent support to the healthcare provider.\n22 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nPerformance evaluation of ML models is critical to verification and/or validation of ML systems intended for clinical reporting of patient samples. Appropriate evaluation metrics based on the MLmodel should be used to evaluate the performance characteristics of the model prior to clinical testing. The importance of incorporating laboratory director oversight and using local data for verification and/or validation cannot be overstated. Measuring the similarity between the development dataset and the verification and/or validation dataset can help assure performance characteristics should generalize as indicated. Changes to the model (eg, retraining with additional data, intended use/indications of use, significant changes to the user interface), should require re-verification of the revised model and documentation of the model version. Additionally, any change to the workflow used to generate the input data needs to be considered. Understanding the preanalytic variables for biospecimen quality may impact model performance characteristics. Furthermore, monitoring for performance defects over time (eg, shift, drift) should be conducted to detect relevant data effects of the ML models. If a model is found to have deteriorated performance, the portion of the clinical test where the MLmodel is incorporated should immediately be stopped from use and remediated appropriately.\nPathology laboratories should be responsible for determining the clinical utility of the ML model prior to verifying and/or validating and for monitoring performance over time. Proper documentation of training and competency should be completed prior to clinical use of the ML-based system. As pathologists verify and validate ML models, they must learn about the scope of application, and its strengths and limitations, in preparation for a future that will incorporate powerful new tools and new management responsibilities.\nThe authors thank James Harrison, MD, for guidance on this concept paper and valuable commentary, Mary Kennedy and Kevin Schap for administrative support, and the College of\nAmerican Pathologists committees and members who otherwise contributed to these recommendations.\nReferences\n1. Wians FH Jr, Gill GW. Clinical and anatomic pathology test volume by specialty and subspecialty among high-complexity CLIA-certified laboratories in 2011. Lab Med. 2013;44(2):163\u2013167. doi:10.1309/LMPGOCRS216SVDZH\n2. US Food and Drug Administration. FDA authorizes software that can help identify prostate cancer. https://www.fda.gov/news-events/press-announcements/ fda-authorizes-software-can-help-identify-prostate-cancer, Accessed November 9, 2021.\n3. US Food and Drug Administration. 510(k) Premarket notification. X100 with full field peripheral blood smear (PBS) Application. https://www.accessdata. fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID\u00bcK201301. Accessed November 9, 2021.\n4. US Food and Drug Administration. 510(k) Premarket notification. CellaVision. https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfPMN/pmn.cfm?start_ search\u00bc1&productcode\u00bcJOY&knumber\u00bc&applicant\u00bcCELLAVISION%20AB. Accessed November 9, 2021.\n5. US Food and Drug Administration. 510(k) Premarket Notification. APAS independence with urine analysis module. https://www.accessdata.fda.gov/scripts/cdrh/ cfdocs/cfpmn/pmn.cfm?ID\u00bcK183648. Accessed December 30, 2021.\n6. US Food and Drug Administration. Premarket approval (PMA). ThinPrep integrated imager. https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpma/pma. cfm?id\u00bcP950039S036. Accessed November 9, 2021.\n7. Chabrun F, Dieu X, Ferre M, et al. Achieving expert-level interpretation of serum protein electrophoresis through deep learning driven by human reasoning. Clin Chem. 2021;67(10):1406\u20131414. doi:10.1093/clinchem/hvab133\n8. Punchoo R, Bhoora S, Pillay N. Applications of machine learning in the chemical pathology laboratory. J Clin Pathol. 2021;74(7):435\u2013442. doi:10.1136/ jclinpath-2021-207393\n9. Baron JM, Mermel CH, Lewandrowski KB, Dighe AS. Detection of preanalytic laboratory testing errors using a statistically guided protocol. Am J Clin Pathol. 2012;138(3):406\u2013413. doi:10.1309/AJCPQIRIB3CT1EJV 10. Rosenbaum MW, Baron JM. Using machine learning-based multianalyte delta checks to detect wrong blood in tube errors. Am J Clin Pathol. 2018;150(6): 555\u2013566. doi:10.1093/ajcp/aqy085 11. Farrell CJL, Giannoutsos J. Machine learning models outperform manual result review for the identification of wrong blood in tube errors in complete blood count results. Int J Lab Hematol. 2022;44(3):497\u2013503. doi:10.1111/ijlh.13820 12. Luo Y, Szolovits P, Dighe AS, Baron JM. Using machine learning to predict laboratory test results. Am J Clin Pathol. 2016;145(6):778\u2013788. doi:10.1093/ ajcp/aqw064 13. Lidbury BA, Richardson AM, Badrick T. Assessment of machine-learning techniques on large pathology data sets to address assay redundancy in routine liver function test profiles. Diagn Berl Ger. 2015;2(1):41\u201351. doi:10.1515/dx-2014-0063\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 23\n14. Poole S, Schroeder LF, Shah N. An unsupervised learning method to identify reference intervals from a clinical database. J Biomed Inform. 2016;59:276\u2013284. doi:10.1016/j.jbi.2015.12.010 15. Wilkes EH, Emmett E, Beltran L, Woodward GM, Carling RS. A machine learning approach for the automated interpretation of plasma amino acid profiles. Clin Chem. 2020;66(9):1210\u20131218. doi:10.1093/clinchem/hvaa134 16. Lee ES, Durant TJS. Supervised machine learning in the mass spectrometry laboratory: a tutorial. J Mass Spectrom Adv Clin Lab. 2021;23:1\u20136. doi:10.1016/j. jmsacl.2021.12.001 17. Yu M, Bazydlo LAL, Bruns DE, Harrison JH Jr. Streamlining quality review of mass spectrometry data in the clinical laboratory by use of machine learning. Arch Pathol Lab Med. 2019;143(8):990\u2013998. doi:10.5858/arpa.2018-0238-OA 18. Demirci F, Akan P, Kume T, Sisman AR, Erbayraktar Z, Sevinc S. Artificial neural network approach in laboratory test reporting: learning algorithms. Am J Clin Pathol. 2016;146(2):227\u2013237. doi:10.1093/ajcp/aqw104 19. Lipkova J, Chen RJ, Chen B, et al. Artificial intelligence for multimodal data integration in oncology. Cancer Cell. 2022;40(10):1095\u20131110. doi:10.1016/ j.ccell.2022.09.012 20. Wong A, Otles E, Donnelly JP, et al. External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients. JAMA Intern Med. 2021;181(8):1065\u20131070. doi:10.1001/jamainternmed.2021.2626 21. Rank N, Pfahringer B, Kempfert J, et al. Deep-learning-based real-time prediction of acute kidney injury outperforms human predictive performance. Npj Digit Med. 2020;3(1):1\u201312. doi:10.1038/s41746-020-00346-8 22. Abd-Elrazek MA, Eltahawi AA, Abd Elaziz MH, Abd-Elwhab MN. Predicting length of stay in hospitals intensive care unit using general admission features. Ain Shams Eng J. 2021;12(4):3691\u20133702. doi:10.1016/j.asej.2021.02.018 23. Ashmore R, Calinescu R, Paterson C. Assuring the machine learning lifecycle: desiderata, methods, and challenges. ACM Comput Surv. 2021;54(5):1\u201339. doi:10.1145/3453444 24. Schaffter T, Buist DSM, Lee CI, et al. Evaluation of combined artificial intelligence and radiologist assessment to interpret screening mammograms. JAMA Netw Open. 2020;3(3):e200265. doi:10.1001/jamanetworkopen.2020.0265 25. American Medical Association. Augmented intelligence in health care. https:// www.ama-assn.org/system/files/2019-01/augmented-intelligence-policy-report.pdf. Accessed November 9, 2021. 26. H-480.940. Augmented intelligence in health care. American Medical Association Web site. https://policysearch.ama-assn.org/policyfinder/detail/augmented%20in telligence?uri\u00bc%2FAMADoc%2FHOD.xml-H-480.940.xml. Accessed November 9, 2021. 27. da Silva LM, Pereira EM, Salles PG, et al. Independent real-world application of a clinical-grade automated prostate cancer detection system. J Pathol. 2021;254(2):147\u2013158. doi:10.1002/path.5662 28. Capper D, Jones DTW, Sill M, et al. DNA methylation-based classification of central nervous system tumours. Nature. 2018;555(7697):469\u2013474. doi: 10.1038/nature26000 29. Aikins JS. Prototypes and production rules: an approach to knowledge representation for hypothesis formation. In: International Joint Conference on Artificial Intelligence; 1979. https://openreview.net/forum?id\u00bcrk44fBMuWr. Accessed April 18, 2022. 30. Aikins JS, Kunz JC, Shortliffe EH, Fallat RJ. PUFF: an expert system for interpretation of pulmonary function data. Comput Biomed Res Int J. 1983;16(3): 199\u2013208. doi:10.1016/0010-4809(83)90021-6 31. Aikins JS. Prototypical knowledge for expert systems: a retrospective analysis. Artif Intell. 1993;59(1):207\u2013211. doi:10.1016/0004-3702(93)90187-G 32. Perry CA. Knowledge bases in medicine: a review. Bull Med Libr Assoc. 1990;78(3):271\u2013282. 33. Evans AJ, Brown RW, Bui MM, et al. Validating whole slide imaging systems for diagnostic purposes in pathology: guideline update from the College of American Pathologists in collaboration with the American Society for Clinical Pathology and the Association for Pathology Informatics. Arch Pathol Lab Med. 2022;146(4):440\u2013450. doi:10.5858/arpa.2020-0723-CP 34. Bui MM, Riben MW, Allison KH, et al. quantitative image analysis of human epidermal growth factor receptor 2 immunohistochemistry for breast cancer: guideline from the College of American Pathologists. Arch Pathol Lab Med. 2019;143(10):1180\u20131195. doi:10.5858/arpa.2018-0378-CP 35. Aziz N, Zhao Q, Bry L, et al. College of American pathologists\u2019 laboratory standards for next-generation sequencing clinical tests. Arch Pathol Lab Med. 2015;139(4):481\u2013493. doi:10.5858/arpa.2014-0250-CP 36. Pressman NJ. Markovian analysis of cervical cell images. J Histochem Cytochem. 1976;24(1):138\u2013144. doi:10.1177/24.1.56387 37. Levine GM, Brousseau P, O\u2019Shaughnessy DJ, Losos GJ. Quantitative immunocytochemistry by digital image analysis: application to toxicologic pathology. Toxicol Pathol. 1987;15(3):303\u2013307. doi:10.1177/019262338701500308 38. Cornish TC. Clinical application of image analysis in pathology. Adv Anat Pathol. 2020;27(4):227\u2013235. doi:10.1097/PAP.0000000000000263 39. Gil J, Wu HS. Applications of image analysis to anatomic pathology: realities and promises. Cancer Invest. 2003;21(6):950\u2013959. doi:10.1081/cnv-120025097 40. Webster JD, Dunstan RW. Whole-slide imaging and automated image analysis: considerations and opportunities in the practice of pathology. Vet Pathol. 2014;51(1):211\u2013223. doi:10.1177/0300985813503570 41. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553): 436\u2013444. doi:10.1038/nature14539\n42. Explainable AI: the basics. Policy Briefing. The Royal Society Web site. https:// royalsociety.org/-/media/policy/projects/explainable-ai/AI-and-interpretability-policybriefing.pdf. Accessed April 18, 2022. 43. Tosun AB, Pullara F, Becich MJ, Taylor DL, Fine JL, Chennubhotla SC. Explainable AI (xAI) for anatomic pathology. Adv Anat Pathol. 2020;27(4):241\u2013250. doi:10.1097/PAP.0000000000000264 44. Chen PHC, Liu Y, Peng L. How to develop machine learning models for healthcare.Nat Mater. 2019;18(5):410\u2013414. doi:10.1038/s41563-019-0345-0 45. Harrison JH, Gilbertson JR, Hanna MG, et al. Introduction to artificial intelligence and machine learning for pathology. Arch Pathol Lab Med. 2021; 145(10):1228\u20131254. doi:10.5858/arpa.2020-0541-CP 46. Clinical Laboratory Improvement Amendments of 1988 (CLIA) Title 42: The Public Health and Welfare. Subpart 2: Clinical Laboratories (42 U.S.C. 263a). https://www.govinfo.gov/content/pkg/USCODE-2011-title42/pdf/USCODE-2011title42-chap6A-subchapII-partF-subpart2-sec263a.pdf. Accessed April 18, 2022. 47. Standard: Establishment and verification of performance specifications. 42 CFR \u00a7 493.1253. LII/Legal Information Institute Web site. https://www.law. cornell.edu/cfr/text/42/493.1253. Accessed November 9, 2022. 48. Pantanowitz L, Hartman D, Qi Y, et al. Accuracy and efficiency of an artificial intelligence tool when counting breast mitoses. Diagn Pathol. 2020;15(1):80. doi:10.1186/s13000-020-00995-z 49. Sandbank J, Nudelman A, Krasnitsky I, et al. Implementation of an AI solution for breast cancer diagnosis and reporting in clinical practice. USCAP 2022 Abstracts: informatics (977\u20131017).Mod Pathol. 2022;35(Suppl 2):1163\u20131210. doi: 10.1038/s41379-022-01042-6 50. Sandbank J, Sebag G, Arad A, et al. Validation and clinical deployment of an AI-based solution for detection of gastric adenocarcinoma and Helicobacter pylori in gastric biopsies. USCAP 2022 Abstracts: gastrointestinal pathology (372-507). Mod Pathol. 2022;35(Suppl 2):493\u2013639. doi:10.1038/s41379-022-01036-4 51. Ehteshami Bejnordi B, Veta M, Johannes van Diest P, et al. diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA. 2017;318(22):2199\u20132210. doi:10.1001/ jama.2017.14585 52. Perincheri S, Levi AW, Celli R, et al. An independent assessment of an artificial intelligence system for prostate cancer detection shows strong diagnostic accuracy.Mod Pathol. 2021;34(8):1588\u20131595. doi:10.1038/s41379-021-00794-x 53. Bulten W, Kartasalo K, Chen PHC, et al. Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge. Nat Med. 2022;28(1):154\u2013163. doi:10.1038/s41591-021-01620-2 54. Steiner DF, MacDonald R, Liu Y, et al. Impact of deep learning assistance on the histopathologic review of lymph nodes for metastatic breast cancer. Am J Surg Pathol. 2018;42(12):1636\u20131646. doi:10.1097/PAS.0000000000001151 55. US Food and Drug Administration. Proposed regulatory framework for modifications to artificial intelligence/machine learning (AI/ML)-based software as a medical device (SAMD). US FDA Artificial Intelligence and Machine Learning Discussion Paper. US https://www.fda.gov/media/122535/download. Accessed December 30, 2021. 56. College of American Pathologists. Individualized quality control plan (IQCP) frequently asked questions. https://documents.cap.org/documents/iqcp-faqs.pdf. Accessed April 18, 2022. 57. US Food and Drug Administration Software as a medical device (SAMD): clinical evaluation\u2014guidance for industry and Food and Drug Administration staff. https://www.fda.gov/media/100714/download. Accessed November 10, 2022. 58. American Society of Mechanical Engineers. Assessing Credibility of Computational Modeling Through Verification and Validation: Application to Medical Devices. New York: American Society of Mechanical Engineers; 2018. 59. Meaning of intended uses. 21 CFR 801.4. https://www.ecfr.gov/current/title21/chapter-I/subchapter-H/part-801/subpart-A/section-801.4. Accessed November 10, 2022. 60. Wilkinson MD, Dumontier M, Aalbersberg IJJ, et al. The FAIR guiding principles for scientific data management and stewardship. Sci Data. 2016;3(1): 160018. doi:10.1038/sdata.2016.18 61. Kush RD, Warzel D, Kush MA, et al. FAIR data sharing: the roles of common data elements and harmonization. J Biomed Inform. 2020;107:103421. doi: 10.1016/j.jbi.2020.103421 62. Barocas S, Hardt M, Narayanan A. Fairness and machine learning. http:// www.fairmlbook.org. Accessed April 19, 2022. 63. Sjoding MW, Dickson RP, Iwashyna TJ, Gay SE, Valley TS. Racial bias in pulse oximetry measurement. N Engl J Med. 2020;383(25):2477\u20132478. doi:10. 1056/NEJMc2029240 64. Buolamwini J, Gebru T. Gender shades: intersectional accuracy disparities in commercial gender classification. In: Proceedings of the 1st Conference on Fairness, Accountability and Transparency. Proc. Machine Learning Res. 2018;81:77\u201391. https://proceedings.mlr.press/v81/buolamwini18a.html. Accessed April 19, 2022. 65. Obermeyer Z, Powers B, Vogeli C, Mullainathan S. Dissecting racial bias in an algorithm used to manage the health of populations. Science. 2019;366(6464): 447\u2013453. doi:10.1126/science.aax2342 66. Howard FM, Dolezal J, Kochanny S, et al. The impact of site-specific digital histology signatures on deep learning model accuracy and bias. Nat Commun. 2021;12(1):4423. doi:10.1038/s41467-021-24698-1 67. Leo P, Lee G, Shih NNC, Elliott R, Feldman MD, Madabhushi A. Evaluating stability of histomorphometric features across scanner and staining variations: prostate cancer diagnosis from whole slide images. J Med Imaging. 2016; 3(4):047502. doi:10.1117/1.JMI.3.4.047502\n24 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\n68. Panch T, Mattie H, Atun R. Artificial intelligence and algorithmic bias: implications for health systems. J Glob Health. 2019;9(2):020318. doi:10.7189/ jogh.09.020318 69. Jobin A, Ienca M, Vayena E. The global landscape of AI ethics guidelines. Nat Machine Intell. 2019;1(9):389\u2013399. doi:10.1038/s42256-019-0088-2 70. Jackson BR, Ye Y, Crawford JM, et al. The ethics of artificial intelligence in pathology and laboratory medicine: principles and practice. Acad Pathol. 2021;8:2374289521990784. doi:10.1177/2374289521990784 71. Howerton D, Anderson N, Bosse D, Granade S, Westbrook G. Good laboratory practices for waived testing sites: survey findings from testing sites holding a certificate of waiver under the clinical laboratory improvement amendments of 1988 and recommendations for promoting quality testing. MMWR Recomm Rep. 2005;54(RR-13):1\u201325; quiz CE1\u20134. 72. Ezzelle J, Rodriguez-Chavez IR, Darden JM, et al. Guidelines on good clinical laboratory practice. J Pharm Biomed Anal. 2008;46(1):18\u201329. doi:10.1016/j. jpba.2007.10.010 73. Tworek JA, Henry MR, Blond B, Jones BA. College of American Pathologists Gynecologic Cytopathology Quality Consensus Conference on good laboratory practices in gynecologic cytology: background, rationale, and organization. Arch Pathol Lab Med. 2013;137(2):158\u2013163. doi:10.5858/arpa.2012-0111-CC 74. Gutman DA, Cobb J, Somanna D, et al. Cancer digital slide archive: an informatics resource to support integrated in silico analysis of TCGA pathology data. J Am Med Inform Assoc. 2013;20(6):1091\u20131098. doi:10.1136/amiajnl-2012001469 75. Fedorov A, Longabaugh WJR, Pot D, et al. NCI imaging data commons. Cancer Res. 2021;81(16):4188\u20134193. doi:10.1158/0008-5472.CAN-21-0950 76. Choi JH, Hong SE, Woo HG. Pan-cancer analysis of systematic batch effects on somatic sequence variations. BMC Bioinformatics. 2017;18(1):211. doi:10.1186/s12859-017-1627-7 77. Goh WWB, Wang W, Wong L. Why batch effects matter in omics data, and how to avoid them. Trends Biotechnol. 2017;35(6):498\u2013507. doi:10.1016/j. tibtech.2017.02.012 78. Kothari S, Phan JH, Stokes TH, Osunkoya AO, Young AN, Wang MD. Removing batch effects from histopathological images for enhanced cancer diagnosis. IEEE J Biomed Health Inform. 2014;18(3):765\u2013772. doi:10.1109/JBHI.2013. 2276766 79. Tom JA, Reeder J, Forrest WF, et al. Identifying and mitigating batch effects in whole genome sequencing data. BMC Bioinformatics. 2017;18(1):351. doi:10. 1186/s12859-017-1756-z 80. Aeffner F, Wilson K, Martin NT, et al. the gold standard paradox in digital image analysis: manual versus automated scoring as ground truth. Arch Pathol Lab Med. 2017;141(9):1267\u20131275. doi:10.5858/arpa.2016-0386-RA 81. Sta\u030alhammar G, Fuentes Martinez N, Lippert M, et al. Digital image analysis outperforms manual biomarker assessment in breast cancer. Mod Pathol. 2016;29(4):318\u2013329. doi:10.1038/modpathol.2016.34 82. Nielsen TO, Leung SCY, Rimm DL, et al. Assessment of Ki67 in breast cancer: updated recommendations from the International Ki67 in Breast Cancer Working Group. J Natl Cancer Inst. 2021;113(7):808\u2013819. doi:10.1093/jnci/djaa201 83. Dolan M, Snover D. Comparison of immunohistochemical and fluorescence in situ hybridization assessment of HER-2 status in routine practice. Am J Clin Pathol. 2005;123(5):766\u2013770. 84. Singer M, Deutschman CS, Seymour CW, et al. The third international consensus definitions for sepsis and septic shock (Sepsis-3). JAMA. 2016;315(8): 801\u2013810. doi:10.1001/jama.2016.0287 85. American College of Chest Physicians/Society of Critical Care Medicine Consensus Conference: definitions for sepsis and organ failure and guidelines for the use of innovative therapies in sepsis. Crit Care Med. 1992;20(6):864\u2013874. 86. Goh KH, Wang L, Yeow AYK, et al. Artificial intelligence in sepsis early prediction and diagnosis using unstructured data in healthcare. Nat Commun. 2021;12(1):711. doi:10.1038/s41467-021-20910-4 87. Elmore JG, Longton GM, Carney PA, et al. Diagnostic concordance among pathologists interpreting breast biopsy specimens. JAMA. 2015;313(11):1122\u20131132. doi:10.1001/jama.2015.1405 88. Viswanathan K, Patel A, Abdelsayed M, et al. Interobserver variability between cytopathologists and cytotechnologists upon application and characterization of the indeterminate category in the Milan system for reporting salivary gland cytopathology. Cancer Cytopathol. 2020;128(11):828\u2013839. doi:10.1002/ cncy.22312 89. Tummers P, Gerestein K, Mens JW, Verstraelen H, van Doorn H. Interobserver variability of the International Federation of Gynecology and Obstetrics staging in cervical cancer. Int J Gynecol Cancer. 2013;23(5):890\u2013894. doi:10. 1097/IGC.0b013e318292da65 90. Thomas S, Hussein Y, Bandyopadhyay S, et al. Interobserver variability in the diagnosis of uterine high-grade endometrioid carcinoma. Arch Pathol Lab Med. 2016;140(8):836\u2013843. doi:10.5858/arpa.2015-0220-OA 91. Pentenero M, Todaro D, Marino R, Gandolfo S. Interobserver and intraobserver variability affecting the assessment of loss of autofluorescence of oral mucosal lesions. Photodiagn Photodyn Ther. 2019;28:338\u2013342. doi:10.1016/j. pdpdt.2019.09.007 92. Ortonne N, Carroll SL, Rodriguez FJ, et al. Assessing interobserver variability and accuracy in the histological diagnosis and classification of cutaneous neurofibromass.Neuro-Oncol Adv. 2020;2(Suppl 1):i117\u2013i123. doi:10.1093/noajnl/vdz050 93. Kwak HA, Liu X, Allende DS, Pai RK, Hart J, Xiao SY. Interobserver variability in intraductal papillary mucinous neoplasm subtypes and application of\ntheir mucin immunoprofiles. Mod Pathol. 2016;29(9):977\u2013984. doi:10.1038/ modpathol.2016.93 94. Klaver CEL, Bulkmans N, Drillenburg P, et al. Interobserver, intraobserver, and interlaboratory variability in reporting pT4a colon cancer. Virchows Arch Int J Pathol. 2020;476(2):219\u2013230. doi:10.1007/s00428-019-02663-0 95. Kang HJ, Kwon SY, Kim A, et al. A multicenter study of interobserver variability in pathologic diagnosis of papillary breast lesions on core needle biopsy with WHO classification. J Pathol Transl Med. 2021;55(6):380\u2013387. doi:10.4132/ jptm.2021.07.29 96. Horvath B, Allende D, Xie H, et al. Interobserver variability in scoring liver biopsies with a diagnosis of alcoholic hepatitis. Alcohol Clin Exp Res. 2017; 41(9):1568\u20131573. doi:10.1111/acer.13438 97. Burchardt M, Engers R, M\u20aculler M, et al. Interobserver reproducibility of Gleason grading: evaluation using prostate cancer tissue microarrays. J Cancer Res Clin Oncol. 2008;134(10):1071\u20131078. doi:10.1007/s00432-008-0388-0 98. Bektas S, Bahadir B, Kandemir NO, Barut F, Gul AE, Ozdamar SO. Intraobserver and interobserver variability of Fuhrman and modified Fuhrman grading systems for conventional renal cell carcinoma. Kaohsiung J Med Sci. 2009;25(11): 596\u2013600. doi:10.1016/S1607-551X(09)70562-5 99. Allard FD, Goldsmith JD, Ayata G, et al. Intraobserver and interobserver variability in the assessment of dysplasia in ampullary mucosal biopsies. Am J Surg Pathol. 2018;42(8):1095\u20131100. doi:10.1097/PAS.0000000000001079 100. Rodriguez FJ, Giannini C. Oligodendroglial tumors: diagnostic and molecular pathology. Semin Diagn Pathol. 2010;27(2):136\u2013145. doi:10.1053/j.semdp.2010. 05.001 101. Samorodnitsky E, Datta J, Jewell BM, et al. Comparison of custom capture for targeted next-generation DNA sequencing. J Mol Diagn. 2015;17(1):64\u201375. doi: 10.1016/j.jmoldx.2014.09.009 102. Campanella G, Hanna MG, Geneslaw L, et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nat Med. 2019;25(8):1301\u20131309. doi:10.1038/s41591-019-0508-1 103. Shipe ME, Deppen SA, Farjah F, Grogan EL. Developing prediction models for clinical use using logistic regression: an overview. J Thorac Dis. 2019; 11(Suppl 4):S574\u2013S584. doi:10.21037/jtd.2019.01.25 104. Park SH, Han K. Methodologic guide for evaluating clinical performance and effect of artificial intelligence technology for medical diagnosis and prediction. Radiology. 2018;286(3):800\u2013809. 105. Moons KG, Kengne AP, Grobbee DE, et al. Risk prediction models: II. External validation, model updating, and impact assessment. Heart. 2012 May; 98(9):691\u2013698. 106. Debray TPA, Vergouwe Y, Koffijberg H, Nieboer D, Steyerberg EW, Moons KGM. A new framework to enhance the interpretation of external validation studies of clinical prediction models. J Clin Epidemiol. 2015;68(3):279\u2013289. doi:10.1016/j.jclinepi.2014.06.018 107. Wu L, Zhang J, Zhou W, et al. Randomised controlled trial of WISENSE, a real-time quality improving system for monitoring blind spots during esophagogastroduodenoscopy.Gut. 2019;68(12):2161\u20132169. doi:10.1136/gutjnl-2018-317366 108. Wang P, Liu X, Berzin TM, et al. Effect of a deep-learning computer-aided detection system on adenoma detection during colonoscopy (CADe-DB trial): a double-blind randomised study. Lancet Gastroenterol Hepatol. 2020;5(4):343\u2013351. doi:10.1016/S2468-1253(19)30411-X 109. Repici A, Badalamenti M, Maselli R, et al. Efficacy of real-time computer-aided detection of colorectal neoplasia in a randomized trial. Gastroenterology. 2020;159(2):512\u2013520.e7. doi:10.1053/j.gastro.2020.04.062 110. Wijnberge M, Geerts BF, Hol L, et al. Effect of a machine learning-derived early warning system for intraoperative hypotension vs standard care on depth and duration of intraoperative hypotension during elective noncardiac surgery: the HYPE randomized clinical trial. JAMA. 2020;323(11):1052\u20131060. doi: 10.1001/jama.2020.0592 111. Wang P, Berzin TM, Glissen Brown JR, et al. Real-time automatic detection system increases colonoscopic polyp and adenoma detection rates: a prospective randomised controlled study. Gut. 2019;68(10):1813\u20131819. doi:10.1136/ gutjnl-2018-317500 112. INFANT Collaborative Group. Computerised interpretation of fetal heart rate during labour (INFANT): a randomised controlled trial. Lancet. 2017; 389(10080):1719\u20131729. doi:10.1016/S0140-6736(17)30568-8 113. Clinical Laboratory Improvement Amendments (CLIA). CLIA verification of performance specifications. https://www.cms.gov/regulations-and-guidance/ legislation/clia/downloads/6064bk.pdf. Accessed April 19, 2022. 114. College of American Pathologists. CAP all common checklist. Test method validation and verification. https://appsuite.cap.org/appsuite/learning/LAP/FFoC/ ValidationVerificationStudies/story_content/external_files/checklistrequirements.pdf. Accessed April 19, 2022. 115. Van Calster B, McLernon DJ, van Smeden M, et al. Calibration: the Achilles heel of predictive analytics. BMC Med. 2019;17(1):230. doi:10.1186/ s12916-019-1466-7 116. Van Hoorde K, Van Huffel S, Timmerman D, Bourne T, Van Calster B. A spline-based tool to assess and visualize the calibration of multiclass risk predictions. J Biomed Inform. 2015;54:283\u2013293. doi:10.1016/j.jbi.2014.12.016 117. van der Ploeg T, Nieboer D, Steyerberg EW. Modern modeling techniques had limited external validity in predicting mortality from traumatic brain injury. J Clin Epidemiol. 2016;78:83\u201389. doi:10.1016/j.jclinepi.2016.03.002 118. Pantanowitz L, Quiroga-Garza GM, Bien L, et al. An artificial intelligence algorithm for prostate cancer diagnosis in whole slide images of core needle\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 25\nbiopsies: a blinded clinical validation and deployment study. Lancet Digit Health. 2020;2(8):e407\u2013e416. doi:10.1016/S2589-7500(20)30159-X 119. Davis SE, Greevy RA, Fonnesbeck C, Lasko TA, Walsh CG, Matheny ME. A nonparametric updating method to correct clinical prediction model drift. J Am Med Inform Assoc. 2019;26(12):1448\u20131457. doi:10.1093/jamia/ocz127 120. Epstein JI, Zelefsky MJ, Sjoberg DD, et al. A contemporary prostate cancer grading system: a validated alternative to the Gleason score. Eur Urol. 2016; 69(3):428\u2013435. doi:10.1016/j.eururo.2015.06.046 121. Hattab EM, Koch MO, Eble JN, Lin H, Cheng L. Tertiary Gleason pattern 5 is a powerful predictor of biochemical relapse in patients with Gleason score 7 prostatic adenocarcinoma. J Urol. 2006;175(5):1695\u20131699; discussion 1699. doi:10.1016/S0022-5347(05)00998-5 122. Garc\u0131\u0301a V, Mollineda RA, Sa\u0301nchez JS. Index of balanced accuracy: a performance measure for skewed class distributions. In: Araujo H, Mendonc\u0327a AM, Pinho AJ, Torres MI, eds. Pattern Recognition and Image Analysis. Lecture Notes in Computer Science. Berlin, Heidelberg, Germany: Springer; 2009:441\u2013448. doi: 10.1007/978-3-642-02172-5_57 123. Delgado R, Tibau XA. Why Cohen\u2019s kappa should be avoided as performance measure in classification. PloS One. 2019;14(9):e0222916. doi:10.1371/ journal.pone.0222916 124. Ben-David A. Comparison of classification accuracy using Cohen\u2019s weighted kappa. Expert Syst Appl. 2008;34(2):825\u2013832. doi:10.1016/j.eswa.2006. 10.022 125. Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics. 2020;21(1):6. doi:10.1186/s12864-019-6413-7 126. Gorodkin J. Comparing two K-category assignments by a K-category correlation coefficient. Comput Biol Chem. 2004;28(5):367\u2013374. doi:10.1016/j. compbiolchem.2004.09.006 127. Baldi P, Brunak S, Chauvin Y, Andersen CA, Nielsen H. Assessing the accuracy of prediction algorithms for classification: an overview. Bioinformatics. 2000;16(5):412\u2013424. doi:10.1093/bioinformatics/16.5.412 128. Moskowitz CS. Using free-response receiver operating characteristic curves to assess the accuracy of machine diagnosis of cancer. JAMA. 2017;318(22):2250\u2013 2251. doi:10.1001/jama.2017.18686 129. Park SH, Choi J, Byeon JS. Key principles of clinical validation, device approval, and insurance coverage decisions of artificial intelligence. Korean J Radiol. 2021;22(3):442\u2013453. doi:10.3348/kjr.2021.0048 130. Vu QD, Graham S, Kurc T, et al. Methods for segmentation and classification of digital microscopy tissue images. Front Bioeng Biotechnol. 2019;7:53. 131. D\u2019Agostino R, Nam B. Evaluation of the performance of survival analysis models: discrimination and calibration measures. Amsterdam, The Netherlands: Elsevier; 2003:1\u201325. 132. Hosmer DW, Lemeshow S. Assessing the fit of the model. In: Hosmer DW, Lemeshow S, eds. Applied Logistic Regression. John Wiley & Sons, Ltd; 2000:143\u2013202. doi:10.1002/0471722146.ch5 133. US Department of Health and Human Services, Food and Drug Administration, Center for Devices and Radiological Health. Statistical guidance on reporting results from studies evaluating diagnostic tests\u2014guidance for industry and FDA staff. US Food and Drug Administration Web site. https://www.fda.gov/ regulatory-information/search-fda-guidance-documents/statistical-guidancereporting-results-studies-evaluating-diagnostic-tests-guidance-industry-and-fda. Accessed September 5, 2022. 134. Morgenthaler S. Exploratory data analysis. WIREs Comput Stat. 2009; 1(1):33\u201344. doi:10.1002/wics.2 135. Ben-Gal I. Outlier detection. In: Maimon O, Rokach L, eds. Data Mining and Knowledge Discovery Handbook. Tel Aviv, Israel: Springer US; 2005: 131\u2013146. doi:10.1007/0-387-25465-X_7 136. Bland JM, Altman DG. Statistical methods for assessing agreement between two methods of clinical measurement. Lancet. 1986;1(8476):307\u2013310. 137. Bland JM, Altman DR. Statistical methods for assessing agreement between measurements. Lancet.1986;1(8476):307\u2013310. 138. Petersen PH, Sto\u0308ckl D, Blaabjerg O, et al. Graphical interpretation of analytical data from comparison of a field method with reference method by use of difference plots. Clin Chem. 1997;43(11):2039\u20132046. 139. Hollis S. Analysis of method comparison studies. Ann Clin Biochem. 1996;33(1):1\u20134. doi:10.1177/000456329603300101 140. Sto\u0308ckl D. Beyond the myths of difference plots. Ann Clin Biochem. 1996; 33(Pt 6):575\u2013577. doi:10.1177/000456329603300618 141. Cornbleet PJ, Gochman N. Incorrect least-squares regression coefficients in method-comparison analysis. Clin Chem. 1979;25(3):432\u2013438. 142. Bureau International des Poids et Mesures (BIPM). International Vocabulary of Metrology \u2013 Basic and General Concepts and Associated Terms (VIM). 3rd ed. https://www.bipm.org/documents/20126/2071204/JCGM_200_2012.pdf/f0e1ad45d337-bbeb-53a6-15fe649d0ff1. Accessed June 1, 2022. 143. McEnroe RJ, Durham AP, Goldford MD, et al. Evaluation of Precision of Quantitative Measurement Procedures; Approved Guideline. 3rd ed. CLSI document EP05-A3. Wayne, PA: Clinical and Laboratory Standards Institute; 2014. https://clsi.org/standards/products/method-evaluation/documents/ep05/. Accessed June 1, 2022. 144. International Organization for Standardization. ISO 16140-1:2016 - Microbiology of the food chain - Method validation - Part 1: Vocabulary. Geneva, Switzerland: International Organization for Standardization; 2016.\n145. Carey RN, Durham AP, Hauck WW, et al. User Verification of Precision and Dstimation of Bias; Approved Guideline. 3rd ed. CLSI document EP15-A3. Wayne, PA: Clinical and Laboratory Standards Institute; 2014. 146. Berte LM. Process Management. Wayne, PA: Clinical and Laboratory Standards Institute. 2015. 147. Health Center for Device and Radiologic Health (CDRH). Marketing submission recommendations for a predetermined change control plan for artificial intelligence/machine learning (AI/ML)-enabled device software functions. US Food and Drug Administration Web site. https://www.fda.gov/regulatory-information/searchfda-guidance-documents/marketing-submission-recommendations-predeterminedchange-control-plan-artificial. Accessed May 13, 2023. 148. Jenkins DA, Sperrin M, Martin GP, Peek N. Dynamic models to predict health outcomes: current status and methodological challenges. Diagn Progn Res. 2018;2(1):23. doi:10.1186/s41512-018-0045-2 149. Toll DB, Janssen KJM, Vergouwe Y, Moons KGM. Validation, updating and impact of clinical prediction rules: a review. J Clin Epidemiol. 2008;61(11): 1085\u20131094. doi:10.1016/j.jclinepi.2008.04.008 150. Kappen TH, Vergouwe Y, van Klei WA, van Wolfswinkel L, Kalkman CJ, Moons KGM. Adaptation of clinical prediction models for application in local settings.Med Decis Making. 2012;32(3):E1\u2013E10. doi:10.1177/0272989X12439755 151. Davis SE, Lasko TA, Chen G, Siew ED, Matheny ME. Calibration drift in regression and machine learning models for acute kidney injury. J Am Med Inform Assoc. 2017;24(6):1052\u20131061. 152. Diamond GA. What price perfection?: calibration and discrimination of clinical prediction models. J Clin Epidemiol. 1992;45(1):85\u201389. doi:10.1016/ 0895-4356(92)90192-P 153. Davis SE, Lasko TA, Chen G, Matheny ME. Calibration drift among regression and machine learning models for hospital mortality. AMIA Annu Symp Proc. 2018;2017:625\u2013634. 154. Sinard JH. An analysis of the effect of the COVID-19 pandemic on case volumes in an academic subspecialty-based anatomic pathology practice. Acad Pathol. 2020;7:2374289520959788. 155. Mann DM, Chen J, Chunara R, Testa PA, Nov O. COVID-19 transforms health care through telemedicine: evidence from the field. J Am Med Inform Assoc. 2020;27(7):1132\u20131135. doi:10.1093/jamia/ocaa072 156. Calabrese F, Pezzuto F, Fortarezza F, et al. Pulmonary pathology and COVID-19: lessons from autopsy. The experience of European pulmonary pathologists. Virchows Arch. 2020;477(3):359\u2013372. doi:10.1007/s00428-02002886-6 157. Di Toro F, Gjoka M, Di Lorenzo G, et al. Impact of COVID-19 on maternal and neonatal outcomes: a systematic review and meta-analysis. Clin Microbiol Infect. 2021;27(1):36\u201346. doi:10.1016/j.cmi.2020.10.007 158. Hanna MG, Reuter VE, Ardon O, et al. Validation of a digital pathology system including remote review during the COVID-19 pandemic. Mod Pathol. 2020;33(11):2115\u20132127. doi:10.1038/s41379-020-0601-5 159. Vigliar E, Cepurnaite R, Alcaraz-Mateos E, et al. Global impact of the COVID-19 pandemic on cytopathology practice: results from an international survey of laboratories in 23 countries. Cancer Cytopathol. 2020;128(12):885\u2013 894. doi:10.1002/cncy.22373 160. Tang YW, Schmitz JE, Persing DH, Stratton CW. Laboratory diagnosis of COVID-19: current issues and challenges. J Clin Microbiol. 2020;58(6):e00512-20. doi:10.1128/JCM.00512-20 161. Davis SE, Greevy RA, Lasko TA, Walsh CG, Matheny ME. Detection of calibration drift in clinical prediction models to inform model updating. J Biomed Inform. 2020;112:103611. doi:10.1016/j.jbi.2020.103611 162. Finlayson SG, Bowers JD, Ito J, Zittrain JL, Beam AL, Kohane IS. Adversarial attacks on medical machine learning. Science. 2019;363(6433):1287\u20131289. doi:10.1126/science.aaw4399 163. Allyn J, Allou N, Vidal C, Renou A, Ferdynus C. Adversarial attack on deep learning-based dermatoscopic image recognition systems: risk of misdiagnosis due to undetectable image perturbations. Medicine (Baltimore). 2020; 99(50):e23568. doi:10.1097/MD.0000000000023568 164. Laleh NG, Truhn D, Veldhuizen GP, et al. Adversarial attacks and adversarial robustness in computational pathology. Nat Commun. 2022;13(1):5711. doi:10.1101/2022.03.15.484515 165. Bortsova G, Gonza\u0301lez-Gonzalo C, Wetstein SC, et al. Adversarial attack vulnerability of medical image analysis systems: unexplored factors. Med Image Anal. 2021;73:102141. doi:10.1016/j.media.2021.102141 166. Raciti P, Sue J, Ceballos R, et al. Novel artificial intelligence system increases the detection of prostate cancer in whole slide images of core needle biopsies.Mod Pathol. 2020;33(10):2058\u20132066. doi:10.1038/s41379-020-0551-y 167. Nishikawa RM, Bae KT. Importance of better human-computer interaction in the era of deep learning: mammography computer-aided diagnosis as a use case. J Am Coll Radiol. 2018;15(1 Pt A):49\u201352. doi:10.1016/j.jacr.2017.08.027 168. Burgoon JK, Bonito JA, Bengtsson B, Cederberg C, Lundeberg M, Allspach L. Interactivity in human-computer interaction: a study of credibility, understanding, and influence. Comput Hum Behav. 2000;16(6):553\u2013574. doi: 10.1016/S0747-5632(00)00029-7 169. Jensen M, Meservy T, Burgoon J, Nunamaker J. Automatic, multimodal evaluation of human interaction. Group Decis Negot. 2010;19:367\u2013389. doi:10. 1007/s10726-009-9171-0 170. Lee EJ. Factors That Enhance Consumer Trust in Human-Computer Interaction: An Examination of Interface Factors and Moderating Influences [dissertation].\n26 Arch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al\nKnoxville: University of Tennessee. https://trace.tennessee.edu/utk_graddiss/2148. Accessed April 19, 2022. 171. Szalma JL, Hancock PA. Noise effects on human performance: a meta-analytic synthesis. Psychol Bull. 2011;137(4):682\u2013707. doi:10.1037/a0023987 172. Rudin C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Machine Intell. 2019; 1(5):206\u2013215. doi:10.1038/s42256-019-0048-x 173. Banerjee I, Bhimireddy AR, Burns JL, et al. Reading race: AI recognises patient\u2019s racial identity in medical images. ArXiv.2107.10356 [Cs.Eess]. [Preprint. Posted online July 21, 2021]. doi:arxiv.org/abs/2107.10356 174. Scho\u0308mig-Markiefka B, Pryalukhin A, Hulla W, et al. Quality control stress test for deep learning-based diagnostic model in digital pathology. Mod Pathol. 2021;34(12):2098\u20132108. doi:10.1038/s41379-021-00859-x 175. Shapley LS. 17. A value for n-person games. In: Kuhn HW, Tucker AW, eds. Contributions to the Theory of Games (AM-28), Volume II. Princeton, NJ: Princeton University Press; 1953:307\u2013318. doi:10.1515/9781400881970-018 176. Lundberg S, Lee SI. A unified approach to interpreting model predictions. arXiv. [Preprint. Posted online Nov 25, 2017.]. doi:10.48550/arXiv.1705.07874 177. Molnar C. Interpretable Machine Learning. https://christophm.github.io/ interpretable-ml-book/. Accessed May 16, 2022. 178. Kim B, Wattenberg M, Gilmer J, et al. Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCAV). arXiv. [Preprint. Posted online June 7, 2018.]. doi:10.48550/arXiv.1711.11279\n179. Ribeiro MT, Singh S, Guestrin C. \u201cWhy should I trust you?\u201d: explaining the predictions of any classifier. arXiv. [Preprint. Posted online February 6, 2016.]. doi:10.48550/arXiv.1602.04938 180. Evans T, Retzlaff CO, Geibler C, et al. The explainability paradox: challenges for xAI in digital pathology. Future Gener Comput Syst. 2022;133:281\u2013 296. doi:10.1016/j.future.2022.03.009 181. Linardatos P, Papastefanopoulos V, Kotsiantis S. Explainable AI: a review of machine learning interpretability methods. Entropy Basel Switz. 2020;23(1): E18. doi:10.3390/e23010018 182. Sears A, Jacko JA, eds. Human-Computer Interaction Fundamentals. Boca Raton, FL: CRC Press; 2010. doi:10.1201/b10368 183. Fitzgibbons PL, Bradley LA, Fatheree LA, et al. Principles of analytic validation of immunohistochemical assays: Guideline from the College of American Pathologists Pathology and Laboratory Quality Center. Arch Pathol Lab Med. 2014;138(11):1432\u20131443. doi:10.5858/arpa.2013-0610-CP 184. College of American Pathologists. Laboratory general checklist. https:// medicalcourier.com/wp-content/uploads/CAP-Lab-Master-Checklist-2017.pdf. Accessed May 12, 2022. 185. Centers for Medicare and Medicaid Service. What do I need to do to assess personnel competency? https://www.cms.gov/regulations-and-guidance/legislation/ clia/downloads/clia_compbrochure_508.pdf. Accessed May 12, 2022. 186. Centers for Disease Control and Prevention. Competency guidelines for public health laboratory professionals. https://www.cdc.gov/mmwr/pdf/other/ su6401.pdf. Accessed May 12, 2022.\nArch Pathol Lab Med Machine Learning Performance Evaluation\u2014Hanna et al 27"
        }
    ],
    "title": "A Concept Paper From the College of American Pathologists",
    "year": 2023
}