{
    "abstractText": "We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization. Unsolicited commercial email, or \u201cspam\u201d, floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content. Using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in reallife applications.",
    "authors": [
        {
            "affiliations": [],
            "name": "Georgios Sakkis"
        },
        {
            "affiliations": [],
            "name": "Ion Androutsopoulos"
        },
        {
            "affiliations": [],
            "name": "Georgios Paliouras"
        },
        {
            "affiliations": [],
            "name": "Vangelis Karkaletsis"
        },
        {
            "affiliations": [],
            "name": "Constantine D. Spyropoulos"
        },
        {
            "affiliations": [],
            "name": "Panagiotis Stamatopoulos"
        }
    ],
    "id": "SP:b87dc7f37252ce2c65b1624aa584781815a0e0da",
    "references": [
        {
            "authors": [
                "W.D. Aha",
                "Kibler D",
                "M.K. Albert"
            ],
            "title": "Instance-Based Learning Algorithms",
            "venue": "\u201cMachine Learning\u201d,",
            "year": 1991
        },
        {
            "authors": [
                "I. Androutsopoulos",
                "J. Koutsias",
                "K.V. Chandrinos",
                "G. Paliouras",
                "C.D. Spyropoulos"
            ],
            "title": "2000a) \u201cAn evaluation of na\u00efve Bayesian anti-spam filtering",
            "venue": "In Proceedings of the Workshop on Machine Learning in the New Information Age,",
            "year": 2000
        },
        {
            "authors": [
                "I. Androutsopoulos",
                "G. Paliouras",
                "V. Karkaletsis",
                "G. Sakkis",
                "C.D. Spyropoulos",
                "P. Stamatopoulos"
            ],
            "title": "Learning to filter spam e-mail: a comparison of a na\u00efve Bayesian and a memorybased approach",
            "venue": "Proceedings of the Workshop",
            "year": 2000
        },
        {
            "authors": [
                "I Androutsopoulos",
                "J Koutsias",
                "K.V. Chandrinos",
                "C.D. Spyropoulos"
            ],
            "title": "2000c) \u201cAn experimental comparison of na\u00efve Bayesian and keyword-based anti-spam filtering with encrypted personal e-mail messages",
            "venue": "In Proceedings of SIGIR 2000,",
            "year": 2000
        },
        {
            "authors": [
                "G.T. Dietterich"
            ],
            "title": "Machine Learning Research: Four Current Directions",
            "venue": "AI Magazine 18(4):97-136.",
            "year": 1997
        },
        {
            "authors": [
                "H.D. Drucker",
                "D. Wu",
                "Vapnik V"
            ],
            "title": "Support Vector Machines for Spam Categorization",
            "venue": "IEEE Transactions On Neural Networks,",
            "year": 1999
        },
        {
            "authors": [
                "Duda",
                "R.O",
                "P.E. Hart"
            ],
            "title": "Bayes decision theory",
            "venue": "Chapter 2 in Pattern Classification and Scene Analysis, pp. 10\u201343, John Wiley.",
            "year": 1973
        },
        {
            "authors": [
                "A.S. Dudani"
            ],
            "title": "The distance-weighted knearest neighbor rule",
            "venue": "IEEE Transactions on Systems, Man and Cybernetics, 6(4):325\u2013327.",
            "year": 1976
        },
        {
            "authors": [
                "J.M. G\u00f3mez Hidalgo",
                "M. Ma\u00f1a L\u03c3p\u00e9z",
                "E. Puertas Sanz"
            ],
            "title": "Combining text and heuristics",
            "year": 2000
        },
        {
            "authors": [
                "R. Kohavi"
            ],
            "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection",
            "venue": "Proceedings of the 12 International Joint Conference on Artificial Intelligence (IJCAI1995), Morgan Kaufmann, pp. 1137\u20131143.",
            "year": 1995
        },
        {
            "authors": [
                "T.M. Mitchell"
            ],
            "title": "Machine Learning",
            "venue": "McGrawHill.",
            "year": 1997
        },
        {
            "authors": [
                "P. Pantel",
                "D. Lin"
            ],
            "title": "SpamCop: a spam classification and organization program",
            "venue": "Learning for Text Categorization \u2013 Papers from the AAAI Workshop, pp. 95\u201398, Madison Wisconsin. AAAI Technical Report WS-98-05.",
            "year": 1998
        },
        {
            "authors": [
                "J.R. Quinlan"
            ],
            "title": "C4.5: Programs for Machine Learning",
            "year": 1993
        },
        {
            "authors": [
                "M. Sahami",
                "S. Dumais",
                "Heckerman D.",
                "E. Horvitz"
            ],
            "title": "A Bayesian approach to filtering junk e-mail",
            "venue": "Learning for Text Categorization \u2013 Papers from the AAAI Workshop, pp. 55\u201362, Madison Wisconsin. AAAI Technical Report WS-",
            "year": 1998
        },
        {
            "authors": [
                "G. Sakkis",
                "I. Androutsopoulos",
                "G. Paliouras",
                "V. Karkaletsis",
                "C.D. Spyropoulos",
                "P. Stamatopoulos"
            ],
            "title": "A memory-based approach to anti-spam filtering",
            "venue": "NCSR \u201cDemokritos\u201d Technical Report,",
            "year": 2001
        },
        {
            "authors": [
                "R.E. Schapire",
                "Y. Singer"
            ],
            "title": "BoosTexter: a boosting-based system for text categorization",
            "venue": "Machine Learning, 39(2/3):135\u2013168.",
            "year": 2000
        },
        {
            "authors": [
                "F. Sebastiani"
            ],
            "title": "Machine Learning in Automated Text Categorization",
            "venue": "Revised version of Technical Report IEI-B4-31-1999, Istituto di Elaborazione dell\u2019Informazione, Consiglio Nazionale delle Ricerche, Pisa, Italy.",
            "year": 2001
        },
        {
            "authors": [
                "D. Wettschereck",
                "W.D. Aha",
                "T. Mohri"
            ],
            "title": "A Review and Comparative Evaluation of Feature Weighting Methods for Lazy Learning Algorithms",
            "venue": "Technical Report AIC-95-012, Naval Research Laboratory, Navy Center for Applied Research in",
            "year": 1995
        },
        {
            "authors": [
                "D. Wolpert"
            ],
            "title": "Stacked Generalization",
            "venue": "Neural Networks, 5(2):241\u2013260.",
            "year": 1992
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "This paper presents an empirical evaluation of stacked generalization, a scheme for combining automatically induced classifiers, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization.\nThe increasing popularity and low cost of email have intrigued direct marketers to flood the mailboxes of thousands of users with unsolicited messages, advertising anything, from vacations to get-rich schemes. These messages, known as spam or more formally Unsolicited Commercial E-mail, are extremely annoying, as they clutter mailboxes, prolong dial-up connections, and often expose minors to unsuitable content (Cranor & Lamacchia, 1998).\nLegal and simplistic technical countermeasures, like blacklists and keyword-based filters, have had a very limited effect so far.1 The success of machine learning techniques in text categorization (Sebastiani, 2001) has recently led to alternative, learning-based approaches (Sahami, et al. 1998; Pantel & Lin, 1998; Drucker, et al. 1999). A classifier capable of distinguishing between spam and non-spam, hereafter legitimate, messages is induced from a manually categorized learning collection of messages, and is then used to identify incoming spam e-mail. Initial results have been promising, and experiments are becoming more systematic, by exploiting recently introduced benchmark corpora, and cost-sensitive evaluation measures (Gomez Hidalgo, et al. 2000; Androutsopoulos, et al. 2000a, b, c).\nStacked generalization (Wolpert, 1992), or stacking, is an approach for constructing classifier ensembles. A classifier ensemble, or committee, is a set of classifiers whose individual decisions are combined in some way to classify new instances (Dietterich, 1997). Stacking combines multiple classifiers to induce a higher-level classifier with improved performance. The latter can be thought of as the president of a committee with the ground-level classifiers as members. Each unseen incoming message is first given to the members; the president then decides on the category of the\n1 Consult www.cauce.org, spam.abuse.net, and www.junkemail.org.\nmessage by considering the opinions of the members and the message itself. Ground-level classifiers often make different classification errors. Hence, a president that has successfully learned when to trust each of the members can improve overall performance.\nWe have experimented with two groundlevel classifiers for which results on a public benchmark corpus are available: a Na\u00efve Bayes classifier (Androutsopoulos, et al. 2000a, c) and a memory-based classifier (Androutsopoulos, et al. 2000b; Sakkis, et al. 2001). Using a third, memory-based classifier as president, we investigated two versions of stacking and two different cost-sensitive scenarios. Overall, our results indicate that stacking improves the performance of the ground-level classifiers, and that the performance of the resulting anti-spam filter is acceptable for real-life applications.\nSection 1 below presents the benchmark corpus and the preprocessing of the messages; section 2 introduces cost-sensitive evaluation measures; section 3 provides details on the stacking approaches that were explored; section 4 discusses the learning algorithms that were employed and the motivation for selecting them; section 5 presents our experimental results followed by conclusions."
        },
        {
            "heading": "1 Benchmark corpus and preprocessing",
            "text": "Text categorization has benefited from public benchmark corpora. Producing such corpora for anti-spam filtering is not straightforward, since user mailboxes cannot be made public without considering privacy issues. A useful public approximation of a user\u2019s mailbox, however, can be constructed by mixing spam messages with messages extracted from spam-free public archives of mailing lists. The corpus that we used, Ling-Spam, follows this approach (Androutsopoulos, et al. 2000a, b; Sakkis, et al. 2001). It is a mixture of spam messages and messages sent via the Linguist, a moderated list about the science and profession of linguistics. The corpus consists of 2412 Linguist messages and 481 spam messages.\nSpam messages constitute 16.6% of LingSpam, close to the rates reported by Cranor and LaMacchia (1998), and Sahami et al. (1998).\nAlthough the Linguist messages are more topicspecific than most users\u2019 e-mail, they are less standardized than one might expect. For example, they contain job postings, software availability announcements and even flame-like responses. Moreover, recent experiments with an encoded user mailbox and a Na\u00efve Bayes (NB) classifier (Androutsopoulos, et al. 2000c) yielded results similar to those obtained with Ling-Spam (Androutsopoulos, et al. 2000a). Therefore, experimentation with Ling-Spam can provide useful indicative results, at least in a preliminary stage. Furthermore, experiments with Ling-Spam can be seen as studies of antispam filtering of open unmoderated lists.\nEach message of Ling-Spam was converted into a vector nxxxxx ,,,, 321 h = , where\nnxx ,,1 are the values of attributes\nnXX ,,1 h . Each attribute shows if a particular word (e.g. \u201cadult\u201d) occurs in the message. All attributes are binary: 1=iX if the word is present; otherwise 0=iX . To avoid treating forms of the same word as different attributes, a lemmatizer was applied, converting each word to its base form.\nTo reduce the dimensionality, attribute selection was performed. First, words occurring in less than 4 messages were discarded. Then, the Information Gain (IG) of each candidate attribute X was computed:\n)()( ),(log),(),( },{},1,0{ cPxP cxPcxPCXIG legitspamcx \u22c5 \u22c5= \u2211 \u2208\u2208\nThe attributes with the m highest IG-scores were selected, with m corresponding to the best configurations of the ground classifiers that have been reported for Ling-Spam (Androutsopoulos, et al. 2000a; Sakkis, et al. 2001); see Section 4."
        },
        {
            "heading": "2 Evaluation measures",
            "text": "Blocking a legitimate message is generally more severe an error than accepting a spam message. Let SL \u2192 and LS \u2192 denote the two error types, respectively, and let us assume that\nSL \u2192 is \u03bb times as costly as LS \u2192 . Previous research has considered three cost\nscenarios, where \u03bb = 1, 9, or 999\n(Androutsopoulos, et al. 2000a, b, c; Sakkis, et al. 2001). In the scenario where \u03bb = 999, blocked messages are deleted immediately.\nSL \u2192 is taken to be 999 times as costly as LS \u2192 , since most users would consider losing\na legitimate message unacceptable. In the scenario where \u03bb = 9, blocked messages are returned to their senders with a request to resend them to an unfiltered address. In this case, SL \u2192 is penalized more than LS \u2192 , to account for the fact that recovering from a blocked legitimate message is more costly (counting the sender\u2019s extra work) than recovering from a spam message that passed the filter (deleting it manually). In the third scenario, where \u03bb = 1, blocked messages are simply flagged as possibly spam. Hence, SL \u2192 is no more costly than LS \u2192 . Previous experiments indicate that the Na\u00efve Bayes ground-classifier is unstable when \u03bb = 999 (Androutsopoulos, et al. 2000a). Hence, we have considered only the cases where \u03bb = 1 or 9.\nLet )(xWL and )(xWS be the confidence of a classifier (member or president) that message x is legitimate and spam, respectively. The classifier classifies x as spam iff:\n\u03bb> )( )( xW xW\nL\nS\nIf )(xWL and )(xWS are accurate estimates of )|( xlegitP and )|( xspamP , respectively, the\ncriterion above achieves optimal results (Duda & Hart, 1973).\nTo measure the performance of a filter, weighted accuracy (WAcc) and its complementary weighted error rate (WErr = 1 \u2013 WAcc) are used (Androutsopoulos, et al. 2000a, b, c; Sakkis, et al. 2001):\nSL\nSSLL NN NN WAcc +\u22c5\u03bb +\u22c5\u03bb = \u2192\u2192\nwhere ZYN \u2192 is the number of messages in category Y that the filter classified as Z ,"
        },
        {
            "heading": "SLLLL NNN \u2192\u2192 += , LSSSS NNN \u2192\u2192 += .",
            "text": "That is, when a legitimate message is blocked, this counts as \u03bb errors; and when it passes the filter, this counts as \u03bb successes.\nWe consider the case where no filter is present as our baseline: legitimate messages are\nnever blocked, and spam messages always pass. The weighted accuracy of the baseline is:\nSL\nLb NN NWAcc +\u22c5\u03bb \u22c5\u03bb =\nThe total cost ratio (TCR) compares the performance of a filter to the baseline:\nLSSL\nS b NN N WErr WErrTCR\n\u2192\u2192 +\u22c5\n==\n\u03bb\nGreater TCR values indicate better performance. For TCR < 1, not using the filter is better.\nOur evaluation measures also include spam recall (SR) and spam precision (SP):\nLSSS\nSS NN N SR \u2192\u2192 \u2192 + =\nSLSS\nSS NN N SP \u2192\u2192 \u2192 + =\nSR measures the percentage of spam messages that the filter blocks (intuitively, its effectiveness), while SP measures how many blocked messages are indeed spam (its safety). Despite their intuitiveness, comparing different filter configurations using SR and SP is difficult: each configuration yields a pair of SR and SP results; and without a single combining measure, like TCR, that incorporates the notion of cost, it is difficult to decide which pair is better.\nIn all the experiments, stratified 10-fold cross-validation was used. That is, Ling-Spam was partitioned into 10 equally populated parts, maintaining the original spam-legitimate ratio. Each experiment was repeated 10 times, each time reserving a different part jS (j = 1, \u2026, 10) for testing, and using the remaining 9 parts as the training set jL ."
        },
        {
            "heading": "3 Stacking",
            "text": "In the first version of stacking that we explored (Wolpert, 1992), which we call cross-validation stacking, the training set of the president was prepared using a second-level 3-fold crossvalidation. Each training set jL was further partitioned into three equally populated parts, and the training set of the president was prepared in three steps. At each step, a different part iLS (i = 1, 2, 3) of jL was reserved, and\nthe members were trained on the union iLL of the other two parts. Each mxxx ,,1 = of\niLS was enhanced with the members\u2019 confidence )(1 xWS and )(2 xWS\nthat x is spam, yielding an enhanced 'iLS with vectors\n)(),(,,,' 211 xWxWxxx SSm\n= . At the end of the 3-fold cross-validation, the president was trained on '''' 321 LSLSLSLj = . It was then tested on jS , after retraining the members on the entire jL and enhancing the vectors of jS with the predictions of the members.\nThe second stacking version that we explored, dubbed holdout stacking, is similar to Kohavi\u2019s (1995) holdout accuracy estimation. It differs from the first version, in two ways: the members are not retrained on the entire jL ; and each partitioning of jL into iLL and iLS leads to a different president, trained on 'iLS , which is then tested on the enhanced jS . Hence, there are 103\u00d7 presidents in a 10-fold experiment, while in the first version there are only 10. In each case, WAcc is averaged over the presidents, and TCR is reported as WErrb over the average WErr.\nHoldout stacking is likely to be less effective than cross-validation stacking, since its classifiers are trained on smaller sets. Nonetheless, it requires fewer computations, because the members are not retrained. Furthermore, during classification the president consults the same members that were used to prepare its training set. In contrast, in crossvalidation stacking the president is tested using members that have received more training than those that prepared its training set. Hence, the model that the president has acquired, which shows when to trust each member, may not apply to the members that the president consults when classifying incoming messages."
        },
        {
            "heading": "4 Inducers employed",
            "text": "As already mentioned, we used a Na\u00efve Bayes (NB) and a memory-based learner as members of the committee (Mitchell 1997; Aha, et al.\n1991). For the latter, we used TiMBL, an implementation of the k-Nearest Neighbor algorithm (Daelemans, et al. 2000).\nWith NB, the degree of confidence )(xWS\nthat x is spam is:\n== )|()( xspamPxW NBS\n\u2211 \u220f\n\u220f\n\u2208 =\n=\n\u22c5\n\u22c5\n=\n},{ 1\n1\n)|()(\n)|()(\nlegitspamk\nm\ni i\nm\ni i\nkxPkP\nspamxPspamP\nNB assumes that mXX ,,1 are conditionally independent given the category (Duda & Hart, 1973).\nWith k-NN, a distance-weighted method is used, with a voting function analogous to the inverted cube of distance (Dudani 1976). The k nearest neighbors ix of x are considered:\n\u2211\n\u2211\n=\n=\u2212\n\u03b4\n= k\ni i\nk\ni ii\nNNk S\nxxd\nxxdxCspam xW\n1\n3\n1\n3\n),(1\n),())(,( )( ,\nwhere )( ixC is the category of neighbor ix , ),( ji xxd is the distance between ix and jx ,\nand 1),( 21 =cc\u03b4 , if 21 cc = , and 0 otherwise. This formula weighs the contribution of each neighbor by its distance from the message to be classified, and the result is scaled to [0,1]. The distance is computed by an attribute-weighted function (Wettschereck, et al. 1995), employing Information Gain (IG):\n),(),( 1 j r i r\nn\nt tji xxIGxxd \u2211 = \u22c5\u2261 \u03b4 ,\nwhere im i i xxx ,,1 l = , jm j j xxx ,,1 l = , and\ntIG is the IG score of tX (Section 1). In Tables 1 and 2, we reproduce the best\nperforming configurations of the two learners on Ling-Spam (Androutsopoulos, et al. 2000b; Sakkis, et al. 2001). These configurations were used as members of the committee.\nThe same memory-based learner was used as the president. However, we experimented with several configurations, varying the neighborhood size (k) from 1 to 10, and\nproviding the president with the m best wordattributes, as in Section 1, with m ranging from 50 to 700 by 50. The same attribute- and distance-weighting schemes were used for the president, as with the ground-level memorybased learner.\nOur motivation for combining NB with k-NN emerged from preliminary results indicating that the two ground-level learners make rather uncorrelated errors. Table 3 shows the average percentages of messages where only one, or both ground-level classifiers fail, per cost scenario (\u03bb) and message category. The figures are for the configurations of Tables 1 and 2. It can be seen that the common errors are always fewer than the cases where both classifiers fail. Hence, there is much space for improved accuracy, if a president can learn to select the correct member."
        },
        {
            "heading": "5 Experimental results",
            "text": "Tables 4 and 5 summarize the performance of the best configurations of the president in our experiments, for each cost scenario. Comparing the TCR scores in these tables with the corresponding scores of Tables 1 and 2 shows that stacking improves the performance of the overall filter. From the two stacking versions, cross-validation stacking is slightly better than holdout stacking. It should also be noted that stacking was beneficial for most of the configurations of the president that we tested, i.e. most sub-optimal presidents outperformed the best configurations of the members. This is encouraging, since the optimum configuration is often hard to determine a priori, and may vary from one user to the other.\nThere was one interesting exception in the positive impact of stacking. The 1-NN and 2-NN (k = 1, 2) presidents were substantially worse than the other k-NN presidents, often performing worse than the ground-level classifiers. We witnessed this behavior in both cost scenarios, and with most values of m (number of attributes). In a \u201cpostmortem\u201d analysis, we ascertained that most messages misclassified by 1-NN and 2-NN, but not the other presidents, are legitimate, with their nearest neighbor being spam. Therefore, the additional errors of 1-NN and 2-NN, compared to the other presidents, are of the SL \u2192 type. Interestingly, in most of\nthose cases, both members of the committee classify the instance correctly, as legitimate. This is an indication, that for small values of the parameter k the additional two features, i.e., the members\u2019 confidence )(1 xWS and )(2 xWS\n, do not enhance but distort the representation of instances. As a result, the close neighborhood of the unclassified instance is not a legitimate, but a spam e-mail. This behavior of the memorybased classifier is also noted in (Sakkis, et al. 2001). The suggested solution there was to use a larger value for k, combined with a strong distance weighting function, such as the one presented in section 4."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper we adopted a stacked generalization approach to anti-spam filtering, and evaluated its performance. The configuration that we examined combined a memory-based and a Na\u00efve Bayes classifier in a two-member committee, in which another memory-based classifier presided. The classifiers that we chose as members of the committee have been evaluated individually on the same data as in our evaluation, i.e. the LingSpam corpus. The results of these earlier studies were used as a basis for comparing the performance of our method.\nOur experiments, using two different approaches to stacking and two different misclassification cost scenarios, show that stacking consistently improves the performance of anti-spam filtering. This is explained by the fact that the two members of the committee disagree more often than agreeing in their misclassification errors. Thus, the president is able to improve the overall performance of the filter, by choosing the right member\u2019s decision when they disagree.\nThe results presented here motivate further work in the same direction. In particular, we are interested in combining more classifiers, such as decision trees (Quinlan, 1993) and support vector machines (Drucker, et al. 1999), within the stacking framework. A larger variety of classifiers is expected to lead the president to more informed decisions, resulting in further improvement of the filter\u2019s performance. Furthermore, we would like to evaluate other\nclassifiers in the role of the president. Finally, it would be interesting to compare the performance of the stacked generalization approach to other multi-classifier methods, such as boosting (Schapire & Singer, 2000)."
        }
    ],
    "title": "Stacking classifiers for anti-spam filtering of e-mail",
    "year": 2001
}