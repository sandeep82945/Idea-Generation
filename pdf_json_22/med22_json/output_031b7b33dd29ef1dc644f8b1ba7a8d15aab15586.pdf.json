{
    "abstractText": "Spatiotemporal and motion feature representations are the key to video action recognition. Typical previous approaches are to utilize 3D CNNs to cope with both spatial and temporal features, but they suffer from huge computations. Other approaches are to utilize (1+2)D CNNs to learn spatial and temporal features in an efficient way, but they neglect the importance of motion representations. To overcome problems with previous approaches, we propose a novel block which makes it possible to alleviate the aforementioned problems, since our block can capture spatial and temporal features more faithfully and efficiently learn motion features. This proposed block includes Motion Excitation (ME), Multi-view Excitation (MvE), and Densely Connected Temporal Aggregation (DCTA). The purpose of ME is to encode feature-level frame differences; MvE is designed to enrich spatiotemporal features with multiple view representations adaptively; and DCTA is to model long-range temporal dependencies. We inject the proposed building block, which we refer to as the META block (or simply \u201cMETA\u201d), into 2D ResNet-50. Through extensive experiments, we demonstrate that our proposed method architecture outperforms previous CNN-based methods in terms of \u201cVal Top-1 %\u201d measure with Something-Something v1 and Jester datasets, while the META yielded competitive results with the Moment-in-Time Mini dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrea Prati"
        },
        {
            "affiliations": [],
            "name": "Luis Javier Garc\u00eda Villalba"
        },
        {
            "affiliations": [],
            "name": "Yuri Yudhaswana Joefrie"
        },
        {
            "affiliations": [],
            "name": "Masaki Aono"
        }
    ],
    "id": "SP:386ac7973e82237aec28836b1a0e938f45230a30",
    "references": [
        {
            "authors": [
                "J.C. Stroud",
                "D.A. Ross",
                "C. Sun",
                "J. Deng",
                "R. Sukthankar"
            ],
            "title": "D3D: Distilled 3D Networks for Video Action Recognition",
            "venue": "In Proceedings of the 2020 IEEE Winter Conference on Applications of Computer Vision, WACV",
            "year": 2020
        },
        {
            "authors": [
                "M. Brezovsk\u00fd",
                "D. Sopiak",
                "M. Oravec"
            ],
            "title": "Action recognition by 3d convolutional network",
            "venue": "In Proceedings of the Elmar-International Symposium Electronics in Marine, Zadar, Croatia,",
            "year": 2018
        },
        {
            "authors": [
                "K. Hara",
                "H. Kataoka",
                "Y. Satoh"
            ],
            "title": "Learning spatio-Temporal features with 3D residual networks for action recognition",
            "venue": "In Proceedings of the 2017 IEEE International Conference on Computer Vision Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "L. Wang",
                "W. Li",
                "L. Van Gool"
            ],
            "title": "Appearance-and-Relation Networks for Video Classification",
            "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "B. Zhou",
                "A. Andonian",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Temporal Relational Reasoning in Videos",
            "venue": "Lect. Notes Comput. Sci. Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinform. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "J. Lin",
                "C. Gan",
                "S. Han"
            ],
            "title": "TSM: Temporal Shift Module for Efficient Video Understanding",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "L. Wang",
                "Y. Xiong",
                "Z. Wang",
                "Y. Qiao",
                "D. Lin",
                "X. Tang",
                "L. Van Gool"
            ],
            "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2016,",
            "year": 2016
        },
        {
            "authors": [
                "A. Diba",
                "M. Fayyaz",
                "V. Sharma",
                "A.H. Karami",
                "M.M. Arzani",
                "R. Yousefzadeh",
                "L.V. Gool"
            ],
            "title": "Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Qiu",
                "T. Yao",
                "T. Mei"
            ],
            "title": "Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "D. Tran",
                "H. Wang",
                "L. Torresani",
                "J. Ray",
                "Y. Lecun",
                "M. Paluri"
            ],
            "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
            "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "C. Li",
                "Z. Hou",
                "J. Chen",
                "Y. Bu",
                "J. Zhou",
                "Q. Zhong",
                "D. Xie",
                "S. Pu"
            ],
            "title": "Team DEEP-HRI Moments in Time Challenge 2018 Technical Report; Hikvision Research Institute: Hangzhou, China 2018",
            "year": 2018
        },
        {
            "authors": [
                "J. Arunnehru",
                "G. Chamundeeswari",
                "S.P. Bharathi"
            ],
            "title": "Human Action Recognition using 3D Convolutional Neural Networks with 3D Motion Cuboids in Surveillance Videos",
            "venue": "Procedia Comput. Sci",
            "year": 2018
        },
        {
            "authors": [
                "J. Carreira",
                "A. Zisserman"
            ],
            "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "X. Wang",
                "A. Gupta"
            ],
            "title": "Videos as Space-Time Region Graphs",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "K. Hara",
                "H. Kataoka",
                "Y. Satoh"
            ],
            "title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet",
            "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.J. Li",
                "L. Kai",
                "F.-F. Li"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "In Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, USA,",
            "year": 2009
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M Bernstein"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "Int. J. Comput. Vis",
            "year": 2014
        },
        {
            "authors": [
                "X. Wang",
                "Z. Miao",
                "R. Zhang",
                "S. Hao"
            ],
            "title": "I3D-LSTM: A New Model for Human Action Recognition",
            "venue": "IOP Conf. Ser. Mater. Sci. Eng",
            "year": 2019
        },
        {
            "authors": [
                "Y.Y. Joefrie",
                "M. Aono"
            ],
            "title": "Action Recognition by Composite Deep Learning Architecture I3D-DenseLSTM",
            "venue": "In Proceedings of the 2019 International Conference of Advanced Informatics: Concepts, Theory and Applications (ICAICTA), Yogyakarta, Indonesia,",
            "year": 2019
        },
        {
            "authors": [
                "R. Mutegeki",
                "D.S. Han"
            ],
            "title": "A CNN-LSTM Approach to Human Activity Recognition",
            "venue": "In Proceedings of the 2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),",
            "year": 2020
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long Short-Term Memory",
            "venue": "Neural Comput",
            "year": 1997
        },
        {
            "authors": [
                "S. Xie",
                "C. Sun",
                "J. Huang",
                "Z. Tu",
                "K. Murphy"
            ],
            "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "M. Zolfaghari",
                "K. Singh",
                "T. Brox"
            ],
            "title": "ECO: Efficient Convolutional Network for Online Video Understanding",
            "venue": "Lect. Notes Comput. Sci. Incl. Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinform",
            "year": 2018
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "H. Fan",
                "J. Malik",
                "K. He"
            ],
            "title": "Slowfast Networks for Video Recognition",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Hao",
                "S. Wang",
                "P. Cao",
                "X. Gao",
                "T. Xu",
                "J. Wu",
                "X. He"
            ],
            "title": "Attention in Attention: Modeling Context Correlation for Efficient Video Classification",
            "venue": "IEEE Trans. Circuits Syst. Video Technol",
            "year": 2022
        },
        {
            "authors": [
                "L. Sevilla-Lara",
                "Y. Liao",
                "F. G\u00fcney",
                "V. Jampani",
                "A. Geiger",
                "M.J. Black"
            ],
            "title": "On the Integration of Optical Flow and Action Recognition",
            "venue": "In Proceedings of the Pattern Recognition, Long Beach, CA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "A. Abdelbaky",
                "S. Aly"
            ],
            "title": "Two-stream spatiotemporal feature fusion for human action recognition",
            "venue": "Vis. Comput",
            "year": 2021
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "A. Pinz",
                "R. Wildes"
            ],
            "title": "Spatiotemporal Multiplier Networks for Video Action Recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "C. Zach",
                "T. Pock",
                "H. Bischof"
            ],
            "title": "A Duality Based Approach for Realtime TV-L 1 Optical Flow",
            "venue": "In Pattern Recognition; Springer: Berlin/Heidelberg, Germany,",
            "year": 2007
        },
        {
            "authors": [
                "L. Wang",
                "Y. Xiong",
                "Z. Wang",
                "Y. Qiao",
                "D. Lin",
                "X. Tang",
                "L.V. Gool"
            ],
            "title": "Temporal Segment Networks for Action Recognition in Videos",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2019
        },
        {
            "authors": [
                "B. Jiang",
                "M. Wang",
                "W. Gan",
                "W. Wu",
                "J. Yan"
            ],
            "title": "STM: Spatiotemporal and motion encoding for action recognition",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "B. Ji",
                "X. Shi",
                "J. Zhang",
                "B. Kang",
                "L. Wang"
            ],
            "title": "TEA: Temporal Excitation and Aggregation for Action Recognition",
            "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wang",
                "Q. She",
                "A. Smolic"
            ],
            "title": "ACTION-Net: Multipath Excitation for Action Recognition",
            "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang"
            ],
            "title": "MEST: An Action Recognition Network with Motion Encoder and Spatio-Temporal Module",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A. Arnab",
                "M. Dehghani",
                "G. Heigold",
                "C. Sun",
                "M. Lucic",
                "C. Schmid"
            ],
            "title": "ViViT: A Video Vision Transformer",
            "venue": "In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, QC,",
            "year": 2021
        },
        {
            "authors": [
                "G. Bertasius",
                "H. Wang",
                "L. Torresani"
            ],
            "title": "Is Space-Time Attention All You Need for Video Understanding",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "T.D. Truong",
                "Q.H. Bui",
                "C.N. Duong",
                "H.S. Seo",
                "S.L. Phung",
                "X. Li",
                "K. Luu"
            ],
            "title": "DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition",
            "venue": "In Proceedings of the Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "K. Li",
                "Y. Wang",
                "P. Gao",
                "G. Song",
                "Y. Liu",
                "H. Li",
                "Y. Qiao"
            ],
            "title": "UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Tian",
                "Y. Yan",
                "X. Min",
                "G. Lu",
                "G. Zhai",
                "G. Guo",
                "Z. Gao"
            ],
            "title": "EAN: Event Adaptive Network for Enhanced Action Recognition",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S.H. Gao",
                "M.M. Cheng",
                "K. Zhao",
                "X.Y. Zhang",
                "M.H. Yang",
                "P. Torr"
            ],
            "title": "Res2Net: A New Multi-scale Backbone Architecture",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "R. Goyal",
                "S.E. Kahou",
                "V. Michalski",
                "J. Materzynska",
                "S. Westphal",
                "H. Kim",
                "V. Haenel",
                "I. Fr\u00fcnd",
                "P. Yianilos",
                "M Mueller-Freitag"
            ],
            "title": "The \u201csomething something\u201d video database for learning and evaluating visual common sense",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "J. Materzynska",
                "G. Berger",
                "I. Bax",
                "R. Memisevic"
            ],
            "title": "The Jester Dataset: A Large-Scale Video Dataset of Human Gestures",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "M. Monfort",
                "A. Andonian",
                "B. Zhou",
                "K. Ramakrishnan",
                "S.A. Bargal",
                "T. Yan",
                "L. Brown",
                "Q. Fan",
                "D. Gutfreund",
                "C Vondrick"
            ],
            "title": "Moments in Time Dataset: One Million Videos for Event Understanding",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2020
        },
        {
            "authors": [
                "P. Goyal",
                "P. Doll\u00e1r",
                "R. Girshick",
                "P. Noordhuis",
                "L. Wesolowski",
                "A. Kyrola",
                "A. Tulloch",
                "Y. Jia",
                "K.H. Facebook"
            ],
            "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "M. Raghu",
                "T. Unterthiner",
                "S. Kornblith",
                "C. Zhang",
                "A. Dosovitskiy"
            ],
            "title": "Do Vision Transformers See Like Convolutional Neural Networks",
            "venue": "Adv. Neural Inf. Process. Syst. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Cai",
                "D. Trimmed Event Recognition (Moments in Time"
            ],
            "title": "Submission to ActivityNet Challenge 2018",
            "venue": "Available online: http: //xxx.lanl.gov/abs/1801.03150",
            "year": 2022
        },
        {
            "authors": [
                "S. Guan",
                "H. Li"
            ],
            "title": "SYSU iSEE Submission to Moments in Time Challenge 2018",
            "venue": "Technical Report; School of Data and Computer Science Sun Yat-Sen University: Guangzhou,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Citation: Joefrie, Y.Y.; Aono, M.\nVideo Action Recognition Using\nMotion and Multi-View Excitation\nwith Temporal Aggregation. Entropy\n2022, 24, 1663. https://doi.org/\n10.3390/e24111663\nAcademic Editors: Andrea Prati, Luis\nJavier Garc\u00eda Villalba and Vincent\nA. Cicirello\nReceived: 25 October 2022\nAccepted: 11 November 2022\nPublished: 15 November 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional affil-\niations.\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: action recognition; multi-view; excitation; multi-layer neural network; temporal convolution; videos"
        },
        {
            "heading": "1. Introduction",
            "text": "Video action recognition is still challenging for researchers and practitioners as it involves both spatiotemporal and motion understanding. In the meantime, as an impact of the growth of technology, more people are involved in social media. They often record, upload and share videos to media platforms. As a result, an abundant number of videos are available to the public. This leads to more researchers engaging in the topic of video understanding. Action recognition, the first step of video understanding, becomes critical in practical applications such as suspicious behavior detection in camera surveillance and video recommendation systems. An action in videos can be recognized based on scenes with less temporal information, while other actions need more temporal aspects to recognize. Such examples of actions with fewer temporal cues are \u2019rafting\u2019 and \u2019haircut\u2019. We can judge the aforementioned actions only by seeing the scene. Contrarily, more temporal information is needed to judge an action involved in a video, e.g., \u2019zooming in with two fingers\u2019 and \u2019picking something up\u2019. With this condition in mind, one must consider having both spatiotemporal and motion information flow in the network. Current existing convolutional neural networks (CNNs) for action recognition can be categorized by the type of convolution kernel, i.e., three-dimensional (3D) and twodimensional (2D) CNN. Several researchers utilized 3D CNNs to learn both spatial and temporal information simultaneously [1\u20133]. While this approach works very well for video\nEntropy 2022, 24, 1663. https://doi.org/10.3390/e24111663 https://www.mdpi.com/journal/entropy\naction recognition tasks, the usage of a CNN-based 3D kernel certainly introduces more parameters compared with the 2D kernel type; hence, the computation cost increases. This will limit the implementation in real-time application. Additionally, 3D CNNs are typically laborious to optimize and prone to overfitting [3]. To overcome the real-time and optimization problem, recently, more researchers utilized 2D CNNs and equipped the network with temporal convolution for characterizing the temporal information [4\u20136]. TSN [7] employs a sparse temporal sampling strategy from the whole video to predict action in it, and this approach influenced several 2D CNN methods afterward. The approach by Lin et al. [6] utilized a shift-part strategy together with a sampling strategy to recognize an action very effectively. The shifting strategy is to shift part of the channels along the temporal axis in order to endow the network with movement learning. However, both approaches lack motion representations and spatiotemporal cues from different views. Meanwhile, since optical flow represents a motion as an input-level frame, many researchers take this modality into consideration. Two main drawbacks of optical flow are that it needs huge space storage and tremendous time for computation, which is not suitable for real-time application. In different approaches from previous methods, several works factorized 3D cubes of a kernel into a (1+2)D kernel configuration [8\u201310] to lessen the heavy computation. Another interesting factorization strategy was introduced by the Deep HRI team [11] in the action recognition competition. They proposed their novel architecture, coined as multi-view CNN (MV-CNN), to act as a 3D convolution and showed a profound increase in the accuracy. Figure 1 illustrates their proposed idea and shows the comparison with two other convolution designs. Assuming k is the kernel with the size of 3: (a) the kernel convolves on temporal (T) and spatial (H \u00d7W) axes simultaneously; (b) temporal and spatial feature learnings are constructed serially; and (c) multi-view feature learnings occur independently, and the resulting feature maps are aggregated with weighted average \u03b1i to produce multi-viewed feature maps. Nevertheless, these factorization strategies still neglect the importance of motion features, which are beneficial for action recognition tasks.\nBased on the aforementioned observation, we propose a novel building block, Motion and Multi-View Excitation and Temporal Aggregation (META). Specifically, META comprises three submodules: (1) Motion Excitation (ME), (2) Multi-view Excitation (MvE), and (3) Densely Connected Temporal Aggregation (DCTA). These three submodules are integrated into the 2D ResNet-50 base model, and it provides the network the ability to learn spatiotemporal and motion features altogether. The ME submodule addresses issues with optical flow problems by calculating feature-level content displacement on the fly during training or inferencing; thus, no space storage is needed. It also introduces an\ninsignificant amount of FLOPs and time to calculate compared with optical flow. The MvE submodule, on the other hand, produces enhanced spatiotemporal representation of output features. This submodule adds a multi-view perspective to the original feature maps, and we design the MvE submodule to be complementary to ME, in that output from the MvE is directly added to the output from ME. For temporal feature learning, one-dimensional (1D) convolution is suitable for such a task. In our work, we insert densely connected 1D convolution layers inside a group of subconvolutions and arrange them in a hierarchical layout to model the temporal representation and long-range temporal dependencies. In a nutshell, the main contribution of our work is as follows:\n1. We design three submodules, including ME, MvE and DCTA, to learn enriched spatiotemporal and motion representations in a very efficient way and in an end-toend manner. 2. We propose META and insert it in 2D ResNet-50 with a few additional model parameters and low computation cost. 3. We conduct extensive experiments on three popular benchmarking datasets, including Something-Something v1, Jester and Moments-in-Time Mini, and the results show the superiority of our approach."
        },
        {
            "heading": "2. Related Work",
            "text": "In this section, we discuss in more detail several related works for action recognition, including MV-CNN and TEA, which highly motivated this work. We also add a Transformer-based approach in this section because it has contributed to recent advances in computer vision, particularly for the task of action recognition. With large-scale video datasets for action recognition available publicly, more 3D CNNs are introduced for action recognition tasks. Researchers successfully implemented them and outperformed state-of-the-art methods [3,9,12\u201315]. Three-dimensional CNNs are thought to be capable of learning both spatial and temporal aspects of a video, which are a very important aspect of action recognition. Among these researchers, Carreira et al. [13] used an ImageNet [16,17] pre-trained cube-shaped \u2013N \u00d7 N \u00d7 N kernel to learn volumetric features. Another work by Hara et al. [15] used a 3D version of vanilla ResNet and proved the superiority of this architecture. Several attempts by [18\u201320] investigate the combination of a CNN and LSTM [21] or a densely connected LSTM. They claim that the LSTM layer can model the temporal relation of a series of features coming from either 2D CNNs or 3D CNNs. An alternative to LSTM for modeling temporal relationships is to use post hoc fusion [5] inside 2D CNNs. Later, a Temporal Shift Module (TSM) by Lin et al. [6] implemented a shifting operator on channel axes to learn temporal features without extra parameters and insignificant extra FLOPs. Afterwards, works proposed by [8\u201310] decomposed 3D convolution into 1D and 2D convolution to distill temporal and spatial features, respectively. This strategy was used to address the heavy computation and quadratic growth of model size when using 3D convolution. A mix of 2D and 3D CNNs in a unified architecture is also used to capture spatial and temporal features at the same time [22]. ECO [23] utilized this mixing strategy with top-heavy architecture. However, mixing the 2D CNN with the 3D CNN in one architecture will inevitably increase model parameters and require more time to optimize the parameters. Meanwhile, partially inspired by the visual system of biological studies on the retinal ganglion cells in primates, Feichtenhofer et al. [24] advocated a two-stream architecture approach. Considering a video may contain more static as well as more dynamic objects, both streams have a different temporal rate, which makes their architecture unique compared with the existing two-stream architectures. Furthermore, learning from the three anatomical planes of the human body, i.e., sagittal, coronal and transverse, the DEEP HRI team [11] tried to simulate 3D convolution in their work. A video clip can be represented as a T\u00d7 H\u00d7W volumetric data, with T, H and W denoting the number of frames, height and width, respectively. They reshape it to 2D data (i.e., T \u00d7 H, T \u00d7W and H \u00d7W) separately\nand operate a shared 2D kernel to convolve over the reshaped 2D data. Figure 2 depicts images when they are seen in different views.\nApart from previous work, SENet [25] also attracts researchers to employ its squeezeand-excitation method. This method uses a global context pooling mechanism to enhance the spatially informative channels and was verified to be effective in image understanding tasks. A recent work by Hao et al. [26] studied the insertion of channel context into the spatio-temporal attention learning block for element-wise feature refinement. In addition to spatiotemporal representation, motion information is also the key difference between action classification and image classification tasks; thus, exploiting motion features is mandatory. A comprehensive study by Sevilla-Lara et al. [27] analyzed the importance of optical flow for action recognition. They analyzed optical flow itself, conducted several experiments using optical flow modality, and proved the contribution to the accuracy. Another several attempts utilized optical flow as an additional input to RGB [7,28]. A two-stream architecture is deployed to learn both optical flow and RGB data, and an average result is generated to predict an action. Feichtenhofer et al. [29] experimented with several fusion schemes so as to enhance spatiotemporal features. While this approach demonstrates excellent results compared with RGB data alone, this approach cannot be implemented in a real-world application, as extracting the pixel-wise optical frame with the TV-L1 method [30] requires heavy computation as well as much storage space. In this light, the RGB difference was introduced [7,31], which is more lightweight. A work by Jiang et al. [32] with their STM module firstly outlined the motion calculation for end-to-end learning in a 2D ConvNet and proved to be effective in capturing instantaneous motion representation as short-range temporal evolution. Furthermore, Li et al. [33] introduced a new block termed as TEA to explore the benefits of the attention mechanism added to the motion calculation previously mentioned. Later, this attentive motion features module was adopted by [34\u201336]. In addition, the authors of TEA suggested overcoming the limitation of long-range temporal representation by introducing multiple temporal aggregations in a hierarchical design. In this work, we propose a motion calculation and a hierarchical structure of local temporal convolutions, similar to the previous work. We explain more details of our work and highlight the difference from the previous work in the subsequent section. As Vision Transformers brought recent breakthroughs in computer vision, specifically for action recognition tasks, many researchers have adopted them as their model [37\u201340] or combined them with 2D CNN [41]. For example, Arnab et al. [37] proposed several factorization variants to model spatial and temporal representation effectively inside a transformer encoder. TimeSformer [38] investigated several self-attention combinations on frame-level patches and suggested that separated attention for spatial and temporal representation applied within each block yielded the best video classification accuracy. Another work by Tian et al. [41] introduced a 2D ResNet with Transformer injected at the top layer before the linear layer to accurately aggregate extracted local cues from preceding blocks into a video representation. Although these current approaches seem promising, a\nTransformer-based network is not suited for real-world applications because it is highly computationally intensive [36]."
        },
        {
            "heading": "3. Our Proposed Method",
            "text": "This section discusses the technical details of our work. Firstly, we present a method to extract motion representations to simulate the optical flow modality. Afterward, our novel Multi-view Excitation is introduced. Lastly, a simple stacking local temporal convolution with dense connection is also discussed here as a part of our improvement strategy. We also include a short discussion regarding comparing our work and TEA. Some notations written in this section are: N, T, C, H and W, indicating batch size, number of frames, number of channels, height and width, respectively."
        },
        {
            "heading": "3.1. Motion Excitation (ME)",
            "text": "Introduced firstly by STM [32], and later enhanced by TEA [33], the motion excitation submodule performs frame difference calculation in a unified framework for end-to-end learning. In principle, motion representation indicates content displacement of two adjacent feature maps, therefore called feature-level-based motion, rather than pixel-level-based motion, as in the concept of optical flow. Figure 3 illustrates the steps to measure approximate feature-level temporal differences. The first step is to reduce the number of channels for efficiency with the ratio r = 16 by applying a 1\u00d7 1 convolution layer Kred to the initial input X, formulated in Equation (1). Then, we slice feature maps at the temporal axis, followed by element-wise subtraction for every adjacent output feature and obtain M at time step t. Before subtraction, a 3\u00d7 3 transformation convolution layer Ktrans f is applied to the output features X\u2032 at the time step (t + 1). Next, we concatenate motion representations M at all time steps according to the temporal axis with 0 padded to the last segment. Concretely, given X \u2208 RNT\u00d7C\u00d7H\u00d7W are input features to the ME submodule, the above processes are expressed as follows:\nX\u2032 = Kred \u2217 X, X\u2032 \u2208 RNT\u00d7 C r \u00d7H\u00d7W (1)\nMt = Ktrans f \u2217 X\u2032t+1 \u2212 X\u2032t, 1 \u2265 t \u2265 T \u2212 1 (2) X\u2032 = concat(Mt, 0), 1 \u2265 t \u2265 T \u2212 1, (3)\nwhere Mt \u2208 RN\u00d7 C r \u00d7H\u00d7W and the last X\u2032 \u2208 RNT\u00d7 Cr \u00d7H\u00d7W .\nAt this point, we have a new X\u2032 as approximate feature-level motion representations. Since we want to emphasize the informative features and suppress less useful ones alongside with [25], we squeeze the global information from each channel of the motion representations by utilizing the global spatial pooling layer. Then, another 1\u00d7 1 2D convolution layer Kexp performs channel expansion to restore the number of channels, and we obtain a new X, as in Equation (4). Lastly, attentive feature maps are obtained by feeding the new X to a sigmoid function \u03b4, while final outputs XME are produced from a multiplication between the initial inputs X and attentive feature maps, as defined by Equation (5).\nX = Kexp \u2217 pool(X\u2032), X \u2208 RNT\u00d7C\u00d71\u00d71 (4) XME = \u03b4(F) \u2217 X, XME \u2208 RNT\u00d7C\u00d7H\u00d7W (5)\nWhen subtracting the feature maps, we only calculate them one time: A collection of feature maps containing [2 \u223c T] timestamps minus [1 \u223c T \u2212 1]."
        },
        {
            "heading": "3.2. Densely Connected Temporal Aggregation (DCTA)",
            "text": "Previously, learning temporal relationships in the task of action recognition was achieved by repeatedly stacking local temporal layers in deep networks. Unfortunately, it raises some problems. It is considered to be harmful to the features because the optimization message transmitted from distant frames has been weakened. To alleviate such a problem, we propose the Densely Connected Temporal Aggregation submodule. We follow the Res2Net design [42] to split feature maps in channel dimension into four subgroups of convolutions separately. Each subgroup consists of temporal and spatial convolutions configured serially, while one subgroup has temporal convolution only. In addition, output features from each subgroup flow to the next convolutional block and the neighboring subgroup through a residual connection, except for one subgroup without a residuallike connection (see DCTA submodule in Figure 4 for details). Thus, the last subgroup aggregately receives refined spatiotemporal features from former subgroups. Regarding the temporal convolution, we arrange the layers in a stacked and densely connected fashion. Notably, its parameters are shared across subgroups. In this work, the number of temporal convolution layers for stacking is three, and these stacked layers are placed in three subgroups having a residual connection. More specifically, the first layer receives the encoded features from the summation of ME and MvE; the second layer receives input features from the first layer; and for the third layer, its input is formed from the summation of all the preceding layers\u2019 output features. Formally,\nX\u20320 = Ktemp \u2217 X X\u20321 = Ktemp \u2217 X\u20320 X\u2032T = Ktemp \u2217 (X\u20320 + X\u20321)  (6) where Ktemp, X, X\u2032i \u2208 RNHW\u00d7C\u00d7T , X\u2032T \u2208 RNHW\u00d7C\u00d7T denote 1D convolution with a kernel size of 3, initial input features, output features from the i\u2013th layer and the final result of the last temporal layer. We omit the necessary permutation and reshape X and X\u2032T for simplicity. After that, a 3\u00d7 3 spatial convolution follows, as stated in the previous paragraph. For all subgroups in the DCTA submodules, the process can mathematically be expressed as:\nX\u20320 = Ktemp \u2217 X X\u20321 = Kspa \u2217 X\u2032T X\u20322 = Kspa \u2217 (X\u20321 + X\u2032T) X\u20323 = Kspa \u2217 (X\u20322 + X\u2032T)  (7)\nwhere X\u2032i \u2208 R NT\u00d7 C4 \u00d7H\u00d7W , Kspa and Ktemp are output features of the i\u2013th subgroup, spatial convolution and part-shift temporal convolution from [6], respectively. Lastly, we concatenate across channel dimensions to obtain the final output features X\u2032:\nX\u2032 = concat([X\u2032i ]), i = [0, 1, 2, 3] (8)\nwhere X\u2032 \u2208 RNT\u00d7C\u00d7H\u00d7W . Notice that in Figure 4, indices of subgroups from left to right are from 0 to 3, correspondingly."
        },
        {
            "heading": "3.3. Multi-View Excitation (MvE)",
            "text": "As illustrated in Figure 5, the MvE submodule has three branches to extract beneficial information from different views, similar to that in [11]. Given an input feature X \u2208 RNT\u00d7C\u00d7H\u00d7W for branch TH, we utilize a 1\u00d7 1 2D convolution layer Kred to reduce the channel number for efficiency with a ratio of r = 16, identical to Equation (1). Then, the tensor dimension is reshaped to comply with the desired dimension, i.e., NT \u00d7 Cr \u00d7 H \u00d7 W \u2192 NW \u00d7 Cr \u00d7 T \u00d7 H. After that, a shared channel-wise convolution layer K is utilized to produce transformed feature maps X\u2032TH . Formally,\nX\u2032TH = K \u2217 X\u2032, X\u2032TH \u2208 RNW\u00d7 C r \u00d7T\u00d7H (9)\nThe last step is to reshape back the tensor dimension, i.e., NW \u00d7 Cr \u00d7 T \u00d7 H \u2192 NT \u00d7 Cr \u00d7 H \u00d7W. The rest of the branches are processed accordingly to produce X\u2032TW and X\u2032HW . If we have obtained all the outputs from the other two branches, then the new X\n\u2032 is a convex combination of the X\u2032i :\nX\u2032 = \u2211 i \u03b1i \u2217 X\u2032i , i \u2208 [TH, TW, HW] (10)\nwhere \u03b1 is a weighted average with constraints of \u2211i \u03b1i = 1 and each of the \u03b1i \u2265 0. We argue that each branch will contribute differently to the performance of the model. The rest of operations are identical to Equations (4) and (5) to obtain attentive multi-view feature maps XMvE.\nThe initial work of the multi-view design was proposed by Li et al. with their team DEEP HRI [11]. Different from their work, our work introduces the excitation algorithm to the MvE submodule so that it has a kind of attention mechanism."
        },
        {
            "heading": "3.4. Meta Block",
            "text": "For comparative purposes, we adopt 2D ResNet-50 as a backbone like other state-ofthe-art methods [33\u201335]. As shown in Figure 6, each \u201cconv2\u201d in all residual blocks (conv2_x until conv5_x) is replaced by META. In total, we insert 16 blocks of META to endow the network with the ability to learn both spatiotemporal and motion representations efficiently. When feeding feature maps to the DCTA submodule, we sum all of the output features generated from ME, MvE and the former convolution block (denoted by XME, XMvE and X, respectively) to obtain X\u2032.\nX\u2032 = XME + XMvE + X, X\u2032 \u2208 RNT\u00d7C\u00d7H\u00d7W (11)"
        },
        {
            "heading": "3.5. Discussion with TEA",
            "text": "We want to highlight the differences between our work and TEA in this subsection. TEA adopted feature-level motion representation and enhanced it by excitation strategy with negligible extramodel parameters. Unlike TEA, which only considers X in parallel with the output of ME, we also added output features from the MvE submodule, as in Equation (11). Moreover, the network enjoys richer spatiotemporal and motion representation features since we re-calibrate the features by both ME and MvE submodules. Regarding temporal aggregation inside the Res2Net module, TEA adopted it to enable their network to model the long-range spatiotemporal relationship by adding a local temporal convolution to each subgroup of convolution. However, in our work, we also added local temporal convolutions in each subgroup of convolution and arranged them in a stacked up and densely connected manner."
        },
        {
            "heading": "4. Experiment and Evaluation",
            "text": "In the following section, we explain our experiments in detail. Firstly, we describe the datasets we used and explain how we implement training and testing strategies, including hyperparameter settings. We also perform certain ablation experiments to investigate the contribution of each component of META. Later, we present the results and analysis along with the discussion."
        },
        {
            "heading": "4.1. Datasets",
            "text": "Our proposed method is evaluated on three large-scale action recognition benchmark datasets, i.e., Something-Something v1, Jester and Moments-in-Time Mini. An action classification on the Something-Something v1 [44], a motion-centric type of dataset with 174 classes, requires temporal understanding to classify an action. This dataset is designed to emphasize the interaction between human and object, for example, \u201cThrowing something\u201d and \u201cThrowing something in the air and catching it\u201d. It contains 108,499 videos, with 86,017 in the training set and 11,522 in the validation set. Jester [45], which is also considered a temporal-related dataset, consists of 118,562 training videos, 14,787 validation videos and fewer categories than the Something-Something v1 dataset, i.e., 27. Example actions are \u201cSwiping up\u201d and \u201cZooming out with two fingers\u201d. The Moments-in-Time Mini dataset [46] is a large-scale human-annotated collection of one hundred thousand short videos corresponding to dynamic events unfolding within three seconds; \u201cboxing\u201d and \u201crepairing\u201d are the two examples of categories. This dataset provides 100,000 videos for training and 10,000 for validation. It involves 200 action categories and offers a balanced\nnumber of videos in each category. While the previous datasets are more temporal-related, the Moments-in-Time Mini dataset can be considered both a temporal- and scene-related dataset. Frames have already been extracted from all videos in the Something-Something v1 and Jester datasets when they are made publicly available. However, in the Moments-inTime Mini dataset, we must extract RGB frames from the videos at 30 frames per second at a resolution of 256 by 256. Figure 7 shows some images with their classes for the aforementioned datasets.\n4.2. Implementation Details 4.2.1. Training\nWe conduct all experiments on one Nvidia Quadro P6000 GPU card with PyTorch as the deep learning framework. We follow a sparse sampling strategy by TSN [7]. We extract T frames randomly from a number of evenly divided segments (in all our experiments, T = 8). Selected frames go through the network, and simple temporal pooling strategy is utilized to averagely predict an action for an entire video. Random scaling and cropping are applied as data augmentation. The size of the shorter side of the frame is cropped to 256 and resized to 224\u00d7 224 to serve as the final frame size; hence, the final input shape is NT \u00d7 3\u00d7 224\u00d7 224. Before the training started, we loaded our base model with weights trained on the popular ImageNet dataset [16,17]. As we adopt the Res2Net module for residing the DCTA submodule, we select the publicly available res2net50 26w 4s (available at: https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_26w_4s-06e791 81.pth, accessed on 2 February 2022) pre-trained weights. Regarding the hyperparameters for the Something-Something v1 and Moments-inTime Mini datasets, the batch size, initial learning rate and dropout rate are set to 8, 0.0025 and 0.5, respectively. Moreover, the learning rates are decreased by factors of 10 at 30, 40 and 45 epochs and stops at 50 epochs. For the Jester dataset, the model is optimized for 30 epochs and the dropout is set to 0.5. Then, a learning rate is started at 0.0025 and reduced by factors of 10 at 10, 20 and 25 epochs. In addition, we unfreeze all instances of batch normalization layers during training. For the network optimizer, we select SGD with a momentum of 0.9 and a weight decay of 5\u00d7 10\u22124. When setting the learning rate and weight decay for the classification layer on the three datasets above, we follow [33], i.e., 5\u00d7 higher than other layers. As a final thought, as suggested by [47], the learning rate must be matched with the batch size, i.e., the corresponding learning rate must be 2\u00d7 higher when the batch size is scaled up by two. For example, if the learning rate changes from 2.5 \u00d7 10\u22123 to 5\u00d7 10\u22123, then batch size should increase from 8 to 16.\n4.2.2. Testing\nWe follow settings from [33] to adopt two methods as testing protocols: (1) efficient protocol, with frames \u00d7 crops \u00d7 clips is 8 \u00d7 1 \u00d7 1 and cropped 224 \u00d7 224 at central region as final frame size; and (2) accuracy protocol, with frames \u00d7 crops \u00d7 clips is 8 \u00d7 3 \u00d7 10, full resolution images (256 \u00d7 256 for final input size for frames) and averaged softmax scores for all clips for final prediction. When comparing with other recent works, we apply the accuracy protocol, as in Tables 1 and 2. For the Moments-in-Time Mini dataset, as in Table 3, we apply the efficient protocol."
        },
        {
            "heading": "4.3. Results on Benchmarking Datasets",
            "text": "We report our experimental results and compare them with state-of-the-art methods. We list TSM to act as the baseline for Tables 1 and 2. Since META is designed to function on CNN-based networks, we primarily compare our work with others whose networks are the same type as META to make relevant comparisons. Nevertheless, we still include recent Transformer-based networks in our comparison to demonstrate that META can achieve competitive accuracy while still being lightweight. We also include some successful predictions of META compared with other works on the three datasets we used.\n4.3.1. Something-Something V1\nSomething-Something v1 can be categorized as a temporal-related dataset; thus, ME and DCTA play important roles here. We divide Table 1 into four compartments; the upper part contains 3D CNNs, followed by 2D-based CNNs, Transformer networks and lastly, our model.\n1 Not counting Latent Motion Code (LMC) module parameters. 2 Re-counted using official public code for digit precision. 3 Our implementation using official public code.\nAccording to the table, our methods outperform the baseline for the 8 \u00d7 1 \u00d71 layout by sizable margins of 4.5% and 4.3% for top-1 and top-5 accuracy, respectively, while the FLOPs are only 1.08\u00d7 higher. Our work is superior in that it significantly outperforms the baseline method\u2019s 16 frames with a 2.9% accuracy improvement and low FLOPs, even when only eight frames and the one clip\u2013one crop methodology are used. For the methods listed in the first compartment, we outperformed their work significantly. We considerably outscored I3D NL+GCN by only using eight frames, about 17\u00d7 fewer FLOPs and fewer than half the parameters the I3D network used. A more competitive result is shown in the third compartment, where we outperform all current state-of-the-art methods in terms of top-1 accuracy. The nearest score to ours is TEA, where we obtain a substantially higher margin (52.1% vs. 51.7%), except top-5 accuracy is 0.3% lower (80.2% vs. 80.5%) when employing 10 clips. For comparison with SMNet [36], a more recent work, we noticeably outperform their work by big margins of 2.3% and 0.6% for top-1 and top-5 accuracy, respectively. This definitely demonstrates our superior submodules of MvE and DCTA combined with ME, considering SMNet also equipped their network with motion encoding. When comparing our work with recent Transformer-based state-of-the-art methods, however, META is inferior to those methods presented in the middle part. Without considering FLOPs and the number of parameters, META is 3.3% less accurate than UniFormer-B in terms of top-1 accuracy and 1.3% lower than EAN RGB+LMC. According to [48], Transformer strength comes from its architecture, which was built to aggregate global information earlier due to self-attention. In addition to striking differences in architecture concept, we found that UniFormer used Kinetics as its pre-trained model, while we only pre-trained our model from ImageNet. Moreover, our model takes eight frames as the input image, while the Transformer-based models require more frames than us to serve as an input image. Figure 8 shows a visual comparison of CNN-based techniques using a ball chart. We report the top-1 accuracy with respect to floating-point operations in gigabyte (GFLOPs). Accuracies are calculated using only center crop and single forward pass unless otherwise specified. The plot demonstrates how we consistently excel over comparable works while keeping FLOPs to a minimum level (only 1.08\u00d7 as many as TSM). For our method and TEA, we find that total accuracy may be improved by a factor of \u00b11.05, at the expense of computational costs that increase to well over a thousand GFLOPs. The plot shows that overall, 2D CNNs may outperform 3D CNNs when the 2D-based network is provided with sufficient temporal feature learning.\n4.3.2. Jester\nLikewise, Jester is classified as a temporal-related dataset. Our experiment result is provided in Table 2. Clearly, our work demonstrates superiority on the Jester dataset in terms of top-1 accuracy compared with the baseline (97.1% vs. 97.0%) and other state-ofthe-art methods, except for ACTION-NET, where we obtained the same accuracy. Our interesting finding according to this table is: Both META and ACTION-NET, which are equipped with a motion representation module, achieved only a slightly higher accuracy than TSM (40.1% of accuracy) without a motion representation module in it. Though, admittedly, we need further experiments to verify this, we think that motion encoding may have less meaning for this dataset. Moreover, results from TEA and MEST confirm our thoughts, as both methods proposed this module, and the performance is inferior compared with ours ({96.5%, 96.6%} vs. 97.1%).\nDifferent from the previous dataset comparison, where our work has lesser predictive top-1 and top-5 accuracies than the methods utilizing Transformer, our work demonstrates very competitive results on the Jester dataset. META barely falls short of DirecFormer\u2019s top-1 accuracy by 1.1% but surprisingly achieves a slightly higher top-5 accuracy (40.2%). With the other two Transformer-based methods, we constantly outperform their works in top-1 and top-5 accuracies with significant gaps, proving the superiority of our work.\n4.3.3. Moments-In-Time Mini\nUnlike the above datasets, this dataset possesses characteristics of temporal-related and scene-related datasets. The performance of our proposed work is still impressive, and Table 3 confirms this. We achieved the highest accuracy in terms of top-5 accuracy. While we obtain lower top-1 accuracy compared with IR-Kinetics400, we want to emphasize that IRKinetics400 utilized a Kinetics-400 [13] dataset as their pre-trained weights, whereas we only used ImageNet pre-trained weights. The closest accuracy to META is from I3D-DenseLSTM (\u22060.9% of top-1 acc.), where in their work, they utilized optical flow modality for encoding motion representations and LSTM to model long-range temporal representation, similar to META. Obviously, META is more efficient than I3D-DenseLSTM, as we estimate motion representations in a unified framework.\n4.3.4. Example of Successful Predictions\nWe illustrate some accurate predictions of META over other works in Figure 9. To obtain the probability score in (a), we re-train the model using the official code publicly available (https://github.com/Phoenix1327/tea-action-recognition) (accessed on 11 July 2022), whereas we only load the model with the official weights (https://github.com/VSense/ACTION-Net) (accessed on 11 July 2022) for (b). In all scenarios, we confidently achieve top-1 accuracy (indicated by a number in parenthesis) with substantial difference in probability score, whereas other works rank below ours. For instance, (a) informs that META exactly predict an action of \u201cLifting something with something on it\u201d while TEA measures such action 8th out of the softmax outputs in descending order. This fact demonstrates our predominance over existing related works in three datasets.\n4.3.5. Learning Curve Analysis\nDuring model training, we generate log statements of accuracies for each iteration and save them in a plain text file for further analysis. Figure 10 shows a training visualization in terms of top-1 and top-5 accuracies, with one crop\u2013one clip for both training phase and inference. It is clear from the visualization that after 50 training epochs the model has not improved. Meanwhile, at epoch 30, the accuracy graph turned upward. This is due to our strategy of changing the learning rate at the epoch with our optimization method SGD."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "We perform some evaluations of our META comprehensively on the SomethingSomething v1 validation dataset and report the result in this subsection. All experiments utilize ImageNet pre-trained weights and are conducted using the efficient protocol. TSM serves as our baseline.\n1. Impact of each module We examine how each submodule affects the performance and present the findings in Table 4. It is clear that, in comparison with the baseline, each submodule continuously improves the performance of the 2D ResNet on video action recognition. The DCTA submodule makes the most contribution, improving top-1 accuracy by 2.4% while being computationally efficient with only a 1.7 G overhead gap and the least number of parameters, whereas the other two add 2.0 G of extra FLOPs. 2. Location of META We examine the number of META implemented inside four convolution blocks toward accuracy. From Table 5, it is evident that better precision can be attained with more profound METAs placed in convolution blocks. Interestingly, META only requires installing one convolution block to dramatically increase the performance, with top-1 and top-5 accuracies exceeding the baseline by 2.6% and 2.9%, respectively."
        },
        {
            "heading": "5. Conclusions",
            "text": "This paper presents a novel building block to overcome the existing problems for the video action recognition task by designing three submodules to construct a META block and integrating it into each residual block of 2D ResNet-50. The proposed block includes excitation of motion and multi-view features followed by densely connected temporal aggregation. While retaining modest computations, our META achieves competitive results on three large-scale datasets compared with its 2D/3D CNN counterparts. Compared with recent Transformer-based networks, our work still achieves competitive results on the Jester dataset, while being inferior on the Something-Something v1 dataset. In the future, we would like to investigate another fusion approach, i.e., channel concatenation in the DCTA submodule, so that all layers are connected, and the current input is the concatenation of the preceding layers. This fusion will guarantee that new information is added to the collective knowledge.\nAuthor Contributions: Conceptualization, Y.Y.J. and M.A.; software, Y.Y.J.; data curation, Y.Y.J.; writing\u2014Original draft, Y.Y.J.; writing\u2014Review and editing, M.A.; visualization, Y.Y.J.; funding acquisition, M.A.; supervision, M.A. All authors have read and agreed to the published version of the manuscript.\nFunding: This work is partially supported by Grant-in-Aid for Scientific Research (C), issue number 22K12040.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Somthing-Something v1: Not applicable; Jester is available at https:// developer.qualcomm.com/software/ai-datasets/jester (accessed on 2 February 2022); Moments-inTime Mini: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest."
        }
    ],
    "title": "Video Action Recognition Using Motion and Multi-View Excitation with Temporal Aggregation",
    "year": 2022
}