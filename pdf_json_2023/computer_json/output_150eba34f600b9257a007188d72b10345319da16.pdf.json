{
    "abstractText": "Semantic localization, i.e., robot self-localization with semantic image modality, is critical in recently emerging embodied AI applications (e.g., point-goal navigation, objectgoal navigation, vision language navigation) and topological mapping applications (e.g., graph neural SLAM, ego-centric topological map). However, most existing works on semantic localization focus on passive vision tasks without viewpoint planning, or rely on additional rich modalities (e.g., depth measurements). Thus, the problem is largely unsolved. In this work, we explore a lightweight, entirely CPU-based, domain-adaptive semantic localization framework, called graph neural localizer. Our approach is inspired by two recently emerging technologies: (1) Scene graph, which combines the viewpointand appearanceinvariance of local and global features; (2) Graph neural network, which enables direct learning/recognition of graph data (i.e., non-vector data). Specifically, a graph convolutional neural network is first trained as a scene graph classifier for passive vision, and then its knowledge is transferred to a reinforcement-learning planner for active vision. Experiments on two scenarios, self-supervised learning and unsupervised domain adaptation, using a photo-realistic Habitat simulator validate the effectiveness of the proposed method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mitsuki Yoshida"
        },
        {
            "affiliations": [],
            "name": "Kanji Tanaka"
        },
        {
            "affiliations": [],
            "name": "Ryogo Yamamoto"
        },
        {
            "affiliations": [],
            "name": "Daiki Iwata"
        }
    ],
    "id": "SP:fad9ce2f68784c09299a4b2f3feec0f96fb62216",
    "references": [
        {
            "authors": [
                "S.S. Desai",
                "S. Lee"
            ],
            "title": "Auxiliary tasks for efficient learning of pointgoal navigation",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 717\u2013725.",
            "year": 2021
        },
        {
            "authors": [
                "J. Ye",
                "D. Batra",
                "E. Wijmans",
                "A. Das"
            ],
            "title": "Auxiliary tasks speed up learning point goal navigation",
            "venue": "Conference on Robot Learning. PMLR, 2021, pp. 498\u2013516.",
            "year": 2021
        },
        {
            "authors": [
                "D.S. Chaplot",
                "D.P. Gandhi",
                "A. Gupta",
                "R.R. Salakhutdinov"
            ],
            "title": "Object goal navigation using goal-oriented semantic exploration",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 4247\u20134258, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Wang",
                "W. Wang",
                "W. Liang",
                "C. Xiong",
                "J. Shen"
            ],
            "title": "Structured scene memory for vision-language navigation",
            "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, 2021, pp. 8455\u20138464.",
            "year": 2021
        },
        {
            "authors": [
                "S. Datta",
                "O. Maksymets",
                "J. Hoffman",
                "S. Lee",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Integrating egocentric localization for more realistic point-goal navigation agents",
            "venue": "Conference on Robot Learning. PMLR, 2021, pp. 313\u2013328.",
            "year": 2021
        },
        {
            "authors": [
                "J.L. Sch\u00f6nberger",
                "M. Pollefeys",
                "A. Geiger",
                "T. Sattler"
            ],
            "title": "Semantic visual localization",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6896\u20136906.",
            "year": 2018
        },
        {
            "authors": [
                "C. Toft",
                "C. Olsson",
                "F. Kahl"
            ],
            "title": "Long-term 3d localization and pose from semantic labellings",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 2017, pp. 650\u2013659.",
            "year": 2017
        },
        {
            "authors": [
                "M. Mancini",
                "S.R. Bulo",
                "E. Ricci",
                "B. Caputo"
            ],
            "title": "Learning deep nbnn representations for robust place categorization",
            "venue": "IEEE Robotics and Automation Letters, vol. 2, no. 3, pp. 1794\u20131801, 2017.",
            "year": 1801
        },
        {
            "authors": [
                "D.S. Chaplot",
                "E. Parisotto",
                "R. Salakhutdinov"
            ],
            "title": "Active neural localization",
            "venue": "arXiv preprint arXiv:1801.08214, 2018.",
            "year": 1801
        },
        {
            "authors": [
                "K. Kim",
                "H. Choi",
                "S. Yoon",
                "K. Lee",
                "H. Ryu",
                "C. Woo",
                "Y. Kwak"
            ],
            "title": "Development of docking system for mobile robots using cheap infrared sensors",
            "venue": "Proceedings of the 1st International Conference on Sensing Technology. Citeseer, 2005, pp. 287\u2013291.",
            "year": 2005
        },
        {
            "authors": [
                "P. Xu",
                "X. Chang",
                "L. Guo",
                "P.-Y. Huang",
                "X. Chen",
                "A.G. Hauptmann"
            ],
            "title": "A survey of scene graph: Generation and application",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst, vol. 1, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wu",
                "S. Pan",
                "F. Chen",
                "G. Long",
                "C. Zhang",
                "S.Y. Philip"
            ],
            "title": "A comprehensive survey on graph neural networks",
            "venue": "IEEE transactions on neural networks and learning systems, vol. 32, no. 1, pp. 4\u201324, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Kurauchi",
                "K. Tanaka",
                "R. Yamamoto",
                "M. Yoshida"
            ],
            "title": "Active domain-invariant self-localization using ego-centric and world-centric maps",
            "venue": "Computer Vision and Machine Intelligence, M. Tistarelli, S. R. Dubey, S. K. Singh, and X. Jiang, Eds. Singapore: Springer Nature Singapore, 2023, pp. 475\u2013487.",
            "year": 2023
        },
        {
            "authors": [
                "G. Zhu",
                "L. Zhang",
                "Y. Jiang",
                "Y. Dang",
                "H. Hou",
                "P. Shen",
                "M. Feng",
                "X. Zhao",
                "Q. Miao",
                "S.A.A. Shah"
            ],
            "title": "Scene graph generation: A comprehensive survey",
            "venue": "arXiv preprint arXiv:2201.00443, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G.V. Cormack",
                "C.L. Clarke",
                "S. Buettcher"
            ],
            "title": "Reciprocal rank fusion outperforms condorcet and individual rank learning methods",
            "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, 2009, pp. 758\u2013 759.",
            "year": 2009
        },
        {
            "authors": [
                "D. Shah",
                "Q. Xie"
            ],
            "title": "Q-learning with nearest neighbors",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X. Liu",
                "F. Zhang",
                "Z. Hou",
                "L. Mian",
                "Z. Wang",
                "J. Zhang",
                "J. Tang"
            ],
            "title": "Self-supervised learning: Generative or contrastive",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 1, pp. 857\u2013876, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Xu",
                "Z. Zeng",
                "C. Lian",
                "Z. Ding"
            ],
            "title": "Few-shot domain adaptation via mixup optimal transport",
            "venue": "IEEE Transactions on Image Processing, vol. 31, pp. 2518\u20132528, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Szot",
                "A. Clegg",
                "E. Undersander",
                "E. Wijmans",
                "Y. Zhao",
                "J. Turner",
                "N. Maestre",
                "M. Mukadam",
                "D. Chaplot",
                "O. Maksymets",
                "A. Gokaslan",
                "V. Vondrus",
                "S. Dharur",
                "F. Meier",
                "W. Galuba",
                "A. Chang",
                "Z. Kira",
                "V. Koltun",
                "J. Malik",
                "M. Savva",
                "D. Batra"
            ],
            "title": "Habitat 2.0: Training home assistants to rearrange their habitat",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Masone",
                "B. Caputo"
            ],
            "title": "A survey on deep visual place recognition",
            "venue": "IEEE Access, vol. 9, pp. 19 516\u201319 547, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Cummins",
                "P. Newman"
            ],
            "title": "Appearance-only slam at large scale with fab-map 2.0",
            "venue": "The International Journal of Robotics Research, vol. 30, no. 9, pp. 1100\u20131123, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "F. Boniardi",
                "A. Valada",
                "R. Mohan",
                "T. Caselitz",
                "W. Burgard"
            ],
            "title": "Robot localization in floor plans using a room layout edge extraction network",
            "venue": "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019, pp. 5291\u20135297.",
            "year": 2019
        },
        {
            "authors": [
                "T. Weyand",
                "I. Kostrikov",
                "J. Philbin"
            ],
            "title": "Planet-photo geolocation with convolutional neural networks",
            "venue": "Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14. Springer, 2016, pp. 37\u201355.",
            "year": 2016
        },
        {
            "authors": [
                "N. Mo",
                "W. Gan",
                "N. Yokoya",
                "S. Chen"
            ],
            "title": "Es6d: A computation efficient and symmetry-aware 6d pose regression framework",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 6718\u20136727.",
            "year": 2022
        },
        {
            "authors": [
                "K.A. Tsintotas",
                "L. Bampis",
                "A. Gasteratos"
            ],
            "title": "The revisiting problem in simultaneous localization and mapping: A survey on visual loop closure detection",
            "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 11, pp. 19 929\u201319 953, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Kim",
                "B. Park",
                "A. Kim"
            ],
            "title": "1-day learning, 1-year localization: Long-term lidar localization using scan context image",
            "venue": "IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 1948\u20131955, 2019.",
            "year": 1948
        },
        {
            "authors": [
                "S. Lowry",
                "N. S\u00fcnderhauf",
                "P. Newman",
                "J.J. Leonard",
                "D. Cox",
                "P. Corke",
                "M.J. Milford"
            ],
            "title": "Visual place recognition: A survey",
            "venue": "ieee transactions on robotics, vol. 32, no. 1, pp. 1\u201319, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "E. Garcia-Fidalgo",
                "A. Ortiz"
            ],
            "title": "ibow-lcd: An appearance-based loopclosure detection approach using incremental bags of binary words",
            "venue": "IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3051\u20133057, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "F. Bonin-Font",
                "A. Burguera Burguera"
            ],
            "title": "Nethaloc: A learned global image descriptor for loop closing in underwater visual slam",
            "venue": "Expert Systems, vol. 38, no. 2, p. e12635, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. S\u00fcnderhauf",
                "S. Shirazi",
                "F. Dayoub",
                "B. Upcroft",
                "M. Milford"
            ],
            "title": "On the performance of convnet features for place recognition",
            "venue": "2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2015, pp. 4297\u20134304.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Song",
                "M. Soleymani"
            ],
            "title": "Polysemous visual-semantic embedding for cross-modal retrieval",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1979\u2013 1988.",
            "year": 2019
        },
        {
            "authors": [
                "B. Zhou",
                "P. Kr\u00e4henb\u00fchl"
            ],
            "title": "Cross-view transformers for real-time map-view semantic segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 13 760\u201313 769.",
            "year": 2022
        },
        {
            "authors": [
                "L. Wang",
                "D. Li",
                "H. Liu",
                "J. Peng",
                "L. Tian",
                "Y. Shan"
            ],
            "title": "Cross-dataset collaborative learning for semantic segmentation in autonomous driving",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 3, 2022, pp. 2487\u20132494.",
            "year": 2022
        },
        {
            "authors": [
                "S.K. Gottipati",
                "K. Seo",
                "D. Bhatt",
                "V. Mai",
                "K. Murthy",
                "L. Paull"
            ],
            "title": "Deep active localization",
            "venue": "IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 4394\u20134401, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "M. Ragab",
                "E. Eldele",
                "W.L. Tan",
                "C.-S. Foo",
                "Z. Chen",
                "M. Wu",
                "C.- K. Kwoh",
                "X. Li"
            ],
            "title": "Adatime: A benchmarking suite for domain adaptation on time series data",
            "venue": "arXiv preprint arXiv:2203.08321, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Zhou",
                "H. Zhao",
                "X. Puig",
                "S. Fidler",
                "A. Barriuso",
                "A. Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 633\u2013 641.",
            "year": 2017
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "H. Zhao",
                "J. Shi",
                "X. Qi",
                "X. Wang",
                "J. Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881\u20132890.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Cao",
                "C. Wang",
                "Z. Li",
                "L. Zhang",
                "L. Zhang"
            ],
            "title": "Spatial-bag-offeatures",
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010, pp. 3352\u20133359.",
            "year": 2010
        },
        {
            "authors": [
                "T. Ohta",
                "K. Tanaka",
                "R. Yamamoto"
            ],
            "title": "Scene graph descriptors for visual place classification from noisy scene data",
            "venue": "ICT Express, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "M. Wang",
                "L. Yu",
                "D. Zheng",
                "Q. Gan",
                "Y. Gai",
                "Z. Ye",
                "M. Li",
                "J. Zhou",
                "Q. Huang",
                "C. Ma",
                "Z. Huang",
                "Q. Guo",
                "H. Zhang",
                "H. Lin",
                "J. Zhao",
                "J. Li",
                "A.J. Smola",
                "Z. Zhang"
            ],
            "title": "Deep graph library: Towards efficient and scalable deep learning on graphs",
            "venue": "CoRR, vol. abs/1909.01315, 2019. [Online]. Available: http://arxiv.org/abs/1909.01315",
            "year": 1909
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM, vol. 60, no. 6, pp. 84\u201390, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Two-stream convolutional networks for action recognition in videos",
            "venue": "Advances in neural information processing systems, vol. 27, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Roy",
                "S. Todorovic"
            ],
            "title": "Monocular depth estimation using neural regression forest",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 5506\u20135514.",
            "year": 2016
        },
        {
            "authors": [
                "O. Kurland",
                "J.S. Culpepper"
            ],
            "title": "Fusion in information retrieval: Sigir 2018 half-day tutorial",
            "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, 2018, pp. 1383\u20131386.",
            "year": 2018
        },
        {
            "authors": [
                "G. Huang"
            ],
            "title": "Particle filtering with analytically guided sampling",
            "venue": "Advanced Robotics, vol. 31, no. 17, pp. 932\u2013945, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Kemker",
                "M. McClure",
                "A. Abitino",
                "T. Hayes",
                "C. Kanan"
            ],
            "title": "Measuring catastrophic forgetting in neural networks",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Laine",
                "T. Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "arXiv preprint arXiv:1610.02242, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "K. Aiba",
                "K. Tanaka",
                "R. Yamamoto"
            ],
            "title": "Detecting landmark misrecognition in pose-graph SLAM via minimum cost multicuts",
            "venue": "IEEE 9th International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications, CIVEMSA 2022, Chemnitz, Germany, June 15-17, 2022. IEEE, 2022, pp. 1\u20135. [Online]. Available: https://doi.org/10.1109/CIVEMSA53371. 2022.9853684",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 5.\n06 14\n1v 5\n[ cs\n.C V\n] 2\n6 D\nec 2\n02 3\nIndex Terms\u2014 graph neural embeddings, active semantic localization, knowledge transfer, domain adaptation\nI. INTRODUCTION\nSemantic localization, i.e., robot self-localization with semantic image modality, is critical in recently emerging embodied AI applications [1] (e.g., point-goal navigation [2], object-goal navigation [3], vision language navigation [4]) and topological mapping applications (e.g., graph neural SLAM, ego-centric topological map). It has become clear that the performance of semantic localization is the bottleneck in these navigation applications [5].\nMost existing works on semantic localization focus on passive vision tasks without viewpoint planning [6], or rely on additional rich modalities (e.g., depth measurements) [7]. Thus, the problem is largely unsolved. Furthermore, current methods rely on expensive GPU environments to train the deep learning models for state recognition [8] and action planning [9]. This limits their application domains and for example, it makes them unapplicable to lightweight robotics applications such as household personal robots [10].\nMotivated by these challenges, in this work we present a lightweight, entirely CPU-based, domain-adaptive semantic localization framework, called graph neural localizer. Our approach is inspired by two recently emerging technologies: (1) Scene graph [11], which combines the viewpoint- and appearance- invariance of local and global features; (2)\n\u2217M. Yoshida, K. Tanaka, R. Yamamoto, and D. Iwata are with Department of Engineering, University of Fukui, Japan. tnkknj@u-fukui.ac.jp\nGraph neural network [12], which enables direct learning/recognition of graph data (i.e., non-vector data). Specifically, a graph convolutional neural network is first trained as a scene graph classifier for passive vision, and then its knowledge is transferred to a next-best-view planner for active vision. Although such a knowledge transfer task from passive to active self-localization has been recently addressed for conventional models [13], it has not been sufficiently explored for the recently-emerging graph neural network models. To this end, detailed implementation issues including scene parsing [14], unsupervised knowledge transfer [15], and efficient reinforcement-learning [16] are implemented, evaluated and discussed. Experiments on two scenarios, selfsupervised learning [17] and unsupervised domain adaptation [18], using a photo-realistic Habitat simulator [19] validate the effectiveness of the proposed method.\nThe contributions of this research are summarized as follows: (1) An entirely CPU-based lightweight framework for solving semantic localization in both active and passive vision tasks is presented. (2) A scene graph and a graph neural network are combined for the first time to solve semantic localization, presenting a new graph neural localizer framework. (3) Experiments show that the proposed method outperforms the baseline method in terms of self-localization performance, computational efficiency, and domain adaptation."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Visual robot self-localization has been extensively studied in various formulations [20] such as image retrieval [21], multi-hypothesis tracking [22], geometric matching [6], place classification [23], and viewpoint regression [24]. It is also closely related to the task of loop closure detection [25], an essential component of visual robot mapping. This work focuses on the classification formulation, where supervised and self-supervised learning of image classifier are directly applicable and have become a predominant approach [26].\nExisting self-localization approaches are broadly divided into two approaches, local and global feature [27], depending on the type of scene model. The local feature approach\ndescribes an image in a viewpoint-invariant manner using a \u201cbag\u201d of keypoint-level features [28]. The global feature approach describes an image in an appearance-invariant manner using a single image-level feature [29]. There is also a hybrid approach called part feature [30], in which scenes are described in a viewpoint-invariant and appearanceinvariant manner using subimage-level features. The scene graph model [11] used in this research can be viewed as an extension of the part feature, in that it can describe not only part features by graph nodes, but also the relationships between parts by graph edges.\nAppearance features, manually designed or deeply learned, are a predominant approach for in-domain self-localization [25]. In contrast, the semantic feature used in this study has proven to be advantageous in cross-domain setups (e.g., cross-modality [31], cross-view [32], cross-dataset [33]) as in [6], which was enabled by recent advances in deep semantic imaging technology [33].\nMost existing works on semantic localization focus on passive self-localization tasks without viewpoint planning [6], or rely on additional rich modalities (e.g., 3d point clouds) [7]. In contrast, in our framework, both the passive and active self-localization tasks are addressed, and both training and deployment stages do not rely on other modalities. Note that semantic localization can be ill-posed for a passive observer, as many viewpoints may not provide any discriminative semantic feature. The active self-localization aims to adapt the observer\u2019s viewpoint trajectory, avoiding non-salient scenes that do not provide a landmark view or moving efficiently toward places that are likely to be most informative with the aim of reducing sensing and computation costs. Most existing works on active localization focus on rich modalities such as RGB image [34] and 3D point clouds [9]. In contrast, the issue of active semantic localization has not been sufficiently explored.\nActive self-localization is also related to other active vision tasks, including first-person-view semantic navigation (e.g., point-goal navigation [2], vision-language navigation [4], object-goal navigation [3]), which has recently emerged in the robotics and vision communities. However, most existing works assume the availability of ground-truth viewpoint information during training episodes. In contrast, the availability of such rich ground-truth is not assumed in this work, but only a sparse reward given upon successful localization is assumed as supervision.\nAs described above, the issues of visual place classification, scene graph, graph neural networks, and active vision have been researched independently. This work is the first to bring together these approaches to address the emerging issue of semantic localization."
        },
        {
            "heading": "III. PROBLEM",
            "text": "The problem of active self-localization is formulated as an instance of discrete-time discounted Markov decision process (MDP) where agents (i.e., robots) interact with probabilistic environments [35]. A time-discounted MDP is a general formulation, consisting of the state set S, the action set A,\nthe state transition distribution P, the reward function R, and the discount rate \u03b3 . In our specific scenario, S is a set of embeddings of first-person-view images, A is a set of possible rotating r or forward f motions, and R is the reward given upon successful localization.\nThe application domain of active self-localization covers various sensor modalities (e.g., monocular cameras, LRF, sonar), workspaces (e.g., university campus, office, underwater, airborne) and various types of domains (e.g., object placement, viewpoint trajectories, weathers, lighting conditions). Nevertheless, rich training environments are provided only for very limited application scenarios. The Habitat simulator [19], which provides photo-realistic RGB images, is one of such valuable training environments in embodied AI. Therefore, it is employed in this work.\nSemantic images are generated from the RGB images by using an independent off-the-shelf model for semantic segmentation, as detailed in Section IV-A. Although ideal low-noise semantic images could be provided by the Habitat simulator, we do not rely on them to focus on the more realistic setting.\nPerformance of an active vision system should be measured in terms of its generalization ability [36]. To do so, it is important to have a domain gap between the training and test domains. In this work, we consider a specific type of cross-domain scenario called \u201ccross-view localization\u201d where domain gap is defined as the difference in the distribution of start viewpoints between the training and deployment domains. Specifically, the amount of domain gap is controlled by two preset parameters, called location difference Txy and bearing difference T\u03b8 . Given Txy and T\u03b8 , the training and test sets of start viewpoints are sampled in the following procedure. First, the test set {(xitest , y i test , \u03b8 i test )} is uniformly sampled from the robot workspace. Then, the training samples {(x,y,\u03b8 )} is sampled by iteratively sampling a location (x,y) that satisfies the condition ((xitest \u2212 x) 2+(yitest \u2212 y) 2)1/2>Txy, and then checking the bearing condition | 6 (\u03b8 itest\u2212\u03b8 )|>T\u03b8 between it and its nearest-neighbor test sample.\nIn experiments, 25 different settings of location and bearing difference (Txy,T\u03b8 ), which are combinations of 5 different settings of location difference Txy (\u2208{0.2, 0.4, 0.6, 0.8, 1.0}), and 5 different settings of bearing difference T\u03b8 (\u2208 {10, 30, 50, 70, 90}) will be considered."
        },
        {
            "heading": "IV. APPROACH",
            "text": "Figure 1 shows the semantic localization framework. It consists of three main modules: (1) a scene parser module that parses an image into a semantic scene graph, (2) a graph embedding module that embeds a semantic scene graph into a state vector, and (3) an action planner module that maps the state vector to an action plan. Each module is detailed in the subsequent subsections."
        },
        {
            "heading": "A. Semantic Scene Graph",
            "text": "Similarity-preserving image-to-graph mapping is one of most essential requirements for scene graph embedding used by an active vision. It is particularly necessary for the agent\u2019s similar behavior to be reproduced in different domains. Most existing models such as deep learning -based scene graph generation [14] are not aimed at localization applications and thus do not have the similarity preserving ability.\nAs an alternative, we employ a conservative two-step heuristics [14], which consists of (1) detecting part regions (i.e., nodes), and (2) inferring inter-part relationships (i.e., edges). For the node generation step, an image of size 512\u00d7512 is segmented into part regions using a semantic segmentation model [37], which consists of ResNet [38] and Pyramid Pooling Module [39] trained on the ADE20K dataset. Each semantic region is represented by a semantic label and a bounding box.\nTo enhance the reproducibility, the semantic labels are re-categorized into 10 coarser meta-classes: \u201cwall,\u201d \u201cfloor,\u201d \u201cceiling,\u201d \u201cbed,\u201d \u201cdoor,\u201d \u201ctable,\u201d \u201csofa,\u201d \u201crefrigerator,\u201d \u201cTV,\u201d and \u201cOther.\u201d Regions with area less than 5000 pixels were considered as dummy objects and removed. For the edge connection step, there are two types of conditions under which a part node pair is connected by an edge. One is that the pair of bounding boxes of those parts overlap. The other is that Euclidean distance between the bounding box pair (i.e., minimum point-to-region distance) is within 20 pixels. In preliminary experiments, we found that this heuristics often works effectivity.\nTo enhance the discriminativity, a spatial attribute of the part region called \u201csize/location\u201d word [40] is used as additional attributes of the node feature. Regarding the \u201csize\u201d word, we classify the part into one of three size-words according to the area of the bounding box: \u201csmall (0)\u201d S< So, \u201cmedium (1)\u201d So \u2264 S < 6So, and \u201clarge (2)\u201d 6So \u2264 S, where So is a constant corresponding to the 1/16 of the image area. Regarding the \u201clocation\u201d word, we discretize the center location of the bounding box by a grid of 3x3=9 cells and define the cell ID (\u2208 [0,8]) as the location word.\nNote that all the attributes above are interpretable semantic words, and only complex appearance/spatial attributes such as real-valued descriptors are not introduced. Finally, the node feature is defined in the joint space of semantic, size, and location words as a 10x3x9=270 dimensional one-hot vector."
        },
        {
            "heading": "B. State Recognizer",
            "text": "A graph convolutional neural network GCN is employed to embed a scene graph into the state vector. The architecture of GCN is identical to that used in our recent publication [41]. The implementation of GCN uses the deep graph library from [42].\nIn the above paper [41], the 2D location (x,y) is used as the state representation for a passive self-localization application, ignoring the bearing attribute \u03b8 . In contrast, in the active self-localization scenario considered in this work, the bearing attribute \u03b8 plays much more important role. That is, the robot should change its behavior depending on the bearing angle even when the location is unchanged. Therefore, the robot\u2019s workspace is modeled as a 3D region in the location-bearing space (x,y,\u03b8 ), and it is partitioned into a regular grid of place classes using the location and bearing resolution of 2 m and 30 deg, respectively.\nAnother key difference is the necessity of knowledge transfer from passive to active vision. The class-specific probability map output by GCN (i.e., passive vision) is often uncalibrated as the knowledge to be transferred. In fact, in most existing works, convolutional neural networks are used as a ranking function or classifier [23], [43], [44], rather than a regressor [45]. The class-specific rank value vector could be naively used as feature to be transferred. However, such a rank vector is often inappropriate as a feature vector because the most confident classes with highest probabilities are assigned the lowest values. Instead, a reciprocal rank vector [15] is used as a feature vector and as the state vector for the reinforcement learning for active vision. Note that reciprocal rank vector is as an alternative and additive feature, and proven to have several desirable properties in the field of multi-modal information fusion [46]."
        },
        {
            "heading": "C. Multi- Hypothesis Tracking",
            "text": "A particle filter is employed for incremental estimation of pose (location/bearing) during the sequential multi-view self-localization [22].\nThe spatial resolution required for tracking particles in the particle filter framework typically needs to be much higher than that of the place classes [34]. Therefore, a simple max pooling is used to convert the particles\u2019 location attributes to the class-specific rank values, regardless of their bearing attributes.\nThe particle set must be initialized at the beginning of a training/test episode. It could be naively initialized by uniformly sampling the location/bearing attributes of the particles. However, this often results in a large number of useless particles, which are lossy in both space and time. Instead, we used a guided sampling strategy [47], in which M = 5000 initial particles are sampled from only those k = 3 place classes that received the highest initial observation likelihood based on the first observation at the first viewpoint (t = 0) in each episode. Empirically, such a guided sampling strategy tends to contribute to a significant reduction in the number of particles and computational cost with no or little loss of localization accuracy."
        },
        {
            "heading": "D. Action Planner",
            "text": "Most reinforcement learning-based action planners suffer from sparse rewards as well as high-dimensionality of the state-action spaces. This issue of \u201ccurse of dimensionality\u201d is addressed by introducing an efficient nearest neighbor - based approximation of Q-learning (NNQL) as in [16].\nIts training procedure is an iterative process of indexing state-action pairs associated with Q values to the database. Importantly, it stores the Q-values for only those points that are experienced in the training stage, rather than every possible point in the high-dimensional state-action space, which is intractable. Note that the number of Q-values to be stored is significantly reduced by this strategy and becomes independent of the dimensionality.\nIts action planning is a process of similarity search over the database using the queried state and averaging the k Qvalues that are linked to k = 4 nearest-neighbor state-action pairs N(s,a) of the state-action pair (s,a):\nQ(s,a) = |N(s,a)|\u22121 \u2211 (s\u2032,a\u2032)\u2282N(s,a) Q(s\u2032,a\u2032). (1)\nExceptionally, if there exists the same state-action pair as (s,a) within the range of quantization error, it is regarded as revisited state-action pair and then, the Q value associated with that state-action pair is directly returned.\nAccording to the theory of Q-learning [35], the Q function\nis updated by:\nQ(st ,at)\u2190 Q(st ,at)+\u03b1[rt+1 + \u03b3 maxQ(st+1,a)\u2212Q(st ,at)], (2) where st+1 is the state to which it is transited from the state st by executing the action at . \u03b1 is the learning rate. \u03b3 is the discount factor.\nIt should be noted that this database allows the Q function at any past checkpoint to be recovered at low cost. Let St and Qt(s,a) denote the state-action pairs stored in the database and Q value linked to (s,a) at t-th episode. For any past checkpoint t \u2032(< t), St\u2032 is a subset of St (i.e., St\u2032\u2282St ). Therefore, Q function at checkpoint t \u2032 can be recovered from Qt\u2032(s,a), which consumes a negligible amount of storage compared to the high-dimensional state-action pairs St . Note that a novel type of Qt\u2032(s,a) \u201cnot-available (N/A)\u201d needs to be introduced for this purpose with little storage overhead. This allows us to store many different versions of the Qfunction in memory at each checkpoint. Such memorystored parameters are known to be beneficial in avoiding catastrophic forgetting [48] and improved learning [49]."
        },
        {
            "heading": "V. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Dataset",
            "text": "The 3D photo-realistic simulator Habitat-Sim and the dataset HM3D are used as training and deployment (test) environments, as detailed in Section III. Three datasets called \u201c00800-TEEsavR23oF,\u201d \u201c00801-HaxA7YrQdEC,\u201d and \u201c00809-Qpor2mEya8F\u201d from the Habitat-Matterport3D Research Dataset (HM3D) are considered and imported to the Habitat simulator. A bird\u2019s eye view of the scene and a firstperson-view image in the environment are shown in Fig. 3.\nB. Implementation Details\nFor GCN, the number of training iterations was set as 100.\nThe batch size was 32. The learning rate was 0.001.\nFor each dataset, a GCN classifier is trained by using\n10,000 training scenes with class labels as supervision.\nIn both training and deployment stages, the robot starts from a random location in the environment, performs an initial sensing, and then executes an episode consisting of a length L = 4 sequence of plan-action-sense cycles.\nThe state for NNQL training and deployment at each viewpoint is represented by the latest class-specific reciprocal rank feature. The reward function returns the value of +1 if the Top-1 predicted place class is correct or \u22121 if it is incorrect. For NNQL training, the number of iterations is set as 10,000. The learning rate \u03b1 of Q function is 0.1. The discount factor \u03b3 is 0.9. The \u03b5-greedy with \u03b5 = (0.1(n+1)+ 1)\u22121 is used for n-th episode. A checkpoint was set every (1000i)-th episode, at which the trainable parameters were saved at each checkpoint, in a compact form as explained in Section IV-D. The number of test episodes was 100.\nWe also conducted experiments on unsupervised domain adaptation (UDA). The domain adaptation here evaluates the models at all checkpoints against a small validation set (size 10) from the test domain and adopts the best performing model.\nThe robot workspace is represented by a 3D region in the location-bearing space and it is partitioned into place classes by a grid-based partitioning with the location resolution of 2 m and bearing resolution of \u03c0/6 rad. The proposed active multi-view method was compared against the baseline single-view and passive multi-view methods. The single-view method differs from the multi-view methods in its problem setting, in that no action planning is considered, but the robot performs the initial sensing and\nlocalization at the initial viewpoint (i.e., t = 0) and then immediately terminates with outputting the self-localization result. The passive multi-view method is different from the active one only in that the next action at each viewpoint is randomly sampled from the action space. Note that this passive multi-view framework is a standard method for passive self-localization and can be viewed as a strong baseline. In particular, as the episode length (number of observed actions per episode) increases, it is guaranteed that the performance of passive multi-view self-localization using this random action planner asymptotically approaches to the performance of best performing NBV planner."
        },
        {
            "heading": "C. Results",
            "text": "For each of the three datasets, we evaluated selflocalization for the above three different methods and for 25 different settings of Txy and T\u03b8 as described in Section III.\nTop-1 accuracy is used as the main performance index. Table I shows the performance results after 10,000 NNQL trainings. Figure 5 shows the results of testing for the experiments on unsupervised domain adaptation.\nRegarding the domain adaptation results, early checkpoint models were also often selected as best models, in some test episodes. However, these models did not perform very well. The reason is that the model acquired in the training domain often already had sufficient generalization ability in this experiment. Unfortunately, under what conditions the domain adaptive model outperforms the pretrained model is still unclear and remains a research topic.\nFigure 4 illustrates examples of actions and view-images in the test stage. The variable t indicates the ID of planaction-sense cycle ID, where t = 0 corresponds to the initial sense at the start viewpoint by the robot.\nThe computational cost for scene graph generation, GCN, and viewpoint planning was 18, 0.10, 10 [ms], which was faster than the real-time.\nIn most datasets, the passive multi-view method and the proposed method showed higher performance than the single-view method. Furthermore, in the unsupervised domain adaptation experiments, the proposed method outperforms the passive multi-view method for all the datasets considered here. Experiments showed that the proposed approach significantly improves accuracy for a wide variety of datasets.\nFigure 4 shows success and failure examples. As can be seen from the figure, behavior of robots moving to locations\nwith dense natural landmark objects often improves visual place recognition performance. For example, at the initial viewpoint, the robot was facing the wall and could not observe any effective landmarks at all, but the robot was able to detect the door by changing the heading direction at the next viewpoint, and the self-localization accuracy was improved using this landmark object. A typical failure case is also shown in Fig. 4, where more than half of the viewpoints in the episode are facing feature-less objects such as walls and windows. Another notable trend is that the recognition success rate decreases when the viewpoint is too close to the object, which results in narrow field-of-view."
        },
        {
            "heading": "VI. APPLICATION: EGOCENTRIC TOPOLOGICAL MAP",
            "text": "The graph neural embedding framework introduced in this study can be considered as an instance of an egocentric topological map in recently emerged topological mapping paradigms such as graph neural SLAM [50].\nOne unresolved issue in topological navigation is active localization using topological maps. Typical frameworks for training action planners, such as reinforcement learning, assume fixed-size grid data as input (e.g., grid map), and thus they are not suitable for topological maps, which are inherently non-grid and variable-size. Moreover, their heavy computational burden will ruin the lightweight advantage of topological maps.\nTo address these issues, we have been developing a novel map scheme, called \u201dego-centric topological map\u201d (Fig. 6). Unlike typical map schemes that require a globally consistent worldcentric model to be precomputed, ego-centric topological map is inherently viewpoint-specific model. It can be obtained directly from egocentric cameras, compactly compressed into graph neural network models, and transferred to action planners and planner training frameworks,\nIn parallel to the work in this paper, we developed a realtime SLAM prototype for egocentric topological maps. The egocentric scene graph and graph neural network introduced in the current paper are used as the core of egocentric topological map model and mapping system, and then modules for visual recognition, memory, search, and matching are added to construct a complete prototype of topological mapping. As a result, not only visual recognition and action planning, but also planner training were achieved within the\nreal-time budget for each viewpoint. Very recently, experiments using the photorealistic Habitat simulator validated effectiveness of the proposed approach."
        },
        {
            "heading": "VII. CONCLUDING REMARKS",
            "text": "In this work, an entirely CPU-based lightweight framework for solving semantic localization in both active and passive self-localization tasks is presented. A scene graph and a graph neural network are combined for the first time to solve semantic localization, presenting a new graph neural localizer framework. Experiments show that the proposed method outperforms the baseline method in terms of selflocalization performance, computational efficiency, and domain adaptation."
        }
    ],
    "title": "Active Semantic Localization with Graph Neural Embedding",
    "year": 2023
}