{
    "abstractText": "Making the contents generated by Large Language Model (LLM) such as ChatGPT, accurate, credible and traceable is crucial, especially in complex knowledgeintensive tasks that require multi-step reasoning and each of which needs knowledge to solve. Introducing Information Retrieval (IR) to provide LLM with external knowledge is good potential to solve this problem. However, where and how to introduce IR into LLM is a big challenge. Previous work has the disadvantage that the wrong knowledge retrieved by IR misleads the LLM or breaks the reasoning chain of LLM. In this paper, we propose a novel framework called Search-in-theChain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the global reasoning chain called Chain-of-Query (CoQ) where each node consists of an IR-oriented query and the answer to the query. Second, IR verifies the answer of each node of CoQ, it corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can mark its missing knowledge in CoQ and IR can provide this knowledge to LLM. These three operations improve the accuracy of LLM for complex knowledge-intensive tasks in terms of reasoning ability and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. SearChain transforms the topology of reasoning from chain to tree, which can modify the reasoning direction. Experiment shows that SearChain outperforms baselines on complex knowledge-intensive tasks including multi-hop question-answering, slot filling, fact checking, and long-form question-answering. \u2217Corresponding author Preprint. Under review. ar X iv :2 30 4. 14 73 2v 6 [ cs .C L ] 2 2 Se p 20 23",
    "authors": [
        {
            "affiliations": [],
            "name": "Shicheng Xu"
        },
        {
            "affiliations": [],
            "name": "Liang Pang"
        },
        {
            "affiliations": [],
            "name": "Xueqi Cheng"
        }
    ],
    "id": "SP:104f9e43a76baf87602a862b87a604967ea89bc2",
    "references": [
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "venue": "CoRR, abs/2302.04023,",
            "year": 2023
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong",
                "Yifan Du",
                "Chen Yang",
                "Yushuo Chen",
                "Zhipeng Chen",
                "Jinhao Jiang",
                "Ruiyang Ren",
                "Yifan Li",
                "Xinyu Tang",
                "Zikang Liu",
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "A survey of large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard",
                "Vassilis Plachouras",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel"
            ],
            "title": "KILT: a benchmark for knowledge intensive language tasks",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Da Yin",
                "Li Dong",
                "Hao Cheng",
                "Xiaodong Liu",
                "Kai-Wei Chang",
                "Furu Wei",
                "Jianfeng Gao"
            ],
            "title": "A survey of knowledge-intensive nlp with pre-trained language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Measuring and narrowing the compositionality gap in language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Nikhil Kandpal",
                "Haikang Deng",
                "Adam Roberts",
                "Eric Wallace",
                "Colin Raffel"
            ],
            "title": "Large language models struggle to learn long-tail knowledge, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Razvan Azamfirei",
                "Sapna R Kudchadkar",
                "James Fackler"
            ],
            "title": "Large language models and the perils of their hallucinations",
            "venue": "Critical Care,",
            "year": 2023
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave"
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "CoRR, abs/2007.01282,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang"
            ],
            "title": "REALM: retrieval-augmented language model pre-training",
            "year": 2002
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik R Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Omar Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei Zaharia"
            ],
            "title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Lei Wang",
                "Wanyu Xu",
                "Yihuai Lan",
                "Zhiqiang Hu",
                "Yunshi Lan",
                "Roy Ka-Wei Lee",
                "Ee-Peng Lim"
            ],
            "title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola"
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman"
            ],
            "title": "STar: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot"
            ],
            "title": "Complexity-based prompting for multi-step reasoning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Nathanael Sch\u00e4rli",
                "Ekin Aky\u00fcrek",
                "Nathan Scales",
                "Xinying Song",
                "Xinyun Chen",
                "Olivier Bousquet",
                "Denny Zhou"
            ],
            "title": "Compositional semantic parsing with large language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Weijia Shi",
                "Mike Lewis",
                "Xilun Chen",
                "Wen-tau Yih",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Nonparametric masked language modeling",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Jordan Hoffmann",
                "Trevor Cai",
                "Eliza Rutherford",
                "Katie Millican",
                "George van den Driessche",
                "Jean-Baptiste Lespiau",
                "Bogdan Damoc",
                "Aidan Clark",
                "Diego de Las Casas",
                "Aurelia Guy",
                "Jacob Menick",
                "Roman Ring",
                "Tom Hennigan",
                "Saffron Huang",
                "Loren Maggiore",
                "Chris Jones",
                "Albin Cassirer",
                "Andy Brock",
                "Michela Paganini",
                "Geoffrey Irving",
                "Oriol Vinyals",
                "Simon Osindero",
                "Karen Simonyan",
                "Jack W. Rae",
                "Erich Elsen",
                "Laurent Sifre"
            ],
            "title": "Improving language models by retrieving from trillions of tokens",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Hongjing Qian",
                "Yutao Zhu",
                "Zhicheng Dou",
                "Haoqi Gu",
                "Xinyu Zhang",
                "Zheng Liu",
                "Ruofei Lai",
                "Zhao Cao",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee",
                "Kristina Toutanova",
                "Llion Jones",
                "Matthew Kelcey",
                "Ming-Wei Chang",
                "Andrew M. Dai",
                "Jakob Uszkoreit",
                "Quoc Le",
                "Slav Petrov"
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer"
            ],
            "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang"
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "In Proceedings of the 2013 Conference on EMNLP,",
            "year": 2013
        },
        {
            "authors": [
                "Petr Baudis",
                "Jan Sediv\u00fd"
            ],
            "title": "Modeling of the question answering task in the yodaqa system",
            "venue": "In Conference and Labs of the Evaluation Forum,",
            "year": 2015
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "In Text Summarization Branches Out,",
            "year": 2004
        },
        {
            "authors": [
                "Angela Fan",
                "Yacine Jernite",
                "Ethan Perez",
                "David Grangier",
                "Jason Weston",
                "Michael Auli"
            ],
            "title": "ELI5: Long form question answering",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning"
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "MuSiQue: Multihop questions via single-hop question composition",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Xanh Ho",
                "Anh-Khoa Duong Nguyen",
                "Saku Sugawara",
                "Akiko Aizawa"
            ],
            "title": "Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps",
            "venue": "In Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant"
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics (TACL), 2021",
            "year": 2021
        },
        {
            "authors": [
                "Omer Levy",
                "Minjoon Seo",
                "Eunsol Choi",
                "Luke Zettlemoyer"
            ],
            "title": "Zero-shot relation extraction via reading comprehension",
            "venue": "In Proceedings of the 2017 Conference on CoNLL),",
            "year": 2017
        },
        {
            "authors": [
                "Hady Elsahar",
                "Pavlos Vougiouklis",
                "Arslen Remaci",
                "Christophe Gravier",
                "Jonathon Hare",
                "Frederique Laforest",
                "Elena Simperl"
            ],
            "title": "T-REx: A large scale alignment of natural language with knowledge base triples",
            "venue": "In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),",
            "year": 2018
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal"
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "year": 2018
        },
        {
            "authors": [
                "Corby Rosset",
                "Chenyan Xiong",
                "Minh Phan",
                "Xia Song",
                "Paul N. Bennett",
                "Saurabh Tiwary"
            ],
            "title": "Knowledge-aware language model pretraining",
            "year": 2007
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R. Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Xuezhi Wang",
                "Yi Tay",
                "Yiming Yang",
                "Denny Zhou"
            ],
            "title": "Recitation-augmented language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Making the contents generated by Large Language Model (LLM) such as ChatGPT, accurate, credible and traceable is crucial, especially in complex knowledgeintensive tasks that require multi-step reasoning and each of which needs knowledge to solve. Introducing Information Retrieval (IR) to provide LLM with external knowledge is good potential to solve this problem. However, where and how to introduce IR into LLM is a big challenge. Previous work has the disadvantage that the wrong knowledge retrieved by IR misleads the LLM or breaks the reasoning chain of LLM. In this paper, we propose a novel framework called Search-in-theChain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the global reasoning chain called Chain-of-Query (CoQ) where each node consists of an IR-oriented query and the answer to the query. Second, IR verifies the answer of each node of CoQ, it corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can mark its missing knowledge in CoQ and IR can provide this knowledge to LLM. These three operations improve the accuracy of LLM for complex knowledge-intensive tasks in terms of reasoning ability and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. SearChain transforms the topology of reasoning from chain to tree, which can modify the reasoning direction. Experiment shows that SearChain outperforms baselines on complex knowledge-intensive tasks including multi-hop question-answering, slot filling, fact checking, and long-form question-answering.\n\u2217Corresponding author\nPreprint. Under review.\nar X\niv :2\n30 4.\n14 73\n2v 6"
        },
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) such as ChatGPT have shown promising performance in various natural language processing tasks [1, 2]. As LLMs are gradually used in various fields, we need to pay attention to how to ensure that the contents generated by LLMs are accurate, credible and traceable, especially in the complex knowledge-intensive tasks that require multi-step reasoning and each step needs corresponding knowledge to solve [3, 4]. Many studies have shown that LLMs have trouble in (1) compositional reasoning over multiple knowledge [5], (2) memorization of long-tail and real-time knowledge [6] and (3) avoiding hallucination that is inconsistent with the facts [7], which affects the accuracy and credibility of LLMs for complex knowledge-intensive tasks. Besides, context-only generation without any supporting evidence causes less traceability and makes people less trust in the LLM-generated content. Retrieval-augmented method is good potential to solve these problems because it combines the knowledge of the model with an external knowledge base [8\u201310].\nHowever, where and how to introduce IR into LLM is not a trivial thing. Directly inserting IR into the reasoning process of LLM such as Self-Ask [5], LTM [11], React [12] and DSP [13] leads to breaking the reasoning chain that is proven to be effective in CoT [14] of LLM . Although AgentGPT and PS [15] first reason and plan sub-questions and then solve them, they are not suitable for scenarios where the next sub-question needs the answer of the previous sub-questions to generate, which is common for complex knowledge-intensive tasks, such as multi-hop QA. Besides, previous methods assume that the answers generated in the reasoning chain are always correct and cannot modify the reasoning direction in time as necessary. Last but not least, when there is a conflict in the knowledge of IR and LLM, for the knowledge that the LLM has correctly memorized, it risks being misled by IR if IR retrieves wrong information. Therefore, it is important to decouple the knowledge of LLM and IR, making sure that IR only provides the knowledge that LLM really needs to avoid the misleading of LLM by IR, which is not considered in previous methods.\nIn this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to effectively combine LLM with IR to solve the above challenges (Figure 1). In SearChain, LLM and IR conduct multiple rounds of interaction. In each round, SearChain performs reasoning, verification and completion. After the interaction, SearChain performs tracing to generate the final content. Specifically, in each round, first, LLM exploits in-context learning to construct a Chain-of-Query (CoQ), which is a global reasoning chain to decompose and solve complex questions. Each node of the chain consists of an IR-oriented query, the answer generated by LLM for this query and a flag indicating whether LLM needs additional knowledge. This design improves the knowledge-reasoning ability (Section 4). Then, IR interacts with each node of CoQ to perform verification and completion. In verification, IR verifies the answer of each node. When the LLM-generated answer is not consistent with the retrieved information and IR gives high confidence, IR gives feedback to LLM to help it\ncorrect the answer and regenerate the correct CoQ. In completion, IR determines whether the node has missing knowledge from the flag of the node and provides this knowledge to LLM to help it regenerate CoQ. LLM gradually generates the correct CoQ through multiple rounds interacting with IR. This design not only avoids IR from breaking the reasoning chain of LLM but also provides LLM with the knowledge it needs in an explainable way to alleviate the misleading of the LLM, which improves the accuracy of LLM for complex knowledge-intensive tasks. IR verifies and corrects the knowledge in the reasoning process of LLM based on the external knowledge base, which improves credibility. After the interaction, SearChain performs tracing to generate the reasoning process and marks references to supporting documents for each reasoning step as the final content returned to the user, which improves the traceability of each knowledge in the generated contents. In addition, SearChain transforms the topology of reasoning complex questions from chain to tree called Tree-ofReasoning (ToR), which enables LLM to dynamically modify the direction of reasoning. Besides, SearChain decouples the knowledge of LLM and IR, which alleviates the misleading of LLM and can also explain which knowledge comes from LLM and which comes from IR for the generated texts.\nThe main contributions can be summarized as (1) We point out the challenges that need to be addressed in introducing IR into LLM from the perspectives of reasoning and knowledge. (2) We propose a novel framework called SearChain to effectively combine LLM and IR. SearChain not only stimulates the knowledge-reasoning ability of LLM but also uses IR to provide the knowledge that LLM really needs based on the external knowledge base, which improves accuracy and credibility. Besides, SearChain can mark references to supporting documents for the knowledge involved in the generated contents, which improves traceability. (3) Experiment shows that SearChain outperforms baselines on complex knowledge-intensive tasks including multi-hop question-answering, slot filling, fact checking and long-form question-answering. Code is available at https://github.com/ xsc1234/Search-in-the-Chain"
        },
        {
            "heading": "2 Related Work",
            "text": "Chain-of-Thought Prompting: [14] proposes the method called Chain-of-thought (CoT) that uses few-shot examples to enable LLM to give intermediate reasoning results when solving complex problems and improves the reasoning ability. [16] uses \"Let\u2019s do it step by step\" as prompt to achieve promising zero-shot performance. Auto-CoT exploits language models to automatically construct few-shot learning examples for CoT [17]. There are also many studies that cover other aspects of CoT such as self-consistency [18], usage of small and medium size models [19] and selection [20]. Besides, there are studies that iteratively use LLM to decompose complex questions and answer sub-questions step by step. These methods include Least-to-Most [11], Dynamic Least-to-Most [21], Self-Ask [5] and DSP [13]. Chain-of-Query of our method is also inspired by CoT. However, previous studies focus on giving intermediate reasoning results or decomposing complex questions and answering sub-questions step by step. They focus on how to solve local sub-questions while ignoring the global planning of the reasoning chain. Although AgentGPT and PS [15] first plan sub-questions and then solve them, they are not suitable for scenarios where the next sub-question needs the answer of the previous sub-questions to generate, which is common for complex knowledge-intensive tasks, such as multi-hop question-answering. CoQ of our method makes LLM construct a global reasoning chain where each node is a query-answer pair. This design not only improves the knowledge-reasoning ability (Section 4) but also provides the interface for IR to be deeply involved in the reasoning process of LLM. Besides, a significant difference is that our method converts the chain to tree, CoQ is the branch of the tree, which enables LLM to dynamically modify the reasoning direction.\nRetrieval-augmented Language Models: Many studies have shown that retrieval-augmented methods can connect language models with external corpus to get promising performance in various natural language tasks such as open-domain question answering [8\u201310], language modeling [22, 23] and enhancing the factuality [24]. Recently, some studies enable LLM to interact with IR via incontext learning [5, 13, 12, 25]. In these methods, the interaction between IR and LLM makes the reasoning of LLM not continuous. LLM can only perform one-step reasoning at each inference. Our method makes LLM generate a global reasoning chain called Chain-of-Query at each inference, the hidden states in generation are used as the sequential dependency, which introduces stronger logical relationship between each reasoning step. Besides, previous methods can only provide information to the LLM but cannot assist LLM in correcting erroneous information or avoid the negative effect of IR on LLM, which makes the reasoning of LLM still in a one-dimensional chain. Our method makes IR\ninteract with each node of the chain. IR only provides LLM with its missing knowledge and corrects the answers that are not consistent with retrieved information when IR is confident enough. This design mitigates the negative effect of IR on LLM and transforms the topology of reasoning from chain to tree to enable LLM to modify the reasoning direction."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we introduce the design of Search-in-the-Chain (SearChain). In SearChain, IR and LLM conduct multiple rounds of interaction. In each round, first, LLM acts as the commander to plan the global reasoning chain for the complex input questions called Chain-of-Query (CoQ). Each node of the CoQ consists of an IR-oriented query, the answer generated by LLM for this query and a flag indicating whether LLM needs additional knowledge. Then, IR interacts with each node of CoQ and performs completion and verification to provide LLM with missing knowledge and correct the wrong answers. LLM regenerates new CoQ based on feedback from IR. Multiple rounds of interaction help LLM gradually generate the correct CoQ according to the external knowledge base, which improves accuracy and credibility. Finally, SearChain performs tracing to generate the reasoning process and marks references to supporting documents for each reasoning step as the final content returned to the user, which improves the traceability of generated content. SearChain transforms the topology of reasoning complex questions from chain to tree called Tree-of-Reasoning (ToR), which enables LLM to dynamically modify the reasoning direction. Besides, SearChain decouples the knowledge of LLM and IR and provides LLM with the knowledge it needs to alleviate the misleading of LLM."
        },
        {
            "heading": "3.1 Chain-of-Query Generation",
            "text": "In SearChain, we use in-context learning [14] to prompt large language model to construct a global reasoning chain for complex knowledge-intensive question Q called Chain-of-Query (CoQ) C:\nC = (q1, a1) \u2192 (q2, a2) \u2192 ... \u2192 (qn, an), (1) which is the branch of Tree-of-Reasoning. Each node (qi, ai) on C consists of an IR-oriented query qi and its answer ai. q1 ... qn are the sub-questions that need to be solved in the reasoning process of solving Q. CoQ generation is applied to each round of interaction between LLM and IR. In the first round, the prompt used to make LLM generate CoQ is shown in Figure 12. The prompt starts with \u201cConstruct a global reasoning chain\u201d to make LLM know that the main task is to generate a global reasoning chain at each inference (global perspective), rather than answer questions directly or generate and solve local sub-questions step by step (local perspective). At each node of the chain, LLM focuses on generating the IR-oriented query and gives the answer if LLM knows. If LLM does not know the answer, it should mark the query with \u201c[Unsolved Query]\u201d, which is a flag indicating\nAlgorithm 1: Description of the Interaction with IR. Initialize :Processed queries: M = null; Correct reasoning path: R = null; Interaction rounds: r = 0; Feedback to LLM: F = null; Tree-of-Reasoning: T = Q; Output :Final Generated Contents. Function IR(qi,ai):\ndi = Retrieval(qi) ; // Retrieve Top-1 document di for qi. g, f = Reader(qi,di) ; // Extract answer g from di and give confidence f. if qi is Unsolved Query then\nR.add(qi,g,di) ; // Record the correct node. return PromptForComplete(qi,g,di);\nif f > \u03b8 and NotEqual(g, ai) then R.add(qi,g,di); return PromptForVerify(qi,g,di);\nR.add(qi,ai,di); return \u201cPass\u201d ; Function Traverse(C):\nforeach (qi, ai) in C do if DuplicateQueryDetection(qi, M) then Continue ; F = IR(qi,ai); M.add(qi); if not F == \u201cPass\u201d then return F ;\nreturn \u201cFinish\u201d ; Function Main(Q,F):\nwhile not (F == \u201cFinish\u201d or r > rmax) do C = ChainGenerate(Q,F) ; // LLM generate the new Chain-of-Query C. T .AddChild(C) ; // Add the new branch to T . F = Traverse(C) ; // Interact with IR. r = r + 1 ; // Update the number of interaction rounds r.\nreturn Tracing(T , R) ; // Generate final result G.\nthe missing of knowledge. In subsequent rounds, when a node needs IR to correct or provide missing knowledge, LLM generates a new CoQ according to the feedback of IR to dynamically modify the reasoning direction. The prompt for this scenario will be introduced in Section 3.2. CoQ avoids IR from breaking the reasoning chain of LLM. Our experiments (Section 4.3) also show that for the difficult sub-question, CoQ enables LLM to solve it by more reasoning steps such as rewriting or further decomposing the sub-question while baselines tend to stop reasoning. It is because baselines focus on solving current local sub-questions while ignoring the global planning of the reasoning chain. The global perspective in CoQ makes LLM try more when facing intermediate difficulties."
        },
        {
            "heading": "3.2 Interaction with Information Retrieval",
            "text": "In each round of interaction, LLM passes the generated CoQ (C) to IR. IR verifies and completes the information for each node (qi, ai) on C and feeds back to LLM to help it generate more correct CoQ as the new branch of ToR. Besides, IR records the corresponding retrieved documents for each node on CoQ as its supporting documents, which enhances the traceability of LLM-generated content. The description of interaction is shown in Algorithm 1. IR interacts with each node (qi, ai) of C, retrieves the Top-1 document di for qi as the supporting document, and judges whether to verify or complete it according to the type of qi. When all the queries on C do not need to be corrected or completed, or the maximum number of interaction rounds is reached, the interaction ends. SearChain traces back the correct reasoning path of ToR and refers to each node of the path to generate the final content with marked references to supporting documents for knowledge of each node.\nVerification. Verification aims to guarantee the correctness of ai in each node (qi, ai) of CoQ based on the external knowledge base, which improves the accuracy and credibility of generated content. Specifically, given the retrieved Top-1 document di for qi, a Reader [26] that has been trained on\nopen-domain QA datasets 2 is introduced to extract the answer g for qi from di with its confidence f :\ns = argmax(softmax(Hws)), e = argmax(softmax(Hwe)),\ng = di[s : e], f = softmax(H[CLS]wf ),\nwhere H is the last hidden states for \u201c[CLS]qi[SEP]di\u201d, H[CLS] is the hidden states of [CLS] token. Then, SearChain judges whether the answer ai given by LLM is consistent with the retrieved information according to (1) whether g appears in ai (for short-form generation tasks such as multihop QA and slot filling) or (2) whether ROUGE [31] between ai and di is greater than the threshold \u03b1 (for long and free-form generation tasks such as ELI5 [32]). If ai is not consistent with retrieved information and Reader is confident enough (f > \u03b8, \u03b8 is a threshold to alleviate the negative effect of IR on LLM), a prompt is constructed to help LLM correct the answer ai. The template of the prompt is: \u201cAccording to the Reference, the answer for qi should be g, you can change your answer and continue constructing the reasoning chain for [Question]: Q. Reference: di.\u201d. This round is over. LLM receives the feedback of IR, gives the new answer a \u2032\ni for q, and generates a new CoQ with (qi, a \u2032 i) as the root node, which is the new branch of Tree-of-Reasoning.\nCompletion. Completion aims to provide LLM with missing knowledge in nodes of CoQ, which improves the accuracy of generated contents. Specifically, in CoQ generation (Section 3.1), LLM marks \u201c[Unsolved Query]\u201d for the unsolvable query. For the unsolvable query q\u2217i , IR extracts the answer g\u2217 from retrieved document d\u2217i as described in Verification. Regardless of whether f is greater than the threshold \u03b8, g\u2217 and d\u2217i will be fed back to the LLM in the form of a prompt because the LLM cannot solve q\u2217i . The template of the prompt is: \u201cAccording to the Reference, the answer for q \u2217 i should be g\u2217, you can give your answer and continue constructing the reasoning chain for [Question]: Q. Reference: d\u2217i .\u201d. This round is over. LLM receives the feedback, gives the answer a \u2217 i to solve the query q\u2217i and generates a new CoQ with (q \u2217 i , a \u2217 i ) as the root node, which is the new branch of ToR.\nTracing. Tracing aims to generate the reasoning process and marks references to supporting documents for each reasoning step as the final content returned to the user, which improves the traceability of each knowledge in the generated content. Specifically, SearChain records the documents retrieved for each node on the correct reasoning path of Tree-of-Reasoning as the supporting documents. SearChain prompts LLM to generate the final content by referring to nodes on the path and mark references to the supporting documents for the corresponding sub-fragments of the generated content (final content of Figure 1). The prompt is \u201cYou can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final Content]. [Query 1]: q1. [Answer 1]: a1 ... [Query m]: qm. [Answer m]: am.\u201d. This design enables the user to acquire the related documents of the knowledge involved in each step of reasoning. We believe that it is a promising task to mark references to supporting documents on sub-fragments of complex content generated by LLM. Our approach provides a novel and effective approach to solve this task by retrieving supporting documents for each sub-questions involved in the reasoning process of LLM without any supervised data (texts with citation annotations) and training of the LLM."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we compare the performance of SearChain with recent related baselines on complex knowledge-intensive tasks and conduct the analysis."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets and Evaluation Metrics. We select four classic complex knowledge-intensive tasks including multi-hop question-answering (HotpotQA (HoPo) [33], Musique (MQ) [34], WikiMultiHopQA (WQA) [35] and StrategyQA (SQA) [36]), slot filling (zsRE [37], T-REx [38]), fact checking (FEVER [39]) and long-form question-answering (ELI5 [32]). These tasks require LLM to perform multi-step reasoning on complex questions, and each step requires corresponding knowledge to solve. As for the evaluation metrics, for ELI5 whose ground truth is long and free-form, we use ROUGE-L [31] as the metric. For other tasks, we use whether the ground truth answer is contained\n2NQ [27], TriviaQA [28], WebQ [29], TREC [30]\nwithin the generated answer (i.e, cover-EM [40]) as the metric. Following DSP [13] and Self-Ask [5], we evaluate the model on full development datasets of MQ and HoPo, BIG-bench [41] datasets on SQA, subsets of WQA, zsRE, T-REx, FEVER and ELI5 (each subset has 1.2k questions).\nBaselines. Our baselines can be divided into two categories, one is about improving the reasoning ability of LLM on complex tasks (CoT [14], CoT-SC [18], Auto-CoT [17], Recite-and-answer [42] and Least-to-Most [11]), and the other is not only introducing IR to LLM but also improving the reasoning ability (Direct 3, Self-Ask [5], ToolFormer 4 [25], React [12] and DSP [13]). AgentGPT and PS [15] use Plan-and-Solve paradigm, we also reproduce this as one of the baselines.\nImplementation. The large language model we used is gpt-3.5-turbo provided from API of OpenAI 5 and the retrieval model we used is ColBERTv2 [43] (following DSP). IR model infers on one Tesla V100 32GB GPU. For HotpotQA, we use Wikipedia 2017 as the corpus, which is officially provided by [33]. For the other datasets, we use the large-scale passage collection built on Wikipedia for open-domain QA as the corpus [26]. Baselines with information retrieval are in the same setting as SearChain. Because most baselines are performed on text-davinci-002, we reproduce their results on gpt-3.5-turbo following the settings in their papers and get better performance. The maximum number of interaction rounds rmax is 5. The thresholds \u03b1 and \u03b8 are set as 0.35 and 1.5 respectively. SearChain uses fewer examples in prompt than baselines, details are introduced in Appendix."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Performance of SearChain and baselines on complex knowledge-intensive tasks are shown in Table 1. (1) Effect of Chain-of-Query. CoQ is the reasoning chain for complex questions in SearChain. We compare it with recent competitive baselines in the setting without IR. SearChain w/o IR outperforms all baselines based on CoT (CoT, Auto-CoT, CoT-SC and Recite-and-answer), which indicates that\n3Directly retrieve relevant documents for complex questions and provide them to LLM in a prompt. 4We reproduce ToolFormer on gpt-3.5-turbo via 4-shot in-context learning. 5https://openai.com/api/\nfocusing on constructing a global reasoning chain consisting of sub-questions is better than just giving intermediate reasoning results. Besides, SearChain w/o IR outperforms Self-Ask w/o IR and Least-to-Most, which indicates that it is more effective to focus on constructing a global reasoning chain at each inference (global perspective) than generating and answering sub-questions step by step (local perspective). (2) Effect of interaction with IR. In the setting with interaction with IR, SearChain again outperforms all the baselines. The paradigm of first generating global CoQ, and then IR interacting with each node of CoQ ensures the coherence of LLM reasoning, which is the disadvantage of Self-Ask, DSP and React. Besides, SearChain decouples the knowledge of LLM and IR. IR judges whether to provide information to LLM according to the confidence and the flag of the node of CoQ, which effectively alleviates misleading LLM. Last but not least, baselines reason in the one-dimensional chain. They cannot dynamically verify and modify the reasoning direction. SearChain makes the topology of reasoning change from chain to tree, which enables LLM dynamically modify the reasoning direction according to the feedback from IR."
        },
        {
            "heading": "4.3 Analysis",
            "text": "Knowledge Decoupling. In this section, we analyze the knowledge sources on the four multi-hop QA datasets. Specifically, we classify knowledge sources into three categories: (1) knowledge of LLM, (2) knowledge that corrected by IR in verification, and (3) knowledge that LLM does not know\nand is provided by IR in completion. We use node of ToR as the statistical granularity to calculate the percentage of nodes from these three sources in the total nodes respectively. The experimental results are shown in Table 2. It is worth noting that even though most of the knowledge comes from LLM, this knowledge is also verified by IR. IR only corrects the inconsistent answer given by LLM when it is confident enough and provides LLM with the missing knowledge, which alleviates the negative effect of IR on LLM and improves utilization of retrieved informa-\ntion. On StrategyQA, LLM has memorized most knowledge that IR can retrieve, so IR provides less knowledge than other datasets.\nPositive and Negative Effects of IR on LLM. (1) Positive. In SearChain, IR can identify the trouble of LLM and effectively help LLM to correct the answers and acquire missing knowledge. We select the questions (SIR) that IR helps to correct or provide knowledge from the datasets used for evaluation in Table 1 (S ) and evaluate the accuracy of SearChain on SIR. We also evaluate the accuracy of SearChain w/o IR on SIR. The results in Table 3 show that w/o IR performs worse on SIR than on S, which indicates that LLM does have trouble with questions that require IR help. w/ IR performs better on SIR, which\nindicates that IR effectively identifies and solves the trouble of LLM. (2) Negative. We point out the risk of IR misleading LLM when there is a conflict in the knowledge of IR and LLM (Section 1). We select the questions (St) that LLM can give the correct answers to and count the percentage that LLM gives incorrect answers after adding IR on St. Table 4 shows SearChain effectively mitigates the negative effect of IR on LLM. It is because that SearChain uses the confidence of IR and the information of CoQ to judge whether to correct LLM or provide LLM with its missing knowledge.\nSearChain vs New Bing in Tracing. We compare the performance of SearChain and New Bing 6 in marking references for generated contents via case study (Figure 3). SearChain can mark references for each knowledge involved in the reasoning process (i.e., each correct node of CoQ) in a more fine-grained manner. While references given by New Bing do not cover all of the knowledge, and in some cases New Bing cannot find the knowledge. More cases are shown in Appendix.\nCoQ vs Baselines in Reasoning. CoQ performs better on reasoning complex questions than baselines. In addition to Table 1, we further analyze the reasoning ability from two aspects: (1) Number of Reasoning Steps. We analyze the number of reasoning steps in different methods in the setting without IR. We conduct the experiment on Musique because Musique has more complex questions. Table 5 shows the average number of reasoning steps on questions with different hops. Our method has more reasoning steps, and the number of reasoning steps increases with the hops\nof the question. This shows that our method has a better perception of the complexity of the questions. (2) Solving Difficult Sub-questions. Baselines focus on solving local sub-questions while ignoring the global planning of the reasoning chain, which leads LLM to tend to stop reasoning rather than try more when a sub-question cannot be solved. In our method, LLM acts as a commander that plans a global reasoning chain that can solve the complex question, when a sub-question cannot be solved, even without the help of IR, LLM can try to further decompose or rewrite the sub-question to continue reasoning. It is because our method focuses on building a global chain that can solve the complex question (global perspective), rather than answering or generating sub-questions step by step (local perspective). Case in Figure 4 shows that CoT and Self-Ask stop the reasoning but SearChain continues reasoning by rewriting the sub-question. More reasoning steps in Table 5 also support that SearChain can try more for difficult sub-questions. More cases are shown in Appendix.\nEfficiency Analysis. We further analyze the difference in running efficiency between SearChain and baselines from the perspective of the number of words in the input (prompt) and output text of LLM and the number of rounds of interaction between IR and LLM 7. Table 6 shows the results of the analysis on four multi-hop QA datasets. Based on Table 1 and Table 6, SearChain significantly improves the performance of the task at the cost of a small increase in running overhead.\n6https://www.bing.com/new 7Running time is not suitable for analysis due to network fluctuations when calling API of OpenAI."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we point out the challenges that should be considered in introducing IR into LLM from perspectives of reasoning and knowledge. Then, we propose a novel framework called SearChain for enabling IR and LLM to interact with each other effectively. SearChain not only stimulates the knowledge-reasoning ability of LLM but also uses IR to provide the knowledge that LLM really needs based on the external knowledge base, which improves accuracy and credibility. Besides, SearChain can mark references to supporting documents for the knowledge involved in the generated contents, which improves the traceability of the contents. In addition, the interaction between IR and LLM in SearChain transforms the topology of reasoning from chain to tree, which enables LLM to dynamically modify reasoning direction. Experimental results on complex knowledge-intensive tasks show SearChain performs better than all baselines. In future work, we will consider how to improve the efficiency of the framework and how to introduce more tools to interact with LLM for more tasks."
        },
        {
            "heading": "A Case Study",
            "text": "In this section, we compare the performance of SearChain and New Bing 8 in adding references to supporting documents for generated content via case study. We also use case study to further analyze why CoQ has stronger reasoning ability than Baselines.\nA.1 Case Study for SearChain vs New Bing in Tracing\nWe compare the performance of SearChain and New Bing in marking references for generated contents via case study (Figure 5 \u223c 13). SearChain can mark references for each knowledge involved in the reasoning process (i.e., each correct node of CoQ) in a more fine-grained manner. While references given by New Bing do not cover all of the knowledge, and in some cases New Bing cannot find the knowledge. SearChain provides a novel perspective that decomposes complex multi-step knowledge-intensive tasks into multiple single-step knowledge reasoning problems, retrieving the supporting documents of knowledge for each step of reasoning, and organizing these reasoning steps with their reference marks as final generated content. This enables the supporting documents to cover every knowledge involved in the generated content, which enhances the traceability of the generated content.\n8https://www.bing.com/new\nA.2 Case Study for CoQ vs Baselines in Reasoning\nBaselines focus on solving local sub-questions while ignoring the global planning of the reasoning chain, which leads LLM to tend to stop reasoning rather than try more when a sub-question cannot be solved. In our method, LLM acts as a commander that plans a global reasoning chain that can solve the complex question, when a sub-question cannot be solved, even without the help of IR, LLM can try to further decompose or rewrite the sub-question to continue reasoning. It is because our method focuses on building a global chain that can solve the complex question (global perspective), rather than answering or generating sub-questions step by step (local perspective). This makes LLM try more when faced with intermediate difficulties to finally solve complex questions. Cases in Figure 14 \u223c 21 show that CoT and Self-Ask stop the reasoning but SearChain continues reasoning by rewriting the sub-question."
        },
        {
            "heading": "B Analysis of IR for LLM in Complex Knowledge-Intensive Tasks",
            "text": "In this section, we analyze the effect of IR on LLM on complex knowledge-intensive tasks from perspectives of knowledge-reasoning ability and knowledge.\nB.1 Knowledge-reasoning Ability Affected by the Existing Form of Knowledge\nWe divide the knowledge of LLM into two forms of existence, one exists in the parameters, and the other exists in the input prompt. In this section, we explore the effect of two different forms on the knowledge-reasoning ability of LLM. The experiments are performed on gpt-3.5-turbo.\nSpecifically, we obtain the sub-questions corresponding to each complex question from the datasets provided by Musique [34], and then we select the complex questions (Sall) that LLM can answer all sub-questions of them from full datasets. Table 7 shows the accuracy of LLM in\nanswering the complex questions in Sall, which indicates that even though the parameters have memorized the answer to each sub-question of the complex question, LLM cannot compose these answers to effectively reason the complex questions.\nFurthermore, we explore the effect of the form of knowledge on reasoning ability. Specifically, we select the complex questions (Si) that LLM can answer the sub-questions for the last i steps. And input the answers of sub-questions for the first n \u2212 i steps (n is the number of sub-questions) in the form of prompt. In this way, when LLM reasoning the complex questions, the knowledge of n\u2212 i sub-questions comes from the prompt, and the knowledge of i sub-questions comes from the parameters. Figure 22(a) shows the performance varies with i, which indicates that when i increases, the knowledge in prompt decreases, the knowledge in parameters increases, and the accuracy of LLM decreases. To prevent the impact of the leakage of sub-questions in the prompt, we also conduct the same experiment on Sall, except that the prompt only contains sub-questions without the answers (knowledge is still from parameters). The results are shown in Figure 22(b). The accuracy of Figure 22(b) is lower than that in Figure 22(a), which further confirms that the LLM has the stronger knowledge-reasoning ability for complex questions when the knowledge exists in the prompt and shows that the paradigm of using IR to provide knowledge for LLM can not only help LLM acquire its unknown knowledge but also improve the knowledge-reasoning ability of LLM.\nB.2 Conflict in the Knowledge of IR and LLM\nWe use five Open-domain question-answering datasets (Natural Questions, TriviaQA, WebQuestions, SQuAD, and HotpotQA) to detect the knowledge in LLM and IR. We select the set of questions that LLM gives correct answers (St) and the set that LLM gives wrong answers (Sf ). We evaluate the probability that IR can rank the correct document at Top-1 on St and Sf respectively. The results shown in Table 8 indicate that although IR can provide LLM with its unknown or wrong knowledge (Top-1 on Sf ), it can also interfere with correct knowledge in LLM because Top-1 on St only reach (60% \u223c 73%) and accuracy of LLM drops from 100% to (80% \u223c 90%). This indicates that decoupling the knowledge of LLM and IR in an explainable way that only provides LLM with its unknown or wrong knowledge to avoid misleading LLM is important."
        }
    ],
    "title": "Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks",
    "year": 2023
}