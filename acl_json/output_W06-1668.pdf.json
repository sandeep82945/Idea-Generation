{
    "abstractText": "In this paper we show that generative models are competitive with and sometimes superior to discriminative models, when both kinds of models are allowed to learn structures that are optimal for discrimination. In particular, we compare Bayesian Networks and Conditional loglinear models on two NLP tasks. We observe that when the structure of the generative model encodes very strong independence assumptions (a la Naive Bayes), a discriminative model is superior, but when the generative model is allowed to weaken these independence assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kristina Toutanova"
        }
    ],
    "id": "SP:16a0997753ad615d1cd0aefb7dd59a3fd8437b2a",
    "references": [
        {
            "authors": [
                "Michael Collins"
            ],
            "title": "Discriminative reranking for natural",
            "year": 2000
        },
        {
            "authors": [
                "Lafferty."
            ],
            "title": "Inducing features of random fields",
            "venue": "IEEE",
            "year": 1997
        },
        {
            "authors": [
                "Joshua T. Goodman"
            ],
            "title": "A bit of progress in language",
            "year": 2001
        },
        {
            "authors": [
                "Mark Johnson"
            ],
            "title": "Joint and conditional estimation",
            "year": 2001
        },
        {
            "authors": [
                "John Lafferty",
                "Andrew McCallum",
                "Fernando Pereira."
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "Proc. 18th International Conf. on Machine Learning, pages 282\u2013289. Morgan Kaufmann, San Francisco, CA.",
            "year": 2001
        },
        {
            "authors": [
                "Andrew McCallum."
            ],
            "title": "Efficiently inducing features of conditional random fields",
            "venue": "Proceedings of UAI.",
            "year": 2003
        },
        {
            "authors": [
                "Andrew Ng",
                "Michael Jordan."
            ],
            "title": "On discriminative vs",
            "venue": "generative classifiers: A comparison of logistic regression and Naive Bayes. In NIPS 14.",
            "year": 2002
        },
        {
            "authors": [
                "Franz Josef Och",
                "Hermann Ney."
            ],
            "title": "Discriminative training and maximum entropy models for statistical machine translation",
            "venue": "Proceedings of ACL, pages 295\u2013302.",
            "year": 2002
        },
        {
            "authors": [
                "Martha Palmer",
                "Dan Gildea",
                "Paul Kingsbury."
            ],
            "title": "The proposition bank: An annotated corpus of semantic roles",
            "venue": "Computational Linguistics.",
            "year": 2005
        },
        {
            "authors": [
                "Judea Pearl."
            ],
            "title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference",
            "venue": "Morgan Kaufmann.",
            "year": 1988
        },
        {
            "authors": [
                "Franz Pernkopf",
                "Jeff Bilmes."
            ],
            "title": "Discriminative versus generative parameter and structure learning of bayesian network classifiers",
            "venue": "Proceedings of ICML.",
            "year": 2005
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Kadri Hacioglu",
                "Valerie Krugler",
                "Wayne Ward",
                "James Martin",
                "Dan Jurafsky."
            ],
            "title": "Support vector learning for semantic argument classification",
            "venue": "Machine Learning Journal.",
            "year": 2005
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Wayne Ward",
                "Kadri Hacioglu",
                "James Martin",
                "Daniel Jurafsky."
            ],
            "title": "Semantic role labeling using different syntactic views",
            "venue": "Proceedings of ACL.",
            "year": 2005
        },
        {
            "authors": [
                "Vasin Punyakanok",
                "Dan Roth",
                "Wen tau Yih."
            ],
            "title": "The necessity of syntactic parsing for semantic role labeling",
            "venue": "Proceedings of IJCAI.",
            "year": 2005
        },
        {
            "authors": [
                "Rajat Raina",
                "Yirong Shen",
                "Andrew Y. Ng",
                "Andrew McCallum."
            ],
            "title": "Classification with hybrid generative/discriminative models",
            "venue": "Sebastian Thrun, Lawrence Saul, and Bernhard Sch\u00f6lkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cam-",
            "year": 2004
        },
        {
            "authors": [
                "Adwait Ratnaparkhi",
                "Jeff Reynar",
                "Salim Roukos."
            ],
            "title": "A maximum entropy model for prepositional phrase attachment",
            "venue": "Workshop on Human Language Technology.",
            "year": 1994
        },
        {
            "authors": [
                "Brian Roark",
                "Murat Saraclar",
                "Michael Collins",
                "Mark Johnson."
            ],
            "title": "Discriminative language modeling with conditional random fields and the perceptron algorithm",
            "venue": "Proceedings of ACL.",
            "year": 2004
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Dan Klein",
                "Christopher D. Manning."
            ],
            "title": "Feature-rich part-of-speech tagging with a cyclic dependency network",
            "venue": "Proceedings of HLT-NAACL.",
            "year": 2003
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Aria Haghighi",
                "Christopher D. Manning."
            ],
            "title": "Joint learning improves semantic role labeling",
            "venue": "Proceedings of ACL.",
            "year": 2005
        },
        {
            "authors": [
                "Bram Vanschoenwinkel",
                "Bernard Manderick."
            ],
            "title": "A weighted polynomial information gain kernel for resolving prepositional phrase attachment ambiguities with support vector machines",
            "venue": "IJCAI.",
            "year": 2003
        },
        {
            "authors": [
                "Ian H. Witten",
                "Timothy C. Bell."
            ],
            "title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression",
            "venue": "IEEE Transactions on Information Theory, 37,4:1085\u20131094.",
            "year": 1991
        },
        {
            "authors": [
                "Nianwen Xue",
                "Martha Palmer."
            ],
            "title": "Calibrating features for semantic role labeling",
            "venue": "Proceedings of EMNLP. 584",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576\u2013584, Sydney, July 2006.c\u00a92006 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005).\nThe superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative mod-\nels can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models.\nGenerative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well.\nIn this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can.\nStructure learning is very efficient for generative models in the form of directed graphical models (Bayesian Networks (Pearl, 1988)), since the optimal parameters for such models can be estimated in closed form. We compare Bayesian Net-\n576\nworks with structure learning to their closely related discriminative counterpart \u2013 conditional loglinear models with structure learning. Our conditional log-linear models can also be seen as Conditional Random Fields (Lafferty et al., 2001), except we do not have a structure on the labels, but want to learn a structure on the features.\nWe compare the two kinds of models on two NLP classification tasks \u2013 prepositional phrase attachment and semantic role labelling. Our results show that the generative models are competitive with or better than the discriminative models. When a small set of interpolation parameters for the conditional probability tables are fit discriminatively, the resulting hybrid generativediscriminative models perform better than the generative only models and sometimes better than the discriminative models.\nIn Section 2, we describe in detail the form of the generative and discriminative models we study and our structure search methodology. In Section 3 we present the results of our empirical study."
        },
        {
            "heading": "2 Model Classes and Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Generative Models",
            "text": "In classification tasks, given a training set of instances D = {[xi, yi]}, where xi are the input features for the i-th instance, and yi is its label, the task is to learn a classifier that predicts the labels of new examples. If X is the space of inputs and Y is the space of labels, a classifier is a function f : X \u2192 Y . A generative model is one that models the joint probability of inputs and labels PD(x, y) through a distribution P\u03b8(x, y), dependent on some parameter vector \u03b8. The classifier based on this generative model chooses the most likely label given an input according to the conditionalized estimated joint distribution. The parameters \u03b8 of the fitted distribution are usually estimated using the maximum joint likelihood estimate, possibly with a prior.\nWe study generative models represented as Bayesian Networks (Pearl, 1988), because their parameters can be estimated extremely fast as the maximizer of the joint likelihood is the closed form relative frequency estimate. A Bayesian Network is an acyclic directed graph over a set of nodes. For every variable Z, let Pa(Z) denote the set of parents of Z. The structure of the Bayesian Network encodes the following set of indepen-\ndence assumptions: every variable is conditionally independent of its non-descendants given its parents. For example, the structure of the Bayesian Network model in Figure 1 encodes the independence assumption that the input features are conditionally independent given the class label.\nLet the input be represented as a vector of m nominal features. We define Bayesian Networks over the m input variables X1, X2, . . . , Xm and the class variable Y . In all networks, we add links from the class variable Y to all input features. In this way we have generative models which estimate class-specific distributions over features P (X|Y ) and a prior over labels P (Y ). Figure 1 shows a simple Bayesian Network of this form, which is the well-known Naive Bayes model.\nA specific joint distribution for a given Bayesian Network (BN) is given by a set of conditional probability tables (CPTs) which specify the distribution over each variable given its parents P (Z|Pa(Z)). The joint distribution P (Z1, Z2, . . . , Zm) is given by:\nP (Z1, Z2, . . . , Zm) = \u220f\ni=1...m\nP (Zi|Pa(Zi))\nThe parameters of a Bayesian Network model given its graph structure are the values of the conditional probabilities P (Zi|Pa(Zi)). If the model is trained through maximizing the joint likelihood of the data, the optimal parameters are the relative frequency estimates: P\u0302 (Zi = v|Pa(Zi) = ~u) =\ncount(Zi=v,Pa(Zi)=~u) count(Pa(Zi)=~u) Here v denotes a value of Zi and ~u denotes a vector of values for the parents of Zi.\nMost often smoothing is applied to avoid zero probability estimates. A simple form of smoothing is add-\u03b1 smoothing which is equivalent to a Dirichlet prior. For NLP tasks it has been shown that other smoothing methods are far superior to add-\u03b1 smoothing \u2013 see, for example, Goodman\n(2001). In particular, it is important to incorporate lower-order information based on subsets of the conditioning information. Therefore we assume a structural form of the conditional probability tables which implements a more sophisticated type of smoothing \u2013 interpolated Witten-Bell (Witten and Bell, 1991). This kind of smoothing has also been used in the generative parser of (Collins, 1997) and has been shown to have a relatively good performance for language modeling (Goodman, 2001).\nTo describe the form of the conditional probability tables, we introduce some notation. Let Z denote a variable in the BN and Z1, Z2, . . . , Zk denote the set of its parents. The probability P (Z = z|Z1 = z1, Z2 = z2, . . . , Zk = zk) is estimated using Witten-Bell smoothing as follows: (below the tuple of values z1, z2, . . . , zk is denoted by z1k).\nPWB(z|z1k) = \u03bb(z1k)\u00d7 P\u0302 (z|z1k) + (1\u2212 \u03bb(z1k))\u00d7 PWB(z|z1k\u22121)\nIn the above equation, P\u0302 is the relative frequency estimator. The recursion is ended by interpolating with a uniform distribution 1\nVz , where Vz\nis the vocabulary of values for the prediction variable Z. We determine the interpolation back-off order by looking at the number of values of each variable. We apply the following rule: the variable with the highest number of values observed in the training set is backed off first, then the variable with the next highest number of values, and so on. Typically, the class variable will be backed-off last according to this rule.\nIn Witten-Bell smoothing, the values of the interpolation coefficients are as follows: \u03bb(z1k) = count(z1k) count(z1k)+d\u00d7|z:count(z,z1k)>0|\n. The weight of the relative frequency estimate based on a given context increases if the context has been seen more often in the training data and decreases if the context has been seen with more different values for the predicted variable z.\nLooking at the form of our conditional probability tables, we can see that the major parameters are estimated directly based on the counts of the events in the training data. In addition, there are interpolation parameters (denoted by d above), which participate in computing the interpolation weights \u03bb. The d parameters are hyper-parameters and we learn them on a development set of samples. We experimented with learning a single d parameter which is shared by all CPTs and learning multiple d parameters \u2013 one for every type of\nconditioning context in every CPT \u2013 i.e., each CPT has as many d parameters as there are back-off levels.\nWe place some restrictions on the Bayesian Networks learned, for closer correspondence with the discriminative models and for tractability: Every input variable node has the label node as a parent, and at most three parents per variable are allowed."
        },
        {
            "heading": "2.1.1 Structure Search Methodology",
            "text": "Our structure search method differs slightly from previously proposed methods in the literature (Heckerman, 1999; Pernkopf and Bilmes, 2005). The search space is defined as follows. We start with a Bayesian Network containing only the class variable. We denote by CHOSEN the set of variables already in the network and by REMAINING the set of unplaced variables. Initially, only the class variable Y is in CHOSEN and all other variables are in REMAINING. Starting from the current BN, the set of next candidate structures is defined as follows: For every unplaced variable R in REMAINING, and for every subset Sub of size at most two from the already placed variables in CHOSEN, consider adding R with parents Sub\u222aY to the current BN. Thus the number of candidate structures for extending a current BN is on the order of m3, where m is the number of variables.\nWe perform a greedy search. At each step, if the best variable B with the best set of parents Pa(B) improves the evaluation criterion, move B from REMAINING to CHOSEN, and continue the search until there are no variables in REMAINING or the evaluation criterion can not be improved.\nThe evaluation criterion for BNs we use is classification accuracy on a development set of samples. Thus our structure search method is discriminative, in the terminology of (Grossman and Domingos, 2004; Pernkopf and Bilmes, 2005). It is very easy to evaluate candidate BN structures. The main parameters in the CPTs are estimated via the relative frequency estimator on the training set, as discussed in the previous section. We do not fit the hyper-parameters d during structure search. We fit these parameters only after we have selected a final BN structure. Throughout the structure search, we use a fixed value of 1 for d for all CPTs and levels of back-off. Therefore we are using generative parameter estimation and discriminative structure search. See Section 4 for discussion on how this method relates to previous work.\nNotice that the optimal parameters of the conditional probability tables of variables already in the current BN do not change at all when a new variable is added, thus making update very efficient. After the stopping criterion is met, the hyper-parameters of the resulting BN are fit on the development set. As discussed in the previous subsection, we fit either a single or multiple hyper-parameters d. The fitting criterion for the generative Bayesian Networks is joint likelihood of the development set of samples with a Gaussian prior on the values log(d). 1\nAdditionally, we explore fitting the hyperparameters of the Bayesian Networks by optimizing the conditional likelihood of the development set of samples. In this case we call the resulting models Hybrid Bayesian Network models, since they incorporate a number of discriminatively trained parameters. Hybrid models have been proposed before and shown to perform very competitively (Raina et al., 2004; Och and Ney, 2002). In Section 3.2 we compare generative and hybrid Bayesian Networks."
        },
        {
            "heading": "2.2 Discriminative Models",
            "text": "Discriminative models learn a conditional distribution P\u03b8(Y | ~X) or discriminant functions that discriminate between classes. Here we concentrate on conditional log-linear models. A simple example of such model is logistic regression, which directly corresponds to Naive Bayes but is trained to maximize the conditional likelihood. 2\nTo describe the form of models we study, let us introduce some notation. We represent a tuple of nominal variables (X1,X2,. . . ,Xm) as a vector of 0s and 1s in the following standard way: We map the tuple of values of nominal variables to a vector space with dimensionality the sum of possible values of all variables. There is a single dimension in the vector space for every value of each input variable Xi. The tuple (X1,X2,. . . ,Xm) is mapped to a vector which has 1s in m places, which are the corresponding dimensions for the values of each variable Xi. We denote this mapping by \u03a6.\nIn logistic regression, the probability of a label Y = y given input features \u03a6(X1, X2, . . . , Xk) =\n1Since the d parameters are positive we convert the problem to unconstrained optimization over parameters \u03b3 such that d = e\u03b3 .\n2Logistic regression additionally does not have the sum to one constraint on weights but it can be shown that this does not increase the representational power of the model.\n~x is estimated as:\nP (y|~x) = exp \u3008 ~wy, ~x\u3009\u2211 y\u2032 exp \u3008 ~wy\u2032 , ~x\u3009\nThere is a parameter vector of feature weights ~wy for each label y. We fit the parameters of the log-linear model by maximizing the conditional likelihood of the training set including a gaussian prior on the parameters. The prior has mean 0 and variance \u03c32. The variance is a hyper-parameter, which we optimize on a development set.\nIn addition to this simple logistic regression model, as for the generative models, we consider models with much richer structure. We consider more complex mappings \u03a6, which incorporate conjunctions of combinations of input variables. We restrict the number of variables in the combinations to three, which directly corresponds to our limit on number of parents in the Bayesian Network structures. This is similar to considering polynomial kernels of up to degree three, but is more general, because, for example, we can add only some and not all bigram conjunctions of variables. Structure search (or feature selection) for log-linear models has been done before e.g. (Della Pietra et al., 1997; McCallum, 2003). We devise our structure search methodology in a way that corresponds as closely as possible to our structure search for Bayesian Networks. The exact hypothesis space considered is defined by the search procedure for an optimal structure we apply, which we describe next."
        },
        {
            "heading": "2.2.1 Structure Search Methodology",
            "text": "We start with an initial empty feature set and a candidate feature set consisting of all input features: CANDIDATES={X1,X2,. . . ,Xm}. In the course of the search, the set CANDIDATES may contain feature conjunctions in addition to the initial input features. After a feature is selected from the candidates set and added to the model, the feature is removed from CANDIDATES and all conjunctions of that feature with all input features are added to CANDIDATES. For example, if a feature conjunction \u3008Xi1 ,Xi2 ,. . .,Xin\u3009 is selected, all of its expansions of the form \u3008Xi1 ,Xi2 ,. . .,Xin ,Xi\u3009, where Xi is not in the conjunction already, are added to CANDIDATES.\nWe perform a greedy search and at each step select the feature which maximizes the evaluation criterion, add it to the model and extend the set\nCANDIDATES as described above. The evaluation criterion for selecting features is classification accuracy on a development set of samples, as for the Bayesian Network structure search.\nAt each step, we evaluate all candidate features. This is computationally expensive, because it requires iterative re-estimation. In addition to estimating weights for the new features, we reestimate the old parameters, since their optimal values change. We did not preform search for the hyper-parameter \u03c3 when evaluating models. We fit \u03c3 by optimizing the development set accuracy after a model was selected. Note that our feature selection algorithm adds an input variable or a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Problems and Datasets",
            "text": "We study two classification problems \u2013 prepositional phrase (PP) attachment, and semantic role labeling.\nFollowing most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to determine if the PP should be attached to the verb or to the object noun phrase. For example, in the sentence: Never [hang]V [a painting]NP [with a peg]PP , the prepositional phrase with a peg can either modify the verb hang or the object noun phrase a painting. Here, clearly, with a peg modifies the verb hang.\nWe follow the common practice in representing the problem using only the head words of these constituents and of the NP inside the PP. Thus the example sentence is represented as the following quadruple: [v:hang n1:painting p:with n2:peg]. Thus for the PP attachment task we have binary labels Att , and four input variables \u2013 v, n1, p, n2.\nWe work with the standard dataset previously used for this task by other researchers (Ratna-\nparkhi et al., 1994; Collins and Brooks, 1995). It is extracted from the the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994). Table 1 shows summary statistics for the dataset.\nThe second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts.\nThere has been plenty of work on machine learning models for semantic role labeling, starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and Ma\u0300rquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004).\nMany features have been proposed for building models for semantic role labeling. Initially, 7 features were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. These are the features we use as well. Table 2 lists the features. State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al., 2005a), as well as global features including the labels of other nodes in parse tree. Nevertheless it is interesting to see how well we can do with these 7 features only.\nWe use the standard training, development, and\ntest sets from the February 2004 version of Propbank. The training set consists of sections 2 to 21, the development set is from section 24, and the test set is from section 23. The number of samples is listed in Table 1. As we can see, the training set size is much larger compared to the PP attachment training set."
        },
        {
            "heading": "3.2 Results",
            "text": "In line with previous work (Ng and Jordan, 2002; Klein and Manning, 2002), we first compare Naive Bayes and Logistic regression on the two NLP tasks. This lets us see how they compare when the generative model is making strong independence assumptions and when the two kinds of models have the same structure. Then we compare the generative and discriminative models with learned richer structures.\nTable 3 shows the Naive Bayes/Logistic regression results for PP attachment. We list results for several conditions of training the Naive Bayes classifier, depending on whether it is trained as strictly generative or as a hybrid model, and whether a single or multiple hyper-parameters d are trained. In the table, we see results for generative Naive Bayes, where the d parameters are trained to maximize the joint likelihood of the development set, and for Hybrid Naive Bayes, where the hyper-parameters are trained to optimize the conditional likelihood. The column H-Params (for hyper-parameters) indicates whether a single or multiple d parameters are learned.\nLogistic regression is more fairly comparable to Naive Bayes trained using a single hyperparameter, because it also uses a single hyperparameter \u03c3 trained on a development set. However, for the generative model it is very easy to train multiple weights d since the likelihood of a development set is differentiable with respect to the parameters. For logistic regression, we may want to choose different variances for the different types of features but the search would be pro-\nhibitively expensive. Thus we think it is also fair to fit multiple interpolation weights for the generative model and we show these results as well.\nAs we can see from the table, logistic regression outperforms both Naive Bayes and Hybrid Naive Bayes. The performance of Hybrid Naive Bayes with multiple interpolation weights improves the accuracy, but performance is still better for logistic regression. This suggests that the strong independence assumptions are hurting the classifier. According to McNemar\u2019s test, logistic regression is statistically significantly better than the Naive Bayes models and than Hybrid Naive Bayes with a single interpolation weight (p < 0.025), but is not significantly better than Hybrid Naive Bayes with multiple interpolation parameters at level 0.05.\nHowever, when both the generative and discriminative models are allowed to learn optimal structures, the generative model outperforms the discriminative model. As seen from Table 4, the Bayesian Network with a single interpolation weight achieves an accuracy of 84.6%, whereas the discriminative model performs at 83.8%. The hybrid model with a single interpolation weight does even better, achieving 85.0% accuracy. For comparison, the model of Collins & Brooks has accuracy of 84.15% on this test set, and the highest result obtained through a discriminative model with this feature set is 84.8%, using SVMs and a polynomial kernel with multiple hyper-parameters (Vanschoenwinkel and Manderick, 2003). The Hybrid Bayes Nets are statistically significantly better than the Log-linear model (p < 0.05), and the Bayes Nets are not significantly better than the Log-linear model. All models from Table 4 are significantly better than all models in Table 3.\nFor semantic role labelling classification of core arguments, the results are listed in Tables 5 and 6. We can see that the difference in performance between Naive Bayes with a single interpolation parameter d \u2013 83.3% and the performance of Logistic regression \u2013 91.1%, is very large. This shows that the independence assumptions are quite\nstrong, and since many of the features are not sparse lexical features and training data for them is sufficient, the Naive Bayes model has no advantage over the discriminative logistic regression model. The Hybrid Naive Bayes model with multiple interpolation weights does better than Naive Bayes, performing at 86.5%. All differences between the classifiers in Table 5 are statistically significant at level 0.01. Compared to the PP attachment task, here we are getting more benefit from multiple hyper-parameters, perhaps due to the diversity of the features for SRL: In SRL, we use both sparse lexical features and non-sparse syntactic ones, whereas all features for PP attachment are lexical.\nFrom Table 6 we can see that when we compare general Bayesian Network structures to general log-linear models, the performance gap between the generative and discriminative models is much smaller. The Bayesian Network with a single interpolation weight d has 93.5% accuracy and the log-linear model has 93.9% accuracy. The hybrid model with multiple interpolation weights performs at 93.7%. All models in Table 6 are in a statistical tie according to McNemar\u2019s test, and thus the log-linear model is not significantly better than the Bayes Net models. We can see that the generative model was able to learn a structure with a set of independence assumptions which are not as strong as the ones the Naive Bayes model makes, thus resulting in a model with performance competitive with the discriminative model.\nFigures 2(a) and 2(b) show the Bayesian Networks learned for PP Attachment and Semantic Role Labeling. Table 7 shows the conjunctions\nlearned by the Log-linear models for PP attachment and SRL.\nWe should note that it is much faster to do structure search for the generative Bayesian Network model, as compared to structure search for the log-linear model. In our implementation, we did not do any computation reuse between successive steps of structure search for the Bayesian Network or log-linear models. Structure search took 2 hours for the Bayesian Network and 24 hours for the log-linear model.\nTo put our results in the context of previous work, other results on core arguments using the same input features have been reported, the best being 91.4% for an SVM with a degree 2 polynomial kernel (Pradhan et al., 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning."
        },
        {
            "heading": "4 Comparison to Related Work",
            "text": "Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001).\n3This result is on an older version of Propbank from July 2002.\nBayesian Networks with special structure of the CPTs \u2013 e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure learning have been previously performed ((Pernkopf and Bilmes, 2005) and (Grossman and Domingos, 2004)) for other, non-NLP domains. There are several important algorithmic differences between our work and that of (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004). We detail the differences here and perform an empirical evaluation of the impact of some of these differences.\nForm of the generative models. The generative models studied in that previous work do not employ any special form of the conditional probability tables. Pernkopf and Bilmes (2005) use a simple smoothing method: fixing the probability of every event that has a zero relative frequency estimate to a small fixed . Thus the model does not take into account information from lower order distributions and has no hyper-parameters that are being fit. Grossman and Domingos (2004) do not employ a special form of the CPTs either and do not mention any kind of smoothing used in the generative model learning.\nForm of the discriminative models. The works (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004) study Bayesian Networks whose parameters are trained discriminatively (by\nmaximizing conditional likelihood), as representatives of discriminative models. We study more general log-linear models, equivalent to Markov Random Fields. Our models are more general in that their parameters do not need to be interpretable as probabilities (sum to 1 and between 0 and 1), and the structures do not need to correspond to Bayes Net structures. For discriminative classifiers, it is not important that their component parameters be interpretable as probabilities; thus this restriction is probably unnecessary. Like for the generative models, another major difference is in the smoothing algorithms. We smooth the models both by fitting a gaussian prior hyperparameter and by incorporating features of subsets of cliques. Smoothing in (Pernkopf and Bilmes, 2005) is done by substituting zero-valued parameters with a small fixed . Grossman and Domingos (2004) employ early stopping using held-out data which can achieve similar effects to smoothing with a gaussian prior.\nTo evaluate the importance of the differences between our algorithm and the ones presented in these works, and to evaluate the importance of fitting hyper-parameters for smoothing, we implemented a modified version of our structure search. The modifications were as follows. For Bayes Net structure learning: (i) no Witten-Bell smoothing is employed in the CPTs, and (ii) no backoffs to lower-order distributions are considered. The only smoothing remaining in the CPTs is an interpolation with a uniform distribution with a fixed weight of \u03b1 = .1. For discriminative log-linear model structure learning: (i) the gaussian prior was fixed to be very weak, serving only to keep the weights away from infinity (\u03c3 = 100) and (ii) the conjunction selection was restricted to correspond to a Bayes Net structure with no features for subsets of feature conjunctions. Thus the only difference between the class of our modified discriminative log-linear models and the class of models considered in (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004) is that we do not restrict the parameters to be interpretable as probabilities.\nThe results shown in Table 8 summarize the results obtained by the modified algorithm on the two tasks. Both the generative and discriminative learners suffered a statistically significant (at level .01) loss in performance. Notably, the log-linear model for PP attachment performs worse than logistic regression with better smoothing.\nIn summary, our results showed that by learning the structure for generative models, we can obtain models which are competitive with or better than corresponding discriminative models. We also showed the importance of employing sophisticated smoothing techniques in structure search algorithms for natural language classification tasks."
        }
    ],
    "title": "Competitive generative models with structure learning for NLP classification tasks",
    "year": 2006
}