{
    "abstractText": "The task of unsupervised semantic segmentation aims to cluster pixels into semantically meaningful groups. Specifically, pixels assigned to the same cluster should share high-level semantic properties like their object or part category. This paper presents MaskDistill: a novel framework for unsupervised semantic segmentation based on three key ideas. First, we advocate a data-driven strategy to generate object masks that serve as a pixel grouping prior for semantic segmentation. This approach omits handcrafted priors, which are often designed for specific scene compositions and limit the applicability of competing frameworks. Second, MaskDistill clusters the object masks to obtain pseudo-ground-truth for training an initial object segmentation model. Third, we leverage this model to filter out low-quality object masks. This strategy mitigates the noise in our pixel grouping prior and results in a clean collection of masks which we use to train a final segmentation model. By combining these components, we can considerably outperform previous works for unsupervised semantic segmentation on PASCAL (+11% mIoU) and COCO (+4% mask AP50). Interestingly, as opposed to existing approaches, our framework does not latch onto low-level image cues and is not limited to object-centric datasets. The code and models are available. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Wouter Van Gansbeke"
        },
        {
            "affiliations": [],
            "name": "Simon Vandenhende"
        },
        {
            "affiliations": [],
            "name": "Luc Van Gool"
        }
    ],
    "id": "SP:05ae5c5581a40f42a5cf38e84d3f455fe69c12fe",
    "references": [
        {
            "authors": [
                "H. Abu Alhaija",
                "S.K. Mustikovela",
                "L. Mescheder",
                "A. Geiger",
                "C. Rother"
            ],
            "title": "Augmented reality meets computer vision: Efficient data generation for urban driving",
            "venue": "scenes. International Journal of Computer Vision (IJCV)",
            "year": 2018
        },
        {
            "authors": [
                "B. Alexe",
                "T. Deselaers",
                "V. Ferrari"
            ],
            "title": "Measuring the objectness of image windows",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
            "year": 2012
        },
        {
            "authors": [
                "P. Arbel\u00e1ez",
                "J. Pont-Tuset",
                "J.T. Barron",
                "F. Marques",
                "J. Malik"
            ],
            "title": "Multiscale combinatorial grouping",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2014
        },
        {
            "authors": [
                "Y.M. Asano",
                "M. Patrick",
                "C. Rupprecht",
                "A. Vedaldi"
            ],
            "title": "Labelling unlabelled videos from scratch with multi-modal self-supervision",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Y.M. Asano",
                "C. Rupprecht",
                "A. Vedaldi"
            ],
            "title": "Self-labelling via simultaneous clustering and representation learning",
            "venue": "In: International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "A. Bearman",
                "O. Russakovsky",
                "V. Ferrari",
                "L. Fei-Fei"
            ],
            "title": "What\u2019s the point: Semantic segmentation with point supervision",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2016
        },
        {
            "authors": [
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Unsupervised learning by predicting noise",
            "venue": "In: International Conference on Machine Learning",
            "year": 2017
        },
        {
            "authors": [
                "C. Burgess",
                "L. Matthey",
                "N. Watters",
                "R. Kabra",
                "I. Higgins",
                "M. Botvinick",
                "A. Lerchner"
            ],
            "title": "Monet: Unsupervised scene decomposition and representation",
            "year": 2019
        },
        {
            "authors": [
                "M. Caron",
                "P. Bojanowski",
                "A. Joulin",
                "M. Douze"
            ],
            "title": "Deep clustering for unsupervised learning of visual features",
            "venue": "European Conference on Computer Vision (ECCV) (2018)",
            "year": 2018
        },
        {
            "authors": [
                "M. Caron",
                "P. Bojanowski",
                "J. Mairal",
                "A. Joulin"
            ],
            "title": "Unsupervised pre-training of image features on noncurated data",
            "venue": "In: International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "M. Caron",
                "I. Misra",
                "J. Mairal",
                "P. Goyal",
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "M. Caron",
                "H. Touvron",
                "I. Misra",
                "H. J\u00e9gou",
                "J. Mairal",
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In: International Conference on Computer Vision (ICCV) (2021)",
            "year": 2021
        },
        {
            "authors": [
                "J. Carreira",
                "C. Sminchisescu"
            ],
            "title": "Cpmc: Automatic object segmentation using constrained parametric min-cuts",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
            "year": 2012
        },
        {
            "authors": [
                "L.C. Chen",
                "G. Papandreou",
                "F. Schroff",
                "H. Adam"
            ],
            "title": "Rethinking atrous convolution for semantic image segmentation",
            "year": 2017
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In: International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "K. He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "S. Xie",
                "K. He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "In: International Conference on Computer Vision (ICCV) (2021)",
            "year": 2021
        },
        {
            "authors": [
                "M.T. Chiu",
                "X. Xu",
                "Y. Wei",
                "Z. Huang",
                "A.G. Schwing",
                "R. Brunner",
                "H. Khachatrian",
                "H. Karapetyan",
                "I. Dozier",
                "G Rose"
            ],
            "title": "Agriculture-vision: A large aerial image database for agricultural pattern analysis",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "J.H. Cho",
                "U. Mall",
                "K. Bala",
                "B. Hariharan"
            ],
            "title": "Picie: Unsupervised semantic segmentation using invariance and equivariance in clustering",
            "year": 2021
        },
        {
            "authors": [
                "M. Cordts",
                "M. Omran",
                "S. Ramos",
                "T. Rehfeld",
                "M. Enzweiler",
                "R. Benenson",
                "U. Franke",
                "S. Roth",
                "B. Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "J. Dai",
                "K. He",
                "J. Sun"
            ],
            "title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation",
            "venue": "In: International Conference on Computer Vision (ICCV)",
            "year": 2015
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2009
        },
        {
            "authors": [
                "K. Desai",
                "J. Johnson"
            ],
            "title": "Virtex: Learning visual representations from textual annotations",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In: International Conference on Learning Representations (ICLR) (2021)",
            "year": 2021
        },
        {
            "authors": [
                "I. Endres",
                "D. Hoiem"
            ],
            "title": "Category-independent object proposals with diverse ranking",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
            "year": 2014
        },
        {
            "authors": [
                "M. Engelcke",
                "A. Kosiorek",
                "O.P. Jones",
                "I. Posner"
            ],
            "title": "Genesis: Generative scene inference and sampling with object-centric latent representations",
            "venue": "In: International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International Journal of Computer Vision (IJCV)",
            "year": 2010
        },
        {
            "authors": [
                "S. Gidaris",
                "P. Singh",
                "N. Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "In: International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "P. Goyal",
                "M. Caron",
                "B. Lefaudeux",
                "M. Xu",
                "P. Wang",
                "V. Pai",
                "M. Singh",
                "V. Liptchinsky",
                "I. Misra",
                "A Joulin"
            ],
            "title": "Self-supervised pretraining of visual features in the wild",
            "year": 2021
        },
        {
            "authors": [
                "P. Goyal",
                "D. Mahajan",
                "A. Gupta",
                "I. Misra"
            ],
            "title": "Scaling and benchmarking self-supervised visual representation learning",
            "venue": "In: International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "K. Grauman",
                "A. Westbury",
                "E. Byrne",
                "Z. Chavis",
                "A. Furnari",
                "R. Girdhar",
                "J. Hamburger",
                "H. Jiang",
                "M. Liu",
                "X Liu"
            ],
            "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
            "year": 2021
        },
        {
            "authors": [
                "K. Greff",
                "R.L. Kaufmann",
                "R. Kabra",
                "N. Watters",
                "C. Burgess",
                "D. Zoran",
                "L. Matthey",
                "M. Botvinick",
                "A. Lerchner"
            ],
            "title": "Multi-object representation learning with iterative variational inference",
            "venue": "In: International Conference on Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "J.B. Grill",
                "F. Strub",
                "F. Altch\u00e9",
                "C. Tallec",
                "P.H. Richemond",
                "E. Buchatskaya",
                "C. Doersch",
                "B.A. Pires",
                "Z.D. Guo",
                "Azar",
                "M.G"
            ],
            "title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "M. Hamilton",
                "Z. Zhang",
                "B. Hariharan",
                "N. Snavely",
                "W.T. Freeman"
            ],
            "title": "Unsupervised semantic segmentation by distilling feature correspondences",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "H. Fan",
                "Y. Wu",
                "S. Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In: Conference on Computer Vision and Pattern Recognition (CVPR) (2016)",
            "year": 2016
        },
        {
            "authors": [
                "S. Hong",
                "H. Noh",
                "B. Han"
            ],
            "title": "Decoupled deep neural network for semi-supervised semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2015
        },
        {
            "authors": [
                "W.C. Hung",
                "Y.H. Tsai",
                "Y.T. Liou",
                "Y.Y. Lin",
                "M.H. Yang"
            ],
            "title": "Adversarial learning for semi-supervised semantic segmentation",
            "year": 2018
        },
        {
            "authors": [
                "J.J. Hwang",
                "S.X. Yu",
                "J. Shi",
                "M.D. Collins",
                "T.J. Yang",
                "X. Zhang",
                "L.C. Chen"
            ],
            "title": "Segsort: Segmentation by discriminative sorting of segments. In: Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "S. Iizuka",
                "E. Simo-Serra",
                "H. Ishikawa"
            ],
            "title": "Let there be color! joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification",
            "year": 2016
        },
        {
            "authors": [
                "P. Isola",
                "D. Zoran",
                "D. Krishnan",
                "E.H. Adelson"
            ],
            "title": "Learning visual groups from co-occurrences in space and time",
            "year": 2015
        },
        {
            "authors": [
                "X. Ji",
                "J.F. Henriques",
                "A. Vedaldi"
            ],
            "title": "Invariant information clustering for unsupervised image classification and segmentation",
            "venue": "In: International Conference on Computer Vision (ICCV) (2019)",
            "year": 2019
        },
        {
            "authors": [
                "J. Johnson",
                "B. Hariharan",
                "L. Van Der Maaten",
                "L. Fei-Fei",
                "C. Lawrence Zitnick",
                "R. Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "A. Khoreva",
                "R. Benenson",
                "J.H. Hosang",
                "M. Hein",
                "B. Schiele"
            ],
            "title": "Simple does it: Weakly supervised instance and semantic segmentation",
            "year": 2017
        },
        {
            "authors": [
                "P. Kr\u00e4henb\u00fchl",
                "V. Koltun"
            ],
            "title": "Efficient inference in fully connected crfs with gaussian edge potentials. In: Advances in Neural Information Processing Systems (NeurIPS",
            "year": 2011
        },
        {
            "authors": [
                "H.W. Kuhn"
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly 2(1-2),",
            "year": 1955
        },
        {
            "authors": [
                "W. Kuo",
                "B. Hariharan",
                "J. Malik"
            ],
            "title": "Deepbox: Learning objectness with convolutional networks",
            "venue": "In: International Conference on Computer Vision (ICCV)",
            "year": 2015
        },
        {
            "authors": [
                "A. Kuznetsova",
                "H. Rom",
                "N. Alldrin",
                "J. Uijlings",
                "I. Krasin",
                "J. Pont-Tuset",
                "S. Kamali",
                "S. Popov",
                "M. Malloci",
                "A Kolesnikov"
            ],
            "title": "The open images dataset v4",
            "venue": "International Journal of Computer Vision (IJCV)",
            "year": 2020
        },
        {
            "authors": [
                "G. Larsson",
                "M. Maire",
                "G. Shakhnarovich"
            ],
            "title": "Colorization as a proxy task for visual understanding",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "D. Lin",
                "J. Dai",
                "J. Jia",
                "K. He",
                "J. Sun"
            ],
            "title": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "T.Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2014
        },
        {
            "authors": [
                "S. Lloyd"
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE Transactions on Information Theory (1982)",
            "year": 1982
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2015
        },
        {
            "authors": [
                "A. Mahendran",
                "J. Thewlis",
                "A. Vedaldi"
            ],
            "title": "Cross pixel optical-flow similarity for self-supervised learning",
            "venue": "Asian Conference of Computer Vision (ACCV)",
            "year": 2018
        },
        {
            "authors": [
                "S. Manen",
                "M. Guillaumin",
                "L. Van Gool"
            ],
            "title": "Prime object proposals with randomized prim\u2019s",
            "venue": "algorithm. In: International Conference on Computer Vision (ICCV)",
            "year": 2013
        },
        {
            "authors": [
                "D.R. Martin",
                "C.C. Fowlkes",
                "D. Tal",
                "J. Malik"
            ],
            "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
            "venue": "In: International Conference on Computer Vision (ICCV)",
            "year": 2001
        },
        {
            "authors": [
                "B.H. Menze",
                "A. Jakab",
                "S. Bauer",
                "J. Kalpathy-Cramer",
                "K. Farahani",
                "J. Kirby",
                "Y. Burren",
                "N. Porz",
                "J. Slotboom",
                "R Wiest"
            ],
            "title": "The multimodal brain tumor image segmentation benchmark (brats)",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
            "year": 2014
        },
        {
            "authors": [
                "S. Minaee",
                "Y.Y. Boykov",
                "F. Porikli",
                "A.J. Plaza",
                "N. Kehtarnavaz",
                "D. Terzopoulos"
            ],
            "title": "Image segmentation using deep learning: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
            "year": 2021
        },
        {
            "authors": [
                "M. Noroozi",
                "P. Favaro"
            ],
            "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2016
        },
        {
            "authors": [
                "M. Noroozi",
                "A. Vinjimoor",
                "P. Favaro",
                "H. Pirsiavash"
            ],
            "title": "Boosting self-supervised learning via knowledge transfer",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Oord",
                "A.v.d",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ouali",
                "C. Hudelot",
                "M. Tami"
            ],
            "title": "Autoregressive unsupervised image segmentation",
            "venue": "European Conference on Computer Vision (ECCV) (2020)",
            "year": 2020
        },
        {
            "authors": [
                "G. Papandreou",
                "L.C. Chen",
                "K.P. Murphy",
                "A.L. Yuille"
            ],
            "title": "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation",
            "venue": "In: International Conference on Computer Vision (ICCV)",
            "year": 2015
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "S. Chintala",
                "G. Chanan",
                "E. Yang",
                "Z. DeVito",
                "Z. Lin",
                "A. Desmaison",
                "L. Antiga",
                "A. Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "D. Pathak",
                "P. Krahenbuhl",
                "J. Donahue",
                "T. Darrell",
                "A.A. Efros"
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "C. Rother",
                "V. Kolmogorov",
                "A. Blake"
            ],
            "title": "Grabcut -interactive foreground extraction using iterated graph cuts",
            "year": 2004
        },
        {
            "authors": [
                "O. Sim\u00e9oni",
                "G. Puy",
                "H.V. Vo",
                "S. Roburin",
                "S. Gidaris",
                "A. Bursuc",
                "P. P\u00e9rez",
                "R. Marlet",
                "J. Ponce"
            ],
            "title": "Localizing objects with self-supervised transformers and no labels. In: British Machine Vision Conference (BMVC",
            "year": 2021
        },
        {
            "authors": [
                "M. Tang",
                "A. Djelouah",
                "F. Perazzi",
                "Y. Boykov",
                "C. Schroers"
            ],
            "title": "Normalized cut loss for weakly-supervised cnn segmentation",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "M. Tang",
                "F. Perazzi",
                "A. Djelouah",
                "I.B. Ayed",
                "C. Schroers",
                "Y. Boykov"
            ],
            "title": "On regularized losses for weakly-supervised cnn segmentation",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Y. Tian",
                "D. Krishnan",
                "P. Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tian",
                "C. Sun",
                "B. Poole",
                "D. Krishnan",
                "C. Schmid",
                "P. Isola"
            ],
            "title": "What makes for good views for contrastive learning",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "J. Uijlings",
                "K. van de Sande",
                "T. Gevers",
                "A. Smeulders"
            ],
            "title": "Selective search for object recognition",
            "venue": "International Journal of Computer Vision (IJCV)",
            "year": 2013
        },
        {
            "authors": [
                "W. Van Gansbeke",
                "S. Vandenhende",
                "S. Georgoulis",
                "M. Proesmans",
                "L. Van Gool"
            ],
            "title": "Scan: Learning to classify images without labels",
            "venue": "European Conference on Computer Vision (ECCV) (2020)",
            "year": 2020
        },
        {
            "authors": [
                "W. Van Gansbeke",
                "S. Vandenhende",
                "S. Georgoulis",
                "L. Van Gool"
            ],
            "title": "Revisiting contrastive methods for unsupervised learning of visual representations",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2021
        },
        {
            "authors": [
                "W. Van Gansbeke",
                "S. Vandenhende",
                "S. Georgoulis",
                "L. Van Gool"
            ],
            "title": "Unsupervised semantic segmentation by contrasting object mask proposals",
            "venue": "In: International Conference on Computer Vision (ICCV) (2021)",
            "year": 2021
        },
        {
            "authors": [
                "S. Vandenhende",
                "S. Georgoulis",
                "L.V. Gool"
            ],
            "title": "Mti-net: Multi-scale task interaction networks for multi-task learning",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "H.V. Vo",
                "P. P\u00e9rez",
                "J. Ponce"
            ],
            "title": "Toward unsupervised, multi-object discovery in large-scale image collections",
            "venue": "European Conference on Computer Vision (ECCV) (2020)",
            "year": 2020
        },
        {
            "authors": [
                "H.V. Vo",
                "E. Sizikova",
                "C. Schmid",
                "P. P\u00e9rez",
                "J. Ponce"
            ],
            "title": "Large-scale unsupervised object discovery. In: Advances in Neural Information Processing Systems (NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "R. Zhang",
                "C. Shen",
                "T. Kong",
                "L. Li"
            ],
            "title": "Dense contrastive learning for self-supervised visual pre-training",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wu",
                "A. Kirillov",
                "F. Massa",
                "W.Y. Lo",
                "R. Girshick"
            ],
            "title": "Detectron2. https://github.com/ facebookresearch/detectron2",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wu",
                "Y. Xiong",
                "S.X. Yu",
                "D. Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Z. Wu",
                "C. Shen",
                "Hengel"
            ],
            "title": "A.v.d.: Bridging category-level and instance-level semantic image segmentation",
            "year": 2016
        },
        {
            "authors": [
                "J. Xu",
                "A.G. Schwing",
                "R. Urtasun"
            ],
            "title": "Learning to segment under various forms of weak supervision",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2015
        },
        {
            "authors": [
                "X. Yan",
                "I. Misra",
                "A. Gupta",
                "D. Ghadiyaram",
                "D. Mahajan"
            ],
            "title": "Clusterfit: Improving generalization of visual representations",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "F. Yu",
                "V. Koltun"
            ],
            "title": "Multi-scale context aggregation by dilated convolutions",
            "venue": "In: International Conference on Learning Representations (ICLR) (2016)",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhan",
                "X. Pan",
                "Z. Liu",
                "D. Lin",
                "C.C. Loy"
            ],
            "title": "Self-supervised learning via conditional motion propagation",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "L. Zhang",
                "G.J. Qi",
                "L. Wang",
                "J. Luo"
            ],
            "title": "Aet vs. aed: Unsupervised representation learning by auto-encoding transformations rather than data",
            "venue": "In: Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros"
            ],
            "title": "Colorful image colorization",
            "venue": "European Conference on Computer Vision (ECCV) (2016)",
            "year": 2016
        },
        {
            "authors": [
                "X. Zhang",
                "M. Maire"
            ],
            "title": "Self-supervised visual representation learning from hierarchical grouping",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS) (2020)",
            "year": 2020
        },
        {
            "authors": [
                "C. Zitnick",
                "P. Dollar"
            ],
            "title": "Edge boxes : Locating object proposals from edges",
            "venue": "European Conference on Computer Vision (ECCV)",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The task of assigning a class label to each pixel in an image \u2013 known as semantic segmentation \u2013 has been researched extensively [54, 59]. Semantic segmentation tools are used in many domains like autonomous driving [20], medical imaging [58], and agriculture [18]. Today, researchers tackle the segmentation task via deep convolutional nets [37] which learn hierarchical image representations from fully-annotated datasets [27, 52] where each pixel is associated with a category label. However, collecting such annotations consumes large amounts of time and money [6]. Therefore, several works explored less labor-intensive forms of annotations to train a segmentation model, e.g., scribbles [51, 69, 70], bounding boxes [21, 45, 64], clicks [6], and image-level tags [64, 70, 85]. Others studied semi-supervised methods [21, 38, 39, 64] that improve the performance by leveraging additional unlabeled images during training. In this paper, we go a step further and learn a segmentation model in a self-supervised way. Specifically, the goal is to learn a clustering function that assigns semantically related pixels to the same cluster without relying on human labeling.\nTo realize this concept, end-to-end methods [9, 43, 63] learned a clustering function by imposing consistency on the cluster assignments of pixels in augmented views of an image. However, these methods tend to latch onto low-level image cues like color or texture (see [19, 74]). In particular, the clusters strongly depend on the network\u2019s initialization leading to degenerate solutions. Unlike these methods, we do not adopt an end-to-end strategy but follow the works discussed next.\n1Code: https://github.com/wvangansbeke/MaskDistill\nPreprint.\nar X\niv :2\n20 6.\n06 36\n3v 1\n[ cs\n.C V\n] 1\n3 Ju\nn 20\nAnother group of works proposed a bottom-up approach for tackling the problem. First, they leverage a low- or mid-level visual prior like edge detection [40, 91] or saliency estimation [76] to find image regions that likely share the same semantics. In a second step, they use the image regions to learn pixel-embeddings that capture semantic information. In particular, the image regions serve as a regularizer which removes the segmentation\u2019s dependence on the network initialization. The pixel-embeddings can subsequently be clustered via K-means to obtain an image segmentation. While bottom-up approaches report better results, they suffer from several drawbacks too. Most importantly, their dependence on a handcrafted prior, e.g., edges or saliency, to group pixels limits their usage. For example, saliency estimation only applies to object-centric images. Additionally, several works require annotations to identify the appropriate image regions. For example, Hwang et al. [40] use boundary annotations from [57].\nThis paper presents MaskDistill, a novel framework that addresses the above limitations. Like bottomup methods, MaskDistill first identifies groups of pixels that likely belong to the same object. Since objectness is a high-level construct [48], we avoid using a handcrafted prior and instead advocate a data-driven approach. We observe that self-supervised vision transformers [12, 17] learn spatially structured image representations. In particular, it\u2019s possible to distill highly accurate object masks through the attention layers in vision transformers [24, 78]. Different from existing works [40, 76, 91] which rely on handcrafted priors, this facilitates the scaling of our framework to more challenging datasets. In particular, handcrafted priors tend to be designed for specific scene compositions. For example, saliency estimation works well for images with few objects (e.g., PASCAL [27]) but fails for more complex scenes (e.g., COCO [52]). Our framework does not suffer from this problem (see Section 4).\nWe cluster the object masks and use the result as pseudo-ground-truth to train an object segmentation model, e.g., Mask R-CNN [36]. As discussed in Section 3.3, this model predicts object mask candidates together with their confidence scores. We empirically observed that higher confidence scores correlate with object masks of better quality (see Figure 5). Based upon this observation, we construct a cleaner set of object masks by leveraging the model\u2019s predictions. In particular, we filter out predictions with low confidence scores for each image. The resulting set of object masks is used as pseudo-ground-truth to train a final semantic segmentation model.\nIn summary, our contributions are: (i) we develop a novel bottom-up framework to tackle the task of unsupervised semantic segmentation (Section 3), (ii) we present a data-driven strategy to get a pixel grouping prior for semantic segmentation based on self-supervised transformer models (Section 3.2), (iii) we analyze the use of confident object mask candidates to refine the segmentation results (Section 3.3), and (iv) we obtain state-of-the-art results on the well-known PASCAL [27] and COCO [52] datasets under the unsupervised setup (Section 4)."
        },
        {
            "heading": "2 Related Work",
            "text": "Unsupervised Semantic Segmentation. Several works tried to segment stuff categories, e.g., sky, grass, mountain, etc. For example, [43, 63] maximized the mutual information between augmented views to learn a segmentation model. Others [9, 19] iteratively refined the segmentation model\u2019s features via a clustering objective. However, these methods rely on the architectural prior which makes them prone to degenerate solutions, and limits their use to small-scale problems, e.g., segmenting roads and vegetation in satellite imagery. We refer to [19, 74, 76] for an analysis. This paper differs from these works in two ways. First, we segment object rather than stuff categories. This setting aligns better with popular segmentation benchmarks, e.g., PASCAL [27], COCO [52], etc. Furthermore, learning object-centric representations is a key component of machine intelligence with applications in augmented reality [1, 31]. Second, unlike the referred works, we do not employ an end-to-end learning strategy which makes our framework less dependent on the architectural prior.\nAs mentioned, we focus on segmenting object categories. Earlier works [40, 75, 91] that studied this problem applied a two-step strategy. First, a handcrafted prior \u2013 e.g., superpixels, boundary maps, or saliency \u2013 is used to find groups of pixels that likely belong to the same object or part. Next, a pixel-level representation is learned that is discriminative of these groups. This allows the representations to be clustered via K-means to get an image segmentation. Our framework differs from these works in three ways. First, we do not use a handcrafted prior. Instead, we leverage the attention mechanism from self-supervised vision transformers [12, 17] to mine object masks as a pixel\ngrouping prior. This data-driven strategy makes fewer assumptions on the scene compositions which eases scaling of our approach (see Section 3.1). Second, unlike earlier works [40, 76, 91], we do not use additional annotations, e.g., boundary maps, to construct our prior. Third, we directly predict the cluster assignments and avoid using K-means as post-processing. For completeness, we include a recent work [34] which also advocates a data-driven approach but considers fewer categories.\nUnsupervised Object Detection. The task of unsupervised object detection aims to produce object candidates without using human annotations. This concept is realized via a class-agnostic objectness scoring function [2] which estimates the probability for an image window to contain an object. Existing methods learned such a function via foreground-background masks[13, 25], superpixels [56, 73] or edge information [92]. Recent approaches [68, 79, 80] have shown promising results on large-scale benchmarks, e.g., COCO [52] and OpenImages [49]. In this work, we employ an object mask distillation strategy that is related to LOST [68]. However, unlike our method, LOST fails to generate multiple object mask candidates per image \u2013 which is critical for the task of semantic segmentation. A few methods [8, 26, 32] do produce several masks per image, but these are limited to small-scale problems (e.g., CLEVR [44]).\nSelf-Supervised Representation Learning. These works learn visual representations from unlabeled images by solving pretext tasks. Some examples include predicting transformations [28, 89], predicting optical flow [55, 88], solving jigsaw puzzles [60, 61], predicting noise [7], performing clustering [4, 5, 9, 10, 86], image colorization [41, 50, 90], inpainting [66], predictive coding [62] etc. The instance discrimination task [15, 17, 35, 62, 83] and its alternatives [12, 16, 33] outperform their supervised counterparts when transferring the representations to various downstream tasks, e.g., object detection. In this work, we explore self-supervised learning to capture objectness, allowing us to mine object mask candidates in a data-driven way."
        },
        {
            "heading": "3 Method",
            "text": "Our approach follows a bottom-up scheme to tackle the unsupervised semantic segmentation task. First, we advocate a data-driven approach to mine object masks via self-supervised vision transformers (Section 3.1). Second, we distill multiple object masks per image via an object segmentation model, i.e., Mask R-CNN (Section 3.2). Third, we discuss how to train a final segmentation model using the found object masks (Section 3.3). As a key component, we use only object masks with high confidence scores. This strategy mitigates the noise introduced during the mask distillation step. Figure 2 shows an overview of our proposed MaskDistill framework.\n3.1 Learning Objectness\nEnd-to-end approaches [9, 43, 63] are unlikely to discover image regions that pertain to high-level object categories [74], e.g., birds, cats, buildings, etc. For this reason, we follow prior work [40, 73, 76], and advocate a bottom-up approach to tackle the task of unsupervised semantic segmentation. In particular, it\u2019s advantageous to break down an image into its different components first, before going after its semantic segmentation. Existing methods achieve this via a handcrafted low-level (e.g., superpixels or edges) or mid-level (e.g., saliency) pixel grouping prior. However, such priors are suboptimal. A lowlevel prior based on superpixels or edges produces an over-segmentation of the image, which yields image regions with low semantic content (see the top row in\nFigure 1). Differently, a mid-level prior can aggregate parts from different objects (see the middle row in Figure 1). To address these drawbacks, we propose to obtain a pixel grouping prior in a data-driven way by relying on self-supervised representation learning. The bottom row in Figure 1 shows some examples. Unlike handcrafted pixel grouping priors, our approach generates object masks that align\nwith true objects. For example, we correctly identify the entire plane in the 1st column, while other methods fail to do so.\nIn this paper, we build on self-supervised vision transformers [12, 17] to mine object masks. The reason for this decision is three-fold. First, transformers reason at patch-level [24, 78] which allows us to construct an affinity graph expressing the similarity between different image regions. Second, self-supervised vision transformers learn rich spatial representations that capture object information [12, 75] which facilitates their use for mining object masks. Moreover, the representations encode detailed information about each image component which can exceed a human-defined taxonomy. Third, self-supervised vision transformers do not rely on human annotations which allows us to take advantage of large unlabeled datasets [30]. Motivated by these findings, we propose to distill object information from the final self-attention layer in the vision transformer [24]."
        },
        {
            "heading": "3.2 Distilling Object Masks Using Self-Attention",
            "text": "Preliminaries. We reshape an image X \u2208 RH\u00d7W\u00d73 into a sequence of N patches. Each patch is of size S \u00d7 S pixels. We refer to the image patches as patch tokens [I]. The patch tokens are further concatenated with a special classification token [CLS] resulting in the input sequence X\u2032 which consists of N + 1 tokens. We use the features {q(h),k(h)} from the final multihead self-attention (MSA) block to compute object masks, where each head h performs a single self-attention operation. The self-supervised vision transformer is initialized with weights from [12]. We refer the interested reader to [24, 78] for more information on the self-attention mechanism in transformers.\nConstruct Affinity Graphs. Following prior work [12, 68, 78], we construct an affinity graph to measure the similarity between image patches. Given the input sequence X \u2032, we compute the affinity vector aCLS as the pairwise similarities between the classification token [CLS] and the patch tokens [I] in the final MSA block. Similarly, the affinity matrix AI measures the pairwise similarities between all pairs of patch tokens [I]. In particular, the element Aij is computed between two tokens of the sequence, i and j, as the dot product of their feature representations, f i and f j where f \u2208 {q(h),k(h)}. Finally, we average the affinities over the attention headsH. The edges ECLS and EI in the graphs, GCLS and GI, are defined by their associated affinity weights, shown in Eq. 1 and 2:\naCLS = 1 |H| \u2211 h\u2208H qCLS(h) \u00b7 k > I (h) aCLS \u2208 R1\u00d7N , (1)\nAI = 1 |H| \u2211 h\u2208H kI(h) \u00b7 k>I (h) AI \u2208 RN\u00d7N . (2)\nSelect Discriminative Tokens. Our goal is to select patch tokens that likely correspond to an object part. In particular, we focus on the top-k responses according to the affinities w.r.t. the [CLS] token aCLS. Formally, we define the set of patches P = {j | E(CLS, j) is top-k \u2208 GCLS}, where E(CLS, j) denotes the directed edge from the classification token [CLS] to a patch token [Ij] in graph GCLS. In addition, we define the patch with the largest (i.e., top-1) affinity in aCLS as the source patch\ns = arg maxj a j CLS. This region tends to correspond to the most discriminative image component, e.g., the beak of a bird, the horn of a rhino, etc.\nConstruct Initial Masks. We generate a single object mask M s \u2208 {0, 1}1\u00d7N per image X based on its source s and proposals P . The source s should belong to the predicted object mask as it represents the object\u2019s most discriminative part. We follow [68] to diffuse the information from s to the proposals P . In particular, only patches in P that are similar to s are further considered as proposals P \u2032 = {j | j \u2208 P \u2227AsjI > 0}. The object mask M s is set to 1 at location j only if\u2211 i\u2208P\u2032 A ij I > 0. Consequently, patch j belongs to the same object as s, if the total sum of pairwise similarities between s and P \u2032 is positive. Finally, the obtained mask is reshaped and upsampled to the original image size (H,W ) using nearest neighbor interpolation, resulting in M \u2208 {0, 1}H\u00d7W .\nDistill Mask R-CNN. To produce multiple object mask candidates per image, we train a region proposal network, i.e., Mask R-CNN [36]. This object segmentation model requires the class c, the bounding box coordinates b, and the foreground-background mask M for each image. Notice that we obtained the object masks and their corresponding bounding box coordinates in the previous step. However, these masks are class-agnostic. In order to assign a class label c to each mask, we apply a clustering algorithm (e.g., K-means [53]) to the output [CLS] tokens of the masked images. Now, we can train Mask R-CNN via the following objective function:\nLobj = Lclass(c\u0302, c) + Lbbox(b\u0302, b) + Lmask(M\u0302 ,M), (3) where c\u0302, b\u0302 and M\u0302 denote the predicted class, bounding box and mask. Importantly, the trained model predicts multiple object mask candidates per image with their associated confidence scores. We leverage these predictions as pseudo-ground-truth to train a segmentation model in the next section.\nDiscussion. Like prior work [12, 68], MaskDistill constructs an affinity graph from the query and key features, respectively q(h) and k(h), in the final MSA block to produce an object mask. However, our method differs in two important components. First, we can generate multiple candidates for each image. This is crucial when tackling scene-centric datasets. Second, we use the top-k affinities in GCLS to generate the patch proposals P and their initial object mask M . Notice that this strategy does not make assumptions about the underlying scene composition as in [68, 76], e.g., the object should be salient or enclose a smaller area than the background. We empirically observe that our proposed approach results in better performance (see Section 4.3)."
        },
        {
            "heading": "3.3 Training a Segmentation Model from Noisy Object Mask Candidates",
            "text": "Consider the set of images X = {X1, . . . ,X |D|} with their corresponding object mask candidates M = {M1, . . . ,MK} and confidence scores S = {s1, . . . , sK} \u2013 obtained via the Mask R-CNN model from Section 3.2.2 Some of the masks will inevitably get assigned to the wrong cluster or won\u2019t align with an object or part. Interestingly, we experimentally observe that masks for which the model is very confident (si \u2248 1) tend to be correct (see experiment in Section 4.2). Unlike previous methods [40, 76, 91], this allows us to leverage confidence scores to suppress the influence of the noise in our prior. Specifically, we only accept confident predictions from Mask R-CNN via a threshold \u03c4 as {M i|si \u2208 S \u2227 si > \u03c4}. Finally, we aggregate the masks belonging to the same image to obtain an initial semantic segmentation per image Y = {Y 1, . . . ,Y |D|}. We only keep the most confident mask when two candidates overlap. The constructed masks serve as pseudo-ground-truth to train a semantic segmentation model.\nFinally, we train a semantic segmentation model \u03a6\u03b8 : RH\u00d7W\u00d73 \u2192 RH\u00d7W\u00d7C parameterized with weights \u03b8. This function terminates in a softmax operation to perform a soft assignment over the clusters C = {1, . . . , C}. To overcome class imbalance while simultaneously obtaining fine-grained segmentation results, we adopt a hard pixel mining strategy based on [84]. The top-k most difficult pixels T are selected in each batch to train \u03a6\u03b8. In particular, the objective function becomes:\nLseg = \u2212 1 |T | \u00b7 |C| \u2211 i\u2208T \u2211 c\u2208C Y (i, c) log Y\u0302 (i, c), (4)\nwhere the obtained segmentation mask Y (i, c) is 1 if pixel i belongs to class c and 0 otherwise. 2K is typically much larger than the number of images |D| in the dataset as Mask R-CNN returns multiple object mask candidates per image."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets. We conduct experiments on two popular benchmarks: PASCAL [27] and COCO [52]. We follow prior work [40, 76] and report our results on the 21 classes of PASCAL. The train_aug and val splits are used for training and evaluation respectively. We use all 80 object categories on COCO \u2013 a considerably challenging setting that is usually not considered for unsupervised semantic segmentation. We follow [68, 80] and use the COCO20k subset for training and testing. This subset was introduced in [79] to evaluate object detection methods for scene-centric images. During K-means clustering, we use all COCO images to improve the clustering performance.\nMask Distillation Setup. We use the ViT-S [24] vision transformer with a patch size of 16\u00d7 16 pixels for constructing the affinity graphs. The weights are initialized via self-supervised pre-training on ImageNet [12]. We select the top-40% most discriminative patch tokens in the graph GCLS after resizing the smallest image side to 640 pixels. In order to assign a category label to each mask, we apply K-means [53] on the output [CLS] tokens when using masked images as input. The Mask R-CNN model consists of a ResNet-50-C4 backbone. The training setup follows [35].\nSegmentation Training Setup. We use a DeepLab-v3 [14] segmentation model with dilated [87] ResNet-50 backbone [37] to facilitate a fair comparison with [76]. The weights are initialized via self-supervised MoCo [17] pre-training on ImageNet. We train the segmentation model for 45 epochs using batches of size 16. The weights are updated through SGD with momentum 0.9 and weight decay 10\u22124. The learning rate is 2 \u00b7 10\u22123 at the start and reduced to 2 \u00b7 10\u22124 after 40 epochs. Further, we use confidence threshold \u03c4 = 0.9 to select the most confident masks from our Mask R-CNN model (see Section 3.3). We keep the mask with the largest confidence score when thresholding excludes all predictions in an image from being used. The cross-entropy loss in Eq. 4 uses the top-20% hardest pixels. Following [76], we freeze the first two ResNet blocks to increase speed.\nEvaluation Protocols. We benchmark our approach via the evaluation protocols from [43, 76]. (i) Linear classifier: We train a 1 \u00d7 1 convolutional layer on top of frozen features to predict the semantic classes. If the pixels are disentangled according to their semantic category, we should be able to solve the segmentation task via a low-capacity (linear) classifier. (ii) Clustering: We directly evaluate the quality of our clusters by comparing our predictions against the ground-truth annotations via Hungarian matching [47]. The semantic segmentation results are evaluated via the mean IoU metric.\n4.2 Ablation Studies\nComponent Analysis. Table 1 analyzes the effect of different components of MaskDistill on the val set of PASCAL. We achieve 39.0% mIoU (first row) when clustering the initial object masks via K-means. Recall that the object masks are obtained via the affinity graph GI from a self-supervised vision transformer (Section 3.2). The results are further improved when\nusing predictions from a Mask R-CNN model trained with the initial object masks (from 39.0% to 42.0% mIoU - second row). This shows that our object mask candidates capture high-level object information, which is hard to achieve through handcrafted priors. Finally, we capitalize on the confidence scores predicted by Mask R-CNN, and show additional gains with our training recipe from Section 3.3. In particular, by using only confident object mask candidates from Mask R-CNN, our segmentation results improve from 42.0% to 45.8% mIoU. For completeness, removing the hard pixel mining strategy, results in 45.5% mIoU. We refer to the supplementary for additional ablation results.\nHyperparameter analysis. We study the influence of the hyperparameters on PASCAL and make the following observations: (i) Figure 3 quantifies the impact of changing the number of cluster C during K-means clustering of the initial object masks (Section 3.2). We adopt the overclustering procedure from [74, 76] and observe that the mask AP metric increases when we increase the amount\n1x 2.5x 5x 7.5x 10x\n10\n12.5\n15\n17.5\n20\n22.5\n25\n27.5\n30\n32.5\nOvercluster factor [C/Cdataset]\nA P m\nk 5 0\n[% ]\nsingle object multi object\nFigure 3: Influence of the number of clusters in K-means. 0.2 0.4 0.6 0.8 1.0\n10\n12.5\n15\n17.5\n20\n22.5\n25\n27.5\n30\n\u2206 \u2248 2.5%\nTop-k\nA P m\nk 5 0\n[% ]\nsingle object multi object class-agnostic class-aware\nFigure 4: Influence of the top-k selection in GCLS. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nof predicted clusters C. This means that the discovered clusters contain pixels of semantically related objects, irrespective of the amount C. (ii) Figure 4 shows the impact of the top-k selection. In order to mitigate the influence of spurious details (e.g., background clutter), we select the top-k patches in GCLS which correspond to the most discriminative patch tokens (see Section 3.2). To strike a good balance between the accuracy and the amount of proposals |P |, we set k to 40% in our experiments. (iii) Figure 5 studies the influence of selecting the most confident object mask candidates with threshold \u03c4 , discussed in Section 3.3. We observe that the mIoU score plateaus around 75%. Finally, these results show that our approach is not very sensitive to the used hyperparameters, i.e., number of clusters C, top-k and threshold \u03c4 . As a result, we use the same setup in all experiments (see Section 4.1).\n4.3 Comparison to State-of-the-art\n4.3.1 Semantic Segmentation\nTable 2: SOTA comparison on PASCAL.\n(a) Linear classifier.\nMethod LC Proxy-tasks: Co-Occurence [42] 13.5 CMP [88] 16.5 Colorization [90] 25.5 Clustering: IIC [43] 28.0 Contrastive learning: Inst. Discr. [83] 26.8 MoCo [35] 45.0 InfoMin [72] 45.2 SwAV [11] 50.7 Handcrafted grouping priors: SegSort [40]\u2020 36.2 Hierarch. Group. [91]\u2020 48.8 MaskContrast [76] 58.4 MaskContrast [76]+CRF 59.5 MaskDistill 58.7 (+0.3) MaskDistill+CRF 62.8 (+3.3)\n(b) Clustering.\nClustering\n4.0 4.3 4.9\n9.8\n4.3 3.7 4.4 4.4\n- -\n35.0 - 45.8 (+10.8) 48.9 (+13.9)\nTable 2 compares our results against the state-of-theart on the PASCAL val set. MaskDistill consistently outperforms prior work under the linear classifier setup (+0.3 without CRF and +3.3% with CRF [46]). Similarly, we report better results under the clustering setup (+10.8% mIoU). Figure 6 visualizes the results for our method. The model can segment semantically meaningful image regions, e.g., dogs, cars, persons, etc. In conclusion, our method learns better dense semantic representations of images than existing approaches. We further analyze our results w.r.t. different groups of works.\n(i) Proxy-tasks: MaskDistill outperforms works that solve proxy-tasks, e.g., optical flow [88] or colorization [90], to learn dense representations. Such proxy tasks do not capture object-level information \u2013 an essential ingredient for tackling semantic segmentation. Differently, we capture such information explicitly by distilling object masks from self-supervised vision transformers. (ii) Clustering: We outperform end-to-end learning methods based on clustering, i.e., IIC [85]. IIC is prone to degenerate solutions, as the network can easily latch onto low-level image cues like color. MaskDistill decouples feature learning and clustering to avoid this behavior. (iii) Contrastive learning: These works [11, 35, 71] learn visual representations via a contrastive loss defined at the image level. This is suboptimal because the semantic segmentation task requires disentangling the representations at the object or part level. MaskDistill achieves this via a two-step approach. (iv) Handcrafted grouping priors: Finally, MaskDistill outperforms methods [40, 76, 91] that relied on handcrafted priors to group pixels. Such priors fail to generalize to a diverse and complex dataset like PASCAL. Differently, we rely on a data-driven approach to model the pixel grouping prior. Surprisingly, we even outperform methods [40, 91] (\u2020) that finetuned the complete ASPP decoder.\nFigure 6: Semantic segmentation results of our method obtained under the clustering setup on PASCAL.\nFigure 7: Instance segmentation results obtained with our confident object mask candidates on COCO20k."
        },
        {
            "heading": "4.3.2 Semantic Instance Segmentation",
            "text": "This section evaluates our object mask candidates by comparing them against the instance segmentation ground-truth masks on PASCAL and COCO20k. We perform this analysis for (i) the initial mask candidates which are used as pseudo-ground-truth to train Mask R-CNN, and (ii) the object masks predictions from the learned Mask R-CNN model. Table 3 compares our results against two other unsupervised object mask generation methods: DINO [12] and LOST [68]. We draw the following conclusions. First, our initial object masks outperform prior work. Our proposed mask distillation step makes fewer assumptions about the scene composition, e.g., the enclosed object area is not required to be smaller than the background [68]. MaskDistill effectively combines the advantages in prior approaches [12, 68] to address this issue (see Section 3.2). Second, the object mask candidates obtained via Mask R-CNN consistently outperform our initial object masks. We conclude that our model can better handle the multi-object setting. Figure 7 shows several examples, where our method can retrieve multiple high-quality object masks per image."
        },
        {
            "heading": "5 Discussion and Limitations",
            "text": "We presented a novel framework for unsupervised semantic segmentation. It first distills object masks from a self-supervised vision transformer. Next, it learns a semantic segmentation model by leveraging\nthe most confident object mask candidates as a pixel grouping prior. This strategy addresses several limitations present in prior works. First, our method learns a pixel grouping prior in a data-driven way, rather than through handcrafted priors, which eases scaling. Second, the segmentation model does not latch onto low-level image features but learns object-level information. Third, our approach can better handle images with multiple objects. Finally, our extensive experimental evaluation shows that our method significantly outperforms the state-of-the-art.\nUndoubtedly, there are still several limitations to our work. First, it\u2019s unclear how the pre-training dataset of the self-supervised vision transformer influences the quality of the object masks. Interestingly, recent research [29, 75] shows that we can use both object- and scene-centric datasets to learn spatially structured representations. This observation suggests that it\u2019s not crucial to train the transformer on a curated dataset, e.g., ImageNet [22]. Also, there\u2019s a possibility to improve the results by scaling the pre-training dataset and model\u2019s sizes.\nAnother limitation of our work is that some instances can appear as a single object mask if their feature representations are strongly correlated, e.g., a motorcyclist on a motorbike. We identify several promising research directions that could potentially address this problem:\n\u2013 Multi-scale grouping: It could be interesting to study pixel grouping priors which incorporate multi-scale features. In particular, such representations represent complementary information at the different scales [3, 77] which could disambiguate between frequently co-occurring objects. \u2013 Pre-training strategy: We used DINO [12] to extract object masks. Future work could study whether better results can be obtained by changing the pre-training method, dataset, or network architecture. For example, alternative pre-training techniques [23, 81] could be more discriminative towards objects \u2013 or their parts \u2013 as they incorporate different inductive biases.\nBroader Impact. The proposed method tackles the task of semantic segmentation without using human annotations. Our evaluation shows that our method obtains promising results on several challenging benchmarks. Therefore, this research could benefit several applications where the semantic segmentation task plays an important role, e.g., medical imaging, autonomous driving, etc. It is hard to quantify the exact societal impact at this moment. This effect will also depend on the intentions of the users and inventors. In particular, we point out that the users of our method should be aware of the different biases present in the used datasets or pre-trained models. Since our approach does not rely on carefully annotated data, such biases could potentially yield unwanted results.\nAcknowledgment. The authors thankfully acknowledge support by Toyota Motor Europe (TME) via the TRACE project."
        },
        {
            "heading": "Supplementary Materials",
            "text": "We discuss the implementation details in Section A, additional ablations in Section B, additional results in Section C and specific failure cases in Section D.\nA Implementation Details\nThis section provides additional implementation details. The code and pre-trained models will be made available upon acceptance. All Mask R-CNN experiments (see Section 3.2) were run on 4 32GB V100 GPUs. The refinement step (see Section 3.3) was run on 2 11GB 1080Ti GPUs. The total training time is around 20 hours. Our approach is implemented with Pytorch [65]."
        },
        {
            "heading": "A.1 Mask R-CNN",
            "text": "We follow He et. al. [17, 35] to generate object mask candidates. In particular, we train Mask R-CNN [36] with a ResNet-50-C4 backbone while using the Detectron2 framework [82]. The model is initialized via self-supervised pre-training on ImageNet [22], i.e., MoCo [17]. The weights of the first two backbone stages are frozen to speedup training. Furthermore, we pick a random value from the interval [480, 800] to resize the smallest image side during training, while the image scale is 800 during inference. The learning rate is set at 0.02 and reduced with a factor 10 after 20k and 22k iterations. The model is trained for a total duration of 24k iterations and learning rate warmup is applied for the first 100 iterations. We refer to [35, 36] for additional details."
        },
        {
            "heading": "A.2 Linear Probing",
            "text": "During linear probing, we train a 1\u00d7 1 convolutional layer on top of the frozen features. This layer is trained for 45 epochs with a batch size of 24. We use the SGD optimizer with weight decay 10\u22124 and momentum 0.9 to update the model weights. The initial learning is set to 0.1 and decreased with a factor of 10 after 25 epochs. We didn\u2019t observe improvements when training longer."
        },
        {
            "heading": "A.3 Semantic Segmentation",
            "text": "We follow the training and evaluation setup by Van Gansbeke et al. [76]. The model is DeepLabv3 [14] with ResNet50 [37] backbone. The model weights are updated using SGD with momentum 0.9 and weight decay 10\u22124. The initial learning rate is 2 \u00b7 10\u22123 and reduced to 2 \u00b7 10\u22124 after 40 epochs of training. The total training duration is 45 epochs with a batch size of 16. We also apply the same RandomHorizontalFlip and ScaleNRotate augmentations during training. The original resolution is used for testing. Finally, the Hungarian algorithm [47] matches the predicted clusters with the ground truth classes as in [43, 74, 76]. The mean intersection over union (mIoU) is used as the evaluation metric."
        },
        {
            "heading": "A.4 Semantic Instance Segmentation",
            "text": "For PASCAL [27], we evaluate on the official VOC2012 object segmentation set (2913 images). Both the VOC2007 and VOC2012 sets are used during training, following [35]. For COCO [52], we evaluate on COCO20k by following prior work [68, 79]. We use the mask average precision (AP) metric from Detectron2 [82] to evaluate the predictions and we report the average over 5 different runs. We consider two scenarios during evaluation: the multi object and single object setting. In the multi object setting, the model must predict all the ground truth masks. In the single object setup, we only keep the mask with the highest confidence score for each image and select the ground truth object with the largest (bounding box) IoU for evaluation. Again, we apply the Hungarian algorithm [47] to match the predicted clusters with the ground truth classes. To compare with prior work, we use the publicly available code. In DINO [12], we sum the attention heads and set the threshold to 0.75. In LOST [68], we take 400 patch proposals. These modifications improve their performances.\nFigure S1: Semantic segmentation results of our method obtained under the clustering setup on PASCAL.\nFigure S2: Instance segmentation results obtained with our confident object mask candidates on PASCAL."
        },
        {
            "heading": "B Additional Ablations",
            "text": "Table S1 complements the component analysis in the main paper (see Table 1). We explore the predictions of the Mask R-CNN model by additionally using its confident bounding box predictions.\nTable S2: Semantic Segmentation Results. We evaluate on the PASCAL val set. (\u2020) indicates that we use a linear probe (see Section A.2). Method backg. aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU MaskContrast [76] 84.4 68.1 23.7 62.6 35.7 0.0 72.8 63.0 46.8 0.0 0.0 8.5 30.6 28.9 49.4 19.4 5.6 34.8 17.2 55.7 27.3 35.0 MaskDistill 84.4 74.7 27.9 70.9 47.5 0.0 72.8 33.2 72.4 0.0 70.0 29.6 38.1 67.5 58.1 28.1 9.2 65.8 20.4 65.5 27.7 45.8 MaskDistill+CRF 85.4 80.3 28.8 74.7 50.4 0.0 72.5 52.1 75.7 0.0 76.5 28.6 38.7 71.3 63.8 32.0 11.2 67.0 20.5 67.7 28.7 48.9 MaskDistill\u2020 88.1 80.5 30.9 76.7 58.2 52.2 75.7 70.1 82.7 12.9 73.3 35.4 78.8 72.2 62.1 52.4 27.9 73.0 19.7 70.5 39.0 58.7 MaskDistill\u2020+CRF 89.8 83.1 34.0 85.9 63.3 45.0 79.1 70.1 86.3 16.9 81.5 38.1 84.0 74.9 69.7 63.0 31.3 78.3 23.3 74.4 46.1 62.8\nWe set the threshold \u03c4 to 0.9 as in the main paper. Unsurprisingly the performance drops when using the bounding box predictions instead of the mask predictions from Mask R-CNN (31.8% vs. 42.0%). Applying GrabCut [67] to the predicted bounding box improves the results (38.2% vs. 31.8%).\nTable S1: Component analysis.\nSetup val mIoU self.sup. vision transformer 39.0 + Mask R-CNN (bbox) 31.8 + GrabCut 38.2 self.sup. vision transformer 39.0 + Mask R-CNN (mask) 42.0 + Segmentation model 45.8 + CRF 48.9 However, it still underperforms the initial object masks from the vision transformer (38.2% vs. 39.0%). This supports the claim that our masks capture high-level object information, which is hard to mimic by relying on handcrafted priors as used in GrabCut. Finally, we point out that multiple CRF [46] iterations produce additional gains (48.9% mIoU vs 45.8%), primarily for detailed structures. However, be aware that in order to set the importance weights of the kernels correctly, a small annotated validation set is ideal. Albeit not a required component of our framework, we conclude that iteratively updating the pseudo-ground-truth with a CRF and the model weights \u03b8 improves the segmentation results."
        },
        {
            "heading": "C Additional Results",
            "text": "This section discusses additional qualitative and quantitative results on the PASCAL dataset in Section C.1 and on the COCO dataset in Section C.2."
        },
        {
            "heading": "C.1 PASCAL",
            "text": "We visualize additional examples from the PASCAL dataset. In particular, Figure S1 displays the learned clusters from our semantic segmentation model \u03a6\u03b8 and Figure S2 shows the confident object mask candidates. Again, we conclude that our approach discovers objects that are semantically meaningful without the necessity for annotations.\nTable S2 presents the IoU score per class. We compare with prior SOTA [76] and observe large improvements for all classes. MaskDistill discovers clusters such as bird, cat and train. Not surprisingly, less discriminative classes, like chair, table or plant, are more difficult to segment. Interestingly, when we apply a linear probe, the features quickly adapt to the semantics of the dataset (i.e., the PASCAL classes). We conclude that the model has learned semantically meaningful pixel-embeddings for different object categories."
        },
        {
            "heading": "C.2 COCO",
            "text": "We show additional qualitative results. Figure S3 displays examples of the confident object mask candidates. In contrast to PASCAL, COCO contains more complex (i.e., scene-centric) images. While the predictions are not perfect, MaskDistill detects and segments various objects fairly accurate.\nFigure S3: Instance segmentation results obtained with our confident object mask candidates on COCO20k."
        },
        {
            "heading": "D Failure Cases",
            "text": "Figure S4 presents several failure cases. These can be grouped as follows:\n\u2013 Merging objects: In some cases, the predicted mask encompasses multiple objects, e.g., \"person\" and \"racket\", \"person\" and \"snowboard\" etc.\n\u2013 Missing objects or parts: The mask excludes certain objects or parts, e.g., \"bike handlebars\". Similarly, our model is unable to detect certain background objects, e.g., \"tennis spectators\". \u2013 Out-of-taxonomy: The model generates object mask candidates that do not belong to the human-defined object categories, e.g., no class \"clock\" in the COCO (things) classes.\nFigure S4: Failure Cases on COCO20k."
        }
    ],
    "title": "Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation",
    "year": 2022
}