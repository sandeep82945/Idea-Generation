{
    "abstractText": "The investigation of image deblurring techniques in dynamic scenes represents a prominent area of research. Recently, deep learning technology has gained extensive traction within the field of image deblurring methodologies. However, such methods often suffer from limited inherent interconnections across various hierarchical levels, resulting in inadequate receptive fields and suboptimal deblurring outcomes. In U\u2010Net, a more adaptable approach is employed, integrating diverse levels of features effectively. Such design not only significantly reduces the number of parameters but also maintains an acceptable accuracy range. Based on such advantages, an improved U\u2010Net model for enhancing the image deblurring effect was proposed in the present study. Firstly, the model structure was designed, incorporating two key components: the MLFF (multilayer feature fusion) module and the DMRFAB (dense multi\u2010receptive field attention block). The aim of these modules is to improve the feature extraction ability. The MLFF module facilitates the integration of feature information across various layers, while the DMRFAB module, enriched with an attention mechanism, extracts crucial and intricate image details, thereby enhancing the overall information extraction process. Finally, in combination with fast Fourier transform, the FRLF (Frequency Reconstruction Loss Function) was proposed. The FRLF obtains the frequency value of the image by reducing the frequency difference. The present experiment results reveal that the proposed method exhibited higher\u2010quality visual effects. Specifically, for the GoPro dataset, the PSNR (peak signal\u2010 to\u2010noise ratio) reached 31.53, while the SSIM (structural similarity index) attained a value of 0.948. Additionally, for the Real Blur dataset, the PSNR achieved 31.32, accompanied by an SSIM score of 0.934.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zuozheng Lian"
        },
        {
            "affiliations": [],
            "name": "Haizhen Wang"
        }
    ],
    "id": "SP:e6864b4c9ed54ec440c3406de78941151760fa35",
    "references": [
        {
            "authors": [
                "S Tang"
            ],
            "title": "Multi-scale image blind deblurring network for dynamic scenes",
            "venue": "J. Softw. 33(9),",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yang"
            ],
            "title": "Accurate recognition method of human body movement blurred image gait features using graph neural network",
            "venue": "Mob. Inf. Syst",
            "year": 2021
        },
        {
            "authors": [
                "A. Dantsker"
            ],
            "title": "Recovering blurred images to recognize field information",
            "venue": "Proceedings 81(1),",
            "year": 2022
        },
        {
            "authors": [
                "R. Chen",
                "Z. Zheng",
                "Y. Yu",
                "H. Zhao",
                "H.Z. Tan"
            ],
            "title": "Fast restoration for out-of-focus blurred images of QR code with edge prior information via image sensing",
            "venue": "IEEE Sens. J. 21(16),",
            "year": 2021
        },
        {
            "authors": [
                "J.M. Zhang",
                "W.J. Feng",
                "T.Y. Yuan",
                "J. Wang",
                "Sangaiah",
                "A.K. SCSTCF"
            ],
            "title": "Spatial-channel selection and temporal regularized correlation filters for visual tracking",
            "venue": "Appl. Soft Comput. 118, 108485",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xu",
                "Y. Zhu",
                "Y. Quan",
                "H. Ji"
            ],
            "title": "Attentive deep network for blind motion deblurring on dynamic scenes",
            "venue": "Comput. Vis. Image Underst. 205(11),",
            "year": 2021
        },
        {
            "authors": [
                "F.F. Yang",
                "X.G. Li",
                "L. Zhuo"
            ],
            "title": "Image deblurring of dynamic scene based on attention residual CODEC network",
            "venue": "J. Appl. Opt",
            "year": 2021
        },
        {
            "authors": [
                "J. Liu",
                "N. He",
                "X. Yin"
            ],
            "title": "Low illuminance image enhancement based on retinex-UNet algorithm",
            "venue": "Comput. Eng. Appl. 56(22),",
            "year": 2020
        },
        {
            "authors": [
                "A. Raj",
                "N.A. Shah",
                "A.K. Tiwari"
            ],
            "title": "A novel approach for fundus image enhancement",
            "venue": "Biomed. Signal Process. Control",
            "year": 2022
        },
        {
            "authors": [
                "G. Chen",
                "Z. Gao",
                "Q. Wang",
                "Q. Luo"
            ],
            "title": "U-net like deep autoencoders for deblurring atmospheric turbulence",
            "venue": "J. Electron. Imag. 28(5),",
            "year": 2019
        },
        {
            "authors": [
                "K Chen"
            ],
            "title": "Robust restoration of low-dose cerebral perfusion CT images using NCS-Unet",
            "venue": "Nucl. Sci. Tech. 33(3),",
            "year": 2022
        },
        {
            "authors": [
                "Fan",
                "Z. et al. SGUNet"
            ],
            "title": "Style-guided UNet for adversely conditioned fundus image super-resolution",
            "venue": "Neurocomputing 465, 238\u2013247",
            "year": 2021
        },
        {
            "authors": [
                "X. Mao",
                "Y. Liu",
                "W Shen"
            ],
            "title": "Deep residual fourier transformation for single image deblurring",
            "venue": "arXiv e-prints,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wu",
                "H. Zhang",
                "Li",
                "et al. Dense block U-net for dynamic scene deblurring. Proc. ICGSP \u201920"
            ],
            "title": "Proceedings of the 4th International Conference on Graphics and Signal Processing",
            "venue": "26\u201329: 46\u201350",
            "year": 2020
        },
        {
            "authors": [
                "P Wang"
            ],
            "title": "Identification of apple leaf diseases by improved deep convolutional neural networks with an attention mechanism",
            "venue": "Front. Plant Sci",
            "year": 2021
        },
        {
            "authors": [
                "H. Jiang",
                "F. Chen",
                "X. Liu",
                "J. Chen",
                "L. Chen"
            ],
            "title": "Thermal wave image deblurring based on depth residual network",
            "venue": "Infrared Phys. Technol",
            "year": 2021
        },
        {
            "authors": [
                "B. Wang",
                "H. Wang",
                "D. Song"
            ],
            "title": "A filtering method for LiDAR point cloud based on multi-scale CNN with attention mechanism",
            "venue": "Remote Sens. 14(23),",
            "year": 2022
        },
        {
            "authors": [
                "Shao",
                "M.-W",
                "Li",
                "Le",
                "Meng",
                "D.-Y",
                "Zuo",
                "W.-M"
            ],
            "title": "Uncertainty guided multi-scale attention network for raindrop removal from a single image",
            "venue": "IEEE Trans. Image Process",
            "year": 2021
        },
        {
            "authors": [
                "L. Zuozheng",
                "W. Haizhen",
                "Z. Qianjun"
            ],
            "title": "An image deblurring method using improved U-net model",
            "venue": "Mob. Inf. Syst. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "X. Li",
                "W. Wang",
                "X. Hu",
                "J. Yang"
            ],
            "title": "Selective Kernel Networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "Z. Qiang",
                "Zijian",
                "Ye",
                "S. Siyu",
                "N. Tianlin",
                "Z. Yuwei"
            ],
            "title": "Remaining useful life prediction of rolling bearings based on convolutional recurrent attention network. Assem",
            "year": 2022
        },
        {
            "authors": [
                "D. Lei",
                "G. Ran",
                "L. Zhang",
                "W. Li"
            ],
            "title": "A spatiotemporal fusion method based on multiscale feature extraction and spatial channel attention mechanism",
            "venue": "Remote Sens. 14(3),",
            "year": 2022
        },
        {
            "authors": [
                "J. Rim",
                "H. Lee",
                "J. Won",
                "S. Cho"
            ],
            "title": "Real-world blur dataset for learning and benchmarking deblurring algorithms",
            "venue": "Proc. 2020 European Conference on Computer Vision",
            "year": 2020
        },
        {
            "authors": [
                "H. Gao",
                "X. Tao",
                "X Shen"
            ],
            "title": "Dynamic scene deblurring with parameter selective sharing and nested skip connections",
            "venue": "Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "A.K. Rorabaugh",
                "S. Ca\u00edno-Lores",
                "T. Johnston",
                "M. Taufer"
            ],
            "title": "High frequency accuracy and loss data of random neural networks trained on image datasets",
            "venue": "Data Brief 40,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wang",
                "F. Li",
                "R. Cong",
                "H. Bai",
                "Y. Zhao"
            ],
            "title": "Adaptive feature fusion network based on boosted attention mechanism for single image dehazing",
            "venue": "Multimed. Tools Appl. 81(8),",
            "year": 2022
        },
        {
            "authors": [
                "J. Sun",
                "W. Cao",
                "Z Xu"
            ],
            "title": "Learning a convolutional neural network for non-uniform motion blur removal",
            "venue": "Proc. 2015 IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2015
        },
        {
            "authors": [
                "S. Nah",
                "T.H. Kim",
                "K.M. Lee"
            ],
            "title": "Deep multi-scale convolutional neural network for dynamic scene deblurring",
            "venue": "Proc. 2017 IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2017
        },
        {
            "authors": [
                "J. Zhang",
                "J. Pan",
                "J Ren"
            ],
            "title": "Dynamic scene deblurring using spatially variant recurrent neural networks",
            "venue": "Proc. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Y Chen"
            ],
            "title": "A deep motion deblurring network using channel adaptive residual module",
            "venue": "IEEE Access",
            "year": 2021
        },
        {
            "authors": [
                "X. Tao",
                "H. Gao",
                "X Shen"
            ],
            "title": "Scale-recurrent network for deep image deblurring",
            "venue": "Proc. 2018 IEEE conference on computer vision and pattern recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "O. Kupyn",
                "V. Budzan",
                "Mykhailych",
                "M. et al. Deblur GAN"
            ],
            "title": "Blind motion deblurring using conditional adversarial networks",
            "venue": "Proc. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2018). 8183\u20138192 (IEEE, 2018). 13 Vol.:(0123456789) Scientific Reports |",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nwww.nature.com/scientificreports"
        },
        {
            "heading": "An image deblurring method using",
            "text": "improved U\u2011Net model based on multilayer fusion and attention mechanism"
        },
        {
            "heading": "Zuozheng Lian & Haizhen Wang *",
            "text": "The investigation of image deblurring techniques in dynamic scenes represents a prominent area of research. Recently, deep learning technology has gained extensive traction within the field of image deblurring methodologies. However, such methods often suffer from limited inherent interconnections across various hierarchical levels, resulting in inadequate receptive fields and suboptimal deblurring outcomes. In U\u2011Net, a more adaptable approach is employed, integrating diverse levels of features effectively. Such design not only significantly reduces the number of parameters but also maintains an acceptable accuracy range. Based on such advantages, an improved U\u2011Net model for enhancing the image deblurring effect was proposed in the present study. Firstly, the model structure was designed, incorporating two key components: the MLFF (multilayer feature fusion) module and the DMRFAB (dense multi\u2011receptive field attention block). The aim of these modules is to improve the feature extraction ability. The MLFF module facilitates the integration of feature information across various layers, while the DMRFAB module, enriched with an attention mechanism, extracts crucial and intricate image details, thereby enhancing the overall information extraction process. Finally, in combination with fast Fourier transform, the FRLF (Frequency Reconstruction Loss Function) was proposed. The FRLF obtains the frequency value of the image by reducing the frequency difference. The present experiment results reveal that the proposed method exhibited higher\u2011quality visual effects. Specifically, for the GoPro dataset, the PSNR (peak signal\u2011 to\u2011noise ratio) reached 31.53, while the SSIM (structural similarity index) attained a value of 0.948. Additionally, for the Real Blur dataset, the PSNR achieved 31.32, accompanied by an SSIM score of 0.934.\nWith the increasing popularity of mobile devices and multimedia communication, images have evolved into the primary carriers of information. Owing to both the inherent constraints of imaging systems and the dynamic and unpredictable nature of the shooting environment in dynamic scenes, image blurring becomes an inevitable occurrence1. Thus, research attention has shifted towards image deblurring methods. Blurred images not only subjectively affect the visual experience2\u20134, but also affect subsequent visual tasks5. Accordingly, addressing image deblurring techniques for dynamic scenes emerges as a crucial problem to solve6.\nConventional image deblurring methods rely on either a known or assumed blur kernel and leverage prior information about the image. However, these methods encounter challenges when dealing with the removal of blur induced by complex factors7. The development of deep neural networks has paved the way for blind image deblurring methods that do not necessitate the estimation of blur kernels. Thus, such methods have gained widespread use. Among the various deep neural network approaches, U-Net stands out in that an improved version of FCN (Fully Convolutional Neural Networks) is incorporated. This enhancement provides U-Net with increased flexibility in integrating features from multiple hierarchical levels, making it a powerful tool in image deblurring. By doing so, the complexity of deep networks is effectively reduced, significantly curtailing the number of parameters while maintaining accuracy within an acceptable range. Additionally, there is a scarcity of research on image deblurring based on U-Net, which provided motivation to explore an improved U-Net for solving image deblurring in dynamic scenes. The main objectives were as follows: (1) Investigate and develop an improved U-Net architecture tailored for image deblurring purposes; and (2) Examine and identify key modules\nOPEN\nCollege of Computer and Control Engineering, Qiqihar University, Qiqihar 161006, China. * email: wanghaizhen1976@163.com\n2 Vol:.(1234567890) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\ncapable of extracting image details and crucial information across different layers, thereby enhancing image feature extraction capabilities and improving visual outcomes.\nRecently, U-Net and its improved versions are gradually being applied in image deblurring related fields such as image enhancement, image restoration, and super-resolution. For example, Liu et\u00a0al.8 proposed RetinexUNet, which uses convolutional neural networks to learn and decompose images. The results were input into the enhancement network for end-to-end training. The described method can enhance images of any size and improve the overall visual effect. Raj et\u00a0al.9 proposed a residual dense connection-based U-Net model for fundus image enhancement, which effectively captures both local and global information. The experiment demonstrated that the proposed model could effectively enhance the visual quality of fundus images. Chen et\u00a0al.10 introduced a U-Net-like deep stacked autoencoder neural network model designed for the restoration of images distorted by atmospheric turbulence. The model fuses low-level and high-level information, greatly ensuring the integrity of information, and obtaining high-quality restored images. Chen et\u00a0al.11 proposed a deep learning method called NCS-Unet. This method incorporates distinctive features from the non-subsampled contourlet transform (NSCT) and the Sobel filter to extract valuable information. Consequently, it enhances the performance of noise reduction and artifact removal in PCT images. To improve low-resolution fundus images, Fan et\u00a0al.12 proposed a style-guided U-Net, which incorporates a series of style-guided U-shape blocks (SUB). SUB enlarges the receptive field and fully fuses the hierarchical features. The experimental results demonstrated that SGUNet was more robust and accurate than other methods. Mao et\u00a0al.13 proposed a Residual Fast Fourier Transform with Convolution Block and used it as the foundation for constructing a deep network. This network is capable of capturing both long-term and short-term interactions in the data while integrating low- and high-frequency residual information. Their experimental results demonstrated improved deblurring performance using this approach. Wu et\u00a0al.14 proposed a U-Net model containing dense blocks for dynamic scene deblurring. The model significantly reduces the inference time.\nAmong the described methods, some8\u201312 are only suitable for image deblurring in specific situations, while others13,14 cannot achieve cross layer flow of feature information, and there is room for improvement in their feature extraction capabilities. The integration of wavelet transforms with deep convolutional neural networks is beneficial for mitigating image blur. Meanwhile, to overcome the vanishing gradient problem, DSC (depthwise separable convolution)15 and residual networks16 can address the described problems. Additionally, skip connections in U-Net can sometimes introduce redundant information. Attention mechanisms have demonstrated their effectiveness in extracting critical and relevant information from the data17\u201319, providing a solution to this issue. Drawing inspiration from the previously mentioned techniques, the present study introduces a novel synthetic image deblurring method. This method combines wavelet transforms, DSC (depthwise separable convolution), residual networks, and attention mechanisms to enhance the capabilities of the U-Net architecture. The aim is to effectively address the challenges associated with image deblurring.\nThe present study offers the following contributions: (1) A 4-layer network based on U-Net was proposed, including one encoder and one decoder with four blocks; (2) A MLFF module was added, which integrates feature information in different layers of the U-Net network, changes the inherent information flow mode in the conventional U-Net network, and integrates feature information of different scales, so that the network can extract more feature information; (3) A DMRFAB module introducing both CAM (channel attention mechanism) and SAM (spatial attention mechanism) was incorporated to extract crucial information from deep features and improve the image deblurring effect accordingly; (4) FFT was introduced into the loss function and FRLF was proposed, which allows for the frequency value of the image to be obtained by reducing the frequency difference of the image, thereby improving the deblurring effect. The proposed model underwent both quantitative and qualitative analysis using the GoPro and Real Blur datasets. The results reveal that the image deblurring quality was significantly enhanced by the proposed model.\nThe rest of the paper is organized as follows. The proposed methodology is presented in detail in section\u00a0\u201cThe proposed method\u201d. The experiment results and discussion are given in section\u00a0\u201cResults\u201d and section\u00a0\u201cDiscussion\u201d, respectively. The conclusion is provided in section\u00a0\u201cConclusion\u201d."
        },
        {
            "heading": "The proposed method",
            "text": "The proposed method was based on an improved U-Net model, which is discussed in this section."
        },
        {
            "heading": "The model structure",
            "text": "The proposed method was based on an improved U-Net model, of which the structure is shown in Fig.\u00a01. The model includes one encoder and one decoder. The encoder uses four blocks to extract features from each layer, with a convolutional structure of 2-2-2-2. Every block, with the exception of the initial block, undergoes a specific process involving one down-sampling operation using Discrete Wavelet Transform (DWT) followed by two convolutional operations. The extracted features from each layer are fused through the MLFF module. The decoder adopts four blocks, with a convolutional structure of 2-3-3-2. Except for the last block, each block undergoes two convolutions and one Inverse Wavelet Transform (IWT). To reduce convolution parameters and computational complexity, and solve the degradation problem in deep networks, convolutional blocks are replaced with DSC (depth-wise separable convolutions) and RDSC (residual depth-wise separable convolutions). The encoder and decoder are connected through DMRFAB to allow for richer and more detailed features to be obtained.\nThe model is mainly composed of DSC, RDSC group including three RDSC, DWT, IWT, MLFF module, and DMRFAB module. The model is a 4-layer network based on U-Net, with the encoder on the left and the decoder on the right. The input image is represented as H \u00d7 W \u00d7 3, where H represents the height of the image, W represents the width of the image, and 3 represents the number of channels in the image. The processing of the model involves the following three stages:\n3 Vol.:(0123456789) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\n(1) In the encoding stage, from top to bottom, the first layer uses one 32-channel DSC and one RDSC group, which is composed of three RDSCs, and transforms the input image into H \u00d7 W \u00d7 32, which is represented as E1. The second layer is entered through the first level DWT, using one 64-channel DSC and one RDSC group to transform the input feature information into H\n4 \u00d7 w 4 \u00d7 64 , which is represented as E2. After entering the third\nlayer through the second level DWT, one DSC and one RDSC group are used to transform the entered feature information into H\n16 \u00d7 w 16 \u00d7 128 , which is represented as E3. After entering the fourth layer through the third\nlevel DWT, the incoming information is transformed into H 32 \u00d7 w 32 \u00d7 256 using one DSC, one RDSC group, and one DMRFAM. Subsequently, the number of channels is reduced to 512 through DSC.\n(2) The feature information of E1, E2, and E3 of the encoder is input into MLFFi (i = 1,2,3) for fusion of different layers of feature information.\n(3) In the decoding stage, IWT is performed on the output information D4 in the fourth layer, and passed through one DSC and one RDSC group with D3, where it is transformed into H\n16 \u00d7 w 16 \u00d7 128 , followed by\none more DSC operation to increase the number of channels to 256. The third layer outputs feature for IWT, and enters one DSC and one RDSC group with D2, transforming the fused information into H\n4 \u00d7 w 4 \u00d7 64 .\nSubsequently, one DSC operation is performed to increase the number of channels to 128. The second layer outputs features for IWT, and D1 enters one DSC and one RDSC group, transforming the fused information into H \u00d7 W \u00d7 32. The convolutional feature information that can be separated by depth becomes H \u00d7 W \u00d7 3. This output is added and fused with the image information from the initial input model to obtain the deblurred image as the final result.\nThe modules of the model are introduced in detail below."
        },
        {
            "heading": "DSC",
            "text": "The improved model introduces DSC, which decreases the number of the model parameters, and makes the network lightweight. The structure of the model is shown in Fig.\u00a02. The DSC is composed of DWC (depth-wise convolution) and PWC (point-wise convolution). DWC divides the multi-channel features of the previous layer into the feature map of a single channel, and then uses a 3 \u00d7 3 convolution kernel for convolution. Subsequently, DWC recombines them, adjusting the size of the feature map from the previous layer while maintaining the same number of channels. The characteristic image attained by DWC is convoluted using PWC, which uses a 1 \u00d7 1 convolution kernel to blend the convolution results from DWC while having the flexibility to alter the number of output channels as needed."
        },
        {
            "heading": "RDSC",
            "text": "RDSC was designed based on the residual network, which can spread detailed information from different layers to promote blur reconstruction quality. It also serves as a mechanism to mitigate the issue of gradient vanishing. The RDSC uses two DSC and two Leaky Relu activation functions, and the structure is shown in Fig.\u00a03. First, DSC, Leaky Relu, and DSC operations are performed for the input information. Then, the obtained features and input information are fused by means of skip connection. Finally, the fusion result is output after Leaky Relu processing."
        },
        {
            "heading": "DWT and IWT",
            "text": "DWT and IWT respectively replace the down-sampling and up-sampling functions of the U-Net model, which can obtain image information at different frequencies, thereby reducing feature information loss during image reconstruction and further mitigating the blurring effect. As shown in Fig.\u00a01, DWT and IWT are respectively\nH\u00d7W\u00d732\nBlurry image\nSharp\nimage\nMLFF1\nMLFF2\nMLFF3\nDWT IWT\nDWT\nDWT IWT\nIWT\n- dense multi-receptive field attention block depth-wise separable convolution\nE1\nD1\nE2\nE3\nD2\nD3\nH\u00d7W\u00d732\nH\u00d7W\u00d73\nE4 D4\n4 644 \u00d7\u00d7 WH\nresidual depth-wise separable convolution\n44 64\u00d7\u00d7 WH\n64\u00d7\u00d7 WH 161664\u00d7\u00d7 WH 1616\nFigure\u00a01. The improved model structure.\n4 Vol:.(1234567890) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nperformed on the results of E1, E2, and E3 in the encoder, and IWT is performed in the decoder. The Haar wavelet is a wavelet basis function which is easy to implement and operate. Therefore, in the present study, a two-dimensional Haar wavelet was adopted for wavelet transform operations to divide the image signal into directional sub-bands. Filtering is an effective method for realizing DWT. Firstly, a one-dimensional high-pass filter represented by \u03d5(x) (as defined in Eq.\u00a0(1)) is utilized to filter and vertically down-sample each column of the image. Subsequently,both \u03d5(x) and \u03c8(x) (defined in Eq.\u00a0(2)) are employed to filter and horizontally downsample each row. This process yields sub-frequency information for IHH and IHL . In the second step, the \u03c8(x) filter is used to filter and vertically down-sample each column of the image. Once again, \u03d5(x) and \u03c8(x) are used to filter and horizontally down-sample each column. This results in sub-frequency information for IHH and IHL . Sub-frequency information for the four parameters is shown in Eqs.\u00a0(3)\u2013(6).\n(1)\u03d5(x) = [\u22121, 1]\n(2)\u03c8(x) = [1, 1]\n(3)IHL ( x, y ) = \u03d5(x)\u03c8(y)\n(4)IHH ( x, y ) = \u03d5(x)\u03d5(y)\n(5)ILH ( x, y ) = \u03c8(x)\u03d5(y)\n1\u00d71 conv\n3\u00d73 conv pointwise convolution\ndepth-wise convolution\ninput output\nFigure\u00a02. The structure of DSC.\ndepth-wise separable convolution\ndepth-wise separable convolution\nx\nskip\nconnection Leaky Relu\nLeaky Relu\nFigure\u00a03. The structure of RDSC.\n5 Vol.:(0123456789) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nThe x and y in Eqs.\u00a0(1)-(6) represent rows and columns for the information of the image; IHL denotes the horizontal high-frequency and vertical low-frequency information of the image; IHH denotes the horizontal and vertical high-frequency information of the image; ILL expresses the horizontal and vertical low-frequency information of the image; and ILH represents the horizontal low-frequency and vertical high-frequency image information. The IWT performs inverse operations on the four sub-images using the aforementioned filter. Thus, IHL , IHH , ILL and ILL are used to fuse into the original image. Therefore, the original image is decomposed by DWT and then reconstructed by IWT without loss of information. Further, multi-level wavelet transforms can be implemented by further processing IHL , IHH , ILL and ILL according to the described method. For the twodimensional Haar wavelet transform, the sum mean value is used for low-frequency information, regarded as \u03c8(x) , while the difference in mean values is used for high-frequency information, regarded as \u03d5(x)."
        },
        {
            "heading": "MLFF module",
            "text": "In the existing improved U-Net networks, the flow of feature information is inflexible, allowing for only horizontal information flow in the same layer, or vertical information flow between upper and lower layers. As such, the proposed model is different in that an MLFF module was designed, which increases the flow of information between different layers of U-Net, and integrates the characteristic information of different layers. However, a straightforward approach involving the addition or concatenation of these information sources can lead to redundancy in the fusion information and may restrict the expressive capacity of the neural network. Drawing inspiration from SKNets20, a dynamic selection mechanism was introduced to promote the expression ability of the network. Therefore, the MLFF module increases the flexibility of feature flow, which reduces the information redundancy of fusion, and improves the performance of the model. The method decreases the number of model parameters and produces a better effect than the simple cascade aggregation method. The structure of MLFF is shown in Fig.\u00a04, including the cross-layer flow details of U-Net three-layer characteristic flow.\nAs shown in Fig.\u00a04 L1 to L3 respectively represent Layer1 to Layer3 of the proposed model, and the MLFF module includes two stages: fusion and selection.\n(1) Fusion stage: E1, E2 and E3 undergo convolution and wavelet transformation, respectively. The number of channels is controlled by convolution (Conv), and the feature size is controlled by wavelet transform (WT). This results in characteristic information represented as H \u00d7 W \u00d7 C for L1, L2, and L3. Subsequently, the feature elements of L1, L2, and L3 are added together to obtain the fused output L.\n(2) Selection stage: S from L are obtained through global average pooling (GAP), so that the feature information is changed from H \u00d7 W \u00d7 C to 1 \u00d7 1 \u00d7 C. The down channel convolution layer is used to change the feature vector S into a more compact feature vector Z, and the feature information changes from 1 \u00d7 1 \u00d7 C to 1 \u00d7 1 \u00d7 r (r = C/8). The feature vector Z is passed through the convolution layer of three parallel ascending channels to obtain three feature vectors Z1, Z2 and Z3, each of which has a feature size of 1 \u00d7 1 \u00d7 C. The activation function Softmax is applied to Z1, Z2, and Z3 to obtain the activated S1, S2, and S3. Subsequently, S1, S2, S3 and L1, L2, and L3 point multiplication operations are respectively employed to adaptively calibrate L1, L2, and L3 feature maps. Finally, the calibrated features are fused to obtain the MLFF output. The output expression of MLFF is shown in Eq.\u00a0(7).\nThe model structure shows that there are three MLFF modules, namely MLFF1 , MLFF2 and MLFF3 . The modules differ only in the fusion part, that is, different layers have different feature transformations before feature fusion, while the subsequent selection parts are the same. MLFF1, MLFF2 and MLFF3 are represented as Eqs.\u00a0(8)-(10).\n(6)ILL ( x, y ) = \u03c8(x)\u03c8(y)\n(7)MLFFout = S1\u00d7 L1+ S2\u00d7 L2+ S3\u00d7 L3\nE2\nE3\n+ L\n\u00d7\n\u00d7\n\u00d7\n+\nSoftmax\nH\u00d7W\u00d7C1\nH\u00d7W\u00d7C2\nH\u00d7W\u00d7C3\nH\u00d7W\u00d7C\n1\u00d71\u00d7C 1\u00d71\u00d7r\n1\u00d71\u00d7C\n1\u00d71\u00d7C\n1\u00d71\u00d7C\nH\u00d7W\u00d7C1\nH\u00d7W\u00d7C2\nH\u00d7W\u00d7C3\nH\u00d7W\u00d7C\nS Z\nZ1\nZ2\nZ3\nS1\nS2\nS3\nE1\nL1\nL2\nL3\nMLFFout\nFusion Selection\nD W\nT D W T\nC o n v\nC o n v\nC o n v\nC o n v\nC o n v\nC o n v\nG A P\nD W\nT C o n v\nFigure\u00a04. The structure of MLFF.\n6 Vol:.(1234567890) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nwhere MLFFi represents the output of MLFF at layer i of the model; Conv(\u00b7) is the convolution kernel of 1 \u00d7 1, which is used to adjust the number of channels to facilitate the operation of wavelet transform; \u2191 represents the feature information of the same level size obtained through the wavelet transform; \u2193 represents the feature size of the same level obtained through the IWT; \u00d7 and + respectively represent point multiplication and addition operations between feature elements; Sij represents the MLFFi fusion multi-layer feature information obtained after the selection stage and activation, specifically the jth feature component; and the values of j and i are 1, 2, 3."
        },
        {
            "heading": "DMRFAB module",
            "text": "In a CNN, the convolutional kernel processes the entire image uniformly without focusing on specific areas. Attention mechanisms can ignore certain irrelevant regional information and focus on the key areas in the image through learning. Different from other methods, the proposed DMRFAB module includes a dense multi-receptive field module both introducing SAM21 and CAM22, which helps multi- receptive field blocks better extract deep feature information, improve feature representation capabilities, and ultimately improve module deblurring performance. The DMRFAB module, illustrated in Fig.\u00a05, comprises four MRFAB units and a bottleneck layer. The MRFAB units are responsible for extracting semantic features from the image, while the bottleneck layer reduces the number of feature inputs, enhancing the model\u2019s efficiency and compactness. A dense connection enhances the transmission of image features and makes more effective use of image features. The DMRFAB is shown in Eq.\u00a0(11).\nwhere [x0, x1, . . . , xi\u22121] indicates the feature map made by the DMRFAB of 0, 1, \u2026, i \u2212 1 layers in series; Hi represents converting multiple input tensors into a single tensor; G(\u00b7) represents the output of the bottleneck layer; \u03b5 is the super parameter of the bottleneck layer, and the filter size used in the bottleneck layer is 1 \u00d7 1. The structure of MRFAB used by the DMRFAB module is shown in Fig.\u00a06.\n(8)MLFF1 = E1\u00d7 S11 + (Conv(E2))\u2193 \u00d7 S12 + (Conv(E3))\u2193 \u00d7 S13\n(9)MLFF2 = (Conv(E1))\u2193 \u00d7 S21 + E2\u00d7 S22 + (Conv(E3))\u2191 \u00d7 S23\n(10)MLFF3 = (Conv(E1))\u2193 \u00d7 S31 + (Conv(E2))\u2193 \u00d7 S32 + E3\u00d7 S33\n(11)Xout = G{(Hi[x0, x1, . . . , xi\u22121]); \u03b5}\nmulti-receptive field attention block\nbottleneck layer\ndense connection\nFigure\u00a05. The DMRFAB module.\nInput Conv(1*1)\nConv (3*3)\nC\nH\u00d7W\u00d7C\nH\u00d7W\u00d7C\nOutput\nCA\nSA\nC\n\u00d7\n\u00d7\nConv(1*1)\nDilation=1\nConv (3*3)\nConv (3*3)\nConv (3*3)\nDilation=2\nDilation=3\nDilation=4\nFigure\u00a06. The structure of MRFAB.\n7 Vol.:(0123456789) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nAs shown in Fig.\u00a06, input information is firstly input into a convolution block using 1 \u00d7 1 convolution kernel, and then feature information with four feature extraction branches is extracted utilizing 3 \u00d7 3 convolution kernel with extensional rates of 1, 2, 3 and 4. The connection operation fuses the parallel feature maps of the five branches as shown in Eq.\u00a0(12). The feature information fused through the connection operation is directed into two modules: the CA module, responsible for implementing the CAM, and the SA module, which implements the spatial attention mechanism. The outputs from these modules are individually point-multiplied with the fused information. Subsequently, the connection operation is applied once more. Finally, the input is processed by a convolution block using a 1 \u00d7 1 convolution kernel, as described in Eq.\u00a0(13). This step serves to fuse and reduce the dimensionality of the feature information. In Eqs.\u00a0(12), (13), R represents the feature map connecting different branch receptive fields; X represents the input of MRFAB; CA (\u2022) represents the operation of the CA module; SA (\u2022) represents the operation of the SA module; C at(\u00b7) represents the connection operation; and M represents the output of MRFAB.\nThe CA module maps the relationship between feature channels through compression and excitation operations. The structure of the CA module, as depicted in Fig.\u00a07, takes an input feature map G with dimensions H \u00d7 W \u00d7 C. First, it employs global average pooling (GAP) to compress the dimensions, resulting in a feature vector d with dimensions 1 \u00d7 1 \u00d7 C, which encodes global context information. Then, the incoming features go through two convolutional layers followed by a sigmoid activation function. The CA module ultimately produces features with a size of 1 \u00d7 1 \u00d7 C.\nThe SA module mainly uses the spatial correlation between features, and its structure is shown in Fig.\u00a08. The SA module can transform the input multi-dimensional features into a one-dimensional feature map with spatial characteristics, and correct the incoming feature information. The SA module takes as input a feature map G with dimensions H \u00d7 W \u00d7 C. It utilizes both maximum pooling and global average pooling, and then combines the results to form a feature map F with dimensions H \u00d7 W \u00d7 2. Following this, a convolutional layer and sigmoid activation function are applied to generate the output feature map Fout with dimensions H \u00d7 W \u00d7 1. The mathematical expression of the SA module is represented in Eq.\u00a0(14).\nwhere G indicates the input characteristics; MAXPool (\u2022) indicates the global maximum pooling operation; AvgPool (\u2022) indicates the global average pooling operation; Cat indicates the connection operation; Conv (\u2022) indicates the convolution operation; and Sigmoid (\u2022) indicates the activation function.\n(12)R = Cat\n\n\nX \u2217W1\u00d71 \u2217W d=1 3\u00d73 X \u2217W1\u00d71 \u2217W d=2 3\u00d73 X \u2217W1\u00d71 \u2217W d=3 3\u00d73 X \u2217W1\u00d71 \u2217W d=4 3\u00d73\nX\n\n\n(13)M = Cat[CA(R) \u2217 R, SA(R) \u2217 R] \u2217W1\u00d71\n(14)Fout = Sigmoid(Conv ( Cat [ MAXPool(G),AvgPool(G) ]) )\nS 1\u00d71\u00d7C\nd\nH\u00d7W\u00d7C 1\u00d71\u00d7C\nsigmoid activation function\nIn p u t\nG A P\nC o n v\nC o n v\nS\nFigure\u00a07. The Structure of CA module.\nC S\nH\u00d7W\u00d7C H\u00d7W\u00d71\nH\u00d7W\u00d71\nH\u00d7W\u00d72\nG\nF\nFout\nglobal average pooling global maximum pooling\nH\u00d7W\u00d71GA P\nG M P\nC o n v\nGAP GMP\nFigure\u00a08. The Structure of SA module.\n8 Vol:.(1234567890) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4"
        },
        {
            "heading": "Results",
            "text": ""
        },
        {
            "heading": "Dataset and training details",
            "text": "In the present study, the GoPro and Real Blur datasets23 were used for the experiments. The GoPro dataset is composed of 3214 clear and blurred image pairs including 22 different scenes. In total, 2103 image pairs were used as the training dataset and 1111 pairs of images were used as the test dataset. The Real Blur dataset is a large-scale dataset of blurry images and clear images in the real world, which is composed of two subsets of the same image content. The first subset consists of images obtained directly from the camera, representing the original unprocessed images. The second subset comprises images generated after being processed by the camera\u2019s image processor. There are 232 scenarios with low light conditions, including nighttime and indoor settings with weak lighting. These scenarios encompass typical real-world scenes. The dataset contains a total of 4738 pairs of images captured in various scenes. The Real Blur dataset serves as valuable research data for evaluating image deblurring techniques based on deep learning models in real-world settings. For the experiments, 3758 image pairs from this dataset were allocated for training the proposed model, while the remaining 980 pairs were reserved for testing and evaluation.\nTo strengthen the generalization ability of the model, data enhancement operations were performed on the GoPro and Real Blur training datasets. The operations included random rotation and adding Gaussian noise. Specifically, the data augmentation included random flips in both horizontal (left to right) and vertical (upside down) directions, as well as rotations at angles of 90, 180, and 270 degrees. Gaussian noise was also introduced with an average value of 0 and a variance of 0.0001. As a result, the GoPro training dataset was expanded from 2103 image pairs to 8412 image pairs, while the Real Blur training dataset grew from 3785 image pairs to 15,032 image pairs through these augmentation techniques.\nIn order to prevent model overfitting, images from the training datasets were randomly cropped to a size of 256 \u00d7 256 pixels. The training period was set to 3000 rounds and the initial learning rate was set to 1e-4, which was halved every 1000 rounds. The batch size was also set to 10. The adopted network optimization method was Adam24, with the parameters of \u03b21 = 0.9 and \u03b22 = 0.999. To expedite the experimental training, the model was trained using a GPU, which is well-suited for computationally intensive image processing tasks. The experimental environment and configuration, as detailed in Table\u00a01, were employed in the present research.\nDesign and comparative analysis of loss function"
        },
        {
            "heading": "Design of loss function",
            "text": "The MSE (mean square error) can allow for the difference between predicted and actual values to be obtained, and is widely used as the loss function for model evaluation. In the present study, MSE was used for model training. The mapping function is shown in Eq.\u00a0(15).\nwhere D indicates the mapping function; b indicates the blurred image; and \u015c represents an image restored from a model. The MSE loss function is shown in Eq.\u00a0(16).\nwhere D represents the mapping function obtained by learning the model; S represents the label image; \u03b8 represents the learning parameters in the network model; and N represents the N pairs of images inputted into the network by the training dataset.\nThe latest research has shown that there are other auxiliary losses besides the loss of image content. In image enhancement tasks, a common approach involves minimizing the distance loss between the input and output in the feature space. This technique has been extensively adopted and has proven effective in achieving improved results 25. There is still a gap between the real image and the restored image, especially in the frequency domain. As is widely known, since the purpose of image deblurring is to recover the lost high-frequency component information, the difference in the frequency domain space should be reduced. Therefore, a frequency reconstruction loss function (FRLF) was proposed in the present study to address the aforementioned problem. The frequency reconstruction loss can be defined as the L1 distance between the real image and the blurred image in the measured frequency. The mathematical expression is shown in Eq.\u00a0(17).\n(15)D(b) = S\u0302\n(16)LMSE(\u03b8) = 1\n2N\nN\u2211\ni=1\n\ufffdD(bi; \u03b8)\u2212 Si\ufffd 2 F\n9 Vol.:(0123456789) Scientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nwhere \u0413 represents Fast Fourier Transform (FFT), which transforms image features from the spatial domain to the frequency domain; \u015c represents the potential image restored by the model; S represents the label image; and N represents the sum of the logarithms of the image input into the FFT. Hence, the loss function used for model training in the present study was Ltotal, which is shown in Eq.\u00a0(18). In the experiments, the parameter \u03bb was set to 0.1.\nComparative analysis of loss function In order to verify the validity of the proposed FRLF, it was compared with two related loss functions, the SSIM loss function and the perceptual loss function26. The functions were applied to the proposed model and tested using the GoPro test data set. All loss functions were loaded using the strategy in Eq.\u00a0(19), where La represents different loss functions. To balance the output of the loss functions and LMSE , different parameters were assigned to the of different loss functions. The performances of the different loss functions in the proposed model are shown in Table\u00a02.\nTable\u00a02 shows that compared with the MSE loss function only, the introduction of the loss function could improve the performance of the model. The SSIM loss function PSNR increased by 1.37\u00a0dB, and the SSIM increased by 0.004. The perceptual loss function PSNR was increased by 1.85\u00a0dB, and the SSIM was increased by 0.013. The introduction of FRLF delivered the most significant improvements. It led to a substantial PSNR increase of 2.07\u00a0dB and a considerable SSIM increase of 0.017. FRLF is particularly effective as it aids the model in recovering high-frequency image details by reducing frequency gaps. Consequently, the conclusion is that FRLF is a straightforward yet highly effective loss function for improving model performance."
        },
        {
            "heading": "Evaluating indicator",
            "text": "Using PSNR and SSIM as evaluation indicators, the proposed model was quantitatively analyzed using the GoPro and Real Blur test datasets, and compared with other models such as the CNN27, the Multi-scale CNN28, spatially variant RNN29, the improved U-Net model19, the CAR GAN30, Attentive deep network6, and BDIRNet33. Those other models were analyzed using the same test datasets with the proposed models. Table\u00a03 shows the results, indicating that the proposed model performed well in terms of the PSNR and SSIM indicators on GoPro.\nTo rigorously assess the performance of the proposed model, a series of experiments were conducted on the Real Blur test dataset. The model\u2019s results were quantitatively compared with those of other existing models, including CNN27, Spatially Variant RNN29, CAR GAN30, and BDIRNet33. The results are shown in Table\u00a04, indicating the proposed model performed better in terms of the PSNR and SSIM indicators on Real Blur."
        },
        {
            "heading": "Model parameters and efficiency analysis",
            "text": "To thoroughly analyze the scale and efficiency of the proposed model, a comparison was made between its running time and model parameter size in contrast to other methods when restoring images using the GoPro test dataset. The comparison results are shown in Table\u00a05, where Time represents the run time required for the model, and Size represents the model parameters size. As shown in Table\u00a05, the proposed model took less time and had smaller model parameters than those of the CNN27, Multi-scale CNN28, SRN-DeblurNet31, and BDIRNet33, but\n(17)LFRLF = 1\n2N\nN\u2211\ni=1\n\u2225\u2225\u2225\u0174 ( S\u0302 ) \u2212 \u0174(S) \u2225\u2225\u2225 1\n(18)Ltotal = LMSE + LFRLF\n(19)Ltotal = LMSE + La\n10\nVol:.(1234567890)\nScientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\ntook longer and had more parameters than the improved U-Net19. Such findings could be attributed to the model adding more modules and increasing the calculation amount compared with it. Therefore, the proposed model has certain advantages in parameter scale and efficiency."
        },
        {
            "heading": "Discussion",
            "text": ""
        },
        {
            "heading": "Visual analysis",
            "text": "To provide further validation of the proposed model\u2019s deblurring effectiveness, this section presents visual results on both the GoPro test dataset and the Real Blur test dataset. The evaluation was conducted by comparing the deblurred images produced by the model to the original clear images, serving as reference, to assess and analyze the quality of image deblurring achieved by the model.\nFigure\u00a09 shows a visual effect comparison on the GoPro test dataset. The images are displayed in a left-to-right sequence, showcasing different aspects of the deblurring process. Starting from the left, there is a magnified view of the initially blurred image. Next, the original clear image serves as a reference for evaluation. Subsequently, the deblurring results from various models are displayed. The restoration image obtained using CNN27 is shown, followed by the result from DeblurGAN32, SRN-DeblurNet31, and CAR GAN30. Finally, the far-right image represents the restoration achieved by the proposed model. This visual comparison allows for a direct assessment of the deblurring effectiveness of each method on the GoPro test dataset, providing valuable insights into the quality of image restoration. As shown in Fig.\u00a09, the restoration image generated by the CNN27 was juxtaposed with the blurred image. The method exhibited a degree of blurring reduction capability; however, it still manifested noticeable artifacts, resulting in an insufficient deblurring effect. In comparison to CNN27, the DeblurGAN32 demonstrated an enhancement in deblurring effectiveness but was not entirely free from artifacts. The visual representation of SRN-DeblurNet31 showed a lack of conspicuous artifacts, yet it suffered from blurriness in ground-level details. CAR GAN30 exhibited superior artifact removal capabilities but falls short in restoring heavily blurred regions. Attentive deep network6 have similar deblurring effects to our method, but their size is slightly larger. In contrast, the proposed model excelled in image deblurring, offering the most notable results by capturing extensive image information, achieving clearer image restoration, and displaying reduced susceptibility to artifacts. This capability effectively mitigated interference from artifacts and other factors.\nTable 4. Performance comparison of various methods on Real Blur.\nMethod CNN 27 Spatially variant RNN 29 CAR GAN 30 BDIRNet 33 Ours\nPSNR(dB) 26.31 29.56 30.69 29.62 31.32\nSSIM 0.840 0.909 0.922 0.86 0.934\nTable 5. Comparison of model running time and model size.\nMethod CNN 27 Multi-scale CNN 28 SRN-DeblurNet 31 Improved U-Net 19 Attentive deep network 6 BDIRNet 33 Ours\nTime (s) 1200 3.2 3.32 0.56 0.28 1.4 0.67\nSize (MB) 50.10 303.60 41.3 22.6 26.34 25.26 24.4\nblurred image\nclear image\nCNN [27] DeblurGAN [32] SRNDeblurNet[31] CAR GAN [30]\nOur model locally magnified blurred image Attentive deep network [6]\nFigure\u00a09. Visual effect comparison on GoPro test dataset.\n11\nVol.:(0123456789)\nScientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nFigure\u00a010 illustrates a visual comparison on the Real Blur test dataset. From left to right, the sequence includes a locally magnified blurred image, a clear image, and restoration images generated by CNN27, Spatially Variant RNN28, SRN-DeblurNet31, and the proposed model, respectively.\nAs depicted in Fig.\u00a010, the restoration image produced by CNN27 exhibited some improvement over the blurred image. Nevertheless, it remained considerably blurry, with a noticeable presence of artifacts. Multi-scale CNN28 achieved a superior outcome compared to CNN27, resulting in a clearer restored image that mitigated the impact of artifacts. However, artifacts were still present in the restored image. SRN-DeblurNet31, BDIRNet33 demonstrated superior artifact removal capabilities in comparison to Multi-scale CNN28, resulting in a clearer restored image. However, the restoration effect for areas with severe blurring was less than ideal. The proposed model performed well on the Real Blur test dataset, achieving effective image restoration with minimal artifacts. Nonetheless, there is still room for improvement when compared to clear images."
        },
        {
            "heading": "Module performance analysis",
            "text": "To assess the effectiveness of the proposed modules, five model experiments were conducted. Model1 excluded only DWT and IWT. Model2 omitted the DMRFAB module alone. Model3 excluded only the SA and CA modules. In Model4, only the MLFF module was not included. Model5 integrated all proposed modules. The results are summarized in Table\u00a06. In Model2, the image evaluation index PSNR reached 30.07, and SSIM reached 0.928. This suggests that the inclusion of DWT and IWT contributed to the improved performance of Model2. These transformations effectively captured contextual and textural information across different image frequencies. Upon introducing the DMRFAB module in Model3, PSNR increased to 31.28\u00a0dB, and SSIM improved to 0.941. This demonstrates the effectiveness of the DMRFAB module in enhancing deblurring capabilities. In Model4, the introduction of the SA and CA modules led to a PSNR of 31.35\u00a0dB and SSIM of 0.943. This can be attributed to the synergy between the attention mechanism and convolution, which prioritizes global information and selects crucial feature information. This, in turn, enhanced the model\u2019s deblurring ability. Finally, in Model 5, where all modules were incorporated, PSNR reached 31.53, and SSIM rose to 0.948. This underscores the positive impact of the proposed modules on enhancing the quality of the restored images. The combination of these modules improved feature extraction, facilitated feature reconstruction, and aided the model in learning the mapping relationship between blurry and clear images."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, an image deblurring method based on U-Net model was proposed, in which an MLFF module and a DMRFAB module was designed. The MLFF module integrates feature information in different layers of the U-Net network, changes the inherent information flow mode in the conventional U-Net network, and integrates feature information of different scales, so that the network can extract more feature information. DMRFAB introduces both a spatial attention mechanism and a channel attention mechanism, explores the relationship between different feature channels and the spatial relationship between different features, overcomes the shortcomings\nclear image\nCNN [27] Multi-scale CNN [28] SRNDeblurNet [31]\nOur model Locally magnified blurred image blurred image\nBDIRNet[33]\nFigure\u00a010. Visual effect comparison on Real Blur test dataset.\n12\nVol:.(1234567890)\nScientific Reports | (2023) 13:21402 | https://doi.org/10.1038/s41598-023-47768-4\nof a single attention mechanism, obtains the information of important parts of the features, and further obtains the deep features, thereby improving the effect of blur removal. Additionally, FFT was introduced into the loss function to obtain the frequency value of the image, reduce the frequency difference of the image, and improve the effect of deblurring. The average PSNR and average SSIM values for the GoPro dataset were 31.53 and 0.948 respectively, while those for the Real Blur dataset were 31.32 and 0.934 respectively, which were higher than those of the other methods. Therefore, the present method can produce a better deblurring effect.\nIn future work, the focus will center on refining the model. This entails efforts to enhance its lightweight characteristics, thereby optimizing its performance on commonly used mobile devices."
        },
        {
            "heading": "Data availability",
            "text": "The datasets generated and/or analyzed during the current study are available from the corresponding author upon reasonable request.\nReceived: 11 February 2023; Accepted: 17 November 2023"
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the Fundamental Research Funds for Higher Education Institutions of Heilongjiang Province (145209126), and the Heilongjiang Province Higher Education Teaching Reform Project under Grant No. SJGY20200770."
        },
        {
            "heading": "Author contributions",
            "text": "H.Z.W. conceptualized and designed the study, collected and analyzed the data, drafted the initial manuscript, and reviewed and revised the manuscript. Z.Z.L. designed the study, analyzed the data, and reviewed and revised the manuscript. All authors approved the final manuscript as submitted and agreed to be accountable for all aspects of the work."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Correspondence and requests for materials should be addressed to H.W.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2023"
        }
    ],
    "title": "An image deblurring method using improved U-Net model based on multilayer fusion and attention mechanism",
    "year": 2023
}