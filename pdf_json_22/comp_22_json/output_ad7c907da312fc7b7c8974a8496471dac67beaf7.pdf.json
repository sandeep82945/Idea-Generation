{
    "abstractText": "Pre-training is prevalent in nowadays deep learning to improve the learned model\u2019s performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients\u2019 data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients\u2019 data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We conclude our paper with an attempt to understand the effect of pre-training on FL. We found that pre-training enables the learned global models under different clients\u2019 data conditions to converge to the same loss basin, and makes global aggregation in FL more stable. Nevertheless, pre-training seems to not alleviate local model drifting, a fundamental problem in FL under non-IID data.",
    "authors": [
        {
            "affiliations": [],
            "name": "FEDERATED LEARNING"
        },
        {
            "affiliations": [],
            "name": "Hong-You Chen"
        },
        {
            "affiliations": [],
            "name": "Cheng-Hao Tu"
        },
        {
            "affiliations": [],
            "name": "Ziwei Li"
        },
        {
            "affiliations": [],
            "name": "Han-Wei Shen"
        },
        {
            "affiliations": [],
            "name": "Wei-Lun Chao"
        }
    ],
    "id": "SP:6c5ce9df36d19c16575f4812febdd7d73ffe4300",
    "references": [
        {
            "authors": [
                "Durmus Alp Emre Acar",
                "Yue Zhao",
                "Ramon Matas",
                "Matthew Mattina",
                "Paul Whatmough",
                "Venkatesh Saligrama"
            ],
            "title": "Federated learning based on dynamic regularization",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Connor Anderson",
                "Ryan Farrell"
            ],
            "title": "Improving fractal pre-training",
            "venue": "In WACV,",
            "year": 2022
        },
        {
            "authors": [
                "Manel Baradad",
                "Jonas Wulff",
                "Tongzhou Wang",
                "Phillip Isola",
                "Antonio Torralba"
            ],
            "title": "Learning to see by looking at noise",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Monik Raj Behera",
                "Sudhir Upadhyay",
                "Suresh Shetty",
                "Sudha Priyadarshini",
                "Palka Patel",
                "Ker Farn Lee"
            ],
            "title": "Fedsyn: Synthetic data generation using federated learning",
            "venue": "arXiv preprint arXiv:2203.05931,",
            "year": 2022
        },
        {
            "authors": [
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Caldas",
                "Peter Wu",
                "Tian Li",
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Virginia Smith",
                "Ameet Talwalkar"
            ],
            "title": "Leaf: A benchmark for federated settings",
            "venue": "arXiv preprint arXiv:1812.01097,",
            "year": 2018
        },
        {
            "authors": [
                "Hong-You Chen",
                "Wei-Lun Chao"
            ],
            "title": "Fedbe: Making bayesian model ensemble applicable to federated learning",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Tianlong Chen",
                "Sijia Liu",
                "Shiyu Chang",
                "Yu Cheng",
                "Lisa Amini",
                "Zhangyang Wang"
            ],
            "title": "Adversarial robustness: From self-supervised pre-training to fine-tuning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Kevin Swersky",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "Big self-supervised models are strong semi-supervised learners",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Gary Cheng",
                "Karan Chadha",
                "John Duchi"
            ],
            "title": "Fine-tuning is fine in federated learning",
            "venue": "arXiv preprint arXiv:2108.07313,",
            "year": 2021
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Scharw\u00e4chter",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset",
            "venue": "In CVPR Workshop on The Future of Datasets in Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Josip Djolonga",
                "Jessica Yung",
                "Michael Tschannen",
                "Rob Romijnders",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Joan Puigcerver",
                "Matthias Minderer",
                "Alexander D\u2019Amour",
                "Dan Moldovan"
            ],
            "title": "On robustness and transferability of convolutional neural networks",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Dumitru Erhan",
                "Aaron Courville",
                "Yoshua Bengio",
                "Pascal Vincent"
            ],
            "title": "Why does unsupervised pre-training help deep learning",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Linus Ericsson",
                "Henry Gouk",
                "Timothy M Hospedales"
            ],
            "title": "How well do self-supervised models transfer",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Amin Fazel",
                "Wei Yang",
                "Yulan Liu",
                "Roberto Barra-Chicote",
                "Yixiong Meng",
                "Roland Maas",
                "Jasha Droppo"
            ],
            "title": "Synthasr: Unlocking synthetic data for speech recognition",
            "year": 2021
        },
        {
            "authors": [
                "Jhair Gallardo",
                "Tyler L Hayes",
                "Christopher Kanan"
            ],
            "title": "Self-supervised training enhances online continual learning",
            "venue": "In BMVC,",
            "year": 2021
        },
        {
            "authors": [
                "Priya Goyal",
                "Mathilde Caron",
                "Benjamin Lefaudeux",
                "Min Xu",
                "Pengchao Wang",
                "Vivek Pai",
                "Mannat Singh",
                "Vitaliy Liptchinsky",
                "Ishan Misra",
                "Armand Joulin"
            ],
            "title": "Self-supervised pretraining of visual features in the wild",
            "venue": "arXiv preprint arXiv:2103.01988,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre H Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Daniel Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Farzin Haddadpour",
                "Mehrdad Mahdavi"
            ],
            "title": "On the convergence of local descent methods in federated learning",
            "venue": "arXiv preprint arXiv:1910.14425,",
            "year": 2019
        },
        {
            "authors": [
                "Yaru Hao",
                "Li Dong",
                "Furu Wei",
                "Ke Xu"
            ],
            "title": "Visualizing and understanding the effectiveness of bert",
            "venue": "arXiv preprint arXiv:1908.05620,",
            "year": 2019
        },
        {
            "authors": [
                "Chaoyang He",
                "Murali Annavaram",
                "Salman Avestimehr"
            ],
            "title": "Group knowledge transfer: Federated learning of large cnns at the edge",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Ross Girshick",
                "Piotr Doll\u00e1r"
            ],
            "title": "Rethinking imagenet pre-training",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kimin Lee",
                "Mantas Mazeika"
            ],
            "title": "Using pre-training can improve model robustness and uncertainty",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Xiaoyuan Liu",
                "Eric Wallace",
                "Adam Dziedzic",
                "Rishabh Krishnan",
                "Dawn Song"
            ],
            "title": "Pretrained transformers improve out-of-distribution robustness",
            "venue": "arXiv preprint arXiv:2004.06100,",
            "year": 2020
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Measuring the effects of non-identical data distribution for federated visual classification",
            "year": 1909
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Federated visual classification with real-world data distribution",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Yi Mao",
                "Pengcheng He",
                "Graham Neubig",
                "Weizhu Chen"
            ],
            "title": "OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Keith Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "arXiv preprint arXiv:1912.04977,",
            "year": 2019
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Martin Jaggi",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank J Reddi",
                "Sebastian U Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Mime: Mimicking centralized stochastic algorithms in federated learning",
            "venue": "arXiv preprint arXiv:2008.03606,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hirokatsu Kataoka",
                "Kazushige Okayasu",
                "Asato Matsumoto",
                "Eisuke Yamagata",
                "Ryosuke Yamada",
                "Nakamasa Inoue",
                "Akio Nakamura",
                "Yutaka Satoh"
            ],
            "title": "Pre-training without natural images",
            "venue": "In ACCV,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel S Kermany",
                "Michael Goldbaum",
                "Wenjia Cai",
                "Carolina CS Valentim",
                "Huiying Liang",
                "Sally L Baxter",
                "Alex McKeown",
                "Ge Yang",
                "Xiaokang Wu",
                "Fangbing Yan"
            ],
            "title": "Identifying medical diagnoses and treatable diseases",
            "venue": "by image-based deep learning. Cell,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Joan Puigcerver",
                "Jessica Yung",
                "Sylvain Gelly",
                "Neil Houlsby"
            ],
            "title": "Big transfer (bit): General visual representation learning",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Simon Kornblith",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Do better imagenet models transfer better",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Ang Li",
                "Jingwei Sun",
                "Xiao Zeng",
                "Mi Zhang",
                "Hai Li",
                "Yiran Chen"
            ],
            "title": "Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking",
            "venue": "In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Qinbin Li",
                "Bingsheng He",
                "Dawn Song"
            ],
            "title": "Model-contrastive federated learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smithy"
            ],
            "title": "Feddane: A federated newton-type method",
            "venue": "53rd Asilomar Conference on Signals, Systems, and Computers,",
            "year": 2019
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "MLSys,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Chaoyang He",
                "Zihang Zeng",
                "Hulin Wang",
                "Yufen Huang",
                "Mahdi Soltanolkotabi",
                "Xiang Ren",
                "Salman Avestimehr"
            ],
            "title": "Fednlp: Benchmarking federated learning methods for natural language processing",
            "year": 2022
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Liu",
                "Fanjin Zhang",
                "Zhenyu Hou",
                "Li Mian",
                "Zhaoyu Wang",
                "Jing Zhang",
                "Jie Tang"
            ],
            "title": "Selfsupervised learning: Generative or contrastive",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Ekdeep Singh Lubana",
                "Chi Ian Tang",
                "Fahim Kawsar",
                "Robert P Dick",
                "Akhil Mathur"
            ],
            "title": "Orchestra: Unsupervised federated learning via globally consistent clustering",
            "year": 2022
        },
        {
            "authors": [
                "Benoit B Mandelbrot"
            ],
            "title": "The fractal geometry of nature, volume 1",
            "venue": "WH freeman New York,",
            "year": 1982
        },
        {
            "authors": [
                "H Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "year": 2017
        },
        {
            "authors": [
                "Sanket Vaibhav Mehta",
                "Darshan Patil",
                "Sarath Chandar",
                "Emma Strubell"
            ],
            "title": "An empirical investigation of the role of pre-training in lifelong learning",
            "venue": "arXiv preprint arXiv:2112.09153,",
            "year": 2021
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Hanie Sedghi",
                "Chiyuan Zhang"
            ],
            "title": "What is being transferred in transfer learning",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "John Nguyen",
                "Kshitiz Malik",
                "Maziar Sanjabi",
                "Michael Rabbat"
            ],
            "title": "Where to begin? exploring the impact of pre-training and initialization in federated learning",
            "venue": "arXiv preprint arXiv:2206.15387,",
            "year": 2022
        },
        {
            "authors": [
                "Liangqiong Qu",
                "Yuyin Zhou",
                "Paul Pu Liang",
                "Yingda Xia",
                "Feifei Wang",
                "Li Fei-Fei",
                "Ehsan Adeli",
                "Daniel Rubin"
            ],
            "title": "Rethinking architecture design for tackling data heterogeneity in federated learning",
            "venue": "arXiv preprint arXiv:2106.06047,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Sashank Reddi",
                "Zachary Charles",
                "Manzil Zaheer",
                "Zachary Garrett",
                "Keith Rush",
                "Jakub Kone\u010dn\u1ef3",
                "Sanjiv Kumar",
                "H Brendan McMahan"
            ],
            "title": "Adaptive federated optimization",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Colorado J Reed",
                "Sean Metzger",
                "Aravind Srinivas",
                "Trevor Darrell",
                "Kurt Keutzer"
            ],
            "title": "Selfaugment: Automatic augmentation policies for self-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Colorado J Reed",
                "Xiangyu Yue",
                "Ani Nrusimha",
                "Sayna Ebrahimi",
                "Vivek Vijaykumar",
                "Richard Mao",
                "Bo Li",
                "Shanghang Zhang",
                "Devin Guillory",
                "Sean Metzger"
            ],
            "title": "Self-supervised pretraining improves self-supervised pretraining",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "year": 2015
        },
        {
            "authors": [
                "Mark Sandler",
                "Andrew Howard",
                "Menglong Zhu",
                "Andrey Zhmoginov",
                "Liang-Chieh Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian U Stich"
            ],
            "title": "Local sgd converges fast and communicates little",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Joel Stremmel",
                "Arjun Singh"
            ],
            "title": "Pretraining federated text models for next word prediction",
            "venue": "In Future of Information and Communication Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Sun",
                "Abhinav Shrivastava",
                "Saurabh Singh",
                "Abhinav Gupta"
            ],
            "title": "Revisiting unreasonable effectiveness of data in deep learning era",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Yue Tan",
                "Guodong Long",
                "Jie Ma",
                "Lu Liu",
                "Tianyi Zhou",
                "Jing Jiang"
            ],
            "title": "Federated learning from pre-trained models: A contrastive learning approach",
            "venue": "arXiv preprint arXiv:2209.10083,",
            "year": 2022
        },
        {
            "authors": [
                "Yonglong Tian",
                "Chen Sun",
                "Ben Poole",
                "Dilip Krishnan",
                "Cordelia Schmid",
                "Phillip Isola"
            ],
            "title": "What makes for good views for contrastive learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Grant Van Horn",
                "Pietro Perona"
            ],
            "title": "The devil is in the tails: Fine-grained classification in the wild",
            "venue": "arXiv preprint arXiv:1709.01450,",
            "year": 2017
        },
        {
            "authors": [
                "Grant Van Horn",
                "Oisin Mac Aodha",
                "Yang Song",
                "Yin Cui",
                "Chen Sun",
                "Alex Shepard",
                "Hartwig Adam",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The inaturalist species classification and detection",
            "year": 2018
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Gauri Joshi",
                "H Vincent Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Orion Weller",
                "Marc Marone",
                "Vladimir Braverman",
                "Dawn Lawrie",
                "Benjamin Van Durme"
            ],
            "title": "Pretrained models for multilingual federated learning",
            "venue": "arXiv preprint arXiv:2206.02291,",
            "year": 2022
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Felix Li",
                "Percy Liang"
            ],
            "title": "Insights into pre-training via simpler synthetic tasks",
            "venue": "arXiv preprint arXiv:2206.10139,",
            "year": 2022
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Yuzhe Yang",
                "Zhi Xu"
            ],
            "title": "Rethinking the value of labels for improving class-imbalanced learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Xin Yao",
                "Tianchi Huang",
                "Rui-Xiao Zhang",
                "Ruiyu Li",
                "Lifeng Sun"
            ],
            "title": "Federated learning with unbiased gradient aggregation and controllable meta updating",
            "year": 1910
        },
        {
            "authors": [
                "Mikhail Yurochkin",
                "Mayank Agarwal",
                "Soumya Ghosh",
                "Kristjan Greenewald",
                "Trong Nghia Hoang",
                "Yasaman Khazaeni"
            ],
            "title": "Bayesian nonparametric federated learning of neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Yue Zhao",
                "Meng Li",
                "Liangzhen Lai",
                "Naveen Suda",
                "Damon Civin",
                "Vikas Chandra"
            ],
            "title": "Federated learning with non-iid data",
            "venue": "arXiv preprint arXiv:1806.00582,",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Agata Lapedriza",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Places: A 10 million image database for scene recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Fan Zhou",
                "Guojing Cong"
            ],
            "title": "On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization",
            "venue": "arXiv preprint arXiv:1708.01012,",
            "year": 2017
        },
        {
            "authors": [
                "Yanlin Zhou",
                "George Pu",
                "Xiyao Ma",
                "Xiaolin Li",
                "Dapeng Wu"
            ],
            "title": "Distilled one-shot federated",
            "year": 2023
        },
        {
            "authors": [
                "Zirui Zhu",
                "Lifeng Sun"
            ],
            "title": "Initialize with mask: For more efficient federated learning",
            "venue": "MultiMedia",
            "year": 2009
        },
        {
            "authors": [
                "Baradad"
            ],
            "title": "pre-training and facilitating downstream tasks (Kataoka et al., 2020)",
            "year": 2021
        },
        {
            "authors": [
                "Grill"
            ],
            "title": "The core idea of these approaches is to treat every image as from a different class and learn to either repulse different images away (i.e., negative pairs) or draw an image and an augmented version of it closer (i.e., positive pairs). This line of approaches is so effective that they can even outperform supervised pre-training in many tasks",
            "year": 2021
        },
        {
            "authors": [
                "Augment Reed"
            ],
            "title": "2021), etc. Here, we propose to exploit one unique way for fractals (or broadly, synthetic data), which is to use the IFS to create a pair of images based on the same set of codes {Si}i=1. We argue that this method can create stronger and more physically-meaningful argumentation: not only do different fractals from the same codes capture intra-class variation, but we also can create a pair of images with different placements of the same I fractals (more like object-level",
            "year": 2021
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2020d) also aims to maximize the similarity between features extracted from a positive image pair (x1,x2). The main difference is that MoCo-V2 adopts contrastive loss, which also takes negative pairs into account",
            "venue": "SimSiam",
            "year": 2020
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2021a); Zhu & Sun (2021) proposed to find a sparse mask",
            "year": 2021
        },
        {
            "authors": [
                "StyleGANs Baradad"
            ],
            "title": "Besides vision tasks, we found many recent works showing the benefits of pre-training on carefully-designed synthetic data for downstream tasks on various modalities such as text (Wu et al., 2022), SQL tables (Jiang et al., 2022), speech (Fazel et al., 2021)",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The increasing attention to data privacy and protection has attracted significant research interests in federated learning (FL) (Li et al., 2020a; Kairouz et al., 2019). In FL, data are kept separate by individual clients. The goal is thus to learn a \u201cglobal\u201d model in a decentralized way. Specifically, one would hope to obtain a model whose accuracy is as good as if it were trained using centralized data.\nFEDAVG (McMahan et al., 2017) is arguably the most widely used FL algorithm, which assumes that every client is connected to a server. FEDAVG trains the global model in an iterative manner, between parallel local model training at the clients and global model aggregation at the server. FEDAVG is easy to implement and enjoys theoretical guarantees of convergence (Zhou & Cong, 2017; Stich, 2019; Haddadpour & Mahdavi, 2019; Li et al., 2020c; Zhao et al., 2018). Its performance, however, can degrade drastically when clients\u2019 data are not IID \u2014 clients\u2019 data are often collected individually and doomed to be non-IID. That is, the accuracy of the federally learned global model can be much lower than its counterpart trained with centralized data. To alleviate this issue, existing literature has explored better approaches for local training (Li et al., 2020b; Karimireddy et al., 2020b; Acar et al., 2021) and global aggregation (Wang et al., 2020a; Hsu et al., 2019; Chen & Chao, 2021).\nIn this paper, we explore a different and rarely studied dimension in FL \u2014 model initialization. In the literature on FL, neural networks are mostly initialized with random weights. Yet in centralized learning, model initialization using weights pre-trained on large-scale datasets (Hendrycks et al., 2019; Devlin et al., 2018) has become prevalent, as it has been shown to improve accuracy, generalizability, robustness, etc. We are thus interested in 1) whether model pre-training is applicable in the context of FL and 2) whether it can likewise improve FEDAVG, especially in alleviating the non-IID issue.\nWe conduct the very first systematic study in these aspects, using visual recognition as the running example. We consider multiple application scenarios, with the aim to make our study comprehensive.\nFirst, assuming pre-trained weights (e.g., on ImageNet (Deng et al., 2009)) are available, we systematically compare FEDAVG initialized with random and pre-trained weights, under different FL settings\nar X\niv :2\n20 6.\n11 48\n8v 3\n[ cs\n.L G\n] 2\n3 M\nar 2\n02 3\nand across multiple visual recognition tasks. These include four image classification datasets, CIFAR10/100 (Krizhevsky et al., 2009), Tiny-ImageNet (Le & Yang, 2015), and iNaturalist (Van Horn et al., 2018), and one semantic segmentation dataset, Cityscapes (Cordts et al., 2016). We have the following major observations. We found that pre-training consistently improves FEDAVG; the relative gain is more pronounced in more challenging FL settings (e.g., severer non-IID conditions across clients). Moreover, pre-training largely closes the accuracy gap between FEDAVG and centralized learning (Figure 1), suggesting that pre-training brings additional benefits to FEDAVG than to centralized learning. We further consider more advanced FL methods (e.g., (Li et al., 2020b; Acar et al., 2021; Li et al., 2021b)). We found that pre-training improves their accuracy but diminishes their gain against FEDAVG, suggesting that FEDAVG is still a strong FL approach if pre-trained weights are available.\nSecond, assuming pre-trained models are not available and there are no real data at the server for pretraining, we explore the use of synthetic data. We investigate several simple yet effective synthetic image generators (Baradad et al., 2021), including fractals which are shown to capture geometric patterns found in nature (Mandelbrot & Mandelbrot, 1982). We propose a new pre-training scheme called Fractal Pair Similarity (FPS) inspired by the inner workings of fractals, which can consistently improve FEDAVG for the downstream FL tasks. This suggests the wide applicability of pre-training to FL, even without real data for pre-training.\nThird, we explore the possibility to directly pretrain with clients\u2019 data. Specifically, we investigate the two-stage training procedure \u2014 self-supervised pre-training, followed by supervised learning \u2014 in a federated setting. Such a procedure has been shown to outperform pure supervised learning in centralized learning (Chen et al., 2020c), but has not been explored in FL. Using the state-of-the-art\nfederated self-supervised approach (Lubana et al., 2022), we not only demonstrate its effectiveness in FL, but also show its compatibility with available pre-trained weights to further boost the performance.\nIntrigued by the improvement brought by pre-training, we make an attempt to understand its underlying effect on FL. We first analyze the training dynamics of FEDAVG. We found that pre-training seems to not alleviate local model drifting (Li et al., 2020b; Karimireddy et al., 2020b), a well-known issue under non-IID data. Nevertheless, it makes global aggregation more stable. Concretely, FEDAVG combines the local models\u2019 weights simply by coefficients proportional to local data sizes. Due to model drifting in local training, these coefficients can be far from optimal (Chen & Chao, 2021). Interestingly, with pre-training, FEDAVG is less sensitive to the coefficients, resulting in a stronger global model in terms of accuracy. Through visualizations of the loss landscapes (Li et al., 2018; Hao et al., 2019), we further found that pre-training enables the learned global models under different data conditions (i.e., IID or various non-IID degrees) to converge to the same loss basin. Such a phenomenon can hardly be achieved without pre-training, even if we initialize FEDAVG with the same random weights. This offers another explanation of why pre-training improves FL. Contributions and scopes. We conduct the very first systematic study on pre-training for FL, including a novel synthetic data generator. We believe that such a study is timely and significant to the FL community. We focus on visual recognition using five image datasets. We go beyond them by further studying semantic segmentation problems on the Cityscape dataset, not merely classification problems. Our extended analyses reveal new insights into FL, opening up future research directions."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Federated learning (FL). FEDAVG (McMahan et al., 2017) is the fundamental FL algorithm. Many works were proposed to improve it, especially to alleviate its accuracy drop under non-IID data. In global aggregation, Wang et al. (2020a); Yurochkin et al. (2019) matched local model weights before\naveraging. Lin et al. (2020); He et al. (2020a); Zhou et al. (2020); Chen & Chao (2021) replaced weight average by model ensemble and distillation. Hsu et al. (2019); Reddi et al. (2021) applied server momentum and adaptive optimization. In local training, to reduce local model drifting \u2014 a problem commonly believed to cause the accuracy drop \u2014Zhao et al. (2018) mixed client and server data; FEDPROX (Li et al., 2020b), FEDDANE (Li et al., 2019), and FEDDYN (Acar et al., 2021) employed regularization toward the global model; SCAFFOLD (Karimireddy et al., 2020a) and MIME (Karimireddy et al., 2020b) used control varieties or server statistics to correct local gradients; Wang et al. (2020b); Yao et al. (2019) modified local update rules.\nWe investigate a rarely studied aspect to improve FEDAVG, initialization. To our knowledge, very few works in FL have studied this (Lin et al., 2022; Stremmel & Singh, 2021; Weller et al., 2022); none are as systematic and comprehensive as ours1. (Qu et al., 2021; Hsu et al., 2020; Cheng et al., 2021) used pre-trained models in their experiments but did not or only briefly analyzed their impacts.\nPre-training. Pre-training has been widely applied in computer vision, natural language processing, and many other application domains to speed up convergence and boost accuracy for downstream tasks (Kolesnikov et al., 2020; Goyal et al., 2021; Radford et al., 2021; Sun et al., 2017; Devlin et al., 2018; Yang et al., 2019; Brown et al., 2020). Many works have attempted to analyze its impacts (Hendrycks et al., 2020; Erhan et al., 2010; He et al., 2019; Djolonga et al., 2021; He et al., 2019; Kornblith et al., 2019). For example, Hendrycks et al. (2019) and Chen et al. (2020a) found that pre-training improves robustness against adversarial examples; Neyshabur et al. (2020) studied the loss landscape when fine-tuning on target tasks; Mehta et al. (2021) empirically showed that pre-training reduces forgetting in continual learning. Despite the ample research on pre-training, its impacts on FL remain largely unexplored. We aim to fill in this missing piece."
        },
        {
            "heading": "3 BACKGROUND: FEDERATED LEARNING",
            "text": "In federated learning (FL), the training data are collected by M clients. Each client m \u2208 [M ] has a training set Dm = {(xi, yi)}|Dm|i=1 , where x is the input (e.g., images) and y is the true label. The goal is to solve the following optimization problem\nmin \u03b8 L(\u03b8) = M\u2211 m=1 |Dm| |D| Lm(\u03b8), where Lm(\u03b8) = 1 |Dm| |Dm|\u2211 i=1 `(xi, yi;\u03b8). (1)\nHere, \u03b8 is the model parameter; D = \u222amDm is the aggregated training set from all clients; L is the empirical risk on D; Lm is the empirical risk of client m; ` is the loss function on a data instance. Federated averaging (FEDAVG). As clients\u2019 data are separated, Equation 1 cannot be solved directly; otherwise, it is centralized learning. A standard way to relax it is FEDAVG (McMahan et al., 2017), which iterates between two steps \u2014 parallel local training at the clients and global aggregation at the server \u2014 for multiple rounds of communication\nLocal: \u03b8\u0303(t)m = arg min\u03b8 Lm(\u03b8), initialized by \u03b8\u0304(t\u22121); Global: \u03b8\u0304(t) \u2190 M\u2211 m=1 |Dm| |D| \u03b8\u0303(t)m . (2)\nThe superscript t indicates the models after round t; \u03b8\u0304(0) denotes the initial model. That is, local training aims to minimize each client\u2019s empirical risk, often by several epochs of stochastic gradient descent (SGD). Global aggregation takes an element-wise average over local model parameters.\nProblem. When clients\u2019 data are non-IID, \u03b8\u0303(t)m would drift away from each other and from \u03b8\u0304(t\u22121), making \u03b8\u0304(t) deviate from the solution of Equation 1 and resulting in a drastic accuracy drop."
        },
        {
            "heading": "4 PRE-TRAINING IS APPLICABLE TO AND IMPORTANT FOR FL",
            "text": "In most of the FL literature that learns neural networks, \u03b8\u0304(0) is initialized with random weights. We thus aim to provide a detailed and systematic study on pre-training for FL. Specifically, we are interested in whether pre-training helps FEDAVG alleviate the accuracy drop in non-IID conditions.\n1A concurrent work by Nguyen et al. (2022) presents an empirical study as well. They focus more on how pre-training affects federated optimization algorithms. We study both the impact and applicability of pre-training for FL, provide further analyses and insights to understand them, and evaluate on large-scale datasets."
        },
        {
            "heading": "4.1 PRE-TRAINING SCENARIOS IN THE CONTEXT OF FL",
            "text": "However, to begin with, we must consider if pre-training is feasible for FL. Namely, can we obtain pre-trained weights in FL applications? We consider two scenarios. First, the server has pre-trained weights or data for pre-training; second, the server has none of them. For instance, in areas like computer vision, many pre-trained models and large-scale datasets are publicly accessible. However, for data-scarce or -costly domains, or privacy-critical domains like medicine, these resources are often not publicly accessible. For the second scenario, we explore the use of synthetic data for pre-training.\nRunning example. Throughout the paper, we use visual recognition as the running example to study both scenarios, even though in reality we usually have pre-trained models for it. The rationales are two-folded. First, it makes our study coherent, i.e., reporting the results on the same tasks. Second, it allows us to assess the gap between collecting real or creating synthetic data for pre-training.\nScenario one. We mainly use weights pre-trained on ImageNet (1K) (Russakovsky et al., 2015; Deng et al., 2009). For downstream FL tasks derived from ImageNet (e.g., Tiny-ImageNet (Le & Yang, 2015)), we use weights pre-trained on Places365 (Zhou et al., 2017) to prevent data leakage. For both cases, pre-training is done by standard supervised training. Scenario two. We use synthetic data for pre-training. We consider image generators like random generative models (Baradad et al., 2021) and fractals (Kataoka et al., 2020) that are not tied to specific tasks. These generators, while producing non-realistic images, have been shown effective for pre-training. We briefly introduce fractals, as they achieve the best performance in our study.\nA fractal image can be generated via an affine Iterative Function System (IFS) (Barnsley, 2014). An IFS generates a fractal by \u201cdrawing\u201d points iteratively on a canvas. The point transition is governed by a small set of 2\u00d7 2 affine transformations (each with a probability), denoted by S . Concretely, given the current point vn \u2208 R2, the IFS randomly samples one affine transformation with replacement from the set S, and uses it to transform vn into the next point vn+1. This process continues until a sufficient number of iterations is reached. The collection of points can then be used to draw a fractal image: by rendering each point as a binary or continuous value on a black canvas. Since S controls the generation process and hence the geometry of the fractal, it can essentially be seen as the fractal\u2019s ID, code, or class label; different codes would generate different fractals. Due to the randomness in the IFS, the same code S can create different but geometrically-consistent fractals. Using these properties of fractals, Kataoka et al. (2020) proposed to sample J different codes, create for each code a number of images, and pre-train a model in a supervised way. Anderson & Farrell (2022) proposed to create more complex images by painting I fractals on one canvas. The resulting image thus has I out of J classes, and can be used to pre-train a model via a multi-label loss.\nIn our study, we however found none of them effective for FL, perhaps due to their limitation on J . We thus propose a novel way for fractal pre-training, inspired by one critical insight \u2014 we can in theory sample infinitely many IFS codes and create fractals with highly diverse geometry. In the extreme, every image can be synthesized with an entirely different code.\nWe propose to pre-train with such fractal images using contrastive or similarity learning (Chen et al., 2020d; Chen & He, 2021). The idea is to treat every image as from a different class and learn to repulse different images away (i.e., negative pairs) or draw an image and its augmented version closer (i.e., positive pairs). We propose a unique way to create positive pairs, which is to use IFS to create a pair of images based on the same codes. We argue that this leads to stronger and physically-meaningful argumentation: not only do different fractals from the same codes capture intra-class variation, but we also can create a pair of images with different placements of the same fractals (more like\nobject-level augmentation). We name our approach fractal pair similarity (FPS) (see Figure 2)."
        },
        {
            "heading": "4.2 EXPERIMENTAL SETUP AND IMPLEMENTATION DETAILS",
            "text": "Data. We conduct the study using five visual recognition datasets: CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), TinyImageNet (Le & Yang, 2015), iNaturalist (Van Horn et al., 2018), and Cityscapes (Cordts et al., 2015). The first four are for image classification; the last is for semantic\nsegmentation. Table 1 summarizes their statistics. (We also include NLP tasks such as sentiment analysis (Caldas et al., 2018) and language modeling (McMahan et al., 2017) in the Suppl.) Non-IID splits. To simulate non-IID conditions across clients, we follow (Hsu et al., 2019) to partition the training set of CIFAR-10, CIFAR-100, and Tiny-ImageNet into M clients. To split the data of class c, we draw an M -dimensional vector qc from Dirichlet(\u03b1), and assign data of class c to client m proportionally to qc[m]. The resulting clients have different numbers of total images and class distributions. The small the \u03b1 is, the more severer the non-IID condition is.\nFor iNaturalist, a dataset for species recognition, we use the data proposed by (Hsu et al., 2020), which are split by Geo locations. There are two versions, GEO-10K (39 clients) and GEO-3K (136 clients). For Cityscapes, a dataset of street scenes, we use the official training/validation sets that contain 18/3 cities in Germany. We split the training data by cities to simulate a realistic scenario. Metrics. We report the averaged accuracy (%) for classification and the mIoU (%) for segmentation on the global test set. For brevity, we denote the test accuracy or mIoU of the model \u03b8 by Atest(\u03b8). Models. We use ResNet20 (He et al., 2016a) for CIFAR-10/100, which is suitable for a 32 \u00d7 32 resolution. We use ResNet18 for Tiny-ImageNet and iNaturalist. For semantic segmentation, we use DeepLabV3+ (Chen et al., 2018) with a MobileNet-V2 (Sandler et al., 2018) backbone. Initialization. We compare random and pre-trained weights. For scenario one (subsection 4.1), we obtain pre-trained weights for ResNet18 and MobileNet-V2 from PyTorch\u2019s and Places365\u2019s official sites. For ResNet20, we pre-train the weights using down-sampled, 32\u00d7 32 ImageNet images. For scenario two using synthetic images, we mainly consider fractals and apply our FPS approach, but will investigate other approaches in subsection 4.5. We apply the scheme proposed by Anderson & Farrell (2022) to sample IFS codes (each with 2 \u223c 4 transformations). For efficiency, we pre-sample a total of 100K IFS codes, and uniformly sample I codes from them to generate each fractal image. We then follow Anderson & Farrell (2022) to color, resize, and flip the fractals. We then apply a similarity learning approach SimSiam (Chen & He, 2021) for pre-training, due to its efficiency and effectiveness. We pre-train the weights for 100 epochs; each epoch has 1M image pairs (or equivalently 2M images). See Figure 2 for an illustration and see the Suppl. for details.\nMore specifically, for ResNet20, we set I = 2 and run the IFS for 1K iterations to render one 32\u00d7 32 image. For ResNet18 and DeepLabV3+, we sample I uniformly from {2, 3, 4, 5} for each image to increase diversity, and run the IFS for 100K iterations to render one 224\u00d7 224 image. Federated learning. We perform FEDAVG for 100 iterative rounds. Each round of local training takes 5 epochs2. We use the SGD optimizer with weight decay 1e\u22124 and a 0.9 momentum, except that on DeepLabV3+ we use an Adam (Kingma & Ba, 2015) optimizer. We apply the standard image pre-processing and data augmentation (He et al., 2016a). We reserve 2% data of the training set as the validation set for hyperparameter tuning (e.g., for the learning rate3). We follow the literature (He et al., 2016b) to decay the learning rate by 0.1 every 30 rounds. We leave more details and the selected hyperparameters in the Suppl."
        },
        {
            "heading": "4.3 EMPIRICAL STUDY ON CIFAR-10 AND TINY-IMAGENET",
            "text": "We first focus on CIFAR-10 and Tiny-ImageNet, on which we create artificial splits of clients\u2019 data to simulate various non-IID conditions. Besides applying FEDAVG to train a model \u03b8FL in an FL way, we also aggregate clients\u2019 data and train the corresponding model \u03b8CL in a centralized learning way. This enables us to assess their accuracy gap \u2206CL-FL = Atest(\u03b8CL)\u2212Atest(\u03b8FL).\n2We found this number quite stable for all experiments and freeze it throughout the paper. 3We use the same learning rate within each dataset, which is selected by FEDAVG without pre-training.\nTable 2: Test accuracy on CIFAR-10 (\u03b1 = 0.3).\nTable 3: Test accuracy on Tiny-ImageNet (\u03b1 = 0.3).\nPre-training improves FEDAVG and closes its gap to centralized learning. We first set \u03b1 = 0.3 (a mild non-IID degree) and M = 10 (i.e., 10 clients), and study the case that all clients participate in every round. Table 2 and Table 3 summarize the results; each row is for different initialization. As shown in the column \u201cFederated,\u201d pre-training consistently improves the accuracy of FEDAVG; the gain is highlighted by the magenta digits. This is not only by pre-training using large-scale real datasets like ImageNet and Places365, but also by pre-training using synthetic images.\nWe further investigate whether pre-training helps bridge the gap between FEDAVG and centralized learning. As shown in the column \u201cCentralized,\u201d pre-training also improves centralized learning. However, by looking at the column \u201c\u2206CL-FL,\u201d which corresponds to the difference between \u201cFederated\u201d and \u201cCentralized,\u201d we found that pre-training closes their gap. The gap on CIFAR-10 was 9.0 without pre-training, and reduces to 2.5 with weights pre-trained on ImageNet. In other words, pre-training seems to bring more benefits to FL than to centralized learning.\nPre-training brings larger gains to more challenging settings. We now consider different federated settings. This includes different numbers of clients M , different non-IID degrees \u03b1, and different percentages of participating clients per round. We also consider different numbers of local training epochs per round, but keep the total local epochs in FEDAVG as 500. For each configuration, we conduct three times of experiments and report the mean and standard deviation.\nFigure 3 summarizes the results, in which we change one variable at a time upon the default setting: M = 10, \u03b1 = 0.3, 100% participation, and 5 local epochs. Across all the settings, we see robust gains by pretraining, either using real or synthetic images. Importantly, when the setting becomes more challenging, e.g., smaller \u03b1 for larger non-IID degrees or larger M for fewer data per client, the gain gets larger. This shows the value of pre-training in addressing the challenge in FL.\nPre-training is compatible with other FL algorithms. We now consider advanced FL methods like FEDPROX (Li et al., 2020b), FEDDYN (Acar et al., 2021), and MOON (Li et al., 2021b). They were proposed to improve FEDAVG, and we want to investigate if pre-training could still improve them. Figure 4 shows the results, using the default federated setting. We found that both pre-training with real (right bars) and\nsynthetic (middle bars) images can consistently boost their accuracy. Interestingly, when pre-training with real data is considered, all these FL methods, including FEDAVG, perform quite similarly. This suggests that FEDAVG is still a strong FL approach if pre-trained models are available.\nPre-training makes network sizes scale better. One challenge in FL is the difficulty to train deeper networks (Chen & Chao, 2021). We investigate if pre-training mitigates this issue. On CIFAR-10 (\u03b1 = 0.3), we study network depths and widths based on ResNet20. As shown in Figure 5, with random initialization, deeper models have quite limited gains; wider models improve more. With pre-training (either FPS or real), both going deeper and wider have notable gains, suggesting that pre-training makes training larger models easier in FL."
        },
        {
            "heading": "4.4 EMPIRICAL STUDY ON LARGE-SCALE AND REAL DATA SPLITS",
            "text": "We now study pre-training for FL on large-scale datasets, iNaturalist 2017 (Van Horn & Perona, 2017) and Cityscapes (Cordts et al., 2016). Both datasets provide geo-location information, and can be split in a realistic way to simulate location-specific clients. (Please see subsection 4.2 for details.)\nTable 4 summarizes the results on iNaturalist 2017 (Van Horn & Perona, 2017), using the GEO-10K (39 clients and 50% clients per round) and GEO-3K (136 clients and 20% clients per round) splits proposed by (Hsu et al., 2020).\nTable 5 summarizes the semantic segmentation results (mIoU) on Cityscapes (Cordts et al., 2016). The training data are split into 18 clients by cities. We consider full client participation at every round. In both tables, we see clear gaps between centralized and federated learning (i.e., \u2206CL-FL) on realis-\ntic non-IID splits, and pre-training with either synthetic (i.e., FPS) or real data notably reduces the gaps. Specifically, compared to random initialization, FPS brings encouraging gains to FEDAVG (> 5% on iNaturalist; > 20% on Cityscapes). The gain is larger than that of centralized learning.\n4.5 ON PRE-TRAINING WITH SYNTHETIC DATA Table 6: Comparison on synthetic pretraining. Means of 3 runs are reported. Init. C10 C100 Tiny Random 74.4 51.4 42.4 Fractal + Multi-label 73.0 51.0 40.9 StyleGAN + SimSiam 79.2 53.0 44.6 Fractal + SimSiam 77.4 51.7 44.2 FPS (ours) + SimSiam 80.5 54.7 45.7 In previous subsections, we show that even without available pre-trained models or real data for pre-training, we can resort to pre-training with synthetic data and still achieve a notable gain for FL. Here, we provide more analyses and discussions.\nComparison of synthetic pre-training methods. For fractals, we compare the three methods in Figure 2. All of them use the same setup in subsection 4.2, except for multi-label learning we choose J =1K IFS codes as it gives the best validation accuracy. We also consider synthetic images generated by (the best version of) random StyleGAN (Baradad et al., 2021; Karras et al., 2019). We use these methods to initialize FEDAVG on CIFAR-10/100 (\u03b1 = 0.1, large non-IID) and Tiny-ImageNet (\u03b1 = 0.3), with M = 10 clients and full participation. Results are in Table 6. Our approach FPS outperforms all the baselines. By taking a deeper look, we found that pre-training with multi-label supervision does not outperform random initialization. We attribute this to both the limited diversity of fractals and the learning mechanism: we tried to increase J but cannot improve due to poor convergence. Self-supervised learning, on the contrary, can better learn from diverse fractals (i.e., each fractal a class). By taking the inner working of fractals into account to create geometrically-meaningful positive pairs, our FPS unleashes the power of synthetic pre-training.\nPrivacy-critical domain. We conduct a study on chest X-ray diagnosis (Kermany et al., 2018). Such a medical image domain is privacy-critical, and has a large discrepancy from ImageNet images. Under the same setup as in subsection 4.3 (\u03b1 = 0.3, M = 10, full participation), FEDAVG initialized with random/FPS/ImageNet achieves 68.9/74.8/69.1% test accuracy. Interestingly, synthetic pre-training outperforms ImageNet pre-training by 5.9%. This showcases when synthetic data can be practically useful in FL: when the problem domain is far from where accessible pre-trained models are trained.\n5 FEDERATED SELF-PRE-TRAINING CAN IMPROVE FL Table 7: Self-pretraining (SP) for FL. Init. SP C10 Tiny\nRandom 7 74.4 42.4 3 79.8 43.9\nFPS 7 79.9 45.7 3 82.2 46.0\nReal 7 87.9 50.3 3 88.0 50.6 Seeing the benefit of pre-training, we investigate another way to obtain a good representation to initialize FL tasks, which is to perform self-supervised learning directly on clients\u2019 decentralized data. Self-supervised learning (Liu et al., 2021) has been shown powerful to obtain good representations, which sometimes even outperform those obtained by supervised learning (Ericsson et al., 2021). Chen et al. (2020c) further showed that a two-stage self-pre-training (SP) procedure \u2014 self-supervised pre-training, followed by supervised learning, on the same data \u2014 can outperform pure supervised learning.\nWe explore such an idea in FL. Starting from the random weights (or even pre-trained weights), we first ignore the labels of clients\u2019 data and apply a state-of-the-art federated self-supervised learning algorithm (Lubana et al., 2022) (for 50 rounds), followed by standard FEDAVG. We use the same setup as in subsection 4.5 for CIFAR-10 (\u03b1 = 0.1) and Tiny-ImageNet (\u03b1 = 0.3), and summarize the results in Table 7. The SP procedure can significantly improve FEDAVG initialized with random weights, without external data. (We confirmed the gain is not merely from that in total we perform more rounds of FL. We extended FEDAVG without SP for 50 rounds but did not see an improvement.) Interestingly, the SP procedure can also improve upon pre-trained weights. These results further demonstrate the importance of initialization for FEDAVG. (See the Suppl. for more discussions.)"
        },
        {
            "heading": "6 AN ATTEMPT TO UNDERSTAND THE EFFECTS, AND CONCLUSION",
            "text": "We take a deeper look at pre-training (using real data) for FL and provide further analyses.\nPreparation and notation. We first identify factors that may affect FEDAVG\u2019s accuracy along its training process. Let us denote by Dtest = {(xi, yi)}Ntesti=1 the global test data, by Ltest(\u03b8) the test loss, and by Atest(\u03b8) the test accuracy. Following Equation 2, we decompose Atest(\u03b8\u0304(t)) after round t by\nAtest(\u03b8\u0304(t\u22121)) + M\u2211 m=1 |Dm| |D| Atest(\u03b8\u0303(t)m )\u2212Atest(\u03b8\u0304(t\u22121))\ufe38 \ufe37\ufe37 \ufe38\n\u2206 (t) L\n+ Atest(\u03b8\u0304(t))\u2212 M\u2211 m=1 |Dm| |D| Atest(\u03b8\u0303(t)m )\ufe38 \ufe37\ufe37 \ufe38\n\u2206 (t) G\n. (3)\nThe first term is the initial test accuracy of round t; the second (\u2206(t)L ) is the average gain by Local training; the third (\u2206(t)G ) is the gain by Global aggregation. A negative \u2206 (t) L indicates that local models after local training (at round t) have somewhat \u201cforgotten\u201d what the global model \u03b8\u0304(t\u22121) has learned (Kirkpatrick et al., 2017). Namely, the local models drift away from the global model.\nAnalysis on the training dynamics. We use the same setup as in subsection 4.3 on CIFAR-10 and Tiny-ImageNet: M = 10, \u03b1 = 0.3. Figure 6 summarizes the values defined in Equation 3. For each combination (dataset + pre-training or not), we show the test accuracy using the global model \u03b8\u0304(t) (\u00d7). We also show the averaged test accuracy using each local model \u03b8\u0303(t)m (\u2022). The red and green arrows indicate the gain by local training (\u2206(t)L ) and global aggregation (\u2206 (t) G ), respectively. For brevity, we only draw the first 50 rounds but the final accuracy is by 100 rounds.\nWe have the following observations. First, pre-training seems to not alleviate local model drifting (Li et al., 2020b; Karimireddy et al., 2020b). Both FEDAVG with and without pre-training have notable negative \u2206(t)L , which can be seen from the slanting line segments: segments with negative slopes (i.e., \u2022 of round t is lower than \u00d7 of round t\u2212 1) indicate drifting. Second, pre-training seems to have a larger global aggregation gain \u2206(t)G (vertical segments from \u2022 to \u00d7), especially in early rounds.\nAnalysis on global aggregation. We conduct a further analysis on global aggregation. In FEDAVG (Equation 2), global aggregation is by a simple weight average, using local data sizes as coefficients. According to (Karimireddy et al., 2020b), this simple average may gradually deviate from the solution of Equation 1 under non-IID conditions, and ultimately lead to a drastic accuracy drop. Here, we analyze if pre-training can alleviate this issue. As it is unlikely to find a unique minimum of Equation 1 to calculate the deviation, we propose an alternative way to quantify the robustness of aggregation.\nOur idea is inspired by Chen & Chao (2021), who showed that the coefficients used by FEDAVG may not optimally combine the local models. This motivates us to search for the optimal convex aggregation and calculate its accuracy gap against the simple average. The smaller the gap is, the more robust the global aggregation is. We define the optimal convex aggregation as follows:\n\u03b8\u0304?(t) = M\u2211 m=1 \u03bb?m\u03b8\u0303 (t) m , where {\u03bb?m} = arg max{\u03bbm\u22650;\u2211m \u03bbm=1}Atest( M\u2211 m=1 \u03bbm\u03b8\u0303 (t) m ). (4)\nThat is, we search for \u03bb = [\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbM ]> in the (M \u2212 1)-simplex that maximizes Atest. We apply SGD to find {\u03bb?m}. (See the Suppl.)\nFigure 7 shows the curves of Atest(\u03b8\u0304(t)) and Atest(\u03b8\u0304?(t)). For Atest(\u03b8\u0304?(t)), we replace the simple weight average by the optimal convex aggregation throughout the entire FEDAVG: at the beginning of each round, we send the optimal convex aggregation back to clients for their local training. It can be seen that Atest(\u03b8\u0304?(t)) outperforms Atest(\u03b8\u0304(t)). The gap is larger for FEDAVG without pre-training than with pretraining. Namely, for FEDAVG with pre-training,\nthe simple weight average has a much closer accuracy to the optimal convex aggregation. Analysis on the loss surface. To see why pre-training leads to robust aggregation, we investigate the variation of Ltest( \u2211M m=1 \u03bbm\u03b8\u0303 (t) m ) across different \u03bb on the simplex. We sample \u03bb for 300 times, construct the global models and calculate the test losses, and compute the 95% confidence interval. As shown in Figure 8, FEDAVG with pre-training has a smaller interval; i.e., a lower-variance loss surface in aggregation. This helps explain why it has a smaller gap between Atest(\u03b8\u0304(t)) and Atest(\u03b8\u0304?(t)).\nWe further visualize the loss landscapes (Li et al., 2018). We use the same random (or pre-trained) weights to initialize FEDAVG on CIFAR-10 (M = 10), for different \u03b1 and for an IID condition. We then gather the final global models and project them onto the loss landscape of L, i.e., the global empirical risk. We found that pre-training enables the global models under different data conditions (including the IID one) to converge to the same loss\nbasin (with a lower loss). In contrast, without pre-training, the global models converge to isolated, and often poor loss basins. This offers another explanation of why pre-training improves FL.\nConclusion. We conduct the very first systematic study on pre-training for federated learning (FL) to explore a rarely studied aspect: initialization. We show that pre-training largely bridges the gap between FL and centralized learning. We make an attempt to understand the underlying effects and reveal several new insights into FL, potentially opening up future research directions."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research is supported in part by grants from the National Science Foundation (IIS-2107077, OAC-2118240, and OAC-2112606), the OSU GI Development funds, and Cisco Systems, Inc. We are thankful for the generous support of the computational resources by the Ohio Supercomputer Center and AWS Cloud Credits for Research. We thank all the feedback from the review committee and have incorporated them."
        },
        {
            "heading": "SUPPLEMENTARY MATERIAL",
            "text": "We provide details omitted in the main paper.\n\u2022 Appendix A: additional details of fractals pre-training (cf. section 4 of the main paper). \u2022 Appendix B: details of experimental setups (cf. section 6 and section 4 of the main paper). \u2022 Appendix C: additional experimental results and analysis (cf. section 6 of the main paper). \u2022 Appendix D: additional discussions."
        },
        {
            "heading": "A ADDITIONAL DETAILS OF FRACTAL PRE-TRAINING",
            "text": ""
        },
        {
            "heading": "A.1 A COMPLETE INTRODUCTION OF OUR FRACTAL PAIR SIMILARITY (FPS)",
            "text": "In section 4 of the main paper, we only provide a condensed introduction of our FPS algorithm due to the page limits. Now, we give a complete version that elaborates more details."
        },
        {
            "heading": "A.1.1 BACKGROUND: SUPERVISED FRACTAL PRE-TRAINING",
            "text": "Fractal generation. A fractal image can be rendered via an affine Iterative Function System (IFS) Kataoka et al. (2020); Barnsley (2014). The IFS rendering process can be thought of as \u201cdrawing\u201d points iteratively on a canvas. The point transition is governed by a set of 2\u00d7 2 affine transformations, which we call an IFS code S:\nS = {( Ak \u2208 R2\u00d72, bk \u2208 R2, pk \u2208 R )}K k=1 . (5)\nHere, an (Ak, bk) pair specifies an affine transformation and pk is the corresponding probability, i.e., \u2211 k pk = 1 and pk \u2265 0,\u2200k \u2208 [K]. Concretely, given S, an IFS generates an image as follows. Starting from an initial point v0 \u2208 R2, it repeatedly samples one transformation k with replacement according to the probability pk, and performs vn+1 = Akvn + bk to arrive at the next point. This stochastic process continues until a sufficient number of iterations is reached. The traveled points {v0, \u00b7 \u00b7 \u00b7 ,vn, \u00b7 \u00b7 \u00b7 } are then used to synthesize a fractal: by rendering each point as a binary or continuous value on a black canvas. Due to the randomness in the IFS, one code can create different but geometrically-similar fractals. Supervised pre-training. While fractal images do not look real, they are diverse and capture the geometric properties of elements found in nature (Mandelbrot & Mandelbrot, 1982), thus suitable for pre-training and facilitating downstream tasks (Kataoka et al., 2020). According to Baradad et al. (2021), diversity and naturalism (i.e., capturing certain structural properties of real data) are two key properties that make for good synthetic data for training vision systems. Pre-training with fractal images thus allows the model to capture features, e.g., structured properties and the diversity of patterns, that can be useful for recognizing real data.\nSince the IFS code controls the generation process and hence the geometry of the fractal, it can essentially be seen as the fractal\u2019s ID or class label. In (Kataoka et al., 2020), the authors proposed to sample J different codes {Sj}Jj=1 and create for each code a number of images to construct a J-class\nclassification dataset. This synthetic labeled dataset is then used to pre-train a neural network via a multi-class loss (e.g., cross-entropy). The following-up work by Anderson & Farrell (2022) proposed to create more complex images by painting multiple (denoted by I) fractals on one canvas. The resulting image thus has I out of J labels, on which a multi-label loss is more suitable for supervised pre-training."
        },
        {
            "heading": "A.1.2 OUR APPROACH: FRACTAL PAIR SIMILARITY (FPS)",
            "text": "We propose a novel way for fractal pre-training, inspired by one critical insight \u2014 we can in theory sample infinitely many IFS codes and create fractals with highly diverse geometric properties. That is, instead of creating a J-class dataset that limits the diversity of codes (e.g., J =1K) but focuses more on intra-class variation, we propose to trade the latter for the former.\nWe propose to sample a new set of codes and create an image that contains multiple fractals on the fly. Namely, for each image, we sample a small set of I codes {Si}Ii=1, generate I corresponding fractals, and composite them into one image. The resulting dataset will have all its images from different classes (i.e., different sets of {Si}Ii=1) in theory. Analogy to self-supervised learning. How can we pre-train a neural network using a dataset whose images are all of the different class labels? Here, we propose to view the dataset as essentially \u201cunlabeled\u201d and draw an analogy to self-supervised learning Liu et al. (2021), especially those based on contrastive learning (Wu et al., 2018; He et al., 2020b; Chen et al., 2020d;b;c; Tian et al., 2020) or similarity learning Grill et al. (2020); Chen & He (2021). The core idea of these approaches is to treat every image as from a different class and learn to either repulse different images away (i.e., negative pairs) or draw an image and an augmented version of it closer (i.e., positive pairs). This line of approaches is so effective that they can even outperform supervised pre-training in many tasks Ericsson et al. (2021). We note that while Baradad et al. (2021) has applied self-supervised learning to fractals, the motivation is very different from ours: the authors directly used the supervised dataset created by Kataoka et al. (2020) but ignored the labels. Fractal Pair Similarity (FPS). Conventionally, to employ contrastive learning or similarity learning, one must perform data augmentation such that a single image becomes a positive pair. Common methods are image-level scale/color jittering, flips, crops, RandAugment Cubuk et al. (2020), SelfAugment Reed et al. (2021), etc. Here, we propose to exploit one unique way for fractals (or broadly, synthetic data), which is to use the IFS to create a pair of images based on the same set of codes {Si}Ii=1. We argue that this method can create stronger and more physically-meaningful argumentation: not only do different fractals from the same codes capture intra-class variation, but we also can create a pair of images with different placements of the same I fractals (more like object-level augmentation). Moreover, this method can easily be compatible with commonly used augmentation. Implementation. We detail the generation of a positive image pair in FPS as follows. First, we randomly sample I distinct IFS codes. Then, each IFS code is used to produce two fractal shapes applied with random coloring, resizing, and flipping. Finally, we obtain two sets of fractals; each set contains I distinct fractal shapes that can be pasted on a black canvas to generate one fractal image. The resulting two fractal images are further applied with image augmentations, such as random resized cropping, color jittering, flipping, etc. We provide more examples of image pairs generated by our FPS in Figure 10. In each pair, the two fractals generated from the same IFS code show intra-class variations, in terms of shapes and colors, and are placed at random locations.\nWe then apply self-supervised learning algorithms for pre-training. We use the IFS code sampling scheme proposed in Anderson & Farrell (2022) (with K \u2208 {2, 3, 4}) and similarly apply scale jittering on each IFS code before we use it to render fractals. For efficiency, we pre-sample a total of 100K IFS codes in advance and uniformly sample I codes from them to generate a pair of images.\nOur PyTorch code implementation is provided at https://github.com/andytu28/FPS_ Pre-training."
        },
        {
            "heading": "A.2 SELF-SUPERVISED LEARNING APPROACHES",
            "text": "After generating fractal image pairs with FPS, we pre-train models by applying two self-supervised learning approaches, SimSiam Chen & He (2021) (similarity-based) and MoCo-V2 Chen et al. (2020d) (contrastive-based). We briefly review them in the following.\nSimSiam. Given a positive image pair (x1, x2), we process them by an encoder network f to extract their features. A prediction MLP head h is then applied to transform the features of one image to match the features of the other. Let p1 = h(f(x1)) and z2 = f(x2). The objective (to learn f and h) is to minimize the negative cosine similarity between them:\nD(p1, z2) = \u2212 p1 ||p1||2 \u00b7 z2 ||z2||2 , (6)\nwhere || \u00b7 ||2 is the l2-norm. Since the relation between x1 and x2 is symmetric, the final loss can be written as follows:\nL = 1 2 D(p1, z2) + 1 2 D(p2, z1). (7)\nMoCo-V2. Similar to SimSiam Chen & He (2021), MoCo-V2 Chen et al. (2020d) also aims to maximize the similarity between features extracted from a positive image pair (x1,x2). The main difference is that MoCo-V2 adopts contrastive loss, which also takes negative pairs into account. Following the naming in MoCo-V2 Chen et al. (2020d), let xq = x1 be the query and xk+ = x2 be the positive key. We also have negative images/keys {xk1 ,xk2 , ...xkN} in the mini-batch. We define q = fq(x q), k+ = fk(xk+), and ki = fk(x k i ), where fq is the encoder for query images and fk is the encoder for keys. The objective function for MoCo-V2 (to learn fq and fk) is written as follows:\nL = \u2212 log exp(q \u00b7 k+/\u03c4)\u2211N i=0 exp(q \u00b7 ki/\u03c4) , (8)\nwhere \u03c4 is a temperature hyper-parameter. Besides the contrastive loss, MoCo-V2 maintains a dictionary to store and reuse features of keys from previous mini-batches, thereby making the negative keys not limited to the current mini-batch. Finally, to enforce stability during training, a momentum update is applied on the key encoder fk.\nIn subsection 4.5 and Table 6 of the main paper, we pre-train ResNet-18 and ResNet-20 for 100 epochs using SimSiam Chen & He (2021) and MoCo-V2 Chen et al. (2020d). Specifically, these neural network architectures are used for f in SimSiam and fq and fk in MoCo-V2. Each epoch consists of 1M image pairs, which are generated on-the-fly, for FPS. For a comparison to StyleGAN in Table 6, we use the pre-generated StyleGAN dataset provided in Baradad et al. (2021), which has 1.3M images. After pre-training, we keep fq (discard fk) from MoCo-V2, following Chen et al. (2020d), and keep f (discard h) from SimSiam, following Chen & He (2021).\nFor SimSiam, we use the SGD optimizer with learning rate 0.05, momentum 0.9, weight decay 1e\u22124, and batch size 256. For MoCo-V2, we use the SGD optimizer with learning rate 0.03, momentum 0.9, weight decay 1e\u22124, and batch size 256. The dictionary size is set to 65, 536. For both SimSiam and MoCo-V2, the learning rate decay follows the cosine schedule (Loshchilov et al., 2017).\nFor other experiments besides Table 6, we mainly use SimSiam for FPS. We use the same training setup as mentioned above (e.g., 100 epochs)."
        },
        {
            "heading": "B FL EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 DATASETS, FL SETTINGS, AND HYPERPARAMETERS",
            "text": "We train FEDAVG for 100 rounds, with 5 local epochs and weight decay 1e\u22124. Learning rates are decayed by 0.1 every 30 rounds. Besides that, we summarize the training hyperparameters for each\nof the federated experiments included in the main paper in Table 9. We always reserve 2% data of the training set as the validation set for hyperparameter searching for finalizing the setups.\nFor pre-processing, we generally follow the standard practice which normalizes the images and applies some augmentations. CIFAR-10/100 images are padded 2 pixels on each side, randomly flipped horizontally, and then randomly cropped back to 32\u00d7 32. For the other datasets with larger resolutions, we simply randomly cropped to the desired sizes and flipped horizontally following the official PyTorch ImageNet training script.\nFor the Cityscapes dataset, we use output stride 16. In training, the images are randomly cropped to 768\u00d7 768 and resized to 2048\u00d7 1024 in testing. To further understand the effects of hyperparameters, we provide more analysis on Tiny-ImageNet in subsection C.4."
        },
        {
            "heading": "B.2 OPTIMAL CONVEX AGGREGATION",
            "text": "We provide more details for learning the optimal convex experiment in section 6. To find the optimal combinations for averaging clients\u2019 weights, we optimize \u03bb using the SGD optimizer, with a learning rate 1e\u2212 5 for 20 epochs (batch size 32) on the global test set. The vector \u03bb is `1 normalized and each entry is constrained to be non-negative (which can be done with a softmax function in PyTorch) to ensure the combinations are convex. Since the optimization problem is not convex, we initialize \u03bb with several different vectors, including uniform initialization, and return the best (in terms of the test accuracy) as {\u03bb?m} for Equation 4."
        },
        {
            "heading": "B.3 FEDERATED SELF-PRE-TRAINING",
            "text": "In section 5, we consider self-pre-training by running a federated self-supervised learning algorithm on the same decentralized training data before starting supervised FEDAVG. We adopt the stateof-the-art algorithm Lubana et al. (2022), which is based on clustering. We follow their stateless configurations and run it with 5 local epochs each round, a 0.003 learning rate, and 32 batch size, for 50 rounds. We set 32/8 local/global clusters for CIFAR-10 and 256/32 for local/global clusters for Tiny-ImageNet.\nWe attribute the improvement by SP to several findings from different but related contexts. Gallardo et al. (2021); Yang & Xu (2020); Liu et al. (2021) showed that self-supervised learning is more robust in learning representations from class-imbalanced and distribution-discrepant data; Reed et al. (2022) showed that the SP procedure helps adapt the representations to downstream tasks. With that being said, federated self-supervised learning is itself an open problem and deserves more attention. (In our preliminary trials, the algorithm by Lubana et al. (2022) can hardly scale up to iNaturalist and Cityscapes. The validation accuracy by k-nearest neighbors hardly improves.)"
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTS AND ANALYSES",
            "text": ""
        },
        {
            "heading": "C.1 SCOPE AND MORE EXPERIMENTS IN NLP",
            "text": "In this paper, we focus on computer vision (CV). We respectfully think this should not degrade our contributions. First, CV is a big area. Second, while many federated learning (FL) works focus on CV, most of them merely studied classification using simple datasets like CIFAR. We go beyond them by experimenting with iNaturalist and Cityscapes (for segmentation), which are realistic and remain challenging even in centralized learning (CL). We believe our focused study in CV and\nTable 10: Sent140 accuracy (100 rounds; local epoch= 5, 10% clients per round).\nInit. / Setting FL CL \u2206CL-FL Random 75.4 80.5 5.1 Pre-trained 83.4 (+8.0) 85.0 (+4.5) 1.6\nTable 11: Shakespear next-character prediction accuracy (30 rounds; local epoch= 10, 10% clients per round, 606 clients in total).\nencouraging results on these two datasets using either real or synthetic pre-training are valuable to the FL community\nThat being said, here we provide two experiments on natural language processing (NLP) to verify that our observations are consistent with that on CV tasks. First, we conduct a sentiment analysis experiment on a large-scale federated Sent140 dataset (Caldas et al., 2019), which has 660K clients and 1.6M samples. We use a pre-trained DistilBERT (Sanh et al., 2019). Second, we experiment with another popular FL NLP dataset Shakespeare next-character prediction task proposed in McMahan et al. (2017). We use a version provided by (Caldas et al., 2019) which contains 606 clients, and use the Penn Treebank dataset (Marcinkiewicz et al., 1994) for pre-training an LSTM of two layers of 256 units each.\nTable 10 and Table 11 show the results for the Sent140 and Shakespeare datasets, respectively. We also see a similar trend in Figure 11 \u2014 pre-training helps more in FL than in CL and largely closes their gap, even if the local model drifts are not alleviated."
        },
        {
            "heading": "C.2 MORE ANALYSIS ON GLOBAL AGGREGATION",
            "text": "In section 6, we discuss how pre-training leads to more robust aggregation where the loss variance is smaller when we sample the convex combinations of local models. In Figure 8, we provide CIFAR-10 and Tiny-ImageNet due to the space limit. In Figure 12 and Figure 13, we also include the\niNaturalist-GEO-3K and Cityscapes. We have a consistent finding \u2014 using pre-training does not reduce local model drift but does lead to a lower-variance loss surface."
        },
        {
            "heading": "C.3 MORE COMPARISONS ON SYNTHETIC PRE-TRAINING",
            "text": "In subsection 4.5, we compare synthetic pre-training methods. We also consider the images generated by (the best version of) a randomly-initialized StyleGAN Baradad et al. (2021); Karras et al. (2019). Due to the space limit, we only list some of the comparisons. Here we provide the complete results in Table 12. Across 3 datasets and 2 self-supervised algorithms, our FPS consistently outperforms the baselines. Since overall our FPS + SimSiam provides the strongest performance, we focus on it for the main paper. Here we also include the FPS + MoCo-V2 algorithm and observe a similar conclusion."
        },
        {
            "heading": "C.4 DISCUSSIONS ON DIFFERENT FEDERATED SETTINGS",
            "text": "To understand how the FL settings affect the observations of pre-training, we further conduct studies on CIFAR-10 and Tiny-ImageNet in Figure 3 of the main text. We focus on the setting in the main text (i.e., 10 clients, Dir(0.3), 100% participation, local epoch= 5) but change one variable of settings such as learning rate scheduling (in Figure 14 and Figure 15), the number of clients, non-IID degree, participation rate, and the number of local epochs.\nFor different learning rate scheduling, we observe similar trends: pre-training does not alleviate client shift much but the test accuracy of the global model after aggregating the local models is higher.\nIn the main paper, we focus on the schedule that decays the learning rate by 0.1 every 30 rounds given its better performance compared to the two (decay by 0.95 every round and no decays) here. In Figure 14 and Figure 15, we also monitor the variance of the test loss of the global models that use different coefficients (100 random samples) to aggregate the local models, i.e., the shaded area in Figure 8. We found the pre-trained ones have lower loss variance quite consistent across different learning rate scheduling and our analyses in section 6.\nFor client numbers, more clients will make the performance drop because each client has fewer data and the resulting local training is prone to over-fitting. However, using pre-training on both real data and our FPS still improves significantly.\nFor the non-IID degree, we manipulate it with the Dirichlet distributions by the \u03b1 parameter. We observe that more non-IID (smaller \u03b1) settings degrade the performance of using random initialization sharply while using pre-training is more robust (smaller accuracy drop).\nFor participation rates and the number of local epochs, we observe that they are not very sensitive variables, as long as they are large enough (e.g., participation rate > 30% and #local epochs > 5). Interestingly, using either fewer or more local epochs does not close the gap between using pre-training and random initialization."
        },
        {
            "heading": "C.5 ANALYSIS WITH FPS",
            "text": "For brevity, in section 6 of the main paper we mainly focus on using random initialization and using real data for pre-training. Here we also provide the analyses on global aggregation using our FPS, following section 6. We observe consistent effects on FL with FPS: it does not alleviate local model drift (Figure 16) but makes global aggregation more stable evidenced by smaller gains with optimal convex combinations (Figure 17) and lower loss variance (Figure 18).\n\u03b8\u0303 (t) m (\u2022). For brevity, we only draw the first 50 rounds but the final accuracy/mIoU are after 100 rounds. CIFAR-10 and Tiny-ImageNet (10 clients, Dir(0.3), 100% participation, local epoch= 5). The experiments follow section 6."
        },
        {
            "heading": "D ADDITIONAL DISCUSSIONS",
            "text": ""
        },
        {
            "heading": "D.1 POTENTIAL NEGATIVE SOCIETAL IMPACTS",
            "text": "Our work discusses how using pre-training on a large dataset can improve FL. While not specific to our discussions, collecting the real dataset could potentially inject data bias or undermine data privacy if the collection process is not carefully designed. However, we believe the concerns are mainly from data collection but not from the learning algorithms.\nTo remedy this, in section 5, we propose an alternative of using synthetic data that does not require any real data and can still improve FL significantly."
        },
        {
            "heading": "D.2 COMPUTATION RESOURCES",
            "text": "We implement all the codes in PyTorch and train the models with GPUs. For experiments with 32\u00d7 32 images, we trained on a 2080 Ti GPU. Pre-training takes about 1 day and FL takes about 8 hours. For experiments with 224 \u00d7 224 images, we trained on an A6000 GPU. Pre-training takes about 2\u2212 3 days. and FL takes about 1 day. For the Cityscape dataset, we trained with an A6000 GPU for about 2 days."
        },
        {
            "heading": "D.3 MORE DISCUSSION ON INITIALIZATION AND PRE-TRAINED MODELS IN FL",
            "text": "In subsection 4.1, we consider pre-training a model in the server and use it as the initialization for further federated training.\nAnother very different and complementary problem on model initialization compared to ours is about selecting part of the parameters of the global model to initialize each round of local training at each client. Several works Li et al. (2021a); Zhu & Sun (2021) proposed to find a sparse mask for each client to improve communication and computation efficiency, which is an orthogonal problem to ours.\nA recent work (Tan et al., 2022) instead uses several frozen pre-trained models in FL and only learns the class prototypes for classification."
        },
        {
            "heading": "D.4 SYNTHETIC DATA FOR FL",
            "text": "Pre-training on synthetic data is a relatively new and fast-growing area in centralized learning. In our paper, we focus on generating synthetic data for images and using them for pre-training for initializing FL. We have shown the superiority of our proposed FPS against other synthetic data generation methods for pre-training, including one based on random StyleGANs Baradad et al. (2021); Karras et al. (2019). Besides vision tasks, we found many recent works showing the benefits of pre-training on carefully-designed synthetic data for downstream tasks on various modalities such as text (Wu et al., 2022), SQL tables (Jiang et al., 2022), speech (Fazel et al., 2021), etc. We believe it is promising to bring these methods into the federated setting and our study can be a valuable reference.\nAnother work FedSyn (Behera et al., 2022) considers training a generator (e.g., GAN) in a federated setting using clients\u2019 data, which is fairly different from our problem of generating synthetic data (without looking at clients\u2019 data) at the server for pre-training the global model."
        }
    ],
    "year": 2023
}