{
    "abstractText": "This paper proposes a unified framework for zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, anaphoricity determination and antecedent identification. In particular, all the three sub-tasks are addressed using tree kernel-based methods with appropriate syntactic parse tree structures. Experimental results on a Chinese zero anaphora corpus show that the proposed tree kernel-based methods significantly outperform the feature-based ones. This indicates the critical role of the structural information in zero anaphora resolution and the necessity of tree kernel-based methods in modeling such structural information. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fang Kong"
        },
        {
            "affiliations": [],
            "name": "Guodong Zhou"
        }
    ],
    "id": "SP:9116f9f89007fc3c50454b9aa0645af8b38e3bb9",
    "references": [
        {
            "authors": [
                "S. Converse."
            ],
            "title": "Pronominal Anaphora Resolution in Chinese",
            "venue": "Ph.D. Thesis, Department of Computer and Information Science. University of Pennsylvania.",
            "year": 2006
        },
        {
            "authors": [
                "M. Collins",
                "N. Duffy."
            ],
            "title": "Convolution kernels for natural language",
            "venue": "NIPS\u20192001:625-632.",
            "year": 2001
        },
        {
            "authors": [
                "A. Ferrandez",
                "J. Peral."
            ],
            "title": "A computational approach to zero-pronouns in Spanish",
            "venue": "ACL'2000:166172.",
            "year": 2000
        },
        {
            "authors": [
                "R. Iida",
                "K. Inui",
                "Y. Matsumoto."
            ],
            "title": "Exploiting syntactic patterns as clues in zero-anaphora resolution",
            "venue": "COLING-ACL'2006:625-632",
            "year": 2006
        },
        {
            "authors": [
                "H. Isozaki",
                "T. Hirao."
            ],
            "title": "Japanese zero pronoun resolution based on ranking rules and machine learning",
            "venue": "EMNLP'2003:184-191",
            "year": 2003
        },
        {
            "authors": [
                "F. Kong",
                "G.D. Zhou",
                "Q.M. Zhu"
            ],
            "title": "Employing the Centering Theory in Pronoun Resolution from the Semantic Perspective",
            "year": 2009
        },
        {
            "authors": [
                "C.N. Li",
                "S.A. Thompson."
            ],
            "title": "Third-person pronouns and zero-anaphora in Chinese discourse",
            "venue": "Syntax and Semantics, 12:311-335.",
            "year": 1979
        },
        {
            "authors": [
                "W. Li."
            ],
            "title": "Topic chains in Chinese discourse",
            "venue": "Discourse Processes, 37(1):25-45.",
            "year": 2004
        },
        {
            "authors": [
                "A. Moschitti"
            ],
            "title": "A Study on Convolution Kernels for Shallow Semantic Parsing, ACL\u20192004",
            "year": 2004
        },
        {
            "authors": [
                "L.H. Qian",
                "G.D. Zhou",
                "F. Kong",
                "Q.M. Zhu",
                "P.D. Qian."
            ],
            "title": "Exploiting constituent dependencies for tree kernel-based semantic relation extraction",
            "venue": "COLING\u20192008:697-704",
            "year": 2008
        },
        {
            "authors": [
                "K. Seki",
                "A. Fujii",
                "T. Ishikawa."
            ],
            "title": "A probabilistic method for analyzing Japanese anaphora intergrating zero pronoun detection and resolution",
            "venue": "COLING'2002:911-917",
            "year": 2002
        },
        {
            "authors": [
                "R. Sasano. D. Kawahara",
                "S. Kurohashi."
            ],
            "title": "A fully-lexicalized probabilistic model for Japanese zero anaphora resolution",
            "venue": "COLING'2008:769-776",
            "year": 2008
        },
        {
            "authors": [
                "W.M. Soon",
                "H.T. Ng",
                "D. Lim."
            ],
            "title": "A machine learning approach to coreference resolution of noun phrase",
            "venue": "Computational Linguistics, 2001, 27(4):521544.",
            "year": 2001
        },
        {
            "authors": [
                "V. Ng",
                "C. Cardie"
            ],
            "title": "Improving machine learning approaches to coreference resolution",
            "venue": "ACL\u20192002: 104-111",
            "year": 2002
        },
        {
            "authors": [
                "X.F. Yang",
                "G.D. Zhou",
                "J. Su",
                "C.L. Chew."
            ],
            "title": "Coreference Resolution Using Competition Learning Approach",
            "venue": "ACL\u20192003:177-184",
            "year": 2003
        },
        {
            "authors": [
                "X.F. Yang",
                "J. Su",
                "C.L. Tan"
            ],
            "title": "A Twin-Candidate Model for Learning-Based Anaphora Resolution",
            "venue": "Computational Linguistics 34(3):327-356",
            "year": 2008
        },
        {
            "authors": [
                "N. Xue",
                "F. Xia",
                "F.D. Chiou",
                "M. Palmer."
            ],
            "title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus",
            "venue": "Natural Language Engineering, 11(2):207-238.",
            "year": 2005
        },
        {
            "authors": [
                "X.F. Yang",
                "J. Su",
                "C.L. Tan."
            ],
            "title": "Kernel-based pronoun resolution with structured syntactic knowledge",
            "venue": "COLING-ACL'2006:41-48.",
            "year": 2006
        },
        {
            "authors": [
                "D. Zelenko",
                "A. Chinatsu",
                "R. Anthony."
            ],
            "title": "Kernel methods for relation extraction",
            "venue": "Journal of Machine Learning Research, 3(2003):1083-1106",
            "year": 2003
        },
        {
            "authors": [
                "S. Zhao",
                "H.T. Ng."
            ],
            "title": "Identification and Resolution of Chinese Zero Pronouns: A Machine Learning Approach",
            "venue": "EMNLP-CoNLL'2007:541-550.",
            "year": 2007
        },
        {
            "authors": [
                "S. Zhao",
                "R. Grishman."
            ],
            "title": "Extracting relations with integrated information using kernel methods",
            "venue": "ACL\u20192005:419-426",
            "year": 2005
        },
        {
            "authors": [
                "G.D. Zhou",
                "F. Kong",
                "Q.M. Zhu."
            ],
            "title": "Contextsensitive convolution tree kernel for pronoun resolution",
            "venue": "IJCNLP'2008:25-31",
            "year": 2008
        },
        {
            "authors": [
                "G.D. Zhou",
                "M. Zhang",
                "D.H. Ji",
                "Q.M. Zhu."
            ],
            "title": "Tree kernel-based relation extraction with contextsensitive structured parse tree information",
            "venue": "EMNLPCoNLL\u20192007:728-736 891",
            "year": 2007
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882\u2013891, MIT, Massachusetts, USA, 9-11 October 2010. c\u00a92010 Association for Computational Linguistics\n* Corresponding author"
        },
        {
            "heading": "1 Introduction",
            "text": "As one of the most important techniques in discourse analysis, anaphora resolution has been a focus of research in Natural Language Processing (NLP) for decades and achieved much success in English recently (e.g. Soon et al. 2001; Ng and Cardie 2002; Yang et al. 2003, 2008; Kong et al. 2009).\nHowever, there is little work on anaphora resolution in Chinese. A major reason for this phe-\nnomenon is that Chinese, unlike English, is a prodrop language, whereas in English, definite noun phrases (e.g. the company) and overt pronouns (e.g. he) are frequently employed as referring expressions, which refer to preceding entities. Kim (2000) compared the use of overt subjects in English and Chinese. He found that overt subjects occupy over 96% in English, while this percentage drops to only 64% in Chinese. This indicates the prevalence of zero anaphors in Chinese and the necessity of zero anaphora resolution in Chinese anaphora resolution. Since zero anaphors give little hints (e.g. number or gender) about their possible antecedents, zero anaphora resolution is much more challenging than traditional anaphora resolution.\nAlthough Chinese zero anaphora has been widely studied in the linguistics research (Li and Thompson 1979; Li 2004), only a small body of prior work in computational linguistics deals with Chinese zero anaphora resolution (Converse 2006; Zhao and Ng 2007). Moreover, zero anaphor detection, as a critical component for real applications of zero anaphora resolution, has been largely ignored.\nThis paper proposes a unified framework for Chinese zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, which detects zero anaphors from a text, anaphoricity determination, which determines whether a zero anaphor is anaphoric or not, and antecedent identification, which finds the antecedent for an anaphoric zero anaphor. To our best knowledge, this is the first systematic work dealing with all the three sub-tasks via a unified framework. Moreover, we release a Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the\n882\nmanually-parsed sentences in the Chinese Treebank (CTB) 6.0. This is done by assigning anaphoric/non-anaphoric zero anaphora labels to the null constituents in a parse tree. Finally, this paper illustrates the critical role of the structural information in zero anaphora resolution and the necessity of tree kernel-based methods in modeling such structural information.\nThe rest of this paper is organized as follows. Section 2 briefly describes the related work on both zero anaphora resolution and tree kernelbased anaphora resolution. Section 3 introduces the overwhelming problem of zero anaphora in Chinese and our developed Chinese zero anaphora corpus, which is available for research purpose. Section 4 presents our tree kernel-based unified framework in zero anaphora resolution. Section 5 reports the experimental results. Finally, we conclude our work in Section 6."
        },
        {
            "heading": "2 Related Work",
            "text": "This section briefly overviews the related work on both zero anaphora resolution and tree kernelbased anaphora resolution."
        },
        {
            "heading": "2.1 Zero anaphora resolution",
            "text": "Although zero anaphors are prevalent in many languages, such as Chinese, Japanese and Spanish, there only have a few works on zero anaphora resolution.\nZero anaphora resolution in Chinese Converse (2006) developed a Chinese zero anaphora corpus which only deals with zero anaphora category \u201c-NONE- *pro*\u201d for dropped subjects/objects and ignores other categories, such as \u201c-NONE- *PRO*\u201d for non-overt subjects in nonfinite clauses. Besides, Converse (2006) proposed a rule-based method to resolve the anaphoric zero anaphors only. The method did not consider zero anaphor detection and anaphoric identification, and performed zero anaphora resolution using the Hobbs algorithm (Hobbs, 1978), assuming the availability of golden anaphoric zero anaphors and golden parse trees.\nInstead, Zhao and Ng (2007) proposed featurebased methods to zero anaphora resolution on the same corpus from Convese (2006). However, they only considered zero anaphors with explicit noun phrase referents and discarded those with split an-\ntecedents or referring to events. Moreover, they focused on the sub-tasks of anaphoricity determination and antecedent identification. For zero anaphor detection, a simple heuristic rule was employed. Although this rule can recover almost all the zero anaphors, it suffers from very low precision by introducing too many false zero anaphors and thus leads to low performance in anaphoricity determination, much due to the imbalance between positive and negative training examples.\nZero anaphora resolution in Japanese Seki et al. (2002) proposed a probabilistic model for the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary. They did not perform zero anaphor detection, assuming the availability of golden zero anaphors. Besides, their model needed a largescale corpus to estimate the probabilities to prevent them from the data sparseness problem.\nIsozaki and Hirao (2003) explored some ranking rules and a machine learning method on zero anaphora resolution. However, they assumed that zero anaphors were already detected and each zero anaphor\u2019s grammatical case was already determined by a zero anaphor detector.\nIida et al. (2006) explored a machine learning method for the sub-task of antecedent identification using rich syntactic pattern features, assuming the availability of golden anaphoric zero anaphors.\nSasano et al. (2008) proposed a fully-lexicalized probabilistic model for zero anaphora resolution, which estimated case assignments for the overt case components and the antecedents of zero anaphors simultaneously. However, this model needed case frames to detect zero anaphors and a largescale corpus to construct these case frames automatically.\nFor Japanese zero anaphora, we do not see any reports about zero anaphora categories. Moreover, all the above related works we can find on Japanese zero anaphora resolution ignore zero anaphor detection, focusing on either anaphoricity determination or antecedent identification. Maybe, it is easy to detect zero anaphors in Japanese. However, it is out of the scope of our knowledge and this paper.\nZero anaphora resolution in Spanish As the only work we can find, Ferrandez and Peral (2000) proposed a hand-engineered rule-based method for both anaphoricity determination and\nantecedent identification. That is, they ignored zero anaphor detection. Besides, they only dealt with zero anaphors that were in the subject position."
        },
        {
            "heading": "2.2 Tree kernel-based anaphora resolution",
            "text": "Although there is no research on tree kernel-based zero anaphora resolution in the literature, tree kernel-based methods have been explored in traditional anaphora resolution to certain extent and achieved comparable performance with the dominated feature-based ones. One main advantage of kernel-based methods is that they are very effective at reducing the burden of feature engineering for structured objects. Indeed, the kernel-based methods have been successfully applied to mine structural information in various NLP techniques and applications, such as syntactic parsing (Collins and Duffy 2001; Moschitti 2004), semantic relation extraction (Zelenko et al. 2003; Zhao and Grishman 2005; Zhou et al. 2007; Qian et al. 2008), and semantic role labeling (Moschitti 2004).\nRepresentative works in tree kernel-based anaphora resolution include Yang et al. (2006) and Zhou et al (2008). Yang et al. (2006) employed a convolution tree kernel on anaphora resolution of pronouns. In particular, a document-level syntactic parse tree for an entire text was constructed by attaching the parse trees of all its sentences to a newadded upper node. Examination of three parse tree structures using different construction schemes (Min-Expansion, Simple-Expansion and FullExpansion) on the ACE 2003 corpus showed promising results. However, among the three constructed parse tree structures, there exists no obvious overwhelming one, which can well cover structured syntactic information. One problem with this tree kernel-based method is that all the constructed parse tree structures are context-free and do not consider the information outside the subtrees. To overcome this problem, Zhou et al. (2008) proposed a dynamic-expansion scheme to automatically construct a proper parse tree structure for anaphora resolution of pronouns by taking predicate- and antecedent competitor-related information into consideration. Besides, they proposed a context-sensitive convolution tree kernel to compute the similarity between the parse tree structures. Evaluation on the ACE 2003 corpus showed that the dynamic-expansion scheme can well cover\nnecessary structural information in the parse tree for anaphora resolution of pronouns and the context-sensitive convolution tree kernel much outperformed other tree kernels."
        },
        {
            "heading": "3 Task Definition",
            "text": "This section introduces the phenomenon of zero anaphora in Chinese and our developed Chinese zero anaphora corpus."
        },
        {
            "heading": "3.1 Zero anaphora in Chinese",
            "text": "A zero anaphor is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap. Figure 1 illustrates an example sentence from Chinese TreeBank (CTB) 6.0 (File ID=001, Sentence ID=8). In this example, there are four zero anaphors denoted as \u0424i (i=1, 2, \u20264). Generally, zero anaphors can be understood from the context and do not need to be specified.\nA zero anaphor can be classified into either anaphoric or non-anaphoric, depending on whether it has an antecedent in the discourse. Typically, a zero anaphor is non-anaphoric when it refers to an extra linguistic entity (e.g. the first or second person in a conversion) or its referent is unspecified in the context. Among the four anaphors in Figure 1, zero anaphors \u0424 1 and \u0424 4 are non-anaphoric while zero anaphors \u04242 and \u04243 are anaphoric, referring to noun phrase \u201c\u5efa\u7b51\u884c\u4e3a/building action\u201d and noun phrase \u201c\u65b0\u533a\u7ba1\u59d4\u4f1a/new district managing committee\u201d respectively.\nChinese zero anaphora resolution is very difficult due to following reasons: 1) Zero anaphors give little hints (e.g. number or gender) about their possible antecedents. This makes antecedent identification much more difficult than traditional anaphora resolution. 2) A zero anaphor can be either anaphoric or non-anaphoric. In our corpus described in Section 3.2, about 60% of zero anaphors are non-anaphoric. This indicates the importance of anaphoricity determination. 3) Zero anaphors are not explicitly marked in a text. This indicates the necessity of zero anaphor detection, which has been largely ignored in previous research and has proved to be difficult in our later experiments.\n(the example is : \u4e3a\u89c4\u8303\u5efa\u7b51\u884c\u4e3a\uff0c\u9632\u6b62\u51fa\u73b0\u65e0\u5e8f\u73b0\u8c61\uff0c\u65b0\u533a\u7ba1\u59d4\u4f1a\u6839\u636e\u56fd\u5bb6\u548c\u4e0a\u6d77\u5e02\u7684\u6709\u5173\u89c4\u5b9a\uff0c\u7ed3\u5408\u6d66 \u4e1c\u5f00\u53d1\u5b9e\u9645\uff0c\u53ca\u65f6\u51fa\u53f0\u4e86\u4e00\u7cfb\u5217\u89c4\u8303\u5efa\u8bbe\u5e02\u573a\u7684\u6587\u4ef6/ In order to standardize the building action and prevent the inorder phenomenon, the standing committee of new zone annouced a series of files to standardize building market based on the related provisions of China and Shanghai in time, and the realities of the development of Pudong are\nconsidered. )"
        },
        {
            "heading": "3.2 Zero anaphora corpus in Chinese",
            "text": "Due to lack of an available zero anaphora corpus for research purpose, we develop a Chinese zero anaphora corpus of 100 documents from CTB 6.0, which adds a layer of annotation to the manuallyparsed sentences. Hoping the public availability of this corpus can push the research of zero anaphora resolution in Chinese and other languages.\nFigure 2: An example sentence annotated in CTB 6.0\nID Cate-gory Description AZ As ZAs\n1 -NONE- *T* Used in topicalization and object preposing constructions 6 742 2 -NONE- * Used in raising and passive constructions 1 2\n3 -NONE- *PRO* Used in control structures. The *PRO* cannot be substituted by an overt constituent. 219 399 4 -NONE- *pro* for dropped subject or object. 394 449 5 -NONE- *RNR* Used for right node raising (Cataphora) 0 36 6 Others Other unknown empty categories 92 92\nTotal (100 documents, 35089 words) 712 1720 Table 1: Statistics on different categories of zero anaphora (AZA and ZA indicates anaphoric zero anaphor and zero anaphor respectively)\nFigure 2 illustrates an example sentence annotated in CTB 6.0, where the special tag \u201c-NONE-\u201d represents a null constituent and thus the occurrence of a zero anaphor. In our developed corpus, we need to annotate anaphoric zero anaphors using those null constituents with the special tag of \u201c- NONE-\u201d.\nTable 1 gives the statistics on all the six categories of zero anaphora. Since we do not consider zero cataphora in the current version, we simply redeem them non-anaphoric. It shows that among 1720 zero anaphors, only 712 (about 40%) are anaphoric. This suggests the importance of anaphoricity determination in zero anaphora resolution.\nTable 3 further shows that, among 712 anaphoric zero anaphors, 598 (84%) are intra-sentential and no anaphoric zero anaphors have their antecedents occurring two sentences before.\nSentence distance AZAs 0 598 1 114\n>=2 0\nTable 3 Distribution of anaphoric zero anaphors over sentence distances\nFigure 3 shows an example in our corpus corresponding to Figure 2. For a non-anaphoric zero anaphor, we replace the null constituent with \u201cE-i NZA\u201d, where i indicates the category of zero anaphora, with \u201c1\u201d referring to \u201c-NONE *T*\u201d etc. For an anaphoric zero anaphor, we replace it with \u201cE-x-y-z-i AZA\u201d, where x indicates the sentence id of its antecedent, y indicates the position of the first word of its antecedent in the sentence, z indicates the position of the last word of its antecedent in the sentence, and i indicates the category id of the null constituent.\nFigure 3: an example sentence annotated in our corpus"
        },
        {
            "heading": "4 Tree Kernel-based Framework",
            "text": "This section presents the tree kernel-based unified framework for all the three sub-tasks in zero anaphora resolution. For each sub-task, different parse tree structures are constructed. In particular, the context-sensitive convolution tree kernel, as proposed in Zhou et al. (2008), is employed to compute the similarity between two parse trees via the SVM toolkit SVMLight.\nIn the tree kernel-based framework, we perform the three sub-tasks, zero anaphor detection, anaphoricity determination and antecedent identification in a pipeline manner. That is, given a zero anaphor candidate Z, the zero anaphor detector is first called to determine whether Z is a zero anaphor or not. If yes, the anaphoricity determiner is then invoked to determine whether Z is an anaphoric zero anaphor. If yes, the antecedent identifier is finally awaked to determine its antecedent. In the future work, we will explore better ways of integrating the three sub-tasks (e.g. joint learning)."
        },
        {
            "heading": "4.1 Zero anaphor detection",
            "text": "At the first glance, it seems that a zero anaphor can occur between any two constituents in a parse tree. Fortunately, an exploration of our corpus shows that a zero anaphor always occurs just before a predicate1 phrase node (e.g. VP). This phenomenon has also been employed in Zhao and Ng (2007) in generating zero anaphor candidates. In particular, if the predicate phrase node occurs in a coordinate structure or is modified by an adverbial node, we only need to consider its parent. As shown in Figure 1, zero anaphors may occur immediately to the left of\u89c4\u8303/guide, \u9632\u6b62/avoid, \u51fa\u73b0/appear, \u6839\u636e /according to, \u7ed3\u5408 /combine, \u51fa\u53f0 /promulgate, which cover the four true zero anaphors. Therefore, it is simple but reliable in applying above heuristic rules to generate zero anaphor candidates.\nGiven a zero anaphor candidate, it is critical to construct a proper parse tree structure for tree kernel-based zero anaphor detection. The intuition behind our parser tree structure for zero anaphor detection is to keep the competitive information\n1 The predicate in Chinese can be categorized into verb predicate, noun predicate and preposition predicate. In our corpus, about 93% of the zero anaphors are driven by verb predicates. In this paper, we only explore zero anaphors driven by verb predicates.\nabout the predicate phrase node and the zero anaphor candidate as much as possible. In particular, the parse tree structure is constructed by first keeping the path from the root node to the predicate phrase node and then attaching all the immediate verbal phrase nodes and nominal phrase nodes. Besides, for the sub-tree rooted by the predicate phrase node, we only keep those paths ended with verbal leaf nodes and the immediate verbal and nominal nodes attached to these paths. Figure 4 shows an example of the parse tree structure corresponding to Figure 1 with the zero anaphor candidate \u03a62 in consideration.\nDuring training, if a zero anaphor candidate has a counterpart in the same position in the golden standard corpus (either anaphoric or nonanaphoric), a positive instance is generated. Otherwise, a negative instance is generated. During testing, each zero anaphor candidate is presented to the learned zero anaphor detector to determine whether it is a zero anaphor or not. Besides, since a zero anaphor candidate is generated when a predicate phrase node appears, there may be two or more zero anaphor candidates in the same position. However, there is normally one zero anaphor in the same position. Therefore, we just select the one with maximal confidence as the zero anaphor in the position and ignore others, if multiple zero anaphor candidates occur in the same position.\nFigure 4: An example parse tree structure for zero anaphor detection with the predicate phrase node and the zero anaphor candidate \u03a62 in black"
        },
        {
            "heading": "4.2 Anaphoricity determination",
            "text": "To determine whether a zero anaphor is anaphoric or not, we limit the parse tree structure between the\nprevious predicate phrase node and the following predicate phrase node. Besides, we only keep those verbal phrase nodes and nominal phrase nodes.\nFigure 5 illustrates an example of the parse tree structure for anaphoricity determination, corresponding to Figure 1 with the zero anaphor \u03a62 in consideration.\nVP\nIPVV\n\u9632\u6b62 NP-SBJ VP\nNN\nNP-OBJ\n\u51fa\u73b0 NP\n\u73b0\u8c61\nVV prevent\nappear\nphenomenon\nFigure 5: An example parse tree structure for anaphoricity determination with the zero anaphor \u03a62 in consideration"
        },
        {
            "heading": "4.3 Antecedent identification",
            "text": "To identify an antecedent for an anaphoric zero anaphor, we adopt the Dynamic Expansion Tree, as proposed in Zhou et al. (2008), which takes predicate- and antecedent competitor-related information into consideration. Figure 6 illustrates an example parse tree structure for antecedent identification, corresponding to Figure 1 with the anaphoric zero anaphor \u03a6 2 and the antecedent candidate \u201c\u5efa\u7b51\u884c\u4e3a/building action\u201d in consideration.\nFigure 6: An example parse tree structure for antecedent identification with the anaphoric zero anaphor \u03a62 and the antecedent candidate \u201c\u5efa\u7b51\u884c\u4e3a/building action\u201d in consideration\nIn this paper, we adopt a similar procedure as Soon et al. (2001) in antecedent identification. Be-\nsides, since all the anaphoric zero anaphors have their antecedents at most one sentence away, we only consider antecedent candidates which are at most one sentence away. In particular, a documentlevel parse tree for an entire document is constructed by attaching the parse trees of all its sentences to a new-added upper node, as done in Yang et al. (2006), to deal with inter-sentential ones."
        },
        {
            "heading": "5 Experimentation and Discussion",
            "text": "We have systematically evaluated our tree kernelbased unified framework on our developed Chinese zero anaphora corpus, as described in Section 3.2. Besides, in order to focus on zero anaphor resolution itself and compare with related work, all the experiments are done on golden parse trees provided by CTB 6.0. Finally, all the performances are achieved using 5-fold cross validation."
        },
        {
            "heading": "5.1 Experimental results",
            "text": "Zero anaphor detection Table 4 gives the performance of zero anaphor detection, which achieves 70.05%, 83.24% and 76.08 in precision, recall and F-measure, respectively. Here, the lower precision is much due to the simple heuristic rules used to generate zero anaphors candidates. In fact, the ratio of positive and negative instances reaches about 1:12. However, this ratio is much better than that (1:30) using the heuristic rule as described in Zhao and Ng (2007). It is also worth to point out that lower precision higher recall is much beneficial than higher precision lower recall as higher recall means less filtering of true zero anaphors and we can still rely on anaphoricity determination to filter out those false zero anaphors introduced by lower precision in zero anaphor detection.\nAnaphoricity determination\nTable 5 gives the performance of anaphoricity determination. It shows that anaphoricity determination on golden zero anaphors achieves very good performance of 89.83%, 84.21% and 86.93 in precision, recall and F-measure, respectively, although useful information, such as gender and number, is not available in anaphoricity determination. This\nindicates the critical role of the structural information in anaphoricity determination of zero anaphors. It also shows that anaphoricity determination on automatic zero anaphor detection achieves 77.96%, 53.97% and 63.78 in precision, recall and Fmeasure, respectively. In comparison with anaphoricity determination on golden zero anaphors, anaphoricity determination on automatic zero anaphor detection lowers the performance by about 23 in F-measure. This indicates the importance and the necessity for further research in zero anaphor detection.\nAntecedent identification\nTable 6 gives the performance of antecedent identification given golden zero anaphors. It shows that antecedent identification on golden anaphoric zero anaphors achieves 88.93%, 68.36% and 77.29 in precision, recall and F-measure, respectively. It also shows that antecedent identification on automatic anaphoricity determination achieves 80.38%, 47.28% and 59.24 in precision, recall and Fmeasure, respectively, with a decrease of about 8% in precision, about 21% in recall and about 18% in F-measure, in comparison with antecedent identification on golden anaphoric zero anaphors. This indicates the critical role of anaphoricity determination in antecedent identification.\nP% R% F golden anaphoric zero ana-\nphors 88.90 68.36 77.29\nanaphoricity determination 80.38 47.28 59.54\nTable 6: Performance of antecedent identification given golden zero anaphors\nOverall: zero anaphora resolution\nTable 7 gives the performance of overall zero anaphora resolution with automatic zero anaphor detection, anaphoricity determination and antecedent identification. It shows that our tree kernelbased framework achieves 77.66%, 31.74% and 45.06 in precision, recall and F-measure. In comparison with Table 6, it shows that the errors caused by automatic zero anaphor detection decrease the performance of overall zero anaphora resolution by about 14 in F-measure, in comparison with golden zero anaphors.\nFigure 7 shows the learning curve of zero anaphora resolution with the increase of the number of the documents in experimentation, with the horizontal axis the number of the documents used and the vertical axis the F-measure. It shows that the F-measure is about 42.5 when 20 documents are used in experimentation. This figure increases very fast to about 45 when 50 documents are used while further increase of documents only slightly improves the performance.\nauto ZA and AZA\n41\n42\n43\n44\n45\n46\n20 30 40 50 60 70 80 90 100\nFigure 7: Learning curve of zero anaphora resolution over the number of the documents in experimentation\nTable 8 shows the detailed performance of zero anaphora resolution over different sentence distance between a zero anaphor and its antecedent. It is expected that both the precision and the recall of intra-sentential resolution are much higher than those of inter-sentential resolution, largely due to the much more dependency of intra-sentential antecedent identification on the parse tree structures.\nSentence distance P% R% F 0 85.12 33.28 47.85 1 46.55 23.64 31.36 2 - - -\nTable 8: Performance of zero anaphora resolution over sentence distances\nTable 9 shows the detailed performance of zero anaphora resolution over the two major zero anaphora categories, \u201c-NONE- *PRO*\u201d and \u201c- NONE- *pro*\u201d. It shows that our tree kernel-based framework achieves comparable performance on them, both with high precision and low recall. This is in agreement with the overall performance.\nID Category P% R% F 3 -NONE- *PRO* 79.37 34.23 47.83 4 -NONE- *pro* 77.03 30.82 44.03\nTable 9: Performance of zero anaphora resolution over major zero anaphora categories"
        },
        {
            "heading": "5.2 Comparison with previous work",
            "text": "As a representative in Chinese zero anaphora resolution, Zhao and Ng (2007) focused on anaphoricity determination and antecedent identification using feature-based methods. In this subsection, we will compare our tree kernel-based framework with theirs in details.\nCorpus Zhao and Ng (2007) used a private corpus from Converse (2006). Although their corpus contains 205 documents from CBT 3.0, it only deals with the zero anaphors under the zero anaphora category of \u201c-NONE- *pro*\u201d for dropped subjects/objects. Furthermore, Zhao and Ng (2007) only considered zero anaphors with explicit noun phrase referents and discarded zero anaphors with split antecedents (i.e. split into two separate noun phrases) or referring to entities. As a result, their corpus is only about half of our corpus in the number of zero anaphors and anaphoric zero anaphors. Besides, our corpus deals with all the types of zero anaphors and all the categories of zero anaphora except zero cataphora.\nMethod Zhao and Ng (2007) applied feature-based methods on anaphoricity determination and antecedent identification with most of features structural in nature. For zero anaphor detection, they used a very simple heuristic rule to generate zero anaphor candidates. Although this rule can recover almost all the zero anaphors, it suffers from very low precision by introducing too many false zero anaphors and thus may lead to low performance in anaphoricity determination, much due to the imbalance between positive and negative training examples with the ratio up to about 1:30.\nIn comparison, we propose a tree kernel-based unified framework for all the three sub-tasks in zero anaphora resolution. In particular, different parse tree structures are constructed for different sub-tasks. Besides, a context sensitive convolution tree kernel is employed to directly compute the similarity between the parse trees.\nFor fair comparison with Zhao and Ng (2007), we duplicate their system and evaluate it on our developed Chinese zero anaphora corpus, using the same J48 decision tree learning algorithm in Weka and the same feature sets for anaphoricity determination and antecedent identification.\nTable 11 gives the performance of the featurebased method, as described in Zhao and Ng (2007), in antecedent identification on our developed corpus. In comparison with our tree kernel-based method, it shows that 1) when using golden anaphoric zero anaphors, the feature-based method performs about 11%, 17% and 15 lower in precision, recall and F-measure, respectively; 2) when golden zero anaphors are given and feature-based anaphoricity determination is applied, the featurebased method performs about 5%, 18% and 17 lower in precision, recall and F-measure, respectively; and 3) when tree kernel-based zero anaphor detection and feature-based anaphoricity determination are applied, the feature-based method per-\n2 We do not apply the simple heuristic rule, as adopted in Zhao and Ng (2007), in zero anaphor detection, due to its much lower performance, for fair comparison on the other two subtsaks..\nforms about 7%, 8% and 10 lower in precision, recall and F-measure, respectively.\nIn summary, above comparison indicates the critical role of the structural information in zero anaphora resolution, given the fact that most of features in the feature-based methods in Zhao and Ng (2007) are also structural, and the necessity of tree kernel methods in modeling such structural information, even if more feature engineering in the feature-based methods may improve the performance to a certain extent."
        },
        {
            "heading": "6 Conclusion and Further Work",
            "text": "This paper proposes a tree kernel-based unified framework for zero anaphora resolution, which can be divided into three sub-tasks: zero anaphor detection, anaphoricity determination and antecedent identification.\nThe major contributions of this paper include: 1) We release a wide-coverage Chinese zero anaphora corpus of 100 documents, which adds a layer of annotation to the manually-parsed sentences in the Chinese Treebank (CTB) 6.0. 2) To our best knowledge, this is the first systematic work dealing with all the three sub-tasks in Chinese zero anaphora resolution via a unified framework. 3) Employment of tree kernel-based methods indicates the critical role of the structural information in zero anaphora resolution and the necessity of tree kernel methods in modeling such structural information.\nIn the future work, we will systematically evaluate our framework on automatically-generated parse trees, construct more effective parse tree structures for different sub-tasks of zero anaphora resolution, and explore joint learning among the three sub-tasks.\nBesides, we only consider zero anaphors driven by a verb predicate phrase node in this paper. In the future work, we will consider other situations. Actually, among the remaining 7% zero anaphors, about 5% are driven by a preposition phrase (PP) node, and 2% are driven by a noun phrase (NP) node. However, our preliminary experiments show that simple inclusion of those PP-driven and NPdriven zero anaphors will largely increase the imbalance between positive and negative instances, which significantly decrease the performance.\nFinally, we will devote more on further developing our corpus, with the ultimate mission of annotating all the documents in CBT 6.0."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was supported by Projects 60873150, 90920004 and 61003153 under the National Natural Science Foundation of China."
        }
    ],
    "title": "A Tree Kernel-based Unified Framework for Chinese Zero Anaphora Resolution",
    "year": 2010
}