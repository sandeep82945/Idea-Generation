{
    "abstractText": "Several large volatility matrix inference procedures have been developed, based on the latent factor model. They often assumed that there are a few of common factors, which can account for volatility dynamics. However, several studies have demonstrated the presence of local factors. In particular, when analyzing the global stock market, we often observe that nation-specific factors explain their own country\u2019s volatility dynamics. To account for this, we propose the Double Principal Orthogonal complEment Thresholding (Double-POET) method, based on multi-level factor models, and also establish its asymptotic properties. Furthermore, we demonstrate the drawback of using the regular principal orthogonal component thresholding (POET) when the local factor structure exists. We also describe the blessing of dimensionality using DoublePOET for local covariance matrix estimation. Finally, we investigate the performance of the Double-POET estimator in an out-of-sample portfolio allocation study using international stocks from 20 financial markets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sung Hoon Choi"
        },
        {
            "affiliations": [],
            "name": "Donggyu Kim"
        }
    ],
    "id": "SP:c4d7f1fc8eb649fd7999012af60a99571af4a448",
    "references": [
        {
            "authors": [
                "S.C. Ahn",
                "A.R. Horenstein"
            ],
            "title": "Eigenvalue ratio test for the number",
            "year": 2013
        },
        {
            "authors": [
                "L. Alessi",
                "M. Barigozzi",
                "M. Capasso"
            ],
            "title": "Improved penalization for determining the number of factors in approximate factor models,",
            "venue": "Statistics & Probability Letters,",
            "year": 2010
        },
        {
            "authors": [
                "A.A. Amini",
                "A. Chen",
                "P.J. Bickel",
                "E. Levina"
            ],
            "title": "Pseudo-likelihood methods for community detection in large sparse networks,",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "T. Ando",
                "J. Bai"
            ],
            "title": "Asset pricing with a general multifactor structure,",
            "venue": "Journal of Financial Econometrics,",
            "year": 2015
        },
        {
            "authors": [
                "S.B. Aruoba",
                "F.X. Diebold",
                "M.A. Kose",
                "M.E. Terrones"
            ],
            "title": "Globalization, the business cycle, and macroeconomic monitoring,",
            "venue": "in NBER international seminar on macroeconomics, University of Chicago Press Chicago, IL,",
            "year": 2011
        },
        {
            "authors": [
                "J. Bai"
            ],
            "title": "Inferential theory for factor models of large dimensions,",
            "year": 2003
        },
        {
            "authors": [
                "J. Bai",
                "S. Ng"
            ],
            "title": "Determining the number of factors in approximate factor models,",
            "year": 2002
        },
        {
            "authors": [
                "J. Bai",
                "P. Wang"
            ],
            "title": "Identification and Bayesian estimation of dynamic factor models,",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "B.S. Bernanke",
                "J. Boivin",
                "P. Eliasz"
            ],
            "title": "Measuring the effects of monetary policy: a factor-augmented vector autoregressive (FAVAR) approach,",
            "venue": "The Quarterly journal of economics,",
            "year": 2005
        },
        {
            "authors": [
                "P.J. Bickel",
                "A. Chen"
            ],
            "title": "A nonparametric view of network models and Newman\u2013Girvan and other modularities,",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "P.J. Bickel",
                "E. Levina"
            ],
            "title": "Covariance regularization by thresholding,",
            "venue": "The Annals of Statistics,",
            "year": 2008
        },
        {
            "authors": [
                "J. Boivin",
                "S. Ng"
            ],
            "title": "Are more data always better for factor analysis?",
            "venue": "Journal of Econometrics,",
            "year": 2006
        },
        {
            "authors": [
                "T. Cai",
                "W. Liu"
            ],
            "title": "Adaptive thresholding for sparse covariance matrix estimation,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2011
        },
        {
            "authors": [
                "T.T. Cai",
                "X. Li"
            ],
            "title": "Robust and computationally feasible community detection in the presence of arbitrary outlier nodes,",
            "venue": "The Annals of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "T.T. Cai",
                "Z. Ma",
                "Y. Wu"
            ],
            "title": "Sparse PCA: Optimal rates and adaptive estimation,",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "B. Recht"
            ],
            "title": "Exact matrix completion via convex optimization,",
            "venue": "Foundations of Computational mathematics,",
            "year": 2009
        },
        {
            "authors": [
                "G. Chamberlain",
                "M. Rothschild"
            ],
            "title": "Arbitrage, Factor Structure, and MeanVariance",
            "venue": "Analysis on Large Asset Markets,\u201d Econometrica,",
            "year": 1983
        },
        {
            "authors": [
                "K. Chaudhuri",
                "F.C. Graham",
                "A. Tsiatas"
            ],
            "title": "Spectral Clustering of Graphs with General Degrees in the Extended Planted Partition Model.",
            "venue": "in COLT,",
            "year": 2012
        },
        {
            "authors": [
                "I. Choi",
                "D. Kim",
                "Y.J. Kim",
                "N.-S"
            ],
            "title": "A multilevel factor model: Identification, asymptotic theory and applications,",
            "venue": "Kwark",
            "year": 2018
        },
        {
            "authors": [
                "J. Fan",
                "Y. Fan",
                "J. Lv"
            ],
            "title": "High dimensional covariance matrix estimation using a factor model,",
            "venue": "Journal of Econometrics,",
            "year": 2008
        },
        {
            "authors": [
                "J. Fan",
                "A. Furger",
                "D. Xiu"
            ],
            "title": "Incorporating global industrial classification standard into portfolio allocation: A simple factor-based large covariance matrix estimator with high-frequency data,",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "J. Fan",
                "D. Kim"
            ],
            "title": "Robust high-dimensional volatility matrix estimation for high-frequency factor model,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "J. Fan",
                "Q. Li",
                "Y. Wang"
            ],
            "title": "Estimation of high dimensional mean regression in the absence of symmetry and light tail assumptions,",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2017
        },
        {
            "authors": [
                "J. Fan",
                "Y. Liao",
                "M. Mincheva"
            ],
            "title": "High dimensional covariance matrix estimation in approximate factor models,",
            "venue": "The Annals of Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "J. Fan",
                "H. Liu",
                "W. Wang"
            ],
            "title": "2018a): \u201cLarge covariance estimation through elliptical factor models,",
            "venue": "The Annals of Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "J. Fan",
                "W. Wang",
                "Y. Zhong"
            ],
            "title": "2018b): \u201cAn \u221e eigenvector perturbation bound and",
            "year": 2018
        },
        {
            "authors": [
                "J. Fan",
                "W. Wang",
                "Z. Zhu"
            ],
            "title": "A shrinkage principle for heavy-tailed data: High-dimensional robust low-rank matrix recovery,",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "J. Fan",
                "J. Zhang",
                "K. Yu"
            ],
            "title": "Vast portfolio selection with gross-exposure constraints,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2012
        },
        {
            "authors": [
                "S. Giglio",
                "D. Xiu"
            ],
            "title": "Asset pricing with omitted factors,",
            "venue": "Journal of Political Economy,",
            "year": 2021
        },
        {
            "authors": [
                "M. Girvan",
                "M.E. Newman"
            ],
            "title": "Community structure in social and biological networks,",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2002
        },
        {
            "authors": [
                "A.W. Gregory",
                "A. C"
            ],
            "title": "Common and country-specific fluctuations in productivity, investment, and the current account,",
            "venue": "Head",
            "year": 1999
        },
        {
            "authors": [
                "L. Hagen",
                "A.B. Kahng"
            ],
            "title": "New spectral methods for ratio cut partitioning and clustering,",
            "venue": "IEEE transactions on computer-aided design of integrated circuits and systems,",
            "year": 1992
        },
        {
            "authors": [
                "B. Hajek",
                "Y. Wu",
                "J. Xu"
            ],
            "title": "Achieving exact cluster recovery threshold via semidefinite programming: Extensions,",
            "venue": "arXiv preprint arXiv:1502.07738",
            "year": 2015
        },
        {
            "authors": [
                "M. Hallin"
            ],
            "title": "L\u01d0ska (2011): \u201cDynamic factors in the presence of blocks,",
            "venue": "Journal of Econometrics,",
            "year": 2011
        },
        {
            "authors": [
                "X. Han"
            ],
            "title": "Shrinkage estimation of factor models with global and group-specific factors,",
            "venue": "Journal of Business & Economic Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "R. Jagannathan",
                "T. Ma"
            ],
            "title": "Risk reduction in large portfolios: Why imposing the wrong constraints helps,",
            "venue": "The Journal of Finance,",
            "year": 2003
        },
        {
            "authors": [
                "I.M. Johnstone",
                "A.Y. Lu"
            ],
            "title": "On consistency and sparsity for principal components analysis in high dimensions,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2009
        },
        {
            "authors": [
                "A. Joseph",
                "B. Yu"
            ],
            "title": "Impact of regularization on spectral clustering,",
            "venue": "The Annals of Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "K. Jung",
                "D. Kim",
                "S. Yu"
            ],
            "title": "Next generation models for portfolio risk management: An approach using financial big data,",
            "venue": "Journal of Risk and Insurance,",
            "year": 2022
        },
        {
            "authors": [
                "D. Kim",
                "J. Fan"
            ],
            "title": "Factor GARCH-It\u00f4 models for high-frequency data with application to large volatility matrix prediction,",
            "venue": "Journal of Econometrics,",
            "year": 2019
        },
        {
            "authors": [
                "M.A. Kose",
                "C. Otrok",
                "C.H. Whiteman"
            ],
            "title": "International business cycles: World, region, and country-specific factors,",
            "venue": "American Economic Review,",
            "year": 2003
        },
        {
            "authors": [
                "C. Lam",
                "Q. Yao"
            ],
            "title": "Factor modeling for high-dimensional time series: inference for the number of factors,",
            "year": 2012
        },
        {
            "authors": [
                "J. Lei",
                "A. Rinaldo"
            ],
            "title": "Consistency of spectral clustering in sparse stochastic block models,",
            "venue": "arXiv preprint arxiv:1312.2050",
            "year": 2013
        },
        {
            "authors": [
                "Z. Ma"
            ],
            "title": "Sparse principal component analysis and iterative thresholding,",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "F. McSherry"
            ],
            "title": "Spectral partitioning of random graphs,",
            "venue": "in Foundations of Computer Science,",
            "year": 2001
        },
        {
            "authors": [
                "E. Moench",
                "S. Ng",
                "S. Potter"
            ],
            "title": "Dynamic hierarchical factor models,",
            "venue": "Review of Economics and Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "S. Negahban",
                "M.J. Wainwright"
            ],
            "title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling,",
            "venue": "The Annals of Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "T. Qin",
                "K. Rohe"
            ],
            "title": "Regularized spectral clustering under the degree-corrected stochastic blockmodel,",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "K. Rohe",
                "S. Chatterjee",
                "B. Yu"
            ],
            "title": "Spectral clustering and the highdimensional stochastic blockmodel,",
            "venue": "The Annals of Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "A.J. Rothman",
                "E. Levina",
                "J. Zhu"
            ],
            "title": "Generalized thresholding of large covariance matrices,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2009
        },
        {
            "authors": [
                "J. Shi",
                "J. Malik"
            ],
            "title": "Normalized cuts and image segmentation,",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "M. Shin",
                "D. Kim",
                "J. Fan"
            ],
            "title": "Adaptive robust large volatility matrix estimation based on high-frequency financial data,",
            "venue": "arXiv preprint arXiv:2102.12752",
            "year": 2021
        },
        {
            "authors": [
                "C. Stein",
                "W. James"
            ],
            "title": "Estimation with quadratic loss,",
            "venue": "Proc. 4th Berkeley Symp. Mathematical Statistics Probability,",
            "year": 1961
        },
        {
            "authors": [
                "J.H. Stock",
                "M.W. Watson"
            ],
            "title": "Forecasting using principal components from a large number of predictors,",
            "venue": "Journal of the American statistical association,",
            "year": 2002
        },
        {
            "authors": [
                "L. Trapani"
            ],
            "title": "A randomized sequential procedure to determine the number of factors,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2018
        },
        {
            "authors": [
                "R. Vershynin"
            ],
            "title": "Introduction to the non-asymptotic analysis of random matrices,",
            "venue": "arXiv preprint arXiv:1011.3027",
            "year": 2010
        },
        {
            "authors": [
                "W. Wang",
                "J. Fan"
            ],
            "title": "Asymptotics of empirical eigenstructure for high dimensional spiked covariance,",
            "venue": "The Annals of Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "\uf8f7\uf8f8 . Proof"
            ],
            "title": "The first statement is followed by Assumption 3.1 and Weyl\u2019s theorem. We have \u03a3\u0302 = BB\u2032 +\u039b\u039b\u2032 +\u03a3u + (\u03a3\u0302\u2212\u03a3) = BB\u2032 +\u03a3E + (\u03a3\u0302\u2212\u03a3). We can treat BB\u2032 as a low rank matrix and the remaining terms as a perturbation matrix",
            "venue": "By Theorem 1 of Fan et al. (2018b),",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 8.\n12 32\n3v 2\n[ ec\non .E\nM ]\nSeveral large volatility matrix inference procedures have been developed, based on the latent factor model. They often assumed that there are a few of common factors, which can account for volatility dynamics. However, several studies have demonstrated the presence of local factors. In particular, when analyzing the global stock market, we often observe that nation-specific factors explain their own country\u2019s volatility dynamics. To account for this, we propose the Double Principal Orthogonal complEment Thresholding (Double-POET) method, based on multi-level factor models, and also establish its asymptotic properties. Furthermore, we demonstrate the drawback of using the regular principal orthogonal component thresholding (POET) when the local factor structure exists. We also describe the blessing of dimensionality using DoublePOET for local covariance matrix estimation. Finally, we investigate the performance of the Double-POET estimator in an out-of-sample portfolio allocation study using international stocks from 20 financial markets.\nKey words: High-dimensionality, low-rank matrix, multi-level factor model, POET, sparsity.\n\u2217Department of Economics, University of Connecticut, Storrs, CT 06269, USA. E-mail: sung hoon.choi@uconn.edu.\n\u2020Corresponding author. College of Business, Korea Advanced Institute of Science and Technology (KAIST), Seoul, Republic of Korea. Email: donggyukim@kaist.ac.kr."
        },
        {
            "heading": "1 Introduction",
            "text": "High dimensional factor analysis and principal component analysis (PCA), which are powerful tools for dimension reduction, have been extensively studied (Bai, 2003; Bernanke et al., 2005; Stock and Watson, 2002). They have various applications in economics and finance, such as forecasting macroeconomic variables and optimal portfolio allocation. Recently, a multi-level factor structure with global and local factors has received increasing attention (Ando and Bai, 2016; Bai and Wang, 2015; Choi et al., 2018; Han, 2021). The global factors impact on all individuals, while the local factors only impact on those in the specific group. The local group can be defined by regional, country, or industry level. In many economic and financial applications, the local or country factors naturally exist. For example, Kose et al. (2003) characterized the comovement of international business cycles on the global, regional, and country levels by imposing a multi-level factor structure on a dynamic factor model. Moench et al. (2013) showed that the local factors play an important role in explaining the U.S. real activities. See also Ando and Bai (2015, 2017); Aruoba et al. (2011); Gregory and Head (1999); Hallin and Li\u030cska (2011) for related articles. Hence, when the local factor structure is ignored, the conventional factor analysis might yield misleading results.\nBased on the factor models, several large volatility matrix estimation procedures have been developed to account for the strong cross-sectional correlation in the stock market. For example, Fan et al. (2008) studied the impacts of covariance matrix estimation on optimal portfolio allocation and portfolio risk assessment when the factors are observable. In contrast, Fan et al. (2013) considered latent factor models and developed the covariance matrix estimation procedure by PCA and thresholding, which is called the principal orthogonal component thresholding (POET). In this procedure, the unobservable factors can be consistently estimated with a large number of assets. Recent studies, such as Ait-Sahalia and Xiu (2017); Fan et al. (2016, 2018a); Fan and Kim (2019); Jung et al. (2022); Wang and Fan (2017), have also been conducted along this direction. However, when considering the global\nmarket, we often observe not only the global common factors but also the nation-specific risk factors (Kose et al., 2003). That is, the single-level factor model cannot sufficiently explain volatility dynamics.\nThis paper proposes a novel large volatility matrix estimation procedure that incorporates a global and national factor structure as well as a sparse idiosyncratic volatility matrix. Specifically, we consider the global asset market, and, to account for the nation-specific risk factors, we assume that there are latent multi-level factors, such as the global common factors and national risk factors. Since the national risk is a regional risk, we further assume that the local factor membership is known. Under this latent multi-level factor models, we first use the PCA procedure to capture the latent global factors. Then, after removing the latent global factors, we apply the PCA procedure in each national group to accommodate the latent national factors. Finally, to account for the sparse idiosyncratic volatility matrix, we adopt an adaptive thresholding scheme, which we call this the Double Principal Orthogonal complEment Thresholding (Double-POET). We then derive convergence rates for DoublePOET and its inverse under different matrix norms. When the local factor membership is unknown, we suggest the regularized spectral clustering method (Amini et al., 2013) to detect the latent local structure. We further discuss the benefit of the proposed DoublePOET compared to the regular POET procedure. For example, if we ignore the local factors and treat them as idiosyncratic and employ the POET procedure, the POET estimator can be inconsistent. When estimating the local volatility matrix for each country, Double-POET can estimate global factors better than the regular POET estimator. That is, we can find the blessing of dimensionality. The empirical study supports the theoretical findings.\nThe rest of the paper is organized as follows. Section 2 sets up the model and proposes the Double-POET estimation procedure. Section 3 presents an asymptotic analysis of the Double-POET estimators. The merits of the proposed method are illustrated by a simulation study in Section 4 and by real data application on portfolio allocation in Section 5. In Section 6, we conclude the study. All proofs are presented in Appendix A."
        },
        {
            "heading": "2 Model Setup and Estimation Procedure",
            "text": "Throughout this paper, let \u03bbmin(A) and \u03bbmax(A) denote the minimum and maximum eigenvalues of a matrix A, respectively. In addition, we denote by \u2016A\u2016F , \u2016A\u20162 (or \u2016A\u2016 for short), \u2016A\u2016\u221e, and \u2016A\u2016max the Frobenius norm, operator norm, l\u221e-norm and elementwise norm, which are defined respectively as \u2016A\u2016F = tr1/2(A\u2032A), \u2016A\u20162 = \u03bb1/2max(A\u2032A), \u2016A\u2016\u221e = maxi \u2211 j |aij|, and \u2016A\u2016max = maxi,j |aij|. When A is a vector, the maximum norm is denoted as \u2016A\u2016\u221e = maxi |ai|, and both \u2016A\u2016 and \u2016A\u2016F are equal to the Euclidean norm. We denote diag(A1, . . . ,An) with the diagonal block entries as A1, . . . ,An."
        },
        {
            "heading": "2.1 Multi-Level Factor Model",
            "text": "We consider the following multi-level factor model:\nyit = b \u2032 iGt + \u03bb gi i \u2032f git + uit, (2.1)\nwhere yit is the observed data for the ith individual at time t, for i = 1, . . . , p, and t = 1, . . . , T ; Gt is a k \u00d7 1 vector of unobservable \u201cglobal\u201d common factors, bi indicates the corresponding factor loadings; f git is an rgi \u00d7 1 vector of unobservable \u201clocal\u201d factors that only affect the group gi, \u03bb gi i indicates the corresponding factor loadings in each group; and uit is an idiosyncratic error component. We denote the cluster or group membership as gi \u2208 {1, . . . , J}. Throughout the paper, we assume that the group membership {gi}pi=1 is known and the global and local factors are latent. In this paper, we consider the nationspecific local factors; thus, the group membership is the country. In addition, the numbers of global factors and the number of local factors in each group are fixed.\nGiven the group membership, for gi = j, we can stack the observations and denote them\nas yt \u2261 (y1t \u2032, . . . , yJt \u2032 )\u2032, where yjt = (y1t, . . . , ypjt) \u2032 and the number of individuals pj within group j such that p = \u2211J\nj=1 pj. Let Ft = (f 1 t \u2032 , . . . , fJt \u2032 )\u2032, where f jt is an rj \u00d7 1 vector of local\nfactors and the number of factors rj for each group j such that r = \u2211J j=1 rj . We define the\np\u00d7 r block diagonal matrix as\n\u039b = diag(\u039b1,\u039b2, . . . ,\u039bJ),\nwhere \u039bj = (\u03bbj1, . . . , \u03bb j pj )\u2032 is a pj \u00d7 rj matrix of local factor loadings for each j. Then, the model (2.1) can be written in vector form as follows:\nyt = BGt +\u039bFt + ut,\nwhere B = (b1, . . . , bp) \u2032 and ut = (u1t, . . . , upt) \u2032. In matrix notation, we have\nY = GB\u2032 + F\u039b\u2032 +U, (2.2)\nwhere Y = (y1, . . . , yT ) \u2032 is a T \u00d7 p matrix of observed data, G = (G1, . . . , GT )\u2032 is a T \u00d7 k matrix of global factors, F = (F1, . . . , FT ) \u2032 is a T \u00d7 r matrix of local factors, and U = (u1, . . . , uT ) \u2032 is a T \u00d7 p matrix of idiosyncratic errors that are uncorrelated with global and local factors. Throughout the paper, we further assume that global and local factors are orthogonal each other.\nIn this paper, the parameter of interest is the p\u00d7 p covariance matrix, \u03a3, of yt as well as\nits inverse. Given model (2.1), the covariance matrix can be written as\n\u03a3 = Bcov(Gt)B \u2032 +\u039bcov(Ft)\u039b \u2032 +\u03a3u, (2.3)\nwhere \u03a3u is a sparse idiosyncratic covariance matrix. We can demonstrate the multi-level factor analysis in the presence of spiked eigenvalues at different levels. Specifically, decomposition (2.3) can be written as\n\u03a3 = Bcov(Gt)B \u2032 +\u03a3E , (2.4)\nwhere \u03a3E = \u039bcov(Ft)\u039b \u2032 + \u03a3u. Decomposition (2.4) is a usual single-level factor-based covariance matrix. Thus, based on (2.4), we can apply the regular POET procedure. However, the eigenvalue of the covariance matrix \u03a3E diverges, which makes the POET procedure inefficient. In Section 3, we discuss this inefficiency of the regular POET procedure. To further accommodate the local factor structure, we decompose the covariance matrix \u03a3E as follows:\n\u03a3jE = \u039b jcov(f jt )\u039b j \u2032 +\u03a3ju, for j = 1, . . . , J, (2.5)\nwhere the jth group covariance matrix \u03a3jE is a pj \u00d7 pj diagonal block of \u03a3E. We assume that the idiosyncratic covariance matrix \u03a3u = (\u03c3u,ij)p\u00d7p is sparse as follows:\nmp = max i\u2264p\n\u2211 j\u2264p |\u03c3u,ij|q, (2.6)\nfor some q \u2208 [0, 1), where mp diverges slowly, such as log p. Intuitively, after removing the global and local factor components, most of pairs are weakly correlated (Bickel and Levina, 2008; Cai and Liu, 2011). Theoretically, since \u2016\u03a3u\u2016 \u2264 \u2016\u03a3u\u20161 = O(mp), when mp = o(pj) for all j \u2264 J , there are distinguished eigenvalues between the local factor components and the idiosyncratic error components. In light of this, in many applications, the sparsity condition on the factor model residuals has been considered (Boivin and Ng, 2006; Fan et al., 2016). Thus, \u03a3jE is the low-rank plus sparse structure, which has been widely used (Ait-Sahalia and Xiu, 2017; Cai et al., 2013; Cande\u0300s and Recht, 2009; Fan and Kim, 2019; Fan et al., 2008, 2013; Johnstone and Lu, 2009; Ma, 2013; Negahban and Wainwright, 2011). When considering the U.S. market only, we have the usual single-level factor-based form. Then, based on (2.5), we can apply the regular POET procedure to estimate the local covariance matrix. However, this approach does not use assets outside of the local group, which causes inefficiency. We also discuss this inefficiency in Section 3. To accommodate the multi-level factor structure, we propose a novel large covariance matrix estimation procedure in the following section."
        },
        {
            "heading": "2.2 Double-POET Procedure",
            "text": "To make model (2.1) identifiable, without loss of generality, we impose normalization conditions: cov(Gt) = Ik and cov(f j t ) = Irj , where Gt and f j t are uncorrelated with each other; both B\u2032B and \u039bj \u2032 \u039bj for j \u2208 {1, . . . , J} are diagonal matrices. We also assume the pervasiveness conditions, such that (i) the eigenvalues of p\u2212a1B\u2032B are strictly greater than zero, and (ii) the eigenvalues of p\u2212a2j \u039b j \u2032\u039bj are strictly greater than zero for each j, where a1, a2 \u2208 (0, 1] are the strengths of global and local factors, respectively. Then, the first k eigenvalues of Bcov(Gt)B \u2032 diverge at rate O(pa1), while the first r eigenvalues of \u039bcov(Ft)\u039b \u2032 diverge at rate O(pca2), which does not grow too fast as pa1 \u2192 \u221e. We note that pc is related to the number of stocks in each country. In addition, all eigenvalues of \u03a3u are bounded. Let \u0393 = diag(\u03b41, . . . , \u03b4k) be the leading eigenvalues of \u03a3 and V = (v1, . . . , vk) be their corresponding leading eigenvectors.\nTo accommodate the multi-level factor model (2.1), we propose a non-parametric esti-\nmator of \u03a3 as follows:\n1. Let \u03b4\u03021 \u2265 \u03b4\u03022 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b4\u0302p be the ordered eigenvalues of the sample covariance matrix\n\u03a3\u0302 = T\u22121 \u2211T\nt=1(yt \u2212 y\u0304)(yt \u2212 y\u0304)\u2032 and {v\u0302i}pi=1 be their corresponding eigenvectors. Then,\nwe compute\n\u03a3\u0302E = \u03a3\u0302\u2212 V\u0302\u0393\u0302V\u0302 \u2032 ,\nwhere \u0393\u0302 = diag(\u03b4\u03021, . . . , \u03b4\u0302k) and V\u0302 = (v\u03021, . . . , v\u0302k).\n2. Define \u03a3\u0302 j E as each pj \u00d7 pj diagonal block of \u03a3\u0302E. For the jth block, let {\u03ba\u0302ji , \u03b7\u0302ji } pj i=1 be\nthe eigenvalues and eigenvectors of \u03a3\u0302 j\nE in decreasing order. Then, we compute the\nprincipal orthogonal complement \u03a3\u0302u as follows:\n\u03a3\u0302u = \u03a3\u0302\u2212 V\u0302\u0393\u0302V\u0302 \u2032 \u2212 \u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032,\nwhere \u03a8\u0302 = diag(\u03a8\u03021, . . . , \u03a8\u0302J) for \u03a8\u0302j = diag(\u03ba\u0302j1, . . . , \u03ba\u0302 j rj ), and the block diagonal matrix\n\u03a6\u0302 = diag(\u03a6\u03021, . . . , \u03a6\u0302J) for \u03a6\u0302j = (\u03b7\u0302j1, . . . , \u03b7\u0302 j rj ) for j = 1, 2, . . . , J .\n3. Apply the adaptive thresholding method on \u03a3\u0302u = (\u03c3\u0302u,ij)p\u00d7p following Bickel and Levina\n(2008) and Fan et al. (2013). Specifically, define \u03a3\u0302 D u as the thresholded error covariance matrix estimator:\n\u03a3\u0302 D u = (\u03c3\u0302 D u,ij)p\u00d7p, \u03c3\u0302 D u,ij =    \u03c3\u0302u,ij , i = j sij(\u03c3\u0302u,ij)I(|\u03c3\u0302u,ij| \u2265 \u03c4ij), i 6= j ,\nwhere an entry-dependent threshold \u03c4ij = \u03c4(\u03c3\u0302u,ii\u03c3\u0302u,jj) 1/2 and sij(\u00b7) is a generalized shrinkage function (e.g., hard or soft thresholding; see Cai and Liu (2011); Rothman et al. (2009)). The thresholding constant \u03c4 will be determined in Theorem 3.1.\n4. The final estimator of \u03a3 is then defined as\n\u03a3\u0302 D = V\u0302\u0393\u0302V\u0302 \u2032 + \u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 + \u03a3\u0302 D u . (2.7)\nThe above procedure can be equivalently represented by a least squares method. In particular, the global and local factor matrices and corresponding loading matrices are estimated as follows. To obtain the global factor part, we first solve the following optimization problem:\n(B\u0302, G\u0302) = argmin B,G\n\u2016Y\u2212GB\u2032\u20162F , (2.8)\nsubject to the normalization constraints such that\n1 T G\u2032G = Ik and B \u2032B is diagonal matrix.\nThe columns of G\u0302/ \u221a T are the eigenvectors corresponding to the k largest eigenvalues of the T \u00d7T matrix T\u22121YY\u2032 and B\u0302 = (b\u03021, . . . , b\u0302p)\u2032 = T\u22121Y\u2032G\u0302. Given G\u0302 and B\u0302, let a T \u00d7 p matrix\nE\u0302 = Y\u2212 G\u0302B\u0302\u2032 \u2261 (E\u03021, . . . , E\u0302J). Then, with E\u0302, we estimate the local factor part as follows:\n(\u039b\u0302j , F\u0302 j) = argmin \u039b j ,F j \u2016E\u0302j \u2212 F j\u039bj\u2032\u20162F , (2.9)\nsubject to the normalization constraints such that\n1 T F j\u2032F j = Irj and \u039b j\u2032\u039bj is diagonal matrix.\nThen, we obtain F\u0302 = (F\u0302 1, . . . , F\u0302 J) and \u039b\u0302 = diag(\u039b\u03021, . . . , \u039b\u0302J), where the columns of F\u0302 j/ \u221a T are the eigenvectors corresponding to the rj largest eigenvalues of the T\u00d7T matrix T\u22121E\u0302jE\u0302j\u2032 and \u039b\u0302j = (\u03bb\u0302j1, . . . , \u03bb\u0302 j pj ) = T\u22121E\u0302j\u2032F\u0302 j for j = 1, 2, . . . , J . Finally, we apply the above adaptive thresholding method to \u03a3\u0302u = T \u22121U\u0302 \u2032 U\u0302, where U\u0302 = Y \u2212 G\u0302B\u0302\u2032 \u2212 F\u0302\u039b\u0302\u2032. Similar to the decomposition (2.3), we have the following substitution estimators:\n\u03a3\u0303 D = B\u0302B\u0302 \u2032 + \u039b\u0302\u039b\u0302 \u2032 + \u03a3\u0302 D u . (2.10)\nSimilar to the proofs of Theorem 1 of Fan et al. (2013), we can show that the estimators (2.7) based on PCA and (2.10) based on least squares approaches are equivalent. In particular, by the Eckart-Young theorem, the estimators B\u0302 and \u039b\u0302j, after normalization, are the first k and rj empirical eigenvectors of the sample covariance matrix \u03a3\u0302 and \u03a3\u0302 j E, respectively. Then, there exist orthogonal matrices H and J such that\n\u2016B\u0302\u2212BH\u2032\u2016max = OP (\u03c9\u0303T ), \u2016\u039b\u0302\u2212\u039bJ\u2032\u2016max = OP (\u03c9T ),\nwhere \u03c9\u0303T and \u03c9T are defined in Section 3.1 and Theorem 3.1, respectively. The above rates can be easily obtained using the preliminary results in Appendix A.\nIn summary, given the knowledge of group membership, we employ PCA on each diagonal block of the remaining components of the sample covariance matrix, after removing the first\nk principal components. Then, we apply thresholding on the remaining residual components. We call this procedure the Double Principal Orthogonal complEment Thresholding (DoublePOET). Double-POET is the two-step estimation procedure, which makes it possible to estimate global and local factors separately by considering the block structure on the local factors. In contrast, if we employ the single step estimation procedure, such as POET, the local factor structure cannot be explained well. We discuss the theoretical inefficiency of POET in Sections 3.1 and 3.3, and the numerical study illustrated in Section 4 shows that Double-POET outperforms POET."
        },
        {
            "heading": "3 Asymptotic Properties",
            "text": "In this section, we establish the asymptotic properties of the Double-POET estimator. To do this, we impose the following technical conditions.\nAssumption 3.1.\n(i) For some constants c \u2208 (0, 1], a1 \u2208 (3+2c5 , 1] and a2 \u2208 (35 , 1], all eigenvalues of B\u2032B/pa1\nand \u039bj\u2032\u039bj/pa2j are strictly bigger than zero as p, pj \u2192 \u221e, for j \u2208 {1, . . . , J} and a1 \u2265 ca2. If a1 = ca2, we further assume that \u03bbmin(B\u2032B)\u2212 \u03bbmax(\u039b\u2032\u039b) \u2265 d\u03bb for some positive constant d\u03bb. In addition, there is a constant d > 0 such that \u2016B\u2016max \u2264 d and \u2016\u039b\u2016max \u2264 d.\n(ii) There exists constants d1, d2 > 0 such that \u03bbmin(\u03a3u) > d1 and \u2016\u03a3u\u20161 \u2264 d2mp.\n(iii) The sample covariance matrix \u03a3\u0302 satisfies\n\u2016\u03a3\u0302\u2212\u03a3\u2016max = OP ( \u221a log p/T ).\nRemark 3.1. Assumption 3.1(i) ensures that the factors are pervasive. This pervasive assumption is essential to analyze low-rank matrices (Fan et al., 2013, 2018b) and is reasonable\nin many financial applications (Bai, 2003; Chamberlain and Rothschild, 1983; Kim and Fan, 2019; Lam and Yao, 2012; Stock and Watson, 2002). For example, under the multi-level factor model, the global factors impact on most of the assets, while the national factors affect only those belonging to each country. This structure of the latent factors implies the pervasive condition, which is related to the incoherence structure (Fan et al., 2018b). To analyze large matrix inferences, we impose the element-wise convergence condition (Assumption 3.1(iii)). Under the sub-Gaussian condition and the mixing time dependency, as considered in Fan et al. (2013), this condition can be easily satisfied (Fan et al., 2018a,b; Vershynin, 2010; Wang and Fan, 2017). We can obtain this condition under the heavy-tailed observations with the condition of bounded fourth moments (Fan et al., 2017, 2018a,b, 2021). Furthermore, when observations are martingales with bounded fourth moments, we can obtain the element-wise convergence condition (Fan and Kim, 2018; Shin et al., 2021).\nTo measure large matrix estimation errors, we consider the relative Frobenius norm,\nintroduced by Stein and James (1961):\n\u2016\u03a3\u0302\u2212\u03a3\u2016\u03a3 = p\u22121/2\u2016\u03a3\u22121/2\u03a3\u0302\u03a3\u22121/2 \u2212 Ip\u2016F .\nNote that the factor p\u22121/2 performs normalization, such that \u2016\u03a3\u2016\u03a3 = 1. Fan et al. (2013) showed that, under this relative Frobenius norm, the POET estimator is consistent as long as p = o(T 2), while the sample covariance is consistent only if p = o(T ) in the approximate single-level factor model. In Section 3.1, we will compare the convergence rates of POET and Double-POET in a multi-level factor model.\nThe following theorem provides the convergence rates under various norms.\nTheorem 3.1. Under Assumption 3.1, suppose that pj \u224d pc for each j and mp = o(pc(5a2\u22123)/2). Let \u03c9T = p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T + 1/p 5 2 a1\u2212 32+c( 5 2 a2\u2212 72 ) + mp/ \u221a pc(5a2\u22123) and \u03c4 \u224d \u03c9T . If\nmp\u03c9 1\u2212q T = o(1), we have\n\u2016\u03a3\u0302Du \u2212\u03a3u\u2016max = OP (\u03c9T ), (3.1) \u2016\u03a3\u0302Du \u2212\u03a3u\u20162 = OP (mp\u03c91\u2212qT ), \u2016(\u03a3\u0302 D u ) \u22121 \u2212\u03a3\u22121u \u20162 = OP (mp\u03c91\u2212qT ), (3.2) \u2016\u03a3\u0302D \u2212\u03a3\u2016max = OP (\u03c9T ), (3.3) \u2016(\u03a3\u0302D)\u22121 \u2212\u03a3\u22121\u20162 = OP (mp\u03c91\u2212qT ). (3.4)\nIn addition, if a1 > 3 4 and a2 > 3 4 , we have\n\u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 = OP ( mp\u03c9 1\u2212q T + p 11 2 \u22125a1+5c(1\u2212a2) log p\nT +\n1\np5a1\u2212 7 2 \u2212c(7\u22125a2)\n+ m2p\np5ca2\u22123c\u2212 1 2\n) . (3.5)\nRemark 3.2. The additional terms 1/p 5 2 a1\u2212 32+c( 5 2 a2\u2212 72 ) and mp/ \u221a pc(5a2\u22123) in \u03c9T result from the estimation of unknown global factors and national factors. They are negligible when the dimensional p is high as long as 0 < c < 5a1\u22123 7\u22125a2 . In contrast, for the regular POET in the single-level factor model, 1/ \u221a p appears in the threshold (Fan et al., 2013). Under the relative Frobenius norm, the upper and lower bounds of c are required to maintain the consistency of the Double-POET estimator. Specifically, when mp = O(1), the national factor estimation requires 1 2(5a2\u22123) < c < 10a1\u22127 14\u221210a2 , which is satisfied when both strength levels of global and local factors are sufficiently large.\nRemark 3.3. The asymptotic theory relies on the relative rates of the number of assets in each group, pj \u224d pc, as well as the number of groups, G \u224d p1\u2212c. We note that the total number of local factors grows at a rate O(p1\u2212c). For simplicity, consider the case of strong global and local factors (i.e., a1 = 1 and a2 = 1). Hierarchically, to effectively estimate the total local factors, there should be sufficiently large number of assets in each group, but the number of group should not grow much faster than the number of assets in each group to enjoy the blessing of dimensionality. Otherwise, the sum of local factor estimation errors will diverge as G grows, which causes the curse of dimensionality. Practically, the number\nof assets in each country is sufficiently larger than the number of countries. Hence, the Double-POET method can account for the global stock market data."
        },
        {
            "heading": "3.1 POET for the Multi-Level Factor Models",
            "text": "In this subsection, we analyze the regular POET of Fan et al. (2013) in the multi-level factor models and compare it with the proposed Double-POET.\nThe regular POET method only captures the global factors and regards both the local factor structure and idiosyncratic terms as the idiosyncratic part. That is, the sparsity level of \u03a3E = (\u03b5ij)p\u00d7p is\n\u00b5p = max i\u2264p\n\u2211 j\u2264p |\u03b5ij|q,\nfor some q \u2208 [0, 1). We note that when q = 0, \u00b5p \u224d pc, which corresponds to the maximum number of nonzero elements in each row of \u03a3E. Then, the thresholding approach is applied to \u03a3\u0302E instead of \u03a3\u0302u to obtain \u03a3\u0302 T E . Therefore, the regular POET estimator is defined as\n\u03a3\u0302 T = V\u0302\u0393\u0302V\u0302 \u2032 + \u03a3\u0302 T E .\nSimilar to the proofs of Fan et al. (2013), we can show that the regular POET yields\n\u2016\u03a3\u0302TE \u2212\u03a3E\u20162 = OP (\u00b5p\u03c9\u03031\u2212qT ), (3.6) \u2016\u03a3\u0302T \u2212\u03a3\u2016\u03a3 = OP ( \u00b5p\u03c9\u0303 1\u2212q T + p 11 2 \u22125a1 log p\nT +\n1\np5a1\u2212 7 2 \u22122c\n) , (3.7)\nwhere \u03c9\u0303T = p 5 2 (1\u2212a1) \u221a log p/T+1/p 5 2 a1\u2212 32\u2212c. We compare the convergence rates of the regular POET and proposed Double-POET. For example, under the relative Frobenius norm, when q = 0, mp = O(1), a1 = 1, and a2 = 1, we have\n\u2016\u03a3\u0302T \u2212\u03a3\u2016\u03a3 = OP ( pc \u221a log p\nT +\n1\np1\u22122c +\n\u221a p log p\nT\n) ,\n\u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 = OP (\u221a log p\nT +\n1\np1\u2212c + pc +\n\u221a p log p\nT +\n1\np 3 2 \u22122c\n+ 1\np2c\u2212 1 2\n) .\nThe number of assets within a group, pj \u224d pc, for some constant c > 0 plays a crucial role in a convergence rate. Theorem 3.1 shows that \u2016\u03a3\u0302D \u2212 \u03a3\u2016\u03a3 can be convergent as long as p = o(T 2) and 1 4 < c < 3 4 . In contrast, the rate of the regular POET estimator, \u2016\u03a3\u0302T \u2212\u03a3\u2016\u03a3, does not converge if c > 1 2 or p\u03b1 > T with \u03b1 = min{1 2 , 2c}. In other words, the regular POET requires the number of assets in each country to be small enough to avoid the curse of dimensionality. However, the number of assets in each country is larger than the number of countries; thus, it is more reasonable to assume c > 1 2 . Therefore, under the global and national factor models, the regular POET does not provide a consistent estimator in terms of the relative Frobenius norm. Furthermore, when the global factors are weak (i.e., a1 < 1) and the local factors are strong (i.e., a2 = 1), Double-POET is equal to or better than POET in terms of the convergence rate under the relative Frobenius norm."
        },
        {
            "heading": "3.2 Orthogonality between the Global and Local Factor Loadings",
            "text": "When the signal of local factors is strong, we often consider the local factors as the global weak factors. Then, we can apply the regular POET method with the global and local factors. Theoretically, to identify the latent factors, we additionally need to impose an orthogonality condition between the global and local factor loadings, B and \u039b. In this section, under this orthogonality condition, we investigate asymptotic behaviors of the Double-POET procedure and compare POET with Double-POET.\nUnder the orthogonality condition, we first obtain the following modified theoretical\nresults for Double-POET.\nTheorem 3.2. Suppose that B and \u039b are orthogonal each other. Under Assumption 3.1, suppose that pj \u224d pc for each j and mp = o(min{p(5a1\u22123)/2, pc(5a2\u22123)/2}). Let \u03c9T,2 = p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T+mp/ \u221a pc(5a2\u22123)+mp/ \u221a p5a1\u22123\u22125c(1\u2212a2) and \u03c4 \u224d \u03c9T,2. If mp\u03c91\u2212qT,2 =\no(1), we have\n\u2016\u03a3\u0302Du \u2212\u03a3u\u2016max = OP (\u03c9T,2), (3.8) \u2016\u03a3\u0302Du \u2212\u03a3u\u20162 = OP (mp\u03c91\u2212qT,2 ), \u2016(\u03a3\u0302 D u ) \u22121 \u2212\u03a3\u22121u \u20162 = OP (mp\u03c91\u2212qT,2 ), (3.9) \u2016\u03a3\u0302D \u2212\u03a3\u2016max = OP (\u03c9T,2), (3.10) \u2016(\u03a3\u0302D)\u22121 \u2212\u03a3\u22121\u20162 = OP (mp\u03c91\u2212qT,2 ). (3.11)\nIn addition, if a1 > 3 4 and a2 > 3 4 , we have\n\u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 = OP ( mp\u03c9 1\u2212q T,2 + p 11 2 \u22125a1+5c(1\u2212a2) log p\nT +\nm2p\np5a1\u2212 7 2 \u22125c(1\u2212a2)\n+ m2p\npc(5a2\u22123)\u2212 1 2\n) .\n(3.12)\nRemark 3.4. The orthogonality condition helps identify the latent global and local factors by reducing perturbation terms. Thus, compared to the results of Theorem 3.1, Theorem 3.2 shows the faster convergence rate, and we can relax the upper bound for c. For example, the additional terms mp/ \u221a pc(5a2\u22123) and mp/ \u221a p5a1\u22123\u22125c(1\u2212a2) in \u03c9T,2 are negligible when 0 < c < 5a1\u22123 5(1\u2212a2) and c \u2264 a1 a2 as p \u2192 \u221e. In addition, when mp = O(1), 12(5a2\u22123) < c < a1\u22120.7 1\u2212a2 and c \u2264 a1 a2 are required to make \u2016\u03a3\u0302D\u2212\u03a3\u2016\u03a3 converge. Therefore, the Double-POET method does not require the upper bound for c when the global and local factors are strong. For example, the number of group, J , can be fixed (i.e., c = 1) as long as a1 \u2265 a2.\nWe now consider the POET estimator that regards both the global and local factors as the global factors under the orthogonality condition. We call this the POET2 estimator. The estimator is then defined as follows:\n\u03a3\u0302 T 2 = V\u03022\u0393\u03022V\u0302 \u2032 2 + \u03a3\u0302 T u,2,\nwhere \u0393\u03022 = diag(\u03b4\u03021, . . . , \u03b4\u0302k+r) and V\u03022 = (v\u03021, . . . , v\u0302k+r). Then, when a1 = a2 = 1, the POET2\nestimator yields\n\u2016\u03a3\u0302Tu,2 \u2212\u03a3u\u20162 = OP (mp\u03c9\u03031\u2212qT,2 ), (3.13) \u2016\u03a3\u0302T2 \u2212\u03a3\u2016\u03a3 = OP ( mp\u03c9\u0303 1\u2212q T,2 + p 15(1\u2212c)+ 1 2 log p\nT +\nm2p\np15c\u2212 27 2\n) , (3.14)\nwhere \u03c9\u0303T,2 = p 7(1\u2212c) \u221a log p/T + mp/p 7c\u22126. We note that the additional term mp/p 7c\u22126 in \u03c9\u0303T,2 can be negligible only when c > 6 7 as p increases. This is because the POET2 estimator includes noises on the off-diagonal blocks of the local factor component. Therefore, it requires the number of groups to be sufficiently small. Otherwise, the POET2 estimator does not perform well. That is, since the local factors are considered as the weak factors of the global factor, the local factor should have enough signals.\nWe compare the convergence rates of the POET2 and Double-POET estimators under\nthe orthogonality condition. When q = 0, mp = O(1), a1 = 1, and a2 = 1, we have\n\u2016\u03a3\u0302T2 \u2212\u03a3\u2016\u03a3 = OP ( p7(1\u2212c) \u221a log p\nT +\n1\np7c\u22126 + p15(1\u2212c)+ 1 2\nlog p\nT +\n1\np15c\u2212 27 2\n) ,\n\u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 = OP (\u221a log p\nT +\n1\npc +\n\u221a p log p\nT +\n1\np2c\u2212 1 2\n) .\nWhen c < 1, the convergece rate of Double-POET is faster than that of POET2. Furthermore, \u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 can be consistent as long as p = o(T 2) and c > 14 , while \u2016\u03a3\u0302 T 2 \u2212\u03a3\u2016\u03a3 can be consistent when p15(1\u2212c)+ 1\n2 = o(T ) and c > 9 10 . This implies that POET2 performs well\nonly for the multi-level factor model with a small number of groups (i.e., weak factors with enough signals). We also note that when the local factor has the same signal as the global factor (c = 1), the POET2 and Double-POET procedures have the same convergence rate."
        },
        {
            "heading": "3.3 Blessing of Dimensionality",
            "text": "In this section, we demonstrate the blessing of dimensionality for country-wise covariance matrix estimation by using the proposed Double-POET procedure. For the country j, we\nhave\nyjt = B jGt + \u039b jf jt + u j t ,\nwhere Bj = (bj1, . . . , b j pj )\u2032, \u039bj = (\u03bbj1, . . . , \u03bb j pj )\u2032, and ujt = (u1t, . . . , upjt) \u2032. Then, the covariance matrix of the jth country can be written as\n\u03a3j = Bjcov(Gt)B j\u2032 + \u039bjcov(f jt )\u039b j\u2032 +\u03a3ju.\nThen, the sparsity level of \u03a3ju is\nmpj := max i\u2264pj\n\u2211\nj\u2264pj\n|\u03c3u,ij|q, for some q \u2208 [0, 1).\nTo estimate the jth country\u2019s covariance matrix \u03a3j , assuming that the common factors include both global and national factors with the orthogonality condition between their factor loadings, the regular POET method can be applied with k+ rj principal components using the jth country\u2019s stock market data (i.e., T \u00d7pj observation matrix), denoted by \u03a3\u0302 j,T . As long as pj \u2192 \u221e, the asymptotic results of Fan et al. (2013) can be directly applied by replacing p by pj \u224d pc for c \u2208 (0, 1]. In contrast, we suggest the Double-POET method by extracting the jth diagonal block of \u03a3\u0302 D , denoted by (\u03a3\u0302 D )j . Then, when estimating the global factor component, Double-POET uses more data, which might result in a more accurate global factor estimator. Specifically, we have the following convergence rates of Double-POET estimator for the local covariance matrix.\nTheorem 3.3. Under the assumptions of Theorem 3.2, the Double-POET estimator satisfies, if mpj\u03c9 1\u2212q T,2 = o(1),\n\u2016(\u03a3\u0302D)j \u2212\u03a3j\u2016max = OP (\u03c9T,2), (3.15) \u2016((\u03a3\u0302D)j)\u22121 \u2212 (\u03a3j)\u22121\u20162 = OP (mpj\u03c91\u2212qT,2 ). (3.16)\nIn addition, if a1 > 3 4 and a2 > 3 4 , we have\n\u2016(\u03a3\u0302D)j \u2212\u03a3j\u2016\u03a3j = OP ( mpj\u03c9 1\u2212q T,2 + p 5(1\u2212a1)+c( 112 \u22125a2) log p\nT +\nm2p\np5a1\u22123+c(5a2\u2212 11 2 ) +\nm2p\npc(5a2\u2212 7 2 )\n) ,\n(3.17)\nwhere \u2016A\u2016\u03a3j = p\u22121/2j \u2016\u03a3j \u22121/2 A\u03a3j \u22121/2\u2016F is the relative Frobenius norm.\nRemark 3.5. We compare the rates of convergence between the Double-POET and POET estimators as follows. Following Fan et al. (2018a), under the assumptions of Theorem 3.3, the regular POET estimator satisfies the following conditions, with \u03c4 \u224d \u03c9\u0308T , if mpj \u03c9\u0308T = o(1), a1 = 1, and a2 = 1:\n\u2016\u03a3\u0302j,T \u2212\u03a3j\u2016max = OP (\u03c9\u0308T ), \u2225\u2225\u2225(\u03a3\u0302 j,T )\u22121 \u2212 (\u03a3j)\u22121\n\u2225\u2225\u2225 2 = OP (mpj \u03c9\u0308 1\u2212q T ),\n\u2016\u03a3\u0302j,T \u2212\u03a3j\u2016\u03a3j = OP ( mpj \u03c9\u0308 1\u2212q T + \u221a pc log p\nT\n) ,\nwhere \u03c9\u0308T = \u221a log p/T + mpj/p c. The overall rates of convergence under each norm are the same. This might be because the convergence rate is dominated by the national factor estimator, which is estimated by the Double-POET and POET procedures, using the same amount of the data. However, when we focus on the global factor components, the DoublePOET procedure indeed can have a faster rate of convergence than POET. For example, when a1 = 1 and a2 = 1, under the max norm, the convergence rates of Double-POET and POET for BjBj\u2032 are OP ( \u221a log p/T + mp/p) and OP ( \u221a log p/T + mp/p c), respectively. In addition, in terms of the relative Frobenius norm, the convergence rates of Double-POET and POET for BjBj\u2032 are OP ( \u221a log p/T + \u221a pc log p/T +mp/p+m 2 p/p 2\u2212c/2) and OP (1/ \u221a T + \u221a pc log p/T+m2p/p 3c/2), respectively. Thus, the global factor estimator of Double-POET has a faster convergence rates than that of POET when c < 1 (under the max norm) or c < 2 3 when mp = O(1) (under the relative Frobenius norm). Furthermore, the numerical study\npresented in Section 4 shows that Double-POET outperforms POET, especially for small pj. This implies that Double-POET enjoys the blessing of dimensionality by incorporating other countries\u2019 observations. That is, the proposed Double-POET model not only presents a way to investigate the global stock market, but also shows the benefits of incorporating financial big data."
        },
        {
            "heading": "3.4 Determination of the Number of Factors",
            "text": "To implement Double-POET, we need to determine the number of factors. In this section, we describe a data-driven method for determining the number of global and national factors.\nWe suggest a modified version of the eigenvalue ratio method proposed by Ahn and Horenstein\n(2013) as follows. We first consider a model selection problem between the multi-level factor model and the single-level factor model, and then propose estimators for the number of factors. Let \u03b4\u0302m be the mth largest eigenvalue of the sample covariance matrix and ER(m) = \u03b4\u0302m/\u03b4\u0302m+1 be the mth eigenvalue ratio. Under the multi-level factor model (i.e., k > 0 and r > 0), the first k eigenvalues of the sample covariance matrix are asymptotically determined by the eigenvalues of Bcov(Gt)B \u2032, the next r = \u2211J j=1 rj eigenvalues by the eigenvalues of \u039bcov(Ft)\u039b \u2032, and the other eigenvalues by those of the idiosyncratic covariance matrix. Accordingly, when a1 = 1 and a2 = 1, ER(m) = OP (1) for m 6= k and m 6= k + r, ER(k) = OP (p 1\u2212c), and ER(k + r) = OP (p c). This implies that there are two diverging eigenvalue ratios when 0 < c < 1, while all other ratios of two adjacent eigenvalues are asymptotically bounded. In contrast, under the single-level factor model, there exists only one diverging eigenvalue ratio. We define\nk\u03021 = max 1\u2264m\u2264kmax ER(m) and k\u03022 = max 1\u2264m6=k\u03021\u2264kmax ER(m),\nfor a prespecified kmax < min(p, T ). Let \u03d5p be the tuning parameter, which grows slowly. In practice, we set \u03d5p = d log p for a positive constant d. We then select the single-level factor\nmodel when ER(k\u03021) > \u03d5p and ER(k\u03022) \u2264 \u03d5p, and one can apply the regular POET with k\u03021 factors. When ER(k\u03022) > \u03d5p, we select the multi-level factor model and estimate the number of global factors k by k\u0302 = min{k\u03021, k\u03022}. Under the following technical conditions, we can show the consistency of k\u0302.\nAssumption 3.2.\n(i) There exist T \u00d7 T and p \u00d7 p positive semidefinite matrices RT and GT such that\nU = R 1/2 T \u03a5G 1/2 p , where the idiosyncratic observation matrix U is defined in (2.2), and \u03a5\u2032 = [\u03c5it]p\u00d7T .\n(ii) \u03c5it indicates i.i.d. random variables with uniformly bounded moments up to the fourth\norder.\n(iii) There are generic positive constants d1, d2 > 0 such that \u03bbmax(RT ) < d1, \u03bbmax(Gp) <\nd1, and \u03bbmin(RT ) > d2.\n(iv) Let y\u2217 = limm\u2192\u221em/p, where m = min(p, T ). Then, there exists a real number d \u2217 \u2208\n(1\u2212 y\u2217, 1] such that the (d\u2217p)th largest eigenvalue of Gp is strictly greater than zero for all p.\nThen, we have the following theorem.\nTheorem 3.4. Suppose Assumptions 3.1\u20133.2 hold, and in addition suppose that mp = o(pca2), c < 5a1\u22123 4\u22122a2 , and max{p 1\u2212ca2, p5(1\u2212a1)+2c(1\u2212a2) log p} = o(T ).\n(i) Given a tunning parameter \u03d5p < min{pa1\u2212ca2 , mp1\u2212ca2 }, the proposed model selection is\nconsistent.\n(ii) When k > 0 and r > 0, there exists a constant d \u2208 (0, 1] such that limm\u2192\u221e Pr(k\u0302 =\nk) = 1, for any kmax \u2208 (k + r, dm\u2212 2(k + r)\u2212 1].\nGiven the consistently estimated number of global factors, we can apply the existing methods to consistently estimate the number of local factors rj (Alessi et al., 2010;\nBai and Ng, 2002; Choi et al., 2018; Giglio and Xiu, 2021; Onatski, 2010; Trapani, 2018). Throughout the paper, we use the eigenvalue ratio method of Ahn and Horenstein (2013) as follows: for each group j, let \u03ba\u0302jr be the rth largest eigenvalues of the pj \u00d7 pj matrix \u03a3\u0302 j E in the Section 2.2. Then, rj can be estimated by\nr\u0302j = max 1\u2264r\u2264rj,max \u03ba\u0302jr \u03ba\u0302jr+1 ,\nfor a predetermined rj,max."
        },
        {
            "heading": "3.5 Unknown Local Group Membership",
            "text": "In this paper, we assume that local factors are governed by the national regional risk factors, which provides the membership of the local factors. However, in practice, the membership of the local factors is unknown. In this section, we discuss how to use the proposed DoublePOET procedure for the unknown local factor membership.\nThe Double-POET procedure is working as long as the membership for the local factor is known. Thus, to harness the Double-POET, we need to classify assets and find the latent local structure. As discussed in the previous sections, we can estimate the global factor even if the local factor structure is unknown. Thus, we can consistently estimate \u03a3E defined in (2.4), which is the remaining covariance matrix after subtracting the global factor part. Under some local factor structure, \u03a3E is a form of a block diagonal matrix and its block membership represents the local factor membership. Thus, we can detect the group membership, based on \u03a3E. Specifically, to adjust the scale problem of the convariance matrix, we calculate the correlation matrix of \u03a3E. Since the sign of the correlations does not contain the membership information, we use the absolute values of the correlations. We denote this matrix by L. We consider L as the adjacency matrix of the local factor network. Based on the adjacency matrix, many models and methodologies have been developed to detect and identify group memberships. Examples include Rati-\noCut (Hagen and Kahng, 1992), Ncut (Shi and Malik, 2000), spectral clustering method (Lei and Rinaldo, 2013; McSherry, 2001; Rohe et al., 2011), regularized spectral clustering (Amini et al., 2013), semi-definite programming (Cai and Li, 2015; Hajek et al., 2015), Newman-Girvan Modularity (Girvan and Newman, 2002), and maximum likelihood estimation (Amini et al., 2013; Bickel and Chen, 2009). To detect the membership matrix, we employ the regularized spectral clustering (RSC) (Amini et al., 2013).\nThe regularized spectral clustering (RSC) is based on the regularized row and column normalized adjacency matrix (or the regularized graph Laplacian) (Chaudhuri et al., 2012; Qin and Rohe, 2013),\nL\u0303deg,a = D \u22121/2 a LD \u22121/2 a ,\nwhere the degree matrix is denoted byD = diag(d\u03021, . . . , d\u0302p) with d\u0302i = \u2211p j=1Lij , Da = D+aI, I denotes the identity matrix, and a \u2265 0 is the regularization parameter. For the numerical study, we use the average node degree as the regularization parameter a. Then, calculate the eigenvector matrix corresponding to the K largest eigenvalue of L\u0303deg,a. Using the eigenvector matrix, we apply the k-means clustering procedure and identify the local factor groups. Unfortunately, we cannot observe \u03a3E , and so we estimate it using the POET procedure. Then, using the plug-in method, we estimate the regularized row and column normalized adjacency matrix. Under some regularity condition, we can show that the ratio of the misclassification goes to zero (Joseph et al., 2016; Qin and Rohe, 2013). With this estimated membership, we can apply the proposed Double-POET."
        },
        {
            "heading": "4 Simulation Study",
            "text": "In this section, we conducted simulations to examine the finite sample performance of the proposed Double-POET method. We considered the following multi-level factor model:\nyit = k\u2211\nl=1\nbilGtl +\nrj\u2211\ns=1\n\u03bbjisf j ts + uit,\nwhere the global factors and national factors, Gtl and f j ts, respectively, were all drawn from N (0, 1). The global factor loadings {bi1, . . . , bik}i\u2264p were drawn from N (\u00b5B, Ik), where each element of \u00b5B is i.i.d. Uniform(\u22120.5, 0.5); for each j, the local factor loadings {\u03bbji1, . . . , \u03bbjirj}i\u2264pj were drawn from N (\u00b5\u03bbj , Irj ), where each element of \u00b5\u03bbj is i.i.d. Uniform(\u22120.3, 0.3).\nWe generated the idiosyncratic errors as follows. Let D = diag(d21, . . . , d 2 p), where each {di} was generated independently from Gamma (\u03b1, \u03b2) with \u03b1 = \u03b2 = 100. We set s = (s1, . . . , sp) \u2032 to be a sparse vector, where each si was drawn from N (0, 1) with probability m\u221a p log p , and si = 0 otherwise. Then, we set a sparse error covariance matrix as \u03a3u = D + ss\u2032 \u2212 diag{s21, . . . , s2p}. In the simulation, we generated \u03a3u until it is positive definite. Note that varying m > 0 enables us to control the sparsity level, and we chose m = 0.3. Finally, we generated {ut}t\u2264T from i.i.d. N (0,\u03a3u).\nIn this simulation study, we chose the number of periods T = 300 and the numbers of factors as k = 3 and rj = 2 for each j. Then, we considered two cases: (i) increasing p from 60 to 600 in increments of 30 with a fixed J = 10 (i.e., each pj = p/10), and (ii) increasing J from 2 to 20 with a fixed pj = 30. Each simulation is replicated 200 times.\nFor comparison, we employed Double-POET, POET, and the sample covariance matrix (SamCov) to estimate the true covariance matrix of y,\u03a3. The estimation errors are measured in the following norms: \u2016\u03a3\u0302 \u2212 \u03a3\u2016\u03a3, \u2016\u03a3\u0302 \u2212 \u03a3\u2016max, and \u2016(\u03a3\u0302)\u22121 \u2212 \u03a3\u22121\u2016, where \u03a3\u0302 is one of the covariance matrix estimators. For the POET estimation, we estimated the covariance matrix with two different numbers of factors: (i) POET uses the k number of factors, and (ii) POET2 uses k + r factors, where r = J \u00d7 rj. For Double-POET, we considered two different cases: (i) D-POET with the known group membership, and (ii) D-POET(RSC) suggested in Section 3.5 with the unknown group membership. The proposed numbers of global and local factors estimation method in Section 3.4 is applied with kmax = 10 + r and rj,max = 10 for each estimation. In addition, we employed the soft thresholding scheme for both POET and Double-POET.\nFigures 1 and 2 plot the averages of \u2016\u03a3\u0302 \u2212 \u03a3\u2016\u03a3, \u2016\u03a3\u0302 \u2212 \u03a3\u2016max and \u2016(\u03a3\u0302)\u22121 \u2212 \u03a3\u22121\u2016 from different methods against p and J , respectively. From Figure 1, we find that D-POET performs the best. When comparing the POET-type procedures, POET2 performs better than POET. This may be because POET ignores important local factors. D-POET(RSC)\nwith the unknown group membership performs better than POET2 under different norms. This confirms that the RSC method in Section 3.5 can detect the membership well. Under the max norm, all estimators except POET perform roughly the same. This is because the thresholding or imposing the local factor structure affects mainly the elements of the covariance matrix that are nearly zero, and the elementwise norm depicts the magnitude of the largest elementwise absolute error. Figure 2 shows the similar results except when J is extremely small. This may be because for the small J (J = 2), we have weak global factors rahter than local factors. We note that the estimation errors of POET2 increase as J grows because the estimator includes more noises on the off-diagonal blocks of the local factor component. The above results support the theoretical findings presented in Section 3.\nWe further explored the performance of Double-POET when the group membership is misclassified. In Figure 3, we report the average errors under different norms against the misclassified rate of the group membership for Double-POET(Mix) with J = 20 and pj = 20. Figure 3 implies that Double-POET method performs better than POET2 unless the misclassification rate is greater than 3%.\nWe now demonstrate the blessing of dimensionality using Double-POET when estimating the local covariance matrix \u03a3j and its inverse. We generated the data as above. The SamCov and POET estimators are obtained using the group sample (i.e., T \u00d7pj observation matrix). For the POET method, we used the number of factors as k + rj = 5 in each estimation. The Double-POET and POET2 estimators for \u03a3j are the jth diagonal block of \u03a3\u0302 D and \u03a3\u0302 T 2 , respectively. We calculated \u2016\u03a3\u0302j \u2212\u03a3j\u2016\u03a3j and \u2016(\u03a3\u0302 j )\u22121 \u2212 (\u03a3j)\u22121\u2016 for j = 1. Note that we do not present the results of max norm, \u2016\u03a3\u0302j \u2212 \u03a3j\u2016max, as all estimators perform very similar to that shown in Figure 1.\nFigure 4 depicts the averages of \u2016\u03a3\u0302j \u2212\u03a3j\u2016\u03a3j and \u2016(\u03a3\u0302 j )\u22121 \u2212 (\u03a3j)\u22121\u2016 for Double-POET, POET, and the sample covariance matrix against pj with a fixed J = 10, while Figure 5 plots their average errors against J with fixed pj = 30. Figures 4 and 5 show that DoublePOET has smaller estimation errors than other methods under different norms. In Figure 4, Double-POET significantly outperforms POET when pj is small, while the estimation error gap between Double-POET and POET decreases as pj grows. The estimation error of POET2 under the relative Frobenius norm tends to increase as pj grows. This is because, even though the global factor component can be estimated more accurately, the estimator includes redundant information on the local factor part. In contrast, Figure 5 shows that, except when J is very small, the Double-POET method constantly dominates the other methods. This might be because by using other local groups\u2019 information, Double-POET\ncan estimate global factors better than POET, especially when pj is small. That is, the proposed Double-POET enjoys the blessing of dimensionality, which corresponds to the theoretical analysis discussed in Section 3.3.\nWe also examined the performance of the modified eigenvalue ratio (MER) estimator introduced in Section 3.4 for the number of global factors. For comparison, we employed alternative estimators: the BIC3 estimator of Bai and Ng (2002), the ED estimator of Onatski (2010), the ER estimator of Ahn and Horenstein (2013), and the estimator of Alessi et al. (2010) (ABC). We used the same data generating process as before but considered different number of global factors, k \u2208 {0, 3, 6}, and sample sizes, p \u2208 {100, 200, 300} and T \u2208 {150, 300}. We set kmax = 10 + r and \u03d5p = 0.3 log p for MER estimator, and kmax = 10 for the other estimators. We calculated the average of k\u0302 and the percentage correctly determining k over 500 replications.\nThe results are reported in Table 1. We find that the proposed MER method performs the best across the different number of global factors. In particular, when k = 0, MER and ED tend to estimate correctly, while other estimators tend to overestimate k. We note that ER does not include the case of k = 0. When k > 0, all estimators except BIC3 perform well, but MER and ER slightly outperform ED and ABC. MER often underestimates k if both p and T are small, but it selects k correctly as p grows. The results demonstrate that the proposed MER method is appropriate for the model selection problem for both the\nmulti-level factor model and the single-level factor model."
        },
        {
            "heading": "5 Application to Portfolio Allocation",
            "text": "In this section, we applied the proposed Double-POET method to a minimum variance portfolio allocation study using global stock data. We collected daily transaction prices of international stock markets over 20 countries by the total market capitalization from January 2, 2016, to December 31, 2021. We selected top 100 firms at most for each country based on the market cap and used weekly log-returns to mitigate the effect of different trading hours.\nWe excluded stocks with missing returns and no variation in this period. This operation leads to total 1892 stocks for this period. The distribution of our sample is presented in Table 2.\nWe calculated the Double-POET, POET, and SamCov estimators for each month. In both Double-POET and POET procedures, we estimated the idiosyncratic volatility matrix based on 11 Global Industrial Classification Standard (GICS) sectors (Ait-Sahalia and Xiu, 2017; Fan et al., 2016). For example, the idiosyncratic components for the different sectors were set to zero, and we maintained these for the same sector. This location-based thresholding preserves positive definiteness and corresponds to the hard-thresholding scheme with sector information. We varied the number of (global) factors k from 1 to 5 for both DoublePOET and POET. To determine the number of local factors for Double-POET, we used the eigenvalue ratio method suggested by Ahn and Horenstein (2013) with rj,max = 10. We also considered POET2 using k + \u221120\nj=1 r\u0302j number of factors from the best performing Double-\nPOET estimator.\nTo analyze the out-of-sample portfolio allocation performance, we considered the follow-\ning constrained minimum variance portfolio allocation problem (Fan et al., 2012; Jagannathan and Ma,\n2003):\nmin \u03c9\n\u03c9T \u03a3\u0302\u03c9, subject to \u03c9\u22a41 = 1, \u2016\u03c9\u20161 \u2264 c,\nwhere 1 = (1, . . . , 1)\u22a4 \u2208 Rp, the gross exposure constraint c was varied from 1 to 4, and \u03a3\u0302 is one of the volatility matrix estimators from Double-POET, POET, and SamCov. We constructed the portfolio at the beginning of each month, based on the stock weights calculated using the data from the past 24 months (T = 104). We then held the portfolio for one month and calculated the square root of the realized volatility using the weekly portfolio log-returns. Their average was used for the out-of-sample risk. We considered five out-of-sample periods: 2018, 2019, 2020, 2021 and the whole period (2018-2021).\nFigure 6 depicts the out-of-sample risks of the portfolios constructed by SamCov, POET, POET2, Double-POET, and Double-POET(RSC) with k = 5 against the exposure con-\nstraint. From Figure 6, we find that Double-POET outperforms POET for the same number of factors k, while k = 3 and k = 5 yields the best performances for Double-POET and POET, respectively. Double-POET(k = 3) reduces the minimum risks by 4.3%\u20136.7% compared to POET(k = 5). We confirmed that for the purpose of portfolio allocation based on the global stock market, the Double-POET based on the global and national factor model outperforms POET based on the single-level factor model. Thus, we can conjecture that incorporating the latent global and national factor models helps account for the global market dynamics. We note that Double-POET(RSC) does not perform well due to misclassified group membership. One possible explanation is that there might be other type of local factors in addition to the nation-specific factors. For example, if the industrial risk and the national risk are nested on the idiosyncratic part after removing the global factors, the suggested RSC method cannot properly detect only the national group membership. This is an interesting research topic to control the unknown nested local groups, so we leave it for a future study.\nWe also conducted the country-wise volatility matrix estimation based on SamCov, POET, POET2, and Double-POET and applied them to the same portfolio allocation problem as before. Specifically, we used the sample of each country for POET and SamCov procedures. The Double-POET and POET2 estimators for each local covariance matrix are obtained by extracting each diagonal block of the large Double-POET and POET2 estimators, respectively. Figure 7 presents the portfolio behavior of the top 100 stocks in the US. Double-POET shows stable results and reduces the minimum risks by 1.8%\u20134.0% compared to POET except the period 2021. We note that a powerful bull market lasted in 2021, and the risks could be sufficiently explained by only the first principal component (i.e., the market factor), so that, POET(k = 1) performs the best in this period. Nevertheless, the overall results indicate that the proposed Double-POET method enjoys the blessing of dimensionality.\nFigure 8 shows the results of other 19 countries for the whole period. Except for five\ncountries (GB, IN, IT, TH, and ZA), Double-POET outperforms POET. We note that the SamCov estimator sometimes outperforms others for a few countries. Overall, these results indicate that Double-POET can accurately estimate the global factors by harnessing other countries\u2019 observations."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper proposes a novel large volatility matrix inference procedure based on the latent global and national factor models. We show the asymptotic behaviors of the proposed Double-POET method and discuss its blessing of dimensionality and efficiency of estimating a large volatility matrix compared to the regular POET procedure. To determine the number of global factors, we extend the eigenvalue ratio procedure (Ahn and Horenstein, 2013). In\naddition, when the membership of the local factors is unknown, we suggest the regularized spectral clustering method to find the latent local structure.\nIn the empirical study, in terms of portfolio allocation, the proposed estimator shows the best performance. It confirms the presence of the national factor structure in global financial markets, which provides the theoretical basis for employing the Double-POET method. In addition, for the country-wise covariance matrix estimation, the Double-POET procedure yields the blessing of dimensionality by accurately estimating the latent global factors using the information outside the local group.\nIn this paper, we focus on the national risk factor as only the local factor. However, in practice, there could be other types of risk factors that are nested in the local level. Thus, it is interesting and important to develop a large volatility matrix estimation procedure based on unknown-membership local factor models with the nested local-level group factors. We leave this for a future study."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors thank the Editor Professor Torben Andersen, the Associate Editor, and two referees for their careful reading of this paper and valuable comments. The research of Donggyu Kim was supported in part by the National Research Foundation of Korea (NRF) (2021R1C1C1003216)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Proof of Theorem 3.1\nWe first provide useful lemmas below. Let {\u03b4i, vi}pi=1 be the eigenvalues and their corresponding eigenvectors of \u03a3 in decreasing order. Let {\u03b4\u0304i, v\u0304i}ki=1 be the leading eigenvalues and eigenvectors of BB\u2032 and the rest zero. Similarly, for each group j, let {\u03baji , \u03b7ji } pj i=1 be the eigenvalues and eigenvectors of \u03a3jE in decreasing order, and {\u03ba\u0304ji , \u03b7\u0304ji } rj i=1 for \u039b j\u039bj \u2032 .\nBy Weyl\u2019s theorem, we have the following lemma under the pervasive conditions.\nLemma A.1. Under Assumption 3.1(i), we have\n|\u03b4i \u2212 \u03b4\u0304i| \u2264 \u2016\u03a3E\u2016 for i \u2264 k, |\u03b4i| \u2264 \u2016\u03a3E\u2016 for i > k,\nand, for i \u2264 k, \u03b4\u0304i/pa1 is strictly bigger than zero for all p. In addition, for each group j, we have\n|\u03baji \u2212 \u03ba\u0304ji | \u2264 \u2016\u03a3ju\u2016 for i \u2264 rj, |\u03baji | \u2264 \u2016\u03a3ju\u2016 for i > rj,\nand, for i \u2264 rj, \u03ba\u0304ji/pa2j is strictly bigger than zero for all pj.\nThe following lemma presents the individual convergence rate of leading eigenvectors\nusing Lemma A.1 and the l\u221e norm perturbation bound theorem of Fan et al. (2018b).\nLemma A.2. Under Assumption 3.1(i), we have the following results.\n(i) We have\nmax i\u2264k \u2016v\u0304i \u2212 vi\u2016\u221e \u2264 C \u2016\u03a3E\u2016\u221e p3(a1\u2212 1 2 ) .\n(ii) For each group j, we have\nmax i\u2264rj \u2016\u03b7\u0304ji \u2212 \u03b7ji \u2016\u221e \u2264 C \u2016\u03a3ju\u2016\u221e p 3(a2\u2212 12 ) j .\nProof. (i) Let B = (b\u03031, . . . , b\u0303k). Then, for i \u2264 k, \u03b4\u0304i = \u2016b\u0303i\u20162 \u224d pa1 from Lemma A.1 and v\u0304i = b\u0303i/\u2016b\u0303i\u2016. Hence, \u2016v\u0304i\u2016\u221e \u2264 \u2016B\u2016max/\u2016b\u0303i\u2016 \u2264 C/ \u221a pa1 . In addition, for V\u0303 = (v\u03041, . . . , v\u0304k), the coherence \u00b5(V\u0303) = pmaxi \u2211k j=1 V\u0303 2 ij/k \u2264 Cp1\u2212a1 , where V\u0303ij is the (i, j) entry of V\u0303. Thus, by Theorem 1 of Fan et al. (2018b), we have\nmax i\u2264k \u2016v\u0304i \u2212 vi\u2016\u221e \u2264 Cp2(1\u2212a1) \u2016\u03a3E\u2016\u221e \u03b3\u0304 \u221a p ,\nwhere the eigengap \u03b3\u0304 = min{\u03b4\u0304i \u2212 \u03b4\u0304i+1 : 1 \u2264 i \u2264 k} and \u03b4k+1 = 0. By the similar argument, we can show the result (ii).\nLemma A.3. Under Assumption 3.1, for i \u2264 k, we have\n|\u03b4\u0302i/\u03b4i \u2212 1| = OP (p1\u2212a1 \u221a log p/T ),\n\u2016v\u0302i \u2212 vi\u2016\u221e = OP  p3(1\u2212a1) \u221a log p\nTp + \u2016\u03a3E\u2016\u221e p3(a1\u2212 1 2 )\n  .\nProof. The first statement is followed by Assumption 3.1 and Weyl\u2019s theorem. We have\n\u03a3\u0302 = BB\u2032 +\u039b\u039b\u2032 +\u03a3u + (\u03a3\u0302\u2212\u03a3) = BB\u2032 +\u03a3E + (\u03a3\u0302\u2212\u03a3).\nWe can treat BB\u2032 as a low rank matrix and the remaining terms as a perturbation matrix. By Theorem 1 of Fan et al. (2018b), Lemma A.2 and Assumption 3.1, we have\n\u2016v\u0302i \u2212 vi\u2016\u221e \u2264 Cp2(1\u2212a1) \u2016\u03a3E + (\u03a3\u0302\u2212\u03a3)\u2016\u221e\npa1 \u221a p\n\u2264 Cp2(1\u2212a1)\u2016\u03a3E\u2016\u221e pa1 \u221a p + Cp2(1\u2212a1) \u2016\u03a3\u0302\u2212\u03a3\u2016max pa1\u22121 \u221a p\n= OP  \n\u2016\u03a3E\u2016\u221e p3(a1\u2212 1 2 ) + p3(1\u2212a1)\n\u221a log p\nTp\n  .\nLemma A.4. Under the assumptions of Theorem 3.1, for i \u2264 rj, we have\n|\u03ba\u0302ji/\u03baji \u2212 1| = OP ( p 5 2 (1\u2212a1)+c(1\u2212a2) \u221a log p/T + 1/p 5a1 2 \u2212 3 2 \u22122c+ca2 ) ,\n\u2016\u03b7\u0302ji \u2212 \u03b7ji \u2016\u221e = OP  p 5 2 (1\u2212a1)+c( 52\u22123a2) \u221a log p\nT +\n1\np 5a1 2 \u2212 3 2 +c(3a2\u2212 72 )\n+ mp\np3c(a2\u2212 1 2 )\n  .\nProof. We have\n\u2016\u03a3E\u2016 \u2264 \u2016\u039b\u039b\u2032\u2016+ \u2016\u03a3u\u2016 \u2264 \u2016\u039b\u039b\u2032\u2016+O(mp) = O(pca2).\nLet BB\u2032 = V\u0303\u0393\u0303V\u0303 \u2032 , where \u0393\u0303 = diag(\u03b4\u03041, . . . , \u03b4\u0304k) and their corresponding leading k eigenvectors V\u0303 = (v\u03041, . . . , v\u0304k). Also, we let \u0393 = diag(\u03b41, . . . , \u03b4k) and the corresponding eigenvectors V = (v1, . . . , vk) of covariance matrix \u03a3. Note that \u2016B\u2016max = \u2016V\u0303\u0393\u0303 1/2\u2016max = O(1) and \u2016\u03a3E\u2016\u221e = O(pc). By Lemmas A.1-A.2, we have\n\u2016V\u0393 12 \u2212 V\u0303\u0393\u0303 1 2\u2016max \u2264 \u2016B\u0393\u0303 \u2212 1 2 (\u0393 1 2 \u2212 \u0393\u0303 1 2 )\u2016max + \u2016(V\u2212 V\u0303)\u0393 1 2\u2016max\n\u2264 C \u2016\u03a3E\u2016 pa1 + C \u2016\u03a3E\u2016\u221e \u221a p5a1\u22123 = o (1) .\nHence, we have \u2016V\u0393 12\u2016max = O(1) and \u2016V\u2016max = O(1/ \u221a pa1). By this fact and the results from Lemmas A.1-A.3, we have\n\u2016V\u0303\u0393\u0303V\u0303\u2032 \u2212V\u0393V\u2032\u2016max \u2264 \u2016V\u0303(\u0393\u0303\u2212 \u0393)V\u0303 \u2032\u2016max + \u2016(V\u0303\u2212V)\u0393(V\u0303\u2212V)\u2032\u2016max + 2\u2016V\u0393(V\u0303\u2212V)\u2032\u2016max\n= O(p\u2212a1\u2016\u0393\u0303\u2212 \u0393\u2016max + \u221a pa1\u2016V\u0303\u2212V\u2016max) = O(1/p 5a1 2 \u2212 3 2 \u2212c),\n\u2016V\u0302\u0393\u0302V\u0302\u2032 \u2212V\u0393V\u2032\u2016max \u2264 \u2016V\u0302(\u0393\u0302\u2212 \u0393)V\u0302 \u2032\u2016max + \u2016(V\u0302\u2212V)\u0393(V\u0302\u2212V)\u2032\u2016max + 2\u2016V\u0393(V\u0302\u2212V)\u2032\u2016max\n= OP (p \u2212a1\u2016\u0393\u0302\u2212 \u0393\u2016max + \u221a pa1\u2016V\u0302\u2212V\u2016max) = OP (p 5 2 (1\u2212a1) \u221a log p/T + 1/p 5a1 2 \u2212 3 2 \u2212c).\nThus, we have\n\u2016V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032\u2016max = OP (p 5 2 (1\u2212a1) \u221a log p/T + 1/p 5a1 2 \u2212 3 2 \u2212c). (A.1)\nThen, we have\n\u2016\u03a3\u0302E \u2212\u03a3E\u2016max \u2264 \u2016\u03a3\u0302\u2212\u03a3\u2016max + \u2016V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032\u2016max\n= OP (p 5 2 (1\u2212a1) \u221a log p/T + 1/p 5a1 2 \u2212 3 2 \u2212c). (A.2)\nTherefore, the first statement is followed by (A.2) and the Weyl\u2019s theorem.\nWe decompose the sample covariance matrix \u03a3\u0302 j\nE for each group j as follows:\n\u03a3\u0302 j E = \u039b j\u039bj\u2032 +\u03a3ju + (\u03a3\u0302 j E \u2212\u03a3jE).\nThen, by Theorem 1 of Fan et al. (2018b), Lemma A.2 and (A.2), we have\n\u2016\u03b7\u0302ji \u2212 \u03b7ji \u2016\u221e \u2264 Cp2(1\u2212a2)j \u2016\u03a3ju + (\u03a3\u0302\nj\nE \u2212\u03a3jE)\u2016\u221e pa2j \u221a pj\n\u2264 Cp2(1\u2212a2)j \u2016\u03a3ju\u2016\u221e pa2j \u221a pj + Cp 2(1\u2212a2) j \u2016\u03a3\u0302jE \u2212\u03a3jE\u2016max pa2\u22121j \u221a pj\n= OP  p 5 2 (1\u2212a1)+c( 52\u22123a2) \u221a log p\nT +\n1\np 5a1 2 \u2212 3 2 +c(3a2\u2212 72 )\n+ mp\np3c(a2\u2212 1 2 )\n  .\nProof of Theorem 3.1. We first consider (3.1). By definition, \u2016\u03a3\u0302Du \u2212 \u03a3\u0302u\u2016max = maxij |sij(\u03c3\u0302ij)\u2212\u03c3\u0302ij| \u2264 maxij \u03c4ij = OP (\u03c4). Hence, it suffices to show \u2016\u03a3\u0302u\u2212\u03a3u\u2016max = OP (\u03c9T ). We have\n\u2016\u03a3\u0302u \u2212\u03a3u\u2016max \u2264 \u2016\u03a3\u0302\u2212\u03a3\u2016max + \u2016V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032\u2016max + \u2016\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2212\u039b\u039b\u2032\u2016max.\nBy Assumption 3.1(iii) and (A.1), we have\n\u2016\u03a3\u0302\u2212\u03a3\u2016max + \u2016V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032\u2016max = OP (p 5 2 (1\u2212a1) \u221a log p/T + 1/p 5a1 2 \u2212 3 2 \u2212c). (A.3)\nFor \u2016\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032\u2016max, we have\n\u2016\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032\u2016max = max j \u2016\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj \u2032\u2016max.\nFor each group j, let \u039bj\u039bj \u2032 = \u03a6\u0303j\u03a8\u0303j\u03a6\u0303j\u2032, where \u03a8\u0303j = diag(\u03ba\u0304j1, . . . , \u03ba\u0304 j rj ) and the corresponding eigenvectors \u03a6\u0303j = (\u03b7\u03041, . . . , \u03b7\u0304rj). In addition, let \u03a8 j = diag(\u03baj1, . . . , \u03ba j rj ) and \u03a6j = (\u03b71, . . . , \u03b7rj) to be the leading eigenvalues and the corresponding eigenvectors of \u03a3jE , respectively. Then, we have\n\u2016\u03a6j\u03a8j 1 2 \u2212 \u03a6\u0303j\u03a8\u0303j 1 2\u2016max \u2264 \u2016\u039bj\u03a8\u0303j \u2212 1 2 (\u03a8j 1 2 \u2212 \u03a8\u0303j 1 2 )\u2016max + \u2016(\u03a6j \u2212 \u03a6\u0303j)\u03a8j 1 2\u2016max\n\u2264 \u2016\u03a3 j u\u2016\npa2j + \u2016\u03a3ju\u2016\u221e\u221a p5a2\u22123j = o(1). (A.4)\nSince \u2016\u039bj\u2016max = \u2016\u03a6\u0303j\u03a8\u0303j 1 2\u2016max = O(1), \u2016\u03a6j\u03a8j 1 2\u2016max = O(1) and \u2016\u03a6j\u2016max = O(1/ \u221a pa2j ). Using this fact and results from Lemmas A.1, A.2 and A.4, we can show\n\u2016\u03a6\u0303j\u03a8\u0303j\u03a6\u0303j\u2032 \u2212 \u03a6j\u03a8j\u03a6j\u2032\u2016max \u2264 O(p\u2212a2j \u2016\u03a8\u0303j \u2212\u03a8j\u2016max + \u221a pa2j \u2016\u03a6\u0303j \u2212 \u03a6j\u2016max) = O(mp/ \u221a pc(5a2\u22123)), \u2016\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u03a6j\u03a8j\u03a6j\u2032\u2016max \u2264 OP (p\u2212a2j \u2016\u03a8\u0302j \u2212\u03a8j\u2016max + \u221a pa2j \u2016\u03a6\u0302j \u2212 \u03a6j\u2016max)\n= OP (p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T + 1/p 5 2 a1\u2212 32+c( 5 2 a2\u2212 72 ) +mp/ \u221a pc(5a2\u22123)).\nBy using these rates, we obtain\n\u2016\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032\u2016max = OP (p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T + 1/p 5 2 a1\u2212 32+c( 5 2 a2\u2212 72 ) +mp/ \u221a pc(5a2\u22123)).\n(A.5)\nBy (A.3) and (A.5), we have\n\u2016\u03a3\u0302u \u2212\u03a3u\u2016max = OP ( p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p\nT +\n1\np 5 2 a1\u2212 32+c( 5 2 a2\u2212 72 )\n+ mp\u221a\npc(5a2\u22123)\n) .\nTherefore, \u2016\u03a3\u0302Du \u2212\u03a3u\u2016max = OP (\u03c4 + \u03c9T ) = OP (\u03c9T ), when \u03c4 is chosen as the same order of \u03c9T = p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T + 1/p 5 2 a1\u2212 32+c( 5 2 a2\u2212 72 ) +mp/ \u221a pc(5a2\u22123).\nConsider (3.2). Similar to the proofs of Theorem 2.1 in Fan et al. (2011), we can show \u2016\u03a3\u0302Du \u2212 \u03a3u\u20162 = OP (mp\u03c91\u2212qT ). In addition, since \u03bbmin(\u03a3u) > c1 and mp\u03c91\u2212qT = o(1), the minimum eigenvalue of \u03a3\u0302 D u is strictly bigger than 0 with probability approaching 1. Then, we have \u2016(\u03a3\u0302Du )\u22121 \u2212\u03a3\u22121u \u20162 \u2264 \u03bbmin(\u03a3u)\u22121\u2016\u03a3\u0302 D u \u2212\u03a3u\u20162\u03bbmin(\u03a3\u0302 D u ) \u22121 = OP (mp\u03c9 1\u2212q T ).\nConsider (3.3). By the results of (A.1), (A.5) and (3.1), we have\n\u2016\u03a3\u0302D \u2212\u03a3\u2016max \u2264 \u2016V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032\u2016max + \u2016\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2212\u039b\u039b\u2032\u2016max + \u2016\u03a3\u0302 D u \u2212\u03a3u\u2016max = OP (\u03c9T ).\nConsider (3.4). Define \u03a3\u0302 D E = \u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 + \u03a3\u0302 D u . We first show that \u2016(\u03a3\u0302 D E) \u22121 \u2212 \u03a3\u22121E \u2016 =\nOP (mp\u03c9 1\u2212q T ). Let J\u0302 = \u03a8\u0302\n1 2 \u03a6\u0302 \u2032 (\u03a3\u0302 D u ) \u22121\u03a6\u0302\u03a8\u0302 1 2 and J\u0303 = \u03a8\u0303 1 2 \u03a6\u0303 \u2032 \u03a3\u22121u \u03a6\u0303\u03a8\u0303 1 2 . Using the Sherman-\nMorrison-Woodbury formula, we have\n\u2016(\u03a3\u0302DE)\u22121 \u2212\u03a3\u22121E \u2016 \u2264 \u2016(\u03a3\u0302 D u ) \u22121 \u2212\u03a3\u22121u \u2016+\u22061,\nwhere \u22061 = \u2016(\u03a3\u0302 D u ) \u22121\u03a6\u0302\u03a8\u0302 1 2 (Ir + J\u0302) \u22121\u03a8\u0302 1 2 \u03a6\u0302 \u2032 (\u03a3\u0302 D u ) \u22121 \u2212 \u03a3\u22121u \u03a6\u0303\u03a8\u0303 1 2 (Ir + J\u0303) \u22121\u03a8\u0303 1 2 \u03a6\u0303 \u2032 \u03a3\u22121u \u2016. Then, the right hand side can be bounded by following terms:\nL1 = \u2016((\u03a3\u0302 D u ) \u22121 \u2212\u03a3\u22121u )\u03a6\u0303\u03a8\u0303 1 2 (Ir + J\u0303) \u22121\u03a8\u0303 1 2 \u03a6\u0303 \u2032 \u03a3\u22121u \u2016, L2 = \u2016\u03a3\u22121u (\u03a6\u0302\u03a8\u0302 1 2 \u2212 \u03a6\u0303\u03a8\u0303 1 2 )(Ir + J\u0303) \u22121\u03a8\u0303 1 2 \u03a6\u0303 \u2032 \u03a3\u22121u \u2016, L3 = \u2016\u03a3\u22121u \u03a6\u0303\u03a8\u0303 1 2 ((Ir + J\u0302) \u22121 \u2212 (Ir + J\u0303)\u22121)\u03a8\u0303 1 2 \u03a6\u0303 \u2032 \u03a3\u22121u \u2016.\nBy Lemma A.4, \u2016\u03a6j\u03a8j 1 2 \u2212 \u03a6\u0302j\u03a8\u0302j 1 2\u2016max \u2264 \u2016\u039bj\u03a8\u0302j \u2212 1 2 (\u03a8j 1 2 \u2212 \u03a8\u0302j 1 2 )\u2016max + \u2016(\u03a6j \u2212 \u03a6\u0302j)\u03a8j 1 2\u2016max =\nOP (\u03c9T ), and by (A.4) and (3.2), we then have\n\u2016\u03a6\u0303\u03a8\u0303 1\n2\u2016 \u2264 max j\n\u2016\u03a6\u0303j\u03a8\u0303j 1 2\u2016 = OP ( \u221a pc),\n\u2016\u03a6\u0302\u03a8\u0302 1 2 \u2212 \u03a6\u0303\u03a8\u0303 1\n2\u2016 \u2264 max j\n\u221a pc\u2016\u03a6\u0302j\u03a8\u0302j 1 2 \u2212 \u03a6\u0303j\u03a8\u0303j 1 2\u2016max = OP (\u221a pc\u03c9T ) ,\nand\n\u2016J\u0302\u2212 J\u0303\u2016 \u2264 \u2016(\u03a8\u0302 1 2 \u03a6\u0302 \u2032 \u2212 \u03a8\u0303 1 2 \u03a6\u0303 \u2032 )(\u03a3\u0302 D u ) \u22121(\u03a6\u0302\u03a8\u0302 1 2 \u2212 \u03a6\u0303\u03a8\u0303 1 2 )\u2016\n+ \u2016(\u03a8\u0302 1 2 \u03a6\u0302 \u2032 \u2212 \u03a8\u0303 1 2 \u03a6\u0303 \u2032 )(\u03a3\u0302 D u ) \u22121\u03a6\u0303\u03a8\u0303 1 2\u2016+ \u2016\u03a8\u0303 1 2 \u03a6\u0303 \u2032 ((\u03a3\u0302 D u ) \u22121 \u2212\u03a3\u22121u )\u03a6\u0303\u03a8\u0303 1 2\u2016\n= OP (p cmp\u03c9 1\u2212q T ).\nSince \u03bbmin(Ir + J\u0303) \u2265 \u03bbmin(J\u0303) \u2265 \u03bbmin(\u03a3\u22121u )\u03bb2min(\u03a6\u0303\u03a8\u0303 1 2 ) \u2265 Cpc, we have \u2016(Ir + J\u0303)\u22121\u2016 = OP (1/p c). Then, L1 = OP (mp\u03c9 1\u2212q T ) by (3.2). In addition, L2 = OP (p \u2212c/2\u2016\u03a6\u0302\u03a8\u0302 1 2 \u2212 \u03a6\u0303\u03a8\u0303 1 2\u2016) ="
        },
        {
            "heading": "OP (\u03c9T ) and L3 = OP (p",
            "text": "c\u2016(Ir + J\u0302)\u22121 \u2212 (Ir + J\u0303)\u22121\u2016) = OP (p\u2212c\u2016J\u0302\u2212 J\u0303\u2016) = OP (mp\u03c91\u2212qT ). Thus, we have\n\u22061 = OP (mp\u03c9 1\u2212q T ), (A.6)\nwhich yields \u2016(\u03a3\u0302DE)\u22121 \u2212\u03a3\u22121E \u2016 = OP (mp\u03c91\u2212qT ).\nLet H\u0302 = \u0393\u0302 1 2 V\u0302 \u2032 (\u03a3\u0302 D E) \u22121V\u0302\u0393\u0302 1 2 and H\u0303 = \u0393\u0303 1 2 V\u0303 \u2032 \u03a3\u22121E V\u0303\u0393\u0303 1 2 . Using the Sherman-Morrison-\nWoodbury formula again, we have\n\u2016(\u03a3\u0302D)\u22121 \u2212\u03a3\u22121\u2016 \u2264 \u2016(\u03a3\u0302DE)\u22121 \u2212\u03a3\u22121E \u2016+\u22062,\nwhere \u22062 = \u2016(\u03a3\u0302 D E) \u22121V\u0302\u0393\u0302 1 2 (Ik + H\u0302) \u22121\u0393\u0302 1 2 V\u0302 \u2032 (\u03a3\u0302 D E) \u22121 \u2212 \u03a3\u22121E V\u0303\u0393\u0303 1 2 (Ik + H\u0303) \u22121\u0393\u0303 1 2 V\u0303 \u2032 \u03a3\u22121E \u2016. By Weyl\u2019s inequality, we have \u03bbmin(\u03a3E) > c since \u03bbmin(\u03a3u) > c and \u03bbmin(\u039b\u039b \u2032) = 0. Hence, \u2016\u03a3\u22121E \u2016 = OP (1). By Lemmas A.1-A.3, we have \u2016V\u0302\u0393\u0302 1 2 \u2212 V\u0303\u0393\u0303 1 2\u2016max = OP (p 5 2 (1\u2212a1) \u221a log p/T + 1/p 5 2 a1\u2212 32\u2212c). Similar to the proof of (A.6), we can show \u22062 = OP (mp\u03c9 1\u2212q T ). Therefore, we have \u2016(\u03a3\u0302D)\u22121 \u2212\u03a3\u22121\u2016 = OP (mp\u03c91\u2212qT ).\nConsider (3.5). We derive the rate of convergence for \u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3. The SVD decomposi-\ntion of \u03a3 is\n\u03a3 = (Vp\u00d7k \u03a6p\u00d7r \u2126p\u00d7(p\u2212k\u2212r))   \u0393k\u00d7k \u03a8r\u00d7r\n\u0398(p\u2212k\u2212r)\u00d7(p\u2212k\u2212r)\n    V\u2032 \u03a6\u2032\n\u2126\u2032\n  .\nNote that \u2126 is used to denote the precision matrix in Section 2.2. Moreover, since all the eigenvalues of \u03a3 are strictly bigger than 0, for any maxtrix A, we have \u2016A\u20162\u03a3 = OP (p \u22121)\u2016A\u20162F . Then, we have\n\u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 \u2264 p\u22121/2 ( \u2016\u03a3\u22121/2(V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032)\u03a3\u22121/2\u2016F\n+ \u2016\u03a3\u22121/2(\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032)\u03a3\u22121/2\u2016F + \u2016\u03a3\u22121/2(\u03a3\u0302 D u \u2212\u03a3u)\u03a3\u22121/2\u2016F\n)\n=: \u2206G +\u2206L +\u2206S\nand\n\u2206S = OP (p \u22121/2\u2016\u03a3\u0302Du \u2212\u03a3u\u2016F ) = OP (\u2016\u03a3\u0302 D u \u2212\u03a3u\u20162) = OP (mp\u03c91\u2212qT ).\nWe have\n\u2206G = p \u22121/2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225   \u0393\u2212 1 2V\u2032 \u03a8\u2212 1 2\u03a6\u2032 \u0398\u2212 1 2\u2126\u2032   (V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032) ( V\u0393\u2212 1 2 \u03a6\u03a8\u2212 1 2 \u2126\u0398\u2212 1 2 ) \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 F\n\u2264 p\u22121/2 ( \u2016\u0393\u22121/2V\u2032(V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032)V\u0393\u22121/2\u2016F + \u2016\u03a8\u22121/2\u03a6\u2032(V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032)\u03a6\u03a8\u22121/2\u2016F\n+ \u2016\u0398\u22121/2\u2126\u2032(V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032)\u2126\u0398\u22121/2\u2016F + 2\u2016\u0393\u22121/2V\u2032(V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032)\u03a6\u03a8\u22121/2\u2016F + 2\u2016\u0393\u22121/2V\u2032(V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032)\u2126\u0398\u22121/2\u2016F + 2\u2016\u03a8\u22121/2\u03a6\u2032(V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032)\u2126\u0398\u22121/2\u2016F )\n=: \u2206G1 +\u2206G2 +\u2206G3 + 2\u2206G4 + 2\u2206G5 + 2\u2206G6.\nIn order to find the convergence rate of relative Frobenius norm, we consider the above terms separately. For \u2206G1, we have\n\u2206G1 \u2264 p\u22121/2 ( \u2016\u0393\u22121/2V\u2032(V\u0302\u0393\u0302V\u0302\u2032 \u2212V\u0393V\u2032)V\u0393\u22121/2\u2016F + \u2016\u0393\u22121/2V\u2032(V\u0393V\u2032 \u2212BB\u2032)V\u0393\u22121/2\u2016F )\n=: \u2206 (a) G1 +\u2206 (b) G1.\nWe bound the two terms separately. We have\n\u2206 (a) G1 \u2264 p\u22121/2 ( \u2016\u0393\u22121/2(V\u2032V\u0302\u2212 I)\u0393\u0302(V\u0302\u2032V\u2212 I)\u0393\u22121/2\u2016F + 2\u2016\u0393\u22121/2(V\u2032V\u0302\u2212 I)\u0393\u0302\u0393\u22121/2\u2016F\n+ \u2016(\u0393\u22121/2(\u0393\u0302\u2212 \u0393)\u0393\u22121/2\u2016F ) =: I + II + III.\nBy Lemma A.3, \u2016V\u2032V\u0302 \u2212 I\u2016F = \u2016V\u2032(V\u0302 \u2212 V)\u2016F \u2264 \u2016V\u0302 \u2212 V\u2016F = OP (p3(1\u2212a1) \u221a log p/T + 1/p3a1\u22122\u2212c). Then, II is of order OP (p 3(1\u2212a1)\u2212 12 \u221a log p/T + 1/p3(a1\u2212 1 2 )\u2212c) and I is of smaller order. In addition, we have III \u2264 \u2016\u0393\u22121/2(\u0393\u0302 \u2212 \u0393)\u0393\u22121/2\u2016 = OP (p1\u2212a1 \u221a log p/T ) by Lemma A.3. Thus, \u2206 (a) G1 = OP (p 1\u2212a1 \u221a log p/T + 1/p3(a1\u2212 1 2 )\u2212c). Similarly, we have\n\u2206 (b) G1 \u2264 p\u22121/2 ( \u2016\u0393\u22121/2(V\u2032V\u0303\u2212 I)\u0393\u0303(V\u0303\u2032V\u2212 I)\u0393\u22121/2\u2016F + 2\u2016\u0393\u22121/2(V\u2032V\u0303\u2212 I)\u0393\u0303\u0393\u22121/2\u2016F\n+ \u2016(\u0393\u22121/2(\u0393\u0303\u2212 \u0393)\u0393\u22121/2\u2016F ) =: I \u2032 + II \u2032 + III \u2032.\nBy sin \u03b8 theorem, \u2016V\u2032V\u0303 \u2212 I\u2016 = \u2016V\u2032(V\u0303 \u2212V)\u2016 \u2264 \u2016V\u0303 \u2212V\u2016 = O(\u2016\u03a3E\u2016/pa1). Then, we have II \u2032 = O(1/pa1\u2212ca2) and I \u2032 is of smaller order. By Lemma A.1, we have III \u2032 = O(1/pa1\u2212ca2). Thus, \u2206 (b) G1 = O(1/p a1\u2212ca2). Then, we obtain\n\u2206G1 = OP\n( p1\u2212a1 \u221a log p\nT +\n1\np3(a1\u2212 1 2 )\u2212c\n+ 1\npa1\u2212ca2\n) . (A.7)\nFor \u2206G3, we have\n\u2206G3 \u2264 p\u22121/2\u2016\u0398\u22121/2\u2126\u2032V\u0302\u0393\u0302V\u0302 \u2032 \u2126\u0398\u22121/2\u2016F + p\u22121/2\u2016\u0398\u22121/2\u2126\u2032V\u0303\u0393\u0303V\u0303 \u2032 \u2126\u0398\u22121/2\u2016F =: \u2206(a)G3 +\u2206 (b) G3.\nBy Lemma A.3, we have\n\u2016\u2126\u2032V\u0302\u2016F = \u2016\u2126\u2032(V\u0302\u2212V)\u2016F = O( \u221a p\u2016V\u0302\u2212V\u2016max) = OP (p3(1\u2212a1) \u221a log p/T + 1/p3a1\u22122\u2212c).\nSince \u2016\u0393\u0302\u2016 = OP (pa1), we have\n\u2206 (a) G3 \u2264 p\u22121/2\u2016\u0398\u22121\u2016\u2016\u2126\u2032V\u0302\u20162F\u2016\u0393\u0302\u2016 = OP (p11/2\u22125a1 log p/T + 1/p5a1\u22127/2\u22122c).\nSimilarly, \u2206 (b) G3 = OP (1/p 5a1\u22127/2\u22122c) because \u2016\u2126\u2032V\u0303\u2016F = O(\u221ap\u2016V\u0303\u2212V\u2016max) = OP (1/p3a1\u22122\u2212c) by Lemma A.2. Then, we obtain\n\u2206G3 = OP\n( p 11 2 \u22125a1 log p\nT +\n1\np5a1\u2212 7 2 \u22122c\n) .\nSimilarly, we can show that the terms \u2206G2, \u2206G4, \u2206G5 and \u2206G6 are dominated by \u2206G1 and \u2206G3. Therefore, we have\n\u2206G = OP\n( p1\u2212a1 \u221a log p\nT +\n1\npa1\u2212ca2 + p\n11 2 \u22125a1 log p\nT +\n1\np5a1\u2212 7 2 \u22122c\n) . (A.8)\nSimilarly, we consider\n\u2206L = p \u22121/2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225   \u0393\u2212 1 2V\u2032 \u03a8\u2212 1 2\u03a6\u2032 \u0398\u2212 1 2\u2126\u2032   (\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2212\u039b\u039b\u2032) ( V\u0393\u2212 1 2 \u03a6\u03a8\u2212 1 2 \u2126\u0398\u2212 1 2 ) \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 F\n\u2264 p\u22121/2 ( \u2016\u0393\u22121/2V\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032)V\u0393\u22121/2\u2016F + \u2016\u03a8\u22121/2\u03a6\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2212\u039b\u039b\u2032)\u03a6\u03a8\u22121/2\u2016F\n+ \u2016\u0398\u22121/2\u2126\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032)\u2126\u0398\u22121/2\u2016F + 2\u2016\u0393\u22121/2V\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2212\u039b\u039b\u2032)\u03a6\u03a8\u22121/2\u2016F + 2\u2016\u0393\u22121/2V\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032)\u2126\u0398\u22121/2\u2016F + 2\u2016\u03a8\u22121/2\u03a6\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2212\u039b\u039b\u2032)\u2126\u0398\u22121/2\u2016F )\n=: \u2206L1 +\u2206L2 +\u2206L3 + 2\u2206L4 + 2\u2206L5 + 2\u2206L6.\nFor \u2206L2, similar to the proof of (A.8), we have\n\u2206L2 \u2264 p\u22121/2 ( \u2016\u03a8\u22121/2\u03a6\u2032(\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u03a6\u03a8\u03a6\u2032)\u03a6\u03a8\u22121/2\u2016F + \u2016\u03a8\u22121/2\u03a6\u2032(\u03a6\u03a8\u03a6\u2032 \u2212\u039b\u039b\u2032)\u03a6\u03a8\u22121/2\u2016F )\n=: \u2206 (a) L2 +\u2206 (b) L2.\nWe have\n\u2206 (a) L2 \u2264 p\u22121/2 ( \u2016\u03a8\u22121/2(\u03a6\u2032\u03a6\u0302\u2212 I)\u03a8\u0302(\u03a6\u0302\u2032\u03a6\u2212 I)\u03a8\u22121/2\u2016F + 2\u2016\u03a8\u22121/2(\u03a6\u2032\u03a6\u0302\u2212 I)\u03a8\u0302\u03a8\u22121/2\u2016F\n+ \u2016(\u03a8\u22121/2(\u03a8\u0302\u2212\u03a8)\u03a8\u22121/2\u2016F ) =: I + II + III.\nBy Lemma A.4, we have \u2016\u03a6\u0302j\u2212\u03a6j\u2016F \u2264 \u221apjrj\u2016\u03a6\u0302j\u2212\u03a6j\u2016max = OP ( p 5 2 (1\u2212a1)+3c(1\u2212a2) \u221a log p/T+ 1/p 5a1 2 \u2212 3 2 +c(3a2\u22124) +mp/p c(3a2\u22122) ) . Because \u03a6\u0302 and \u03a6 are block diagonal matrices, we have\n\u2016\u03a6\u0302\u2212\u03a6\u20162F = G\u2211\nj=1\n\u2016\u03a6\u0302j\u2212\u03a6j\u20162F = OP ( p1\u2212c(p5(1\u2212a1)+6c(1\u2212a2) log p\nT +\n1\np5a1\u22123+2c(3a2\u22124) + m2p p2c(3a2\u22122) ) ) .\nThen, II is of order OP (p 5 2 (1\u2212a1)+c( 52\u22123a2) \u221a log p/T + 1/p 5 2 a1\u2212 32+c(3a2\u2212 7 2 ) + mp/p 3c(a2\u2212 12 )) and I is of smaller order. By Lemma A.4, we have III = OP (p 5 2 (1\u2212a1)+c(1\u2212a2) \u221a log p/T + 1/p 5 2 a1\u2212 32\u22122c+ca2). Thus, \u2206\n(a) L2 = OP (p 5 2 (1\u2212a1)+c(1\u2212a2)\n\u221a log p/T+1/p 5 2 a1\u2212 32\u22122c+ca2+mp/p 3c(a2\u2212 12 )).\nSimilarly, we have\n\u2206 (b) L2 \u2264 p\u22121/2 ( \u2016\u03a8\u22121/2(\u03a6\u2032\u03a6\u0303\u2212 I)\u03a8\u0303(\u03a6\u0303\u2032\u03a6\u2212 I)\u03a8\u22121/2\u2016F + 2\u2016\u03a8\u22121/2(\u03a6\u2032\u03a6\u0303\u2212 I)\u03a8\u0303\u03a8\u22121/2\u2016F\n+ \u2016(\u03a8\u22121/2(\u03a8\u0303\u2212\u03a8)\u03a8\u22121/2\u2016F ) =: I \u2032 + II \u2032 + III \u2032.\nBy sin \u03b8 theorem, \u2016\u03a6\u2032\u03a6\u0303 \u2212 I\u2016 \u2264 \u2016\u03a6\u0303 \u2212\u03a6\u2016 \u2264 maxj \u2016\u03a6\u0303j \u2212 \u03a6j\u2016 \u2264 O(mp/pca2). Then, we have II \u2032 = O(mp/p ca2) and I \u2032 is of smaller order. By Lemma A.1, we have III \u2032 = O(mp/p ca2).\nThus, \u2206 (b) L2 = O(mp/p ca2). Then, we obtain\n\u2206L2 = OP\n( p 5 2 (1\u2212a1)+c(1\u2212a2) \u221a log p\nT +\n1\np 5 2 a1\u2212 32\u22122c+ca2\n+ mp pca2\n) . (A.9)\nFor \u2206L3, we have\n\u2206L3 \u2264 p\u22121/2\u2016\u0398\u22121/2\u2126\u2032\u03a6\u0302\u03a8\u0302\u03a6\u0302 \u2032 \u2126\u0398\u22121/2\u2016F + p\u22121/2\u2016\u0398\u22121/2\u2126\u2032\u03a6\u0303\u03a8\u0303\u03a6\u0303 \u2032 \u2126\u0398\u22121/2\u2016F =: \u2206(a)L3 +\u2206 (b) L3.\nSince \u2016\u03a8\u0302\u2016 = OP (pca2), we have\n\u2206 (a) L3 \u2264 p\u22121/2\u2016\u0398\u22121\u2016\u2016\u2126\u2032(\u03a6\u0302\u2212\u03a6)\u20162F\u2016\u03a8\u0302\u2016\n= OP\n( p 11 2 \u22125a1+5c(1\u2212a2) log p\nT +\n1\np5a1\u2212 7 2 \u2212c(7\u22125a2)\n+ m2p\np5ca2\u22123c\u2212 1 2\n) .\nSimilarly, by Lemma A.2, \u2206 (b) L3 = OP (m 2 p/p 5ca2\u22123c\u22121/2) because \u2016\u03a6\u0303j \u2212 \u03a6j\u2016F \u2264 \u221apjrj\u2016\u03a6\u0303j \u2212 \u03a6j\u2016max = O(mp/pc(3a2\u22122)) and \u2016\u2126\u2032\u03a6\u0303\u20162F \u2264 \u2016\u03a6\u0303\u2212\u03a6\u20162F = \u2211G j=1 \u2016\u03a6\u0303j\u2212\u03a6j\u20162F = O(m2p/p3c(2a2\u22121)\u22121). Similarly, we can show \u2206L1, \u2206L4, \u2206L5 and \u2206L6 are dominated by \u2206L2 and \u2206L3. Therefore, we have\n\u2206L = OP\n( p 5 2 (1\u2212a1)+c(1\u2212a2)\n\u221a log p\nT +\n1\np 5 2 a1\u2212 32\u22122c+ca2\n+ mp pca2\n+ p 11 2 \u22125a1+5c(1\u2212a2) log p\nT +\n1\np5a1\u2212 7 2 \u2212c(7\u22125a2)\n+ m2p\np5ca2\u22123c\u2212 1 2\n) . (A.10)\nCombining the terms \u2206G, \u2206L and \u2206S together, we complete the proof of (3.5).\nA.2 Proof for Section 3.1\nWe consider (3.6). Similar to the proof of Theorem 2.1 in Fan et al. (2011), we can obtain \u2016\u03a3\u0302TE \u2212\u03a3E\u20162 = OP (\u00b5p\u03c9\u03031\u2212qT ) from (A.2).\nConsider (3.7). We have\n\u2016\u03a3\u0302T \u2212\u03a3\u2016\u03a3 \u2264 p\u22121/2 ( \u2016\u03a3\u22121/2(V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032)\u03a3\u22121/2\u2016F + \u2016\u03a3\u22121/2(\u03a3\u0302 T E \u2212\u03a3E)\u03a3\u22121/2\u2016F )\n=: \u2206G +\u2206E\nand\n\u2206E = OP (p \u22121/2\u2016\u03a3\u0302TE \u2212\u03a3E\u2016F ) = OP (\u2016\u03a3\u0302 T E \u2212\u03a3E\u20162) = OP (\u00b5p\u03c9\u03031\u2212qT ).\nCombining the terms \u2206E and \u2206G from (A.8), we have\n\u2016\u03a3\u0302T \u2212\u03a3\u2016\u03a3 = OP ( \u00b5p\u03c9\u0303 1\u2212q T + p 11 2 \u22125a1 log p\nT +\n1\np5a1\u2212 7 2 \u22122c\n) .\nA.3 Proofs for Section 3.2\nUnder the orthogonality condition, we have the following modified rates of convergence. Define W = [B \u039b], which is a p \u00d7 (k + r) loading matrix. Let {\u03b4\u0304i, v\u0304i}k+ri=1 be the leading eigenvalues and eigenvectors of WW\u2032 in the decreasing order and the rest zero.\nLemma A.5. Under the assumptions of Theorem 3.2, we have the following results.\n(i) By Weyl\u2019s theorem, we have\n|\u03b4i \u2212 \u03b4\u0304i| \u2264 \u2016\u03a3u\u2016 for i \u2264 k + r, |\u03b4i| \u2264 \u2016\u03a3u\u2016 for i > k + r,\n|\u03b4i \u2212 \u03b4\u0304i| \u2264 \u2016\u03a3u\u2016 for i \u2264 k + r and |\u03b4i| \u2264 \u2016\u03a3u\u2016 for i > k + r. In addition, for all p, \u03b4\u0304i/p a1 and \u03b4\u0304i/p ca2 are strictly bigger than zero for i \u2264 k and k < i \u2264 k+r, respectively.\n(ii) We have\nmax i\u2264k\n\u2016v\u0304i \u2212 vi\u2016\u221e = O ( mp\np3(a1\u2212 1 2 )\n) ,\nmax i\u2264k\n\u2016v\u0302i \u2212 vi\u2016\u221e = OP  p3(1\u2212a1) \u221a log p\nTp +\nmp\np3(a1\u2212 1 2 )\n  .\nProof. (ii) Under the orthogonality condition, we have\n\u03a3\u0302 = BB\u2032 +\u039b\u039b\u2032 +\u03a3u + (\u03a3\u0302\u2212\u03a3) = WW\u2032 +\u03a3u + (\u03a3\u0302\u2212\u03a3).\nWe can treat WW\u2032 as a low rank matrix and the remaining terms as a perturbation matrix. Then, similar to Lemmas A.2 and A.3, we can show\nmax i\u2264k \u2016v\u0304i \u2212 vi\u2016\u221e \u2264 Cp2(1\u2212a1) \u2016\u03a3u\u2016\u221e \u03b3\u0304 \u221a p ,\nmax i\u2264k\n\u2016v\u0302i \u2212 vi\u2016\u221e \u2264 Cp2(1\u2212a1) \u2016\u03a3u + (\u03a3\u0302\u2212\u03a3)\u2016\u221e\n\u03b3\u0304 \u221a p\n= OP  \n\u2016\u03a3u\u2016\u221e p3(a1\u2212 1 2 ) + p3(1\u2212a1)\n\u221a log p\nTp\n  ,\nwhere the eigengap \u03b3\u0304 = min{\u03b4\u0304i \u2212 \u03b4\u0304i+1 : 1 \u2264 i \u2264 k}.\nLemma A.6. Under the assumptions of Theorem 3.2, for i \u2264 rj, we have\n|\u03ba\u0302ji/\u03baji \u2212 1| = OP ( p 5 2 (1\u2212a1)+c(1\u2212a2) \u221a log p\nT +\nmp\np 5a1 2 \u2212 3 2 \u2212c(1\u2212a2)\n) ,\n\u2016\u03b7\u0302ji \u2212 \u03b7ji \u2016\u221e = OP  p 5 2 (1\u2212a1)+c( 52\u22123a2) \u221a log p\nT +\nmp\np 5a1 2 \u2212 3 2 +c(3a2\u2212 52 )\n+ mp\np3c(a2\u2212 1 2 )\n  .\nProof. Similar to the proof of Lemma A.4 using results from Lemma A.5, we can obtain\n\u2016V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032\u2016max = OP (p 5 2 (1\u2212a1) \u221a log p/T +mp/p 5a1 2 \u2212 3 2 ). (A.11)\nThen, we have\n\u2016\u03a3\u0302E \u2212\u03a3E\u2016max \u2264 \u2016\u03a3\u0302\u2212\u03a3\u2016max + \u2016V\u0302\u0393\u0302V\u0302 \u2032 \u2212BB\u2032\u2016max\n= OP (p 5 2 (1\u2212a1) \u221a log p/T +mp/p 5a1 2 \u2212 3 2 ). (A.12)\nTherefore, the first statement is followed by (A.12) and the Weyl\u2019s theorem. In addition, by Theorem 1 of Fan et al. (2018b) and (A.12), we have\n\u2016\u03b7\u0302ji \u2212 \u03b7ji \u2016\u221e \u2264 Cp2(1\u2212a2)j \u2016\u03a3ju + (\u03a3\u0302\nj\nE \u2212\u03a3jE)\u2016\u221e pa2j \u221a pj\n= OP  p 5 2 (1\u2212a1)+c( 52\u22123a2) \u221a log p\nT +\nmp\np 5a1 2 \u2212 3 2 +c(3a2\u2212 52 )\n+ mp\np3c(a2\u2212 1 2 )\n  .\nProof of Theorem 3.2. Using the similar proof of (A.5) and results from Lemmas A.5\nand A.6, we can obtain\n\u2016\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032\u2016max = OP (p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T +mp/p 5 2 a1\u2212 32\u2212 5 2 c(1\u2212a2) +mp/ \u221a pc(5a2\u22123)).\n(A.13)\nBy Assumption 3.1(iii), (A.11) and (A.13), we have\n\u2016\u03a3\u0302u \u2212\u03a3u\u2016max = OP ( p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p\nT + mp\u221a p5a1\u22123\u22125c(1\u2212a2) + mp\u221a pc(5a2\u22123)\n) .\nTherefore, \u2016\u03a3\u0302Du \u2212\u03a3u\u2016max = OP (\u03c4 + \u03c9T,2) = OP (\u03c9T,2), when \u03c4 is chosen as the same order of \u03c9T,2 = p 5 2 (1\u2212a1)+ 52 c(1\u2212a2) \u221a log p/T +mp/ \u221a p5a1\u22123\u22125c(1\u2212a2) +mp/ \u221a pc(5a2\u22123). Similar to the proofs of Theorem 3.1, we can obtain (3.8) \u2013 (3.11).\nConsider (3.12). First, we have\n\u2016\u03a3\u0302D \u2212\u03a3\u2016\u03a3 \u2264 p\u22121/2 ( \u2016\u03a3\u22121/2(V\u0302\u0393\u0302V\u0302\u2032 \u2212BB\u2032)\u03a3\u22121/2\u2016F\n+ \u2016\u03a3\u22121/2(\u03a6\u0302\u03a8\u0302\u03a6\u0302\u2032 \u2212\u039b\u039b\u2032)\u03a3\u22121/2\u2016F + \u2016\u03a3\u22121/2(\u03a3\u0302 D u \u2212\u03a3u)\u03a3\u22121/2\u2016F\n)\n=: \u2206G\u2032 +\u2206L\u2032 +\u2206S\u2032\nand\n\u2206S\u2032 = OP (p \u22121/2\u2016\u03a3\u0302Du \u2212\u03a3u\u2016F ) = OP (\u2016\u03a3\u0302 D u \u2212\u03a3u\u20162) = OP (mp\u03c91\u2212qT,2 ).\nBy Lemma A.5, we have\n\u2016V\u2032V\u0302\u2212 I\u2016F = \u2016V\u2032(V\u0302\u2212V)\u2016F \u2264 \u2016V\u0302\u2212V\u2016F = OP (p3(1\u2212a1) \u221a log p/T +mp/p 3a1\u22122), \u2016V\u2032V\u0303\u2212 I\u2016F = \u2016V\u2032(V\u0303\u2212V)\u2016F \u2264 \u2016V\u0303\u2212V\u2016F = O(mp/p3a1\u22122).\nThen, similar to the proof of (A.8), we can obtain\n\u2206G\u2032 = OP\n( p1\u2212a1 \u221a log p\nT + mp pa1 + p 11 2 \u22125a1 log p T +\nm2p\np5a1\u2212 7 2\n) . (A.14)\nBy Lemma A.6, we have\n\u2016\u03a6\u0302\u2212\u03a6\u20162F = J\u2211\nj=1\n\u2016\u03a6\u0302j \u2212 \u03a6j\u20162F\n= OP\n( p1\u2212c ( p5(1\u2212a1)+6c(1\u2212a2) log p\nT + m2p p5a1\u22123\u22126c(1\u2212a2) + m2p p2c(3a2\u22122)\n)) ,\n\u2016\u03a6\u0303\u2212\u03a6\u20162F = J\u2211\nj=1\n\u2016\u03a6\u0303j \u2212 \u03a6j\u20162F = O(m2p/p2c(3a2\u22122)\u2212(1\u2212c)).\nThen, similar to the proof of (A.10), we can obtain\n\u2206L\u2032 = OP\n( p 5 2 (1\u2212a1)+c(1\u2212a2)\n\u221a log p\nT +\nmp\np 5 2 a1\u2212 32\u2212c(1\u2212a2)\n+ mp pca2\n+ p 11 2 \u22125a1+5c(1\u2212a2) log p\nT +\nm2p\np5a1\u2212 7 2 \u22125c(1\u2212a2)\n+ m2p\np5ca2\u22123c\u2212 1 2\n) .\nCombining the terms \u2206G\u2032 , \u2206L\u2032 and \u2206S\u2032 together, we complete the proof of (3.12).\nProofs for (3.13) and (3.14). We consider (3.13). Let W = (w\u03031, . . . , w\u0303k+r). Then, \u03b4\u0304i = \u2016w\u0303i\u20162 \u224d p for i \u2264 k, \u03b4\u0304i = \u2016w\u0303i\u20162 \u224d pc for k < i \u2264 k + r by Lemma A.5(i), and v\u0304i = w\u0303i/\u2016w\u0303i\u2016. Hence, \u2016v\u0304i\u2016\u221e \u2264 \u2016W\u2016max/\u2016w\u0303i\u2016 \u2264 C/ \u221a pc for i \u2264 k + r. In addition, for\nV\u03032 = (v\u03041, . . . , v\u0304k+r), the coherence \u00b5(V\u03032) = p\nk+r maxi \u2211k+r j=1 V\u0303 2 2,ij \u2264 Cp1\u2212c, where V\u03032,ij is the\n(i, j) entry of V\u03032. Thus, by Theorem 1 of Fan et al. (2018b) and r \u224d p1\u2212c, we have\nmax i\u2264k+r \u2016v\u0304i \u2212 vi\u2016\u221e \u2264 C(k + r)4p2(1\u2212c) \u2016\u03a3u\u2016\u221e pc \u221a p = O\n( mp\np7c\u2212 11 2\n) ,\nmax i\u2264k+r\n\u2016v\u0302i \u2212 vi\u2016\u221e \u2264 C(k + r)4p2(1\u2212c) \u2016\u03a3u + (\u03a3\u0302\u2212\u03a3)\u2016\u221e\npc \u221a p\n= OP\n( mp\np7c\u2212 11 2\n+ p 13 2 \u22127c \u221a log p\nT\n) .\nBy the similar argument of (A.1), we can obtain \u2016V\u03022\u0393\u03022V\u0302 \u2032 2\u2212WW\u2032\u2016max = OP (p7(1\u2212c) \u221a log p/T+ mp/p 7c\u22126). Hence, we have\n\u2016\u03a3\u0302u,2\u2212\u03a3u\u2016max \u2264 \u2016\u03a3\u0302\u2212\u03a3\u2016max + \u2016V\u03022\u0393\u03022V\u0302 \u2032 2 \u2212WW\u2032\u2016max = OP (p7(1\u2212c) \u221a log p/T +mp/p 7c\u22126).\nSimilar to the proof of (3.2), we can obtain \u2016\u03a3\u0302Tu,2 \u2212 \u03a3u\u20162 = OP (mp\u03c9\u03031\u2212qT,2 ), where \u03c9\u0303T,2 = p7(1\u2212c) \u221a log p/T +mp/p 7c\u22126.\nConsider (3.14). We have\n\u2016\u03a3\u0302T2 \u2212\u03a3\u2016\u03a3 \u2264 p\u22121/2 ( \u2016\u03a3\u22121/2(V\u03022\u0393\u03022V\u0302 \u2032 2 \u2212WW\u2032)\u03a3\u22121/2\u2016F + \u2016\u03a3\u22121/2(\u03a3\u0302 T u,2 \u2212\u03a3u)\u03a3\u22121/2\u2016F )\n=: \u2206G2 +\u2206S2\nand\n\u2206S2 = OP (p \u22121/2\u2016\u03a3\u0302Tu,2 \u2212\u03a3u\u2016F ) = OP (\u2016\u03a3\u0302 T u,2 \u2212\u03a3u\u20162) = OP (mp\u03c9\u03031\u2212qT,2 ).\nNote that \u2016V\u03022 \u2212V2\u2016F \u2264 \u221a p2\u2212c\u2016v\u0302i \u2212 vi\u2016\u221e = OP (mp/ \u221a p15c\u221213 + p 15 2 (1\u2212c) \u221a log p/T ), where V2 = (v1, . . . , vk+r) is the leading eigenvectors of covariance matrix \u03a3. Similar to the proof of (A.8), we can obtain\n\u2206G2 = OP\n( (p7\u2212 15 2 c + p1\u2212c) \u221a log p\nT + mp pc + mp p 15 2 c\u22126 + p15(1\u2212c)+ 1 2 log p T +\nm2p\np15c\u2212 27 2\n) .\nCombining the terms \u2206G2 and \u2206S2 , we have\n\u2016\u03a3\u0302T2 \u2212\u03a3\u2016\u03a3 = OP ( mp\u03c9\u0303 1\u2212q T,2 + p 15(1\u2212c)+ 1 2 log p\nT +\nm2p\np15c\u2212 27 2\n) .\nA.4 Proof of Theorem 3.3\nProof of Theorem 3.3. We consider (3.15). By (3.10), for each group j, we have\n\u2016(\u03a3\u0302D)j \u2212\u03a3j\u2016max \u2264 \u2016\u03a3\u0302 D \u2212\u03a3\u2016max = OP (\u03c9T,2).\nConsider (3.16). Since \u03bbmin(\u03a3 j u) > c1 and mpj\u03c9 1\u2212q T,2 = o(1), the minimum eigenvalue of\n(\u03a3\u0302 D u ) j is strictly bigger than 0 with probability approaching 1. Then, we have \u2016((\u03a3\u0302Du )j)\u22121 \u2212 (\u03a3ju) \u22121\u20162 \u2264 \u03bbmin(\u03a3ju)\u22121\u2016(\u03a3\u0302 D u ) j \u2212\u03a3ju\u20162\u03bbmin((\u03a3\u0302 D u ) j)\u22121 = OP (mpj\u03c9 1\u2212q T,2 ). Similar to the proofs of (3.4), we can show the statement.\nConsider (3.17). We denote (A)j the jth diagonal block for a matrix A. We have\nBjBj\u2032 = (BB\u2032)j = (V\u0303\u0393\u0303V\u0303 \u2032 )j = V\u0303 j \u0393\u0303V\u0303 j\u2032 , where V\u0303 j is the jth pj\u00d7k submatrix of V\u0303. Similarlly, (V\u0302\u0393\u0302V\u0302 \u2032 )j = V\u0302 j \u0393\u0302V\u0302 j\u2032 , where V\u0302 j is the jth pj \u00d7 k submatrix of V\u0302. For each group j, we have\n(\u03a3\u0302 D )j = V\u0302 j \u0393\u0302V\u0302 j\u2032 + \u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 + (\u03a3\u0302 D u ) j.\nBy SVD, we have \u03a3j = Vpj\u0393pjV \u2032 pj , where Vpj = (V j ,\u03a6j,\u2126j) and \u0393pj = diag(\u0393,\u03a8 j,\u0398j). Then, we have\n\u2016(\u03a3\u0302D)j \u2212\u03a3j\u2016\u03a3j \u2264 (pj)\u22121/2 (\u2225\u2225\u2225(\u03a3j)\u2212 12 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212BjBj\u2032 ) (\u03a3j)\u2212 1 2 \u2225\u2225\u2225 F\n+ \u2225\u2225\u2225(\u03a3j)\u2212 12 (\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)(\u03a3j)\u2212 12 \u2225\u2225\u2225 F + \u2225\u2225\u2225(\u03a3j)\u2212 12 ( (\u03a3\u0302 D u ) j \u2212\u03a3ju ) (\u03a3j)\u2212 1 2 \u2225\u2225\u2225 F ) =: \u2206g +\u2206l +\u2206s.\nSimilar to the proofs of Theorem 2.1 in Fan et al. (2011), we can show \u2016(\u03a3\u0302Du )j \u2212 \u03a3ju\u20162 =\nOP (mpj\u03c9 1\u2212q T,2 ). Then, we have\n\u2206s = OP (p \u22121/2 j \u2016(\u03a3\u0302 D u ) j \u2212\u03a3ju\u2016F ) = OP (\u2016(\u03a3\u0302 D u ) j \u2212\u03a3ju\u20162) = OP (mpj\u03c91\u2212qT,2 ).\nWe have\n\u2206g \u2264 p\u22121/2j (\u2225\u2225\u2225\u0393\u22121/2Vj\u2032 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212 BjBj\u2032 ) Vj\u0393\u22121/2 \u2225\u2225\u2225 F\n+ \u2225\u2225\u2225\u03a8j\u22121/2\u03a6j\u2032 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212 BjBj\u2032 ) \u03a6j\u03a8j \u22121/2 \u2225\u2225\u2225 F + \u2225\u2225\u2225\u0398j\u22121/2\u2126j\u2032 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212 BjBj\u2032 ) \u2126j\u0398j \u22121/2 \u2225\u2225\u2225 F + 2 \u2225\u2225\u2225\u0393\u22121/2Vj\u2032 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212 BjBj\u2032 ) \u03a6j\u03a8j\n\u22121/2 \u2225\u2225\u2225 F + 2 \u2225\u2225\u2225\u0393\u22121/2Vj\u2032 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212 BjBj\u2032 ) \u2126j\u0398j \u22121/2 \u2225\u2225\u2225 F\n+ 2 \u2225\u2225\u2225\u03a8j\u22121/2\u03a6j\u2032 ( V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2212 BjBj\u2032 ) \u2126j\u0398j \u22121/2 \u2225\u2225\u2225 F ) =: \u2206g1 +\u2206g2 +\u2206g3 + 2\u2206g4 + 2\u2206g5 + 2\u2206g6.\nUsing the similar proof of (A.7) and results from Lemma A.5, we can obtain\n\u2206g1 = OP\n( p1\u2212a1 \u221a log p\nT + mp pa1\n) .\nFor \u2206g3, we have\np \u22121/2 j (\u2225\u2225\u2225\u0398j\u22121/2\u2126j\u2032V\u0302 j \u0393\u0302V\u0302 j\u2032 \u2126j\u0398j \u22121/2 \u2225\u2225\u2225 F + \u2225\u2225\u2225\u0398j\u22121/2\u2126j\u2032V\u0303 j \u0393\u0303V\u0303 j\u2032 \u2126j\u0398j \u22121/2 \u2225\u2225\u2225 F ) =: \u2206 (a) g3 +\u2206 (b) g3 .\nBy Lemma A.5, we have\n\u2016V\u0302j \u2212Vj\u2016F = OP ( \u221a pc\u2016V\u0302\u2212V\u2016max) = OP (\u221a pc(p3(1\u2212a1) \u221a log p/Tp+mp/p 3(a1\u2212 12 )) ) .\nSince \u2016\u0393\u0302\u2016 = OP (pa1), we then have\n\u2206 (a) g3 \u2264 p\u22121/2j \u2016\u0398j \u22121\u2016\u2016\u2126j\u2032(V\u0302j \u2212Vj)\u20162F\u2016\u0393\u0302\u2016 = OP (p5(1\u2212a1)+c/2 log p/T +m2p/p5a1\u22123\u2212c/2).\nSimilarly, since \u2016V\u0303j\u2212Vj\u2016F = OP ( \u221a pc\u2016V\u0303\u2212V\u2016max) = OP (mp/p3(a1\u22121/2)\u2212c/2) by Lemma A.5, \u2206 (b) g3 = OP (m 2 p/p 5a1\u22123\u2212c/2). Then, we obtain\n\u2206g3 = OP\n( p5(1\u2212a1)+ c 2 log p\nT +\nm2p\np5a1\u22123\u2212 c 2\n) .\nSimilarly, we can show that the terms \u2206g2, \u2206g4, \u2206g5 and \u2206g6 are dominated by \u2206g1 and \u2206g3. Thus, we have\n\u2206g = OP\n( p1\u2212a1 \u221a log p\nT + mp pa1 + p5(1\u2212a1)+ c 2 log p T +\nm2p\np5a1\u22123\u2212 c 2\n) .\nWe have\n\u2206l \u2264 p\u22121/2j (\u2225\u2225\u2225\u0393\u22121/2Vj\u2032(\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)Vj\u0393\u22121/2 \u2225\u2225\u2225 F\n+ \u2225\u2225\u2225\u03a8j\u22121/2\u03a6j\u2032(\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)\u03a6j\u03a8j\u22121/2 \u2225\u2225\u2225 F + \u2225\u2225\u2225\u0398j\u22121/2\u2126j\u2032(\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)\u2126j\u0398j\u22121/2 \u2225\u2225\u2225 F + 2 \u2225\u2225\u2225\u0393\u22121/2Vj\u2032(\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)\u03a6j\u03a8j\u22121/2\n\u2225\u2225\u2225 F + 2 \u2225\u2225\u2225\u0393\u22121/2Vj\u2032(\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)\u2126j\u0398j\u22121/2 \u2225\u2225\u2225 F\n+ 2 \u2225\u2225\u2225\u03a8j\u22121/2\u03a6j\u2032(\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032 \u2212 \u039bj\u039bj\u2032)\u2126j\u0398j\u22121/2 \u2225\u2225\u2225 F )\n=: \u2206l1 +\u2206l2 +\u2206l3 + 2\u2206l4 + 2\u2206l5 + 2\u2206l6.\nFor \u2206l2, similar to the proof of (A.9) using results from Lemma A.6, we can obtain\n\u2206l2 = OP\n( p 5 2 (1\u2212a1)+c(1\u2212a2) \u221a log p\nT +\nmp\np 5 2 a1\u2212 32\u2212c(1\u2212a2) + mp pca2\n) .\nFor \u2206l3, we have\n\u2206l3 \u2264 p\u22121/2j (\u2225\u2225\u2225\u0398j\u22121/2\u2126j\u2032\u03a6\u0302j\u03a8\u0302j\u03a6\u0302j\u2032\u2126j\u0398j\u22121/2 \u2225\u2225\u2225 F + \u2225\u2225\u2225\u0398j\u22121/2\u2126j\u2032\u03a6\u0303j\u03a8\u0303j\u03a6\u0303j\u2032\u2126j\u0398j\u22121/2 \u2225\u2225\u2225 F ) =: \u2206 (a) l3 +\u2206 (b) l3 . By Lemma A.6, we have \u2016\u03a6\u0302j\u2212\u03a6j\u2016F \u2264 \u221apjrj\u2016\u03a6\u0302j\u2212\u03a6j\u2016max = OP ( p 5 2 (1\u2212a1)+3c(1\u2212a2) \u221a log p/T+\nmp/p 5a1 2 \u2212 3 2 \u22123c(1\u2212a2) +mp/p c(3a2\u22122) ) and \u2016\u03a8\u0302j\u2016 = OP (pca2), we have\n\u2206 (a) l3 \u2264 p \u22121/2 j \u2016\u0398j \u22121\u2016\u2016\u2126j\u2032(\u03a6\u0302j \u2212 \u03a6j)\u20162F\u2016\u03a8\u0302j\u2016\n= OP\n( p5(1\u2212a1)+c( 11 2 \u22125a2) log p\nT +\nm2p\np5a1\u22123+c(5a2\u2212 11 2 ) +\nm2p\npc(5a2\u2212 7 2 )\n) .\nSimilarly, by Lemma A.2, \u2016\u03a6\u0303j \u2212 \u03a6j\u2016F = OP ( \u221a pc\u2016\u03a6\u0303j \u2212 \u03a6j\u2016max) = OP (mp/pc(3a2\u22122)), then \u2206 (b) l3 = OP (m 2 p/p c(5a2\u2212 72 )). Similarly, we can show \u2206l1, \u2206l4, \u2206l5 and \u2206l6 are dominated by \u2206l2 and \u2206l3. Thus, we have\n\u2206l = OP\n( p 5 2 (1\u2212a1)+c(1\u2212a2)\n\u221a log p\nT +\nmp\np 5 2 a1\u2212 32\u2212c(1\u2212a2)\n+ mp pca2\n+ p5(1\u2212a1)+c( 11 2 \u22125a2) log p\nT +\nm2p\np5a1\u22123+c(5a2\u2212 11 2 ) +\nm2p\npc(5a2\u2212 7 2 )\n) .\nCombining the terms \u2206g, \u2206l and \u2206s together, we complete the proof of (3.17).\nA.5 Proof of Theorem 3.4\nLemma A.7. Under the assumptions of Theorem 3.4, for i = 1, . . . , [dm]\u2212 2(k + r), there exist constants c, c > 0 such that\nc+ oP (1) \u2264 m\u03b4\u0302k+r+i/p \u2264 c+ oP (1).\nProof. Similar to the proofs of Lemma A.9 in Ahn and Horenstein (2013), we can show the statement.\nLemma A.8. Under the multi-level factor model and assumptions of Theorem 3.4, for i = 1, . . . , k, we have\n\u03b4\u0302i = \u03b4\u0304i +OP\n( p \u221a log p\nT\n) +OP (p ca2) .\nIn addition, for i = k + 1, . . . , k + r, we have\n\u03b4\u0302i = \u03b4\u0304i +OP\n( p 5 2 (1\u2212a1)+c \u221a log p\nT +\n1\np 5 2 a1\u2212 32\u22122c\n) +OP (mp) .\nProof. By Lemmas A.1 and A.3, the first statement is showed. The order of |\u03b4\u0302i \u2212 \u03b4\u0304i| for i = k + 1, . . . , k + r is the same as that of |\u03ba\u0302ji \u2212 \u03ba\u0304ji | for i = 1, . . . , rj and each group j. Then the second statement is showed from Lemmas A.1 and A.4.\nProof of Theorem 3.4. (i) When k > 0 and r > 0, by Lemmas A.1 and A.8, \u03b4\u0302i/\u03b4\u0302i+1 =\n\u03b4\u0304i/\u03b4\u0304i+1 + oP (1) = OP (1) for i = 1, 2, . . . , k \u2212 1, k + 1 . . . , k + r \u2212 1. In addition, we have\n\u03b4\u0302k\n\u03b4\u0302k+1 =\n\u03b4\u0304k +OP\n( p \u221a log p/T ) +OP (p ca2)\n\u03b4\u0304k+1 +OP\n( p 5 2 (1\u2212a1)+c \u221a log p/T + 1/p 5 2 a1\u2212 32\u22122c ) +OP (mp)\n= OP ( pa1\u2212ca2 ) .\nBy Lemmas A.7 and A.8, we have\n\u03b4\u0302k+r\n\u03b4\u0302k+r+1 \u2265\n\u03b4\u0304k+r +OP\n( p 5 2 (1\u2212a1)+c \u221a log p/T + 1/p 5 2 a1\u2212 32\u22122c ) +OP (mp)\np[c+ oP (1)]/m = OP\n( m\np1\u2212ca2\n) ,\nwhich diverges to \u221e if p1\u2212ca2 \u226a T . By Lemma A.7, for i = 1, . . . , [dm] \u2212 2(k + r) \u2212 1, \u03b4\u0302k+r+i/\u03b4\u0302k+r+i+1 \u2264 (c+ oP (1))/(c+ oP (1)).\nWhen k = 0 and r > 0, for i \u2264 r, we have |\u03b4\u0304i\u2212\u03b4i| \u2264 \u2016\u03a3ju\u2016 = O(mp) and |\u03b4\u0302i\u2212\u03b4i| \u2264 pc\u2016\u03a3\u0302 j\u2212 \u03a3j\u2016max = OP (pc \u221a log p/T ). Then, \u03b4\u0302i/\u03b4\u0302i+1 = \u03b4\u0304i/\u03b4\u0304i+1 + oP (1) = OP (1) for i = 1, 2, . . . , r \u2212 1. In addition, by Lemmas A.7, we have\n\u03b4\u0302r\n\u03b4\u0302r+1 \u2265\n\u03b4\u0304r +OP\n( pc \u221a log p/T ) +OP (mp)\np[c+ oP (1)]/m = OP\n( m\np1\u2212ca2\n) ,\nwhich diverges to \u221e if p1\u2212ca2 \u226a T . By Lemma A.7, for i = 1, . . . , [dm]\u22122r\u22121, \u03b4\u0302r+i/\u03b4\u0302r+i+1 \u2264 (c+ oP (1))/(c+ oP (1)).\nWhen k > 0 and r = 0, for i \u2264 k, we have |\u03b4\u0304i \u2212 \u03b4i| \u2264 \u2016\u03a3u\u2016 = O(mp) and |\u03b4\u0302i \u2212 \u03b4i| =\nOP (p \u221a log p/T ). Then, \u03b4\u0302i/\u03b4\u0302i+1 = OP (1) for i = 1, 2, . . . , k \u2212 1. In addition, we have\n\u03b4\u0302k\n\u03b4\u0302k+1 \u2265\n\u03b4\u0304k +OP\n( p \u221a log p/T ) +OP (mp)\np[c+ oP (1)]/m = OP\n( m\np1\u2212a1\n) ,\nwhich diverges to \u221e if p1\u2212a1 \u226a T . By Lemma A.7, for i = 1, . . . , [dm]\u22122k\u22121, \u03b4\u0302k+i/\u03b4\u0302k+i+1 \u2264 (c+ oP (1))/(c+ oP (1)). These results show the consistency of model selection.\n(ii) Under the multi-level factor model (i.e., k > 0 and r > 0), the above results imply\nthat k\u0302 is consistent by using the fact that k < k + r."
        }
    ],
    "title": "Large Volatility Matrix Analysis Using Global and National Factor Models",
    "year": 2022
}