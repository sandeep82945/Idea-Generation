{
    "abstractText": "We study the computational problem of the stationarity test for the empirical loss of neural networks with ReLU activation functions. Our contributions are: 1. Hardness: We show that checking a certain first-order approximate stationarity concept for a piecewise linear function is co-NP-hard. This implies that testing a certain stationarity concept for a modern nonsmooth neural network is in general computationally intractable. As a corollary, we prove that testing so-called firstorder minimality for functions in abs-normal form is co-NP-complete, which was conjectured by Griewank and Walther (2019, SIAM J. Optim., vol. 29, p284). 2. Regularity: We establish a necessary and sufficient condition for the validity of an equality-type subdifferential chain rule in terms of Clarke, Fr\u00e9chet, and limiting subdifferentials of the empirical loss of two-layer ReLU networks. This new condition is simple and efficiently checkable. 3. Robust algorithms: We introduce an algorithmic scheme to test near-approximate stationarity in terms of both Clarke and Fr\u00e9chet subdifferentials. Our scheme makes no false positive or false negative error when the tested point is sufficiently close to a stationary one and a certain qualification is satisfied. This is the first practical and robust stationarity test approach for two-layer ReLU networks. Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Sha Tin, N.T., Hong Kong SAR. E-mail: tianlai@se.cuhk.edu.hk. Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Sha Tin, N.T., Hong Kong SAR. E-mail: manchoso@se.cuhk.edu.hk.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lai Tian"
        }
    ],
    "id": "SP:9069c0207d675725ee8bded38919dcf58a3b19b4",
    "references": [
        {
            "authors": [
                "A.A. Ahmadi",
                "J. Zhang"
            ],
            "title": "On the complexity of finding a local minimizer of a quadratic function over a polytope",
            "venue": "Mathematical Programming,",
            "year": 2022
        },
        {
            "authors": [
                "S. Arora",
                "S. Du",
                "W. Hu",
                "Z. Li",
                "R. Wang"
            ],
            "title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "D. Bertsimas",
                "J.N. Tsitsiklis"
            ],
            "title": "Introduction to Linear Optimization, volume 6",
            "venue": "Athena Scientific Belmont, MA,",
            "year": 1997
        },
        {
            "authors": [
                "S. Bubeck",
                "R. Eldan",
                "Y.T. Lee",
                "D. Mikulincer"
            ],
            "title": "Network size and size of the weights in memorization with two-layers neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "J.V. Burke",
                "A.S. Lewis",
                "M.L. Overton"
            ],
            "title": "Approximating subdifferentials by random sampling of gradients",
            "venue": "Mathematics of Operations Research,",
            "year": 2002
        },
        {
            "authors": [
                "F.H. Clarke"
            ],
            "title": "Optimization and Nonsmooth Analysis",
            "year": 1990
        },
        {
            "authors": [
                "D. Davis",
                "D. Drusvyatskiy"
            ],
            "title": "Stochastic model-based minimization of weakly convex functions",
            "venue": "SIAM Journal on Optimization,",
            "year": 2019
        },
        {
            "authors": [
                "D. Davis",
                "D. Drusvyatskiy",
                "S. Kakade",
                "J.D. Lee"
            ],
            "title": "Stochastic subgradient method converges on tame functions",
            "venue": "Foundations of Computational Mathematics,",
            "year": 2020
        },
        {
            "authors": [
                "D. Davis",
                "D. Drusvyatskiy",
                "Y.T. Lee",
                "S. Padmanabhan",
                "G. Ye"
            ],
            "title": "A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M.R. Garey",
                "D.S. Johnson"
            ],
            "title": "Computers and Intractability, volume 174",
            "year": 1979
        },
        {
            "authors": [
                "A. Griewank"
            ],
            "title": "On stable piecewise linearization and generalized algorithmic differentiation",
            "venue": "Optimization Methods and Software,",
            "year": 2013
        },
        {
            "authors": [
                "A. Griewank",
                "A. Walther"
            ],
            "title": "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation",
            "year": 2008
        },
        {
            "authors": [
                "A. Griewank",
                "A. Walther"
            ],
            "title": "First-and second-order optimality conditions for piecewise smooth objective functions",
            "venue": "Optimization Methods and Software,",
            "year": 2016
        },
        {
            "authors": [
                "A. Griewank",
                "A. Walther"
            ],
            "title": "Relaxing kink qualifications and proving convergence rates in piecewise smooth optimization",
            "venue": "SIAM Journal on Optimization,",
            "year": 2019
        },
        {
            "authors": [
                "J.-B. Hiriart-Urruty",
                "C. Lemar\u00e9chal"
            ],
            "title": "Fundamentals of Convex Analysis",
            "venue": "Springer Science & Business Media,",
            "year": 2004
        },
        {
            "authors": [
                "Z.-D. Huang",
                "G.-C. Ma"
            ],
            "title": "On the computation of an element of Clarke generalized Jacobian for a vector-valued max function",
            "venue": "Nonlinear Analysis: Theory, Methods & Applications,",
            "year": 2010
        },
        {
            "authors": [
                "M.I. Jordan",
                "T. Lin",
                "M. Zampetakis"
            ],
            "title": "On the complexity of deterministic nonsmooth and nonconvex optimization",
            "venue": "arXiv preprint arXiv:2209.12463,",
            "year": 2022
        },
        {
            "authors": [
                "K.A. Khan",
                "P.I. Barton"
            ],
            "title": "Evaluating an element of the Clarke generalized Jacobian of a composite piecewise differentiable function",
            "venue": "ACM Transactions on Mathematical Software,",
            "year": 2013
        },
        {
            "authors": [
                "S. Kong",
                "A.S. Lewis"
            ],
            "title": "The cost of nonconvexity in deterministic nonsmooth optimization",
            "venue": "arXiv preprint arXiv:2210.00652,",
            "year": 2022
        },
        {
            "authors": [
                "G. Kornowski",
                "O. Shamir"
            ],
            "title": "Oracle complexity in nonsmooth nonconvex optimization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "G. Kornowski",
                "O. Shamir"
            ],
            "title": "On the complexity of finding small subgradients in nonsmooth optimization",
            "venue": "arXiv preprint arXiv:2209.10346,",
            "year": 2022
        },
        {
            "authors": [
                "C. Lemar\u00e9chal",
                "F. Oustry",
                "C. Sagastiz\u00e1bal"
            ],
            "title": "The U -Lagrangian of a convex function",
            "venue": "Transactions of the American Mathematical Society,",
            "year": 2000
        },
        {
            "authors": [
                "A.S. Lewis"
            ],
            "title": "Active sets, nonsmoothness, and sensitivity",
            "venue": "SIAM Journal on Optimization,",
            "year": 2002
        },
        {
            "authors": [
                "J. Li",
                "A.M.-C. So",
                "W.-K. Ma"
            ],
            "title": "Understanding notions of stationarity in nonsmooth optimization: A guided tour of various constructions of subdifferential for nonsmooth functions",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "T. Lin",
                "Z. Zheng",
                "M.I. Jordan"
            ],
            "title": "Gradient-free methods for deterministic and stochastic nonsmooth nonconvex optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M.R. Metel",
                "A. Takeda"
            ],
            "title": "Perturbed iterate SGD for Lipschitz continuous loss functions",
            "venue": "Journal of Optimization Theory and Applications,",
            "year": 2022
        },
        {
            "authors": [
                "G.F. Montufar",
                "R. Pascanu",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "On the number of linear regions of deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "B. Mordukhovich",
                "Y. Shao"
            ],
            "title": "Nonsmooth sequential analysis in Asplund spaces",
            "venue": "Transactions of the American Mathematical Society,",
            "year": 1996
        },
        {
            "authors": [
                "K.G. Murty",
                "S.N. Kabadi"
            ],
            "title": "Some NP-complete problems in quadratic and nonlinear programming",
            "venue": "Mathematical Programming,",
            "year": 1987
        },
        {
            "authors": [
                "A.S. Nemirovskij",
                "D.B. Yudin"
            ],
            "title": "Problem Complexity and Method Efficiency in Optimization",
            "year": 1983
        },
        {
            "authors": [
                "Y. Nesterov"
            ],
            "title": "Introductory Lectures on Convex Optimization: A Basic Course, volume 87",
            "venue": "Springer Science & Business Media,",
            "year": 2003
        },
        {
            "authors": [
                "Y. Nesterov"
            ],
            "title": "Lexicographic differentiation of nonsmooth functions",
            "venue": "Mathematical Programming,",
            "year": 2005
        },
        {
            "authors": [
                "P.M. Pardalos",
                "S.A. Vavasis"
            ],
            "title": "Open questions in complexity theory for numerical optimization",
            "venue": "Mathematical Programming,",
            "year": 1992
        },
        {
            "authors": [
                "R.T. Rockafellar"
            ],
            "title": "Extensions of subgradient calculus with applications to optimization",
            "venue": "Nonlinear Analysis: Theory, Methods & Applications,",
            "year": 1985
        },
        {
            "authors": [
                "R.T. Rockafellar",
                "R.J.-B. Wets"
            ],
            "title": "Variational Analysis, volume 317",
            "venue": "Springer Science & Business Media,",
            "year": 2009
        },
        {
            "authors": [
                "I. Safran",
                "G. Vardi",
                "J.D. Lee"
            ],
            "title": "On the effective number of linear regions in shallow univariate ReLU networks: Convergence guarantees and implicit bias",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "S. Scholtes"
            ],
            "title": "Introduction to Piecewise Differentiable Equations",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "L. Tian",
                "A.M.-C. So"
            ],
            "title": "No dimension-free deterministic algorithm computes approximate stationarities of Lipschitzians",
            "venue": "arXiv preprint arXiv:2210.06907,",
            "year": 2022
        },
        {
            "authors": [
                "L. Tian",
                "K. Zhou",
                "A.M.-C. So"
            ],
            "title": "On the finite-time complexity and practical computation of approximate stationarity concepts of Lipschitz functions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "G. Wang",
                "G.B. Giannakis",
                "J. Chen"
            ],
            "title": "Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "C. Yun",
                "S. Sra",
                "A. Jadbabaie"
            ],
            "title": "Efficiently testing local optimality and escaping saddles for ReLU networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "H. Lin",
                "S. Jegelka",
                "A. Jadbabaie",
                "S. Sra"
            ],
            "title": "Complexity of finding stationary points of nonsmooth nonconvex functions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 2.\n12 26\n1v 1\n[ m\nat h.\nO C\n] 2\n3 Fe\nb 20\n1. Hardness: We show that checking a certain first-order approximate stationarity concept for a piecewise linear function is co-NP-hard. This implies that testing a certain stationarity concept for a modern nonsmooth neural network is in general computationally intractable. As a corollary, we prove that testing so-called firstorder minimality for functions in abs-normal form is co-NP-complete, which was conjectured by Griewank and Walther (2019, SIAM J. Optim., vol. 29, p284).\n2. Regularity: We establish a necessary and sufficient condition for the validity of an equality-type subdifferential chain rule in terms of Clarke, Fre\u0301chet, and limiting subdifferentials of the empirical loss of two-layer ReLU networks. This new condition is simple and efficiently checkable.\n3. Robust algorithms: We introduce an algorithmic scheme to test near-approximate stationarity in terms of both Clarke and Fre\u0301chet subdifferentials. Our scheme makes no false positive or false negative error when the tested point is sufficiently close to a stationary one and a certain qualification is satisfied. This is the first practical and robust stationarity test approach for two-layer ReLU networks.\n\u2217Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Sha Tin, N.T., Hong Kong SAR. E-mail: tianlai@se.cuhk.edu.hk.\n\u2020Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Sha Tin, N.T., Hong Kong SAR. E-mail: manchoso@se.cuhk.edu.hk."
        },
        {
            "heading": "1 Introduction",
            "text": "The theoretical analysis of ReLU neural network training is challenging from the optimization perspective, though the empirical performance of various \u201cgradient\u201d-based algorithms is surprisingly good. A key difficulty comes from the entanglement of nonconvexity and nonsmoothness in the objective function of the empirical loss, which causes not only the notion of gradient from classical analysis meaningless, but also the subdifferential set from convex analysis vacuous. Consequently, the study of such a nonconvex nondifferentiable function requires the use of tools from variational analysis (Rockafellar and Wets, 2009).\nFor a continuously differentiable function f : Rd \u2192 R, a point x \u2208 Rd is called stationary (or critical) if \u2207f(x) = 0. However, the situation is much more complicated when f is nondifferentiable at x. Indeed, there are many different stationarity concepts (see Definition 6) for nonsmooth functions (Li et al., 2020; Cui and Pang, 2021). For general Lipschitz functions, recently, under the oracle complexity framework of Nemirovskij and Yudin (1983), substantial progress has been made on the design of provable algorithms for finding approximately stationary (in the sense of perturbed) points (Zhang et al., 2020; Tian et al., 2022; Davis et al., 2022; Lin et al., 2022; Metel and Takeda, 2022; Kong and Lewis, 2022) and also on establishing the hardness of computing such approximate stationary points (Kornowski and Shamir, 2022a; Tian and So, 2022; Kornowski and Shamir, 2022b; Jordan et al., 2022).\nAs a complement to these developments, in this paper, we consider the complexity of and robust algorithms for checking whether a given neural network is an (approximately) stationary one with respect to the empirical loss. This is a task already considered by Yun et al. (2018). We emphasize that \u201cchecking\u201d and \u201cfinding\u201d are two very different computational problems. While the co-NP-hardness of checking the local optimality of a given point in smooth nonconvex programming was shown by Murty and Kabadi (1987) in 1987, the complexity of \u201cfinding\u201d a local minimizer was an open question proposed by Pardalos and Vavasis (1992) since 1992, and is recently settled by Ahmadi and Zhang (2022).\nGiven a neural network with smooth elemental components, testing the (approximate) stationarity of a point is simply an application of the classic gradient chain rule. In a modern computational environment, this is usually done by using Algorithmic Differentiation (AD) (Griewank and Walther, 2008) software, e.g., PyTorch and TensorFlow. A natural question that arises is whether testing the stationarity for a piecewise smooth function (e.g., empirical loss of a ReLU network) is as easy as testing for a smooth one. Surprisingly, we show (in Theorem 10) that such testing is, in general, computationally intractable.\nThe difficulty here is due to the failure of an exact (equality-type) subdifferential chain rule. For a general locally Lipschitz function, the calculus rules are only known to hold in the form of set inclusions rather than equalities, except in several special cases (see Fact 8). This prevents one from computing the subdifferential set of the empirical loss with that of elemental components. Thus, to facilitate the tractability of stationarity testing, it is of interest to find out a condition, under which an equality-type chain rule holds, and the subdifferential set of the empirical loss can be characterized. By contrast, given a first-order oracle providing the whole generalized subdifferential set at the queried point in the oracle framework (Kornowski and Shamir, 2022a; Tian and So, 2022; Kornowski and Shamir, 2022b; Jordan et al., 2022), the stationarity testing task reduces to a simple linear program, which can be solved by interior-point methods in polynomial\ntime. However, in practice, even computing an element in the generalized subdifferential for a nonsmooth function can be highly non-trivial (Burke et al., 2002; Nesterov, 2005; Huang and Ma, 2010; Khan and Barton, 2013). Therefore, a condition for the validity of the exact chain rule could be useful for subgradient computation and stationarity testing and analysis.\nThe most closely related work to ours is the one by Yun et al. (2018). They considered a two-layer ReLU network and introduced a theoretical algorithm to sequentially check Clarke stationarity (see Definition 6), Fre\u0301chet stationarity, and a certain secondorder optimality condition. For Fre\u0301chet stationarity testing, they proposed to verify the nonnegativity of a directional derivative in every possible direction, for which a trivial test in the worst case requires checking exponentially many inequalities. By exploiting polyhedral geometry, they showed that it suffices to check only extreme rays, which can be done in polynomial time. A limitation of the work (Yun et al., 2018) (see also the discussion in (Yun et al., 2018, Section 5)) is that the algorithm therein can only perform exact stationarity testing (see Section 5.1). That is to say if the objective function is x 7\u2192 |x|, then the algorithm in (Yun et al., 2018) will certify stationarity if and only if x = 0. However, as pointed out by Yun et al. (2018, Section 5), in practice, such an exact nondifferentiable point is almost impossible to reach. Therefore, it is desirable to have a robust stationarity testing algorithm that works for points sufficiently close to a stationary one. In other words, we are interested in testing so-called near-approximate stationarity (see Definition 25). We mention that, without exploiting structures in the nonsmooth objective function, such robust testing is impossible in general (Tian and So, 2022, Theorem 2.7)."
        },
        {
            "heading": "1.1 Our Results and Techniques",
            "text": "Hardness. Our first main result shows that checking certain first-order approximate stationarity concept for an unconstrained piecewise differentiable function is co-NP-hard (see Theorem 10). This implies that testing a certain stationarity concept for a shallow modern convolutional neural network is co-NP-hard (see Corollary 12). Our reduction is from the 3-satisfiability (3SAT) to a stationarity testing problem. As a corollary, we prove that testing so-called first-order minimality (FOM) for functions in abs-normal form is co-NP-complete (see Corollary 11) and give an affirmative answer to a conjecture of Griewank and Walther (2019, SIAM J. Optim., vol. 29, p284).\nOur other results concern the empirical loss of a two-layer ReLU network, which was also studied by Yun et al. (2018). Given the training data {(xi, yi)}Ni=1 \u2286 Rd\u00d7R with the x = (x\u0303, 1) parametrization, we first make the following blanket assumptions.\nAssumption 1 (Blanket assumptions). The loss function \u2113 : R\u00d7 R\u2192 R is smooth and has locally Lipschitz gradient. For simplicity of notation, we write \u2113i(\u00b7) for \u2113(\u00b7, yi). For any i \u2208 [N ], we assume xi 6= 0, which is superfluous for the x = (x\u0303, 1) parametrization.\nThe empirical loss of a two-layer ReLU neural network with H hidden nodes can be written as\nL(u1,w1, . . . , uH ,wH) :=\nN\u2211\ni=1\n\u2113i\n( H\u2211\nk=1\nuk \u00b7max { w\u22a4k xi, 0 }) .\nRegularity. By na\u0308\u0131vely abusing the convex subdifferential chain rule for L, we consider the following \u201cgeneralized subdifferential\u201d of the empirical loss L as\nG\u0303 := N\u2211\ni=1\n\u03c1i \u00b7 H\u220f\nk=1\n{ max { w\u22a4k xi, 0 }} \u00d7 { { uk \u00b7 xi \u00b7 1w\u22a4 k xi>0 } if w\u22a4k xi 6= 0,\nuk \u00b7 xi \u00b7 [0, 1] if w\u22a4k xi = 0,\nwith \u03c1i := \u2113 \u2032 i (\u2211H k=1 uk \u00b7max { w\u22a4k xi, 0 }) ,\u2200i \u2208 [N ]. This \u201cgeneralized subdifferential\u201d is popular in practical computation and theoretical analysis. For example, see (Wang et al., 2019, Equation (9)), (Arora et al., 2019, Section 3.1), and (Safran et al., 2022, Equations (5) and (6)). However, as L is nonconvex and nonsmooth, we can only assert a fuzzy chain rule (see (Clarke, 1990, Section 2.3)) for the Clarke subdifferential \u2202CL of L, which is a set inclusion \u2202CL(u1,w1, . . . , uH ,wH) \u2286 G\u0303 rather than an equation.\nOur second main result is a necessary and sufficient condition for the validity of a series of equality-type subdifferential chain rules for the empirical loss of this shallow ReLU network. We show that, under this regularity condition, exact chain rules hold for three commonly used generalized subdifferentials, i.e., Clarke (see Definition 2 and Theorem 14), limiting (see Definition 4 and Theorem 16), and Fre\u0301chet (see Definition 3 and Theorem 17). It is notable that while sufficient conditions for the equality-type calculus rules are rather rich in the literature (see (Rockafellar and Wets, 2009, Chapter 10)), a necessary condition is rarely seen, let alone an efficiently computable, necessary and sufficient condition in our Theorem 14.\nRobust algorithms. Our third main result is an algorithmic scheme to test the so-called near-approximate stationarity (see Definition 25) in terms of both Clarke and Fre\u0301chet subdifferentials. We show that, for an approximate stationary point x\u2217, any point that is sufficiently close to x\u2217 can be certified (with Algorithm 4) as near-approximate stationary. Our technique is a new rounding scheme (see Algorithm 3) motivated by the notion of active manifold identification (Lewis, 2002; Lemare\u0301chal et al., 2000) in the literature. This new rounding scheme is capable of identifying the activation pattern of the target stationary point and finding a nearby point with the same pattern. One notable application of such a near-approximate stationarity test is to obtain a termination criterion for algorithms that only have asymptotic convergence results. For example, every limiting point of the sequence generated by the stochastic subgradient method has been shown to be Clarke stationary (see Definition 6) by Davis et al. (2020, Corollary 5.11), but it is still unclear when to terminate the algorithm, and how to certify the obtained point is at least close to some Clarke stationary point, as the norm of any vector in the subdifferential is almost surely lower bounded away from zero during the entire trajectory (consider running the subgradient method on x 7\u2192 |x|).\nNotation. Scalars, vectors and matrices are denoted by lowercase letters, boldface lower case letters, and boldface uppercase letters, respectively. The notation used in this paper is mostly standard: B\u03b5(x) := {v : \u2016v \u2212 x\u2016 6 \u03b5} (we may write Bd\u03b5(x) to emphasize the dimension); dist(x, S) := infv\u2208S \u2016v \u2212 x\u2016 for a closed set S, which is defined as +\u221e if the set S = \u2205; Conv(S) denotes the convex hull of the set S; the vector ei denotes the i-th column of identity matrix I; R+ := {x \u2208 R : x > 0}; \u03c0i denotes the project to the i-th argument operator; i.e., \u03c0i (\u220fn j=1 Sj ) := Si for sets {Si}ni=1; the extended-real R is defined as R \u222a {\u2212\u221e,+\u221e}; the addition of two sets is always understood in the sense of Minkowski; Z := Z \u222a {\u2212\u221e,\u221e}; [m] := {1, . . . ,m} for any integer m > 1.\nOrganization. We introduce the background on generalized differentiation theory and formal definitions of stationarity concepts in Section 2. Then, in Section 3, we present our main hardness results. The necessary and sufficient condition of the validity of chain rule in terms of various subdifferential constructions is presented in Section 4. We discuss the robust algorithms to test near-approximate stationarity concepts in Section 5. All proofs are deferred to the Appendices."
        },
        {
            "heading": "2 Preliminaries",
            "text": "The following construction of subdifferential by Clarke (1990, Theorem 2.5.1) is classic.\nDefinition 2 (Clarke subdifferential). Given a point x, the Clarke subdifferential of a locally Lipschitz function f at x is defined by\n\u2202Cf(x) := Conv { s : \u2203x\u2032\u2192x,\u2207f(x\u2032) exists,\u2207f(x\u2032)\u2192s } .\nFor a locally Lipschitz function, the Clarke subdifferential is always nonempty, convex, and compact (Clarke, 1990, Proposition 2.1.2(a)). The following set generated by a directional derivative f \u2032 is known as the Fre\u0301chet subdifferential of f (Rockafellar and Wets, 2009, Exercise 8.4).\nDefinition 3 (Fre\u0301chet subdifferential). Given a point x, the Fre\u0301chet subdifferential of a locally Lipschitz and directional differentiable function f at x is defined by\n\u2202\u0302f(x) := { s : s\u22a4d 6 f \u2032(x;d) for all d } .\nThe set-valued mapping \u2202\u0302f of Fre\u0301chet subdifferential of f is not outer semicontinuous (see (Rockafellar and Wets, 2009, Definition 5.4)), which means that given x\u03bd \u2192 x,g\u03bd \u2192 g with g\u03bd \u2208 \u2202\u0302f(x\u03bd), we cannot assert g \u2208 \u2202\u0302f(x). The following limiting subdifferential (or the Mordukhovich subdifferential) (Rockafellar and Wets, 2009, Definition 8.3(b)) is more robust for analysis.\nDefinition 4 (Limiting subdifferential). Given a point x, the limiting subdifferential of a locally Lipschitz and directional differentiable function f at x is defined by\n\u2202f(x) := lim sup x\u2032\u2192x\n\u2202\u0302f(x\u2032),\nwhere the outer limit is taken in the sense of Kuratowski (see, e.g., (Rockafellar and Wets, 2009, p152, Equation 5(1))).\nIn the following result, we record a generalized Fermat\u2019s rule for optimality conditions and the relationship among the aforementioned three subdifferentials.\nFact 5 (Rockafellar and Wets (2009, Theorem 8.6, 8.49, 10.1)). Given a locally Lipschitz function f : Rd \u2192 R and a point x \u2208 Rd, then we have \u2202\u0302f(x) \u2286 \u2202f(x) \u2286 \u2202Cf(x). If the point x is a local minimizer of the function f , then it holds that 0 \u2208 \u2202\u0302f(x).\nWe are now ready to state the definitions of various stationarity concepts.\nDefinition 6 (Stationarity concepts). Given a locally Lipschitz function f : Rd \u2192 R, we say that the point x \u2208 Rd is an\n\u2022 \u03b5-Clarke stationary point if dist ( 0, \u2202Cf(x) ) 6 \u03b5; \u2022 \u03b5-Fre\u0301chet stationary point if dist ( 0, \u2202\u0302f(x) ) 6 \u03b5; \u2022 \u03b5-limiting stationary point if dist ( 0, \u2202f(x) ) 6 \u03b5.\nThe following Clarke regularity for locally Lipschitz and directional differentiable functions is a classic notion related to the validity of various subdifferential calculus rules; see (Clarke, 1990, Definition 2.3.4) and (Rockafellar and Wets, 2009, Corollary 8.11).\nDefinition 7 (Clarke regularity). For a locally Lipschitz directional differentiable function f : Rd \u2192 R and a point x, one has f is Clarke regular at x if \u2202Cf(x) = \u2202\u0302f(x).\nWe record some basic equality-type calculus rules for Clarke subdifferential as follows; see (Clarke, 1990, Proposition 2.3.3, Theorem 2.3.10), and (Rockafellar, 1985, Proposition 2.5). We refer the reader to (Rockafellar and Wets, 2009, Chapter 10) for similar calculus rules for Fre\u0301chet and limiting subdifferentials.\nFact 8 (Calculus rules). Let f : Rd \u2192 R, g : Rd \u2192 R be two locally Lipschitz functions. \u2022 If f is strictly differentiable at x, then \u2202C(f + g)(x) = \u2207f(x) + \u2202Cg(x);\n\u2022 If h(x,y) = f(x) + g(y), then \u2202Ch(x,y) = \u2202Cf(x)\u00d7 \u2202Cg(y);\n\u2022 Given a strictly differentiable mapping G : Rn \u2192 Rd and a point y \u2208 Rn, if the function f (or \u2212f) is Clarke regular at G(y), then f \u25e6 G (or \u2212f \u25e6 G) is Clarke regular at y and \u2202C [f \u25e6 G](y) = (JG(y))\u22a4\u2202Cf(G(y)), where JG is the Jacobian of mapping G. The equality also holds when JG is surjective.\nRemark 9. The sum rule is a special case of the chain rule, which does not hold for Lipschitz functions trivially. For example, consider \u2202C [| \u00b7 | \u2212 | \u00b7 |](0) = {0} ( \u2202C [| \u00b7 |](0) + (\u2212\u2202C [| \u00b7 |](0)) = [\u22122, 2]. The empirical loss of a ReLU network is in general not Clarke regular. To see this, let f(x, y) = max{x, 0}\u2212max{y, 0}. It is easy to verify neither f nor \u2212f is Clarke regular. Another remark here is on the notion of partial subdifferentiation; see (Rockafellar and Wets, 2009, Corollary 10.11) and (Clarke, 1990, p48). In general, we cannot say much about the relationship between \u2202f(x,y) and \u2202xf(x,y)\u00d7\u2202yf(x,y) (see (Clarke, 1990, Example 2.5.2)), except the following inclusion (Clarke, 1990, Proposition 2.3.16): \u2202xf(x,y)\u00d7 \u2202yf(x,y) \u2286 \u03c01\u2202f(x,y)\u00d7 \u03c02\u2202f(x,y)."
        },
        {
            "heading": "3 Hardness of Stationarity Testing",
            "text": "For smooth nonconvex programming, co-NP-hardness has been shown for local optimality testing (Murty and Kabadi, 1987, Theorem 2) and second-order sufficient condition testing (Murty and Kabadi, 1987, Theorem 4). However, in the nonsmooth case, we show that checking a first-order necessary condition approximately in terms of certain subdifferential is already co-NP-hard. Theorem 10 (Testing of piecewise linear functions). Given a 3 \u221a d-Lipschitz piecewise linear function f : Rd \u2192 R in the form of max\u2013min representation1 with integer data. For any \u03b7 \u2208 (d,+\u221e] \u2229 Z, checking whether the point 0 \u2208 Zd satisfying dist ( 0, \u2202\u0302f(0) ) 6 1/\u221a\u03b7 is co-NP-hard, and checking whether 0 \u2208 \u2202\u0302f(0) is strongly co-NP-hard. 1Any piecewise linear function f : Rd \u2192 R can be written using a max-min representation as f(x) = max16i6l minj\u2208Mi a \u22a4 j x + bj , where Mi \u2286 [m] is a finite index set; see (Scholtes, 2012, Proposition 2.2.2). The input data are d \u2208 N,m \u2208 N, l \u2208 N, {(aj , bj)} m j=1, and {Mi} l i=1.\nWe compare Theorem 10 with the classic hardness result of Murty and Kabadi (1987). In (Murty and Kabadi, 1987), checking the local optimality of a simply constrained indefinite quadratic problem (Murty and Kabadi, 1987, Problem 1) and of an unconstraint quartic polynomial objective (Murty and Kabadi, 1987, Problem 11) are both co-NP-complete. However, these hardness results are inapplicable for checking first-order necessary conditions. In fact, for any hard construction f : Rn \u2192 R in (Murty and Kabadi, 1987) and a given point x \u2208 Qn, testing 0 \u2208 \u2202\u0302f(x) can be done in polynomial time with respect to the input size. In Theorem 10, we show that for a class of simple unconstrained piecewise differentiable functions, even an approximate test of the first-order necessary condition 0 \u2208 \u2202\u0302f(x) for a certain point x is already computationally intractable.\nNonsmooth functions in real-world applications usually contain structures that can be exploited in theoretical analysis and algorithmic design. A subclass of piecewise differentiable functions, termed Cdabs or functions representable in abs-normal form, and defined as the composition of smooth functions and the absolute value function, is introduced by Griewank (2013); see Appendix A for a brief introduction and (Griewank and Walther, 2019, Definition 2.1) for details. An important corollary of our hard construction concerns the complexity of checking an optimality condition for functions in Cdabs. The following result gives an affirmative answer to a conjecture of Griewank and Walther (2019, p284):\nCorollary 11 (Testing of abs-normal form). Testing first order minimality (FOM) for a piecewise differentiable function given in the abs-normal form is co-NP-complete.\nNow, we report another notable corollary about the complexity of testing a certain stationarity concept for the empirical loss of a modern convolutional neural network.\nCorollary 12 (Testing of loss of nonsmooth networks). Let f : Rd \u2192 R be the empirical loss function of a shallow neural network with ReLU activation function, max-pooling operator, and convolution operator. Suppose the width of the first layer is m. Then, for any \u03b7 \u2208 (m,+\u221e] \u2229 Z, testing the 1/\u221a\u03b7-Fre\u0301chet stationarity dist ( 0, \u2202\u0302f(\u03b8) ) 6 1/\u221a\u03b7 for a certain \u03b8 \u2208 Qd is co-NP-hard, and testing 0 \u2208 \u2202\u0302f(\u03b8) for \u03b8 is strongly co-NP-hard.\nCorollary 12 shows a computational tractability separation for the stationarity test between smooth and nonsmooth networks. In the smooth setting, given the gradient of every component function, we can compute the gradient norm of the loss function by iteratively applying chain rule. But in the nonsmooth case, while the subdifferential of every elemental function can be computed easily, the validity of the subdifferential chain rule like those in Fact 8 is not justified, which turns out to cause a serious computational hurdle in stationarity test (strong co-NP-hardness)."
        },
        {
            "heading": "4 Regularity Conditions",
            "text": "In this section, we study the regularity conditions for the validity of the equality-type chain rule in terms of Clarke, Fre\u0301chet, and limiting subdifferentials of the empirical loss of two-layer ReLU networks."
        },
        {
            "heading": "4.1 Setup",
            "text": "For simplicity of reference, we introduce the following notation, which will be used in various subdifferential constructions of the empirical loss L.\nDefinition 13. Let the parameters {(uk,wk)}Hk=1 be given. We define the following shorthands:\n(a) We write constants \u03c1i := \u2113 \u2032 i (\u2211H k=1 uk \u00b7max { w\u22a4k xi, 0 }) for any i \u2208 [N ].\n(b) For any k \u2208 [H] and wk \u2208 Rd, we define the following two indices sets:\nI+k (wk) := { i : w\u22a4k xi = 0, uk \u00b7 \u03c1i > 0, i \u2208 [N ] } , I\u2212k (wk) := { i : w\u22a4k xi = 0, uk \u00b7 \u03c1i < 0, i \u2208 [N ] } .\nWe may write I+k and I\u2212k when the reference point wk is clear from the context.\n(c) For any k \u2208 [H], we define the following nonempty convex compact set GCk \u2286 Rd related to the Clarke subdifferential:\nGCk := \u2211\ni\u2208[N ]\\(I+ k \u222aI\u2212 k )\nuk\u03c1i \u00b7 1w\u22a4 k xi>0\n\u00b7 xi + \u2211\nj\u2208I+ k \u222aI\u2212 k\nuk\u03c1j \u00b7 xj \u00b7 [0, 1].\n(d) For any k \u2208 [H], we define the following nonempty compact set GLk \u2286 Rd related to the limiting subdifferential:\nGLk := \u2211\ni\u2208[N ]\\(I+ k \u222aI\u2212 k )\nuk\u03c1i \u00b7 1w\u22a4 k xi>0 \u00b7 xi\n+ \u2211\nj\u2208I+ k\nuk\u03c1ixj \u00b7 [0, 1] +    \u2211\nj\u2208I\u2212 k\nuk\u03c1i \u00b7 1d\u22a4xj>0 \u00b7 xj : \u2203d \u2208 Rd, min t\u2208I\u2212\nk\n\u2223\u2223\u2223x\u22a4t d \u2223\u2223\u2223 > 0    .\n(e) For any k \u2208 [H], we define the following convex compact set GFk \u2286 Rd related to the Fre\u0301chet subdifferential:\nGFk := \u2211\ni\u2208[N ]\\(I+ k \u222aI\u2212 k )\nuk\u03c1i \u00b7 1w\u22a4 k xi>0\n\u00b7 xi + \u2211\nj\u2208I+ k\nuk\u03c1jxj \u00b7 [0, 1] + { \u2205 if \u2223\u2223I\u2212k \u2223\u2223 > 0,\n0 if \u2223\u2223I\u2212k \u2223\u2223 = 0.\n(f) If an equation holds for all the three subdifferentials, i.e., Clarke/limiting/Fre\u0301chet subdifferentials (\u2202Cf/\u2202f/\u2202\u0302f), we will write the equation simply with \u2202\u22b3f and also G\u22b3k (for G C k /G L k /G F k ). For example, if the equation \u2202\u22b3fk(wk) = G \u22b3 k holds , then we\nget \u2202Cfk(wk) = G C k , \u2202fk(wk) = G L k , and \u2202\u0302fk(wk) = G F k ."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Theorem 14 (Clarke chain rule). Under Assumption 1, we claim that the exact Clarke subdifferential chain rule holds for L at a given point (u1,w1, . . . , uH ,wH), that is\n\u2202CL(u1,w1, . . . , uH ,wH) =\nH\u220f\nk=1\n{ N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7GCk ,\nif and only if the data points {xi}Ni=1 satisfy the following Span Qualification (SQ):\n\u22c3 16k6H span ( {xi}i\u2208I+ k ) \u2229 span ( {xj}j\u2208I\u2212 k ) = {0}. (SQ)\nRemark 15. Note that for any k \u2208 [H], the indices sets I\u2212k and I+k can be computed in O(Nd). Then, checking SQ is no harder than checking the Linear Independence Constraint Qualification (LICQ) in nonlinear programming and can be done with, e.g., Zassenhaus algorithm.\nTheorem 16 (Limiting chain rule). Under Assumption 1, we claim that the exact limiting subdifferential chain rule holds for L at a given point (u1,w1, . . . , uH ,wH), that is\n\u2202L(u1,w1, . . . , uH ,wH) =\nH\u220f\nk=1\n{ N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7GLk ,\nif and only if the data points {xi}Ni=1 satisfy SQ.\nFor Fre\u0301chet subdifferential, the situation is different as the default chain rule is the re-\nverse set inclusion \u2202\u0302L(u1,w1, . . . , uH ,wH) \u2287 \u220fH\nk=1 {\u2211N i=1 \u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7GFk ; see\n(Rockafellar and Wets, 2009, Corollary 10.9, Theorem 10.49). If \u2202\u0302L(u1,w1, . . . , uH ,wH) = \u2205, we have the exact chain rule trivially, as GFk can only be the empty set. Therefore, the interesting case is when the Fre\u0301chet subdifferential is nonempty.\nTheorem 17 (Fre\u0301chet chain rule). Under Assumption 1, for any given point such that the subdifferential \u2202\u0302L(u1,w1, . . . , uH ,wH) 6= \u2205, we have the following exact chain rule for the empirical loss L\n\u2202\u0302L(u1,w1, . . . , uH ,wH) =\nH\u220f\nk=1\n{ N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7GFk ,\nif and only if the data points {xi}Ni=1 satisfy SQ."
        },
        {
            "heading": "4.3 Discussion",
            "text": "There are several existing regularity conditions related to the validity of exact chain rule of the empirical loss. We briefly introduce them here and defer the details to the Definition 54 in Appendix C.5.\nDefinition 18 (Regularities). We consider the following regularity conditions:\n\u2022 General position data: (Montufar et al., 2014, Section 2.2), (Yun et al., 2018, Assumption 2), and (Bubeck et al., 2020);\n\u2022 Linear Independence Kink Qualification (LIKQ): (Griewank and Walther, 2019, Definition 2.6) and (Griewank and Walther, 2016, Definition 2);\n\u2022 Linearly Independent Activated Data (LIAD): Let the index set Jk := {j : w\u22a4k xj = 0}. For any fixed k \u2208 [H], the data points {xi}i\u2208Jk are linearly independent.\nThe general position assumption is from the study of hyperplane arrangement. If the data points are generated from an absolutely continuous probability measure (with respect to the Lebesgue measure), then they are in general position almost surely. The LIKQ is introduced by Griewank and Walther (2016, Definition 2) to ensure an efficient Fre\u0301chet stationarity test for piecewise differentiable function represented in abs-normal form. See Appendix A for a brief introduction. The LIAD condition is natural and equivalent to the subjectivity condition in Fact 8. Let us present the following result, in which we establish the relationship among SQ and the three other regularity conditions in Definition 18.\nProposition 19 (Regularity comparison). For the empirical loss of a shallow ReLU network under Assumption 1, we have the following relationship:\ngeneral position =\u21d2 LIKQ \u21d0\u21d2 LIAD =\u21d2 SQ.\nWe exhibit two examples to show the one-side arrows in Proposition 19 are strict.\nExample 20 (SQ ; LIAD). Let the function f : R4 \u2192 R be given as\nf(x, y, z, b) := max{2y+b, 0}+max{2x+2z+b, 0}+max{x+y+z+b, 0}\u2212max{x\u2212z+b, 0}.\nConsider x = y = z = b = 0. It is easy to verify that SQ is satisfied but not LIAD. Besides, f is nonconvex, nonsmooth, and non-separable. Neither f nor \u2212f is Clarke regular. But by Theorem 14, the equality-type subdifferential sum rule still holds.\nExample 21 (LIAD ; general position). Let the function f : R3 \u2192 R be given as\nf(x, y, b) := max{\u22122y + b, 0}+max{\u2212y + b, 0}+max{x+ b, 0} \u2212max{y + b, 0}.\nConsider x = y = 1 and b = \u22121. LIAD is satisfied, but the data is not in general position. In practice, for data x \u2208 Rd, if the features of data include a discrete-valued component, e.g., x1 \u2208 {\u22121,+1}, then the points {xi}Ni=1 are rarely in general position, as at least half of them must lie in the same affine hyperplane {y : e\u22a41 y = 1} or {y : e\u22a41 y = \u22121}. Remark 22 (GLk for general position data). Besides, if the data points are in general position, we have the following compact representation for GLk\nGLk = \u2211\ni\u2208[N ]\\(I+ k \u222aI\u2212 k )\nuk\u03c1i \u00b7 1w\u22a4 k xi>0\n\u00b7 xi + \u2211\nj\u2208I+ k\nuk\u03c1jxj \u00b7 [0, 1] + \u2211\nj\u2032\u2208I\u2212 k\nuk\u03c1j\u2032xj\u2032 \u00b7 {0, 1}.\nThe following corollary concerning the Clarke regularity of all local minimizers could be of independent interest.\nCorollary 23. If at a point, SQ is satisfied and the empirical loss function L has nonempty Fre\u0301chet subdifferential here, then the function L is Clarke regular at that point. Consequently, with data in general position, L is Clarke regular at every local minimizer."
        },
        {
            "heading": "5 Testing of Stationarity Concepts",
            "text": "To perform the stationarity test, we need the following quantitative regularities to characterize the curvature of the pieces in the empirical loss.\nAssumption 24. In this section, we further assume that for any i \u2208 [N ], the norm of data \u2016xi\u20162 6 R and the function \u2113i is L\u2113-Lipschitz continuous with an L\u2113\u2032-Lipschitz continuous gradient \u2113\u2032i"
        },
        {
            "heading": "5.1 Exact Stationarity Test",
            "text": "As an immediate illustration of the results in Section 4, we record the following exact testing schemes for Clarke and Fre\u0301chet stationary points. Compared with the developments in (Yun et al., 2018) which check the Fre\u0301chet stationarity from the primal perspective and use polyhedral geometry to avoid redundant computation, by using Theorem 17, our treatment for Fre\u0301chet stationarity is transparent and its correctness is self-evident.\nAlgorithm 1 Exact Stationarity Test (Clarke)\n1: procedure ETest-C(u1,w1, . . . , uH ,wH , x1, . . . ,xN ) 2: compute {\u03c1i}Ni=1, I+k (wk), and I\u2212k (wk) for any k \u2208 {1, . . . ,H}; 3: if Span Qualification (SQ) is not satisfied then 4: return not-SQ; 5: end if 6: for k \u2208 {1, . . . ,H} do 7: compute \u03b51,k \u2190 \u2223\u2223\u2223 \u2211N i=1 \u03c1i \u00b7max { w\u22a4k xi, 0\n}\u2223\u2223\u2223; 8: compute \u03b52,k \u2190 dist ( 0, GCk ) ; \u22b2 convex QP 9: end for\n10: return \u221a\u2211H k=1 (\u03b51,k) 2 + (\u03b52,k) 2; 11: end procedure\nClarke stationarity. Suppose that SQ is satisfied at the point (u1,w1, . . . , uH ,wH). By Theorem 14, it is a Clarke stationarity point of L if and only if, for any k \u2208 [H],\n(a) 0 = \u2211N i=1 \u03c1i \u00b7max { w\u22a4k xi, 0 } ; (b) 0 \u2208\u2211i\u2208[N ]\\(I+ k \u222aI\u2212 k ) uk\u03c1i \u00b7 1w\u22a4k xi>0 \u00b7 xi + \u2211 j\u2208I+ k \u222aI\u2212 k uk\u03c1jxj \u00b7 [0, 1].\nCondition (a) is a simple equality test and condition (b) can be checked by solving a linear programming problem. Algorithm 1 is for testing \u03b5-Clarke stationary points.\nAlgorithm 2 Exact Stationarity Test (Fre\u0301chet)\n1: procedure ETest-F(u1,w1, . . . , uH ,wH , x1, . . . ,xN ) 2: compute {\u03c1i}Ni=1, I\u2212k (wk), and I+k (wk) for any k \u2208 {1, . . . ,H}; 3: if Span Qualification in (SQ) is not satisfied then 4: return not-SQ; 5: end if 6: for k \u2208 {1, . . . ,H} do 7: if I\u2212k (wk) 6= \u2205 then 8: return +\u221e; 9: end if\n10: compute \u03b51,k = \u2223\u2223\u2223 \u2211N i=1 \u03c1i \u00b7max { w\u22a4k xi, 0 }\u2223\u2223\u2223; 11: compute \u03b52,k = dist ( 0, GFk ) ; \u22b2 convex QP 12: end for\n13: return \u221a\u2211H k=1 (\u03b51,k) 2 + (\u03b52,k) 2; 14: end procedure\nFre\u0301chet stationarity. Suppose that SQ is satisfied at the point (u1,w1, . . . , uH ,wH). By Theorem 17, it is a Fre\u0301chet stationarity point of L if and only if, for any k \u2208 [H],\n(a) 0 = \u2211N i=1 \u03c1i \u00b7max { w\u22a4k xi, 0 } ;\n(b) I\u2212k = \u2205; (c) 0 \u2208\u2211i\u2208[N ]\\(I+ k \u222aI\u2212 k ) uk\u03c1i \u00b7 1w\u22a4k xi>0 \u00b7 xi + \u2211 j\u2208I+ k uk\u03c1jxj \u00b7 [0, 1].\nSimilarly, all above conditions can be checked in polynomial time with Algorithm 2."
        },
        {
            "heading": "5.2 Robust Stationarity Test",
            "text": "In this subsection, we introduce our main algorithmic results. First, we formally define the notion of stationarities that we are aiming to check; see (Davis and Drusvyatskiy, 2019; Kornowski and Shamir, 2022a; Tian et al., 2022) for results on finding near-approximately stationary points for Lipschitz functions.\nDefinition 25 (Near-Approximate Stationarity, NAS). Given a locally Lipschitz function f : Rd \u2192 R, we say that the point x \u2208 Rd is an\n\u2022 (\u03b5, \u03b4)-Clarke NAS point, if dist ( 0,\u222ay\u2208B\u03b4(x)\u2202Cf(y) ) 6 \u03b5; \u2022 (\u03b5, \u03b4)-Fre\u0301chet NAS point, if dist ( 0,\u222ay\u2208B\u03b4(x)\u2202\u0302f(y) ) 6 \u03b5.\nWe consider a constructive approach, that is, we certify the (\u03b5, \u03b4)-Clarke NAS of a point x for the function f only if we find a point y \u2208 B\u03b4(x) satisfying dist(0, \u2202Cf(y)) 6 \u03b5. Note that, in any time, if a point y \u2208 B\u03b4(x) passes the exact stationarity test, say, with Algorithm 1, then x must be an (\u03b5, \u03b4)-Clarke NAS point. In other words, there is no false positive in the test. The question is that, if x is sufficiently closed to a Clarke stationary point, can we always find a point y near x such that y is \u03b5-Clarke stationary? That is to say, we need to control the false negative of our robust test. Without exploiting structures in the objective function, finding such a point is impossible in general (Tian and So, 2022, Theorem 2.7). Our technique is a new rounding scheme (see Algorithm 3), which is motivated by the notion of active manifold identification (Lewis, 2002; Lemare\u0301chal et al., 2000) in the literature. This new rounding scheme is capable to identify the activation pattern of the target stationary point that x is sufficiently close to.\nNow, suppose that f is L-smooth and a point x\u2217 satisfies \u2016\u2207f(x\u2217)\u2016 6 \u03b5. Without knowing the concrete structure of f , what we can say for any point y \u2208 B\u03b4(x\u2217) is that \u2016\u2207f(y)\u2016 6 \u03b5+L \u00b7\u03b4, which is the best result we can hope for our test, as we do not assume any concrete structure in the loss \u2113i except their smoothness. Such an estimation cannot hold trivially for a nonsmooth function. Consider f(x) = |x| and x\u2217 = 0. For any \u03b4 > 0 and 0 6= y \u2208 B\u03b4(x\u2217), we have |f \u2032(y)| = 1."
        },
        {
            "heading": "5.2.1 Testing Clarke NAS",
            "text": "We define two constants that will be used in the analysis.\nDefinition 26 (Clarke). Given a point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) with a Euclidean norm B \u2208 [0,+\u221e), we define the following constants about the separation and curvature of pieces around this point:\nAlgorithm 3 Neural Rounding (Clarke)\n1: procedure Rnd-C(u1,w1, . . . , uH ,wH , \u03b4, x1, . . . ,xN ) 2: compute R = max16i6N \u2016xi\u2016; 3: for k \u2208 {1, . . . ,H} do 4: compute w\u0302k by solving the following convex QP\nw\u0302k = argmin z\u2208Rd\n\u2016z \u2212wk\u20162\ns.t. z\u22a4xi > 2R \u00b7 \u03b4, \u2200i \u2208 [N ] : x\u22a4i wk > R \u00b7 \u03b4, z\u22a4xi 6 \u22122R \u00b7 \u03b4, \u2200i \u2208 [N ] : x\u22a4i wk < \u2212R \u00b7 \u03b4, z\u22a4xi = 0, \u2200i \u2208 [N ] : \u2223\u2223\u2223x\u22a4i wk \u2223\u2223\u2223 6 R \u00b7 \u03b4.\n5: end for 6: return (u1, w\u03021, . . . , uH , w\u0302H); 7: end procedure\nAlgorithm 4 Robust Stationarity Test (General)\n1: procedure RTest(ETest, Rnd, u1,w1, . . . , uH ,wH , \u03b4, x1, . . . ,xN ) 2: (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) = Rnd(u1,w1, . . . , uH ,wH , \u03b4,x1, . . . ,xN ); 3: if \u2016(u1,w1, . . . , uH ,wH)\u2212 (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H)\u2016 > \u03b4 then 4: return +\u221e; 5: end if 6: return ETest(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H , x1, . . . ,xN ); 7: end procedure\n\u2022 Separation: CClarke\u03c4 := 1 4R \u00b7min {\u2223\u2223x\u22a4i w\u2217k \u2223\u2223 : i \u2208 [N ], k \u2208 [H],x\u22a4i w\u2217k 6= 0 } ;\n\u2022 Curvature: CClarke\u00b5 := poly(B,R,L\u2113, L\u2113\u2032 , N,H). 2\nRemark 27. If for any i \u2208 [N ] and k \u2208 [H], it holds x\u22a4i w\u2217k = 0, then we define the separation constant CClarke\u03c4 := +\u221e, as in the optimization of extended-real-valued functions, inf \u2205 = +\u221e. It is notable that, while the separation constant CClarke\u03c4 is usually unknown when running the testing algorithm, the curvature constant CClarke\u00b5 can be easily estimated when the candidate network and the radius \u03b4 are given.\nTheorem 28 (Robust Clarke test). Let an \u03b5-Clarke stationary point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) satisfying SQ be given. For any 0 < \u03b4 6 CClarke\u03c4 and any\n(u1,w1, . . . , uH ,wH) \u2208 B\u03b4 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) ,\nif the output point (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) of Algorithm 3 satisfies SQ, then we have\ndist ( 0, \u2202CL(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) ) 6 \u03b5+ CClarke\u00b5 \u00b7 \u03b4.\nIn Theorem 28, we show that for a point that is sufficiently closed to an \u03b5-Clarke stationary one, and a properly chosen parameter \u03b4 > 0, one can correctly certify the near-approximate stationarity of this point in the style as if the function L is smooth by\n2See Appendix D.1 for the exact value.\ncalling Algorithm 4 with RTest(ETest-C,Rnd-C, \u00b7 \u00b7 \u00b7 ). A natural question here is how to choose a proper parameter \u03b4, as the separation constant CClarke\u03c4 is usually unknown. It turns out that a simple line search will work for that.\nRemark 29 (Line search). Set the initial value of radius \u03b4 to, say, \u03b40 = 1. Then, in the t-th iteration, run Algorithm 4 with parameter \u03b4t and set \u03b4t+1 = \u03b4t/2. Note that for a sufficiently small \u03b4, the rounding scheme in Algorithm 3 becomes superfluous, as for any i \u2208 [N ] and k \u2208 [H] such that x\u22a4i wk 6= 0, we have |x\u22a4i wk| > 2R \u00b7 \u03b4 for a small \u03b4. Therefore, we can stop the line search within at most\n\u2308 log2 ( 2R / min {\u2223\u2223\u2223x\u22a4i wk \u2223\u2223\u2223 : i \u2208 [N ], k \u2208 [H],x\u22a4i wk 6= 0 })\u2309\niterations. It is immediate that, if (u1,w1, . . . , uH ,wH) \u2208 BCClarke\u03c4 /2 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) , then there exists a radius \u03b4t \u2208 [CClarke\u03c4 /2, CClarke\u03c4 ] in the iteration sequence such that\ndist ( 0, \u2202CL(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) ) 6 \u03b5+ CClarke\u00b5 \u00b7 \u03b4t.\nThis search scheme also works for the Fre\u0301chet NAS test and we will not repeat that."
        },
        {
            "heading": "5.2.2 Testing Fre\u0301chet NAS",
            "text": "Algorithm 5 Neural Rounding (Fre\u0301chet)\n1: procedure Rnd-F(w1, . . . ,wH , \u03b4, x1, . . . ,xN ) 2: compute R = max16i6N \u2016xi\u2016 and Cu = L\u2113\u2032(4HRB2 + 1); 3: for k \u2208 {1, . . . ,H} do 4: compute w\u0302k by solving the following QP\nw\u0302k = argmin z\u2208Rd\n\u2016z \u2212wk\u20162\ns.t. z\u22a4xi > 2R \u00b7 \u03b4, \u2200i \u2208 [N ] : x\u22a4i wk > R \u00b7 \u03b4, z\u22a4xi 6 \u22122R \u00b7 \u03b4, \u2200i \u2208 [N ] : x\u22a4i wk < \u2212R \u00b7 \u03b4, z\u22a4xi = 0, \u2200i \u2208 [N ] : \u2223\u2223\u2223x\u22a4i wk \u2223\u2223\u2223 6 R \u00b7 \u03b4.\n5: set u\u0302k = uk; 6: if mini:w\u22a4\nk xi=0 uk \u00b7 \u03c1i 6 2Cu \u00b7 \u03b4 then 7: set u\u0302k = 0; 8: end if\n9: end for\n10: return (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H); 11: end procedure\nUnlike the Clarke case, we need the following extra nondegeneracy condition on \u2113i to identify the pattern of {u\u2217k}k and avoid the Fre\u0301chet subdifferential being empty.\nAssumption 30. Given a point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H), we assume that for any i \u2208 [N ]\nsuch that mink\u2208[H] |x\u22a4i w\u2217k| = 0, we have \u2113\u2032i (\u2211H k=1 u \u2217 k \u00b7max { (w\u2217k) \u22a4xi, 0 }) 6= 0.\nThe following two constants will be used in the analysis.\nDefinition 31 (Fre\u0301chet). Given a point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) with a Euclidean norm B \u2208 [0,+\u221e), we define two constants concerning the separation and curvature of pieces around this point:\n\u2022 Separation: CFre\u0301chet\u03c4 := min { mini\u2208[N ],k\u2208[H],\nx\u22a4i w \u2217 k 6=0\n|x\u22a4i w\u2217k| 4R ,min i\u2208[N ],k\u2208[H],\nx\u22a4i w \u2217 k =0,u\u2217 k \u00b7\u03c1\u2217i>0\nu\u2217 k \u00b7\u03c1\u2217i\nL\u2113\u2032(4HRB 2+1)\n} ;\n\u2022 Curvature: CFre\u0301chet\u00b5 := poly(B,R,L\u2113, L\u2113\u2032 , N,H). 3\nThen, for Fre\u0301chet NAS test, we have the following result similar to Theorem 28.\nTheorem 32 (Robust Fre\u0301chet test). Let an \u03b5-Fre\u0301chet stationary point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) satisfying SQ be given. For any 0 < \u03b4 6 CFre\u0301chet\u03c4 and any\n(u1,w1, . . . , uH ,wH) \u2208 B\u03b4 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) ,\nif the output point (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) of Algorithm 5 satisfies SQ, then we have\ndist ( 0, \u2202\u0302L(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) ) 6 \u03b5+ CFre\u0301chet\u00b5 \u00b7 \u03b4."
        },
        {
            "heading": "A Abs-Normal Form of Piecewise Differentiable Functions",
            "text": "We briefly review the abs-normal representation of a subclass of piecewise differentiable functions. See (Griewank, 2013; Griewank and Walther, 2016) for details."
        },
        {
            "heading": "A.1 The General Framework",
            "text": "The abs-normal representation (Griewank, 2013) is a piecewise linearization scheme concerning a certain subclass of piecewise differentiable functions in the sense of Scholtes (2012). In this subclass, functions are defined as compositions of smooth functions and the absolute value function. By identifies max{a, b} = (a + b)/2 + |a \u2212 b|/2,min{a, b} = (a + b)/2 \u2212 |a \u2212 b|/2, and max{x, 0} = x/2 + |x|/2, composition with these nonsmooth elemental functions can also be represented in the abs-normal form.\nLet \u03d5 : Rd \u2192 R be a function in such subclass. By numbering all input to the absolute value functions in the evaluation order as \u201cswitching variables\u201d zi for i \u2208 {1, . . . , s}, the function x 7\u2192 y = \u03d5(x) can be written in the following abs-normal form:\nz = F (x,p), y = f(x,p),\nwhere x \u2208 Rd,p \u2208 Rs+, the smooth mapping F : Rd \u00d7Rs+ \u2192 Rs, and the smooth function f : Rd \u00d7 Rs \u2192 R. As the numbering of {zi}i is in the evaluation order, zi is a function of zj only if j < i. In sum, we have\ny = \u03d5(x) = f(x, |z(x)|),\nwhere z(x) a successive evaluation of {zi}si=1 with given x. To see such an evaluation of z(x) is well-defined, note that z1 = F1(x) and for any 1 < i 6 s,\nzi = Fi(x, |z1|, \u00b7 \u00b7 \u00b7 , |zi\u22121|). 3See Appendix D.2 for the exact value.\nWe remark that, similar to the Difference of Convex (DC) decomposition in DC programming, the function \u03d5 may have many different abs-normal decomposition. The following vectors and matrices are useful when study the function in abs-normal form:\na := \u2202 \u2202x f(x,p) \u2208 Rd, Z := \u2202 \u2202x F (x,p) \u2208 Rs\u00d7d, b := \u2202\n\u2202p f(x,p) \u2208 Rs, L := \u2202 \u2202p F (x,p) \u2208 Rs\u00d7s.\nFor any \u03c3 \u2208 {\u22121, 1}s, we will denote by \u03a3 := Diag(\u03c3) \u2208 {\u22121, 0, 1}s\u00d7s. Let us define (see also (Griewank and Walther, 2016, Equation (11)))\n\u2207z\u03c3 := (I \u2212L\u03a3)\u22121Z \u2208 Rs\u00d7d,\nwhich will play a key role in the definition of LIKQ (see Definition 54)."
        },
        {
            "heading": "A.2 Abs-Normal Form of Shallow ReLU Networks",
            "text": "We rewrite the empirical loss of the shallow ReLU network with absolute value functions as\nL(u1,w1, \u00b7 \u00b7 \u00b7 , uH ,wH) = N\u2211\ni=1\n\u2113i\n( H\u2211\nk=1\nuk 2 \u00b7 ( w\u22a4k xi + \u2223\u2223\u2223w\u22a4k xi \u2223\u2223\u2223 )) .\nThen, as there are N \u00b7 H absolute value evaluations in total, we define the switching variable z \u2208 RNH and the smooth mapping F as\nzN(k\u22121)+i = FN(k\u22121)+i(u1,w1, \u00b7 \u00b7 \u00b7 , uH ,wH) = w\u22a4k xi, \u2200k \u2208 [H], i \u2208 [N ].\nThe smooth function f in the abs-normal form can be written as\ny = f(u1,w1, \u00b7 \u00b7 \u00b7 , uH ,wH ,p) = N\u2211\ni=1\n\u2113i\n( H\u2211\nk=1\nuk 2 \u00b7 ( w\u22a4k xi + pN(k\u22121)+i\n)) ,\nwhere p \u2208 RNH+ . Consequently, the matrix L = 0, which implies the function L is \u201csimply switched\u201d in the sense of Griewank and Walther (2016). For the matrix Z and any k \u2208 [H], i \u2208 [N ], the (N(k \u2212 1) + i)-th row of Z \u2208 RNH\u00d7H(d+1) can be written as\nH\u220f\nk\u2032=1\n0\u00d7 1k\u2032=k \u00b7 x\u22a4i \u2208 R1\u00d7H(d+1)."
        },
        {
            "heading": "B Proofs for Section 3",
            "text": ""
        },
        {
            "heading": "B.1 The Problems",
            "text": "Problem 33 (3SAT). Given a collection of clauses {Ci(x)}ni=1 on Boolean variables x \u2208 {0, 1}m such that clause Ci(x) is limited to a disjunction of at most three literals for any 1 6 i 6 n. Let the following formula of C(x) in conjunctive normal form be given\nC(x) := n\u2227\ni=1\nCi(x).\nIs there an x \u2208 {0, 1}m satisfying C(x) = 1?\nProblem 34 (Piecewise Linear Test, PLT). Suppose \u03b5 \u2208 [0, 1\u221a m ) and the input data {yi}3ni=1 \u2286 Zm be given. Let us define a function fPLT : Rm \u2192 R as\nfPLT(d) := max 16i6n\n\u2212 3\u2211\nj=1\nmax { d\u22a4y3(i\u22121)+j , 0 } .\nIs there a vector g \u2208 Rm satisfying \u2016g\u2016 6 \u03b5 and\nfPLT(d) > \u3008g,d\u3009, \u2200d \u2208 Rm? (PLT)\nIts complement is given by\n\u2200g \u2208 B\u03b5(0),\u2203d \u2208 Rm : fPLT(d) < \u3008g,d\u3009. (PLT)\nProblem 35 (Neural Network Test, NNT). Suppose \u03b5 \u2208 [0, 1\u221a m ]. Let the input data\nY = [ y1 \u00b7 \u00b7 \u00b7 y3n ] \u2286 Zm\u00d73n be given. Let us define fNNT : R3n \u00d7 Rm \u2192 R as\nfNNT(u,w) := max 16i6n\n3\u2211\nj=1\nu3(i\u22121)+j \u00b7max { w\u22a4y3(i\u22121)+j , 0 } .\nIs (\u221213n,0m) an \u03b5-Fre\u0301chet stationary point of fNNT, i.e., dist ( 0, \u2202\u0302fNNT(\u221213n,0m) ) 6 \u03b5?\nProblem 36 (Abs-Normal Form Test, ANFT). Suppose a piecewise linear function is given in the abs-linear form with vectors and matrices a \u2208 Rn, b \u2208 Rs,Z \u2208 Rs\u00d7n,L \u2208 Rs\u00d7s. Is there a definite signature vector \u03c3 \u2208 {\u22121, 1}s such that the following system with respect to \u00b5\u03c3 \u2208 Rs is incompatible\na\u22a4 + (b\u2212 \u00b5\u03c3)\u22a4 ( Diag(\u03c3) \u2212L )\u22121 Z = 0, 0 6 \u00b5\u03c3 \u2208 Rs?"
        },
        {
            "heading": "B.2 Hardness of Piecewise Linear Test",
            "text": "Lemma 37. Problem 34 (PLT) is co-NP-hard.\nProof. We have to show that PLT is an element of the complexity class NP-hard. 3SAT in Problem 33 is known to be strongly NP-complete (Garey and Johnson, 1979). We give a polynomial-time reduction from 3SAT to PLT. Given any instance of 3SAT, we get clauses {Ci(x)}ni=1 for x \u2208 {0, 1}m. We will refer literals in Ct(x) by their positions. For example, given Ct(x) = xi \u2228 (\u00acxj)\u2228xk, we say the literal xi occurs in Ct(x) at position 1, the literal \u00acxj occurs in Ct(x) at position 2, and the literal xk occurs in Ct(x) at position 3. We construct the data {yi}3ni=1 \u2286 Zm as follows\nyi = { ek if Boolean xk occurs in C\u230a(i\u22121)/3\u230b+1(x) at position i\u2212 3\u230a(i \u2212 1)/3\u230b \u2212ek if Boolean \u00acxk occurs in C\u230a(i\u22121)/3\u230b+1(x) at position i\u2212 3\u230a(i \u2212 1)/3\u230b .\nNote the following positive 1-homogeneous function in the construction of PLT\nfPLT(d) = max 16i6n\n\u2212 3\u2211\nj=1\nmax { d\u22a4y3(i\u22121)+j , 0 } .\nSuppose that for any 0 6 \u2016g\u2016 6 \u03b5, there exists d \u2208 Rm such that fPLT(d) < \u3008g,d\u3009. We will exhibit an x \u2208 {0, 1}m such that the given 3SAT is satisfied. Let g = 0 and there exists d \u2208 Rm such that fPLT(d) < 0. For any i \u2208 [m], let\nxi = { 1 if di > 0 0 if di 6 0 .\nWe show C(x) = 1. By fPLT(d) < 0, we get for any i \u2208 [n] 3\u2211\nj=1\nmax { d\u22a4y3(i\u22121)+j , 0 } > 0,\nwhich implies that there exists a j\u2032 \u2208 {1, 2, 3} such that d\u22a4y3(i\u22121)+j\u2032 > 0. Let the index of the Boolean literal occurs in Ci(x) at position j\n\u2032 be k. Now we consider two cases. If xk occurs in Ci(x) at position j \u2032, then y3(i\u22121)+j\u2032 = ek. We get d \u22a4y3(i\u22121)+j\u2032 = d\n\u22a4ek = dk > 0. So, by definition, xk = 1 which implies Ci(x) = 1. Otherwise, if\n\u00acxk occurs in Ci(x) at position j\u2032, then y3(i\u22121)+j\u2032 = \u2212ek. We get d\u22a4y3(i\u22121)+j\u2032 = \u2212d\u22a4ek = \u2212dk > 0. So \u00acxk = 1 by definition, which implies Ci(x) = 1. This shows that C(x) = \u2227n i=1Ci(x) = 1 and the given 3SAT is satisfied. Conversely, we show that if there exists a vector g such that 0 6 \u2016g\u2016 6 \u03b5 and infd fPLT(d) > \u3008g,d\u3009, then 3SAT cannot be satisfied. Suppose to the contrary that there exists x \u2208 {0, 1}m such that C(x) = 1. For any i \u2208 [m], let\ndi = { 1 if xi = 1 \u22121 if xi = 0 .\nAs \u2227n\ni=1Ci(x) = 1, for any i \u2208 [n], there exists a literal of clause Ci(x) that is satisfied. Let the index of this literal be k\u2032 and the position of it in Ci(x) be j\u2032. We consider two cases. If literal xk\u2032 occurs in Ci(x) at position j\n\u2032, then y3(i\u22121)+j\u2032 = ek\u2032 . As Ci(x) = 1 due to literal xk\u2032 , we get xk\u2032 = 1 and dk\u2032 = 1 by definition. Then, for such i \u2208 [n], we get\n3\u2211\nj=1\nmax { d\u22a4y3(i\u22121)+j , 0 } > max { d\u22a4y3(i\u22121)+j\u2032 , 0 } = max{dk\u2032 , 0} = 1.\nOtherwise, if literal \u00acxk\u2032 occurs in Ci(x) at position j\u2032, then y3(i\u22121)+j\u2032 = \u2212ek\u2032 . As Ci(x) = 1 due to literal\n\u00acxk\u2032 , we get xk\u2032 = 0 and dk\u2032 = \u22121 by definition. Then, for any i \u2208 [n], we get\n3\u2211\nj=1\nmax { d\u22a4y3(i\u22121)+j , 0 } > max { d\u22a4y3(i\u22121)+j\u2032 , 0 } = max{\u2212dk\u2032 , 0} = 1.\nThis gives\n\u3008g,d\u3009 6 fPLT(d) 6 \u22121 < \u2212\u03b5 \u00b7 \u221a m 6 \u2212\u2016g\u2016 \u00b7 \u2016d\u2016 6 \u2212|\u3008g,d\u3009| 6 \u3008g,d\u3009,\na contradiction. Hence Problem 34 is in the class co-NP-hard.\nWhile it is not clear whether the Problem 34 with a positive \u03b5 is an element of the complexity class co-NP, we show that, when \u03b5 = 0, Problem 34 is in co-NP.\nLemma 38. If \u03b5 = 0, then Problem 34 is in the complexity class of co-NP.\nProof. For \u03b5 = 0, we only need to test fPLT(d) > 0,\u2200d \u2208 Rm. Given any d \u2208 Rm checking whether fPLT(d) < 0 can be done in O(mn log n) time. If the answer to Problem 34 is yes, by homogeneity in fPLT, there exist a direction d and a vector s \u2208 {1, 2, 3}n such that fPLT(d) 6 \u22121 and d\u22a4y3(i\u22121)+si > 1 for any i \u2208 [n]. There are only 3n elements in the set {1, 2, 3}n and all resulting [ ys1 \u00b7 \u00b7 \u00b7 y3n\u22123+sn ] are integer matrix of polynomial length relative to the input size of Problem 34. So the certificate d can be obtained by solving a linear program in polynomial time. Therefore, if there exists d \u2208 Rm such that fPLT(d) < 0, then a nondeterministic algorithm can find s \u2208 {1, 2, 3}n and d\u2032 \u2208 Qm satisfying fPLT(d\n\u2032) 6 \u22121 < 0 in polynomial time. Thus, Problem 34 with \u03b5 = 0 is an element of the complexity class co-NP.\nProof of Theorem 10. We first note that Problem 34 can be written in the standard maxmin form in polynomial time by the following elementary identify:\n\u2212 3\u2211\ni=1\nmax{ti, 0} = min { 3\u2211\ni=1\nsi \u00b7 ti : sk \u2208 {\u22121, 0},\u2200k \u2208 {1, 2, 3} } .\nBesides, it holds fPLT(d) = fPLT(0) + f \u2032 PLT (0;d) = f \u2032 PLT (0;d). By Definition 3, we know\ndist ( 0, \u2202\u0302fPLT(0) ) 6 \u03b5 if and only if there exists a vector g \u2208 Rm satisfying 0 6 \u2016g\u2016 6 \u03b5 and fPLT(d) > \u3008g,d\u3009,\u2200d \u2208 Rm, which is the definition of Problem 34. Note that if \u03b5 = 0, in the reduction from 3SAT in the proof of Lemma 37, all numerical parameters are bounded by a polynomial of the input size. The proof completes by Lemma 37."
        },
        {
            "heading": "B.3 Hardness of Abs-Normal Form Test",
            "text": "Proof of Corollary 11. We first show that PLT in Problem 34 can be written in the absnormal form in polynomial time. For ease of notation, let qi(d) := \u2212 \u22113 j=1max { d\u22a4y3(i\u22121)+j , 0 } for any i \u2208 [n]. Then, we can rewrite every qi in the abs-linear form as\nzi(d) = y \u22a4 i d, \u2200i \u2208 [n].\nqi(d,p) = \u2212 1\n2\n3\u2211\nj=1\nd\u22a4y3(i\u22121)+j \u2212 1\n2\n3\u2211\nj=1\np3(i\u22121)+j , \u2200i \u2208 [n].\nNote that the function fPLT can be expressed as\ny := fPLT(d) = max 16i6n\nqi(d, |z|) = max{. . . ,max{q1(d, |z|), q2(d, |z|)}, . . . , qn(d, |z|)},\nwhich can be written in abs-normal form as\nzi = Fi(q, |z|) = 1\n2i\u22122 \u00b7 q1 +\ni\u22121\u2211\nt=2\n( qt + p3n+t\u22121 ) \u2212 qi, \u22003n+ 1 6 i 6 4n \u2212 1,\ny = f(q,p) = 1\n2n\u22121 \u00b7 q1 +\nn\u2211\nt=2\n( qt + p3n+t\u22121 ) .\nIn sum, we have\nzi =   \ny\u22a4i w for 1 6 i 6 3n\n1\n2i\u22123n\u22121 \u00b7 q1 +\ni\u22123n\u22121\u2211\nt=2\n1 2i\u22123n\u2212t \u00b7 ( qt + p3n+t\u22121 ) for 3n+ 1 6 i 6 4n \u2212 1 .\nThen, we know\nfPLT(d) = f(d, |z(d)|) = 1\n2n\u22121 \u00b7 q1 +\nn\u2211\nt=2\n( qt + |z3n+t\u22121(d)| ) .\nThen, the matrices L,Z,a, b can be computed in polynomial time. We note that infd fPLT(d) > 0 if and only if the function fPLT is first-order minimal in abs-normal form and this is shown in the discussion below (Griewank and Walther, 2019, Equation (2)) (see also (Griewank and Walther, 2016, p3)). Then, the answer of ANFT in Problem 36 for the abs-normal form of fPLT is No if and only if 0 is a Fre\u0301chet stationary point of fPLT. Then, by Lemma 37, ANFT in Problem 36 is NP-hard. To see ANFT is in NP, for any given \u03c3 \u2208 {\u22121, 1}s, the computation of the vector a\u22a4+b\u22a4 ( Diag(\u03c3)\u2212L )\u22121 Z and the matrix ( Diag(\u03c3) \u2212 L )\u22121 Z can be done in polynomial time. Then, ANFT for a given \u03c3 reduces to check the infeasibility of a linear system, which is in P. In sum, we have shown ANFT in Problem 36 is NP-complete, which implies a general test of FOM without kink qualification in (Griewank and Walther, 2019, Theorem 4.1) is co-NP-complete."
        },
        {
            "heading": "B.4 Hardness of Neural Network Test",
            "text": "Lemma 39. Problem 35 (NNT) is co-NP-hard. If \u03b5 = 0, Problem 35 is co-NP-complete.\nProof. We first prove that (\u221213n,0m) is an \u03b5-Fre\u0301chet stationary point of fNNT if and only if there exists gw \u2208 Bm\u03b5 (0) such that infd\u2208Rm fPLT(d) > \u3008gw,d\u3009 with the same input data {yi}3ni=1 \u2286 Zm. By (Rockafellar and Wets, 2009, Exercise 8.4) and fNNT is B-differentiable; see (Cui and Pang, 2021, Definition 4.1.1), we get dist ( 0, \u2202\u0302fNNT(\u221213n,0m) ) 6 \u03b5 if and only if there exists (gu,gw) \u2208 B3n+m\u03b5 (03n+m) such that\nf \u2032NNT(\u221213n,0m;du,dw) > \u3008du,gu\u3009+ \u3008dw,gw\u3009, \u2200du \u2208 R3n,dw \u2208 Rm. (\u266f)\nUsing the chain rule of directional derivative for B-differentiable function (Cui and Pang, 2021, Proposition 4.1.2(a)), we have\nf \u2032NNT(\u221213n,0m;du,dw) = max 16i6n\n\u2212 3\u2211\nj=1\nmax { y\u22a43(i\u22121)+jd w, 0 } = fPLT(d w).\nFor any gu,gw, consider dw = 0 and du = gu. We get that Equation (\u266f) holds if and only if gu = 03m and infdw\u2208Rm fPLT(dw) > \u3008gw,dw\u3009, which completes the proof by the co-NPhardness of Problem 34 in Lemma 37 and co-NP-completeness if \u03b5 = 0 in Lemma 38.\nProof of Corollary 12. Note that Problem 35 can be represented by the empirical loss of a convolutional neural network with N = 1 and architecture\n\u21131\n( max-pooling \u25e6 convu \u25e6 ReLU \u25e6 convw(Y ) ) ,\nwhere \u21131(t) = t and Y = [ y1 \u00b7 \u00b7 \u00b7 y3n ] \u2286 Zm\u00d73n. If \u03b5 = 0, in the reduction from 3SAT to PLT, then to NNT, all numerical parameters are bounded by a polynomial of the input size. The proof completes by Lemma 39."
        },
        {
            "heading": "C Proofs for Section 4",
            "text": ""
        },
        {
            "heading": "C.1 Proof Roadmap",
            "text": "Recall the loss function L of shallow ReLU neural network:\nL(u1,w1, . . . , uH ,wH) :=\nN\u2211\ni=1\n\u2113i\n( H\u2211\nk=1\nuk \u00b7max { w\u22a4k xi, 0 }) .\nSet constants \u03c1i := \u2113 \u2032 i (\u2211H k=1 uk \u00b7max { w\u22a4k xi, 0 }) for any i \u2208 [N ]. Let us first consider a\npartially linearized loss function L defined by\nL(u1,w1, . . . , uH ,wH) := N\u2211\ni=1\n\u03c1i \u00b7 ( H\u2211\nk=1\nuk \u00b7max { w\u22a4k xi, 0\n})\n=\nH\u2211\nk=1\nLk(uk,wk) := ( uk \u00b7 N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }) .\nBy exploiting the smoothness of {\u2113i}Ni=1 and a Lagrange scalarization technique in Theorem 46, we will show that\n\u2202\u22b3L(u1,w1, . . . , uH ,wH) = \u2202\u22b3L(u1,w1, . . . , uH ,wH).\nThen, we focus on the linearized L. By separation of {(uk,wk)}k and using again the Lagrange scalarization technique in form of Corollary 48, we have\n\u2202\u22b3L(u1,w1, . . . , uH ,wH) (a) =\nH\u220f\nk=1\n\u2202\u22b3Lk(uk,wk)\n(b) =\nH\u220f\nk=1\n{ N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7 \u2202\u22b3 [ Lk(uk, \u00b7) ] (wk),\nwhere (a) is due to (Rockafellar, 1985, Proposition 2.5) and (Rockafellar and Wets, 2009, Proposition 10.5); (b) is by Corollary 48. Therefore, it holds\n\u2202\u22b3L(u1,w1, . . . , uH ,wH) = H\u220f\nk=1\n{ N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7 \u2202\u22b3 [ Lk(uk, \u00b7) ] (wk),\nwhich implies that the validity of exact chain rule of L rely on a careful study of Lk(uk, \u00b7). In particular, if we have the exact chain rule for any k \u2208 [H] as follows\n\u2202\u22b3 [ Lk(uk, \u00b7) ] (wk) = G \u22b3 k, (1)\nthen we get the validity of exact chain rule for L. That is \u2202\u22b3L(u1,w1, . . . , uH ,wH) =\nH\u220f\nk=1\n{ N\u2211\ni=1\n\u03c1i \u00b7max { w\u22a4k xi, 0 }} \u00d7G\u22b3k.\nTo prove Equation (1), we need a fine-grained analysis of Lk(uk, \u00b7). First, we isolate the nonsmooth part out by rewritting\n[ Lk(uk, \u00b7) ] (wk) =\n\u2211\ni\u2208[N ]\\(I+ k \u222aI\u2212 k )\nuk\u03c1i \u00b7max { w\u22a4k xi, 0 } + fk(wk),\nwhere we define\nfk(wk) := \u2211\ni\u2208I+ k\nuk\u03c1i \u00b7max { w\u22a4k xi, 0 } \u2212 \u2211\nj\u2208I\u2212 k\n(\u2212uk\u03c1j) \u00b7max { w\u22a4k xj, 0 } .\nWhat remaining is to study the subdifferential of this non-separable piecewise linear function fk for any k \u2208 [H] and figure out conditions, under which\n\u2202\u22b3fk(wk) = G\u0303 \u22b3 k := G \u22b3 k \u2212\n\u2211\ni\u2208[N ]\\(I+ k \u222aI\u2212 k )\nuk\u03c1i \u00b7 1w\u22a4 k xi>0 \u00b7 xi.\nThis will be done in Appendix C.4."
        },
        {
            "heading": "C.2 Technical Lemmas",
            "text": "Lemma 40 (Gordan, cf. (Bertsimas and Tsitsiklis, 1997, Exercise 4.26)). Let A \u2208 Rn\u00d7m be given. Then, exactly one of the following statements is true:\n\u2022 There exists an x \u2208 Rm such that Ax < 0.\n\u2022 There exists a y \u2208 Rn such that A\u22a4y = 0 with y > 0,y 6= 0.\nLemma 41. Let A,B,C be sets in Rn. Suppose further that A is convex and closed, and C is nonempty and bounded. If the strict inclusion A ( B holds, then we can assert A+ C ( B + C.\nProof. Let xb \u2208 B\\A. The claim is trivial when A = \u2205. Choose x\u2032a \u2208 A and set \u03b4 := \u2016xb \u2212 x\u2032a\u2016. As A is closed, the following xa is well-defined\nxa := argmin a\u2208A \u2016a\u2212 xb\u2016 = argmin a\u2208A\u2229B\u03b4(xb) \u2016a\u2212 xb\u2016.\nLet d := xb \u2212 xa. As xb /\u2208 A and A is closed, we know \u2016d\u2016 > 0. By the optimality condition and convexity of A, we know \u3008a \u2212 xa,d\u3009 6 0,\u2200a \u2208 A, which implies \u3008d,a\u3009 6 \u3008d,xa\u3009,\u2200a \u2208 A. As C is bounded, we know \u3008c,d\u3009 6 \u2016c\u2016 \u00b7 \u2016d\u2016 < +\u221e,\u2200c \u2208 C. Let xc be\nxc \u2208 \u03b5\u2013argmax c\u2208C \u3008c,d\u3009,\nwhere 0 < \u03b5 < \u2016d\u20162. We claim xb + xc /\u2208 A + C. Suppose not. Therefore, there exist ya \u2208 A,yc \u2208 C such that ya + yc = xb + xc. However, we compute\n\u3008d,xb + xc\u3009 = \u3008d,d + xa\u3009+ \u3008d,xc\u3009 > \u2016d\u20162 + \u3008d,ya\u3009+ \u3008d,yc\u3009 \u2212 \u03b5 > \u3008d,ya + yc\u3009,\nwhich gives the contradiction.\nRemark 42. Though the claim seems straightforward, Lemma 41 is indeed non-trivial. We record the following counterexamples when different conditions are removed.\n\u2022 C is empty: A+C = B + C = \u2205.\n\u2022 C is unbounded: if C = Rn and A,B are nonempty, then A+ C = B + C = Rn.\n\u2022 A is nonconvex: if A = B\\B1/4, B = B, C = B, then A+ C = B + C = B2.\n\u2022 A is not closed: if A = B\u25e6, B = B, C = B\u25e6, then A+ C = B + C = B\u25e62.\nLemma 43. Let {pi}ni=1 be linearly independent. Define a convex set C = \u2211n\ni=1 pi \u00b7 [0, 1]. For any s \u2208 {0, 1}n, the point p =\u2211ni=1 si \u00b7 pi is an extreme point of C. Proof. Suppose not and p = 12x1 + 1 2x2 = \u2211n i=1 si \u00b7 pi with p 6= x1 = \u2211n i=1 \u03b1i \u00b7 pi \u2208 C\nand p 6= x2 = \u2211n\ni=1 \u03b2i \u00b7 pi \u2208 C. We know \u03b1i \u2208 [0, 1] and \u03b2i \u2208 [0, 1] for any i \u2208 [n] by definition. Thus, it holds\nn\u2211\ni=1\nsi \u00b7 pi = n\u2211\ni=1\n( \u03b1i + \u03b2i\n2\n) \u00b7 pi.\nAs {pi}ni=1 are linearly independent, we know that, for any i \u2208 [n], it holds si = ( \u03b1i+\u03b2i 2 ) \u2208 {0, 1}. If si = 0, we have \u03b1i = \u03b2i = 0. Meanwhile, we know \u03b1i = \u03b2i = 1 if si = 1. Therefore, it holds x1 = x2 = p, a contradiction. Lemma 44. Let a function g : Rd \u2192 R be w 7\u2192 \u2212\u2211mj=1max{y\u22a4j w, 0}. If there exists j \u2208 [m] such that w\u22a4yj = 0 and yj 6= 0, then we have \u2202\u0302g(w) = \u2205. Proof. Suppose not and let u \u2208 \u2202\u0302g(w). We write\ng(w) = \u2212 \u2211\nj:w\u22a4yj 6=0 max{w\u22a4yj, 0} + g0(w),\nwhere we define g0(w) := \u2212 \u2211\nk:w\u22a4yk=0 max{w\u22a4yk, 0}. Then, by (Rockafellar and Wets,\n2009, Exercise 8.8(c)), we have\n\u2202\u0302g(w) = \u2212 \u2211\nj:w\u22a4yj 6=0 1w\u22a4yj>0 \u00b7 yj + \u2202\u0302g0(w).\nLet u\u2032 = u+ \u2211\nj:w\u22a4yj 6=0 1w\u22a4yj>0 \u00b7yj and we know u\u2032 \u2208 \u2202\u0302g0(w). By (Rockafellar and Wets, 2009, Exercise 8.4), for any d \u2208 Rd, it holds\nu\u2032\u22a4d 6 g\u20320(w;d) = \u2212 \u2211\nk:w\u22a4yk=0\n1d\u22a4yk>0 \u00b7 d \u22a4yk 6 0.\nLet d = u\u2032 and we know \u2016u\u2032\u20162 6 g\u20320(w;u\u2032) 6 0. Thus, u\u2032 = 0. Let d be any yj such that w\u22a4yj = 0 and yj 6= 0. Then, we have\n0 = u\u2032\u22a4yj 6 g \u2032 0(w;yj) 6 \u2212\u2016yj\u20162 < 0,\na contradiction.\nDefinition 45 (Bouligand subdifferential, c.f. (Cui and Pang, 2021, Definition 4.3.1)). Given a point x, the Bouligand subdifferential of a locally Lipschitz function f at x is defined by\n\u2202Bf(x) := { g : \u2203{x\u03bd} \u2192 x and {\u2207f(x\u03bd)} \u2192 g s.t. \u2207f(x\u03bd) exists for any \u03bd } ."
        },
        {
            "heading": "C.3 Partial Linearization via Lagrange Scalarization",
            "text": "The following theorem is a powerful and general principle.\nTheorem 46 (Partial linearization). Let a point x \u2208 Rd and a locally Lipschitz f : Rd \u2192 R be given in form of composition f(x) = h \u25e6 G(x), where the gradient of h : Rn \u2192 R is locally Lipschitz near G(x) and G : Rd \u2192 Rn is locally Lipschitz near x. Suppose h and G are directionally differentiable. Then, we have\n\u2202\u22b3f(x) = \u2202\u22b3\n[ \u2329 \u2207h ( G(x) ) , G(\u00b7) \u232a ] (x).\nProof. Let the partially linearized f at x be f\u0304 : Rd \u2192 R defined as\nf\u0304(y) := \u2329 \u2207h ( G(x) ) , G(y) \u232a .\nFor the limiting subdifferential version, the claim directly follows from a margin function chain rule (Mordukhovich and Shao, 1996, Theorem 6.5). The Clarke subdifferential version directly follows from the relation between Clarke and limiting subdifferential (Rockafellar and Wets, 2009, Theorem 8.49) and (Mordukhovich and Shao, 1996, Theorem 6.5). However, as the proof of (Mordukhovich and Shao, 1996, Theorem 6.5) uses a perturbation argument to approximate \u2202f with \u03b5-Fre\u0301chet subdifferential, the machinery is somehow complicated. Here we give an elementary proof for the Clarke version from the primal perspective using tools from convex analysis. We show f\u25e6(x;v) = f\u0304\u25e6(x;v) for any v \u2208 Rd. Note that the Clarke generalized subderivative can be written as\nf\u25e6(x;v) = lim sup x\u2032\u2192x t\u05810 \u2206tf(x \u2032)(v)\n= lim \u03b5\u05810 sup \u2016x\u2032\u2212x\u20166\u03b5 sup 0<t<\u03b5\n\u2206tf(x \u2032)(v),\nwhere the difference quotient function \u2206tf(x \u2032) : Rd \u2192 R of f at x\u2032 and direction v is defined by\n\u2206tf(x \u2032)(v) := f(x\u2032 + tv)\u2212 f(x\u2032) t .\nWe assume h is Lh-smooth near g(x) and G is LG-Lipschitz near x. We will use the following estimation (see (Nesterov, 2003, Lemma 1.2.3)) if h is Lh-smooth at z \u2208 Rn:\n\u2212Lh 2 \u2016z\u2032 \u2212 z\u20162 6 h(z\u2032)\u2212 h(z) \u2212\n\u2329 \u2207h(z),z\u2032 \u2212 z \u232a 6\nLh 2 \u2016z\u2032 \u2212 z\u20162.\nTo prove f\u25e6(x;v) > f\u0304\u25e6(x;v), we compute as follows\n\u2206tf(x \u2032)(v) =\nh ( G(x\u2032 + tv) ) \u2212 h ( G(x\u2032) )\nt\n> 1\nt\n\u2329 \u2207h ( G(x\u2032) ) , G(x\u2032 + tv)\u2212G(x\u2032) \u232a \u2212 Lh\n2t\n\u2225\u2225G(x\u2032 + tv)\u2212G(x\u2032) \u2225\u22252\n> 1\nt\n\u2329 \u2207h ( G(x) ) , G(x\u2032 + tv)\u2212G(x\u2032) \u232a \u2212 LhL 2 g\n2 \u2016v\u20162 \u00b7 t\u2212 LhL2g\u2016v\u2016 \u00b7 \u2016x\u2212 x\u2032\u2016\n= \u2206tf\u0304(x \u2032)(v)\u2212 LhL\n2 g\n2 \u2016v\u20162 \u00b7 t\u2212 LhL2g\u2016v\u2016 \u00b7 \u2016x\u2212 x\u2032\u2016\nTherefore, for any v \u2208 Rd, we know f\u25e6(x;v) = lim\n\u03b5\u05810 sup \u2016x\u2032\u2212x\u20166\u03b5 sup 0<t<\u03b5 \u2206tf(x\n\u2032)(v),\n(i) > lim \u03b5\u05810 sup \u2016x\u2032\u2212x\u20166\u03b5 sup 0<t<\u03b5 \u2206tf\u0304(x \u2032)(v)\u2212 ( lim \u03b5\u05810 LhL 2 g 2 \u2016v\u20162 \u00b7 \u03b5 ) \u2212 ( lim \u03b5\u05810 LhL 2 g\u2016v\u2016 \u00b7 \u03b5 )\n= lim \u03b5\u05810 sup \u2016x\u2032\u2212x\u20166\u03b5 sup 0<t<\u03b5\n\u2206tf\u0304(x \u2032)(v)\n= f\u0304\u25e6(x;v),\nwhere in (i) we use sup f\u2212g > sup f\u2212sup g. For the converse direction f\u25e6(x;v) 6 f\u0304\u25e6(x;v), we just compute similarly. We have proved f\u25e6(x;v) = f\u0304\u25e6(x;v),\u2200v \u2208 Rd. The claim follows from the correspondence between sublinear f\u25e6 and convex \u2202Cf (Clarke, 1990, Proposition 2.1.5).\nNow we show the relation holds for Fre\u0301chet subdifferential. As h and G are locally Lipschitz and directional differentiable, they are Bouligand-differentiable (B-differentiable) according to (Cui and Pang, 2021, Definition 4.1.1). Then, by (Cui and Pang, 2021, Proposition 4.1.2(a)), we know that\nf \u2032(x;d) = h\u2032 ( G(x);G\u2032(x;d) ) = \u2329 \u2207h ( G(x) ) , G\u2032(x; d) \u232a ,\nwhere the directional derivative G\u2032(x;d) is defined element-wise as (G\u2032i(x;v)) n i=1 according\nto (Cui and Pang, 2021, Definition 1.1.4). Thus, combined with f\u0304 \u2032(x;d) = \u2329 \u2207h ( G(x) ) , G\u2032(x; d) \u232a , we have shown f\u0304 \u2032(x;d) = f \u2032(x;d) for any d, which implies\n\u2202\u0302f(x) = \u2202\u0302 [ \u2329 \u2207h ( G(x) ) , G(\u00b7) \u232a ] (x)\nby (Rockafellar and Wets, 2009, Exercise 8.4) (note that for B-differentiable f , the subderivative df(x)(d) in (Rockafellar and Wets, 2009, Exercise 8.4) is equal to the directional derivative f \u2032(x;d) by (Rockafellar and Wets, 2009, Exercise 9.15)).\nRemark 47. Theorem 46 is fundamentally different from the classic exact chain rule as the exact chain rule does not hold even for very simple function. Consider h(a, b) = a\u2212 b and G(x) = (|x|, |x|). We have \u2202C [h\u25e6G](0) = {0} ( [\u22121, 1]+[\u22121, 1] = [\u22122, 2]. In contrast, by Theorem 46, we have \u2202C [h \u25e6 G](0) = \u2202C [| \u00b7 | \u2212 | \u00b7 |](0) = {0}. One should compare Theorem 46 with (Clarke, 1990, Theorem 2.3.9, Theorem 2.3.10). Besides, Theorem 46 implies (Clarke, 1990, Theorem 2.3.9(ii)).\nCorollary 48. Let f : R\u00d7Rd \u2192 R be f(u,x) = u \u00b7 g(x), where g : Rd \u2192 R is a Lipschitz function. Then, we have \u2202\u22b3f(u,x) = {g(x)} \u00d7 \u2202\u22b3[u \u00b7 g](x). Proof. Let h : R\u00d7R\u2192 R be h(a, b) = a \u00b7 b. It is easy to see h is smooth at any (a, b). Let C(u,x) = (u, g(x)), u\u0304 = u and x\u0304 = x. As f(u,x) = h \u25e6C(u,x), by Theorem 46, we know\n\u2202\u22b3f(u,x) = \u2202\u22b3 [ g(x\u0304) \u00b7 u+ u\u0304 \u00b7 g(x) ] (u,x) = {g(x)} \u00d7 \u2202\u22b3[u \u00b7 g](x),\nas required."
        },
        {
            "heading": "C.4 Exact Chain Rule of a Non-Separable Piecewise Linear Function",
            "text": "In this section, we consider the validity of the exact subdifferential chain rule of a simple piecewise-linear function, which is defined by\nfPL(w) :=\nn\u2211\ni=1\nmax { x\u22a4i w, 0 } \u2212 m\u2211\nj=1\nmax { y\u22a4j w, 0 } ."
        },
        {
            "heading": "C.4.1 Chain Rule for Clarke Subdifferential",
            "text": "Theorem 49 (Clarke). Suppose x\u22a4i w = y \u22a4 j w = 0 for any i \u2208 [n], j \u2208 [m]. We have the exact Clarke subdifferential chain rule\n\u2202CfPL(w) = G C PL\n:= n\u2211\ni=1\nxi \u00b7 [0, 1] + m\u2211\nj=1\n(\u2212yj) \u00b7 [0, 1]\nif and only if span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) = {0}.\nProof. We have divided the proof into Lemma 50 and Lemma 51.\nLemma 50 (Necessary). If there exists v \u2208 Rd such that\n0 6= v \u2208 span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) ,\nthen \u2202CfPL(w) ( G C PL .\nProof. We first prove that assuming certain regularity on {xi}ni=1 and {yj}mj=1 is without loss of generality. Let the indices set Jx \u2286 [n] be a selection from {xi}ni=1 such that {xi}i\u2208Jx are linearly independent and satisfy\nspan ( {xi}i\u2208Jx ) = span ( {xi}ni=1 ) .\nSimilarly, we define Jy \u2286 [m] for {yj}mj=1. Then, we write\nfPL(w) = fPL1(w) + fPL2(w),\nwhere fPL1(w) := \u2211\ni\u2208[n]\\Jx max\n{ x\u22a4i w, 0 } \u2212 \u2211\nj\u2208[m]\\Jy max\n{ y\u22a4j w, 0 } ,\nfPL2(w) := \u2211\ni\u2208Jx max\n{ x\u22a4i w, 0 } \u2212 \u2211\nj\u2208Jy max\n{ y\u22a4j w, 0 } .\nBy the fuzzy sum rule (Clarke, 1990, Proposition 2.3.3), we know\n\u2202CfPL(w) \u2286 \u2202CfPL1(w) + \u2202CfPL2(w) \u2286 \u2202CfPL1(w) +GCPL2 \u2286 GCPL,\nwhere we define GC PL2 := \u2211 i\u2208Jx xi \u00b7 [0, 1]+ \u2211 j\u2208Jy(\u2212yj) \u00b7 [0, 1]. Thus, to prove \u2202CfPL(w) ( GC PL , by Lemma 41 and (Clarke, 1990, Proposition 2.1.2(a)), we only need to show \u2202CfPL2(w) ( G C PL2. So, by abuse of notation and focus on fPL2, we assume {xi}ni=1 are linearly independent. Similarly, we assume {yj}mj=1 are linearly independent. In the following, we may use Lemma 41 and above argument implicitly to assume regularity for simplicity. As v \u2208 span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) , we write\n0 6= v = n\u2211\ni=1\nai \u00b7 xi = m\u2211\nj=1\nbi \u00b7 yj. (2)\nIt is safe to assume ai 6= 0, bj 6= 0,\u2200i \u2208 [n], j \u2208 [m]. Fix ym. We can further assume {xi}ni=1\u222a{yj}m\u22121j=1 are linearly independent. To see this, suppose to the contrary {xi}ni=1\u222a {yj}m\u22121j=1 are not linearly independent, we get 0 = \u2211n i=1 pi \u00b7 xi + \u2211m\u22121 j=1 qj \u00b7 yj. We know\nthat there exist j\u2032 \u2208 [m \u2212 1] such that qj\u2032 6= 0, as otherwise by linear independence of {xi}mi=1, for any i \u2208 [n], it holds that pi = 0, hence that {xi}ni=1 \u222a {yj}m\u22121j=1 are linearly independent. As qj\u2032 6= 0, we have\nyj\u2032 = \u2212 n\u2211\ni=1\n(pi/qj\u2032) \u00b7 xi \u2212 \u2211\nj\u2208[m\u22121]\\{j\u2032} (qj/qj\u2032) \u00b7 yj .\nPlug in to Equation (2) and yj\u2032 is removed. Repeat this procedure and by abuse of notation, we have {xi}ni=1 \u222a {yj}m\u22121j=1 are linearly independent. After that, we exam {ai}i and {bj}j . We remove xi if ai = 0 and remove yj if bj = 0, which is without of generality by Lemma 41. It is possible that all {yj}m\u22121j=1 are removed and we get m = 1 and ym \u2208 span ( {xi}ni=1 ) . But as ym 6= 0, we always have n > 1. Then, we can write\nym =\nn\u2211\ni=1\n\u03b1ixi +\nm\u22121\u2211\nj=1\n\u03b2jyj, (3)\nwith \u03b1i 6= 0, \u03b2j 6= 0 for any i \u2208 [n], j \u2208 [m\u2212 1]. Note that, for such {xi}ni=1 and {yj}m\u22121j=1 , we have the exact chain rule\n\u2202C\n  n\u2211\ni=1\nmax { x\u22a4i \u00b7, 0 } \u2212 m\u22121\u2211\nj=1\nmax { y\u22a4j \u00b7, 0 }   (w) = n\u2211\ni=1\nxi \u00b7 [0, 1] + m\u22121\u2211\nj=1\n(\u2212yj) \u00b7 [0, 1]\nby using (Clarke, 1990, Theorem 2.3.10) and linear independence. We proceed to show that \u2202CfPL(w) ( G C PL by exhibiting an element in GC PL \\\u2202CfPL(w). Let \u03b8 \u2208 Rn+m+ and we define\n\u03b8i =    |\u03b1i| for 1 6 i 6 n |\u03b2i\u2212n| for n+ 1 6 i 6 n+m\u2212 1,\n1 for i = m+ n A =  \nsgn(\u03b11) \u00b7 x\u22a41 ... sgn(\u03b1n) \u00b7 x\u22a4n sgn(\u03b21) \u00b7 y\u22a41\n... sgn(\u03b2m\u22121) \u00b7 y\u22a4m\u22121\n\u2212y\u22a4m\n  \u2208 R(n+m)\u00d7d.\nNote that\nA\u22a4\u03b8 = n\u2211\ni=1\n\u03b1ixi +\nm\u22121\u2211\nj=1\n\u03b2jyj \u2212 ym = 0.\nBy Gordan\u2019s Theorem in Lemma 40, we have certified the nonexistence of direction d \u2208 Rd such that \n  sgn(\u03b1i) \u00b7 d\u22a4xi < 0 for i \u2208 [n] sgn(\u03b2j) \u00b7 d\u22a4yj < 0 for j \u2208 [m\u2212 1]\nd\u22a4ym > 0 . (4)\nBy \u2212A\u22a4\u03b8 = 0, similarly, we certify the nonexistence of direction d \u2208 Rd such that    sgn(\u03b1i) \u00b7 d\u22a4xi > 0 for i \u2208 [n] sgn(\u03b2j) \u00b7 d\u22a4yj > 0 for j \u2208 [m\u2212 1]\nd\u22a4ym < 0 . (5)\nLet the Bouligand subdifferential of fPL at w be \u2202BfPL(w); see (Cui and Pang, 2021, Definition 4.3.1). Define\n\u22071 :=\nm\u2211\ni=1\n1\u03b1i>0 \u00b7 xi \u2212 m\u22121\u2211\nj=1\n1\u03b2j>0 \u00b7 yj, (compare (4))\n\u22072 :=\nm\u2211\ni=1\n1\u03b1i<0 \u00b7 xi \u2212 m\u22121\u2211\nj=1\n1\u03b2j<0 \u00b7 yj \u2212 ym. (compare (5))\nBy (Cui and Pang, 2021, Proposition 4.4.8(c)) and the nonexistences of d for (4) and (5), we have proved that \u22071,\u22072 /\u2208 \u2202BfPL(w). Let us define a set\nG C PL :=\nn\u2211\ni=1\nxi \u00b7 {0, 1} + m\u2211\nj=1\n(\u2212yj) \u00b7 {0, 1} \u2286 Rd.\nBesides, using (Cui and Pang, 2021, Proposition 4.4.8(c)), we have \u2202BfPL(w) \u2286 GCPL\\{\u22071,\u22072}. Then, with (Rockafellar and Wets, 2009, Theorem 9.61), it follows that\n\u2202CfPL(w) = Conv(\u2202BfPL(w)) \u2286 Conv(GCPL\\{\u22071,\u22072}).\nTherefore, to prove \u2202CfPL(w) ( G C PL , we only need to show\n\u22071 \u2208 GCPL\\Conv ( G C PL\\{\u22071,\u22072} ) .\nTo this end, we define two sets satisfying G C PL = P1 \u222a P2 as\nP1 := n\u2211\ni=1\nxi \u00b7 {0, 1} + m\u22121\u2211\nj=1\n(\u2212yj) \u00b7 {0, 1},\nP2 :=\nn\u2211\ni=1\nxi \u00b7 {0, 1} + m\u22121\u2211\nj=1\n(\u2212yj) \u00b7 {0, 1} \u2212 ym.\nThus, we can writeG C PL\\{\u22071,\u22072} \u2286 (P1\\{\u22071})\u222a(P2\\{\u22072}). It is evident that\u22071 \u2208 GPL.\nIf \u22071 \u2208 Conv ( G C PL\\{\u22071,\u22072} ) , we have\n\u22071 = \u03bbg P1 + (1 \u2212 \u03bb)gP2 , with gP1 \u2208 Conv (P1\\{\u22071}) ,gP2 \u2208 Conv (P2\\{\u22072}) .\nWe now show that it must be \u03bb = 1 by considering three cases:\nCase 1. \u2203i \u2208 [n] : \u03b1i > 0. Without loss of generality, we assume \u03b11 > 0. Note that for any gP2 \u2208 Conv (P2\\{\u22072}), using the representation of ym in Equation (3), we have\ngP2 =\nn\u2211\ni=1\n(\u03b3i \u2212 \u03b1i) \u00b7 xi + m\u22121\u2211\nj=1\n(\u03b3n+j + \u03b2j) \u00b7 (\u2212yj),\nwhere \u03b3k \u2208 [0, 1],\u2200k \u2208 [n+m\u2212 1]. Similarly, we write gP1 \u2208 Conv (P1\\{\u22071}) as\ngP1 =\nn\u2211\ni=1\n\u00b5i \u00b7 xi + m\u22121\u2211\nj=1\n\u00b5n+j \u00b7 (\u2212yj),\nwhere \u00b5k \u2208 [0, 1],\u2200k \u2208 [n+m\u2212 1]. Therefore, we know\n\u22071 = \u03bbg P1 + (1\u2212 \u03bb)gP2\n=\nn\u2211\ni=1\n( \u03bb \u00b7 \u00b5i + (1\u2212 \u03bb) \u00b7 (\u03b3i \u2212 \u03b1i) ) \u00b7 xi +\nm\u22121\u2211\nj=1\n( \u03bb \u00b7 \u00b5n+j + (1\u2212 \u03bb) \u00b7 (\u03b3n+j + \u03b2j) ) \u00b7 (\u2212yj)\n=\nm\u2211\ni=1\n1\u03b1i>0 \u00b7 xi \u2212 m\u22121\u2211\nj=1\n1\u03b2j>0 \u00b7 yj . (by the definition of \u22071)\nAs {xi}ni=1 \u222a {yj}m\u22121j=1 are linearly independent, it holds\n\u03bb \u00b7 \u00b51 + (1\u2212 \u03bb) \u00b7 (\u03b31 \u2212 \u03b11) = 1\u03b11>0 = 1.\nIf 0 6 \u03bb < 1, we have\n1 = \u03bb \u00b7 \u00b51 + (1\u2212 \u03bb) \u00b7 (\u03b31 \u2212 \u03b11) 6 1\u2212 (1\u2212 \u03bb) \u00b7 \u03b11 < 1,\nwhich gives the contradiction.\nCase 2. \u2200i \u2208 [n] : \u03b1i < 0 but \u2203j \u2208 [m\u2212 1] : \u03b2j > 0. Suppose \u03b21 > 0. Then, we write\ny1 =\nn\u2211\ni=1\n(\u2212\u03b1i/\u03b21) \u00b7 x1 + m\u22121\u2211\nj=2\n(\u2212\u03b2j/\u03b21) \u00b7 yj \u2212 (1/\u03b21) \u00b7 ym.\nNote that {xi}ni=1\u222a{yj}mj=1 are linearly independent. By abuse of notation and swapping ym and y1, we still write ym = \u2211n i=1 \u03b1ixi + \u2211m\u22121 j=1 \u03b2jyj. Then, we have \u2200i \u2208 [n] : \u03b1i > 0 and the situation reduces to the Case 1.\nCase 3. \u2200i \u2208 [n], j \u2208 [m \u2212 1] : \u03b1i < 0, \u03b2j < 0. In that case, we have \u22071 = 0. By a similar manipulation as these in Case 1, we have\n\u22071 = n\u2211\ni=1\n( \u03bb \u00b7 \u00b5i + (1\u2212 \u03bb) \u00b7 (\u03b3i \u2212 \u03b1i) ) \u00b7 xi +\nm\u22121\u2211\nj=1\n( \u03bb \u00b7 \u00b5n+j + (1\u2212 \u03bb) \u00b7 (\u03b3n+j + \u03b2j) ) \u00b7 (\u2212yj)\n= 0.\nAs {xi}ni=1 \u222a {yj}m\u22121j=1 are linearly independent, it holds\n\u03bb \u00b7 \u00b51 + (1\u2212 \u03bb) \u00b7 (\u03b31 \u2212 \u03b11) = 1\u03b11>0 = 0.\nIf 0 6 \u03bb < 1, we have\n0 = \u03bb \u00b7 \u00b51 + (1\u2212 \u03bb) \u00b7 (\u03b31 \u2212 \u03b11) > \u2212(1\u2212 \u03bb) \u00b7 \u03b11 > 0,\nwhich gives the contradiction. Therefore, we have shown \u03bb = 1 which implies \u22071 \u2208 Conv (P1\\{\u22071}) . However, as {xi}ni=1 \u222a {yj}m\u22121j=1 are linearly independent, \u22071 is an extreme point of Conv(P1) by Lemma 43. Thus, we know \u22071 /\u2208 Conv (P1\\{\u22071}) by definition, a contradiction.\nLemma 51 (Sufficient). If the following condition holds\nspan ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) = {0},\nthen \u2202CfPL(w) = G C PL .\nProof. We first do a general preparation that will be reused in other developments. Let X = [ x1 \u00b7 \u00b7 \u00b7 xn ] \u2208 Rd\u00d7n and Y = [ y1 \u00b7 \u00b7 \u00b7 ym ] \u2208 Rd\u00d7m be given. The thin-SVD of X can be written as X = Ux\u03a3xV \u22a4 x with Ux \u2208 St(d, rx),\u03a3x \u2208 Rrx\u00d7rx ,Vx \u2208 St(n, rx), and rx = rank(X). Similarly, for Y , we have Y = Uy\u03a3yV \u22a4 y with Uy \u2208 St(d, ry),\u03a3y \u2208\nRry\u00d7ry ,Vy \u2208 St(m, ry), and ry = rank(Y ). As span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) = {0}, we know U\u22a4x Uy = 0. Therefore, we can write\nRn+m \u220b [ X\u22a4\nY \u22a4\n] \u00b7w = [ Vx\u03a3x\nVy\u03a3y\n] \u00b7 ( z := [ z1 z2 ] \u2208 Rrx+ry ) ,\nwhere z = U\u22a4w and U := [ Ux Uy ] \u2208 St(d, rx + ry).\nLet an auxiliary function hPL : R rx \u00d7Rry \u2192 R be\nhPL(z1,z2) :=\nn\u2211\ni=1\nmax { e\u22a4i Vx\u03a3xz1, 0 } \ufe38 \ufe37\ufe37 \ufe38 hPL1(z1) \u2212 m\u2211 j=1 max { e\u22a4j Vy\u03a3yz2, 0 } \ufe38 \ufe37\ufe37 \ufe38 hPL2(z2) .\nAs hPL is separable with respect to z1 and z2, by (Rockafellar, 1985, Proposition 2.5) and (Rockafellar and Wets, 2009, Proposition 10.5), we know\n\u2202\u22b3hPL(z1,z2) = \u2202\u22b3hPL1(z1)\u00d7 \u2202\u22b3[\u2212hPL2](z2).\nNote that fPL(w) = hPL(U \u22a4 x w,U \u22a4 y w). We compute\n\u2202\u22b3fPL(w) = \u2202\u22b3 [ hPL(U \u22a4 x \u00b7,U\u22a4y \u00b7) ] (w)\n(a) = U\u2202\u22b3 [ hPL(\u00b7, \u00b7) ] ( U\u22a4x w,U \u22a4 y w )\n(b) = Ux\u2202\u22b3 [ hPL1 ] ( U\u22a4x w ) +Uy\u2202\u22b3[\u2212hPL2] ( U\u22a4y w )\n(c) = \u2202\u22b3 [ hPL1 ( U\u22a4x \u00b7 )] (w) + \u2202\u22b3 [ \u2212hPL2 ( U\u22a4y \u00b7 )] (w), (\u2666)\nwhere (a) is using (Clarke, 1990, Theorem 2.3.10), (Rockafellar and Wets, 2009, Exercise 10.7), and U is full column rank; (b) is from \u2202\u22b3hPL(z1,z2) = \u2202\u22b3hPL1(z1)\u00d7 \u2202\u22b3[\u2212hPL2](z2); (c) is using the reasoning in (a) for h1, h2 separately.\nIn particular for Clarke subdifferential, we know \u2202C [\u2212hPL2] = \u2212\u2202C [hPL2] using (Clarke, 1990, Proposition 2.3.1). As hPL2 is convex, \u2202C [hPL2] is equal to the convex subdifferential of hPL2 by (Clarke, 1990, Proposition 2.2.7). Then, by (Hiriart-Urruty and Lemare\u0301chal, 2004, \u00a7D, Corollary 4.3.2), a direct computation gives\n\u2202CfPL(w) = \u2202C [ hPL1 ( U\u22a4x \u00b7 )] (w) + ( \u2212\u2202C [ hPL2 ( U\u22a4y \u00b7 )] (w) ) = GCPL,\nas required.\nProof of Theorem 14. According to the argument in Appendix C.1, we only need to consider the Clarke subdifferential \u2202Cfk(wk) for every k \u2208 [H]. It is showed in Theorem 49 that we have\n\u2202Cfk(wk) = G\u0303 C k ,\nif and only if the following span qualification is satisfied:\nspan ( {xi}i\u2208I+\nk\n) \u2229 span ( {xj}j\u2208I\u2212\nk\n) = {0}.\nThen, put all k \u2208 [H] cases together, and Theorem 14 is proved."
        },
        {
            "heading": "C.4.2 Chain Rule for Limiting Subdifferential",
            "text": "Theorem 52 (Limiting). Suppose x\u22a4i w = y \u22a4 j w = 0 and yj 6= 0 for any i \u2208 [n], j \u2208 [m]. We have the exact limiting subdifferential chain rule\n\u2202fPL(w) = G L PL :=\nn\u2211\ni=1\nxi \u00b7 [0, 1] +   \u2212 m\u2211\nj=1\nyj \u00b7 1d\u22a4yj>0 : d \u2208 Rd, min16t6m \u2223\u2223\u2223d\u22a4yt \u2223\u2223\u2223 > 0   \nif and only if span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) = {0}.\nProof. (Sufficient) We begin with the general argument in the proof of Lemma 51 until Equation (\u2666). After that, we will focus on the proof of\n\u2202 [ \u2212hPL2 ( U\u22a4y \u00b7 )] (w) =   \u2212 m\u2211\nj=1\nyj \u00b7 1d\u22a4yj>0 : d \u2208 Rd, min16t6m \u2223\u2223\u2223d\u22a4yt \u2223\u2223\u2223 > 0    =: G L PL2.\nFor the ease of notation, we denote q(w) := \u2212hPL2 ( U\u22a4y w ) = \u2212\u2211mj=1max{y\u22a4j w, 0}. Note that by the definition of limiting subdifferential (see Definition 4), we have\n\u2202q(w) = lim sup w\u2032\u2192w\n\u2202\u0302q(w\u2032) = { g : \u2203{w\u03bd} \u2192 w and {g\u03bd} \u2192 g s.t. g\u03bd \u2208 \u2202\u0302q(w\u03bd),\u2200\u03bd } .\nLet g \u2208 \u2202q(w). Then, there exist {w\u03bd}\u03bd and {g\u03bd}\u03bd such that w\u03bd \u2192 w,g\u03bd \u2208 \u2202\u0302q(w\u03bd), and g\u03bd \u2192 g. We can assume for any \u03bd and any j \u2208 [m], we have w\u22a4\u03bd yj 6= 0, as otherwise, by Lemma 44, \u2202\u0302q(wk) = \u2205 and gk is undefined. Then, for any \u03bd, the function q is strictly differentiable at w\u03bd , which implies\n{g\u03bd} = \u2202\u0302q(w\u03bd) =   \u2212 m\u2211\nj=1\nyj \u00b7 1(w\u03bd\u2212w)\u22a4yj>0    \u2286 G L PL2.\nAs GL PL2 is a finite set, it is trivially closed with the usual Euclidean metric. We have \u2202q(w) \u2286 GL PL2. For the reverse direction, let g \u2032 \u2208 GL PL2. Then, there exists d such that\ng\u2032 = \u2212 m\u2211\nj=1\nyj \u00b7 1d\u22a4yj>0\nwith d\u22a4yj 6= 0 for any j \u2208 [m]. Let w\u03bd = w+d/\u03bd. We get w\u22a4\u03bd yj = \u03bd\u22121d\u22a4yj 6= 0 for any j \u2208 [m]. Then, we know the function q is strictly differentiable at w\u03bd and {g\u03bd} = \u2202\u0302q(w\u03bd). Thus, for any \u03bd, we get g\u03bd = g \u2032. Consequently, we get g\u2032 \u2208 \u2202\u0302q(w) and GL PL2 \u2286 \u2202q(w).\n(Necessary) Suppose 0 6= v \u2208 span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) and\n\u2202fPL(w) = \u2202 [ hPL1 ( U\u22a4x \u00b7 )] (w) + \u2202 [ \u2212hPL2 ( U\u22a4y \u00b7 )] (w) = GLPL.\nThen, by taking a convex hull on both size and using (Rockafellar and Wets, 2009, Theorem 8.49), we get\n\u2202CfPL(w) = \u2202C [ hPL1 ( U\u22a4x \u00b7 )] (w) + \u2202C [ \u2212hPL2 ( U\u22a4y \u00b7 )] (w) = Conv(GLPL) = G C PL,\nwhich is a contradiction to Lemma 50.\nProof of Theorem 16. According to the argument in Appendix C.1, we only need to consider the limiting subdifferential \u2202fk(wk) for every k \u2208 [H]. It is showed in Theorem 52 that we have\n\u2202fk(wk) = G\u0303 L k ,\nif and only if the following span qualification is satisfied:\nspan ( {xi}i\u2208I+\nk\n) \u2229 span ( {xj}j\u2208I\u2212\nk\n) = {0}.\nThen, put all k \u2208 [H] cases together, and Theorem 16 is proved."
        },
        {
            "heading": "C.4.3 Chain Rule for Fre\u0301chet Subdifferential",
            "text": "Theorem 53 (Fre\u0301chet). Suppose x\u22a4i w = y \u22a4 j w = 0 and yj 6= 0 for any i \u2208 [n], j \u2208 [m]. For any given w such that \u2202\u0302fPL(w) 6= \u2205, we have the following exact chain rule\n\u2202\u0302fPL(w) = G F PL\n:= n\u2211\ni=1\nxi \u00b7 [0, 1] + {\n\u2205 if m > 0, {0} if m = 0.\nif and only if span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) = {0}.\nProof. (Sufficient) We begin with the general argument in the proof of Lemma 51 until Equation (\u2666). We will focus on the proof of\n\u2202\u0302 [ \u2212hPL2 ( U\u22a4y \u00b7 )] (w) = GFPL2 :=\n{ \u2205 if m > 0,\n{0} if m = 0.\nFor the ease of notation, we denote q(w) := \u2212hPL2 ( U\u22a4y w ) = \u2212\u2211mj=1max{y\u22a4j w, 0}. Then, by Lemma 44, we know that if there exists j \u2208 [m] such that w\u22a4yj = 0 and yj 6= 0, then we have \u2202\u0302q(w) = \u2205. If m = 0, then q(w) = 0 and GF\nPL2 = {0}. The claim follows trivially. (Necessary) Suppose 0 6= v \u2208 span ( {xi}ni=1 ) \u2229 span ( {yj}mj=1 ) . There exists yj 6= 0\nas otherwise v /\u2208 {0} \u2287 span ( {yj}mj=1 ) . Then, we get m > 0 and GF PL = \u2205. Thus, from the assumption that \u2202\u0302fPL(w) 6= \u2205, we know \u2202\u0302fPL(w) ) GFPL = \u2205 by definition.\nProof of Theorem 17. According to the argument in Appendix C.1, we only need to consider the Fre\u0301chet subdifferential \u2202\u0302fk(wk) for every k \u2208 [H]. It is showed in Theorem 53 that we have\n\u2202\u0302fk(wk) = G F k ,\nif and only if the following span qualification is satisfied:\nspan ( {xi}i\u2208I+\nk\n) \u2229 span ( {xj}j\u2208I\u2212\nk\n) = {0}.\nThen, put all k \u2208 [H] cases together, and Theorem 17 is proved."
        },
        {
            "heading": "C.5 Proofs for Section 4.3",
            "text": "Definition 54 (Regularities). We consider the following regularity conditions:\n\u2022 General position data (Yun et al., 2018, Assumption 2): No d data points {x\u0303i}i \u2286 Rd\u22121 lie on the same affine hyperplane, which is equivalent to the nonexistence of w \u2208 Rd and index set J \u2286 [N ] with |J | > d such that w\u22a4xj = 0 for any j \u2208 J .\n\u2022 Linear Independence Kink Qualification (LIKQ) (Griewank and Walther, 2016, Definition 2), (Griewank and Walther, 2019, Definition 2.6): Let the j-th row of the matrix \u2207z\u03c3 in Appendix A be v\u22a4j . We define the following index set\n\u03b1 := { N(k \u2212 1) + i : w\u22a4k xi = 0 } = {j : zj = 0}.\nLIKQ is satisfied if the vectors {vi}i\u2208\u03b1 are linearly independent.\n\u2022 Linearly Independent Activated Data (LIAD): Let the index set Jk := {j : w\u22a4k xj = 0}. For any fixed k \u2208 [H], the data points {xi}i\u2208Jk are linearly independent.\nProof of Proposition 19. For the relation general position =\u21d2 LIAD, it directly follows from (Yun et al., 2018, Lemma 1). By the analysis in Appendix A.2, we know LIKQ is satisfied for the empirical loss of two-layer ReLU network if and only if\n{ H\u220f\nk\u2032=1\n0\u00d7 1k\u2032=k \u00b7 xi }\n(N(k\u22121)+i)\u2208\u03b1\nare linearly independent. It is easy to see that LIKQ holds if and only if, for any given k \u2208 [H], the data points {xi}i\u2208{j:w\u22a4\nk xj=0} are linearly independent. Thus, we have the\nrelation LIAD \u21d0\u21d2 LIKQ. Note that I+k \u222a I\u2212k = {j : w\u22a4k xj = 0}. If {xj}j\u2208I+ k \u222aI\u2212 k are linearly independent, then it is evident that\nspan ( {xi}i\u2208I+\nk\n) \u2229 span ( {xj}j\u2208I\u2212\nk\n) = {0},\nwhich implies LIAD =\u21d2 SQ.\nProof of Corollary 23. Under SQ, if the Fre\u0301chet subdifferential is nonempty, we get I\u2212k = \u2205 for any k \u2208 [H]. By Theorem 14 and Theorem 17, we have \u2202CL and \u2202\u0302L are equal at that point. Then, Clarke regularity follows from Definition 7. By Proposition 19, if the data points are in general position, then they satisfy SQ. Using (Rockafellar and Wets, 2009, Theorem 10.1), the Fre\u0301chet subdifferential is nonempty at every local minimizer, which completes the proof."
        },
        {
            "heading": "D Proofs for Section 5",
            "text": ""
        },
        {
            "heading": "D.1 Testing Clarke NAS",
            "text": "Proof of Theorem 28. We consider an \u03b5-Clarke stationary point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) with\n\u2016(u\u22171,w\u22171, . . . , u\u2217H ,w\u2217H)\u2016 6 B.\nBy Theorem 14, we know there exists g\u2217 \u2208 \u2202CL(u\u22171,w\u22171, . . . , u\u2217H ,w\u2217H) such that\n\u2016g\u2217 =: (g\u22171 ,g\u22171 , . . . , g\u2217H ,g\u2217H)\u2016 = dist ( 0, \u2202CL(u \u2217 1,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) 6 \u03b5.\nNote that u\u0302i = ui for any i \u2208 [H] in the returned vector (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) of Algorithm 3. In this subsection, we will write ui rather than u\u0302i for simplicity. Given a positive radius \u03b4 \u2208 (0, CClarke\u03c4 ], we aim to show that, for any\n(u1,w1, . . . , uH ,wH) \u2208 B\u03b4 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) ,\nwe can certify that the rounded point returned by Algorithm 3 satisfies\ndist ( 0, \u2202CL(u1, w\u03021, . . . , uH , w\u0302H) ) 6 \u03b5+ CClarke\u00b5 \u00b7 \u03b4,\nwhere CClarke\u00b5 < +\u221e is a constant depending on the curvature that we will discuss later. We define the following shorthands for convenience\n\u03c1\u2217i := \u2113 \u2032 i\n( H\u2211\nk=1\nu\u2217k \u00b7max { (w\u2217k) \u22a4xi, 0 }) , \u2200i \u2208 [N ],\n\u03c1\u0302i := \u2113 \u2032 i\n( H\u2211\nk=1\nuk \u00b7max { w\u0302\u22a4k xi, 0 }) , \u2200i \u2208 [N ].\nRecall the definition of the rounded {w\u0302k}k and we define indices sets J<k ,J =k ,J >k as\nw\u0302k = argmin z\u2208Rd\n\u2016z \u2212wk\u20162\ns.t. z\u22a4xi > 2R \u00b7 \u03b4, \u2200i \u2208 J >k := { j \u2208 [N ] : x\u22a4j wk > R \u00b7 \u03b4 } ,\nz\u22a4xi 6 \u22122R \u00b7 \u03b4, \u2200i \u2208 J <k := { j \u2208 [N ] : x\u22a4j wk < \u2212R \u00b7 \u03b4 } , z\u22a4xi = 0, \u2200i \u2208 J =k := { j \u2208 [N ] : \u2223\u2223\u2223x\u22a4j wk \u2223\u2223\u2223 6 R \u00b7 \u03b4 } .\nWe consider the following quantity related to the point w\u2217k for any k \u2208 [H]:\n\u03c4k := min\n{ min\ni:x\u22a4i w \u2217 k >0\nx\u22a4i w \u2217 k, \u2212 max\ni:x\u22a4i w \u2217 k <0\nx\u22a4i w \u2217 k\n} .\nNote that 0 < \u03b4 6 CClarke\u03c4 6 \u03c4k 4R . For any i \u2208 [N ] such that x\u22a4i w\u2217k > 0, we have\nx\u22a4i wk = x \u22a4 i w \u2217 k \u2212 x\u22a4i (w\u2217k \u2212wk)\n> x\u22a4i w \u2217 k \u2212 \u2016xi\u2016 \u00b7 \u2016w\u2217k \u2212wk\u2016 > \u03c4k \u2212R \u00b7 \u03b4 > 3R \u00b7 \u03b4 > R \u00b7 \u03b4.\nThus, we know { i : x\u22a4i w \u2217 k > 0 } \u2286 J >k . Similarly, for any i \u2208 [N ] such that x\u22a4i w\u2217k < 0, we have\nx\u22a4i wk = x \u22a4 i w \u2217 k + x \u22a4 i (wk \u2212w\u2217k)\n6 x\u22a4i w \u2217 k + \u2016xi\u2016 \u00b7 \u2016w\u2217k \u2212wk\u2016 6 \u2212\u03c4k +R \u00b7 \u03b4 6 \u22123R \u00b7 \u03b4 < \u2212R \u00b7 \u03b4,\nwhich implies { i : x\u22a4i w \u2217 k < 0 } \u2286 J<k . We have, for any i \u2208 [N ] such that x\u22a4i w\u2217k = 0, it holds \u2223\u2223\u2223x\u22a4i wk \u2223\u2223\u2223 6 \u2223\u2223\u2223x\u22a4i w\u2217k \u2223\u2223\u2223+ \u2223\u2223\u2223x\u22a4i (wk \u2212w\u2217k) \u2223\u2223\u2223\n6 \u2223\u2223\u2223x\u22a4i w\u2217k \u2223\u2223\u2223+ \u2016xi\u2016 \u00b7 \u2016w\u2217k \u2212wk\u2016\n6 R \u00b7 \u03b4.\nSo, we know { i : x\u22a4i w \u2217 k = 0 } \u2286 J=k . As J<k ,J =k ,J >k are disjoint and [N ] = J <k \u2294J=k \u2294J>k , we know { i : x\u22a4i w \u2217 k < 0 } = J<k , { i : x\u22a4i w \u2217 k = 0 } = J=k , { i : x\u22a4i w \u2217 k > 0 } = J>k .\nMeanwhile, as w\u0302k is feasible to the quadratic program in Algorithm 3, we get { i : x\u22a4i w\u0302k < 0 } = J<k , { i : x\u22a4i w\u0302k = 0 } = J=k , { i : x\u22a4i w\u0302k > 0 } = J>k ,\nwhich implies I+k (w\u2217k) \u222a I\u2212k (w\u2217k) = J =k = I+k (w\u0302k) \u222a I\u2212k (w\u0302k) and 1x\u22a4i w\u2217k>0 = 1i\u2208J>k = 1x\u22a4i w\u0302k>0 for any k \u2208 [H]. It is evident that\n\u2016(w\u03021, . . . , w\u0302H)\u2212 (w1, . . . ,wH)\u2016 6 \u2016(w1, . . . ,wH)\u2212 (w\u22171, . . . ,w\u2217H)\u2016 6 \u03b4,\nas, for any k \u2208 [H], w\u2217k is feasible to the quadratic program for computing w\u0302k in Algorithm 3. Therefore, we know\n(u1, w\u03021, . . . , uH , w\u0302H) \u2208 B\u03b4 ( (u1,w1, . . . , uH ,wH) ) .\nBy triangle inequality, it holds that\n(u1, w\u03021, . . . , uH , w\u0302H) \u2208 B2\u03b4 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) .\nUsing Theorem 14, we get\ndist ( 0, \u2202CL(u1, w\u03021, . . . , uH , w\u0302H) )\n6 \u2016g\u2217\u2016+ dist ( g\u2217, \u2202CL(u1, w\u03021, . . . , uH , w\u0302H) )\n6 \u03b5+ H\u2211\nk=1\n\u2223\u2223\u2223\u2223\u2223g \u2217 k \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223+ H\u2211\nk=1\ndist ( g\u2217k, \u2202CLk(w\u0302k) ) ,\nwhere we define Lk(w\u0302k) = \u2211N\ni=1 uk\u03c1\u0302i \u00b7max{x\u22a4i w\u0302k, 0}. We first compute N\u2211\ni=1\n\u2223\u2223\u2223\u03c1\u0302i \u2212 \u03c1\u2217i \u2223\u2223\u2223 = N\u2211\ni=1\n\u2223\u2223\u2223\u2223\u2223\u2113 \u2032 i ( H\u2211\nk=1\nuk \u00b7max { w\u0302\u22a4k xi, 0 }) \u2212 \u2113\u2032i ( H\u2211\nk=1\nu\u2217k \u00b7max { (w\u2217k) \u22a4xi, 0 })\u2223\u2223\u2223\u2223\u2223\n6 L\u2113\u2032 \u00b7 N\u2211\ni=1\n\u2225\u2225\u2225\u2225\u2225 H\u2211\nk=1\n( uk \u00b7max { w\u0302\u22a4k xi, 0 } \u2212 u\u2217k \u00b7max { (w\u2217k) \u22a4xi, 0 })\u2225\u2225\u2225\u2225\u2225\n6 L\u2113\u2032 \u00b7 N\u2211\ni=1\nH\u2211\nk=1\n( |uk| \u00b7 \u2016xi\u2016 \u00b7 \u2016w\u0302k \u2212w\u2217k\u2016+ \u2016w\u2217k\u2016 \u00b7 \u2016xi\u2016 \u00b7 |uk \u2212 u\u2217k| )\n6 3L\u2113\u2032NHBR \u00b7 \u03b4 =: C1 \u00b7 \u03b4.\nWe now upper bound the second term \u2211H\nk=1 \u2223\u2223\u2223g\u2217k \u2212 \u2211N i=1 \u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223. Note that\n\u2223\u2223\u2223\u2223\u2223g \u2217 k \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 N\u2211\ni=1\n\u03c1\u2217i \u00b7max { (w\u2217k) \u22a4xi, 0 } \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223\n6 \u2223\u2223\u2223\u2223\u2223 N\u2211\ni=1\n\u03c1\u2217i \u00b7max { (w\u2217k) \u22a4xi, 0 } \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { (w\u2217k) \u22a4xi, 0 }\u2223\u2223\u2223\u2223\u2223\n\ufe38 \ufe37\ufe37 \ufe38 =:T k\n1\n+ \u2223\u2223\u2223\u2223\u2223 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { (w\u2217k) \u22a4xi, 0 } \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223 \ufe38 \ufe37\ufe37 \ufe38\n=:T k 2\n.\nNow we estimate these two terms. For T k1 , we compute\nT k1 6 N\u2211\ni=1\n\u2016w\u2217k\u2016 \u00b7 \u2016xi\u2016 \u00b7 |\u03c1\u0302i \u2212 \u03c1\u2217i | 6 BR \u00b7 N\u2211\ni=1\n|\u03c1\u0302i \u2212 \u03c1\u2217i | 6 BR \u00b7 C1 \u00b7 \u03b4 =: C2 \u00b7 \u03b4.\nFor T k2 , we see that\nT k2 6 N\u2211\ni=1\n|\u03c1\u0302i| \u00b7 \u2016xi\u2016 \u00b7 \u2016wk \u2212w\u2217k\u2016 6 2L\u2113\u2032NR \u00b7 \u03b4 =: C3 \u00b7 \u03b4.\nSummarizing, we have\nH\u2211\nk=1\n\u2223\u2223\u2223\u2223\u2223g \u2217 k \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223 6 H\u2211\nk=1\nT k1 + T k 2 6 H(C2 + C3) \u00b7 \u03b4 =: C4 \u00b7 \u03b4.\nWe proceed to upper bound \u2211H k=1 dist ( g\u2217k, \u2202CLk(w\u0302k) ) .\nBy Theorem 14, we know that there exist \u03bej \u2208 [0, 1],\u2200j \u2208 I+k (w\u2217k)\u222a I\u2212k (w\u2217k) such that the Clarke subgradient g\u2217k \u2208 \u2202CLk(w\u2217k) can be written as\ng\u2217k := \u2211\ni\u2208[N ]\\ ( I+ k (w\u2217 k )\u222aI\u2212 k (w\u2217 k ) ) u\u2217k\u03c1 \u2217 i \u00b7 1x\u22a4i w\u2217k>0 \u00b7 xi +\n\u2211\nj\u2208I+ k (w\u2217 k )\u222aI\u2212 k (w\u2217 k )\nu\u2217k\u03c1 \u2217 j \u00b7 xj \u00b7 \u03bej.\nNow, we are well prepared to upper bound dist ( g\u2217k, \u2202CLk(w\u0302k) ) . Let\ng\u0302k := \u2211\ni\u2208[N ]\\ ( I+ k (w\u0302k)\u222aI\u2212k (w\u0302k)\n) uk\u03c1\u0302i \u00b7 1x\u22a4i w\u0302k>0 \u00b7 xi +\n\u2211\nj\u2208I+ k (w\u0302k)\u222aI\u2212k (w\u0302k)\nuk\u03c1\u0302j \u00b7 xj \u00b7 \u03bej,\nwhich, by Theorem 14, belongs to the Clarke subdifferential \u2202CLk(w\u0302k). We upper bound\ndist ( g\u2217k, \u2202CLk(w\u0302k) ) 6 \u2016g\u0302k \u2212 g\u2217k\u2016\nwith\n\u2016g\u0302k \u2212 g\u2217k\u2016\n= \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n\u2211\ni\u2208[N ]\\ ( I+ k (w\u0302k)\u222aI\u2212k (w\u0302k) )\n( uk\u03c1\u0302i \u2212 u\u2217k\u03c1\u2217i ) \u00b7 1x\u22a4\ni w\u0302k>0\n\u00b7 xi + \u2211\nj\u2208I+ k (w\u0302k)\u222aI\u2212k (w\u0302k)\n( uk\u03c1\u0302j \u2212 u\u2217k\u03c1\u2217j ) \u00b7 xj \u00b7 \u03bej \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n6 \u2211\n16i6N\n\u2016xi\u2016 \u00b7 \u2223\u2223\u2223uk\u03c1\u0302i \u2212 u\u2217k\u03c1\u2217i \u2223\u2223\u2223\n6 R \u00b7 \u2211\n16i6N\n( |uk| \u00b7 \u2223\u2223\u2223\u03c1\u0302i \u2212 \u03c1\u2217i \u2223\u2223\u2223+ |\u03c1\u2217i | \u00b7 \u2223\u2223\u2223uk \u2212 u\u2217k \u2223\u2223\u2223 )\n6 BR \u00b7 C1 \u00b7 \u03b4 +NRL\u2113\u2032 \u00b7 \u03b4.\nThen, we have\nH\u2211\nk=1\ndist ( g\u2217k, \u2202CLk(w\u0302k) ) 6 H\u2211\nk=1\n\u2016g\u0302k \u2212 g\u2217k\u2016 6 H(BR \u00b7 C1 +NRL\u2113\u2032) \u00b7 \u03b4 =: C5 \u00b7 \u03b4.\nIn sum, we have proved that\ndist ( 0, \u2202CL(u1, w\u03021, . . . , uH , w\u0302H) ) 6 \u03b5+ CClarke\u00b5 \u00b7 \u03b4,\nwhere CClarke\u00b5 := C4 + C5 = poly(B,R,L\u2113, L\u2113\u2032 , N,H)."
        },
        {
            "heading": "D.2 Testing Fre\u0301chet NAS",
            "text": "Proof of Theorem 32. Some steps in the computation are similar to these in the proof of Theorem 28 in Appendix D.1, and we may skip them for simplicity. We consider an \u03b5-Fre\u0301chet stationary point (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) with \u2016(u\u22171,w\u22171, . . . , u\u2217H ,w\u2217H)\u2016 6 B. By Theorem 17, there exists a regular subgradient g\u2217 \u2208 \u2202\u0302L(u\u22171,w\u22171, . . . , u\u2217H ,w\u2217H) such that\n\u2016g\u2217 =: (g\u22171 ,g\u22171 , . . . , g\u2217H ,g\u2217H)\u2016 = dist ( 0, \u2202\u0302L(u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) 6 \u03b5.\nGiven a positive radius \u03b4 \u2208 (0, CFre\u0301chet\u03c4 ], we aim to show that, for any\n(u1,w1, . . . , uH ,wH) \u2208 B\u03b4 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) ,\nwe can certify the rounded point returned by Algorithm 5 satisfying\ndist ( 0, \u2202\u0302L(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) ) 6 \u03b5+ CFre\u0301chet\u00b5 \u00b7 \u03b4,\nwhere CFre\u0301chet\u00b5 < +\u221e is a constant depending on the curvature that we will discuss later. Similar to Appendix D.1, we define the following shorthands for convenience\n\u03c1\u2217i := \u2113 \u2032 i\n( H\u2211\nk=1\nu\u2217k \u00b7max { (w\u2217k) \u22a4xi, 0 }) , \u2200i \u2208 [N ],\n\u03c1\u0302i := \u2113 \u2032 i\n( H\u2211\nk=1\nu\u0302k \u00b7max { w\u0302\u22a4k xi, 0 }) , \u2200i \u2208 [N ].\nWe consider the following quantity related to the point w\u2217k for any k \u2208 [H]:\n\u03c4k := min\n{ min\ni:x\u22a4i w \u2217 k >0\nx\u22a4i w \u2217 k, \u2212 max\ni:x\u22a4i w \u2217 k <0\nx\u22a4i w \u2217 k\n} .\nWe use the same indices sets J<k ,J =k ,J >k for computing the rounded {w\u0302k}k as those in Appendix D.1. Note that 0 < \u03b4 6 CFre\u0301chet\u03c4 6 \u03c4k 4R . The argument in Appendix D.1 shows that { i : x\u22a4i w \u2217 k < 0 } = J<k , { i : x\u22a4i w \u2217 k = 0 } = J=k , { i : x\u22a4i w \u2217 k > 0 } = J>k .\nMeanwhile, as w\u0302k is feasible to the quadratic program in Algorithm 5, we get\n{ i : x\u22a4i w\u0302k < 0 } = J<k , { i : x\u22a4i w\u0302k = 0 } = J=k , { i : x\u22a4i w\u0302k > 0 } = J>k ,\nwhich implies I+k (w\u2217k) \u222a I\u2212k (w\u2217k) = J =k = I+k (w\u0302k) \u222a I\u2212k (w\u0302k) and 1x\u22a4i w\u2217k>0 = 1i\u2208J>k = 1x\u22a4i w\u0302k>0 for any k \u2208 [H]. It is evident that\n\u2016(w\u03021, . . . , w\u0302H)\u2212 (w1, . . . ,wH)\u2016 6 \u2016(w1, . . . ,wH)\u2212 (w\u22171, . . . ,w\u2217H)\u2016,\nas, for any k \u2208 [H], w\u2217k is feasible to the quadratic program for computing w\u0302k in Algorithm 5.\nHowever, the identification of w\u2217k is not sufficient to bound dist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) , as the index set I\u2212k (w\u0302k) may not be an empty set, which, by Theorem 17, implies GFk = \u2205 and dist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) = +\u221e. We are thus looking for the identification of {u\u2217k}Hk=1. We define a constant Cu := L\u2113\u2032(4HRB 2 + 1) and consider the following quantity related to the point u\u2217k for any k \u2208 [H]:\n\u03c4 \u2032k := min { u\u2217k \u00b7 \u03c1\u2217i : i \u2208 J =k , u\u2217k \u00b7 \u03c1\u2217i > 0 } .\nNote that 0 < \u03b4 6 CFre\u0301chet\u03c4 6 \u03c4 \u2032 k 4Cu . Fix any k \u2208 [H] and we consider two cases. If u\u2217k 6= 0, by Theorem 17 and Assumption 30, we know u\u2217k \u00b7 \u03c1\u2217i > 0 for any i \u2208 J =k . Then, for any i \u2208 J=k , we have\nuk \u00b7 \u03c1i = u\u2217k \u00b7 \u03c1\u2217i + (uk \u00b7 \u03c1i \u2212 u\u2217k \u00b7 \u03c1\u2217i ) > u\u2217k \u00b7 \u03c1\u2217i \u2212 |uk \u00b7 \u03c1i \u2212 u\u2217k \u00b7 \u03c1\u2217i | > \u03c4 \u2032k \u2212 |uk| \u00b7 |\u03c1i \u2212 \u03c1\u2217i | \u2212 |\u03c1\u2217k| \u00b7 |uk \u2212 u\u2217k| > \u03c4 \u2032k \u2212 Cu \u00b7 \u03b4 > 3Cu \u00b7 \u03b4,\nwhich by the rounding step of u\u0302k in Algorithm 5 implies if u \u2217 k 6= 0, then u\u0302k = uk and\nu\u0302k \u00b7 \u03c1\u0302i > uk \u00b7 \u03c1i \u2212 |uk| \u00b7 |\u03c1\u0302i \u2212 \u03c1i| > 3Cu \u00b7 \u03b4 \u2212 Cu \u00b7 \u03b4 > 0.\nIf u\u2217k = 0, we can see that\n|uk \u00b7 \u03c1i| = |uk \u00b7 \u03c1i \u2212 u\u2217k \u00b7 \u03c1\u2217k| 6 Cu\u03b4,\nwhich implies u\u0302k = 0 by rounding step of u\u0302k in Algorithm 5. Thus, we have proved that for any k \u2208 [H] and i \u2208 J=k , we get u\u0302k \u00b7 \u03c1\u0302i > 0, hence that I\u2212k (w\u0302k) = I\u2212k (w\u2217k) = \u2205, and finally that I+k (w\u0302k) = I+k (w\u2217k). By Theorem 17, we conclude that \u2202\u0302L(u\u03021, w\u03021, . . . , u\u0302H , u\u0302H) 6= \u2205.\nSummarizing, we have\n\u2225\u2225\u2225(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H)\u2212 (u1,w1, . . . , uH ,wH) \u2225\u2225\u2225 2\n= \u2225\u2225\u2225(w\u03021, . . . , w\u0302H)\u2212 (w1, . . . ,wH) \u2225\u2225\u2225 2 + \u2211\nk:u\u2217 k =0\n|u\u0302k \u2212 uk|2 (uk = u\u0302k if u\u2217k 6= 0)\n6 \u2225\u2225\u2225(w1, . . . ,wH)\u2212 (w\u22171, . . . ,w\u2217H) \u2225\u2225\u2225 2 + \u2211\nk:u\u2217 k =0\n|u\u2217k \u2212 uk|2, (by u\u2217k = u\u0302k = 0)\n6 \u2225\u2225\u2225(u1,w1, . . . , uH ,wH)\u2212 (u\u22171,w\u22171, . . . , u\u2217H ,w\u2217H) \u2225\u2225\u2225 2 6 \u03b42.\nThis shows by triangle inequality that\nB2\u03b4 ( (u\u22171,w \u2217 1, . . . , u \u2217 H ,w \u2217 H) ) \u220b (u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) \u2208 B\u03b4 ( (u1,w1, . . . , uH ,wH) ) .\nUsing Theorem 17, we get\ndist ( 0, \u2202\u0302L(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) ) 6 \u2016g\u2217\u2016+ dist ( g\u2217, \u2202\u0302L(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) )\n6 \u03b5+ H\u2211\nk=1\n\u2223\u2223\u2223\u2223\u2223g \u2217 k \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223+ H\u2211\nk=1\ndist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) ,\nwhere we define Lk(w\u0302k) = \u2211N\ni=1 u\u0302k\u03c1\u0302i \u00b7max{x\u22a4i w\u0302k, 0}. We first compute N\u2211\ni=1\n\u2223\u2223\u2223\u03c1\u0302i \u2212 \u03c1\u2217i \u2223\u2223\u2223 = N\u2211\ni=1\n\u2223\u2223\u2223\u2223\u2223\u2113 \u2032 i ( H\u2211\nk=1\nu\u0302k \u00b7max { w\u0302\u22a4k xi, 0 }) \u2212 \u2113\u2032i ( H\u2211\nk=1\nu\u2217k \u00b7max { (w\u2217k) \u22a4xi, 0 })\u2223\u2223\u2223\u2223\u2223\n6 L\u2113\u2032 \u00b7 N\u2211\ni=1\nH\u2211\nk=1\n( |u\u0302k| \u00b7 \u2016xi\u2016 \u00b7 \u2016w\u0302k \u2212w\u2217k\u2016+ \u2016w\u2217k\u2016 \u00b7 \u2016xi\u2016 \u00b7 |u\u0302k \u2212 u\u2217k| )\n6 4L\u2113\u2032NHBR \u00b7 \u03b4 =: C1 \u00b7 \u03b4.\nWe now upper bound the second term \u2211H\nk=1 \u2223\u2223\u2223g\u2217k \u2212 \u2211N i=1 \u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223. A computation similar to that in Appendix D.1 shows that\nH\u2211\nk=1\n\u2223\u2223\u2223\u2223\u2223g \u2217 k \u2212 N\u2211\ni=1\n\u03c1\u0302i \u00b7max { w\u0302\u22a4k xi, 0 }\u2223\u2223\u2223\u2223\u2223 6 H(C2 + C3) \u00b7 \u03b4 =: C4 \u00b7 \u03b4.\nwhere C2 := BR \u00b7 C1 and C3 := 2L\u2113\u2032NR. We proceed to upper bound \u2211H k=1 dist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) . By Theorem 17, we know that there exist \u03bej \u2208 [0, 1],\u2200j \u2208 I+k (w\u2217k) such that g\u2217k \u2208 \u2202\u0302Lk(w\u2217k) can be written as\ng\u2217k := \u2211\ni\u2208[N ]\\ ( I+ k (w\u2217 k )\u222aI\u2212 k (w\u2217 k ) ) u\u2217k\u03c1 \u2217 i \u00b7 1x\u22a4i w\u2217k>0 \u00b7 xi +\n\u2211\nj\u2208I+ k (w\u2217 k )\nu\u2217k\u03c1 \u2217 j \u00b7 xj \u00b7 \u03bej.\nNow, we are well prepared to upper bound dist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) . Let\ng\u0302k := \u2211\ni\u2208[N ]\\ ( I+ k (w\u0302k)\u222aI\u2212k (w\u0302k)\n) u\u0302k\u03c1\u0302i \u00b7 1x\u22a4 i w\u0302k>0\n\u00b7 xi + \u2211\nj\u2208I+ k (w\u0302k)\nu\u0302k\u03c1\u0302j \u00b7 xj \u00b7 \u03bej ,\nwhich, by Theorem 17, belongs to the Fre\u0301chet subdifferential \u2202\u0302Lk(w\u0302k). We proceed to upper bound dist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) 6 \u2016g\u0302k \u2212 g\u2217k\u2016 with\n\u2016g\u0302k \u2212 g\u2217k\u2016\n= \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n\u2211\ni\u2208[N ]\\ ( I+ k (w\u0302k)\u222aI\u2212k (w\u0302k) )\n( u\u0302k\u03c1\u0302i \u2212 u\u2217k\u03c1\u2217i ) \u00b7 1x\u22a4i w\u0302k>0 \u00b7 xi + \u2211\nj\u2208I+ k (w\u0302k)\n( u\u0302k\u03c1\u0302j \u2212 u\u2217k\u03c1\u2217j ) \u00b7 xj \u00b7 \u03bej \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n6 R \u00b7 \u2211\n16i6N\n( |u\u0302k| \u00b7 \u2223\u2223\u2223\u03c1\u0302i \u2212 \u03c1\u2217i \u2223\u2223\u2223+ |\u03c1\u2217i | \u00b7 \u2223\u2223\u2223u\u0302k \u2212 u\u2217k \u2223\u2223\u2223 )\n6 BR \u00b7 C1 \u00b7 \u03b4 + 2NRL\u2113\u2032 \u00b7 \u03b4.\nThen, we have\nH\u2211\nk=1\ndist ( g\u2217k, \u2202\u0302Lk(w\u0302k) ) 6 H\u2211\nk=1\n\u2016g\u0302k \u2212 g\u2217k\u2016 6 H(BR \u00b7 C1 + 2NRL\u2113\u2032) \u00b7 \u03b4 =: C5 \u00b7 \u03b4.\nIn sum, we have proved that\ndist ( 0, \u2202\u0302L(u\u03021, w\u03021, . . . , u\u0302H , w\u0302H) ) 6 \u03b5+ CFre\u0301chet\u00b5 \u00b7 \u03b4,\nwhere CFre\u0301chet\u00b5 := C4 + C5 = poly(B,R,L\u2113, L\u2113\u2032 , N,H)."
        }
    ],
    "title": "Testing Stationarity Concepts for ReLU Networks: Hardness, Regularity, and Robust Algorithms",
    "year": 2023
}