{
    "abstractText": "Session-based recommendation is a challenging problem in the real-world scenes, e.g., ecommerce, short video platforms, and music platforms, which aims to predict the next click action based on the anonymous session. Recently, graph neural networks (GNNs) have emerged as the stateof-the-art methods for session-based recommendation. However, we find that there exist two limitations in these methods. One is the item transition relations are not fully exploited since the relations are not explicitly modeled. Another is the long-range dependencies between items cannot be captured effectively due to the limitation of GNNs. To solve the above problems, we propose a novel approach for session-based recommendation, called Transition Relation Aware Self-Attention (TRASA). Specifically, TRASA first converts the session to a graph and then encodes the shortest path between items through the gated recurrent unit as their transition relation. Then, to capture the long-range dependencies, TRASA utilizes the selfattention mechanism to build the direct connection between any two items without going through intermediate ones. Also, the transition relations are incorporated explicitly when computing the attention scores. Extensive experiments on three real-word datasets demonstrate that TRASA outperforms the existing state-of-the-art methods consistently.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guanghui Zhu"
        },
        {
            "affiliations": [],
            "name": "Haojun Hou"
        },
        {
            "affiliations": [],
            "name": "Jingfan Chen"
        },
        {
            "affiliations": [],
            "name": "Chunfeng Yuan"
        },
        {
            "affiliations": [],
            "name": "Yihua Huang"
        }
    ],
    "id": "SP:8826d2c261893c4737858083b4e8eb515c890555",
    "references": [
        {
            "authors": [
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "On the bottleneck of graph neural networks and its practical implications",
            "venue": "arXiv preprint arXiv:2006.05205,",
            "year": 2020
        },
        {
            "authors": [
                "Deng Cai",
                "Wai Lam. Graph transformer for graph-to-sequence learning. In Proceedings of the AAAI Conference on Artificial Intelligence"
            ],
            "title": "volume 34",
            "venue": "pages 7464\u20137471,",
            "year": 2020
        },
        {
            "authors": [
                "Chen",
                "Wong",
                "2020] Tianwen Chen",
                "Raymond ChiWing Wong"
            ],
            "title": "Handling information loss of graph neural networks for session-based recommendation",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Chen et al",
                "2021] Chen Chen",
                "Jie Guo",
                "Bin Song"
            ],
            "title": "Dual attention transfer in session-based recommendation with multi-dimensional integration",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Cho et al",
                "2014] Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Caglar Gulcehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078,",
            "year": 2014
        },
        {
            "authors": [
                "Priyanka Gupta",
                "Diksha Garg",
                "Pankaj Malhotra",
                "Lovekesh Vig",
                "Gautam Shroff"
            ],
            "title": "Niser: Normalized item and session representations to handle popularity bias",
            "venue": "arXiv preprint arXiv:1909.04276,",
            "year": 2019
        },
        {
            "authors": [
                "Bal\u00e1zs Hidasi",
                "Alexandros Karatzoglou",
                "Linas Baltrunas",
                "Domonkos Tikk"
            ],
            "title": "Sessionbased recommendations with recurrent neural networks",
            "venue": "arXiv preprint arXiv:1511.06939,",
            "year": 2015
        },
        {
            "authors": [
                "Yujia Li",
                "Daniel Tarlow",
                "Marc Brockschmidt",
                "Richard Zemel"
            ],
            "title": "Gated graph sequence neural networks",
            "venue": "arXiv preprint arXiv:1511.05493,",
            "year": 2015
        },
        {
            "authors": [
                "Jing Li",
                "Pengjie Ren",
                "Zhumin Chen",
                "Zhaochun Ren",
                "Tao Lian",
                "Jun Ma. Neural attentive session-based recommendation"
            ],
            "title": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management",
            "venue": "pages 1419\u20131428,",
            "year": 2017
        },
        {
            "authors": [
                "Qimai Li",
                "Zhichao Han",
                "Xiao-Ming Wu"
            ],
            "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
            "venue": "Thirty-Second AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Liu et al",
                "2018] Qiao Liu",
                "Yifu Zeng",
                "Refuoe Mokhosi",
                "Haibin Zhang"
            ],
            "title": "Stamp: short-term attention/memory priority model for session-based recommendation",
            "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2018
        },
        {
            "authors": [
                "Qiu et al",
                "2019] Ruihong Qiu",
                "Jingjing Li",
                "Zi Huang",
                "Hongzhi Yin"
            ],
            "title": "Rethinking the item order in session-based",
            "year": 2019
        },
        {
            "authors": [
                "Steffen Rendle",
                "Christoph Freudenthaler",
                "Lars Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation"
            ],
            "title": "In Proceedings of the 19th international conference on World wide web",
            "venue": "pages 811\u2013820,",
            "year": 2010
        },
        {
            "authors": [
                "Badrul Sarwar",
                "George Karypis",
                "Joseph Konstan",
                "John Riedl. Item-based collaborative filtering recommendation algorithms"
            ],
            "title": "In Proceedings of the 10th International Conference on World Wide Web",
            "venue": "WWW \u201901, page 285\u2013295, New York, NY, USA,",
            "year": 2001
        },
        {
            "authors": [
                "Guy Shani",
                "David Heckerman",
                "Ronen I Brafman",
                "Craig Boutilier. An mdp-based recommender system"
            ],
            "title": "Journal of Machine Learning Research",
            "venue": "6(9),",
            "year": 2005
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin. Attention is all you need"
            ],
            "title": "In Advances in neural information processing systems",
            "venue": "pages 5998\u20136008,",
            "year": 2017
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "arXiv preprint arXiv:1710.10903,",
            "year": 2017
        },
        {
            "authors": [
                "Wang et al",
                "2015] Pengfei Wang",
                "Jiafeng Guo",
                "Yanyan Lan",
                "Jun Xu",
                "Shengxian Wan",
                "Xueqi Cheng"
            ],
            "title": "Learning hierarchical representation model for nextbasket recommendation",
            "venue": "In Proceedings of the 38th International ACM SIGIR conference on Research and Development",
            "year": 2015
        },
        {
            "authors": [
                "Wang et al",
                "2020] Ziyang Wang",
                "Wei Wei",
                "Gao Cong",
                "Xiao-Li Li",
                "Xian-Ling Mao",
                "Minghui Qiu"
            ],
            "title": "Global context enhanced graph neural networks for session-based recommendation",
            "venue": "In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development",
            "year": 2020
        },
        {
            "authors": [
                "Shoujin Wang",
                "Longbing Cao",
                "Yan Wang",
                "Quan Z Sheng",
                "Mehmet A Orgun",
                "Defu Lian. A survey on session-based recommender systems"
            ],
            "title": "ACM Computing Surveys (CSUR)",
            "venue": "54(7):1\u201338,",
            "year": 2021
        },
        {
            "authors": [
                "Shu Wu",
                "Yuyuan Tang",
                "Yanqiao Zhu",
                "Liang Wang",
                "Xing Xie",
                "Tieniu Tan. Session-based recommendation with graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence"
            ],
            "title": "volume 33",
            "venue": "pages 346\u2013353,",
            "year": 2019
        },
        {
            "authors": [
                "Chengfeng Xu",
                "Pengpeng Zhao",
                "Yanchi Liu",
                "Victor S Sheng",
                "Jiajie Xu",
                "Fuzhen Zhuang",
                "Junhua Fang",
                "Xiaofang Zhou. Graph contextualized selfattention network for session-based recommendation. In IJCAI"
            ],
            "title": "volume 19",
            "venue": "pages 3940\u20133946,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recommender systems (RS) play a very important role in many real-world web applications, e.g., e-commerce, short video platforms, and music platforms. The reason behind the great success of RS is that it can address the information overload problem by making personalized recommendation for every user. Traditional RS make recommendations based on users\u2019 profiles and their all historical activities. However, this information cannot be accessed in some scenarios, where only the behaviors of an anonymous user in a short\n\u2217Contact Author\nperiod is available. To improve the quality of recommendation for anonymous users, session-based recommendation is proposed, which aims to make a better prediction for the next click action based on an anonymous session. Figure 1 shows a session composed of multiple user-item interactions that happen together in a continuous period of time.\nDue to its highly practical value, there have been many studies about session-based recommendation [Wang et al., 2021]. Early methods are mainly based on item similarities [Sarwar et al., 2001] or Markov Chains [Shani et al., 2005; Rendle et al., 2010; Wang et al., 2015]. Due to the success of deep learning, many deep learning-based methods have been applied in session-based recommendation. They capture the user\u2019s interest by using recurrent neural networks (RNNs) [Hidasi et al., 2015], applying attention mechanism [Liu et al., 2018] or utilizing both [Li et al., 2017]. Recently, graph neural network (GNN)-based methods have become the most popular methods used in session-based recommendation and have become the state-of-the-arts [Wu et al., 2019; Qiu et al., 2019; Xu et al., 2019; Chen and Wong, 2020; Wang et al., 2020; Chen et al., 2021]. Although the GNNbased methods have achieved promising results in sessionbased recommendation, we find that they still have two limitations.\nFirst, the transition relations between items are not fully exploited. As shown in Figure 1, v5 is the third item clicked after v3. The transition relation between v3 and v5 is v3\nnext\u2212\u2212\u2212\u2192 v4 next\u2212\u2212\u2212\u2192 v2 next\u2212\u2212\u2212\u2192 v5. In GNN-based methods, although these transition relations can be reflected in the graph topology to some extent, they are not modeled explicitly in the message passing process. The position embeddings can capture partial information of transition relations [Wang et al., 2020]. However, position embeddings treat the session as a sequence and every position is encoded as a unique vector. But the same item may appear multiple times in a session, e.g., v2 in Figure 1. Thus, it is inappropriate that the same item has multiple different position embeddings in a session.\nar X\niv :2\n20 3.\n06 40\n7v 1\n[ cs\n.I R\n] 1\n2 M\nar 2\n02 2\nSecond, the long-range dependencies between items can not be captured effectively. Even though GNNs can model graph-structured data, they can not be stacked many layers due to the over-smoothing or over squashing problems [Li et al., 2018; Alon and Yahav, 2020]. GNN-based methods usually achieve the best performance using 1 to 3 layers, which means the item dependencies over 3-hop neighbors are hardly captured. However, the length of real-world sessions is usually greater than 3 [Wang et al., 2021] and the long-range dependencies do exist. In Figure 1, even though the distance between v1 and v5 is 5, they still have a strong dependency since they are both phones. Since a session depicts a user\u2019s short-term interest, it is reasonable to assume that every item appeared in the same session should have a strong or weak relationship that cannot be ignored.\nTo address the two limitations, we propose a novel method for session-based recommendation, called Transition Relation Aware Self-Attention (TRASA). A session sequence is first converted to a graph where the nodes represent different items and edges represent their transition order in the original session sequence. To model the transition relations between items accurately, a relation encoder is introduced to encode all the relations from a graph perspective explicitly. Specifically, it encodes the shortest path between two items by GRU (Gated Recurrent Unit [Cho et al., 2014]) as their transition relation. To capture the long-range dependencies between items, a self-attention mechanism is applied to get the potential relation between any two items without going through intermediate ones. The transition relations are utilized when computing the attention scores. Finally, the graph is reverted to its original sequence to get the final session representation and make prediction. We summarize our contributions as follows:\n\u2022 To the best of our knowledge, we are the first to propose to encode the shortest path between any two items as their transition relations from a graph perspective in session-based recommendation.\n\u2022 We apply a self-attention mechanism to make direct connections between any two items in a session, which can capture the long-range dependencies effectively. And we incorporate the transition relations when calculating the attention scores.\n\u2022 We conduct extensive experiments on three real-world datasets and the results demonstrate that our method outperforms the state-of-the-art methods."
        },
        {
            "heading": "2 Related Work",
            "text": "The existing session-based recommendation methods can be divided into the following categories. Traditional Methods. Since a session is a sequence of items clicked in chronological order, Markov Chain-based methods have been proposed in session-based recommendation. Markov Chain-based methods map a session into a Markov Chain and predict the next clicked item based on the last one. MDP [Shani et al., 2005] applies Markov decision processes to model the recommendation process as a sequential process. As an improvement, FPMC [Rendle et\nal., 2010] utilizes both Markov chains and matrix factorization to capture sequential effects and long-term user taste simultaneously. Deep Learning-based Methods. These methods are mainly based on RNNs. GRU4REC [Hidasi et al., 2015] is the first to apply RNN in session-based recommendation where multiple GRU layers are stacked to make the prediction. NARM [Li et al., 2017] employs a hybrid encoder using attention mechanism to capture the user\u2019s main purpose in the current session. To alleviate the bias introduced by time series, STAMP [Liu et al., 2018] proposes a short-term attention/priority model with a novel attention mechanism to capture users\u2019 interest instead of using RNN. GNN-based Methods. Due to the ability to model the complex relationships among items, GNNs have been received increasing attention in session-based recommendation. SRGNN [Wu et al., 2019] is the first to model sessions as graph structured data. It converts a session sequence into an unweighted directed graph and applies gated graph neural network (GGNN) [Li et al., 2015] to capture the complex item transitions in the session. FGNN [Qiu et al., 2019] formulates the session recommendation as a graph classification problem. It encodes a session sequence to a weighted directed graph and stacks multiple weighted graph attention layers to get accurate item representations. GC-SAN [Xu et al., 2019] also models the session sequence as a weighted directed graph. In GC-SAN, the local dependencies between items are captured by GGNN while the long-range dependencies are extracted through a self-attention network. LESSR [Chen and Wong, 2020] converts a session into two kinds of graphs. One aims to generate a losslessly encoded graph and the other aims to address the long-range dependency problem. Two different layers are designed to learn item representations. GCEGNN [Wang et al., 2020] learns two levels of item representations from both session graph and global graph, where the session graph is constructed based on the current session and the global graph is constructed from items\u2019 neighbors in all sessions. Graph attention network [Velic\u030ckovic\u0301 et al., 2017] is applied to learn item representations. DAT-MDI [Chen et al., 2021] combines dual tansfer with GNNs to learn crossdomain representation for session-based recommendation.\nUnlike all the existing methods, we propose the first selfattention-based method that incorporates the transition relations between items for session-based recommendation."
        },
        {
            "heading": "3 Preliminary: Self-Attention Mechanism",
            "text": "The self-attention mechanism is originally used to model the direct relation between a source vector and any other context vectors (including itself) in a sequence. It first computes the attention scores between the source vector and the other context vectors. Then, all the context vectors are aggregated based on the attention scores to get the new representation of the source vector. Formally, given a source vector x \u2208 Rd and the set of context vectors {y1,y2, ...,yn} with the same dimension, the attention scores are calculated by their dotproduct after applying two different linear transformations.\nscore(x,yi) = (Wqx) T Wkyi (1)\nwhere Wq,Wk \u2208 Rd\u00d7d \u2032\nare learnable parameter matrices. Then the attention scores are normalized and a softmax\nfunction is applied to get the final attention scores.\nai = exp\n( score (x,yi) / \u221a d\u2032 )\n\u2211n j=1 exp ( score (x,yj) / \u221a d\u2032 ) (2)\nFinally, all the context vectors are aggregate based on the attention score to get the final output attn (the new representation of the source vector) after a linear transformation.\nattn = n\u2211 i=1 aiWvyi (3)\nwhere Wv \u2208 Rd \u2032\u00d7d is a learnable parameter matrix."
        },
        {
            "heading": "4 Methodology",
            "text": ""
        },
        {
            "heading": "4.1 Problem Definition",
            "text": "The session-based recommendation problem is to predict the next item based on the historically interacted items of the active session [Wang et al., 2021]. We first present a formulation of session-based recommendation as below.\nLet set V = {v1, v2, ..., vn} denote all items that appear in all sessions. An anonymous session can be represented as a sequence Ss = {vs1, vs2, ..., vsl }, where each item corresponds to an item in V . Ss indicates chronologically ordered useritem interactions in a continuous period of time. The goal of session-based recommendation is to predict the top-K items from V that are most likely to be clicked by the user in a session. A typical model for session-based recommendation outputs probabilities y\u0302 for all items, where each element in y\u0302 represents the recommendation score for the corresponding item. Then the top-K items with highest scores will be recommended to the user.\nFigure 2 shows the workflow of the proposed method TRASA for session-based recommendation, which consists\nof four stages: graph construction, item representation learning, session representation, and prediction."
        },
        {
            "heading": "4.2 Graph Construction",
            "text": "In our method, a session Ss is modeled as a directed graph Gs = (Vs, Es). Vs \u2286 V is the node set corresponding to unique item in the original session and Es = {esij} is the edge set. Inspired by [Wang et al., 2020], Es has four types. As shown in Figure 2 (a), if (vsi , v s j ) are adjacent items in session Ss, two different types of directed edges are added between them. One is from vsi to v s j , i.e., NXT edge. The other is from vsj to v s i , i.e., PRE edge. In addition, if (v s j , v s i ) also appears in session Ss, the edge between vsi and v s j will be changed to NPL edge. It is obvious that edges in the graph depict the click order of items in the original session. Finally, a self loop is added to each item because we assume that an item is also related to itself. We call this type SELF edge. We assign each type of edge a learnable embedding, which is later used to encode the transition relations between items."
        },
        {
            "heading": "4.3 Item Representation Learning",
            "text": "After constructing the graph, we encode the shortest path between any two items as their transition relation and use the self-attention mechanism to learn the item representations.\nRelation Encoder Given the graph, we calculate the shortest path for each pair of items and use it to encode their transition relation [Cai and Lam, 2020]. Suppose the shortest path between item vsi and vsj is v s i NEXT\u2212\u2212\u2212\u2212\u2212\u2192 vsk NEXT\u2212\u2212\u2212\u2212\u2212\u2192 vsk\u2032\nNPL\u2212\u2212\u2212\u2192 vsj . Since the user\u2019s clicks are in chronological order, we use recurrent neural network with GRU to transform the shortest path to a fixedlength vector. Specifically, we employ the bi-directional GRU to encode the path. The input for the GRU is the edge sequence NEXT \u2192 NEXT \u2192 NPL and the last hidden states of the GRU in both directions are concatenated as the final relation for item vsi and v s j .\nRelation Aware Self-Attention Next, we calculate the attention score aij between item vsi and vsj . Their relation is rij which is encoded by the relation encoder. To incorporate transition relations in self-attention layers, rij is first split into two relations ri\u2192j and rj\u2192i, corresponding to the relation from vsi to v s j and the relation from vsj to v s i separately:\n[ri\u2192j ; rj\u2192i] = Wrrij (4)\nwhere Wr \u2208 R2d\u00d7d is the learnable parameter and d is the dimension of relation embeddings.\nThen, the attention score is calculated based on both item embeddings and their transition relations:\naij = (hi + ri\u2192j)W T q Wk(hj + rj\u2192i) (5)\nFollowing the transformer [Vaswani et al., 2017] architecture, we use multi-head attention to get multi-head outputs and they are concatenated and projected to get the final values. As shown in Figure 2 (b), a feed forward network and two residual connections are also applied in self-attention layers. Finally, mulitple self-attention layers are stacked to achieve better performance."
        },
        {
            "heading": "4.4 Session Representation",
            "text": "After generating the item representations Hg = {hg1,h g 2, ...,h g m}. We need to get the final session representation. We first convert the graph to its original sequence Hs = {hs1,hs2, ...,hsl }. Inspired by [Wang et al., 2020], we introduce reversed position embedding to keep the sequential position information. Intuitively, items have the same distance from the last item in the session should have the same importance for predicting the next click. The reversed position embeddings can portray this importance accurately. The position embeddings correspond to a learnable vector set Ps = {p1,p2, ...,pl}, which are added to the item representations to get the final item representations.\nhsi = h s i + pl\u2212i+1 (6)\nSince items clicked by users recently can reflect their current interests effectively, the recent clicked items have a strong influence to predict the next clicked item. Following [Wu et al., 2019; Chen and Wong, 2020], we regard the last item representation in a session as a user\u2019s current interest, i.e., hsl . Then the user\u2019s preference is computed based on all the items in the session and the current interest. Since each item in a session should have different contributions to the final session representation, we aggregate all the item representations using different weights.\nsh = l\u2211 i=1 \u03b3ih s i (7)\nwhere the weight \u03b3i is decided by both the current item representation and the user\u2019s current interest (the last item representation). A soft-attention mechanism is applied to compute the weight.\ni = q T (W4h s i +W5h s l + b3) (8)\n\u03b3i = exp ( i)\u2211l j=1 exp ( j)\n(9)\nwhere W5,W6 \u2208 Rd\u00d7d, q \u2208 Rd, and b3 \u2208 Rd are learnable parameters."
        },
        {
            "heading": "4.5 Prediction",
            "text": "After obtaining the final representation for each session, we can use it to compute the probabilities for all candidate items in V . To alleviate the long-tail problem in recommendation [Gupta et al., 2019] and make our model get a better convergence, we perform L2 normalization for all item embeddings in V . Then, for each item in V , we calculate its score based on its embedding and the session representation as follows:\nz\u0302i = s T hhvi (10)\nThe softmax function is leveraged to get the final predicted probability:\ny\u0302 = softmax (z\u0302) (11)\nwhere y\u0302 denotes the probabilities of all items in V to be clicked next in the current session.\nFinally, we employ cross-entropy of the prediction and the ground truth as the objective function to train model parameters.\nL(y\u0302) = \u2212 n\u2211\ni=1\nyilog(y\u0302i) + (1\u2212 yi)log(1\u2212 y\u0302i) (12)\nwhere y is the one-hot encoding vector of the ground truth item and y\u0302i is the probability of item vi."
        },
        {
            "heading": "5 Experiments",
            "text": "We conduct extensive experiments to verify the effectiveness of the proposed method TRASA and mainly answer the following questions:\n\u2022 RQ1: Does TRASA achieve the state-of-the-art performance compared to the existing methods?\n\u2022 RQ2: How does each component of TRASA affect the performance?\n\u2022 RQ3: How do different hyper-parameter settings affect the model performance?"
        },
        {
            "heading": "5.1 Datasets and Preprocessing",
            "text": "We use three publicly available real-world datasets, named Diginetica1, Gowalla2, Last.fm3. The three datasets are commonly used in literatures of session-based recommendation. We first preprocess these three datasets following [Chen and Wong, 2020]. Sessions of length 1 and items with less than 5 occurrences in all sessions are filtered in all three datasets. After that, we apply data augmentation for all sessions. For example, given a session S = {v1, v2, ...vl}, we generate the sequences and its corresponding labels as ([v1], v2), ([v1, v2], v3), ..., ([v1, v2, ...vl\u22121], vl) for both training data and testing data. The statistics of the three preprecessed datasets are summarized in Table 1.\n1https://competitions.codalab.org/competitions/11161 2https://snap.stanford.edu/data/loc-Gowalla.html 3http://ocelma.net/MusicRecommendationDataset/lastfm-\n1K.html"
        },
        {
            "heading": "5.2 Baseline Algorithms and Evaluation Metrices",
            "text": "We compare TRASA with the following baselines that involves traditional methods, deep learning-based methods, and the SOTA GNN-based methods.\n\u2022 Item-KNN [Sarwar et al., 2001] recommends items based on the consine similarity between items.\n\u2022 FPMC [Rendle et al., 2010] utilizes both Markov chains and matirx factorization to capture user\u2019s interest.\n\u2022 GRU4Rec [Hidasi et al., 2015] is a RNN-based method which stacks multiple GRU layers to model sessions.\n\u2022 NARM [Li et al., 2017] employs a hybrid encoder using attention mechanism to capture the user\u2019s main purpose in the current session.\n\u2022 STAMP [Liu et al., 2018] uses a short-term attention/priority model with attention mechanism to capture user\u2019s interest.\n\u2022 SR-GNN [Wu et al., 2019] converts a session to a graph and applies GNN to learn item representations.\n\u2022 GC-SAN [Xu et al., 2019] utilizes GGNN to capture the local dependencies and extracts the long-range dependencies by a self-attention network.\n\u2022 LESSR [Chen and Wong, 2020] proposes to convert the session into two kinds of graphs and applies corresponding GNN layers to learn item embeddings.\n\u2022 GCE-GNN [Wang et al., 2020] learns two levels of item representations from the session graph and global graph through graph attention networks and uses both of them to make prediction.\nNote that all the methods use the same preprocessed datasets. The reason for no comparison with DAT-MDI [Chen et al., 2021] is that DAT-MDI uses multiple datasets to implement cross-domain recommendation and no publicly available code has been found for DAT-MDI. Following previous works [Wu et al., 2019; Wang et al., 2020], we adapt two commonly used metrics: P@N (Precision) and MRR@N (Mean Reciprocal Rank) as evaluation metrics."
        },
        {
            "heading": "5.3 Parameter Setup",
            "text": "In TRASA, we set the item embedding size d = 64 for all three datasets. The batch-size is 512 for Diginetica and Gowalla and 2048 for Last.fm. All parameters are initialized using a Gaussian distribution with a mean of 0 and a standard deviation of 0.02. We use the Adam optimizer with the initial learning rate 0.01, which will decay by 0.1 after every 3 epoch. The L2 penalty is set 10\u22125 and dropout ratio is 0.2. We select other hyper-parameters using a validation set which is a random 10% subset of training data."
        },
        {
            "heading": "5.4 Overall Comparison(RQ1)",
            "text": "The performance comparison is summarized in Table 2. It can be seen that TRASA can achieve the best performance across all three datasets in terms of the two evaluation metrics, which demonstrates the effectiveness of TRASA.\nFor traditional methods, FPMC performs poorly, indicating that the assumption on the independence of successive items in Markov Chain-based methods is not sufficient. ItemKNN performs better than FPMC, which shows that considering the similarity between items does make a contribution to prediction. However, traditional methods are not competitive enough compared to other methods.\nFor deep learning-based methods, GRU4Rec performs worst. It simply stacks multiple GRU layers to make prediction. However, its performance improvement is still demonstrated on two datasets compared with traditional methods, which indicates that the deep learning-based methods have more powerful representation capabilities. NARM performs better than GRU4Rec. NARM combines both the sequential behavior and the main purpose to constitute users\u2019 preferences. It proves that simply applying RNN in session-based recommendation is insufficient. STAMP gets the best results in all deep learning-based methods by explicitly taking users\u2019 general and current interests into account.\nAs the first GNN-based method, SR-GNN outperforms all deep learning-based methods. To capture the long-range dependencies, GC-SAN applies self-attention layers after the GNN layers and LESSR builds a short-cut graph to connect all items directly. Both GC-SAN and LESSR can achieve better performance than SR-GNN, proving that the long-range dependency can indeed affect the performance. GCE-GNN is the best among all the GNN-based methods, which indicates that considering two levels of item embeddings from session graph and global graph are beneficial for prediction.\nIn contrast, TRASA outperforms the other methods consistently on all three datasets. It is because that we use selfattention mechanism to capture the long-range dependencies. Furthermore, to keep the item transition relations in the session, we employ the output of the relation encoder as supplementary information when computing the attention scores."
        },
        {
            "heading": "5.5 Deeper Model Analysis (RQ2)",
            "text": "We conduct further analysis of TRASA to find out what exactly contributes to the performance improvement. Impact of each component. We conduct experiments to find out the effect of each component in TRASA.\n\u2022 WO-POS: without the position embedding. \u2022 WO-REL-POS: without the relation encoder and posi-\ntion embedding. \u2022 WO-SAN: without the self-attention layers.\nFrom Table 3, we can observe that self-attention layers play a very important role in TRASA. The model performance drops dramatically after removing self-attention layers, showing that it is necessary to learn the connections between different items. By comparing WO-POS and WO-REL-POS, we can conclude that encoding the transition relations explicitly is very effective in session-based recommendation. In addition, the position embedding also makes a contribution to improve the model performance and utilizing the relation encoder and position embedding simultaneously can achieve a better model performance. Impact of different methods to represent sessions. We explore different session representation methods.\n\u2022 SAN: directly using the self-attention function to get a session representation.\n\u2022 SUM: using the sum of item embeddings as a session representation.\n\u2022 GRAPH: getting a session representation without converting the graph to its original sequence.\nAs shown in Table 4, in the Diginetica dataset, SAN performs poorly compared to other methods. GRAPH has the closest performance to our method and SUM has a comparable performance as well. In the Gowalla and Last.fm datasets, both SAN and SUM have a comparable performance to our method and they perform better than GRAPH. In addition, we find that if we do not revert the graph to its original sequence, MRR@20 will drop. This phenomenon is consistent across all three datasets. To sum up, different session representation methods have different performance on different datasets but our method always has the best performance which demonstrates that aggregating the item embeddings based on user\u2019s current interest is an effective way to represent the session."
        },
        {
            "heading": "5.6 Hyper-parameter study (RQ3)",
            "text": "In this section, we explore the effect of key hyperparameters in TRASA. We use the Diginetica and Gowalla datasets to study the effect of the embedding size and number of selfattention layers. The results are shown in Figure 3.\nImpact of the embedding size. When the embedding size is small, TRASA does not perform well because embeddings with a small dimension cannot adequately encode all item characteristics. As the embedding size becomes larger, the performance improves accordingly. But when the embedding size gets too large (over 64 in our experimental setup), the performance drops, which indicates that larger embedding size is not always better due to the overfitting problem.\nImpact of the number of self-attention layers. The models with different embedding sizes perform differently as the number of self-attention layers increases. When the embedding size is small, the model performance will keep growing as the number of layers increases. When the embedding size becomes larger, the model performance first increases and then decreases. In addition, with a larger embedding size, the model performance drops more and faster. It is probably because with a larger embedding size, the model representation capability is more powerful, and it becomes more prone to overfitting."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we proposed a novel method called TRASA for session-based recommendation. To the best of our knowledge, TRASA is the first to introduce the self-attention mechanism for item representation learning. Specifically, TRASA first converts the session into a directed graph. To model the item transition relations explicitly, TRASA encodes the shortest path between items as their transition relations. To capture the long-range dependencies, TRASA utilizes self-attention to make direct connections between any two items. Meanwhile, the transition relations are incorporated when computing the attention scores. Extensive experiments on three realworld datasets demonstrate that TRASA outperforms the existing state-of-the-art methods."
        }
    ],
    "title": "Transition Relation Aware Self-Attention for Session-based Recommendation",
    "year": 2022
}