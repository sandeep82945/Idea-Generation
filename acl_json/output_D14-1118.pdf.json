{
    "abstractText": "Estimating questions\u2019 difficulty levels is an important task in community question answering (CQA) services. Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads. However, they suffer from data sparseness problem as each question only gets a limited number of comparisons. Moreover, they cannot handle newly posted questions which get no comparisons. In this paper, we propose a novel question difficulty estimation approach called Regularized Competition Model (RCM), which naturally combines question-user comparisons and questions\u2019 textual descriptions into a unified framework. By incorporating textual information, RCM can effectively deal with data sparseness problem. We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions, again by leveraging textual similarities. Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions, RCM performs the estimation task significantly better than existing methods, demonstrating the advantage of incorporating textual information. More interestingly, we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words.",
    "authors": [
        {
            "affiliations": [],
            "name": "Quan Wang"
        },
        {
            "affiliations": [],
            "name": "Jing Liu"
        },
        {
            "affiliations": [],
            "name": "Bin Wang"
        },
        {
            "affiliations": [],
            "name": "Li Guo"
        }
    ],
    "id": "SP:97d51fb436aad709fee5733156e704eaf86f1a96",
    "references": [
        {
            "authors": [
                "Mark S. Ackerman",
                "David W. McDonald."
            ],
            "title": "Answer garden 2: merging organizational memory with collaborative help",
            "venue": "Proceedings of the 1996 ACM Conference on Computer Supported Cooperative Work, pages 97\u2013105.",
            "year": 1996
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi."
            ],
            "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
            "venue": "Advances in Neural Information Processing Systems, pages 585\u2013591.",
            "year": 2001
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi",
                "Vikas Sindhwani."
            ],
            "title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples",
            "venue": "Journal of Machine Learning Research, 7:2399\u20132434.",
            "year": 2006
        },
        {
            "authors": [
                "Stephen Boyd",
                "Lin Xiao",
                "Almir Mutapcic."
            ],
            "title": "Subgradient methods",
            "venue": "Lecture Notes of EE392o, Stanford University.",
            "year": 2003
        },
        {
            "authors": [
                "Deng Cai",
                "Xiaofei He",
                "Xiaoyun Wu",
                "Jiawei Han."
            ],
            "title": "Non-negative matrix factorization on manifold",
            "venue": "Proceedings of the 8th IEEE International Conference on Data Mining, pages 63\u201372.",
            "year": 2008
        },
        {
            "authors": [
                "David Carmel",
                "Elad Yom-Tov",
                "Adam Darlow",
                "Dan Pelleg"
            ],
            "title": "What makes a query difficult",
            "venue": "In Proceedings of the 29th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2006
        },
        {
            "authors": [
                "Olivier Chapelle."
            ],
            "title": "Training a support vector machine in the primal",
            "venue": "Neural Computation, 19(5):1155\u20131178.",
            "year": 2007
        },
        {
            "authors": [
                "Xi Chen",
                "Paul N. Bennett",
                "Kevyn Collins-Thompson",
                "Eric Horvitz."
            ],
            "title": "Pairwise ranking aggregation in a crowdsourced setting",
            "venue": "Proceedings of the 6th ACM International Conference on Web Search and Data Mining, pages 193\u2013202.",
            "year": 2013
        },
        {
            "authors": [
                "Fan RK. Chung"
            ],
            "title": "Spectral Graph Theory, volume 92",
            "year": 1997
        },
        {
            "authors": [
                "Kenneth Church."
            ],
            "title": "How many multiword expressions do people know",
            "venue": "Proceedings of the ACLHLT Workshop on Multiword Expressions: from Parsing and Generation to the Real World, pages 137\u2013144.",
            "year": 2011
        },
        {
            "authors": [
                "Mark Claypool",
                "Anuja Gokhale",
                "Tim Miranda",
                "Pavel Murnikov",
                "Dmitry Netes",
                "Matthew Sartin."
            ],
            "title": "Combining content-based and collaborative filters in an online newspaper",
            "venue": "Proceedings of the ACM SIGIR workshop on Recommender Systems.",
            "year": 1999
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and Psychological Measurement, 20(1):37\u201346.",
            "year": 1960
        },
        {
            "authors": [
                "Kevyn Collins-Thompson",
                "James P. Callan."
            ],
            "title": "A language modeling approach to predicting reading difficulty",
            "venue": "Proceedings of the 2004 Conference of",
            "year": 2004
        },
        {
            "authors": [
                "Kevyn Collins-Thompson",
                "Paul N Bennett",
                "Ryen W White",
                "Sebastian de la Chica",
                "David Sontag."
            ],
            "title": "Personalizing web search results by reading level",
            "venue": "Proceedings of the 20th ACM International Conference on Information and Knowledge Man-",
            "year": 2011
        },
        {
            "authors": [
                "Thomas Cover",
                "Peter Hart."
            ],
            "title": "Nearest neighbor pattern classification",
            "venue": "IEEE Transactions on Information Theory, 13(1):21\u201327.",
            "year": 1967
        },
        {
            "authors": [
                "Zhicheng Dou",
                "Ruihua Song",
                "Ji Rong Wen."
            ],
            "title": "A large-scale evaluation and analysis of personalized search strategies",
            "venue": "Proceedings of the 16th International Conference on World Wide Web, pages 581\u2013590.",
            "year": 2007
        },
        {
            "authors": [
                "Ralf Herbrich",
                "Tom Minka",
                "Thore Graepel."
            ],
            "title": "Trueskill: a bayesian skill rating system",
            "venue": "Advances in Neural Information Processing Systems, pages 569\u2013576.",
            "year": 2006
        },
        {
            "authors": [
                "Anna Huang."
            ],
            "title": "Similarity measures for text document clustering",
            "venue": "Proceedings of the 6th New Zealand Computer Science Research Student Conference, pages 49\u201356.",
            "year": 2008
        },
        {
            "authors": [
                "Rense Lange",
                "Juan Moran",
                "Warren R. Greiff",
                "Lisa Ferro."
            ],
            "title": "A probabilistic rasch analysis of question answering evaluations",
            "venue": "Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2004
        },
        {
            "authors": [
                "Baichuan Li",
                "Irwin King."
            ],
            "title": "Routing questions to appropriate answerers in community question answering services",
            "venue": "Proceedings of the 19th ACM International Conference on Information and Knowledge Management, pages 1585\u20131588.",
            "year": 2010
        },
        {
            "authors": [
                "Jing Liu",
                "Young-In Song",
                "Chin-Yew Lin."
            ],
            "title": "Competition-based user expertise score estimation",
            "venue": "Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 425\u2013434.",
            "year": 2011
        },
        {
            "authors": [
                "Jing Liu",
                "Quan Wang",
                "Chin-Yew Lin",
                "Hsiao-Wuen Hon."
            ],
            "title": "Question difficulty estimation in community question answering services",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85\u201390.",
            "year": 2013
        },
        {
            "authors": [
                "Kevin Kyung Nam",
                "Mark S. Ackerman",
                "Lada A. Adamic."
            ],
            "title": "Questions in, knowledge in?: a study of naver\u2019s question answering community",
            "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 779\u2013788.",
            "year": 2009
        },
        {
            "authors": [
                "Larry Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd."
            ],
            "title": "The pagerank citation ranking: bringing order to the web",
            "venue": "Technical Report, Stanford University.",
            "year": 1999
        },
        {
            "authors": [
                "Joseph Lee Rodgers",
                "W. Alan Nicewander."
            ],
            "title": "Thirteen ways to look at the correlation coefficient",
            "venue": "The American Statistician, 42(1):59\u201366.",
            "year": 1988
        },
        {
            "authors": [
                "Gerard Salton",
                "Christopher Buckley."
            ],
            "title": "Termweighting approaches in automatic text retrieval",
            "venue": "Information Processing & Management, 24(5):513\u2013 523.",
            "year": 1988
        },
        {
            "authors": [
                "Andrew I. Schein",
                "Alexandrin Popescul",
                "Lyle H. Ungar",
                "David M. Pennock."
            ],
            "title": "Methods and metrics for cold-start recommendations",
            "venue": "Proceedings of the 25th International ACM SIGIR Conference on Research and Development in Information",
            "year": 2002
        },
        {
            "authors": [
                "Sarah E. Schwarm",
                "Mari Ostendorf."
            ],
            "title": "Reading level assessment using support vector machines and statistical language models",
            "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523\u2013530.",
            "year": 2005
        },
        {
            "authors": [
                "Kazunari Sugiyama",
                "Kenji Hatano",
                "Masatoshi Yoshikawa."
            ],
            "title": "Adaptive web search based on user profile constructed without any effort from users",
            "venue": "Proceedings of the 13th International Conference on World Wide Web, pages 675\u2013684.",
            "year": 2004
        },
        {
            "authors": [
                "Peter Welinder",
                "Steve Branson",
                "Serge Belongie",
                "Pietro Perona."
            ],
            "title": "The multidimensional wisdom of crowds",
            "venue": "Advances in Neural Information Processing Systems, pages 2424\u20132432.",
            "year": 2010
        },
        {
            "authors": [
                "Jacob Whitehill",
                "Paul Ruvolo",
                "Tingfan Wu",
                "Jacob Bergsma",
                "Javier R Movellan."
            ],
            "title": "Whose vote should count more: optimal integration of labels from labelers of unknown expertise",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Jiang Yang",
                "Lada Adamic",
                "Mark Ackerman."
            ],
            "title": "Competing to share expertise: the taskcn knowledge sharing community",
            "venue": "Proceedings of the 2nd International AAAI Conference on Weblogs and Social Media.",
            "year": 2008
        },
        {
            "authors": [
                "Elad Yom-Tov",
                "Shai Fine",
                "David Carmel",
                "Adam Darlow."
            ],
            "title": "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval",
            "venue": "Proceedings of the 28th International ACM SIGIR Confer-",
            "year": 2005
        },
        {
            "authors": [
                "Dengyong Zhou",
                "Olivier Bousquet",
                "Thomas Navin Lal",
                "Jason Weston",
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "Learning with local and global consistency",
            "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.",
            "year": 2004
        },
        {
            "authors": [
                "Yanhong Zhou",
                "Gao Cong",
                "Bin Cui",
                "Christian S. Jensen",
                "Junjie Yao."
            ],
            "title": "Routing questions to the right users in online communities",
            "venue": "Proceedings of the 25th IEEE International Conference on Data Engineering, pages 700\u2013711.",
            "year": 2009
        },
        {
            "authors": [
                "Dengyong Zhou",
                "John C Platt",
                "Sumit Basu",
                "Yi Mao."
            ],
            "title": "Learning from the wisdom of crowds by minimax entropy",
            "venue": "Advances in Neural Information Processing Systems, pages 2204\u20132212.",
            "year": 2012
        },
        {
            "authors": [
                "Xiaojin Zhu",
                "John Lafferty."
            ],
            "title": "Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning",
            "venue": "Proceedings of the 22nd International Conference on Machine Learning, pages 1052\u2013",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115\u20131126, October 25-29, 2014, Doha, Qatar. c\u00a92014 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Recent years have seen rapid growth in community question answering (CQA) services. They have been widely used in various scenarios, including general information seeking on the web1, knowl-\n1http://answers.yahoo.com/\nedge exchange in professional communities2, and question answering in massive open online courses (MOOCs)3, to name a few.\nAn important research problem in CQA is how to automatically estimate the difficulty levels of questions, i.e., question difficulty estimation (QDE). QDE can benefit many applications. Examples include 1) Question routing. Routing questions to appropriate answerers can help obtain quick and high-quality answers (Li and King, 2010; Zhou et al., 2009). Ackerman and McDonald (1996) have demonstrated that routing questions by matching question difficulty level with answerer expertise level will make better use of answerers\u2019 time and expertise. This is even more important for enterprise question answering and MOOCs question answering, where human resources are expensive. 2) Incentive mechanism design. Nam et al. (2009) have found that winning point awards offered by reputation systems is a driving factor for user participation in CQA services. Assigning higher point awards to more difficult questions will significantly improve user participation and satisfaction. 3) Linguistics analysis. Researchers in computational linguistics are always interested in investigating the correlation between language and knowledge, to see how the language reflects one\u2019s knowledge (Church, 2011). As we will show in Section 5.4, QDE provides an automatic way to quantitatively measure the knowledge levels of words.\nLiu et al. (2013) have done the pioneer work on QDE, by leveraging question-user comparisons extracted from the question answering threads. Specifically, they assumed that the difficulty level of a question is higher than the expertise level of the asker (i.e. the user who asked the question), but lower than that of the best answerer (i.e. the user who provided the best answer). A TrueSkill al-\n2http://stackoverflow.com/ 3http://coursera.org/\n1115\ngorithm (Herbrich et al., 2006) was further adopted to estimate question difficulty levels as well as user expertise levels from the pairwise comparisons among them. To our knowledge, it is the only existing work on QDE. Yang et al. (2008) have proposed a similar idea, but their work focuses on a different task, i.e., estimating difficulty levels of tasks in crowdsourcing contest services.\nThere are two major drawbacks of previous methods: 1) data sparseness problem and 2) coldstart problem. By the former, we mean that under the framework of previous work, each question is compared only twice with the users (once with the asker and the other with the best answerer), which might not provide enough information and contaminate the estimation accuracy. By the latter, we mean that previous work only deals with wellresolved questions which have received the best answers, but cannot handle newly posted questions with no answers received. In many real-world applications such as question routing and incentive mechanism design, however, it is usually required that the difficulty level of a question is known instantly after it is posted.\nTo address the drawbacks, we propose further exploiting questions\u2019 textual descriptions (e.g., title, body, and tags) to perform QDE. Preliminary observations have shown that a question\u2019s difficulty level can be indicated by its textual description (Liu et al., 2013). We take advantage of the observations, and assume that if two questions are close in their textual descriptions, they will also be close in their difficulty levels, i.e., the smoothness assumption. We employ manifold regularization (Belkin et al., 2006) to characterize the assumption. Manifold regularization is a wellknown technique to preserve local invariance in manifold learning algorithms, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). Then, we propose a novel Regularized Competition Model (RCM), which formalizes QDE as minimizing a loss on questionuser comparisons with manifold regularization on questions\u2019 textual descriptions. As the smoothness assumption offers extra information for inferring question difficulty levels, incorporating it will effectively deal with data sparsity. Finally, we adopt a K-Nearest Neighbor approach (Cover and Hart, 1967) to perform cold-start estimation, again by leveraging the smoothness assumption.\nExperiments on two publicly available data sets\ncollected from Stack Overflow show that 1) RCM performs significantly better than existing methods in the QDE task for both well-resolved and cold-start questions. 2) The performance of RCM is insensitive to the particular choice of the term weighting schema (determines how a question\u2019s textual description is represented) and the similarity measure (determines how the textual similarity between two questions is measured). The results demonstrate the advantage of incorporating textual information for QDE. Qualitative analysis further reveals that RCM might provide an automatic way to quantitatively measure the knowledge levels of words.\nThe main contributions of this paper include: 1) We take fully advantage of questions\u2019 textual descriptions to address data sparseness problem and cold-start problem which previous QDE methods suffer from. To our knowledge, it is the first time that textual information is introduced in QDE. 2) We propose a novel QDE method that naturally combines question-user comparisons and questions\u2019 textual descriptions into a unified framework. The proposed method performs QDE significantly better than existing methods. 3) We demonstrate the practicability of estimating difficulty levels of cold-start questions purely based on their textual descriptions, making various applications feasible in practice. As far as we know, it is the first work that considers cold-start estimation. 4) We explore how a word\u2019s knowledge level can be automatically measured by RCM.\nThe rest of the paper is structured as follows. Section 2 describes the problem formulation and the motivation of RCM. Section 3 presents the details of RCM. Section 4 discusses cold-start estimation. Section 5 reports experiments and results. Section 6 reviews related work. Section 7 concludes the paper and discusses future work."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "A CQA service provides a platform where people can ask questions and seek answers from others. Given a CQA portal, consider a specific category where questions on the same topic are asked and answered, e.g., the \u201cC++ programming\u201d category of Stack Overflow. When an asker ua posts a question q in the category, there will be several answerers to answer the question. Among all the received answers, a best one will be chosen\nby the asker or voted by the community. The answerer who provides the best answer is called the best answerer ub. The other answerers are denoted by O = {uo1 , uo2 , \u00b7 \u00b7 \u00b7 , uoM }. A question answering thread (QA thread) is represented as a quadruplet (q, ua, ub,O). Collecting all such QA threads in the category, we get M users and N questions, denoted byU = {u1, u2, \u00b7 \u00b7 \u00b7 , uM} and Q = {q1, q2, \u00b7 \u00b7 \u00b7 , qN} respectively. Each user um is associated with an expertise score \u03b8m, representing his/her expertise level. A larger \u03b8m indicates a higher expertise level of the user. Each question qn is associated with a difficulty score \u03b2n, representing its difficulty level. A larger \u03b2n indicates a higher difficulty level of the question. Difficulty scores (as well as expertise scores) are assumed to be comparable with each other in the specified category. Besides, each question qn has a textual description, and is represented as a V-dimensional term vector dn, where V is the vocabulary size.\nThe question difficulty estimation (QDE) task aims to automatically learn the question difficulty scores (\u03b2n\u2019s) by utilizing the QA threads T = {(q, ua, ub,O) : q \u2208 Q} as well as the question descriptions D = {d1, d2, \u00b7 \u00b7 \u00b7 , dN} in the specified category. Note that in Section 2 and Section 3, we consider estimating difficulty scores of resolved questions, i.e., questions with the best answers selected or voted. Estimating difficulty scores of unresolved questions, e.g., newly posted ones, will be discussed in Section 4."
        },
        {
            "heading": "2.2 Competition-based Methods",
            "text": "Liu et al. (2013) have proposed a competitionbased method for QDE. The key idea is to 1) extract pairwise competitions from the QA threads and 2) estimate question difficulty scores based on extracted competitions.\nTo extract pairwise competitions, it is assumed that question difficulty scores and user expertise scores are expressed on the same scale. Given a QA thread (q, ua, ub,O), it is further assumed that: Assumption 1 (pairwise comparison assumption) The difficulty score of question q is higher than the expertise score of the asker ua, but lower than that of the best answerer ub. Moreover, the expertise score of the best answerer ub is higher than that of the asker ua, as well as any answerer in O.4\n4The difficulty score of question q is not assumed to be lower than the expertise score of any answerer in O, since such a user may just happen to see the question and respond to it, rather than knowing the answer well.\nGiven the assumption, there are (|O| + 3) pairwise competitions extracted from the QA thread, including 1) one competition between the question q and the asker ua, 2) one competition between the question q and the best answerer ub, 3) one competition between the best answerer ub and the asker ua, and 4) |O| competitions between the best answerer ub and each of the answerers in O. The question q is the winner of the first competition, and the best answerer ub is the winner of the remaining (|O| + 2) competitions. These pairwise competitions are denoted by\nCq = {ua\u227aq, q\u227aub, ua\u227aub, uo1 \u227aub, \u00b7 \u00b7 \u00b7 , uoM \u227aub} , where i \u227a j means that competitor j beats competitor i in a competition. Let\nC = \u222a q\u2208Q Cq (1)\nbe the set containing all the pairwise competitions extracted from T .\nGiven the competition set C, Liu et al. (2013) further adopted a TrueSkill algorithm (Herbrich et al., 2006) to learn the competitors\u2019 skill levels (i.e. the question difficulty scores and the user expertise scores). TrueSkill assumes that the practical skill level of each competitor follows a normal distribution N ( \u00b5, \u03c32 ) , where \u00b5 is the average skill level and \u03c3 is the estimation uncertainty. Then it updates the estimations in an online mode: for a newly observed competition with its win-loss result, 1) increase the average skill level of the winner, 2) decrease the average skill level of the loser, and 3) shrink the uncertainties of both competitors as more data has been observed. Yang et al. (2008) have proposed a similar competitionbased method to estimate tasks\u2019 difficulty levels in crowdsourcing contest services, by leveraging PageRank (Page et al., 1999) algorithm."
        },
        {
            "heading": "2.3 Motivating Discussions",
            "text": "The methods introduced above estimate competitors\u2019 skill levels based solely on the pairwise competitions among them. The more competitions a competitor participates in, the more accurate the estimation will be. However, according to the pairwise comparison assumption (Assumption 1), each question participates in only two competitions, one with the asker and the other with the best answerer. Hence, there might be no enough information to accurately infer its difficulty score. We call this the data sparseness problem.\nTaking advantage of additional metadata has been demonstrated to be an effective way of dealing with data sparsity in various applications such as collaborative filtering (Claypool et al., 1999; Schein et al., 2002) and personalized search (Dou et al., 2007; Sugiyama et al., 2004). The rationale behind is to bridge the gap among users/items by leveraging their similarities based on the metadata. As for QDE, preliminary observations have shown that a question\u2019s difficulty level can be indicated by its textual description (Liu et al., 2013). As an example, consider the QA threads in the \u201cmathematics\u201d category of Stack Overflow. Divide the questions into three groups: 1) low difficulty, 2) medium difficulty, and 3) high difficulty, according to their difficulty scores estimated by TrueSkill. Figure 1 visualizes the frequency distribution of tags in each group, where the size of each tag is in proportion to its frequency in the group. The results indicate that the tags associated with the questions do have the ability to reflect the questions\u2019 difficulty levels, e.g., low difficulty questions usually have tags such as \u201chomework\u201d and \u201ccalculus\u201d, while high difficulty ones usually have tags such as \u201cgeneral topology\u201d and \u201cnumber theory\u201d. We further calculate the Pearson correlation coefficient (Rodgers and Nicewander, 1988) between 1) the gap between the averaged difficulty scores in each two groups and 2) the Euclidean distance between the aggregated textual descriptions in each two groups . The result is r = 0.6424, implying that the difficulty gap is positively correlated with the textual distance. In other words, the more similar two questions\u2019 textual descriptions are, the more close their difficulty levels are. Therefore, we take the textual information to bridge the difficulty gap among questions, by assuming that\nAssumption 2 (smoothness assumption) If two questions qi and q j are close in their textual de-\nscriptions di and d j, they will also be close in their difficulty scores \u03b2i and \u03b2 j.\nThe smoothness assumption brings us additional information about question difficulty scores by inferring textual similarities. It serves as a supplement to the pairwise competitions, and might help address the data sparseness problem which previous methods suffer from."
        },
        {
            "heading": "3 Modeling Text Similarities for QDE",
            "text": "This section presents a novel Regularized Competition Model (RCM) for QDE, which combines the pairwise competitions and the textual descriptions into a unified framework. RCM can alleviate the data sparseness problem and perform more accurate estimation."
        },
        {
            "heading": "3.1 Regularized Competition Model",
            "text": "We start with several notations. As question difficulty scores can be directly compared with user expertise scores, we take questions as pseudo users. Let \u03b8\u0304 \u2208 RM+N denote the skill levels (i.e. the expertise scores and the difficulty scores) of all the (pseudo) users:\n\u03b8\u0304i = { \u03b8i, 1 \u2264 i \u2264 M, \u03b2i\u2212M , M < i \u2264 M + N,\nwhere \u03b8\u0304i is the i-th entry of \u03b8\u0304. The first M entries are the user expertise scores, denoted by \u03b8\u0304u \u2208 RM . The last N entries are the question difficulty scores, denoted by \u03b8\u0304q \u2208 RN . Let \u03b8\u0304(u)i and \u03b8\u0304(q)i denote the i-th entries of \u03b8\u0304u and \u03b8\u0304q respectively.\nExploiting Pairwise Competitions. We define a loss on each pairwise competition i \u227a j:\n\u2113 ( \u03b8\u0304i, \u03b8\u0304 j ) = max ( 0, \u03b4 \u2212 ( \u03b8\u0304 j \u2212 \u03b8\u0304i ))p , (2)\nwhere p is either 1 or 2. The loss is defined on the skill gap between the two competitors, i.e., \u03b8\u0304 j \u2212 \u03b8\u0304i,\nmeasuring the inconsistency between the expected outcome and the actual outcome. If the gap is larger than a predefined threshold \u03b4, competitor j would probably beat competitor i in the competition, which coincides with the actual outcome. Then the loss will be zero. Otherwise, there is a higher chance that competitor j loses the competition, which goes against the actual outcome. Then the loss will be greater than zero. The smaller the gap is, the higher the chance of inconsistency becomes, and the greater the loss will be. Note that the threshold \u03b4 can take any positive value since we do not pose a norm constraint on \u03b8\u0304.5 Without loss of generality we take \u03b4 = 1 throughout this paper. As we will show in Section 3.2, the loss defined in Eq. (2) has some similarity with the SVM loss (Chapelle, 2007). We name it hinge loss when p = 1, and quadratic loss when p = 2.\nGiven the competition set C, estimating skill levels of (pseudo) users then amounts to solving the following optimization problem:\nmin \u03b8\u0304 \u2211 (i\u227a j)\u2208C \u2113 ( \u03b8\u0304i, \u03b8\u0304 j ) + \u03bb1 2 \u03b8\u0304 T \u03b8\u0304, (3)\nwhere the first term is the empirical loss measuring the total inconsistency; the second term is a regularizer to prevent overfitting; and \u03bb1 \u2265 0 is a trade-off coefficient. It is also a competition-based QDE method, called Competition Model (CM).\nExploiting Question Descriptions. Manifold regularization is a well-known technique used in manifold learning algorithms to preserve local invariance, i.e., nearby points are likely to have similar embeddings (Belkin and Niyogi, 2001). In QDE, the smoothness assumption expresses similar \u201cinvariance\u201d, i.e., nearby questions (in terms of textual similarities) are likely to have similar difficulty scores. Hence, we characterize the assumption with the following manifold regularizer:\nR = 1 2 N\u2211 i=1 N\u2211 j=1 ( \u03b8\u0304 (q) i \u2212 \u03b8\u0304(q)j )2 wi j\n= \u03b8\u0304 T q D\u03b8\u0304q \u2212 \u03b8\u0304Tq W\u03b8\u0304q = \u03b8\u0304Tq L\u03b8\u0304q, (4)\nwhere wi j is the textual similarity between question i and question j; W \u2208 RN\u00d7N is the similarity matrix with the (i, j)-th entry being wi j; D \u2208 RN\u00d7N is a diagonal matrix with the i-th entry on the diagonal being dii = \u2211N j=1 wi j; and L = D\u2212W \u2208 RN\u00d7N\n5Given any \u03b8\u0304i, \u03b8\u0304 j, and \u03b4, there always exists a linear transformation which keeps the sign of ( \u03b4 \u2212 ( \u03b8\u0304 j \u2212 \u03b8\u0304i )) unchanged.\nis the graph Laplacian (Chung, 1997). Minimizing R results in the smoothness assumption: for any questions i and j, if their textual similarity wi j is\nhigh, the difficulty gap ( \u03b8\u0304\n(q) i \u2212 \u03b8\u0304(q)j\n)2 will be small.\nA Hybrid Method. Combining Eq. (3) and Eq. (4), we obtain RCM, which amounts to the following optimization problem:\nmin \u03b8\u0304 \u2211 (i\u227a j)\u2208C \u2113 ( \u03b8\u0304i, \u03b8\u0304 j ) + \u03bb1 2 \u03b8\u0304 T \u03b8\u0304 + \u03bb2 2 \u03b8\u0304 T q L\u03b8\u0304q. (5)\nHere \u03bb2 \u2265 0 is also a trade-off coefficient. The advantages of RCM include 1) It naturally formalizes QDE as minimizing a manifold regularized loss function, which seamlessly integrates both the pairwise competitions and the textual descriptions. 2) By incorporating textual information, it can address the data sparseness problem which previous methods suffer from, and perform significantly better in the QDE task."
        },
        {
            "heading": "3.2 Learning Algorithm",
            "text": "Redefine the k-th pairwise competition (assumed to be carried out between competitors i and j) as (xk, yk). xk \u2208 RM+N indicates the competitors:\nx(k)i = 1, x (k) j = \u22121, and x(k)l = 0 for any l , i, j,\nwhere x(k)l is the l-th entry of xk. yk \u2208 {1,\u22121} is the outcome: if competitor i beats competitor j, yk = 1; otherwise, yk = \u22121. The objective in Eq. (5) can then be rewritten as\nL ( \u03b8\u0304 ) = |C|\u2211 k=1 max ( 0, 1 \u2212 yk ( \u03b8\u0304 T xk ))p + 1 2 \u03b8\u0304 T Z\u03b8\u0304,\nwhere Z = ( \u03bb1IM 0\n0 \u03bb1IN + \u03bb2L\n) is a block matrix; IM \u2208\nRM\u00d7M and IN \u2208 RN\u00d7N are identity matrices; p = 1 corresponds to the hinge loss, and p = 2 the quadratic loss. It is clear that the loss defined in Eq. (2) has the same format as the SVM loss.\nThe objectiveL is differentiable for the quadratic loss but non-differentiable for the hinge loss. We employ a subgradient method (Boyd et al., 2003) to solve the optimization problem. The algorithm starts at a point \u03b8\u03040 and, as many iterations as needed, moves from \u03b8\u0304t to \u03b8\u0304t+1 in the direction of the negative subgradient:\n\u03b8\u0304t+1 = \u03b8\u0304t \u2212 \u03b3t\u2207L ( \u03b8\u0304t ) ,\nAlgorithm 1 Regularized Competition Model Require: competition set C and description setD 1: \u03b8\u03040 \u2190 1 2: for t = 0 : T \u2212 1 do 3: Kt \u2190 { k : 1 \u2212 yk ( \u03b8\u0304 T t xk ) > 0\n} 4: \u2207L ( \u03b8\u0304t ) \u2190 calculated by Eq. (6)\n5: \u03b8\u0304t+1 \u2190 \u03b8\u0304t \u2212 \u03b3t\u2207L ( \u03b8\u0304t )\n6: \u0398t+1 \u2190 { \u03b8\u03040, \u03b8\u03041, \u00b7 \u00b7 \u00b7 , \u03b8\u0304t+1 } 7: \u03b8\u0304t+1 \u2190 arg min\u03b8\u0304\u2208\u0398t+1 L ( \u03b8\u0304 ) 8: end for 9: return \u03b8\u0304T\nwhere \u03b3t > 0 is the learning rate. The subgradient is calculated as\n\u2207L ( \u03b8\u0304t ) =  Z\u03b8\u0304t \u2212 \u2211 k\u2208Kt yk xk, p=1, Z\u03b8\u0304t + 2 \u2211\nk\u2208Kt xk xTk \u03b8\u0304t \u2212 2 \u2211 k\u2208Kt yk xk, p=2, (6)\nwhere Kt = { k : 1 \u2212 yk ( \u03b8\u0304 T t xk ) > 0 } . As it is not always a descent method, we keep track of the best point found so far (Boyd et al., 2003):\n\u03b8\u0304t+1 = arg min \u03b8\u0304\u2208\u0398t+1\nL ( \u03b8\u0304 ) ,\nwhere\u0398t+1 = { \u03b8\u03040, \u03b8\u03041, \u00b7 \u00b7 \u00b7 , \u03b8\u0304t+1 } . The whole procedure is summarized in Algorithm 1. Convergence. For constant learning rate (i.e., \u03b3t = \u03b3), Algorithm 1 is guaranteed to converge to within some range of the optimal value, i.e.,\nlim t\u2192\u221eL\n( \u03b8\u0304t ) \u2212 L\u2217 < \u03f5,\nwhere L\u2217 denotes the minimum of L(\u00b7), and \u03f5 is a constant defined by the learning rate \u03b3. For more details, please refer to (Boyd et al., 2003). During our experiments, we set the iteration number as T = 1000 and the learning rate as \u03b3t = 0.001, and convergence was observed.\nComplexity. For both the hinge loss and the quadratic loss, the time complexity (per iteration) and the space complexity of RCM are both O ( |C| + \u03b7N2 ) . Here, |C| is the total number of competitions, M and N are the numbers of users and questions respectively, and \u03b7 is the ratio of non-zero entries in the graph Laplacian L.6 In the analysis, we have assumed that M \u226a \u03b7N2 and N \u226a \u03b7N2.\n6Owing to the sparse nature of questions\u2019 textual descriptions, the graph Laplacian L is usually sparse, with about 70% entries being zero according to our experiments."
        },
        {
            "heading": "4 Cold-Start Estimation",
            "text": "Previous sections discussed estimating difficulty scores of resolved questions, from which pairwise competitions could be extracted. However, for newly posted questions without any answers received, no competitions could be extracted and none of the above methods work. We call it the cold-start problem.\nWe heuristically apply a K-Nearest Neighbor (KNN) approach (Cover and Hart, 1967) to coldstart estimation, again by leveraging the smoothness assumption. The key idea is to propagate difficulty scores from well-resolved questions to cold-start ones according to their textual similarities. Specifically, suppose that there exists a set of well-resolved questions whose difficulty scores have already been estimated by a QDE method. Given a cold-start question q\u2217, we first pick K well-resolved questions that are closest to q\u2217 in textual descriptions, referred to as the nearest neighbors. The difficulty score of question q\u2217 is then predicted as the averaged difficulty scores of its nearest neighbors. The KNN method bridges the gap between cold-start and well-resolved questions by inferring their textual similarities, and might effectively deal with the cold-start problem."
        },
        {
            "heading": "5 Experiments",
            "text": "We have conducted experiments to test the effectiveness of RCM in estimating difficulty scores of both well-resolved and cold-start questions. Moreover, we have explored how a word\u2019s difficulty level can be quantitatively measured by RCM."
        },
        {
            "heading": "5.1 Experimental Settings",
            "text": "Data Sets. We obtained a publicly available data set of Stack Overflow between July 31, 2008 and August 1, 20127, containing QA threads in various categories. We considered the categories of \u201cC++ programming\u201d and \u201cmathematics\u201d, and randomly sampled about 10,000 QA threads from each category, denoted by SO/CPP and SO/Math respectively. For each question, we took the title and body fields as its textual description. For both data sets, stop words in a standard list8 and words whose total frequencies are less than 10 were removed. Table 1 gives the statistics of the data sets.\n7http://blog.stackoverflow.com/category/cc-wiki-dump/ 8http://jmlr.org/papers/volume5/lewis04a/a11-smart-\nstop-list/english.stop\nFor evaluation, we randomly sampled 600 question pairs from each data set, and asked annotators to compare the difficulty levels of the questions in each pair. We had two graduate students majoring in computer science annotate the SO/CPP questions, and two majoring in mathematics annotate the SO/Math questions. For each question, only the title, body, and tags were exposed to the annotators. Given a question pair (q1, q2), the annotators were asked to give one of the three labels: q1 \u227b q2, q2 \u227b q1, or q1 = q2, which respectively means that question q1 has a higher, lower, or equal difficulty level compared with question q2. We used Cohen\u2019s kappa coefficient (Cohen, 1960) to measure the inter-annotator agreement. The result is \u03ba = 0.7533 on SO/CPP and \u03ba = 0.8017 on SO/Math, indicating that the inter-annotator agreement is quite substantial on both data sets. After removing the question pairs with inconsistent labels, we got 521 annotated SO/CPP question pairs and 539 annotated SO/Math question pairs.\nWe further randomly split the annotated question pairs into development/test/cold-start sets, with the ratio of 2:2:1. The first two sets were used to evaluate the methods in estimating difficulty scores of resolved questions. Specifically, the development set was used for parameter tuning and the test set was used for evaluation. The last set was used to evaluate the methods in cold-start estimation, and the questions in this set were excluded from the learning process of RCM as well as any baseline method.\nBaseline Methods. We considered three baseline methods: PageRank (PR), TrueSkill (TS), and CM, which are based solely on the pairwise competitions.\n\u2022 PR first constructs a competitor graph, by creating an edge from competitor i to competitor j if j beats i in a competition. A PageRank algorithm (Page et al., 1999) is then utilized to estimate the relative importance of the nodes, i.e., question difficulty scores and user expertise scores. The damping factor was set from 0.1 to 0.9 in steps of 0.1.\n\u2022 TS has been applied to QDE by Liu et al.\n(2013). We set the model parameters in the same way as they suggested.\n\u2022 CM performs QDE by solving Eq. (3). We set \u03bb1 in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.\nWe compared RCM with the above baseline methods. In RCM, both parameters \u03bb1 and \u03bb2 were set in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.\nEvaluation Metric. We employed accuracy (ACC) as the evaluation metric:\nACC = # correctly judged question pairs\n# all question pairs .\nA question pair is regarded as correctly judged if the relative difficulty ranking given by an estimation method is consistent with that given by the annotators. The higher the accuracy is, the better a method performs."
        },
        {
            "heading": "5.2 Estimation for Resolved Questions",
            "text": "The first experiment tested the methods in estimating difficulty scores of resolved questions.\nEstimation Accuracies. We first compared the estimation accuracies of PR, TS, CM, and RCM on the test sets of SO/CPP and SO/Math, obtained with the best parameter settings determined by the development sets. Table 2 gives the results, where \u201cH\u201d denotes the hinge loss and \u201cQ\u201d the quadratic loss. In RCM, to calculate the graph Laplacian L, we adopted Boolean term weighting schema and took Jaccard coefficient as the similarity measure. From the results, we can see that 1) RCM performs significantly better than the baseline methods on both data sets (t-test, p-value < 0.05), demonstrating the advantage of exploiting questions\u2019 textual descriptions for QDE. 2) The improvements of RCM over the baseline methods on SO/Math are greater than those on SO/CPP, indicating that the textual descriptions of the SO/Math questions are more powerful in reflecting their difficulty levels. The reason is that the SO/Math questions are much more heterogeneous, belonging to various subfields of mathematics. The difficulty gaps among different subfields are sometimes obvious (e.g., a question in topology in general has a higher difficulty level than a question in linear algebra), making the textual descriptions more powerful in distinguishing the difficulty levels.\nGraph Laplacian Variants. We further investigated the performances of different term weighting schemas and similarity measures in the graph\nLaplacian. The term weighting schema determines how a question\u2019s textual description is represented. We explored a Boolean schema, three TF schemas, and three TFIDF schemas (Salton and Buckley, 1988). The similarity measure determines how the textual similarity between two questions is calculated. We explored the Cosine similarity and the Jaccard coefficient (Huang, 2008). Detailed descriptions are given in Table 3.\nFigure 2 and Figure 3 show the estimation accuracies of the RCM variants on the test sets of SO/CPP and SO/Math respectively, again obtained with the best parameter settings determined by the development sets. The performance of CM is also given (the straight lines in the figures).9 From the results, we can see that 1) All the RCM variants can improve over CM on both data sets, and most of the improvements are significant (ttest, p-value < 0.05). This further demonstrates that the effectiveness of incorporating textual descriptions is not affected by the particular choice of the term weighting schema or similarity measure. 2) Boolean term weighting schema performs the best, considering different similarity measures, loss types, and data sets collectively. 3) Jaccard\n9CM performs better than PR and TS on both data sets.\ncoefficient performs as well as Cosine similarity on SO/Math, but almost consistently better on SO/CPP. Throughout the experiments, we adopted Boolean term weighting schema and Jaccard coefficient to calculate the graph Laplacian."
        },
        {
            "heading": "5.3 Estimation for Cold-Start Questions",
            "text": "The second experiment tested the methods in estimating difficulty scores of cold-start questions. We employed Boolean term weighting schema to represent a cold-start question, and utilized Jaccard Coefficient to select its nearest neighbors.\nFigure 4 and Figure 5 list the cold-start estimation accuracies of different methods on SO/CPP and SO/Math respectively, with different K values (the number of nearest neighbors). As the accuracy oscillates drastically with a K value smaller than 11 on SO/CPP and smaller than 6 on SO/Math, we report the results with K \u2208 [11, 20] on SO/CPP and K \u2208 [6, 15] on SO/Math. The averaged (over different K values) cold-start estimation accuracies are further given in Table 4. All the results are reported on the cold-start sets, with the optimal parameter settings adopted in Section 5.2. From the results, we can see that 1) Cold-start estimation is possible, and can achieve a considerably high accuracy by choosing a proper method (e.g. RCM), making applications such as better question routing and better incentive mechanism\ndesign feasible in practice. 2) As the value of K varies, RCM (the red/blue solid line) performs almost consistently better than CM with the same loss type (the red/blue dotted line), as well as PR and TS (the gray dotted lines), showing the advantages of RCM in the cold-start estimation. 3) The cold-start estimation accuracies on SO/Math are higher than those on SO/CPP, again demonstrating that the textual descriptions of the SO/Math questions are more powerful in reflecting their difficulty levels. This is consistent with the phenomenon observed in Section 5.2."
        },
        {
            "heading": "5.4 Difficulty Levels of Words",
            "text": "The third experiment explored how a word\u2019s difficulty level can be measured by RCM automatically and quantitatively.\nOn both SO/CPP and SO/Math, we evenly split the range of question difficulty scores (estimated by RCM) into 10 buckets, and assigned questions to the buckets according to their difficulty scores. A larger bucket ID indicates a higher difficulty level. Then, given a word w, we calculated its frequency in each bucket as follows:\nfi(w) = # questions in bucket i where w occurs\n# all questions in bucket i .\nTo make the frequency meaningful, buckets with less than 50 questions were discarded. We picked\nfour words from each data set as examples. Their normalized frequencies in different buckets are shown in Figure 6 and Figure 7. On SO/CPP, we can observe that \u201carray\u201d and \u201cstring\u201d occur most frequently in questions with lower difficulty levels, \u201cvirtual\u201d higher, and \u201cmultithread\u201d the highest. It coincides with the intuition: \u201carray\u201d and \u201cstring\u201d are usually related to some basic concepts in programming language, while \u201cvirtual\u201d and \u201cmultithread\u201d usually discuss more advanced topics. Similar phenomena can be observed on SO/Math. The results indicate that RCM might provide an automatic way to measure the difficulty levels of words."
        },
        {
            "heading": "6 Related Work",
            "text": "QDE is relevant to the problem of estimating task difficulty levels and user expertise levels in crowdsourcing services (Yang et al., 2008; Whitehill et al., 2009). Studies on this problem fall into two categories: 1) binary response based and 2) partially ordered response based. In the first category, binary responses (i.e. whether the solution provided by a user is correct or not) are observed, and techniques based on item response theory are further employed (Whitehill et al., 2009; Welinder et al., 2010; Zhou et al., 2012). In the second category, partially ordered responses (i.e. which of the two given solutions is better) are observed, and pairwise comparison based methods are further adopted (Yang et al., 2008; Liu et al., 2013). QDE belongs to the latter.\nThe most relevant work to ours is a pairwise comparison based approach proposed by Liu et al. (2013) to estimate question difficulty levels in CQA services. They have also demonstrated that a similar approach can be utilized to estimate user expertise levels (Liu et al., 2011). Yang et al. (2008) and Chen et al. (2013) have also proposed pairwise comparison based methods, for task difficulty estimation and rank aggregation in crowdsourcing settings. Our work differs from previous pairwise comparison based methods in that it further utilizes textual information, formalized as a manifold regularizer.\nManifold regularization is a geometrically motivated framework for machine learning, enforcing the learning model to be smooth w.r.t. the geometrical structure of data (Belkin et al., 2006). Within the framework, dimensionality reduction (Belkin and Niyogi, 2001; Cai et al., 2008) and semisupervised learning (Zhou et al., 2004; Zhu and Lafferty, 2005) algorithms have been constructed. In dimensionality reduction, manifold regularization is utilized to guarantee that nearby points will have similar low-dimensional representations (Cai et al., 2008), while in semi-supervised learning it is utilized to ensure that nearby points will have similar labels (Zhou et al., 2004). In our work, we assume that nearby questions (in terms of textual similarities) will have similar difficulty levels.\nPredicting reading difficulty levels of text is also a relevant problem (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005). It is a key to automatically finding materials at appreciate reading levels for students, and also helps in personalized web search (Collins-Thompson et al., 2011). In the task of predicting reading difficulty levels, documents targeting different grade levels are taken as ground truth, which can be easily obtained from the web. However, there is no\nnaturally annotated data for our QDE task on the web. Other related problems include query difficulty estimation for search engines (Carmel et al., 2006; Yom-Tov et al., 2005) and question difficulty estimation for automatic question answering systems (Lange et al., 2004). In these tasks, query/question difficulty is system-oriented and irrelevant with human knowledge, which is a different setting from ours."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "In this paper, we have proposed a novel method for estimating question difficulty levels in CQA services, called Regularized Competition Model (RCM). It takes fully advantage of questions\u2019 textual descriptions besides question-user comparisons, and thus can effectively deal with data sparsity and perform more accurate estimation. A K-Nearest Neighbor approach is further adopted to estimate difficulty levels of cold-start questions. Experiments on two publicly available data sets show that RCM performs significantly better than existing methods in the estimation task, for both wellresolved and cold-start questions, demonstrating the advantage of incorporating textual information. It is also observed that RCM might automatically measure the knowledge levels of words.\nAs future work, we plan to 1) Enhance the efficiency and scalability of RCM. The complexity analysis in Section 3.2 indicates that storing and processing the graph Laplacian is a bottleneck of RCM. We would like to investigate how to deal with the bottleneck, e.g., via parallel or distributed computing. 2) Apply RCM to non-technical domains. For non-technical domains such as the \u201cnews and events\u201d category of Yahoo! Answers, there might be no strongly distinct notions of \u201cexperts\u201d and \u201cnon-experts\u201d, and it might be more difficult to distinguish between \u201chard questions\u201d and \u201ceasy questions\u201d. It is worthy investigating whether RCM still works on such domains."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank the anonymous reviewers for their helpful comments. This work is supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (grant No. XDA06030200), the National Key Technology R&D Program (grant No. 2012BAH46B03), and the National Natural Science Foundation of China (grant No. 61272427)."
        }
    ],
    "title": "A Regularized Competition Model for Question Difficulty Estimation in Community Question Answering Services",
    "year": 2014
}