{
    "abstractText": "The problem of personalization in Information Retrieval has been under study for a long time. A well-known issue related to this task is the lack of publicly available datasets to support a comparative evaluation of personalized search systems. To contribute in this respect, this paper introduces SE-PEF (StackExchange Personalized Expert Finding), a resource useful for designing and evaluating personalized models related to the Expert Finding (EF) task. The contributed dataset includes more than 250k queries and 565k answers from 3 306 experts, which are annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. The results of the preliminary experiments conducted show the appropriateness of SE-PEF to evaluate and to train effective EF models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pranav Kasela"
        },
        {
            "affiliations": [],
            "name": "Gabriella Pasi"
        },
        {
            "affiliations": [],
            "name": "Raffaele Perego"
        }
    ],
    "id": "SP:dcf3aa6d74139321de3713913339f0977c73484f",
    "references": [
        {
            "authors": [
                "Arian Askari",
                "Suzan Verberne",
                "Gabriella Pasi"
            ],
            "title": "Expert Finding in Legal Community Question Answering",
            "venue": "In European Conference on Information Retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Elias Bassani"
            ],
            "title": "ranx: A Blazing-Fast Python Library for Ranking Evaluation and Comparison",
            "venue": "Notes in Computer Science,",
            "year": 2022
        },
        {
            "authors": [
                "Elias Bassani",
                "Pranav Kasela",
                "Gabriella Pasi"
            ],
            "title": "Denoising Attention for Query-aware User Modeling in Personalized Search",
            "year": 2023
        },
        {
            "authors": [
                "Elias Bassani",
                "Pranav Kasela",
                "Alessandro Raganato",
                "Gabriella Pasi"
            ],
            "title": "A Multi-Domain Benchmark for Personalized Search Evaluation",
            "venue": "In Proceedings of the 31st ACM International Conference on Information",
            "year": 2022
        },
        {
            "authors": [
                "Elias Bassani",
                "Luca Romelli"
            ],
            "title": "Ranx.Fuse: A Python Library forMetasearch",
            "venue": "In Proceedings of the 31st ACM International Conference on Information",
            "year": 2022
        },
        {
            "authors": [
                "Marco Braga",
                "Alessandro Raganato",
                "Gabriella Pasi"
            ],
            "title": "Personalization in BERT with Adapter Modules and Topic Modelling",
            "venue": "In Italian Information Retrieval Workshop",
            "year": 2023
        },
        {
            "authors": [
                "Silvia Calegari",
                "Gabriella Pasi"
            ],
            "title": "Personal ontologies: Generation of user profiles based on the YAGO ontology",
            "venue": "Information Processing & Management 49,",
            "year": 2013
        },
        {
            "authors": [
                "Arash Dargahi Nobari",
                "Sajad Sotudeh Gharebagh",
                "Mahmood Neshati"
            ],
            "title": "Skill Translation Models in Expert Finding",
            "venue": "In Proceedings of the 40th International ACM SIGIR Conference on Research and Development",
            "year": 2017
        },
        {
            "authors": [
                "Jinwen Guo",
                "Shengliang Xu",
                "Shenghua Bao",
                "Yong Yu"
            ],
            "title": "Tapping on the Potential of Q&aCommunity by RecommendingAnswer Providers. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (Napa Valley, California, USA) (CIKM \u201908)",
            "venue": "Association for Computing Machinery,",
            "year": 2008
        },
        {
            "authors": [
                "Pranav Kasela",
                "Gabriella Pasi",
                "Raffaele Perego"
            ],
            "title": "SE-PQA: Personalized Community Question Answering",
            "year": 2023
        },
        {
            "authors": [
                "Zeyu Li",
                "Jyun-Yu Jiang",
                "Yizhou Sun",
                "Wei Wang"
            ],
            "title": "Personalized Question Routing via Heterogeneous Network Embedding",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence 33,",
            "year": 2019
        },
        {
            "authors": [
                "Shuyi Lin",
                "Wenxing Hong",
                "Dingding Wang",
                "Tao Li"
            ],
            "title": "A survey on expert finding techniques",
            "venue": "Journal of Intelligent Information Systems 49,",
            "year": 2017
        },
        {
            "authors": [
                "Hongtao Liu",
                "Zhepeng Lv",
                "Qing Yang",
                "Dongliang Xu",
                "Qiyao Peng"
            ],
            "title": "ExpertBert: Pretraining Expert Finding",
            "venue": "In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM SIGIR-AP \u201923,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoyong Liu",
                "W Bruce Croft",
                "Matthew Koll"
            ],
            "title": "Finding experts in community-based question-answering services",
            "venue": "In Proceedings of the 14th ACM international conference on Information and knowledge management. Association for Computing Machinery,",
            "year": 2005
        },
        {
            "authors": [
                "Zhengyi Ma",
                "Zhicheng Dou",
                "Guanyue Bian",
                "Ji-Rong Wen"
            ],
            "title": "PSTIE: Time Information Enhanced Personalized Search",
            "venue": "In Proceedings of the 29th ACM International Conference on Information",
            "year": 2020
        },
        {
            "authors": [
                "Craig Macdonald",
                "Iadh Ounis"
            ],
            "title": "Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task. In Proceedings of the 15th ACM International Conference on Information and Knowledge Management (Arlington, Virginia, USA) (CIKM \u201906)",
            "venue": "Association for Computing Machinery,",
            "year": 2006
        },
        {
            "authors": [
                "Qiyao Peng",
                "Hongtao Liu",
                "Yinghui Wang",
                "Hongyan Xu",
                "Pengfei Jiao",
                "Minglai Shao",
                "Wenjun Wang"
            ],
            "title": "Towards a Multi-View Attentive Matching for Personalized Expert Finding",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Kasela Pranav",
                "Pasi Gabriella",
                "Perego Raffaele"
            ],
            "title": "SE-PEF: a Resource for Personalized Expert Finding",
            "year": 2023
        },
        {
            "authors": [
                "Kasela Pranav",
                "Pasi Gabriella",
                "Perego Raffaele"
            ],
            "title": "SE-PQA: a Resource for Personalized Community",
            "venue": "Question Answering",
            "year": 2023
        },
        {
            "authors": [
                "Fatemeh Riahi",
                "Zainab Zolaktaf",
                "Mahdi Shafiei",
                "Evangelos Milios"
            ],
            "title": "Finding Expert Users in Community Question Answering",
            "venue": "In Proceedings of the 21st International Conference on World Wide Web (Lyon, France) (WWW \u201912 Companion). Association for Computing Machinery,",
            "year": 2012
        },
        {
            "authors": [
                "Fei Xu",
                "Zongcheng Ji",
                "Bin Wang"
            ],
            "title": "Dual Role Model for Question Recommendation in Community Question Answering",
            "venue": "In Proceedings of the 35th International ACM SIGIR Conference on Research and Development",
            "year": 2012
        },
        {
            "authors": [
                "Zhou Zhao",
                "Lijun Zhang",
                "Xiaofei He",
                "Wilfred Ng"
            ],
            "title": "Expert Finding for QuestionAnswering via Graph RegularizedMatrix Completion",
            "venue": "IEEE Transactions on Knowledge and Data Engineering 27,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Information systems\u2192Expert search; Information retrieval.\nKEYWORDS Question Answering, Expert Finding, User Model, Personalization. ACM Reference Format: Pranav Kasela, Gabriella Pasi, and Raffaele Perego. 2023. SE-PEF: a Resource for Personalized Expert Finding. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP \u201923), November 26\u201328, 2023, Beijing, China. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3624918.3625335"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Expert finding (EF) is awell-studied problem in community question answering (cQA). The aim of EF in a cQA scenario, is to identify users, namely the experts, that might be able to answer correctly a given question on a specific topic. This task is important for many applications, e.g., crowd-sourcing, and for cQA platforms that wish to increase user engagement by precisely identifying the experts to whom to propose the questions about a given topic.\nPersonalization is gaining traction in many IR [3, 4, 7, 15] and NLP [6] tasks, but it is not largely adopted in EF due to the lack of publicly-available, large-scale datasets containing user-related information. In this research paper, building upon our previous\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR-AP \u201923, November 26\u201328, 2023, Beijing, China \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0408-6/23/11. https://doi.org/10.1145/3624918.3625335\nwork [10], in which the authors presented a dataset for personalized community question answering, we introduce SE-PEF (StackExchange - Personalized Expert Finding), a large dataset rich in user-level features that can be leveraged for training, evaluating and comparing both personalized and non-personalized models for the Expert Finding task. SE-PEF comprises around 250k questions and 560k associated answers provided by 3,306 experts, and it inherits a rich set of features modeling the social interactions within the user community. To train personalized models, we keep the user-related data as they are provided in the original dataset: users\u2019 past questions and answers, their own social autobiography, their reputation score, and the number of profile views that they have received.\nIn the case of EF, personalization can improve the perceived service quality in different ways. For example, when the requesting user is interested in multiple topics, identifying an expert by considering also the requesting user\u2019s interests can improve the trust in the answer received. A similar effect can be obtained by preferring experts that are closer to the requesting users based on past interactions or follower/followee dynamics. In summary, the contribution of this paper is the following:\n\u2022 We provide and make available the SE-PEF dataset, as a public resource consisting of a comprehensive corpus including around 255k questions and 560k answers provided by 3306 expert users. The richness and variety of features provided with the dataset enable its use for the design and evaluation of personalized EF methods. \u2022 We report a preliminary comparison of the performance of different EF methods applied to the questions, answers, and users in SE-PEF. The results confirm that models based on deep learning outperform in effectiveness traditional retrieval models and that by exploiting personalization features we can obtain a significant performance boost.\nThe rest of the paper is organized as follows. Section 2 introduces the SE-PEF dataset and reports some statistics about its content. Moreover, the section details the EF task addressed in this paper by using SE-PEF. Section 3 compares SE-PEF with respect to other publicly-available resources in the field. Section 4 presents a preliminary comparison of traditional and personalized models for EF applied to SE-PEF. In Section 5 we discuss the utility and the practical implications of the new resource. Finally, Section 6 concludes the work and draws future lines of investigation.\nar X\niv :2\n30 9.\n11 68\n6v 2\n[ cs\n.I R\n] 5\nO ct\n2 02\n3\n2 THE SE-PEF DATASET The dataset proposed in [10] is based on StackExchange1 and available under a CC BY-SA 4.0 license. It comprises questions and answers from 50 different stackexchange communities, written between 2008-09-10 and 2022-09-25. There are around 1.1 million questions and 2.1 million answers. The training, validation and test splits are based on a temporal condition and are already provided on zenodo[19].\nIn [10] the authors show that personalization is more useful if multiple communities are used together in this dataset rather than using a single community to create the dataset. Meanwhile, previous works that use StackExchange for EF tasks focus only on a single community or a portion of a community, thus neglecting the domain diversity characterizing the questions and the various experts [8, 13, 17].\n2.1 Accessing the SE-PEF dataset SE-PEF dataset is made publicly available on zenodo2 according to the conditions detailed in the included CC BY-SA 4.0. license agreement and the code used for data creation, training, hyperparameter optimization, and testing are available on github3.\n2.2 SE-PEF Definition In the following, we introduce the specific instance of EF task in which we are interested and illustrate how to address it by using the resources in SE-PEF.\n1https://stackexchange.com 2https://doi.org/10.5281/zenodo.8332747[18] 3https://github.com/pkasela/SE-PEF\nOur EF task shares the same goal as the question-answering task: satisfy users\u2019 needs in a cQA forum in the most effective way. In a cQA forum, a user may ask a question that does not have any related answers in the answer collection. Since not receiving any answer can create a sense of frustration in a user posting a question, it is important for the community and the platform to identify and, eventually, notify domain experts who may be able to answer the question correctly. Finding good matches between unanswered questions and expert users can improve remarkably the engagement with the community. In fact, on the one hand, users posting a question can receive correct answers from the alerted experts in a short time; on the other hand, expert users can dedicate their time to answering questions specifically related to their expertise rather than searching for questions that they can respond.\nFormally, let E be a set of expert users {\ud835\udc521, . . . , \ud835\udc52\ud835\udc58 }. Given a question q asked by user u, the EF task consists in retrieving from E a list of \ud835\udc58 experts {\ud835\udc52\ud835\udc5e,1, . . . , \ud835\udc52\ud835\udc5e,\ud835\udc58 } ordered by their likelihood of answering correctly to question q.\nStackExchange data has been used in several EF papers, e.g., in [8, 13, 17]. These works however mostly focus on solving the expert finding task for a single community. SE-PEF incorporates instead information frommultiple communities to provide a dataset that can be used also to investigate models for generalist cQA forums that may not have separate channels for the discussed topics.\nTo create the dataset, we define as best answer for a given question the answer selected as the best one by the user who asked the question, if available; otherwise, we assume the best answer to the one with the highest score, if it has received a score greater than a fixed threshold \ud835\udefe\ud835\udc60 4. We note that this assumption, for the best answer being the most voted answer if no answer has been flagged as best by the user asking the question, is used only for the expert detection procedure, which will be explained subsequently and not as relevance judgement for the test data. In the test set we only consider the best answer, the answer explicitly labeled as such by the user asking the question. Exploiting high-scored answers as the best answers allows us to increase the number of questions successfully answered. Indeed this choice is justified by the observation that 87.6% of the answers, which are selected as best ones by the user asking the question, are also the most up-voted ones. On the other hand, we have observed that many users, once they satisfy the information need with a good answer, do not bother to mark the answer as the best.\nAt this point, to identify the set of experts E, we follow the procedure indicated by Dargahi et al. [8] for their StackOverflow dataset:\n\u2022 For each community\ud835\udc36 , letU be the set of users, andB the set of best answers computed as explained above in the community\ud835\udc36 . For each user u \u2208 U, letA\ud835\udc62,\ud835\udc36 = {\ud835\udc4e\ud835\udc62,1, \ud835\udc4e\ud835\udc62,2, . . . , \ud835\udc4e\ud835\udc62,\ud835\udc5b} be the set of answers given by u in \ud835\udc36; \u2022 Remove all users who do not have at least\ud835\udefe\ud835\udc4e answers selected as best answers, i.e. define:"
        },
        {
            "heading": "E\u2032 = U \\ {\ud835\udc62 \u2208 U, s.t. |A\ud835\udc62,\ud835\udc36 \u2229 B| < \ud835\udefe\ud835\udc4e}",
            "text": "4The \ud835\udefe thresholds used for SE-PEF are reported at the end of Section 3.\nE\ud835\udc36 = {\ud835\udc52 \u2208 E\u2032 s.t. \ud835\udc4e\ud835\udc5f\ud835\udc52\u2032,\ud835\udc36 \u2265 \ud835\udc4e\ud835\udc5f\ud835\udc36 } The final set of experts E is defined as the union of the sets of experts found for each community. The above process ensures that the selected experts have a high level of engagement and write high-quality answers having a high acceptance rate.\nIn Figure 2 we show the basic structure of the JSON file provided for training, validation, and test. The user_questions, user_answers contain the identifiers (ids) of the questions and the answers, written before the current question timestamp, of the user asking the question. The expert_questions, and expert_answers contain the ids of the questions and the answers of the expert that has given the best answer. The data is provided also with a collection of questions and a collection of answers; they are two very simple JSON files, where the keys correspond to the ids of the questions and answers respectively. The values of the keys are constituted by the texts of the questions and answers respectively. The data is provided also with multiple data-frames, curated from the original data found from archive.org, which can be used to add more features. These features are described on the Stack Exchange website.5"
        },
        {
            "heading": "3 COMPARISON WITH AVAILABLE DATASETS",
            "text": "Concerning the EF task, there are plenty of datasets available [12], and some of them are based on data from cQAwebsites. For example, StackExchange is used to create a pre-trained BERTmodel for the EF task in [13]. However, the work focuses only on designing an EF pretraining framework based on a specific augmentedmasked language model able to learn the question-expert matching task. Other EF 5https://meta.stackexchange.com/questions/2677/database-schema-documentationfor-the-public-data-dump-and-sede\ndatasets derived from cQA forums come from: StackOverflow [8, 20], Yahoo Answers![9, 21], Wondir [14] and Quora [22]. Recently, a domain-specific expert finding task was tackled using Avvo [1], a legal cQA website, but in this case, personalization is not possible due to the fact that users are anonymous. In Table 1 we report the basic dataset statistics of some of the commonly used datasets in EF for comparison.\nA common issue with the existing datasets is that the experts are, in many cases, not well-defined, and determining what makes a user an expert is not trivial. Furthermore, most works among those previously cited either rely on a private dataset, or refer to a specific domain and make very strong assumptions simplifying the task addressed. Conversely, SE-PEF will be made publicly available, it has a well-defined definition of an expert, which is inspired by reasonable hypothesis common to other works [11, 13, 17]. Furthermore, it provides a rich set of social features usable for personalization and combines data from multiple communities, which, as we have already stated, increases dataset diversity and opens the possibility of exploiting cross-domain user information for EF.\nTo build the SE-PEF for EF we followed the procedure detailed in Section 2.2, by setting \ud835\udefe\ud835\udc60 = 5 and \ud835\udefe\ud835\udc4e = 10. Finally, we also remove from the training dataset the questions answered by experts who previously posted less than 5 answers to avoid the cold start problem for expert modeling. Using this procedure, we obtain SE-PEF, from starting from the dataset presented in [10], including 81,252 users, 3,306 experts, 252,501 queries (218,647 for training, 16,710 for validation, and 19,995 for testing), and 564,690 answers.\n4 PRELIMINARY EXPERIMENTS WITH SE-PEF This section provides a concise overview of the experimental setup and introduces the methods employed to showcase the capabilities of SE-PEF in the EF task, defined and discussed in Section 2.2. Finally, we report and discuss the results of the conducted experiments."
        },
        {
            "heading": "4.1 Experimental settings",
            "text": "For our EF task, we use a retrieval-based approach [16], and simply cast the EF task to a cQA task where we use the similarity scores of the retrieved documents as experts\u2019 scores. We explain this in detail in the following paragraphs.\nWe adopt a two-stage ranking architecture that prioritizes efficiency and recall in the first stage. The primary objective of this first stage is to select for each query a set of candidate documents that are eventually re-ranked in a second stage by a precision-oriented ranker. The first stage is based on Elastic Search6, and uses BM25 as a fast ranker. We use the same BM25 hyperparameters as indicated in [10]: 1 and 1.75 for b and k1, respectively. In the second, precision-oriented stage, to re-rank the retrieved documents we utilize a linear combination of the set of available scores that includes the BM25 score, the similarity score computed by a neural re-ranker, and, when used, the score computed by a personalization model exploiting the user history. In all the experiments the second stage re-ranks the top-100 results retrieved with BM25.\n6https://www.elastic.co/\nNon-personalized models. As neural re-ranker in the second stage we use the following two models used also in [10]:\n\u2022 DistilBERT. This model is obtained by fine-tuning the pretrained distilbert-base-uncased model7 for the task of answer retrieval tackled in [10]. We use the same training data and experimental settings used in [10]. \u2022 MiniLM, based on MiniLM-L6-H384-uncased8. This model is used as it is, without any fine-tuning.\nPersonalizedmodel for EF. For building the EF personalizedmodel we exploit the folksonomy arising from tags, very similar to the one employed in [10]. This model, which we also call TAG from hereon, aims at capturing the similarities among the topics addressed by the asker in their current and previous questions, and the ones in which a considered expert answered in the past. Given a question q, asked by user u at time t, let \ud835\udc47\ud835\udc62,\ud835\udc61 be the set of tags assigned by u to all theirs questions posted before t (including q). \ud835\udc47\ud835\udc62,\ud835\udc61 thus represents the interests of u as expressed in their previous interactions. The authors of the answers to query q do not have the possibility of tagging explicitly their answers, so for each answer, we consider the tags associated with the answered questions.\nThe way we represent the expert user is slightly different: the expertise, in this case, is based on a pre-computed, static representation \ud835\udc47\ud835\udc52 of each expert \ud835\udc52 in SE-PEF. This representation considers the tags \ud835\udc47 \u2032\ud835\udc52 of all the questions answered by \ud835\udc52 included only in the training set. To build \ud835\udc47\ud835\udc52 from \ud835\udc47 \u2032\ud835\udc52 we perform an additional step consisting in discarding the tags with a frequency lower than the median frequency of tags in \ud835\udc47 \u2032\ud835\udc52 . This tag pruning step reduces the noise coming from the possible presence of non-relevant tags that might have appeared as additional tags in a few questions answered but the expert might not be an expert on those topics. As for the previous task, the EF TAG score \ud835\udc60\ud835\udc52,\ud835\udc5e for expert \ud835\udc52 is finally computed as\n\ud835\udc60\ud835\udc52,\ud835\udc5e = |\ud835\udc47\ud835\udc52 \u2229\ud835\udc47\ud835\udc62,\ud835\udc5e | |\ud835\udc47\ud835\udc62,\ud835\udc5e | + 1\nScore computation and combination. Given the list of answers \ud835\udc34 retrieved and re-ranked with the above models, we observe that\n7https://huggingface.co/distilbert-base-uncased 8https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\nsome users could have authored multiple answers included in \ud835\udc34. This is potentially an important feature for characterizing their expertise. Therefore, to obtain the expert-level score, we sum up the scores assigned to all the answers in \ud835\udc34 coming from the same expert. Moreover, since the TAGmodel returns a score for all experts in the dataset, even those not having an answer in \ud835\udc34, we assume that these experts receive a score contribution equal to 0 from BM275 and the non-personalized models. Finally, the scores from BM25, personalized and non-personalized models are combined by computing the weighted sum of the normalized scores from the models, using weights \ud835\udf06\ud835\udc35\ud835\udc4025, \ud835\udf06\ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47 /\ud835\udc40\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc3f\ud835\udc40 , and \ud835\udf06\ud835\udc47\ud835\udc34\ud835\udc3a , with \u2211 \ud835\udc56 \ud835\udf06\ud835\udc56 = 1. The \ud835\udf06 values are optimized on the validation set by performing a grid search in the interval [0, 1] with step 0.1.\nEvaluation Metrics. For the task of expert finding we utilize the following evaluationmetrics: Precision at 1 (P@1), Recall at 3 (R@3), Recall at 5 (R@5), and Mean Reciprocal Rank at 5 (MRR@5) as our evaluation metrics. The cutoffs are set low as we prioritize identifying experts at the top of the ranked lists. All the metrics are computed by using the ranx library [2, 5]."
        },
        {
            "heading": "4.2 Experimental Results",
            "text": "The results are reported instead in Table 2. The symbol * indicates a statistically significant improvement over the respective non-personalized method not using any contribution from the TAG model. Statistical significance is assessed with a Bonferronicorrected two-sided paired student\u2019s t-test with 99% confidence. The column labeled \ud835\udf06 reports the optimized weights, found using the\nvalidation set, used for combining the scores computed by BM25, DistilBERT / MiniLM, and TAG models. In the cases in which the optimal weight for the BM25 score is equal to 0 \u2013 i.e., BM25 does not contribute to re-ranking \u2013 we omit BM25 from the name of the model and \ud835\udf061 = 0 from the weights column.\nDifferently from the cQA task tackled by the authors of [10], we observe that on EF the performance gap of DistilBERT vs. MiniLMSBERT is sensibly reduced. The best-performingmodel among the ones tested is in fact DistilBERT + TAG which significantly outperforms both DistilBERT and MiniLMSBERT. Analogously to the cQA task, personalization is very effective for EF. The contribution of the TAG model allows for significantly improving all the non-personalized methods, with a performance boost exceeding three points in MRR@5 for the DistilBERT model. By looking at the optimized \ud835\udf06 weights reported in all three tables, we see that the TAG model contribution is much higher for the EF task (\ud835\udf06\ud835\udc47\ud835\udc34\ud835\udc3a \u2265 .5) than for the one obtained by the authors of [10] (\ud835\udf06\ud835\udc47\ud835\udc34\ud835\udc3a \u2264 .3)."
        },
        {
            "heading": "5 UTILITY AND PREDICTED IMPACT",
            "text": "The SE-PEF resource we make available to the research community is a step ahead toward a fair and robust evaluation of personalization approaches in Expert Finding. The features inherited from [10] include explicit signals to create relevance judgments and a large amount of historical user-level information to design and test classical and novel personalization methods. We expect the SE-PEF dataset being useful for many researchers and practitioners working in personalized IR and the application of machine/deep learning techniques for personalization. In recent years, significant efforts have been dedicated to the study of personalization techniques. However, there is still a lack of a comprehensive dataset for evaluating and comparing different approaches, which makes the comparison between different methods less reliable or, worse, not possible at all.\nFor this reason, we expect that the proposed dataset will impact the research community working on personalized EF as it provides a common ground of evaluation built on questions, answers, and experts from real users socially interacting via a community-oriented web platform.\nIn this proposal, the expert can have different domain backgrounds and share interests and knowledge in various communities. We also expect that training training on such rich and diverse data, like SE-PEF, should produce a more robust and generalizable model."
        },
        {
            "heading": "6 CONCLUSION AND FUTUREWORK",
            "text": "SE-PEF (StackExchange - Personalized Expert Finding) is an extension of a previous work [10], which presents a large real-world dataset for personalized cQA. The data inherits a rich set of userlevel features modeling the interactions among the members of the online communities.\nOur study provided a detailed description of the data creation and training process. Furthermore, we illustrated the methodologies adopted, explicitly focusing on IR techniques. We discussed how the similarity scores computed can be aggregated and combined to target the EF task adopted. For the retrieval, we adopted\na two-stage architecture, where the second stage utilizes for reranking an optimized combination of the scores generated by BM25, DistilBERT/MiniLMSBERT, and TAG models.\nThe preliminary experiments conducted proved the effectiveness of personalization on this dataset, surpassing methods that rely on pre-trained and fine-tuned large language models by a statistically significant margin. We expect other researchers to develop more complex strategies to improve results on the SE-PEF resource. We leave such research as future work for us and the IR community working on personalized IR. Acknowledgements. Funding for this research has been provided by: PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000013 - \u201cFAIR - Future Artificial Intelligence Research\u201d - Spoke 1 \u201dHuman-centered AI\u201d funded by the European Union (EU) under the NextGeneration EU programme; the EU\u2019s Horizon Europe research and innovation programme EFRA (Grant Agreement Number 101093026). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the EU or European Commission-EU. Neither the EU nor the granting authority can be held responsible for them."
        }
    ],
    "title": "SE-PEF: a Resource for Personalized Expert Finding",
    "year": 2023
}