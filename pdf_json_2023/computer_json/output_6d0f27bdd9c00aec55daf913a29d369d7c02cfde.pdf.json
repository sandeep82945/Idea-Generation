{
    "abstractText": "Zero-shot object detection aims to localize and recognize objects of unseen classes. Most of existing works face two problems: the low recall of RPN in unseen classes and the confusion of unseen classes with background. In this paper, we present the first method that combines DETR and meta-learning to perform zero-shot object detection, named Meta-ZSDETR, where model training is formalized as an individual episode based meta-learning task. Different from Faster R-CNN based methods that firstly generate class-agnostic proposals, and then classify them with visual-semantic alignment module, Meta-ZSDETR directly predict class-specific boxes with class-specific queries and further filter them with the predicted accuracy from classification head. The model is optimized with meta-contrastive learning, which contains a regression head to generate the coordinates of class-specific boxes, a classification head to predict the accuracy of generated boxes, and a contrastive head that utilizes the proposed contrastive-reconstruction loss to further separate different classes in visual space. We conduct extensive experiments on two benchmark datasets MS COCO and PASCAL VOC. Experimental results show that our method outperforms the existing ZSD methods by a large margin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lu Zhang"
        },
        {
            "affiliations": [],
            "name": "Chenbo Zhang"
        },
        {
            "affiliations": [],
            "name": "Jiajia Zhao"
        },
        {
            "affiliations": [],
            "name": "Jihong Guan"
        },
        {
            "affiliations": [],
            "name": "Shuigeng Zhou"
        }
    ],
    "id": "SP:0186e837422ce97f32dc803b6f231befdfe3dc07",
    "references": [
        {
            "authors": [
                "Zeynep Akata",
                "Florent Perronnin",
                "Zaid Harchaoui",
                "Cordelia Schmid"
            ],
            "title": "Label-embedding for attribute-based classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Ankan Bansal",
                "Karan Sikka",
                "Gaurav Sharma",
                "Rama Chellappa",
                "Ajay Divakaran"
            ],
            "title": "Zero-shot object detection",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Ankan Bansal",
                "Karan Sikka",
                "Gaurav Sharma",
                "Rama Chellappa",
                "Ajay Divakaran"
            ],
            "title": "Zero-shot object detection",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Maxime Bucher",
                "St\u00e9phane Herbin",
                "Fr\u00e9d\u00e9ric Jurie"
            ],
            "title": "Improving semantic embedding consistency by metric learning for zero-shot classiffication",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Shiming Chen",
                "Wenjie Wang",
                "Beihao Xia",
                "Qinmu Peng",
                "Xinge You",
                "Feng Zheng",
                "Ling Shao"
            ],
            "title": "Free: Feature refinement for generalized zero-shot learning",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Berkan Demirel",
                "Ramazan Gokberk Cinbis",
                "Nazli Ikizler-Cinbis"
            ],
            "title": "Zero-shot object detection by hybrid region embedding",
            "venue": "arXiv preprint arXiv:1805.06157,",
            "year": 2018
        },
        {
            "authors": [
                "Berkan Demirel",
                "Ramazan Gokberk Cinbis",
                "Nazli Ikizler-Cinbis"
            ],
            "title": "Zero-shot object detection by hybrid region embedding",
            "venue": "In BMVC,",
            "year": 2018
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International journal of computer vision,",
            "year": 2009
        },
        {
            "authors": [
                "Yanwei Fu",
                "Timothy M Hospedales",
                "Tao Xiang",
                "Zhenyong Fu",
                "Shaogang Gong"
            ],
            "title": "Transductive multi-view embedding for zero-shot recognition and annotation",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Ross Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Dikshant Gupta",
                "Aditya Anantharaman",
                "Nehal Mamgain",
                "Vineeth N Balasubramanian",
                "CV Jawahar"
            ],
            "title": "A multi-space approach to zero-shot object detection",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Zongyan Han",
                "Zhenyong Fu",
                "Jian Yang"
            ],
            "title": "Learning the redundancy-free features for generalized zero-shot object recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nasir Hayat",
                "Munawar Hayat",
                "Shafin Rahman",
                "Salman Khan",
                "Syed Waqas Zamir",
                "Fahad Shahbaz Khan"
            ],
            "title": "Synthesizing the unseen for zero-shot object detection",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Nasir Hayat",
                "Munawar Hayat",
                "Shafin Rahman",
                "Salman Khan",
                "Syed Waqas Zamir",
                "Fahad Shahbaz Khan"
            ],
            "title": "Synthesizing the unseen for zero-shot object detection",
            "venue": "In ACCV,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Peiliang Huang",
                "Junwei Han",
                "De Cheng",
                "Dingwen Zhang"
            ],
            "title": "Robust region feature synthesizer for zero-shot object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Siddhesh Khandelwal",
                "Anirudth Nambirajan",
                "Behjat Siddiquie",
                "Jayan Eledath",
                "Leonid Sigal"
            ],
            "title": "Frustratingly simple but effective zero-shot detection and segmentation: Analysis and a strong baseline",
            "venue": "arXiv preprint arXiv:2302.07319,",
            "year": 2023
        },
        {
            "authors": [
                "Dahun Kim",
                "Tsung-Yi Lin",
                "Anelia Angelova",
                "In So Kweon",
                "Weicheng Kuo"
            ],
            "title": "Learning open-world object proposals without learning to classify",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Elyor Kodirov",
                "Tao Xiang",
                "Shaogang Gong"
            ],
            "title": "Semantic autoencoder for zero-shot learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Zhihui Li",
                "Lina Yao",
                "Xiaoqin Zhang",
                "Xianzhi Wang",
                "Salil Kanhere",
                "Huaxiang Zhang"
            ],
            "title": "Zero-shot object detection with textual descriptions",
            "venue": "In AAAI,",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Tianying Liu",
                "Lu Zhang",
                "Yang Wang",
                "Jihong Guan",
                "Yanwei Fu",
                "Jiajia Zhao",
                "Shuigeng Zhou"
            ],
            "title": "Recent few-shot object detection algorithms: A survey with performance comparison, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Qiaomei Mao",
                "Chong Wang",
                "Shenghao Yu",
                "Ye Zheng",
                "Yuqi Li"
            ],
            "title": "Zero-shot object detection with attributes-based category similarity",
            "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs, 67(5):921\u2013925,",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Christian Puhrsch",
                "Armand Joulin"
            ],
            "title": "Advances in pretraining distributed word representations",
            "venue": "arXiv preprint arXiv:1712.09405,",
            "year": 2017
        },
        {
            "authors": [
                "Sanath Narayan",
                "Akshita Gupta",
                "Fahad Shahbaz Khan",
                "Cees GM Snoek",
                "Ling Shao"
            ],
            "title": "Latent embedding feedback and discriminative features for zero-shot classification",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Shafin Rahman",
                "Salman Khan",
                "Nick Barnes"
            ],
            "title": "Improved visual-semantic alignment for zero-shot object detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Shafin Rahman",
                "Salman Khan",
                "Nick Barnes"
            ],
            "title": "Improved visual-semantic alignment for zero-shot object detection",
            "venue": "In AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Shafin Rahman",
                "Salman Khan",
                "Fatih Porikli"
            ],
            "title": "Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts",
            "venue": "In ACCV,",
            "year": 2018
        },
        {
            "authors": [
                "Shafin Rahman",
                "Salman H Khan",
                "Fatih Porikli"
            ],
            "title": "Zeroshot object detection: Joint recognition and localization of novel concepts",
            "venue": "International Journal of Computer Vision, 128:2979\u20132999,",
            "year": 2020
        },
        {
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Hamid Rezatofighi",
                "Nathan Tsoi",
                "JunYoung Gwak",
                "Amir Sadeghian",
                "Ian Reid",
                "Silvio Savarese"
            ],
            "title": "Generalized intersection over union: A metric and a loss for bounding box regression",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Edgar Schonfeld",
                "Sayna Ebrahimi",
                "Samarth Sinha",
                "Trevor Darrell",
                "Zeynep Akata"
            ],
            "title": "Generalized zero-and few-shot learning via aligned variational autoencoders",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Flood Sung",
                "Yongxin Yang",
                "Li Zhang",
                "Tao Xiang",
                "Philip HS Torr",
                "Timothy M Hospedales"
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yongqin Xian",
                "Tobias Lorenz",
                "Bernt Schiele",
                "Zeynep Akata"
            ],
            "title": "Feature generating networks for zero-shot learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yongqin Xian",
                "Saurabh Sharma",
                "Bernt Schiele",
                "Zeynep Akata"
            ],
            "title": "f-vaegan-d2: A feature generating framework for any-shot learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Caixia Yan",
                "Xiaojun Chang",
                "Minnan Luo",
                "Huan Liu",
                "Xiaoqin Zhang",
                "Qinghua Zheng"
            ],
            "title": "Semantics-guided contrastive network for zero-shot object detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Caixia Yan",
                "Qinghua Zheng",
                "Xiaojun Chang",
                "Minnan Luo",
                "Chung-Hsing Yeh",
                "Alexander G Hauptman"
            ],
            "title": "Semanticspreserving graph propagation for zero-shot object detection",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Lu Zhang",
                "Yang Wang",
                "Jiaogen Zhou",
                "Chenbo Zhang",
                "Yinglu Zhang",
                "Jihong Guan",
                "Yatao Bian",
                "Shuigeng Zhou"
            ],
            "title": "Hierarchical few-shot object detection: Problem, benchmark and method",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia, MM",
            "year": 2022
        },
        {
            "authors": [
                "Lu Zhang",
                "Shuigeng Zhou",
                "Jihong Guan",
                "Ji Zhang"
            ],
            "title": "Accurate few-shot object detection with support-query mutual guidance and hybrid loss",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Minyi Zhao",
                "Bingjia Li",
                "Jie Wang",
                "Wanqing Li",
                "Wenjing Zhou",
                "Lan Zhang",
                "Shijie Xuyang",
                "Zhihang Yu",
                "Xinkun Yu",
                "Guangze Li"
            ],
            "title": "Towards video text visual question answering: benchmark and baseline",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shizhen Zhao",
                "Changxin Gao",
                "Yuanjie Shao",
                "Lerenhan Li",
                "Changqian Yu",
                "Zhong Ji",
                "Nong Sang"
            ],
            "title": "Gtnet: Generative transfer network for zero-shot object detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ye Zheng",
                "Li Cui"
            ],
            "title": "Zero-shot object detection with transformers",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2021
        },
        {
            "authors": [
                "Ye Zheng",
                "Ruoran Huang",
                "Chuanqi Han",
                "Xi Huang",
                "Li Cui"
            ],
            "title": "Background learnable cascade for zero-shot object detection",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Ye Zheng",
                "Ruoran Huang",
                "Chuanqi Han",
                "Xi Huang",
                "Li Cui"
            ],
            "title": "Background learnable cascade for zero-shot object detection",
            "venue": "In ACCV,",
            "year": 2020
        },
        {
            "authors": [
                "Ye Zheng",
                "Jiahong Wu",
                "Yongqiang Qin",
                "Faen Zhang",
                "Li Cui"
            ],
            "title": "Zero-shot instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Pengkai Zhu",
                "Hanxiao Wang",
                "Venkatesh Saligrama"
            ],
            "title": "Don\u2019t even look once: Synthesizing features for zero-shot detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable {detr}: Deformable transformers for end-to-end object detection",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Object detection [36] is one of the most fundamental tasks in computer vision. Most existing object detection methods require huge amounts of annotated training data, which is expensive and time-consuming to acquire. Meanwhile, in reality novel categories constantly emerge, and there is seriously lack or even nonexistent of visual data of\n*correspondence author\nthose novel categories for model training, such as endangered species in the wild. The above issues motivates the investigation of zero-shot object detection, which aims to localize and recognize objects of unseen classes.\nA mainstream framework of the existing works that are based on Faster R-CNN, is illustrated in Fig. 1(a), where the RPN remains unchanged and the RoI classification head is replaced with different visual-semantic alignment modules, such as mapping to the same embedding space to calculate similarity between proposals and semantic vectors [2, 42, 43, 34, 49, 12, 27, 19, 7], synthesizing visual features from semantic vectors [18, 14, 47, 52, 29] etc.\nHowever, we observe that the existing methods are suboptimal, due to their obvious inherent shortcomings: i) The proposals from RPN are often not reliable enough to cover all unseen classes objects in an image because of lacking training data, which has also been identified by a recent study [20]. ii) The confusion between background and un-\nar X\niv :2\n30 8.\n09 54\n0v 1\n[ cs\n.C V\n] 1\n8 A\nseen classes is an intractable problem. Although many previous works have tried to tackle it [51, 14, 2], the results are still unsatisfactory.\nRecently, object detection frameworks based on the Transformer have gained widespread popularity, such as DETR [5], Deformable DETR [53], etc. Such architectures are RPN-free and background-free, i.e., they do not involve RPN and background class, which are naturally conducive to building zero-shot object detection methods. However, how to build a ZSD method based on DETR detectors poses new challenges. An intuitive idea is to replace DETR\u2019s classification head with a zero-shot classifier based on cosine similarity [48]. However, such a method simply treats DETR as a large RPN for proposals generation, the overall framework is essentially the same as previous works.\nIn this paper, we present the first method that fully explores DETR detectors and meta-learning to perform zeroshot object detection, named Meta-ZSDETR, which can solve the two problems mentioned above that have plagued the field of ZSD for many years, and achieves the stateof-the-art performance. The comparison of Meta-ZSDETR with previous methods is shown in Fig. 1. Different from the previous works that firstly generate class-agnostic proposals and then classify them with visual-semantic alignment module, our method utilizes semantic vectors to guide both proposal generation and classification, which greatly improves the recall of unseen classes. Meanwhile, there is no background class in DETR detectors, which means the confusion between background and unseen classes is no more existent.\nIn order to detect unseen classes, we formalize the training process as an individual episode based meta-learning task. In each episode, we randomly sample an image I and a set of classes C\u03c0 , which contains the positive classes that appear in I and negative classes that do not appear. The metalearning task is to make the model learn to detect all positive classes of C\u03c0 on image I . Through the meta-learning task, the training and testing can be unified, i.e., in the model testing, we need only to employ the unseen classes as the set C\u03c0 . To enable the model to detect an arbitrary class set, we firstly fuse each object query with a projected semantic vector from the class set C\u03c0 , which transfers the query from class-agnostic to class-specific. Then, the decoder takes the class-specific query as input and predicts the locations of class-specific boxes, together with the probabilities that the boxes belong to the fused class. To achieve the above goal, we propose meta-contrastive learning, where all predictions are split into three different types and different combinations of them are chosen to optimize three different heads, i.e., the regression head to generate the locations of classspecific boxes, the classification head to predict the accuracy of generated boxes, and the contrastive head to separate different classes in visual space for performance improving\nwith a contrastive-reconstruction loss. The bipartite matching and loss calculation are performed in a class-by-class manner, and the final loss is averaged over all classes in the sampled class set C\u03c0 .\nIn summary, our major contributions are as follows:\n\u2022 We present the first method that explores DETR and meta-learning to perform zero-shot object detection, which formalizes the training as an individual episode based meta-learning task and ingeniously tackles the two problems that plague ZSD for years.\n\u2022 We propose to train the decoder to directly predict class-specific boxes with class-specific queries as input, under the supervision of our meta-contrastive learning that contains three different heads.\n\u2022 We conduct extensive experiments on two benchmark datasets MSCOCO and PASCAL VOC to evaluate the proposed method Meta-ZSDETR. Experimental results show that our method outperforms the existing ZSD methods."
        },
        {
            "heading": "2. Related work",
            "text": ""
        },
        {
            "heading": "2.1. Zero-shot learning",
            "text": "Zero-shot learning (ZSL) aims to classify images of unseen classes that do not appear during training. There are two main streams in ZSL: embedding based methods and generative based methods. The key idea of embedding based methods is to learn an embedding function that maps the semantic vectors and visual features into the same embedding space, where the visual features and semantic vectors can be compared directly [1, 4, 10, 22, 39, 46]. Generative based methods aim to synthesize unseen visual features with variational autoencoder [21] and generative adversarial networks [41], which convert the ZSL into a fully supervised way [6, 13, 38, 40]."
        },
        {
            "heading": "2.2. Zero-shot object detection",
            "text": "Zero-shot object detection (ZSD) has received a great deal of research interest in recent years. Most of ZSD methods are built on Faster R-CNN [11], YOLO [35] and RetinaNet [24, 26]. The process of these methods can be summarized as: generating class-agnostic proposals and classifying proposals into seen/unseen and background classes. The main difference of these methods is that different visual-semantic alignment methods are used to complete the classification of proposals. These methods can be divided into two categories: mapping the semantic vectors and visual features to the same embedding space to calculate similarity [2, 42, 43, 34, 49, 12, 27, 19, 7] and synthesizing visual features from semantic vectors [18, 14, 47, 52, 29]. Although previous works have paid great efforts,\nthere are some problems that still have no satisfactory solution, such as the low recall of class-agnostic RPN for unseen classes and the confusion between background and unseen classes. These problems may be caused by the incompatibility of ZSD task and proposals-based architecture such as Faster R-CNN.\nDifferent from previous works, Meta-ZSDETR is the first work built on Deformable DETR with meta-learning, where the semantic vectors are guided for class-specific boxes generation, instead of class-agnostic proposals in previous works, resulting in a higher unseen recall and precision. Meanwhile, since there is no background class in DETR detectors, the confusion between background and unseen classes is non-existent."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Problem definition",
            "text": "Zero-shot object detection (ZSD) aims to detect objects of unseen classes with model trained on the seen classes. Formally, the class space C in ZSD is divided into seen classes Cs and unseen classes Cu, where C = Cs \u222a Cu and Cs \u2229 Cu = \u2205. The training set contains objects of seen classes, where each image I is provided with ground-truth class labels and bounding boxes coordinates. While the test set may contain only unseen objects (ZSD setting) or both seen and unseen classes (GZSD setting). During the training and testing, the semantic vectors W = {Ws,Wu} is provided for both seen and unseen classes."
        },
        {
            "heading": "3.2. Revisit standard DETR in object detection",
            "text": "To begin with, we review the pipeline of a standard DETR in generic object detection, which contains the following steps: set prediction, optimal bipartite matching and loss calculation."
        },
        {
            "heading": "3.2.1 Set prediction",
            "text": "For an image I , the global representation xI is extracted by the backbone f\u03d5 and Transformer encoder g\u03c8 successively, which can be expressed as:\nxI = g\u03c8(f\u03d5(I)) (1)\nThen, the decoder g\u03b8 infers N object predictions Y\u0302 , where N is determined by the number of object queries Q that serve as learnable positional embedding:\nY\u0302 = g\u03b8(xI ,Q) (2)\nwhere Y\u0302 = {(c\u0302i, b\u0302i)}Ni=1 and Q = {qi}Ni=1. For each object query qi, the decoder g\u03b8 will output a prediction box, which contains two parts: the predicted class c\u0302i and predicted box location b\u0302i."
        },
        {
            "heading": "3.2.2 Optimal bipartite matching",
            "text": "The optimal bipartite matching is to find the minimal-cost matching between the predictions Y\u0302 = {(c\u0302i, b\u0302i)}Ni=1 and ground-truth boxes Y = {(ci, bi)}Ni=1 (padded with no object \u2205). Therefore, we search for a permutation of N elements \u03c3 \u2208 SN with lowest cost:\n\u03c3\u0302 = argmin \u03c3\u2208SN N\u2211 i=1 [ Lcls(ci, c\u0302\u03c3i) + Lloc(bi, b\u0302\u03c3i) ] (3)\nwhere Lcls(ci, c\u0302\u03c3i) and Lloc(bi, b\u0302\u03c3i) are matching cost for class prediction and box location with index \u03c3i, respectively. Bipartite matching produces one-to-one assignments, where each prediction (c\u0302i, b\u0302i) is assigned to either a ground-truth box (ci, bi) or \u2205 (no object). The permutation for lowest cost is calculated with Hungarian algorithm."
        },
        {
            "heading": "3.2.3 Hungarian loss",
            "text": "Hungarian loss is a widely used loss function in DETR, which takes the following form:\nLHug = N\u2211 i=1 [ Lcls(ci, c\u0302\u03c3\u0302i) + 1{ci \u0338=\u2205}Lloc(bi, b\u0302\u03c3\u0302i) ] (4)\nwhere \u03c3\u0302 is the optimal assignment computed in Eq.(3). Lcls is the loss for classification, which usually takes the form of focal loss [24] or cross-entropy loss. Lloc is the location loss and usually contains l1 loss and GIoU loss [37].\nChallenge: Since the standard DETR can only locate the boxes and predict the classes of objects in training set, it is unable to detect unseen classes. In this paper, we utilize the meta-learning to make the model learn to detect objects according to the inputed semantic vectors, so that the model has the ability to detect objects of any category, as long as the semantic vector of the corresponding category is input."
        },
        {
            "heading": "3.3. Framework",
            "text": "We present the framework of Meta-ZSDETR in Fig. 2, which is based on Deformable DETR. Meta-ZSDETR follows the paradigm of meta-learning. The training is performed by episode based meta-learning task. In each episode, we randomly sample an image I and a class set C\u03c0 . The meta-learning task of each episode is to make the model learn to detect all appeared classes in C\u03c0 on image I . Specifically, the image feature is firstly extracted by backbone and Transformer encoder as in Eq.(1). In order for the decoder to detect categories in C\u03c0 , we add the projected semantic vectors of classes C\u03c0 to the object queries, making the queries class-specific. Then, the decoder takes the queries as input and predicts the class-specific boxes directly. To achieve this, the model is optimized with meta-contrastive learning, which contains a regression head to generate the\ncoordinates of class-specific boxes, a classification to predict the accuracy of generated boxes and a contrastive head that utilize the proposed contrastive-reconstruction loss to further separate different classes in visual space."
        },
        {
            "heading": "3.4. Meta-ZSDETR with class-specific queries",
            "text": "To enable the model to detect any unseen class, we fuse the object queries with class semantic information, and make the model learn to predict the bounding boxes for the fused classes. Such a process is carried out in each metalearning task.\nSpecifically, in each episode, we randomly sample a class set C\u03c0 and an image I , where the C\u03c0 satisfies C\u03c0 \u2286 Cs and each element is unique. Meanwhile, C\u03c0 = C+\u03c0 \u222a C\u2212\u03c0 , where C+\u03c0 is the positive classes that appeared in image I and C\u2212\u03c0 is the randomly sampled negative classes that do not appear in I for contrast. Meanwhile, we denote the size of C\u03c0 as L(C\u03c0), where L(\u00b7) is the operation that calculate size. The positive rate L(C + \u03c0 )\nL(C\u03c0) is set to \u03bb\u03c0 , which is a hyperparameter.\nThen, the corresponding semantic vectors W\u03c0 of class set C\u03c0 are projected from semantic space to the visual space with a linear layer hW :\nW\u0303\u03c0 = hW(W\u03c0) (5)\nwhere W\u0303\u03c0 is the projected semantic vectors of class set C\u03c0 . Since L(W\u0303\u03c0) \u226a N , i.e. the number of semantic vectors is smaller than the number of object queries Q, we expand W\u0303\u03c0 by duplicating each element in W\u0303\u03c0 for T times, which satisfies L(W\u0303\u03c0) \u00b7 T \u2a7e N and L(W\u0303\u03c0) \u00b7 (T \u2212 1) < N . For redundant elements more than N , we drop them.\nThen, the projected semantic vectors W\u0303\u03c0 is added to object queries Q as follows:\nQ\u03c0 = Q\u2295 W\u0303\u03c0 (6)\nwhere Q\u03c0 = {q\u03c0i }Ni=1 is the class-specific queries that will be inputed into the Transformer decoder g\u03b8 with image feature xI to generate predictions:\nY\u0302 = g\u03b8(xI ,Q\u03c0) (7)\nY\u0302 = {(\u03b4\u0302i, b\u0302i)}Ni=1 is the set of predictions, where b\u0302i is predicted box location generated with query q\u03c0i and \u03b4\u0302i is the probability of box b\u0302i belonging to the fused class, i.e. the class of semantic vector that fused to query q\u03c0i . Meanwhile, different from the standard DETR that the classification head determines the class of predicted boxes, the class of predicted box b\u0302i in Meta-ZSDETR is class-specific and is determined by the class of corresponding query and \u03b4\u0302i only has one dimension to represent the probability of b\u0302i belongs to the fused class."
        },
        {
            "heading": "3.5. Meta-contrastive learning",
            "text": "In order for the regression head of decoder g\u03b8 to generate more accurate class-specific box coordinate b\u0302i and classification head to have a stronger discriminative ability of further judging the location accuracy of generated b\u0302i, we propose the meta-contrastive learning to train the heads of decoder g\u03b8, i.e. a regression head to generate class-specific boxes, a classification head to filter inaccurate boxes, and moreover a contrastive head to further separate different classes in visual space, which will improve the performance of both seen classes and unseen classes.\nMeta-contrastive learning performs the matching and optimization in a class-by-class manner. As shown in Fig. 3, for each class c\u03c0j \u2208 C\u03c0 , the decoder takes queries of class c\u03c0j as input, and the predictions are split into three types: 1) The positive predictions that are assigned to GT box of class c\u03c0j by class-specific bipartite matching. 2) The negative predictions that are assigned to any other classes than c\u03c0j . 3) The negative predictions that belong to background.\nThen, we takes different combinations of three types\nof predictions to train three heads: a) For the classification head, we take all predictions for the training and give them corresponding positive/negative class-specific targets for class c\u03c0j . b) For the regression head, since the regression head aims to generate class-specific boxes of class c\u03c0j , we only utilize the positive predictions for optimization. c) For the contrastive head, since our intention is to separate different classes in visual space, we only use positive predictions of class c\u03c0j and negative predictions of other classes for contrastive-reconstruction loss."
        },
        {
            "heading": "3.5.1 Class-specific bipartite matching",
            "text": "Our meta-contrastive learning performs the matching in a class-by-class manner. For class c\u03c0j \u2208 C\u03c0 , the predictions generated by queries of class c\u03c0j are selected for bipartite matching, which is denoted as Y\u0302c\u03c0j = {(\u03b4\u0302\u03c4i , b\u0302\u03c4i)} T\u03c4 i=1, where {\u03c4i}T\u03c4i=1 are the indexes for queries of class c\u03c0j in N queries. Since each query is duplicated for T or T\u22121 times, we denote the length uniformly as T\u03c4 .\nThen, the ground-truth matching targets are revised for each class c\u03c0j \u2208 C\u03c0 . The class-specific matching labels are denoted as \u2206c\u03c0j = {\u03b4 c\u03c0j i } T\u03c4 i=1 (padded with no objects \u2205), which satisfy:\n\u03b4 c\u03c0j i =\n{ 1, ci = c \u03c0 j\n0, ci \u0338= c\u03c0j (8)\nwhere ci is the origin class label for ground-truth box bi. The revised matching labels are generated according to whether ci is equal to c\u03c0j . Then, the matching targets Yc\u03c0j = {(\u03b4 c\u03c0j i , bi)} T\u03c4 i=1 is utilized for bipartite matching of class c\u03c0j , where a permutation of T\u03c4 elements \u03c3 \u2208 ST\u03c4 with lowest cost is search as:\n\u03c3\u0302 = argmin \u03c3\u2208ST\u03c4 T\u03c4\u2211 i=1 [ Lcls(\u03b4 c\u03c0j i , \u03b4\u0302\u03c4\u03c3i ) + Lloc(bi, b\u0302\u03c4\u03c3i ) ] (9)\nwhere \u03c3\u0302 is the optimal assignment between matching targets Yc\u03c0j and predictions Y\u0302c\u03c0j . Lcls and Lloc are the same as that in Eq.(3)."
        },
        {
            "heading": "3.5.2 Loss function",
            "text": "Based on \u03c3\u0302, predictions in Y\u0302c\u03c0j are split into three types mentioned above, i.e. positive and two types of negative ones, and loss function of class c\u03c0j for three heads are calculated as follows: Lc\u03c0j = T\u03c4\u2211 i=1 [ Lcls(\u03b4 c\u03c0j i , \u03b4\u0302\u03c4\u03c3\u0302i ) + 1(ci=c \u03c0 j ) Lloc(bi, b\u0302\u03c4\u03c3\u0302i ) ] + Lcont\n(10) where the loss Lc\u03c0j for class c \u03c0 j is composed of classification loss Lcls, regression loss Lloc and contrastivereconstruction loss Lcont.\nClassification loss. Lcls takes all predictions as input and makes the model learn to distinguish whether a predicted box belongs to c\u03c0j . The predicted box has the label \u03b4 c\u03c0j i = 1 if and only if it is assigned to a ground-truth box with class label c\u03c0j . We implement Lcls with focal loss [24]. Regression loss. Since our intention is to generate classspecific boxes, i.e. input decoder with query of class c\u03c0j and output box of class c\u03c0j , we only select the ground-truth box with class label c\u03c0j for optimization, i.e. \u03b4 c\u03c0j i = 1, making the predicted boxes to be closer to GT boxes with class label c\u03c0j . The Lloc is implement with l1 loss and GIoU loss [37].\nContrastive-reconstruction loss. In ZSD tasks, the original visual space is not well-structured due to the lack of discriminative information, many previous works solved it by introducing a reconstruction loss to guide the distribution of visual features. Here, we combine the reconstruction loss and contrastive loss [16, 45, 44] to bring a higher intraclass compactness and inter-class separability of the visual structure.\nIn particular, we project the last hidden features of decoder to the semantic space, where the projected hidden features of positive predictions are constraint to be as close as possible to \u03c9\u03c0j , which is the semantic vector of class c \u03c0 j , and the negative ones are constraint to be as far as possible to \u03c9\u03c0j . Formally, we denote the last hidden feature of optimal box (\u03b4\u0302\u03c4\u03c3\u0302i , b\u0302\u03c4\u03c3\u0302i ) that matched to GT box (ci, bi) as z\u03c4\u03c3\u0302k and the Lcont is formulated as:\nLcont = 1\nNpos T\u03c4\u2211 i=1 1(ci=c\u03c0j ) Lcont(z\u03c4\u03c3\u0302i ) (11)\nLcont(z\u03c4\u03c3\u0302i ) = \u2212 log exp[h\u03c1(z\u03c4\u03c3\u0302i ) \u00b7 \u03c9 \u03c0 j /\u03ba]\u2211T\u03c4\nk=1 1(ck \u0338=\u2205) exp[h\u03c1(z\u03c4\u03c3\u0302k ) \u00b7 \u03c9 \u03c0 j /\u03ba] (12)\nwhere h\u03c1 is a linear layer that project the hidden feature to the semantic space. Npos is the number of positive predictions of class c\u03c0j . \u03ba is a temperature hyper-parameter as in InfoNCE [30]. The optimization of the above loss function increases the instance-level similarity between projected hidden features of positive predictions with semantic vector \u03c9\u03c0j and space the negative ones. As a result, visual features of the same class will form a tighter cluster.\nTotal loss function. We compute the loss function with Eq.(10) for each class c\u03c0j \u2208 C\u03c0 , separately. For negative classes C\u2212\u03c0 , only classification loss is calculated. Finally, the loss of current episode is averaged over all classes in sampled class set C\u03c0:\nL = 1 L(C\u03c0) L(C\u03c0)\u2211 j=1 Lc\u03c0j (13)\nwhere L(C\u03c0) is the number of classes in C\u03c0 . We utilize L for model optimization."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Datasets and splits",
            "text": "Following previous works [42, 18], we takes two benchmark datasets for evaluation: PASCAL VOC 2007+2012 [9] and MS COCO 2014 [25].\nDatasets: PASCAL VOC contains 20 classes of objects for object detection. More specifically, the PASCAL VOC 2,007 is composed of 2,501 training images, 2,510 validation images, and 5,011 test images. The PASCAL VOC 2012 dataset comprises 5,717 training images and 5,823 validation images, without test images released. MS COCO 2014 is a benchmark dataset designed for object detection and semantic segmentation tasks, which contains 82,783 training and 40,504 validation images from 80 categories. For PASCAL VOC and MS COCO, we adopt the FastText [28] to extract the semantic vectors following [14, 18].\nSeen/unseen splits: All splits follow the previous setting [42, 18]. For PASCAL VOC, we use a 16/4 split proposed in [7]. For MS COCO, we follow the same procedures described in [2, 31, 18] to take 2 different splits, i.e. 48/17 and 65/15. For all datasets, the images that contains unseen classes in the training set are removed to guarantee that unseen objects will not be available during training."
        },
        {
            "heading": "4.2. Evaluation protocols",
            "text": "We adopt the widely-used evaluation protocols proposed in [2, 7]. For PASCAL VOC, mAP with IoU threshold 0.5 is used to evaluate the performance. For MS COCO, mAP and recall@100 with three different IoU threshold (i.e. 0.4,0.5 and 0.6) are utilized for evaluation. For GZSD setting that contains both seen and unseen classes, the performance is evaluated by Harmonic Mean (HM)."
        },
        {
            "heading": "4.3. Implementation details",
            "text": "We build Meta-ZSDETR on Deformable DETR [53] with ResNet-50 [17] as backbone. The number of queries N is set to 900. For the sampled class set C\u03c0 , the positive classes consist of the classes that appear in the image and the negative classes is sampled from Cs. The positive rate \u03bb\u03c0 is set to 0.5. The number of Transformer encoder layers and decoder layers is set to 6. The temperature hyperparameter \u03ba in Eq.(12) is set to 0.2. We train our model for total 500,000 iterations with batch size 16, i.e. each iteration contains 16 episodes in parallel. Following Deformable DETR, different coefficients are utilized to weight different loss functions, where 1.0 is used for classification loss, 5.0 is used for l1 loss of regression head, 2.0 is used for GIoU loss of regression head and 1.0 is used for contrastivereconstruction loss. More details can refer to our code."
        },
        {
            "heading": "4.4. Comparison with existing methods",
            "text": ""
        },
        {
            "heading": "4.4.1 PASCAL VOC",
            "text": "We present the results of PASCAL VOC in Tab. 1, where we can see that our method performs best among all existing methods under both ZSD and GZSD settings, and lift the\nmAP in PASCAL VOC to a higher level. Specifically, in ZSD setting, Meta-ZSDETR achieves 70.3 mAP and outperform the second-best model ContrastZSD [42] by a large margin of 4.6 mAP, which is the first time to boost the performance of ZSD setting on PASCAL VOC to over 70 mAP.\nFor GZSD setting, our method also achieves SOTAs in all three metrics, i.e. mAP on seen classes, unseen classes and harmonic mean, which brings about 4.4, 9.8 and 7.8 points improvement, respectively. It is worth noting that the improvement of our method on unseen classes in GZSD setting is extremely large, which proves that our method has a strong generalization on unseen classes, and can alleviate the problem that the unseen classes tend to be misclassified into seen classes to a certain extent. We also report class-wise mAP in ZSD setting in Tab. 2, where our method achieves the best performance on 3 classes."
        },
        {
            "heading": "4.4.2 MS COCO",
            "text": "We perform experiments on MS COCO, where the results of ZSD setting is shown in Tab. 3 and the results of GZSD setting is shown in Tab. 4. We can see that Meta-ZSDETR achieves the best results in all metrics under all settings.\nFor ZSD setting, we can see that mAP of our method in 48/17 and 65/15 splits outperforms the second-best by a margin of 1.7 and 2.7 mAP, respectively, which demonstrates that our method generalizes well to unseen classes. Meanwhile, we can see that Recall@100 decrease as the IoU increases in all methods. Compared with other methods, Meta-ZSDETR has a smaller drop, which is benefit\nMethod Split Recall@100 mAP\nS U HM S U HM\nPL [32] 48/17 38.2 26.3 31.2 35.9 4.1 7.4 BLC [50] 48/17 57.6 46.4 51.4 42.1 4.5 8.2 ZSDTR [48] 48/17 74.3 48.4 60.5 48.5 5.6 9.5 Robust-Syn [18] 48/17 59.7 58.8 59.2 42.3 13.4 20.4 ContrastZSD [42] 48/17 65.7 52.4 58.3 45.1 6.3 11.1 Meta-ZSDETR 48/17 74.3 59.0 65.8 48.7 14.6 22.5\nPL [32] 65/15 36.4 37.2 36.8 34.1 12.4 18.2 BLC [50] 65/15 56.4 51.7 53.9 36.0 13.1 19.2 SU [15] 65/15 57.7 53.9 55.8 36.9 19.0 25.1 ZSDTR [48] 65/15 69.1 59.5 61.1 40.6 13.2 20.2 Robust-Syn [18] 65/15 58.6 61.8 60.2 37.4 19.8 26.0 ContrastZSD [42] 65/15 62.9 58.6 60.7 40.2 16.5 23.4 Meta-ZSDETR 65/15 71.1 65.4 68.1 45.9 21.7 29.5\nTable 4. GZSD performance of Recall@100 and mAP with IoU=0.5 on MS COCO dataset.\nLloc Lcls Lcont Seen Unseen \u2713 39.9 14.5 \u2713 \u2713 44.8 20.6 \u2713 \u2713 40.6 15.3 \u2713 \u2713 \u2713 45.9 21.7\nTable 5. Ablation study of different combinations of loss functions.\nfrom that our decoder can generate more accurate boxes with class semantic information as input.\nFor GZSD setting, our method achieves SOTAs in both seen and unseen classes. The harmonic means of mAP under 48/17 and 65/15 splits are improved from 20.4 to 22.5, and from 26.0 to 29.5, demonstrating the effectiveness and superiority of our method. Meanwhile, the Recall@100 also improves due to the powerful class-specific boxes generation capabilities. We also report the class-wise AP in 65/15 split of MS COCO, which can be found in our supplementary material."
        },
        {
            "heading": "4.5. Ablation study",
            "text": "We analyze the effects of various components in MetaZSDETR. Unless otherwise specified, the experiments are carried out on MS COCO with 65/15 split under GZSD setting and use mAP with IoU=0.5 as metric.\nEffects of different loss functions. Here, we analyze the effects of three loss functions in meta-contrastive learning. We utilize different combinations of regression loss Lloc, classification loss Lcls and contrastive-reconstruction loss Lcont to optimize the model, and show the results in Tab. 5. Since the regression loss is necessary, we keep it for all combinations. For model without classification\nloss Lcls, we do not perform the boxes filter and directly use the class-specific boxes predicted from regression head, where the scores are generated randomly. As we can see, if the model is only trained with regression head to generate class-specific boxes, it can achieve a mAP of 14.5 in unseen classes, which is relatively low, but also surpasses many previous methods. Adding the classification loss will greatly boost the performance of unseen classes to 20.6 mAP, which thanks to the powerful discriminative ability of the classification head that can filter inaccurate boxes. Meanwhile, the contrastive-reconstruction loss can improve the performance with and without the classification head about 1 point. Finally, the combination of three losses achieves the best performance.\nStudy for training of different heads. As describe, based on the class-specific bipartite matching for class c\u03c0j , all predictions are split into three different types: the positive predictions Y\u0302pos assigned to GT boxes of c\u03c0j , the negative predictions Y\u0302other assigned to other classes and the negative predictions Y\u0302bg that belong to background. Here, we study different combinations of them to train three heads and the results are shown in Tab. 6. We can see that: 1) For classification head, since it aims to filter all kinds of negative predictions, using all predictions to train it can achieve the best performance. 2) For regression head, if we train it with GT boxes of all classes, i.e. using Y\u0302pos and Y\u0302other, the regression head will degenerate into a class-agnostic RPN, which will greatly reduce the recall of unseen classes, thus lead to a lower mAP. 3) For contrastive head, on one hand, if we only use Y\u0302pos for training, it will degenerate into a reconstruction loss, which has been widely used in previous works and it will bring a 0.4 mAP improvement in unseen classes compared with the version without it. On the other hand, compared with using all predictions, removing background predictions will make the contrastive head focus on distinguishing Y\u0302other and inputed semantic vectors of class c\u03c0j , thus brings more improvement.\nVisualization for contrastive-reconstruction loss. Here, we study the influence of contrastive-reconstruction\nloss on visual space by visualizing the distribution of hidden features with t-SNE. We visualize the last hidden features of decoder in unseen classes of PASCAL VOC. The result is shown in Fig. 4. As we can see, our contrastivereconstruction loss can further separate different classes in visual space and bring a higher intra-class compactness and inter-class separability of the visual structure.\nEffect of number of queries and positive rate. We study the effect of number of queries N and positive rate \u03bb\u03c0 of sampled class set C\u03c0 . We found that \u03bb\u03c0 have different influence under different number of queries N . We change the positive rate \u03bb\u03c0 in different settings of N and report the mAP of converged model in unseen classes. In each episode, we control \u03bb\u03c0 by sampling different number of negative classes. All models are trained for 500,000 episodes. The results are shown in Fig. 5. As we can see, a larger N tends to have a better performance due to a higher recall, and of course a higher amount of calculation. Meanwhile, when N is small (e.g. 100), a small positive rate will greatly reduce the amount of positive queries for training, thereby reducing the model performance. When N is large (e.g. 900), the number of positive queries is guaranteed and more negative queries are need for the classification head to learn to distinguish among them. Therefore, we can see the best performance is achieved when \u03bb\u03c0 is 0.5 and N is 900."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we present the first work that combine DETR and meta-learning to perform zero-shot object detection, which formalize the training as individual episode based meta-learning task. In each episode, we randomly sample an image and a class set. The meta-learning task is to make the model learn to detect all appeared classes of the sampled class set on the image. To achieve this, we train the decoder to directly predict class-specific boxes with classspecific queries as input, under the supervision of our metacontrastive learning that contains three different heads. We conduct extensive experiments on the benchmark datasets MSCOCO and PASCAL VOC. Experimental results show that our method outperforms the existing ZSD methods. In the future, we will focus on further performance improvement.\nAcknowledgement. Jihong Guan was supported by National Natural Science Foundation of China (NSFC) under grant No. U1936205."
        }
    ],
    "title": "Meta-ZSDETR: Zero-shot DETR with Meta-learning",
    "year": 2023
}