{
    "abstractText": "Recently significant progress has been made in human action recognition and behavior prediction using deep learning techniques, leading to improved vision-based semantic understanding. However, there is still a lack of high-quality motion datasets for small bio-robotics, which presents more challenging scenarios for long-term movement prediction and behavior control based on third-person observation. In this study, we introduce RatPose, a bio-robot motion prediction dataset constructed by considering the influence factors of individuals and environments based on predefined annotation rules. To enhance the robustness of motion prediction against these factors, we propose a Dual-stream Motion-Scenario Decoupling (DMSD) framework that effectively separates scenario-oriented and motion-oriented features and designs a scenario contrast loss and motion clustering loss for overall training. With such distinctive architecture, the dual-branch feature flow information is interacted and compensated in a decomposition-then-fusion manner. Moreover, we demonstrate significant performance improvements of the proposed DMSD framework on different difficulty-level tasks. We also implement long-term discretized trajectory prediction tasks to verify the generalization ability of the proposed dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaofeng Liu"
        },
        {
            "affiliations": [],
            "name": "Jiaxin Gao"
        },
        {
            "affiliations": [],
            "name": "Yaohua Liu"
        },
        {
            "affiliations": [],
            "name": "Nenggan Zheng"
        },
        {
            "affiliations": [],
            "name": "Risheng Liu"
        }
    ],
    "id": "SP:bf9953924457739f5b1c05524de2205d213340cb",
    "references": [
        {
            "authors": [
                "Jonathan R Wolpaw",
                "Niels Birbaumer",
                "William J Heetderks",
                "Dennis J McFarland",
                "P Hunter Peckham",
                "Gerwin Schalk",
                "Emanuel Donchin",
                "Louis A Quatrano",
                "Charles J Robinson",
                "Theresa M Vaughan"
            ],
            "title": "Brain-computer interface technology: a review of the first international meeting",
            "venue": "IEEE transactions on rehabilitation engineering,",
            "year": 2000
        },
        {
            "authors": [
                "Luis Fernando Nicolas-Alonso",
                "Jaime Gomez-Gil"
            ],
            "title": "Brain computer interfaces, a review",
            "year": 2012
        },
        {
            "authors": [
                "Arunabha M Roy"
            ],
            "title": "Adaptive transfer learning-based multiscale feature fused deep convolutional neural network for eeg mi multiclassification in brain\u2013computer interface",
            "venue": "Engineering Applications of Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Zhang",
                "Yuanqing Li",
                "Yongyong Yan",
                "Hao Zhang",
                "Shaoyu Wu",
                "Tianyou Yu",
                "Zhenghui Gu"
            ],
            "title": "Control of a wheelchair in an indoor environment based on a brain\u2013computer interface and automated navigation",
            "venue": "IEEE transactions on neural systems and rehabilitation engineering,",
            "year": 2015
        },
        {
            "authors": [
                "Akshansh Gupta",
                "RK Agrawal",
                "Jyoti Singh Kirar",
                "Baljeet Kaur",
                "Weiping Ding",
                "Chin-Teng Lin",
                "Javier Andreu-Perez",
                "Mukesh Prasad"
            ],
            "title": "A hierarchical metamodel for multi-class mental task based brain-computer",
            "venue": "interfaces. Neurocomputing,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas B Moeslund",
                "Adrian Hilton",
                "Volker Kr\u00fcger",
                "Leonid Sigal"
            ],
            "title": "Visual analysis of humans",
            "year": 2011
        },
        {
            "authors": [
                "Reinhard Klette",
                "Dimitris N Metaxas",
                "Bodo Rosenhahn"
            ],
            "title": "Human Motion: Understanding, Modelling, Capture, and Animation",
            "year": 2008
        },
        {
            "authors": [
                "Catalin Ionescu",
                "Dragos Papava",
                "Vlad Olaru",
                "Cristian Sminchisescu"
            ],
            "title": "Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Hueihan Jhuang",
                "Estibaliz Garrote",
                "Xinlin Yu",
                "Vinita Khilnani",
                "Tomaso Poggio",
                "Andrew D Steele",
                "Thomas Serre"
            ],
            "title": "Automated home-cage behavioural phenotyping of mice",
            "venue": "Nature communications,",
            "year": 2010
        },
        {
            "authors": [
                "Raghav Goyal",
                "Samira Ebrahimi Kahou",
                "Vincent Michalski",
                "Joanna Materzynska",
                "Susanne Westphal",
                "Heuna Kim",
                "Valentin Haenel",
                "Ingo Fruend",
                "Peter Yianilos",
                "Moritz Mueller-Freitag"
            ],
            "title": "The\" something something\" video database for learning and evaluating visual common sense",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammadreza Zolfaghari",
                "Kamaljeet Singh",
                "Thomas Brox"
            ],
            "title": "Eco: Efficient convolutional network for online video understanding",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Alex Andonian",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Temporal relational reasoning in videos",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Alex Andonian",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Temporal relational reasoning in videos",
            "venue": "European Conference on Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "Ji Lin",
                "Chuang Gan",
                "Song Han"
            ],
            "title": "Tsm: Temporal shift module for efficient video understanding",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Luca Franceschi",
                "Michele Donini",
                "Paolo Frasconi",
                "Massimiliano Pontil"
            ],
            "title": "Forward and reverse gradient-based hyperparameter optimization",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Risheng Liu",
                "Pan Mu",
                "Xiaoming Yuan",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "A general descent aggregation framework for gradient-based bi-level optimization",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Risheng Liu",
                "Yaohua Liu",
                "Shangzhi Zeng",
                "Jin Zhang"
            ],
            "title": "Augmenting iterative trajectory for bilevel optimization: Methodology, analysis and extensions, 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Keywords: Rat Dataset \u00b7 Feature Decoupling \u00b7 Video Prediction."
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, significant advancements in neuroscience and biomedical engineering have facilitated the development of Brain-Computer Interface (BCI) [1,2,3], which offer potential benefits for rehabilitating neuromotor disorder [4] and controlling small animals [5] such as rats, beetles and doves. These animals endowed with autonomous intelligence and dexterous and agile bodies are allowed to perform special military missions or emergency rescues after natural disasters where human intervention is difficult. However, the intention of animals and the uncertainty associated with changes in the environment and individual organisms pose significant challenges for effective manipulation by humans. Therefore, it is of\nar X\niv :2\n30 5.\n18 31\n0v 2\n[ cs\n.C V\n] 2\n1 Ju\ngreat significance to acquire high-quality movement data and perform a comprehensive analysis of robotic movement behavior based on varying environmental and individual factors to regulate biological behavior.\nThe inference and prediction of human motion [6,7] based on visual observation has been an extensively researched topic. Representative works include the HUMAN3.6M dataset [8], which predicts human pose, and the LMBRD dataset [9], which recognizes different behaviors of housed mice. Additionally, the \u2019something something\u2019 dataset [10] is proposed for human-object interaction, which predicts actions performed by humans with respect to different objects. These datasets use videos to capture the meta-action and interaction between the target creatures and their surrounding environment, leading to improved predictions and better understanding of their behavior. However, these datasets generally focus on the current specific actions of organisms, without considering the movement outcomes in the future, which limits the exploration of the organisms\u2019 moving motivation.\nDeep learning based action recognition methods [11,12] and large-scale video datasets have made significant progress in recent years, and different architectures or modules are proposed to improve the temporal modeling capabilities of deep learning models. TRN [13] introduces a temporal relation network module to learn and reason about temporal dependencies between video frames at multiple time scales. Similarly, TIN [14] improves the temporal feature extraction based on TSM and proposes a temporal interlacing operator to fuse the temporal and spatial information. While these methods have shown some success in action recognition, their limitations in solely focusing on the temporal and spatial features between adjacent frames, without taking into account environmental and biological factors present in the video, thus hinder their performance in predicting the future movement of rats with high levels of uncertainty."
        },
        {
            "heading": "1.1 Our Contribution",
            "text": "To boost the investigation on motion behavior analysis of bio-robots and further research of behavioral control prototype incorporating animal moving intention, we first build a dataset RatPose which focuses on the bio-robot\u2019s movement prediction based on collected third-person video sequences. The movement data is collected in consideration of influence factors including varying individuals and different environments (e.g., Open Field and Maze), covering up to 5 scenarios, 6 individuals and 1023 data pairs. We also conduct detailed analysis of the individuals\u2019 differences and how it influences the performance of bio-robot\u2019s motion prediction. Based on the above analysis, we propose a Dual-stream Motion-Scenario Decoupling (DMSD) framework to effectively decompose the scenario-oriented and motion-oriented features. Then we propose a motion clustering loss to measure the distance between features and the nearest simulated cluster center and combine the scenario contrast loss to construct the overall loss function. Finally, we demonstrate the significant performance improvement of the proposed DMSD framework on single scenario and multiple scenarios motion prediction tasks. Furthermore, we implement our method to tackle more challenging discretized\ntrajectory prediction based on long-term video sequences to show its generalization performance over different individuals and environments. In summary, our contributions can be summarized as follows:\n\u2013 Based on predefined annotation rules, we construct the first bio-robot\u2019s motion prediction dataset, namely RatPose, with full consideration of varying individuals and environments, and conduct comprehensive analysis of the influence of individuals and prediction intervals for proper task settings, which contributes to further research on the behavioral control prototype incorporating all kinds of uncertainties. \u2013 We propose a Dual-stream Motion-Scenario Decoupling (DMSD) framework to decompose the scenario-oriented and motion-oriented features, and design a novel motion clustering loss together with the scenario contrast loss to facilitate the robustness over varying environments and states of individuals. \u2013 Extensive experiments under difference difficulty levels and ablation studies have demonstrate the effectiveness over existing methods (34.3% and 29.6% relative improvement of Top 1 accuracy under single and multiple scenario, respectively). We also verify the generalization performance of this framework on more challenging discretized trajectory prediction tasks."
        },
        {
            "heading": "2 Problem Set-up",
            "text": "We focus on the long-term movements of rats to monitor their moving intentions, rather than their specific detailed actions, from an overhead perspective. To approximately estimate the intention, we aim to predict the motion patterns of rats and model it into a classification problem, which has potential applications in various fields, including bio-robot controlling."
        },
        {
            "heading": "2.1 Basic Settings",
            "text": "In this study, we address a unique problem that differs from the conventional action recognition tasks. While action recognition typically focuses on identifying what a human is doing or how they are interacting with objects, our goal is to predict the future moving position of rats. Unlike humans, animals do not always act logically or rationally, making this problem more challenging. Moreover, we use a different labeling strategy that is based on the outcome of the motion rather than the motion itself.\nIn our problem, we aim to classify motion results into five categories, namely top, down, left, right, and middle, as illustrated in Fig. 1. To determine the label of an object, we examine the distance it has moved relative to its body length. If the object moves a short distance, less than a predefined proportion of its body length, we label it as middle, indicating that it has stayed on the ground. Otherwise, we determine the label based on the direction of the body center\u2019s movement in pixel level. Specifically, we use \u03b8 to denote the counterclockwise angle between the width axis of the image and the motion vector after a short\ntime t, \u03c1 to denote the pixel length of the motion vector, and r to denote the predefined length in pixel level. Then we can define a set of the predicted motions: M = {up : \u03c1 > r, 34\u03c0 < \u03b8 + 2k\u03c0 < 5 4\u03c0; left : \u03c1 > r, 5 4\u03c0 < \u03b8 + 2k\u03c0 < 7 4\u03c0; down : \u03c1 > r, 74\u03c0 < \u03b8 + 2k\u03c0 < 9 4\u03c0; right : \u03c1 > r, 9 4\u03c0 < \u03b8 + 2k\u03c0 < 11 4 \u03c0;middle : \u03c1 < r}, where k \u2208 Z. Here we set t = 3s and r is 1/10 of the pixel length of the image. Our problem can be characterized as a classification task with a unique focus and approach. In addition, our approach enables us to extract valuable insights from redundant video data in biological experiments, which may not be suitable for traditional action recognition tasks."
        },
        {
            "heading": "2.2 Advanced settings",
            "text": "Predicting the motion patterns of animals is a challenging task due to the unpredictable nature of their movements. A range of factors, including environmental, biological, and psychological aspects, contribute to the significant variance in animal motion outcomes. Therefore, it is crucial to consider these factors when developing a solution. In this study, we aim to address this issue by taking into account the visual differences between various scenarios and visual perception difficulties. To achieve this goal, we propose three different problem settings: a single scenario, multiple scenarios, and challenging scenarios ranging from easy to hard, as Fig. 1 reveals. The first problem setting involves collecting data from a few rats in a single scenario, while the second problem setting involves collecting data from a few rats in various scenarios. Finally, the third problem setting involves collecting data in a single scenario where the visual images are affected by simulated smoke using a mask placed on the camera. Except the visual challenge, there only exists 1 video per class for training or finetune in the challenging setting. By incorporating these settings, we hope to develop a comprehensive understanding of animal motion patterns and contribute to the advancement of related research fields."
        },
        {
            "heading": "3 The RatPose Dataset",
            "text": "To better understanding rat\u2019s behaviors from visual perspective from a wide range of diverse environments and generalize to new settings, we propose RatPose, a dataset for sharing different rats\u2019 moving experience in various scenarios."
        },
        {
            "heading": "3.1 Data Collection Process",
            "text": "Visual data was collected using an overhead camera to capture a vertical view of the environment, as illustrated in Fig. 2. To enhance the diversity of the data, videos were recorded of rats in various environments, including a simple maze with eight arms (Maze), a laboratory simulation of the real environment (Labsim), and an open field environment in a gym (Open Field). In order to expand the range of scenarios and increase the variability of the data, we considered the open-field environment as the baseline and introduced variations such as adding grass as the ground or placing toys as barriers."
        },
        {
            "heading": "3.2 Dataset Features",
            "text": "RatPose is full of challenging, and the challenging mainly comes from the natural insurance affected by many factors. We primarily consider the influences caused by individual differences, and differences in individual states. In order to demonstrate individual differences involved in modeling rat locomotor behavior, we utilize video data collected from six distinct rats. As shown in 3, the dataset is randomly partitioned into training and testing sets based on the individual rat, and a baseline model is trained using the training set and evaluated on multiple testing sets. Analysis of the resulting correlation matrix indicates that the model performs well in predicting the behavior of the corresponding rat, but struggles\nto generalize to new rats. And in the right subfigure, although the video data is collected with only a one-day time interval, there are notable variations in the state of the rat under observation. The observed heterogeneity in the rat\u2019s condition results in a substantial discrepancy between the predicted and actual outcomes of its behavioral actions. These features suggest when facing the situation lack of sufficient data, model is fragile when meeting data full of diversity, especially in the situation that test data is not identically distributed."
        },
        {
            "heading": "4 The Proposed Method",
            "text": "This section describes the workflow of the Dual-stream Motion-Scenario Decoupling (DMSD) framework and the training loss. The overall architecture is shown in Fig. 4, mainly including three parts: the pre-decoupling operator, deep feature extractor, and the future motion predictor."
        },
        {
            "heading": "4.1 Dual-stream Motion-Scenario Decoupling (DMSD)",
            "text": "As illustrated, pre-decoupling operator contains Motion Removal Module (MRM) denoted by \u03d5 and Scenario Removal Module (SRM) as \u03c8 to get the scenario relative input u and the motion relative input v. We then input the parallel pre-decoupling terms u and v into the dual feature extractor to obtain the deep scenario features s and the deep motion features m for prediction, with the guidance of scenario contrast loss and motion clustering loss. The pre-decoupling operator and deep feature extractor compose a dual branch, jointly parameterized by \u03b8 and serves as the backbone to extract features. The future motion\npredictor uses temporal shift head 3 denoted as M to fuse and output the action vector to predict the motion probability y. The process can be formulated as\ny =M ( (N(x\u2212 \u03d5(\u2207x)), N(x\u2212 \u03c8(x))) ) , (1)\nwhere x = {It|t = 0, 1, 2, 3...} presents the input video sequence and \u2207x = {It \u2212 I0|t = 0, 1, 2, 3...} presents the differences among x, indicating movement of the target.\nPre-decoupling Operator. To decompose the mixture components in a natural manner, we initially use MRM and SRM to decouple the input image sequences into different terms v and u. The MRM module is compromised by a convolution layer designed to expand the dimension, two feature shift module dedicated to recombining features and feature reweighting, and a convolution layer intended to reduce the dimension. We apply MRM module \u03d5 on \u2207x and a residual-like subtraction operation to obtain the scenario relative term u. We get this term by removing the motion component from the input sequences, enabling us to extract scenario features effectively. The SRM module shares the same network architecture with MRM while it has different parameters and input. We apply SRM module \u03c8 directly on x to get a general representation of the input scenario. Similarly, we use the subtraction operator as well as SRM to remove scenario information and obtain the motion relative term v by removing the background component from the original input. As shown in Figure 4, for input sequence x, u present the scenario information and show the viable domain for rat\u2019s movement as the unreachable part is filled in black. Meanwhile, v represents 3 See work [15] for the detailed implementation of temporal shift head.\nthe movement of the rat in pixel space, with its spatial information. By doing so, we pre-decouple the input sequence into different terms concentrated on different factors.\nDeep Feature Extractor. In addition to the pre-decoupling function, we use deep feature extractor for deep feature extraction and better factors decoupling. The deep feature extractor the deep feature extractor contains two temporal shift resnet 4 denoted as N to extract and fuse the temporal and spatial features of u and v. Besides, we propose the scenario contrast loss and motion clustering loss 5 to better detect the correlations among data and decouple the coupling relationship between s and m.\nFuture Motion Predictor. After applying the dual branch, we utilize the Future Motion Predictor to fuse s and m features and generate the predicted output y. Future Motion Predictor facilitates the integration and decoding of temporal and spatial features, alongside motion and scenario features, to yield robust and effective prediction outcomes. Notably, our approach demonstrate remarkable resilience to changes in environmental perspectives, with a prediction accuracy of up to 48.8% even under such conditions."
        },
        {
            "heading": "4.2 Loss Function",
            "text": "We propose several losses to explore the similarity of action behaviors in different scenarios and the differences in scenario factors under different states. Among them, we optimize both dual-branch network\u2019s parameters \u03b8 and the clustering centers r using the feature decoupling loss Lf , which is expressed as follows:\nLf = \u03bbs \u00b7 Lsc + \u03bbm \u00b7 Lmc, (2)\nwhere Lsc and Lmc are the proposed scenario contrast loss and motion clustering loss. \u03bbs and \u03bbm are the corresponding weights. In our training, we empirically set \u03bbs = 0.1 and \u03bbm = 1. In addition, we introduce the classification loss Lcls optimize the whole networks parameters \u03b8 and \u03c9. The process of optimization is carried out alternately in every iteration step.\nScenario Contrast Loss: In order to increase the sensitivity of the scene feature representation to slight changes in the environment, we utilize the technique of contrastive learning. We consider videos captured in the same scenario at the same time as positive samples, denoted as s+, while all others are considered as negative samples, denoted as s\u2212. Drawing inspiration from contrastive learning for visual representations [17], we formulate scenario contrast loss as:\nLsc = \u2212 log exp(sim(s, s+))\u2211\ns\u2032\u2208s\u2212 exp(sim(s, s \u2032)) . (3)\nMotion Clustering Loss: We employ a series of trainable representation clustering centers rc with label c. To measure the distance between each feature 4 See work [15,16] for the detailed implementation of temporal shift resnet. 5 We provide more details about the proposed scenario contrast loss and motion clus-\ntering loss in section 4.2.\nand the nearest cluster center, we calculate the minimum L2 distance between the feature and the center as the nearest auxiliary distance:\nD(m|c) = min r ||m\u2212 r||2 s.t. r \u2208 rc. (4)\nNext, we use the softmax function on the opposite number of the nearest auxiliary distance between each class feature center as the probability and apply cross-entropy to obtain the final motion clustering loss. This approach allows us to learn distributions of feature representations and obtain better decision boundaries:\nLmc = CE( e\u2212D(m|z)\u2211 k e \u2212D(m|z) , z). (5)\nHere, z denotes the ground truth label, and CE denotes the cross-entropy loss."
        },
        {
            "heading": "5 Experiments",
            "text": "In this study, all models are trained and tested on a server equipped with an Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz and an NVIDIA A40 GPU. The training process is conducted using the mmaction framework [18] with default training hyper-parameters. We adopt a sampling strategy where every 8th frame of the video is selected and resized to 224\u00d7 224. For each video, we sample the last 8 frames according to this rule. For longer video online prediction, we predict the position distribution every 3 seconds for the subsequent 3 seconds."
        },
        {
            "heading": "5.1 Quantitative Evaluation",
            "text": "In order to evaluate the effectiveness of our proposed method, we conducted experiments on both single and multiple scenarios tasks, as table 1 reveals. To demonstrate the superiority of our approach, we performed a quantitative comparison with four state-of-the-art methods, namely TRN [13], TSM [15], TIN [14], and TSF [19]. The experimental results clearly indicate that our method achieves significantly better performance than the other methods in terms of both mean class accuracy and top-1-accuracy, providing strong evidence for the effectiveness of our proposed approach.\nWe conduct ablation experiments on our auxiliary loss, as table 2 reveals. The S1 approach represents the network trained solely with the classification loss, while S2 and S3 show the results obtained by integrating the Scenario Contrast and the Motion Clustering Loss, respectively. Accs and Accm represent the top-1 accuracy in single scenario and multiple scenarios respectively. To train the approaches other than S1 an iterative joint training strategy was applied.\nTo demonstrate the efficacy of our training strategy, we conducted a comprehensive comparative analysis against several state-of-the-art few shot learning techniques, including RHG [20], BDA [21], and IAPTT [22]. In this context, we regarded the feature extraction layer as an upper-level challenge while considering the subsequent classification layer as a lower-level problem. Employing a bilevel optimization approach facilitated the seamless acquisition of a robust universal feature extraction module. As evident from the results presented in Table 3, traditional methods for few-shot learning struggle to acquire a viable representation and, in some cases, even perform worse than direct training. In stark contrast, our proposed approach not only surpasses the performance of other methods but also significantly enhances the final outcome. The superiority of our technique in capturing essential features and its ability to adapt to limited training data highlight its potential as a groundbreaking solution in the field of few-shot learning.\nWe observe that utilizing the motion clustering loss and scenario contrast loss resulted in significant improvements in one or more of the evaluation metrics presented in the table. Furthermore, the combined use of these two losses during training leads to a substantial increase in the performance of the final model, particularly in a single scenario setting, where the top-1 accuracy is improved by approximately 52% compared to the basic dual-branch model. This indicates that the motion clustering loss and scenario contrast loss are effective in improving the overall performance of the model."
        },
        {
            "heading": "5.2 Qualitative Evaluation",
            "text": "To illustrate the generalizability and robustness of our method, we conduct qualitative evaluation employ several highly challenging video segments for practical experimentation. Notably, the data in these segments differs not only in the individual rats used for data collection but also in the camera angles employed for filming, as compared to those present in the RatPose training set.\nAs depicted in Fig. 5, the first row of the figure shows a difference between the shooting angle of the video footage and the angle of the training data. Despite this slight visual discrepancy, the prediction accuracy of TRN and other methods considerably decline. In contrast, our method is found to be robust to changes in shooting angles, as its prediction accuracy remained unaffected.\nAdditionally, the rats in the second row of the figure display more vigorous movements than those in the dataset, performing complex activities within a short period in the maze environment. This leads to a decrease in prediction accuracy for all methods, reflecting the notable individual differences among rats and the difficulty of real-time prediction of their movement intentions. Nonetheless, our method was able to maintain a relatively reasonable level of prediction accuracy even under these challenging circumstances, further attesting to the superiority of our approach."
        },
        {
            "heading": "6 Conclusion",
            "text": "In conclusion, recent advancements in bio-robotics with Brain-Computer Interface (BCI) techniques have made it possible to directly control animals such as\nrats, beetles, and doves by humans. However, the highly unpredictable and difficult to control behaviors of these animals make it crucial to develop accurate and effective methods for predicting their movements. This paper proposes a novel dataset called RatPose for predicting the heading direction of rats\u2019 movement, which shifts the attention towards forecasting the future movement outcomes of organisms. The paper also proposes a visual prediction model that effectively decouples motion from the environment for this dataset, achieving superior performance when compared to state-of-the-art action recognition methods.\nAcknowledgements This work is partially supported by the National Key R&D Program of China (2020YFB1313503), the National Natural Science Foundation of China (U22B2052), the Fundamental Research Funds for the Central Universities and the Major Key Project of PCL (PCL2021A12)."
        }
    ],
    "title": "Motion-Scenario Decoupling for Rat-Aware Video Position Prediction: Strategy and Benchmark",
    "year": 2023
}