{
    "abstractText": "This work reports deep-learning-unique first-order and second-order phase transitions, whose phenomenology closely follows that in statistical physics. In particular, we prove that the competition between prediction error and model complexity in the training loss leads to the second-order phase transition for nets with one hidden layer and the first-order phase transition for nets with more than one hidden layer. The proposed theory is directly relevant to the optimization of neural networks and points to an origin of the posterior collapse problem in Bayesian deep learning. Understanding neural networks is a fundamental problem in both theoretical deep learning and neuroscience. In deep learning, learning proceeds as the parameters of different layers become correlated so that the model responds to an input in a meaningful way. This is reminiscent of an ordered phase in physics, where the microscopic degrees of freedom behave collectively and coherently. Meanwhile, regularization effectively prevents the overfitting of the model by reducing the correlation between model parameters in a manner similar to the effect of an entropic force in physics. One thus expects a phase transition in the model behavior from the regime where the regularization is negligible to the regime where it is dominant. In the long history of statistical physics of learning [10, 23, 18, 2], a series of works studied the under-tooverparametrization (UO) phase transition in the context of linear regression [12, 13, 23, 9]. Recently, this type of phase transition has seen a resurgence of interest [8, 16]. One recent work by [15] deals with the UO transition in a deep linear model. However, the UO phase transition is not unique to deep learning because it appears in both shallow and deep models and also in non-neural-network models [3]. To understand deep learning, we need to identify what is unique about deep neural networks. In this work, we address the fundamental problem of the loss landscape of a deep neural network and prove that there exist phase transitions in deep learning that can be described precisely as the firstand second-order phase transitions with a striking similarity to physics. We argue that these phase transitions can have profound implications for deep learning, such as the importance of symmetry breaking for learning and the qualitative difference between shallow and deep architectures. We also show that these phase transitions are unique to machine learning and deep learning. They are unique to machine learning because they are caused by the competition between the need to make predictions more accurate and the need to make the model simpler. These phase transitions are also deep-learning unique because they only appear in \u201cdeeper\u201d models. For a multilayer linear net with stochastic neurons and trained with L2 regularization, 1. we identify an order parameter and effective landscape that describe the phase transition between a trivial phase and a feature learning phase as the L2 regularization hyperparameter is changed (Theorem 3); 2. we show that finite-depth networks cannot have the zeroth-order phase transition (Theorem 2);",
    "authors": [
        {
            "affiliations": [],
            "name": "Liu Ziyin"
        },
        {
            "affiliations": [],
            "name": "Masahito Ueda"
        }
    ],
    "id": "SP:92014bcddcd53e611797b575ef1e9ea6050e3b3e",
    "references": [
        {
            "authors": [
                "A. Alemi",
                "B. Poole",
                "I. Fischer",
                "J. Dillon",
                "R.A. Saurous",
                "K. Murphy"
            ],
            "title": "Fixing a broken elbo",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Bahri",
                "J. Kadmon",
                "J. Pennington",
                "S.S. Schoenholz",
                "J. Sohl-Dickstein",
                "S. Ganguli"
            ],
            "title": "Statistical mechanics of deep learning",
            "venue": "Annual Review of Condensed Matter Physics,",
            "year": 2020
        },
        {
            "authors": [
                "M. Belkin",
                "D. Hsu",
                "J. Xu"
            ],
            "title": "Two models of double descent for weak features",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2020
        },
        {
            "authors": [
                "G.M. Benedek",
                "A. Itai"
            ],
            "title": "Learnability with respect to fixed distributions",
            "venue": "Theoretical Computer Science,",
            "year": 1991
        },
        {
            "authors": [
                "B. Dai",
                "D. Wipf"
            ],
            "title": "Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789",
            "year": 2019
        },
        {
            "authors": [
                "K. Friston"
            ],
            "title": "The free-energy principle: a rough guide to the brain? Trends in cognitive sciences, 13(7):293\u2013301",
            "year": 2009
        },
        {
            "authors": [
                "M. Hardt",
                "T. Ma"
            ],
            "title": "Identity matters in deep learning",
            "venue": "arXiv preprint arXiv:1611.04231",
            "year": 2016
        },
        {
            "authors": [
                "T. Hastie",
                "A. Montanari",
                "S. Rosset",
                "R.J. Tibshirani"
            ],
            "title": "Surprises in high-dimensional ridgeless least squares interpolation",
            "venue": "arXiv preprint arXiv:1903.08560",
            "year": 2019
        },
        {
            "authors": [
                "D. Haussler",
                "M. Kearns",
                "H.S. Seung",
                "N. Tishby"
            ],
            "title": "Rigorous learning curve bounds from statistical mechanics",
            "venue": "Machine Learning,",
            "year": 1996
        },
        {
            "authors": [
                "J.J. Hopfield"
            ],
            "title": "Neural networks and physical systems with emergent collective computational abilities",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 1982
        },
        {
            "authors": [
                "K. Kawaguchi"
            ],
            "title": "Deep learning without poor local minima",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "A. Krogh",
                "J.A. Hertz"
            ],
            "title": "Generalization in a linear perceptron in the presence of noise",
            "venue": "Journal of Physics A: Mathematical and General,",
            "year": 1992
        },
        {
            "authors": [
                "A. Krogh",
                "J.A. Hertz"
            ],
            "title": "A simple weight decay can improve generalization",
            "venue": "In Advances in neural information processing systems,",
            "year": 1992
        },
        {
            "authors": [
                "T. Laurent",
                "J. Brecht"
            ],
            "title": "Deep linear networks with arbitrary loss: All local minima are global",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Q. Li",
                "H. Sompolinsky"
            ],
            "title": "Statistical mechanics of deep linear neural networks: The backpropagating kernel renormalization",
            "venue": "Physical Review X,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liao",
                "R. Couillet",
                "M.W. Mahoney"
            ],
            "title": "A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "J. Lucas",
                "G. Tucker",
                "R. Grosse",
                "M. Norouzi"
            ],
            "title": "Don\u2019t blame the elbo! a linear vae perspective on posterior collapse",
            "year": 2019
        },
        {
            "authors": [
                "C.H. Martin",
                "M.W. Mahoney"
            ],
            "title": "Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning",
            "year": 2017
        },
        {
            "authors": [
                "P. Mianjy",
                "R. Arora"
            ],
            "title": "On dropout and nuclear norm regularization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "S. Sonoda",
                "N. Murata"
            ],
            "title": "Transport analysis of infinitely deep neural network",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "H. Tanaka",
                "D. Kunin"
            ],
            "title": "Noether\u2019s learning dynamics: Role of symmetry breaking in neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "V. Vapnik"
            ],
            "title": "Estimation of dependences based on empirical data",
            "year": 2006
        },
        {
            "authors": [
                "T.L. Watkin",
                "A. Rau",
                "M. Biehl"
            ],
            "title": "The statistical mechanics of learning a rule",
            "venue": "Reviews of Modern Physics,",
            "year": 1993
        }
    ],
    "sections": [
        {
            "text": "This work reports deep-learning-unique first-order and second-order phase transitions, whose phenomenology closely follows that in statistical physics. In particular, we prove that the competition between prediction error and model complexity in the training loss leads to the second-order phase transition for nets with one hidden layer and the first-order phase transition for nets with more than one hidden layer. The proposed theory is directly relevant to the optimization of neural networks and points to an origin of the posterior collapse problem in Bayesian deep learning.\nUnderstanding neural networks is a fundamental problem in both theoretical deep learning and neuroscience. In deep learning, learning proceeds as the parameters of different layers become correlated so that the model responds to an input in a meaningful way. This is reminiscent of an ordered phase in physics, where the microscopic degrees of freedom behave collectively and coherently. Meanwhile, regularization effectively prevents the overfitting of the model by reducing the correlation between model parameters in a manner similar to the effect of an entropic force in physics. One thus expects a phase transition in the model behavior from the regime where the regularization is negligible to the regime where it is dominant. In the long history of statistical physics of learning [10, 23, 18, 2], a series of works studied the under-tooverparametrization (UO) phase transition in the context of linear regression [12, 13, 23, 9]. Recently, this type of phase transition has seen a resurgence of interest [8, 16]. One recent work by [15] deals with the UO transition in a deep linear model. However, the UO phase transition is not unique to deep learning because it appears in both shallow and deep models and also in non-neural-network models [3]. To understand deep learning, we need to identify what is unique about deep neural networks.\nIn this work, we address the fundamental problem of the loss landscape of a deep neural network and prove that there exist phase transitions in deep learning that can be described precisely as the first- and second-order phase transitions with a striking similarity to physics. We argue that these phase transitions can have profound implications for deep learning, such as the importance of symmetry breaking for learning and the qualitative difference between shallow and deep architectures. We also show that these phase transitions are unique to machine learning and deep learning. They are unique to machine learning because they are caused by the competition between the need to make predictions more accurate and the need to make the model simpler. These phase transitions are also deep-learning unique because they only appear in \u201cdeeper\u201d models. For a multilayer linear net with stochastic neurons and trained with L2 regularization,\n1. we identify an order parameter and effective landscape that describe the phase transition between a\ntrivial phase and a feature learning phase as the L2 regularization hyperparameter is changed (Theorem 3);\n2. we show that finite-depth networks cannot have the zeroth-order phase transition (Theorem 2);\n3. we prove that:\nar X\niv :2\n20 5.\n12 51\n0v 1\n[ cs\n.L G\n] 2\n5 M\nay 2\n(a) depth-0 nets (linear regression) do not have a phase transition (Theorem 1);\n(b) depth-1 nets have the second-order phase transitions (Theorem 4);\n(c) depth-D nets have the first-order phase transition (Theorem 5) for D > 1;\n(d) infinite-depth nets have the zeroth-order phase transition (Theorem 6).\nThe theorem statements and proofs are presented in the Supplementary Section B. To the best of our knowledge, we are the first to identify second-order and first-order phase transitions in the context of deep learning. Our result implies that one can precisely classify the landscape of deep neural models according to the Ehrenfest classification of phase transitions."
        },
        {
            "heading": "Results",
            "text": "Formal framework. Let `(w,a) be a differentiable loss function that is dependent on the model parameter w and a hyperparameters a. The loss function ` can be decomposed into a data-dependent feature learning term `0 and a data-independent term aR(w) that regularizes the model at strength a:\n`(w,a) = Ex[`0(w,x)] + aR(w). (1)\nLearning amounts to finding the global minimizer of the loss:\n\u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 L(a) \u2236= minw `(w,a); w\u2217 \u2236= arg minw `(w,a). (2)\nNaively, one expects L(a) to change smoothly as we change a. If L changes drastically or even discontinuously when one perturb a, it becomes hard to tune a to optimize the model performance. Thus, that L(a) is wellbehaved is equivalent to that a is an easy-to-tune hyperparameter. We are thus interested in the case where the tuning of a is difficult, which occurs when a phase transition comes into play.\nIt is standard to treat the first term in Eq. (1) as an energy. To formally identify the regularization term\nas an entropy, its coefficient must be proportional to the temperature:\naR(w) = T\n2\u03c32 R(w), (3)\nwhere \u03c32 controls the fluctuation of w at zero temperature. We note that this identification is consistent with many previous works, where the term that encourages a lower model complexity is identified as an \u201centropy\u201d [9, 22, 4, 6, 15]. In this view, learning is a balancing process between the learning error and the model complexity. Intuitively, one expects phase transitions to happen when one term starts to dominate the other, just like thermodynamic phase transitions that take place when the entropy term starts to dominate the energy.\nIn this setting, the partition function is Z(a) = \u222b dw exp[\u2212`(w,a)/T ]. We consider a special limit of the partition function, where both T and 2\u03c32 are made to vanish with their ratio held fixed at T /2\u03c32 = \u03b3. In this limit, one can find the free energy with the saddle point approximation, which is exact in the zerotemperature limit:\nF (a) = lim T\u21920, \u03c32\u21920, T /2\u03c32=\u03b3 \u2212T log\u222b dw exp[\u2212`(w,a)/T ] = min w `(w,a). (4)\nWe thus treat L as the free energy.\nDefinition 1. L(a) is said to have the nth-order phase transition in a at a = a\u2217 if n is the smallest integer such that d n\ndan L(a)\u2223a=a\u2217 is discontinuous.\nWe formally define the order parameter and effective loss as follows.\nDefinition 2. b = b(w) \u2208 R is said to be an order parameter of `(w,a) if there exists a function \u00af\u0300 such that for all a, minw \u00af\u0300(b(w), a) = L(a), where \u00af\u0300 is said to be an effective loss function of `.\nIn other words, an order parameter is a one-dimensional quantity whose minimization on \u00af\u0300 gives L(a). The existence of an order parameter suggests that the original problem `(w,a) can effectively be reduced to a low-dimensional problem that is much easier to understand. Physical examples are the average magnetization in the Ising model and the average density of molecules in a water-to-vapor phase transition. A dictionary of the corresponding concepts between physics and deep learning is given in Table 1.\nOur theory deals with deep linear nets, the primary minimal model for deep learning. It is well-established that the landscape of a deep linear net can be used to understand that of nonlinear networks [11, 7, 14]. The most general type of deep linear nets, with L2 regularization and stochastic neurons, has the following loss:\nExE (1), (2),..., (D) \u239b\n\u239d\nd0,d0,d0,...d0\n\u2211 i,i1,i2,...,iD\nUiD (D) iD ... (2) i2 W (2) i2i1 (1) i1 W (1) i1i xi \u2212 y\n\u239e\n\u23a0\n2\n\u00b4\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b8\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b6 L0\n+\u03b3\u2223\u2223U \u2223\u222322 + D\n\u2211 i=1 \u03b3\u2223\u2223W (i)\u2223\u22232F\n\u00b4\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b8\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b6 L2 reg.\n, (5)\nwhere x is the input data, y the label, U and W (i) the model parameters, D the network depth, the noise in the hidden layer (e.g., dropout), d0 the width of the model, and \u03b3 the weight decay strength. We build on the recent results established in [24]. Let b \u2236= \u2223\u2223U \u2223\u2223/d0. Ref. [24] shows that all the global minima of Eq. (5) must take the form U = f(b) and Wi = fi(b), where f and fi are explicit functions of the hyperparameters. Ref. [24] further shows that there are two regimes of learning, where, for some range of \u03b3, the global minimum is uniquely given by b = 0, and for some other range of \u03b3, some b > 0 gives the global minimum. When b = 0, the model outputs a constant 0, and so this regime is called the \u201ctrivial regime,\u201d and the regime where b = 0 is not the global minimum is called the \u201cfeature learning regime.\u201d In this work, we prove that the transition between these two regimes corresponds to a phase transition in the Ehrenfest sense (Definition 1), and therefore one can indeed refer to these two regimes as two different phases.\nNo-phase-transition theorems. The first result we prove is that there is no phase transition in any hyperparameter (\u03b3, E[xxT ], E[xy], E[y2]) for a simple linear regression problem. In our terminology, this corresponds to the case of D = 0. The fact that there is no phase transition in any of these hyperparameters means that the model\u2019s behavior is predictable as one tunes the hyperparameters. In the parlance of physics, a linear regressor operates within the linear-response regime.\nTheorem 2 shows that a finite-depth net cannot have zeroth-order phase transitions. This theorem can be seen as a worst-case guarantee: the training loss needs to change continuously as one changes the hyperparameter. We also stress that this general theorem applies to standard nonlinear networks as well. Indeed, if we only consider the global minimum of the training loss, the training loss cannot jump. However, in practice, one can often observe jumps because the gradient-based algorithms can be trapped in local minima. The following theory offers a direct explanation for this phenomenon.\nPhase Transitions in Deeper Networks. Theorem 4 shows that the quantity b is an order parameter describing any phase transition induced by the weight decay parameter in Eq. (5). Let b = \u2223\u2223U \u2223\u2223/du, A0 \u2236= E[xxT ], and ai be the i-th eigenvalue of A0. The effective loss landscape is\n\u00af\u0300(b, \u03b3) \u2236= \u2212\u2211 i\nd2D0 b 2DE[x\u2032y]2i\ndD0 (\u03c3 2 + d0)Daib2D + \u03b3\n+Ex[y2] + \u03b3Dd20b 2, (6)\nwhere x\u2032 is a rotation of x. See Figure 1 for an illustration. The complicated landscape for D > 1 implies that neural networks are susceptible to initialization schemes and entrapment in meta-stable states is common (see Supplementary Section A.1).\nTheorem 5 shows that when D = 1 in Eq. (5), there is a second-order phase transition precisely at\n\u03b3 = \u2223\u2223E[xy]\u2223\u2223. (7)\nIn machine learning language, \u03b3 is the regularization strength and \u2223\u2223E[xy]\u2223\u2223 is the signal. The phase transition occurs precisely when the regularization dominates the signal. In physics, \u03b3 and \u2223\u2223E[xy]\u2223\u2223 are proportional to the temperature and energy, respectively. The phase transition occurs exactly when the entropy dominates the energy. Also, the phase transition for a depth-1 linear net is independent of the number of parameters of the model. For D > 1, the size of the model does play a role in influencing the phase transition. However, \u03b3 remains the dominant variable controlling this phase transition. This independence of the model size is an advantage of the proposed theory because our result becomes directly relevant for all model sizes, not just the infinitely large ones that the previous works often adopt.\nFor D \u2265 2, we show that there is a first-order phase transition between the two phases at some \u03b3 > 0. However, an analytical expression for the critical point is not known. In physics, first-order phase transitions are accompanied by latent heat. Our theory implies that this heat is equivalent to the amount of random noise we have to inject into the model parameters to escape from a local to the global minimum for a deep model. We illustrate the phase transitions studied in Figure 2. We also experimentally demonstrate that the same phase transitions take place in deep nonlinear networks with the corresponding depths (Supplementary Section A.3). While infinite-depth networks are not used in practice, they are important from a theoretical point of view [20] because they can be used for understanding a (very) deep network that often appears in the deep learning practice. Our result shows that the limiting landscape has a zeroth-order phase transition at \u03b3 = 0. In fact, zeroth-order phase transitions do not occur in physics, and it is a unique feature of deep learning.\nRelevance of symmetry breaking. The phase transitions we studied also involve symmetry breaking. This can be seen directly from the effective landscape in Eq. (6). The loss is unaltered as one flip the sign of b, and therefore the loss is symmetric in b. Figure 3 illustrates the effect and importance of symmetry breaking on the gradient descent dynamics. Additionally, this observation may also provide an alternative venue for studying general symmetry-breaking dynamics because the computation with neural networks is both accurate and efficient.\nMean-Field Analysis. The crux of our theory can be understood by applying a simplified \u201cmean-field\u201d analysis of the loss function in Eq. (5). Let each weight matrix be approximated by a scalar U = bD+1, Wi = bi, ignore the stochasticity due to i, and let x be one-dimensional. One then obtains a simplified mean-field loss:\nEx \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 (c0x D+1 \u220f i=1 bi \u2212 y) 2\u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 + \u03b3 D \u2211 i=1 cib 2 i , (8)\nwhere ci\u2019s are constants. The first term can be interpreted as a special type of (D+1)-body interaction. We now perform a second mean-field approximation, where all the bi take the same value b:\n`\u221d c\u20320E[x 2 ]b2D+2 \u2212 c\u20321E[xy]b D+1 + \u03b3c\u20322b 2 + const. (9)\nHere, c\u20320, c \u2032 1 and c \u2032 2 are structural constants, only depending on the model (depth, width, etc). The first and the third terms monotonically increase in b, encouraging a smaller b. The second term monotonically decreases in bD+1E[xy], encouraging a positive correlation between b and the feature E[xy]. The leading and lowest-order terms regularize the model, while the intermediate term characterizes learning. For D = 0, the loss is quadratic and has no transition. For D = 1, the loss is identical to the Landau free energy, and a phase transition occurs when the second-order term flips sign: c\u20322\u03b3 = c \u2032 1E[xy]. For D > 1, the origin is always a local minimum, dominated by the quadratic term. This leads to a first-order phase transition. When D \u2192\u221e, the leading terms become discontinuous in b, and one obtains a zeroth-order phase transition. This simple analysis highlights one important distinction between physics and machine learning: in physics, the most common type of interaction is a two-body interaction, whereas, for machine learning, the common interaction is many-body and tends to infinite-body as D increases.\nOne implication is that L2 regularization may be too strong for deep learning because it creates a trivial phase. Our result also suggests a way to avoid the trivial phase. Instead of regularizing by \u03b3\u2223\u2223w\u2223\u222322, one might consider \u03b3\u2223\u2223w\u2223\u2223d+22 , which is the lowest-order regularization that does not lead to a trivial phase. The effectiveness of this suggested method is confirmed in Supplementary Section A.2.\nPosterior Collapse in Bayesian Deep Learning. Our results also identify an origin of the well-known problem posterior collapse problem in Bayesian deep learning. Posterior collapse refers to the learning failure where the learned posterior distribution coincides with the prior, and so no learning has happened even after training [5, 1, 17]. Our results offer a direct explanation for this posterior collapse problem. In the Bayesian interpretation, the training loss in Eq. (5) is the exact negative log posterior, and the trivial phase exactly corresponds to the posterior collapse: the global minimum of the loss is identical to the global maximum of the prior term. Our results thus imply that (1) posterior collapse is a unique problem of deep learning because it does not occur in shallow models, and (2) posterior collapse happens as a direct consequence of the competition between the prior and the likelihood. This means that it is not a good idea to assume a Gaussian prior for the deep neural network models. The suggested fix also leads to a clean and Bayesian-principled solution to the posterior collapse problem by using a prior log p(w)\u221d \u2212\u2223\u2223w\u2223\u2223D+22 ."
        },
        {
            "heading": "Discussion",
            "text": "The striking similarity between phase transitions in neural networks and statistical-physics phase transitions lends a great impetus to a more thorough investigation of deep learning through the lens of thermodynamics and statistical physics. We now outline a few major future steps:\n1. Instead of classification by analyticity, can we classify neural networks by symmetry and topological\ninvariants?\n2. What are other possible phases for a nonlinear network? Does a new phase emerge?\n3. Can we find any analogy of other thermodynamic quantities such as volume and pressure? More\nbroadly, can we establish thermodynamics for deep learning?\n4. Can we utilize the latent heat picture to devise better algorithms for escaping local minima in deep\nlearning?\nThis work shows that the Ehrenfest classification of phase transitions aligns precisely with the number of layers in deep neural networks. We believe that the statistical-physics approach to deep learning will bring about fruitful developments in both fields of statistical physics and deep learning."
        },
        {
            "heading": "A Additional Experiments",
            "text": ""
        },
        {
            "heading": "A.1 Sensitivity to the Initial Condition",
            "text": "Our result suggests that the learning of a deeper network is quite sensitive to the initialization schemes we use. In particular, for D > 1, some initialization schemes converge to the trivial solutions more easily, while others converge to the nontrivial solution more easily. Figure 4 plots the converged loss of a D = 2 model for two types of initialization: (a) larger initialization, where the parameters are initialized around zero with the standard deviation s = 0.3 and (b) small initialization with s = 0.01. The value of s is thus equal to the expected norm of the model at initialization, and a small s means that it is initialized closer to the trivial phase and a larger s means that it is initialized closer to the learning phase. We see that across a wide range of \u03b3, one of the initialization schemes gets stuck in a local minimum and does not converge to the global minimum. In light of the latent heat picture, the reason for the sensitivity to initial states is clear: one needs to inject additional energy to the system to leave the meta-stable state; otherwise, the system may become stuck for a very long time. The existing initialization methods are predominantly data-dependent. However, our result (also see [24]) suggests that the size of the trivial minimum is data-dependent, and our result thus highlights the importance of designing data-dependent initialization methods in deep learning."
        },
        {
            "heading": "A.2 Removing the Trivial Phase",
            "text": "We also explore our suggested fix to the trivial learning problem. Here, instead of regularization the model by \u03b3\u2223\u2223w\u2223\u222322, we regularize the model by \u03b3\u2223\u2223w\u2223\u2223 D+2 2 . The training loss and the model norm b are plotted in Figure 5. We find that the trivial phase now completely disappears even if we go to very high \u03b3. However, we note that this fix only removes the local maximum at zero, but zero remains a saddle point from which it takes the system a long time to escape."
        },
        {
            "heading": "A.3 Nonlinear Networks",
            "text": "We expect our theory to also apply to deep nonlinear networks that can be locally approximated by linear net at the origin, e.g., a network with tanh activations. As shown in Figure 6, the data shows that a tanh net also features a second-order phase transition for D = 1 and a first-order phase transition for D = 2.\nOne notable exception that our theory may not apply is the networks with the ReLU activation because these networks are not differentiable at zero (i.e., in the trivial phase). However, there are smoother (and empirically better) alternatives to ReLU, such as the swish activation function, to which the present theory should also be relevant."
        },
        {
            "heading": "B Main Results",
            "text": ""
        },
        {
            "heading": "B.1 Theorem Statements",
            "text": "For a simple ridge linear regression, the minimization objective is\n`(W ) = Ex (\u2211 i Wixi \u2212 y)\n2\n+ \u03b3\u2223\u2223W \u2223\u22232. (10)\nTheorem 1. There is no phase transition in any hyperparameter (\u03b3, A0, E[xy], E[y 2]) in a simple ridge linear regression for any \u03b3 \u2208 (0,\u221e).\nThe following result shows that for a finite depth, L(\u03b3) must be continuous in \u03b3.\nTheorem 2. For any finite D > 0 and \u03b3 \u2208 [0,\u221e), L(\u03b3) has no zeroth-order phase transition with respect to \u03b3.\nNote that this theorem allows the weight decay parameter to be 0, and so our results also extend to the\ncase when there is no weight decay.\nThe following theorem shows that there exists order parameters describing any phase transition induced\nby the weight decay parameter in Eq. (5).\nTheorem 3. Let b = \u2223\u2223U \u2223\u2223/du, and let\n\u00af\u0300(b, \u03b3) \u2236= \u2212\u2211 i\nd2D0 b 2DE[x\u2032y]2i\ndD0 (\u03c3 2 + d0)Daib2D + \u03b3\n+Ex[y2] + \u03b3Dd20b 2. (11)\nThen, b is an order parameter of Eq. (5) for the effective loss \u00af\u0300.\nHere, the norm of the last layer is referred to as the order parameter. The meaning of this choice should be clear. The norm of the last layer is zero if and only if all weights of the last layer is zero, and the model is a trivial model. The model can only learn something when the order parameter is nonzero. Additionally, we note that the choice of the order parameter is not unique and there are other choices for the order parameter (e.g., the norm of any other layer, or the sum of the norms of all layers).\nThe following theorem shows that when D = 1 in Eq. (5), there is a second-order phase transition with\nrespect to \u03b3.\nTheorem 4. Equation (5) has the second-order phase transition between the trivial and feature learning phases at1\n\u03b3 = \u2223\u2223E[xy]\u2223\u2223. (12)\nNow, we show that for D \u2265 2, there is a first-order phase transition.\nTheorem 5. Let D \u2265 2. There exists a \u03b3\u2217 > 0 such that the loss function Eq. (5) has the first-order phase transition between the trivial and feature learning phases at \u03b3 = \u03b3\u2217.\nTheorem 6. Let L(D)(\u03b3) denote the loss function for a fixed depth D as a function of \u03b3. Then, for \u03b3 \u2208 [0,\u221e) and some constant r,\nL(D)(\u03b3)\u2192 \u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 r if \u03b3 = 0; E[y2] otherwise. (13)\nThe constant r is, in general, not equal to E[y2]. For example, in the limit \u03c3 \u2192 0, r converges to the loss value of a simple linear regression, which is not equal to E[y2] as long as E[xy] \u2260 0.\n1When the two layers have different regularization strengths \u03b3u and \u03b3w, one can show that the phase transition occurs precisely at \u221a \u03b3u\u03b3w = \u2223\u2223E[xy]\u2223\u2223."
        },
        {
            "heading": "B.2 Proof of Theorem 1",
            "text": "Proof. The global minimum of Eq. (10) is\nW\u2217 = (A0 + \u03b3I) \u22121E[xy]. (14)\nThe loss of the global minimum is thus\nL = Ex (\u2211 i Wixi \u2212 y)\n2\n+ \u03b3\u2223\u2223W \u2223\u22232 (15)\n=WTA0W \u2212 2W TE[xy] +E[y2] + \u03b3\u2223\u2223W \u2223\u22232 (16)\n= E[xy]T A0\n(A0 + \u03b3I)2 E[xy] \u2212 2E[xy]T\n1\nA0 + \u03b3I E[xy] +E[y2] + \u03b3E[xy]T\n1\n(A0 + \u03b3I)2 E[xy] (17)\n= \u2212E[xy]T (A0 + \u03b3I)\u22121E[xy] +E[y2], (18)\nwhich is infinitely differentiable for any \u03b3 \u2208 (0,\u221e) (note that A0 is always positive semi-definite by definition). \u25fb"
        },
        {
            "heading": "B.3 Proof of Theorem 2",
            "text": "Proof. For any fixed and bounded w, `(w,\u03b3) is continuous in \u03b3. Moreover, `(w,\u03b3) is a monotonically increasing function of \u03b3. This implies that L(\u03b3) is also an increasing function of \u03b3 (but may not be strictly increasing).\nWe now prove by contradiction. We first show that L(\u03b3) is left-continuous. Suppose that for some D,\nL(\u03b3) is not left-continuous in \u03b3 at some \u03b3\u2217. By definition, we have\nL(\u03b3\u2217 \u2212 ) = min w `(w,\u03b3\u2217 \u2212 ) \u2236= `(w\u2032, \u03b3\u2217 \u2212 ), (19)\nwhere w\u2032 is one of the (potentially many) global minima of L(\u03b3\u2217 \u2212 ). Since L(\u03b3) is not left-continuous by assumption, there exists \u03b4 > 0 such that for any > 0,\nL(\u03b3\u2217 \u2212 ) < L(\u03b3\u2217) \u2212 \u03b4, (20)\nwhich implies that\n`(w\u2032, \u03b3\u2217 \u2212 ) = L(\u03b3\u2217 \u2212 ) < L(\u03b3\u2217) \u2212 \u03b4 \u2264 `(w\u2032, \u03b3\u2217) \u2212 \u03b4. (21)\nNamely, the left-discontinuity implies that for all > 0,\n`(w\u2032, \u03b3\u2217 \u2212 ) \u2264 `(w\u2032, \u03b3\u2217) \u2212 \u03b4. (22)\nHowever, by definition of `(w,\u03b3), we have\n`(w,\u03b3) \u2212 `(w,\u03b3 \u2212 ) = \u2223\u2223w\u2223\u22232. (23)\nThus, by choosing < \u03b4/\u2223\u2223w\u2223\u22232, the relation in (21) is violated. Thus, L(\u03b3) must be left-continuous.\nIn a similar manner, we can prove that L is right-continuous. Suppose that for some D, L(\u03b3) is not\nright-continuous in \u03b3 at some \u03b3\u2217. Let \u03b3 > 0. By definition, we have\nL(\u03b3\u2217 + ) = min w `(w,\u03b3\u2217 + ) \u2236= `(w\u2032, \u03b3\u2217 + ), (24)\nwhere w\u2032 is one of the (potentially many) global minima of L(\u03b3\u2217 + ). Since L(\u03b3) is not right-continuous by assumption, there exists \u03b4 > 0 such that for any > 0,\nL(\u03b3\u2217 + ) > L(\u03b3\u2217) + \u03b4, (25)\nwhich implies that\n`(w\u2032, \u03b3\u2217 + ) = L(\u03b3\u2217 + ) > L(\u03b3\u2217) + \u03b4 \u2265 `(w\u2032, \u03b3\u2217) + \u03b4. (26)\nNamely, the right-discontinuity implies that for all > 0,\n`(w\u2032, \u03b3\u2217 + ) \u2265 `(w\u2032, \u03b3\u2217) + \u03b4. (27)\nHowever, by definition of `(w,\u03b3), we have\n`(w,\u03b3 + ) \u2212 `(w,\u03b3) = \u2223\u2223w\u2223\u22232. (28)\nThus, by choosing < \u03b4/\u2223\u2223w\u2223\u22232, the relation in (26) is violated. Thus, L(\u03b3) must be right-continuous.\nTherefore, L(\u03b3) is continuous for all \u03b3 > 0. By definition, this means that there is no zeroth-order phase transition in \u03b3 for L. Additionally, note that the above proof does not require \u03b3 \u2260 0, and so we have also shown that L(\u03b3) is right-continuous at \u03b3 = 0. \u25fb"
        },
        {
            "heading": "B.4 Proof of Theorem 3",
            "text": "Proof. By Theorem 3 of Ref. [24], any global minimum of Eq. (5) is given by the following set of equations for some b \u2265 0: \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa \u23a8 \u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 U = \u221a d0brD; W (i) = brir T i\u22121; W (1) = r1E[xy]T d D\u2212 12 0 b D [dD0 (\u03c3 2 + d0) Db2DA0 + \u03b3] \u22121 , (29)\nwhere ri = (\u00b11, ...,\u00b11) is an arbitrary vertex of a di-dimensional hypercube for all i. Therefore, the global minimum must lie on a one-dimensional space indexed by b \u2208 [0,\u221e). Let f(x) specify the model as\nf(x) \u2236= d,d1,d2,...dD\n\u2211 i,i1,i2,...,iD\nUiD (D) iD ... (2) i2 W (2) i2i1 (1) i1 W (1) i1i x, (30)\nand let \u03b7 denote the set of all random noises i.\nSubstituting Eq. (29) in Eq. (5), one finds that within this subspace, the loss function can be written as\n`(w,\u03b3) = ExE\u03b7(f(x) \u2212 y)2 +L2 reg. (31) = Ex,\u03b7[f(x)2] \u2212 2Ex,\u03b7[yf(x)] +Ex[y2] +L2 reg. (32)\n=\u2211 i\nd3D0 (\u03c3 2 + d0) Db4DaiE[x\u2032y]2i [dD0 (\u03c3 2 + d0)Daib2D + \u03b3]2 \u2212 2\u2211 i\nd2D0 b 2DE[x\u2032y]2i\ndD0 (\u03c3 2 + d0)Daib2D + \u03b3\n+Ex[y2] +L2 reg., (33)\nwhere the L2 reg. term is\nL2 reg. = \u03b3Dd 2 0b 2 + \u03b3\u2211\ni\nd2D0 b 2D \u2223\u2223E[x\u2032y]i\u2223\u22232\n[dD0 (\u03c3 2 + d0)Db2Dai + \u03b3]2\n. (34)\nCombining terms, we can simplify the expression for the loss function to be\n\u2212\u2211 i\nd2D0 b 2DE[x\u2032y]2i\n[dD0 (\u03c3 2 + d0)Daib2D + \u03b3]\n+Ex[y2] + \u03b3Dd20b 2. (35)\nWe can now define the effective loss by\n\u00af\u0300(b, \u03b3) \u2236= \u2212\u2211 i\nd2D0 b 2DE[x\u2032y]2i\n[dD0 (\u03c3 2 + d0)Daib2D + \u03b3]\n+Ex[y2] + \u03b3Dd20b 2. (36)\nThen, the above argument shows that, for all \u03b3,\nmin b \u00af\u0300(b, \u03b3) = min w `(w,\u03b3). (37)\nBy definition 2, b is an order parameter of ` with respect to the effective loss \u00af\u0300(b, \u03b3). This completes the proof. \u25fb"
        },
        {
            "heading": "B.5 Two Useful Lemmas",
            "text": "Before continuing the proofs, we first prove two lemmas that will simplify the following proofs significantly.\nLemma 1. If L(\u03b3) is differentiable, then for at least one of the global minima b\u2217,\nd\nd\u03b3 L(\u03b3) =\u2211\ni\nd2D0 b 2D \u2217 E[x\u2032y]2i\n[dD0 (\u03c3 2 + d0)Daib2D\u2217 + \u03b3]\n2 +Dd20b 2 \u2217 \u2265 0. (38)\nProof. Because L is differentiable in \u03b3, one can find the derivative for at least one of the global minima\nb\u2217\nd\nd\u03b3 L(\u03b3) =\nd\nd\u03b3 \u00af\u0300(b\u2217(\u03b3), \u03b3) (39)\n= \u2202\n\u2202b\u2217 \u00af\u0300(b\u2217, \u03b3)\n\u2202b\u2217\n\u2202\u03b3 + \u2202 \u2202\u03b3 \u00af\u0300(b\u2217, \u03b3) (40)\n= \u2202\n\u2202\u03b3 \u00af\u0300(b\u2217, \u03b3) (41)\n=\u2211 i\nd2D0 b 2D \u2217 E[x\u2032y]2i\n[dD0 (\u03c3 2 + d0)Daib2D\u2217 + \u03b3]\n2 +Dd20b 2 \u2217 \u2265 0, (42)\nwhere we have used the optimality condition \u2202 \u2202b\u2217 \u00af\u0300(b\u2217(\u03b3), \u03b3) = 0 in the second equality. \u25fb"
        },
        {
            "heading": "B.6 Proof of Theorem 4",
            "text": "Proof. By definition 1, it suffices to only prove the existence of phase transitions on the effective loss. For D = 1, the effective loss is\n\u00af\u0300(b, \u03b3) = \u2212d1b 2E[xy]T [b2(\u03c32 + d1)A + \u03b3I] \u22121E[xy] +E[y2] + \u03b3d1b 2. (43)\nBy Theorem 1 of Ref. [24], the phase transition, if exists, must occur precisely at \u03b3 = \u2223\u2223E[xy]\u2223\u2223. To prove that \u03b3 = \u2223\u2223E[xy]\u2223\u2223 has a second-order phase transition, we must check both its first derivative and second derivative.\nWhen \u03b3 \u2192 \u2223\u2223E[xy]\u2223\u2223 from the right, we find that the all derivatives of L(\u03b3) are zero because the loss is identically equal to E[y2]. We now consider the derivative of L when \u03b3 \u2192 \u2223\u2223E[xy]\u2223\u2223 from the left. We first need to find the minimizer of Eq. (43). Because Eq.(43) is differentiable, its derivative in b must be equal to 0 at the global minimum\n\u2212 2\u03b3d1bE[xy]T [b2(\u03c32 + d1)2A + \u03b3I]\u22122E[xy] + 2\u03b3d1b = 0. (44)\nFinding the minimizer b is thus equivalent to finding the real roots of a high-order polynomial in b. When \u03b3 \u2265 \u2223\u2223E[xy]\u2223\u2223, the solution is unique [24]: b20 = 0, (45)\nwhere we labeled the solution with the subscript 0 to emphasize that this solution is also the zeroth-order term of the solution in a perturbatively small neighborhood of \u03b3 = \u2223\u2223E[xy]\u2223\u2223. From this point, we define a shifted regularization strength: \u2206 \u2236= \u03b3 \u2212 \u2223\u2223E[xy]\u2223\u2223. When \u2206 < 0, the condition (44) simplifies to\nE[xy]T [b2(\u03c32 + d1)A + \u03b3I]\u22122E[xy] = 1. (46)\nBecause the polynomial is not singular in \u2206, one can Taylor expand the (squared) solution b2 in \u2206:\nb(\u03b3)2 = \u03b20 + \u03b21\u2206 +O(\u2206 2 ). (47)\nWe first Substitute (47) in (44) to find2\n\u03b20 = 0. (48)\n2Note that alternatively, \u03b20 = 0 is implied by the no-zeroth-order transition theorem.\nOne can then again substitute Eq. (47) in Eq. (44) to find \u03b21. To the first order in b 2, Eq. (44) reads\n1\n\u03b32 \u2223\u2223E[xy]\u2223\u22232 \u2212 2b2\n(\u03c32 + d1)\n\u03b33 \u2223\u2223E[xy]\u2223\u22232A0 = 1 (49)\n\u21d0\u21d2 \u22122\u03b21\u2206 (\u03c32 + d1)\n\u03b33 \u2223\u2223E[xy]\u2223\u22232A0 = 2\n\u2206\n\u2223\u2223E[xy]\u2223\u2223 (50)\n\u21d0\u21d2 \u03b21 = \u2212 1\n(\u03c32 + d1)\n\u2223\u2223E[xy]\u2223\u22232\n\u2223\u2223E[xy]\u2223\u22232A0 (51)\nSubstituting this first-order solution to Lemma 1, we obtain that\nd\nd\u03b3 L(\u03b3)\u2223\u03b3=\u2223\u2223E[xy]\u2223\u2223\u2212 \u223c b\n2 \u2217 = 0 =\nd\nd\u03b3 L(\u03b3)\u2223\u03b3=\u2223\u2223E[xy]\u2223\u2223+ . (52)\nThus, the first-order derivative of L(\u03b3) is continuous at the phase transition point.\nWe now find the second-order derivative of L(\u03b3). To achieve this, we also need to find the second-order\nterm of b2 in \u03b3. We expand b2 as\nb(\u03b3)2 = 0 + \u03b21\u2206 + \u03b22\u2206 2 +O(\u22063). (53)\nTo the second order in b2, (44) reads\n1\n\u03b32 \u2223\u2223E[xy]\u2223\u22232 \u2212 2b2\n(\u03c32 + d1)\n\u03b33 \u2223\u2223E[xy]\u2223\u22232A0 + 3b\n4 (\u03c3 2 + d1) 2\n\u03b34 \u2223\u2223E[xy]\u2223\u22232A20 = 1 (54)\n\u21d0\u21d2 \u03b32\u2223\u2223E[xy]\u2223\u22232 \u2212 2b2(\u03c32 + d1)\u03b3\u2223\u2223E[xy]\u2223\u22232A0 + 3b 4 (\u03c32 + d1) 2 \u2223\u2223E[xy]\u2223\u22232A20 = \u03b3 4 (55) \u21d0\u21d2\u22062E20 \u2212 2\u03b22\u2206 2 (\u03c32 + d1)E0E 2 1 \u2212 2\u03b21\u2206 2 (\u03c32 + d1)E 2 1 + 3\u03b2 2 1\u2206 2 (\u03c32 + d1) 2E22 = 6E 2 0\u2206 2 (56) \u21d0\u21d2 \u03b22 = 3\u03b221(\u03c3 2 + d1) 2E22 \u2212 5E 2 0 \u2212 2\u03b21(\u03c3 2 + d1)E 2 1\n2(\u03c32 + d1)E0E21 , (57)\nwhere, from the third line, we have used the shorthand E0 \u2236= \u2223\u2223E[xy]\u2223\u2223, E1 \u2236= \u2223\u2223E[xy]\u2223\u2223A0 , and E2 \u2236= \u2223\u2223E[xy]\u2223\u2223A20 . Substitute in \u03b21, one finds that\n\u03b22 = 3E0(E\n2 2 \u2212E 2 1)\n2(\u03c32 + d1)E41 . (58)\nThis allows us to find the second derivative of L(\u03b3). Substituting \u03b21 and \u03b22 into Eq. (43) and expanding\nto the second order in \u2206, we obtain that\nL(\u03b3) = \u2212d1b 2E[xy]T [b2(\u03c32 + d1)A + \u03b3I] \u22121E[xy] +E[y2] + \u03b3d1b 2 (59)\n= \u2212d1(\u03b21\u2206 + \u03b22\u2206 2 )E[xy]T [(\u03b21\u2206 + \u03b22\u22062)(\u03c32 + d1)A0 + \u03b3I]\u22121E[xy] + \u03b3d1(\u03b21\u2206 + \u03b22\u2206). (60)\nAt the critical point,\nd2\nd\u03b32 L(\u03b3)\u2223\u03b3=\u2223\u2223E[xy]\u2223\u2223\u2212 = \u2212d1\u03b22E0 + d1\u03b2\n2 1(\u03c3 2 + d1) E21 E20 + d1\u03b21 + d1\u03b21 + d1\u03b22E0 (61)\n= 2d1\u03b21 + d1\u03b2 2 1(\u03c3 2 + d1) E21 E20\n(62)\n= d1\u03b21 (63)\n= \u2212 d1\n\u03c32 + d1\n\u2223\u2223E[xy]\u2223\u22232\n\u2223\u2223E[xy]\u2223\u22232A0 . (64)\nNotably, the second derivative of L from the left is only dependent on \u03b21 and not on \u03b22.\nd2\nd\u03b32 L(\u03b3)\u2223\u03b3=\u2223\u2223E[xy]\u2223\u2223\u2212 = \u2212 d1 \u03c32 + d1\n\u2223\u2223E[xy]\u2223\u22232\n\u2223\u2223E[xy]\u2223\u22232A0 < 0. (65)\nThus, the second derivative of L(\u03b3) is discontinuous at \u03b3 = \u2223\u2223E[xy]\u2223\u2223. This completes the proof. \u25fb Remark. Note that the proof suggests that close to the critical point, b \u223c \u221a \u2206, in agreement with the Landau theory."
        },
        {
            "heading": "B.7 Proof of Theorem 5",
            "text": "Proof. By definition, it suffices to show that d d\u03b3 L(\u03b3) is not continuous. We prove by contradiction. Suppose that d d\u03b3 L(\u03b3) is everywhere continuous on \u03b3 \u2208 (0,\u221e). Then, by Lemma 1, one can find the derivative for at least one of the global minima b\u2217\nd\nd\u03b3 L(\u03b3) =\u2211\ni\nd2D0 b 2D \u2217 E[x\u2032y]2i\n[dD0 (\u03c3 2 + d0)Daib2D\u2217 + \u03b3]\n2 + \u03b3Dd20b 2 \u2217 \u2265 0. (66)\nBoth terms in the last line are nonnegative, and so one necessary condition for d d\u03b3 L(\u03b3) to be continuous is that both of these two terms are continuous in \u03b3.\nIn particular, one necessary condition is that \u03b3Dd20b 2 \u2217 is continuous in \u03b3. By Proposition 3 of Ref. [24],\nthere exist constants c0, c1 such that 0 < c0 \u2264 c1, and\n\u23a7\u23aa\u23aa \u23a8 \u23aa\u23aa\u23a9 b\u2217 = 0 if \u03b3 < c0; b\u2217 > 0, if \u03b3 > c1. (67)\nAdditionally, if b\u2217 > 0, b\u2217 must be lower-bounded by some nonzero value [24]:\nb\u2217 \u2265 1\nd0 (\n\u03b3\n\u2223\u2223E[xy]\u2223\u2223 )\n1 D\u22121\n> 1\nd0 ( c1 \u2223\u2223E[xy]\u2223\u2223 )\n1 D\u22121\n> 0. (68)\nTherefore, for any D > 1, b\u2217(\u03b3) must have a discontinuous jump from 0 to a value larger than 1 d0 ( c0\u2223\u2223E[xy]\u2223\u2223) 1 D\u22121 , and cannot be continuous. This, in turn, implies that d d\u03b3 L(\u03b3) jumps from zero to a nonzero value and cannot be continuous. This completes the proof. \u25fb"
        },
        {
            "heading": "B.8 Proof of Theorem 6",
            "text": "Proof. It suffices to show that a nonzero global minimum cannot exist at a sufficiently large D, when one fixes \u03b3. By Proposition 3 of Ref. [24], when \u03b3 > 0, any nonzero global minimum must obey the following two inequalities:\n1\nd0 [\n\u03b3\n\u2223\u2223E[xy]\u2223\u2223 ]\n1 D\u22121\n\u2264 b\u2217 \u2264 [ \u2223\u2223E[xy]\u2223\u2223\nd0(\u03c32 + d0)Damax ]\n1 D+1\n, (69)\nwhere amax is the largest eigenvalue of A0. In the limit D \u2192\u221e, the lower bound becomes\n1\nd0 [\n\u03b3\n\u2223\u2223E[xy]\u2223\u2223 ]\n1 D\u22121 \u2192 1\nd0 . (70)\nThe upper bound becomes\n[ \u2223\u2223E[xy]\u2223\u2223\nd0(\u03c32 + d0)Damax ]\n1 D+1 \u2192\n1\n\u03c32 + d0 . (71)\nBut for any \u03c32 > 0, 1 d0 < 1 \u03c32+d0 . Thus, the set of such b \u2217 is empty.\nOn the other hand, when \u03b3 = 0, the global minimizer has been found in Ref. [19] and is nonzero, which\nimplies that L(0) < E[y2]. This means that L(\u03b3) is not continuous at 0. This completes the proof. \u25fb"
        }
    ],
    "title": "Exact Phase Transitions in Deep Learning",
    "year": 2022
}