{
    "abstractText": "We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70% in some layers of the 66b model) are \u201cdead\u201d, i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner.",
    "authors": [
        {
            "affiliations": [],
            "name": "Elena Voita"
        },
        {
            "affiliations": [],
            "name": "Javier Ferrando"
        },
        {
            "affiliations": [],
            "name": "Christoforos Nalmpantis"
        }
    ],
    "id": "SP:1c1911ea652b29cea642e7cb1cc6458eb9225d59",
    "references": [
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein."
            ],
            "title": "Neural module networks",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2016
        },
        {
            "authors": [
                "ing",
                "Yuhuai Wu",
                "Kelvin Xu",
                "Yunhan Xu",
                "Linting Xue",
                "Pengcheng Yin",
                "Jiahui Yu",
                "Qiao Zhang",
                "Steven Zheng",
                "Ce Zheng",
                "Weikang Zhou",
                "Denny Zhou",
                "Slav Petrov",
                "Yonghui Wu"
            ],
            "title": "Palm 2 technical report",
            "year": 2023
        },
        {
            "authors": [
                "Joris Baan",
                "Maartje ter Hoeve",
                "Marlies van der Wees",
                "Anne Schuth",
                "Maarten de Rijke"
            ],
            "title": "Understanding multi-head attention in abstractive summarization",
            "year": 2019
        },
        {
            "authors": [
                "Anthony Bau",
                "Yonatan Belinkov",
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "James Glass."
            ],
            "title": "Identifying and controlling important neurons in neural machine translation",
            "venue": "International Conference on Learning Representations, New Orleans.",
            "year": 2019
        },
        {
            "authors": [
                "Jason Baumgartner",
                "Savvas Zannettou",
                "Brian Keegan",
                "Megan Squire",
                "Jeremy Blackburn"
            ],
            "title": "The pushshift reddit dataset",
            "year": 2020
        },
        {
            "authors": [
                "teusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Gon\u00e7alo M. Correia",
                "Vlad Niculae",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Adaptively sparse transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei."
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Candlish",
                "Dario Amodei",
                "Christopher Olah."
            ],
            "title": "Softmax linear units",
            "venue": "Https://transformercircuits.pub/2022/solu/index.html.",
            "year": 2022
        },
        {
            "authors": [
                "Jackson Kernion",
                "Liane Lovitt",
                "Kamal Ndousse",
                "Dario Amodei",
                "Tom Brown",
                "Jack Clark",
                "Jared Kaplan",
                "Sam McCandlish",
                "Chris Olah."
            ],
            "title": "A mathematical framework for transformer circuits",
            "venue": "Transformer Circuits Thread.",
            "year": 2021
        },
        {
            "authors": [
                "Angela Fan",
                "Edouard Grave",
                "Armand Joulin"
            ],
            "title": "Reducing transformer depth on demand with struc",
            "year": 2020
        },
        {
            "authors": [
                "Javier Ferrando",
                "Gerard I. G\u00e1llego",
                "Ioannis Tsiamas",
                "Marta R. Costa-juss\u00e0."
            ],
            "title": "Explaining how transformers use context to build predictions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2020
        },
        {
            "authors": [
                "Mor Geva",
                "Jasmijn Bastings",
                "Katja Filippova",
                "Amir Globerson"
            ],
            "title": "Dissecting recall of factual associations in auto-regressive language models",
            "year": 2023
        },
        {
            "authors": [
                "Mor Geva",
                "Avi Caciularu",
                "Kevin Wang",
                "Yoav Goldberg."
            ],
            "title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy."
            ],
            "title": "Transformer feed-forward layers are keyvalue memories",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Hanna",
                "Ollie Liu",
                "Alexandre Variengien"
            ],
            "title": "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "year": 2023
        },
        {
            "authors": [
                "Adi Haviv",
                "Ori Ram",
                "Ofir Press",
                "Peter Izsak",
                "Omer Levy."
            ],
            "title": "Transformer language models without positional encodings still learn positional information",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382\u20131390,",
            "year": 2022
        },
        {
            "authors": [
                "Ronghang Hu",
                "Jacob Andreas",
                "Trevor Darrell",
                "Kate Saenko."
            ],
            "title": "Explainable neural computation via stack neural module networks",
            "venue": "Proceedings of the European conference on computer vision (ECCV).",
            "year": 2018
        },
        {
            "authors": [
                "Alon Jacovi",
                "Oren Sar Shalom",
                "Yoav Goldberg."
            ],
            "title": "Understanding convolutional neural networks for text classification",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56\u201365,",
            "year": 2018
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133657, Florence, Italy",
            "venue": "Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2020
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Justin Johnson",
                "Li Fei-Fei"
            ],
            "title": "Visualizing and understanding recurrent networks",
            "year": 2015
        },
        {
            "authors": [
                "Amirhossein Kazemnejad",
                "Inkit Padhi",
                "Karthikeyan Natesan Ramamurthy",
                "Payel Das",
                "Siva Reddy"
            ],
            "title": "The impact of positional encoding on length generalization in transformers",
            "year": 2023
        },
        {
            "authors": [
                "Tushar Khot",
                "Daniel Khashabi",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal."
            ],
            "title": "Text modular networks: Learning to decompose tasks in the language of existing models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Louis Kirsch",
                "Julius Kunze",
                "David Barber."
            ],
            "title": "Modular networks: Learning to decompose neural computation",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
            "year": 2018
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Alexey Romanov",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "Revealing the dark secrets of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton."
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc.",
            "year": 2012
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Matt Gardner",
                "Yonatan Belinkov",
                "Matthew E. Peters",
                "Noah A. Smith."
            ],
            "title": "Linguistic knowledge and transferability of contextual representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
            "year": 2019
        },
        {
            "authors": [
                "Andr\u00e9 F.T. Martins",
                "Ram\u00f3n F. Astudillo."
            ],
            "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48,",
            "year": 2016
        },
        {
            "authors": [
                "Pedro Henrique Martins",
                "Zita Marinho",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Sparse text generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4252\u20134273, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex J Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Vlad Niculae",
                "Mathieu Blondel."
            ],
            "title": "A regularized framework for sparse and structured neural attention",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Ben Peters",
                "Vlad Niculae",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Sparse sequence-to-sequence models",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504\u20131519, Florence, Italy. Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Rafal Jozefowicz",
                "Ilya Sutskever"
            ],
            "title": "Learning to generate reviews and discovering sentiment",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Blog, 1(8):9.",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Eric Michael Smith",
                "Y-Lan Boureau",
                "Jason Weston."
            ],
            "title": "Recipes for building an open-domain chatbot",
            "venue": "Proceedings of the 16th Conference of",
            "year": 2021
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "BERT rediscovers the classical NLP pipeline",
            "venue": "In",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Veit",
                "Michael J Wilber",
                "Serge Belongie."
            ],
            "title": "Residual networks behave like ensembles of relatively shallow networks",
            "venue": "Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.",
            "year": 2016
        },
        {
            "authors": [
                "Elena Voita",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Elena Voita",
                "Pavel Serdyukov",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Context-aware neural machine translation learns anaphora resolution",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Elena Voita",
                "David Talbot",
                "Fedor Moiseev",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Wang",
                "Alexandre Variengien",
                "Arthur Conmy",
                "Buck Shlegeris",
                "Jacob Steinhardt"
            ],
            "title": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
            "year": 2022
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer"
            ],
            "title": "Opt: Open pretrained transformer language models",
            "year": 2022
        },
        {
            "authors": [
                "Sumu Zhao",
                "Damian Pascual",
                "Gino Brunner",
                "Roger Wattenhofer"
            ],
            "title": "Of non-linearity and commutativity in bert",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The range of capabilities of language models expands with scale and at larger scales models become so strong and versatile that a single model can be integrated into various applications and decisionmaking processes (Brown et al., 2020; Kaplan et al., 2020; Wei et al., 2022; Ouyang et al., 2022; OpenAI, 2023; Anil et al., 2023). This increases interest and importance of understanding the internal\n\u2217Work done as part of internship at Meta AI.\nworkings of these large language models (LLMs) and, specifically, their evolution with scale. Unfortunately, scaling also increases the entry threshold for interpretability researchers since dealing with large models requires a lot of computational resources. In this work, we analyze a family of OPT models up to 66b parameters and deliberately keep our analysis very lightweight so that it could be done using a single GPU.\nWe focus on neurons inside FFNs, i.e. individual activations in the representation between the two linear layers of the Transformer feedforward blocks (FFNs). Differently from e.g. neurons in the residual stream, FFN neurons are more likely to represent meaningful features: the elementwise nonlinearity breaks the rotational invariance of this representation and encourages features to align with the basis dimensions (Elhage et al., 2021). When such a neuron is activated, it updates the residual stream by pulling out the corresponding row of the second FFN layer; when it is not activated, it does not update the residual stream (Figure 6).1 Therefore, we can interpret functions of these FFN neurons in two ways: (i) by understanding when they are activated, and (ii) by interpreting the corresponding updates coming to the residual stream.\nFirst, we find that in the first half of the network, many neurons are \u201cdead\u201d, i.e. they never activate on a large collection of diverse data. Larger models are more sparse in this sense: for example, in the 66b model more that 70% of the neurons in some layers are dead. At the same time, many of the alive neurons in this early part of the network are reserved for discrete features and act as indicator functions for tokens and n-grams: they activate if and only if the input is a certain token or an n-gram. The function of the updates coming from these token detectors to the residual stream is also very\n1Since OPT models have the ReLU activation function, the notion of \u201cactivated\u201d or \u201cnot activated\u201d is trivial and means non-zero vs zero.\nar X\niv :2\n30 9.\n04 82\n7v 1\n[ cs\n.C L\n] 9\nS ep\n2 02\n3\nsurprising: at the same time as they promote concepts related to the potential next token candidate (which is to be expected according to Geva et al. (2021, 2022)), they are explicitly targeted at removing information about current input, i.e. their triggers. This means that in the bottom-up processing where a representation of the current input token gets gradually transformed into a representation for the next token, current token identity is removed by the model explicitly (rather than ends up implicitly \u201cburied\u201d as a result of additive updates useful for the next token). To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream.\nFinally, we find that some neurons are responsible for encoding positional information regardless of textual patterns. Similarly to token and n-gram detectors, many of these neurons act as indicator functions of position ranges, i.e. activate for positions within certain ranges and do not activate otherwise. Interestingly, these neurons often collaborate. For example, the second layer of the 125m model has 10 positional neurons whose indicated positional ranges are in agreement: together, they efficiently cover all possible positions and no neuron is redundant. In a broader picture, positional neurons question the key-value memory view of the FFN layers stating that \u201ceach key correlates with textual patterns in the training data and each value induces a distribution over the output vocabulary\u201d (Geva et al., 2021, 2022). Neurons that rely on position regardless of textual pattern indicate that FFN layers can be used by the model in ways that do not fit the key-value memory view. Overall, we argue that the roles played by these layers are still poorly understood.\nOverall, we find neurons that:\n\u2022 are \u201cdead\u201d, i.e. never activate on a large diverse collection of data;\n\u2022 act as token- and n-gram detectors that, in addition to promoting next token candidates, explicitly remove current token information;\n\u2022 encode position regardless of textual content which indicates that the role of FFN layers extends beyond the key-value memory view.\nWith scale, models have more dead neurons and token detectors and are less focused on absolute position."
        },
        {
            "heading": "2 Data and Setting",
            "text": "Models. We use OPT (Zhang et al., 2022), a suite of decoder-only pre-trained transformers that are publicly available. We use model sizes ranging from 125M to 66B parameters and take model weights from the HuggingFace model hub.2\nData. We use data from diverse sources containing development splits of the datasets used in OPT training as well as several additional datasets. Overall, we used (i) subsets of the validation and test part of the Pile (Gao et al., 2020) including Wikipedia, DM Mathematics, HackerNews, (ii) Reddit3 (Baumgartner et al., 2020; Roller et al., 2021), (iii) code data from Codeparrot4.\nFor the experiments in Section 3 when talking about dead neurons, we use several times more data. Specifically, we add more data from Wikipedia, DM Mathematics and Codeparrot, as well as add new domains from the Pile5: EuroParl, FreeLaw, PubMed abstracts, Stackexchange.\nOverall, the data used in Section 3 has over 20M tokens, in the rest of the paper \u2013 over 5M tokens.\nSingle-GPU processing. We use only sets of neuron values for some data, i.e. we run only forward passes of the full model or its several first layers. Since large models do not fit in a single GPU, we load one layer at a time keeping the rest of the layers on CPU. This allows us to record neuron activations for large models: all the main experiments in this paper were done on a single GPU."
        },
        {
            "heading": "3 Dead Neurons",
            "text": "Let us start from simple statistics such as neuron activation frequency (Figure 1).\nMany neurons are \u201cdead\u201d. First, we find that many neurons never activate on our diverse data, i.e. they can be seen as \u201cdead\u201d. Figure 1a shows that the proportion of dead neurons is very substantial: e.g., for the 66b model, the proportion of dead neurons in some layers is above 70%. We also see that larger models are more sparse because (i) they\n2https://huggingface.co/models 3Pushshift.io Reddit dataset is a previously existing dataset extracted and obtained by a third party that contains preprocessed comments posted on the social network Reddit and hosted by pushshift.io.\n4https://huggingface.co/datasets/codeparrot/ codeparrot-clean\n5https://huggingface.co/datasets/EleutherAI/ pile\nhave more dead neurons and (ii) the ones that are alive activate less frequently (Figure 1b).\nOnly first half of the model is sparse. Next, we notice that this kind of sparsity is specific only to early layers. This leads to a clear distinction between the first and the second halves of the network: while the first half contains a solid proportion of dead neurons, the second half is fully \u201calive\u201d. Additionally, layers with most dead neurons are the ones where alive neurons activate most rarely.\nPacking concepts into neurons. This difference in sparsity across layers might be explained by \u201cconcept-to-neuron\u201d ratio being much smaller in the early layers than in the higher layers. Intuitively, the model has to represent sets of encoded in a layer concepts by \u201cspreading\u201d them across available neurons. In the early layers, encoded concepts are largely shallow and are likely to be discrete (e.g., lexical) while at the higher layers, networks learn high-level semantics and reasoning (Peters et al., 2018; Liu et al., 2019; Jawahar et al., 2019; Tenney et al., 2019; Geva et al., 2021). Since the number of possible shallow patterns is not large and, potentially, enumerable, in the early layers the model can (and, as we will see later, does) assign dedicated neurons to some features. The more neurons are available to the model, the easier it is to do so \u2013 this agrees with the results in Figure 1 showing that larger models are more sparse. Differently, the space of fine-grained semantic concepts is too large compared to the number of available neurons which makes it hard to reserve many dedicated neuron-concept pairs.6\nAre dead neurons completely dead? Note that the results in Figure 1a can mean one of the two\n6There can, however, be a few specialized neurons in the higher layers. For example, BERT has neurons responsible for relational facts (Dai et al., 2022).\nthings: (i) these neurons can never be activated (i.e. they are \u201ccompletely dead\u201d) or (ii) they correspond to patterns so rare that we never encountered them in our large diverse collection of data. While the latter is possible, note that this does not change the above discussion about sparsity and types of encoded concepts. On the contrary: it further supports the hypothesis of models assigning dedicated neurons to specific concepts."
        },
        {
            "heading": "4 N-gram-Detecting Neurons",
            "text": "Now, let us look more closely into the patterns encoded in the lower half of the models and try to understand the nature of the observed above sparsity. Specifically, we analyze how neuron activations depend on an input n-gram. For each input text with tokens x1, x2, ..., xS , we record neuron activations at each position and if a neuron is activated (i.e., non-zero) at position k, we say that the n-gram (xk\u2212n+1, . . . , xk) triggered this neuron.\nIn Sections 4.1-4.4 we talk about unigrams (i.e., tokens) and come to larger n-grams in Section 4.5."
        },
        {
            "heading": "4.1 Number of N-grams Triggering a Neuron",
            "text": "First, let us see how many n-grams are able to trigger each neuron. For each neuron we evaluate the number of n-grams that cover at least 95% of the neuron\u2019s activations. For the bottom half of the network, Figure 2 shows how neurons in each layer are categorized by the number of covering them n-grams (we show unigrams here and larger\nn-grams in Appendix A). We see that, as anticipated, neurons in larger models are covered by less n-grams. Also, the largest models have a substantial proportion of neurons that are covered by as few as 1 to 5 tokens. This agrees with our hypothesis in the previous section: the model spreads discreet shallow patterns across specifically dedicated neurons.7"
        },
        {
            "heading": "4.2 Token-Detecting Neurons",
            "text": "Presence of neurons that can be triggered by only a few (e.g., 1-5) tokens point to the possibility that some neurons act as token detectors, i.e. activate if and only if the input is one of the corresponding tokens, regardless of the previous context. To find such neurons, we (1) pick neurons that can be triggered by only 1-5 tokens, (2) gather tokens that are covered by this neuron (if the neuron activates at least 95% of the time the token is present), (3) if altogether, these covered tokens are responsible for at least 95% of neuron activations.8\nFigure 3a shows that there are indeed a lot of token-detecting neurons. As expected, larger models have more such neurons and the 66b model has overall 5351 token detectors. Note that each token detector is responsible for a group of several tokens that, in most of the cases, are variants of the same word (e.g., with differences only in capitalization, presence of the space-before-word special symbol, morphological form, etc.). Figure 5 (top) shows examples of groups of tokens detected by token-detecting neurons.\nInterestingly, the behavior of the largest models (starting from 13b of parameters) differs from that\n7Note that the 350m model does not follow the same pattern as all the rest: we will discuss this model in Section 6.\n8We exclude the begin-of-sentence token from these computations because for many neurons, this token is responsible for the majority of the activations.\nof the rest. While for smaller models the number of token detectors increases then goes down, larger models operate in three monotonic stages and start having many token-detecting neurons from the very first layer (Figures 3). This already shows qualitative differences between the models: with more capacity, larger models perform more complicated reasoning with more distinct stages."
        },
        {
            "heading": "4.3 Ensemble-Like Behaviour of the Layers",
            "text": "Now, let us look at \u201cdetected\u201d tokens, i.e. tokens that have a specialized detecting them neuron. Figure 3b shows the number of detected tokens in each layer as well as cumulative over layers number of detected tokens. We see that, e.g., the 66b model focuses on no more than 1.5k tokens in each layer but over 10k tokens overall. This means that across layers, token-detecting neurons are responsible for largely differing tokens. Indeed, Figure 4 shows that in each following layer, detected tokens mostly differ from all the tokens covered by the layers below. All in all, this points to an ensemblelike (as opposed to sequential) behavior of the layers: layers collaborate so that token-detecting neurons cover largely different tokens in different layers. This divide-and-conquer-style strategy allows larger models to cover many tokens overall and use their capacity more effectively.\nOriginally, such an ensemble-like behavior of deep residual networks was observed in computer vision models (Veit et al., 2016). For transformers, previous evidence includes simple experiments showing that e.g. dropping or reordering layers does not influence performance much (Fan et al., 2020; Zhao et al., 2021)."
        },
        {
            "heading": "4.4 Token Detectors Suppress Their Triggers",
            "text": "Now let us try to understand the role of tokendetecting neurons in the model by interpreting how\nthey update the residual stream. Throughout the layers, token representation in the residual stream gets transformed from the token embedding for the current input token9 to the representation that encodes a distribution for the next token. This transformation happens via additive updates coming from attention and FFN blocks in each layer. Whenever an FFN neuron is activated, the corresponding row of the second FFN layer (multiplied by this neuron\u2019s value) is added to the residual stream (see illustration in Figure 6). By projecting this FFN row onto vocabulary, we can get an interpretation of this update (and, thus, the role of this neuron) in terms of its influence on the output distribution encoded in the residual stream.\n9For OPT models, along with an absolute positional embedding.\nCurrent token suppression: implicit or explicit? Previously, this influence was understood only in terms of the top projections, i.e. tokens that are promoted (Geva et al., 2021, 2022). This reflects an existing view supporting implicit rather than explicit loss of the current token identity over the course of layers. Namely, the view that the current identity gets \u201cburied\u201d as a result of updates useful for the next token as opposed to being removed by the model explicitly. In contrast, we look not only at the top projections but also at the bottom: if these projections are negative, the corresponding tokens are suppressed by the model (Figure 6).\nExplicit token suppression in the model. We find that often token-detecting neurons deliberately suppress the tokens they detect. Figure 5 shows several examples of token-detecting neurons along with the top promoted and suppressed concepts. While the top promoted concepts are in line with previous work (they are potential next token candidates which agrees with Geva et al. (2021, 2022)), the top suppressed concepts are rather unexpected: they are exactly the tokens triggering this neuron. This means that vector updates corresponding to these neurons point in the direction of the next token candidates at the same time as they point away from the tokens triggering the neuron. Note that this is not trivial since these updates play two very different roles at the same time. Overall, for over 80% of token-detecting neurons their corresponding updates point in the negative direction from the triggering them tokens (although, the triggering tokens are not always at the very top suppressed concepts as in the examples in Figure 6).\nOverall, we argue that models can have mech-\nanisms that are targeted at removing information from the residual stream which can be explored further in future work."
        },
        {
            "heading": "4.5 Beyond Unigrams",
            "text": "In Appendix A, we show results for bigrams and trigrams that mirror our observations for unigrams: (i) larger models have more specialized neurons, (ii) in each layer, models cover mostly new n-grams. Interestingly, for larger n-grams we see a more drastic gap between larger and smaller models."
        },
        {
            "heading": "5 Positional Neurons",
            "text": "When analyzing dead neurons (Section 3), we also noticed some neurons that, consistently across diverse data, never activate except for a few first token positions. This motivates us to look further into how position is encoded in the model and, specifically, whether some neurons are responsible for encoding positional information."
        },
        {
            "heading": "5.1 Identifying Positional Neurons",
            "text": "Intuitively, we want to find neurons whose activation patterns are defined by or, at least, strongly depend on token position. Formally, we identify neurons whose activations have high mutual information with position. For each neuron, we evaluate mutual information between two random variables:\n\u2022 act \u2013 neuron is activated or not ({Y,N}),\n\u2022 pos \u2013 token position ({1, 2, . . . , T}).\nFormal setting. We gather neuron activations for full-length data (i.e., T = 2048 tokens) for Wikipedia, DM Mathematics and Codeparrot. Let fr (pos) n be activation frequency of neuron n at position pos and frn be the total activation frequency of this neuron. Then the desired mutual informa-\ntion is as follows:10\nI(act, pos) = 1\nT \u00b7 T\u2211 pos=1 [ fr(pos)n \u00b7 log fr (pos) n frn +\n(1\u2212 fr(pos)n ) \u00b7 log 1\u2212 fr(pos)n 1\u2212 frn\n] .\nChoosing the neurons. We pick neurons with I(act, pos) > 0.05, i.e. high mutual information with position \u2013 this gives neurons whose activation frequency depends on position rather than content. Indeed, if e.g. a neuron is always activated within certain position range regardless of data domain, we can treat this neuron as responsible for position; at least, to a certain extent."
        },
        {
            "heading": "5.2 Types of Positional Neurons",
            "text": "After selecting positional neurons, we categorize them according to their activation pattern, i.e. activation frequency depending on position (Figure 7).\nOscillatory. These neurons are shown in purple in Figure 7. When such a pattern is strong (top row), the activation pattern is an indicator function of position ranges. In other words, such a neuron is activated if and only if the position falls into a certain set. Note that since the activation pattern does not change across data domains, it is defined solely by position and not the presence of some lexical or semantic information.\nBoth types of activation extremes. These are the neurons whose activation pattern is not oscillatory but still has intervals where activation frequency reaches both \u201cactivation extremes\u201d: 0 (never activated) and 1 (always activated). Most frequently, such a neuron is activated only for positions less than or greater than some value and not activated otherwise. Similarly to oscillatory neurons, when\n10For more details, see appendix B.1.\nsuch a pattern is strong (Figure 7, top row), it is also (almost) an indicator function.\nOnly one type of activation extremes. Differently from the previous two types, activation patterns for these neurons can reach only one of the extreme values 0 or 1 (Figure 7, green). While this means that they never behave as indicator functions, there are position ranges where a neuron being activated or not depends solely on token position.\nOther. Finally, these are the neurons whose activation patterns strongly depend on position but do not have intervals where activation frequency stays 0 or 1 (Figure 7, yellow). Typically, these activation patterns have lower mutual information with position than the previous three types.\nStrong vs weak pattern. We also distinguish \u201cstrong\u201d and \u201cweak\u201d versions of each type which we will further denote with color intensity (Figure 7, top vs bottom rows). For the first three types of positional neurons, the difference between strong and weak patterns lies in whether on the corresponding position ranges activation frequency equals 0 (or 1) or close, but not equals, to 0 (or 1). For the last type, this difference lies in how well we can predict activation frequency on a certain position knowing this value for the neighboring positions (informally, \u201cthin\u201d vs \u201cthick\u201d graph)."
        },
        {
            "heading": "5.3 Positional Neurons Across the Models",
            "text": "For each of the models, Figure 8 illustrates the positional neurons across layers.\nSmall models encode position more explicitly. First, we notice that smaller models rely substantially on oscillatory neurons: this is the most frequent type of positional neurons for models smaller than 6.7b of parameters. In combination with many \u201cred\u201d neurons acting as indicator functions for wider position ranges, the model is able to derive token\u2019s absolute position rather accurately. Interestingly, larger models do not have oscillatory neurons and rely on more generic patterns shown with red- and green-colored circles. We can also see that from 13b to 66b, the model loses two-sided red neurons and uses the one-sided green ones more. This hints at one of the qualitative differences between smaller and larger models: while the former encode absolute position more accurately, the latter ones are likely to rely on something more meaningful than absolute position. This complements recent work showing that absolute position encoding is harmful for length generalization in reasoning tasks (Kazemnejad et al., 2023). Differently from their experiments with same model size but various positional encodings, we track changes with scale. We see that, despite all models being trained with absolute positional encodings, stronger models tend to abstract away from absolute position.\nPositional neurons work in teams. Interestingly, positional neurons seem to collaborate to cover the full set of positions together. For example, let us look more closely at the 10 strongly oscillatory neurons in the second layer of the 125m model (shown with dark purple circles in Figure 8). Since they act as indicator functions, we can plot position ranges\nindicated by each of these neurons. Figure 9 shows that (i) indicated position ranges for these neurons are similar up to a shift, (ii) the shifts are organized in a \u201cperfect\u201d order in a sense that altogether, these ten neurons efficiently cover all positions such that none of these neurons is redundant.\nThe two stages within the model. Finally, Figure 8 reveals two stages of up-and-downs of positional information within the model: roughly, the first third of the model and the rest. Interestingly, preferences in positional patterns also change between the stages: e.g., preference for \u201cred\u201d neurons changes to oscillatory purple patterns for the 1.3b and 2.7b models, and \u201cred\u201d patterns become less important in the upper stage for the 13b and 30b models. Note that the first third of the model corresponds to the sparse stage with the dead neurons and n-gram detectors (Sections 3, 4). Therefore, we can hypothesize that in these two stages, positional information is first used locally to detect shallow patterns, and then more globally to use longer contexts and help encode semantic information.\nPreviously, the distinct bottom-up stages of processing inside language models were observed in Voita et al. (2019a). The authors explained that the way representations gain and lose information throughout the layers is defined by the training objective and why, among other things, positional information should (and does) get lost. This agrees with our results in this work: we can see that while there are many positional patterns in the second stage, they are weaker than in the first stage."
        },
        {
            "heading": "5.4 Positional Neurons are Learned Even Without Positional Encoding",
            "text": "Recently, it turned out that even without positional encoding, autoregressive language models still learn positional information (Haviv et al., 2022). We hypothesize that the mechanism these \u201cNoPos\u201d models use to encode position is positional neurons. To confirm this, we train two versions of the 125m model, with and without positional encodings, and\ncompare the types of their positional neurons.\nSetup. We trained 125m models with the standard OPT setup but smaller training dataset: we used OpenWebText corpus (Gokaslan and Cohen, 2019), an open clone of the GPT-2 training data (Radford et al., 2019). This dataset contains 3B tokens (compared 180B for OPT).\nPositional neurons without positional encoding. Figure 10 shows positional neurons in two 125m models: trained with and without positional encoding. We see that, indeed, the model without positional encoding also has many strong positional patterns. Note, however, that the NoPos model does not have oscillatory neurons which, in combination with other positional neurons, allow encoding absolute position rather accurately. This means that the NoPos model relies on more generic patterns, e.g. \u201cred\u201d neurons encoding whether a position is greater/less than some value.\nOscillatory neurons require longer training. Finally, we found that oscillatory patterns appear only with long training. Figure 11 shows positional patterns learned by the baseline 125m model trained for 50k, 150k and 300k training batches. We see that all models have very strong positional patterns, but only the last of them has oscillatory neurons. Apparently, learning absolute position requires longer training time."
        },
        {
            "heading": "5.5 Doubting FFNs as Key-Value Memories",
            "text": "Current widely held belief is that feed-forward layers in transformer-based language models operate as key-value memories. Specifically, \u201ceach key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary\u201d (Geva et al. (2021, 2022); Dai et al. (2022); Meng et al. (2022); Ferrando et al. (2023), among others). While in Section 4.4 we confirmed that this is true for some of the neurons, results in this section reveal that FFN layers can be used by the model in ways that do not fit the key-value memory view. In particular, activations of strong positional neurons are defined by position regardless of textual content, and the corresponding values do not seem to encode meaningful distributions over vocabulary. This means that the role of these neurons is different from matching textual patterns to sets of the next token candidates. In a broader context, this means that the roles played\nby Transformer feed-forward layers are still poorly understood."
        },
        {
            "heading": "6 The 350m Model: The Odd One Out",
            "text": "As we already mentioned above, the 350m model does not follow the same pattern as the rest of the models. Specifically, it does not have dead neurons (Section 3) and its neuron activations do not seem to be sparse with respect to triggering them n-grams as we saw for all the other models in Figure 2.11\nModeling bits affect interpretability. This becomes less surprizing when noticing that the 350m model is implemented differently from all the rest: it applies LayerNorm after attention and feedforward blocks, while all the other models \u2013 before.12 Apparently, such seemingly minor implementation details can affect interpretability of model components rather significantly. Indeed, previous work also tried choosing certain modeling aspects to encourage interpretability. Examples of such work include choosing an activation function to increase the number of interpretable neurons (Elhage et al., 2022), large body of work on sparse softmax variants to make output distributions or attention more interpretable (Martins and Astudillo (2016); Niculae and Blondel (2017); Peters et al.\n11There are, however, positional neurons; see Figure 16 in Appendix B.2).\n12https://github.com/huggingface/transformers/ blob/main/src/transformers/models/opt/modeling_ opt.py\n(2019); Correia et al. (2019); Martins et al. (2020), among others), or more extreme approaches with explicit modular structure that is aimed to be interpretable by construction (Andreas et al. (2016); Hu et al. (2018); Kirsch et al. (2018); Khot et al. (2021), to name a few). Intuitively, choosing ReLU activation function as done in the OPT models can be seen as having the same motivation as developing sparse softmax variants: exact zeros in the model are inherently interpretable."
        },
        {
            "heading": "7 Additional Related Work",
            "text": "Historically, neurons have been a basic unit of analysis. Early works started from convolutional networks first for images (Krizhevsky et al., 2012) and later for convolutional text classifiers (Jacovi et al., 2018). Similar to our work, Jacovi et al. (2018) also find n-gram detectors; although, for small convolutional text classifiers this is an almost trivial observation compared to large Transformerbased language models as in our work. For recurrent networks, interpretable neurons include simple patterns such as line lengths, brackets and quotes (Karpathy et al., 2015), sentiment neuron (Radford et al., 2017) and various neurons in machine translation models, such as tracking brackets, quotes, etc, as well as neurons correlated with higher-level concepts e.g. verb tense (Bau et al., 2019). For Transformer-based BERT, Dai et al. (2022) find that some neurons inside feedforward blocks are responsible for storing factual knowledge. Larger units of analysis include attention blocks (Voita et al. (2018, 2019b); Clark et al. (2019); Kovaleva et al. (2019); Baan et al. (2019); Correia et al. (2019), etc), feed-forward layers (Geva et al., 2021, 2022) and circuits responsible for certain tasks (Wang et al., 2022; Geva et al., 2023; Hanna et al., 2023)."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors thank Nicola Cancedda, Yihong Chen, Igor Tufanov and FAIR London team for fruitful discussions and helpful feedback."
        },
        {
            "heading": "A N-gram-Detecting Neurons",
            "text": "A.1 Number of N-grams Triggering a Neuron\nFigure 12 shows how neurons in each layer are categorized by the number of covering them bigrams, Figure 13 \u2013 trigrams. As expected, neurons in larger models are covered by less n-grams.\nA.2 Trigram-Detecting Neurons\nSimilarly to token-detecting neurons in Section 4.2, we also find neurons that are specialized on 3- grams. Specifically, we (1) pick neurons that are covered by only 1-50 trigrams, (2) gather trigrams that are covered by this neuron (if the neuron activated at least 95% of the time the trigram is present), (3) if altogether, these covered trigrams are responsible for at least 95% of neuron activa-\ntions. Figure 14 shows the results. Overall, the results further support our main observations: larger models have more neurons responsible for n-grams. Interestingly, when looking at trigrams rather than tokens, at 30b of parameters we see a drastic jump in the number of covered n-grams. This indicates that one of the qualitative differences between larger and smaller models lies in the expansion of the families of features they are able to represent.\nA.3 Ensemble-Like Layer Behavior\nFigure 15 shows the number of covered trigrams in each layer. We see that in each layer, models cover largely new trigrams."
        },
        {
            "heading": "B Positional Neurons",
            "text": "B.1 Mutual Information\nFor each neuron, we evaluate mutual information between two random variables:\n\u2022 act \u2013 neuron is activated or not ({Y,N}),\n\u2022 pos \u2013 token position ({1, 2, . . . , T}).\nFormal setting. We gather neuron activations for full-length data (i.e., T = 2048 tokens) for Wikipedia, DM Mathematics and Codeparrot. Let fr (pos) n be activation frequency of neuron n at position pos and frn be the total activation frequency of this neuron.\nThen the desired mutual information is as follows:\nI(act, pos) =\n= \u2211 act T\u2211 pos=1\n1 p(pos) p(act|pos) \u00b7 log p(act|pos) p(act) =\nSince we only feed full-length texts, all positions appear with the same frequency: p(pos) = 1/T .\n= 1\nT \u00b7 \u2211 act\u2208{Y,N} T\u2211 pos=1 p(act|pos)\u00b7log p(act|pos) p(act) =\n= 1\nT \u00b7 T\u2211 pos=1 p(act = Y |pos)\u00b7log p(act = Y |pos) p(act = Y ) +\n1 T \u00b7 T\u2211\npos=1\n(1\u2212p(act = Y |pos))\u00b7log 1\u2212p(act=Y |pos) 1\u2212 p(act = Y ) =\n= 1\nT \u00b7 T\u2211 pos=1 [ fr(pos)n \u00b7 log fr (pos) n frn +\n(1\u2212 fr(pos)n ) \u00b7 log 1\u2212 fr(pos)n 1\u2212 frn\n] .\nB.2 Positional Neurons for the 350m Model The results are shown in Figure 16.\nFigure 16: Positional neurons in the 350m model. Each circle corresponds to a single neuron, colors and their intensity correspond to the types of patterns shown in Figure 7."
        }
    ],
    "title": "Neurons in Large Language Models: Dead, N-gram, Positional",
    "year": 2023
}