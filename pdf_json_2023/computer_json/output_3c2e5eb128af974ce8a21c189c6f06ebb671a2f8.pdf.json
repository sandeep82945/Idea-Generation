{
    "abstractText": "We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human-interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-inthe-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to improve the quality of the output significantly, in terms of listener preference (4:1).",
    "authors": [
        {
            "affiliations": [],
            "name": "Dan Andrei Iliescu"
        },
        {
            "affiliations": [],
            "name": "Devang Savita Ram Mohan"
        },
        {
            "affiliations": [],
            "name": "Tian Huey Teh"
        },
        {
            "affiliations": [],
            "name": "Zack Hodari"
        }
    ],
    "id": "SP:48257298c31e89a5331969197f5cb8bad0b01603",
    "references": [
        {
            "authors": [
                "X. An",
                "F.K. Soong",
                "S. Yang",
                "L. Xie"
            ],
            "title": "Effective and direct control of neural TTS prosody by removing interactions between different attributes",
            "venue": "Neural Networks,",
            "year": 2021
        },
        {
            "authors": [
                "M. Babianski",
                "K. Pokora",
                "R. Shah",
                "R. Sienkiewicz",
                "D. Korzekwa",
                "V. Klimkov"
            ],
            "title": "On granularity of prosodic representations in expressive text-to-speech",
            "year": 2022
        },
        {
            "authors": [
                "Y. Bengio",
                "A. Courville",
                "P. Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "R. Bommasani",
                "D.A. Hudson",
                "E. Adeli",
                "R. Altman",
                "S. Arora",
                "S. von Arx",
                "M.S. Bernstein",
                "J. Bohg",
                "A. Bosselut",
                "E Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 1877
        },
        {
            "authors": [
                "G.A. Bryant"
            ],
            "title": "Verbal irony in the wild",
            "venue": "Pragmatics and Cognition,",
            "year": 2011
        },
        {
            "authors": [
                "S. Calhoun"
            ],
            "title": "The centrality of metrical structure in signaling information structure: A probabilistic perspective",
            "year": 2010
        },
        {
            "authors": [
                "C. Chai",
                "G. Li"
            ],
            "title": "Human-in-the-loop techniques in machine learning",
            "venue": "IEEE Data Eng. Bull.,",
            "year": 2020
        },
        {
            "authors": [
                "Choi",
                "H.-S",
                "J. Yang",
                "J. Lee",
                "H. Kim"
            ],
            "title": "Nansy++: Unified voice synthesis with neural analysis and synthesis",
            "venue": "arXiv preprint arXiv:2211.09407,",
            "year": 2022
        },
        {
            "authors": [
                "O. Elharrouss",
                "N. Almaadeed",
                "S.A. Al-Maadeed",
                "Y. Akbari"
            ],
            "title": "Image inpainting: A review",
            "venue": "Neural Processing Letters,",
            "year": 2019
        },
        {
            "authors": [
                "E. Gironzetti"
            ],
            "title": "Prosodic and multimodal markers of humor",
            "venue": "In The Routledge handbook of language and humor,",
            "year": 2017
        },
        {
            "authors": [
                "A. Gravano",
                "J. Hirschberg"
            ],
            "title": "Turn-yielding cues in taskoriented dialogue",
            "venue": "In Proc. SIGDIAL,",
            "year": 2009
        },
        {
            "authors": [
                "G.E. Henter",
                "J. Lorenzo-Trueba",
                "X. Wang",
                "J. Yamagishi"
            ],
            "title": "Deep encoder-decoder models for unsupervised learning of controllable speech synthesis",
            "venue": "arXiv preprint arXiv:1807.11470,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Hodari"
            ],
            "title": "Synthesising prosody with insufficient context. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Z. Hodari",
                "O. Watts",
                "S. King"
            ],
            "title": "Using generative modelling to produce varied intonation for speech synthesis",
            "venue": "In Proc. Speech Synthesis Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Hodari",
                "C. Lai",
                "S. King"
            ],
            "title": "Perception of prosodic variation for speech synthesis using an unsupervised discrete representation of F0",
            "venue": "In Proc. Speech Prosody,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Hodari",
                "A. Moinet",
                "S. Karlapati",
                "J. Lorenzo-Trueba",
                "T. Merritt",
                "A. Joly",
                "A. Abbas",
                "P. Karanasou",
                "T. Drugman"
            ],
            "title": "CAMP: A two-stage approach to modelling prosody in context",
            "venue": "In Proc. International Conference on Speech and Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Hsu",
                "W.-N",
                "Y. Zhang",
                "J.R. Glass"
            ],
            "title": "Unsupervised learning of disentangled and interpretable representations from sequential data",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Hsu",
                "W.-N",
                "Y. Zhang",
                "R. Weiss",
                "H. Zen",
                "Y. Wu",
                "Y. Cao",
                "Y. Wang"
            ],
            "title": "Hierarchical generative modeling for controllable speech synthesis",
            "venue": "In Proc. International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In Proc. International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "N. Kalchbrenner",
                "E. Elsen",
                "K. Simonyan",
                "S. Noury",
                "N. Casagrande",
                "E. Lockhart",
                "F. Stimberg",
                "A. van den Oord",
                "S. Dieleman",
                "K. Kavukcuoglu"
            ],
            "title": "Efficient neural audio synthesis",
            "venue": "arXiv preprint arXiv:1802.08435,",
            "year": 2018
        },
        {
            "authors": [
                "M. Kang",
                "J. Lee",
                "S. Kim",
                "I. Kim"
            ],
            "title": "Fast DCTTS: Efficient deep convolutional text-to-speech",
            "venue": "arXiv preprint arXiv:2104.00624,",
            "year": 2021
        },
        {
            "authors": [
                "S. Karlapati",
                "P. Karanasou",
                "M. Lajszczak",
                "A. Abbas",
                "A. Moinet",
                "P. Makarov",
                "R. Li",
                "A. van Korlaar",
                "S. Slangen",
                "T. Drugman"
            ],
            "title": "Copycat2: A single model for multi-speaker tts and many-to-many fine-grained prosody transfer",
            "year": 2022
        },
        {
            "authors": [
                "M. Kim",
                "S.J. Cheon",
                "B.J. Choi",
                "J.J. Kim",
                "N.S. Kim"
            ],
            "title": "Expressive text-to-speech using style tag",
            "venue": "In Proc. Interspeech",
            "year": 2021
        },
        {
            "authors": [
                "V. Klimkov",
                "S. Ronanki",
                "J. Rohnke",
                "T. Drugman"
            ],
            "title": "Fine-grained robust prosody transfer for single-speaker neural text-to-speech",
            "venue": "In Proc. Interspeech,",
            "year": 2019
        },
        {
            "authors": [
                "M. Mirza",
                "S. Osindero"
            ],
            "title": "Conditional generative adversarial nets",
            "venue": "arXiv preprint arXiv:1411.1784,",
            "year": 2014
        },
        {
            "authors": [
                "D.S.R. Mohan",
                "V. Hu",
                "T.H. Teh",
                "A. Torresquintero",
                "C.G.R. Wallis",
                "M. Staib",
                "L. Foglianti",
                "J. Gao",
                "S. King"
            ],
            "title": "Ctrl-P: Temporal control of prosodic variation for speech synthesis",
            "venue": "In Proc. Interspeech, Brno, Czech Republic,",
            "year": 2021
        },
        {
            "authors": [
                "Oord",
                "A. v. d",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "L. Ouyang",
                "J. Wu",
                "X. Jiang",
                "D. Almeida",
                "C.L. Wainwright",
                "P. Mishkin",
                "C. Zhang",
                "S. Agarwal",
                "K. Slama",
                "A Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "arXiv preprint arXiv:2203.02155,",
            "year": 2022
        },
        {
            "authors": [
                "D. Povey",
                "A. Ghoshal",
                "G. Boulianne",
                "L. Burget",
                "O. Glembek",
                "N. Goel",
                "M. Hannemann",
                "P. Motlicek",
                "Y. Qian",
                "P. Schwarz",
                "J. Silovsky",
                "G. Stemmer",
                "K. Vesely"
            ],
            "title": "The kaldi speech recognition toolkit",
            "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding",
            "year": 2011
        },
        {
            "authors": [
                "N. Prateek",
                "M. \u0141ajszczak",
                "R. Barra-Chicote",
                "T. Drugman",
                "J. Lorenzo-Trueba",
                "T. Merritt",
                "S. Ronanki",
                "T. Wood"
            ],
            "title": "In other news: A bi-style text-to-speech model for synthesizing newscaster voice with limited data",
            "venue": "arXiv preprint arXiv:1904.02790,",
            "year": 2019
        },
        {
            "authors": [
                "T. Raitio",
                "R. Rasipuram",
                "D. Castellani"
            ],
            "title": "Controllable Neural Text-to-Speech Synthesis Using Intuitive Prosodic Features",
            "venue": "In Proc. Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "A. Ramesh",
                "P. Dhariwal",
                "A. Nichol",
                "C. Chu",
                "M. Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "Liu",
                "T.-Y"
            ],
            "title": "FastSpeech 2: Fast and high-quality end-to-end text-tospeech",
            "venue": "arXiv preprint arXiv:2006.04558,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ren",
                "M. Lei",
                "Z. Huang",
                "S. Zhang",
                "Q. Chen",
                "Z. Yan",
                "Z. Zhao"
            ],
            "title": "ProsoSpeech: Enhancing prosody with quantized vector pre-training in text-to-speech",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "M. Schr\u00f6der"
            ],
            "title": "Emotional speech synthesis: A review",
            "venue": "In Proc. Eurospeech, Aalborg,",
            "year": 2001
        },
        {
            "authors": [
                "Y. Shin",
                "Y. Lee",
                "S. Jo",
                "Y. Hwang",
                "T. Kim"
            ],
            "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
            "venue": "In Proc. Interspeech 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A. Sini",
                "S.L. Maguer",
                "D. Lolive",
                "E. Delais-Roussarie"
            ],
            "title": "Introducing prosodic speaker identity for a better expressive speech synthesis control",
            "venue": "Speech Prosody",
            "year": 2020
        },
        {
            "authors": [
                "K. Sohn",
                "H. Lee",
                "X. Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "year": 2015
        },
        {
            "authors": [
                "G. Sun",
                "Y. Zhang",
                "R.J. Weiss",
                "Y. Cao",
                "H. Zen",
                "Y. Wu"
            ],
            "title": "Fully-hierarchical fine-grained prosody modeling for interpretable speech synthesis",
            "venue": "In Proc. International Conference on Speech and Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "D. Talkin",
                "W.B. Kleijn"
            ],
            "title": "A robust algorithm for pitch tracking (RAPT)",
            "venue": "Speech coding and synthesis,",
            "year": 1995
        },
        {
            "authors": [
                "P. Taylor"
            ],
            "title": "Text-to-speech synthesis",
            "venue": "Cambridge university press,",
            "year": 2009
        },
        {
            "authors": [
                "T. Tran"
            ],
            "title": "Neural Models for Integrating Prosody in Spoken Language Understanding",
            "venue": "PhD thesis, University of Washington,",
            "year": 2020
        },
        {
            "authors": [
                "M. Tschannen",
                "O. Bachem",
                "M. Lucic"
            ],
            "title": "Recent advances in autoencoder-based representation learning",
            "venue": "Third workshop on Bayesian Deep Learning (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "A. van den Oord",
                "O. Vinyals",
                "K. Kavukcuoglu"
            ],
            "title": "Neural discrete representation learning",
            "venue": "In Proc. International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "M. Wagner",
                "D.G. Watson"
            ],
            "title": "Experimental and theoretical advances in prosody: A review",
            "venue": "Language and cognitive processes,",
            "year": 2010
        },
        {
            "authors": [
                "V. Wan",
                "C. Chan",
                "T. Kenter",
                "J. Vit",
                "R. Clark"
            ],
            "title": "CHiVE: Varying prosody in speech synthesis with a linguistically driven dynamic hierarchical conditional variational network",
            "venue": "In Proc. International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wang",
                "D. Stanton",
                "Y. Zhang",
                "R. Skerry-Ryan",
                "E. Battenberg",
                "J. Shor",
                "Y. Xiao",
                "F. Ren",
                "Y. Jia",
                "R.A. Saurous"
            ],
            "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
            "venue": "arXiv preprint arXiv:1803.09017,",
            "year": 2018
        },
        {
            "authors": [
                "C. Winkler",
                "D. Worrall",
                "E. Hoogeboom",
                "M. Welling"
            ],
            "title": "Learning likelihoods with conditional normalizing flows, 2020",
            "year": 2020
        },
        {
            "authors": [
                "X. Wu",
                "L. Xiao",
                "Y. Sun",
                "J. Zhang",
                "T. Ma",
                "L. He"
            ],
            "title": "A survey of human-in-the-loop for machine learning",
            "venue": "Future Generation Computer Systems,",
            "year": 2022
        },
        {
            "authors": [
                "D. Xin",
                "L. Ma",
                "J. Liu",
                "S. Macke",
                "S. Song",
                "A. Parameswaran"
            ],
            "title": "Accelerating human-in-the-loop machine learning: Challenges and opportunities",
            "venue": "In Proceedings of the second workshop on data management for end-to-end machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "J. Yamagishi",
                "T. Masuko",
                "T. Kobayashi"
            ],
            "title": "HMM-based expressive speech synthesis - towards TTS with arbitrary speaking styles and emotions",
            "venue": "In Proc. of Special Workshop in Maui (SWIM),",
            "year": 2004
        },
        {
            "authors": [
                "J. Yu",
                "Z. Lin",
                "J. Yang",
                "X. Shen",
                "X. Lu",
                "T.S. Huang"
            ],
            "title": "Generative image inpainting with contextual attention",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Many applications of deep learning are either supported by human effort or designed as tools for humans (Wu et al., 2022). However, for models that generate highly-structured data\u2014such as text, speech, or images\u2014humans cannot easily fix or change the model\u2019s output. This is because the features in high-dimensional data are strongly correlated, so changing one requires changing many others. To allow human intervention, models must combine human intuition with statistical patterns learned by deep models.\nControlling generative models is an active area of research (Sohn et al., 2015; Mirza & Osindero, 2014; Winkler et al., 2020). Existing control methods fall on a spectrum from fine-grained to coarse-grained. For example, in-painting for image generation (Rombach et al., 2022) represents finegrained control: users provide information relating to the precise location in an image. In contrast, prompting of generative language models (Ouyang et al., 2022) is more\nAudio (waveform)\nTraining data\nOutput prosody (acoustic feats)\nLatent space\nSparse control space (acoustic feats)\nUser\nFigure 1. We propose a framework for controlling prosody in textto-speech systems based on missing value imputation on sparse inputs from the user. Control with sparse inputs is efficient, robust and faithful.\ncoarse-grained as it impacts more global changes in the model\u2019s output.\nWe propose that sparse inputs are an effective solution for fine-grained control of highly-structured data. When forced to provide a full set of conditioning inputs, the user may choose values that are imprecise or self-contradicting. Instead, we propose a framework whereby the user provides a few input values and the model fills in the rest.\nOur approach to this formulation of the problem is the Multiple Instance Conditional Variational Autoencoder (MICVAE). This model is uniquely suited to conditioning on the sparse values provided by a human user. We will refer to the values provided by the user as \u201cdriving\u201d values, since they are the interface through which the user controls the model.\nSince we are the first to propose a solution to HitL control by conditioning on sparse inputs, we formulate some requirements for what constitutes a successful model. These\nar X\niv :2\n30 3.\n09 44\n6v 1\n[ ee\nss .A\nS] 1\n4 M\nar 2\n02 3\ntraits are used to inform both the design of our proposed model and the evaluation methods.\n1. Efficiency. The user should be able to achieve the desired output with only a few input values. We measure efficiency as the improvement brought to the output by each additional driving value.\n2. Robustness. The model should allow users to control the output in a predictible way regardless of the pattern in which users choose to drive values. Users may want to provide any values in any pattern. The model should generate realistic outputs under any conditions.\n3. Faithfulness. The output should agree with the driving values; i.e. the model should generate a valid datapoint that is closest to the driving values provided by the user.\nIn particular, we explore sparse HitL control for the task of prosody generation. Prosody is a key component in human speech as it can convey emotion, attitude, a speaker\u2019s intentions, and other communicative functions (Hodari, 2022, Chapter 2). Prosody is communicated through perceived changes in intonation, loudness, phrasing, timing, and voice quality (e.g. creaky voice) (Wagner & Watson, 2010). These perceptual factors are hard to measure, and so are typically approximated using acoustic correlates, such as: fundamental frequency (F0; correlate of intonation), energy (correlate of loudness), and phoneme and silence durations (correlate of phrasing and timing) (Mohan et al., 2021).\nThese three acoustic correlates can be extracted automatically from the speech signal and they form the basis for the control space of our model. We refer to them as Prosodic Acoustic Features (PAFs).\nSparse human-in-the-loop control adds a priori human domain expertise on top of statistical patterns learned directly from the data. Text-to-speech models can benefit greatly from human intuition and knowledge when it comes to choosing the right rendition for a particular sentence. Different possible renditions of a single utterance can be seen in Figure 4. However, existing conditioning paradigms are either imprecise or require manual completion of inputs that can be more easily done by a statistical model.\nWe design a range of quantitative evaluation procedures to validate that MICVAE is an effective method for HitL control. Firstly, we introduce a procedure called iterative refinement as a proxy for human control in order to evaluate efficiency. Next, we demonstrate the robustness of MICVAE to different patterns of sparsity by comparing with an equivalent CVAE model that does not use the Multiple-Instance encoder. Finally, we measure faithfulness through subjective listening tests to assess the perceived change in prosody resulting from a small number of driving PAFs.\nWe present the following contributions:\n\u2022 We propose MICVAE, a novel probabilistic generative model for sparse of human-in-the-loop control that is robust to differing patterns of sparsity.\n\u2022 We demonstrate that using sparsity as an interface for control leads to efficient and faithful HitL control.\n\u2022 We introduce iterative refinement, a novel automated evaluation procedure designed to simulate HitL control of generative models."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Human-in-the-loop Control",
            "text": "Research combining human effort with machine learning is increasing rapidly (Wu et al., 2022). This is in part because the development of deep learning models is an iterative process (Xin et al., 2018). Iterative labelling is one way to speed up this iteration using human effort, where humans label only the most informative data before re-training the model and repeating (Chai & Li, 2020). The human-in-theloop can be responsible for quality control; i.e. correcting mistakes made by the model. Alternatively, they may provide their preferred solution\u2014important for generation tasks where there is no single right answer, such as text, speech, and image generation.\nUnfortunately, there is limited literature on the use of humans for supporting models at inference time. This is likely because doing so is expensive and reserved for revenuegenerating products, something about which companies publish fewer details. Despite this, there are many examples of companies using HitL to support inference (and likely closing the loop using iterative labeling), such as Lilt, Rev, Verbit, Unbabel, and Speechmatics.\nHowever, in the case of highly structured data, it is prohibitively costly for humans to correct or create outputs manually. For example, a human could not easily correct the reflection on a surface in a generated image. Currently there are a few approaches to resolve this: one is to simply collect more data at additional expense, or to have humans undertake the slow and imperfect task of fixing mistakes in predictions of highly structured data. In computer vision, inpainting provides a solution (Elharrouss et al., 2019)."
        },
        {
            "heading": "2.2. Prosody",
            "text": "Prosody serves many communicative purposes: augmenting the lexical content (Calhoun, 2010; Tran, 2020; Clark & Brennan, 1991) and conveying additional meaning (Gravano & Hirschberg, 2009; Bryant, 2011; Gironzetti, 2017; Ben-David et al., 2016). Prosody is communicated through suprasegmental changes in speech. This perceived varia-\ntion includes intonation, loudness, timing, and voice quality. These perceptual correlates are often modelled with acoustic correlates such as F0, energy, durations, and spectral tilt (Wan et al., 2019; Klimkov et al., 2019; Ren et al., 2020; Raitio et al., 2020; Mohan et al., 2021).\nProsody prediction is a challenging task as we lack the relevant context information that humans use to plan prosodic choices (Goodwin et al., 1992; Wagner & Watson, 2010). Furthermore, even if we were to provide additional contextual information, there may be no single best prosodic rendition for a given situation (Hodari et al., 2019)."
        },
        {
            "heading": "2.3. Interpretable representation learning",
            "text": "Learnt representations can be useful for a variety of downsteam tasks (Bommasani et al., 2021). Many representation learning methods have been proposed specifically for speech (van den Oord et al., 2017; Oord et al., 2018; Baevski et al., 2020; Hsu et al., 2021) and prosody (Wang et al., 2018; Kang et al., 2021; Hodari et al., 2021; Karlapati et al., 2022; Babianski et al., 2022).\nHowever, not all representations are amenable for human interaction. Various methods can be applied to encourage interpretability (Bengio et al., 2013; Tschannen et al., 2018). Structure can be imposed on the latent variables through independence assumptions (An et al., 2021) and hierarchical assumptions (Hsu et al., 2017; 2019). Self-supervised losses can also be used to disentangle concepts (Sun et al., 2020; Choi et al., 2022). Discrete latent variables are another useful method to make representations more usable for humans (van den Oord et al., 2017; Hodari et al., 2020; Ren et al., 2022).\nInstead of striving for interpretable representations, it is also possible to focus on the requirements of specific downstream tasks. For prosody control, categorical control of representations has been successful in providing an interface for human interaction, such as for emotions (Schro\u0308der, 2001; Yamagishi et al., 2004; Henter et al., 2018) and styles (Prateek et al., 2019; Kim et al., 2021; Shin et al., 2022)."
        },
        {
            "heading": "2.4. Conditioning Generative Models",
            "text": "Generative models are a broad class of probabilistic models known for their ability to produce realistic unconditional samples for text (Brown et al., 2020), images (Rombach et al., 2022; Ramesh et al., 2022), and speech (van den Oord et al., 2017).\nFor many applications, conditional sampling\u2014i.e. control\u2014 is necessary. Fortunately, variational inference provides the tools to derive conditional generative models, for example: conditional variational autoencoders (Sohn et al., 2015), conditional generative adversarial networks (Mirza & Osindero, 2014), and conditional normalising flows (Winkler\net al., 2020).\nConditional generation has made great strides using text prompts for conditional sampling (Ouyang et al., 2022; Ramesh et al., 2022; Kim et al., 2021). Image in-painting is another form of conditional sampling, where part of an image is occluded and the available portions act as the conditioning information (Yu et al., 2018; Ramesh et al., 2022)."
        },
        {
            "heading": "3. Multiple-Instance CVAE",
            "text": "In order to enable users to perform sparse control for highlystructured data, we propose a novel generative model. Our model, called Multiple Instance Conditional Variational Autoencoder (MICVAE), sits within the paradigm of the Conditional VAE (Sohn et al., 2015). The high-level architecture is illustrated in Figure 2, where user driving values are encoded into a latent space and then decoded conditional on phone embeddings into a full PAF sequence.\nMICVAE\u2019s encoder is designed specifically for sparse control. To support HitL control of highly structured data, MICVAE must be capable of dealing with sparse information to allow for efficient control. This is achieved by embedding the sparse driving PAFs independently and combining them into a sentence-level latent space using self-attention. In other words, we represent the K driving values as a bag-offeatures.\nMICVAE should be faithful to the intention of the user where possible. However, manually specifying values for highly-structured data in a consistent fashion is difficult, meaning user driven values may conflict with what is pos-\nsible in reality. By using a CVAE architecture, MICVAE\u2019s probabilistic encoder will map user driven values to an approximate posterior which maximises the likelihood of the inputs. Thus, MICVAE is not blindly faithful to user driven values, but finds the point on the data manifold that is closest to the inputs. This point on the manifold can then be decoded to produce consistent PAFs."
        },
        {
            "heading": "3.1. Multiple-Instance Encoder",
            "text": "The unique component of our model is its encoder, which comprises a self-attention mechanism adapted from Ilse et al. (2018). The encoder architecture is illustrated in Figure 3.\nThe encoder works by separately computing a value vector vk \u2208 RD for each driving PAF xk \u2208 R, where k \u2208 {1 : K} counts the driving PAFs. The results are then aggregated into a single embedding for the entire sentence z\u2032 \u2208 RD using a weighted average. z\u2032 is then projected to parameterise a latent variable. K is the number of driving PAFs in the sentence, so K \u2264 T \u00d7 3. T is the number of phones in\nthe sentence and 3 is the number of PAF dimensions (F0, energy and duration). The attention weights ak \u2208 [0, 1] are computed as a softmax of the key-query products bk. The softmax operation ensures that the result is invariant to the number of inputs provided, and thus to the pattern of sparsity.\nz\u2032 = K\u2211 k=1 ck = K\u2211 k=1 akvk, ak = exp(bk)\u2211K l=1 exp(bk)\n(1)\nz = N (z\u00b5, z\u03c3), z\u00b5 = Uz\u2032T , z\u03c3 = Sz\u2032T (2)\nThe key-query products bk \u2208 RD are computed in the specific way introduced by Ilse et al. (2018), so that we obtain an attention vector instead of an attention matrix. This is because we want our results to aggregate into a single latent vector rather than a set. To achieve this, we perform a weighted dot-product between keys and queries instead of a cross-product. We use a different non-linearity for each one in order to break the symmetry.\nbk = w T (tanh(QhTk ) sigm(Kh T k )) (3)\nThe value vector vk is also computed by passing the intermediate representation hk \u2208 RH through a perceptron:\nvk = tanh(Vh T k ) (4)\nThe final piece of the puzzle is generating the intermediate representations hk in a way that captures the location in the sentence and feature type of xk. For this, we concatenate each driving PAF xk with two positional encodings, one for the location pk \u2208 RP and the other for the type of feature fk \u2208 RF (F0, energy or duration). The location encoding pk is a sinusoidal series that follows the standard approach of Vaswani et al. (2017), while the feature encoding fk consists of one of three learned vectors.\nhk = ReLU(E [xk,pk, fk] T ) (5)\nThe trainable parameters of our model are the linear mappings (K,Q) \u2208 RL\u00d7H , w \u2208 RD\u00d7L, V \u2208 RD\u00d7H ,E \u2208 RH\u00d7(1+P+F ), and the feature encodings f \u2208 RF\u00d73. We choose the following values for the dimensional hyperparameters: H = 64, D = 32, L = 64, F = 8, P = 8."
        },
        {
            "heading": "3.2. Content Encoders",
            "text": "In addition to the Multiple-Instance encoder used to enable sparse control, our model uses three other encoders to capture the content that is being generated. The largest of these\nis the phone encoder which captures the pronunciation of the words to be synthesised. Our phone encoder projects one-hot phone identity features in 384 dimensions, this is followed by 3 1-dimensional convolution banks with a kernel size of 5 and batch norm after each convolution (Ioffe & Szegedy, 2015), the embeddings are then passed through a bi-directional LSTM. The final phone embeddings are 384 dimensions. Our speaker encoder simply projects the onehot speaker identity to 32 dimensions. Our style encoder projects the one-hot style labels to 16 dimensions.\nFinally, the speaker embedding and style embedding are upsampled to phone level (by repeating it) and summed with the phone embeddings to create the content embeddings."
        },
        {
            "heading": "3.3. Prosodic Acoustic Feature Decoder",
            "text": "The decoder of MICVAE generates the prosodic acoustic features (PAFs) using a sample from the approximate posterior, i.e. z\u0303 \u223c N (z\u00b5, z\u03c3), and the content embeddings. The sample z\u0303 is upsampled to phone level and concatenated with the content embeddings, this is the input for the decoder.\nOur PAF decoder has the same architecture as the acoustic feature predictor from Mohan et al. (2021). Using 4 bidirectional GRU layers with dimensionality: 64, 64, 32, 32. This is followed by a 16-dimensional perceptron with a tanh non-linearity, and finally projected to 3 dimensions, which represents the number of prosodic features: F0, energy, and duration."
        },
        {
            "heading": "4. Baseline: Masked CVAE",
            "text": "We designed MICVAE to be robust to patterns of sparsity. However, it is hard to benchmark whether it is more or less robust to human behaviour without a baseline. We introduce Masked CVAE as a straightforward baseline for sparse control.\nMasked CVAE has the same architecture as MICVAE, except with a different encoder, and thus different method of supporting sparse control. Masked CVAE\u2019s encoder is a multi-layer Recurrent Neural Network. In Masked CVAE, each of the three feature streams has an additional binary feature concatenated, which represents whether that value is driven or missing. Any driven value is assigned a boolean value of 1, and all other positions are set to 0. Masked CVAE\u2019s encoder takes a sequence of 6-dimensional vectors as input (as opposed to 3 for MICVAE). The additional 3 dimensions are used to indicate whether each corresponding PAF is driven or missing.\nMasked CVAE is not invariant to the number of driven values like MICVAE is, thus during training it must be exposed to examples with sparse control. To train Masked CVAE we select a fixed percentage P of sparsity and for\neach sentence during training we randomly mask out P% of PAF values. In this way the model is able to support sparse control."
        },
        {
            "heading": "5. Evaluation",
            "text": "For sparse control to be useful it must have several qualities: efficiency, robustness, and faithfulness. We design different objective and subjective evaluations to assess how MICVAE performs on these 3 criteria.\nWe simulate human-in-the-loop control using prosody extracted from a dataset of prosodic renditions. The ultimate use-case for our system is human-driven control, we use this automated evaluation as a proxy to 1) ensure that the experiments are reproducible and 2) enable larger scale experiments which would be costly to run using with humans.\nBy simulating HitL control, we are hoping to achieve a proof of concept of sparse control that can be easily reproduced. We do not claiming to perfectly reproduce human control, and we leave UX experiments with human users for future work."
        },
        {
            "heading": "5.1. Experimental Setup",
            "text": "In order to evaluate control in a reproducible and scalable way, we simulate it using prosodic features extracted from human speech. Some of those features are fed as input to the models in a sparse fashion. We call those features the \u201cdriving\u201d features.\nBy simulating human control, we are able to run our qualitative assessments for hundreds of utterances and many different conditions; it is also possible to easily reproduce experiments. Our quantitative results and the stimuli used for our subjective listening tests that follow are all generated using this method.\nWe created a dataset of prosodic renditions specifically for simulating prosody control. 30 voice actors performed different utterances in the test set, such that we gathered 10 renditions of each test utterance. We extracted the F0, energy, and duration for all renditions. The average length of a sentence is ~80 phonemes.\nTo simulate control for a given sentence in the test set, a rendition is chosen and a certain subset of values are selected to be driven. The driven values are actually extracted from a rendition of the same sentence by a different actor. We use this prosodic feature transplant during evaluation as a proxy for the way a human might have controlled the speech generation. To our knowledge, we are the first to propose such an automatic evaluation procedure to simulate human input.\nThe presence of multiple renditions per sentence allows\nus to test the flexibility of the model under different user behaviours. At test time, the model generates outputs conditional on driving PAFs of one actor but the speaker conditioning of another actor.\nWe measure the quality of control by computing the similarity between the output of the controlled model and the ground-truth PAFs. The process comprises 3 steps: First, extract the prosodic features from the ground-truth audio. Then, condition the model a subset of the PAFs selected according to a predefined pattern. Finally, compute the RMSE between the output PAFs and the ground-truth PAFs.\nTo ensure that this metric accurately measures control, we use mismatched driving and target speakers at test-time; i.e., The driving PAFs are extracted from speaker A\u2019s audio (driving speaker), but the speaker label comes from speaker B (target speaker). The speaker mismatch is necessary to ascertain if the prosody control is due to the driving PAFs rather than the speaker label. Otherwise, if we tested with the same target-driving speakers, a model with worse control abilities might produce better output prosody simply because it better uses the information from the speaker label.1 This is possible because the speaker label contains a significant amount of information about the prosody, since speaker identity and prosody are closely correlated (Sini et al., 2020)."
        },
        {
            "heading": "5.1.1. DATA",
            "text": "We use a proprietary Latin American Spanish speech dataset with 32 speakers (17 female, 15 male). The data consists of ~38 hours across ~26,000 utterances, 800 of which are\n1Note that we train our models with the same driving and target speaker.\nheld back for validation and 182 for testing. The data is expressive and contains various speaking styles, including emotions such as happy, sad, angry, disgust, and joy, as well as styles like storytelling and conversational."
        },
        {
            "heading": "5.1.2. TEXT-TO-SPEECH PIPELINE",
            "text": "Given prosodic features from MICVAE, we synthesise speech that follows this prosodic specification using a TTS system conditioned on PAFs. Text-to-speech (TTS) synthesis involves a text processing front-end and an acoustic backend. During text processing we perform text normalisation and grapheme-to-phoneme conversion to create a phonetic representation of a given sentence (Taylor, 2009). Our backend uses an acoustic model to predict mel-spectrograms and a neural vocoder to synthesise the final waveform. Our acoustic model is based on Ctrl-P (Mohan et al., 2021), a Tacotron-2 like attention-based model conditioned on: F0, energy, duration, speaker identity, and speaking style. Our neural vocoder is a WaveRNN (Kalchbrenner et al., 2018) that synthesises 16-bit 24kHz waveforms.\nWe prepare the data as follows. Phone identity, punctuation tokens, silence tokens, word boundary tokens, start and end of sentence tokens are represented with a one-hot vector. We apply pre-emphasis to the waveforms. We extract melspectrograms using 128 bins, a frame length of 50ms, and a frame shift of 10ms. F0 is extracted using RAPT (Talkin & Kleijn, 1995). Energy is computed using the frame-wise root-mean-square of the waveform. Durations are extracted using a Kaldi forced aligner (Povey et al., 2011). Melspectrograms, F0, energy, and durations are mean-variance normalised on a per-speaker basis."
        },
        {
            "heading": "5.1.3. METHODS",
            "text": "We give a brief summary of each method considered in our evaluation.2\nMICVAE Our proposed model using self-attention with positional embeddings and \u201cfeature-stream\u201d embeddings.\nMasked CVAE A conditional variational autoencoder using a mask feature to represent missing values. Unless otherwise specified, it is trained with 50% of values randomly masked. Masked CVAE is intended to act as a baseline approach to this problem.\nNOCONTROL The default prosody model: predicts PAFs from phones, speaker identity, and style code. Uses the same decoder as the MICVAE.\nCRUDECONTROL A na\u0131\u0308ve system that forcefully modifies predicted values individually, this straw-man represents a controllable model that is entirely inconsistent. Its default prosody is generated with NOCONTROL, and then modified manually with the driving PAFs."
        },
        {
            "heading": "5.2. Evaluation Results",
            "text": "Using the simulated control approach described above we design 4 evaluations to assess the efficiency, robustness, and faithfulness of MICVAE."
        },
        {
            "heading": "5.2.1. EFFICIENCY (OBJECTIVE EVALUATION)",
            "text": "We show that MICVAE provides an efficient method for HitL control. Intuitively, efficiency means that the user only needs to drive a few PAFs before the output reflects their intention. In the context of our simulation experiment, efficiency requires that the root mean squared error (RMSE) between the generated PAFs and the ground-truth PAFs decreases faster than the equivalent error of a naive solution such as CRUDE CONTROL.\nTo create stimuli for this evaluation, we run a simulated control schedule which we call iterative refinement. We repeatedly predict prosodic features for a single sentence, each time providing an additional conditioning value, specifically, the value with the greatest RMSE in the previous generation step.\nThis allows us to measure the objective performance as we provide successively more conditioning information, starting\u2014as a human with\u2014with the most salient.\nThe results shown in Figure 5 demonstrate that MICVAE is able to recreate the prosodic features of the ground-truth\n2Samples demonstrating TTS for these systems can be found here, anonymous-submission-563098.github.io/sparse-control\nmore accurately than CRUDECONTROL for the amounts of sparsity that are realistic for control (between 4 and 70 PAFs). It is practically infeasible to require a human to specify over 70 values per utterance for conditioning. For fewer than 4 PAFs, it becomes more likely there are multiple feasible prosodic renditions thus inferring the intended prosody becomes a noisier task. In our final evaluation, we show that even with 4 input PAFs, our model can control prosody in a meaningful way."
        },
        {
            "heading": "5.2.2. ROBUSTNESS (OBJECTIVE EVALUATION)",
            "text": "In this section, we investigate how robust MICVAE is with respect to different patterns of sparsity as compared to the baseline, Masked CVAE. For robust HitL control, users should be free to choose which PAFs to drive and which to leave missing. The model should impute realistic values regardless of the pattern of sparsity in the input. To evaluate which method is more robust, we compare the error rates as we vary the number of driven PAFs.\nWe perform simulated control several times for MICVAE with different numbers of randomly chosen driving PAFs (0, 6, 12, 36, 72, 256). This process is then repeated for three different versions of Masked CVAE: one trained with no driving PAFs, one with 50% driving PAFs provided, and one with all driving PAFs.\nIn Figure 6, we see that MICVAE outperforms all the Masked methods, even though they have the same number of parameters and a similar complexity. We also observe that the Masked methods behave differently because they are not robust to the different sparsity patterns seen during training. The performance of the Masked CVAE trained with\n0% driving PAFs barely changes with the number of testing driving PAFs, because it hasn\u2019t learned how to make use of the driving PAFs. The performance of the Masked CVAE trained with 50% driving PAFs starts out worse than the 0% model but improves with the number of training PAFs. Finally, the performance of the Masked CVAE trained with 100% driving PAFs follows the same pattern as the 50% model, only worse in terms of RMSE, demonstrating that masking is necessary for this model."
        },
        {
            "heading": "5.2.3. FAITHFULNESS (SUBJECTIVE EVALUATION)",
            "text": "Any controllable model must reflect the user intention faithfully in the output. We explore how faithful MICVAE is with respect to the driving PAFs provided by the user.\nSince we are interested in faithfulness with respect to the user\u2019s intention (rather than the precise driving values), we perform an A/B/R listening test, asking human evaluators to select which audio sounds closest to a reference audio. We compare MICVAE with 4 input PAFs (~1%) provided against MICVAE with no input PAFs provided. The reference is the corresponding driving ground truth audio.\n20 native Latin American Spanish speakers took part in the test and were paid on average \u00a39 per hour. The test included 30 screens and the stimuli were generated with simulated control.\nIn Figure 7, we see that MICVAE with 4 driving PAFs is voted as being significantly closer to the reference than MICVAE with no driving PAFs. In addition to faithfulness, this result highlights the efficiency of our model as it is able to generate a perceptible change in prosody with only 4 values controlled. This subjective evaluation demonstrates that using sparse control is an efficient and effective approach to producing a desired outcome for highly-structured data."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this work, we tackle the limitations of control in generative models and propose a new control framework by sparse inputs. We define a set of desirable attributes for the generative model: efficiency, robustness and faithfulness. We propose a generative model adapted from the Masked CVAE better suited to sparse control, and demonstrate empirically that it possesses those qualities. To create a large number of testing stimuli in a repeatable fashion, we simulate HitL control using natural prosodic features from a testing dataset containing repeated utterances. We believe our evaluations and results lay the groundwork for broader studies of human-in-the-loop control of generative models using humans instead of simulating control."
        }
    ],
    "title": "Controlling High-Dimensional Data With Sparse Input",
    "year": 2023
}