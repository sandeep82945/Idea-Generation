{
    "abstractText": "International Max Planck Research School for Intelligent Systems, University of T\u00fcbingen, Germany Institute of Computer Science and Campus Institute Data Science, University of G\u00f6ttingen, Germany Max Planck Institute for Dynamics and Self-Organization, G\u00f6ttingen, Germany Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, USA Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA Institute for Theoretical Physics, University of T\u00fcbingen, Germany",
    "authors": [
        {
            "affiliations": [],
            "name": "Ivan Ustyuzhaninov"
        },
        {
            "affiliations": [],
            "name": "Max F. Burg"
        },
        {
            "affiliations": [],
            "name": "Santiago A. Cadena"
        },
        {
            "affiliations": [],
            "name": "Jiakun Fu"
        },
        {
            "affiliations": [],
            "name": "Taliah Muhammad"
        },
        {
            "affiliations": [],
            "name": "Kayla Ponder"
        },
        {
            "affiliations": [],
            "name": "Emmanouil Froudarakis"
        },
        {
            "affiliations": [],
            "name": "Zhiwei Ding"
        },
        {
            "affiliations": [],
            "name": "Matthias Bethge"
        },
        {
            "affiliations": [],
            "name": "Andreas S. Tolias"
        },
        {
            "affiliations": [],
            "name": "Alexander S. Ecker"
        }
    ],
    "id": "SP:a2779837c328749adc4f65fc90d565686e96a963",
    "references": [
        {
            "authors": [
                "Y. Yu",
                "X. Zheng"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous",
            "year": 2015
        },
        {
            "authors": [
                "J.L. Gauthier",
                "P Zhou"
            ],
            "title": "Caiman an open source tool for scalable calcium",
            "year": 2019
        },
        {
            "authors": [
                "J.D. Hunter"
            ],
            "title": "Matplotlib: A 2d graphics environment",
            "venue": "Computing in science & engineering,",
            "year": 2007
        },
        {
            "authors": [
                "J.B. Hamrick",
                "J. Grout",
                "S Corlay"
            ],
            "title": "Jupyter Notebooks \u2013 a publishing",
            "year": 2016
        },
        {
            "authors": [
                "W McKinney"
            ],
            "title": "Data structures for statistical computing in python",
            "year": 2010
        },
        {
            "authors": [
                "D Merkel"
            ],
            "title": "Docker: lightweight linux containers for consistent development",
            "year": 2014
        },
        {
            "authors": [
                "T.A. Pologruto",
                "B.L. Sabatini",
                "K. Svoboda"
            ],
            "title": "Scanimage: flexible software",
            "year": 2003
        },
        {
            "authors": [
                "S. Van Der Walt",
                "S.C. Colbert",
                "G. Varoquaux"
            ],
            "title": "The numpy array: a structure for efficient numerical computation",
            "venue": "Computing in science & engineering,",
            "year": 2011
        },
        {
            "authors": [
                "P. Virtanen",
                "R. Gommers",
                "T.E. Oliphant",
                "M. Haberland",
                "T. Reddy",
                "D. Cournapeau",
                "E. Burovski",
                "P. Peterson",
                "W. Weckesser",
                "J Bright"
            ],
            "title": "Scipy 1.0: fundamental algorithms for scientific computing in python",
            "venue": "Nature methods,",
            "year": 2020
        },
        {
            "authors": [
                "M.L. Waskom"
            ],
            "title": "Seaborn: statistical data visualization",
            "venue": "Journal of Open Source Software,",
            "year": 2021
        },
        {
            "authors": [
                "D. Yatsenko",
                "J. Reimer",
                "A.S. Ecker",
                "E.Y. Walker",
                "F. Sinz",
                "P. Berens",
                "A. Hoenselaar",
                "R.J. Cotton",
                "A.S. Siapas",
                "A.S. Tolias"
            ],
            "title": "Datajoint: managing big scientific data using matlab or python",
            "year": 2015
        },
        {
            "authors": [
                "D. Yatsenko",
                "E.Y. Walker",
                "A.S. Tolias"
            ],
            "title": "Datajoint: a simpler relational data model",
            "venue": "arXiv preprint arXiv:1807.11104",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Digital twin reveals combinatorial code of non-linear computations in the mouse\nprimary visual cortex Ivan Ustyuzhaninov1,7, Max F. Burg1,2,7, Santiago A. Cadena1,2,7, Jiakun Fu4,5, Taliah Muhammad4,5, Kayla Ponder4,5,\nEmmanouil Froudarakis4,5, Zhiwei Ding4,5, Matthias Bethge7, Andreas S. Tolias4,5,6, and Alexander S. Ecker2,3,\n1International Max Planck Research School for Intelligent Systems, University of T\u00fcbingen, Germany 2Institute of Computer Science and Campus Institute Data Science, University of G\u00f6ttingen, Germany\n3Max Planck Institute for Dynamics and Self-Organization, G\u00f6ttingen, Germany 4Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, USA\n5Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA 6Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA\n7Institute for Theoretical Physics, University of T\u00fcbingen, Germany\nMore than a dozen excitatory cell types have been identified in the mouse primary visual cortex (V1) based on transcriptomic, morphological and in vitro electrophysiological features. However, the functional landscape of excitatory neurons with respect to their responses to visual stimuli is currently unknown. Here, we combined large-scale twophoton imaging and deep learning neural predictive models to study the functional organization of mouse V1 using digital twins. Digital twins enable exhaustive in silico functional characterization providing a bar code summarizing the input-output function of each neuron. Clustering the bar codes revealed a continuum of function with around 30 modes. Each mode represented a group of neurons that exhibited a specific combination of stimulus selectivity and nonlinear response properties such as cross-orientation inhibition, size-contrast tuning and surround suppression. These non-linear properties were expressed independently spanning all possible combinations across the population. This combinatorial code provides the first large-scale, datadriven characterization of the functional organization of V1. This powerful approach based on digital twins is applicable to other brain areas and to complex non-linear systems beyond the brain.\nCorrespondence: Alexander S. Ecker, ecker@cs.uni-goettingen.de"
        },
        {
            "heading": "Introduction",
            "text": "Understanding the functional organization of the primary visual cortex (V1) has been a longstanding goal in neuroscience. It has long been known that V1 extracts information about local orientation Hubel & Wiesel (1959), often in a phase-invariant manner (Hubel & Wiesel, 1962). Researchers have described additional V1 nonlinearities, including direction selectivity (Adelson & Bergen, 1985) and various forms of nonlinear contextual modulation (Blakemore & Tobin, 1972; Cavanaugh et al., 2002; DeAngelis et al., 1992; Gilbert & Wiesel, 1990; Heeger, 1992; Lamme, 1995; Morrone et al., 1982). However, although we know many of the building blocks of V1 function, we do not know how they are organized at the population level.\nFirst, we do not know whether there exists a distinct num-\nber of functional cell types, each of which implements a specific computation, or whether there is a continuum of function, where cells do not fall into discrete types. Second, independent of whether V1 functions are discrete or form a continuum, we currently do not know how the different nonlinear effects described previously are organized at the population level: are they strongly correlated \u2013 for instance because they are caused by a common computational mechanism \u2013 or are they present independently of each other within the population?\nA major roadblock in revealing the functional organization of V1 has been that traditional experiments probing the well-known nonlinear mechanisms do not scale well. Large-scale population recordings are inefficient, because stimuli need to be optimized to an individual neuron\u2019s receptive field location, preferred orientation and spatial frequency. In addition, probing all nonlinear mechanisms in the same neurons is difficult because only a limited number of stimuli can be shown in an experiment.\nWe have overcome these limitations by combining largescale population recordings with natural stimuli and training high-performance predictive models based on deep neural networks (Antol\u00edk et al., 2016; Batty et al., 2016; Cadena et al., 2019; Cotton et al., 2020; Klindt et al., 2017; Lurz et al., 2020; Sinz et al., 2018; Walker et al., 2019). These models are capable of jointly modeling thousands of neurons in a completely data-driven way providing a digital twin: an in silico approximation of the function of primary visual cortex (Fig. 1A). First, this approach allows us to quantify the similarity of neurons\u2019 response properties on the set of natural stimuli by computing a compact, lowdimensional vector representation of each neuron\u2019s function (its bar code). This representation is independent of the neuron\u2019s receptive field location and its preferred orientation and provides an unbiased metric to measure the similarity of two neurons\u2019 functions. It therefore provides a principled way to study the functional organization of V1. Second, the digital twin allows us to carry out experiments with arbitrary stimuli in silico, essentially with-\nUstyuzhaninov et al. | bioR\u03c7iv | February 10, 2022 | 1\u201314\nout limitations of experimental time to generate hypothesis which can then verified back in vivo using the inception loop paradigm (Bashivan et al., 2019; Walker et al., 2019). This systematic functional analysis allows us to gain interpretable insights from the model and link to existing literature.\nWe found that the functional organization is not entirely uniform, revealing a number of high-density modes. We therefore used the bar codes to cluster functionally similar neurons, which allowed us to analyze the neurons\u2019 functional properties at the cluster level.\nCrucially, our analysis revealed that classical non-linear properties of neurons in V1 are expressed independently of each other. For instance, knowing the extent of a neuron\u2019s non-linearity along the simple-complex cell axis does not provide much information about its degree of surround suppression or cross-orientation inhibition. Moreover, there exist functional clusters expressing all combinations of nonlinear properties (including none or all), suggesting that V1 neurons might be described with a combinatorial code in the space of basic nonlinear computations.\nOverall, our results suggest the following answers to the two questions posed above. The functional organization of V1 appears to form a continuum; however, it is not uniform and there are high-density modes in the space of V1 neurons\u2019 functions. This organization is consistent with recent work using transcriptomic, morphological, and electrophysiolocal properties, which showed that cortical neurons are organized in families with a continuum of properties within them rather than distinct cell types Gouwens et al. (2020); Network (2021); Scala et al. (2021). With respect to classical nonlinearities, V1 neurons can be described by a combinatorial code where each nonlinear computation is expressed along an independent axis across the population. Such factorized codes have computational advantages such as higher coding capacity (Fusi et al., 2016)."
        },
        {
            "heading": "Results",
            "text": "Large-scale recording and predictive modeling. We recorded the activity of more than 45,000 excitatory neurons in layer 2/3 of the primary visual cortex of seven mice using a wide-field two-photon microscope (Sofroniew et al., 2016, Fig. 1A). While we imaged, the mice were head-fixed on a linear treadmill and were viewing natural images, which covered roughly 120\u00b0 \u00d7 90\u00b0 of their visual field (Fahey et al., 2019; Walker et al., 2019). Next, we selected up to 2,000 neurons from each mouse and fitted a single predictive model for all mice (Lurz et al., 2020). The model is based on a convolutional neural network (CNN). It takes as input the image on the screen and outputs a prediction of the response of each neuron (Fig. 1C). The model achieved single trial test correlation of 0.42, and oracle correlation of 0.69. From this model, we obtained a 128-dimensional vector representation of each\nneuron\u2019s function. These vectors can be thought of as \u201cbar codes\u201d summarizing the neuron\u2019s stimulus-response function (Fig. 1D).\nTo describe the neurons\u2019 functional diversity, we removed two well-known factors of variation across V1 neurons: receptive field position and preferred orientation. The bar codes we obtained from our model were independent of receptive field position and preferred orientation of the neuron: if the responses of two neurons could be made identical by applying a constant shift and rotation to all images, these two neurons would obtain the same bar code. We achieved this property by using a rotation-equivariant CNN (Ecker et al., 2019; Ustyuzhaninov et al., 2020). Having bar codes that are independent of receptive field location and preferred orientation is extremely useful, because it removes two \u201ctrivial\u201d axes of variation and allows us to focus on and visualize more subtle aspects of the neurons\u2019 selectivity or nonlinear processing.\nPredictive modeling reveals functional clusters. We first asked whether V1 neurons are organized into discrete functional types or rather form a continuum. A 2D t-SNE embedding (van der Maaten & Hinton, 2008) of the bar codes (Fig. 1E) revealed several modes \u2013 or regions of high density. These modes correspond to groups of functionally similar neurons. While there is no strong evidence for discrete functional types, it is not a uniform distribution either. We performed k-means clustering (MacQueen et al., 1967) (using 50 clusters) to identify the modes of the distribution and simplify downstream analysis.\nNeurons within functional clusters have similar MEIs. Given that V1 neurons can be organized into functional clusters, we aim at characterizing these clusters. We start by computing the preferred stimulus of each neuron, sometimes referred to as the most exciting image (MEI), which is optimized, using the model, to maximize the neuron\u2019s predicted activity (Fig. 2A). They have been shown to provide a faithful snapshot of neural computations (Bashivan et al., 2019; Walker et al., 2019), and therefore provide convenient single-image visualizations of a neuron\u2019s selectivity. Neurons within the same cluster had similar MEIs (up to location and rotation), while MEIs of neurons in different clusters tended to be different (Fig. 1F). This result provides a first piece of evidence that the clusters are a meaningful way of describing the functional organization of V1.\nNote that our data does reproduce the large degree of heterogeneity of MEIs and their frequent striking deviations from Gabor filters (Fig. 1F) that have been reported previously for mouse V1 (Walker et al., 2019).\nCluster MEIs visualize functional similarities between the neurons. To focus more on commonalities of functionally similar neurons, we next computed optimal stimuli at the cluster level. To do so, we computed cluster MEIs: image templates that maximize average activity of neurons in\n2 | bioR\u03c7iv Ustyuzhaninov et al. | Digital twin of the mouse primary visual cortex\nUstyuzhaninov et al. | Digital twin of the mouse primary visual cortex bioR\u03c7iv | 3\neach cluster when accounting for each neuron\u2019s receptive field location and preferred orientation (Fig. 2B). Cluster MEIs show a systematic variation along the different axes of the t-SNE embedding (Fig. 1G): Neighboring clusters tend to have visually similar cluster MEIs. There appears to be a global pattern in the t-SNE space with clusters on the right having oriented, Gabor-like MEIs with higher frequencies and multiple cycles within the envelope, while those in the middle having lower frequencies and fewer cycles and those towards the bottom left having more symmetric and circular MEIs.\nCluster MEIs visualize common patterns of cluster computations, and since they are designed to capture similarities rather than differences between the neurons, they exhibit less variability than individual MEIs. Quantitatively this amounts to cluster MEIs driving the neurons to around\n60% of activity of their individual MEIs.\nIn silico experiments provide an interpretable characterization of functional clusters. While MEIs provide convenient visualizations of a neuron\u2019s or a cluster\u2019s computations, they capture only a single point \u2013 the maximum \u2013 of the tuning function (Fig. 2A). Our predictive model, however, provides a prediction for arbitrary stimuli. We used the model as an in silico replica of V1 to perform experiments. Unlike with experiments in the real brain, in the model we are not limited in terms of experimental time. This allowed us to replicate a number of classical experiments in silico and compute tuning curves with respect to a variety of different non-linear properties. Specifically, we used Gabor stimuli whose parameters were optimized for each cluster to quantify strength of orientation selectivity\n4 | bioR\u03c7iv Ustyuzhaninov et al. | Digital twin of the mouse primary visual cortex\n(Fig. 2C), phase invariance (Fig. 2D), size-contrast tuning (Fig. 2E) and cross-orientation inhibition (Fig. 2F). Optimizing Gabors for clusters rather than individual neurons prevented the possibility of a few neurons within a cluster having very stimuli in comparison to the rest of the neurons in the cluster due to optimization instability. Furthermore, the cluster optimal Gabors drive the neurons to 87% of their individual optimal Gabor activity thus these stimuli are only mildly suboptimal for individual neurons.\nSize-contrast tuning curves reveal non-linear surround suppression effects (Born & Tootell, 1991; DeAngelis et al., 1992), while cross-orientation inhibition is a nonlinear interaction that arises when two orthogonal Gabor patterns are superimposed (Morrone et al., 1982). In addition, we computed the optimal center-surround stimulus (difference of Gaussian; Fig. 2A) and quantified the degree of response nonlinearity using a generalized linear model baseline (see methods). From these in silico experiments, we obtain a sample from the joint tuning distribution for more than 10,000 neurons. This enables us to study the statistical dependencies between the different nonlinear effects, overcoming the limitations of previous in vivo experiments that could only study each effect in isolation.\nIn silico experiments reveal shared tuning properties within functional clusters. The results from the set of in silico experiments support the functional clustering (Fig. 3). Many of the modes in the t-SNE embedding are distinguishable based on one or more of the tuning properties. In contrast, neurons within most of the clusters exhibit similar tuning strengths to the different types of non-linarities: all 50 clusters were significantly different from the overall population tuning distribution based on at least one tuning property in Fig.3 and 15 clusters were significantly different based on all properties (twosided Kolmogorov-Smirnov test at \u03b1 = 0.01; in the case of random cluster assignments these values are 0 and 1 respectively).\nNon-linear tuning properties are independent of each other. Next, we investigated how different non-linear properties relate to each other. Qualitatively, it appears that different non-linear properties have different distributions (Fig. 3; compare the color patterns in the t-SNE plots). This qualitative impression is also confirmed by quantitative metrics (Fig. 4A). As expected, non-linearity is correlated with specific nonlinear properties like phase invariance, surround suppression and cross-orientation inhibition, but not with orientation selectivity. In addition, orientation selectivity is correlated with phase invariance and cross-orientation inhibition. Importantly, there is no correlation between the three non-linear properties phase invariance, surround suppression and cross-orientation inhibition, suggesting that these properties are independently exhibited from each other. This result is surprising, since cross-orientation inhibition and surround suppression have both been hypothesized to arise from a common mecha-\nnism: divisive normalization (Carandini & Heeger, 2012).\nBecause phase invariance, cross-orientation inhibition and surround suppression are tested using oriented Gabors as stimuli, we restricted our subsequent analyses to those clusters for which optimal Gabors are decent stimuli in comparison to MEIs (i.e. clusters with above average values of the Gabor vs MEI index). We binarized the tuning indices for these non-linear properties assigning each cluster to either high or low tuning category (Fig. 4B) by setting a threshold at the average population tuning strength and examined the combinations of these three tuning properties (Fig. 4C). We can see there are clusters exhibiting every possible combination of the three binary tuning properties, suggesting that these properties indeed appear to be independent from each other, and that V1 neurons might employ a combinatorial code with respect to these nonlinearities.\nFor the analysis of remaining clusters (i.e. clusters with below average values of the Gabor vs MEI index, Fig. 4D) we considered orientation, non-linearity, and DoG vs MEI tuning. These clusters are mostly linear tuned and unoriented with their cluster MEIs ranging from mostly centersurround shapes in the bottom-right and central parts of the t-SNE space to various complex shapes in the left part of the t-SNE space.\nIn silico tuning curves are good approximations of in vivo tuning. Since our analysis is based on in silico tuning curves, one concern could be that although the CNN model predicts neural activities with high accuracy, the corresponding in silico tuning curves might be different from the in vivo tuning curves we are aiming to approximate. We verified that it is not the case by directly comparing the in silico and in vivo tuning curves for the same neurons. Specifically, we recorded a dataset containing two V1 scans of the same neurons in the same mouse. We used natural images as stimuli for the first scan which allowed us to fit a rotation-equivariant CNN model. For the second scan we used Gabor stimuli allowing us to compute size tuning curves in vivo and compare them to the in silico tuning curves obtained from the model fitted to the first scan (Fig. 5A).\nThe stimuli for the size tuning experiment should ideally be constructed based on optimal Gabors for every neuron, however, that is infeasible in vivo for a sufficiently large population of neurons. To overcome this limitation, we investigated to what extent optimal Gabors could be replaced with suboptimal ones. We found that tuning curves obtained using Gabors driving neurons to at least 50% of their optimal Gabor activities are very similar to the tuning curves obtained with optimal Gabors (Fig. 5C), and that only 20 different Gabors can be chosen to drive 75% of the population to at least 50% of optimal Gabor activities (Fig. 5D-E). This observation allowed us to use these 20 different Gabors as stimuli for the Gabor scan, which revealed that the correlation between in vivo and in sil-\nUstyuzhaninov et al. | Digital twin of the mouse primary visual cortex bioR\u03c7iv | 5\nNon-linearity\nGabor vs DoG\nG ab or D oG\nGabor vs MEI\nG ab or M EI\nOrientation tuning\nCross-orientation inhibition (COI)\nPref contrastO rt\nh co\nnt ra\nst\nPref contrastO rt\nh co\nnt ra\nst\nPhase invariance\nSurround suppression\nA B E\nG\nF\nC D\nico tuning curves is about 70% of the oracle correlation (Fig. 5B), which is similar to the performance of the CNN model trained and tested on natural images."
        },
        {
            "heading": "Discussion",
            "text": "We built a functional description of the mouse primary visual cortex based on neural representations in a high performing CNN model predicting responses of a large population of neurons on arbitrary natural images. Such an approach allows us to account for all aspects of the neuronal stimulus-response function captured by the model, instead of only a few nonlinear effects as in classical in vivo experiments with parametric stimuli. Thus, our analysis is not constrained by specific hypotheses of neural functions or the choice of parametric stimuli. An important limitation of our study is that we focus on the bottom-up aspects of stimulus processing; we did not consider how top-down, behavioral modulation of neural responses affects the responses of different neurons.\nOur examination revealed that the V1 functional landscape can be described by around 30 modes of functionally similar neurons which, however, do not appear to be discrete cell types but rather high density areas in the continuous functional space. This finding is in agreement with various recent studies. For example, Scala et al. (2021) studied mouse primary motor cortex neurons based on transcriptomic and morpho-electric properties. They found that this brain area is organized into a few broad transcriptomic families with continuum of morpho-electric features in each family, making the authors question the existence of discrete transcriptomic cell types. Gouwens et al. (2020) conducted a similar study of interneurons in mouse\nprimary visual cortex discovering both discrete and continuous variation of morpho-electric properties within the transcriptomic types. Overall, a growing body of literature suggests that mouse neocortex is organized in a complex way and cannot be adequately described by either discrete cell types or a uniform continuum of neurons. This notion of mouse neocortex organization is qualitatively different from the mouse retina, which exhibits discrete cell types (Baden et al., 2016), raising interesting directions for future research: how are other areas of neocortex organized based on various neural properties? and how do they relate to lower level brain areas projecting into the corresponding neocortical areas?\nAs we investigated the variability of individual neurons within the functional modes, we found visual differences between cluster and single neuron MEIs. We believe this is an expected consequence of clustering, which abstracts away individual neurons\u2019 differences and reduces the complexity of the V1 functional space by focusing on similarities between neurons. To better understand the properties of functional modes, their correspondence to cluster MEIs and relate them to previous work, we performed in silico experiments, which revealed that neurons belonging to the same mode exhibit common response patterns. This observation suggests that neurons in the same functional cluster may form computational cliques important for downstream processing, offering an interesting direction for future research. For example, integrated approaches combining functional characterization using digital twins and connectomics data can determine the connectivity of neurons within and across functional clusters and commonalties of their inputs and projection targets (Bae et al., 2021).\n6 | bioR\u03c7iv Ustyuzhaninov et al. | Digital twin of the mouse primary visual cortex\nOrientation\nSuppression COI\nPhase\nSuppression\nPhase CO I\nSu pp\n, p ha se Supp, COI\nPhase, COI\nAll None\nA\nC\nB\nOrientation Non-linearity DoG vs MEI\nD\nFig. 4. A. Pairwise distributions of the in silico tuning properties. Dots in each of the plots correspond to clusters (50 dots in total) showing average tuning strength of neurons in the cluster. Red dots correspond to clusters with the above average value of the Gabor vs MEI tuning strength (i.e. those clusters for which optimal Gabors are good stimuli relative to MEIs; in the following called Gabor-like clusters), gray dots correspond to all other clusters. Correlation coefficients and the p-values (in brackets) under the null hypothesis that the correlation is zero are shown above the plots for Gabor-like clusters (red) and for all clusters (black). B. Subsets of t-SNE embeddings corresponding to Gabor-like clusters colored according to the binarized strengths of orientation, phase, suppression and plaids tuning. Grey clusters correspond to low tuning strength (average cluster tuning strength is less than entire population tuning strength average value), clusters of the other color correspond to high tuning strength. C. Subsets of t-SNE embeddings corresponding to Gabor-like clusters showing 8 possible combinations of low/high values of phase, suppression and plaids tuning. The color code is illustrated with a color triangle; clusters colored with the colors in the vertices of the triangle exhibit high value of the corresponding tuning property and low values of other two properties. Clusters colored with the colors in the edges of the triangle exhibit high values of the tuning properties in the adjacent vertices and low value of the other tuning property. D. Subsets of t-SNE embeddings corresponding to non Gabor-like clusters colored according to the binarized strengths of orientation, non-linearity and DoG vs MEI tuning.\nUstyuzhaninov et al. | Digital twin of the mouse primary visual cortex bioR\u03c7iv | 7\nN eu\nro n\nac tiv\nity\nGabor size Neuron\n3\nNeuron 1Neuron 2\nN eu\nro n\nac tiv\nity\nGabor size\nCNN model\nHow similar?\nN eu\nro n\n1 Neuron 2\nNeuron 3\nA\nC D E\nB\nFig. 5. A. In vivo verification experiment paradigm. We compare in vivo Gabor size tuning curves to their in silico counterparts computed for the same population of neurons. B. Correlations between in vivo and in silico Gabor size tuning curves measured as percentage of oracle correlation (error bars shows the standard deviation). We measure correlation between two types of tuning curves. The \u201cmouse - model\u201d one corresponds to the in vivo and in silico tuning curves obtained using the same experimental stimuli, which are based on suboptimal Gabors (see main text for more details). In the \u201cmouse - optimal\u201d case, in vivo curves are computed using suboptimal Gabors, but the in silico curves are computed using optimal Gabors for the corresponding neurons. Moreover, these correlations are computed for different subsets of neurons (x-axis) chosen such that there exists an experimental stimulus for every neruon in the subset driving this neuron to a certain percentage of the optimal Gabor activity. C. Examples of Gabor size-contrast in silico tuning curves for 5 randomly chosen neruons (rows) computed using suboptimal Gabors activating the neuron to a certain percentage of the optimal Gabor activity (first 5 columns; numbers above the first row show the suboptimal Gabor activity as percentage of the optimal Gabor activity) and using the optimal Gabors (last column). D. Percentages of neurons in the population (x-axis) that can be driven to a certain percentage of the optimal Gabor activity (y-axis) using at least one of the Gabors in the sets of 1, 10 and 20 Gabors (different curves) chosen to maximize the number of neurons activated by these stimuli. E. 20 Gabor stimuli corresponding the 20 Gabors curve in panel D and used to construct size-tuning stimuli for the in vivo experiment.\nLarge-scale in-silico experiments allowed us to study statistical dependencies between various nonlinear phenomena known from single-neuron in-vivo experiments. We found these effects to be independent of each other, suggesting that V1 might employ a combinatorial code between modes of functionally similar cells. The mechanisms leading to such a code in V1 and their implications for downstream processing remain unclear. A speculation that lies at hand is that there might be a basis of independent non-linear computations serving different purposes in downstream processing, thereby building a foundation for specializations in higher visual areas. As a potential verification of this hypothesis and as a question in itself, future experimental work could investigate if neurons of the same functional cluster project to the same downstream area.\nFinally, we verified that in silico experiments in a highperforming CNN model provide a good approximation of the in vivo tuning curves, thus substantiating our results based on the analysis of in silico tuning curves. Our in vivo verification is consistent with the findings of Walker et al. (2019) who report that in silico model MEIs also highly activate actual neurons. Overall, these observations suggest that high-perfroming CNN models can be considered \u201cdigital twins\u201d of real neurons, a paradigm that has started being explored relatively recently but already provided significant insights into the brain and has a potential of becoming the main tool for future research.\nACKNOWLEDGEMENTS\nThe research was supported by the German Federal Ministry of Education and Research (BMBF) via the Competence Center for Machine Learning (FKZ 01IS18039A); the German Research Foundation (DFG) grant EC 479/1-1 (A.S.E.), the Collaborative Research Center (SFB 1233, Robust Vision) and the Cluster of Excellence \u201cMachine Learning \u2013 New Perspectives for Science\u201d (EXC 2064/1, project number 390727645); the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002); the National Eye Institute of the National Institutes of Health under Award Numbers U19MH114830 (A.S.T.) R01MH109556 (AST), P30EY002520, and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."
        },
        {
            "heading": "AUTHOR CONTRIBUTIONS",
            "text": "I.U., M.B., A.S.T and A.S.E. designed the study; J.F., T.M., K.P., E.F. and Z.D. performed imaging experiments and pre-processing of raw data; I.U., A.S.E. developed the in silico analysis framework; I.U., M.F.B., S.A.C. and A.S.E analyzed the data; I.U., M.F.B. and S.A.C. wrote the original draft; I.U., M.F.B, S.A.C. and A.S.E. reviewed and edited the manuscript with the input from M.B. and A.S.T."
        },
        {
            "heading": "Methods",
            "text": "Visual stimuli. We used gray-scale ImageNet images (Russakovsky et al., 2015) as visual stimuli in the data collection experiment. The number of images varied across the scans (Tab. 1) with 4692 images in the intersection, i.e. presented to a mouse in each of the scans. For the test set we used 100 images each repeated 10 times; for some scans and some images there were fewer repeats, in which case we resampled the recording to have 10 repeats in every scan. The screen was 55 \u00d7 31 cm at a distance of 15 cm, covering roughly 120\u00b0 \u00d7 90\u00b0. Each image was presented for 500 ms followed by a blank screen lasting between 300 ms and 500 ms. The response of a neuron to a given stimulus is represented as a number of spikes in the time interval between 50 ms to 350 ms following the stimulus onset.\nCNN model and rotation-invariant clustering. We used the same architecture and training of the rotationequivariant CNN model as in Ustyuzhaninov et al. (2020). For the clustering of aligned feature vectors (rotationinvariant clustering) we used the k-Means algorithm (MacQueen et al., 1967) with 50 clusters, which we empirically found to provide a good balance between clusters being small enough to contain similar neurons and the total number of clusters being relatively small.\nGLM model. We also fitted a GLM model (McCullagh & Nelder, 2019) to every neuron in the recorded dataset to evaluate the non-linearity of neurons or clusters (as measured by the non-linearity index, see below). We used Poisson likelihood as the noise model and log link function to ensure the predicted neural activities are non-negative. We cross-validated the L2 regularization coefficient for every neuron separately by considering 48 log-spaced values in [0.1,5].\nOptimal stimuli. Classical experiments that we aim at replicating in silico measure changes in neural activity in response to a certain type of transformations of an input stimulus (e.g. an orientation tuning experiment might use Gabor stimuli in different orientations). Ideally the input stimuli should be optimized for every neuron separately which is infeasible in vivo, but can be achieved in silico using a CNN model. In this section we describe the stimuli we use in the in silico experiments and how we optimize them for individual neurons or clusters.\nPer neuron optimal Gabors. For every neuron we compute a Gabor stimulus which maximizes the predicted activity of this neuron. We parametrize such stimuli in terms of a spatial location (rx, ry), size \u03c3, spatial frequency \u03bd, contrast a, orientation \u03d5 and phase \u03c4 . Specifically the value of the pixel in the i-th row and j-th column of an input image\nis defined using the following expression\nx[i, j] = a2 exp ( \u221212 (i\u2032)2 +(j\u2032)2 \u03c32/4 ) cos ( 2\u03c0 \u00b7 i\u2032 \u00b7\u03bd+ \u03c4 ) ,\n(1)\nwhere (i\u2032, j\u2032) = (i\u2212 rx, j\u2212 ry) [R(\u03d5)]T (2)\nwith R(\u03d5) being a 2D rotation matrix by an angle \u03d5.\nFor each neuron we iterate over a large set of Gabor stimuli parameterized according to (2) and record the parameter set corresponding to a stimulus with the highest CNN predicted activity. This process is illustrated in Fig. 2A.\nPer cluster optimal Gabors. In addition to optimising Gabors for each neuron separately, we also find optimal Gabors for the entire clusters. The idea is to find a single Gabor stimulus that maximizes the average activity of neurons in the cluster and hence servres as a single image representations of a cluster computation. However, since we the clusters are explicitly constructed to contain neurons with different receptive field locations and preferred orientations, we constrain the stimuli to be identical for all neurons in the cluster apart from having neuron-specific spatial locations (i.e. receptive field centers) and orientations (see an illustration in Fig. 2A).\nTo find such an optimal Gabor for a specific cluster, we iterate over a large set of Gabors with locations and orientations set to a fixed value and all other parameters varying, generate stimuli for individual neurons by shifting and rotating the Gabor at the current iteration (Fig. 2A), and compute the average predicted activity of the neurons in the cluster. We call the Gabor stimulus (or rather the set of stimuli up to a spatial location and orientation) corresponding to the maximal average predicted cluster activity the cluster optimal Gabor.\nPer neuron MEIs. An MEI is an input stimulus that activates the neuron the most, and as such serves as a useful visualization of the computation implemented by the neuron. It is important to keep in mind that an MEI is only a local maximum of a function mapping the input stimuli to the neural activity. While we aim at describing the entire function rather than only its maximum, MEIs nevertheless provide convenient and insightful summaries of computations performed by each of the neurons.\nWe compute MEI for every neuron separately by stating with a noise stimulus and iteratively optimising it to maximise the predicted activity of the neuron. Specifically, if the CNN prediction of activity of the n-th neuron when presented with a stimulus x is fn(x), we compute an MEI as a solution of the following optimisation problem:\nxMEIn = argmax x [f(x)\u2212\u03b3 \u00b7\u03b1(x)] , (3)\nwhere \u03b1(x) is a regularisation function (e.g. \u03b1(x) = ||x||2) enforcing smoothness of the resulting MEI.\n10 | bioR\u03c7iv Ustyuzhaninov et al. | Digital twin of the mouse primary visual cortex\nPer cluster MEIs. Such MEIs are computed jointly for all neurons in the entire cluster to maximise the average predicted activity of all neurons in the cluster. Similarly to cluster optimal Gabors, these stimuli are constrained to be identical for all neurons in the cluster apart from having neuron-specific spatial locations (i.e. receptive field centers) and orientations (Fig. 2A). To implement these constraints in practice, we decompose cluster MEIs in a steerable basis and iteratively optimise the coefficients in this basis to maximize the predicted average activity of neurons in the cluster. Therefore we use a parametric model\nof cluster MEIs rather than a non-parametric representation of single-neuron MEIs, which reduces the spaces of possible stimuli (for single-neurons MEIs this space consists of all possible images, while in this case it consists only of the span of the steerable basis), but allows us to compute stimuli rotations as linear transformations of the coefficients.\nWe consider the following parametrization of the stimuli\nx = \u03b21 \u00b7\u03c81 + . . .+\u03b2k \u00b7\u03c8k = \u03b2T\u03a8, (4)\nwhere \u03a8 = (\u03c81, . . . ,\u03c8k)T is a vector of the first k basis\nUstyuzhaninov et al. | Digital twin of the mouse primary visual cortex bioR\u03c7iv | 11\nfunctions in some steerable basis (we use Hermite polynomials). Rotations of images in such a parametrization correspond to linear transformations of coefficients \u03b2, specifically, a stimulus rotated by an angle \u03d5 can be written as\nrotate(x,\u03d5) = (R(\u03d5)\u03b2)T \u03a8 (5)\nfor a corresponding rotation matrix R(\u03d5).\nWe denote a translation of an image x by rx pixels along the first axis and by ry pixels along the second axis as shift(x, rx, ry). Using center locations of optimal Gabors (rxn, r y n) as receptive fields centers for corresponding neurons, we compute the cluster MEI xMEIc for a cluster c containing m neurons with indices c1, . . . , cm as a solution of the following problem\nxMEIc = (\u03b2\u2217)T\u03a8, where (6)\n\u03b2\u2217 = argmax \u03b2 max \u03d51,...,\u03d5m [ 1 m m\u2211 i=1 Aci ] , (7) Aci = fci(shift[rotate(x,\u03d5i), r 1 ci , r2ci ]) (8)\nExamples of cluster MEI are shown in Fig. 3.\nOptimal differences of Gaussians (DoG). Another class of stimuli we consider are differences of Gaussians, which allow us to probe to what extent the receptive of a neuron has a center-surround structure. We parametrize such stimuli in terms of their spatial locations r = (r1, r2), sizes \u03c3cen and \u03c3sur of the center and surround Gaussians, as well as their relative contrasts acen and asur according to the following equation:\nx = acen (1+asur)g(rcen,\u03c3cen)\u2212asur g(rsur,\u03c3sur), (9) with g(r,\u03c3)[i, j] = exp ( \u2212 (r1\u2212 i) 2 +(r2\u2212 j)2\n2\u03c32\n) . (10)\nSimilarly to optimal Gabors and MEIs, we find optimal DoG stimuli both for every neuron and cluster by iterating over a large set of parameter values and recording the parameter combination corresponding to the highest predicted activity of an individual neuron or average activity of all neurons in the cluster. In the case of per cluster stimuli, we constrain every neuron in the cluster to have exactly the same optimal stimulus apart from differences in spatial locations (Fig. 3A).\nIn silico experiments. In this section we describe specific in silico experiments that we perform using the optimal stimuli discussed in the previous section.\nOrientation and phase tuning. Optimal Gabors enable us to compute standard orientation and phase tuning curves. We use both per neuron and per cluster optimal Gabors to obtain stimuli for this experiment by varying the orientation and phase parameters and keeping all other parameters fixed. Examples of such stimuli are shown in Fig. 2B.\nFor every neuron we compute numerical indices reflecting the tuning strength of the neuron. Specifically, we compute the F1/F0 summary statistics for phase tuning and F2/F0 statistics. These statistics are ratios of the absolute values of the first and second (reflecting periods of 2\u03c0 for phase tuning and \u03c0 for orientation tuning) Fourier coefficients to the mean value of the tuning curve, or alternatively the ratios of amplitudes of the est fitting sine curves to the means of the tuning curves as illustrated in Fig. 2B. If r(s) = (r1(s1), . . . , rm(sm)) are responses of a neuron to Gabors with a parameter of interest (orientation of phase) taking values s = (s1, . . . ,sm), these indices are defined as\nFk/F0 =\n\u2223\u2223\u2223\u2223\u2223 m\u2211j=1rj exp(iksj) \u2223\u2223\u2223\u2223\u2223\nm\u2211 j=1 rj\n. (11)\nGabor size-contrast tuning curves. We construct stimuli for this experiment by using per neuron or per cluster optimal Gabors with all parameters fixed except for the size and contrast as illustrated in Fig. 2B. The resulting sizecontrast tuning curves allow us to characterize neural computations in terms of surround or contrast suppression effects, which have been widely studied in the existing literature.\nDenoting predicted activity of n-th neuron when presented a stimulus at the contrast level c\u2208 {1, . . . ,C} and size level s \u2208 {1, . . . ,S} as gnc,s, we compute the following suppression and contrast indices to numerically evaluate the tuning strength:\nSuppression index = gnC \u2212gnC,S\ngnC , gnC = maxs g n s,C ; (12)\nContrast index = gnS\u2212gnC,S\ngnS , gnS = maxc g n S,c. (13)\nThese two indices are highly correlated (\u03c1 = 0.95, p < 0.001) which is why we use only the suppression index for the analysis. This correlation apparently stems from using Gabors as stimuli for this experiment. Indeed, increasing the size of a Gabor also increases the range of each pixel value, which is a similar effect to increasing the contrast.\nPlaid stimuli. Another experiment we do with optimal Gabor stimuli is the one aimed at probing neurons or clusters for a potential effect of cross-orientation inhibition. To do some we construct plaid stimuli by superimposing two Gabors on each other, the optimal one and the one orthogonal to the optimal one, while varying the contrasts of both of these stimuli. Examples of such stimuli are shown in Fig. 2. The corresponding tuning curves allow us to see potential non-linear suppressing effect of increasing contrast of the orthogonal stimulus known as cross-orientation inhibition.\n12 | bioR\u03c7iv Ustyuzhaninov et al. | Digital twin of the mouse primary visual cortex\nDenoting predicted activity of n-th neuron when presented a plaid stimulus with an optimal Gabor at the contrast level p \u2208 {1, . . . ,P} and an orthogonal Gabor at the contrast level o \u2208 {1, . . . ,O} as gnp,o, we compute the following numerical index to quantify the tuning strength:\nPlaids index = gnO\u2212gnP,O\ngnO , gnO = maxp g n p,C . (14)\nComparison of optimal Gabor, DoG, and MEI stimuli. Optimal MEIs capture a wide variety of patterns in the receptive field, while optimal Gabors and DoGs are explicitly designed to represent a particular pattern. Comparing the responses to these stimuli allows us to quantify to what extent the receptive field captured by the MEI can be modelled by oriented gratings (Gabors) or center surround patterns (DoG).\nFor every neuron we normalize all three (Gabor, DoG, MEI) optimal stimuli to have same energy (L2 norm) at E different energy levels; such a normalization ensures that the stimuli have approximately the same contrast. Denoting predicted activity of n-th neuron when presented a Gabor, DoG or MEI at the energy level e as gne , d n e , m n e respectively, we compute the following summary statistics for comparing the responses to these stimuli:\nGabor vs DoG = 1 E E\u2211 e=1 gne dne , (15)\nGabor vs MEI = 1 E E\u2211 e=1 gne mne , (16)\nDoG vs MEI = 1 E E\u2211 e=1 dne mne . (17)\nNon-linearity index. For every neuron we quantify the nonlinearity computations implemented by this neuron by comparing the predictions of the GLM and the CNN models. Specifically, for the n-th neuron we denote the GLM predictive correlation on the test set as CGLMn , and the same quantity computed for the CNN model as CCNNn . Then we compute the non-linearity index as follows:\nNon-linearity index = C CNN n\nCGLMn . (18)"
        },
        {
            "heading": "In vivo verfification.",
            "text": "Experimental details. The experimental setting for the ImageNet scan in vivo verification was the same as in the main experiment (see above). The Gabor stimuli were presented in maximum contrast in 5 different sizes (\u03c3 \u2208 [8,13.2,21.8,35.9,59.3]).\nSelection of suboptimal stimuli. We find a small number of Gabor stimuli activating many neurons to a certain percentage of their optimal Gabor activities (Fig.5D-E) using the following procedure:\n\u2022 We compute predicted activities for every neuron for a large selection of Gabors located in the center of the image (the same set of stimuli that we used for finding the optimal Gabor apart from the differences in spatial locations; see above),\n\u2022 We greedily choose the Gabor that activated most of the neurons to a given percentage of their optimal Gabor activity until we selected the required number of stimuli (we use 20 for the experiment).\nData availability. All figures were generated from raw or processed data. The data generated and/or analyzed during the current study are available from the corresponding author upon request. No publicly available data was used in this study. All code and data will be available online upon the publication.\nCode availability. Experiments and analyses were performed using custom software developed using the following tools: ScanImage 2018a (Pologruto et al., 2003), CaImAn v.1.0 (Giovannucci et al., 2019), DataJoint v.0.11.0 (Yatsenko et al., 2015, 2018), TensorFlow v.1.15.0 (Abadi et al., 2015), NumPy v.1.17.3 (Van Der Walt et al., 2011), SciPy v.1.5.4 (Virtanen et al., 2020), Docker v.19.03.12 (Merkel et al., 2014), Matplotlib v.3.1.1 (Hunter, 2007), seaborn v.0.11.1 (Waskom, 2021), pandas v.1.1.5 (McKinney et al., 2010) and Jupyter Notebook v 6.0.1 (Kluyver et al., 2016). The code will be publicly available upon the publication.\nAnimal research statement. All experimental procedures complied with guidelines approved by the Baylor College of Medicine Institutional Animal Care and Use Committee (IACUC).\nCompeting interests. A.S.T. holds equity ownership in Vathes LLC, which provides development and consulting for the framework (DataJoint) used to develop and operate the data analysis pipeline for this publication."
        }
    ],
    "title": "Digital twin reveals combinatorial code of non-linear computations in the mouse primary visual cortex",
    "year": 2022
}