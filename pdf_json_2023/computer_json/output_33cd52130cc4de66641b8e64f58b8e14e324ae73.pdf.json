{
    "abstractText": "Deep learning-based fine-grained network intrusion detection systems (NIDS) enable different attacks to be responded to in a fast and targeted manner with the help of large-scale labels. However, the cost of labeling causes insufficient labeled samples. Also, the real fine-grained traffic shows a long-tailed distribution with great class imbalance. These two problems often appear simultaneously, posing serious challenges to fine-grained NIDS. In this work, we propose a novel semi-supervised finegrained intrusion detection framework, SF-IDS, to achieve attack classification in the label-limited and highly class imbalanced case. We design a self-training backbone model called RI1DCNN to boost the feature extraction by reconstructing the input samples into a multichannel image format. The uncertainty of the generated pseudo-labels is evaluated and used as a reference for pseudo-label filtering in combination with the prediction probability. To mitigate the effects of fine-grained class imbalance, we propose a hybrid loss function combining supervised contrastive loss and multi-weighted classification loss to obtain more compact intra-class features and clearer interclass intervals. Experiments show that the proposed SF-IDS achieves 3.01% and 2.71% Marco-F1 improvement on two classical datasets with 1% labeled, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinran Zheng"
        },
        {
            "affiliations": [],
            "name": "Shuo Yang"
        },
        {
            "affiliations": [],
            "name": "Xingjun Wang"
        }
    ],
    "id": "SP:fcdcbc89e8cfa2cc15faeb48760a70cd8385bc05",
    "references": [
        {
            "authors": [
                "H. Yao",
                "D. Fu",
                "P. Zhang",
                "M. Li",
                "Y. Liu"
            ],
            "title": "Msml: A novel multilevel semi-supervised machine learning framework for intrusion detection system",
            "venue": "IEEE Internet of Things Journal, vol. 6, no. 2, pp. 1949\u20131959, 2018.",
            "year": 1949
        },
        {
            "authors": [
                "Y. Hou",
                "S.G. Teo",
                "Z. Chen",
                "M. Wu",
                "C.-K. Kwoh",
                "T. Truong-Huu"
            ],
            "title": "Handling labeled data insufficiency: Semi-supervised learning with self-training mixup decision tree for classification of network attacking traffic",
            "venue": "IEEE Transactions on Dependable and Secure Computing, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Dong",
                "Y. Xia",
                "T. Peng"
            ],
            "title": "Network abnormal traffic detection model based on semi-supervised deep reinforcement learning",
            "venue": "IEEE Transactions on Network and Service Management, vol. 18, no. 4, pp. 4197\u20134212, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P.A.A. Resende",
                "A.C. Drummond"
            ],
            "title": "A survey of random forest based methods for intrusion detection systems",
            "venue": "ACM Computing Surveys (CSUR), vol. 51, no. 3, pp. 1\u201336, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "J. Gu",
                "S. Wang"
            ],
            "title": "An effective intrusion detection framework based on svm with feature augmentation",
            "venue": "Knowledge-Based Systems, vol. 136, pp. 130\u2013139, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X.-B. Li"
            ],
            "title": "A scalable decision tree system and its application in pattern recognition and intrusion detection",
            "venue": "Decision Support Systems, vol. 41, no. 1, pp. 112\u2013130, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Li",
                "B. Fang",
                "L. Guo",
                "Y. Chen"
            ],
            "title": "Network anomaly detection based on tcm-knn algorithm",
            "venue": "Proceedings of the 2nd ACM symposium on Information, computer and communications security, 2007, pp. 13\u201319.",
            "year": 2007
        },
        {
            "authors": [
                "S.S. Roy",
                "A. Mallik",
                "R. Gulati",
                "M.S. Obaidat",
                "P.V. Krishna"
            ],
            "title": "A deep learning based artificial neural network approach for intrusion detection",
            "venue": "International Conference on Mathematics and Computing. Springer, 2017, pp. 44\u201353.",
            "year": 2017
        },
        {
            "authors": [
                "M. Abdel-Basset",
                "V. Chang",
                "H. Hawash",
                "R.K. Chakrabortty",
                "M. Ryan"
            ],
            "title": "Deep-ifs: intrusion detection approach for industrial internet of things traffic in fog environment",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 17, no. 11, pp. 7704\u20137715, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Imrana",
                "Y. Xiang",
                "L. Ali",
                "Z. Abdul-Rauf"
            ],
            "title": "A bidirectional lstm deep learning approach for intrusion detection",
            "venue": "Expert Systems with Applications, vol. 185, p. 115524, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.K. Sahu",
                "D.P. Mohapatra",
                "J.K. Rout",
                "K.S. Sahoo",
                "Q.-V. Pham",
                "N.-N. Dao"
            ],
            "title": "A lstm-fcnn based multi-class intrusion detection using scalable framework",
            "venue": "Computers and Electrical Engineering, vol. 99, p. 107720, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "L. Huang",
                "C.Q. Wu",
                "Z. Li"
            ],
            "title": "An effective convolutional neural network based on smote and gaussian mixture model for intrusion detection in imbalanced dataset",
            "venue": "Computer Networks, vol. 177, p. 107315, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "X. Chen",
                "D. Guo",
                "M. Song",
                "Y. Teng",
                "X. Wang"
            ],
            "title": "Pccn: parallel cross convolutional neural network for abnormal network traffic flows detection in multi-class imbalanced network traffic flows",
            "venue": "IEEE Access, vol. 7, pp. 119 904\u2013119 916, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Zoph",
                "G. Ghiasi",
                "T.-Y. Lin",
                "Y. Cui",
                "H. Liu",
                "E.D. Cubuk",
                "Q. Le"
            ],
            "title": "Rethinking pre-training and self-training",
            "venue": "Advances in neural information processing systems, vol. 33, pp. 3833\u20133845, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Li",
                "W. Chen",
                "Z. Wei",
                "X. Luo",
                "B. Su"
            ],
            "title": "Semi-wtc: A practical semi-supervised framework for attack categorization through weighttask consistency",
            "venue": "arXiv preprint arXiv:2205.09669, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M.N. Rizve",
                "K. Duarte",
                "Y.S. Rawat",
                "M. Shah"
            ],
            "title": "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
            "venue": "International Conference on Learning Representations, 2021. [Online]. Available: https: //openreview.net/forum?id=-ODN6SbiUU",
            "year": 2021
        },
        {
            "authors": [
                "Y. Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ser. ICML\u201916. JMLR.org, 2016, p. 1050\u20131059.",
            "year": 2016
        },
        {
            "authors": [
                "H. Han",
                "W.-Y. Wang",
                "B.-H. Mao"
            ],
            "title": "Borderline-smote: a new oversampling method in imbalanced data sets learning",
            "venue": "International conference on intelligent computing. Springer, 2005, pp. 878\u2013887.",
            "year": 2005
        },
        {
            "authors": [
                "C. Huang",
                "Y. Li",
                "C.C. Loy",
                "X. Tang"
            ],
            "title": "Learning deep representation for imbalanced classification",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 5375\u20135384.",
            "year": 2016
        },
        {
            "authors": [
                "P. Khosla",
                "P. Teterwak",
                "C. Wang",
                "A. Sarna",
                "Y. Tian",
                "P. Isola",
                "A. Maschinot",
                "C. Liu",
                "D. Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 18 661\u201318 673, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Sohn",
                "D. Berthelot",
                "N. Carlini",
                "Z. Zhang",
                "H. Zhang",
                "C.A. Raffel",
                "E.D. Cubuk",
                "A. Kurakin",
                "C.-L. Li"
            ],
            "title": "Fixmatch: Simplifying semisupervised learning with consistency and confidence",
            "venue": "Advances in neural information processing systems, vol. 33, pp. 596\u2013608, 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Intrusion detection, semi-supervised learning, imbalanced classification.\nI. INTRODUCTION\nThe fine-grained network intrusion detection system (NIDS) can help experts to take targeted measures to address the impact of cyber attacks. Deep learning has brought significant performance improvements to existing systems with continuous development, but it relies heavily on large-scale labeled samples. This leads to two persistent and often co-occurring general challenges. The first one is the lack of labeled samples. Labeling data is always a costly task and often requires the support of domain experts. The second one is real finegrained traffic often tends to show a long-tailed distribution with severe class imbalance, which creates \u201clabel bias\u201d during training, making the decision boundary driven by the head class, resulting in poor classification performance.\nSeveral works [1]\u2013[3] related to the above challenges are proposed, but they focus on one of them while assuming the other does not exist, and few expect to address both simultaneously. Semi-supervised learning is a common approach used in the case of insufficient labeled samples. A typical idea is selftraining, which generates pseudo-labels for unlabeled samples to expand the labeled dataset. Highly class imbalance reduces the accuracy of pseudo-labels on minority classes, poisoning the labeled samples. For the second challenge, current works\n\u2217Contributed Equally \u2021Corresponding Author\nhave researched the class imbalance problem under supervised learning. These methods rely on label information to rebalance the data, and using them only for limited labeled samples may cause overfitting. Also, the long-tailed shape of finegrained traffic sample distribution is more challenging than the plain class imbalance (e.g., imbalance in binary classification), which has not been explored in depth, especially in semisupervised learning scenarios.\nBased on the above issues, we propose SF-IDS to achieve fine-grained intrusion detection in the case of insufficient labeled samples and high-class imbalance. Specifically, SFIDS uses self-training to flexibly exploit the supervised value of unlabeled samples and their pseudo-labels. The RI-1DCNN is proposed as a backbone model to enhance the feature extraction capability by reconstructing the input samples into multichannel images. In addition, the uncertainty and prediction probability of pseudo-labels is used as the basis for label filtering. To address the extreme class imbalance problem, we design a hybrid loss function with a combination of supervised contrastive learning and multi-weighted classification loss to enable good feature representation to correct biased classifiers. The prototype system of this framework is implemented in this paper, and the major contributions are the following four folds:\n\u2022 We propose SF-IDS to achieve fine-grained network intrusion detection. This is the first time that self-trainingbased semi-supervised learning is used in NIDS to address both the labeled sample shortage and long-tailed attack classification challenges. \u2022 We design RI-1DCNN as the backbone model of SF-IDS to enhance the traffic feature extraction ability by reconstructing multichannel images in training. An uncertaintybased label filter is proposed to mitigate pseudo-label noise by evaluating uncertainty and resampling. \u2022 We design a hybrid loss combining supervised contrastive loss and multi-weighted classification loss as the learning objective to obtain tighter intra-class features and clearer classification boundaries under severe class imbalance. \u2022 We evaluate the proposed model on two classical datasets: NSL-KDD and CICIDS2017. The experimental results show that SF-IDS still has excellent fine-grained attack classification capability with only 1% labeled samples."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Network intrusion detection system (NIDS) has been receiving a lot of attention over the past decade. Some early works focus on traditional machine learning, such as random\nar X\niv :2\n30 8.\n00 54\n2v 1\n[ cs\n.C R\n] 1\nA ug\n2 02\n3\nforest [4], SVM [5], and decision tree [6], whose performance depends on effective feature engineering. As attackers continuously update their behavior, finding adaptive features becomes difficult. Recently, the automatic feature extraction ability of deep learning has received increasing attention, KNN [7], DNN [8], RNN [9], and other models have been used in the field of intrusion detection. Imrana et al. [10] uses BiLSTM to focus on more contextual information and solve the gradient disappearance problem to improve the detection accuracy of two minority attacks, U2R and R2L. Sahu et al. [11] proposes LSTM combined with FCNN for extracting Spatio-temporal features of traffic achieves good results in six datasets on intrusion detection. The good performance of these works is usually based on two assumptions: sufficient labeled samples and a class-balanced dataset. However, this often cannot be satisfied simultaneously in practical applications.\nZhang et al. [12] proposes the model aiming to solve the class imbalance problem under supervised learning, which combines SMOTE model and under-sampling for clustering based on Gaussian Mixture Model (GMM) to solve the class imbalance problem by resampling. Zhang et al. [13] proposes parallel cross convolutional neural network to extract feature representations and avoid the neglect of few-sample categories. These commonly used methods often require labels as the basis for rebalancing the dataset or feature learning, which may lead to overfitting in case of insufficient labels.\nSemi-supervised learning is often used to overcome the lack of labeled samples. Among them, self-training is a competitive method that can flexibly adapt to data of different sizes [14] and generate pseudo-labels for unlabeled samples thus achieving the performance goals of supervised learning. Hou et al. [2] uses mixup to generate augmented samples and filters reliable pseudo-labels based on consistency. Decision trees are used as classification models. Li et al. [15] proposes a semi-supervised framework, Semi-WTC, which uses active\nlearning to extract key unlabeled data for manual labeling and artificially assigns modified feature weights to hard-to-score samples. However, the class imbalance problem is still the reason for the poor accuracy of existing methods in minority classes, especially in fine-grained classification tasks. Severe class imbalance leads to biased pseudo-labels, which further degrades the performance of the model.\nIn fact, there is a relative lack of methods that enable semisupervised fine-grained intrusion detection while addressing the insufficient labels and class imbalance. Therefore, the goal of our work is precisely to propose an efficient framework that can simultaneously solve the insufficient labels and imbalances in fine-grained attack classification."
        },
        {
            "heading": "III. PROPOSED MODEL",
            "text": ""
        },
        {
            "heading": "A. Overview",
            "text": "This section discusses the proposed imbalanced semisupervised learning framework, SF-IDS, and how it copes with the lack of labeled samples and the imbalanced class distribution of fine-grained attacks. Fig. 1 illustrates the overall architecture of the framework. SF-IDS implements three main steps of self-training using a newly designed backbone model RI-1DCNN. First, RI-1DCNN is trained by original labeled samples. Later, the above model is used to generate pseudolabels y\u0302u for unlabeled samples DU . The uncertainty of the pseudo-labels is evaluated at the same time as one of the references for mitigating the label noise. After filtering, some of the pseudo labels y\u0302\u2032u are retained. Finally, the original labeled samples and filtered pseudo-labeled samples are combined as new input to retrain the model. The second and third steps are performed several times to update pseudo-labels, allowing the model to gain stronger generalization. We propose a hybrid loss as the optimization objective. Supervised contrastive loss and multi-weighted cross-entropy loss are jointly optimized so that good features guide unbiased classifiers."
        },
        {
            "heading": "B. RI-1DCNN",
            "text": "The CNN model is suitable for extracting feature associations to generate good representations, which helps to aggregate intra-class features more accurately. Generally, it performs well in image processing, while the processed traffic data is similar to tabular data, which limits its capability. Therefore, we propose the RI-1DCNN as a self-training backbone model to reconstruct the input samples into a multichannel image format. We first add a fully-connected (FC) layer before the convolutional layers to extend the input features to provide enough virtual pixels for subsequent operations. The backpropagation process allows the FC layer to learn the correct feature ordering and thus give a specific meaning to the image. Then, the features are reshaped into multi-channel images, five convolutional layers are used to extract fine feature associations. A residual connection is added to lightweight the model and avoids gradient disappearance. To jointly learn the feature extractor and classifier, RI-1DCNN has a projection head and a classification head. The projection head is a narrower hidden layer, which maps the sample representation r into a lower dimension vector z = Proj(r) \u2208 RDP of size DP for generating feature embeddings. Then, \u21132 normalization is applied to z so that the inner product can be used as distance measurements. The classification header is used to output the gradients to evaluate the multi-weight classification loss."
        },
        {
            "heading": "C. Uncertainty-based Label Filter",
            "text": "Pseudo-label noise affects the performance of self-training. A common approach is to use the prediction probability as the confidence to select reliable labels. However, the prediction probability does not fully reflect the reliability of the classification. [16] Thus, we propose an uncertainty-based label filtering strategy. The uncertainty of the pseudo-labels and the prediction probability are combined as filtering references. To be specific, we use MC-Dropout [17] to obtain an uncertainty measure by calculating the standard deviation of T stochastic forward passes. The random stops of each neuron of dropout layers conform to the Bernoulli distribution, so the predicted probability can be expressed as the following equation:\np\u0303ic \u2248 1\nT T\u2211 t=1 softmax ( fW\u0302t(xi) ) , (1)\nwhere W\u0302t denotes the model parameters at each sampling, and f(\u00b7) represent the model. By definition, the uncertainty of the label is shown as follows:\nu (p\u0303ic) = \u221a\u221a\u221a\u221a 1 T T\u2211 t=1 ( fW\u0302t(xi)\u2212 p\u0303ic )2 . (2)\nWe assume that reliable pseudo-labels have low uncertainty and high predictive probability. f\u0303i is used to denote the filter that the pseudo-label yi of a sample xi is considered reliable only if f\u0303i of that sample is not equal to 0.\nf\u0303i = 1 [u (p\u0303ic) \u2264 \u03bap] \u2217 1 [p\u0303ic \u2265 \u03c4p] , (3)\nwhere \u03bap and \u03c4p denote the thresholds of uncertainty and probability, respectively. Not all pseudo-labels are put back into the original training set, as this may lead to an increase in class imbalance and neglect of hard-to-classify samples. We use Borderline-SMOTE [18] to correct the pseudo-label imbalance problem by generating some samples near the classification boundary. In order not to destroy the learned feature representations in the imbalance case, we resample the pseudo-labels in a form close to the labeled sample distribution and control the degree of class imbalance (the ratio of the number of samples from the largest class to the smallest class) of the pseudo-labeled dataset within a certain threshold."
        },
        {
            "heading": "D. Hybrid Loss",
            "text": "By rethinking the results of self-training based semisupervised NIDS, we identify the key factors affecting the performance: 1) The class imbalance causes the feature distribution learned from typical cross-entropy can be highly skewed [19]. This leads to biased classifiers and obtaining the incorrect pseudo-labels. 2) Some hard-to-classify attack samples tend to exhibit similar patterns to normal attack samples, which makes the feature distributions close and the model difficult to obtain clear classification boundaries. Based on the above issues, we propose a hybrid loss function consisting of supervised contrastive loss and multi-weighted cross-entropy loss to obtain compact intra-class features and clear inter-class boundaries to improve attack classification.\na) Supervised Feature Contrastive Loss: Inspired by [20], we construct supervised contrastive loss for intrusion detection tasks to learn better feature representations from imbalanced data. The output of the projection head in RI1DCNN is the feature zi of the anchor xi. All samples with the same label as xi are considered as positive pairs, which are defined as { x+i } = {xj | yj = yi, i \u0338= j}. { z+i } represents the set of their feature vectors. yi is the label of sample xi. The following equation shows the supervised contrastive loss:\nLSCL = Nb\u2211 i=1 Li, (4)\nLi = \u22121\u2223\u2223{z+i }\u2223\u2223 \u2211 zj\u2208{z+i } log exp (zi \u00b7 zj/\u03c4)\u2211 zk,k \u0338=i exp (zi \u00b7 zk/\u03c4) , (5)\nwhere Nb denotes the batch size. \u03c4 \u2208 R+ is a scalar temperature parameter used to adjust the model\u2019s focus on the distance between samples. LSCL computes a weighted average of the similarity between xi and its all positive pairs, flexibly including an arbitrary number of positive samples, and optimizing the agreements between them.\nb) Multi-weighted Classification Loss: The intrusion detection dataset has the high-class imbalance and contains lowfrequency classes with small sample sizes. Directly using the sample number proportion of classes as classification loss weights may lead to overfitting and corrupting the original feature representations. For this, we designed a smooth class imbalance weight wi:\nwi = log (Nmin + n)\nlog (Ni + n) , (6)\nwhere Nmin is the number of samples in the smallest class and Ni is the number of samples in the class i. The constant n is used to compensate the low-frequency classes to prevent over-correction, and log is for smoothing the distribution.\nThe confusion of normal and attack samples also affects the performance of classification. We propose probabilistic reset weights wp to make the model more focused on such mistakes. All types of attack traffic are considered as a uniform anomaly class A and the class of normal samples is N . According to the error type of the prediction, the original gradient of the corresponding sample is fine-tuned. Equation 7 shows the calculation of the weights.\nwpi = 1y\u0302i \u0338=yi1y\u0302i\u2208N \u2217 \u03b1+ 1y\u0302i \u0338=yi1y\u0302i\u2208A \u2217 \u03b1, (7)\nwhere y\u0302i denotes the predicted label and yi denotes the true label. When the model confounds the normal and abnormal sample, the parameter \u03b1 is activated to adjust the model\u2019s attention to such misclassification. The weighted classification loss LWCE is calculated as follows:\nLWCE = \u2212 1\nK K\u2211 i=1 M\u2211 c=1 wigic log (wpipic) , (8)\nwhere K and M denote the number of samples and categories, respectively, and gic = {0, 1} is the sign function, which takes the value 1 if sample i is in the correct category c. pic is the probability that sample i is in category c.\nc) Calculate hybrid loss: The hybrid loss LHY combines supervised contrastive loss LSCL and multiple weighted classification loss LWCE :\nLHY = (1\u2212 \u03b2) \u2217 LWCE + \u03b2 \u2217 LSCL. (9)\nThe model should focus more on obtaining a good feature representation in the early stages of training, and on improving the classification performance in the later stage. Thus, we use the variable parameter \u03b2, which varies inversely with epoch, to adjust the weights of different losses, making better feature learning to help simplify the training of the classifier."
        },
        {
            "heading": "IV. EXPERIMENTAL RESULT",
            "text": "In this section, we design three experiments to evaluate the performance of the SF-IDS. First, we validate the finegrained classification performance with 1% labeled samples in different datasets. Besides some classical supervised models, we choose the well-performing semi-supervised model FixMatch [21] and the state-of-the-art semi-supervised intrusion detection model Semi-WTC [15] as the baselines. The results indicate that SF-IDS improves all metrics, especially MarcoF1, and has generalizability across different datasets. Second, we evaluated the performance with different label ratios and different unlabeled sample sizes, and SF-IDS obtained a boost in precision and Marco-F1. Finally, a simple ablation\nexperiment validates the effectiveness of the proposed hybrid loss. The experiments are implemented using the deep learning framework Pytorch and run on a server with GeForce RTX 3090Ti and Intel(R) Xeon(R) Gold 6230R 2.10GHz CPUs."
        },
        {
            "heading": "A. Dataset and Evaluation Measures",
            "text": "The classic NSL-KDD dataset and the CICIDS2017 dataset were used to comprehensively evaluate the SF-IDS.\n\u2022 NSL-KDD: This dataset contains 41 features for each sample. It has 77,054 normal and 71,463 abnormal traffic. 80% of the dataset was selected for training and the remaining 20% as the test set. 1% of the training samples are labeled. The categories with too few samples after segmentation are combined together as an additional category called \u201cattack\u201d. After that, 11 categories are involved in the experiments. \u2022 CICIDS2017: This dataset is a representation of real network traffic data, which contains 2,830,743 traffic samples, each with 78 attributes. It is extremely unbalanced, with only 24.5% of the attack samples. Equally, 80% of the data is used as the training set, with 1% samples of it labeled. We keep the original fine-grained attack categories as much as possible and merge the categories whose samples are too sparse after segmentation, with 11 categories finally being kept.\nWe encode the features that are not suitable for the deep learning model and standardize the numerical features. Table I shows the number of 1% labeled samples used for training and the size of the test dataset. The evaluation metrics include Accuracy, precision, recall, and F1-score, where precision and F1-score are given more attention in the imbalanced classification task. Notably, we use Marco-F1 instead of the original Micro-F1, which focuses on each category equally and reflect the classification performance more objectively."
        },
        {
            "heading": "B. Effectiveness of Fine-grained Classification",
            "text": "The proposed hybrid loss involves the choice of several hyperparameters, among which \u03c4 and \u03b1 need to be given extra attention due to their impact on fine category classification. We trained the full amount of data several times to determine their values. \u03c4 is used for supervised contrastive loss, the smaller\nthe parameter the more the model focuses on differentiating the most similar difficult samples. But too small the \u03c4 can destroy the learned semantics. Here, \u03c4 is set to 0.05. \u03b1 is used to penalize the model for misclassification of normal and attack samples, and the smaller the \u03b1 the larger the penalty. Thus, we set \u03b1 = 0.95 for training. The choice of other hyperparameters is determined according to the dataset and\nthe different demands on the task. We evaluate the performance of SF-IDS on the NSLKDD and CICIDS2017 datasets with 1% labeled samples and present the results in Table II and Table III, respectively. The name of each category name is represented in the column No. of the Table I. Precision is used to denote the classification effectiveness for each category. To obtain more objective results, each value is the average of multiple evaluations, expressed as a percentage, and here five are chosen randomly. According to Table II, it can be seen that the proposed SF-IDS achieves the best results for the four performance evaluation metrics with only 1% labeled NSL-KDD dataset. Compared with the optimal comparison models, precision improves by 2.84% and Marco-F1 improves by 3.00%. This is due to the fact that SF-IDS makes full use of the value of unlabeled data and combines the hybrid loss function to obtain more compact class features and clearer classification boundaries. Moreover, SF-IDS achieves the best accuracy rate in 7 of the 11 finegrained attack categories of NSL-KDD. In contrast, some traditional machine learning and supervised models are limited by the labeled sample size, making feature learning difficult. FixMatch requires data augmentation, which may not work well for traffic data. Semi-WTC resamples the data before training, which makes it practically difficult to adapt the model to the extremely unbalanced class distribution. Table III validates the performance of SF-IDS for fine-grained classification with the lack of labeled samples on the CICIDS2017 dataset. The SF-IDS achieves the best overall metrics with a 3.08% improvement in precision and a 2.71% improvement in MarcoF1. It also has the most SOTA results in the fine-grained classification. The performance on both datasets demonstrates that SF-IDS has good generalization capability."
        },
        {
            "heading": "C. Performance in Different Label Ratios",
            "text": "To further validate the ability of SF-IDS to utilize labeled and unlabeled samples, we compared precision and Marco-F1 for different label ratios (1%, 5%, and 10%) and different unlabeled sample sizes (0%, 50%, and 100%). The bestperforming baseline, Semi-WTC, is used as the comparison model. Table IV shows the results, L and UL denote labeled and unlabeled samples. Even using only labeled samples, the hybrid loss and RI-1DCNN still give SF-IDS excellent classification ability, which makes the pseudo-labels more reliable. In addition, the improvement of the label ratio and the increase of unlabeled samples can improve the metrics. Among them, Marco-F1 boosts the highest by 3.89%. Semi-WTC requires artificially modifying the feature weights of hardto-score samples, lacks the ability to generalize to different data distributions and relies on experience. Meanwhile, its backbone model consists of only some linear layers stacked feature extraction capability is weak."
        },
        {
            "heading": "D. Effectiveness of Hybrid Loss",
            "text": "A simple ablation experiment is conducted to verify the effectiveness of the hybrid loss LHY as well as the addition of supervised contrastive loss LSCL and multi-weighted classification loss LWCE alone. The backbone model is trained supervised on 1% labeled NSL-KDD dataset, where LSCL and LWCE are alternately turned off and the baseline is the same model with original cross-entropy loss. TableV shows the results. LSCL and LWCE improve the Marco-F1 by 1.33% and 1.41%, respectively. Experiments show that the hybrid loss tightens the feature distribution of each category by labels, and constructs a clear classification boundary with the help of effective class weights, which improves the classification performance of Marco-F1 by 2.05%."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we propose an imbalanced semi-supervised learning framework, SF-IDS, for fine-grained intrusion detection, including a self-training backbone model RI-1DCNN and a hybrid loss, addressing both the insufficient labels and the long-tailed classification problem. The RI-1DCNN reconstructs the input samples as multichannel images by extending the neurons and assigning real meanings to them to enable the convolutional model to better extract feature associations. Uncertainty is used as a reference for pseudo-label filtering to solve the label noise problem. Also, the hybrid loss integrates supervised contrastive learning and multi-weighted classification loss to achieve intra-class feature compactness and classifier correction. SF-IDS has significant performance gains in two datasets with 1% labeled samples. The framework has the potential to be applied to more cybersecurity domains to solve similar problems in a generalized manner."
        }
    ],
    "title": "SF-IDS: An Imbalanced Semi-Supervised Learning Framework for Fine-grained Intrusion Detection",
    "year": 2023
}