{
    "abstractText": "Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for largescale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy of our proposed algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hongchang Gao"
        }
    ],
    "id": "SP:cf7a5817f0129a0101d3df8b0431b1b9c67928ce",
    "references": [
        {
            "authors": [
                "K. Balasubramanian",
                "S. Ghadimi",
                "A. Nguyen"
            ],
            "title": "Stochastic multilevel composition optimization algorithms with level-independent convergence rates",
            "venue": "SIAM Journal on Optimization,",
            "year": 2022
        },
        {
            "authors": [
                "T. Chen",
                "Y. Sun",
                "W. Yin"
            ],
            "title": "Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization",
            "venue": "arXiv preprint arXiv:2008.10847,",
            "year": 2020
        },
        {
            "authors": [
                "W. Cong",
                "M. Ramezani",
                "M. Mahdavi"
            ],
            "title": "On the importance of sampling in training gcns: Tighter analysis and variance reduction",
            "venue": "arXiv e-prints, pages arXiv\u20132103,",
            "year": 2021
        },
        {
            "authors": [
                "A. Cutkosky",
                "F. Orabona"
            ],
            "title": "Momentum-based variance reduction in non-convex sgd",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "C. Fang",
                "C.J. Li",
                "Z. Lin",
                "T. Zhang"
            ],
            "title": "Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "H. Gao",
                "H. Huang"
            ],
            "title": "Periodic stochastic gradient descent with momentum for decentralized training",
            "venue": "arXiv preprint arXiv:2008.10435,",
            "year": 2020
        },
        {
            "authors": [
                "H. Gao",
                "H. Huang"
            ],
            "title": "Fast training method for stochastic compositional optimization problems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "S. Ghadimi",
                "A. Ruszczynski",
                "M. Wang"
            ],
            "title": "A single timescale stochastic approximation method for nested stochastic optimization",
            "venue": "SIAM Journal on Optimization,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Hua",
                "K. Miller",
                "A.L. Bertozzi",
                "C. Qian",
                "B. Wang"
            ],
            "title": "Efficient and reliable overlay networks for decentralized federated learning",
            "venue": "SIAM Journal on Applied Mathematics,",
            "year": 2022
        },
        {
            "authors": [
                "W. Jiang",
                "B. Wang",
                "Y. Wang",
                "L. Zhang",
                "T. Yang"
            ],
            "title": "Optimal algorithms for stochastic multi-level compositional optimization",
            "venue": "arXiv preprint arXiv:2202.07530,",
            "year": 2022
        },
        {
            "authors": [
                "A. Koloskova",
                "T. Lin",
                "S.U. Stich",
                "M. Jaggi"
            ],
            "title": "Decentralized deep learning with arbitrary communication compression",
            "venue": "arXiv preprint arXiv:1907.09356,",
            "year": 2019
        },
        {
            "authors": [
                "A. Koloskova",
                "S. Stich",
                "M. Jaggi"
            ],
            "title": "Decentralized stochastic optimization and gossip algorithms with compressed communication",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "X. Lian",
                "J. Liu"
            ],
            "title": "Revisit batch normalization: New understanding from an optimization view and a refinement via composition optimization",
            "venue": "arXiv preprint arXiv:1810.06177,",
            "year": 2018
        },
        {
            "authors": [
                "X. Lian",
                "C. Zhang",
                "H. Zhang",
                "C.-J. Hsieh",
                "W. Zhang",
                "J. Liu"
            ],
            "title": "Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent",
            "venue": "arXiv preprint arXiv:1705.09056,",
            "year": 2017
        },
        {
            "authors": [
                "L.M. Nguyen",
                "J. Liu",
                "K. Scheinberg",
                "M. Tak\u00e1\u010d. Sarah"
            ],
            "title": "A novel method for machine learning problems using stochastic recursive gradient",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Song",
                "W. Li",
                "K. Jin",
                "L. Shi",
                "M. Yan",
                "W. Yin",
                "K. Yuan"
            ],
            "title": "Communication-efficient topologies for decentralized learning with o(1) consensus rate",
            "venue": "arXiv preprint arXiv:2210.07881,",
            "year": 2022
        },
        {
            "authors": [
                "H. Sun",
                "S. Lu",
                "M. Hong"
            ],
            "title": "Improving the sample and communication complexity for decentralized nonconvex optimization: Joint gradient estimation and tracking",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "M. Wang",
                "E.X. Fang",
                "H. Liu"
            ],
            "title": "Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions",
            "venue": "Mathematical Programming,",
            "year": 2017
        },
        {
            "authors": [
                "R. Xin",
                "U.A. Khan",
                "S. Kar"
            ],
            "title": "A near-optimal stochastic gradient method for decentralized non-convex finite-sum optimization",
            "venue": "arXiv preprint arXiv:2008.07428,",
            "year": 2020
        },
        {
            "authors": [
                "S. Yang",
                "M. Wang",
                "E.X. Fang"
            ],
            "title": "Multilevel stochastic gradient methods for nested composition optimization",
            "venue": "SIAM Journal on Optimization,",
            "year": 2019
        },
        {
            "authors": [
                "B. Ying",
                "K. Yuan",
                "Y. Chen",
                "H. Hu",
                "P. Pan",
                "W. Yin"
            ],
            "title": "Exponential graph is provably efficient for decentralized deep training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "H. Yu",
                "L. Wang",
                "B. Wang",
                "M. Liu",
                "T. Yang",
                "S. Ji"
            ],
            "title": "Graphfm: Improving large-scale gnn training via feature momentum",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "H. Yuan",
                "W. Hu"
            ],
            "title": "Stochastic recursive momentum method for non-convex compositional optimization",
            "venue": "arXiv preprint arXiv:2006.01688,",
            "year": 2020
        },
        {
            "authors": [
                "H. Yuan",
                "X. Lian",
                "J. Liu"
            ],
            "title": "Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization",
            "venue": "arXiv preprint arXiv:1912.13515,",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "L. Xiao"
            ],
            "title": "A composite randomized incremental gradient method",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "L. Xiao"
            ],
            "title": "A stochastic composite gradient method with incremental variance reduction",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "L. Xiao"
            ],
            "title": "Multilevel composite stochastic optimization via nested variance reduction",
            "venue": "SIAM Journal on Optimization,",
            "year": 2021
        },
        {
            "authors": [
                "S. Zhao",
                "Y. Liu"
            ],
            "title": "Distributed stochastic compositional optimization problems over directed networks",
            "venue": "arXiv preprint arXiv:2203.11074,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Stochastic multi-level compositional optimization problems cover many new machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which require efficient optimization algorithms for largescale applications. This paper studies the decentralized stochastic multi-level optimization algorithm, which is challenging because the multi-level structure and decentralized communication scheme may make the number of levels affect the order of the convergence rate. To this end, we develop two novel decentralized optimization algorithms to deal with the multi-level function and its gradient. Our theoretical results show that both algorithms can achieve the level-independent convergence rate for nonconvex problems under much milder conditions compared with existing single-machine algorithms. To the best of our knowledge, this is the first work that achieves the level-independent convergence rate under the decentralized setting. Moreover, extensive experiments confirm the efficacy of our proposed algorithms."
        },
        {
            "heading": "1 Introduction",
            "text": "In recent years, some new learning paradigms, such as model-agnostic meta-learning [6], have been proposed to handle realistic machine learning applications, which are typically beyond the class of traditional stochastic optimization. Some examples include bilevel optimization, minimax optimization, compositional optimization, and so on. Of particular interest in this paper is the learning paradigm that can be formulated as the stochastic multi-level compositional optimization problem. More particularly, we are interested in the decentralized setting where data are distributed on different devices and the device performs peer-to-peer communication to exchange information with its neighboring devices. Mathematically, the loss function is defined as follows:\nmin x\u2208Rd\nF (x) = 1\nN N\u2211 n=1 Fn(x) , where Fn(x) = f (K) n \u25e6 f (K\u22121)n \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f (2)n \u25e6 f (1)n (x) , (1.1)\nwhere x \u2208 Rd is the model parameter of a machine learning model, N devices compose a communication network, and Fn(x) is the loss function on the n-th device, for \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K \u2212 1,K}, f (k)n (\u00b7) = E \u03be (k) n [f (k) n (\u00b7; \u03be(k)n )] : Rdk\u22121 \u2192 Rdk is the k-th level function on the n-th device, where \u03be(k)n denotes the data distribution for the k-th level function on the n-th device. It can observed that the input of f (k) n (\u00b7) is the output of f (k\u22121) n (\u00b7).\nThe stochastic multi-level compositional optimization (multi-level SCO) problem covers a wide range of machine learning models. For instance, the multi-step model-agnostic meta-learning [6] can be formulated as a multi-level SCO problem. The stochastic training of graph neural networks also belongs to the class of multi-level SCO problem [23, 3]. The neural network with batch-normalization is actually a multi-level SCO problem [14]. The challenge of optimizing the multi-level SCO problem lies in that the stochastic gradient is not an unbiased estimator of the full gradient when the inner-level functions are nonlinear. To address this challenge, a couple of stochastic multi-level compositional gradient descent (multi-level SCGD) algorithms have been proposed recently. For instance, [21] proposed the first stochastic multi-level compositional gradient descent algorithm. However, due to the nested structure of the loss function, the order of its convergence rate depends on the number of levels K 1, where a larger K results in\n\u2217Temple University, hongchang.gao@temple.edu 1Throughout this paper, the level-dependent convergence rate means that the number of levels K affects the order of the convergence rate, e.g., O(\u03f5\u2212K), while the level-independent convergence rate indicates that K does not affect its order but may affect its coefficient, e.g., O(K\u03f5\u22122).\nar X\niv :2\n30 6.\n03 32\n2v 1\n[ cs\n.L G\n] 6\nJ un\n2 02\n3\na slower convergence rate. As such, it cannot match the traditional stochastic gradient descent algorithm. Later, some algorithms [28, 1, 11] were proposed to achieve the level-independent convergence rate via leveraging the variance-reduced estimator. For instance, [28] exploits the SPIDER [16, 5] estimator for both the stochastic function value f (k) n (\u00b7; \u03be(k)n ) and stochastic gradient \u2207f (k)n (\u00b7; \u03be(k)n ) of each level function to improve the convergence rate. However, existing stochastic multi-level compositional optimization algorithms have some limitations. On the one hand, they only focus on the single-machine setting. As such, they cannot be used to solve Eq. (1.1) for the distributed multi-level SCO problem. In particular, it is unclear if the level-independent convergence rate is still achievable under the decentralized setting. More particularly, it is unclear whether the consensus error caused by the decentralized communication scheme will make the level-independent convergence rate unachievable when there are multi-level inner functions. On the other hand, under the single-machine setting, those algorithms with the variance-reduced estimator have some unrealistic operations, limiting their applications in real-world tasks. In particular, they apply the variance reduction technique to the stochastic gradient of each level function f (k) n , i.e., k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K}, which requires the clipping operation, e.g., Algorithm 3 in [28], or the projection operation, e.g., Eq. (3) in [11], to upper bound the variancereduced gradient. These operations either result in a very small learning rate or depend on unknown hyperparameters. These limitations motivate us to 1) develop the decentralized optimization algorithms for Eq. (1.1) to facilitate the training of large-scale multi-level SCO problems, 2) propose the practical algorithm based on the variance-reduced stochastic gradient under mild conditions, and 3) establish the level-independent convergence rate for the proposed algorithms.\nTo this end, we developed two novel decentralized multi-level stochastic compositional gradient descent algorithms, both of which can achieve the level-independent convergence rate. They have the following contributions. 1) Specifically, our first algorithm demonstrates how to achieve the level-independent convergence rate with a novel combination of the inner-level function estimator and the momentum technique. Our second algorithm improves the convergence rate with a novel strategy of applying the variance-reduced estimator. In particular, unlike existing algorithms, which apply the variance reduction approach to both each level function value f (k) n (\u00b7; \u03be(k)n ) and each level function\u2019s gradient \u2207f (k)n (\u00b7; \u03be(k)n ), our algorithms leverage the variance-reduced estimator for each level function value f (k) n (\u00b7; \u03be(k)n ) and the whole gradient \u2207Fn(x; \u03ben). As such, our algorithms do not require a small learning rate or strong assumption to guarantee convergence. Meanwhile, it is more friendly to implement. 2) Besides the novel algorithmic design, we established the level-independent convergence rate of our two algorithms under the decentralized setting. In particular, our first algorithm, which leverages the momentum approach for the whole gradient, enjoys the convergence rate of O(\u03f5\u22124) to achieve the \u03f5-stationary point for nonconvex problems. Our second algorithm, which exploits the variance reduction approach for the whole gradient, can achieve the convergence rate of O(\u03f5\u22123) for nonconvex problems. As far as we know, this is the first decentralized optimization work for multi-level SCO with theoretical guarantees. 3) Extensive experiments on multi-step model-agnostic meta-learning task confirm the effectiveness of our algorithms."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Stochastic Two-level Compositional Optimization",
            "text": "The stochastic two-level compositional optimization problem has been extensively studied in the past few years. In particular, to address the biased gradient estimator problem, [19] developed the stochastic compositional gradient descent algorithm for the first time, where the moving-average approach is applied to the estimation of the inner-level function to control the estimation error. However, its sample complexity is as large as O(\u03f5\u22128) for nonconvex problems, which is worse than O(\u03f5\u22124) of the standard stochastic gradient descent algorithm for non-compositional optimization problems. Then, [9] applied the momentum approach to stochastic compositional gradient so that it improves the sample complexity to O(\u03f5\u22124). On the contrary, [2] leverages the variance-reduced estimator [4] for the inner-level function, which can also achieve the sample complexity of O(\u03f5\u22124). To further improve the convergence rate, a couple of works exploit the variance-reduced approach to control the estimation error for both the inner-level function value and its gradient. For instance, [25] leverages the SPIDER variance reduction approach [16, 5] to improve the sample complexity to O(\u03f5\u22123) for stochastic nonconvex problems. However, this algorithm requires a large batch size. To address this problem, [24] employs the STORM variance reduction approach [4], which can also achieve\nthe sample complexity of O(\u03f5\u22123), but with a small batch size. As for the nonconvex finite-sum compositional problem, a couple of works [26, 27, 25] also utilize the variance-reduction approaches to improve the sample complexity to match the counterpart for non-compositional problems."
        },
        {
            "heading": "2.2 Stochastic Multi-level Compositional Optimization",
            "text": "Even though the aforementioned algorithms can achieve desired sample complexity for the two-level compositional problem, it is non-trivial to extend them to the multi-level problem for achieving the same sample complexity. For instance, [21] developed an accelerated stochastic compositional gradient descent algorithm for the stochastic multi-level compositional optimization problem, which can only achieve the sample complexity of O(\u03f5\u2212(7+K)/2) for nonconvex problems. Obviously, this sample complexity depends on the number of function levels K, which is far from satisfied. Later, [1] extends the momentum approach [9] to the multi-level problem, obtaining the O(\u03f5\u22126) sample complexity, which is worse than the counterpart [9] for the two-level problem. Then, they add a correction term when using the moving-average approach to estimate each level function so that the sample complexity is improved to O(\u03f5\u22124), which can match the standard momentum stochastic gradient descent algorithm. In [2], the STORM variance-reduction approach is leveraged to estimate each level function, which can also result in the sample complexity of O(\u03f5\u22124). In [28], the SPIDER variance-reduction approach is exploited to estimate both each level function and its gradient so that it can achieve the sample complexity of O(\u03f5\u22123). However, this algorithm requires a large batch size. Moreover, it requires a small learning rate to guarantee the Lipschitz continuousness of the variance-reduced gradient. Recently, [11] leverage the STORM variance-reduction approach to estimate each level function and its gradient, resulting in the sample complexity of O(\u03f5\u22123) with the mini-batch size of O(1). However, this algorithm implicitly uses a strong assumption, i.e., the variance-reduced gradient shares the same Lipschitz constant as the original gradient. Thus, these algorithms with the sample complexity O(\u03f5\u22123) are not practical for real-world applications. Moreover, it is unclear how to obtain the level-independent sample complexity under the decentralized setting."
        },
        {
            "heading": "2.3 Decentralized Optimization",
            "text": "Decentralized optimization has been extensively studied for the non-compositional optimization problem in recent years. In particular [15] found that the decentralized stochastic gradient descent (SGD) algorithm shares the same convergence rate asymptotically with the standard parallel SGD since the communication topology only affects the high-order term of its convergence rate. Then, numerous decentralized optimization algorithms have been developed. For example, [18, 20] studied how to improve the sample complexity via leveraging variance reduction approaches. [12, 13, 7] investigated how to reduce the communication cost via compressing the communicated variables or skipping the communication round. [17, 10, 22] explored how to design efficient communication topology. However, all these algorithms are based on the stochastic gradient, which is an unbiased estimator of the full gradient. They cannot be directly extended to the stochastic compositional optimization problem because its stochastic gradient is a biased estimator of the full gradient. Recently, to address this problem, [8] developed the decentralized stochastic compositional gradient descent algorithm for the two-level stochastic compositional problem, whose sample complexity is O(\u03f5\u22126). [29] leveraged the STORM-like approach to estimate the inner-level function and improved the sample complexity to O(\u03f5\u22124). However, these algorithms only focus on the two-level problem. It is unclear how to apply them to the multi-level problem to achieve the level-independent sample complexity."
        },
        {
            "heading": "3 Decentralized Stochastic Multi-level Compositional Optimiza-",
            "text": "tion\nIn this section, we present the details of our proposed algorithms under the decentralized setting. Here, it is assumed the devices compose a communication graph and perform peer-to-peer communication. The adjacency matrix W of this graph satisfies the following assumption.\nAssumption 1. W = [wij ] \u2208 RN\u00d7N is a symmetric and doubly stochastic matrix. Its eigenvalues satisfy |\u03bbN | \u2264 |\u03bbN\u22121| \u2264 \u00b7 \u00b7 \u00b7 \u2264 |\u03bb2| < |\u03bb1| = 1.\nUnder this assumption, we can denote the spectral gap as 1 \u2212 \u03bb where \u03bb = |\u03bb2|. Then, we propose two decentralized optimization algorithms for solving Eq. (1.1) in the following two subsections.\nAlgorithm 1 Decentralized Stochastic Multi-level Compositional Gradient Descent with Momentum\nInput: xn,0 = x0, \u03b1 > 0, \u03b2 > 0, \u00b5 > 0, \u03b7 > 0. 1: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 2: u\n(0) n,t = xn,t,\n3: for k = 1, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do 4: if t == 0 then 5: u\n(k) n,t = f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ),\n6: else 7: u\n(k) n,t = (1\u2212 \u03b2\u03b7)(u (k) n,t\u22121 \u2212 f (k) n (u (k\u22121) n,t\u22121; \u03be (k) n,t )) + f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ),\n8: end if 9: v\n(k) n,t = \u2207f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ),\n10: end for 11: v\n(K) n,t = \u2207f (K) n (u (K\u22121) n,t ; \u03be (K) n,t ), gn,t = v (1) n,tv (2) n,t \u00b7 \u00b7 \u00b7 v (K\u22121) n,t v (K) n,t ,\n12: if t == 0 then 13: mn,t = gn,t, yn,t = mn,t 14: else 15: mn,t = (1\u2212 \u00b5\u03b7)mn,t\u22121 + \u00b5\u03b7gn,t, yn,t = \u2211 n\u2032\u2208Nn wnn\u2032yn\u2032,t\u22121 +mn,t \u2212mn,t\u22121, 16: end if 17: xn,t+ 12 = \u2211 n\u2032\u2208Nn wnn\u2032xn\u2032,t \u2212 \u03b1yn,t, xn,t+1 = xn,t + \u03b7(xn,t+ 12 \u2212 xn,t), 18: end for"
        },
        {
            "heading": "3.1 Decentralized Multi-level SCGD with Momentum",
            "text": "Challenges. The momentum technique is commonly used in optimization. However, facilitating it to multilevel SCGD is non-trivial. Under the single-machine setting, [1] developed the first multi-level SCGD with momentum algorithm, which applies the moving-average approach to each inner-level function and the whole gradient. However, this straightforward extension can only achieve the O(\u03f5\u22126) sample complexity, which is worse than O(\u03f5\u22124) of the two-level algorithm. Then, [1] introduced a correction term to the inner-level function estimator to address this problem. However, this correction term requires to compute the gradient (See its Algorithm 2), which is too complicated and unclear if it works under the decentralized setting. Especially, it is unclear whether the consensus error caused by the decentralized communication topology will worsen the convergence rate in the presence of multi-level inner functions. Therefore, a natural question follows: How to design an efficient decentralized multi-level SCGD with momentum algorithm to achieve the level-independent convergence rate (sample complexity) O(\u03f5\u22124)?\nTo answer this question, in Algorithm 1, we develop the decentralized stochastic multi-level compositional gradient descent with momentum (D-M-SCGDM) algorithm. Specifically, to achieve the level-independent sample complexity, which can match the decentralized SGD with momentum algorithm for non-compositional problem, we leverage the STORM-like approach to estimate the k-th level function (where k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K\u2212 1}), which is shown below:\nu (k) n,t = (1\u2212 \u03b2\u03b7)(u (k) n,t\u22121 \u2212 f (k)n (u (k\u22121) n,t\u22121; \u03be (k) n,t )) + f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ) , (3.1)\nwhere \u03b2 > 0, \u03b7 > 0 are two hyperparameters satisfying \u03b2\u03b7 < 1, u (k) n,t is the estimation of the k-th level function f (k) n (u (k\u22121) n,t ) on the n-th device. Note that when \u03b2\u03b7 = 1, u (k) n,t becomes the stochastic function value f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ), which can lead to the level-dependent convergence rate [21].\nIt is worth noting that we do not apply this variance-reduction approach to the stochastic gradient\nv (k) n,t = \u2207f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ). Instead, we just use the standard stochastic gradient of each level function. After we obtain the stochastic gradient of each level function, we combine them to get the whole stochastic gradient of Fn(x), which is shown in Line 11. Then, we compute the momentum of this whole stochastic gradient in Line 15, where \u00b5 > 0 is a hyperparameter satisfying \u00b5\u03b7 < 1. After that, we leverage the gradient-tracking approach in Line 15 to communicate the momentum between different devices according\nto the communication topology, which is defined below: yn,t = \u2211\nn\u2032\u2208Nn\nwnn\u2032yn,t\u22121 +mn,t \u2212mn,t\u22121 , (3.2)\nwhere Nn = {n\u2032|wnn\u2032 > 0} denotes the neighbors of the n-th device and wnn\u2032 is the edge weight of the communication graph. Finally, we can leverage yn,t to update the model parameter on the corresponding device, which is shown in Line 17, where \u03b1 > 0 is a hyperparameter.\nNote that Eq. (3.1) has been used for non-momentum algorithm under the single-machine setting in [2]. Therefore, it is still unclear how it affects the convergence for the momentum algorithm or the decentralized setting. In fact, this is the first time to apply Eq. (3.1) to the momentum algorithm. We believe this novel algorithmic design can also be applied to the single-machine setting to accelerate existing algorithms, e.g., [2]. Moreover, to the best of our knowledge, this is the first decentralized optimization algorithm for the stochastic multi-level compositional optimization problem. Meanwhile, this new algorithmic design brings new challenges for convergence analysis due to the interaction between the estimator of each level function and momentum. We will address these challenges and show this algorithm can achieve the O(\u03f5\u22124) sample complexity in Section 4.\nAlgorithm 2 Decentralized Stochastic Multi-level Compositional Variance-Reduced Gradient Descent\nInput: xn,0 = x0, \u03b1 > 0, \u03b2 > 0, \u00b5 > 0, \u03b7 > 0. 1: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 2: u\n(0) n,t = xn,t,\n3: for k = 1, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do 4: if t == 0 then 5: With batch size S compute u\n(k) n,t = f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ), v (k) n,t = \u2207f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ),\n6: else 7: u\n(k) n,t = (1\u2212 \u03b2\u03b72)(u (k) n,t\u22121 \u2212 f (k) n (u (k\u22121) n,t\u22121; \u03be (k) n,t )) + f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ),\n8: v (k) n,t = \u2207f (k) n (u (k\u22121) n,t ; \u03be (k) n,t ),\n9: end if 10: end for 11: if t == 0 then 12: v\n(K) n,t = \u2207f (k) n (u (K\u22121) n,t ; \u03be (K) n,t ) with batch size S, mn,t = g \u03bet n,t, yn,t = mn,t,\n13: else 14: v\n(K) n,t = \u2207fn(u (K\u22121) n,t ; \u03be (K) n,t ),\n15: mn,t = (1\u2212 \u00b5\u03b72)(mn,t\u22121 \u2212 g\u03betn,t\u22121) + g \u03bet n,t, yn,t = \u2211 n\u2032\u2208Nn wnn\u2032yn,t\u22121 +mn,t \u2212mn,t\u22121, 16: end if 17: xn,t+ 12 = \u2211 n\u2032\u2208Nn wnn\u2032xn\u2032,t \u2212 \u03b1yn,t, xn,t+1 = xn,t + \u03b7(xn,t+ 12 \u2212 xn,t), 18: end for"
        },
        {
            "heading": "3.2 Decentralized Multi-level SCGD with Variance Reduction",
            "text": "To improve the convergence rate, in Algorithm 2, we propose our second algorithm: decentralized stochastic multi-level compositional variance-reduced gradient descent algorithm (D-M-SCVRGD).\nSimilar to Algorithm 1, we leverage the standard STORM approach 2 to estimate each level function, which is shown in Line 7. Different from Algorithm 1, we do not exploit the momentum to update model parameters. Instead, we take advantage of the variance-reduced gradient for local update, which is defined below:\nmn,t = (1\u2212 \u00b5\u03b72)(mn,t\u22121 \u2212 g\u03betn,t\u22121) + g \u03bet n,t , (3.3)\nwhere \u00b5 > 0 is a hyperparameter satisfying \u00b5\u03b72 < 1, the stochastic gradient g\u03betn,t and g \u03bet n,t\u22121 are defined below:\ng\u03betn,t\u22121 = \u2207f (1)n (u (0) n,t\u22121; \u03be (1) n,t)\u2207f (2)n (u (1) n,t\u22121; \u03be (2) n,t) \u00b7 \u00b7 \u00b7 \u2207f (K\u22121)n (u (K\u22122) n,t\u22121 ; \u03be (K\u22121) n,t )\u2207f (K)n (u (K\u22121) n,t\u22121 ; \u03be (K) n,t ) , g\u03betn,t = \u2207f (1)n (u (0) n,t; \u03be (1) n,t)\u2207f (2)n (u (1) n,t; \u03be (2) n,t) \u00b7 \u00b7 \u00b7 \u2207f (K\u22121)n (u (K\u22122) n,t ; \u03be (K\u22121) n,t )\u2207f (K)n (u (K\u22121) n,t ; \u03be (K) n,t ) .\n(3.4)\n2Compared with Algorithm 1, \u03b7 is replaced with \u03b72 when estimating each level function.\nThen, based on this variance-reduced gradient, we exploit the gradient-tracking approach to update the model parameter on each device, which is shown in Lines 15 and 17.\nDiscussion. Here, we would like to emphasize the novelty on the algorithmic design in Algorithm 2. Under the single-machine setting, existing variance-reduced multi-level compositional gradient descent algorithms [28, 11] apply the variance-reduction approach to each level function and its stochastic gradient. For instance, [11] computes the variance-reduced gradient for each level function:\nv (k) n,t = (1\u2212 \u03b2\u03b72)(v (k) n,t\u22121 \u2212\u2207f (k)n (u (k\u22121) n,t\u22121; \u03be (k) n,t )) +\u2207f (k)n (u (k\u22121) n,t ; \u03be (k) n,t ) . (3.5)\nThis kind of variance-reduced gradient for each level function suffers from some limitations. On the theoretical analysis side, when bounding the gradient variance of the whole function Fn(\u00b7), it requires v(k)n,t to be upper bounded in all levels and iterations. To do that, [28] uses a clipping operation, which may result in a very tiny update (See \u03b3t in Algorithm 3 of [28]), while [11] employs a projection operation to guarantee v (k) n,t is upper bounded by the Lipschitz constant of the standard gradient (See Eq. (3) in [11]), which is an unknown hyperparameter so that it is not feasible in practice. On the implementation side, these algorithms are not friendly for practical applications. For instance, when applying them to the stochastic training of graph neural networks (GNN), computing the variance-reduced gradient for each level function (i.e., each layer of GNN) requires to manipulate the backpropagation in each layer, which is not easy to implement.\nOn the contrary, our Algorithm 2 just computes the standard stochastic gradient for each level function. This can naturally avoid the aforementioned impractical operations since the standard stochastic gradient is easy to bound under the commonly used assumptions. Meanwhile, it is easy to compute. However, using standard stochastic gradient of each level function may introduce a large variance. Then, a natural question follows: Can Algorithm 2 achieve the O(\u03f5\u22123) sample complexity as [28, 11] when not using the variance reduction approach for each level function\u2019s gradient? In Section 4, we provide an affirmative answer: Our Algorithm 2 can still achieve the O(\u03f5\u22123) sample complexity, even though we don\u2019t use the variance reduced gradient for each level function.\nAll in all, our algorithm is novel and we believe our idea can be leveraged to improve existing singlemachine algorithms [28, 11]."
        },
        {
            "heading": "4 Convergence Analysis",
            "text": "To establish the convergence rate of our algorithms, we introduce the following assumptions, which are commonly used in existing multi-level compositional optimization works [21, 28, 11].\nAssumption 2. For \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} and \u2200y1, y2 \u2208 Rdk\u22121 , there exists Lk > 0 such that \u2225\u2207f (k)(y1) \u2212 \u2207f (k)(y2)\u2225 \u2264 Lk\u2225y1 \u2212 y2\u2225. Additionally, Fn(x) is LF -smooth 3.\nAssumption 3. For \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} and \u2200y \u2208 Rdk\u22121 , there exists Ck > 0 such that E[\u2225\u2207f (k)(y; \u03be)\u22252] \u2264 C2k and \u2225\u2207f (k)(y)\u22252 \u2264 C2k .\nAssumption 4. For \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K} and \u2200y1, y2 \u2208 Rdk\u22121 , there exist \u03c3k > 0 and \u03b4k > 0 such that E[\u2225\u2207f (k)(y; \u03be)\u2212\u2207f (k)(y)\u22252] \u2264 \u03c32k and E[\u2225f (k)(y; \u03be)\u2212 f (k)(y)\u22252] \u2264 \u03b42k.\nBased on these assumptions, we denote Ak = ( \u2211K\u22121 j=k ( Lj+1 \u220fK i=1 Ci\nCj+1\n\u220fj i=k+1 Ci)) 2 and Bk = \u220fK j=1 C 2 j\nC2k for k \u2208 {1, \u00b7 \u00b7 \u00b7 ,K \u2212 1}, as well as Dk = ( \u220fK j=1 C 2 j )L 2 k+1\nC2k+1 for k \u2208 {0, \u00b7 \u00b7 \u00b7 ,K \u2212 1}. In our paper, we use z\u0304t to denote\nthe mean value across devices for any variables. Theorem 1. Given Assumptions 1-4, by setting \u00b5 > 0, \u03b2 > 0, \u03b1 \u2264 min{(1\u2212 \u03bb)2/ \u221a \u03b1\u03031, 1/(4 \u221a \u03b1\u03032)}, \u03b7 \u2264\nmin{\u03c9\u0303k/(8\u03b2 \u2211K\u22121 j=1 \u03c9\u0303jC 2 j \u220fj i=k+1(2C 2 i )), 1/(2\u03b1LF ), 1/\u03b2, 1/\u00b5, 1} for \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K \u2212 1}, Algorithm 1 has the following convergence rate:\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207F (x\u0304t)\u22252] \u2264 2(F (x0)\u2212 F (x\u2217)) \u03b1\u03b7T +O( \u00b5K T ) +O( K \u03b7T ) +O( K \u00b5\u03b7T )\n+O(\u03b22\u00b52\u03b73K) +O(\u00b52\u03b7K) +O(\u00b53\u03b72K) +O(\u03b22\u03b72K) +O(\u00b5\u03b7K) +O(\u03b22\u03b7K) ,\n(4.1)\n3Based on the smoothness of each level function, it is easy to prove Fn is smooth [21, 28, 11] so that we directly assume it is smooth.\nwhere \u03c9\u0303k = 2 \u03b2 ((12Ak+8Dk)\u00b5+2Ak+2\u03b2 \u2211K\u22121 j=k+1(20AjC 2 j +8DjC 2 j ) \u220fj i=k+1(2C 2 i )) for k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K\u22121}, and \u03c9\u0303K+1 = L 2 F + 8( 2L2F \u00b52 + 8L 2 F + 4KD0 + K \u2211K\u22121 k=1 (20AkC 2 k + 2\u03c9\u0303kC 2 k + 8DkC 2 k)( \u220fk\u22121 j=1 (2C 2 j ))), \u03b1\u03031 = 4\u03c9\u0303K+1 + 8L 2 F /\u00b5 2 + 32L2F + 16KD0 + 4K \u2211K\u22121 k=1 (20AkC 2 k + 2\u03c9\u0303kC 2 k + 8DkC 2 k)( \u220fk\u22121 j=1 (2C 2 j )), \u03b1\u03032 = 2L 2 F /\u00b5 2 +\n8L2F + 4KD0 +K \u2211K\u22121 k=1 (20AkC 2 k + 2\u03c9\u0303kC 2 k + 8DkC 2 k)( \u220fk\u22121 j=1 (2C 2 j )).\nCorollary 1. Given Assumptions 1-4, by setting \u00b5 = O(1), \u03b2 = O(1), \u03b1 = O((1 \u2212 \u03bb)2), \u03b7 = O(\u03f52), T = O((1\u2212 \u03bb)\u22122\u03f5\u22124), Algorithm 1 can achieve the \u03f5-stationary point, i.e., 1T \u2211T\u22121 t=0 E[\u2225\u2207F (x\u0304t)\u22252] \u2264 \u03f52.\nRemark 1. Given \u00b5 = O(1) and \u03b2 = O(1), the hyperparameters \u03b1\u0303i (i = 1, 2) and \u03c9\u0303k (k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K \u2212 1,K + 1}) are independent of the learning rate and spectral gap. Thus, they do not affect the order of the convergence rate.\nRemark 2. From Corollary 1, we can know that the convergence rate of Algorithm 1 is O((1 \u2212 \u03bb)\u22122\u03f5\u22124), which is independent of the number of function levels. Meanwhile, it indicates that the dependence on the spectral gap is O((1\u2212\u03bb)\u22122). When the communication graph is fully connected, the convergence rate becomes O(\u03f5\u22124), which can match the single-machine momentum algorithm [1]. All in all, the level-independent convergence rate is achievable under the dencetralized setting.\nRemark 3. Since the mini-batch size is O(1), the sample complexity is O((1 \u2212 \u03bb)\u22122\u03f5\u22124). Moreover, the communication complexity is O((1\u2212 \u03bb)\u22122\u03f5\u22124). Theorem 2. Given Assumptions 1-4, by setting \u00b5 > 0, \u03b2 > 0, \u03b1 \u2264 min{(1\u2212 \u03bb)2/ \u221a \u03b1\u03031, 1/(4 \u221a \u03b1\u03032)},\n\u03b7 \u2264 min{0.5 \u221a \u03c9\u0303k/(2\u03b2 \u2211K\u22121 j=1 \u03c9\u0303jC 2 j ( \u220fj i=k+1(2C 2 i ))), 1/(2\u03b1LF ), 1/ \u221a \u03b2, 1/ \u221a \u00b5, 1} for \u2200k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K \u2212 1}, Algorithm 2 has the following convergence rate:\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207F (x\u0304t)\u22252] \u2264 2(F (x0)\u2212 F (x\u2217)) \u03b1\u03b7T +O( K \u03b72TS ) +O( K \u00b5\u03b72TS ) +O( \u00b5\u03b7K TS ) +O( K \u03b7T ) +O(\u03b22\u03b73K) +O(\u00b52\u03b73K) +O(\u03b22\u03b72K) +O( \u03b22\u03b72K\n\u00b5 ) +O(\u00b5\u03b72K) +O(\u00b5\u03b22\u03b75K) +O(\u00b53\u03b75K) ,\n(4.2)\nwhere \u03c9\u0303k = 16Dk \u00b5N +24Dk+ 4Ak \u03b2 +16 \u2211K\u22121 j=1 (( 2 \u00b5N +3)DjC 2 j )( \u220fj i=k+1(2C 2 i )) for k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K\u22121}, \u03c9\u0303K+2 =\n16( \u2211K\u22121\nk=1 (( 8K \u00b5N +12K)DkC 2 k+2\u03c9\u0303kC 2 k)( \u220fk\u22121 j=1 (2C 2 j ))+ 4KD0 \u00b5N +6KD0)+2L 2 F , \u03b1\u03031 = 2\u03c9\u0303K+2+4K[ \u2211K\u22121 k=1 (( 8 \u00b5N +\n12)DkC 2 k+2\u03c9\u0303kC 2 k)( \u220fk\u22121 j=1 (2C 2 j ))+ 4D0 \u00b5N +6D0], \u03b1\u03032 = K \u2211K\u22121 k=1 (( 8 \u00b5N +12)DkC 2 k+2\u03c9\u0303kC 2 k)( \u220fk\u22121 j=1 (2C 2 j ))+ 4KD0 \u00b5N + 6KD0.\nCorollary 2. Given Assumptions 1-4, by setting \u00b5 = O(1), \u03b2 = O(1), \u03b1 = O((1 \u2212 \u03bb)2), S = O(\u03f5\u22121), \u03b7 = O(\u03f5), T = O((1\u2212 \u03bb)\u22122\u03f5\u22123), Algorithm 2 can achieve \u03f5-stationary point.\nRemark 4. Given \u00b5 = O(1) and \u03b2 = O(1), the hyperparameters \u03b1\u0303i (i = 1, 2) and \u03c9\u0303k (k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K \u2212 1,K + 2}) also do not affect the order of the convergence rate.\nRemark 5. From Corollary 2, we can know that the convergence rate of Algorithm 1 is O((1\u2212 \u03bb)\u22122\u03f5\u22123)), which is also independent of the number of function levels and has the dependence on the spectral gap with O((1 \u2212 \u03bb)\u22122). Moreover, this convergence rate is better than our Algorithm 1. Additionally, when the communication graph is fully connected, the convergence rate can match the single-machine algorithms [28, 11], but our Algorithm 2 requires much milder operations than [28, 11].\nRemark 6. Since the mini-batch size is O(1) except the first iteration, the sample complexity is O((1 \u2212 \u03bb)\u22122\u03f5\u22123)). Similarly, we can know that the communication complexity is O((1\u2212 \u03bb)\u22122\u03f5\u22123))."
        },
        {
            "heading": "5 Experiment",
            "text": "In this section, we apply our proposed algorithms to the multi-step model-agnostic meta-learning task to verify the performance of our algorithms."
        },
        {
            "heading": "5.1 Multi-Step Model-Agnostic Meta-Learning",
            "text": "Model-agnostic meta-learning (MAML) [6] is to learn an initialization model that can be adapted to a new task via a couple of steps of stochastic gradient descent. Basically, the one-step MAML under the decentralized setting is defined as below:\nmin x\u2208Rd\n1\nN N\u2211 n=1 Ei\u223cPn,task,\u03b6n\u223cDn,queryi [Ln,i (y; \u03b6n)] , (5.1)\nwhere y = x\u2212 \u03bdE\u03ben\u223cDn,supporti\u2207Ln,i (x; \u03ben) , (5.2)\nwhere Eq. (5.2) denotes one-step gradient descent, \u03bd is the learning rate, Pn,task denotes the task distribution on the n-th device, Dn,queryi (Dn,supporti) represents the query (support) set of the i-th task on the n-th device. This one-step update can be viewed as a two-level compositional optimization problem. If taking multiple gradient descent steps, this problem becomes a multi-level compositional optimization problem [11, 2]. Therefore, we can apply our algorithms to the multi-step MAML problem. In our experiment, we will focus on two tasks: regression and classification problems."
        },
        {
            "heading": "5.2 Experimental Settings and Results",
            "text": "Regression. For the regression problem, we follow [6] to generate a sinewave dataset. Specifically, when generating the sine wave, the amplitude is randomly picked from [0.1, 5.0], the phase is from [0, \u03c0], and the input is from [\u22125, 5]. The model used for this task is a fully-connected neural network with the dimensionality as [1, 40, 40, 1]. For the support set, the meta-batch size (tasks) on each device is set to 200 and the number of samples for each task is 10. For the query set, the meta-batch size is 500 and the number of samples in each task is also 10. Moreover, the number of gradient descent updates in Eq. (5.2) is 3 so that it is a four-level compositional optimization problem. The learning rate \u03bd is 0.01.\nClassification. In this experiment, we use Omniglot dataset, which has 1,623 characters (tasks) and each character has 20 images. 1,200 tasks are used as the support set and the left tasks are used as the query set. Following [6], we employ the 5-way-1-shot setting. The model we used has four convolutional layers, where each layer has 64 3\u00d7 3 filters, and one linear layer. The meta-batch size (tasks) on each device is set to 8. The number of gradient descent updates in Eq. (5.2) is set to 3 so that it is also a four-level compositional optimization problem. The learning rate \u03bd is 0.01 too.\nIn our experiments, we select \u00b5 and \u03b2 from {1, 3, 5, 7, 9}, and fix \u03b1 to 1.0. Additionally, we set \u03f52 = 0.1. Then, we set the learning rate \u03b7 = \u03f52 for Algorithm 1 in terms of Corollary 1, and \u03b7 = \u03f5 for Algorithm 2 according to Corollary 2. Moreover, we use four devices in our experiments. The topology we used includes the ring graph and random graph. Here, the random graph is generated from an Erdos-Renyi random graph with the edge probability being 0.4. As for the baseline algorithm, we use the standard decentralized SGD (DSGD) [15] since there does not exist other decentralized multi-level compositional algorithms. In our experiments, the learning rate of DSGD is 0.1\nIn Figure 1, we report the support and query loss function values versus the number of iterations. It is easy to find that our two algorithms outperform the standard DSGD algorithm. The reason is that our algorithms leverage the variance-reduction approach to control the estimation error for each level function. Moreover, our second algorithm D-M-SCVRGD converges faster than the first algorithm D-M-SCGDM, which confirms our theoretical results.\nIn Figure 2, we show the loss function value on the support set and the accuracy on the query set. It can also be found that our two algorithms outperform the baseline algorithm and D-M-SCVRGD converges faster than D-M-SCGDM, which further confirms the correctness of our theoretical results."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we developed two novel decentralized stochastic multi-level compositional optimization algorithms. They both can achieve the level-independent convergence rate with practical operations. Extensive experimental results confirm the effectiveness of our algorithms. We believe our novel algorithmic design and theoretical analysis strategies can benefit the development of multi-level compositional optimization problems."
        }
    ],
    "title": "Stochastic Multi-Level Compositional Optimization Algorithms over Networks with Level-Independent Convergence Rate",
    "year": 2023
}