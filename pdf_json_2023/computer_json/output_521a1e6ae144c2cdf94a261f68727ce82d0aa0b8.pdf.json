{
    "abstractText": "Serverless computing is an emerging cloud computing paradigm, which allows software engineers to develop applications at the granularity of function (called serverless functions). However, multiple identical runs of the same serverless functions can show different performance (i.e., response latencies) due to the highly dynamic underlying environment where these functions are executed. We conduct the first work to study serverless function performance to raise awareness of this variance among researchers. We investigate 59 related research papers published in top-tier conferences, and observe that only 40.68% of them use multiple runs to quantify the variance of serverless function performance. Then we extract 65 serverless functions used in these papers and find that the performance of these serverless functions can differ by up to 338.76% (44.15% on average), indicating a large magnitude of the variance. Furthermore, we find that 61.54% of these functions can have unreliable performance results at the low number of repetitions that are widely adopted in the serverless computing literature.",
    "authors": [
        {
            "affiliations": [],
            "name": "JINFENG WEN"
        },
        {
            "affiliations": [],
            "name": "ZHENPENG CHEN"
        },
        {
            "affiliations": [],
            "name": "FEDERICA SARRO"
        },
        {
            "affiliations": [],
            "name": "XUANZHE LIU"
        }
    ],
    "id": "SP:79d65e10a8a384a60fb7886e2086934addd4da1d",
    "references": [
        {
            "authors": [
                "Gojko Adzic",
                "Robert Chatley"
            ],
            "title": "Serverless computing: economic and architectural impact",
            "venue": "In Proceedings of the 11th Joint Meeting on Foundations of Software Engineering",
            "year": 2017
        },
        {
            "authors": [
                "Istemi Ekin Akkus",
                "Ruichuan Chen",
                "Ivica Rimac",
                "Manuel Stein",
                "Klaus Satzke",
                "Andre Beck",
                "Paarijaat Aditya",
                "Volker Hilt"
            ],
            "title": "SAND: Towards high-performance serverless computing",
            "venue": "In Proceedings of the 2018 USENIX Annual Technical Conference",
            "year": 2018
        },
        {
            "authors": [
                "Lixiang Ao",
                "Liz Izhikevich",
                "Geoffrey M Voelker",
                "George Porter"
            ],
            "title": "Sprocket: A serverless video processing framework",
            "venue": "In Proceedings of the ACM Symposium on Cloud Computing",
            "year": 2018
        },
        {
            "authors": [
                "Kwabena Ebo Bennin",
                "Jacky Keung",
                "Akito Monden",
                "Passakorn Phannachitta",
                "Solomon Mensah"
            ],
            "title": "The significant effects of data sampling approaches on software defect prioritization and classification",
            "venue": "In Proceedings of the ACM/IEEE International Symposium on Empirical Software Engineering and Measurement",
            "year": 2017
        },
        {
            "authors": [
                "David Bermbach",
                "Jonathan Bader",
                "Jonathan Hasenburg",
                "Tobias Pfandzelter",
                "Lauritz Thamsen"
            ],
            "title": "AuctionWhisk: Using an auction-inspired approach for function placement in serverless fog platforms",
            "venue": "Software: Practice and Experience 52,",
            "year": 2022
        },
        {
            "authors": [
                "Per Nikolaj D Bukh"
            ],
            "title": "The art of computer systems performance analysis, techniques for experimental design, measurement, simulation and modeling",
            "year": 1992
        },
        {
            "authors": [
                "Joao Carreira",
                "Sumer Kohli",
                "Rodrigo Bruno",
                "Pedro Fonseca"
            ],
            "title": "From warm to hot starts: Leveraging runtimes for the serverless era",
            "venue": "In Proceedings of the Workshop on Hot Topics in Operating Systems",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Cohen"
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and psychological measurement 20,",
            "year": 1960
        },
        {
            "authors": [
                "Marcin Copik",
                "Grzegorz Kwasniewski",
                "Maciej Besta",
                "Michal Podstawski",
                "Torsten Hoefler"
            ],
            "title": "SeBS: A serverless benchmark suite for Function-as-a-Service computing",
            "venue": "In Proceedings of the 22nd International Middleware Conference",
            "year": 2021
        },
        {
            "authors": [
                "David A Dickey",
                "Wayne A Fuller"
            ],
            "title": "Distribution of the estimators for autoregressive time series with a unit root",
            "venue": "Journal of the American statistical association 74,",
            "year": 1979
        },
        {
            "authors": [
                "Simon Eismann",
                "Long Bui",
                "Johannes Grohmann",
                "Cristina Abad",
                "Nikolas Herbst",
                "Samuel Kounev"
            ],
            "title": "Sizeless: Predicting the optimal size of serverless functions",
            "venue": "In Proceedings of the 22nd International Middleware Conference",
            "year": 2021
        },
        {
            "authors": [
                "Simon Eismann",
                "Diego Elias Costa",
                "Lizhi Liao",
                "Cor-Paul Bezemer",
                "Weiyi Shang",
                "Andr\u00e9 van Hoorn",
                "Samuel Kounev"
            ],
            "title": "A case study on the stability of performance tests for serverless applications",
            "venue": "Journal of Systems and Software",
            "year": 2022
        },
        {
            "authors": [
                "Simon Eismann",
                "Joel Scheuner",
                "Erwin Van Eyk",
                "Maximilian Schwinger",
                "Johannes Grohmann",
                "Nikolas Herbst",
                "Cristina Abad",
                "Alexandru Iosup"
            ],
            "title": "The state of serverless applications: collection, characterization, and community consensus",
            "venue": "IEEE Transactions on Software Engineering 48,",
            "year": 2021
        },
        {
            "authors": [
                "Nafise Eskandani",
                "Guido Salvaneschi"
            ],
            "title": "The Wonderless dataset for serverless computing",
            "venue": "In Proceedings of the 2021 IEEE/ACM 18th International Conference on Mining Software Repositories",
            "year": 2021
        },
        {
            "authors": [
                "Sadjad Fouladi",
                "Riad S Wahby",
                "Brennan Shacklett",
                "William Zeng",
                "Rahul Bhalerao",
                "Anirudh Sivaraman",
                "George Porter",
                "Keith Winstein"
            ],
            "title": "Encoding, fast and slow: low-Latency video processing using thousands of tiny threads",
            "venue": "In Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Fuerst",
                "Prateek Sharma"
            ],
            "title": "Locality-aware load-balancing for serverless clusters",
            "venue": "In Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing",
            "year": 2022
        },
        {
            "authors": [
                "Andy Georges",
                "Dries Buytaert",
                "Lieven Eeckhout"
            ],
            "title": "Statistically rigorous java performance evaluation",
            "venue": "ACM SIGPLAN Notices 42,",
            "year": 2007
        },
        {
            "authors": [
                "Giovani Guizzo",
                "Federica Sarro",
                "Mark Harman"
            ],
            "title": "Cost measures matter for mutation testing study validity",
            "venue": "In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
            "year": 2020
        },
        {
            "authors": [
                "Hassan B Hassan",
                "Saman A Barakat",
                "Qusay I Sarhan"
            ],
            "title": "Survey on serverless computing",
            "venue": "Journal of Cloud Computing",
            "year": 2021
        },
        {
            "authors": [
                "Sen He",
                "Glenna Manns",
                "John Saunders",
                "Wei Wang",
                "Lori Pollock",
                "Mary Lou Soffa"
            ],
            "title": "A statistics-based performance testing methodology for cloud applications",
            "venue": "In Proceedings of the 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
            "year": 2019
        },
        {
            "authors": [
                "Zhipeng Jia",
                "Emmett Witchel"
            ],
            "title": "Nightcore: efficient and scalable serverless computing for latency-sensitive, interactive microservices",
            "venue": "In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
            "year": 2021
        },
        {
            "authors": [
                "Jiawei Jiang",
                "Shaoduo Gan",
                "Yue Liu",
                "Fanlin Wang",
                "Gustavo Alonso",
                "Ana Klimovic",
                "Ankit Singla",
                "Wentao Wu",
                "Ce Zhang"
            ],
            "title": "Towards demystifying serverless machine learning training",
            "venue": "In Proceedings of the 2021 International Conference on Management of Data",
            "year": 2021
        },
        {
            "authors": [
                "Eric Jonas",
                "Johann Schleier-Smith",
                "Vikram Sreekanti",
                "Chia-Che Tsai",
                "Anurag Khandelwal",
                "Qifan Pu",
                "Vaishaal Shankar",
                "Joao Carreira",
                "Karl Krauth",
                "Neeraja Yadwadkar",
                "Joseph E. Gonzalez",
                "Raluca Ada Popa",
                "Ion Stoica",
                "David A. Patterson"
            ],
            "title": "Cloud programming simplified: A Berkeley view on serverless computing",
            "venue": "arXiv preprint arXiv:1902.03383",
            "year": 2019
        },
        {
            "authors": [
                "Jeongchul Kim",
                "Kyungyong Lee"
            ],
            "title": "Functionbench: A suite of workloads for serverless cloud function service",
            "venue": "In Proceedings of the IEEE 12th International Conference on Cloud Computing",
            "year": 2019
        },
        {
            "authors": [
                "Ana Klimovic",
                "Yawen Wang",
                "Patrick Stuedi",
                "Animesh Trivedi",
                "Jonas Pfefferle",
                "Christos Kozyrakis"
            ],
            "title": "Pocket: Elastic ephemeral storage for serverless analytics",
            "venue": "In Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation",
            "year": 2018
        },
        {
            "authors": [
                "J Richard Landis",
                "Gary G Koch"
            ],
            "title": "The measurement of observer agreement for categorical data",
            "venue": "Biometrics 33,",
            "year": 1977
        },
        {
            "authors": [
                "Jean-Yves Le Boudec"
            ],
            "title": "Performance evaluation of computer and communication systems",
            "venue": "Epfl Press Lausanne",
            "year": 2010
        },
        {
            "authors": [
                "Valentina Lenarduzzi",
                "Annibale Panichella"
            ],
            "title": "Serverless testing: Tool vendors\u2019 and experts\u2019 points of view",
            "venue": "IEEE Software 38,",
            "year": 2020
        },
        {
            "authors": [
                "Yongkang Li",
                "Yanying Lin",
                "Yang Wang",
                "Kejiang Ye",
                "Cheng-Zhong Xu"
            ],
            "title": "Serverless computing: state-of-the-art, challenges and opportunities",
            "venue": "IEEE Transactions on Services Computing",
            "year": 2022
        },
        {
            "authors": [
                "Zijun Li",
                "Yushi Liu",
                "Linsong Guo",
                "Quan Chen",
                "Jiagan Cheng",
                "Wenli Zheng",
                "Minyi Guo"
            ],
            "title": "FaaSFlow: enable efficient workflow execution for function-as-a-service",
            "venue": "In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
            "year": 2022
        },
        {
            "authors": [
                "Changyuan Lin",
                "Hamzeh Khazaei"
            ],
            "title": "Modeling and optimization of performance and cost of serverless applications",
            "venue": "IEEE Transactions on Parallel and Distributed Systems 32,",
            "year": 2020
        },
        {
            "authors": [
                "Xuanzhe Liu",
                "Jinfeng Wen",
                "Zhenpeng Chen",
                "Ding Li",
                "Junkai Chen",
                "Yi Liu",
                "Haoyu Wang",
                "Xin Jin"
            ],
            "title": "FaaSLight: general application-Level cold-start latency optimization for Function-as-a-Service in serverless computing",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023
        },
        {
            "authors": [
                "Ashraf Mahgoub",
                "Edgardo Barsallo Yi",
                "Karthick Shankar",
                "Sameh Elnikety",
                "Somali Chaterji",
                "Saurabh Bagchi"
            ],
            "title": "ORION and the three rights: Sizing, bundling, and prewarming for serverless DAGs",
            "venue": "In Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation",
            "year": 2022
        },
        {
            "authors": [
                "Pascal Maissen",
                "Pascal Felber",
                "Peter Kropf",
                "Valerio Schiavoni"
            ],
            "title": "FaaSdom: A benchmark suite for serverless computing",
            "venue": "In Proceedings of the 14th ACM International Conference on Distributed and Event-based Systems",
            "year": 2020
        },
        {
            "authors": [
                "Aleksander Maricq",
                "Dmitry Duplyakin",
                "Ivo Jimenez",
                "Carlos Maltzahn",
                "Ryan Stutsman",
                "Robert Ricci"
            ],
            "title": "Taming performance variability",
            "venue": "In Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation",
            "year": 2018
        },
        {
            "authors": [
                "Ingo M\u00fcller",
                "Renato Marroqu\u00edn",
                "Gustavo Alonso"
            ],
            "title": "Lambada: Interactive data analytics on cold data using serverless cloud infrastructure",
            "venue": "In Proceedings of the 2020 International Conference on Management of Data",
            "year": 2020
        },
        {
            "authors": [
                "Alessandro Vittorio Papadopoulos",
                "Laurens Versluis",
                "Andr\u00e9 Bauer",
                "Nikolas Herbst",
                "J\u00f3akim Von Kistowski",
                "Ahmed Ali-Eldin",
                "Cristina L Abad",
                "Jos\u00e9 Nelson Amaral",
                "Petr Tuma",
                "Alexandru Iosup"
            ],
            "title": "Methodological principles for reproducible performance evaluation in cloud computing",
            "venue": "IEEE Transactions on Software Engineering 47,",
            "year": 2019
        },
        {
            "authors": [
                "Liam Patterson",
                "David Pigorovsky",
                "Brian Dempsey",
                "Nikita Lazarev",
                "Aditya Shah",
                "Clara Steinhoff",
                "Ariana Bruno",
                "Justin Hu",
                "Christina Delimitrou"
            ],
            "title": "HiveMind: a hardware-software system stack for serverless edge swarms",
            "venue": "In Proceedings of the 49th Annual International Symposium on Computer Architecture",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Perron",
                "Raul Castro Fernandez",
                "David DeWitt",
                "Samuel Madden"
            ],
            "title": "Starling: A scalable query engine on cloud functions",
            "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data",
            "year": 2020
        },
        {
            "authors": [
                "Hung Viet Pham",
                "Shangshu Qian",
                "Jiannan Wang",
                "Thibaud Lutellier",
                "Jonathan Rosenthal",
                "Lin Tan",
                "Yaoliang Yu",
                "Nachiappan Nagappan"
            ],
            "title": "Problems and opportunities in training deep learning software systems: an analysis of variance",
            "venue": "In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering",
            "year": 2020
        },
        {
            "authors": [
                "Sasko Ristov",
                "Stefan Pedratscher",
                "JakobWallnoefer",
                "Thomas Fahringer"
            ],
            "title": "Daf: Dependency-aware faasifier for node.js monolithic applications",
            "venue": "IEEE Software 38,",
            "year": 2020
        },
        {
            "authors": [
                "Joel Scheuner",
                "Philipp Leitner"
            ],
            "title": "Function-as-a-service performance evaluation: A multivocal literature review",
            "venue": "Journal of Systems and Software",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Shahrad",
                "Jonathan Balkind",
                "David Wentzlaff"
            ],
            "title": "Architectural implications of function-as-a-service computing",
            "venue": "In Proceedings of the 52nd annual IEEE/ACM international symposium on microarchitecture",
            "year": 2019
        },
        {
            "authors": [
                "Mohammad Shahrad",
                "Rodrigo Fonseca",
                "\u00cd\u00f1igo Goiri",
                "Gohar Chaudhry",
                "Paul Batum",
                "Jason Cooke",
                "Eduardo Laureano",
                "Colby Tresness",
                "Mark Russinovich",
                "Ricardo Bianchini"
            ],
            "title": "Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider",
            "venue": "In Proceedings of the 2020 USENIX Annual Technical Conference",
            "year": 2020
        },
        {
            "authors": [
                "Vaishaal Shankar",
                "Karl Krauth",
                "Kailas Vodrahalli",
                "Qifan Pu",
                "Benjamin Recht",
                "Ion Stoica",
                "Jonathan Ragan-Kelley",
                "Eric Jonas",
                "Shivaram Venkataraman"
            ],
            "title": "Serverless linear algebra",
            "venue": "In Proceedings of the 11th ACM Symposium on Cloud Computing",
            "year": 2020
        },
        {
            "authors": [
                "Samuel Sanford Shapiro",
                "Martin B Wilk"
            ],
            "title": "An analysis of variance test for normality (complete samples)",
            "venue": "Biometrika 52,",
            "year": 1965
        },
        {
            "authors": [
                "Arjun Singhvi",
                "Arjun Balasubramanian",
                "Kevin Houck",
                "Mohammed Danish Shaikh",
                "Shivaram Venkataraman",
                "Aditya Akella"
            ],
            "title": "Atoll: A scalable low-latency serverless platform",
            "venue": "In Proceedings of the ACM Symposium on Cloud Computing",
            "year": 2021
        },
        {
            "authors": [
                "Davide Taibi",
                "Josef Spillner",
                "Konrad Wawruch"
            ],
            "title": "Serverless computing-where are we now, and where are we heading",
            "venue": "IEEE Software 38,",
            "year": 2020
        },
        {
            "authors": [
                "Alexandru Uta",
                "Alexandru Custura",
                "Dmitry Duplyakin",
                "Ivo Jimenez",
                "Jan Rellermeyer",
                "Carlos Maltzahn",
                "Robert Ricci",
                "Alexandru Iosup"
            ],
            "title": "Is big data performance reproducible in modern cloud networks",
            "venue": "In Proceedings of the 17th USENIX symposium on networked systems design and implementation",
            "year": 2020
        },
        {
            "authors": [
                "Ao Wang",
                "Shuai Chang",
                "Huangshi Tian",
                "Hongqi Wang",
                "Haoran Yang",
                "Huiba Li",
                "Rui Du",
                "Yue Cheng"
            ],
            "title": "FaaSNet: Scalable and fast provisioning of custom serverless container runtimes at Alibaba cloud function compute",
            "venue": "In Proceedings of the 2021 USENIX Annual Technical Conference",
            "year": 2021
        },
        {
            "authors": [
                "Liang Wang",
                "Mengyuan Li",
                "Yinqian Zhang",
                "Thomas Ristenpart",
                "Michael Swift"
            ],
            "title": "Peeking behind the curtains of serverless platforms",
            "venue": "In Proceedings of the 2018 USENIX Annual Technical Conference",
            "year": 2018
        },
        {
            "authors": [
                "Jinfeng Wen",
                "Zhenpeng Chen",
                "Xin Jin",
                "Xuanzhe Liu"
            ],
            "title": "Rise of the planet of serverless computing: a systematic review",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023
        },
        {
            "authors": [
                "Jinfeng Wen",
                "Zhenpeng Chen",
                "Yi Liu",
                "Yiling Lou",
                "Yun Ma",
                "Gang Huang",
                "Xin Jin",
                "Xuanzhe Liu"
            ],
            "title": "An empirical study on challenges of application development in serverless computing",
            "venue": "In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
            "year": 2021
        },
        {
            "authors": [
                "Jinfeng Wen",
                "Yi Liu",
                "Zhenpeng Chen",
                "Junkai Chen",
                "Yun Ma"
            ],
            "title": "Characterizing commodity serverless computing platforms",
            "venue": "Journal of Software: Evolution and Process (2021),",
            "year": 2021
        },
        {
            "authors": [
                "Yuncheng Wu",
                "Tien Tuan Anh Dinh",
                "Guoyu Hu",
                "Meihui Zhang",
                "Yeow Meng Chee",
                "Beng Chin Ooi"
            ],
            "title": "Serverless data science - are we there yet? a case study of model serving",
            "venue": "In Proceedings of the 2022 International Conference on Management of Data",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Yu",
                "Qingyuan Liu",
                "Dong Du",
                "Yubin Xia",
                "Binyu Zang",
                "Ziqian Lu",
                "Pingchao Yang",
                "Chenggang Qin",
                "Haibo Chen"
            ],
            "title": "Characterizing serverless platforms with serverlessbench",
            "venue": "In Proceedings of the 2020 ACM Symposium on Cloud Computing",
            "year": 2020
        },
        {
            "authors": [
                "Hong Zhang",
                "Yupeng Tang",
                "Anurag Khandelwal",
                "Jingrong Chen",
                "Ion Stoica"
            ],
            "title": "Caerus: NIMBLE task scheduling for serverless analytics",
            "venue": "In Proceedings of the 8th USENIX Symposium on Networked Systems Design and Implementation",
            "year": 2021
        },
        {
            "authors": [
                "Laiping Zhao",
                "Yanan Yang",
                "Yiming Li",
                "Xian Zhou",
                "Keqiu Li"
            ],
            "title": "Understanding, predicting and scheduling serverless workloads under partial interference",
            "venue": "In Proceedings of the International Conference for High Performance Computing,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Revisiting the Performance of Serverless Computing: An Analysis of Variance\nJINFENG WEN, Peking University, China\nZHENPENG CHEN, University College London, United Kingdom\nFEDERICA SARRO, University College London, United Kingdom\nXUANZHE LIU, Peking University, China\nServerless computing is an emerging cloud computing paradigm, which allows software engineers to develop applications at the granularity of function (called serverless functions). However, multiple identical runs of the same serverless functions can show different performance (i.e., response latencies) due to the highly dynamic underlying environment where these functions are executed. We conduct the first work to study serverless function performance to raise awareness of this variance among researchers. We investigate 59 related research papers published in top-tier conferences, and observe that only 40.68% of them use multiple runs to quantify the variance of serverless function performance. Then we extract 65 serverless functions used in these papers and find that the performance of these serverless functions can differ by up to 338.76% (44.15% on average), indicating a large magnitude of the variance. Furthermore, we find that 61.54% of these functions can have unreliable performance results at the low number of repetitions that are widely adopted in the serverless computing literature.\nAdditional Key Words and Phrases: serverless computing, variance of serverless function performance"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Serverless computing is an emerging cloud computing paradigm that frees software engineers from tedious and errorprone infrastructure management. It is predicted that serverless computing can be used in 50% of the global enterprises by 2025 [7]. Moreover, its market size will increase from $3 billion in 2017 to nearly $22 billion by 2025 [1]. To date, serverless computing has been adopted in a wide range of software applications [36, 44, 59, 68]. It allows software engineers to focus on the application logic and implement it as a series of event-driven and stateless functions (called serverless functions). To support the execution of serverless functions, major cloud providers have rolled out serverless platforms, such as AWS Lambda [5] and Google Cloud Functions [13].\nThe emergence of serverless computing poses new challenges to the development and deployment of software applications, and thus attracts increasing attention from various research communities, such as Software Engineering [55, 75], Operating Systems [56, 73], Services Computing [52], and Computer Networks [36]. For example, the SE research community has studied a broad range of topics about serverless computing, including its development characteristics [34], programming frameworks [26], application migration [64], economic impact [22], testing/debugging [51], and performance optimization [55]. Among these topics, performance is demonstrated to be the most widely studied [52, 65, 75].\nHowever, it is challenging to obtain accurate and reliable performance results of serverless computing. Specifically, multiple identical runs of the same serverless functions can show different performance results (i.e., response latencies) due to the following reasons: (1) The underlying infrastructure of serverless platforms is highly dynamic. Serverless functions need to be executed on serverless platforms. As a kind of cloud environment, serverless platforms are susceptible to multi-tenancy, resource management, and networking [56, 60]. In addition, they are black-box to the software developers and offer obscure instance scheduling strategies and unpredictable invocation patterns. These\nAuthors\u2019 addresses: Jinfeng Wen, Peking University, Beijing, China, jinfeng.wen@stu.pku.edu.cn; Zhenpeng Chen, University College London, London, United Kingdom, zp.chen@ucl.ac.uk; Federica Sarro, University College London, London, United Kingdom, f.sarro@ucl.ac.uk; Xuanzhe Liu, Peking University, Beijing, China, liuxuanzhe@pku.edu.cn. Manuscript submitted to ACM\nar X\niv :2\n30 5.\n04 30\n9v 1\n[ cs\n.S E\n] 7\nM ay\n2 02\n3\ncharacteristics make the execution environment of serverless functions highly variable and dynamic [37, 61, 62]. (2) Serverless functions are deployed at high density due to the lightweight feature. Serverless functions generally require a small memory size [67, 70]. They are thus deployed and hosted in small function instances, causing a high-density deployment environment to increase the risk of disruption [61, 81]. (3) Serverless functions are often executed concurrently. Concurrency increases the probability of resource contention and cold starts of serverless functions, introducing additional uncertainty [56, 66, 74]. The three factors result in the potential variance of serverless function performance. Nevertheless, to the best of our knowledge, this variance has not been systematically studied in the literature.\nTo fill this knowledge gap, we conduct the first empirical study to investigate the awareness of the variance of serverless function performance among researchers and characterize the magnitude of this variance. To this end, we first collect and analyze 59 related research papers from top-tier conferences (RQ1). We find that only 40.68% of the papers use multiple runs to quantify the variance of serverless function performance. Moreover, none of the papers justify the reason for the number of repetitions that they choose.\nMotivated by this observation, we select 65 serverless functions used in these papers and analyze their response latencies in multiple runs to reveal the magnitude of the performance variance (RQ2). We find that serverless function performance can differ by up to 338.76% (44.15% on average) across different runs. Furthermore, we use the coefficient of variance [40, 58, 72, 81], which is a widely-adopted metric to measure the degree of variability, to evaluate the performance results of multiple runs for each serverless function that we study. The results show that the coefficient of variance can be up to 42.81%, indicating a large extent of the variance [58, 72, 81].\nGiven such a significant variance, we further explore the distribution characteristics of serverless function performance (RQ3). We find that the performance of more than 90% of the serverless functions follows a non-normal distribution. The results suggest the use of non-parametric statistical analysis, which does not assume data normality, to analyze the performance results of serverless functions in future research.\nFinally, we evaluate the reliability of the serverless function performance obtained at the different numbers of repetitions commonly used in the serverless computing literature (RQ4). We find that 61.54% of the serverless functions that we study show unreliable performance in this case. Moreover, 98.46% of the functions require being executed at least 50 times to achieve reliable performance, but only 37.50% of the collected papers that report experiment repetitions execute serverless functions more than 50 times.\nBased on our findings, we have identified several valuable implications and actionable suggestions for software practices and future research on serverless computing. Additionally, we have included suggestions for future work in this area. To enable others to replicate and extend our study, we have publicly released the data and scripts used in our research [21]. In addition, we have curated and made publicly available a benchmark dataset for future performance testing and optimization of serverless computing."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "This section introduces the background knowledge of serverless computing and the performance of serverless functions."
        },
        {
            "heading": "2.1 Serverless computing",
            "text": "Serverless computing is a popular cloud computing paradigm, which allows software developers to focus on their application logic without having to manage complex and error-prone underlying tasks. Function-as-a-Service (FaaS) is its most prominent implementation [23, 30, 45, 71, 76]. In the FaaS fashion, software engineers develop applications as\nManuscript submitted to ACM\na series of event-driven and stateless dedicated functions, called serverless functions. Then they deploy and execute serverless functions on serverless platforms, such as AWS Lambda [5] and Google Cloud Functions [13].\nFig. 1 illustrates the process of developing, deploying, and executing serverless functions. 1\u25cb First, software developers implement serverless functions using programming interfaces as event-driven functions [75]. These functions are generally written in high-level programming languages, e.g., Python and JavaScript [3, 16, 35]. 2\u25cbMeanwhile, developers define the rules to bind their serverless functions and the related events, such as HTTP requests and data changes of cloud storage. 3\u25cb Then, serverless functions and required dependent libraries are packaged together and deployed to serverless platforms. In this stage, developers can specify the required configurations, including language runtime, memory size, and function timeout [75]. 4\u25cb When the serverless functions are triggered by pre-defined events, the serverless platforms automatically launch or reuse the required function instances (e.g., VMs or containers) with restricted resources (e.g., CPU and memory) to process these requests. This frees developers from complex server management and makes them enjoy seamless resource elasticity. 5\u25cb After the requests are completed, developers pay for the number of requests and the actually allocated or consumed resources [54, 75]."
        },
        {
            "heading": "2.2 Performance of serverless functions",
            "text": "The serverless computing literature has widely studied the prediction and optimization of serverless function performance [23, 32, 43, 52, 54, 65, 73, 75]. More details are described in Section 9. In our study, the serverless function performance that we focus on is the end-to-end response latency (abbreviated as e2eTime), defined as the time period from request sending to request completion [54, 55, 65, 77]. It is the most widely-adopted metric for serverless function performance, and the serverless computing community primarily proposes new techniques to optimize this metric [37, 56, 73, 81]. In addition, it is a common metric used by practitioners as the reflection of user experience satisfaction [34, 76, 77].\nServerless function performance includes cold-start performance and warm-start performance. Specifically, when the serverless platform launches new function instances to serve requests, the serverless function experience cold-start invocations, which produce the end-to-end response latency containing the instance preparation process. This latency represents the cold-start performance. Subsequent invocations of the same function can reuse the launched function instances within a short keep-alive time (e.g., 7 minutes for AWS Lambda [2]). This way makes the serverless function experience warm-start invocations. If there are no requests, the platform turns the launched instances into an idle state and automatically releases resources. Generally, due to no instance preparation process in warm starts, the warm-start performance is smaller than the cold-start performance for the same serverless function.\nManuscript submitted to ACM"
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Fig. 2 shows an overview of our methodology. To understand the variance of serverless function performance, we first identify relevant research papers and collect serverless functions from them. Based on them, we answer four research questions (RQs) related to the variance of serverless function performance.\nRQ1: To what extent the current literature has tackled the variance of serverless function performance? This RQ investigates whether and how researchers take the variance of serverless function performance into account when studying serverless computing.\nRQ2: How variable is the performance of serverless functions? This RQ aims to unveil the magnitude of the variance of serverless function performance.\nRQ3: What are the distribution characteristics of serverless function performance? We further analyze the distribution characteristics of the performance results. The characteristics can help researchers and engineers to select suitable statistical techniques to report a reliable overall result based on the variable serverless function performance of multiple runs.\nRQ4: How reliable is the serverless function performance at different repetitions? This RQ aims to explore the reliability of serverless function performance obtained at different repetitions. Moreover, we aim to provide implications on how to determine the appropriate number of repetitions, so as to obtain a more reliable performance.\nTo address RQ1, we collect research papers related to serverless computing. Because studies on serverless computing have been published in academic venues across various research communities [36, 52, 55, 56, 73, 75], we gather papers published in top-tier conferences from different communities recommended by Computer Science Rankings (CSRankings) [8]. We focus on top-tier conference papers because we believe that they are representative of the current state of research in our field and offer a broad view of the most recent findings. However, we acknowledge that including papers from other venues (i.e., journals or other conferences) in our study would provide a more comprehensive view of the research landscape. In future work, we plan to expand our study to include more papers to further enhance the scope of our research.\nThe process for collecting research papers is as follows. First, we collect all the papers published in these conferences between 2014 (the year serverless computing was popularized [45, 75, 76]) and the date we collect the papers (October 12, 2022). Then, the first two authors select the research papers related to serverless computing by independently reading the title and abstract of these papers. If the authors disagree on the relevance of a paper, an arbitrator, who has ten years of cloud computing experience, is involved in discussing and helping reach an agreement. To measure the inter-rater agreement level of the first two authors during the independent labeling, we use Cohen\u2019s Kappa (\ud835\udf05) [29], which is the most widely-used agreement evaluation metric [72, 75, 76]. The value of \ud835\udf05 is 0.946, indicating an almost perfect agreement [49] and a reliable labeling procedure. Finally, we obtain a total of 68 research papers related to serverless computing. The number of papers per year is shown in Fig. 3. We observe an overall increasing trend. The Manuscript submitted to ACM\nnumber of relevant papers increased from 1 paper in 2017 to 22 papers in 2022. This implies that serverless computing is gaining increasing attention from the research community, demonstrating the timeliness and urgency of our study.\nFurthermore, the first two authors jointly filter research papers that do not report the performance results of serverless functions. Finally, 59 out of the 68 research papers (86.76%) are retained for our final analysis. This is consistent with the findings reported by previous work [52, 65, 75] that most serverless computing papers are related to serverless function performance."
        },
        {
            "heading": "3.1 Collection and analysis of serverless functions",
            "text": "To answer RQ2 to RQ4, we collect serverless functions used in the 59 research papers. We focus on serverless functions executed on AWS Lambda and Google Cloud Functions, which are the most widely-adopted public serverless platforms [16, 34, 41]. Moreover, we select serverless functions that are open-sourced and have documentation to assist with the execution. In addition, we focus on serverless functions written in Python and JavaScript, which are the two most widely-used programming languages for serverless functions [3, 16, 34, 35, 65]. According to these criteria, we collect 65 serverless functions covering a wide range of tasks, such as Web requests, video processing, scientific computing, machine learning, and natural language processing. Moreover, most serverless functions are included in different common benchmarks from the serverless computing community, such as FunctionBench [46], ServerlessBench [79], AWS Sample [6], SeBS [30], and FaaSDom [57]. Table 1 shows the distributions of the executed serverless platforms and programming languages for the collected 65 serverless functions. 60 serverless functions are executed on AWS Lambda, while 5 functions are executed on Google Cloud Functions. It can be because the advent of AWS Lambda was the main reason for the popularity of serverless computing [45, 75, 76]. Moreover, 53 serverless functions are written in Python, while 12 functions are written in JavaScript. Such a distribution can be attributed to the increasing number of tasks that use popular Python third-party libraries, e.g., Pandas and Scikit Learn.\nWe then analyze the performance of the 65 serverless functions. We execute these functions with the configurations and the serverless platform used in their original papers. If no specific configurations are given, serverless functions\nManuscript submitted to ACM\nare executed using the default configurations of platforms. At the time of our study, the default memory size of AWS Lambda is 128 MB [9], and its function timeout time is 3 seconds [11]. For Google Cloud Functions, the default memory size and function timeout time are 256 MB [10] and 60 seconds [12], respectively. Once insufficient memory size or function timeout is detected during function executions, we keep executing the serverless functions by increasing the memory size or function timeout time until they can be executed successfully. We execute each serverless function for \ud835\udc5b rounds. In each round, we execute the function twice using the same input and configurations to obtain the cold-start and warm-start performance, respectively. We set each round to half an hour, because we find that this duration allows the serverless function to be executed twice, and the resources used for execution will be released to ensure that the start of the next round is still a cold start. In addition, in each round, the second execution starts five seconds after the completion of the first execution, because we find that this duration can ensure the successful warm start of the serverless function. Finally, we obtain 65 \u00d7 (\ud835\udc5b + \ud835\udc5b) = 130\ud835\udc5b data points (65\ud835\udc5b for cold start and 65\ud835\udc5b for warm start) about the response latency of serverless functions.\nWe denote the 65 serverless functions as Func1, Func2, ..., and Func65. The number mapping information about 65 serverless functions has been provided in our repository [21]. We also provide raw performance data generated by each serverless function."
        },
        {
            "heading": "4 RQ1: CURRENT LITERATURE",
            "text": "In RQ1, we investigate how the variance of serverless function performance has been tackled in the serverless computing literature. To this end, we perform a literature review based on the selected 59 research papers. According to previous work about performance reproducibility [72], there are already standard reporting criteria being used for summarizing the paper\u2019s experimental design. We apply these criteria to analyze our selected papers. Specifically, we unveil the following information:\n\u2022 1: The mean or median performance values; \u2022 2: Confidence (e.g., confidence intervals) or variability (e.g., standard deviation, coefficient of variance, or percentiles); \u2022 3: The number that an experiment was repeated.\nSuch information can assist us in further understanding the reliability of the provided evidence and the study\u2019s conclusions. First, the first two authors separately read the full text of the paper to find out if the paper contains the aforementioned information. Note that a research paper may involve multiple pieces of information. In addition, if a paper uses the experiment repetition (i.e., information 3), the first two authors further provide the specific number of repetitions. Second, we calculate the inter-rater agreement during the independent labeling for each type of information. Values of Cohen\u2019s Kappa (\ud835\udf05) [29] are 0.914, 0.897, and 0.865, respectively, meaning a perfect agreement and reliable labeling process [49]. Finally, encountered conflicts are discussed to reach an agreement by the first two authors and the third arbitrator.\nResults:We observe that researchers lack awareness of the variance of serverless function performance. Only 40.68% of the papers use multiple runs when conducting experiments or analyses. Specifically, Fig. 4 shows the results of our literature review. 45.76% (27 out of 59) of the papers report means or medians. Only 28.81% (17 out of 59) of the papers report statistical variation data (e.g., percentiles). In addition, 40.68% (24 out of 59) of the papers report the number of repetitions they used. In other words, nearly 60% of the papers do not report how many times they repeated the experiments, indicating that either no repetition is performed at all, or a specific number of repetitions is used but this Manuscript submitted to ACM\ninformation is omitted in the paper. These results indicate that research papers related to serverless computing lack an adequate description of results and awareness of the variance of serverless function performance.\nWe further analyze these papers that do report experiment repetitions. For these papers, we find that only 16.67% (accounting for 6.78% of all surveyed papers) mention that the use of repeated experiments is to alleviate the randomness of results or the effect of measurement errors. Fig. 5 shows the number of repetitions used in these papers. Note that a paper may use multiple numbers of repetitions. The results show that there are 11 kinds of numbers of repetitions. However, none of these papers justify or explain the reason for the number of repetitions chosen. In addition, we observe that 75% of these papers use no more than 50 repetitions, while 37.50% of them use more than 50 repetitions. The top 3 frequently used repetitions are 10, 50, and 1,000, which account for 20.83%, 16.67%, and 16.67% of these papers, respectively. However, if the number of repetitions is too low (e.g., 10 repetitions), the reliability of the obtained performance of serverless functions may not be guaranteed (as explained in Section 7), and it could lead to wrong or ambiguous conclusions. If the number of repetitions is too large (e.g., 1,000 repetitions), it will cause a huge runtime overhead, e.g., long experimentation time and high costs. Researchers pay for the number of requests and the actually allocated/consumed resources of serverless functions [54, 75]. Thus, a too-small or too-large number of repetitions may be inappropriate in serverless-related experiments. These results imply that the awareness of the variance of serverless function performance is still in the infancy stage.\nFinding 1: Among the 59 surveyed papers, 45.76% report the mean or median of serverless function performance, while 28.81% report statistical variability data (e.g., percentiles and confidence intervals). Only 40.68% of the papers use multiple runs to quantify the variance of serverless function performance. However, none of the papers justify the reason for the number of repetitions that they choose. 75% of the papers that report experiment repetitions use no more than 50 repetitions. Moreover, 10 repetitions are the most frequently used. Overall, these results indicate that the awareness of the variance of serverless function performance is still in the infancy stage."
        },
        {
            "heading": "5 RQ2: VARIANCE MEASUREMENT",
            "text": "To explore the magnitude of the variance of serverless function performance, we analyze the response latency of 65 serverless functions over 50 runs from two aspects: coefficient of variance (CV) and boxplot, which are commonly used in performance analysis or variance analysis [40, 58, 63, 72, 81]. Specifically, CV is known as the dispersion coefficient, and it is a statistical indicator that measures the degree of data variability. CV represents the ratio of the standard deviation to the mean. A big value of CV means a large degree of performance variance. The boxplot can provide a\nManuscript submitted to ACM\nvisual representation of the dispersion degree for the performance results generated by each serverless function. Our study does not use the standard deviation because compared measurements have different scales and units. We use the performance results of each serverless function with 50 repetitions to explore the magnitude of the variance. This is due to the fact that 75% of the papers reporting on the number of repeated runs executed no more than 50 repetitions. We adopt this upper value to assume that most papers use better performance results than the original setup. We investigate whether the assumed better performance results suffer from a large magnitude of performance variance. Next, we discuss the results analyzed in terms of CV and boxplots.\nResults: (1) CV: We analyze CV values calculated from performance results generated by each serverless function under cold and warm starts. Fig. 6 shows these CV values, where cold-start CV values are ordered in ascending order.\nFirst, we observe that serverless function performance has different degrees of variance under cold and warm starts. Specifically, cold-start CV values range from 1.23% to 31.18%, while warm-start CV values range from 0.86% to 42.81%, differing by as much as 49 times and thus showing a wide range of variance. On average, the cold-start CV value is 6.21%, while the warm-start CV value is 8.73%. The median of cold-start CV values is 5.26%, while that of warm-start CV values is 6.70%. These results indicate that serverless functions executed in cold and warm starts produce different degrees of variance in performance. However, providing only these CV values is not a straightforward way to understand how the performance of serverless functions actually fluctuates. Thus, we give examples of the raw performance results of the serverless function. Fig. 7 shows 50 data points of performance obtained in cold starts for two serverless functions, which are numbered Func22 and Func2. We observe that multiple runs of the same serverless function produce different and fluctuating performance results. The CV values of Func22 and Func2 are 2.04% and 5.88%, respectively. The difference between the maximum and minimum values of Func22 achieves 6.76 seconds, and that of Func2 is 7.24 seconds. General, serverless functions execute short-lived and milliseconds-level tasks [48, 70]. Thus, second-level performance variance is severe for serverless functions. Moreover, from the right violin plot of Fig. 7, the distributions of data points of performance for different serverless functions are drastically different, being clustered in different positions. The data points of Func22 are distributed near the middle of its violin plot, while those of Func2 are distributed towards the bottom. Overall, we observe a significant variance in serverless function performance from two examples, where CV values are 2.04% and 5.88%, respectively. Therefore, we infer that other serverless functions with larger CV values than those shown in the examples will produce more significant fluctuations in performance.\nWe further check the serverless functions with high CV values, e.g., Func30 with warm-start CV value = 42.81%. The maximum and minimum values of performance results of Func30 differ by 338.76%, where the maximum value Manuscript submitted to ACM\nreaches 4.39 times its minimum value. The possible reason is that Func30 needs to transfer the file from the cloud storage (e.g., AWS S3 [4]) to the function instance through the network. However, the network may be unstable, and its fluctuation is unpredictable. We check all serverless functions, and the corresponding maximum and minimum values of the performance can differ by a mean of 44.15% and a median of 32.19%.\nSecond, we observe that the variance of serverless function performance is slightly more severe under warm starts than under cold starts. Specifically, the warm-start CV values for 67.69% (44/65) of the serverless functions are greater than those of the corresponding cold starts. We also calculate the number of the functions whose CV values are greater than 10%, which generally indicates a large degree of variance [58, 72, 81]. In cold starts, there are 7.69% (5/65) functions, while warm starts have 32.31% (21/65). This implies that warm-start performance has a more severe variance than cold-start performance. The possible reason is that executing tasks in the reused instances (i.e., warm starts) may be more susceptible to resource contention and underlying policies in serverless platforms.\n(2) Boxplot: We use boxplots to visually show the distributions of performance results of serverless functions. Since serverless functions have different levels of latency granularity, we apply the commonly used min-max normalization method [14] to normalize each set of performance results generated by the serverless function to the 0 to 1 interval. Figs. 8 and 9 show the normalized boxplots about the performance of 65 serverless functions under cold starts and warm starts, respectively. The bottom of the box represents the value at the 25th percentile, while its top is the 75th percentile. The line in the box is the median, i.e., 50th percentile. Dots outside of the whiskers are outliers. We observe that most performance results (i.e., from the 25th percentile to the 75th percentile) of the serverless functions do not fall near the middle, i.e., 0.5 of 0 to 1 interval. This phenomenon occurs under both cold starts and warm starts. Moreover, the range sizes of most performance results of the serverless functions are inconsistent, indicating that there may not be a fixed distribution pattern for serverless function performance.\nTo further determine if the data distribution of serverless function performance is concentrated or skewed, we check whether the median of performance results falls within an error interval. This interval represents the 1% error above or below the middle value calculated from the maximum and minimum values. If the median is in this interval, the data distribution for serverless function performance is determined to be concentrated, and otherwise, it is skewed. The results show that distributions of the performance results of more than 92% of the serverless functions are skewed both under cold starts and warm starts. We also try to determine the direction in which most performance results of the serverless function are biased to the boxplot. We calculate the sum of distances of all performance results of each serverless function from its maximum andminimum values, respectively. The results show that most performance results of over 92% (cold starts: 63/65; warm starts: 60/65) of the serverless functions are skewed towards the corresponding minimum values, and another side of boxplots has a long tail. This can be observed visually in Figs. 8 and 9, where most boxplots are in the middle and lower part of the 0 to 1 interval.\nManuscript submitted to ACM\nFu nc 1 Fu nc 2 Fu nc 3 Fu nc 4 Fu nc 5 Fu nc 6 Fu nc 7 Fu nc 8 Fu nc 9 Fu nc 10 Fu nc 11 Fu nc 12 Fu nc 13 Fu nc 14 Fu nc 15 Fu nc 16 Fu nc 17 Fu nc 18 Fu nc 19 Fu nc 20 Fu nc 21 Fu nc 22 Fu nc 23 Fu nc 24 Fu nc 25 Fu nc 26 Fu nc 27 Fu nc 28 Fu nc 29 Fu nc 30 Fu nc 31 Fu nc 32 Fu nc 33 Fu nc 34 Fu nc 35 Fu nc 36 Fu nc 37 Fu nc 38 Fu nc 39 Fu nc 40 Fu nc 41 Fu nc 42 Fu nc 43 Fu nc 44 Fu nc 45 Fu nc 46 Fu nc 47 Fu nc 48 Fu nc 49 Fu nc 50 Fu nc 51 Fu nc 52 Fu nc 53 Fu nc 54 Fu nc 55 Fu nc 56 Fu nc 57 Fu nc 58 Fu nc 59 Fu nc 60 Fu nc 61 Fu nc 62 Fu nc 63 Fu nc 64 Fu nc 65\nN or\nm al\niz ed\ne2 eT\nim e\nfo r\nw ar\nm st\nar ts 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n1"
        },
        {
            "heading": "6 RQ3: DISTRIBUTION CHARACTERISTICS",
            "text": "In RQ3, we investigate the distribution characteristics that serverless function performance actually has. These characteristics help researchers and software engineers select suitable statistical techniques to analyze variable serverless function performance of multiple runs. In our study, we focus on stationarity and normality of distributions, being explained as follows.\nOn one hand, most statistical methods (e.g., confidence intervals) are to assume data stationarity [58]. The stationarity means that characteristics (e.g., median and percentile) of the underlying distribution will not change over time, i.e., most uncertainty factors and potential impacts of platforms have been included and reflected in the performance results. A stationary distribution can facilitate the performance analysis and make experiments more reliable. If the generated performance results follow a non-stationary distribution, it means that conducted experiments would not allow for a fair and reliable comparison. In this section, we check the stationarity of serverless function performance using the Augmented Dickey-Fuller test [31] (abbreviated as ADF test), which is a common statistical test to check whether the given data is stationary or not [17, 58]. The null hypothesis of the ADF test is that performance results follow a non-stationary distribution. When running the ADF test for a set of performance results, we can obtain a \ud835\udf0c value. The \ud835\udf0c value is greater than 0.05, allowing us to accept the null hypothesis, i.e., the performance generated by the serverless function is non-stationary. If the \ud835\udf0c value is less than 0.05, we reject the null hypothesis to get the conclusion that the performance generated by the serverless function follows a stationary distribution.\nOn the flip side, the statistical methods commonly used for the performance analysis of computer systems tend to assume that performance results are normally distributed [27, 42]. We investigate whether serverless function performance also follows a normal distribution. We use the Shapiro-Wilk test [69] (abbreviated as W test), which is considered the most powerful normality test in most situations [15]. In the W test, its null hypothesis is that the performance results come from the population with the normal distribution. When running theW test, we also obtain a \ud835\udf0c value. At a \ud835\udf0c value greater than 0.05, we can accept the null hypothesis to indicate that serverless function performance Manuscript submitted to ACM\nfollows a normal distribution; otherwise, we reject the null hypothesis and describe the serverless function performance as following a non-normal distribution.\nWe apply stationarity and normality checks to the performance results of 50 runs of each serverless function. These performance results are obtained in different instants of the day. The specific analysis results are as follows.\nResults: We observe that performance results generated by most serverless functions are stationary. Moreover, they follow a non-normal distribution. Fig. 10 shows the CDF about all \ud835\udf0c values, which are obtained by running the ADF tests on performance results of 50 runs of each serverless function for checking stationarity. We observe that most \ud835\udf0c values are less than 0.05, i.e., rejecting the null hypothesis and presenting strong evidence for stationarity. Specifically, the performance results of 96.92% (63/65) and 90.77% (59/65) of the serverless functions are stationary in cold starts and warm starts, respectively. Thus, we conclude that, despite the significant variance that serverless function performance shows, the performance of most serverless functions follows a stationary distribution, which includes most uncertainty factors and potential impacts from the underlying environment of serverless platforms.\nFig. 11 shows the CDF about all \ud835\udf0c values obtained from W tests, which are applied to the performance results of 50 runs of each serverless function for checking normality. We observe that most \ud835\udf0c values are less than 0.05, i.e., rejecting the null hypothesis and presenting strong evidence for normality. Specifically, the performance results of 92.31% (60/65) and 90.77% (59/65) of the serverless functions follow a normal distribution in cold starts and warm starts, respectively. In this situation, non-parametric analysis methods, which do not assume normality, are more appropriate for the performance analysis of serverless functions. For example, using non-parametric confidence intervals for the median may be effective, and they can also work for the normality distribution [58].\nFinding 3: Despite the significant variance, the performance of over 90% of the serverless functions is stationary, meaning that most uncertainty factors from serverless platforms are included and reflected in serverless function performance. The performance of over 90% of the functions follows a non-normal distribution. This implies that statistical methods that do not assume normality (e.g., non-parametric analysis) are more appropriate for the performance analysis of serverless functions."
        },
        {
            "heading": "7 RQ4: RELIABILITY AND REPETITIONS",
            "text": "In RQ4, we investigate the reliability of serverless function performance obtained at different repetitions. First, we use the performance results of the serverless function at 50 repetitions as standard result of serverless function performance, since 75% of the surveyed papers that report repetitions use no more than 50 times. Then, we compare the serverless function performance obtained at low repetitions that are used in the surveyed papers, i.e., 3, 4, 5, 10, 15, and 20,\nManuscript submitted to ACM\nshown in Fig. 5. From RQ3, we find that non-parametric methods are appropriate for the performance analysis of serverless functions. Thus, we use the most common metric of interest, e.g., median, tail percentile, and confidence intervals [58, 72], to compare performance. We calculate the median performance and tail performance (e.g., 90th percentile) of the serverless function at different low repetitions. Meanwhile, we calculate the corresponding 95% confidence interval for the median and confidence interval for the 90th percentile at 50 repetitions, as adopted by the work [72]. The 95% confidence interval for the median or confidence interval for the 90th percentile represents the range in which we could find the true median or 90th percentile with 95% probability if we could perform infinite repetitions. Thus, when a median or 90th percentile obtained at the low repetition lies outside the 95% confidence interval for the median or confidence interval for the 90th percentile obtained at the high repetition, it indicates that there is a 95% probability that this median or 90th percentile is inaccurate. The calculations of the confidence interval for the median and confidence interval for the 90th percentile can refer to the work [50, 58].\nResult: We observe that the serverless functions produce inaccurate median and tail performance under low repetitions. Specifically, Fig. 12 shows the percentage of the serverless functions in the case where the obtained median or 90th percentile is inaccurate under different low repetitions. When the number of repetitions is 3, for 56.92% (37/65) of the serverless functions in cold starts, the obtained median falls outside of the confidence intervals obtained at 50 repetitions, i.e., the obtained median is inaccurate, while warm starts have 61.54% (40/65) of the functions. For the 90th percentile, 55.38% (36/65) of the functions have inaccurate tail performance at 3 repetitions (warm start: 61.54% (40/65)). This indicates that experiments with low repetitions have a high risk of reporting unreliable performance of serverless functions. We also observe that increasing the number of repetitions can make the percentage of inaccurate results decrease. However, at the 10-repetition most frequently used by the surveyed papers, 26.15% (17/65) of the functions still show inaccurate median performance, and 38.46% (25/65) of functions show inaccurate tail performance in cold starts. These results imply that serverless function performance is not reliable under low repetitions commonly used in most papers.\nWe try to find regularities in how to determine the appropriate repetitions for serverless functions to obtain reliable performance. We first observe the changes in top and bottom bounds for the confidence interval obtained at different repetitions. As an example, Fig. 13 shows the changes in the bounds of the confidence intervals for the median, with regard to Func2. We do not plot the bounds of confidence interval for the 90th percentile since repetitions of 50 and below are not enough to obtain them [49, 50]. Moreover, we do not plot the confidence intervals for the median at smaller repetitions, e.g., 3, 4, and 5 in Fig. 12, because these repetitions are also insufficient to calculate them [49, 50]. Instead, we compare the number of repetitions from 10 to 50 in 5-step increments. From Fig. 13, confidence intervals gradually Manuscript submitted to ACM\nbecome tight as the number of repetitions increases. The line representing the difference between the top bound and bottom bound also shows a gradual downward trend. However, when the number of repetitions is small, e.g., 20 to 25 repetitions, the bounds of the confidence interval may have slight fluctuations. It is reasonable that the distribution with a small number of results may not be stable. We also check changes in the bounds of confidence intervals from other serverless functions, showing a similar result to Fig. 13. Overall, performing more repetitions for serverless functions can achieve a tight confidence interval for the median, which increases confidence in claiming the obtained results are close to the population distribution.\nLeveraging the observation about tight confidence intervals for the median, we further explore how many repetitions may be required to achieve a sufficiently narrow confidence interval for serverless function performance. This interval is a desired interval representing satisfactory performance, where the empirical median differs from the observed true median by no more than the \ud835\udc5f error margin at a given confidence level, e.g., 95%. In other words, when the confidence interval for the median obtained from results at a certain repetition drops within the \ud835\udc5f error margin of the corresponding observed true median performance, the desired confidence interval is obtained to stop running the serverless function, and the current number of repetitions is regarded as the appropriate repetition for the serverless function. We show the results for \ud835\udc5f = 0.5% and 1% in Fig. 14. When \ud835\udc5f = 0.5%, in cold starts, 98.46% (64/65) of the serverless functions require being executed at least 50 times to get the desired confidence interval, while warm starts have 80.00% (52/65) of the functions. After we relax \ud835\udc5f to 1%, there are still 69.23% (45/65) functions that need to be executed repeatedly at least 50 times in cold starts. There are 55.38% (36/65) functions in warm starts. However, according to the aforementioned Fig. 5, only 37.50% of the papers that report experiment repetitions execute serverless functions more than 50 times. Overall, these results indicate that serverless functions require far more repetitions than the ones commonly used in most surveyed papers.\nFinding 4: In the premise of non-parametric analysis, up to 61.54% of the serverless functions show an unreliable performance when a small number of repetitions is performed, as commonly adopted in the surveyed studies. Performing more repetitions for serverless functions can achieve tight confidence intervals for the median. Up to 98.46% of the serverless functions need to be executed at least 50 times to achieve the desired performance, but only 37.50% of the papers that report experiment repetitions use more than 50 times to execute functions. These results indicate that serverless functions require far more repetitions than the ones commonly used in the literature.\nManuscript submitted to ACM"
        },
        {
            "heading": "8 DISCUSSION",
            "text": ""
        },
        {
            "heading": "8.1 Implications, suggestions, and future work",
            "text": "Correct data analysis. Serverless function performance follows a stationary distribution and non-normal distribution, as shown in RQ3. This indicates some of the standard default assumptions of statistical analysis in the common performance analysis may be broken. In light of this finding, it is advisable to identify general and standard assumptions, such as testing for stationarity [31] and normality [69], when analyzing the performance of serverless functions. If performance results are not stationary, performance acquisition for serverless functions can be restricted to the time period in which stationarity holds, or repeated over a longer time frame, etc. When performance results do not follow a normal distribution, non-parametric analysis techniques [39] can be applied.\nResearch reliability and reproducibility. Variance of serverless function performance reduces the reliability of research conclusions and the reproducibility of serverless computing-related experiments. From the result of RQ1 we obserdved that most research papers related to serverless function performance lack an adequate description of performance results. We call on researchers to be aware of and pay more attention to the variance of serverless function performance. Meanwhile, they require to give detailed experimental designs. One practical strategy is to provide a specific and non-arbitrary number of repetitions. This number may be set to be greater than 50 if possible, but it is not recommended to be too large (e.g., 1,000) due to the high runtime overhead. Based on the obtained performance data, researchers can report comprehensive results (including mean, median, coefficient of variance, percentile, confidence intervals, and distribution) to compare solutions from multiple dimensions and provide strong evidence for conclusions. Moreover, making raw data of serverless function performance publicly available allows other researchers to calculate any metric of interest and facilitates researchers to derive valuable observations across different studies.\nServerless functions have two start types: cold starts and warm starts. Some studies have focused on optimizing cold-start performance [43, 73], while others have aimed to ensure efficient warm-start performance [28, 54]. In RQ2, we find that the variance of serverless function performance is slightly more severe under warm starts than under cold starts. Generally, the higher the performance variance, the greater the risk that the experiment will report unreliable results. Thus, we advise paying more attention to the variance of serverless function performance in warm starts. One solution is to perform few and not simultaneous invocations of warm starts at a time to decrease the potential risk of resource contention from multiple function instances. Additionally, since serverless function performance has different degrees of variance under cold and warm starts, the conducted experimental designs regarding performance acquisition, e.g., setting of the number of repetitions, need to be considered separately in different start types, if possible.\nThe serverless computing-related researchers often presented new approaches to improve serverless function performance [23, 43, 73]. These approaches were compared with other existing baselines to demonstrate the effectiveness in performance improvement. When comparing the improvement in serverless function performance produced by different approaches, we suggest using the Mann-Whitney U-test [20]. It is a statistical test that assesses the similarity of sample distributions, and it does not assume the normality of distributions. Moreover, studies from other topics, e.g., deep learning [63] and hardware system [58], have used it to measure the performance difference. If there is a statistically significant difference, researchers can further compute the effect size to check if the difference has a meaningful effect. Cliff\u2019s \ud835\udeff [18] can be applied to obtain the effect size because it does not also assume data normality. Generally, |\ud835\udeff | \u2265 0.428 is considered a large effect [25, 47].\nNetwork and storage optimization from the developers\u2019 perspective. From the analysis of RQ2, we found that the serverless functions integrating external cloud services (such as AWS S3 [4]) exhibit a high coefficient of\nManuscript submitted to ACM\nvariance. This can be attributed to network requests associated with external storage services. From a developer perspective, one can choose the closest service availability region to the region where the storage service is located for serverless functions to reduce network latency and fluctuations. For stored data, developers can apply compression and serialization techniques [19] to reduce the volume of data in the network transmission process.\nNew performance testing approaches. Performing more repetitions for serverless functions will obtain a tight confidence interval about performance, as shown in RQ4. This means that we can be more confident that the distribution of the obtained performance results is close to the population distribution. Therefore, the tight confidence interval can provide a powerful basis for the accuracy of serverless function performance. However, the serverless computing scenario may require a stricter confidence interval constraint as the stopping condition for repeated runs of the experiment because the serverless functions are short-lived, and the underlying infrastructure is highly dynamic. In future work, we will develop such a performance testing technique to help developers determine accurate serverless function performance to reduce manual effort.\nSpecialized performance bug detection. The design of the underlying environment of serverless platforms is agnostic to software developers. Other reasons for the significant performance variance may stem from the unique programming model characteristics of serverless computing, such as event-driven design and the separation pattern of compute and storage [75]. A research opportunity could be to design specialized performance bug detection techniques to identify and fix specific bugs or issues that directly impact the performance of a serverless function.\nVariance-aware serverless functions. The variance analysis of serverless function performance presented in our work opens up intriguing avenues for future research. For instance, serverless functions can be designed to be variance-aware in the future. New techniques or algorithms (such as efficient variance modeling) may be desirable. Additionally, there is a need to investigate strategies and mechanisms to make serverless functions more adaptive and responsive to variance. This will empower serverless functions to autonomously adjust their configurations or resource allocations based on the observed variance, ensuring optimal performance and stability."
        },
        {
            "heading": "8.2 Threats to validity",
            "text": "Construct validity. We calculate relevant metrics (e.g., coefficient of variance) and statistical tests (e.g., ADF test and W test) based on the realistic performance results generated by serverless functions. There is a threat that the collected results may not match the corresponding serverless function. To minimize this threat, we carefully collect results and double-check whether these results belong to the corresponding serverless function. Moreover, we carefully calculate metrics, apply statistical tests, and verify the required assumptions. Internal validity. In our methodology, we rely on top-tier conferences to collect the related papers. We also label the reporting information of each paper in RQ1. We cannot guarantee that the collected papers are complete and the summarized information is correct. To mitigate these threats, the first two authors independently read the title and abstract parts of each paper and then select relevant ones. Moreover, they separately read the full text of the selected papers to determine specific information. We calculate the inter-rater agreement, and the values imply that our labeling procedures are reliable. Additionally, an experienced arbitrator resolves the conflicts to reach an agreement with the first two authors.\nIn RQ2, we do not further explore variance sources or behaviors of the underlying platform. However, this threat is unavoidable since cloud providers may change the platform\u2019s policies, and policies are opaque and uncontrollable to external users. Moreover, the cloud-based underlying mechanism is complex due to large-scale server management, and thus it is difficult to distinguish variance sources. In addition, task types of serverless functions are diverse, and it is\nManuscript submitted to ACM\nchallenging to clearly summarize the behavior changes of the serverless platform. Our study mainly aims to reveal that the serverless function performance has significant variance when executed on serverless platforms. We also try to arouse awareness and attention about this problem and give actionable suggestions to alleviate it. In future work, we will design a lightweight performance testing approach to determine the number of repetitions with reliable serverless function performance to conduct valid experiments. External validity. In RQ3, we explore the distribution characteristics of serverless function performance. Serverless functions with different functionalities may have inconsistent distribution characteristics. This may lead to the limited generalizability of the characteristics that we find. To alleviate this threat, we summarize the characteristics of the performance of the 65 serverless functions. These functions have a wide range of task types, including machine learning, scientific computing, and Web requests. Therefore, the distribution characteristics that we summarize belong to the characteristics exhibited by most serverless functions. Certainly, some serverless functions may follow a non-stationary distribution or normal distribution. In this situation, we refer the reader to the suggestions about correct data analysis discussed in Section 8.1."
        },
        {
            "heading": "9 RELATEDWORK",
            "text": ""
        },
        {
            "heading": "9.1 Serverless computing performance",
            "text": "Serverless computing is an emerging development paradigm for software developers, and it has been widely exploited in various software applications, e.g., video processing [24, 36], machine learning [44, 78], scientific computing [53, 68], and big data analytics [59, 80]. The SE community has also shown a growing interest in serverless computing [22, 33, 34, 51, 55, 64, 75, 76]. For example, Eismann et al. [34] analyzed the usage characteristics of serverless-based applications and revealed the performance importance. Wen et al. [76] explored specific development challenges containing performance issues when developing serverless-based applications. The stability of performance measurements [33] on the serverless platform was investigated under varying configurations regarding the load or concurrency level. A general approach called FaaSLight [55] was presented to improve the cold-start performance of serverless functions. We conduct a timely analysis of the variance of serverless function performance and try to provide actionable suggestions to alleviate it.\nIn the serverless computing literature, the most widely studied topic is the performance of serverless computing [52, 65, 75]. These studies have focused on performance prediction [32, 54] and performance optimization [23, 43, 73]. For performance prediction, existing studies have leveraged the history data (e.g., memory size or resource consumption) to predict the serverless function performance [32, 54]. However, even though comprehensive history factors are considered, it is challenging to predict an accurate performance since the serverless function executed on the serverless platform shows different performance results each time. For performance optimization [23, 43, 73], existing studies have proposed new solutions (e.g., new systems or platforms) and then used serverless functions to evaluate them. The serverless function performance is the key to verifying the effectiveness of solutions. However, we find that serverless function performance has a significant variance. Moreover, existing related studies (even influential studies from top-tier conferences) often overlooked the variance of serverless function performance. To the best of our knowledge, this variance has not also been studied in the serverless computing literature."
        },
        {
            "heading": "9.2 Performance variance analysis",
            "text": "Existing studies have focused on analyzing performance variance for different domains. For example, Maricq et al. [58] conducted a study on the performance variance of hardware, where they analyzed collected data to demystify the Manuscript submitted to ACM\nquantities of variance. Uta et al. [72] investigated the performance variance of networks when executing big data workloads, aiming to understand their impact on performance reproducibility. Additionally, Georges et al. [38] presented a survey of Java performance evaluation methodologies. Although the significance of performance variance has been acknowledged by various communities, the serverless computing community still lacks awareness of the variance of serverless function performance. In the context of serverless computing, the underlying environments are complex and highly dynamic. However, the magnitude of performance variance and performance characteristics in the new programming model - serverless functions, remain unclear. While it is true that many practitioners and researchers may also have their suspicions about the performance variance of serverless computing, there is no scientific measurement of the effect size of it. In this paper, we provide an empirical study to properly measure and understand the magnitude of the problem. Without this, all we are left with is a \u201cbelief\u201d rather than quantitative scientific evidence."
        },
        {
            "heading": "10 CONCLUSION",
            "text": "We conducted the first empirical study to investigate the variance of serverless function performance. First, we performed a literature review of 59 related papers, showing that attention to this variance is still in the infancy stage. Second, we analyzed the performance of 65 serverless functions collected from these papers obtained over multiple runs. We observed a significant performance variance. Then, we explored the distribution characteristics of serverless function performance, finding that the performance of over 90% of the serverless functions follows a non-normal distribution, and thus is more appropriate to use non-parametric analysis that does not assume normality. Finally, we analyzed the reliability of serverless function performance obtained by executing a low number of repetitions, as commonly done in our collected papers. We found that 61.54% of the functions have unreliable performance. Moreover, 98.46% of the functions required being executed at least 50 times to achieve reliable performance, but only 37.50% of the collected papers that report experiment repetitions execute serverless functions more than 50 times. This implies that serverless functions require far more repetitions than the ones commonly used in the literature."
        }
    ],
    "title": "Revisiting the Performance of Serverless Computing: An Analysis of Variance",
    "year": 2023
}