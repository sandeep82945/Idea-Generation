{
    "abstractText": "This study presents a novel approach to bone age assessment (BAA) using a multi-view, multi-task classification model based on the Sauvegrain method. A straightforward solution to automating the Sauvegrain method, which assesses a maturity score for each landmark in the elbow and predicts the bone age, is to train classifiers independently to score each region of interest (RoI), but this approach limits the accessible information to local morphologies and increases computational costs. As a result, this work proposes a self-accumulative vision transformer (SAT) that mitigates anisotropic behavior, which usually occurs in multi-view, multi-task problems and limits the effectiveness of a vision transformer, by applying token replay and regional attention bias. A number of experiments show that SAT successfully exploits the relationships between landmarks and learns global morphological features, resulting in a mean absolute error of BAA that is 0.11 lower than that of the previous work. Additionally, the proposed SAT has four times reduced parameters than an ensemble of individual classifiers of the previous work. Lastly, this work also provides informative implications for clinical practice, improving the accuracy and efficiency of BAA in diagnosing abnormal growth in adolescents.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hong-Jun Choi(B"
        },
        {
            "affiliations": [],
            "name": "Dongbin Na"
        },
        {
            "affiliations": [],
            "name": "Kyungjin Cho"
        },
        {
            "affiliations": [],
            "name": "Byunguk Bae"
        },
        {
            "affiliations": [],
            "name": "Seo Taek Kong"
        },
        {
            "affiliations": [],
            "name": "Hyunjoon An(B"
        }
    ],
    "id": "SP:a34e4b695172863bbef2a120d59f3cee184f5d03",
    "references": [
        {
            "authors": [
                "K.S. Ahn",
                "B. Bae",
                "W.Y. Jang",
                "J.H. Lee",
                "S. Oh",
                "B.H. Kim",
                "S.W. Lee",
                "H.W. Jung",
                "J.W. Lee",
                "J Sung"
            ],
            "title": "Assessment of rapidly advancing bone age during puberty on elbow radiographs using a deep neural network model",
            "venue": "European Radiology 31(12), 8947\u20138955",
            "year": 2021
        },
        {
            "authors": [
                "H. Chefer",
                "S. Gur",
                "L. Wolf"
            ],
            "title": "Transformer interpretability beyond attention visualization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 782\u2013791",
            "year": 2021
        },
        {
            "authors": [
                "S. Chen",
                "T. Yu",
                "P. Li"
            ],
            "title": "MVT: multi-view vision transformer for 3d object recognition",
            "venue": "32nd British Machine Vision Conference 2021, BMVC 2021, Online, November 22-25, 2021. p. 349. BMVA Press",
            "year": 2021
        },
        {
            "authors": [
                "A. Dim\u00e9glio",
                "Y.P. Charles",
                "J.P. Daures",
                "V. de Rosa",
                "B. Kabor\u00e9"
            ],
            "title": "Accuracy of the sauvegrain method in determining skeletal age during puberty",
            "venue": "JBJS 87(8), 1689\u20131696",
            "year": 2005
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net",
            "year": 2021
        },
        {
            "authors": [
                "P. Foret",
                "A. Kleiner",
                "H. Mobahi",
                "B. Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net",
            "year": 2021
        },
        {
            "authors": [
                "X. Geng",
                "Z.H. Zhou",
                "Y. Zhang",
                "G. Li",
                "H. Dai"
            ],
            "title": "Learning from facial aging patterns for automatic age estimation",
            "venue": "Proceedings of the 14th ACM international conference on Multimedia. pp. 307\u2013316",
            "year": 2006
        },
        {
            "authors": [
                "W.W. Greulich",
                "S.I. Pyle"
            ],
            "title": "Radiographic atlas of skeletal development of the hand and wrist",
            "venue": "Stanford University Press",
            "year": 1959
        },
        {
            "authors": [
                "G. Guo",
                "G. Mu",
                "Y. Fu",
                "T.S. Huang"
            ],
            "title": "Human age estimation using bio-inspired features",
            "venue": "2009 IEEE conference on computer vision and pattern recognition. pp. 112\u2013119. IEEE",
            "year": 2009
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778",
            "year": 2016
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4700\u20134708",
            "year": 2017
        },
        {
            "authors": [
                "H. Lee",
                "S. Tajmir",
                "J. Lee",
                "M. Zissen",
                "B.A. Yeshiwas",
                "T.K. Alkasab",
                "G. Choy",
                "S. Do"
            ],
            "title": "Fully automated deep learning system for bone age assessment",
            "venue": "Journal of digital imaging 30(4), 427\u2013441",
            "year": 2017
        },
        {
            "authors": [
                "D. Neimark",
                "O. Bar",
                "M. Zohar",
                "D. Asselmann"
            ],
            "title": "Video transformer network",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3163\u20133172",
            "year": 2021
        },
        {
            "authors": [
                "H. Pan",
                "H. Han",
                "S. Shan",
                "X. Chen"
            ],
            "title": "Mean-variance loss for deep age estimation from a face",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5285\u20135294",
            "year": 2018
        },
        {
            "authors": [
                "S. Park",
                "G. Kim",
                "Y. Oh",
                "J.B. Seo",
                "S.M. Lee",
                "J.H. Kim",
                "S. Moon",
                "J. Lim",
                "J.C. Ye"
            ],
            "title": "Multi-task vision transformer using low-level chest x-ray feature corpus for COVID-19 diagnosis and severity quantification",
            "venue": "Medical Image Anal. 75, 102299",
            "year": 2022
        },
        {
            "authors": [
                "J. Sauvegrain",
                "H. Nahum",
                "H. Bronstein"
            ],
            "title": "Study of bone maturation of the elbow",
            "venue": "Annales de radiologie. vol. 5, pp. 542\u2013550",
            "year": 1962
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings",
            "year": 2015
        },
        {
            "authors": [
                "C. Spampinato",
                "S. Palazzo",
                "D. Giordano",
                "M. Aldinucci",
                "R. Leonardi"
            ],
            "title": "Deep learning for automated skeletal bone age assessment in x-ray images",
            "venue": "Medical image analysis 36, 41\u201351",
            "year": 2017
        },
        {
            "authors": [
                "Z. Sun",
                "H. Jiang",
                "L. Ma",
                "Z. Yu",
                "H. Xu"
            ],
            "title": "Transformer based multi-view network for mammographic image classification",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Conference, Singapore, September 18\u201322, 2022, Proceedings, Part III. pp. 46\u201354. Springer",
            "year": 2022
        },
        {
            "authors": [
                "J.M. Tanner",
                "R. Whitehouse",
                "N. Cameron",
                "W. Marshall",
                "M. Healy",
                "H Goldstein"
            ],
            "title": "Assessment of skeletal maturity and prediction of adult height (TW2 method)",
            "venue": "Saunders London",
            "year": 2001
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X Wang"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence 43(10), 3349\u20133364",
            "year": 2020
        },
        {
            "authors": [
                "S. Xie",
                "R. Girshick",
                "P. Doll\u00e1r",
                "Z. Tu",
                "K. He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492\u20131500",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Keywords: Bone age assessment \u00b7 Vision transformer \u00b7 Sauvegrain method \u00b7 Multi-view \u00b7 Multi-task"
        },
        {
            "heading": "1 Introduction",
            "text": "Bone age assessment (BAA) is widely used to diagnose precocious or delayed puberty. Deep neural network (DNN) has demonstrated remarkable success in a wide range of safety-critical applications, and it also has been actively adopted in BAA with successful applications to computer-aided diagnosis systems [12,18]. To develop an application that predicts bone age, the DNN can be trained with several clinical criteria, including Greulich-Pyle [8], Tanner-Whitehouse [20], and Sauvegrain method [16]. Among these, the Sauvegrain method, which evaluates the skeletal age based on four elbow landmarks (i.e., lateral condyle, trochlea, proximal, olecranon) from two different views (see Fig 1(a)), is more adequate for\nar X\niv :2\n30 3.\n16 55\n7v 2\n[ cs\n.C V\n] 3\n0 M\nthe age of puberty [1,4]. Specifically, the Sauvegrain method has an important trait in that the relationship among landmarks has a correlation on the label space (see Fig. 1(b)).\nA recent study has used deep learning algorithms to apply the Sauvegrain method by training an individual convolutional neural network (CNN) to assess the maturity point for each landmark [1]. This approach involves each classification model inferring a score for a region of interest (RoI), and the aggregate of scores from multiple models is used to determine skeletal age.\nAlthough the previous study shows that the classification performance of the CNNs is comparable to experts, their approach has two limitations; First, while ground-truth labels between inter-landmarks have a strong correlation, the incorporated model that each classifier trained with single landmark images can produce misclassified predictions with high variance. These inaccurate predictions may confuse radiologists when interpreting the model\u2019s decisions. Second, the previous method requires excessive computational costs both training and inference because multiple landmark networks should be trained independently.\nTo address the above issues, this study poses a novel approach to solving the multi-view and multi-task problem for Sauvegrain-based BAA using a vision transformer (ViT) [5] instead of an ensemble of single-view CNNs. By leveraging an attention mechanism in ViT, the model learns effective relations within input sequences which consist of multi-view inputs. Moreover, we can reduce the number of parameters and computational costs by adopting shallow RoI-specific classifiers at the top of the shared encoder. Although ViT has been applied to multi-view [3,13,19] and multi-task [15] problems, but not when they coexist.\nHowever, we find that the vanilla ViT trained with a multi-view and multitask (MV-MT) manner suffers from poor optimization and generalization. One of the reasons is that inter-landmarks are often excessively accentuated. As a result, anisotropic behavior in the attention layer leads to sub-par classification perfor-\nmance. To overcome the above challenge, we propose the self-accumulative vision transformer (SAT) that accumulates their intra-region information by two components: (1) token replay that prevents semantic representations of tokens with the same landmark from being overwhelmed by other regional tokens by using residual connections between class tokens and their corresponding regional tokens, and (2) regional attention bias (RAB), modified self-attention mechanism, to impose an intra-region attention. Despite having significantly fewer parameters, the proposed SAT predicts maturity scores across landmarks much more accurately and outperforms other state-of-the-art models on most landmarks."
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 Preliminary",
            "text": "MV-MT Vision Transformer. Here we describe how we train the vanilla MV-MT ViT as its variant for ordinal classification. We adopt a hybrid vision transformer where a patch-wise projection module is replaced by a CNN encoder with an average pooling E : X \u2192 Rd (e.g., ResNet [10]) Therefore, a RoI image x \u2208 RH\u00d7W\u00d7C is processed with the embedding network: e0r = E(xr), where r \u2208 {1, ..., R} indicates the landmark index and R = 5.\nBefore feeding them to the ViT, learnable [CLS] tokens z\u03030 := [ e\u030301, . . . , e\u0303 0 R ] are prepended to the embedded sequence of regional tokens z0 := [ e01, . . . , e 0 R ] . A sequence of tokens [z\u03030, z0] \u2208 R2R\u00d7d is then fed to the ViT with L encoder layers\n{hl}Ll=1. For obtaining R RoI predictions, we use the final [CLS] tokens z\u0303L to classify the maturity scores with each RoI-specific classifier which consists single dense layer. The detailed comparison illustrations for architectures between SVST, MV-ST, and MV-MT are shown in the supplementary section. Ordinal Classification. To estimate the bone age of an individual, where their classes have an ordered relation, our method handles the ordinal classification. Therefore, we adopt the mean-variance loss [14] as our loss function. In our framework, which addresses multi-view and multi-task, each region r is associated with different numbers of classes (scores)Kr. The maturity score probability distribution pr of the region r is calculated by forwarding an embedding vector e\u0303Lr introduced by the last L-th encoder layer into a classification head Fr(e\u0303Lr ). Consequently, we can get the probability value for k-th label of the region r as pr,k (k \u2208 {1, 2, ...,Kr}). Given a predicted probability pr,k value over Kr possible scores and yr its ground-truth label, the mean loss L\u00b5(x) for an region image x is defined as:\nL\u00b5(x) = 1\nR R\u2211 r=1 (\u00b5r \u2212 yr)2 = 1 R R\u2211 r=1 ( Kr\u2211 k=1 k \u2217 pr,k \u2212 yr)2. (1)\nWe utilize the mean squared error (MSE) loss for reducing the difference between predicted mean \u00b5r = Ey\u0302\u223cpr [y\u0302] and the underlying ground-truth score yr. Similarly, the variance loss L\u03c32(x) for an region image x is defined as:\nL\u03c32(x) = 1\nR R\u2211 r=1 Kr\u2211 k=1 pr,k \u2217 (k \u2212 \u00b5r)2. (2)\nThus, considering the dataset size of N , mean loss and variance loss is calculated as 1N \u2211N i=1 L\u00b5(xi) and 1 N \u2211N i=1 L\u03c32(xi) respectively. Finally our model is optimized by following total loss:\nLtotal(x) = Lce(x) + \u03bb\u00b5 \u00b7 L\u00b5(x) + \u03bb\u03c32 \u00b7 L\u03c32(x), (3)\nwhere Lce is the cross-entropy loss, coefficient \u03bb\u00b5 and \u03bb\u03c32 is a hyperparameter to adjust the weight of each loss function. In our work, we have found that \u03bb\u00b5 and \u03bb\u03c32 works best at 0.2 and 0.05 respectively."
        },
        {
            "heading": "2.2 Analysis on Anisotropic Relations between Landmarks",
            "text": "When training the vanilla ViT as described above, we have observed that the attention module emphasizes excessively on inter-RoI patches. As shown in Fig. 3(a), olecranon has gained most of the attention from inter RoIs. Indeed, this result can be interpreted as natural behavior when assessing bone age, as an interpretation of bone age using olecranon alone is the simple yet effective method in clinical practice [4]. However, to obtain a better elaborate and accurate interpretation of bone age, scores from all RoIs based on their morphology is essential to be obtained in the Sauvegrain method [4]. Thus, true correlations between\nmaturity scores are better to be more isotropic, i.e. discrepancies are allowed for inter-landmarks but predictions on a single landmark must be identical. To resolve the disparity between ViT and expected behaviors, we propose two modifications: (1) token replay method that repeatedly adds patch embeddings to intermediary features with corresponding tokens, and (2) RAB, which explicitly imposes regional bias on the attention map."
        },
        {
            "heading": "2.3 Self-accumulative Vision Transformer",
            "text": "Token Replay. Regional predictions (classification heads) do not necessarily prioritize their corresponding regional tokens. Since each class token in the last encoder layer that is used for computing the each maturity score utilizes the self-attention mechanism with multiple regional tokens and other class tokens, their own region-specific semantic information could be hindered and mixed with other tokens. Thus, they could not preserve their own region-specific information. In contrast, we argue that intentionally considered isotropic behavior could be efficient to improve the classification performance. Therefore token replay is designed to preserve region-specific signals in predicting maturity scores by \u201creplaying\u201d input regional tokens z0 = [e1, . . . , eR] so that z0 are added to [CLS] tokens encoding as z\u0303l = hl ([z\u0303l\u22121, zl\u22121]) + [z0, 0] at each layer l. Fig. 2(b) illustrates how token replay works in each encoder layer.\nTake the final features z\u0303L+z0 (with a slight abuse of notation) as an example used by classifier heads to predict maturity stages. Similar to how information propagation is improved through the use of residual connections in neural networks [10,11], classification heads are guaranteed to attribute weights to regional patches. Fig. 3(b) illustrates how token replay improves optimization and generalization, confirming the need that each [CLS] tokens should be accentuated by intra-regional signals. Regional Attention Bias (RAB). Attention modules are designed to underscore more relevant query-key token pairs. Recall the expected behavior of\nthe maturity score prediction model is to attribute intra-regional features. Regional predictions in attention modules as-is are not incited to prioritize their corresponding regional tokens. Consequently, the class-regional token attention relevance scores [2] are observed in left Fig. 3(a) to be highly anisotropic conflicting with their desired behavior.\nTo remedy this anisotropic behavior, as shown in Fig. 2(c), we explicitly add a matrix B to each attention A\u0303 where A\u0303 \u2208 R2R\u00d72R denote the pre-softmax attention at a fixed layer and B is a 2 \u00d7 2 block matrix with 0s on all blocks other than the second block DR. Thus, the top-right side of the matrix B has values of DR which denotes the R-dimensional diagonal matrix. RAB dr in DR is computed by the following equation\ndr = tanh (br + 1) \u2217 0.5. (4)\nHere br denotes the learnable scalar for each region. By amplifying both forward and back-propagation with RAB from the beginning, intra-regional attention is emphasized throughout training. Thus, the attention to intra-landmarks is increasingly emphasized with SAT as presented in Fig. 3(a)."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Environments",
            "text": "Dataset. Elbow AP and lateral pairs of radiographs were collected from three anonymous tertiary hospitals 4. The dataset consists of 4,615 radiograph pairs of AP and lateral view positions collected from the two institutions are used for training and validation. In addition, 164 pairs of AP and lateral radiograph images collected from another different institution were used for the test dataset. Three and five researchers with more than 7 years of experience labeled the data providing the maturity score of each landmark for each dataset respectively (e.g., lateral condyle, trochlea, olecranon, proximal). Lastly, landmarks have been extracted from each AP and lateral view elbow radiographs with a key point detection network as used in previous work.[1]. Each extracted landmark is resized by 384\u00d7384. More detailed statistics for the dataset are represented in the table in the supplementary material. Baseline algorithms. Due to lack of Sauvegrain related works, we compare the previous work [1] and its variants to evaluate the BAA performance of SAT: an ensemble of R single-view single-task CNNs (SV-ST CNNs), multi-view singletask CNNs (MV-ST CNNs), and multi-view multi-task CNN (MV-MT CNN), an ensemble of R multi-view single-task ViTs (MV-ST ViTs) and multi-view multi-task ViT (MV-MT ViT). Training details. The CNN used in ViT to embed the token is ResNet18 [10]. We set the depth to 12 with 6 heads and the embedding dimension to 384 for training the SAT and its variants. Models were trained using the SAM optimizer 4 Data sources are currently undergoing disclosure procedures, then will be revealed.\n[6] with cosine annealing on a batch size of 16 for 30 epochs. The initial learning rate is 0.01. For the data augmentation, we have rotated the image randomly between \u00b115\u00b0, shifted randomly to \u00b132 pixels, and flipped horizontally 5."
        },
        {
            "heading": "3.2 Experimental Results",
            "text": "Performance Comparison. Table 1 presents the number of parameters and mean absolute error (MAE) of each landmark, their sum, and their conversion to BAA. The performance of MAE has been evaluated on the test set using five-fold cross-validation on the training set (80% training and 20% validation). The SAT model outperforms other compared methods, achieving 0.261 MAE on BAA. Our results showed that the multi-view strategy is effective in both CNNbased and ViT-based architectures for solving the Sauvegrain method. However, the multi-task problem remains challenging, as demonstrated by the inferior performance of MV-MT CNN and MV-MT ViT compared to MV-ST CNNs and MV-ST ViTs, respectively. It could be interpreted as multi-task methods were detrimentally affected by over-reliance on inter-landmark inputs. Interestingly, our SAT model even outperformed MV-ST ViTs with significantly fewer parameters, suggesting that SAT explicitly emphasizes the attention of landmarks isotropically and benefits from both multi-view information and the interplay between landmarks, while addressing the multi-task problem. Comparison using cumulative score [7,9] is reported in the supplementary section. Ablation Study. The last two rows in Table 1 show the influence of each SAT component. We have removed each component of our SAT and reported the BAA results. Compared with MV-MT ViT, both token replay and RAB are crucial for the performance, however, the former contribute the most when combined together as shown in Table 1. Nonetheless, utilizing RAB consistently\n5 Github repository URL will be updated after the review.\noutperforms ViT and induces higher relevance maps in its attention which is desirable in understanding what the classifier attributes its predictions to.\nComparisons between other CNN encoders are reported in Table 2. We have observed that ResNet-18 shows the most efficient computational cost. Though other models, such as VGG and ResNext, shows better BAA results than ResNet18, their computational cost is not efficient as ResNet. There is no statistically significant difference between the ResNet-18 and other models. Thus, we have used the baseline encoder of SAT for ResNet-18 which has the least number of parameters. Case Analysis. To verify that SAT indeed has lower variance in predicting the maturity score in real data, we have analyzed the 3 cases where SV-ST CNNs [1] has shown the largest MAE in summation of scores in Fig. 4. Prediction score for SV-ST CNNs and SAT is reported with ground truth score. As shown in Fig 4, not only the prediction error of SV-ST CNNs but also the variance in the scores is much bigger than SAT does. It demonstrates that SV-ST CNNs have limitations when applied to Sauvegrain methods, where the maturity score of each RoI is highly correlated. On the other hand, SAT shows better prediction on each RoI and lower variance than SV-ST CNNs, demonstrating that SAT could be a practical solution for the hard cases."
        },
        {
            "heading": "4 Conclusion",
            "text": "This work studied the Sauvegrain-based BAA and identified issues with DNNs trained for the MV-MT ordinal classification problem. Ensembling CNNs increases computational costs and vanilla ViT leads to anisotropic attention and prediction discrepancies. To address these issues, this work introduced SAT, consisting of token replay and regional attention bias techniques, which were effective in mitigating these problems. This approach has broader implications for training ViT for MV-MT ordinal classification. Applied to Sauvegrain-based BAA, SAT is clinically meaningful in assisting diagnosis of precocious and delayed maturity in adolescents."
        }
    ],
    "title": "Self-accumulative Vision Transformer for Bone Age Assessment Using the Sauvegrain Method",
    "year": 2023
}