{
    "abstractText": "Composed image retrieval which combines a reference image and a text modifier to identify the desired target image is a challenging task, and requires the model to comprehend both vision and language modalities and their interactions. Existing approaches focus on holistic multi-modal interaction modeling, and ignore the composed and complimentary property between the reference image and text modifier. In order to better utilize the complementarity of multi-modal inputs for effective information fusion and retrieval, we move the multi-modal understanding to fine-granularity at concept-level, and learn the multi-modal concept alignment to identify the visual location in reference or target images corresponding to text modifier. Toward the end, we propose a NEUral COncept REasoning (NEUCORE) model which incorporates multi-modal concept alignment and progressive multimodal fusion over aligned concepts. Specifically, considering that text modifier may refer to semantic concepts not existing in the reference image and requiring to be added into the target image, we learn the multi-modal concept alignment between the text modifier and the concatenation of reference and target images, under multiple-instance learning framework with image and sentence level weak supervision. Furthermore, based on aligned concepts, to form discriminative fusion features of the input modalities for accurate target image retrieval, we propose a progressive fusion strategy with unified execution architecture instantiated by the attended language semantic concepts. Our proposed approach is evaluated on three datasets and achieves state-of-the-art results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shu Zhao"
        },
        {
            "affiliations": [],
            "name": "Huijuan Xu"
        }
    ],
    "id": "SP:77ae9b468c841be3500ab8fbe45762130606b0a3",
    "references": [
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein"
            ],
            "title": "Learning to compose neural networks for question answering. In HLT-NAACL, pages 1545\u20131554",
            "venue": "The Association for Computational Linguistics,",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein"
            ],
            "title": "Neural module networks. In CVPR, pages 39\u201348",
            "venue": "IEEE Computer Society,",
            "year": 2016
        },
        {
            "authors": [
                "Muhammad Umer Anwaar",
                "Egor Labintcev",
                "Martin Kleinsteuber"
            ],
            "title": "Compositional learning of image-text query for image retrieval",
            "venue": "In WACV,",
            "year": 2021
        },
        {
            "authors": [
                "Alberto Baldrati",
                "Marco Bertini",
                "Tiberio Uricchio",
                "Alberto Del Bimbo"
            ],
            "title": "Effective conditioned and composed image retrieval combining clip-based features",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Tamara L. Berg",
                "Alexander C. Berg",
                "Jonathan Shih"
            ],
            "title": "Automatic attribute discovery and characterization from noisy web data",
            "venue": "In ECCV (1),",
            "year": 2010
        },
        {
            "authors": [
                "Pranit Chawla",
                "Surgan Jandial",
                "Pinkesh Badjatiya",
                "Ayush Chopra",
                "Mausoom Sarkar",
                "Balaji Krishnamurthy"
            ],
            "title": "Leveraging style and content features for text conditioned image retrieval",
            "venue": "In CVPR Workshops,",
            "year": 2021
        },
        {
            "authors": [
                "Lichang Chen",
                "Guosheng Lin",
                "Shijie Wang",
                "Qingyao Wu"
            ],
            "title": "Graph edit distance reward: Learning to edit scene graph",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Yanbei Chen",
                "Shaogang Gong",
                "Loris Bazzani"
            ],
            "title": "Image search with text feedback by visiolinguistic attention learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Dzmitry Bahdanau",
                "Fethi Bougares",
                "Holger Schwenk",
                "Yoshua Bengio"
            ],
            "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "Ginger Delmas",
                "Rafael Sampaio de Rezende",
                "Gabriela Csurka",
                "Diane Larlus"
            ],
            "title": "ARTEMIS: attention-based retrieval with text-explicit matching and implicit similarity",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2021
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Yichong Xu",
                "Zhe Gan",
                "Jianfeng Wang",
                "Shuohang Wang",
                "Lijuan Wang",
                "Chenguang Zhu",
                "Pengchuan Zhang",
                "Lu Yuan",
                "Nanyun Peng",
                "Zicheng Liu",
                "Michael Zeng"
            ],
            "title": "An empirical study of training end-to-end vision-and-language transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyuan Fang",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Lin Liang",
                "Zhe Gan",
                "Lijuan Wang",
                "Yezhou Yang",
                "Zicheng Liu"
            ],
            "title": "Injecting semantic concepts into end-to-end image captioning",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xinqian Gu",
                "Hong Chang",
                "Bingpeng Ma",
                "Shutao Bai",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "title": "Clotheschanging person re-identification with RGB modality only",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoxiao Guo",
                "Hui Wu",
                "Yu Cheng",
                "Steven Rennie",
                "Gerald Tesauro",
                "Rog\u00e9rio Schmidt Feris"
            ],
            "title": "Dialog-based interactive image retrieval",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Ronghang Hu",
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Kate Saenko"
            ],
            "title": "Learning to reason: End-to-end module networks for visual question answering",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Xun Huang",
                "Serge J. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, pages 1510\u20131519",
            "venue": "IEEE Computer Society,",
            "year": 2017
        },
        {
            "authors": [
                "Maximilian Ilse",
                "Jakub M. Tomczak",
                "Max Welling"
            ],
            "title": "Attention-based deep multiple instance learning",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Huaizu Jiang",
                "Ishan Misra",
                "Marcus Rohrbach",
                "Erik G. Learned-Miller",
                "Xinlei Chen"
            ],
            "title": "In defense of grid features for visual question answering",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens van der Maaten",
                "Judy Hoffman",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross B. Girshick"
            ],
            "title": "Inferring and executing programs for visual reasoning",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Jongseok Kim",
                "Youngjae Yu",
                "Hoeseong Kim",
                "Gunhee Kim"
            ],
            "title": "Dual compositional learning in interactive image retrieval",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Seungmin Lee",
                "Dongwan Kim",
                "Bohyung Han"
            ],
            "title": "Cosmo: Content-style modulation for image retrieval with text feedback",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath R. Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq R. Joty",
                "Caiming Xiong",
                "Steven Chu-Hong Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Haoxuan You",
                "Zhecan Wang",
                "Alireza Zareian",
                "Shih-Fu Chang",
                "Kai-Wei Chang"
            ],
            "title": "Unsupervised vision-and-language pre-training without parallel images and captions. In NAACL-HLT, pages 5339\u20135350",
            "venue": "Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei",
                "Yejin Choi",
                "Jianfeng Gao"
            ],
            "title": "Oscar: Object-semantics aligned pretraining for vision-language tasks",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Zhi Li",
                "Lu He",
                "Huijuan Xu"
            ],
            "title": "Weakly-supervised temporal action detection for fine-grained videos with hierarchical atomic actions",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Yongfei Liu",
                "Chenfei Wu",
                "Shao-Yen Tseng",
                "Vasudev Lal",
                "Xuming He",
                "Nan Duan"
            ],
            "title": "KD-VLP: improving end-to-end vision-and-language pretraining with object knowledge distillation. In NAACL-HLT (Findings), pages 1589\u20131600",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Zheyuan Liu",
                "Cristian Rodriguez Opazo",
                "Damien Teney",
                "Stephen Gould"
            ],
            "title": "Image retrieval on real-life images with pre-trained vision-and-language models",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2019
        },
        {
            "authors": [
                "Haoyu Lu",
                "Nanyi Fei",
                "Yuqi Huo",
                "Yizhao Gao",
                "Zhiwu Lu",
                "Ji-Rong Wen"
            ],
            "title": "COTS: collaborative twostream vision-language pre-training model for cross-modal retrieval",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Zhekun Luo",
                "Shalini Ghosh",
                "Devin Guillory",
                "Keizo Kato",
                "Trevor Darrell",
                "Huijuan Xu"
            ],
            "title": "Disentangled action recognition with knowledge bases. In NAACL-HLT, pages 559\u2013572",
            "venue": "Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Haoyu Ma",
                "Handong Zhao",
                "Zhe Lin",
                "Ajinkya Kale",
                "Zhangyang Wang",
                "Tong Yu",
                "Jiuxiang Gu",
                "Sunav Choudhary",
                "Xiaohui Xie"
            ],
            "title": "EI-CLIP: entity-aware interventional contrastive learning for e-commerce cross-modal retrieval",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaojian Ma",
                "Weili Nie",
                "Zhiding Yu",
                "Huaizu Jiang",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Song-Chun Zhu",
                "Anima Anandkumar"
            ],
            "title": "Relvit: Concept-guided vision transformer for visual relational reasoning",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2022
        },
        {
            "authors": [
                "Jiayuan Mao",
                "Chuang Gan",
                "Pushmeet Kohli",
                "Joshua B. Tenenbaum",
                "Jiajun Wu"
            ],
            "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross B. Girshick",
                "Jian Sun"
            ],
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Tal Ridnik",
                "Emanuel Ben Baruch",
                "Nadav Zamir",
                "Asaf Noy",
                "Itamar Friedman",
                "Matan Protter",
                "Lihi Zelnik-Manor"
            ],
            "title": "Asymmetric loss for multi-label classification",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ramprasaath R. Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal"
            ],
            "title": "LXMERT: learning cross-modality encoder representations from transformers. In EMNLP, pages 5099\u20135110",
            "venue": "Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor S. Lempitsky"
            ],
            "title": "Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Nam Vo",
                "Lu Jiang",
                "Chen Sun",
                "Kevin Murphy",
                "Li-Jia Li",
                "Li Fei-Fei",
                "James Hays"
            ],
            "title": "Composing text and image for image retrieval - an empirical odyssey",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Hui Wu",
                "Yupeng Gao",
                "Xiaoxiao Guo",
                "Ziad Al-Halah",
                "Steven Rennie",
                "Kristen Grauman",
                "Rog\u00e9rio Feris"
            ],
            "title": "Fashion IQ: A new dataset towards retrieving images by natural language feedback",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Hui Wu",
                "Min Wang",
                "Wengang Zhou",
                "Houqiang Li",
                "Qi Tian"
            ],
            "title": "Contextual similarity distillation for asymmetric image retrieval",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Kexin Yi",
                "Jiajun Wu",
                "Chuang Gan",
                "Antonio Torralba",
                "Pushmeet Kohli",
                "Josh Tenenbaum"
            ],
            "title": "Neuralsymbolic VQA: disentangling reasoning from vision and language understanding",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Shu Zhao",
                "Dayan Wu",
                "Wanqian Zhang",
                "Yu Zhou",
                "Bo Li",
                "Weiping Wang"
            ],
            "title": "Asymmetric deep hashing for efficient hash code compression",
            "venue": "In ACM Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Shu Zhao",
                "Dayan Wu",
                "Yucan Zhou",
                "Bo Li",
                "Weiping Wang"
            ],
            "title": "Rescuing deep hashing from dead bits problem",
            "venue": "In IJCAI,",
            "year": 2021
        },
        {
            "authors": [
                "Luowei Zhou",
                "Hamid Palangi",
                "Lei Zhang",
                "Houdong Hu",
                "Jason J. Corso",
                "Jianfeng Gao"
            ],
            "title": "Unified vision-language pre-training for image captioning and VQA",
            "venue": "In AAAI,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Composed image retrieval [Vo et al., 2019, Chen et al., 2020b, Liu et al., 2021, Delmas et al., 2022] aims to identify target image, corresponding to the input query composed of a reference image and a text modifier describing how the reference image should be modified, as illustrated in Figure 1. Compared to traditional image-to-image retrieval task [Gu et al., 2022, Zhao et al., 2020, 2021, Wu et al., 2022] and text-to-image retrieval task [Ma et al., 2022a, Lu et al., 2022] where single modality is provided as input, composed image retrieval is a challenging task as it requires joint vision and language understanding to retrieve the corresponding target image.\nExisting works tackle this problem by directly fusing the multi-modal features after single modality encoding [Liu et al., 2021, Baldrati et al., 2022]. This type of approaches first process input modality as a whole and lack fine-grained multi-modal understanding at concept-level, preventing the model from performing concept-level composition, while most of the time the text modifier specifies partial semantic editing for the reference image. Therefore, we propose a NEUral COncept REasoning (NEUCORE) model to mine and align the visual concepts in reference and target images with the semantic concepts in text modifier, to enhance the semantic consistency between the multi-\nPreprint. Under review.\nar X\niv :2\n31 0.\n01 35\n8v 1\n[ cs\n.C V\n] 2\nO ct\n2 02\nmodal feature composition and the target image feature. Specifically, our NEUCORE model consists of two main components, i.e., multi-modal concept alignment and progressive multi-modal fusion over concepts.\nFirstly, we propose to mine and align visual and semantic concepts under the weak supervision of image-text pair where the ground truth mapping of visual concepts in images and semantic concepts in sentences is unknown, instead of relying on object detectors [Ren et al., 2015] to generate region proposals and object tags to realize multi-modal concept alignment which suffers from the problems of heavy computational load and limited concept label space [Fang et al., 2022]. A multipleinstance learning framework with candidate visual concepts as instance, is designed with practical considerations of concept existence uncertainty, imbalance and noisy optimization, for tackling image-text pair weak supervision. Notably, a text modifier specifies the change to the reference image, and the change operations not only include attribute editing over the existing concepts in reference image, but also involve adding concepts into reference image or removing concepts from the reference image, which will cause the concept existence uncertainty among reference and target images. For example, in Figure 1, the shepherd dog in the text modifier appears in the reference image, but the golden retriever appears in both the reference and target images. To overcome this uncertainty, we concatenate the reference and target image tokens corresponding to candidate visual concepts, and employ a transformer [Vaswani et al., 2017] to jointly encode the reference and target images considering the nice property of patch-level feature encoding in transformer.\nOn top of visual encoding of reference and target images using transformer, the optimization for visual and semantic concept alignment is achieved by attention based multiple instance learning [Ilse et al., 2018] under the supervision of the semantic concepts parsed from the text modifier. Practically, the semantic concepts from each text modifier are represented by the multi-label representation in the concept label space which is constructed by the semantic concepts from all text modifiers in training data. Considering that each text modifier only mentions very limited semantic concepts in each pair of reference and target images compared to the large concept label space, there exists the problem of positive-negative imbalance during the optimization. Besides, the partial semantic editing property of the text modifier for the reference image may cause the situation that, some visual concepts are not referred by the text modifier and mislabeled as negative labels in the multi-label representation, according to the labeling rule that only the semantic concepts mentioned in the text modifier will be set as positive in the concept label space for that example. An asymmetric loss is applied to alleviate the imbalance and mislabeling problems during the concept alignment optimization. After the optimization, the visual tokens are assigned with semantic meaning and aligned with the semantic concepts referred in the text modifier.\nSecondly, after aligning visual and semantic concepts, we are able to fuse multi-modal features at fine granularity instead of holistic visual and text features [Delmas et al., 2022, Liu et al., 2021] for final target image retrieval. A progressive multi-modal fusion module is proposed to gradually fuse the aligned concepts from the reference image and the text modifier in a sequential way with\neach step having its own focus. Progressive multi-modal fusion over concepts involves two subproblems of how to generate the fusion operation sequence, and the specific design for each fusion operation. We propose a unified fusion operation design which can be instantiated by different sequence indicators to realize different fusion operations, through leveraging the advantage that the normalization layer can fuse its own preserved features, obtained from sequence indicators, with input visual features [Ulyanov et al., 2017]. The unified design overcomes the time-consuming drawback of previous hand-crafted fusion designs and removes the dependence on expert knowledge [Andreas et al., 2016b, Mao et al., 2019, Yi et al., 2018]. The fusion operation sequence is sequentially generated by the co-attention of global encoding over individual word embedding in the text modifier, and each sequence step focuses on local semantic concept feature guided by the global semantic context of text modifier. The attended local semantic concept feature is the sequence indicator at each sequence step, and is used to drive the instantiation of the unified fusion operation, taking on the role of a meta-learner. Our proposed fusion sequence generation method is optimized with the final retrieval loss without single step supervision needed as in sequence-to-sentence methods [Hu et al., 2017, Chen et al., 2020a, Mao et al., 2019, Yi et al., 2018, Johnson et al., 2017], and is able to deal with diverse sentences compared to language parser based approaches [Andreas et al., 2016b,a].\nTo summarize our contributions, we introduce a model NEUCORE for composed image retrieval consisting of a multi-modal concept alignment module and a multi-modal fusion module over aligned concepts. Our NEUCORE model learns fine-grained multi-modal concept alignment under image and sentence level weak supervision with actual influencing factors considered. Reference image and text modifier are progressively fused using a unified fusion operation over aligned concepts and under the sequence guidance of attended local semantic concepts, to gain a representation for target image retrieval with more semantic consistency. We validate our proposed model on three datasets. The results show that our method consistently outperforms the state-of-the-art, demonstrating the effectiveness of our approach."
        },
        {
            "heading": "2 Related Work",
            "text": "Composed Image Retrieval. Composed image retrieval task receives a reference image and a text modifier describing how the reference image should be modified to obtain the desired target image. TIRG [Vo et al., 2019] fuses the query image and text into one multi-modal feature vector through a gating and residual mechanism. VAL [Chen et al., 2020b] designs a composite module inserted in multiple layers of the visual encoder to preserve the visual information and modify it according to the text modifier. DCNet [Kim et al., 2021] devises a Correction Network as a regularizer to maintain semantic consistency before and after the feature composition process for better text modification. CoSMo [Lee et al., 2021] disentangles features into content and style by introducing a content modulator and a style modulator, and then composes them with text modifiers to retrieve target images conditioned on both content and style information. CIRPLANT [Liu et al., 2021] employs a large pre-trained vision-language model named OSCAR [Li et al., 2020] to fuse vision and language information by leveraging the rich knowledge in the pre-trained model. ARTEMIS [Delmas et al., 2022] achieves impressive results by decomposing the task into an image-image retrieval task and a text-image retrieval task, employing two attention modules for image-image and text-image, and then fusing these attended features to retrieve target images. However, these previous methods encode the reference image or text modifier into a holistic feature representation, and ignore the fine-grained information in composed image retrieval where the composed and complimentary property between the reference image and text modifier inputs happens. In this paper, our approach moves the multimodal understanding to fine granularity at concept-level, and models the interactions between visual and semantic concepts for composed image retrieval.\nMulti-Modal Concept Alignment. Multi-modal concept alignment aims to align visual concept space with semantic concept space to enable the fine-grained understanding and interaction between visual and semantic concepts [Li et al., 2022, Luo et al., 2022]. A line of methods [Liu et al., 2022, Li et al., 2021b, Zhou et al., 2020, Li et al., 2020, Tan and Bansal, 2019] employ a pre-trained object detector [Ren et al., 2015] to generate region proposals and their object tags as the visual and semantic concepts, and then align them for downstream tasks. However, the object detectors limit the number of concepts as they are typically trained on limited pre-defined object categories. Some works investigate detector-free-based methods that use grid (patch) features [Fang et al., 2022, Jiang et al., 2020, Ma et al., 2022b] to predict visual concepts under weak supervision. In this paper, our\nmodel mines and aligns multi-modal concepts under the setting that the text modifier only contains partial semantic editing property, which is challenging to find correspondence to align concepts for composed image retrieval.\nMulti-Modal Fusion. Multi-modal fusion approaches blend visual and text features for various vision and language tasks. Two categories of methods are typically used for multi-modal fusion. One is concatenating the visual and text tokens together and feeding them into a transformer to get fused features [Zhou et al., 2020, Dou et al., 2022]. The second category of methods is employing cross-attention to exchange and fuse information between visual and text tokens [Tan and Bansal, 2019, Li et al., 2021a].\nMoving forward, to improve the semantic consistency for target image retrieval in composed image retrieval, our method fuses the visual and text features progressively in a sequential way on top of the aligned fine-grained multi-modal concepts. To generate a fusion sequence, some works employ language parsers to extract the sequence from language modality [Andreas et al., 2016b,a]. However, language parsers may not perform well on diverse sentences and lack generalization ability. A second typical method is to learn the sequence from data by training a sequence-to-sequence model to decode the action sequence from the input text [Hu et al., 2017, Chen et al., 2020a, Mao et al., 2019, Yi et al., 2018, Johnson et al., 2017], while these methods typically need sequence level annotation which are annotation expensive. Our method learns the fusion sequence through global query driven co-attention over local semantics without sequence level annotation and ensures the generalization ability, and each fusion step is a unified design with specific instantiation controlled by the meta-learner from the generated fusion sequence, so there is a focus at each step in the progressive fusion process to improve the semantic consistency for target retrieval."
        },
        {
            "heading": "3 Method",
            "text": "Given a reference image Ir and a text modifier T , the composed image retrieval task aims to combine them to identify the target image It. Previous approaches holistically process each input modality and then fusion, and is lack of the fine-grained compositional understanding. In this paper, we propose Neural Concept Reasoning (NEUCORE) to tackle the composed image retrieval by mining and aligning multi-modal concepts, and progressively fusing input modalities over concepts, as illustrated in Figure 2. The explanation of the symbols used in this paper is listed in supplementary material.\nFor feature encoding, the text modifier feature q and contextualized word features t are encoded by a text encoder ET . An image encoder EI is employed to extract reference and target image features,\nand the reference tokens fr and target tokens f t are obtained by flattening encoded visual features: q, t = ET (T ), f r = EI(I r), f t = EI(I t), (1)\nwhere superscripts r and t indicate that a feature belongs to the reference or target image, respectively."
        },
        {
            "heading": "3.1 Multi-Modal Concept Alignment",
            "text": "To model the fine-grained vision and language alignment between reference image and text modifier, we propose to mine and align visual concepts with semantic concepts from image-text pair data, which surpasses previous detector-based methods limited to small pre-defined label space. Due to the lack of concept-level supervision, we extract semantic concepts from text modifiers using a language parser as pseudo labels and apply the pseudo labels as image-level supervision. However, it is still challenging to learn multi-modal concept alignment with pseudo semantic concept labels at image level. Specifically, given a semantic concept, we cannot determine whether the correspondent visual concept appears in reference image or target image. For example, a text modifier \u201cRemove a dog\u201d means a dog concept exists in the reference image, not in the target image. And a text modifier \u201cAdd a cat\u201d denotes a cat concept belonging to the target image, not in the reference image. Formally, given an input (Ir, It, or T ), a concept set contains all concepts in the input, denoted as C(\u00b7). However, we cannot determine c \u2208 C(Ir) or c \u2208 C(It), where c \u2208 C(T ). To resolve this ambiguity, we combine visual tokens of Ir and It to form a larger token set Irt = [Ir, It] and C(T ) \u2282 C(Irt), where [\u00b7, \u00b7] denotes concatenation, considering that a concept c described in T must exist in Ir or It (or both).\nSpecifically, we parse1 semantic concepts from the text modifier and embed each semantic concept wc via GloVe [Pennington et al., 2014] word embedding:\nwc = Embedding(c). (2) where c \u2208 M and M is the concept vocabulary constructed by semantic concepts from all text modifiers.\nThen, we obtain frt by concatenating reference and target tokens, and employ a transformer Trans to exchange context and find correspondence:\nfrt = Trans([fr, f t]). (3)\nAfter modeling relation between reference and target tokens, we adopt a token-wise softmax to acquire attention weights a, and then use the weights to summarize visual tokens to get a visual concept feature frta which contains visual foreground information:\na = Softmax(frt), frta = a \u00b7 frt. (4)\nFinally, multi-modal alignment score s is calculated by the concept features and embeddings of semantic concepts:\ns = frta \u00b7wc. (5)\nHowever, employing vanilla classification loss functions, such as the binary cross-entropy loss function, to optimize the score s leads to poor alignment. Text modifier only explicitly describes partial concepts compared to abundant visual concepts in the pair of input images, denoted as C(Irt) = C(Irt) \u2212 C(T ) and C(T ) \u2acb C(Irt). However, all concepts belonging to C(Irt) are viewed as negative labels, leading to an increased risk of misclassifying visual concepts as background. On the other hand, the number of concept categories |M| is much larger than positive concepts described in each text modifier, i.e., |C(Irt)| \u226a |M \u2212 C(Irt)|, causing high imbalance between positive and negative concept labels in the multi-label representation. As a result, the problems of mislabeling and imbalance hurt the training process, leading to incorrect concept alignment.\nTherefore, we introduce an asymmetric loss [Ridnik et al., 2021] for multi-modal concept alignment to balance the visual and semantic concepts dynamically and discard possibly mislabeled concepts:\ns \u2032 = sigmoid(s)\nLc = \u2212 1\nN \u2211 i\u2208P (1\u2212 s \u2032 i) \u03b2+ log(s \u2032 i) + \u2211 j\u2208N (s \u2032 j) \u03b2\u2212 log(1\u2212 s \u2032 j)  , (6) 1https://spacy.io/\nwhere P and N are the positive and negative set, respectively. \u03b2+ and \u03b2\u2212 are two hyper-parameters that control the degree of focus on positive and negative concepts."
        },
        {
            "heading": "3.2 Progressive Multi-Modal Fusion over Concepts",
            "text": "After obtaining aligned multi-modal concepts, the reference image feature fr and text modifier feature q are progressively fused in a sequential way over aligned concepts to identify the target concept feature f ta. We propose to decompose the sequential fusion steps from the text modifier without step level supervision. Specifically, to focus on distinct semantic contexts of a text modifier, K independent fully connected layers FCi, i = 1, 2, \u00b7 \u00b7 \u00b7 ,K are employed to project text modifier feature q. We adopt the Multi-Head Attention (MHA) [Vaswani et al., 2017] to extract the indicator vector for each semantic fusion step in the fusion sequence S:\nS = (MHA(FC1(q), t, t),MHA(FC2(q), t, t), \u00b7 \u00b7 \u00b7 ,MHA(FCK(q), t, t)) (7)\nTo progressively combine the reference image and text modifier over aligned concepts, we propose to instantiate specific operators from a meta-fusion architecture according to the generated fusion steps, which surpasses previous methods with time-consuming hand-crafted fusion operator design and the limit to expert knowledge. Our basic idea is that fusion steps can be clustered into semantic fusion groups, like \u201cADD\u201d and \u201cREMOVE,\u201d although the expressions within each group may vary in natural language. Therefore, we devise a transformer-based [Vaswani et al., 2017] meta-fusion module MetaFusion and allow it to be instantiated for specific semantic fusion groups. Specifically, we employ a fully connected layer to generate parameters FC(Si) according to fusion steps\u2019 indicators, where i = 1, 2, \u00b7 \u00b7 \u00b7 ,K and Si \u2208 S, and initialize the normalization layers [Huang and Belongie, 2017] in the transformer with these parameters:\n\u00b5i = FC(Si), \u03c3i = FC(Si). (8)\nAfter transformer fusion instantiation, reference image tokens exchange information with aligned concepts in multi-head attention and fuse with text modifier information in the normalization layer:\nf\u0302 \u2032\ni\u22121 = NL(f\u0302i\u22121;\u00b5i, \u03c3i),\nQ,K,V = Wf\u0302 \u2032\ni\u22121,\nf\u0302 \u2032\u2032 i\u22121 = MHA(Q,K,V) + f\u0302 \u2032 i\u22121, f\u0302 \u2032\u2032\u2032 i\u22121 = NL(f\u0302 \u2032\u2032 i\u22121;\u00b5i, \u03c3i),\nf\u0302i = FFN(f\u0302 \u2032\u2032\u2032 i\u22121) + f\u0302 \u2032\u2032\u2032 i\u22121,\n(9)\nwhere NL is a normalization layer; FFN is a feedforward network; W is a weight matrix; f\u03020 = fr. After K fusion steps, we obtain f\u0302K as the modified image feature to match the target images.\nFinally, the concept matching score m can be calculated by:\nm = f\u0302K \u00b7 f ta, (10) where f ta is the visual concept feature of target image.\nTo train our proposed NEUCORE model, given mini-batch data, the model is optimized by the batch-based classification loss, which is demonstrated to be an efficient optimization loss function for the composed image retrieval task in previous approaches [Vo et al., 2019, Lee et al., 2021, Delmas et al., 2022]:\nLm = \u2212 1\nN N\u2211 i=1 log exp{\u03b3m(Iri ,Ti, C(Iri ), Iti)}\u2211 j exp{\u03b3m(Iri ,Ti, C(Iri ), Itj)} , (11)\nwhere \u03b3 is a temperature parameter. m(Iir,T i, C(Iir), I i t) is the matching score (cosine similarity), consisting of the concept matching score which is described in Equation (10) and the context matching score which is described in [Delmas et al., 2022]. Combining with the loss function of concept alignment, the final loss function is: L = Lm + \u03b1Lc, (12) where \u03b1 control the trade-off between the two loss functions.\nThe learning algorithm of our NEUCORE model is summarized in supplementary material."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "To evaluate our NEUCORE model, we extensively conduct experiments on three widely used datasets: Shoes [Guo et al., 2018], FashionIQ[Wu et al., 2021], and CIRR [Liu et al., 2021].\nShoes is constructed from the Attribute Discovery Dataset [Berg et al., 2010]. In [Guo et al., 2018], authors additionally label natural language query sentences for the composed image retrieval task based on attribute labels in the original dataset. The dataset consists of 9k training triplets and 1.7k test queries.\nFashionIQ covers three fashion categories: Dress, Top tee, and Shirt. It contains 46k images for training, and 15k images for validation and testing. There are 18k queries for training, 12k queries for validation, and 12k queries for testing. Each query has two captions describing how to modify from the reference image to the target image.\nCIRR consists of over 36k open-domain images with human-generated text modifier. It is a more challenging dataset due to the richness of visual information and the diversity of language queries. Following [Liu et al., 2021], 36k triplets are split into 80% for training, 10% for validation, and 10% for testing in our experiments."
        },
        {
            "heading": "4.2 Evaluation Protocol",
            "text": "Following [Delmas et al., 2022], we report composed image retrieval performance in Recall within topK (R@K). Particularly, for CIRR dataset, following [Liu et al., 2021], we additionally report Recall within top-K on the visually similar subset (Rs@K), where the subset of candidate target images is visually similar to the correct target image, and it requires the fine-grained understanding ability of both vision and language modalities and their interactions. Following previous works [Delmas et al., 2022], we evaluate NEUCORE model on the test set of CIRR dataset, the validation set of Shoes dataset, and the validation set of FashionIQ dataset."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "We employ ResNet [He et al., 2016] pre-trained on ImageNet as the image encoder. For the text encoder, we adopt BiGRU [Cho et al., 2014] to encode sentence q and obtain the hidden states as contextualized word features t. The concept alignment module consists of 2 transformer layers [Dosovitskiy et al., 2021]. The batch size is 32. Following [Delmas et al., 2022], we freeze the image encoder during the first 8 epochs. Then the model is trained for 50 epochs. We use the AdamW optimizer [Loshchilov and Hutter, 2019] and set the initial learning rate to 5\u00d7 10\u22124 with a decay of 0.5 every 10 epochs. \u03b2+ and \u03b2\u2212 are 1 and 4, respectively. K is 3. \u03b3 is 2.65926. \u03b1 is 200. The model is trained on one NVIDIA RTX A5000 GPU."
        },
        {
            "heading": "4.4 Main Results",
            "text": "We compare the performance of our proposed NEUCORE model with previous SOTA works on three benchmarks.\nResults on CIRR Dataset. Table 1 shows the results. Our proposed NEUCORE model achieves 3.79 average gain compared to the SOTA approach, ARTEMIS. Specifically, NEUCORE model\nimproves 1.5, 3.3, 2.26, and 1.62 in K = 1, K = 2, K = 10, and K = 50 of R@K, respectively. It demonstrates that NEUCORE model can better retrieve target images according to reference images and text modifiers. Moreover, NEUCORE model improves 4.28, 4.86, and 3.25 in K = 1, K = 2, and K = 3 of Rs@K, indicating the recall of top-K on a visual similar subset, which is a more challenging metric and evaluates a model\u2019s fine-grained understanding ability on a visual similar target subset. Under the same pre-training condition without using the vision-language pre-trained weights, denoted as CIRPLANT, our model outperforms CIRPLANT significantly on all metrics. Furthermore, when the CIRPLANT model is initialized with the pre-trained weights from the OSCAR model [Li et al., 2020] trained on 6.5 million image-caption pairs, our model can still outperform it in Rs@1 and Rs@2 by 5.07 and 4.03, respectively. It indicates our proposed multi-modal concept alignment module can effectively mine and align fine-trained multi-modal concepts and the multimodal fusion can fuse the reference image feature and text modifier feature over concepts to identify the target image feature.\nResults on Shoes Dataset. The results are shown in Table 2. Our proposed NEUCORE model improves 1.04, 2.37, and 1.44 in K = 1, K = 10, and K = 50 of Recall compared to the SOTA method ARTEMIS [Delmas et al., 2022] on this dataset. And it achieves 1.62 improvement of average improvement, (R@1 +R@10 +R@50)/3.\nResults on FashionIQ Dataset. Table 3 illustrates the main results. Our proposed NEUCORE model still obtains state-of-the-art results of R@10 and R@50. Due to the limited space, detailed results with more evaluation metrics on the FashionIQ dataset can be found in supplementary material."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "To show the effectiveness of each component of our model design, ablation study for different variants are conducted on the CIRR validation set as CIRR dataset contains more concepts and is better to evaluate the fine-grained understanding ability of models.\nMulti-modal Concept Alignment Module. The ablation study results of the concept alignment module are illustrated in Table 4. \u201cReference Only\u201d and \u201cTarget Only\u201d denote that we do not concatenate the reference and target image tokens and only use reference or target image tokens to align with semantic concepts. The performance decrease of these two variants demonstrates that our concept alignment module with the concatenation of reference and target images can help alleviate the ambiguity problem of corresponding visual concepts, and improve multi-modal concept mining and alignment for composed image retrieval task. \u201cCross-Entropy Loss\u201d means that the asymmetric loss for optimizing the concept alignment described in Equation (6) is replaced with the binary cross-entropy loss. The ablation results show the performance decrease, and confirm the existence of the problems about positive-negative imbalance and mislabeling in multi-label concept classification. Our proposed concept alignment module with asymmetric loss can help alleviate these problems.\nProgressive Multi-Modal Fusion Module. The ablation study results of the progressive multi-modal fusion module are shown in Table 5. It confirms the effectiveness of our progressive multi-modal fusion module with automatic fusion sequence generation and unified fusion module design. \u201cRemove Progressive Fusion module\u201d means that we remove the multi-modal progressive fusion module and directly fuse concatenation of the reference image feature and text modifier feature. The decreased ablation results demonstrate that our proposed fusion module can better identify the target feature by progressively fusing the reference image feature and text modifier feature over aligned concepts with each step having focus. \u201cLayer norm\u201d denote we use vanilla layer normalization instead of adaptive instance normalization without adaptive instantiation. The results show adaptive instance normalization can fuse features better in composed image retrieval task."
        },
        {
            "heading": "4.6 Analysis of Zero-Shot Concepts",
            "text": "Compared with previous approaches, our proposed NEUCORE model mines and aligns visual concepts and semantic concepts. It aligns the visual embedding space and semantic embedding space and transfers knowledge from language to vision, which can improve the zero-shot concept recognition ability. To demonstrate it, we create a data split from CIRR validation set, named CIRRzs. Specifically, we parse the concepts from the training and validation sets. Next, we compute their difference set to obtain zero-shot concepts, i.e., not seen during training time. Then we only keep these samples that contain zero-shot concepts to create zero-shot data split Dzs, resulting in 316 zero-shot concepts and 350 samples.\nResults are reported in Table 6. Compared to the SOTA approach ARTEMIS which fuses holistic multi-modal features for composed image retrieval, NEUCORE model improves 6.57, 7.43, and 4.28 in K = 1, K = 2, and K = 3 of recall within top-K of subset Rs, respectively. Moreover, we also present the results of removing the multi-modal concept alignment module, which resulted in performance drops of 5.72, 9.43, and 6.28 in R@k, respectively. It illustrates the concept alignment module improves the zero-shot concept recognition ability by aligning visual concepts with semantic concepts represented by word embedding, which transfers the knowledge from word embedding to visual concepts."
        },
        {
            "heading": "4.7 Qualitative Results",
            "text": "Figure 3 shows the retrieval examples from a restricted subset of the CIRR validation set Liu et al. [2021], where candidate target images are visually similar. It requires learning fine-grained vision and language features and their interactions. The results show our proposed NEUCORE model can understand the content of text modifier and compose the reference image feature and text modifier feature to identify the target image feature."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a model named NEUCORE to tackle the composed image retrieval task, which consists of multi-modal concept alignment module and progressive multi-modal fusion module. Multi-modal concept alignment module mines and aligns visual concepts from images with semantic concepts from text modifiers, and the progressive multi-modal fusion module progressively fuses the reference image feature with the text modifier feature over aligned concepts to identify the target image feature. Extensive experiments demonstrate our proposed NEUCORE model learns fine-grained multi-modal alignment and their interactions at concept-level from image-text paired data."
        },
        {
            "heading": "A List of Symbols",
            "text": "The list of symbols and notations used in this paper is shown in Table 7."
        },
        {
            "heading": "B Concept Source",
            "text": "Pseudo concept labels are extracted according to part-of-speech. To evaluate the effectiveness of different concept types, we extract \u201cNoun,\u201d \u201cAdj,\u201d, \u201cVerb,\u201d and \u201cAdv\u201d from the CIRR validation set and combine them as pseudo labels. The results are shown in Table 8. It demonstrates \u201cNoun + Adj + Verb + Adv\u201d achieves the best result, indicating that the number of concepts may affect the performance. More concepts help the model learn richer features."
        },
        {
            "heading": "C Detailed Results on Fashion IQ dataset",
            "text": "Table 9 illustrates the detailed results on Fashion IQ validation set. It demonstrates that our proposed NEUCORE model can outperform the SOTA method ARTEMIS in most of the metrics. This also validates that progressive fusion with aligned multi-modal concept alignment can improve the composed image retrieval task."
        },
        {
            "heading": "D Qualitative Results",
            "text": "We provide more qualitative retrieval examples from a restricted subset of the CIRR validation set [Liu et al., 2021] where candidate target images are visually similar. It is challenging because the model needs to learn fine-grained vision and language features and their interactions. The retrieval examples are shown in Figure 4. Results demonstrate our NEUCORE model can understand the\ncontent of text modifier, find correct correspondence between visual concepts and semantic concepts, and compose the reference image feature and text modifier feature to identify the target image feature.\nE Visualization of Concept Alignment\nOur NEUCORE model can mine and align visual concepts with semantic concepts. We employ Grad-CAM [Selvaraju et al., 2017] to identify visual concepts corresponding to semantic concepts on CIRR dataset. The correct visualization results are shown in Figure 5. It demonstrates that our NEUCORE model can align semantic concepts to visual concepts in images under image level weak supervision. We also show some failure visualization cases in Figure 6, where these semantic concepts are describing more abstract visual concepts. We expect that using more advanced vision and language pre-trained models can help alleviate these failure cases.\nClover Boundary"
        },
        {
            "heading": "F List of Zero-shot Concepts on CIRR_zs Dataset",
            "text": "To demonstrate our NEUCORE model can deal with novel zero-shot concepts, we create a data split from CIRR validation set, named CIRR_zs. The zero-shot concepts in CIRR_zs are listed as follows:\nplus, winglike, unicorn, lounger, camisole, wound, gentle, ibex, hinged, silicone, vary, jajantic, crayon, zone, servicing, rollaway, sorted, description, ended, secondhand, rop, softie, winner, makw, furnished, birn, moblie, hospital, orient, gose, bulk, cum, whote, pilot, hyppopotamus, sharo, simillar, thread, aniamls, celebration, committee, freestyle, scubadiver, decorated, counch, help, dia, fatter, goldtone, shovel, earing, huddled, florescent, handy, aggressive, sis, undecipherable, attitude, asembled, settle, celebrity, vietnamise, clapping, suv, cologne, wax, prepared, law, unrisen, boklane, screwlike, carcas, treet, monk, handleless, repaired, atheletic, biting, commercial, shrimp, entree, ticker, graze, intense, portait, buddy, swin, engineer, chemistry, unobvious, avenue, seashelle, gummie, misssig, empanada, puzzle, hazelnut, driverside, maze, seller, gnu, another, anmial, gummy, knob, mounted, thermal, furth, seagal, handing, wolflike, clap, barnlike, gape, clover, medusa, under, siringe, vegatation, raelistic, outrigger, cinnamon, brwon, screwtop, rocksurface, snorkeler, creative, wingspan, coastal, innocent, ostrich, volture, eatm, gril, cyclist, on, aless, pinecone, comedy, blueprint, spaghetti, inverted, fried, lea, treeline, unmanned, sweatshirt, swirly, silvertone, lengthy, coverge, attenae, multipack, stocking, hypopotamus, mosque, continental, baggage, stickering, core, silve, hollowed, swimwear, noodle, eyeliner, sphinx, multilayer, morevie, abdoman, hartebe, thumbnail, bouqet, boundary, glide, palican, boklaine, trianglular, steer, orthodox, assembly, canister, terrestrial, variable, spoonful, value, hyppopotomus, liner, wardrove, blackc, ribbontail, diameter, multimeter, sculture, droopy, hi, cachrro, removed, prayer, scent, italian, trimming, coulple, zest, calimari, jogger, frock, healthier, box(es, rabie, supine, sape, mkaing, dungeness, cubicle, last, unrenovated, miror, patterened, dool, pallet, femal, denser, mongoose, pelikan, mottled, mechanic, 50ml, affectionate, wintery, pitchet, cuttingboard, starbuck, thong, churchlike, handler, technology, hyypopotomus, bib, wipe, swollen, tong, hawaiian, lingerie, tupperware, hexagon, flatcap, omlette, makin, direciton, pearl, princess, hooded, fa\u00e7ade, playin, skincare, smaler, screwdriver, spill, safe, throny, gopher, furier, embrace, luminous, choir, heard, taupe, bouse, unbrand, tentacled, carrier, gutted, oral, mouthed, squid, active, triney, landscaping, coffe, state, visibility, tortoise, buffet, erotic, bloody, barking, puppeis, account, risen, peper, anima, sandwedge, mold, guide, religious, largre, panty, sipper, differnt, chubbier, rwmove, welcoming, countryard, tattoed, compression, interacting, dove, veggie, descriptive, feminine, highway, orientation, bride, potote, disc, rocklike."
        },
        {
            "heading": "G Algorithm for Learning the NEUCORE model",
            "text": "The overall learning algorithm for our proposed NEUCORE model is illustrated in Algorithm 1."
        },
        {
            "heading": "H Limitations and Future Work",
            "text": "Our proposed model, NEUCORE, can mine and align multi-modal concepts without concept-level supervision. However, the improvement in Fashion IQ is relatively small than CIRR. It is mainly because Fashion IQ contains domain-specific concepts, like \u201csuede\u201d and \u201cbely.\u201d CIRR is more diverse and covers more concepts. Recently, large vision-language (VL) models have achieved significant progress. Our model does not employ these VL models currently and only focuses on model side,\nAlgorithm 1 The overall learning algorithm for the NEUCORE model. Input: Reference image Ir; Target image It; Text modifier T . Output: Matching score m; Parameters of the NEUCORE model.\n1: Obtain semantic concepts C(T ) from a language parser. 2: repeat 3: # Encode vision and language features 4: Extract reference visual tokens fr, target visual tokens f t, contextualized word features t, and sentence feature q by Equation (1). 5: 6: # Multi-modal concept Alignment. Section 3.1 7: Obtain semantic concept word embeddings wc by Equation (2). 8: Concatenate reference and target visual tokens to get concatenated tokens [fr, fr] and feed them to transformer layer to exchange their information and obtain frt by Equation (3). 9: Apply a token-wise softmax operation for concatenated tokens frt to get attention weights\na and weighted summary concatenated tokens frt according to the attention weights a by Equation (4).\n10: Calculate the multi-modal alignment score s by Equation (5). 11: 12: # Progressive multi-modal fusion over concepts. Section 3.2 13: Generate fusion sequence S by Equation (7). 14: Progressively fuse the reference image tokens fr and the text modifier feature stored in fusion steps S over concepts wc by Equation (8). 15: Calculate the concept matching score m by Equation (9). 16: 17: # Loss function 18: Calculate the multi-modal concept alignment loss value Lc by Equation (6). 19: Calculate the matching loss value Lm by Equation (10). 20: Calculate the final loss value by Equation (11) and optimize it by BP algorithm. 21: until Convergence or reach maximum iterations.\nbut potentially these VL models could help our model learn more domain-specific concepts. On the other hand, we decompose the text modifier to generate a fusion sequence and progressively fuse the reference image feature and text modifier feature over aligned multi-modal concepts. Large language models (LLM) can also be utilized to generate a more accurate fusion sequence."
        }
    ],
    "title": "NEUCORE: Neural Concept Reasoning for Composed Image Retrieval",
    "year": 2023
}