{
    "abstractText": "In order to accurately identify the high voltage transmission line fittings and guide the line patrol robot to achieve the corresponding obstacle-crossing action according to different fittings, a real-time detection network based on deep learning DeblurGANv2 and YOLOv5 target detection algorithm was proposed. In view of the problems of large network model, large amount of computation and low operation efficiency of YOLOv5 algorithm, the lightweight improvement of YOLOv5 algorithm was carried out to improve the detection speed of the algorithm. Shufflenetv2 was used to replace CSPDarknet-53 in the original network, and ECA module was introduced into Shufflenetv2.Enhance the model's focus on valid features. At the same time, DeblurGANv2 super-resolution reconstruction algorithm is introduced to deblur the image of the fittings to generate high-quality images and improve the recognition accuracy. Experimental results based on homemade datasets show that,compared with the original YOLOv5 model, the parameter amount of the proposed method is reduced by 50.2%, the weight file size is only 5.3M, a reduction of 62.7%, and mAP@ (0.5) is increased by 1.3%. The proposed method can successfully detect the high voltage transmission line fittings and ensure the accuracy of fuzzy target detection. INDEX TERMS Fittings recognition, Image deblurring, YOLOv5",
    "authors": [
        {
            "affiliations": [],
            "name": "GUANGQING CHEN"
        },
        {
            "affiliations": [],
            "name": "SHIKAI WANG"
        },
        {
            "affiliations": [],
            "name": "YONGKANG LIU"
        },
        {
            "affiliations": [],
            "name": "GAOBIN QIN"
        },
        {
            "affiliations": [],
            "name": "JIDAI"
        },
        {
            "affiliations": [],
            "name": "AIQIN SUN"
        },
        {
            "affiliations": [],
            "name": "LIANG YUAN"
        }
    ],
    "id": "SP:86bfb828aede574f43a24be8ced321451e41691a",
    "references": [
        {
            "authors": [
                "China Electricity Council"
            ],
            "title": "Annual Development Report of China's Power Industry 2020",
            "venue": "2021",
            "year": 2021
        },
        {
            "authors": [
                "M.J. Bharata Reddy",
                "B.K. Chandra",
                "D.K. Mohanta"
            ],
            "title": "A DOST based approach for the condition monitoring of 11 kV distribution line insulators",
            "venue": "IEEE Transactions on Dielectrics and Electrical Insulation, vol. 18, no. 2, pp. 588-595, April 2011, doi: 10.1109/TDEI.2011.5739465.",
            "year": 2011
        },
        {
            "authors": [
                "T. Wang",
                "C. Yao",
                "J Liu",
                "L.Z. Chao",
                "G.H. Shao",
                "H.G. Qun",
                "Y.J. Ming"
            ],
            "title": "Research on automatic identification of substation circuit breaker based on shape priors",
            "venue": "Applied Mechanics and Materials, Kunming, China. , Vol. 511, pp. 923-926, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "C.-Y. Fu"
            ],
            "title": "SSD: Single shot multibox detector",
            "venue": "Proc. Eur. Conf. Comput. Vis.,",
            "year": 2016
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 779\u2013788.",
            "year": 2016
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Yolo9000: Better, faster, stronger",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 7263\u20137271.Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C]. Conference on Computer Vision and Pattern Recognition, 2017: 7263-7271.",
            "year": 2017
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "YOLOv3: An incremental improvement",
            "venue": "2018, arXiv:1804.02767. [Online]. Available: http://arxiv.org/abs/1804.02767",
            "year": 2018
        },
        {
            "authors": [
                "A. Bochkovskiy",
                "C.-Y. Wang",
                "H.-Y.M. Liao"
            ],
            "title": "YOLOv4: Optimal speed and accuracy of object detection",
            "venue": "2020, arXiv:2004.10934. [Online]. Available: http://arxiv.org/abs/2004.10934",
            "year": 2020
        },
        {
            "authors": [
                "C.C. Yu"
            ],
            "title": "Design of vision system for power tower climbing robot",
            "venue": "M.S. thesis, Southwest Jiaotong University. Chengdu,",
            "year": 2020
        },
        {
            "authors": [
                "R. S Li",
                "Y.L. Zhang",
                "D.H. Zhai",
                "D. Xu"
            ],
            "title": "Transmission Line Pin defect detection based on Improved SSD",
            "venue": ". High Voltage Technology, vol. 47, no. 11, pp. 3795-3802, Apr. 2021, doi:10.13336/j.1003- 6520.hve.20201650.",
            "year": 2021
        },
        {
            "authors": [
                "R. Girshick",
                "J. Donahue",
                "T. Darrell",
                "J. Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2014, pp. 580-587.",
            "year": 2014
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast R-CNN",
            "venue": "Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 1440\u20131448.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards realtime object detection with region proposal networks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137\u20131149, Jun. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Liu",
                "H. Jiang",
                "J. Chen",
                "J. Chen",
                "S. Zhuang",
                "X. Miao"
            ],
            "title": "Insulator Detection in Aerial Images Based on Faster Regions with Convolutional Neural Network",
            "venue": "2018 IEEE 14th ICCA, Anchorage, Alaska, USA, 2018, pp. 1082-1086",
            "year": 2018
        },
        {
            "authors": [
                "Fu"
            ],
            "title": "Research on power line safety detection algorithm based on cascading deep learning",
            "venue": "M.S. thesis. Sichuan University, Chengdu,",
            "year": 2021
        },
        {
            "authors": [
                "Y.M. Ma",
                "Y. Zhang"
            ],
            "title": "Insulator Detection algorithm based on Improved Faster-RCNN",
            "venue": "Journal of Computer Applications, vol. 42, no. 2, pp. 631-637, Sep. 2022, doi: 10.11772/j.issn.1001- 9081.2021020342",
            "year": 2022
        },
        {
            "authors": [
                "W C.-Y. Wang",
                "H.-Y.M. Liao",
                "Y.-H. Wu",
                "P.-Y. Chen",
                "J.-W. Hsieh",
                "I.-H. Yeh"
            ],
            "title": "CSPNet: A new backbone that can enhance learning capability of CNN",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) Workshops, Jun. 2020, pp. 390\u2013391.",
            "year": 2020
        },
        {
            "authors": [
                "T.-Y. Lin",
                "P. Doll\u00e1r",
                "R. Girshick",
                "K. He",
                "B. Hariharan",
                "S. Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 936\u2013944.",
            "year": 2017
        },
        {
            "authors": [
                "H Li",
                "J P Xiong"
            ],
            "title": "An et al., \"Pyramid Attention Network for Semantic Segmentation[J]\", 2018",
            "year": 2018
        },
        {
            "authors": [
                "O. Kupyn",
                "V. Budzan",
                "M. Mykhailych",
                "D. Mishkin",
                "J. Matas"
            ],
            "title": "DeblurGAN: Blind motion deblurring using conditional adversarial networks",
            "venue": "Proc. IEEE Conf. Comput. Vision Pattern Recognit.,",
            "year": 2018
        },
        {
            "authors": [
                "M.F. Liang",
                "L. Li"
            ],
            "title": "Enhanced YOLOv3 fuzzy Object Detection Based on adversarial Neural Network",
            "venue": "Computer Applications and Software, vol. 38, no. 10, pp. 221-228, Oct. 2021, doi: 10.3969/j.issn.1000- 386x.2021.10.035",
            "year": 2021
        },
        {
            "authors": [
                "M Sandler",
                "A howard",
                "M Zhu",
                "et"
            ],
            "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "C. Li",
                "M. Wand"
            ],
            "title": "Precomputed real-time texture synthesis with markovian generative adversarial networks,",
            "year": 2016
        },
        {
            "authors": [
                "N. Ma",
                "X. Zhang",
                "H.T. Zheng",
                "J. Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "European Conference on Computer Vision, Munich,2018,pp. 116-131.",
            "year": 2018
        },
        {
            "authors": [
                "R. Prajit",
                "B. Zoph",
                "Q.V. Le"
            ],
            "title": "Searching for activation functions",
            "venue": "arXiv preprint arXiv 1710.05941,",
            "year": 2017
        },
        {
            "authors": [
                "A. Howard"
            ],
            "title": "Searching for MobileNetV3",
            "venue": "2019 IEEE/CVF International Conference on Computer Vision, Seoul, Korea, 2019, pp. 1314-1324.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Niu",
                "G. Zhong",
                "H. Yu"
            ],
            "title": "A review on the attention mechanism of deep learning",
            "venue": "Neurocomputing,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\npatrol robot to achieve the corresponding obstacle-crossing action according to different fittings, a real-time detection network based on deep learning DeblurGANv2 and YOLOv5 target detection algorithm was proposed. In view of the problems of large network model, large amount of computation and low operation efficiency of YOLOv5 algorithm, the lightweight improvement of YOLOv5 algorithm was carried out to improve the detection speed of the algorithm. Shufflenetv2 was used to replace CSPDarknet-53 in the original network, and ECA module was introduced into Shufflenetv2.Enhance the model's focus on valid features. At the same time, DeblurGANv2 super-resolution reconstruction algorithm is introduced to deblur the image of the fittings to generate high-quality images and improve the recognition accuracy. Experimental results based on homemade datasets show that,compared with the original YOLOv5 model, the parameter amount of the proposed method is reduced by 50.2%, the weight file size is only 5.3M, a reduction of 62.7%, and mAP@ (0.5) is increased by 1.3%. The proposed method can successfully detect the high voltage transmission line fittings and ensure the accuracy of fuzzy target detection.\nINDEX TERMS Fittings recognition, Image deblurring, YOLOv5\nI. INTRODUCTION The stability of the power supply system is crucial since the electricity sector is a cornerstone of national development and construction[1]. Electricity is mostly transported across regions by overhead high-voltage transmission lines, whose stability and safety are crucial to ensuring the power system is continuously operational. One of the most effective methods of intelligent inspection now in use, which can successfully address the issues of manual inspection's poor efficiency, high labor intensity, and high risk, is the deployment of inspection robots[2,3]. In the inspection process of the inspection robot, the inspection robot is equipped with sensors and cameras to navigate in the transmission line, identify the anti-shock hammers, spacer bars, dangling wire clamps and other gold tools on the line, and then execute the corresponding crossing procedures to conduct online inspection to discover obstacles and collect data for offline inspection.\nWith the continuous deepening and optimization of convolutional neural network on the convolutional layer, deep learning algorithms based on classification and object detection have also been applied more deeply and widely in hardware detection. The deep learning algorithm mainly extracts and learns the features of the obstacle target through the backbone neural network layer, that is, the\nextracted feature information is not artificially limited, but independently resolves various details of interest in the target obstacle. Deep learning-based target detection algorithms can be broadly classified into two categories: single-stage algorithms and two-stage algorithms. Among them, representative algorithms of single-stage algorithms include Single Shot Multibox Detector (SSD)[4] and You Only Look Once(YOLO)[5-8] series, etc. Canyu Cai[9] used Mobilenet to replace YOLOv3's backbone feature extraction network Darknet-53 and further optimized the original a priori box clustering method using K-means clustering algorithm, and the recognition accuracy of foot nails could reach more than 95%, Ruisheng Li et al[10] proposed an improved SSD model based on residual network and multi-level feature fusion strategy, introduced the Resnet50 residual network to replace the original feature extraction network VGG16, expanded the sensory field while retaining more shallow semantic information, and improved the performance of small target detection, and achieved a transmission line pin defect detection recall rate of more than 82.7%. Another category is two-stage detection algorithms, and representative algorithms include R-CNN[11], Fast R-CNN[12], Faster R-CNN[13], etc. Liu et al[14] proposed to combine Region Proposal Networks(RPN) and Fast R-CNN for training, and to form\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\na unified network sharing convolutional layers , to implement a unified insulator detection network based on Faster R-CNN, achieving 94% accuracy for insulator identification. The algorithm uses the RPN network to generate detection frames directly on the input feature map, but because the RPN network generates as many as 2000 candidate frames, it is easy to cause the missed detection of the target, and the original Region of interest(RoI) Pooling of the Faster R-CNN causes the loss of accuracy by the two rounding operations. Based on the above shortcomings, Lei Fu and Yao-Ming Ma et al. proposed a series of improvements to the algorithm such as candidate frame constraint, feature extraction module replacement, and addition of attention mechanism module to further improve the accuracy of detecting the gold tool[15,16].\nWith the continuous optimization and development of deep learning algorithms, it can provide more intelligent strategies and efficient ways for circuit detection tasks. Compared with traditional image processing algorithms and machine learning algorithms, the deep learning algorithm can maximize its strong feature expression ability and quickly and accurately detect objects to be detected from complex scenes with the help of current GPU devices. Of course, the model of deep learning algorithm usually has a large number of parameters and a large amount of computation, so there are still some problems when it is deployed on the line patrol robot for offline operation. Based on the above problems, we have made the following improvements\uff1a 1)We propose an improved detection model of transmission line hardware based on YOLOv5, which is improved in three aspects on the basis of the original YOLOv5s network(The YOLOv5 networks mentioned below are all yolov5s networks). Firstly, a better and more excellent lightweight neural network ShufflenetV2 is used as the feature extraction network, which reduces the\nparameters and computational complexity of the network model. Secondly, the H-Swish activation function was used to replace the Leaky ReLU function in the Neck part and the trunk part to further improve the model detection accuracy. Then, ECA module is added to the backbone network, which can make the network more effective in feature extraction.\n2)The DeblurGANv2 super-resolution reconstruction algorithm is introduced to reconstruct the input image and make the inspection image clear, so as to solve the problem of motion blur in the image collected by the inspection robot due to factors such as wind swing and body vibration, and improve the accuracy of detection.\n3) In this paper, a large number of experiments are carried out with self-built data sets to verify the detection performance of the proposed model. The experimental results show that our method has strong robustness, and the accuracy and speed of the detector can meet the actual requirements.\nThe rest of this paper is organized as follows. Section II introduces the YOLOv5 model and the model of DeblurGANv2. Section III introduces the improved model of YOLOv5. Section IV describes the experiments and analysis of the results. Section V concludes the paper and gives an outlook."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "YOLO is a popular computer vision algorithm for real-time object detection and recognition. It uses convolutional neural networks to perform object detection and classification simultaneously. Compared with traditional object detection algorithms, YOLO has faster speed and higher accuracy. Super-resolution reconstruction based on deep learning aims to design the corresponding convolutional neural network for nonlinear transformation to extract high-level abstract features of the image, and train a deep learning model that\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\ncan obtain the mapping relationship between low-resolution images and high-resolution images.\nA. YOLOv5\nDefine The design of the overall network structure of YOLOv5 still continues the scheme of YOLOv3, divided into four parts: Input, Backbone, Neck, and Head. Compared with YOLOv4, YOLOv5 adds the Focus module and cross stage partial network(CSPNet)[17], and Fig 1 shows the network structure of the YOLOv5 model.\n(1) Inputs. To enhance the robustness of the model, YOLOv5 employs mosaic data enhancement, adaptive image scaling, and adaptive anchor frame calculation. The mosaic data enhancement stitches four images by flipping, scaling, and random arrangement, as a way to enrich the background of detected objects and improve the recognition of small targets. Adaptive anchor frame calculation compares the output predicted frame with the real frame based on the initial anchor frame, calculates the difference between them, and iterates the network parameters in reverse to obtain the most suitable anchor frame value.\n(2) Backbone network. The backbone network of YOLOv5 consists of the Focus and CSP modules. The Focus module is used in the front-end of Backbone to slice the input images from width W and height H, and to merge the channels of the low-resolution feature maps obtained after the slicing process. the CSP module draws on the design idea of cross-stage local networks to reduce the model parameters from the perspective of network construction, thus increasing the inference speed while ensuring the model accuracy.\n(3) Neck. The feature pyramid structure of Feature Pyramid Networks[18] and Path Aggregation Network[19] is used in the neck part of YOLOv5 to further strengthen the scale invariance of the model. The double-tower feature fusion structure can effectively aggregate important information between multiple layers.\n(4) Output. YOLOv5 uses multi-scale prediction to output three feature maps of different sizes to detect three types of targets, small, medium and large, respectively. each grid of the feature map corresponds to three prediction frames, and the object position is predicted using the prediction frame offset, while the probability that the target belongs to each category is output.\nB. DeblurGANv2\nThe schematic diagram of DeblurGANv2 is shown in Fig 3, and the network structure of DeblurGANv2 mainly consists of two parts: the fuzzy generator and the discriminator[20].\nImage BI is used as the fuzzy image input and its fuzzy information is unknown. By building a generative adversarial\nnetwork with feedback loops, generator G\nG can generate\npseudo-images similar to the original clear image to continuously improve the discriminatory ability of\ndiscriminator D\nD during the training process, while\ndiscriminator D\nD can continuously feed the discriminatory\nresults to the generator, which in turn can guide the generator's adjustment to generate a clear image that satisfies the task[21]. During the training process, the game confrontation is continuously played and complemented with each other, and the respective capabilities are enhanced simultaneously, and the optimization of the generative adversarial network is completed when the generator finally simulates the construction of a clear pseudo-image SI that is not distinguished from the original clear image by the discriminator.\nDeblurGANv2 generator network is divided into three main parts, as shown in Fig. 4, DeblurGANv2 chooses MobileNetv2[22] as the backbone network for feature extraction, replaces the normal convolution with depthseparable convolution, uses the FPN network structure for feature fusion in the middle part, and fuses the middle output layer using upsampling operation in the head. After the generator network processing, the obtained feature maps are summed pixel by pixel with the original blurred map to output a clear reconstructed image. The discriminator adopts two-\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\nscale discriminator to discriminate the reconstructed image, including the global scale discriminator and the local scale discriminator, both of which take Markovian Patch discriminator(PatchGAN) as the basic network architecture[23]. PatchGAN network can be used to obtain the overall pixel distribution of the image, effectively learn the complete spatial context information of the image, and realize the identification of the generated image."
        },
        {
            "heading": "III. METHODOLOGY",
            "text": "The vision inspection system designed in this paper is mainly applied to the identification and detection of transmission line fixtures of the inspection robot, but the inspection robot is equipped with an embedded vision processing platform with limited performance, and YOLOv5 has the problem of slow detection speed in the embedded vision processing platform, which is difficult to meet the real-time detection requirements. The backbone network is an important part of the model, and the backbone network model of YOLOv5 is more complex and has a large number of parameters. Therefore, in order to ensure that the inspection robot obtains better real-time detection performance while meeting the recognition rate, this paper makes targeted improvements to the YOLOv5 target detection algorithm from the perspective of model lightweighting to improve the detection speed of the model for the target.\nA. Backbone network optimization\nConsider adjusting the lightweight model, introducing a lightweight backbone network for mobile with stronger feature extraction capability, and replacing the backbone network CSPDarkNet53 of YOLOv5 with the lightweight backbone network ShuffleNetv2[24] in an attempt to achieve a coordinated balance of lightweight, accuracy, and efficiency. The core module of the huffleNetv2 network is the ShuffleNetv2 unit, whose structure is shown in Fig 5(a). In the basic unit a, the input feature maps are equally divided by channel. In the main branch, the input feature map is replaced by Group Convolution operation using 1\u00d71 ordinary convolution operation, and then the feature extraction process of the main branch is completed by 3\u00d73 DW Conv and then 1\u00d71 ordinary convolution operation; the bypass branch on the left side borrows the residual network structure to do constant mapping through Shortcut jump connection. Then the two branches are Concat by channel, and then Channel Shuffle is performed to obtain the output feature map. The basic unit b no longer divides the input image into channels, but adds a 3\u00d73 deep convolution operation and a 1\u00d71 normal convolution operation to the left branch, and then performs a Concat cascade operation to multiply the number of output channels and effectively downsample the feature map, thus reducing the amount of network computation.\nB. Using H-swish to activate the function\nChoosing the appropriate activation function allows the neck network to extract features better. the Swish activation function[25] leads to a significant improvement in the performance of the neural network compared to ReLU, as shown in (1), (2).\n6( ) ( ( ,0),6)ReLU x min max x= (1)\n( ) ( ) 1 e\nx Swish x x sigmoid x\ne\u2212 =  = + (2)\nHowever, Swish uses the Sigmoid function for computation,\nwhich is computationally expensive and its advantage is not obvious when applied to shallower network layers. the HSwish activation function[26] replaces the Sigmoid function with the less computationally intensive ReLU6, as shown in (3).\n6( 3)\n- ( ) 6\nReLU x H Swish x x + =  (3)\nThe H-Swish activation function not only solves the problem of \"necrosis\" of neurons in which the gradient of the Leaky ReLU function disappears at the zero point, but also significantly reduces the computational complexity compared with the Swish function, which helps to improve the inference speed. Therefore, the H-Swish activation function is used to replace the Leaky ReLU function after each convolutional layer in the Neck part and the backbone part.\nC. Introducing attention mechanism\nThe attention mechanism is similar to the human perceptual process, which allows the model to selectively focus on some information and is a resource allocation scheme for the model that can effectively address information overload and process more important information with effective computational\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\nresources[27]. Applying the attention mechanism to neural networks allows the network model to target and filter out the valuable features in the network layer when dealing with a large amount of complex feature information, and therefore the ECA attention module is introduced in this paper.\nThe ECA[28] attention mechanism is a lightweight attention mechanism based on the improved Squeeze-andExcitation Networks(SE) attention mechanism. The ECA module first pools each channel globally on average individually, and then replaces it with a fast 1D convolutional layer of number k fully connected layers to efficiently capture the information of local cross-channel interactions. Finally, a Sigmoid function is used to generate channel weights. the structure of the ECA module is shown in Fig 6.\nThe ShuffleNetv2 module is improved to address the problem of high computational power of the YOLOv5 algorithm, and it is applied to the YOLOv5 backbone network. Then the lightweight and efficient ECA channel attention module is added. To minimize the number of parameters and complexity, the ECA attention mechanism is only added and fused in the native ShuffleNetv2 module, and no other parts of YOLOv5 are added, and the design method of ShuffleNetv2ECA (SHE) unit module is proposed, and the basic unit SHE1 is shown in Fig 7(a), and the downsampling unit SHE2 is shown in Fig 7(b). The improved ShuffleNetv2-ECA unit module is not only more powerful in feature extraction, but also the added ECA attention module involves only a small number of parameters, which does not significantly increase\nthe number of channels and size of the input feature map, and the memory overhead is negligible, ensuring the original efficient performance.\nChannel Split\n1x1 Conv\n3x3 DWConv (stride =1)\n1x1 Conv\nConcat\nChannel Shuffle\nBN H-Swish\nBN\nBN H-Swish\n1x1 Conv\n3x3 DWConv (stride =2)\n1x1 Conv\n3x3 DWConv (stride =2)\n1x1 Conv\nConcat\nChannel Shuffle\nBN\nBN H-Swish\nBN H-Swish\nBN\nBN H-Swish\nECA ECA\nX Output\nX Input Input X\nX Output\n(a) ShuffleNetv2-ECA basic unit (b) ShuffleNetv2-ECA downsampling unit\nFIGURE 7. Shufflenetv2-ECA improved module structure, (a) Shufflenet V2-ECA basic unit; (b)Shufflenetv2-eca downsampling unit.\nD. Improved network structure\nIn the lightweight design of the YOLOv5 network architecture, this paper considers that although the Focus module can integrate spatial information into the channel, its process of frequent slicing operations about the pixels and sub-pixels of the feature map will increase excessive memory consumption, which is not in line with the lightweight design principle, so the Focus module is removed. Secondly, the main backbone layer refers to the native ShuffleNetv2 1x network architecture, and the Backbone is built by stacking ShuffleNetv2-ECA units alternately, the SHE1 module can efficiently perform feature extraction while ensuring the same number of input and output channels, and the SHE2 module can effectively realize the downsampling of the input feature map. sampling. The native ShuffleNetv2 units are repeatedly stacked in ratios of 1:3, 1:7, and 1:3, and the overall structure of the improved YOLOv5Shuffle network model is shown in Fig. 8.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\nE. Detection Image Improvement\nImage samples as the basis of model detection, clear image samples contain good features such as texture details and contour information of the target to be detected, from which the metal tool target detection model can learn rich finegrained features to obtain better target detection accuracy. During the operation of the inspection robot, the camera often shakes due to various factors, resulting in motion blur in the captured images. The unprocessed motion blurred images not only have poor recognition effect, but also tend to slow down the recognition speed. Therefore, in order to obtain clear highvoltage inspection images as much as possible, this section introduces the DeblurGANv2 super-resolution reconstruction algorithm for blind deblurring of the metallic tool images, aiming to overcome the problems of blurred imaging images, low quality and insignificant regions of interest due to the limitations of the image acquisition system or the acquisition environment itself.\nThe blurred distortion of the image is an important factor affecting the target detection, and it is crucial to judge the sharpness of the image. To accomplish the distinction between clear and non-clear images, this paper uses the Tenengrad algorithm based on image gradient for the evaluation of reference-free images.The Tenengrad algorithm uses the Sobel operator to extract the horizontal and vertical gradients of image pixels, calculates the gradient values of pixel points, and then evaluates the distinction according to the Tenengrad algorithm. Let the convolution kernels of the Sobel operator in\nthe horizontal and vertical directions be xG and yG , respectively. Then according to (4), the gradient value of point ( , )x y in image I can be obtained as follows:\n( , ) ( , ) ( , )x yG x y G I x y G I x y=  +  (4)\n1 0 1\n2 0 2\n1 0 1\nxG\n\u2212    = \u2212    \u2212 \n(5)\n1 2 1\n0 0 0\n1 2 1\nyG\n    =    \u2212 \u2212 \u2212 \n(6)\nThen the Tenengrad evaluation function of the image is defined as follows, with being the given edge detection threshold.\n  2\n( ) ( , ) , ( ( , ) ) x y D f G x y G x y T=  (7)\n( )D f is defined as the sharpness of the image, and the\nlarger the value of the solution, the clearer it is; conversely, the blurrier it is.\nAccording to the above analysis, if the calculated value of the judging function exceeds the predefined threshold, it is marked as a clear image and can be directly used as the input image of the target detection algorithm; on the contrary, it is a blurred image and needs to be deblurred by super-resolution reconstruction before it can be used as the input image. Fig. 9 shows the discrimination results of clear and blurred images, where Fig. 9(a) is a clear image with a calculated D value of 2212 and Fig. 9(b) is a blurred image with a calculated ( )D f\nvalue of 11.91. After several experiments, ( )D f =50 is\nfinally selected as the threshold value for image clarity. The blurred value of the input image is judged, and if the sharpness\n( )D f value of two consecutive frames is greater than 50, the\nimage is used as the input image of YOLOv5-Shuffle, and if the ( )D f value is less than 50, the blurred image is\nreconstructed using the DeblurGANv2 algorithm and then used as the input image."
        },
        {
            "heading": "IV. EXPERIMENTAL AND ANALYSIS",
            "text": "The experimental process is divided into three main parts: dataset construction, model training and object detection. Firstly, a self-built data set was constructed by collecting and preprocessing images. Then, the model weights were obtained by adjusting the parameters and model training. Finally, the trained weights were used for object detection, and the detection results of different methods were compared and analyzed.\nA. Sample Data Set\nSince there is no publicly available dataset for transmission line fixtures, we created a dataset for transmission line fixture inspection. The data sources for the image library are images taken by the patrol robots with cameras, images taken by UAVs, and images taken by outdoor self-built simulated lines. The dataset is labeled with four types of objects, namely, anti-vibration hammers, spacer bars, wire clips, and insulators. The dataset is divided into train and Val by 8:2, as shown in Table 1.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\nThis experiment is conducted under Ubuntu 18.04 operating system with Intel(R) Core(TM) i7-10750H processor and NVIDIA GTX1650Ti graphics card, using PyTorch framework to build a deep learning platform. The improved YOLOv5-Shuffle model is set to have a training batch size of 4, a training image input size of 640\u00d7640, and a preset number of training iterations of 300 epochs during the training process. The SGD algorithm with momentum of 0.937 is used to optimize the instrument.\nC. Evaluation Criteria\nThe performance of the target detection algorithm is evaluated by metrics including Recall, Precision, Average Precision (AP), Mean Average Precision (mAP), and Frames Per Second(FPS). Recall indicates the ability to find a certain type of sample in the dataset. The accuracy rate indicates the accuracy of finding out the samples. Precision indicates the accuracy of finding a sample. There are limitations in using both single recall and precision as evaluation metrics. A coordinate axis with precision as the vertical coordinate and recall as the horizontal coordinate is established, and the area under the P-R curve is defined as the Average Accuracy AP, and the calculation formula is shown in (10). The mean average accuracy for all categories can be obtained from (11); FPS is defined as the number of images processed by the algorithm per second.\n100% TP\nPrecision TP FP =  +\n(8)\n100% TP\nRecall TP FN =  +\n(9)\n1\n( ) ( ) 100% n\ni AP P i R i = =  (10)\n1 ( ) 100%\nK k AP k\nmAP K\n==   (11)\nwhere TP denotes the number of samples for which the model is positive and itself is positive; FP denotes the number of samples for which the model is positive but itself is negative; FN denotes the number of samples for which the model is negative but itself is positive; is the number of categories currently accrued; and N is the total number of categories.\nThe Peak Signal to Noise Ratio (PSNR) and Structural\nSimilarity Index (SSIM) are mainly used to evaluate the image. SSIM has a value between -1 and 1, which is meaning the higher is better. Smaller MSE values, on the other hand, imply a more positive outcome.\n1 1\n2\n0 0\n1 ( , ) ( , )\nm n\ni j\nMSE x i j y i j mn\n\u2212 \u2212\n= =\n= \u2212 (12)\n2\n10\n(2 1) 10\nn\nPSNR log MSE\n \u2212 =  \n \n(13)\nWhere ( , )x i j \u3001 ( , )y i j represent the grayscale value of\nthe reference image at spatial position ( , )i j and the grayscale\nvalue of the original clear image at spatial position ( , )i j ,\nrespectively, MSE is the mean square error of the two images to be compared, and n is the number of bits per pixel.\nThe evaluation value of SSIM can be analyzed and calculated based on the combined effect of three key features: brightness, contrast, and structural information, which leads to the evaluation result of image quality. The calculation formula is shown in (14).\n1 2 2 2 2 2\n1 2\n(2 )(2 ) ( , )\n( )(\nx x xy\nx y x y\nC C SSIM x y\nC C\n  \n   \n+ + =\n+ + + + \uff09 (14)\nwhere x is the average of x, y is the average of y, 2 x\nis the variance of x, 2\ny is the variance of y, and xy is the\ncovariance of x and y. C1 and C2 are small constants.\nD. Analysis Of Training Results Of YOLOv5-Shuffle Network Model\nAccording to the experimental environment configuration, YOLOv5-Shuffle and YOLOv5 are trained under the same hyperparameter settings in this paper, and the training results are shown in Fig. 11, which shows the values of various metrics obtained from the improved YOLOv5-Shuffle model and the native YOLOv5 model trained with the same dataset and the same equipment. Meanwhile, under the same experimental conditions, YOLOv4 , SSD and Faster R-CNN models were added for training effect comparison. The improved YOLOv5-Shuffle model reached 94.0%, while the accuracy rate of YOLOv5 was 92.8%, indicating a better average accuracy.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n8 VOLUME XX, 2017\nTable 2 shows the overall parameters of the improved YOLOv5-Shuffle model and the original YOLOv5 model, including Params, Size and FOLPs. Params: refers to the number of parameters of the model, that is, the number of weights and bias items of all classes trained in the model; Size: indicates the storage space occupied by the model file. FOLPs: Refers to the number of floating-point operations per second and can be used to estimate the computational resources required by a model during inference.\nIn Table 3 we compile the statistics of the above training results. The comparison of the training results shows that the overall parameters and computational complexity of the improved YOLOv5-Shuffle model in this paper are much lower than those of the YOLOv5 model, and the performance of the improved medium is improved by 1.2% in terms of accuracy and 35.8% in terms of detection speed, which provides a great convenience for model deployment in embedded devices. Therefore, the improved model not only improves the detection speed but also still maintains a very good detection performance.\nE. Analysis of DeblurGANv2 Training Result\nFig. 12 shows the loss loss variation curve of DeblurGANv2 model training, and it can be seen that the model gradually converges when the iteration rounds reach 100 epochs, and the loss value tends to level off and obtains the final model weights after 300 epochs are completed.\nAs shown in Fig. 13(a), the PSNR plot obtained by training and Fig. 13(b), the SSIM plot obtained by training, the DeblurGAN algorithm finally reached a PSNR value of 27.6 dB after 300 rounds of iterative training, while the SSIM value finally reached 8.62, and the model obtained a good image reconstruction performance.\nThe screened blurred images were input to the DeblurGAN network model with a training number of 100 epochs and 300 epochs for super-resolution image reconstruction, and the local details of the reconstructed images were enlarged, and the results are shown in Fig. 14. The detailed contours of specific targets in the images are better revealed, and the resolution of the reconstructed images is visually significantly improved. The PSNR value of Fig. 14(a) with the original blurred image is 21.3 dB and the SSIM value is 0.82, and the\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n11 VOLUME XX, 2017\nPSNR value of Fig. 14(b) with the original blurred image is 26.3 dB and the SSIM value is 0.87. The experimental results prove that the super-resolution reconstruction of motion blurred images using DeblurGAN can effectively obtain considerable improvement in terms of visual perception and image quality."
        },
        {
            "heading": "F. Experimental Results and Analysis",
            "text": "To further test the actual detection effect of the improved YOLOv5-Shuffle model, three images were randomly selected for comparison with the YOLOv5 model. Fig. 15(a) shows the improved YOLOv5-Shuffle detection effect, and Fig. 15(b) shows the YOLOv5 detection effect. It can be found that in the first set of test images, both models can identify all the anti-shock hammers and wire clips better, among which, the confidence level obtained by YOLOv5-Shuffle for antishock hammer detection is slightly better than that of YOLOv5, while the detection of wire clips is slightly worse; in the second set of test images, the detection accuracy of both models is comparable for spacer bars, while YOLOv5 is slightly better for anti-shock hammer detection. In the third set of test images, YOLOv5 can detect all the anti-vibration hammers, but the precision is lower, while the YOLOv5Shuffle model in this paper misses one anti-vibration hammer, but obtains a better confidence level. The overall test results show that the detection accuracy of the improved YOLOv5Shuffle model in this paper is not much different from that of the native YOLOv5, and can meet the requirements of detection accuracy.\nFig. 16 shows the effect of using the reconstruction algorithm to improve the detection accuracy of the YOLOv5 algorithm. The left column blurred image in the YOLOv5 target detection without DeblurGANv2 algorithm processing, the images with different degrees of blurring showed line clip misdetection, missed detection and low confidence level. After deblurring by DeblurGANv2 algorithm, the detection effect is improved, and the confidence of detection meets the requirements of inspection.\nIn order to further evaluate the actual performance of DeblurGAN's images, 100 pieces of gold tools with blurring degree in the range of 20~50 were selected, and the YOLOv5 algorithm was used to complete the comparison experiments on the blurred images and the reconstructed images, and the experimental results were counted.In order to quantitatively\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSHIKAI WANG: Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2\n11 VOLUME XX, 2017\nanalyze the algorithm performance, the recognition rate is defined as an index. A total of M images are selected for detection, and the results are counted for the detected images, and the number of images that are correctly detected and have no misses or false detections is recorded as m. Then, the recognition rate A of detection can be obtained as shown in (12).\n100% m\nA M =  (12)\nAs shown in the table 4, the recognition rate is 37% using YOLOv5 for the detection of blurred images of gold tools in motion, 68% after combining DeblurGAN with YOLOv5 for the detection of blurred images of gold tools, and 70% after combining DeblurGAN with the improved YOLOv5-Shuffle for the detection of blurred images of gold tools, which is a 33% improvement in recognition rate."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, a detection network based on generative adversarial network defuzzification network module (DeblurGANv2) cascaded with YOLOv5 is proposed and used in transmission line fixture detection for inspection robots. In order to reduce the parameter size of the network and improve the detection speed, shufflenetv2 is used instead of CSPDarknet-53 in the original network, and the ECA module is inserted to enhance the focus of the model on features. In addition, DeblurGANv2 super-resolution reconstruction algorithm is introduced to deblur the metallic tool images to solve the problem of blurred imaging due to shaking of the robot by external factors. Experiments show that the improved YOLOv5 model improves the detection speed by 35.8% while ensuring the detection accuracy, and, in the blurred image detection experiments, the recognition rate improves by 33%. In the future, the positioning technology of distance determination will be integrated to perform distance determination of high-voltage transmission line patrol robots and line obstacles to achieve more accurate patrol functions."
        }
    ],
    "title": "Research on transmission line hardware identification based on improved YOLOv5 and DeblurGANv2",
    "year": 2023
}