{
    "abstractText": "It might reasonably be expected that running multiple experiments for the same task using the same data and model would yield very similar results. Recent research has, however, shown this not to be the case for many NLP experiments. In this paper, we report extensive coordinated work by two NLP groups to run the training and testing pipeline for three neural text simplification models under varying experimental conditions, including different random seeds, run-time environments, and dependency versions, yielding a large number of results for each of the three models using the same data and train/dev/test set splits. From one perspective, these results can be interpreted as shedding light on the reproducibility of evaluation results for the three NTS models, and we present an indepth analysis of the variation observed for different combinations of experimental conditions. From another perspective, the results raise the question of whether the averaged score should be considered the \u2018true\u2019 result for each model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maja Popovi\u0107"
        },
        {
            "affiliations": [],
            "name": "Mohammad Arvan"
        },
        {
            "affiliations": [],
            "name": "Natalie Parde"
        },
        {
            "affiliations": [],
            "name": "Anya Belz"
        }
    ],
    "id": "SP:2af1f50756d37ad198edd416231b40c7fdae2b40",
    "references": [
        {
            "authors": [
                "Mohammad Arvan",
                "Lu\u00eds Pina",
                "Natalie Parde"
            ],
            "title": "Reproducibility in computational linguistics: Is source code enough",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Arvan",
                "Lu\u00eds Pina",
                "Natalie Parde."
            ],
            "title": "Reproducibility of Exploring Neural Text Simplification Models: A Review",
            "venue": "Proceedings of the 15th International Natural Language Generation Conference (INLG 2022), Waterville, ME.",
            "year": 2022
        },
        {
            "authors": [
                "Anya Belz",
                "Shubham Agarwal",
                "Anastasia Shimorina",
                "Ehud Reiter."
            ],
            "title": "ReproGen: Proposal for a shared task on reproducibility of human evaluations in NLG",
            "venue": "Proceedings of the 13th International Conference on Natural Language Generation, pages",
            "year": 2020
        },
        {
            "authors": [
                "Anya Belz",
                "Maja Popovic",
                "Simon Mille."
            ],
            "title": "Quantified reproducibility assessment of NLP results",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16\u201328, Dublin, Ireland. Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Anya Belz",
                "Anastasia Shimorina",
                "Shubham Agarwal",
                "Ehud Reiter."
            ],
            "title": "The ReproGen shared task on reproducibility of human evaluations in NLG: Overview and results",
            "venue": "Proceedings of the 14th International Conference on Natural Language Gen-",
            "year": 2021
        },
        {
            "authors": [
                "Anya Belz",
                "Anastasia Shimorina",
                "Maja Popovi\u0107",
                "Ehud Reiter."
            ],
            "title": "The 2022 reprogen shared task on reproducibility of evaluations in nlg: Overview and results",
            "venue": "Proceedings of the 2022 ReproGen Shared Task on Reproducibility of Evaluations in",
            "year": 2022
        },
        {
            "authors": [
                "Waterville",
                "Maine"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "NLG (ReproGen",
            "year": 2022
        },
        {
            "authors": [
                "sunaga",
                "Jiaxuan You",
                "Matei A. Zaharia",
                "Michael Zhang",
                "Tianyi Zhang",
                "Xikun Zhang",
                "Yuhui Zhang",
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang"
            ],
            "title": "On the opportunities and risks of foundation models. ArXiv",
            "year": 2021
        },
        {
            "authors": [
                "Ant\u00f3nio Branco",
                "Nicoletta Calzolari",
                "Piek Vossen",
                "Gertjan Van Noord",
                "Dieter van Uytvanck",
                "Jo\u00e3o Silva",
                "Lu\u00eds Gomes",
                "Andr\u00e9 Moreira",
                "Willem Elbers"
            ],
            "title": "A shared task of a new, collaborative type to foster reproducibility: A first exercise in the area of language",
            "year": 2020
        },
        {
            "authors": [
                "Boyuan Chen",
                "Mingzhi Wen",
                "Yong Shi",
                "Dayi Lin",
                "Gopi Krishnan Rajbahadur",
                "Zhen Ming Jiang."
            ],
            "title": "Towards training reproducible deep learning models",
            "venue": "44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pitts-",
            "year": 2022
        },
        {
            "authors": [
                "Yanran Chen",
                "Jonas Belouadi",
                "Steffen Eger."
            ],
            "title": "Reproducibility issues for bert-based evaluation metrics",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2965\u20132989, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Michael Cooper",
                "Matthew Shardlow."
            ],
            "title": "CombiNMT: An exploration into neural text simplification models",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5588\u20135594, Marseille, France. European Language",
            "year": 2020
        },
        {
            "authors": [
                "Carol J Feltz",
                "G Edward Miller."
            ],
            "title": "An asymptotic test for the equality of coefficients of variation from k populations",
            "venue": "Statistics in medicine, 15(6):647\u2013 658.",
            "year": 1996
        },
        {
            "authors": [
                "Post"
            ],
            "title": "The sockeye neural machine translation",
            "year": 2018
        },
        {
            "authors": [
                "Wei Wu"
            ],
            "title": "Aligning sentences from stan",
            "year": 2015
        },
        {
            "authors": [
                "Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evalu",
            "year": 2002
        },
        {
            "authors": [
                "Maja Popovi\u0107",
                "Anya Belz"
            ],
            "title": "A reproduction",
            "year": 2021
        },
        {
            "authors": [
                "Anya Belz"
            ],
            "title": "Reproducing a manual evaluation",
            "year": 2022
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 2746\u20132757 July 9-14, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Recently there has been a promising surge of interest in reproducibility of NLP models, supported by challenges (Pineau et al., 2021), shared tasks (Belz et al., 2020), conference tracks (Carpuat et al., 2022), and even the Reality Check theme at this conference. The outcome of this surge in interest has been a flurry of reproducibility studies and related investigations (Belz et al., 2022a; Arvan et al., 2022a; Chen et al., 2022b). However, the collective findings from these efforts have been alarming.\nWith interest in reproducibility growing, the evidence is mounting that scores are substantially affected by changes not only to arbitrary factors like random seed and different data splits, but also by incidental factors such as the type of GPU on which an experiment is run, and the run-time environment. In many cases, near-identical scores can be guaranteed only when an experiment is re-run in\nfully containerised form. In effect, this means that even perfect sharing of information (once regarded as the answer to all our reproducibility problems1 (Sonnenburg et al., 2007)) cannot guarantee identical results in all cases.\nAll this raises questions about reporting, experimental design and the informativeness of scores regarding the relative merits of different methods. Underlying these is the question of where the boundary lies \u2013 seemingly between the two extremes. On the one hand, exploration of methodological variations and reporting of separate scores is part and parcel of method development. On the other hand, arbitrary and incidental factors such as random seed are not part of method development, because they do not generalise to future applications of the same method. For the former, clearly, comparing and reporting different scores is important; for the latter, how to interpret, address or report variation in scores is an open question.\nIn this paper, we tackle this question by conducting a systematic and comprehensive investigation coordinated across two NLP groups to study the variation of the results across three neural text simplification (NTS) models under many different experimental conditions. We experiment with different random seeds, run-time environments, and dependency versions to ensure broad coverage of our study. We observe that reporting average score and its coefficient of variation is a more reliable standard than reporting the maximum value, and we urge researchers to record all methodological conditions, control incidental ones, and abstract away arbitrary factors to promote the reproducibility of their scientific contributions.\n1\"Reproducibility would be quite easy to achieve in machine learning simply by sharing the full code used for experiments\" (Sonnenburg et al., 2007).\n2746"
        },
        {
            "heading": "2 Task and Experimental Set-up",
            "text": "Our starting point for this exploration is the first neural text simplification system reported by Nisioi et al. (2017). This work was selected because it is suitable for our purposes: the authors provided a repository2 which contains comprehensive information about the original work and the resources, thus facilitating repeat runs of their experiments and exploration of variation on their experimental conditions, which is not often the case for NLP papers. Moreover, the work has been reproduced before (Cooper and Shardlow, 2020; Popovic\u0301 and Belz, 2021; Popovic\u0301 et al., 2022; Belz et al., 2022a; Arvan et al., 2022b) as part of the REPROLANG 2020 (Branco et al., 2020) and ReproGen 2021/2022 (Belz et al., 2021, 2022b) shared tasks, which represents another reference point to choose it.\nIn the following subsections we describe the four different systems (\u00a72.2), the single data set/split and four text processing variants (\u00a72.3), and the two evaluation methods (\u00a72.4) which were included in our exploration, either because they were part of the original study or because we added them. \u00a72.5 provides an overview of the incidental and arbitrary variation arising in our different runs which we also analysed."
        },
        {
            "heading": "2.1 Task Background",
            "text": "Briefly, text simplification aims to transform a specified text into a simpler form while retaining the same meaning. This is potentially useful for a broad range of real-world applications, because it makes the text readable and understandable for wider audiences and also easier to process by automatic NLP tools. The notion of simplicity itself may be tied to a variety of factors ranging from lexical complexity to content coverage or sentence/document structure. Automatic text simiplification (ATS) can be rule-based or data-based. Many data-based techniques approach the task of simplifying text by adopting methods from machine translation (MT), which is also the case for our experiments. Our work does not seek to develop innovations in ATS specifically, but rather to use ATS models as a convenient case study for studying variation of results. Nonetheless, we provide this background to facilitate fuller understanding of the problem scope and goals of the reproduced systems.\n2https://github.com/senisioi/NeuralTextSimpli fication"
        },
        {
            "heading": "2.2 Systems",
            "text": "Nisioi et al. (2017)\u2019s original work is one of the first which explored neural networks for ATS (neural ATS, or NTS). They used Long Short-Term Memory (LSTM) recurrent neural networks with attention in an encoder-decoder architecture. Two models were trained: one standard neural MT model (which we call LSTM), and one (LSTM-w2v) using external pre-trained word2vec word representations (Mikolov et al., 2013). All their experiments were carried out using the openNMT tool3 (Klein et al., 2017). The used version is the initial version based on LuaTorch,4 released in December 2016.\nThe authors provided information about all necessary external libraries and specific Python and Lua dependencies, and also released the two models they trained (LSMT and LSTM-w2v). It is worth noting that the source code uses Python 2.7 and Torch. The Python environment uses older versions of openNMT, NLTK, and gensim. This version of openNMT is no longer maintained and most of the libraries and dependencies have become obsolete, and it is therefore advised not to use this version anymore but to switch to one of the two newer ones (openNMT-py based on PyTorch or openNMT-tf based on TensorFlow). Therefore, it has become extremely challenging to recreate the same environment to regenerate and retrain the models using the released source code.\nOther than variation in the libraries and environments, we conduct a random search for the LSTM models using the original repository. In this scenario, all the hyper-parameters are kept the same except the random seed. Knowing that the random seed affects the weight initialisation, the data order used in training, and the sampling used in the generation, we suspected that we might observe a wide range of results.\nGiven that LSTM models generally have been superseded by transformer models (Vaswani et al., 2017), we additionally trained a transformer model on the data provided by the authors, using another publicly available tool, Sockeye.5 We used two versions of the tool: the first version, based on MXNet (Hieber et al., 2018), and the newest (third) version based on PyTorch (Hieber et al., 2022). We treat these two versions as two different systems using the same model type. Thus to summarise,\n3https://opennmt.net/ 4https://github.com/OpenNMT/OpenNMT 5https://awslabs.github.io/sockeye/index.html\nour systems are:\n\u2022 LSTM/OpenNMT: Nisioi et al. (2017)\u2019s LSTM neural MT model implemented as the first version of the OpenNMT tool.\n\u2022 LSTM-w2v/OpenNMT: Nisioi et al. (2017)\u2019s LSTM neural MT model, using external pre-trained word2vec representations implemented as the first version of the OpenNMT tool.\n\u2022 Transformer/Sockeye v1 (MXNet): Our updated version of the NTS model, using a transformer model instead of an LSTM, implemented as the first version of the Sockeye tool based on MXNet.\n\u2022 Transformer/Sockeye v3 (PyTorch): Our updated version of the NTS model, using a transformer model instead of an LSTM implemented as the newest (third) version of the Sockeye tool based on PyTorch.\nWe report results achieved under numerous conditions for each of these systems, ensuring broad coverage and supporting the robustness of the investigation."
        },
        {
            "heading": "2.3 Data Set and Text Processing",
            "text": "Nisioi et al.\u2019s (2017) repository contains the preprocessed data set, but not the original data nor the pre-processing scripts. Their data set was a popular corpus of parallel English Wikipedia and Simple English Wikipedia (EW-SEW) articles (Hwang et al., 2015), and we used the same data for our experiments. The corpus statistics for the parallel data in both the training and tests sets are presented in Table 1. We report the number of sentences and words and the overall vocabulary size for each partition (original/simplified \u00d7 train/test) of the data.\nIn the original paper, it is reported that Named Entities were treated separately: they were first identified, then replaced by an \u2018unknown\u2019 symbol for the training, and for generating output, each \u2018unknown\u2019 symbol was replaced by the word with the highest probability score from the attention layer. However, no scripts or guidelines were provided for it. Also, it was not mentioned that the words were segmented into sub-word units, which is nowadays the standard for all state-of-the-art neural systems. Word segmentation enables better coverage of large vocabularies and treatment\nof rare and unseen words. The standard word segmentation method for the Sockeye tool is byte-pair encoding (BPE) (Sennrich et al., 2016), which is one of the most widely used segmentation methods. According to the Sockeye guidelines, segmentation is performed after the original text is tokenised. In our experiments, we explored both original and additionally tokenised data, both with BPE word segmentation.\nAfter generating outputs with our transformer models, sub-word units are joined together to form original words. This is usually followed by a detokenisation step. However, since the outputs of the original models are all tokenised, we evaluated both versions: tokenised and detokenised. Finally, due to lack of special treatment of named entities, the transformer outputs contain a number of \u2018unknown\u2019 symbols, referring to unseen sub-word units. We computed metric scores for two versions of the output: with \u2018unknown\u2019 symbols left in place, and with \u2018unknown\u2019 symbols removed."
        },
        {
            "heading": "2.4 Evaluation",
            "text": "We performed automatic evaluation of generated outputs using the script provided by the authors which calculates two metrics: BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). Previous work also explored differences arising from different BLEU implementations (Popovic\u0301 and Belz, 2021), but these are not relevant to present purposes. BLEU is based on matching between the generated text and a manually simplified reference text, while SARI compares the generated text both to the reference text as well as to the original text."
        },
        {
            "heading": "2.5 Methodological, Arbitrary and Incidental Variations",
            "text": "Table 2 provides an overview of the experimental conditions (first column) for which we explored\nvariation. The conditions are grouped into three categories: (i) methodological factors, i.e., variation in the methods used in a solution for a task with the aim of improving performance, where better performance can to some degree be expected to generalise to similar types of tasks; (ii) arbitrary factors where an arbitrary (often random) selection is made with respect to a given parameter; and (iii) incidental factors, where selection is not under the direct control of the system creators, e.g., changes from one version of a dependency to another. All of these conditions may be reasonably expected to vary during replication experiments.\nMethodological factors may occur when the group replicating a given model decides to update some component of its design based on recent findings. An example in our own work reported here is the inclusion of the transformer-based model, based on the recent success of these models for a wide range of NLP tasks in the time since Nisioi et al. (2017)\u2019s publication.\nArbitrary factors may occur due to underreporting of necessary parameters in the original work. For instance, if a hyper-parameter must be specified in order for the model to run but no specifications are provided by the model creators, the group replicating the work may select that hyperparameter randomly or using their own heuristic.\nIncidental factors may occur due to library or package updates, rendering the versions reported in the original publication obsolete. It also may occur in different run-time environments, for example running experiments on different computers.\nBy including each of these factors in our study,\nwe sought to ensure broad coverage of the range of results variation that may realistically occur when attempting to replicate a previously reported model."
        },
        {
            "heading": "3 Results",
            "text": "We report the results from both team A and team B, for each of the studied conditions. While both teams struggled to get the original repository to a working state, team A failed to install all the required dependencies as many are deprecated. Team B reported similar concerns about reproducing and reusing the original source code; however, ultimately, they managed to get the repository to a running state.\nTable 3 shows the two automatic scores generated by the evaluation script provided by the authors for all explored variations (see Table 2), grouped together by system: LSTM, LSTM-w2v, Transformer Sockeye v1 and Transformer Sockeye v3. Where they exist, results provided by the authors of the original paper are included as well. For random seed search, we included two worst and best-performing models in this table, while full results of this search can be found in Appendix.\nAveraged scores for each of the three models together with the standard deviations and coefficients of variation (Belz et al., 2022a) are presented in Table 4. For each of the models, \u2018all\u2019 refers to the average value of all scores for this model presented in Table 3. For the LSTM model, \u2018random seed\u2019 is averaged only over the random seed scores, and \u2018other\u2019 is averaged over all scores except the random seed scores. For the transformer model, \u2018v1\u2019 means only the scores from version 1, and \u2018v3\u2019 means only the scores from version 3.\nAccording to the averaged SARI score, the transformer model performs best; however, the newest version performs worse than the old one. According to the averaged BLEU score,6 LSTM-v2w and Transformer have very similar performance, but the newest version of the transformer is the best of all while the first version is the worst.\nWe used the R package cvequality (Version 0.2.0; (Marwick and Krishnamoorthy, 2019)) to test for significant differences of coefficients of variation (CV). This package implements two of the most widely used statistical significance tests, proposed by Feltz and Miller (1996) and Krishnamoorthy and Lee (2014). The null hypothesis for each of the two automatic metrics is that there is no difference in CV between the three models.\nWe use the results reported in the Table 4 corresponding to the row \u2018all\u2019 for the three model\n6The reason for slightly different scores on original outputs is yet another source of variation which we did not explore here, namely incidental variations of BLEU scores related to dependencies and run-time environment.\nvariants. Conducting the two tests resulted in the statistical significance values shown in Table 5. We observe that neither test statistics nor p-value suggest statistical significance when setting \u03b1 = 0.05. Therefore, we cannot reject the null hypothesis."
        },
        {
            "heading": "4 Discussion",
            "text": "Nisioi et al. (2017) reported that using pre-trained word embeddings improves the model\u2019s performance. Results in Table 3 and Table 4 suggest that while this may be true, the differences are too small to draw clear conclusions. For one model alone, the LSTM variant, we have observed BLEU scores ranging from 84.47 to 89.59; the average, on the other hand, is 87.90 with the CV of 1.36. Compared to LSTMs, transformer models have a higher variance in their performance. This can be attributed to the transformer\u2019s complexity and the fact that they are harder to train. Also, variations in tokenisation were included only in the transformer models. The performance difference between the best and worst transformer models is even higher\nthan LSTM variants. With a 13.42 BLEU score difference, assessing true performance of the model is a challenging task. Judging the results by the average BLEU score (Table 4), we can observe that the transformer model trained using v3 of the Sockeye tool outperforms the rest of the models. This model achieves an average BLEU of 91.24 with a CV of 1.91. To put the CV into context, this value is higher than three other LSTM variants but lower than the rest of the transformer models. As it can be expected, using an averaged performance metric and CV enables a better comparison between models in different conditions.\nBesides the mentioned analysis, we found it hard to provide distinct and unique observations from the results. This is likely due to the fact that the results are not conclusive and the variance is high. We do not believe this is a flaw in our experimental design but rather a good representation of the complexities of comparing different models across varying conditions. The number of experiments conducted in this study is more than 60, a number that exceeds the number of experiments conducted in most other studies by a large margin.\nOne of the concerning issues we encountered is the issue of software deprecation. While this is not a new problem, and it is as old as software itself, it is becoming more and more prevalent. This is\ndue to extreme reliance on empirical results and the complexity of publications that utilise neural networks. Often source codes use several external libraries and dependencies, any of which may become deprecated at any time. Increased availability of source code and the abundance of tools are signs of a healthy research community. Seeing new tools and libraries developed and improved daily is encouraging. At the same time, we believe researchers should practice caution when introducing new tools and libraries into their experiments, as doing so may shorten the usability of their source code."
        },
        {
            "heading": "4.1 Addressing Experimental Variation in Experimental Design",
            "text": "Many factors can affect the results of an experiment. Some of these factors are under the experimenter\u2019s control, and some are not. Before we address these variations, we highlight that scientific experiments are developed as a counterpart to abstraction of real-world problems. Data sets are created with this in mind, consisting of training, validation, and test sets of which the latter, in particular, is created to represent unseen real-world data. Research on improving the generalisation of machine learning algorithms is another good example of leveraging scientific experiments to understand real-world challenges.\nWe can use another analogy to explore these variations further. Bogosort is a sorting algorithm that generates random permutations of the input until the input is sorted. While in the best case, it may take O(n) steps to sort the input, its worst-case performance is unbounded, making it impractical to use. Theoretically, it is possible to find the random seed that achieves best-case performance for a specific input; nonetheless, the slightest change in\nhardware, environment, or even the input itself will render this seed useless. Although neural networks are far more complicated than a simple sorting algorithm, the basis of reliance on the evidence is the same. Similar to Bogosort, recording all the random numbers used in an experiment is possible (Chen et al., 2022a), but the question is: should we? We do not think so. Instead of optimising the random seed or other arbitrary factors, researchers should focus on the methods that minimize the impact of these variables. Ultimately, we believe the correct approach for conducting scientific experiments is to thoroughly report methodological variations, control incidental variations, and abstract away arbitrary variations."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this work, we conducted a series of experiments for a single task using the same data under different experimental conditions. We categorized these conditions into three different categories: methodological, arbitrary, and incidental. We report the results of our experiments to demonstrate the wide results variation that can occur due to these factors.\nWe propose that researchers should record all methodological conditions, control incidental ones, and abstract away arbitrary factors. Lastly, we observed that using average score and its coefficient of variation (CV) instead of the maximum value provides far more reliable results. We recommend that researchers adopt this practice when documenting the findings from their own studies.\nWe are aware that this is easier said than done. We are, however, optimistic that the field can move closer to this ideal over time. In the meantime, it is our hope that this recommendation highlights the contrast between what is currently a common practice (unfortunately, inadequate recording and reporting that do not address necessary factors for reproducibility) and what is needed to support successful, reproducible research in our field.\nLimitations\nOur work is limited by several factors. First, our findings are supported only by experiments on a single NLP task (neural text simplification). We selected this task because it offered an intriguing sandbox for studying varying experimental conditions, ranging from differences in random seeds to modifications in compile-time and run-time environments and dependency versions. Comparing the\nmultifaceted outcomes arising from these experiments facilitated greater quantified estimations of the degree of reproducibility for the selected NTS systems. However, the dimensions of variation that we explored in this work are common to many NLP tasks; none are unique only to text simplification. Because of this, we believe that our findings would generalise broadly across NLP tasks.\nWe used a single data set, the same as in the original paper by Nisioi et al. (2017), to foster controlled study of our other experimental variables. The data set comprises aligned sentences between English Wikipedia and Simple English Wikipedia. Thus, it is unclear whether our findings would be similar if the study was conducted using data from other languages, including those with richer morphology such as Czech or Arabic.\nFinally, although we conducted a robust set of experiments for the selected models across two research groups, our experiments are limited to a small set of NTS models due to the extensive set of conditions tested for each model. Although these models vary in their architecture, we do not know if other NTS models may be more or less stable across experimental conditions. Taken together, the limitations accompanying our findings suggest compelling avenues for future research.\nEthics Statement\nThis research was guided by a broad range of ethical considerations, taking into account factors associated with environmental impact, equitable access, and reproducibility. We summarize those that we consider most critical in this section. It is our hope that by building a holistic understanding of these factors, we develop improved perspective of the challenges associated with reproducibility studies and the positive broader impacts that improved reproducibility standards may promote.\nEnvironmental Impact. In this work, we seek to study the complex and murky relationship between experimental conditions and experimental outcomes. To address research questions surrounding this relationship, we conduct many experimental runs to replicate the same models across an extensive set of variable conditions. Although necessary for justifying our claims, a downside of this process is that it may produce environmental harm. One might argue that the advantages of assurance that the \u2018true\u2019 evaluation score is found do not outweigh the disadvantages of repeatedly\nrunning models that are known to produce large carbon footprints (Strubell et al., 2019). We attenuate this risk by controlling for as many variables allowable (e.g., data set and architectural variations) while still fostering robust study of our core question, to minimize the number of experimental runs required.\nEquitable Access. A concern closely related to environmental impact is that of equitable access to this line of research. By studying a problem that requires many repeated experimental runs with subtle variations, we may exclude disadvantaged researchers from performing meaningful follow-up studies, since they may not have the requisite resource bandwidth (Bommasani et al., 2021, \u00a75.6). However, although reproducibility studies themselves may pose a barrier to entry for researchers with limited access to compute hardware, the innovations resulting from these studies (e.g., improved community standards for reproducibility of reported results) may stand to greatly benefit marginalised researchers, by minimising the potential for bottlenecks in attempting to perform impossible and costly replications to establish performance baselines.\nReproducibility. To ensure reproducibility of our own work, we report all experimental parameters, computational budget, and computing infrastructure used. We discuss our experimental setups in depth, as they are the primary focus of this study. We report descriptive statistics about our results to enhance transparency of our findings, and we report all implementation settings (e.g., package version number) needed to successfully replicate our work. Although reproducibility studies are not specified as an intended use of the referenced systems (Nisioi et al., 2017), this use is compatible with the original access conditions and the authors have consented to the paper\u2019s use in numerous reproducibility studies since its publication (Belz et al., 2022b)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was conducted with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106_P2 at the ADAPT SFI Research Centre at Dublin City University. ADAPT, the SFI Research Centre for AI-Driven Digital Content Technology, is funded by Science Foundation Ireland through the SFI Research Centres Programme."
        },
        {
            "heading": "A LSTM Random Seed Search",
            "text": "We provide the full experimental results from the random seed search in this appendix. For each variant, we include its perplexity, SARI, and BLEU score.\nACL 2023 Responsible NLP Checklist"
        },
        {
            "heading": "A For every submission:",
            "text": "A1. Did you describe the limitations of your work?\nLeft blank.\nA2. Did you discuss any potential risks of your work? Left blank.\nA3. Do the abstract and introduction summarize the paper\u2019s main claims? Left blank.\nA4. Have you used AI writing assistants when working on this paper? Left blank."
        },
        {
            "heading": "B Did you use or create scientific artifacts?",
            "text": "Left blank.\nB1. Did you cite the creators of artifacts you used? Left blank.\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts? Left blank.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Left blank.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Left blank."
        },
        {
            "heading": "C Did you run computational experiments?",
            "text": "Left blank.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Left blank.\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Left blank.\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Left blank."
        },
        {
            "heading": "D Did you use human annotators (e.g., crowdworkers) or research with human participants?",
            "text": "Left blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Left blank.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants\u2019 demographic (e.g., country of residence)? Left blank.\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Left blank.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Left blank.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Left blank."
        }
    ],
    "title": "Exploring Variation of Results from Different Experimental Conditions",
    "year": 2023
}