{
    "abstractText": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wentao Gong"
        },
        {
            "affiliations": [],
            "name": "Haonan Tong"
        },
        {
            "affiliations": [],
            "name": "Sihua Wang"
        },
        {
            "affiliations": [],
            "name": "Zhaohui Yang"
        },
        {
            "affiliations": [],
            "name": "Xinxin He"
        }
    ],
    "id": "SP:9f823228e5c4723215e5aa3cc10a8d61aecb5088",
    "references": [
        {
            "authors": [
                "N. Farsad",
                "M. Rao",
                "A. Goldsmith"
            ],
            "title": "Deep learning for joint sourcechannel coding of text",
            "venue": "Proc. of the IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Calgary, AB, Canada, Apr. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Huang",
                "X. Tao",
                "F. Gao",
                "J. Lu"
            ],
            "title": "Deep learning-based image semantic coding for semantic communications",
            "venue": "Proc. of the IEEE Global Communications Conference (GLOBECOM), Madrid, Spain, Dec. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Tong",
                "Z. Yang",
                "S. Wang",
                "Y. Hu",
                "W. Saad",
                "C. Yin"
            ],
            "title": "Federated learning based audio semantic communication over wireless networks",
            "venue": "Proc. of the IEEE Global Communications Conference (GLOBECOM), Madrid, Spain, Dec. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Xie",
                "Z. Qin",
                "X. Tao",
                "K.B. Letaief"
            ],
            "title": "Task-oriented multi-user semantic communications",
            "venue": "IEEE Journal on Selected Areas in Communications, vol. 40, no. 9, pp. 2584\u20132597, Jul. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Zhou",
                "R. Li",
                "Z. Zhao",
                "C. Peng",
                "H. Zhang"
            ],
            "title": "Semantic communication with adaptive universal transformer",
            "venue": "IEEE Wireless Communications Letters, vol. 11, no. 3, pp. 453\u2013457, Dec. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zhou",
                "R. Li",
                "Z. Zhao",
                "Y. Xiao",
                "H. Zhang"
            ],
            "title": "Adaptive bit rate control in semantic communication with incremental knowledge-based HARQ",
            "venue": "IEEE Open Journal of the Communications Society, vol. 3, pp. 1076\u20131089, Jul. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Huang",
                "F. Gao",
                "X. Tao",
                "Q. Du",
                "J. Lu"
            ],
            "title": "Toward semantic communications: Deep learning-based image semantic coding",
            "venue": "IEEE Journal on Selected Areas in Communications, vol. 41, no. 1, pp. 55\u2013 71, Nov. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "Proc. of the IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, Canada, Oct. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Q. Pan",
                "H. Tong",
                "J. Lv",
                "T. Luo",
                "Z. Zhang",
                "C. Yin",
                "J. Li"
            ],
            "title": "Image segmentation semantic communication over Internet of Vehicles",
            "venue": "arXiv preprint arXiv:2210.05321, Oct. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Mao",
                "R. Netravali",
                "M. Alizadeh"
            ],
            "title": "Neural adaptive video streaming with pensieve",
            "venue": "Proc. of ACM Conference on Special Interest Group on Data Communication (SIGCOMM), Los Angeles, California, USA, Aug. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Mnih",
                "A.P. Badia",
                "M. Mirza",
                "A. Graves",
                "T. Lillicrap",
                "T. Harley",
                "D. Silver",
                "K. Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "Proc. of the International conference on machine learning (ICML), New York City, USA, Jun. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "H. Riiser",
                "P. Vigmostad",
                "C. Griwodz",
                "P. Halvorsen"
            ],
            "title": "Commute path bandwidth traces from 3G networks: Analysis and applications",
            "venue": "Proc. of ACM Multimedia Systems Conference (MMSYS), Oslo, Norway, Feb. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "G.J. Brostow",
                "J. Fauqueur",
                "R. Cipolla"
            ],
            "title": "Semantic object classes in video: A high-definition ground truth database",
            "venue": "Pattern Recognition Letters, vol. 30, no. 2, pp. 88\u201397, Jan. 2009.",
            "year": 2009
        },
        {
            "authors": [
                "T.-Y. Huang",
                "R. Johari",
                "N. McKeown",
                "M. Trunnell",
                "M. Watson"
            ],
            "title": "A buffer-based approach to rate adaptation: Evidence from a large video streaming service",
            "venue": "Proc. of ACM Conference on Special Interest Group on Data Communication (SIGCOMM), Chicago, Illinois, USA, Aug. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Yin",
                "A. Jindal",
                "V. Sekar",
                "B. Sinopoli"
            ],
            "title": "A control-theoretic approach for dynamic adaptive video streaming over http",
            "venue": "Proc. of ACM Conference on Special Interest Group on Data Communication (SIGCOMM), London, United Kingdom, Aug. 2015.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 8.\n00 53\n1v 1\n[ cs\n.N I]\n1 A\nug 2\n02 3\nAdaptive Bitrate Video Semantic Communication\nover Wireless Networks\nWentao Gong\u2217, Haonan Tong\u2217, Sihua Wang\u2217, Zhaohui Yang\u2020, Xinxin He\u2217, and Changchuan Yin\u2217\nBeijing Key Laboratory of Network System Architecture and Convergence\nBeijing University of Posts and Telecommunications, Beijing, China \u2020 College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China Emails: {gongwentao, hntong, sihuawang, hxx 9000, ccyin}@bupt.edu.cn, yang zhaohui@zju.edu.cn Abstract\u2014This paper investigates the adaptive bitrate (ABR)\nvideo semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.\nIndex Terms\u2014Semantic communication, adaptive bitrate, video semantic segmentation, Actor-Critic\nI. INTRODUCTION\nFuture wireless network is required to support Internet of everything (IoE) which provides ultimate user experiences through ubiquitous connectivity and sensing techniques. In practical scenarios such as smart home and autonomous driving, wireless devices must transmit massive amounts of data (e.g., texts, audios, videos) to achieve ubiquitous sensing. However, due to the huge data amount of videos, it is a large overhead for wireless networks with limited spectrum resources to support pervasive video transmission. Thus, this motivates the implementation of video semantic communication techniques which can reduce the transmitted data amount through transmitting small-size video semantic information. For instance, in autonomous driving scenario, vehicles primarily concentrate on the locations of pedestrians and buildings for obstacle avoidance, rather than all the pixels sampled by vehicle cameras. In such applications, by substituting raw data transmission with obstacle-related semantic information, semantic communication techniques can reduce the bandwidth occupation of\nvideo transmission while maintaining the accuracy of taskoriented communication. However, the adaptive scheme for video semantic communication over dynamic wireless networks has not been well designed.\nRecent works [1]\u2013[4] have investigated the efficiency and robustness of semantic communication. In [1], an efficient text codec was proposed to achieve lower word error rate with high transmission efficiency. Considering the image data, the authors in [2] employed a generative adversarial networks (GANs)-based image semantic codec to enhance the semantic consistency of the received images which reduced the amount of the transmitted data. Besides, in [3], an autoencoder was proposed for audio semantic communication to improve the spectrum efficiency while maintaining accuracy. Furthermore, the authors in [4] proposed an efficient semantic communication system for the multimodal multi-user scenarios which improved visual question answering (VQA) accuracy under low signal to noise ratio (SNR) conditions. However, focusing on the accurate semantic extraction, existing works in [1]\u2013[4] have not considered the delay guarantee problem for semantic communication, especially with varying transmission environments.\nThe prior contributions [5]\u2013[7] have explored adaptive mechanisms for semantic communication in varying transmission environments. In [5], an adaptive transformer based codec was introduced to the text semantic communication system, enabling the system to flexibly adapt to channel variations. In [6], the authors proposed an adaptive bit-length scheme for text semantic communication to reduce the transmitted bit amount avoiding a significant accuracy decrease. Furthermore, in [7], the authors improved the results of the reconstructed image through an adaptive coding method that assigns different numbers of bits to different semantic important levels. However, existing studies [5]\u2013[7] have not considered an adaptive video bitrate mechanism tailored for video semantic transmission. Given that video semantic communication has a stronger demand for delay guarantee compared to other adaptive systems, it calls for an adaptive bitrate (ABR) mechanism that adapts to varying network conditions while meeting the requirements of high semantic accuracy and low delay.\nTo address this issue, in this paper, we propose an adaptive bitrate video semantic communication (ABRVSC) system to adapt the bitrate of video semantic information to the network variation. The main contributions of this paper are as follows:\n1) We develop an ABRVSC system in which devices adaptively adjust the bitrate of video semantic information according to network conditions. 2) We first define the quality of experience (QoE) for video semantic communication and formulate a QoE maximization problem to improve the semantic accuracy while meet the delay requirements of video semantic communication. 3) To solve the proposed problem, we then propose a swin transformer-based semantic codec for extracting semantic information, and further, introduce an Actor-Critic based ABR algorithm for the semantic codec to select the appropriate bitrate and to guarantee low transmission delay and high semantic accuracy. 4) Simulation results with CamVid dataset demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which improves the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.\nThe rest of this paper is organized as follows. Section II introduces the system model and problem formulation. Section III provides detailed descriptions of the proposed codec and Actor-Critic based video ABR algorithm. Simulation results are presented in Section IV. Conclusion is drawn in Section V."
        },
        {
            "heading": "II. SYSTEM MODEL",
            "text": "We consider an ABRVSC system consisting of a device and a server, as shown in Fig. 1. In the considered system, the device (e.g., vehicle) must transmit video semantic information which contains locations of key objects (e.g, humans and vehicles) in the sensing area, to an edge server. Then the edge server utilizes the information to sense the road environment, thus enhancing the server\u2019s ability to manage the road situation. To efficiently achieve this goal of video data transmission, the device must extract the accurate video semantic segmentation features using a semantic encoder and the server must reconstruct the video segmentation using a semantic decoder. Meanwhile, due to the dynamic characteristics of wireless network environments, the bitrate of transmitted information cannot be always guaranteed during a long time when the channel seriously degrades. As a result, the video semantic segmentation features must be\nencoded into multiple bitrates to guarantee both low transmission delay and high semantic accuracy by adjusting the bitrate when network condition varies. To meet the aforementioned requirements, an ABR module and a multiple bitrate semantic encoder (MBSE) module are deployed at the device. The ABR module first selects the appropriate bitrate, and then the MBSE encodes the video into semantic features with the determined bitrate. Given that the selection of bitrate precedes the encoding process, the computational complexity of our proposed system aligns with that of systems using a fixed bitrate. As such, the proposed scheme is suitable for deployment on the device for video semantic encoding. On the server side, the received semantic features, which have passed through the wireless channel, are processed by a deployed multiple bitrate semantic decoder (MBSD) module to reconstruct the required video segmentation.\nIn this section, we first present the details of the video semantic transmission model in the developed ABRVSC scheme. Subsequently, we introduce the metric for semantic accuracy. Finally, we propose a QoE model for the ABRVSC scheme and formulate the QoE maximization problem.\nA. Video Semantic Transmission Model\nWe assume that a device collects videos using a visual sensor (e.g., a camera) and preprocesses the video data (denoising and enhancing) to generate the raw video v with an initial bitrate B. The raw video v consists of n video chunks {v1,v2, ...,vn}. Before transmission, the device must select an appropriate video bitrate bn \u2208 B for each chunk vn according to the ABR algorithm, to adapt to the network conditions, where B is the set of bitrates available for selection. Then, the bitrate bn and video chunk vn are delivered to the MBSE module for semantic extraction and coding.\nDue to the varying network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if the device continuously transmit fixed bitrate video semantic information. Therefore, it is necessary to pretrain MBSE models with different compression ratios offline. The MBSE module selects the compression ratio according to the bitrate and video chunk for semantic extraction encoding. The bitrate of video chunk vn can be given by:\nbn = B\ncn , (1)\nwhere cn is the compression ratio of the video chunk vn.\nGiven the video chunk vn, semantic feature of vn needs to be extracted before transmission. Let En(\u00b7, bn) be the encoding function of the MBSE module which encodes video with bitrate bn. The relationship between the semantic feature xn and input vn can be given by:\nxn = En(vn, bn). (2)\nThe encoded semantic feature is transmitted over wireless channels and the received semantic feature at the receiver is given by:\nyn = hxn + z, (3)\nwhere yn is the semantic feature received by the semantic decoder, h is the Rayleigh fading channel coefficient, and z is the additive noise which follows a Gaussian distribution, z \u223c N ( 0, \u03c32I ) , with \u03c32 being variance and I being the identity matrix.\nThe server selects the corresponding decoder model to decode the received semantic feature yn. The function of the MBSD module is represented by Dn(\u00b7, bn). Thus, the correlation between the output video segmentation on and the received semantic features yn is given by:\non = Dn(yn, bn). (4)"
        },
        {
            "heading": "B. The Metric for Semantic Accuracy",
            "text": "A video chunk consists of a continuous sequence of frames. In the ABRVSC scheme, the objective of semantic segmentation is to assign a label to each pixel in a frame, denoting the object or region to which it belongs. This process is driven by the primary objective of enhancing the average accuracy of model-based segmentation across all frames. Consequently, semantic segmentation tasks utilize MIoU as the accuracy metric, which is calculated by:\nMIoU = 1\nc\nc\u22121 \u2211\ni=0\npii c\u22121 \u2211\nj=0\npij + c\u22121 \u2211\nj=0\npji \u2212 pii , (5)\nwhere c is the total number of categories and pij is the count of pixels in on predicted as category j, which actually belong to category i. MIoU assesses the segmentation accuracy of the model for all categories and is given an average value, ranging from 0 to 1. A higher MIoU means a more accurate segmentation."
        },
        {
            "heading": "C. QoE Model and Problem Formulation",
            "text": "Video transmission performance is closely related to user experience. In order to evaluate the user experience of video semantic communication, we propose a QoE model based on MIoU which is given by:\nQoEn = \u03b1MIoUn \u2212 \u03b2Tn \u2212 |bn \u2212 bn\u22121|, (6) where \u03b1 and \u03b2 are hyperparameters, bn is the bitrate of video chunk vbn at the current moment, |bn \u2212 bn\u22121| is the bitrate switching smoothness, and Tn is the rebuffering time. Rebuffering, a result of exhausting data in the buffer area necessitating data reloading for continued playback, can significantly\ncontribute to transmission delay. The aim of our ABRVSC scheme is to maximize QoE during video transmission over wireless networks, and the problem is formulated as follows,\nmax bn\n1\nN\nN \u2211\nn=1\nQoEn\ns.t. bn \u2208 B. (7)\nwhere N is the number of total video chunks. From (6), we can see that the QoE maximization problem (7) involves MIoU, delay, and smoothness optimization. MIoU optimization relies on accurate semantic information extraction from the video, while delay and smoothness depend on proper bitrate selection based on accurate network condition prediction. To tackle these challenges, we proposed a semantic codec and an ABR algorithm which are explained in detail in Section III."
        },
        {
            "heading": "III. SEMANTIC CODEC AND ABR ALGORITHM",
            "text": "With the aim of maximizing the QoE for the device in the ABRVSC scheme, we delve into the critical components responsible for realizing this objective, including the MBSE module, the MBSD module, and the ABR algorithm. The design and implementation of these components are adapted to optimize the QoE, considering the precise semantic representation of the video and ABR algorithm. In this section, we first introduce the MBSE and MBSD modules, which implement a multiplebitrate video codec via deploying multiple compression ratios. Subsequently, we provide a detailed description of the video semantic communication ABR algorithm."
        },
        {
            "heading": "A. Semantic Encoder",
            "text": "In order to extract the video semantic information, we adopt the scheme that processes video frames. The video frames are extracted using H.264 encoding standard (a widely adopted standard for video compression) and then fed into the semantic encoder. In the semantic encoder as shown in Fig. 2, a swin transformer-based model [8] is employed to perform semantic segmentation on the input video frames, separating and identifying various objects within each frame. Key semantic features, including object types, shapes, and locations, are extracted from the frame. Next, the semantic encoder encodes the extracted semantic features, generating a compact semantic representation, which is then transmitted over the wireless channel.\nThe semantic encoder [8] is composed of a patch partition layer, four stages, a downsampling layer, and a convolutional layer. Each stage contains a common unit with several swin\ntransformer Blocks (STBs) that can partition the input frame into non-overlapping local windows and conduct self-attention calculations within each local window. This self-attention mechanism enables STBs to allocate varying importance to distinct input elements while handling particular elements. Concurrently, STBs employ a hierarchical architecture with shifted windows, allowing the STBs to capture both local and global contextual information. These characteristics allow the model to selectively concentrate on particular regions of the input frames that are more pertinent to the objective of semantic segmentation, thereby enhancing its performance in this task. The self-attention [8] operation of the window can be given by:\nAttention(Q,K,V ) = Softmax( QKT\u221a\nd + \u03c4)V , (8)\nwhere Q, K, and V are the corresponding query, key, and value matrices for the features of each window, respectively. In (8), d is the dimension of the K matrix and \u03c4 is the relative position deviation.\nFollowing the downsampling layer, the features are passed through a convolutional layer consisting of G filters, each with size 1 and stride 1. The video compression ratio of the semantic encoder can be controlled by adjusting the value of G. When the values for G are 128, 64, 32, and 16, the corresponding compression ratios are 6, 12, 24, and 48, respectively."
        },
        {
            "heading": "B. Semantic Decoder",
            "text": "In the semantic decoder, the receiver reconstructs semantic features from the transmitted representation, with the objective of achieving maximal semantic information recovery of the original frames. Then, the segmentation frames are reconstructed using decoded semantic features, minimizing frame quality loss while preserving semantic information. Finally, the reconstructed frames are combined into a video.\nFig. 3 presents the semantic decoder for video segmentation, which primarily comprises several deconvolutional layers, multiple upsampling layers, a softmax activation layer, and an argmax layer [9]. The first three deconvolution layers aim to mitigate the effect of noise. Subsequently, in order to complete the details of the frame, three upsampling layers are interleaved between the deconvolutional layers, allowing the feature dimensions to gradually match the input in terms of length and width. After activation of the softmax activation layer, the obtained feature dimension is H\u00d7W \u00d7 c, where H and W are respectively the height and width of each frame within the input video chunk, and c is the number of categories for semantic segmentation. The value of the last dimension signifies the prediction probability of pixel multi-class classification. Lastly, semantic segmentations of video frames are acquired through the argmax operation with a dimension of H \u00d7W .\nThe objective of the decoder is to accurately classify each pixel in a frame. Therefore, we utilize the cross entropy of the multi-class classification for each pixel as the loss function. For a batch of frames, the loss function of the entire system can be given by:\nLoss =\n\u2212 S \u2211\ns=1\nH\u00d7W \u2211\nl=1\nc\u22121 \u2211\ni=0\n(ps,l,i log(p\u0302s,l,i))\nS \u00d7H \u00d7W , (9)\nwhere S is the batch size, p\u0302s,l,i is the predicted probability that the pixel at position l in the s frame is classified into category i, and the value of ps,l,i \u2208 {0, 1} is the label of the frame pixel at position l in the s frame that belongs to category i."
        },
        {
            "heading": "C. Actor-Critic based Video ABR Algorithm",
            "text": "To guarantee both low transmission delay and high semantic accuracy under various network conditions, the device must select an appropriate bitrate based on the video ABR algorithm. Consequently, we employ Actor-Critic reinforcement learning (RL) for ABR, which efficiently converges by leveraging temporal difference error. The Actor-Critic based ABR model is shown in Fig. 4. In the Actor-Critic algorithm, there are two main components: Actor network and Critic network. For each video chunk, state sn, which indicates the communication network condition, is input into the neural network of an agent. The RL agent deployed on the device selects the transmission bitrate bn as the action an. The agent then executes the action, observes the reward from environmental feedback, and the RL environment evolves to the next. After the action is performed, the Critic estimates the state value based on the action an and network state sn, providing the Actor with an evaluation of the action, denoted by value qn. The Actor then updates its model parameters according to qn.\nFor each video chunk, the input state space [10] consists of six parameters: sn = (t,d,u, Cn, Bn, Ln). t is the throughput vector for the past k video chunks, d is download time vector for the past k video chunks, and u is a vector containing m available sizes for the next video chunk. They are processed by a Temporal Convolutional Network (TCN) layer with 64 filters, kernel size 3, and stride 1. Here, we employ the TCN due to its superior performance in processing time series data.\nAlgorithm 1 Video Bitrate Adaptive Algorithm based on Deep Reinforcement Learning.\n1: Initialization: Initialize policy network parameter \u03b8 and value network parameter w. Set the hyperparameter \u03b1, \u03b2 and discounting factor \u03b3. Initialize the environment and get the initial state s0. Select action a0 according to the policy network \u03c0(a|s0; \u03b8). 2: while training epochs < Total training epochs M do: 3: while video chunks < Total video chunks N do: 4: Observe the network state sn and randomly sample the bitrate an according to the policy function \u03c0(\u00b7|sn; \u03b8n). 5: Play the video chunk with bitrate an and get the new network state sn+1 and reward rn. 6: \u03b4n = qn \u2212 (rn + \u03b3 \u00b7 qn+1) \u22b2 calculate TD error 7: dw,n = \u2202q(sn,an;w) \u2202w\n|w=wn \u22b2 calculate derivative 8: wn+1 \u2190 wn \u2212 \u03c2 \u00b7 \u03b4n \u00b7 dw,n \u22b2 update parameter w 9: d\u03b8,n = \u2202 log \u03c0(an|sn,\u03b8) \u2202\u03b8\n|\u03b8=\u03b8n \u22b2 calculate derivative 10: \u03b8n+1 \u2190 \u03b8n + \u03c3 \u00b7 qn \u00b7 d\u03b8,n \u22b2 update parameter \u03b8 11: end while 12: end while 13: Output: Final policy network parameter \u03b8\u2217, value network pa-\nrameter w\u2217 and reward curve.\nThis is largely attributed to its unique utilization of causal convolution, dilated convolution, and residual operations. The current buffer occupation Cn, the last video chunk bitrate Bn, and the number of remaining video chunks Ln are extracted by a fully connected neural network with 128 neurons. The output features of these layers are then transmitted to a hidden layer of 128 neurons. The hidden layer is fully connected to an output layer which uses the softmax function to select the action an. Different from the Actor, the Critic employs a linear activation at the output layer to evaluate the QoE of the video service. The training objective is to maximize the reward value rn, which corresponds to the QoE introduced in Section II. The reward function is given by:\nrn = \u03b1MIoUn \u2212 \u03b2Tn \u2212 |bn \u2212 bn\u22121|. (10)\nWe use V\u03c0(sn) = \u2211\na \u03c0(a|sn)Q\u03c0(sn, a) as the state value function, representing the average value of all actions. Then, V\u03c0(sn) can be approximated by a neural network weighted with w, which is given by:\nV\u03c0(sn) \u2248 V (sn; \u03b8, w) = \u2211\na\n\u03c0(a|sn; \u03b8)q(sn, a;w). (11)\nThe value network (Critic network) q(sn, a;w) is updated based on V (sn; \u03b8, w) to enhance the scoring accuracy, which in turn allows for better estimation of the sum of future expected rewards, bringing V (sn; \u03b8, w) closer to the actual average value. The policy network (Actor network) \u03c0(a|sn; \u03b8) is updated according to q(sn, a;w) in order to increase the state value V (sn; \u03b8, w). The complete Actor-Critic based ABR algorithm is presented in Algorithm 1, which leverages Temporal Difference (TD) error to guide both the value function estimation and the action selection optimization.\nMoreover, to address the parallel training problem of ActorCritic in multi-core or distributed computing environments, we adopt [11] the Asynchronous Advantage Actor-Critic (A3C)\nalgorithm. This approach employs multiple agents with each possessing its own Actor and Critic. These agents train asynchronously and subsequently aggregate their experiences into a global network."
        },
        {
            "heading": "IV. SIMULATION RESULTS",
            "text": "In our simulations, the device has a 60 seconds video playback buffer and employs four different bitrates: B = {160 kbps, 320 kbps, 640 kbps, 1280 kbps}. The specific experimental parameter settings are presented in Table I. To evaluate the performance of our algorithm in a real network environment, we adopt a mixed dataset of FCC and HSDPA [12] as our network bandwidth dataset and CamVid as the video dataset [13], respectively. In the network bandwidth dataset, we utilize 75% dataset for training the model, and 25% for testing. For the video dataset, the dataset is partitioned into 367 frames for training, 101 for validation, and 233 for testing. We compare our algorithm with a) OCRNet+JPEG (semantic segmentation model with JPEG coding), b) Buffer-Based (BB) Algorithm [14], c) Model Predictive Control (MPC) Algorithm [15], and d) Fixed bitrate transmission schemes, which bitrates \u2208 B. In Fig. 5, we show the cumulative distribution function (CDF) of MIoU for all considered schemes with different network conditions. The CDF indicates the probability distribution of a random variable. From Fig. 5, we see that although MIoU of the ABRVSC scheme is slightly lower than that of 1280 kbps scheme, the ABRVSC scheme can achieve 151.85%, 28.30%, 7.94%, 33.33%, and 11.48% gain in terms of MIoU compared to 160 kbps, 320 kbps, 640 kbps, BB and MPC schemes, respectively. This is due to the fact that the 1280 kbps scheme pursues high semantic accuracy at the cost of delay caused by huge transmission data amount, while the Actor-Critic based ABR algorithm enables the ABRVSC scheme to select high bitrates in the case of high bandwidth, thus improving MIoU. Fig. 5 demonstrates that the ABRVSC scheme can attain high MIoU for video semantic communication.\nFig. 6 shows the CDF of transmission delay for all considered schemes with different network traces. From Fig. 6, we see that although the average transmission delay of the ABRVSC scheme is higher than the 160 kbps scheme, the ABRVSC scheme can reduce 3.83%, 37.61%, 59.17%, 18.58%, and 0.85% average transmission delay compared to 320 kbps, 640 kbps, 1280 kbps, BB and MPC schemes, respectively. This is because that the 160 kbps scheme at the cost of a lot of semantic accuracy in exchange for low transmission delay, while the accuracy of the network condition prediction enables the ABRVSC scheme to select low bitrates in the case of low bandwidth, thus reducing transmission delay. Fig. 6 reflects the outperformance of ABRVSC scheme on guaranteeing low video service transmission delay.\nIn Fig. 7, we show the CDF of QoE for all considered schemes with different network conditions. From Fig. 7, we\nsee that the ABRVSC scheme can achieve 166.49%, 94.09%, 36.57%, and 24.49% gain of the QoE compared to 160 kbps, 320 kbps, BB and MPC schemes, respectively. This is due to the fact that the ABRVSC scheme can adjust the bitrate in time according to the real-time channel state to maximize the utilization efficiency of the channel resources, so as to obtain better QoE. Fig. 7 demonstrates that the ABRVSC scheme outperforms the baselines in terms of QoE.\nFig. 8 shows how the MIoU of ABRVSC scheme and traditional coding scheme change as the bitrate varies. From Fig. 8, we see that under low bitrates, the MIoU of the ABRVSC scheme surpasses that of the traditional scheme. Although at the bitrate of 1280 kbps, the ABRVSC scheme has a slightly lower MIoU compared to the traditional scheme, when the bitrate is 160 kbps, 320 kbps, and 640 kbps, the MIoU of the ABRVSC scheme is nearly 92.86%, 107.03%, and 15.38% higher than that of the traditional scheme, respectively. This is due to the fact that traditional schemes extract all the details of the video frame, and completely reconstruct the encoded frame under a high bitrate while the ABRVSC scheme can effectively extract the semantic information of video frames thus achieving better performance with low bitrates. Fig. 8 demonstrates that the proposed ABRVSC scheme can achieve higher semantic accuracy under low bitrates compare to traditional coding scheme."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we proposed an ABRVSC system and first defined the QoE for video semantic communication and formulated a QoE maximization problem. To solve problem, we\nthen proposed a swin transformer-based semantic codec and introduced an Actor-Critic based ABR algorithm to the semantic codec. Simulation results demonstrates that at low bitrates, the MIoU of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme increases the QoE in video semantic communication, which exhibits more robustness against network variations."
        }
    ],
    "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
    "year": 2023
}