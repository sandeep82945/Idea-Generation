{
    "abstractText": "Unmanned aerial vehicles (UAVs) are promising remote sensors capable of reforming remote sensing applications. However, for artificial intelligence (AI)-guided tasks, like land cover mapping and ground-object mapping, most deep learning-based architectures fail to extract scale-invariant features, resulting in poor performance accuracy. In this context, the article proposes a superpixel-aided multiscale convolutional neural network (CNN) architecture to avoid misclassification in complex urban aerial images. The proposed framework is a twotier deep learning-based segmentation architecture. In the first stage, a superpixel-based simple linear iterative cluster (SLIC) algorithm produces superpixel images with crucial contextual information. The second stage comprises a multiscale CNN architecture that uses these information-rich superpixel images to extract scale-invariant features for predicting the object class of each pixel. Two UAV-image-based aerial image datasets: NITRDrone dataset and urban drone dataset (UDD) are considered to perform the experimentation. The proposed model outperforms the considered state-of-the-art methods with an intersection of union (IoU) of 76.39% and 86.85% on UDD and NITRDrone datasets, respectively. Experimentally obtained results prove that the proposed architecture performs superior by achieving better performance accuracy in complex and challenging scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tanmay Kumar Behera"
        },
        {
            "affiliations": [],
            "name": "Sambit Bakshi"
        }
    ],
    "id": "SP:cb99b008145e195cc956b9f30d92e6ded45853cf",
    "references": [
        {
            "authors": [
                "A.-V. Emilien",
                "C. Thomas",
                "H. Thomas"
            ],
            "title": "UAV & Satellite Synergies for Optical Remote Sensing Applications: A Literature Review,",
            "venue": "Science of Remote Sensing,",
            "year": 2021
        },
        {
            "authors": [
                "E. Shelhamer",
                "J. Long",
                "T. Darrell"
            ],
            "title": "Fully Convolutional Networks for Semantic Segmentation,",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "L.-C. Chen",
                "Y. Zhu",
                "G. Papandreou",
                "F. Schroff",
                "H. Adam"
            ],
            "title": "Encoder- Decoder with Atrous Separable Convolution for Semantic Image Segmentation,",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "A. Garcia-Garcia",
                "S. Orts-Escolano",
                "S. Oprea",
                "V. Villena-Martinez",
                "J. Garcia-Rodriguez"
            ],
            "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation,",
            "venue": "arXiv preprint arXiv:1704.06857,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Guo",
                "Y. Liu",
                "T. Georgiou",
                "M.S. Lew"
            ],
            "title": "A Review of Semantic Segmentation using Deep Neural Networks,",
            "venue": "International Journal of Multimedia Information Retrieval,",
            "year": 2018
        },
        {
            "authors": [
                "B. Cheng",
                "G. Liu",
                "J. Wang",
                "Z. Huang",
                "S. Yan"
            ],
            "title": "Multi-task Low-rank Affinity Pursuit for Image Segmentation,",
            "venue": "in 2011 International Conference on Computer Vision,",
            "year": 2011
        },
        {
            "authors": [
                "Z. Li",
                "X.-M. Wu",
                "S.-F. Chang"
            ],
            "title": "Segmentation using Superpixels: A Bipartite Graph Partitioning Approach,",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "H. Tong",
                "F. Tong",
                "W. Zhou",
                "Y. Zhang"
            ],
            "title": "Purifying SLIC Superpixels to Optimize Superpixel-based Classification of High Spatial Resolution Remote Sensing Image,",
            "venue": "Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "D. Comaniciu",
                "P. Meer"
            ],
            "title": "Mean Shift: A Robust Approach Toward Feature Space Analysis,",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2002
        },
        {
            "authors": [
                "M.A. Fischler",
                "J.M. Tenenbaum",
                "H.C. Wolf"
            ],
            "title": "Detection of Roads and Linear Structures in Low-Resolution Aerial Imagery Using a Multisource Knowledge Integration Technique,",
            "venue": "Readings in Computer Vision. Elsevier,",
            "year": 1987
        },
        {
            "authors": [
                "U. Stilla"
            ],
            "title": "Map-aided Structural Analysis of Aerial Images,",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing,",
            "year": 1995
        },
        {
            "authors": [
                "T. Leung",
                "J. Malik"
            ],
            "title": "Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons,",
            "venue": "International Journal of Computer Vision, vol. 43,",
            "year": 2001
        },
        {
            "authors": [
                "C. Schmid"
            ],
            "title": "Constructing Models for Content-based Image Retrieval,",
            "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2001
        },
        {
            "authors": [
                "P. Viola",
                "M. Jones"
            ],
            "title": "Rapid Object Detection using a Boosted Cascade of Simple Features,",
            "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2001
        },
        {
            "authors": [
                "B. Fr\u00f6hlich",
                "E. Bach",
                "I. Walde",
                "S. Hese",
                "C. Schmullius",
                "J. Denzler"
            ],
            "title": "Land Cover Classification of Satellite Images using Contextual Information,",
            "venue": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "P. Tokarczyk",
                "J.D. Wegner",
                "S. Walk",
                "K. Schindler"
            ],
            "title": "Features, Color Spaces, and Boosting: New Insights on Semantic Classification of Remote Sensing Images,",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing,",
            "year": 2015
        },
        {
            "authors": [
                "D. Chai",
                "W. F\u00f6rstner",
                "F. Lafarge"
            ],
            "title": "Recovering Line-Networks in Images by Junction-Point Processes,",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        },
        {
            "authors": [
                "M. Ortner",
                "X. Descombes",
                "J. Zerubia"
            ],
            "title": "Building Outline Extraction from Digital Elevation Models Using Marked Point Processes,",
            "venue": "International Journal of Computer Vision, vol. 72,",
            "year": 2007
        },
        {
            "authors": [
                "J.D. Wegner",
                "J.A. Montoya-Zegarra",
                "K. Schindler"
            ],
            "title": "A Higher-Order CRF Model for Road Network Extraction,",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        },
        {
            "authors": [
                "Y. Wang",
                "W. Ding",
                "B. Zhang",
                "H. Li",
                "S. Liu"
            ],
            "title": "Superpixel Labeling Priors and MRF for Aerial Video Segmentation,",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2019
        },
        {
            "authors": [
                "K. Fukushima",
                "S. Miyake"
            ],
            "title": "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition ,",
            "venue": "Competition and Cooperation in Neural Nets. Springer,",
            "year": 1982
        },
        {
            "authors": [
                "Y. LeCun",
                "B. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W. Hubbard",
                "L.D. Jackel"
            ],
            "title": "Backpropagation Applied to Handwritten Zip Code Recognition,",
            "venue": "Neural Computation,",
            "year": 1989
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet Classification with Deep Convolutional Neural Networks,",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein"
            ],
            "title": "ImageNet Large Scale Visual Recognition Challenge,",
            "venue": "International Journal of Computer Vision, vol. 115,",
            "year": 2015
        },
        {
            "authors": [
                "N. Attari",
                "F. Ofli",
                "M. Awad",
                "J. Lucas",
                "S. Chawla"
            ],
            "title": "Nazr-CNN: Fine- Grained Classification of UAV Imagery for Damage Assessment,",
            "venue": "IEEE International Conference on Data Science and Advanced Analytics (DSAA),",
            "year": 2017
        },
        {
            "authors": [
                "D. Du",
                "Y. Qi",
                "H. Yu",
                "Y. Yang",
                "K. Duan",
                "G. Li",
                "W. Zhang",
                "Q. Huang",
                "Q. Tian"
            ],
            "title": "The Unmanned Aerial Vehicle Benchmark: Object detection and Tracking,",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV). Springer,",
            "year": 2018
        },
        {
            "authors": [
                "C. Farabet",
                "C. Couprie",
                "L. Najman",
                "Y. LeCun"
            ],
            "title": "Learning Hierarchical Features for Scene Labeling,",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "X.X. Zhu",
                "D. Tuia",
                "L. Mou",
                "G.-S. Xia",
                "L. Zhang",
                "F. Xu",
                "F. Fraundorfer"
            ],
            "title": "Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources,",
            "venue": "IEEE Geoscience and Remote Sensing Magazine,",
            "year": 2017
        },
        {
            "authors": [
                "A. Van Etten"
            ],
            "title": "You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery,",
            "venue": "arXiv preprint arXiv:1805.09512,",
            "year": 2018
        },
        {
            "authors": [
                "M. Dai",
                "J. Hu",
                "J. Zhuang",
                "E. Zheng"
            ],
            "title": "A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo- Localization,",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "Q. Liu",
                "Y. Wang"
            ],
            "title": "Road Extraction by Deep Residual U-Net,",
            "venue": "IEEE Geoscience and Remote Sensing Letters,",
            "year": 2018
        },
        {
            "authors": [
                "F. Bastani",
                "S. He",
                "S. Abbar",
                "M. Alizadeh",
                "H. Balakrishnan",
                "S. Chawla",
                "S. Madden",
                "D. DeWitt"
            ],
            "title": "RoadTracer: Automatic Extraction of Road Networks from Aerial Images,",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "T.K. Behera",
                "P.K. Sa",
                "M. Nappi",
                "S. Bakshi"
            ],
            "title": "Satellite IoT Based Road Extraction from VHR Images Through Superpixel-CNN Architecture,",
            "venue": "Big Data Research,",
            "year": 2022
        },
        {
            "authors": [
                "T. Kattenborn",
                "J. Leitloff",
                "F. Schiefer",
                "S. Hinz"
            ],
            "title": "Review on Convolutional Neural Networks (CNN) in Vegetation Remote Sensing,",
            "venue": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation,",
            "venue": "in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer,",
            "year": 2015
        },
        {
            "authors": [
                "J. Xie",
                "N. He",
                "L. Fang",
                "P. Ghamisi"
            ],
            "title": "Multiscale Densely-Connected Fusion Networks for Hyperspectral Images Classification,",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhou",
                "H. Kong",
                "L. Wei",
                "D. Creighton",
                "S. Nahavandi"
            ],
            "title": "On Detecting Road Regions in a Single UAV Image,",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2016
        },
        {
            "authors": [
                "L. Sommer",
                "T. Schuchert",
                "J. Beyerer"
            ],
            "title": "Comprehensive Analysis of Deep Learning-Based Vehicle Detection in Aerial Images,",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology,",
            "year": 2018
        },
        {
            "authors": [
                "T.K. Behera",
                "S. Bakshi",
                "P.K. Sa"
            ],
            "title": "Aerial Data Aiding Smart Societal Reformation: Current Applications and Path Ahead,",
            "venue": "IEEE IT Professional,",
            "year": 2020
        },
        {
            "authors": [
                "T.K. Behera",
                "M.A. Khan",
                "S. Bakshi"
            ],
            "title": "Brain MR Image Classification Using Superpixel-Based Deep Transfer Learning,",
            "venue": "IEEE Journal of Biomedical and Health Informatics,",
            "year": 2022
        },
        {
            "authors": [
                "T.K. Behera",
                "S. Bakshi",
                "P.K. Sa"
            ],
            "title": "Vegetation Extraction from UAV-based Aerial Images through Deep Learning,",
            "venue": "Computers and Electronics in Agriculture,",
            "year": 2022
        },
        {
            "authors": [
                "F. Yu",
                "V. Koltun"
            ],
            "title": "Multi-Scale Context Aggregation by Dilated Convolutions,",
            "venue": "arXiv preprint arXiv:1511.07122,",
            "year": 2015
        },
        {
            "authors": [
                "S. J\u00e9gou",
                "M. Drozdzal",
                "D. Vazquez",
                "A. Romero",
                "Y. Bengio"
            ],
            "title": "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "T.K. Behera",
                "S. Bakshi",
                "P.K. Sa",
                "M. Nappi",
                "A. Castiglione",
                "P. Vijayakumar",
                "B.B. Gupta"
            ],
            "title": "The NITRDrone Dataset to Address the Challenges for Road Extraction from Aerial Images,",
            "venue": "Journal of Signal Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Chen",
                "Y. Wang",
                "P. Lu",
                "G. Wang"
            ],
            "title": "Large-Scale Structure from Motion with Semantic Constraints of Aerial Images,",
            "venue": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Springer,",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization,",
            "venue": "International Conference on Learning Representations,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014VHR, Deep Learning, Aerial Image, UAV, CNN, Mutliscale CNN, Superpixel, SLIC, Semantic Segmentation\nI. INTRODUCTION\nOUR world has come a long way since the launch ofthe first satellite into space, and we are in an era fifty centuries ahead of it, significantly changing our daily lives. The technological advancement in space technology and remote sensing (RS) sector can be analyzed from the evergrowing number of operated satellites around the earth since 1957 till date. As per one statistic, a remarkable jump of 1, 070 satellites is noticed in 2019 to 2020 making the total number of satellites to 3, 368 [1].\nA massive number of very high resolution (VHR) images are on a daily basis by these earth observation satellites such as the WorldView series, Landsat series, and RESOURCESAT [2], [3]. These captured images have been used to address many societal issues at a higher level through different RS\nManuscript received xxxx xx, 2022; revised xxxx xx, 2022. Tanmay Kumar Behera, Sambit Bakshi, Pankaj Kumar Sa are with the Department of Computer Science and Engineering, National Institute of Technology Rourkela, India, 769008. (e-mail: tanmay.nitr@gmail.com, sambitbaksi@gmail.com, pankajsa@nitrkl.ac.in)\nMichele Nappi is with the Department of Computer Science, University of Salerno, Italy (e-mail: mnappi@unisa.it)\napplications. However, certain gaps in satellite-based RS applications make it difficult to go through the tropical regions, which are mostly covered by clouds [4]. This opens up a space for the new edge remote sensors in the form of UAVs that can truly improve the spatial, temporal, and spectral resolution of satellite-captured data at different scales. UAVs can help satellites overcome their limitations and accomplish particular tasks through real-time assessment and monitoring actions in different scenarios. These small devices have taken their usage to a whole different level, managing various issues of our dayto-day lives through several RS applications such as traffic management, urban management in smart cities, land cover classification, fishery management, forest area management, etc. at a lower scale as compared to the satellites.\nMostly, the images captured by the UAVs are of high resolution and provide a detailed view of a particular area in a scene. Among several image data acquisition tasks for UAV-based RS images, semantic segmentation is one of the emerging and challenging areas for computer vision researchers. Here, the task is to predict the pixel-level object class according to the semantic information represented by that pixel in the captured aerial image. Recent years have witnessed tremendous progress in deep learning-based approaches like CNNs, which have proved their significance in attending semantic segmentation tasks [5]\u2013[7].\nUAV-based aerial image analysis systems differ from satellite image analysis systems concerning their use cases and approaches to solving tasks in various application domains. Some of these applications include detecting objects, such as roads, buildings, vegetation, and vehicles that play a vital role in critical applications like military target identification and damage estimation and rescue operations in natural disasters [8], [9]. Therefore, developing a robust aerial image segmentation algorithm is needed for such critical tasks. However, several inherent challenges, such as image resolution, large field of view (FOV), and diversified and complex backgrounds, make the task more challenging (in UAV-inspired segmentation tasks). Many popular semantic segmentation frameworks designed for satellite-captured images are unsuitable for UAVborne remote sensing image-based tasks. It is generally due to the specificity of UAV-captured RS images. Another area for improvement of these popular approaches is that the purpose of UAV-inspired remote sensing differs from satellite remote sensing. Satellite-borne RS images focus on object extraction and land cover analysis in a larger area. In contrast, UAV-\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 borne RS images are meant to extract information at a smaller scale in a smaller area. Hence, these large numbers of highresolution UAV remote sensing images aim to analyze the objects more accurately. It is because the UAV-borne remote sensing images possess richer contextual information to work on addressing the UAV RS-inspired tasks."
        },
        {
            "heading": "A. Motivation",
            "text": "1) Motivation of using superpixel algorithm: A group of pixels can be termed a superpixel, where the members of the superpixels share some common attributes compared to the non-members. As suggested by definition, the use of superpixel techniques is more beneficial for image segmentation tasks [10], [11]. Superpixel images have several advantages, such as reducing the computational cost by representing pixels inside a superpixel. Thus, they can be used to reduce the overhead incurred by the deep learning frameworks in terms of time and memory. Similarly, superpixels can extract essential regional features, which are more distinctive than the standard pixel-wise features used in several computer vision tasks. They are adaptive due to their shape and size, containing more local and spatial features [12]. Thus, having these features, superpixels can be generated at different scales, which can be used in multiscale-inspired applications with specific parameter settings [13].\n2) Motivation of using a multiscale architecture: The traditional segmentation techniques could perform better due to their low generalization ability. Thus, developing a deep learning-based robust framework is essential to strengthening the aerial image segmentation process. However, certain underlying complications in these deep learning frameworks could lead to false classification. The issues lie within the process through which the image patches are fed to the architecture during training. The CNN architecture misses many high-level feature sets with strict image sizes, thereby losing crucial contextual information. These missing features are essential in multi-object semantic segmentation, especially in aerial images. The different flight heights of a UAV can create ambiguity for a model leading to poor generalization for several small-scale objects. It is where a multiscale sampling process can become a savior in extracting and gathering the spatial-level object features. Multiscale features are desirable to realize the abstraction of the image at different scales. Introducing a multiscaling process to the CNN framework can help it learn multiple heterogeneous scale-invariant features, which can lower the misclassification rate."
        },
        {
            "heading": "B. Contribution",
            "text": "In this work, we have proposed a multiscale CNN framework for UAV-captured images. In addition to this, the proposed approach benefits from the SLIC-inspired superpixel techniques to generate the superpixel images, which act as the input for the multiscale CNN architecture. Some of the major contributions of this work are summarized as follows:\n\u2022 The proposed deep-learning framework is a two-staged architecture for aerial scene segmentation. The first stage uses the UAV-captured images to perform coarse-level\nsegmentation using the SLIC superpixel technique to generate superpixel images. Superpixels carry more spatial information than normal pixels and provide a more compact and convenient representation. Hence, they are useful for computationally demanding applications. \u2022 In the second stage, a multiscale CNN architecture is proposed to analyze the given superpixels for pixel-level classification. Here, the superpixel images are sampled at different scales to the multiscale module to extract the scale-invariant features to perform multiclass segmentation. \u2022 The proposed model is evaluated over the two UAVborne aerial image datasets to ensure the robustness of the proposed architecture in real-world settings. \u2022 Moreover, the model is also evaluated by changing some important parameters to show its improved behavior with the superpixel and multiscale convolution to detect smallscale ground objects.\nThe rest of the manuscript is organized as follows: The existing semantic object segmentation approaches are discussed in Section II. Similarly, Section III presents the different methodologies used in the proposed approach, which is followed by Section IV. Section IV and Section V discuss the detailed structure of implementation and overview of the obtained results, respectively. Similarly, a discussion section is also added as Section VI. Finally, Section VII briefly describes the conclusion drawn from the whole article."
        },
        {
            "heading": "II. RELATED WORK AND BACKGROUND STUDY",
            "text": "This section briefly discusses the different approaches proposed by the researchers in aerial scene understanding. The evolution of deep learning and multi-scale learning algorithms towards aerial scene understanding problems are discussed in this section."
        },
        {
            "heading": "A. Traditional Approaches in Aerial Image Segmentation",
            "text": "Aerial images are the images of the earth captured from above it, where the space-borne remote sensors or satellites were the only option until the UAV-based technology pitched in this work to leverage the load incurred on a satellite at a lower scale. These devices have been widely used in various RS applications such as ground objects detection: cars, roads, buildings, trees, and pedestrians, which is an essential aspect of many projects viz. agriculture mapping, urban mapping, forest mapping, etc. The UAV images are a bit complicated compared to the satellite images due to the detailed and vast population of diversified objects, making the task more complex and challenging. Previously attempted research works by computer vision researchers are based on the rule descriptor influenced methods for object-level feature extraction, specifically in building extraction [14], road detection [15], [16]. However, due to poor generalization concerning aerial data, the hierarchical rule-based approaches miss out on several significant features. Conventional classifiers employed machine learning techniques that extract the local features from the input pixel intensities through simple arithmetic combinations [17], [18]. Researchers also proposed discriminating classifiers\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3 Scale 1\nScale 2\nScale M\n+\nPrediction\nMultiscale CNN\nImgae Tiles of a same image at different scales\n(Encoder-Decoder Structure)\nInput Image\nEncoder-Decoder Module-1\nEncoder-Decoder Module-2\nEncoder-Decoder Module-M\nScale 1\nScale 2\nScale M\nFigure 1. Architectural framework of multiscale CNN architecture\nlike boosting and random forest to evaluate the redundant local feature maps for training purposes [19]\u2013[21]. In an aerial image segmentation problem, the global features are equally essential as local features [22]. In [23], authors have used marked point processes to build architectural models and road network topologies through probabilistic priors defined for global knowledge gain. Conditional random fields, also known as CRFs, are also used for object-level segmentation, and detection from the aerial images [24]. Similarly, Wang et al. [25] have proposed a fusion approach using a superpixel-based labeling technique and Markov random field towards aerial video segmentation."
        },
        {
            "heading": "B. Deep Learning Approaches in Aerial Image Segmentation",
            "text": "Unlike conventional machine-learning techniques, deeplearning algorithms have no requirement for feature definition steps. They learn the critical distinguishing features from an input dataset according to the provided task. These methodologies were proposed back in the 80\u2019s at a time when there was limited computing power and available training data [26], [27]. These algorithms announced their return with [28] in 2012 and managed to achieve impressive outcomes for the ImageNet challenge [29], creating hope in the research community with tons of opportunities. Several layers are stacked on one another in the proposed baseline models to learn and analyze the essential local-global feature sets from the input images. One of the crucial aspects of deep CNN architectures lies in its ability to parallelize both training and inference through GPUs.\nCNN has started its journey with the image classification problem, and in a short period, they have been successfully able to address computer vision problems like object detection [30], tracking [31], and object-level segmentation [32]. The usage of convolution network frameworks has not been restricted to classical classification tasks but can also be noticed in aerial scene parsing using RS images [33]. A number\nof common RS tasks in this domain comprise buildings extraction [34], [35], road networks extraction [36]\u2013[38], vegetation extraction [39]. Aerial scene understanding based on an encoder-decoder-based fully convolutional network (FCN) structure is proposed by [5], [40] that yields an explicit label image depicting the contexts associated with each pixel. Then the extracted feature maps propagate through an expansion module to up-sample the reduced image back to the original resolution. In [41], Xie et al. have proposed a multiscale densely-connected CNN architecture for remote sensing-based hyperspectral aerial image (HSAI) classification. Similarly, Fan et al. [42] have presented a superpixel-aided deep-sparserepresentation technique to construct hierarchical architecture to understand HSAI context information. This gathered information (features) obtained from the multi-layered network is concatenated and trained by a support vector machine (SVM) classifier. Moreover, for small-scale applications, UAV usage is increasing, and collected data have been utilized in many crucial RS applications. Computer vision researchers [43], [44] have provided several solution approaches to address the existing issues using deep learning-based architectures. Authors have recommended a deep learning-based framework inspired by Fast R-CNN and Faster R-CNN for vehicle extraction from aerial images [45]. The two networks are combined to gather important feature space, which can be used to detect vehicles semantically. Moreover, datasets are the backbone of the success behind deep learning frameworks. A thorough and detailed analysis of the available UAV image datasets for computer vision researchers in conducting research towards UAV-inspired applications is presented in [46]."
        },
        {
            "heading": "III. PROPOSED METHODOLOGY",
            "text": "The manuscript proposes a superpixel-aided multiscale deep learning framework that semantically segments the aerial im-\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 Su pe rp ix el Segmentation scale 1\nscale 3\nscale 2\nFigure 2. Generation of images at multiple scales from the superpixel images\nages captured by UAVs. This section discusses each module used in the proposed deep architecture."
        },
        {
            "heading": "A. Overview",
            "text": "The proposed framework consists of two modules: a superpixel module and a multiscale CNN module to work on extracting the scale-invariant features. In the backend of the architecture, the superpixel algorithm works to determine the essential scale-invariant features. As the first phase of the whole segmentation process, the superpixel technique narrows down the texture and color-based features. These extracted features are considered the input to the second phase of the proposed framework, where these superpixel images are used to produce the final segmentation map. The superpixel images help the deep learning architecture to be implemented quickly, reducing the overall training and validation/testing time (in most instances). The architectural overview is presented in Fig. 1. Each module of the proposed architecture is explained in the following subsections."
        },
        {
            "heading": "B. Superpixel Method",
            "text": "A number of pixels sharing common characteristics can be referred to as a superpixel. They can carry more information than simple pixels and provide a more convenient and compact representation that could be useful for computationally demanding applications. Some of these applications include medical imaging [47], object detection, scene segmentation, video surveillance, etc. Among the superpixel algorithms, Simple Linear Iterative Clustering, also known as SLIC, has been widely used [48], [49].\nGenerally, SLIC-based superpixel algorithms generate relatively uniform and compact superpixels based on the spatial and color proximity of pixels in an image plane. Five dimensional (5D) [la\u03b2xy] space is utilized by this approach, where [la\u03b2] represents the pixel color vector and [xy] indicates the position of a pixel. Hence, it should be normalized so that the\nEuclidean distance can be employed in 5D space. Hence, the maximum spatial distance within a cluster should lie within a sampling interval, S, and can be represented as follows:\nS =\n\u221a N\nK (1)\nwhere, N = Number of pixels in the input image K = Number of Superpixels required N K = Approximate area of a superpixel\nThe superpixel algorithm considers the desired number of superpixels of approximately equal sizes (K). The cluster centers Ck can be represented as Ck = [lk, ak, bk, xk, yk], where k varies between a range of 1 to K at a regular interval of S within a grid. The spatial extent of a superpixel is generally S2 (approximate area of a superpixel). Thus, an assumption can be made corresponding to its cluster center that associated pixels fall within a region 2S\u00d72S area around the superpixel head-on xy plane. Hence, the normalized distance (Ds) can be calculated as the sum of the lab color space distance (dla\u03b2) and XY plane distance (dxy) normalized by the grid interval S and is given as follows:\nDs = dla\u03b2 + ( m\nS ) \u2217 dxy (2)\nwhere, dla\u03b2 = \u221a (lk \u2212 li)2 + (ak \u2212 ai)2 + (bk \u2212 bi)2\ndxy = \u221a (xk \u2212 xi)2 + (yk \u2212 yi)2\nm = Maximum Color Distance Like spatial distance, the color-related distance plays a crucial role in estimating the normalized distance (Ds) in SLIC algorithm. Estimating color distance is a complex job as the color-based distance may vary rapidly from cluster to image and image to image. Thus, to avoid such a problem, a constant m is introduced that controls the compactness of a superpixel. The higher the value of m, the more compact the cluster is. Reducing the compactness factor (m) (lied within [5\u2212 40]) gives us images that are more closely related to the original images keeping the relevant object features.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5 The Superpixel module acts as the first-level optimizer to transform complex aerial images into more compact-sized superpixel images. Pixels representing a single superpixel share similar visual attributes in a superpixel image. Thus, the superpixel images carry more information values than the usual ones. The UAV-based VHR aerial imageries are given as inputs to the superpixel module to produce the superpixel images using a SLIC-based algorithm. It is a lineartime algorithm and can generate superpixel images that are lightweight in terms of memory space, thus consuming less storage space. They can provide a compact and convenient representation of standard images, which can be very useful for computationally demanding applications that process RS images in a low-bandwidth environment. Further optimization takes place at the CNN module on these superpixel images."
        },
        {
            "heading": "C. Convolutional Neural Network",
            "text": "Convolutional neural networks (also known as CNNs/ConvNets) are enhanced neural networks most commonly applied to analyze visual images. The structure of CNN is distinctive; one convolutional layer stacks upon another, followed by a few pooling layers, and finally, a few Fully-connected layers (for image classification) or upsampling layers (for image segmentation). The convolutional layer is the core of a CNN, which extracts the high-level features through the local perception and weight-sharing mechanism of the kernels/filters. The pooling layer can be considered the backbone of CNN used as a stuffing layer of a sandwich between the two slices of convolutional layers. It is used to enhance efficiency and avoid over-fitting in training procedures. It downsamples the input feature map using a non-linear max function that reduces the number of parameters to be used for calculations in the following Convolutional layers. The deep architecture used in the proposed multiscale CNN (MCNN) approach is an encoder-decoder-based convolutional framework (also known as AerialSegNet [50]) that is composed of four stages: (a) Contraction Path: The input RGB images get decomposed to provide spatial and temporal features through convolution operations. (b) Dense modules: Each stage of the architecture contains densely connected modules to pass the learned feature maps to the follow-up stages to enhance the feature set without increasing the number of parameters. (c) Bottleneck layer: At this stage, the extracted features from the contraction path are then fed to the decoder blocks in the expansion path. (d) Expansion path: Here, the shrunken image (in the encoder path) is reshaped to its original shape to produce the desired segmented map through some deconvolution operation (using transpose convolution or bilinear interpolation techniques). The overview of the architecture can be seen from Fig. 1\u2019s middle blocks, where the combined use of dense connections and skip connections can be observed."
        },
        {
            "heading": "D. Multiscale module",
            "text": "The correctness and accuracy of the image segmentation model need to integrate pixel-level accuracy concerning mul-\ntiscale context reasoning. Deep CNNs combine multiscale context feature maps based on consecutive pooling, and convolution layers reduce resolution [28]. Moreover, the dense/deeper layers require context information in addition to full resolution [51]. The input images can be downscaled and upscaled with proper interpolation technique to get the multiscaled resolution images Fig. 2. As mentioned in Fig. 1, these multiscale images were given as inputs to the corresponding CNN modules to obtain the scale-invariant feature sets. Each CNN framework processes an image scene with different scales extracting the multiscale feature maps, which are further aggregated to form a multiscale context feature map that can predict pixel-level object class. The aggregation process is performed under the resize and concatenation process to make the process simple. The process of aggregation can be understood from the following equations:\nMimg = Ds + Us + Iimg (3)\nwhere,\nDs = dsf1(Iimg) + dsf2(Iimg) + ...+ dsfm(Iimg) (4)\nUs = us1/f1(Iimg)+us1/f2(Iimg)+ ...+us1/fm(Iimg) (5)\nHere, ds, us, represent downsampling and upsampling of an input image, respectively. Similarly, f, Iimg and Mimg denote the scale factor used for downsampling or upsampling, input image and the obtained multiscale feature map, respectively.\nIn our experiment, we have used 512\u00d7 512 image tiles as input, which are then upscaled and downscaled by a factor of 2 to get 256 \u00d7 256, 1024 \u00d7 1024 resolution images. All these three different resolution images are trained individually through the encoder-decoder CNN architecture to fetch the multiscale feature maps that decide the pixel class."
        },
        {
            "heading": "IV. EXPERIMENTATION",
            "text": "In order to access the performance of the proposed ensemble superpixel-MCNN architecture, extensive experimentation has been conducted on the NITRDrone scene understanding dataset and is described in the section IV-A. Moreover, the proposed approach is compared to some of the chosen stateof-the-art methodologies of semantic segmentation tasks, viz. [5]\u2013[7], [40], [52]."
        },
        {
            "heading": "A. Data Description",
            "text": "To perform the experimentation, we have considered the following two datasets:\n1) NITRDrone Dataset: The NITRDrone dataset 1 [53] is proposed and built on seeing the rising demand for UAVbased applications for scene understanding that uses semantic segmentation-based techniques. The dataset contains around 101 number of variable resolution of VHR images captured with the help of DJI Phantom 4 and DJI Mavic drone having ground sampling distance (GSD) of 0.025 sq. cm/pixel. The resolution of an image in the dataset can be any of the following 1280\u00d7 720, 4000\u00d7 3000, 4096\u00d7 2160. A pixel can\n1https://github.com/drone-vision/NITRDrone-Dataset\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 Background\nRoad\nVegetation\nOccluded road\nbelong to any of the four different considered classes named \u201croad\u201d, \u201cvegetation\u201d, \u201coccluded road,\u201d and \u201c background .\u201d Some of the sample images and their corresponding ground truths of the dataset are presented in Fig. 3.\n2) Urban Drone Dataset (UDD): The UDD is a UAVbased image dataset that was proposed by Chen et al. towards semantic segmentation problems in computer vision [54]. The dataset is collected by a UAV DJI Phantom 4 operated at an altitude of 60m to 100m. The considered resolution for each image in the dataset is either 3000\u00d74000 or 4096\u00d72160. This dataset has been divided into three types, UDD-3, UDD-5, and UDD-6, that have three, four, and five classes, respectively, of which we have considered UDD-4, on which the proposed model is implemented and validated. As mentioned, UDD4 has four-pixel classes named vegetation, buildings, roads, vehicles, and others (denoted for the rest of the object in a scene other than the mentioned classes). The dataset comprises of two sets training sets and a validation set consisting of 160 images and 45 frames, respectively. Sample images and the\nmasks are shown in Fig. 4."
        },
        {
            "heading": "B. System Setup",
            "text": "In the proposed architecture, the first stage is meant for the SLIC superpixel algorithm to produce superpixel images. These superpixel images are considered as inputs for the second stage and are sampled at different scales to multiple deep CNN frameworks, which are trained to extract the required features for further classification of the pixels into one of the four classes in NITRDrone dataset and one of the five classes in UDD. The flow of operations to perform the experimentation is presented in Fig. 5. The implementation and validation of the proposed architecture are carried out on the datasets mentioned above and compared with the benchmark and peer-reviewed state-of-the-art methods. All the considered models are implemented with the help of the deep learning library PyTorch 2 [55] and are trained with NVIDIA TITAN V graphics card having 12GB of GPU memory."
        },
        {
            "heading": "C. Dataset Pre-processing",
            "text": "The proposed architecture is evaluated on the semantic drone datasets NITRDrone dataset [53] and UDD [54]. The resolution of the images of the considered datasets is of different sizes, such as 1280\u00d7720, 4000\u00d73000, 4096\u00d72160. Hence, we apply a sliding window technique with a constant stride that works over these images to extract the image tiles of 576\u00d7 576 from both datasets. Through this operation, we are able to generate around 3, 470 number of images from the NITRDrone dataset and 3, 500 number of images from the UDD dataset. Out of the total number of images extracted from the NITRDrone dataset, we have considered 2, 590, 880 images as training and testing sets, respectively. Similarly, for the UDD, 3, 100 images are considered for training the model, and the rest 400 images are equally divided among validation and testing set."
        },
        {
            "heading": "D. Pre-processing with SLIC",
            "text": "It is the first phase of segmentation in our proposed architecture. The image tiles produced by the sliding window are fed to this module. One of the popular superpixel algorithms, SLIC, is applied to produce semi-segmented superpixel images. There are two important parameters of SLIC algorithms: N and m representing the number of superpixels in a superpixel image and compactness control parameter, respectively. They play a vital role in preserving the natural properties of the ground objects. We have considered different combinations of N and m to find out the best combination with which we can apply the SLIC algorithm on the raw input images that preserve the integral properties of the objects to be segmented. The value of N and m are initialized to certain constant values as N = [500, 1000] and m = [5, 15, 25, 35]. Thus, eight types of superpixel images are generated from this module, which will be the inputs for the next stage of CNN implementation.\n2https://pytorch.org/docs/stable/index.html.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7 (Input Image) (Cropped image) SLIC Prediction? (Road) (or) (Vegetation) (Multiscale CNN Module) (SLIC-Segmentation Result) Encoder-Decoder CNN Encoder-Decoder CNN Encoder-Decoder CNN\n(Multiscale Image Sampling)\nTrained Model\nTesting Images\n(or) (Occlusions)\nFigure 5. Flow diagram of the experimentation"
        },
        {
            "heading": "E. Training",
            "text": "1) Input Pre-processing: The images from the superpixel module can be denoted as Image X . These Image X are collected at the CNN module, where they have to pass through a simple pre-processing step before considering for training. Image X are then down-sampled from 576\u00d7576 to 512\u00d7512 (can be denoted as Image Y ) through random cropping or center cropping techniques. These cropped images (Image Y ) are used by the deep CNN framework to train individual models. The resolution of the input and target images for the proposed architectures and state-of-the-art methods remains the same. The only difference lies in the type of images considered in both cases: the proposed approach uses superpixel aerial images, whereas the state-of-the-art models use stock aerial imageries.\n2) Target Pre-processing: The corresponding target or the masks need to be down-scaled to 512 \u00d7 512 as per the input images described in the previous section. However, the training is performed with the one-hot coded masks. The one-hot coded target images are then color-coded with different colors for each object class for better visualization. These RGB colorcoded masks are presented in Fig. 3 and 4.\n3) Implementation Details: Adaptive moment estimation (Adam) [56] is considered the optimizer, and Cross-Entropy (CE) is used as a loss function. The learning rate is initialized to 5e \u2212 3, whereas momentum and batch size are initialized to 0.9, and 3, respectively. A weight decay of 0.002 is introduced to handle the problem of over-fitting in the long run during training. ReLU activation function [57] is employed to improve the convergence and accuracy of the network. Moreover, two types of augmentation techniques are applied to the superpixel images making the number of images stand double, thus extending the training process. The learning rate is set to be decreased after every 30 epochs by a factor of 0.002 to maintain the regularization. The training operation continues until the learning rate reaches 10\u221220. After 450 epochs, the proposed architecture seems to converge, which can be observed through minor changes in loss and accuracy."
        },
        {
            "heading": "F. Loss Function",
            "text": "The choice of the loss function is vital in carrying out neural network-based optimization. The loss-weighting scheme of\nthe network architecture targets the interior pixels and the border of the segmented objects. The Cross-Entropy (CE) loss, also known as logarithmic or logistic loss, is chosen to train the baseline models. The predicted class probability is compared with the truly desired class output 0 or 1. The corresponding loss/score of the corresponding pixel class is obtained to check for the deviation from the actual (true) value. And as a penalization, the weights will travel backward to recorrect the same for a better understanding of the object feature map. SoftMax differential function (Si) is also used with CE, which aims at minimizing the loss during training, i.e., smaller the loss value better the model. Cross-entropy can be defined as follows:\nLCE = \u2212 n\u2211\ni=1\nTi log(Si) (6)\nwhere, Ti and Si are the truth value \u2208 [0, 1] and the SoftMax Probability for ith class, respectively."
        },
        {
            "heading": "G. Tasks and metrics",
            "text": "The primary objective of the proposed SLIC-M -CNN framework is scene parsing and segments the aerial images as per the given number of objects (four for the NITRDrone dataset and five for the UDD). In order to analyze the architecture\u2019s performance, both quantitative and qualitative results play vital roles. Widely acceptable performance metrics, such as precision, recall, F-Score, the intersection of union, and overall accuracy, are used to examine the performance of the proposed framework. These metrics can be formulated as per the Eqs. 7- 11.\nP = TP\n(TP + FP ) (7)\nR = TP\n(TP + FN) (8)\nIoU = Overlapping Area\nArea of Union\n= TP\n(TP + FP + FN)\n(9)\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nHere, P , R, IoU , F , and A represent precision, recall, dice score, intersection over union, and overall accuracy, respectively. Similarly, TP and TN stand for True Positive and True Negative, respectively, which can be explained by the number of predicted pixels belonging to the same class as the ground truth. Additionally, FP and FN denote False Positive and False Negative, respectively."
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS AND OBSERVATION",
            "text": "This section presents the obtained results from the proposed model through our experiment. It also discusses an extensive comparison of these results with state-of-the-art methodologies. It highlights the improvements achieved through the proposed architecture in semantically segmenting the object classes from the UAV images."
        },
        {
            "heading": "A. Observation",
            "text": "As discussed earlier, the SLIC-based superpixel algorithm works based on two core parameters: N and m to decide the number of superpixels in a superpixel image and a scale variation parameter, thus playing an important role in estimating the size of a superpixel, respectively. The value of m falls within a range of [5, 35]. The greater the value of m, the more compact the cluster.\nIn the experiment, we have considered m as [5, 15, 25, 35]. That means when m = 5, each image patch (superpixel) in the superpixel image is of size 5 \u00d7 5. Small scale patches (m = 5) expose the features inside a superpixel efficiently. In contrast, the enormous value (m = 25/35)works at the border regions of an object to distinguish it from the others. It helps the training block be exposed to meaningful, distinguished features to learn about the object it needs to segment. However, to prove the efficiency, every possible combination of m and N is considered. The produced superpixel images are then given as input to the CNN block, and the outcomes are listed in the Tables I and II.\nThe resulting superpixel images from the SLIC module are accepted as inputs at the CNN module for the training operation, and the trained model is used on the validation/test set to obtain the results. At the CNN module, the superpixel images are gone through M number of CNN architectures (M = No. of multiscale images) constitute the multiscale CNN/ConvNet architecture. The main idea of using a multiscale ConvNet (M CNN) architecture (Fig 1) is to have a vast feature space of the ground objects at different scales sampled through a random multi-sampling process. The ConvNet module in the M-CNN architecture is an encoder-decoder architecture inspired by the skip connection mechanisms (dense module within a stage) that passes the previously learned parameters in the encoder stage to the following equivalent decoder stage. This architecture can determine the edge-level object features and the imbalanced occlusion class objects. These features are crucial from the perspective of a segmentation task, as even a few pixels\u2019 misclassifications may affect the accuracy of the architecture."
        },
        {
            "heading": "B. State-of-the-art Comparison",
            "text": "The proposed model is also compared with the state-of-theart methodologies based on the evaluation matrices described in the previous section. The baseline models are validated with the raw input images considered for multiscale CNN and Aerial SegNet architectures. The training process for these models is performed for around 450 epochs till the convergence occurs. The obtained results from the experiments are listed in Table III and also in Fig. 6.\nAs shown in Fig. 6, it can be clear that among the state-ofthe-art models, U-Net [40], FCN-32s [5], and DeepLab-plusexception [7] manage to performs well to segment the vegetation and road class pixels thus achieving a reasonable 75\u221280% IoU score. However, they have failed to capture the outlines of different class objects resulting in a slight drop in accuracy. This is where the multiscale feature fusion technique looks useful in aggregating the scale-invariant features that help fetch the missing feature sets, thus improving the accuracy. Hence, it can be concluded from Tables III and IV that the proposed model can perform better than the existing methods, such as [5], [40] in terms of segmenting the pixel class and achieving a smoother boundary of the objects. Moreover, along with the performance measures like F Score and IoU, we have also considered precision and recall. It can be observed from Table III that from the mean precision (mPrecision)\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\npoint of view, the DeepLab V3+Xception [7] performs better (on the NITRDrone dataset) in terms of precision score than the proposed architecture. However, there is a miss, and it can be explained by seeing the table that there is a massive gap between the recall and precision score, which is entirely unacceptable from the perspective of a semantic segmentation task. At this point, the proposed framework acts superior maintaining a descent of true positives and true negatives as can be judged based on the obtained scores mentioned in\nTable III. Similarly, a comparison of improvement achieved through the proposed architecture is also presented in Table V."
        },
        {
            "heading": "VI. DISCUSSION",
            "text": "To provide a better comprehensive comparison of the proposed approach, various experimental observations corresponding to external factors, such as space complexity, are also noted, which are discussed in this section."
        },
        {
            "heading": "A. Parameter Comparison",
            "text": "The number of learnable parameters plays a crucial role in accessing the performance of the model in terms of speed and memory. Hence, a comparative study is presented in Table VI. It can be observed that state-of-the-art models, except a few, such as FC DensenNet-103 [52], AerialSegNet [50], and UNet [40] having ResNet-18 as the backbone. However, the proposed architecture overcomes the underlined issues of these\nbaselines, achieving better performance accuracy while having less trainable parameters than most of the considered baselines. Therefore, it can be deployed on various edge-end devices (like UAVs), where memory and computing power are constraints. In the next subsection, we discuss the optimization that has been achieved through the use of superpixel."
        },
        {
            "heading": "B. Space Efficiency",
            "text": "The superpixel technique provides a partially segmented image that helps the CNN module extract the object-level features while reducing the space complexity. As per our study, images with smaller m performs slightly better than others due to their feature preservation properties that are pretty close to a natural image (with meaningful information. From the space consumption point of view, it is pretty clear that the superpixel images with the highest space complexity are also 60% lesser than the original images while performing better or equal than with the original images. A bar graph representing the space consumption of all the considered images is presented in Fig. 8 and Fig. 9. Similarly, among the superpixel images considered for the experiment, space consumption (spc) can be arranged in a decreasing order like spc(5) > spc(15) > spc(25) > spc(35). Considering an example, if N = 1000, then spc(1000 5) > spc(1000 15) > spc(1000 25) > spc(1000 35). This space complexity matters a lot for the proposed approach to get implemented over IoT and network, as transferring the superpixel images (over the network) would require a low-bandwidth connection making bandwidth available for the other network operations. Thus, IoT-based remote sensing applications can be benefited from the proposed architecture.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12"
        },
        {
            "heading": "C. Observed Limitations",
            "text": "The current study has a few limitations, which are presented below. Under low-light conditions, the model performs poorly (at the beginning of the training) in segmenting similar-looking objects. For example, the road surface may look similar to the rooftop (the tar-covered sheet), creating confusion for the model and leading to low accuracy in terms of IoU. Similarly, for the minor class objects, such as the occlusion class in the NITRDrone dataset and vehicle class in the UDD, the model invests a lot of time in obtaining the required feature maps before correctly classifying the minor object class pixels. Moreover, one more limitation can also be seen corresponding to the increased number of trainable parameters due to the multiple CNN modules. This may bring CNN architectural issues. In future work, a few cues, as presented in [58], can be considered to develop a multiscale CNN architecture, and its effectiveness can be verified."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "This article presents a superpixel-based multiscale CNN framework to address UAV aerial image-based semantic segmentation problems. The first-level segmentation is achieved using the SLIC superpixel algorithm that produces superpixel images from the input UAV images, which are the input for the CNN architecture for final segmentation. The proposed CNN architecture collectively uses the strength of skip connections and the multiscale context aggregation strategy to extract the crucial scale-invariant features that can uniquely classify a pixel as per the object classes. The multiscale CNN module is good at extracting scale-invariant features that are essential from a UAV imagery point of view, as the same ground objects may look small or large as per the operating height of UAV. Further, the proposed architecture is evaluated (on the NITRDrone and Urban drone dataset) and compared with the stateof-the-art methods. The experimentally obtained results prove the superiority of the ensemble framework (of the superpixel technique and the deep multiscale architecture) in segmenting the UAV aerial images. Moreover, the proposed architecture provides a robust solution towards semantic segmentation for object classes like road, vehicle, and vegetation, which the other considered state-of-the-art methodologies failed to do. The proposed approach can be integrated with the roboticsbased AI solution to provide intelligent road extraction and vegetation detection through panoptic aerial imageries of UAVs. Similarly, the proposed approach can be combined with IoT and cloud concepts to actively analyze critical operations, such as disaster management and carrying out surveys.\nAs an extension to this work, different superpixel techniques, such as SLICO and SEEDS, may be tested to have a better-performing superpixel technique for road and vegetation extraction from aerial images. Similarly, the proposed architecture can also be implemented in a simulated IoT environment to demonstrate the efficiency of this approach in managing low-bandwidth operations."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research is supported by the following projects:\n1) Project titled \u201cDeep learning applications for computer vision task\u201d funded by NITROAA with support of Lenovo P920 and Dell Inception 7820 workstation and NVIDIA Corporation with support of NVIDIA Titan V and Quadro RTX 8000 GPU. 2) Project titled \u201cApplications of Drone Vision using Deep Learning\u201d funded by Technical Education Quality Improvement Programme (referred to as TEQIP-III), National Project Implementation Unit, Government of India."
        }
    ],
    "title": "Superpixel-based Multiscale CNN Approach towards Multiclass Object Segmentation from UAV-captured Aerial Images",
    "year": 2023
}