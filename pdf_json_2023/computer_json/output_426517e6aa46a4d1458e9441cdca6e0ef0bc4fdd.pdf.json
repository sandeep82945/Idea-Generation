{
    "abstractText": "Generative Adversarial Networks (GANs) and their variants have achieved remarkable success on natural images. However, their performance degrades when applied to remote sensing (RS) images, and the discriminator often suffers from the overfitting problem. In this paper, we examine the differences between natural and RS images and find that the intrinsic dimensions of RS images are much lower than those of natural images. As the discriminator is more susceptible to overfitting on data with lower intrinsic dimension, it focuses excessively on local characteristics of RS training data and disregards the overall structure of the distribution, leading to a faulty generation model. In respond, we propose a novel approach that leverages the real data manifold to constrain the discriminator and enhance the model performance. Specifically, we introduce a learnable information-theoretic measure to capture the real data manifold. Building upon this measure, we propose manifold alignment regularization, which mitigates the discriminator\u2019s overfitting and improves the quality of generated samples. Moreover, we establish a unified GAN framework for manifold alignXingzhe Su, Wenwen Qiang, Zeen Song, Changwen Zheng, Fengge Wu Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China Fuchun Sun Science & Technology on Integrated Information System Laboratory, Department of Computer Science and Technology, Tsinghua University, Beijing, China Corresponding author: Wenwen Qiang, E-mail: qiangwenwen@iscas.ac.cn ment, applicable to both supervised and unsupervised RS image generation tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingzhe Su"
        },
        {
            "affiliations": [],
            "name": "Wenwen Qiang"
        },
        {
            "affiliations": [],
            "name": "Zeen Song"
        },
        {
            "affiliations": [],
            "name": "Changwen Zheng"
        },
        {
            "affiliations": [],
            "name": "Fengge Wu"
        },
        {
            "affiliations": [],
            "name": "Fuchun Sun"
        }
    ],
    "id": "SP:8ced0399be6feb39cfec0662b6da358e5fc995ed",
    "references": [
        {
            "authors": [
                "D Bang",
                "H Shim"
            ],
            "title": "Mggan: Solving mode collapse",
            "year": 2021
        },
        {
            "authors": [
                "S Bell-Kligler",
                "A Shocher",
                "M Irani"
            ],
            "title": "Remote Sensing Letters",
            "year": 2019
        },
        {
            "authors": [
                "P Esser",
                "R Rombach",
                "B Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "C Gong",
                "J Han",
                "X Lu"
            ],
            "title": "Remote sensing image scene classification: Benchmark and state of the art",
            "venue": "Proceedings of the IEEE",
            "year": 2017
        },
        {
            "authors": [
                "IJ Goodfellow",
                "J Pouget-Abadie",
                "M Mirza",
                "B Xu",
                "D Warde-Farley",
                "S Ozair",
                "A Courville",
                "Y Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume",
            "year": 2014
        },
        {
            "authors": [
                "I Gulrajani",
                "F Ahmed",
                "M Arjovsky",
                "V Dumoulin",
                "AC Courville"
            ],
            "title": "Improved training of wasserstein gans. Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "J He",
                "W Shi",
                "K Chen",
                "L Fu",
                "C Dong"
            ],
            "title": "Gcfsr: a generative and controllable face super resolution method without facial and gan priors",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "M Heusel",
                "H Ramsauer",
                "T Unterthiner",
                "B Nessler",
                "S Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "K Jiang",
                "Z Wang",
                "P Yi",
                "G Wang",
                "T Lu",
                "J Jiang"
            ],
            "title": "Edge-enhanced gan for remote sensing image superresolution",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing",
            "year": 2019
        },
        {
            "authors": [
                "L Jiang",
                "B Dai",
                "W Wu",
                "CC Loy"
            ],
            "title": "2021a) Deceive d: Adaptive pseudo augmentation for gan training with limited data",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Y Jiang",
                "S Chang",
                "Z Wang"
            ],
            "title": "2021b) Transgan: Two pure transformers can make one strong gan, and that can scale up",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "M Kang",
                "J Park"
            ],
            "title": "Contragan: Contrastive learning for conditional image generation",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "T Karras",
                "S Laine",
                "T Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "T Karras",
                "M Aittala",
                "J Hellsten",
                "S Laine",
                "J Lehtinen"
            ],
            "title": "Aila T (2020a) Training generative adversarial networks with limited data",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "T Karras",
                "S Laine",
                "M Aittala",
                "J Hellsten",
                "J Lehtinen",
                "T Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recogni-",
            "year": 2020
        },
        {
            "authors": [],
            "title": "Photo-realistic single image super",
            "year": 2017
        },
        {
            "authors": [
                "A ford",
                "X Chen"
            ],
            "title": "Improved techniques for train",
            "year": 2016
        },
        {
            "authors": [
                "A Srivastava",
                "L Valkov",
                "C Russell",
                "MU Gutmann",
                "C Sutton"
            ],
            "title": "Veegan: Reducing mode collapse in gans using implicit variational learning. Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "HY Tseng",
                "L Jiang",
                "C Liu",
                "MH Yang",
                "W Yang"
            ],
            "title": "Regularizing generative adversarial networks under limited data",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Y Wei",
                "X Luo",
                "L Hu",
                "Y Peng",
                "J Feng"
            ],
            "title": "An improved unsupervised representation learning generative adversarial network for remote sensing image scene classification",
            "venue": "Remote Sensing Letters",
            "year": 2020
        },
        {
            "authors": [
                "Y Xiong",
                "S Guo",
                "J Chen",
                "X Deng",
                "L Sun",
                "X Zheng",
                "W Xu"
            ],
            "title": "Improved srgan for remote sensing image super-resolution across locations and sensors. Remote Sensing",
            "year": 2020
        },
        {
            "authors": [
                "L Xu",
                "MI Jordan"
            ],
            "title": "On convergence properties of the em algorithm for gaussian mixtures",
            "year": 1996
        },
        {
            "authors": [
                "C Yang",
                "Y Shen",
                "Y Xu",
                "B Zhou"
            ],
            "title": "Data-efficient instance generation from instance discrimination",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Y Yang",
                "S Newsam"
            ],
            "title": "Bag-of-visual-words and spatial extensions for land-use classification",
            "venue": "Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems,",
            "year": 2010
        },
        {
            "authors": [
                "F Yu",
                "A Seff",
                "Y Zhang",
                "S Song",
                "T Funkhouser",
                "J Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans",
            "year": 2015
        },
        {
            "authors": [
                "Y Yu",
                "X Li",
                "F Liu"
            ],
            "title": "Attention gans: Unsupervised deep feature learning for aerial scene classification",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing",
            "year": 2019
        },
        {
            "authors": [
                "H Zhang",
                "I Goodfellow",
                "D Metaxas"
            ],
            "title": "Odena A (2019a) Self-attention generative adversarial networks",
            "venue": "In: International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "H Zhang",
                "Z Zhang",
                "A Odena",
                "H Lee"
            ],
            "title": "2019b) Consistency regularization for generative adversarial networks",
            "venue": "In: International Conference on Learning Representations",
            "year": 2019
        },
        {
            "authors": [
                "B Zhao",
                "S Zhang",
                "C Xu",
                "Y Sun",
                "C Deng"
            ],
            "title": "Deep fake geography? when geospatial data encounter artificial intelligence",
            "venue": "Cartography and Geographic Information Science",
            "year": 2021
        },
        {
            "authors": [
                "S Zhao",
                "Z Liu",
                "J Lin",
                "JY Zhu",
                "S Han"
            ],
            "title": "Differentiable augmentation for data-efficient gan training",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "W Zhou",
                "S Newsam",
                "C Li",
                "Z Shao"
            ],
            "title": "Patternnet: A benchmark dataset for performance evaluation of remote sensing image retrieval. ISPRS journal of photogrammetry and remote sensing",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "and their variants have achieved remarkable success on natural images. However, their performance degrades when applied to remote sensing (RS) images, and the discriminator often suffers from the overfitting problem. In this paper, we examine the differences between natural and RS images and find that the intrinsic dimensions of RS images are much lower than those of natural images. As the discriminator is more susceptible to overfitting on data with lower intrinsic dimension, it focuses excessively on local characteristics of RS training data and disregards the overall structure of the distribution, leading to a faulty generation model. In respond, we propose a novel approach that leverages the real data manifold to constrain the discriminator and enhance the model performance. Specifically, we introduce a learnable information-theoretic measure to capture the real data manifold. Building upon this measure, we propose manifold alignment regularization, which mitigates the discriminator\u2019s overfitting and improves the quality of generated samples. Moreover, we establish a unified GAN framework for manifold align-\nXingzhe Su, Wenwen Qiang, Zeen Song, Changwen Zheng, Fengge Wu Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China\nFuchun Sun Science & Technology on Integrated Information System Laboratory, Department of Computer Science and Technology, Tsinghua University, Beijing, China\nCorresponding author: Wenwen Qiang, E-mail: qiangwenwen@iscas.ac.cn\nment, applicable to both supervised and unsupervised RS image generation tasks.\nKeywords Image Generation \u00b7 Generative Adversarial Networks \u00b7 Remote Sensing \u00b7 Data Manifold"
        },
        {
            "heading": "1 Introduction",
            "text": "The image generation task (Xu and Jordan, 1996) is a fundamental problem in computer vision and has attracted significant attention from the research community. Various approaches have been proposed to address this challenge, including Generative Adversarial Networks (GANs), Variational Autoencoders, flow-based models, and so on. Among others, GANs (Goodfellow et al., 2014), in particular, have revolutionized the field by introducing a novel framework based on a zerosum game between a generator and a discriminator. This framework has demonstrated remarkable success in generating realistic images. Since the introduction of GANs, they have become a cornerstone technique in various vision applications, such as image superresolution (Ledig et al., 2017; Bell-Kligler et al., 2019; He et al., 2022), style transfer (Park et al., 2019; Wang et al., 2021; Richardson et al., 2021), and image editing (Ling et al., 2021; Patashnik et al., 2021; Pan et al., 2023).\nDespite the remarkable success of current GAN methods, they still face various challenging problems, with the overfitting of the discriminator being a notable issue. The discriminator, equipped with an excessive number of parameters, can tend to memorize the training data rather than learning a meaningful distribution. This can result in a high real/fake classification accuracy on the training dataset but a lower accuracy on the validation split. When the discriminator becomes\nar X\niv :2\n30 5.\n19 50\n7v 2\n[ cs\n.C V\n] 1\n4 Ju\nl 2 02\noverfit to the training samples, its feedback to the generator becomes less meaningful, leading to training divergence, excessive memorization, and limited generalization. Our experiments reveal that the overfitting problem of the discriminator is more pronounced when working with remote sensing (RS) data, as highlighted in Fig. 1. Specifically, we conducted the same experiment as outlined in (Karras et al., 2020a) on both the NWPU-RESISC45 and FFHQ256 datasets, maintaining the same training image size and dataset size for both. Notably, we observed that the GAN model diverged earlier when trained on RS data (Fig. 1(a)). Furthermore, Fig. 1(b) showcases the discriminator\u2019s outputs during training, which align with the FID curves depicted in Fig. 1(a). The discernible drop in accuracy measured on a separate validation set further corroborates the overfitting problem of the discriminator specifically on the NWPU dataset. More experiment results can be found in the Appendix.\nTo identify possible causes, we analyze the differences between RS images and natural images in section 3.2. We find that the support of the distribution of RS data has lower dimension than that of natural images, implying that RS images can be embedded into lower-dimensional data manifold than natural\nimages. Intuitively, if the training dataset has lowerdimensional data manifold, the relationships between data samples could be simple, making the discriminator easier to overfit. To illustrate this issue, we perform a toy experiment on synthetic data in Section 3.2, as described in Fig.3. The experiment clearly demonstrates that the discriminator becomes inclined to memorize the local characteristics of the training data while disregarding the global structure of the distribution during the training process. This behavior adversely affects the discriminator\u2019s outputs and hinders the generator from effectively capturing the true data distribution, ultimately leading to a faulty generation model. Ideally, the discriminator should be capable of capturing the underlying manifold of both the training and generated data, and maximizing the difference between these two manifolds. This observation sheds light on potential solutions to mitigate the overfitting problem of the discriminator by applying manifold constraint. By imposing this constraint, we can urge the discriminator to focus on the underlying data manifold and limit the complexity of parameter space of the discriminator, thereby alleviating the overfitting problem.\nIn this paper, we propose a novel approach that leverages the real data manifold to mitigate the overfitting problem of the discriminator and enhance the quality of generated samples. Our methods neither entail heavy computational overheads nor require modifications of the network structure. Without loss of generality, an assumption that we will carry through our study is that the real datasets are not uniformly distributed in their ambient space, but are distributed over a union of low-dimensional nonlinear submanifolds. We first design a learnable information-theoretic measure to evaluate the intrinsic geometric of the training samples. Then we propose novel regularization terms based on this measure in loss functions of GAN models, which we call manifold alignment constraint (MAC). The MAC encourages the discriminator to misalign the manifold of the generated samples with that of the real images, thereby focusing on the underlying data manifold of the real images and reducing the complexity of the discriminator\u2019s parameter space. On the other hand, the generator aims to align the manifold of the generated samples with that of the real images, effectively capturing the intrinsic characteristics of the real data and improving the overall performance of the generative model. We theoretically and empirically prove the efficacy of MAC in manifold alignment during GAN training. Finally, we establish a unified GAN framework for supervised and unsupervised RS images generation. In summary, the contributions of this paper are the following:\n\u2013 We empirically demonstrate that RS images have\nlower-dimensional data manifold than natural images, which leads to the discriminator easily overfitting to the RS training distribution, neglecting the underlying data manifold. \u2013 We propose an information-theoretic measure to\nevaluate the difference between the real and generated data manifolds, which helps constrain the discriminator and enhance the performance of the generator. \u2013 We design novel learning paradigms under super-\nvised and unsupervised settings, and establish a unified GAN framework for supervised and unsupervised RS images generation. \u2013 We theoretically and empirically prove the efficacy\nof our method. Extensive experiments on three RS datasets and three natural datasets validate the versatility of our approach."
        },
        {
            "heading": "2 Related Work",
            "text": "Generative adversarial networks. GANs are notorious for training instability and mode collapse. Various adversarial losses have been proposed to stabilize the training or improve the convergence of the GAN models (Goodfellow et al., 2014; Mroueh and Sercu, 2017; Arjovsky et al., 2017). Additionally, numerous efforts have been made to address this issue using regularization methods (Mescheder et al., 2018; Miyato et al., 2018; Gulrajani et al., 2017; Srivastava et al., 2017), or modifying network architectures (Zhang et al., 2019a; Brock et al., 2018; Karras et al., 2019, 2020b; Esser et al., 2021; Jiang et al., 2021b; Metz et al., 2017). Other than these problems, the overfitting of the discriminator is also a common challenge in GANs.\nThe overfitting problem occurs when the discriminator becomes overly complex with a large number of parameters, resulting in memorization of the training data rather than learning the underlying data distribution. This leads to poor generalization and a lack of diversity in generated samples, particularly when the training dataset is limited. Recent methods (Zhang et al., 2019b; Jiang et al., 2021a; Zhao et al., 2020; Karras et al., 2020a) have tackled this issue by leveraging data augmentation techniques to increase data diversity and prevent overfitting. Other than data augmentation, regularization methods such as Lecam (Tseng et al., 2021) have been proposed to enhance the generalization of GAN models trained on limited data. InsGen (Yang et al., 2021) proposes a contrastive learning objective to enhance the adversarial loss in the few-shot generation setting. MoCA (Li et al., 2022) proposes prototypebased memory modulation module to improve the gen-\nerator network of a GAN. FastGAN (Liu et al., 2020) introduces a self-supervised discriminator and a SkipLayer channel-wise Excitation (SLE) module for efficient few-shot image synthesis. However, these methods primarily focus on the general distribution of the training data and do not explicitly consider the real data manifold.\nPrevious works (Park et al., 2017; Li et al., 2018) introduce geometry constraints to GANs loss functions. These methods utilize statistical mean and radius to approximate the geometry of the real data, but they may lack accurate constraints on the data manifold and may lose important geometrical information. Other approaches (Ni et al., 2022; Bang and Shim, 2021; Khayatkhoei et al., 2018) require auxiliary networks to map the generated data into the submanifolds of the real data. In this paper, we focus on intrinsic dimensions of training images, and minimizing distance between real and synthetic data in the feature space. Our methods do not introduce heavy computational overhead or require modifications to the network structure.\nGAN in the RS field. In this paper, we focus on RS RGB images. Existing GAN models in the RS field can be divided into two types according to their applications. This first kind is augmenting training samples (Lin et al., 2017; Yu et al., 2019; Wei et al., 2020; Chen et al., 2021). The second kind deals with imagegeneration-related tasks such as image translation (Bejiga et al., 2020; Rui et al., 2021; Chen et al., 2020; Zhao et al., 2021) and image super-resolution (Jiang et al., 2019; Xiong et al., 2020). However, these methods do not delve into the characteristics of RS images, and the generated images are of low quality. In this paper, we explore the intrinsic dimensions of RS images, and introduce RS data manifold constraints in the loss functions. Note that this is the first work to focus on the manifold alignment on RS image generation tasks."
        },
        {
            "heading": "3 Preliminary Study and Analysis",
            "text": "In this section, we first introduce the basic framework of GANs. Then we explore the differences between RS images and natural images. Finally, we analyze plausible reasons about the poor results of the GAN models on RS images and derive the motivation for this paper.\n3.1 Preliminary GANs\nThe GAN model aims to learn the distribution of training samples. Based on the idea of the zero-sum game, a GAN model consists of a generator G and a discriminator D. The generator aims to generate realistic samples\nto fool the discriminator, while the discriminator tries to distinguish between real and fake samples. When the model reaches the final equilibrium point, the generator will model the target distribution and produce counterfeit samples, which the discriminator will fail to discern. Let VD and VG denote the training objectives of the discriminator D and the generator G, respectively. The training of the GAN frameworks can be generally illustrated as follows:\nmax D VD = E x\u223cPdata [D(x)]\u2212 E z\u223cpz [D(G(z))] (1)\nmax G VG = E z\u223cpz [D(G(z))] (2)\nwhere the input vector z of G is usually sampled from the normal distribution.\n3.2 Problem Analysis and Motivations\nAs we mentioned above, current GAN methods often suffer from the overfitting problem of the discriminator, as shown in Fig.1. In order to identify the plausible causes, we analyze the differences between RS images and natural images.\nWe randomly sample 10,000 images from the NWPU dataset and convert these images to grayscale. We convert the images to one-dimensional vector, denoted as X = [x1, x2, . . . , xN ], where the vector xi concatenates all pixels of an image. Then, we get the aver-\nage vector x\u0304 = 1N \u2211N\ni=1 xi. We calculate x\u0302i = xi \u2212 x\u0304 and get X\u0302 = [x\u03021, x\u03022, . . . , x\u0302N ]. Fig.2 shows singular value decomposition on the matrix M = X\u0302X\u0302T (M =\nUSV T, S = diag ( \u03c3k ) ) in sorted order and logarithmic\nscale ( { log ( \u03c3k )} ). We perform the same experiment on FFHQ dataset and ImageNet dataset as shown in Fig.2. The singular values commonly correspond to important information implied in the matrix, and the importance is positively related to the magnitude of the singular value. We observe that the singular values of RS images\nare smaller than those of natural images, and there are fewer singular values with relatively large magnitudes of RS images compared with natural images. Thus, we conclude that the support of the distribution of RS data has lower dimension than those of natural images in the input space.\nBased on our empirical findings above, RS images are distributed on lower-dimensional manifold than natural images. Intuitively, if the training data has a high-dimensional data manifold, the discriminator must learn a more complex decision boundary to distinguish between real and fake samples. This can make it more difficult for the discriminator to overfit to the training data, as it must learn a more generalized representation of the data distribution. In contrast, as depicted in Fig.3, when the training data has a low-dimensional data manifold, such as the swiss roll-shaped manifold illustrated in Fig.3(a), the discriminator can more easily memorize the training data and overfit to the limited variations within the data. Without sufficient training samples, the discriminator tends to neglect the real data manifold and produces biased outputs. Consequently, the generator fails to learn the true data manifold, resulting in a faulty generative model, as depicted in Fig.3(b).\nThe optimal discriminator should be able to capture the underlying manifold of training data and generated data, and maximize the difference between these two manifolds. Inspired by the observation, we propose applying a manifold constraint on the discriminator. In this way, we can guide the discriminator to focus on the underlying data manifold, which helps limit the complexity of the discriminator\u2019s parameter space and alleviate the overfitting problem. Furthermore, by ensuring the alignment between the manifold of generated samples and the real data manifold, the generator can effectively learn the intrinsic structure of the real data. This alignment enhances the quality and diversity of the generated samples, leading to improved performance (Fig.3(c)).\nIn practice, it is intractable to learn the data manifold in the high-dimensional ambient space RD. Be aware that we assume the data x is distributed over a union of low-dimensional nonlinear submanifolds \u222akj=1Mj \u2282 RD, where each submanifold Mj is of dimension dj << D. Given this assumption, the images from each submanifold Mj \u2282 RD can be mapped to a linear subspace Sj \u2282 Rd, which we call the feature space. It is easier to evaluate the structure of the representations of images in the feature space, which also reflects the data manifold in image space. Consequently, the representations of images should have following properties:\n\u2013 Between-Mode Uniformity: Representations from\ndifferent modes or classes should be highly uncorrelated. \u2013 Within-Mode Similarity: Representations from the\nsame mode or class should be relatively correlated. \u2013 Maximally Variance: Representations should have\nas large dimension as possible to cover all the modes or classes and be variant for the same mode or class.\nIn order to obtain qualified representations, we need a new measure to evaluate the goodness of the resulting representations in terms of all these properties. In respond, we propose a novel information-theoretic measure, and design new regularization terms regarding the manifold alignment of RS images, which will be elaborated below."
        },
        {
            "heading": "4 Methods",
            "text": "In this section, we first introduce the new informationtheoretic measure to evaluate the intrinsic geometric of the distribution of features. Then we design novel regularization terms based on this measure, which we call manifold alignment constraint (MAC). We theoretically and empirically prove the efficacy of MAC in manifold alignment on GAN training. Finally, we establish a unified GAN framework for supervised and unsupervised RS images generation as shown in Fig.4. Next, we will introduce in detail.\n4.1 Manifold Alignment Constraint\nThe key idea in this paper is to find a principled measure which could accurately and directly represent intrinsic geometric or statistical properties of the distribution. To achieve this, we turn to information theory, which suggests that the uniform distribution contains the most information and has the maximum information entropy. From this perspective, a more dispersed distribution indicates more information, whereas a more compact distribution indicates less information.\nWe can therefore evaluate the information contained in the features to represent their intrinsic geometric properties. Building on this intuition, we could design a measure of compactness for the distribution of a random variable or from its finite samples.\nNotably, the singular vectors associated with larger singular values represent the dominant stretching directions in the data. Consequently, larger singular values and a greater number of large singular values correspond to more scattered data, while smaller singular values and fewer large singular values signify more\ncompact data. Thus we propose to use \u2211\ni=1 \u03bb 2 i to eval-\nuate the compactness of the distribution of features, where \u03bbi denotes the singular value. As the square of the Frobenius norm of the matrix M equals to the sum of the squares of the singular values. Meanwhile, the square of the Frobenius norm equals to the trace of MMT. Thus, we use the trace of MMT in our experiments.\u2211 i=1 \u03bb2i = \u2225M\u22252F = Tr(MMT) (3)\nwhere \u03bbi is singular value and Tr is the trace operator.\nTo ensure between-mode uniformity, it is desirable that features of different modes or classes are maximally uncorrelated with each other. Therefore, they together should span a space of the largest possible volume or dimension, and the Frobenius norm of the whole set Z should be as large as possible. Conversely, learned features of the same mode or class should be highly correlated and coherent. Therefore, each mode or class should only span a space or subspace of a very small volume, and the Frobenius norm should be as small as possible. To evaluate whether the representation of images satisfy the three properties mentioned above, we design a novel measure based on Eq.3.\nLTr(Z) = 1\n2n Tr (ZZT)\u2212 k\u2211 j=1 Tr ( ZCjZT ) (4) where Z = [z1, . . . , zn] \u2282 Rd\u00d7n denotes the representations of images, C = { Cj \u2208 Rn\u00d7n }k j=1 is a set of\npositive diagonal matrices whose diagonal entries denote the membership of the n samples in the k classes. In the supervised setting, k is the number of classes. If the sample i belongs to the mode or class j, then Cj(i, i) = 1. Otherwise, Cj(i, i) = 0. In the unsupervised setting, we provide a novel learning paradigm for matrix Cj , which will be elaborated in the next section.\nBy maximizing the first term of LTr, we aim to encourage a uniform distribution across the entire set of images. On the other hand, by minimizing the second term of LTr, we intend to enforce compactness within the distribution of generated images that belong to the same mode or class. By balancing these two terms, we could force the distributions of the generated images belonging to different modes or classes to be dispersed. Consequently, the representations of generated images satisfy the three conditions mentioned above, which in turn ensures the generated images are distributed over a union of low-dimensional nonlinear submanifolds. However, this constraint alone is insufficient, and we also need to evaluate the disparity of the submanifolds between generated images and real images. Inspired by mutual information, we design the following measure:\nLMAC(Z,Z\u2032) =LTr(Z,Z\u2032)\u2212 1\n2 LTr(Z)\u2212\n1 2 LTr(Z\u2032) , (5)\nwhere Z denotes the representations of real images, while Z \u2032 is the representations of generated images. LTr(Z,Z \u2032) = 12n (Tr(Z\u0303Z\u0303 T)\u2212 \u2211k j=1 Tr(Z\u0303C\u0303 jZ\u0303T)), where Z\u0303 = Z \u222a Z \u2032, C\u0303j \u2282 R2n\u00d72n. In the supervised setting, if the sample i belong to the mode or class j, then\nCj(i, i) = 1, Cj \u2032 (i, i) = 1, C\u0303j(i, i) = 1. In the unsupervised setting, Cj , Cj \u2032 and C\u0303j are all learnable matrices.\nWe theoretically prove that the above quantity precisely measures the volume of the space between Z and Z \u2032. The proof will be elaborated in the next section. By maximizing the MAC, the discriminator is encouraged to push the manifold of the generated samples to be misaligned with that of the real images. On the other hand, by minimizing the MAC, the generator aims to align the manifold of the generated samples with that of the real images. Based on Eq.5, we refine the loss functions of GAN models regarding manifold alignment and establish a unified GAN framework for supervised and unsupervised RS images generation.\n4.2 Loss Functions\nWe incorporate the manifold alignment constraint (MAC) as regularization terms in the loss functions of GAN models, as depicted in Eq.6 and Eq.7, where \u03bb and \u03b3 are hyperparameters. The representations Z and Z \u2032 are learned by the discriminator.\nmin D LD = E z\u223cpz [D(G(z))]\u2212 E x\u223cPdata [D(x)]\n\u2212 \u03bbLMAC(Z,Z \u2032) (6)\nmin G LG = \u2212 E z\u223cpz [D(G(z))] + \u03b3LMAC(Z,Z \u2032) (7)\nSupervised. For the generator, we use the same architecture as previous models and add the label in-\nformation as additional input. However, for the discriminator, we do not use the label information directly as previous class-conditional models. Instead, we incorporate the MAC to ensure that the generated images align with the classes of real images. The Cj(i, i) in MAC denotes the relationship of sample i and class j, and Cj(i, i) = 1 means that the sample i belongs to the mode or class j. Additionally, we add a fully connected layer at the final block of the discriminator as an extra branch to learn representations Z, which is incoherent to the original unconditional output.\nUnsupervised. For the unsupervised setting, the relationship matrix C and representations Z have various implementations. For the matrix C ={ Cj \u2208 Rn\u00d7n }k j=1 , we design a novel learning paradigm. Specifically, we set k = n and learn a matrix Cj for each sample j. We train a three-layer MLP network F to learn the relationship between sample i and sample j in the feature space. Hence, Cj(i, i) = F (zi, zj), where zi and zj are representations of sample xi and sample xj , respectively. Then, we use a pretrained encoders fpre to obtain the prior feature representations, denoted as Z\u0304 = {z\u03041, ..., z\u0304n}, where z\u0304i = fpre (xi), i \u2208 {1, ..., n}. Then, regarding the j-th element z\u0304j in Z\u0304 as the anchor, we define that:\nCjpro(i, i) = exp(\u2225z\u0304i \u2212 z\u0304j\u222522 / \u03c4) (8)\nwhere z\u0304i is the i-th sample in Z\u0304. Then, we can obtain: Cjpro = [ C\u0304jpro(1, 1), ..., C\u0304jpro(n, n) ] (9)\nwhere C\u0304jpro(k, k) = Cjpro(k, k) /\u2211n\nh=1 C pro(h, h). We\nin turn treat the samples in z\u0304i as anchors and obtain Cpro = [ C1pro , ..., Cnpro ]T . To this end, we give the prior constraint Cpro of C based on a pre-trained encoder and the Euclidean distance of different pairs of samples. Since C = F (Z), the loss function of network F is as follows:\nLcon = \u03b2 \u2225Cpro \u2212 F (Z)\u222522 (10)\nThe network F is trained together with the discriminator, \u03b2 is the hyperparameter. In the optimal case, the network F will learn the relationship between the samples, and F (zi, zj) = 1 if sample xi and sample xj are from the same mode or class. In our experiments, we find that the choice of the pre-trained encoder has no impact on the final results.\nFor the representations Z, it has been shown that different network layers are responsible for different levels of detail in the images. Empirically, the latter blocks of the network have more effect on the style (e.g. texture and color) of the image whereas the earlier blocks impact the coarse structure or content of the image\n(Patashnik et al., 2021). Thus, we choose features from a shallow network layer of the discriminator as representations Z in our experiments."
        },
        {
            "heading": "5 Theoretical and Empirical Analysis",
            "text": "In this section, we present theoretical and empirical analysis of the proposed method, MAC. Firstly, we establish a theoretical connection between our method and information theory, and prove that the proposed method can precisely measure the distance between generated images and real images in the feature space. Secondly, we conduct two toy experiments on synthetic data to demonstrate the effectiveness of our approach empirically.\n5.1 Theoretical Analysis\nThis section provides provability results and properties of our method. First, we prove the theoretical connection between our method and information theory. Then, we prove our method can precisely measure the distance between generated images and real images in the feature space. Proposition 1 12nTr ( ZZT ) where Z =[\nz1, . . . , zn ]\n\u2282 Rd\u00d7n can measure the compactness of a distribution from its finite samples Z.\nProof Based on the first-order Taylor series approximation, log det(C + D) \u2248 log det(C) + Tr ( DTC\u22121 ) , we can get following equations.\n1\n2n Tr(ZZT) =\n1 2 ( 1 n Tr(ZZT) + log det(I))\n\u2248 1 2 log det(I + 1 n ZZT)\nIn information theory, rate distortion can be used to measure the \u201ccompactness\u201d of a random distribution (Cover and Thomas, 2006). Given a random variable z and a prescribed precision \u03f5 > 0, the rate distortion R(z, \u03f5) is the minimal number of binary bits needed to encode z such that the expected decoding error is less\nthan \u03f5. Given finite samples Z = [ z1, . . . , zm ] \u2282 Rd\u00d7m, the total number of bits needed is given by the following\nexpression: L(Z, \u03f5) .= ( m+d 2 ) log det ( I + dm\u03f52ZZ T ) . As the sample size m is large, our approach can be seen as an approximation of the code rate distortion, which completes our proof.\nBased on the above derivation, the proposed mea-\nsure can be rewritten as:\nLTr \u2248 1\n2 log det\n( I + \u03b1ZZT ) \u2212\nk\u2211 j=1 \u03b3j 2 log det ( I + \u03b1jZC jZT ) (11)\nwhere \u03b1 = dn\u03f52 , \u03b1j = d Tr(Cj)\u03f52 , \u03b3j = Tr(Cj) n for j = 1, . . . , k.\nTherefore, our method LTr can measure the volume of the space spanned by representations, making it an effective method to promotes embedding of data into multiple independent subspaces. Since the features of each mode or class, Zj and Z \u2032 j , are similar to subspaces or Gaussians, their \u201cdistance\u201d can be measured by the rate distortion. Hence, our method LMAC(Z,Z \u2032) can precisely measure the distance between generated images and real images in the feature space.\n5.2 Empirical Analysis\nTo demonstrate the effectiveness of our proposed method on GAN training, we conduct two toy experiments on synthetic data. In the first experiment, we generated synthetic data points that are uniformly distributed on three vortex lines (Fig.5(a)). We train the original GAN architecture without and with our\nmethod for 10,000 iterations, and the results are shown in Fig.5(b) and Fig.5(c), respectively. From the comparison in the figure, it is clear that the original GAN fails to capture the three modes, while our proposed method evenly spreads the probability mass and captures all three modes of the training data.\nThe second experiment is supervised generation on a 2D mixture of eight Gaussians evenly arranged in a circle. Moreover, the circle of the Gaussian mixture lies in a hyperplane in a 3D space (Fig.6(a)). Therefore, the generator has to search for 2D submanifolds in a 3D space. We present the results of the original CGAN in Fig.6(b) and our proposed method in Fig.6(c), both trained for 10,000 iterations. From the comparison in the figure, it is clear that the original CGAN only learns one mode, while our proposed method evenly spreads the probability mass and converges to all eight modes. Additionally, we observe that the data mass generated by our proposed method is heavily concentrated within the mode, whereas the data mass generated by the original CGAN scatters around the mode."
        },
        {
            "heading": "6 Experiments",
            "text": "In this section, we first provide the details of our experimental setup. We use three remote sensing datasets to evaluate our method: the UC Merced Land Use Dataset (Yang and Newsam, 2010), NWPU-RESISC45 Dataset (Gong et al., 2017) and PatternNet Dataset (Zhou et al., 2018). Details of these datasets can be found in the appendix. We also conduct experiments on three natural datasets: FFHQ256 Dataset (Karras et al., 2019), LSUN-cat Dataset (Yu et al., 2015) and Cifar10 Dataset (Krizhevsky and Hinton, 2009). We present the quantitative and qualitative results of the proposed method, as well as the comparison with existing methods. Finally, we provide more ablation and analysis of different components of our method.\n6.1 Experiment Setup\nImplementation details: For unsupervised generation, we base our method on two different models StyleGAN2 (Karras et al., 2020b) and BigGAN (Brock et al., 2018) with different augmentation methods ADA (Karras et al., 2020a) and APA (Jiang et al., 2021a), and regularization method LeCam (Tseng et al., 2021). We also compare our method with few-shot generation models FastGAN (Liu et al., 2020), InsGen (Yang et al., 2021) and MoCA (Li et al., 2022). We use the official PyTorch implementation of StyleGAN2+ADA, FastGAN, InsGen and Style-\nGAN2+MoCA. For BigGAN, APA and LeCam, we use the implementations provided by (Kang and Park, 2020). For class-conditional generation, we base our method on TC-GAN (Shahbazi et al., 2022). We compare our method with MSGAN (Mao et al., 2019), TC-GAN (Shahbazi et al., 2022), DivCo (Liu et al., 2021), ContraGAN (Kang and Park, 2020), conditional BigGAN and StyleGAN2. We use the official PyTorch implementation of these methods. We set \u03bb = 1,\u03b3 = 1,and \u03b2 = 1 in our experiments, and the ablation study on these hyperparameters are available in the Appendix.\nEvaluation Metrics: We evaluate our method using Frechet inception distance (FID) (Heusel et al., 2017), as the most commonly-used metric for measuring the quality and diversity of images generated by GAN models. We also include Kernel Inception Distance (KID) (Bin\u0301kowski et al., 2018) as a metric that is unbiased by empirical bias (Xu et al., 2018).\n6.2 Unsupervised Generation\nWe first visualize the generated images of our method in Fig.7. We run the models for 5 times and select the categories with higher occurrences. In comparison to StyleGAN2, our method produces a more even distribution of natural and artificial landscapes. The training curve of our method (based on StyleGAN2) on NWPU dataset is shown in Fig.8(a), and Fig.8(b)\nis the output of the discriminator during training. We can observe that the overfitting problem of the discriminator is alleviated. We also conduct a similar experiment in Section 3.2 on NWPU dataset. Specifically, we compare the singular values of the images generated by StyleGAN2 and our proposed method. The results are depicted in Figure 9, and they clearly demonstrate that our method, which incorporates the manifold constraint, enables the discriminator to focus on the underlying data manifold and capture its essential characteristics. As a result, the images generated by our method exhibit more effective and meaningful informa-\ntion. Additional visual results and experiments on other datasets are available in the Appendix.\nThen, we provide a quantitative comparison with the well-established baselines. Results are reported in Table 1 and Table 2. The red numbers in Table 1 indicate the improvement of the GAN models after using our method. In general, the FID and KID scores for our proposed method indicate a significant and consistent advantage over all the compared methods. In detail, our comparison experiments can be divided into three types. First, under different model structures, BigGAN and StyleGAN2, our method is robust, which proves that our approach can be applied to any model architecture. Second, under different augmentation methods, ADA and APA, our method still performs well. This shows that the proposed approaches can be applied to other GAN models along with existing augmentation approaches. Third, combined with the regularization method LeCam, our method is still effective. The proposed method can be viewed as an effective complement to existing regularization methods. Furthermore, we conduct a comparative analysis between our method and existing few-shot generation methods, as showcased in Table 2. Given the severity of the overfitting problem in few-shot generation tasks, it is crucial to compare the performance of our approach with these methods. In our experiments, InsGen, MoCA and our method are all implemented based on StyleGAN2+ADA. The results in Table 2 clearly demonstrate that our method consistently outperforms the existing methods, further highlighting its effectiveness in addressing the overfitting challenge. Other than\nRS datasets, we also conduct experiments on three natural datasets: FFHQ256 Dataset, LSUN-cat Dataset and Cifar10 Dataset. The results are shown in Table 3, and we use the StyleGAN2+ADA as the baseline. We use Inception Score (IS) (Salimans et al., 2016) as image quality metric on Cifar10 for better comparison with previous works. \u201dUG\u201d and \u201dCG\u201d represent unsupervised generation and conditional generation, respectively. These experiments validate the efficacy of our approach on different datasets.\nTo further assess the discriminator\u2019s ability to capture the underlying data manifold, we perform unsupervised classification experiments on the UC-Merced and NWPU-RESISC45 datasets. In these experiments, we utilize the learned representations Z as features and employ a regularized linear L2-SVM classifier, following the methodology employed in previous studies Lin et al. (2017)Yu et al. (2019). The experimental results, presented in Table 4, clearly demonstrate the superior performance of our method compared to StyleGAN2+ADA across all tests. Our approach consistently achieves the best results in the majority of the experiments, which proves that our method is capable of learning improved representations that facilitate accurate data classification. These experiments provide further confirmation that by applying the manifold constraint, the discriminator is able to focus on the underlying data manifold and capture its essential characteristics.\n6.3 Class-conditional Generation\nWe first present the generated images of classconditional experiment in Fig.10. Unlike previous class-\nUCLand NWPU PN Methods\nFID KID FID KID FID KID\nMSGAN 137.52 53.66 83.55 38.35 122.68 44.76\nTC-GAN 80.14 35.29 35.26 15.86 49.25 20.03\nContraGAN 133.91 54.62 117.29 50.76 122.55 50.18\nBigGAN 144.66 61.24 89.29 40.62 132.35 55.29\nStyleGAN2+DivCo 145.51 60.82 90.16 40.88 135.61 55.33\nStyleGAN2+ADA 143.41 60.74 83.40 36.29 120.20 51.28\nOur Method 64.33 30.29 18.33 6.33 30.35 11.28\nconditional methods, our method do not use the label information directly, thus our class-conditional generation approach during inference process is different from other methods. The labels of our generated images are not consistent with those of training dataset. For example, the airport class corresponds to label \u201d1\u201d in the training dataset, while the label \u201d1\u201d correspond to the\nriver class in the generated images. We use pre-trained classification model ResNET50 to re-assign the labels of generated images. Then we use the new labels to control the generated images. In Fig.10, we observe that the compared models, MSGAN and StyleGAN2+ADA, suffer from mode collapse on the RS datasets, while our method generates diverse images. Next, we provide a quantitative comparison with well-established baselines: MSGAN, TC-GAN, DivCo, ContraGAN, conditional BigGAN and StyleGAN2. The experimental results are shown in the Table 5. Our method outperforms the compared methods as they all suffer from the mode\ncollapse problem. These empirical results confirm the efficacy of our approach. More visual results are available in the Appendix.\n6.4 Ablation Study\nIn this section, we provide further ablation and analysis over different components of our method. More ablation experiments are available in the Appendix.\nRelationship Matrix C. For the unsupervised generation, we provide several different methods to construct relationship matrix C. The first kind is based on SimCLR and K-means. We use the pre-trained SimCLR model to extract the features of RS data and then cluster these features using K-means. The resulting clustering centroids serve as prototypes of the training data. In this experiment we choose 20 and 40 clustering centroids, respectively. The second kind is the learnable matrix proposed in this paper. We choose SimCLR, CLIP (Radford et al., 2021), and ResNet50 (pretrained on the ImageNet) as the pre-trained encoder. Table 6 shows the results of the ablation study on UCLand, NWPU-RESISC45 and PatternNet datasets. We can observe that the choice of the pre-trained encoder has little impact on the results. In the following experiments, we choose the learnable approach and the SimCLR model as the pre-trained encoder.\nRepresentations Z. For the unsupervised generation, we conduct ablation studies on features from different network layers. As different network layers are related to different levels of details in the generated image, and the earlier blocks of the network impact the coarse structure or content of the image. We conduct experiments on NWPU-RESISC45 dataset with StyleGAN2+ADA model, which consists of 14 blocks. We choose 5 blocks for comparison, and the results are shown in Table 10. We empirically choose the outputs of 8th block as the representations Z."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this study, we aim to address the challenges posed by remote sensing (RS) images in the context of generative adversarial networks (GANs). We observe that RS images exhibit lower intrinsic dimensions compared to natural images, resulting in difficulties for the discriminator and an increased risk of overfitting. To tackle these issues, we propose an information-theoretic measure to capture the real data manifold and introduce manifold alignment regularization to effectively constrain the discriminator and improve the generator\u2019s performance. We present novel learning paradigms for\nboth supervised and unsupervised RS image generation tasks, and establish a unified GAN framework. The efficacy of our method is validated through theoretical analysis and extensive experiments conducted on three RS datasets and three natural datasets, demonstrating the versatility and effectiveness of our approach.\nData Availability\nThe benchmark datasets can be downloaded from the literature cited in Subsubsection 6.\nConflict of interest\nThe authors declare no conflict of interest."
        },
        {
            "heading": "A Motivating Experiments",
            "text": "To illustrate the overfitting problem of the discriminator on remote sensing (RS) data, we conduct the same experiments as Fig.1 (in the main paper) on PatternNet (PN) dataset. We randomly sample 30,000 images from the PN and FFHQ256 datasets, repectively. The training curves of StyleGAN2 are shown in Fig11(a). The GAN model still diverges earlier on PN dataset than FFHQ256 dataset. In Fig11(b), we show the outputs of the discriminator during training. The experiment results further prove our finding that the overfitting problem of the discriminator is more severe on RS data. We also conduct the same experiments as Fig.2 (in the main paper) on PN dataset. The results are shown in Fig.12, which are consistent with the main paper."
        },
        {
            "heading": "B Experiment Setup",
            "text": "B.1 Datasets\nWe use three remote sensing datasets to evaluate our method: the UC Merced Land Use (UCLand) Dataset, NWPURESISC45 (NWPU) Dataset and PatternNet (PN) Dataset.\nThe UC Merced Land Use Dataset is one of the most widely used datasets in the field of remote sensing scene classification. It has 21 scene categories, each with 100 images. Each image has the size 256x256 and a spatial resolution of 0.3m. The images in the dataset come from more than 20 cities in the United States, including Las Vegas, Los Angeles, Miami, Santa Barbara, and Seattle.\nThe NWPU-RESISC45 Dataset has 31,500 images covering more than 100 countries and regions around the world. It has 45 categories with 700 images in each category. Each image is 256\u00d7256 pixels in size. The spatial resolution of this dataset is up to 0.2m and the lowest is 30m. In addition, the images are varied in lighting, shooting angle, imaging conditions, and so on.\nThe PatternNet Dataset is a large-scale high-resolution remote sensing dataset. It has 38 categories with 800 images in each category. Each image is 256\u00d7256 pixels in size. The spatial resolution of this dataset varies from 0.06m per pixel to 4.7m per pixel. The images in PatternNet are collected from Google Earth imagery or via the Google Map API for some US cities.\nB.2 More Ablation Study\nRegularizing generator vs. discriminator.. For the unsupervised generation, our default method add regularization on the loss functions of both generator G and discriminator D. In this experiment, we investigate the effectiveness of separately regularizing G and D. Table 8 presents the results of\nthe ablation study on UCLand, NWPU-RESISC45 and PatternNet datasets. The No Regularization version yields poor results as expected. Adding the regularization method on D already brings significant improvement to the model under different datasets. As proposed in our final method, adding the regularization on both G and D achieves the best results.\nHyperparameters. For the unsupervised generation, we conduct the ablation study on the hyperparameters \u03bb, \u03b3 and \u03b2 using TC-GAN+MAC on UCLand, NWPU-RESISC45 and PatternNet datasets. The FID scores are shown in Table 9, Table 10, Table 11. Based on the experiment results, we set \u03bb = 1, \u03b3 = 1 and \u03b2 = 1 in the following experiments. For the class-conditional generation, we use the same hyperparameters \u03bb = 1, \u03b3 = 1 as the unsupervised generation.\nB.3 Unsupervised Generation Results\nWe first show the training curves of our experiments on NWPU and PN datasets in Fig.13. Compared with the wellestablished models, our method not only alleviates the overfitting problem of the discriminator, but also gets better quality scores. As depicted in Fig.14, we also compare the singular values of the images generated by StyleGAN2+ADA and our proposed method on PN dataset. The results are consistent with the main paper.\nMoreover, we split the training classes into three categories: small, medium, large, according to the proportion of artificial landscapes. The categories are shown in Table.12,\nTable.13 and Table.14, and we can observe that these categories are distributed non-uniformly on the PN and UCLand datasets. Then, we train the classification model ResNET50 on the training datasets and classify the generated images into the three categories. The visual results are shown in Fig.15, Fig.16 and Fig.17, respectively. Our method is based on StyleGAN2+ADA. On the UCLand and PN datasets, the compared models learn the biased distribution and produce more artificial landscapes. On the NWPU datasets, the compared models tend to produce more natural landscapes, although the distribution of the three categories are almost uniform on this dataset. In comparison, our method produces a more even distribution of natural and artificial landscapes, which further prove that our method can learn the real data manifold of the RS data.\nB.4 Class-conditional Generation Results\nThe visual results of class-conditional generation are shown in Fig.18, Fig.19 and Fig.20, respectively."
        }
    ],
    "title": "A Unified GAN Framework Regarding Manifold Alignment for Remote Sensing Images Generation",
    "year": 2023
}