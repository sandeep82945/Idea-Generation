{
    "abstractText": "We consider physics-informed neural networks (PINNs) [Raissi et al., J. Comput. Phys. 278 (2019) 686-707] for forward physical problems. In order to find optimal PINNs configuration, we introduce a hyper-parameter optimization (HPO) procedure via Gaussian processes-based Bayesian optimization. We apply the HPO to Helmholtz equation for bounded domains and conduct a thorough study, focusing on: (i) performance, (ii) the collocation points density r and (iii) the frequency \u03ba, confirming the applicability and necessity of the method. Numerical experiments are performed in two and three dimensions, including comparison to finite element methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Paul Escapil-Inchausp\u00e9a"
        },
        {
            "affiliations": [],
            "name": "Gonzalo A. Ruza"
        },
        {
            "affiliations": [],
            "name": "Adolfo Ib\u00e1\u00f1ez"
        }
    ],
    "id": "SP:2b0466701b305df85774a3b40b34a5b503df6615",
    "references": [
        {
            "authors": [
                "J.-C. N\u00e9d\u00e9lec"
            ],
            "title": "Acoustic and Electromagnetic Equations: Integral Representations for Harmonic Problems",
            "venue": "Vol. 144, Springer Science & Business Media",
            "year": 2001
        },
        {
            "authors": [
                "O. Steinbach"
            ],
            "title": "Numerical Approximation Methods for Elliptic Boundary Value Problems: Finite and Boundary Elements",
            "venue": "Texts in Applied Mathematics, Springer New York",
            "year": 2007
        },
        {
            "authors": [
                "A. Ern",
                "J. Guermond"
            ],
            "title": "Theory and Practice of Finite Elements",
            "venue": "Applied Mathematical Sciences, Springer New York",
            "year": 2004
        },
        {
            "authors": [
                "S.N. Chandler-Wilde",
                "I.G. Graham",
                "S. Langdon",
                "E.A. Spence"
            ],
            "title": "Numerical-asymptotic boundary integral methods in high-frequency acoustic scattering",
            "venue": "Acta Numerica 21 ",
            "year": 2012
        },
        {
            "authors": [
                "S. Sauter",
                "C. Schwab"
            ],
            "title": "Boundary Element Methods",
            "venue": "Springer Series in Computational Mathematics, Springer Berlin Heidelberg",
            "year": 2010
        },
        {
            "authors": [
                "I.M. Babuska",
                "S.A. Sauter"
            ],
            "title": "Is the Pollution Effect of the FEM Avoidable for the Helmholtz Equation Considering High Wave Numbers",
            "venue": "SIAM Review 42 (3) ",
            "year": 2000
        },
        {
            "authors": [
                "Y. Bengio",
                "I. Goodfellow",
                "A. Courville"
            ],
            "title": "Deep learning",
            "venue": "Vol. 1, MIT press Cambridge, MA, USA",
            "year": 2017
        },
        {
            "authors": [
                "L. Lu",
                "X. Meng",
                "Z. Mao",
                "G.E. Karniadakis"
            ],
            "title": "DeepXDE: A deep learning library for solving differential equations",
            "venue": "SIAM Review 63 (1) ",
            "year": 2021
        },
        {
            "authors": [
                "Y. Khoo",
                "J. Lu",
                "L. Ying"
            ],
            "title": "Solving parametric PDE problems with artificial neural networks",
            "venue": "European Journal of Applied Mathematics 32 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "L. Scarabosio"
            ],
            "title": "Deep Neural Network Surrogates for Nonsmooth Quantities of Interest in Shape Uncertainty Quantification",
            "venue": "SIAM/ASA Journal on Uncertainty Quantification 10 (3) ",
            "year": 2022
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics 378 ",
            "year": 2019
        },
        {
            "authors": [
                "Y. Chen",
                "L. Lu",
                "G.E. Karniadakis",
                "L.D. Negro"
            ],
            "title": "Physics-informed neural networks for inverse problems in nano-optics and metamaterials",
            "venue": "Opt. Express 28 (8) ",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "J. Duan",
                "G.E. Karniadakis"
            ],
            "title": "Learning and meta-learning of stochastic advection\u2013diffusion\u2013reaction systems from sparse measurements",
            "venue": "European Journal of Applied Mathematics 32 (3) ",
            "year": 2021
        },
        {
            "authors": [
                "D. Zhang",
                "L. Lu",
                "L. Guo",
                "G.E. Karniadakis"
            ],
            "title": "Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems",
            "venue": "Journal of Computational Physics 397 ",
            "year": 2019
        },
        {
            "authors": [
                "X. Meng",
                "G.E. Karniadakis"
            ],
            "title": "A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems",
            "venue": "Journal of Computational Physics 401 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Wang",
                "X. Yu",
                "P. Perdikaris"
            ],
            "title": "When and why PINNs fail to train: A neural tangent kernel perspective",
            "venue": "Journal of Computational Physics 449 ",
            "year": 2022
        },
        {
            "authors": [
                "K. Zubov",
                "Z. McCarthy",
                "Y. Ma",
                "F. Calisto",
                "V. Pagliarino",
                "S. Azeglio",
                "L. Bottero",
                "E. Luj\u00e1n",
                "V. Sulzer"
            ],
            "title": "A",
            "venue": "Bharambe, et al., NeuralPDE: Automating physics-informed neural networks (PINNs) with error approximations, arXiv preprint arXiv:2107.09443 ",
            "year": 2021
        },
        {
            "authors": [
                "L.D. McClenny",
                "M.A. Haile",
                "U.M. Braga-Neto"
            ],
            "title": "TensorDiffEq: Scalable Multi-GPU Forward and Inverse Solvers for Physics Informed Neural Networks",
            "venue": "arXiv preprint arXiv:2103.16034 ",
            "year": 2021
        },
        {
            "authors": [
                "E. Haghighat",
                "R. Juanes"
            ],
            "title": "Sciann: A keras/tensorflow wrapper for scientific computations and physics-informed deep learning using artificial neural networks",
            "venue": "Computer Methods in Applied Mechanics and Engineering 373 ",
            "year": 2021
        },
        {
            "authors": [
                "S. Markidis"
            ],
            "title": "The old and the new: Can physics-informed deep-learning replace traditional linear solvers",
            "venue": "Frontiers in big Data ",
            "year": 2021
        },
        {
            "authors": [
                "J. Bergstra",
                "R. Bardenet",
                "Y. Bengio",
                "B. K\u00e9gl"
            ],
            "title": "Algorithms for Hyper-Parameter Optimization",
            "venue": "in: Advances in Neural Information Processing Systems, Vol. 24, Curran Associates, Inc.",
            "year": 2011
        },
        {
            "authors": [
                "J. Snoek",
                "H. Larochelle",
                "R.P. Adams"
            ],
            "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
            "venue": "Advances in neural information processing systems 25 ",
            "year": 2012
        },
        {
            "authors": [
                "T. Yu",
                "H. Zhu"
            ],
            "title": "Hyper-parameter optimization: A review of algorithms and applications",
            "venue": "arXiv preprint arXiv:2003.05689 ",
            "year": 2020
        },
        {
            "authors": [
                "L. Lu",
                "R. Pestourie",
                "W. Yao",
                "Z. Wang",
                "F. Verdugo",
                "S.G. Johnson"
            ],
            "title": "Physics-Informed Neural Networks with Hard constraints for Inverse Design",
            "venue": "SIAM Journal on Scientific Computing 43 (6) ",
            "year": 2021
        },
        {
            "authors": [
                "Z.-Q.J. Xu",
                "Y. Zhang",
                "T. Luo",
                "Y. Xiao",
                "Z. Ma"
            ],
            "title": "Frequency principle: Fourier analysis sheds light on deep neural networks",
            "venue": "arXiv preprint arXiv:1901.06523 ",
            "year": 2019
        },
        {
            "authors": [
                "S. Mishra",
                "R. Molinaro"
            ],
            "title": "Estimates on the generalization error of physics-informed neural networks for approximating PDEs",
            "venue": "IMA Journal of Numerical Analysis ",
            "year": 2022
        },
        {
            "authors": [
                "D.C. Liu",
                "J. Nocedal"
            ],
            "title": "On the limited memory BFGS method for large scale optimization",
            "venue": "Mathematical Programming 45 (1) ",
            "year": 1989
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980 ",
            "year": 2014
        },
        {
            "authors": [
                "L. Franceschi",
                "M. Donini",
                "P. Frasconi",
                "M. Pontil"
            ],
            "title": "A bridge between hyperparameter optimization and learning-to-learn",
            "venue": "arXiv preprint arXiv:1712.06283 ",
            "year": 2017
        },
        {
            "authors": [
                "A. Logg",
                "G.N. Wells"
            ],
            "title": "DOLFIN: Automated finite element computing",
            "venue": "ACM Transactions on Mathematical Software (TOMS) 37 (2) ",
            "year": 2010
        },
        {
            "authors": [
                "G.C. Diwan",
                "A. Moiola",
                "E.A. Spence"
            ],
            "title": "Can coercive formulations lead to fast and accurate solution of the Helmholtz equation",
            "venue": "Journal of Computational and Applied Mathematics 352 ",
            "year": 2019
        },
        {
            "authors": [
                "E.A. Spence"
            ],
            "title": "Wavenumber-Explicit Bounds in Time-harmonic Acoustic Scattering",
            "venue": "SIAM Journal on Mathematical Analysis 46 (4) ",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "We consider physics-informed neural networks (PINNs) [Raissi et al., J. Comput. Phys. 278 (2019) 686-707] for forward physical problems. In order to find optimal PINNs configuration, we introduce a hyper-parameter optimization (HPO) procedure via Gaussian processes-based Bayesian optimization. We apply the HPO to Helmholtz equation for bounded domains and conduct a thorough study, focusing on: (i) performance, (ii) the collocation points density r and (iii) the frequency \u03ba, confirming the applicability and necessity of the method. Numerical experiments are performed in two and three dimensions, including comparison to finite element methods.\nKeywords: physics-informed neural networks, hyper-parameter optimization, Bayesian optimization, Helmholtz equation"
        },
        {
            "heading": "1. Introduction",
            "text": "Having efficient methods to solve physical problems is key in several fields ranging from electronic design automation to optics and acoustics [1]. Physics can often be described by partial differential equations (PDEs) with suitable boundary conditions (BCs), i.e. as boundary value problems (BVPs) [1, 2]. Under appropriate conditions on the domain and the source term, BVPs are known to be well-posed on a continuous level [3]. Amongst other physical problems, acoustic wave behavior is often described by Helmholtz equations [1], whose underlined operator is coercive [2, Section 3.6]\u2014of the form elliptic+compact operator.\nTraditional schemes for solving BVPs include finite element methods (FEM) [2, 3], spectral methods or boundary element methods (BEM) [4, 5], the latter being commonly used for unbounded domains.\nThese techniques benefit from an enriched theory, including precise convergence bounds for both the solution error and iterative solvers [2]. They have been the state-of-the art solution in engineering applications over the past decades. However, their resolution can be numerically expensive (in particular for growing wavenumbers in propagation problems [6]), and they do not naturally incorporate additional information (e.g., sensors). Furthermore, they are linear by nature (i.e. they involve bounded linear operators), and they do not adapt with high dimensional problems.\nAs opposed to the previous techniques, deep learning (DL) [7] has shown to be a promising research area. Recent advances in hardware capabilities (GPU acceleration, increase in computational power) made it possible to use deep neural networks (DNNs) to represent complex problems. They allow simulating high-dimensional mappings [8, 9] with direct application to uncertainty quantification [10].\n\u2217Corresponding author Email addresses: paul.escapil@edu.uai.cl (Paul Escapil-Inchausp\u00e9), gonzalo.ruz@uai.cl (Gonzalo A. Ruz)\nPreprint submitted to Elsevier February 1, 2023\nar X\niv :2\n20 5.\n06 70\n4v 2\n[ m\nat h.\nN A\n] 3\n0 Ja\nn 20\n23\nOne type of DNNs are physics-informed neural networks which were introduced recently in [11]. They encode the boundary value problem in the loss function and rely on automatic differentiation. Their strength is that they combine the aforementioned strengths of both DL and classical numerical analysis.\nThe physics (PDE, BCs), and more generally, the additional information the user has about the problem, can be plugged into the DNN. This allows for genericity and simplicity. Among others, PINNs have proved to be useful to solve inverse problems [12] or stochastic PDEs [13, 14, 15]. Also refer to [16] and the references therein.\nSeveral active open-source libraries have been created, demonstrating a growing interest in PINNs. We mention DeepXDE [8] and [17, 18, 19]. These are built on top of DL libraries such as TensorFlow, Keras, or PyTorch.\nBy nature, PINNs exhibit a number of hyper-parameters (HPs) such as: the learning rate, the width and depth for the DNN, the activation function and the weights for the loss function. Notice that one could include more HPs, such as the optimization algorithm, the learning rate function complexity (e.g. using a learning rate scheduler), or the collocation points distribution. The high dimensionality for the HPs search space makes it difficult to find proper configurations\u2014this means configurations that lead to satisfactory generalization results. Furthermore, slow training has been a frequent concern for the PINNs community [19, 20].\nTo remedy those concerns, HPO [21]\u2014also referred to as (HP) tuning\u2014is an insightful solution. Many approaches exist, such as grid (resp. random) search optimization [21]. Still, grid search suffers from the curse of dimensionality, and random search can be very inefficient. Furthermore, each assessment of HPs leads to train a DNN, advocating for limiting the total number of configurations. To mitigate these drawbacks, (Gaussian processes-based) Bayesian optimization [22, 23] is a method of choice. It consists in applying GPbased regression to the previous assessed HPs in order to predict the next (most likely) best configuration.\nFinally, it is known that \u201cthe direct usage of PINN in scientific applications is still far from meeting computational performance and accuracy requirements\u201d [12], as compared to traditional solvers. It could be studied more accurately how far GPU-PINN is from meeting traditional FEM (e.g., in terms of orders of magnitude, for both the memory requirements and computational times).\nIn this work, we apply HPO via Gaussian processes-based Bayesian optimization to mitigate the poor training of PINNs. We focus on forward problems for Helmholtz operator. It is chosen as it is: a linear (yet coercive) operator, which encompasses the issues related to the oscillatory modes [6]; and it paves the way towards more complex cases including fluid mechanics and electromagnetic waves simulation [1].\nAlso, to our knowledge, application of PINNs to this operator was not studied thoroughly before (except from [12, 24]). We conduct an exhaustive overview of tuning concerning: (i) performance, (ii) the collocation point density \u2014or precision\u2014r and (iii) the wavenumber \u03ba. We consider two- (resp. three-) dimensional cases with Dirichlet (resp. Neumann) BCs, and compare the PINNs to FEM. Numerical results are performed with the state-of-the-art DeepXDE1 Python library [8]. Remark that a tutorial was added by the authors to DeepXDE documentation.\nOur main practical findings are as follows: HPO leads to a significant reduction for the loss (e.g. 7 orders of magnitude in two dimensions for \u03ba = 4\u03c0), hence the necessity of the tuning procedure. According to HPO for Helmholtz problems, our numerical results issue the following practical guidance for the PINNs configuration:\n\u2022 Quite shallow DNNs, with depth L\u2212 1 \u2208 {2, 3}, and constant width N \u2208 [250, 500];\n\u2022 Learning rates \u03b1 \u2208 [10\u22124, 10\u22123], and \u03c3 = sin activation function;\n\u2022 Small BCs loss term w\u0393 \u2208 [1, 10].\n1https://github.com/lululxvi/deepxde/\nMoreover, we observe that the training deteriorates with increasing \u03ba\u2014due to the frequency-principle [20, 25]\u2014and precision r.\nThis work is structured as follows: we introduce PINNs in Section 2 and the Bayesian HPO in Section 3. Consequently, we conduct exhaustive numerical experiments in Section 4 and discuss further research avenues in Section 5."
        },
        {
            "heading": "2. Physics-informed neural networks (PINNs)",
            "text": "Set d \u2265 1 and consider a bounded computational domain D \u2282 Rd with boundary \u0393 := \u2202D and exterior unit normal field n.\nFor any scalar field u, consider the following problem:\nN [u] = f in D, B[u] = g on \u0393,\nwith N a potentially nonlinear differential operator, and B the boundary conditions (BCs) operator. For example, the Helmholtz operator reads:\nN [u] := \u2212\u2206u\u2212 \u03ba2u,\nwith \u03ba the wavenumber. As BCs, we introduce:\n(Dirichlet BC): B[u] := u|\u0393 = g on \u0393, (Neumann BC): B[u] := \u2207u|\u0393 \u00b7 n = g on \u0393.\n(1)\nThroughout this manuscript, we assume that that the problems under consideration are well-posed (refer to [2, Section 4.4] for more details). Let \u03c3 be a smooth activation function. Given an input x \u2208 D \u2282 Rd and following [8, Section 2.1], we define an L-layer neural network with Nl neurons in the l-th layer for 1 \u2264 l \u2264 L\u2212 1 (N0 = d and NL = 1). For 1 \u2264 l \u2264 L, let us denote the weight matrix and bias vector in the l-th layer by Wl \u2208 RNl\u00d7Nl\u22121 and bl \u2208 RNl , respectively. The solution u can be approximated by a deep (feedforward) neural network defined as follows:\ninput layer: x \u2208 Rd, hidden layers: zl(x) = \u03c3(Wlzl\u22121(x) + bl) \u2208 RNl ,\nfor 1 \u2264 l \u2264 L\u2212 1, output layer: zL(x) = WLzL\u22121(x) + bL \u2208 R.\n(2)\nThis results in the representation u\u03b8(x) := zL(x), with\n\u03b8 := {(W1,b1), \u00b7 \u00b7 \u00b7 , (WL,bL)}\nthe (trainable) parameters\u2014or weights\u2014in the network. For the sake of simplicity, we set:\n\u0398 := R|\u03b8|. (3)\nAcknowledge that [26, Eq. (2.11)]\n|\u0398| =: dim(\u0398) = L\u2211 l=1 Nl(Nl\u22121 + 1). (4)\nNext, we introduce collocation points (sensors) for both the domain TD := {xDi } |TD| i=1 and the boundary T\u0393 := {x\u0393i } |T\u0393| i=1 , and assume that we have observations {ui(xui )} |Tu| i=1 on Tu := {xui } |Tu| i=1 . Finally, we set\nT := TD \u222a T\u0393 \u222a Tu.\nNotice that the formulation remains valid for unlabelled data, i.e. Tu = {}. For any wD, w\u0393, wu > 0, the weighted composite loss function is defined as:\nL\u03b8 = wDLD\u03b8 + w\u0393L\u0393\u03b8 + wuLu\u03b8 , (5)\nwhere LD\u03b8 := 1\nND \u2211 x\u2208TD \u2223\u2223(N [u\u03b8, k]\u2212 f)(x)\u2223\u22232, L\u0393\u03b8 := 1\nN\u0393 \u2211 x\u2208T\u0393 \u2223\u2223(B[u\u03b8]\u2212 g)(x)\u2223\u22232 and\nLu\u03b8 := 1\nNu \u2211 x\u2208Tu \u2223\u2223(u\u03b8 \u2212 ui)(x)\u2223\u22232. A schematic representation of a PINN is shown in Figure 1.\nRemark that the PDE and BCs are incorporated throughout the loss function, and evaluated via automatic differentiation. The optimization (training) procedure allows to define the control weights \u03b8 \u2208 \u0398 of the network. For this purpose, the sensors are partitioned into a training and testing data:\nT train\u00b7 and T test\u00b7 ,\nrespectively, with \u00b7 being either D,\u0393, u. Accordingly, we introduce T train and T test, and the losses Ltrain\u03b8 and Ltest\u03b8 . We define the target as the training loss function Ltrain\u03b8 . In parallel, when we are with an exact solution, we use the relative l2-norm error taken on the testing sensors T testD :\nLmetric\u03b8 = \u2016u\u03b8 \u2212 u\u20162 \u2016u\u20162 ,\nwith vectors u,u\u03b8 \u2208 R|T test D | with coefficients ui := u(xDi ) and u\u03b8,i = u\u03b8(xDi ) respectively.\nFor Dirichlet BCs, one can enforce hard constraint BCs [24, Section 2.3] by applying a transformation to the net:\nu\u0302\u03b8(x) := g(x) + `(x)u\u03b8(x) x \u2208 D (6)\nwhere `(x) = 0, x \u2208 \u0393, `(x) > 0, x \u2208 D. (7)\nWe are seeking: \u03b8? = argmin\u03b8\u2208\u0398(L\u03b8) (8)\nas being the inner optimization problem. Next, the HPO will introduce an outer counterpart. An approximation to (8) is delivered via an iterative optimizer such as ADAM (see Algorithm 1). Notice that further application of L-BFGS [27] can improve training [8].\nAlgorithm 1 ADAM [28, Algorithm 1] applied to loss function L(\u03b8). g2t indicates the elementwise square. In this work, we use the default settings for \u03b21 = 0.9, \u03b22 = 0.999 and = 10\u22127 (resp. = 10\u22128) for TensorFlow (resp. PyTorch).\nInput: \u03b1 (learning rate) Input: \u03b21, \u03b22 \u2208 [0, 1) (exponential decay rates for the moment estimates) Input: \u03b80 (initial trainable parameters) Initialize 1st and 2nd moment vectors: m0 \u2190 0, v0 \u2190 0 for k = 0 to K \u2212 1 do gk+1 \u2190 \u2207l(\u03b8k) mk+1 \u2190 \u03b21 \u00b7mk + (1\u2212 \u03b21) \u00b7 gk+1 vk+1 \u2190 \u03b22 \u00b7 vk + (1\u2212 \u03b22) \u00b7 g2k+1 m\u0302k+1 \u2190 mk+1/(1\u2212 \u03b2k+11 ) v\u0302k+1 \u2190 vk+1/(1\u2212 \u03b2k+12 ) \u03b8k+1 \u2190 \u03b8k \u2212 \u03b1 \u00b7 m\u0302k+1/( \u221a v\u0302k+1 + )\nend for Output: \u03b8+K \u2190 argmink\u2208{0,\u00b7\u00b7\u00b7 ,K} l(\u03b8k)\nApplication of ADAM optimizer to the PINNs with loss Ltrain\u03b8 leads to:\n\u03b8+K := argmink\u2208{0,\u00b7\u00b7\u00b7 ,K} L train \u03b8k\n(9)\nand final best approximate u\u03b8+K . Still, u\u03b8k depends on the DNN setting, and the losses\nlosstrain := Ltrain \u03b8+K and loss \u2261 losstest :=Ltest \u03b8+K\n(10)\ncan be considerable, even for large K. As a consequence, we tune the PINNs in order to find optimal configurations."
        },
        {
            "heading": "3. Bayesian HPO",
            "text": "Following [13, Section 4.3], we employ Bayesian HPO to seek for the optimal PINNs configuration. In this work, we opt for optimizing five HPs namely:\n1. Learning rate \u03b1; 2. Width N : number of nodes per layer; 3. Depth L\u2212 1: number of dense layers; 4. Activation function \u03c3; 5. Weights for the boundary term error w\u0393.\nNotice that we restrict to constant-width DNNs, i.e. Nl = N for N = 1, \u00b7 \u00b7 \u00b7 , L\u2212 1. The search space is \u039b, the Cartesian product of all HP ranges. Every \u03bb \u2208 \u039b writes as:\n\u03bb = [\u03b1,N,L\u2212 1, \u03c3, w\u0393].\nThe HPO of PINNs can be represented by a bi-level optimization problem [29]:\n\u03bb? = argmin\u03bb\u2208\u039b Ltest\u03b8? [\u03bb] with \u03b8? = min \u03b8\u2208\u0398 Ltrain\u03b8 [\u03bb]. (11)\nIn the same fashion as with the inner loop in Figure 1, we aim at applying an optimizer to the outer loop. Beforehand, notice that:\n\u2022 dim(\u039b) = 5. Grid search becomes impractical, as a 10 points per dimension grid would lead to 105 outer loops;\n\u2022 The inner loop in Figure 1 is potentially computationally intensive and time consuming, justifying a more subtle approach than (pure) random search.\nA natural choice is to apply GP-based Bayesian HPO to obtain approximates to \u03bb?. This method is a good trade-off between brute force grid search and random search. It consists in applying a GP-based regressor to the previous configurations in order to predict the best next HP setting. In Algorithm 2 we summarize the HPO. The acquisition function in Algorithm 2 allows to define the next point at each iteration m. For the\nAlgorithm 2 GP-based Bayesian HPO [23] for \u03bb\nInput: gacq (acquisition function), loss (loss function) Input: \u03bb0 (initial HPs)\nfor m = 0 to M \u2212 1 do Apply GP regression to (\u03bbi, loss(\u03bbi)), i = 0, \u00b7 \u00b7 \u00b7 ,m : \u21d2 loss(\u03bb) \u223c GP(\u00b5(\u03bb), k(\u03bb, \u03bb\u2032)) \u03bbm+1 \u2190 argmin\u03bb gacq(\u00b5(\u03bb), k(\u03bb, \u03bb\u2032)) Sample loss(\u03bbm+1)\nend for Output: (\u03bbM , loss(\u03bbM ))\nsake of clarity, m is coined as an iteration, and k as an epoch, respectively. A common acquisition function is the negative expected improvement, namely:\n\u2212 EI(\u03bb) = \u2212E[max(f(\u03bb)\u2212 f(\u03bb+m), 0)] (12)\nwhere \u03bb+m is the best point observed so far and E is the expected value. Concerning the GP, one generally resorts to constant \u00b5(\u03bb) and to Mat\u00e9rn or squared exponential covariance kernel k(\u03bb, \u03bb\u2032)2. Categorical data such as the activation function \u03c3 can be transformed into continuous variables (see for example the setting later on in Section 4.1).\nThe outer loop is conducted throughout an M -step GP-based Bayesian HPO. For each \u03bb \u2208 \u039b, we define the loss function as being the best (inner) ADAM epoch for k \u2208 [0,K] with loss as in (10):\nloss[\u03bb] = Ltest\u03b8 [\u03bb] for \u03b8 = argmink=0,\u00b7\u00b7\u00b7 ,K Ltrain\u03b8k [\u03bb]. (13)\nThe bi-level optimization produces the final solution u\u03b8+K [\u03bb + M ]. We are now ready to perform the numerical experiments."
        },
        {
            "heading": "4. Numerical experiments",
            "text": "In what follows, we set up the numerical experiment\u2019s protocol.\n2https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html"
        },
        {
            "heading": "4.1. Methodology",
            "text": "Throughout, we tune PINNs via HPO defined previously in Section 3. Our focus is the Helmholtz equation in two and three dimensions. This has been implemented in the open-source PINNs library DeepXDE 1.1.14 [8]. Simulations were performed in single float precision on a AMAX DL-E48A AMD Rome EPYC server with 8 Quadro RTX 8000 Nvidia GPUs\u2014each one with a 48 GB memory. DeepXDE supports TensorFlow 1.x, TensorFlow 2.x and PyTorch as backends, and is with single GPU acceleration. GP-based Bayesian HPO is performed with the novel high-level HPO library HPOMax3 built upon Scikit-Optimize4. For reproducibility purposes, results are performed with global random seed 11.\nIn this section, we solve the Helmholtz equation in D = [0, 1]d, d = 2, 3. For any pulsation \u03c9 \u2208 N, \u03c9 \u2265 1, and \u03ba := 2\u03c0\u03c9, the problem reads:\n\u2212\u2206u\u2212 \u03ba2u = f in D, B[u] = 0 on \u0393.\nwherein d, f and u are summarized for both the Dirichlet and Neumann cases in Table 1. For Dirichlet case, we enforce hard constraint BCs by applying the transformation:\nu\u0302\u03b8(x) = (1\u2212 x2)(1\u2212 y2)u\u03b8(x)\nto the net. Notice that the usage of this transformation is common, but that other choices are possible as long as they enforce the BCs. Finally, a comprehensive tutorial was added to the DeepXDE documentation5.\nThe training (resp. testing) collocation points are generated randomly with a precision of r = 10 (resp. r = 30) points per wavelength per dimension, with 2\u03c0\u03ba the wavelength. The inner loop is solved via ADAM over K = 50,000 epochs with Glorot uniform initialization [7, Chapter 8]. The HPO is run over M = 100 iterations, with negative expected improvement in (12) as the acquisition function. The default parameters are set to:\nDirichlet: \u03bb0 = [10\u22123, 4, 50, sin],\nNeumann: \u03bb0 = [10\u22123, 3, 275, sin, 400].\nIn Table 2, we summarize notations for HPs. We list the associated symbol and range for each HP. We use the default parameters of the gp_minimize Scikit-optimize function (more details in documentation6). Regarding the search space, integer variables are processed as continuous variables in the HPO. Furthermore, categorical variables are converted into normalized binary vectors, and applied an argmin function upon. For example, \u03c3 \u2208 {tanh, sin} is transformed into a two-dimensional vector with [1, 0] as \u03c3 = tanh and [0, 1] as \u03c3 = sin. Therefore, if the surrogate prediction is [0.4, 0], one will use a tanh function at next iteration. Additionally, the first 10 iterations are performed with random points generation.\n3https://github.com/pescap/HPOMax/ 4https://github.com/scikit-optimize/scikit-optimize/ 5https://deepxde.readthedocs.io/en/latest/demos/pinn_forward/helmholtz.2d.dirichlet.hpo.html 6https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html"
        },
        {
            "heading": "4.2. Dirichlet case: HPO",
            "text": "To begin with, we focus on the Dirichlet case. We set d = 2, \u03c9 = 2\u2014i.e \u03ba = 4\u03c0. The collocation points are with: |T train| = 400 and |T test| = 3,600.\nHPO leads to an optimal configuration at step 97 where\n\u03bb+M = \u03bb97 = [10 \u22124, 2, 275, sin] (14)\nwith loss[\u03bb97] = 1.13\u00d7 10\u22123 and |\u039897| = 77,001. (15)\nOn the left side of Figure 2, we show {loss\u00b7[\u03bbm]}99m=0 for \u00b7 \u2208 {train, test} ordered in ascending order. It\nallows to analyze the distribution of loss\u00b7, \u00b7 \u2208 {train, test}, over the iterations. On the right side of Figure 2, we represent the best loss[\u03bb] at step m. Acknowledge that the latter consists in the output of the HPO at step m (refer to Algorithm 2.) We observe that the training and testing errors are close together for the best and worst configurations, although some of them do not accomplish this pattern in the middle part of the figure (for instance, iteration 11 is with losstrain[\u03bb11] = 1.60 and losstest[\u03bb11] = 13.2). Furthermore, only few \u03bbm lead to small loss. To illustrate this, it is remarked that the 80% worst configurations are with more\nthan 102 as test error. Also, the loss ranges from 10\u22123 to 104, which implies a high variability in the training results. These two remarks justify the core of the tuning procedure. Next, we analyze the performance of HPO along iteration m for m = 0, \u00b7 \u00b7 \u00b7 ,M \u2212 1. On the right side, we show HPO best iterate at step m. First, notice the correlation between the test and metric curves. In this simulation, an improvement in the test loss comes with improvement in the solution accuracy. Second, the test loss is with a jump of more than 3 orders of magnitude at step 16 (loss[\u03bb16] = 2.94 \u00d7 10\u22123 and lossmetrics[\u03bb16] = 2.75 \u00d7 10\u22124). Next, the best configuration is found at step 97 (loss[\u03bb97] = 1.13 \u00d7 10\u22123 and lossmetric[\u03bb97] = 1.19 \u00d7 10\u22124). To reformulate, a good configuration was found at step 16, and a slightly more optimal (by around a factor of 2) emerges at step 97. Concerning execution times, HPO takes 10.0 hours. Each iteration m takes 6 minutes on average. The standard deviation is of 3 minutes and 8 seconds. The number of trainable parameters is 4.23\u00d7 105 \u00b1 6.38\u00d7 105 (average \u00b1 standard deviation).\nNext, we focus on the HPs. In Figure 3, we represent the partial dependence plots of the objective function. It estimates the relative dependence of each dimension on the loss (after averaging out all other dimensions). We remark that the partial dependence is bounded away from a constant for all HPs, testifying for a complex loss function depending on all the HPs. We remark that some areas (in light salmon color) show better behaviors. To sum up, the plot suggests using a small learning rate. For the number of dense nodes, several zones appear, but we focus on the area for the minimum. It suggests to use:\n\u03b1 \u2248 10\u22124, N \u2208 [250, 500], L\u2212 1 \u2264 3, \u03c3 = sin . (16)\nNotice in the last row that \u03c3 = sin leads to the best and the worst error losses with respect to \u03b1 and N . To finish the analysis, we scrutinize the top configurations more into detail. In Figure 4, we show a global overview of HPO in a manner that allows to observe the HPOs, the loss and number of trainable parameters. Again, we remark that very few configurations are with an error lesser than 1. Now, we can find patterns in the best configurations. To begin with, notice that the 10 best configurations are with sin\nactivation function. Furthermore, some configurations are with heavy computational cost, due to the DNN with e.g. 10\u00d7 500 or 8\u00d7 500 deep\u00d7width. As inferred in Figure 4, the best configurations are with learning rates around 10\u22124 and few layers L \u2212 1. As hinted before, several configurations do not converge at all, justifying HPO."
        },
        {
            "heading": "4.3. Dirichlet case: Best configuration",
            "text": "We focus on the best iterate \u03bb+M = \u03bb97 = [10\n\u22124, 2, 275, sin]. In Figure 5, we represent u\u03b8 the solution obtained by the PINN (top). We compare it to the exact solution u (bottom). We also represent the 400 training collocation points T trainD , which allow to define the loss function. Remark that u\u03b8 and u coincide."
        },
        {
            "heading": "1.2 0.8 0.4 0.0 0.4 0.8 1.2",
            "text": "For the sake of completeness, we showcase the pointwise loss in Figure 6 (left). Furthermore, we also represent the pointwise residual error |u\u03b8\u2212u| (right). The pointwise error ranges between 0 and 1.6\u00d7 10\u22123. Remark that the largest error for |u\u03b8 \u2212u| concentrates away from the boundary. Furthermore, we represent\nthe convergence for the ADAM optimizer in Figure 7. The optimal test error\u2014as well as for testing and metric errors\u2014is obtained at epoch k = 50,000. Remark that the training and testing error coincide for all epochs. Also, the metric is highly correlated with the testing error. The execution time is 196.6 seconds.\n4.4. Dirichlet case: (h, \u03ba)-analysis We complete the analysis for the Dirichlet case by performing an (h, \u03ba)-analysis. To that extent, we solve HPO with the same setting as in Section 4.2, for \u03c9 = {2, 4, 6} and levels {T1, T3, T5} with\n|Tl| = n2l with nl := 10\u00d7 2l\u22121 for l = {1, 3, 5}.\nTo sum up, T1, T3, T5 correspond to setting nx = 10, 40, 160 points per dimension. This leads to\n|T1| = 100, |T3| = 1,600, |T5| = 25,600.\nThe configurations and their respective precision r are depicted in Table 3. In Figure 8, we plot the result\nof HPO for all those configurations. For \u03c9 = 2, we remark that increasing level leads to better results. Also, all levels lead to satisfactory ranges. A important statement is that results deteriorate with increasing frequencies (from left to right). For \u03c9 = 4 and \u03c9 = 6, the best results are obtained with T3, while T1 does not converge (due to the small r). In the same fashion, we represent the metric lossmetric[\u03bbm] along HPO in Figure 9. Here, we observe that for \u03c9 = 4, the optimal metric is obtained for T3, despite not showing the best loss in Figure 8. The deterioration with \u03c9 is made clear here. We see that the results for \u03c9 = 6 are not satisfactory, despite showing an improvement during HPO. For the sake of completeness, we sum up the Top 5 configurations for all the settings in Figure 10. Notice that the HPs recommendation in (16) remain valid.\nIt is surprising to notice that higher levels do not necessarily lead to more successful HPO. Acknowledge that for a given \u03bb, some results exist concerning the generalization error according to |T train| and the training error [26, Theorem 2.6]. Yet, in our setting we are varying both \u03bb and |T train| at the same time. Our results hint at understanding how these parameters interact more into detail, but are out of the scope of this manuscript."
        },
        {
            "heading": "4.5. Dirichlet case: Comparison to FEM",
            "text": "To finish, we compare the best configuration for each setting to FEM. The FEM solutions are obtained with DOLFINx [30] with 10 points per wavelength, and piecewise polynomial elements of order 2. We use both a direct solver (LU factorization) and iterative solver (unpreconditioned GMRES with relative tolerance 10\u22125, no restart and maximum number of iterations 10,000). The results are summarized in Table 4. We start with accuracy (second column \u201cmetric\u201d). We remark that tuned PINNs outperform FEM in terms of accuracy for \u03c9 = 2 and for \u03c9 = 4, T3 (in bold notation). Remark that the error decreases with the level for FEM, by virtue of the quasi-optimality result for FEM for high enough precision [3]. Still, the tuned PINNs\nfollow this pattern only for small frequency \u03c9 = 2. Then, the best accuracy is obtained on T3 for \u03c9 \u2208 {4, 6}. Next, we focus on the third column with execution times. Both FEM alternatives outperform PINNs in all cases, by around a factor of 100. In this case, the fastest method is LU factorization. These results show the limitation of PINNs with the frequency, and pave the way toward working on frequency stable schemes."
        },
        {
            "heading": "4.6. Neumann case:HPO",
            "text": "We apply the exact same procedure to the 3D Neumann case. We define nx := 10r, T train = T test, |TD| = n2x, and |T\u0393| := 2d\u22121dnd\u22121x = 16n2x.\nThe HPO process leads to: \u03bb+M = \u03bb92 = [10 \u22124, 10, 207, sin, 1.0]\nalong with loss[\u03bb92] = 1.69 and Lmetric\u03b8+K [\u03bb92] = 3.7\u00d7 10 \u22121,\nSimilar to Figure 2, we plot the loss for m in Figure 11. Again, the training and test errors are similar.\nThe loss ranges from 1.69 to 1.19\u00d7 104. Notice that the improvement induced by the HPO (right) is lesser than for the Dirichlet case. This is due to\n\u03bb0 = [10 \u22123, 3, 275, sin, 400]\nwith loss[\u03bb0] = 6.62 and lossmetric[\u03bb0] = 1.83\u00d7 10\u22121,\nas being an effective configuration. Acknowledge that \u03bb0 for the Neumann case was inspired by \u03bb+M for the Dirichlet case in (14). This hints at some similarities between optimal HPs for different problems. Also, lossmetric[\u03bb] does not reach its minimum value at iteration m = 97.\nWe scrutinize the top configurations more into detail. In Figure 12, we show a global overview of HPO in a manner that allows to observe the HPs, the loss and number of trainable parameters.\nTo finish, we compare PINNs performance to FEM. For PINN, we choose the second configuration:\n\u03bb56 = [1.54\u00d7 10\u22123, 3, 292, sin, 19.1]\nas it has fewer trainable parameters. Next, we compare this configuration to FEM. The results are summarized in Table 5. We define Ndofs as the linear system size, and nnz. the number of non-zero values for the stiffness matrix. A few remarks: (i) |\u0398| is smaller than nnz. (by a factor of 10 for \u03bb56), (ii) the metric error is higher for PINNs by more than a factor of 10, and (iii) FEM keeps outperforming PINNs. Concerning execution time, LU and PINNs show more similar results, and GMRES is the best solution, as direct inversion does not scale well with dimension."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this work, we applied HPO via Gaussian processes-based Bayesian optimization to enhance the training of PINNs. We focused on forward problems for the Helmholtz operator, and carried out complete numerical experiments, with respect to performance, (h, \u03ba)-analysis, and dimension. We compared the fitted PINNs with FEM. Numerical results: (i) confirm the performance and necessity of HPO, (ii) give a further insight on which could be good HPs for this problem, and (iii) pave the way toward running more efficient PINNs.\nFurther research include application to other operators, and enhancing the presented tuning procedure. Also, notice the active research area on PINNs theory. For example, error bounds [26] would be valuable to obtain optimality conditions for (h, \u03ba) and to obtain bounded generalization and training errors. Both theoretical and empirical perspectives will allow to get closer to that objective. Also, it would be interesting to perform \u03ba-analysis [31, 32], and to introduce compression procedure for HPO."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank FES-UAI postdoc grant, ANID PIA/BASAL FB0002, and ANID/PIA/ ANILLOS ACT210096, for financially supporting this research."
        }
    ],
    "title": "Hyper-parameter tuning of physics-informed neural networks: Application to Helmholtz problems",
    "year": 2023
}