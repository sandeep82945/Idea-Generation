{
    "abstractText": "While many natural language inference (NLI) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no NLI dataset that involves diverse types of spatial expressions and reasoning. We fill this gap by semi-automatically creating an NLI dataset for spatial reasoning, called SpaceNLI.1 The data samples are automatically generated from a curated set of reasoning patterns (see Figure 1), where the patterns are annotated with inference labels by experts. We test several SOTA NLI systems on SpaceNLI to gauge the complexity of the dataset and the system\u2019s capacity for spatial reasoning. Moreover, we introduce a Pattern Accuracy and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system\u2019s performance on pattern-based generated data samples. Based on the evaluation results we find that the systems obtain moderate results on the spatial NLI problems but lack consistency per inference pattern. The results also reveal that nonprojective spatial inferences (especially due to the \u201cbetween\u201d preposition) are the most challenging ones.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lasha Abzianidze"
        },
        {
            "affiliations": [],
            "name": "Joost Zwarts"
        },
        {
            "affiliations": [],
            "name": "Yoad Winter"
        }
    ],
    "id": "SP:e376ec23365b085aeab722847f1683eb5c16a737",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2015
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Robin Cooper",
                "Dick Crouch",
                "Jan Van Eijck",
                "Chris Fox",
                "Josef Van Genabith",
                "Jan Jaspars",
                "Hans Kamp",
                "David Milward",
                "Manfred Pinkal",
                "Massimo Poesio",
                "Steve Pulman",
                "Ted Briscoe",
                "Holger Maier",
                "Karsten Konrad"
            ],
            "title": "FraCaS: A Framework for Computa",
            "year": 1996
        },
        {
            "authors": [
                "F. Liu",
                "Phoebe Mulcaire",
                "Qiang Ning",
                "Sameer Singh",
                "Noah A. Smith",
                "Sanjay Subramanian",
                "Reut Tsarfaty",
                "Eric Wallace",
                "Ally Zhang",
                "Ben Zhou"
            ],
            "title": "Evaluating models\u2019 local decision boundaries via contrast sets",
            "venue": "In Findings of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Max Glockner",
                "Vered Shwartz",
                "Yoav Goldberg."
            ],
            "title": "Breaking NLI systems with sentences that require simple lexical inferences",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Swabha Swayamdipta",
                "Omer Levy",
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A. Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "year": 2021
        },
        {
            "authors": [
                "Annette Herskovits."
            ],
            "title": "Language and Spatial Cognition: an interdisciplinary study of the prepositions in English",
            "venue": "Studies in Natural Language Processing. Cambridge University Press, London.",
            "year": 1986
        },
        {
            "authors": [
                "Md Mosharaf Hossain",
                "Venelin Kovatchev",
                "Pranoy Dutta",
                "Tiffany Kao",
                "Elizabeth Wei",
                "Eduardo Blanco."
            ],
            "title": "An analysis of natural language inference benchmarks through the lens of negation",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Drew A Hudson",
                "Christopher D Manning."
            ],
            "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6700\u20136709.",
            "year": 2019
        },
        {
            "authors": [
                "Paloma Jeretic",
                "Alex Warstadt",
                "Suvrat Bhooshan",
                "Adina Williams."
            ],
            "title": "Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens van der Maaten",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross Girshick."
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "Proceedings of the IEEE Conference on Computer",
            "year": 2017
        },
        {
            "authors": [
                "Pratik Joshi",
                "Somak Aditya",
                "Aalok Sathe",
                "Monojit Choudhury."
            ],
            "title": "TaxiNLI: Taking a ride up the NLU hill",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 41\u201355, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Divyansh Kaushik",
                "Eduard Hovy",
                "Zachary C Lipton."
            ],
            "title": "Learning the difference that makes a difference with counterfactually augmented data",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Najoung Kim",
                "Roma Patel",
                "Adam Poliak",
                "Patrick Xia",
                "Alex Wang",
                "Tom McCoy",
                "Ian Tenney",
                "Alexis Ross",
                "Tal Linzen",
                "Benjamin Van Durme",
                "Samuel R. Bowman",
                "Ellie Pavlick"
            ],
            "title": "Probing what different NLP tasks teach machines about function word",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Kober",
                "Sander Bijl de Vroe",
                "Mark Steedman."
            ],
            "title": "Temporal and aspectual entailment",
            "venue": "Proceedings of the 13th International Conference on Computational Semantics - Long Papers, pages 103\u2013119, Gothenburg, Sweden. Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "Albert: A lite bert for self-supervised learning of language representations",
            "year": 2020
        },
        {
            "authors": [
                "Moritz Laurer",
                "Wouter van Atteveldt",
                "Andreu Casas",
                "Kasper Welbers"
            ],
            "title": "Less Annotating, More Classifying \u2013 Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "WANLI: Worker and AI collaboration for natural language inference dataset creation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6826\u20136847, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "Cite arxiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Bill MacCartney."
            ],
            "title": "Natural language inference",
            "venue": "Phd thesis, Stanford University.",
            "year": 2009
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Roshanak Mirzaee",
                "Hossein Rajaby Faghihi",
                "Qiang Ning",
                "Parisa Kordjamshidi."
            ],
            "title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Naik",
                "Abhilasha Ravichander",
                "Norman Sadeh",
                "Carolyn Rose",
                "Graham Neubig."
            ],
            "title": "Stress test evaluation for natural language inference",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340\u20132353,",
            "year": 2018
        },
        {
            "authors": [
                "Seungho Nam."
            ],
            "title": "The Semantics of Locative Prepositional Phrases in English",
            "venue": "Phd thesis, University of California, Los Angeles.",
            "year": 1995
        },
        {
            "authors": [
                "Yixin Nie",
                "Haonan Chen",
                "Mohit Bansal."
            ],
            "title": "Combining fact extraction and verification with neural semantic matching networks",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI).",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Alicia Parrish",
                "William Huang",
                "Omar Agha",
                "Soo-Hwan Lee",
                "Nikita Nangia",
                "Alexia Warstadt",
                "Karmanya Aggarwal",
                "Emily Allaway",
                "Tal Linzen",
                "Samuel R. Bowman"
            ],
            "title": "Does putting a linguist in the loop improve NLU data",
            "year": 2021
        },
        {
            "authors": [
                "Adam Poliak",
                "Jason Naradowsky",
                "Aparajita Haldar",
                "Rachel Rudinger",
                "Benjamin Van Durme."
            ],
            "title": "Hypothesis only baselines in natural language inference",
            "venue": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages",
            "year": 2018
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Kyle Richardson",
                "Hai Hu",
                "Lawrence S. Moss",
                "Ashish Sabharwal."
            ],
            "title": "Probing natural language inference models through semantic fragments",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Ap-",
            "year": 2020
        },
        {
            "authors": [
                "Felipe Salvatore",
                "Marcelo Finger",
                "Roberto Hirata Jr."
            ],
            "title": "A logical-based corpus for cross-lingual evaluation",
            "venue": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 22\u201330, Hong Kong, China.",
            "year": 2019
        },
        {
            "authors": [
                "Pontus Stenetorp",
                "Sampo Pyysalo",
                "Goran Topi\u0107",
                "Tomoko Ohta",
                "Sophia Ananiadou",
                "Jun\u2019ichi Tsujii"
            ],
            "title": "brat: a web-based tool for NLP-assisted text annotation",
            "venue": "In Proceedings of the Demonstrations at the 13th Conference of the European Chapter",
            "year": 2012
        },
        {
            "authors": [
                "Alane Suhr",
                "Mike Lewis",
                "James Yeh",
                "Yoav Artzi."
            ],
            "title": "A corpus of natural language for visual reasoning",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217\u2013223, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Masatoshi Tsuchiya."
            ],
            "title": "Performance impact caused by hidden bias of training data for recognizing textual entailment",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European",
            "year": 2018
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes",
                "Sumit Chopra",
                "Tom\u00e1s Mikolov."
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "venue": "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Hitomi Yanaka",
                "Koji Mineshima",
                "Daisuke Bekki",
                "Kentaro Inui"
            ],
            "title": "Do neural models learn systematicity of monotonicity inference in natural language",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Hitomi Yanaka",
                "Koji Mineshima",
                "Daisuke Bekki",
                "Kentaro Inui",
                "Satoshi Sekine",
                "Lasha Abzianidze",
                "Johan Bos"
            ],
            "title": "Can neural networks understand monotonicity reasoning",
            "venue": "In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Hitomi Yanaka",
                "Koji Mineshima",
                "Daisuke Bekki",
                "Kentaro Inui",
                "Satoshi Sekine",
                "Lasha Abzianidze",
                "Johan Bos."
            ],
            "title": "HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning",
            "venue": "Proceedings of the Eighth Joint",
            "year": 2019
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Ruslan Salakhutdinov",
                "Quoc V. Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Zhang",
                "Rachel Rudinger",
                "Kevin Duh",
                "Benjamin Van Durme."
            ],
            "title": "Ordinal common-sense inference",
            "venue": "Transactions of the Association for Computational Linguistics, 5:379\u2013395.",
            "year": 2017
        },
        {
            "authors": [
                "Joost Zwarts",
                "Yoad Winter."
            ],
            "title": "Vector space semantics: A model-theoretic analysis of locative prepositions",
            "venue": "Journal of logic, language and information, 9:169\u2013211.",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Natural language inference (NLI) is a popular task that evaluates NLP systems on text reasoning skills. In the task, a system has to predict an inference relation from a premise text to a hypothesis sentence/phrase. Usually, the task is three- or two-way classification, depending on whether in the inference labels of entailment, neutral, and contradiction, the latter two are merged into non-entailment. The task is intended for evaluation of NLP systems on reasoning, however, the systems with competitive results on NLI benchmarks are often exploiting dataset biases (Tsuchiya 2018; Poliak et al. 2018; Gururangan et al. 2018; McCoy et al. 2019, inter\n1https://github.com/kovvalsky/SpaceNLI\nalia) and their performance suffers from out-ofdistribution NLI sample problems (Glockner et al., 2018).\nTo better evaluate the reasoning skills of NLI systems, a series of works have been (semi-)automatically or manually creating NLI datasets that specialize in certain semantic phenomena. While some of these datasets come with a training part, most of them are intended solely for evaluation. For example, several datasets have been dedicated to monotonicity reasoning (Yanaka et al., 2019b,a, 2020), negation was targeted by Hossain et al. (2020), the dataset by Kober et al. (2019) focuses on temporal and aspectual inferences, Jeretic et al. (2020) semi-automatically generated NLI problems for implicatures and presuppositions. There are also NLI datasets that cover several semantic phenomena, having a separate section for each of the phenomena (Cooper et al. 1996; ar X\niv :2\n30 7.\n02 26\n9v 1\n[ cs\n.C L\n] 5\nJ ul\n2 02\n3\nRichardson et al. 2020, inter alia).\nWhile spatial reasoning has been included in several multi-modal QA datasets (Antol et al., 2015; Suhr et al., 2017; Johnson et al., 2017; Hudson and Manning, 2019) and in a couple of text-based QA datasets (Weston et al., 2016; Mirzaee et al., 2021), to the best of our knowledge, no NLI dataset has specifically covered it.2 This paper fills the gap by semi-automatically creating an NLI dataset for spatial inferences. First, we collected a diverse set of NLI problems inspired by the inference examples found in the literature on spatial semantics. Second, the NLI problems were manually converted into NLI patterns (see Figure 1), and finally, we automatically generated a large number of NLI problems from the patterns.\nThe paper makes two main contributions:\nC1. SpaceNLI: the spatial NLI dataset with diverse types of spatial inferences; The inference labels of the generated problems are highly faithful (97%) to the labels of the corresponding original patterns.\nC2. Pattern accuracy and its curve: they measure systems\u2019 performance on patterns and the consistency in predictions on samples from the same patterns.\nThe conducted experiments answer the following research questions:\nQ1. How much spatial reasoning current SOTA NLI systems are capable of?\nA1. We found out that the SOTA NLI systems have problems with fine-grained spatial inferences. Their performance drops at least by 24% compared to their results on common NLI datasets. Moreover, their consistency in predictions is sensitive to irrelevant lexical substitutions.\nQ2. What types of spatial inference problems are easy or challenging for the SOTA NLI systems?\nA2. The results showed that the non-projective spatial relations are most challenging for the models. This was mainly due to difficulty associated with \u201cbetween\u201d and its frequent occurrence in the evaluation dataset.\n2Even the FraCaS dataset (Cooper et al., 1996; MacCartney, 2009), which was curated by linguists and semanticists, doesn\u2019t cover spatial semantics within its nine sections."
        },
        {
            "heading": "2 Spatial expressions and inferences",
            "text": ""
        },
        {
            "heading": "2.1 Types of spatial expressions",
            "text": "Spatial expressions consist of spatial prepositions and other expressions with spatial information (e.g., far, the left of, and in front of ). They usually describe a relation between two entities, the figure and the ground. The site or path of the figure is the focus of the discussion and is characterized with respect to the ground. For example, in (91) and (101) from Figure 1, Mary is a figure and garden a ground. John is also a figure in the premise of (101).\nSpatial expressions are roughly divided into locative and directional expressions, where locatives can be further classified into projective and nonprojective (Herskovits, 1986). The locative expressions describe static, locative relations between the figure and the ground while directional ones describe a more dynamic relation involving a movement and/or path. An example with a directional preposition is Cindi walked into the market. The spatial expressions in Figure 1 are all locative except for from, which is directional. These locative expressions are non-projective since they require only the spatial location of the figure and the ground. In contrast, projective locatives additionally require further information from the ground in terms of a deictic frame of reference (i.e., an orientation structure). For example, the site of the house is not sufficient to interpret Mary\u2019s location in Mary is behind the house, it requires knowledge about the frame of reference of the house, in particular, what counts as a back side of the house."
        },
        {
            "heading": "2.2 Types of spatial inferences",
            "text": "We characterize spatial inferences depending on the type of spatial expressions licensing them. An inference might depend on several spatial expressions of a different type, which makes partitioning the inferences challenging, if not impossible. We define the following classes that represent a coarsegrained partition of spatial inferences. The classes will be later referred to in \u00a73.3\nArgument orientation In spatial literature, an argument orientation entailment identifies which\n3Licensing contradiction and neutral problems will be assumed from the perspective of a related entailment problem. For example, we assume that the neutral problem (16) in Table 1 is licensed in the same way as its related entailment (15). Put differently, one can see (16) as an adversary to (15) and assume that solving (15) requires competence comparable to the one required for solving (16).\nID Class Premise(s) L Hypothesis\n15 Dir John threw the ball into the box. E The ball went into the box.\n16 Dir John threw the ball at the box. N The ball went into the box.\n31a Dir Los Angeles is in California. John came from California. N John came from Los Angeles.\n38 NonP John is in the garden. The garden is in the church. E John is in the church.\n41 Dir John drove through the tunnel. E John was in the tunnel.\n47a Dir Cindi walked into the market. E Cindi was outside the market.\n56c Proj The trash can is to the right of the tree from John. C The tree is to the right of the trash can from John.\n70 Proj Mary is between the tree and the house. The tree is behind the house. E Mary is behind the house.\n80 NonP The cat is between the house and the fence. The cat is between the fence and the tree. C The cat is between the house and the tree.\n99*d Proj The bucket is above the bowl. The pencil is above the bowl. N The bucket is below the pencil.\nargument of the verb is the figure of the spatial expression. For instance, (91) in Figure 1 show that Mary is the figure of the locative PP in the garden. In its original interpretation, the argument orientation entailment is not restricted to spatial expressions of a particular type. Here, we restrict the class of argument orientation to the entailment problems (and their neutral and contradiction counterparts) that come close to resolving a PP attachment. For example, correctly resolving the PP attachment in (91) boils down to the hypothesis. The problems in this class contain a hypothesis with a copula and a predicative spatial PP, where the PP is contrasted to a tightly related PP in the premise(s). For more examples of the NLI problems in the argument orientation class, see Table 1.\nDirectional The directional class contains spatial inferences where directional spatial expressions play the key role. Examples of such inferences are given in Table 1. Some of these NLI problems pertain to a path-place relation: (47a) shows that walking into infers being outside;4 (41) entails being in the tunnel from the premise that states that the driving path was through the tunnel. (31a) combines a part-whole relation with the movement path.\n4Since moving along the path is related to the change of the location, sometimes spatial entailments interfere with tense and aspect.\nProjective This class contains inferences that hinge on a frame of reference introduced by projective spatial expressions. In principle, the frame of reference can introduce six directions that can be referred to using the expressions like front, behind, left, right, above, below, under, on top of, etc. (see the examples of NLI problems in Table 1). The NLI problems that contain on top of as only projective spatial expression, and when its projective interpretation is not crucial for the inference, are put in a different class.\nNon-projective We classify a problem as having non-projective inference if the inference is driven only by non-projective spatial expressions. Therefore, an occurrence of non-projective spatial expressions in a problem is necessary but not sufficient for assigning the problem to this class, e.g., see directional problems (31a) and (41). NLI problems that depend on spatial expressions with the semantics of order and proximity are also in this class, see between (80) and far (100) in Table 1."
        },
        {
            "heading": "3 Dataset construction",
            "text": ""
        },
        {
            "heading": "3.1 Pattern construction",
            "text": "Patterns are labeled NLI problems with NPs replaced by variables as illustrated in Figure 1. The NLI patterns are obtained from the seed NLI problems. To collect the latter, we extracted the initial 56 problems from Zwarts and Winter (2000) and Nam (1995), where a majority of the problems\nwere labeled as entailment due to obvious biases in the semantic literature towards licensing entailment. To create a representative and challenging NLI dataset for machine learning, we applied several revision phases to the problems: introducing new problems that either cover new semantic aspects of spatial expression or serve as a perturbed version of an existing problem.\nIn the initial revision phase, four annotators divided the extracted problems and created slightly modified versions of them with an inference label different from the original.5 This was motivated by the current trends in the literature on adversarial, stress, and debiased datasets (Naik et al. 2018; Ribeiro et al. 2020; Kaushik et al. 2020; Gardner et al. 2020, inter alia). For example, (16) is a perturbed example of (15). Where possible, NLI problems of a new type were also created using the similar spatial expressions found in the extracted problems.\nTo validate the resulting pool of NLI problems (in total 162), following (Zhang et al., 2017), they were labeled on a 5-point Likert scale by three annotators.6 After collecting the 5-point annotations, for each annotator, we picked a mapping of 5- point to 3-point that maximizes the inter-annotator agreement (avg. Cohen\u2019s \u03ba = .71). The problems without majority labels were discarded and 111 problems remained.\n5The annotators for the pattern construction consist of the authors of the paper, two linguist students, and one AI student. The guideline for creating inference problems can be found in the supplementary material.\n6The question was to what extent the hypothesis sentence is true, given that the premises are true, with choices: definitely false, most likely false, unknown, most likely true, definitely true. We used two additional choices, difficult (unable to annotate due to the complex reasoning it requires) and skip (presence of an ungrammatical or nonsensical sentence). We used the brat annotation tool (Stenetorp et al., 2012) for labeling. The annotation guideline is included in the supplementary material.\nTo better balance the inference labels and increase the coverage of spatial expressions, a second revision phase was carried out on the remaining problems. In several cases, problems with low annotator agreement were revised, e.g., changing the tense where it caused confusion or replacing a preposition with a weaker version (at 7\u2192near). All the new and revised problems (in total 63) were validated based on three samples: each problem was manually converted into a pattern by replacing NPs with variables, and three random NLI samples per pattern were generated (see \u00a73.2 for details), which were subsequently validated by three annotators.\nFinally, a third revision phase was carried out on the remaining problems to additionally decrease the overall and spatial type-specific label imbalance. The collected problems (in total 160) were treated as a seed by converting them into NLI patterns to generate a large amount of sample NLI problems from them. To illustrate the coverage of spatial expressions in the collected patterns, Table 2 gives the complete list of spatial expressions for each entailment class."
        },
        {
            "heading": "3.2 Sample generation",
            "text": "We manually created NLI patterns from the initially collected NLI problems (\u00a73.1) by replacing NPs with placeholders and specifying selection restrictions for them imposed by the verbs, spatial expressions, and gold inference labels (see Figure 1). The selection restrictions imposed by spatial expressions are subtle and can affect gold labels or the naturalness of sentences. For example, if the figure is much larger than the ground, it can make the sentence infelicitous: the apple on the fridge and the apple near the fridge are preferred to the fridge under the apple and the fridge near the apple. Inferences driven by proximity-related spatial expressions are sensitive to the size of the objects.\nFor instance, based on our conducted validations, Cindi is opposite to the cat is more likely to be neutral to Cindi is far from the cat, but the school is opposite to the house is more likely to contradict the school is far from the house.\nTo meet selection restrictions and allow relative diversity of NPs in the generated samples, we defined a mini world with a domain containing 171 entities corresponding to common and proper nouns. The entities are organized in a taxonomy with 20 subclasses covering general types of entities (e.g., person, animal, vehicle), the projections of an argument in certain argument structures (e.g., enter in X , be in X , throw X), compatibility with projective spatial expressions, and size categories (S for entities comparable to small objects like book and cat, M to persons, and L to vehicles). Binary and ternary relations are defined based on the set unions of the products of entity sets and subclasses.\nTo automatize the sampling of sound NLI problems from the patterns, we formatted the mini world in YAML and NLI patterns in XML. We implemented a procedure that samples problems from the patterns by filling in NP placeholders with definite NPs from the mini world and respecting the pattern-specific selection restrictions. For sanity checking, the procedure verifies that it can generate corresponding seed NLI problems for each pattern.\nTo measure how faithfully the inference labels are transferred from seed and pattern NLI problems to the corresponding NLI samples, we used sampled problems in the second phase of validation when validating new NLI problems (see \u00a73.1). The results showed that 79% of samples were unanimously labeled with the original label. After filtering out patterns with a relatively low agreement, this ratio increased to 97% for the samples generated from the validated patterns.\nThe NLI problems sampled from the same pattern or related patterns are string-wise very close to each other, sometimes differing only in terms of occurrences of a single NP. Regardless of this similarity, we expect such problems to pose a challenge for NLI systems based on large language models (LLMs) as it has been shown that their predictions can be sensitive to a single-word substitution (Glockner et al., 2018; Gururangan et al., 2018). In addition to NPs, one could have allowed the replacement of other phrases in the NLI patterns, but this would have significantly complicated the definition of the mini world and generation of\nnatural and sound NLI samples.\nLLM-based NLI models\nTraining data SNLI +\nMNLI SpaceNLI Acc PA0.95 PA1.0"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Sample dataset",
            "text": "We uniformly generated a spatial dataset of 32,000 NLI samples from 160 NLI patterns, i.e., 200 samples per pattern. We used the mini world as described in \u00a7 3.2. The dataset statistics are given in Table 3. The inference labels are relatively balanced: each label being represented by at least 30%\nof the problems. Each spatial inference type counts at least 20% of the overall problems and 23% of label-specific problems. In contrast to the common biases in NLI datasets, a majority of the problems with negation are labeled as entailment, not contradiction. This is due to perturbed problems introduced in the revision phases (\u00a7 3.1). Around 39% of problems have multiple premises, where three-premised problems occur only in the directional problems, the argument orientation problems contain only single-premised problems, and most of the multi-premised problems are in the nonprojective problems. We refer to the generated dataset as SpaceNLI and use it in subsequent experiments.7"
        },
        {
            "heading": "4.2 Evaluating SOTA NLI systems",
            "text": ""
        },
        {
            "heading": "4.2.1 Standard accuracy",
            "text": "We selected NLI models that have results comparable to the state of the art in NLI and evaluate them on SpaceNLI. The models were chosen based on their availability, tractable size, and high average accuracy (> 90%) on the SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets (see Table 4). The models are based on various large language models (LLMs) like DeBERTaV3 (He et al., 2023), BART (Lewis et al., 2020), ALBERT (Lan et al., 2020), XLNet (Yang et al., 2020), etc. (see Table 4). The LLMs are fine-tuned on several NLI train datasets: SNLI, MNLI, FEVER-NLI (Nie et al., 2019), ANLI (Nie et al., 2020), LingNLI (Parrish et al., 2021), WANLI (Liu et al., 2022). We use the models from the HuggungFace model hub8 and provide them with the corresponding hub names in Table 4.\nThe results in Table 4 show that DeBERTaV3-L#2 trained on a large collection of training datasets (885K problems in total) generalizes best on the spatial reasoning (66.5%), achieving a substantial improvement (\u2265 6.9%) over the other models.9\n7We make the collection of the patterns, the generation code, and the sample dataset publicly available upon the acceptance of the paper.\n8https://huggingface.co/models 9The second best, DeBERTaV3-L#1, is based on the same LLM fine-tuned on a different combination of NLI datasets. Note that Laurer et al. (2022) deliberately removed SNLI from the training set as it negatively affected the accuracy of the model in their experiments."
        },
        {
            "heading": "4.2.2 Consistency & pattern accuracy",
            "text": "To evaluate the models on the consistency of their predictions for NLI problems from the same pattern, we define the pattern accuracy (PA) score and its curve. The PA curve records the PA score of a model for each consistency threshold. Informally, the PA score with a consistency threshold t is a ratio of NLI patterns for which model gets at least t portion of the samples generated from them. For example, the PA of 50% with a threshold 90% means that there are a half of the NLI patterns such that for each pattern a model is able to correctly classify at least 90% of its sample problems. The formal definition of the PA with a threshold t is:\nPAt(Y\u0302 ,y) = 1\nN N\u2211 i=1 [\u2211Mi k=1 \u03b4(y\u0302 i k = y i) Mi \u2265 t ] where Y\u0302 = (y\u0302ik)1\u2264i\u2264N,1\u2264k\u2264Mi are predictions for kth sample of ith pattern, N is the number of patterns, Mi is the number of samples for ith pattern, y = (yi)1\u2264i\u2264N gold labels of ith pattern, and \u03b4 is the Kronecker delta.\nWhile DeBERTaV3-L#2 gets the best score on the SpaceNLI problems, based on the PA scores in Table 4, it shows high consistency (PA0.95 or PA1.0) in fewer NLI patterns than the other two competing models, DeBERTaV3-L#1 and ALBERT-XXLv2. PA curves of the NLI mod-\nels provide a closer look at this contrast (see Figure 2). While the curve of DeBERTaV3-L#2 outperforms other models by a margin, it is noteworthy that it does this by classifying sample problems of the patterns which it can hardly solve half of the time (this is visible in the complete curves in Appendix A). It drastically decreases after 95% of consistency while ALBERT-XXLv2 and DeBERTAV2L#1 maintain very high consistency for > 47% of NLI patterns. This demonstrates that a highperforming model is not necessarily the most consistent across patterns.\nRoBERTa-L and BART-L obtain similar accuracy scores, but RoBERTa-L is more consistent in more NLI patterns than BART-L while the latter gets slightly more NLI problems for inconsistently predicted patterns. The complete curves in Appendix A shows how the curves swap places after the consistency threshold of 50. This shows that the standard accuracy (i.e., based on NLI problem samples) can blur the fine distinction in consistency between the models.\nThe dispersion of the curves at the lowest end of the consistency threshold is twice larger than at the highest end. This shows that the model predictions more diverge in coverage of patterns than in consistency per pattern. In other words, the contrast confirms the sensitivity of the models towards the inference-preserving word substitutions."
        },
        {
            "heading": "4.2.3 Few-shot learning experiments",
            "text": "We measured the difficulty of the SpaceNLI problems in terms of few-shot learning experiments. We used 100 samples per pattern as a test set while other 100 samples per pattern were used for drawing a few samples for each pattern. In this way, the patterns are fully shared between the training and test sets, but no sample NLI problem is in both sets. For each number of shots, we carried out the sample drawing process three times. We used two NLI models: a high performing NLI model RoBERTaLSMFA from Nie et al. (2020) and a vanilla NLI model based on the large RoBERTa pretrained language model (Liu et al., 2019). The results of the few-shot experiments are in Figure 3.\nFinetuning RoBERTa-LSMFA on a single sample of each pattern increases the sample-based accuracy on the test set by 14%. Each additional sample further boosts the model\u2019s accuracy. The almost perfect accuracy (>99%) is reached when 20 samples per pattern are seen during the finetuning. The results show that the lexical variability poses a chal-\nlenge to the high-performing NLI model as it needs to be finetuned on at least five samples for every pattern of the test set to achieve a high score.\nThe challenge coming from the lexical variability and the SpaceNLI patterns is further emphasized by the relatively low results of RoBERTa Large. Even after being finetuned on the 20 samples of each NLI pattern, the model is still far from the high performance on unseen samples (but seen patterns). The relatively low results can be also partially attributed to the low ratio between the number of training samples and the large number of the model\u2019s trainable parameters."
        },
        {
            "heading": "5 Analysis",
            "text": "To find out what type of inferences the models find challenging, we analyze the models\u2019 performance per inference type. Figure 5 shows the sampleand pattern-based accuracy scores of the models per spatial inference types as defined in \u00a72.2. The model ranking based on the sample accuracy varies across the inference types. For instance, the best model, DeBERTaV3-L#2, remains at the top of the rankings for all inference types with quite a margin except for the projective type. On average, non-projective spatial inferences are the most challenging for the models. The easiest of the types is argument orientation, the type that is closest to the PP attachment task. For the other inference types, projective inferences are harder than directional ones. The apparent distinction in the scores between the inference types is also preserved for the PA0.95 score (shown with the dark bars in Figure 5).\nThe fine-grained analysis additionally shows that the best model, DeBERTaV3-L#2, suffers least in terms of consistency on the projective inferences while its performance on this inference type is not among the best.\nBased on the results in Figure 5, the nonprojective NLI patterns and samples are the most challenging for the SOTA models. When looking closer at the set of non-projective problems, it turns out that it contains a high number of problems (46%) with the spatial expression \u201cbetween\u201c (as shown in Table 2), and these problems are specially challenging due to complex semantics of \u201cbetween\u201d. The average accuracy of the models on such NLI samples is 41.6%. This is lower than\nthe average sample-based accuracy (46.1%) on entire SpaceNLI and much lower than the average sample-based accuracy (54.1%) on the other part of the non-projective samples.\nWe further zoom in on the NLI patterns and measure a model\u2019s probabilistic predictions for the patterns. Namely, following Swayamdipta et al. (2020), we measure a model\u2019s confidence and variability. Originally the dataset cartography (Swayamdipta et al., 2020) was used to analyze the training dynamics of a model across the epochs and identify training samples that are easy or difficult for learning. In contrast, we use dataset cartography for analyzing evaluation dynamics across patterns and identifying easy and hard ones.10\nFigure 4 illustrates the pattern-based evaluation dynamics of RoBERTa-L (Nie et al., 2020), an average model based on the evaluations. For instance, NLI pattern (102f) happens to have one of the most variable samples according to the model predictions: the mean and the standard deviation of the probabilities the model assigns to the entailment class of the samples of (102f) are 0.45 and 0.35, respectively.\n(102f) NP1 has hidden NP2 behind NP3. entailment NP2 is not in NP3.\nThe evaluation cartography shows that the predictions vary mostly for entailment patterns (in green). Most of the hard patterns are neutral ones (in blue) and vice versa. Contradiction patterns (in red) tend to be easy with some variability.\n10Put differently, iterative classification of the same training sample across epochs, is replaced with the classification of the same NLI pattern based on its samples."
        },
        {
            "heading": "6 Related work",
            "text": "Several works have automatically sampled NLI problems from curated patterns/templates. Jeretic et al. (2020) generated the implicature and presupposition diagnostic dataset IMPPRES from predefined templates. McCoy et al. (2019) constructed the HANS dataset by designing templates of NLI problems that support or refute certain inference heuristics, which were later used to generate NLI problems. Richardson et al. (2020) used the template language from Salvatore et al. (2019) to produce NLI problems involving negation, Boolean connectives, quantifiers, cardinals, conditionals, and comparatives. These works all use restricted vocabulary while generating samples from the patterns.\nWith its pattern-based construction and restricted vocabulary, SpaceNLI comes close to the IMPPRES (Jeretic et al., 2020) and HANS (McCoy et al., 2019) datasets. Unlike these datasets, SpaceNLI involves multiple-premised problems and puts more emphasis on satisfying selection restrictions to prevent nonsensical sentences.\nBased on the nature of NLI problems, SpaceNLI resembles FraCaS (Cooper et al., 1996) as both contain inference problems often found in textbooks on formal semantics. Unlike FraCaS, the inference labels of patterns in SpaceNLI are quite balanced and the number of spatial NLI patterns is twice the size of the largest section in FraCaS.\nThere have been attempts to identify semantic phenomena in existing NLI datasets, including aspects of spatial reasoning. By looking up certain keywords, Kim et al. (2019) automatically detect NLI problems in MultiNLI (Williams et al., 2018) that might contain spatial expressions. They create a mutated sample from the original NLI problem by negating the sentence with the potential spatial expression. Joshi et al. (2020) annotate MultiNLI problems based on the semantic aspects required by the inference label. Their taxonomic categories include the spatial subcategory, grouped with the relational, temporal, causal, and co-reference subcategories.\nThe problems in SpaceNLI are substantially diverse from a semantic perspective than the MultiNLI problems that were identified by Kim et al. (2019) and Joshi et al. (2020). The MultiNLI dataset is crowd-elicited and doesn\u2019t have problems with sufficient depth in spatial reasoning."
        },
        {
            "heading": "7 Conclusion",
            "text": "To the best of our knowledge, we have created the first spatial inference dataset that involves diverse spatial inference types. The structure and the evaluation protocol are unique as we focus on performance on the NLI patterns and consistency across the samples in the pattern, instead of focusing on mere quantitative accuracy based on the NLI problems/samples. The evaluation protocol tests models whether they can consistently recognize inference patterns while generalizing over irrelevant lexical substitutions. The more consistent a model is in its predictions, the less unexpected its behavior becomes.\nThe SOTA NLI models show moderate generalization capacity on spatial problems. While the top-performing model gets the highest overall accuracy, it is ranked third when it comes to the consistency of predictions inside the patterns: predicting at least 95% of the samples per pattern.\nThe introduced pattern accuracy (PA) curves provide a more fine-grained distinction between the models: the models with comparable standard accuracy scores might substantially differ in the consistency of their predictions. Overall the performance of models drops ca. 10% when raising the consistency threshold to 95%. This illustrates that the predictions of the SOTA models are sensitive to lexical replacements that have no effect on the semantics of the inference.\nThe evaluation results revealed that the most challenging inference type is associated with nonprojective locatives mainly due to the complex semantics of \u201cbetween\u201d while the argument orientation type is the easiest. The latter is somewhat expected as the problems in the argument orientation type are close to the task of PP attachment which LLMs are expected to be good at."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No 742204). We would like to acknowledge the help from three student assistants with the data annotation and thank the anonymous reviewers for their helpful comments."
        },
        {
            "heading": "A Results",
            "text": "Table 5 represents the extended version of Table 4. Note that the area under the curve corresponds to the standard accuracy based on the NLI problems."
        }
    ],
    "title": "SpaceNLI: Evaluating the Consistency of Predicting Inferences In Space",
    "year": 2023
}