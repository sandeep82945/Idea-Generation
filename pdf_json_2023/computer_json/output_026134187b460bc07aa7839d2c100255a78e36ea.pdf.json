{
    "abstractText": "Voice-controlled systems are becoming ubiquitous in many IoT-specific applications such as home/industrial automation, automotive infotainment, and healthcare. While cloud-based voice services (e.g., Alexa, Siri) can leverage high-performance computing servers, some use cases (e.g., robotics, automotive infotainment) may require to execute the natural language processing (NLP) tasks offline, often on resource-constrained embedded devices. Large language models such as BERT and its variants are primarily developed with compute-heavy servers in mind. Despite the great performance of BERT models across various NLP tasks, their large size and numerous parameters pose substantial obstacles to offline computation on embedded systems. Lighter replacement of such language models (e.g., DistilBERT and TinyBERT) often sacrifice accuracy, particularly for complex NLP tasks. Until now, it is still unclear (a) whether the state-of-the-art language models, viz., BERT and its variants are deployable on embedded systems with a limited processor, memory, and battery power and (b) if they do, what are the \u201cright\u201d set of configurations and parameters to choose for a given NLP task. This paper presents an exploratory study of modern language models under different resource constraints and accuracy budgets to derive empirical observations about these resource/accuracy trade-offs. In particular, we study how the four most commonly used BERT-based language models (e.g., BERT, RoBERTa, DistilBERT, and TinyBERT) perform on embedded systems. We tested them on a Raspberry Pi-based robotic platform with three hardware configurations and four datasets running various NLP tasks. Our findings can help designers to understand the deployability and performance of modern language models, especially those based on BERT architectures, thus saving a lot of time wasted in trial-and-error efforts.",
    "authors": [
        {
            "affiliations": [],
            "name": "SOUVIKA SARKAR"
        },
        {
            "affiliations": [],
            "name": "MOHAMMAD FAKHRUDDIN BABAR"
        },
        {
            "affiliations": [],
            "name": "Washington"
        },
        {
            "affiliations": [],
            "name": "MDMAHADI HASSAN"
        },
        {
            "affiliations": [],
            "name": "SHUBHRA KANTI"
        },
        {
            "affiliations": [],
            "name": "KARMAKER (SANTU"
        }
    ],
    "id": "SP:856198ee0a750d4ae414960cbae0d7b7f256df6d",
    "references": [
        {
            "authors": [
                "Eleni Adamopoulou",
                "Lefteris Moussiades"
            ],
            "title": "Chatbots: History, technology, and applications",
            "venue": "Machine Learning with Applications",
            "year": 2020
        },
        {
            "authors": [
                "Jeff A. Bilmes",
                "Xiao Li",
                "Jonathan Malkin",
                "Kelley Kilanski",
                "Richard Wright",
                "Katrin Kirchhoff",
                "Amar Subramanya",
                "Susumu Harada",
                "James Landay",
                "Patricia Dowden",
                "Howard Chizeck"
            ],
            "title": "The Vocal Joystick: A Voice-Based Human-Computer Interface for Individuals with Motor Impairments",
            "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
            "year": 2005
        },
        {
            "authors": [
                "Petter Bae Brandtz\u00e6g",
                "Asbj\u00f8rn F\u00f8lstad"
            ],
            "title": "Why People Use Chatbots. In Internet Science - 4th International Conference, INSCI 2017",
            "venue": "Thessaloniki, Greece,",
            "year": 2017
        },
        {
            "authors": [
                "Peter F Brown",
                "John Cocke",
                "Stephen A Della Pietra",
                "Vincent J Della Pietra",
                "Frederick Jelinek",
                "Robert L Mercer",
                "Paul Roossin"
            ],
            "title": "A statistical approach to language translation",
            "venue": "In Coling Budapest",
            "year": 1988
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh",
                "Daniel M. Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "CoRR abs/2005.14165 (2020)",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Urvashi Khandelwal",
                "Christopher D. Manning",
                "Quoc V. Le"
            ],
            "title": "BAM! Born-Again Multi-Task Networks for Natural Language Understanding",
            "venue": "In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Matthias De Lange",
                "Rahaf Aljundi",
                "Marc Masana",
                "Sarah Parisot",
                "Xu Jia",
                "Ale\u0161 Leonardis",
                "Gregory Slabaugh",
                "Tinne Tuytelaars"
            ],
            "title": "A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence",
            "year": 2021
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Dana Movshovitz-Attias",
                "Jeongwoo Ko",
                "Alan Cowen",
                "Gaurav Nemade",
                "Sujith Ravi"
            ],
            "title": "GoEmotions: A dataset of fine-grained emotions",
            "year": 2020
        },
        {
            "authors": [
                "Leon Derczynski",
                "Eric Nichols",
                "Marieke van Erp",
                "Nut Limsopatham"
            ],
            "title": "Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition",
            "venue": "In Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Processing Natural Language on Embedded Devices",
            "year": 2019
        },
        {
            "authors": [
                "Raia Hadsell",
                "Dushyant Rao",
                "Andrei A Rusu",
                "Razvan Pascanu"
            ],
            "title": "Embracing change: Continual learning in deep neural networks",
            "venue": "Trends in cognitive sciences 24,",
            "year": 2020
        },
        {
            "authors": [
                "Brandi House",
                "JonathanMalkin",
                "JeffA. Bilmes"
            ],
            "title": "The VoiceBot: a voice controlled robot arm",
            "venue": "In Proceedings of the 27th International Conference on Human Factors in Computing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Forrest N. Iandola",
                "Albert E. Shaw",
                "Ravi Krishna",
                "Kurt Keutzer"
            ],
            "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks",
            "venue": "In Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, SustaiNLP@EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu"
            ],
            "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
            "venue": "Online Event,",
            "year": 2020
        },
        {
            "authors": [
                "Biing-Hwang Juang",
                "Lawrence R Rabiner"
            ],
            "title": "Automatic speech recognition\u2013a brief history of the technology development",
            "venue": "Georgia Institute of Technology. Atlanta Rutgers University and the University of California. Santa Barbara",
            "year": 2005
        },
        {
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaodong Liu",
                "YuWang",
                "Jianshu Ji",
                "Hao Cheng",
                "Xueyun Zhu",
                "Emmanuel Awa",
                "Pengcheng He",
                "Weizhu Chen",
                "Hoifung Poon",
                "Guihong Cao",
                "Jianfeng Gao"
            ],
            "title": "The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "year": 2019
        },
        {
            "authors": [
                "Edward Loper",
                "Steven Bird"
            ],
            "title": "Nltk: The natural language toolkit",
            "venue": "arXiv preprint cs/0205028",
            "year": 2002
        },
        {
            "authors": [
                "Xiaoling Lv",
                "Minglu Zhang",
                "Hui Li"
            ],
            "title": "Robot control based on voice command",
            "venue": "IEEE International Conference on Automation and Logistics",
            "year": 2008
        },
        {
            "authors": [
                "Christopher D Manning",
                "Mihai Surdeanu",
                "John Bauer",
                "Jenny Rose Finkel",
                "Steven Bethard",
                "David McClosky"
            ],
            "title": "The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations",
            "year": 2014
        },
        {
            "authors": [
                "Rajesh Kannan Megalingam",
                "Avinash Hegde Kota",
                "Vijaya Krishna Tejaswi P"
            ],
            "title": "Optimal Approach to Speech Recognition with ROS",
            "venue": "6th International Conference on Communication and Electronics Systems (ICCES). 111\u2013116",
            "year": 2021
        },
        {
            "authors": [
                "Rajesh Kannan Megalingam",
                "Racharla Shriya Reddy",
                "Yannam Jahnavi",
                "Manaswini Motheram"
            ],
            "title": "ROS Based Control of Robot Using Voice Recognition",
            "venue": "Third International Conference on Inventive Systems and Control (ICISC)",
            "year": 2019
        },
        {
            "authors": [
                "Paul Michel",
                "Omer Levy",
                "Graham Neubig"
            ],
            "title": "Are Sixteen Heads Really Better than One",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Anthony G Oettinger"
            ],
            "title": "Automatic language translation",
            "year": 2013
        },
        {
            "authors": [
                "Morgan Quigley",
                "Ken Conley",
                "Brian P. Gerkey",
                "Josh Faust",
                "Tully Foote",
                "Jeremy Leibs",
                "Rob Wheeler",
                "Andrew Y. Ng"
            ],
            "title": "ROS: an open-source Robot Operating System",
            "venue": "ICRA Workshop on Open Source Software. Accessed:May",
            "year": 2009
        },
        {
            "authors": [
                "Erik F Sang",
                "Fien De Meulder"
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "year": 2003
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf"
            ],
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR abs/1910.01108 (2019)",
            "year": 2019
        },
        {
            "authors": [
                "Benedikt Schmidt",
                "Reuben Borrison",
                "Andrew Cohen",
                "Marcel Dix",
                "Marco G\u00e4rtler",
                "Martin Hollender",
                "Benjamin Kl\u00f6pper",
                "Sylvia Maczey",
                "Shunmuga Siddharthan"
            ],
            "title": "Industrial virtual assistants: Challenges and opportunities",
            "venue": "In Proceedings of the 2018 ACM International ,",
            "year": 2018
        },
        {
            "authors": [
                "Sudeep Sharan",
                "Trung Quoc Nguyen",
                "Peter Nauth",
                "Rui Araujo"
            ],
            "title": "Implementation and testing of voice control in a mobile robot for navigation",
            "venue": "IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)",
            "year": 2019
        },
        {
            "authors": [
                "Heung-Yeung Shum",
                "Xiao-dong He",
                "Di Li"
            ],
            "title": "From Eliza to XiaoIce: challenges and opportunities with social chatbots",
            "venue": "Frontiers of Information Technology & Electronic Engineering",
            "year": 2018
        },
        {
            "authors": [
                "Siqi Sun",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu"
            ],
            "title": "Patient Knowledge Distillation for BERT Model Compression",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Hongkun Yu",
                "Xiaodan Song",
                "Renjie Liu",
                "Yiming Yang",
                "Denny Zhou"
            ],
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Thierry Tambe",
                "ColemanHooper",
                "Lillian Pentecost",
                "Tianyu Jia",
                "En-Yu Yang",
                "MarcoDonato",
                "Victor Sanh",
                "Paul N.Whatmough",
                "AlexanderM. Rush",
                "David Brooks",
                "Gu-Yeon Wei"
            ],
            "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference",
            "venue": "In MICRO \u201921: 54th Annual IEEE/ACM International Symposium on Microarchitecture, Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Raphael Tang",
                "Yao Lu",
                "Linqing Liu",
                "Lili Mou",
                "Olga Vechtomova",
                "Jimmy Lin"
            ],
            "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks",
            "venue": "CoRR abs/1903.12136 (2019)",
            "year": 2019
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du",
                "YaGuang Li",
                "Hongrae Lee",
                "Huaixiu Steven Zheng",
                "Amin Ghafouri",
                "Marcelo Menegali",
                "Yanping Huang",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "James Qin",
                "Dehao Chen",
                "Yuanzhong Xu",
                "Zhifeng Chen",
                "Adam Roberts",
                "Maarten Bosma",
                "Yanqi Zhou",
                "Chung-Ching Chang",
                "Igor Krivokon",
                "Will Rusch",
                "Marc Pickett",
                "Kathleen S. Meier-Hellstern",
                "Meredith Ringel Morris",
                "Tulsee Doshi",
                "Renelito Delos Santos",
                "Toju Duke",
                "Johnny Soraker",
                "Ben Zevenbergen",
                "Vinodkumar Prabhakaran",
                "Mark Diaz",
                "Ben Hutchinson",
                "Kristen Olson",
                "Alejandra Molina",
                "Erin Hoffman-John",
                "Josh Lee",
                "Lora Aroyo",
                "Ravi Rajakumar",
                "Alena Butryna",
                "Matthew Lamm",
                "Viktoriya Kuzmina",
                "Joe Fenton",
                "Aaron Cohen",
                "Rachel Bernstein",
                "Ray Kurzweil",
                "Blaise Aguera-Arcas",
                "Claire Cui",
                "Marian Croak",
                "Ed H. Chi",
                "Quoc Le"
            ],
            "title": "LaMDA: Language Models for Dialog Applications",
            "year": 2022
        },
        {
            "authors": [
                "Henry Tsai",
                "Jason Riesa",
                "Melvin Johnson",
                "Naveen Arivazhagan",
                "Xin Li",
                "Amelia Archer"
            ],
            "title": "Small and Practical BERT Models for Sequence Labeling",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Iulia Turc",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation",
            "venue": "CoRR abs/1908.08962 (2019)",
            "year": 2019
        },
        {
            "authors": [
                "Andrea Vanzo",
                "Danilo Croce",
                "Emanuele Bastianelli",
                "Roberto Basili",
                "Daniele Nardi"
            ],
            "title": "Grounded language interpretation of robotic commands through structured learning",
            "venue": "Artificial Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "Anjali Verma",
                "Deepak Kumar",
                "Hariom Maurya",
                "Anuj Kumar",
                "Mr Prabhakant Dwivedi"
            ],
            "title": "Voice Control Robot Using Arduino",
            "venue": "International Research Journal of Modernization in Engineering Technology and Science",
            "year": 2020
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat"
            ],
            "title": "Q8BERT: Quantized 8Bit BERT",
            "venue": "In Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition,",
            "year": 2019
        },
        {
            "authors": [
                "Yichen Zhang",
                "Zhiguo Lu",
                "Changhui Wang",
                "Chong Liu",
                "Yunong Wang"
            ],
            "title": "Voice control dual arm robot based on ROS system",
            "venue": "IEEE International Conference on Intelligence and Safety for Robotics (ISR),",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "Processing Natural Language on Embedded Devices: HowWell Do Modern Models Perform?",
            "text": "SOUVIKA SARKAR\u2217, Auburn University, USA MOHAMMAD FAKHRUDDIN BABAR,Washington State University, USA MDMAHADI HASSAN\u2217, Auburn University, USA MONOWAR HASAN,Washington State University, USA SHUBHRA KANTI KARMAKER (SANTU)\u2217, Auburn University, USA\nVoice-controlled systems are becoming ubiquitous in many IoT-specific applications such as home/industrial automation, automotive infotainment, and healthcare. While cloud-based voice services (e.g., Alexa, Siri) can leverage high-performance computing servers, some use cases (e.g., robotics, automotive infotainment) may require to execute the natural language processing (NLP) tasks offline, often on resource-constrained embedded devices. Large language models such as BERT and its variants are primarily developed with compute-heavy servers in mind. Despite the great performance of BERT models across various NLP tasks, their large size and numerous parameters pose substantial obstacles to offline computation on embedded systems. Lighter replacement of such language models (e.g., DistilBERT and TinyBERT) often sacrifice accuracy, particularly for complex NLP tasks. Until now, it is still unclear (a) whether the state-of-the-art language models, viz., BERT and its variants are deployable on embedded systems with a limited processor, memory, and battery power and (b) if they do, what are the \u201cright\u201d set of configurations and parameters to choose for a given NLP task. This paper presents an exploratory study of modern language models under different resource constraints and accuracy budgets to derive empirical observations about these resource/accuracy trade-offs. In particular, we study how the four most commonly used BERT-based language models (e.g., BERT, RoBERTa, DistilBERT, and TinyBERT) perform on embedded systems. We tested them on a Raspberry Pi-based robotic platform with three hardware configurations and four datasets running various NLP tasks. Our findings can help designers to understand the deployability and performance of modern language models, especially those based on BERT architectures, thus saving a lot of time wasted in trial-and-error efforts.\nCCS Concepts: \u2022 Computer systems organization \u2192 Embedded systems; Robotics; \u2022 Human-centered computing \u2192 Interaction devices.\nAdditional Key Words and Phrases: BERT, Transformers, Neural Networks, Gaze Detection, Text Tagging"
        },
        {
            "heading": "ACM Reference Format:",
            "text": "Souvika Sarkar, Mohammad Fakhruddin Babar, Md Mahadi Hassan, Monowar Hasan, and Shubhra Kanti Karmaker (Santu). 2023. Processing Natural Language on Embedded Devices: How Well Do Modern Models Perform?. 1, 1 (September 2023), 18 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\nAuthors\u2019 addresses: Souvika Sarkar, szs0239@auburn.edu, Auburn University, , , , USA, ; Mohammad Fakhruddin Babar, Washington State University, , Pullman, USA, m.babar@wsu.edu; Md Mahadi Hassan, mzh0167@auburn.edu, Auburn University, , , , USA, ; Monowar Hasan, Washington State University, , Pullman, USA, monowar.hasan@wsu.edu; Shubhra Kanti Karmaker (Santu), sks0086@auburn.edu, Auburn University, , , , USA, .\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Association for Computing Machinery. XXXX-XXXX/2023/9-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nar X\niv :2\n30 4.\n11 52\n0v 3\n[ cs\n.C L\n] 1\n2 Se\np 20\n23"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The natural language processing (NLP) domain and the emergence of large languagemodels rapidly transform how we interact with technology. With the proliferation of IoT-specific applications, the demand for voice-controlled services that can perform tasks by responding to spoken commands is overgrowing. Use cases of NLP applications, especially those that need embedded and mobile systems, include home automation, healthcare, industrial control, and automotive infotainment. To design a dialogue-based interaction system for a target device, we need models that are feasible to (a) run on the hardware and (b)meet the desired level of accuracy. Although some services such as digital assistants (e.g., Alexa, Siri, Cortana) may leverage cloud resources for processing human voices, there exist applications (e.g., offline home/industrial robots, automotive infotainment, battlefield/military equipment) that may not have network connectivity, thus require to synthesize NLP tasks on the embedded device itself. The challenge is to understand the feasibility of running a large language model on resource-limited devices. Transformer-based architectures [46], especially BERT-based models [11], have established themselves as popular state-of-the-art baselines for many NLP tasks, including Intent Classification (IC), Sentiment Classification (SC), and Named Entity Recognition (NER). However, a well-known criticism of BERT-based architectures is that they are data-hungry and consume a lot of memory and energy; therefore, deploying them in embedded systems is challenging. In fact, due to their excessive size (431 MB) and parameters (110 M), deploying a pre-trained BERT model (called \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 ) in resource-constrained embedded devices is often impractical, especially at the production level with certain minimum accuracy/performance requirements. Lighter versions of BERT (e.g., DistilBERT [33] and TinyBERT [16]) often result in accuracy losses. The degree of degradation in performance depends on the difficulty of the task, especially since those models often cannot perform well on complex NLP tasks, including emerging entity [9] or mixed emotion detection [8]. Therefore, designers must make an inevitable trade-off between an accurate model and one that can run smoothly in a resource-constrained environment. Unfortunately, developers often have little idea about this trade-off and have to spend a lot of time conducting trial-and-error experiments to find a suitable architecture that is feasible for the target (resource-constrained) hardware and meets a desired level of accuracy.\nFrom a developer\u2019s perspective, it is still unclear what is the \u201cright\u201d BERT-based architecture to use for a given NLP task that can strike a suitable trade-off between the resources available and the minimum accuracy desired by the user. Due to the staggering size of the \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 model, we experiment with different \u201cdistilled\u201d BERT models (e.g., DistilBERT and TinyBERT) for IC, SC, and NER tasks. However, existing ready-to-use distilled models perform poorly on some SC and NER datasets (Sec. 5). Hence there is a need to explore other models that can better optimize the efficiency/accuracy trade-offs. This research performs an exploratory study of BERT-based models1 under different resource constraints and accuracy budgets to derive empirical data about these resource/accuracy trade-offs. We aim to answer the following questions: (a) how can we determine the suitable BERT architecture that runs on a target hardware and meets user-defined performance requirements (accuracy, inference time)? (b) what are the trade-offs between accuracy and model size as we perform optimizations (such as pruning) to run them on limited embedded device memory? and (c)what are the implications of performing pruning on accuracy and corresponding resource usage, including memory, inference time, and energy consumption? In answer to those questions, we observe the overhead of running various BERT architectures from an off-the-shelf Raspberry Pi [29]-based six-degrees-of-freedom (6-DOF) robot platform. Our experiments suggest that some BERT models (in particular those are \u201cdistilled\u201d) failed to achieve desired performance goals (e.g., F1 score) for various NLP tasks. Further, although pruning can reduce model size, it does not significantly help in energy efficiency.\n1Note: there exist other large language models, such as GPT [5] from OpenAI and LaMDA [41] from Google. However, they are even more resource hungry than BERT (thus less suitable for embedded deployment), and some of them are close-sourced. Hence, our initial study limits on BERT-based architectures.\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nOur study fills the gap between simulated studies and real-world scenarios, as no prior work has deployed these models on embedded platforms. Our implementation and related documentation are publicly available [28]. The findings of this work can help designers to choose alternative BERT-based architectures under given resource constraints, thus saving development time and hassle. In summary, we made the following contributions in this paper.\n\u2022 Our study systematically investigates the performance of BERT-based language models on a real robotic platform, more precisely on a six-degrees-of-freedom (6-DOF) robot testbed. We analyzed the trade-offs between complexity and accuracy in resource-constrained environments across multiple NLP tasks. (Sec. 4- Sec. 5).\n\u2022 We explore the potential design options for various BERT-based architectures on various resource-limited platforms, viz., Raspberry Pi boards with 2 GB, 4 GB, and 8 GB of RAM configurations. We developed a comprehensive lookup table through empirical observations encompassing significant design parameters such as inference timings, memory usage, and energy consumption (Sec. 5).\nWe now start with the state-of-the-art techniques (Sec. 2). We then introduce the problem statement and present selected datasets (Sec. 3). Section 4 describes our experiment methodology before we discuss our findings in Sec. 5."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "We discuss related research along two dimensions: (a) BERT-based models and their efficient variants and (b) using NLP on embedded devices.\nBERT-based Models and their Variants. The performance of BERT comes at a high computation and memory cost, which makes on-device inference really challenging. To mitigate this issue, researchers have proposed knowledge distillation approaches from the original BERT model, for example, (a) \u201cfinetune\u201d the BERT model to improve task-specific knowledge distillation [42, 43], (b) use Bi-LSTM models [40] for knowledge distillation from BERT, (c) leverage single-task models to teach a multi-task model [6], (d) distillation of knowledge from an ensemble of BERT into a single BERT [19], (e) TinyBERT [16] uses a layer-wise distillation strategy for BERT in both the pre-training and fine-tuning stages, and (f) DistilBERT [33] halves the depth of the BERT model through knowledge distillation in the pre-training stage and an optional fine-tuning stage. On a different direction, the Patient Knowledge Distillation approach [37] compresses an original large model (\u201cteacher\u201d) into an equally-effective lightweight shallow network (\u201cstudent\u201d). Other BERT models (e.g., SqueezeBERT [15], MobileBERT [38], Q8BERT [50], ALBERT [18]) can also reduce resource consumption than the vanilla BERT. EdgeBERT [39], an algorithm-hardware co-design approach, performs latency-aware energy optimizations for multi-task NLP problems. However, unlike ours, EdgeBERT (a) does not apply attention heads pruning, and (b) does not report scores on downstream NLP tasks on real-world embedded systems.\nNLP for Embedded Platforms. Researchers have explored NLP techniques to facilitate natural communication between humans and embedded devices, especially in the context of voice-controlled cognitive robots. For example, Megalingam et al. [25] presents a voice recognition tool that compares the user\u2019s input commands with the stored data. Zhang et al. [51] propose a ROS-based robot that analyzes commands using an offline grammar recognition library. Megalingam et al. [24] propose a cost-efficient speech processing module running on ROS that can provide natural language responses to the user. There also exists ROS-integrated independent speech recognition packages [35, 52] as well as Arduino-based [47] and custom [22] voice-control robot platforms. House et al. [14] a voice-controlled robotic arm (named VoiceBot) for individuals with motor impairments [2]. However, most of these works focused on rule-based approaches, and we note that transformer architectures are still\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nunder-explored in terms of their practical deployment challenges in real-world embedded and robotic devices, which is the focus of this study."
        },
        {
            "heading": "2.1 Uniqueness of Our Work",
            "text": "While existing work can reduce the size of BERT models through distillation and pruning, from a system design perspective, it is still difficult and tedious for a developer to find out the \u201cright\u201d BERT-based architecture to use in an embedded platform. To date, it is also unclear which lighter version of BERT would strike the optimal trade-off between the resources available in an embedded device and the minimum accuracy desired. Our empirical evaluation and design space exploration (see Sec. 5) can help the system and machine learning engineers to pick suitable architectures depending on target system configuration and performance constraints (e.g., accuracy, \ud835\udc391 score). To the best of our knowledge, this work is one of the first efforts to study the feasibility of deploying BERT-based models in real-world resource-constrained embedded platforms.\n3 PROBLEM STATEMENT & DATASETS\nWe aim to study how language models can be optimally deployed to accomplish dialog processing in embedded devices. The core technical challenge of any dialog system is to accurately understand and interpret user \u201cutterances\u201d and perform the right \u201caction\u201d accordingly. At a fundamental level, user utterance understanding relies on the following three basic NLP tasks2: (a) Intent classification (IC) \u2014 to understand the need of the user, (b) Sentiment Classification (SC) \u2014 to understand user emotions, and (c) Named-entity Recognition (NER) \u2014 to extract related entities such as persons or objects.\n2Section 3.1 formally presents these three NLP tasks.\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nFigure 1 presents the workflow of the dialogue-based systems considered in this work. The user initiates the interaction by providing a spoken command to the voice-controlled device (marker 1\u25cb in Fig. 1). The device employs existing automatic speech recognition techniques [17, 49] to convert the user\u2019s speech into texts as most language models take textual input ( 2\u25cb). The system then runs an \u201cintent classifier\u201d (Sec. 3.1.1) and analyzes the extracted text to determine the user\u2019s intention ( 3\u25cb). The classifier identifies the relevant user intentions for the given command ( 4\u25cb). For instance, the intents for the given command \u201cCan you please go to my study room and turn off the lights?\u201d could be identified as \u201cMotion\u201d and \u201cChange operational state,\u201d as it instructed the device to move from its current position. Simultaneously, a \u201csentiment classifier\u201d(Sec. 3.1.2) is employed to extract the user sentiment ( 5\u25cb). In this case, the extracted sentiment is \u201cNeutral\u201d as the command does not express any emotion ( 6\u25cb). In addition, the dialog system utilizes a \u201cnamed entity recognizer\u201d (Sec. 3.1.3) to identify specific \u201centities\u201d ( 7\u25cb- 8\u25cb). For example, the entities, in this case, are \u201cstudy room\u201d (location) and \u201clights\u201d (object). Once the user\u2019s intention and relevant entities are identified, the control application running on the embedded device ( 9\u25cb carries out the specified task and sends a response back to the user (10\u25cb). In this paper, we focus on understanding how the language models perform on embedded platforms for IC, SC, and NER tasks (e.g., steps 3\u25cb- 8\u25cb)."
        },
        {
            "heading": "3.1 NLP Tasks under Consideration",
            "text": "Recall from our earlier discussion that intent/sentiment classification and named-entity recognition are fundamental NLP tasks for any voice-controlled interactive system, chatbots, and virtual assistants. We now formally introduce IC, SC, and NER tasks.\n3.1.1 Intent Classification. To produce an accurate response, reduce backtracking, and minimize user frustration, Intent Classification (IC) is needed to identify which subsequent action a robot needs to perform depending on the user\u2019s utterance. A formal definition of IC can be given as follows:\nDefinition 1. Given a collection of user utterances\ud835\udc48 = {\ud835\udc621, \ud835\udc622, ..., \ud835\udc62\ud835\udc5b}, and a set of intent labels \ud835\udc3c\ud835\udc65 = {\ud835\udc561, \ud835\udc562, ..., \ud835\udc56\ud835\udc5a}, classify each utterance \ud835\udc62 \ud835\udc57 \ud835\udf16 \ud835\udc48 with one to more intents labels from \ud835\udc3c\ud835\udc65 .\nImportantly, a user might have more than one intent while speaking to a robot and understanding the implicit or explicit intent expressed in a statement is essential to capturing the user\u2019s needs. For example, consider the following command: \u201cCan you please go to my study room and turn off the lights?\u201d The command wants the robot to turn off the lights in the study room; the relevant intent here is \u201cChange Operational State\u201d. However, the statement also expressed another intent related to \u201cMotion\u201d, as the command requires the robot to change location. Without identifying all the underlying intents, the system cannot perform the right next step. Hence, recognizing and understanding all types of intents stated in an utterance is crucial for accomplishing the eventual goal.\n3.1.2 Sentiment Classification. Sentiment analysis is regarded as an important task for accurate user modelling in natural dialog-based interactions, where user utterances are usually classified into multiple emotion/sentiment labels. A formal definition can be given as follows:\nDefinition 2. Given a collection of user utterances \ud835\udc48 = {\ud835\udc621, \ud835\udc622, ..., \ud835\udc62\ud835\udc5b}, and a set of sentiment labels \ud835\udc46\ud835\udc65 = {\ud835\udc601, \ud835\udc602, ..., \ud835\udc60\ud835\udc5a}, classify each expression \ud835\udc62 \ud835\udc57 \ud835\udf16 \ud835\udc48 with one to more sentiment labels from \ud835\udc46\ud835\udc65 .\nFor example, the following user utterance, \u201cOMG, yep!!! That is the final answer. Thank you so much!\u201d will be classified with sentiment labels \u201cgratitude\u201d and, \u201capproval\u201d. Similarly, statements such as \u201cThis caught me off guard for real. I\u2019m actually off my bed laughing\u201d will be labeled as \u201csurprise\u201d and \u201camusement.\u201d\n3.1.3 Named-entity Recognition. Named entity recognition (NER) \u2014 often referred to as entity chunking, extraction, or identification \u2014 is a sub-task of information extraction that seeks to locate and classify named entities mentioned in unstructured text. An entity can be expressed by a single word or a series of words that\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nconsistently refer to the same thing. Each detected entity is further classified into a predetermined category. The formal definition of the NER task can be given as follows:\nDefinition 3. Given a collection of statements/texts \ud835\udc46 = {\ud835\udc601, \ud835\udc602, ..., \ud835\udc60\ud835\udc5b}, and a set of entity labels \ud835\udc38\ud835\udc65 = {\ud835\udc521, \ud835\udc522, ..., \ud835\udc52\ud835\udc5a}, all the words/tokens in the text will be classified with an entity label \ud835\udc52\ud835\udc56 \ud835\udf16 \ud835\udc38\ud835\udc65 .\nNER can be framed as a sequence labelling task that is performed in two steps, first, detecting the entities from the text, and second, classifying them into different categories. A named entity recognizer model classifies each word/phrase representing an entity into one of the four types: (a) persons (PER), (b) objects (OBJ), (c) locations (LOC), and (d) miscellaneous names (MISC)."
        },
        {
            "heading": "3.2 Datasets",
            "text": "Our study includes the following datasets3: (a) HuRIC (for IC), (b) GoEmotion (for SC), and (c) CoNLL and WNUT17 (for NER), as we present below.\n3.2.1 Intent Classification: HuRIC. For IC, we use Human Robot Interaction Corpus (HuRIC) [45], which is the state-of-the-art single-class classification dataset. The basic idea of HuRIC is to build a reusable corpus for human-robot interaction in natural language for a specific application domain, i.e., house service robots. HuRIC includes a wide range of user utterances given to a robot representing different situations in a house environment. Table 1 presents some statistics of HuRIC.\n3.2.2 Sentiment Classification: GoEmotion. We use GoEmotion [8] dataset from Google AI for the SC task. GoEmotion is a human-annotated dataset of 58,000 Reddit comments extracted from popular English-language subreddits and labeled with 27 emotion categories. As the largest fully annotated English language fine-grained emotion dataset to date, the GoEmotion taxonomy was designed with both psychology and data applicability in mind Table 2 presents some statistics of GoEmotion.\n3Appendix A presents additional details about these datasets.\n, Vol. 1, No. 1, Article . Publication date: September 2023.\n3.2.3 Named-entity Recognition: CoNLL & WNUT17. For NER we consider two datasets, viz., CoNLL [32] and WNUT17 [9].\nCoNLL. CoNLL-2003 [32] was released as a part of CoNLL-2003 shared task: language-independent named entity recognition. The English corpus from this shared task consists of Reuters news stories between August 1996 and August 1997, each annotated with the entities associated with them. The data set consists of a training file, a development file, and a test file. The details of CoNLL-2003 are presented in Table 3.\nWNUT17. While the CoNLL corpus is based on news stories, we were looking for a dataset that contains user utterances such as those available on HuRIC. Unfortunately, we could not find such a NER dataset but discovered a very similar corpus that contains user-generated text, named WNUT2017 [9]. The dataset\u2019s shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Identifying entities in noisy text is really challenging, even for human annotators, due to novel entities and surface forms. In this dataset, user comments were mined from different social media platforms because they are large, and samples can be mined along different dimensions, such as texts from/about geo-specific areas, about home aid, and particular topics and events. Table 4 summarizes the dataset properties."
        },
        {
            "heading": "4 EXPERIMENTAL SETUP",
            "text": "We now summarize BERT architectures and configurations used in our experiments (Sec. 4.1 and Sec. 4.2). For our experiments, we choose the robot testbed for demonstration purposes (Sec. 4.3). However, our implementation is modular and can be ported into other platforms such as smartphones and IoT devices. To measure the performance of each task (IC, SC, NER), we use three popular metrics (i.e., Precision, Recall, and \ud835\udc391 score). Section 4.4 lists the design questions explored in our investigation. Our implementation is available on a public repository [28].\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "4.1 Off-the-Shelf BERT Variants",
            "text": "We use a pre-trained base variant of BERT [10], RoBERTa [20], DistilBERT [33], TinyBERT [16] model from Huggingface4 and finetune the models on respective datasets (i.e., HuRIC, GoEmotion, CoNLL, and WNUT17). Table 5 presents the hyper-parameter used in our experiments."
        },
        {
            "heading": "4.2 Pruning and Custom Configurations",
            "text": "We also experiment with custom, smaller BERT configurations. Due to the resource constraints (e.g., memory and energy limits) of embedded devices, it is necessary to explore different variants of BERT-based models that can be optimized to run on the device. We can reduce the model size on two fronts: (a) by reducing the layer size and (b) by pruning various attributes. In our study, we experiment with different layer combinations of BERT models and test their performance on different hardware configurations, which are presented in Table 9, and 10. With two layers of \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 (instead of 12), the model size reduces significantly, but so does the accuracy (in terms of \ud835\udc391 score). Still, these models give better accuracy than the distilled models on complex NLP tasks. Also, where \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 model with 12 layers cannot run on a resource-constrained device, using a lesser number of layers enable a model to execute on tiny devices with good accuracy compared to the distilled methods (DistilBERT and TinyBERT).\nFor further shrinking of the model, pruning can be applied to weights, neurons, layers, channels, and attention heads, depending on the heuristic used. In this paper, we focus on pruning attention heads, which plays an important role in the model\u2019s performance and contributes a large number of parameters. Although multi-headed attention is a driving force behind many recent state-of-the-art models, Michel et al. [26] finds that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed without significantly impacting performance. In fact, in some layers, attention heads can even be reduced to a single head. Based on this fact, we experiment with reducing the size of \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 by dynamically pruning the attention heads. Each attention head provides a distribution of attention for the given input sequence. For each attention head, we calculate the head importance by calculating the entropy of each head. After that, we mask all the attention heads with the lowest entropy. This process is repeated for each layer of the encoder. After masking the heads, we calculate the overall \ud835\udc391 score of the masked model and determine the drop in \ud835\udc391 score compared to the original unpruned model. If the drop is less than a predefined threshold, we prune the masked attention heads. We repeat the process until the drop in \ud835\udc391 score reaches the predefined threshold. This pruning procedure reduces the model size significantly while maintaining the desired model performance.\n4https://huggingface.co/.\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "4.3 Testbed & Hardware Configurations",
            "text": "We evaluate the BERT models on a six-degrees-of-freedom (6-DOF) robot testbed. We used Raspberry Pi 4 Model B as the main computing unit of the robot. We attached the Adafruit servo extension board [13] to control the six servos used for robot movement. For voice processing, we used ReSpeaker microphone [31] and leveraged the existing python library (i.e., speech recognition) for voice-to-text conversion. For energy measurements during the inference steps, we used UM25C energy meter [44]. Figure 2 depicts our experimental setup. For better design-space exploration with various hardware resources, we used four variants viz., Raspberry Pi boards with 2 GB, 4 GB, and 8 GB of RAM, respectively. We used Robot Operating System (ROS) [30] (version 2) for controlling the robot and processing the input data (see Appendix B for details)."
        },
        {
            "heading": "4.4 Design Challenges & ResearchQuestions",
            "text": "We conducted extensive experiments to investigate the following research questions (RQs).\n\u2022 RQ 1. Given specific user-defined constraints, such as system resources (processor, memory) and performance budgets (accuracy, inference time), what is the optimal (if any) BERT-based architecture satisfying those constraints? \u2022 RQ 2. What is the accuracy vs. model-size trade-off as we prune the models? \u2022 RQ 3. What are the trade-offs of accuracy and corresponding resource usage (e.g., memory, inference-time, energy consumptions) as we perform pruning?"
        },
        {
            "heading": "5 RESULTS",
            "text": "In this section, we report our results for the three basic NLP tasks, i.e., IC, SC, and NER for both existing (e.g., BERT, RoBERTa, DistilBERT, and TinyBERT) and custom BERT architectures."
        },
        {
            "heading": "Intent Classification Task (Dataset: HuRIC)",
            "text": ", Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "Multi-label Sentiment Detection Task (Dataset: GoEmotion)",
            "text": ""
        },
        {
            "heading": "Named-entity Recognition Task Dataset: CoNLL",
            "text": ""
        },
        {
            "heading": "Dataset: WNUT17",
            "text": ""
        },
        {
            "heading": "5.1 Experience with Existing BERT Variants",
            "text": "Intent Classification (IC). Recall from our earlier discussion (Sec. 3.2.1) that we used the HuRIC dataset for IC. Table 6 presents our findings after running IC tasks using the HuRIC dataset on our robot testbed. We observed that all the models performed similarly on this dataset, achieving more than 90% \ud835\udc391 Score.\nMulti-label Sentiment Classification (SC). We next analyze the performance of multi-label SC tasks on our robot platform. As mentioned in Sec. 3.2.2, we used GoEmotion [8] dataset for this task. GoEmotion includes direct user-to-user conversation text and labels them with many user emotions. Table 7 summarizes the performance of all the models for this task. Interestingly, for this task, DistilBERT and TinyBERT failed drastically, as they achieved a very low \ud835\udc391 Score. The failure of distilled models can be attributed to the difficulty of the task. Multi-label SC requires each utterance to be classified with more than one sentiment. Therefore, this dataset is not a straightforward positive-negative sentiment detection.\nNamed-entity Recognition (NER). As we mention in Sec. 3.2.3, we use two different datasets to test the NER task. Table 8 summarizes the performance over both the NER datasets and shows that for the CoNLL dataset. In this setup, all models performed comparatively the same. However, the performance of distilled models dropped sharply for the WNUT17 dataset (which focuses on identifying unusual, previously-unseen entities). This drop tends to be due to the difficulty of analyzing this task, as NER evaluates the ability to detect and classify novel, emerging, singleton-named entities in noisy inputs. In summary, our findings are as follows.\n\u2022 All models achieved impressive F1 scores (>90%) for IC task. \u2022 DistilBERT and TinyBERT struggled with the multi-label SC task as none achieved an F1 score of more than 15%. \u2022 All BERT models excelled in the NER task on the CoNLL dataset and accurately recognized named entities (resulting in >90% F1 scores). \u2022 Distilled models showed a performance decline for the NER task on the WNUT17 dataset with the F1 score dropping to less than 5%, indicating difficulty with dataset intricacies.\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "5.2 Exploration with Custom Architectures",
            "text": "Based on our experiment results (Tables 6\u20138), we further explore different alternative BERT-based architectures by reducing the layers and pruning the attention heads from the original \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 model. For this exploration, we primarily focus on the challenging tasks, i.e., multi-label SC and emerging NER where off-the-shelf models (e.g., DistilBERT and TinyBERT) failed to perform.\nTables 9\u201310 presents our exploration findings.5 We discuss our observations in Sec. 5.2.1 and provide answers to the research questions posed in Sec. 4.4. Before we proceed with the discussion, we present a brief overview of the attributes reported in Tables 9\u201310. \u2022 \ud835\udc391 Threshold (\ud835\udf03 ): The \ud835\udf03 -cells represents what percentage of the \ud835\udc391 score (with respect to \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 ) is retained by the models. In our experiment, we varied \ud835\udf03 between 50% to 90% and reported the model details in respective columns. For example, \ud835\udf03 set to 80% implies the \ud835\udf0380 column. \u2022 Platform: Indicates the memory capacity of the Raspberry Pi board attached to the robot. \u2022 Layer : Represents the number of layers retained. \u2022 Model Size: The size of the saved model after training. \u2022 Parameters: This metric indicates the total number of parameters in the saved model. \u2022 Pruning: Pruning percentage represents the reduction in size from the \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 model. For example, a pruning percentage of 70% implies that the pruned model is 70% smaller than \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 . \u2022 Energy Consumption: The average energy consumed (in watts) by the robot controller (Raspberry Pi) during the inference of a given command. \u2022 Memory Consumption: Maximum memory usage (in megabytes) of the corresponding NLP task running on the robot during the inference time. \u2022 Inference Time: Depending on the specific task, the average time required for the model to infer the appropriate Intent, Sentiment or Entity from a given command.\n5.2.1 Observations. We now discuss our major observations and address the research questions introduced in Sec. 4.4.\nSelecting \u201csuitable\u201d model subject to given constraints [RQ 1]. We can address this specific research question by inspecting Table 9 and 10. Note that Table 9 and 10 provide information on themodel size, performance, parameters, and pruning for the SC and NER tasks, respectively. Let us assume a system designer is looking for suitable NER models for a 2 GB embedded platform that maintains approximately 70% of BERT\u2019s accuracy (\ud835\udf0370). In this case, we can (a) scan through the NER performance metrics (i.e., Table 10), and (b) observe from 2 GB Platform row and \ud835\udf0370 column that a six-layered and pruned (45% reduced) BERT model can run on a 2 GB platform and attain 70% of \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 \u2019s original \ud835\udc391 score. Hence, our exploration (and similar experiments along this line) can aid the designers to select appropriate models with desired performance guarantees.\nAccuracy and model-size trade-offs for pruned architectures [RQ 2]. Table 9 and Table 10 further provide insights on the pruning vs. \ud835\udc391 score trade-off. For example, in Table 9, the 2 GB Platform row shows a set of models that can run on that system. The same row and \ud835\udf0380 (80% \ud835\udc391 score threshold) column show that even pruning 55% of a \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 model with four layers can retain 80% of original \ud835\udc391 score of \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 , while the model size can be reduced to 198.7 MB from 441.55 MB. Tables 9 and 10 indicate that although pruning has only a minor impact on memory consumption, it does not have a significant effect on energy consumption. Furthermore, our analysis of Table 9 suggests that inference time is directly proportional to the size of the model, implying that decreasing the size of the model leads to a corresponding decrease in inference time.\n5Note: We omit the results for IC on custom BERT architectures as existing models suffice to perform this task.\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "Size (MB), Params= Parameters (Million), EC=Energy Consumption (W), MC= Memory (MB), and IT= Inference Time (s).",
            "text": ""
        },
        {
            "heading": "Size (MB), Params= Parameters (Million), EC=Energy Consumption (Watt), MC= Memory (MB), and IT= Inference Time (s).",
            "text": "Accuracy vs. system resource trade-offs for pruned architectures [RQ 3]. If a user has precise requirements for inference time and memory consumption, one can scan Table 9 and 10 to pick the optimum model that meets those requirements. For instance, if we want to find NERmodels that can make inferences in less than 0.35 seconds\n, Vol. 1, No. 1, Article . Publication date: September 2023.\non an embedded platform that has 4 GB of memory, the corresponding Platform row (4 GB) in Table 10, shows us the model parameters that can satisfy this requirement(e.g., two-layered, four-layered). Since both of them are feasible for the chosen platform, designers can choose any of them based on the required application performance. As an example, if we pick a two-layered BERT model, the accuracy is 60%, and the memory consumption is 711.9 MB. In contrast, if we select the six-layered BERT model, it can achieve 90% accuracy, with a cost of higher memory consumption of 735.8 MB. Hence, at the expense of 3.36% higher memory consumption, it is possible to get 20% more accuracy. Such a lookup-based approach allows the designers to perform a desired cost-benefit analysis.\nSince many embedded platforms used for NLP tasks (e.g., voice-controlled robots, voice assistants, IoT devices) are battery-operated, energy consumption for inferring user commands is a crucial parameter. Hence we also analyze the energy usage of the NLP tasks. For any selected BERT model, one can also find the system energy consumption from Table 9 and 10, for two different task respectively. From the design space exploration table, it is evident that during the inference of a given command, energy consumption does not vary significantly over various models.\nOur key findings for custom BERT architectures are listed below.\n\u2022 Pruning helps in reduction in size (upto 67%) while maintaining at least 50% of \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 \u2019s F1 score. \u2022 The time required for inference is directly related to the size of the model, i.e., a smaller model size results in a reduction in inference time. \u2022 Pruning of attention heads has only a minor impact on memory consumption, i.e., reduction in size does not reduce memory consumption. \u2022 Pruning of attention heads has little effect on energy consumption, i.e., it does not improve energy usage significantly."
        },
        {
            "heading": "6 DISCUSSION & LIMITATIONS",
            "text": "In this research, we explore different custom architectures of BERT-based language models from the practical viewpoint of deploying them in a resource-constrained embedded system. We show that it is not always feasible to shrink the size of \u201cfinetuned\u201d \ud835\udc35\ud835\udc38\ud835\udc45\ud835\udc47\ud835\udc35\ud835\udc4e\ud835\udc60\ud835\udc52 model that can satisfy specific user-defined accuracy/performance budgets. We also report which models are deployable to resource-constrained devices with given user requirements. We believe our empirical findings will help the developers quickly narrow down the plausible BERT-based architecture for target applications, thus saving development time and effort. While we tested the NLP models on robotic platforms, they can be ported to other embedded systems, such as smartphones and mobile/IoT devices. Thus the empirical study is applicable in broader human-centric application domains, including chatbots [1, 3, 36], virtual assistants [34, 48], and language translation [4, 27]. Our study is limited to BERT-based models for four existing datasets (i.e., may not generalize to other language models and datasets). However, our evaluation framework is modular and can be retrofitted to other architectures/datasets without loss of generality. While shrinking models have made it possible to deploy them on resource-constrained embedded devices, their performance on new datasets or tasks is often limited. One potential solution to mitigate this issue is to utilize continual learning techniques [7, 12], as they allow models to continuously learn and evolve based on increasing data input while retaining previously acquired knowledge. Our future work will explore the feasibility of employing continual learning for embedded devices. One of the challenges to figuring out the optimal BERT-based architecture is the lack of application-specific (viz., voice-controlled robots for home automation) datasets. Existing datasets either (a) do not have enough examples for training deep learning models or (b) do not provide complex, practical queries to test the robustness of a given model. Building suitable datasets for IoT-specific human-centric applications such as voice-control home/industrial automation is an interesting open research problem.\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we present an explorative study of BERT-based neural architectures in terms of the feasibility of deploying them on resource-constrained systems, which have become ubiquitous nowadays. Our study will allow the designers to select the \u201cright\u201d architecture and parameters depending on the resource constraints and performance requirements. This will also save a lot of time on the developer\u2019s end as they will be able to make informed choices regarding which BERT-based architecture to use during development based on their NLP application scenario and the availability of resources at hand. Our approach is general and can be adapted to multiple ubiquitous computing domains, such as voice-controlled home and industrial automation, precision agriculture, and medical robots."
        },
        {
            "heading": "ACKNOWLEDGMENTS REFERENCES",
            "text": "[1] Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technology, and applications. Machine Learning with Applications\n2 (2020), 100006. [2] Jeff A. Bilmes, Xiao Li, Jonathan Malkin, Kelley Kilanski, Richard Wright, Katrin Kirchhoff, Amar Subramanya, Susumu Harada, James\nLanday, Patricia Dowden, and Howard Chizeck. 2005. The Vocal Joystick: A Voice-Based Human-Computer Interface for Individuals with Motor Impairments. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Vancouver, British Columbia, Canada, 995\u20131002. https://aclanthology.org/H051125 [3] Petter Bae Brandtz\u00e6g and Asbj\u00f8rn F\u00f8lstad. 2017. Why People Use Chatbots. In Internet Science - 4th International Conference, INSCI 2017, Thessaloniki, Greece, November 22-24, 2017, Proceedings (Lecture Notes in Computer Science, Vol. 10673), Ioannis Kompatsiaris, Jonathan Cave, Anna Satsiou, Georg Carle, Antonella Passani, Efstratios Kontopoulos, Sotiris Diplaris, and Donald McMillan (Eds.). Springer, 377\u2013392. https://doi.org/10.1007/978-3-319-70284-1_30 [4] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, Robert L Mercer, and Paul Roossin. 1988. A statistical approach to language translation. In Coling Budapest 1988 Volume 1: International Conference on Computational Linguistics. John von Neumann Society for Computing Sciences, Budapest. [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. CoRR abs/2005.14165 (2020). arXiv:2005.14165 https://arxiv.org/abs/2005.14165 [6] Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. 2019. BAM! Born-Again Multi-Task Networks for Natural Language Understanding. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics, 5931\u20135937. https://doi.org/10.18653/v1/p19-1595 [7] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence 44, 7 (2021), 3366\u20133385. [8] Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. 2020. GoEmotions: A dataset of fine-grained emotions. arXiv preprint arXiv:2005.00547 (2020). [9] Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. 2017. Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin (Eds.). Association for Computational Linguistics, 140\u2013147. https://doi.org/10.18653/v1/w17-4418 [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018). arXiv:1810.04805 http://arxiv.org/abs/1810.04805 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 4171\u20134186. https://doi.org/10.18653/ v1/n19-1423\n, Vol. 1, No. 1, Article . Publication date: September 2023.\n[12] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. 2020. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences 24, 12 (2020), 1028\u20131040. [13] Pi Servo HAT. [n. d.]. https://www.adafruit.com/product/2327 [14] Brandi House, JonathanMalkin, and JeffA. Bilmes. 2009. The VoiceBot: a voice controlled robot arm. In Proceedings of the 27th International\nConference on Human Factors in Computing Systems, CHI 2009, Boston, MA, USA, April 4-9, 2009, Dan R. Olsen Jr., Richard B. Arthur, Ken Hinckley, Meredith Ringel Morris, Scott E. Hudson, and Saul Greenberg (Eds.). ACM, 183\u2013192. https://doi.org/10.1145/1518701.1518731 [15] Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt Keutzer. 2020. SqueezeBERT: What can computer vision teach NLP about efficient neural networks?. In Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, SustaiNLP@EMNLP 2020, Online, November 20, 2020, Nafise Sadat Moosavi, Angela Fan, Vered Shwartz, Goran Glavas, Shafiq R. Joty, Alex Wang, and Thomas Wolf (Eds.). Association for Computational Linguistics, 124\u2013135. https://doi.org/10.18653/v1/2020.sustainlp-1.17 [16] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language Understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of ACL, Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 4163\u20134174. https://doi.org/10.18653/v1/2020.findings-emnlp.372 [17] Biing-Hwang Juang and Lawrence R Rabiner. 2005. Automatic speech recognition\u2013a brief history of the technology development. Georgia Institute of Technology. Atlanta Rutgers University and the University of California. Santa Barbara 1 (2005), 67. [18] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=H1eA7AEtvS [19] Xiaodong Liu, YuWang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He, Weizhu Chen, Hoifung Poon, Guihong Cao, and Jianfeng Gao. 2020. The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020, Asli Celikyilmaz and Tsung-Hsien Wen (Eds.). Association for Computational Linguistics, 118\u2013126. https://doi.org/10.18653/v1/2020.acldemos.16 [20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692 [21] Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. arXiv preprint cs/0205028 (2002). [22] Xiaoling Lv, Minglu Zhang, and Hui Li. 2008. Robot control based on voice command. In 2008 IEEE International Conference on Automation\nand Logistics. IEEE, 2490\u20132494. https://doi.org/10.1109/ICAL.2008.4636587 [23] Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014. The Stanford\nCoreNLP natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations. The Association for Computer Linguistics, 55\u201360. [24] Rajesh Kannan Megalingam, Avinash Hegde Kota, and Vijaya Krishna Tejaswi P. 2021. Optimal Approach to Speech Recognition with ROS. In 2021 6th International Conference on Communication and Electronics Systems (ICCES). 111\u2013116. https://doi.org/10.1109/ ICCES51350.2021.9489085 [25] Rajesh Kannan Megalingam, Racharla Shriya Reddy, Yannam Jahnavi, and Manaswini Motheram. 2019. ROS Based Control of Robot Using Voice Recognition. In 2019 Third International Conference on Inventive Systems and Control (ICISC). 501\u2013507. https: //doi.org/10.1109/ICISC44355.2019.9036443 [26] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really Better than One?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 14014\u201314024. https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html [27] Anthony G Oettinger. 2013. Automatic language translation. In Automatic Language Translation. Harvard University Press. [28] NLP on Embedded-Devices. 2023. https://github.com/CPS2RL/NLP-on-Embedded-Devices. Accessed:May 15. [29] Raspberry Pi. 2023. https://www.raspberrypi.com/. https://www.raspberrypi.com/ Accessed:May 15. [30] Morgan Quigley, Ken Conley, Brian P. Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y. Ng. 2009. ROS: an\nopen-source Robot Operating System. ICRA Workshop on Open Source Software. Accessed:May 15. [31] Respeaker. 2017. https://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0/ [32] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.\narXiv preprint cs/0306050 (2003). [33] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. CoRR abs/1910.01108 (2019). arXiv:1910.01108 http://arxiv.org/abs/1910.01108 [34] Benedikt Schmidt, Reuben Borrison, Andrew Cohen, Marcel Dix, Marco G\u00e4rtler, Martin Hollender, Benjamin Kl\u00f6pper, Sylvia Maczey, and\nShunmuga Siddharthan. 2018. Industrial virtual assistants: Challenges and opportunities. In Proceedings of the 2018 ACM International\n, Vol. 1, No. 1, Article . Publication date: September 2023.\nJoint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers. 794\u2013801. [35] Sudeep Sharan, Trung Quoc Nguyen, Peter Nauth, and Rui Araujo. 2019. Implementation and testing of voice control in a mobile robot\nfor navigation. In 2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM). IEEE, 145\u2013150. [36] Heung-Yeung Shum, Xiao-dong He, and Di Li. 2018. From Eliza to XiaoIce: challenges and opportunities with social chatbots. Frontiers\nof Information Technology & Electronic Engineering 19 (2018), 10\u201326. [37] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient Knowledge Distillation for BERT Model Compression. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 4322\u20134331. https://doi.org/10.18653/v1/D19-1441 [38] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 2158\u20132170. https://doi.org/10.18653/v1/2020.acl-main.195 [39] Thierry Tambe, ColemanHooper, Lillian Pentecost, Tianyu Jia, En-Yu Yang,MarcoDonato, Victor Sanh, Paul N.Whatmough, AlexanderM. Rush, David Brooks, and Gu-Yeon Wei. 2021. EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference. In MICRO \u201921: 54th Annual IEEE/ACM International Symposium on Microarchitecture, Virtual Event, Greece, October 18-22, 2021. ACM, 830\u2013844. https://doi.org/10.1145/3466752.3480095 [40] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling Task-Specific Knowledge from BERT into Simple Neural Networks. CoRR abs/1903.12136 (2019). arXiv:1903.12136 http://arxiv.org/abs/1903.12136 [41] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog Applications. CoRR abs/2201.08239 (2022). arXiv:2201.08239 https://arxiv.org/abs/2201.08239 [42] Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazhagan, Xin Li, and Amelia Archer. 2019. Small and Practical BERT Models for Sequence Labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 3630\u20133634. https://doi.org/10.18653/v1/D19-1374 [43] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation. CoRR abs/1908.08962 (2019). arXiv:1908.08962 http://arxiv.org/abs/1908.08962 [44] UM25C. 2017. https://www.makerhawk.com/collections/frontpage/products/makerhawk-um25c-usb-tester-bluetooth-usb-meter-typec-current-meter-usb-power-meter-dc-24-000v-5-0000a-usb-cable-tester-1-44-inch-color-lcd-multimeter-voltage-tester-usb-loadqc-2-0-qc-3-0 [45] Andrea Vanzo, Danilo Croce, Emanuele Bastianelli, Roberto Basili, and Daniele Nardi. 2020. Grounded language interpretation of robotic commands through structured learning. Artificial Intelligence 278 (2020). https://doi.org/10.1016/j.artint.2019.103181 [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [47] Anjali Verma, Deepak Kumar, Hariom Maurya, Anuj Kumar, and Mr Prabhakant Dwivedi. 2020. Voice Control Robot Using Arduino. International Research Journal of Modernization in Engineering Technology and Science 2 (2020), 04. [48] Ryen WWhite. 2018. Skill discovery in virtual assistants. Commun. ACM 61, 11 (2018), 106\u2013113. [49] Dong Yu and Li Deng. 2016. Automatic speech recognition. Vol. 1. Springer. [50] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8BERT: Quantized 8Bit BERT. In Fifth Workshop on Energy\nEfficient Machine Learning and Cognitive Computing - NeurIPS Edition, EMC2@NeurIPS 2019, Vancouver, Canada, December 13, 2019. IEEE, 36\u201339. https://doi.org/10.1109/EMC2-NIPS53020.2019.00016 [51] Yichen Zhang, Zhiguo Lu, Changhui Wang, Chong Liu, and Yunong Wang. 2018. Voice control dual arm robot based on ROS system. 2018 IEEE International Conference on Intelligence and Safety for Robotics (ISR), 232\u2013237. https://doi.org/10.1109/IISR.2018.8535942 [52] Yi Zhang and Shi Chuan Xu. 2015. ROS Based Voice-Control Navigation of Intelligent Wheelchair. In Engineering Decisions for Industrial Development (Applied Mechanics and Materials, Vol. 733). Trans Tech Publications Ltd, 740\u2013744. https://doi.org/10.4028/www.scientific. net/AMM.733.740\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        },
        {
            "heading": "APPENDIX",
            "text": "In the following section, we formally, discuss the datasets (Appendix. A), and present an overview of the ROS architecture (Appendix. B)."
        },
        {
            "heading": "A DATASET A.1 Human Robot Interaction Corpus (HuRIC)",
            "text": "To use the HuRIC dataset in our task, we extracted the intent information from frame semantics. The original dataset6 contains many intents that are close semantically; hence, we merged a few similar intents after analyzing the command. For instance, based on most of the commands in the dataset, we merged intent \u201cReleasing\u201d and intent \u201cPlacing\u201d to a single intent \u201cPlacing\u201d. The HuRIC dataset contains a mixture of simple and complex commands. The difference between simple and complex commands can be clarified using a few examples, such as a command such as \u201cBring the book on the table in the kitchen\u201d has a single intent as, \u201cBring\u201d, whereas command like \u201cPlease go to the kitchen and inspect the sink\u201d has two different intents, namely \u201cMotion\u201d and \u201cInspect\u201d. However, there are very few examples of complex commands in the dataset."
        },
        {
            "heading": "A.2 GoEmotion",
            "text": "In contrast to the basic six emotions, which include only one positive emotion (joy), GoEmotion taxonomy includes twelve positive emotions. Additionally, the taxonomy includes eleven negative, four ambiguous emotion categories, and one neutral emotion, making it widely suitable for conversation understanding tasks that require a subtle differentiation between emotion expressions.\nThe goal was to build a large dataset focused on conversational data, where emotion is a critical component of communication. Because the Reddit platform offers a large, publicly available volume of content that includes direct user-to-user conversation, it is a valuable resource for emotion analysis. So, the authors built GoEmotion using Reddit comments from 2005 (the start of Reddit) to January 2019, sourced from subreddits with at least 10K comments, excluding deleted and non-English comments. The authors created a taxonomy seeking to jointly maximize three objectives: (i) provide the greatest coverage of the emotions expressed in Reddit data; (ii) provide the greatest coverage of types of emotional expressions, and (iii) limit the overall number of emotions and their overlap. Such a taxonomy allows data-driven, fine-grained emotion understanding while also addressing potential data sparsity for some emotions.\nEstablishing the taxonomy was an iterative process to define and refine the emotion label categories. During the data labelling stages, the authors considered a total of 56 emotion categories. From this sample, they identified and removed emotions that were scarcely selected by raters, had low inter-rater agreement due to similarity to other emotions, or were difficult to detect from the text. The authors also added emotions that were frequently suggested by raters and were well represented in the data. Finally, they refined emotion category names to maximize interpretability, leading to the high inter-rater agreement, with 94% of examples having at least two raters agreeing on at least one emotion label."
        },
        {
            "heading": "A.3 CoNLL",
            "text": "For the training and development set, ten days\u2019 worth of data were taken from the files representing the end of August 1996. For the test set, the texts were from December 1996. The pre-processed raw data covers the month of September 1996. All data files contain one word per line with empty lines representing sentence boundaries. At the end of each line, there is a tag that states whether the current word is inside a named entity or not. The\n6https://github.com/crux82/huric.\n, Vol. 1, No. 1, Article . Publication date: September 2023.\ndata contains entities of four types: persons (PER), organizations (ORG), locations (LOC), and miscellaneous names (MISC).\nA.4 WNUT 2017 As discussed in 3.2.3, user comments were mined from different social media platforms. We present some details about the dataset in the following. Documents were drawn from various English-speaking subreddits over January-March 2017. These were selected based on volume, for a variety of regions and granularities. For example, country- and city-level subreddits were included, as well as non-geo-specific forums such as /r/homeassistant, r/HomeImprovement, and /r/restaurants. Documents were filtered to include only those between 20 and 400 characters in length, split into sentences, and tagged with the NLTK [21] and Stanford CoreNLP [23].\nThe corpus also includes comments from online video-sharing platforms (e.g., YouTube). Comments are drawn from the all-time top 100 videos across all categories within certain parts of the anglosphere. One hundred top-level comments were drawn from each video. Non-English comments were removed. Twitter samples were drawn from time periods matching recent natural disasters, specifically the Rigopiano avalanche and the Palm Sunday shootings. This was intended to select content about emerging events that may contain highly specific and novel toponyms.\nAnother set of user-generated content was drawn from StackExchange. In particular, title posts and comments, which were posted between January-May 2017 and also associated with five topics (including movies, politics, physics, sci-fi, and security), were downloaded from archive.org. From these title posts and comments, 400 samples were uniformly drawn for each topic."
        },
        {
            "heading": "B ROBOT OPERATING SYSTEM (ROS)",
            "text": "We used Robot Operating System (ROS) [30] (version 2) for controlling the robot and processing the input data. ROS-based architectures typically consist of two major components: (i) Node: a process that performs computation and (b) Topic: communication channels used to transmit data between publishers and subscribers. A ROS system consists of many small programs (nodes) which connect and continuously exchange messages. A single topic can have multiple publishers and subscribers. Nodes \u201csubscribe\u201d or \u201cpublish\u201d to a topic. When multiple nodes access the same topic simultaneously, the requests are managed using first-in, first-out queues.\nWe used standard ROS publisher-subscriber (pub-sub) models in our experiments. Figure 3 presents a high-level overview of our experiment setup. For example, \u201cName Entity Recognition\u201d and \u201cIntent Recognition\u201d nodes send messages to the ROS topic (e.g., robot_function). The \u201cRobot Driver\u201d and nodes receive the messages from the corresponding topic and perform the task.\n, Vol. 1, No. 1, Article . Publication date: September 2023."
        }
    ],
    "title": "Processing Natural Language on Embedded Devices: HowWell Do Modern Models Perform?",
    "year": 2023
}