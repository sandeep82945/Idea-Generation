{
    "abstractText": "Diffusion models generate samples by reversing a fixed forward diffusion process. Despite already providing impressive empirical results, these diffusion models algorithms can be further improved by reducing the variance of the training targets in their denoising score-matching objective. We argue that the source of such variance lies in the handling of intermediate noise-variance scales, where multiple modes in the data affect the direction of reverse paths. We propose to remedy the problem by incorporating a reference batch which we use to calculate weighted conditional scores as more stable training targets. We show that the procedure indeed helps in the challenging intermediate regime by reducing (the trace of) the covariance of training targets. The new stable targets can be seen as trading bias for reduced variance, where the bias vanishes with increasing reference batch size. Empirically, we show that the new objective improves the image quality, stability, and training speed of various popular diffusion models across datasets with both general ODE and SDE solvers. When used in combination with EDM (Karras et al., 2022), our method yields a current SOTA FID of 1.90 with 35 network evaluations on the unconditional CIFAR-10 generation task. The code is available at https://github.com/Newbeeer/stf",
    "authors": [
        {
            "affiliations": [],
            "name": "Yilun Xu"
        },
        {
            "affiliations": [],
            "name": "Shangyuan Tong"
        },
        {
            "affiliations": [],
            "name": "Tommi Jaakkola"
        }
    ],
    "id": "SP:92d9728b7fd85706dfba3698a685af0cff126a22",
    "references": [
        {
            "authors": [
                "Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "J\u00f6rg Bornschein",
                "Yoshua Bengio"
            ],
            "title": "Reweighted wake-sleep",
            "venue": "arXiv preprint arXiv:1406.2751,",
            "year": 2014
        },
        {
            "authors": [
                "Yuri Burda",
                "Roger Grosse",
                "Ruslan Salakhutdinov"
            ],
            "title": "Importance weighted autoencoders",
            "venue": "arXiv preprint arXiv:1509.00519,",
            "year": 2015
        },
        {
            "authors": [
                "Jooyoung Choi",
                "Jungbeom Lee",
                "Chaehun Shin",
                "Sungwon Kim",
                "Hyunwoo Kim",
                "Sungroh Yoon"
            ],
            "title": "Perception prioritized training of diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Defazio",
                "Francis Bach",
                "Simon Lacoste-Julien"
            ],
            "title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "J.R. Dormand",
                "P.J. Prince"
            ],
            "title": "A family of embedded runge-kutta formulae",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 1980
        },
        {
            "authors": [
                "V\u0131\u0301ctor Elvira",
                "Luca Martino"
            ],
            "title": "Advances in importance sampling",
            "venue": "Wiley StatsRef: Statistics Reference Online,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Grover",
                "Jiaming Song",
                "Ashish Kapoor",
                "Kenneth Tran",
                "Alekh Agarwal",
                "Eric J Horvitz",
                "Stefano Ermon"
            ],
            "title": "Bias correction of learned generative models using likelihood-free importance weighting",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Tim Hesterberg"
            ],
            "title": "Weighted average importance sampling and defensive mixture",
            "venue": "distributions. Technometrics,",
            "year": 1995
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey E Hinton",
                "Peter Dayan",
                "Brendan J Frey",
                "Radford M Neal"
            ],
            "title": "The \u201cwake-sleep\u201d algorithm for unsupervised neural networks",
            "year": 1995
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen",
                "Peter Dayan"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Rie Johnson",
                "Tong Zhang"
            ],
            "title": "Accelerating stochastic gradient descent using predictive variance reduction",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "year": 2006
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusion-based generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Shitong Luo",
                "Wei Hu"
            ],
            "title": "Diffusion probabilistic models for 3d point cloud generation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Alberto Maria Metelli",
                "Matteo Papini",
                "Francesco Faccio",
                "Marcello Restelli"
            ],
            "title": "Policy optimization via importance sampling",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Art B. Owen"
            ],
            "title": "Monte Carlo theory, methods and examples",
            "year": 2013
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Severi Rissanen",
                "Markus Heinonen",
                "A. Solin"
            ],
            "title": "Generative modelling with inverse heat",
            "venue": "dissipation. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian J. Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training",
            "venue": "gans. ArXiv,",
            "year": 2016
        },
        {
            "authors": [
                "Chence Shi",
                "Shitong Luo",
                "Minkai Xu",
                "Jian Tang"
            ],
            "title": "Learning gradient fields for molecular conformation generation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "ArXiv, abs/2010.02502,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Improved techniques for training score-based generative models",
            "venue": "ArXiv, abs/2006.09011,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Adith Swaminathan",
                "Thorsten Joachims"
            ],
            "title": "The self-normalized estimator for counterfactual learning",
            "venue": "In NIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "Chong Wang",
                "X. Chen",
                "Alex Smola",
                "E. Xing"
            ],
            "title": "Variance reduction for stochastic gradient optimization",
            "venue": "In NIPS,",
            "year": 2013
        },
        {
            "authors": [
                "Ziyu Wang",
                "Shuyu Cheng",
                "Li Yueru",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "A wasserstein minimum velocity approach to learning unnormalized models",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion GANs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Minkai Xu",
                "Lantao Yu",
                "Yang Song",
                "Chence Shi",
                "Stefano Ermon",
                "Jian Tang"
            ],
            "title": "Geodiff: a geometric diffusion model for molecular conformation",
            "venue": "generation. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Xu",
                "Ziming Liu",
                "Max Tegmark",
                "Tommi Jaakkola"
            ],
            "title": "Poisson flow generative models",
            "venue": "arXiv preprint arXiv:2209.11178,",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Xu",
                "Ziming Liu",
                "Yonglong Tian",
                "Shangyuan Tong",
                "Max Tegmark",
                "T. Jaakkola"
            ],
            "title": "Pfgm++: Unlocking the potential of physics-inspired generative models",
            "venue": "ArXiv, abs/2302.04265,",
            "year": 2023
        },
        {
            "authors": [
                "Shuo Yang",
                "Ping Luo",
                "Chen Change Loy",
                "Xiaoou Tang"
            ],
            "title": "From facial parts responses to face detection: A deep learning approach",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "VP\u2019s transition kernel can be reparameterized in the form of N (x, \u03c32",
            "year": 2022
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "We consider the CIFAR-10 and CelebA 642 in image generation tasks. Following Song & Ermon (2020), we first center-crop the CelebA images and then resize them to 64\u00d7 64. For VE/VP, we use the same set of hyper-parameters and the NCSN++/DDPM++ backbones and the continuous-time training objectives for forward SDEs in Song et al. (2021b). For EDM, we adopt the improved hyper-parameters and architectures for NCSN++",
            "year": 2022
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "EDM. To measure the stability of converged VE models, we repeat the experiment 3 times on CIFAR-10 for DSM and STF objectives, using different random seeds. We quantitatively study the training overhead of STF. All the numbers are measured on two NVIDIA A100",
            "venue": "GPUs. In Table 3 and Table 4,",
            "year": 2022
        },
        {
            "authors": [
                "pler (Song"
            ],
            "title": "2021a) for VP. For RK45 sampler of VE, we use the function implemented in scipy.integrate.solve ivp with the tolerances atol=1e\u22125/1e\u22124, rtol=1e\u22125/1e\u22124",
            "year": 2021
        },
        {
            "authors": [
                "Song"
            ],
            "title": "2021b), we set the terminal time to 1e\u2212 5/1e\u2212 3 for VE/VP. For EDM, we adopt Heun\u2019s 2nd order method and the discretization scheme in (Karras",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) have recently achieved impressive results on a wide spectrum of generative tasks, such as image generation (Nichol et al., 2022; Song et al., 2021b), 3D point cloud generation (Luo & Hu, 2021) and molecular conformer generation (Shi et al., 2021; Xu et al., 2022a). These models can be subsumed under a unified framework in the form of Ito\u0302 stochastic differential equations (SDE) (Song et al., 2021b). The models learn time-dependent score fields via score-matching (Hyva\u0308rinen & Dayan, 2005), which then guides the reverse SDE during generative sampling. Popular instances of diffusion models include variance-exploding (VE) and variance-preserving (VP) SDE (Song et al., 2021b). Building on these formulations, EDM (Karras et al., 2022) provides the best performance to date.\nWe argue that, despite achieving impressive empirical results, the current training scheme of diffusion models can be further improved. In particular, the variance of training targets in the denoising scorematching (DSM) objective can be large and lead to suboptimal performance. To better understand the origin of this instability, we decompose the score field into three regimes. Our analysis shows that the phenomenon arises primarily in the intermediate regime, which is characterized by multiple modes or data points exerting comparable influences on the scores. In other words, in this regime, the sources of the noisy examples generated in the course of the forward process become ambiguous. We illustrate the problem in Figure 1(a), where each stochastic update of the score model is based on disparate targets.\nWe propose a generalized version of the denoising score-matching objective, termed the Stable Target Field (STF) objective. The idea is to include an additional reference batch of examples that are used to calculate weighted conditional scores as targets. We apply self-normalized importance sampling to aggregate the contribution of each example in the reference batch. Although this process can substantially reduce the variance of training targets (Figure 1(b)), especially in the intermediate regime,\n\u2217Equal Contribution.\nar X\niv :2\n30 2.\n00 67\n0v 2\n[ cs\n.L G\n] 1\n7 Fe\nb 20\n23\nit does introduce some bias. However, we show that the bias together with the trace-of-covariance of the STF training targets shrinks to zero as we increase the size of the reference batch.\nExperimentally, we show that our STF objective achieves new state-of-the-art performance on CIFAR10 unconditional generation when incorporated into EDM (Karras et al., 2022). The resulting FID score (Heusel et al., 2017) is 1.90 with 35 network evaluations. STF also improves the FID/Inception scores for other variants of score-based models, i.e., VE and VP SDEs (Song et al., 2021b), in most cases. In addition, it enhances the stability of converged score-based models on CIFAR-10 and CelebA 642 across random seeds, and helps avoid generating noisy images in VE. STF accelerates the training of score-based models (3.6\u00d7 speed-up for VE on CIFAR-10) while obtaining comparable or better FID scores. To the best of our knowledge, STF is the first technique to accelerate the training process of diffusion models. We further demonstrate the performance gain with increasing reference batch size, highlighting the negative effect of large variance.\nOur contributions are summarized as follows: (1) We detail the instability of the current diffusion models training objective in a principled and quantitative manner, characterizing a region in the forward process, termed the intermediate phase, where the score-learning targets are most variable (Section 3). (2) We propose a generalized score-matching objective, stable target field, which provides more stable training targets (Section 4). (3) We analyze the behavior of the new objective and prove that it is asymptotically unbiased and reduces the trace-of-covariance of the training targets by a factor pertaining to the reference batch size in the intermediate phase under mild conditions (Section 5). (4) We illustrate the theoretical arguments empirically and show that the proposed STF objective improves the performance, stability, and training speed of score-based methods. In particular, it achieves the current state-of-the-art FID score on the CIFAR-10 benchmark when combined with EDM (Section 6)."
        },
        {
            "heading": "2 BACKGROUND ON DIFFUSION MODELS",
            "text": "In diffusion models, the forward process1 is an SDE with no learned parameter, in the form of:\ndx = f(x, t)dt+ g(t)dw,\nwhere x \u2208 Rd with x(0) \u223c p0 being the data distribution, t \u2208 [0, 1], f : Rd \u00d7 [0, 1] \u2192 Rd, g : [0, 1] \u2192 R, and w \u2208 Rd is the standard Wiener process. It gradually transforms the data distribution to a known prior as time goes from 0 to 1. Sampling of diffusion models is done via a corresponding reverse-time SDE (Anderson, 1982):\ndx = [ f(x, t)\u2212 g(t)2\u2207x log pt(x) ] dt\u0304+ g(t)dw\u0304,\nwhere \u00b7\u0304 denotes time traveling backward from 1 to 0. Song et al. (2021b) proposes a probability flow ODE that induces the same marginal distribution pt(x) as the SDE: dx =\n1For simplicity, we focus on the version where the diffusion coefficient g(t) is independent of x(t).\n[ f(x, t)\u2212 12g(t) 2\u2207x log pt(x) ]\ndt\u0304. Both formulations progressively recover p0 from the prior p1. We estimate the score of the transformed data distribution at time t, \u2207x log pt(x), via a neural network, s\u03b8(x, t). Specifically, the training objective is a weighted sum of the denoising score-matching (Vincent, 2011):\nmin \u03b8\nEt\u223cqt(t)\u03bb(t)Ex\u223cp0Ex(t)\u223cpt|0(\u00b7|x) [ \u2016s\u03b8(x(t), t)\u2212\u2207x(t) log pt|0(x(t)|x)\u201622 ] , (1)\nwhere qt is the distribution for time variable, e.g., U [0, 1] for VE/VP (Song et al., 2021b) and a log-normal distribution for EDM Karras et al. (2022), and \u03bb(t) = \u03c32t is the positive weighting function to keep the time-dependent loss at the same magnitude (Song et al., 2021b), and pt|0(x(t)|x) is the transition kernel denoting the conditional distribution of x(t) given x2. Specifically, diffusion models \u201cdestroy\u201d data according to a diffusion process utilizing Gaussian transition kernels, which result in pt|0(x(t)|x) = N (\u00b5t, \u03c32t I). Recent works (Xu et al., 2022b; Rissanen et al., 2022) have also extended the underlying principle from the diffusion process to more general physical processes where the training objective is not necessarily score-related."
        },
        {
            "heading": "3 UNDERSTANDING THE TRAINING TARGET IN SCORE-MATCHING OBJECTIVE",
            "text": "The vanilla denoising score-matching objective at time t is:\n`DSM(\u03b8, t) = Ep0(x)Ept|0(x(t)|x)[\u2016s\u03b8(x(t), t)\u2212\u2207x(t) log pt|0(x(t)|x)\u2016 2 2], (2)\nwhere the network is trained to fit the individual targets \u2207x(t) log pt|0(x(t)|x) at (x(t), t) \u2013 the \u201cinfluence\u201d exerted by clean data x on x(t). We can swap the order of the sampling process by first sampling x(t) from pt and then x from p0|t(\u00b7|x(t)). Thus, s\u03b8 has a closed form minimizer:\ns\u2217DSM(x(t), t) = Ep0|t(x|x(t))[\u2207x(t) log pt|0(x(t)|x)] = \u2207x(t) log pt(x(t)). (3)\nThe score field is a conditional expectation of \u2207x(t) log pt|0(x(t)|x) with respect to the posterior distribution p0|t. In practice, a Monte Carlo estimate of this target can have high variance (Owen, 2013; Elvira & Martino, 2021). In particular, when multiple modes of the data distribution have comparable influences on x(t), p0|t(\u00b7|x(t)) is a multi-mode distribution, as also observed in Xiao et al. (2022). Thus the targets\u2207x(t) log pt|0(x(t)|x) vary considerably across different x and this can strongly affect the estimated score at (x(t), t), resulting in slower convergence and worse performance in practical stochastic gradient optimization (Wang et al., 2013).\nTo quantitatively characterize the variations of individual targets at different time, we propose a metric \u2013 the average trace-of-covariance of training targets at time t:\nVDSM(t) = Ept(x(t)) [ Tr(Covp0|t(x|x(t))(\u2207x(t) log pt|0(x(t)|x))) ] = Ept(x(t))Ep0|t(x|x(t)) [ \u2016\u2207x(t) log pt|0(x(t)|x))\u2212\u2207x(t) log pt(x(t))\u201622 ] . (4)\nWe use VDSM(t) to define three successive phases relating to the behavior of training targets. As shown in Figure 2(a), the three phases partition the score field into near, intermediate, and far regimes (Phase 1\u223c3 respectively). Intuitively, VDSM(t) peaks in the intermediate phase (Phase 2), where multiple distant modes in the data distribution have comparable influences on the same noisy perturbations, resulting in unstable targets. In Phase 1, the posterior p0|t concentrates around one single mode, thus low variation. In Phase 3, the targets remain similar across modes since limt\u21921 pt|0(x(t)|x) \u2248 p1 for commonly used transition kernels.\nWe validate this argument empirically in Figure 2(b), which shows the estimated VDSM(t) for a mixture of two Gaussians as well as a subset of CIFAR-10 dataset (Krizhevsky et al., 2009) for a more\n2We omit \u201c(0)\u201d from x(0) when there is no ambiguity.\nrealistic setting. Here we use VE SDE, i.e., pt|0(x(t)|x) = N ( x, \u03c32m( \u03c3M \u03c3m )2tI )\nfor some \u03c3m and \u03c3M (Song et al., 2021b). VDSM(t) exhibits similar phase behavior across t in both toy and realistic cases. Moreover, VDSM(t) reaches its maximum value in the intermediate phase, demonstrating the large variations of individual targets. We defer more details to Appendix C."
        },
        {
            "heading": "4 TREATING SCORE AS A FIELD",
            "text": "The vanilla denoising score-matching approach (Equation 3) can be viewed as a Monte Carlo estimator, i.e., \u2207x(t) log pt(x(t)) = Ep0|t(x|x(t))[\u2207x(t) log pt|0(x(t)|x)] \u2248 1 n \u2211n i=1\u2207x(t) log pt|0(x(t)|xi) where xi is sampled from p0|t(\u00b7|x(t)) and n = 1. The variance of a Monte Carlo estimator is proportional to 1n , so we propose to use a larger batch (n) to counter the high variance problem described in Section 3. Since sampling directly from the posterior p0|t is not practical, we first apply importance sampling with the proposal distribution p0. Specifically, we sample a large reference batch BL = {xi}ni=1 \u223c pn0 and get the following approximation:\n\u2207x(t) log pt(x(t)) \u2248 1\nn n\u2211 i=1 p0|t(xi|x(t)) p0(xi) \u2207x(t) log pt|0(x(t)|xi).\nThe importance weights can be rewritten as p0|t(x|x(t))/p0(x) = pt|0(x(t)|x)/pt(x(t)). However, this basic importance sampling estimator has two issues. The weights now involve an unknown normalization factor pt(x(t)) and the ratio between the prior and posterior distribution can be large in high dimensional spaces. To remedy these problems, we appeal to self-normalization techniques (Hesterberg, 1995) to further stabilize the training targets:\n\u2207x(t) log pt(x(t)) \u2248 n\u2211 i=1 pt|0(x(t)|xi)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xi). (5)\nWe term this new training target in Equation 5 as Stable Target Field (STF). In practice, we sample the reference batch BL = {xi}ni=1 from pn0 and obtain x(t) by applying the transition kernel to the \u201cfirst\u201d training data x1. Taken together, the new STF objective becomes:\n`STF(\u03b8, t) = E{xi}ni=1\u223cpn0 Ex(t)\u223cpt|0(\u00b7|x1)[\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) \u2225\u2225\u22252 2 ] . (6)\nWhen n = 1, STF reduces to the vanilla denoising score-matching (Equation 2). When n > 1, STF incorporates a reference batch to stabilize training targets. Intuitively, the new weighted target assigns larger weights to clean data with higher influence on x(t), i.e., higher transition probability pt|0(x(t)|x). Similar to our analysis in Section 3, we can again swap the sampling process in Equation 6 so that, for a perturbation x(t), we sample the reference batch BL = {xi}ni=1 from p0|t(\u00b7|x(t))p n\u22121 0 , where the first element involves the posterior, and the rest follow the data distribution. Thus, the minimizer of the new objective (Equation 6) is (derivation can be found in Appendix B.1)\ns\u2217STF(x(t), t) = Ex1\u223cp0|t(\u00b7|x(t))E{xi}ni=2\u223cpn\u221210 [ n\u2211 k=1 pt|0(x(t)|xk)\u2211 j pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) ] .\n(7)\nNote that although STF significantly reduces the variance, it introduces bias: the minimizer is no longer the true score. Nevertheless, in Section 5, we show that the bias converges to 0 as n \u2192 \u221e, while reducing the trace-of-covariance of the training targets by a factor of n when p0|t \u2248 p0. We further instantiate the STF objective (Equation 6) with transition kernels in the form of pt|0(x(t)|x) = N (x, \u03c32t I), which includes EDM (Karras et al., 2022), VP (through reparameterization) and VE (Song et al., 2021b):\nEx1\u223cp0|t(\u00b7|x(t))E{xi}ni=2\u223cpn\u221210 \u2225\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 1\u03c32t n\u2211 k=1 exp ( \u2212\u2016x(t)\u2212xk\u2016 2 2 2\u03c32t ) \u2211 j exp ( \u2212\u2016x(t)\u2212xj\u2016 2 2\n2\u03c32t\n) (xk \u2212 x(t))\u2225\u2225\u2225\u22252 2  .\nTo aggregate the time-dependent STF objective over t, we sample the time variable t from the training distribution qt and apply the weighting function \u03bb(t). Together, the final training objective for STF is Et\u223cqt(t) [\u03bb(t)`STF(\u03b8, t)]. We summarize the training process in Algorithm 1. The small batch size |B| is the same as the normal batch size in the vanilla training process. We defer specific use cases of STF objectives combined with various popular diffusion models to Appendix A.\nAlgorithm 1 Learning the stable target field Input: Training iteration T , Initial model s\u03b8, dataset D, learning rate \u03b7. for t = 1 . . . T do\nSample a large reference batch BL from D, and subsample a small batch B = {xi}|B|i=1 from BL Uniformly sample the time {ti}|B|i=1 \u223c qt(t)|B| Obtain the batch of perturbed samples {xi(ti)}|B|i=1 by applying the transition kernel pt|0 on B Calculate the stable target field of BL for all xi(ti):\nvBL(xi(ti)) = \u2211 x\u2208BL pti|0(xi(ti)|x)\u2211\ny\u2208BL pti|0(xi(ti)|y)\n\u2207xi(ti) log pti|0(xi(ti)|x) Calculate the loss: L(\u03b8) = 1|B| \u2211|B| i=1 \u03bb(ti)\u2016s\u03b8(xi(ti), ti)\u2212 vBL(xi(ti))\u201622\nUpdate the model parameter: \u03b8 = \u03b8 \u2212 \u03b7\u2207L(\u03b8) end for return s\u03b8"
        },
        {
            "heading": "5 ANALYSIS",
            "text": "In this section, we analyze the theoretical properties of our approach. In particular, we show that the new minimizer s\u2217STF(x(t), t) (Equation 7) converges to the true score asymptotically (Section 5.1). Then, we show that the proposed STF reduces the trace-of-covariance of training targets propositional to the reference batch size in the intermediate phase, with mild conditions (Section 5.2)."
        },
        {
            "heading": "5.1 ASYMPTOTIC BEHAVIOR",
            "text": "Although in general s\u2217STF(x(t), t) 6= \u2207x(t) log pt(x(t)), the bias shrinks toward 0 with a increasing n. In the following theorem we show that the minimizer of STF objective at (x(t), t), i.e., s\u2217STF(x(t), t), is asymptotically normal when n\u2192\u221e. Theorem 1. Suppose \u2200t \u2208 [0, 1], 0 < \u03c3t <\u221e, then\n\u221a n ( s\u2217STF(x(t), t)\u2212\u2207x(t) log pt(x(t)) ) d\u2212\u2192 N (0, Cov(\u2207x(t)pt|0(x(t)|x)) pt(x(t))2 ) (8)\nWe defer the proof to Appendix B.2. The theorem states that, for commonly used transition kernels, s\u2217STF(x(t), t) \u2212 \u2207x(t) log pt(x(t)) converges to a zero mean normal, and larger reference batch size (n) will lead to smaller asymptotic variance. As can be seen in Equation 8, when n \u2192 \u221e, s\u2217STF(x(t), t) highly concentrates around the true score\u2207x(t) log pt(x(t))."
        },
        {
            "heading": "5.2 TRACE OF COVARIANCE",
            "text": "We now highlight the small variations of the training targets in the STF objective compared to the DSM. As done in Section 3, we study the trace-of-covariance of training targets in STF:\nVSTF(t) = Ept(x(t))\n[ Tr ( Covp0|t(\u00b7|x(t))pn\u221210 ( n\u2211 k=1 pt|0(x(t)|xk)\u2211 j pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) ))] .\nIn the following theorem we compare VSTF with VDSM. In particular, we can upper bound VSTF(t) by Theorem 2. Suppose \u2200t \u2208 [0, 1], 0 < \u03c3t <\u221e, then\nVSTF(t) \u2264 1\nn\u2212 1\n( VDSM(t) + \u221a 3d\n\u03c32t\n\u221a Ept(x(t))Df ( p0(x) \u2016 p0|t(x|x(t)) )) +O ( 1\nn2\n) ,\nwhere Df is an f-divergence with f(y) = {\n(1/y \u2212 1)2 (y < 1.5) 8y/27\u2212 1/3 (y \u2265 1.5) . Further, when n d and\np0|t(x|x(t)) \u2248 p0(x) for all x(t), VSTF(t) / VDSM(t)n\u22121 .\nWe defer the proof to Appendix B.3. The second term that involves f -divergence Df is necessary to capture how the coefficients, i.e., pt|0(x(t)|xk)/ \u2211 j pt|0(x(t)|xj) used to calculate the weighted score target, vary across different samples x(t). This term decreases monotonically as a function of t. In Phase 1, p0|t(x|x(t)) differs substantially from p0(x) and the divergence term Df dominates. In contrast to the upper bound, both VSTF(t) and VDSM(t) have minimal variance at small values of t since the training target is always dominated by one x. The theorem has more relevance in Phase 2, where the divergence term decreases to a value comparable to VDSM(t). In this phase, we empirically observe that the ratio of the two terms in the upper bound ranges from 10 to 100. Thus, when we use a large reference batch size (in thousands), the theorem implies that STF offers a considerably lower variance (by a factor of 10 or more) relative to the DSM objective. In Phase 3, the second term vanishes to 0, as pt \u2248 pt|0 with large \u03c3t for commonly used transition kernels. As a result, STF reduces the average trace-of-covariance of the training targets by at least n\u2212 1 times in the far field. Together, we demonstrate that the STF targets have diminishing bias (Theorem 1) and are much more stable during training (Theorem 2). These properties make the STF objective more favorable for diffusion models training with stochastic gradient optimization."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "In this section, we first empirically validate our theoretical analysis in Section 5, especially for variance reduction in the intermediate phase (Section 6.1). Next, we show that the STF objective improves various diffusion models on image generation tasks in terms of image quality (Section 6.2). In particular, STF achieves state-of-the-art performance on top of EDM. In addition, we demonstrate that STF accelerates the training of diffusion models (Section 6.3), and improves the convergence speed and final performance with an increasing reference batch size (Section 6.3)."
        },
        {
            "heading": "6.1 VARIANCE REDUCTION IN THE INTERMEDIATE PHASE",
            "text": "The proposed Algorithm 1 utilizes a large reference batch to calculate the stable target field instead of the individual target. In addition to the theoretical analysis in Section 5, we provide further empirical study to characterize the intermediate phase and verify the variance reduction effects by STF. Apart from V (t), we also quantify the average divergence between the posterior p0|t(\u00b7|x(t)) and the data distribution p0 at time t (introduced in Theorem 2): D(t) = Ept(x(t)) [ Df ( p0|t(x|x(t)) \u2016 p0(x) )] . Intuitively, the number of high-density modes in p0|t(\u00b7|x(t)) grows as D(t) decreases. To investigate their behaviors, we construct two synthetic datasets: (1) a 64-dimensional mixture of two Gaussian components (Two Gaussians), and (2) a subset of 1024 images of CIFAR-10 (CIFAR-10-4096).\nFigure 3(a) and Figure 3(b) show the behaviors of VDSM(t) and D(t) on Two Gaussian and CIFAR10-4096. In both settings, VDSM(t) reaches its peak in the intermediate phase (Phase 2), while D(t) gradually decreases over time. These results agree with our theoretical understanding from Section 3. In Phase 2 and 3, several modes of the data distribution have noticeable influences on the scores, but only in Phase 2 are the influences much more distinct, leading to high variations of the individual target\u2207x(t) log pt|0(x(t)|x),x \u223c p0|t(\u00b7|x(t)).\nFigure 3(c) and Figure 3(d) further show the relationship between VSTF(t) and the reference batch size n. Recall that when n = 1, STF degenerates to individual target and VSTF(t) = VDSM(t). We observe that VSTF(t) decreases when enlarging n. In particular, the predicted relation VSTF(t) / VDSM(t)/(n\u2212 1) in Theorem 2 holds for the two Gaussian datasets where Df is small. On the high dimensional dataset CIFAR-10-4096, the stable target field can still greatly reduce the training target variance with large reference batch sizes n."
        },
        {
            "heading": "6.2 IMAGE GENERATION",
            "text": "We demonstrate the effectiveness of the new objective on image generation tasks. We consider CIFAR-10 (Krizhevsky et al., 2009) and CelebA 64 \u00d7 64 (Yang et al., 2015) datasets. We set the reference batch size n to 4096 (CIFAR-10) and 1024 (CelebA 642). We choose the current state-ofthe-art score-based method EDM (Karras et al., 2022) as the baseline, and replace the DSM objective with our STF objective during training. We also apply STF to two other popular diffusion models, VE/VP SDEs (Song et al., 2021b). For a fair comparison, we directly adopt the architectures and the hyper-parameters in Karras et al. (2018) and Song et al. (2021b) for EDM and VE/VP respectively. In particular, we use the improved NCSN++/DDPM++ models (Karras et al., 2022) in the EDM scheme. To highlight the stability issue, we train three models with different seeds for VE on CIFAR-10. We provide more experimental details in Appendix D.1.\nNumerical Solver. The reverse-time ODE and SDE in scored-based models are compatible with any general-purpose solvers. We use the adaptive solver RK45 method (Dormand & Prince, 1980; Song et al., 2021b) (RK45) for VE/VP and the popular DDIM solver (Song et al., 2021a) for VP. We adopt Heun\u2019s 2nd order method (Heun) and the time discretization proposed by Karras et al. (2022) for EDM. For SDEs, we apply the predictor-corrector (PC) sampler used in (Song et al., 2021b). We denote the methods in a objective-sampler format, i.e., A-B, where A \u2208 {DSM, STF} and B \u2208 {RK45, PC, DDIM, Heun}. We defer more details to Appendix D.2. Results. For quantitative evaluation of the generated samples, we report the FID scores (Heusel et al., 2017) (lower is better) and Inception (Salimans et al., 2016) (higher is better). We measure the sampling speed by the average NFE (number of function evaluations). We also include the results of several popular generative models (Karras et al., 2020; Ho et al., 2020; Song & Ermon, 2019; Xu et al., 2022b) for reference.\nTable 1 and Table 2 report the sample quality and the sampling speed on unconditional generation of CIFAR-10 and CelebA 642. Our main findings are: (1) STF achieves new state-ofthe-art FID scores for unconditional generation on CIFAR-10 benchmark. As shown in Ta-\nble 1, The STF objective obtains a FID of 1.90 when incorporated with the EDM scheme. To the best of our knowledge, this is the lowest FID score on the unconditional CIFAR-10 generation task. In addition, the STF objective consistently improves the EDM across the two architectures. (2) The STF objective improves the performance of different diffusion models. We observe that the STF objective improves the FID/Inception scores of VE/VP/EDM on CIFAR-10, for most ODE and SDE samplers. STF consistently provides performance gains for VE across datasets. Remarkably, our objective achieves much better sample quality using ODE samplers for VE, with an FID score gain of 3.39 on CIFAR-10, and 2.22 on Celeba 642.\nTable 2: FID and NFE on CelebA 642\nMethods/NFEs FID \u2193 NFE \u2193\nCelebA 642 - RK45\nVE (DSM) 7.56 260 VE (STF) 5.34 266\nCelebA 642 - PC\nVE (DSM) 9.13 2000 VE (STF) 8.28 2000\nFor VP, STF provides better results on the popular DDIM sampler, while suffering from a slight performance drop when using the RK45 sampler. (3) The STF objective stabilizes the converged VE model with the RK45 sampler. In Appendix E.1, we report the standard deviations of performance metrics for converged models with different seeds on CIFAR-10 with VE. We observe that models trained with the STF objective give more consistent results, with a smaller standard deviation of used metrics.\nWe further provide generated samples in Appendix F. One interesting observation is that when using the RK45 sampler for VE on CIFAR-10, the generated samples from the STF objective do not contain noisy images, unlike the vanilla DSM objective."
        },
        {
            "heading": "6.3 ACCELERATING TRAINING OF DIFFUSION MODELS",
            "text": "The variance-reduction techniques in neural network training can help to find better optima and achieve faster convergence rate (Wang et al., 2013; Defazio et al., 2014; Johnson & Zhang, 2013). In Figure 4, we demonstrate the FID scores every 50k iterations during the course of training. Since our goal is to investigate relative performance during the training process, and because the FID scores computed on 1k samples are strongly correlated with the full FID scores on 50k sample (Song & Ermon, 2020), we report FID scores on 1k samples for faster evaluations. We apply ODE samplers for FID evaluation, and measure the training time on two NVIDIA A100 GPUs. For a fair comparison, we report the average FID scores of models trained by the DSM and STF objective on VE versus the wall-clock training time (h).\nThe STF objective achieves better FID scores with the same training time, although the calculation of the target field by the reference batch introduces slight overhead (Algorithm 1). In Figure 4(a), we show that the STF objective drastically accelerates the training of diffusion models on CIFAR-10. The STF objective achieves comparable FID scores with 3.6\u00d7 less training time (25h versus 90h). For CelebA 642 datasets, the training time improvement is less significant than on CIFAR-10. Our hypothesis is that the STF objective is more effective when there are multiple well-separated modes in data distribution, e.g., the ten classes in CIFAR-10, where the DSM objective suffer from relatively larger variations in the intermediate phase. In addition, the converged models have better final performance when pairing with the STF on both datasets."
        },
        {
            "heading": "6.4 EFFECTS OF THE REFERENCE BATCH SIZE",
            "text": "According to our theory (Theorem 2), the upper bound of the trace-of-covariance of the STF target decreases proportionally to the reference batch size. Here we study the effects of the reference batch size (n) on model performances during training. The FID scores are evaluated on 1k samples using the RK45 sampler. As shown in Figure 5, models converge faster and produce better samples when increasing n. It suggests that smaller variations of the training targets can indeed speed up training and improve the final performances of diffusion models."
        },
        {
            "heading": "7 RELATED WORK",
            "text": "Different phases of diffusion models. The idea of diffusion models having different phases has been explored in prior works though the motivations and definitions vary (Karras et al., 2022; Choi et al., 2022). Karras et al. (2022) argues that the training targets are difficult and unnecessary to learn in the very near field (small t in our Phase 1), whereas the training targets are always dissimilar to the true targets in the intermediate and far field (our Phase 2 and Phase 3). As a result, their solution is sampling t with a log-normal distribution to emphasize the relevant region (relatively large t in our Phase 1). In contrast, we focus on reducing large training target variance in the intermediate and far field, and propose STF to better estimate the true target (cf. Karras et al. (2022)). Choi et al. (2022) identifies a key region where the model learns perceptually rich contents, and determines the training weights \u03bb(t) based on the signal-to-noise ratio (SNR) at different t. As SNR is monotonically decreasing over time, the resulting up-weighted region does not match our Phase 2 characterization. In general, our proposed STF method reduces the training target variance in the intermediate field and is complementary to previous improvements of diffusion models.\nImportance sampling. The technique of importance sampling has been widely adopted in machine learning community, such as debiasing generative models (Grover et al., 2019), counterfactual learning (Swaminathan & Joachims, 2015) and reinforcement learning (Metelli et al., 2018). Prior works using importance sampling to improve generative model training include reweighted wakesleep (RWS) (Bornschein & Bengio, 2014) and importance weighted autoencoders (IWAE) (Burda et al., 2015). RWS views the original wake-sleep algorithm (Hinton et al., 1995) as importance sampling with one latent variable, and proposes to sample multiple latents to obtain gradient estimates with lower bias and variance. IWAE utilizes importance sampling with multiple latents to achieve greater flexibility of encoder training and tighter log-likelihood lower bound compared to the standard variational autoencoder (Kingma & Welling, 2013; Rezende et al., 2014).\nVariance reduction for Fisher divergence. One popular approach to score-matching is to minimize the Fisher divergence between true and predicted scores (Hyva\u0308rinen & Dayan, 2005). Wang et al. (2020) links the Fisher divergence to denoising score-matching (Vincent, 2011) and studies the large variance problem (in O(1/\u03c34t )) of the Fisher divergence when t\u2192 0. They utilize a control variate to reduce the variance. However, this is typically not a concern for current diffusion models as the time-dependent objective can be viewed as multiplying the Fisher divergence by \u03bb(t) = \u03c32t , resulting in a finite-variance objective even when t\u2192 0."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "We identify large target variance as a significant training issue affecting diffusion models. We define three phases with distinct behaviors, and show that the high-variance targets appear in the intermediate phase. As a remedy, we present a generalized score-matching objective, Stable Target Field (STF), whose formulation is analogous to the self-normalized importance sampling via a large reference batch. Albeit no longer an unbiased estimator, our proposed objective is asymptotically unbiased and reduces the trace-of-covariance of the training targets, which we demonstrate theoretically and empirically. We show the effectiveness of our method on image generation tasks, and show that STF improves the performance, stability, and training speed over various state-of-the-art diffusion models. Future directions include a principled study on the effect of different reference batch sampling procedures. Our presented approach is uniformly sampling from the whole dataset {xi}ni=2 \u223c p n\u22121 0 , so we expect that training diffusion models with a reference batch of more samples in the neighborhood of x1 (the sample from which x(t) is perturbed) would lead to an even better estimation of the score field. Moreover, the three-phase analysis can effectively capture the behaviors of other physics-inspired generative models, such as PFGM (Xu et al., 2022b) or the more advanced PFGM++ (Xu et al., 2023). Therefore, we anticipate that STF can enhance the performance and stability of these models further."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We are grateful to Benson Chen for reviewing an early draft of this paper. We would like to thank Hao He and the anonymous reviewers for their valuable feedback. YX and TJ acknowledge support from MIT-DSTA Singapore collaboration, from NSF Expeditions grant (award 1918839) \u201cUnderstanding the World Through Code\u201d, and from MIT-IBM Grand Challenge project. ST and TJ also acknowledge support from the ML for Pharmaceutical Discovery and Synthesis Consortium (MLPDS)."
        },
        {
            "heading": "A STF SPECIFIED WITH POPULAR SGMS",
            "text": "Here, we detail the practically used STF objectives in Section 6, which are built on the popular instances of SGMs, e.g., VE, VP (Song et al., 2021b), and EDM (Karras et al., 2022).\nVE and EDM For VE and EDM, the transition kernel is in the form of pt|0(x(t)|x) = N ( x, \u03c32t I ) t \u2208 [0, 1].\nVE has \u03c3t = \u03c3m ( \u03c3M \u03c3m )t for some fixed \u03c3m and \u03c3M . EDM has \u03c3t = (t\u03c3 1 \u03c1 M + (1\u2212 t)\u03c3 1 \u03c1 m)\u03c1 for some \u03c3m and \u03c3M , with \u03c1 set to 7 in practice. The STF objective for both VE and EDM at x(t) is then in the following form:\nEx1\u223cp0|t(\u00b7|x(t))E{xi}ni=2\u223cpn\u221210 \u2225\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 1\u03c32t n\u2211 k=1 exp ( \u2212\u2016x(t)\u2212xk\u2016 2 2 2\u03c32t ) \u2211 j exp ( \u2212\u2016x(t)\u2212xj\u2016 2 2\n2\u03c32t\n) (xk \u2212 x(t))\u2225\u2225\u2225\u22252 2  . VP VP in its original formulation has the transition kernel as\npt|0(x(t)|x) = N ( e\u2212 1 4 t 2(\u03b2M\u2212\u03b2m)\u2212 12 t\u03b2mx, I \u2212 Ie\u2212 12 t 2(\u03b2M\u2212\u03b2m)\u2212t\u03b2m ) ,\nfor some \u03b2m and \u03b2M . The STF objective for VP at x(t) is\nEx1\u223cp0|tE{xi}ni=2\u223cpn\u221210 \u2225\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 1\u03c32t n\u2211 k=1 exp ( \u2212\u2016x(t)\u2212e \u03b2txk\u201622 2\u03c32t ) \u2211 j exp ( \u2212\u2016x(t)\u2212e \u03b2txj\u201622 2\u03c32t ) (e\u03b2txk \u2212 x(t)) \u2225\u2225\u2225\u22252 2  , where \u03b2t = \u2212 14 t 2(\u03b2M\u2212\u03b2m)\u2212 12 t\u03b2m, and \u03c3t = \u221a\n1\u2212 e2\u03b2t . Note that as shown in Karras et al. (2022), VP\u2019s transition kernel can be reparameterized in the form of N (x, \u03c32t I) with a correspondingly revised sampling process. Adopting this formulation, we would have the STF objective for VP the same as the one for VE and EDM with a different \u03c3t."
        },
        {
            "heading": "B PROOFS",
            "text": "B.1 DERIVATION OF EQUATION 7\nRecall that the STF objective (Equation 6) at time t is\n`STF(\u03b8, t) = E{xi}ni=1\u223cpn0 Ex(t)\u223cpt|0(\u00b7|x1)[\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) \u2225\u2225\u22252 2 ] .\nSwapping the sampling order and we get\n`STF(\u03b8, t) = Ex(t)\u223cptE{xi}ni=1\u223cp0|t(\u00b7|x(t))pn\u221210[\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) \u2225\u2225\u22252 2 ] .\nThis means that with input x(t) and t, the model is optimized with\nE{xi}ni=1\u223cp0|t(\u00b7|x(t))pn\u221210 [\u2225\u2225\u2225s\u03b8(x(t), t)\u2212 n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) \u2225\u2225\u22252 2 ] .\nTaking its derivative w.r.t. s\u03b8(x(t), t) results in\nE{xi}ni=1\u223cp0|t(\u00b7|x(t))pn\u221210\n[ 2 ( s\u03b8(x(t), t)\u2212\nn\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk)\n)] .\nSetting it to 0, we have\ns\u03b8(x(t), t)\u2212 E{xi}ni=1\u223cp0|t(\u00b7|x(t))pn\u221210 [ n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) ] = 0.\nThus, we arrive at the minimizer of the STF objective (Equation 7):\ns\u2217STF(x(t), t) = Ex1\u223cp0|t(\u00b7|x(t))E{xi}ni=2\u223cpn\u221210 [ n\u2211 k=1 pt|0(x(t)|xk)\u2211 j pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk) ] .\nB.2 PROOF FOR THEOREM 1\nTheorem 1. Suppose \u2200t \u2208 [0, 1], 0 < \u03c3t <\u221e, then\n\u221a n ( s\u2217STF(x(t), t)\u2212\u2207x(t) log pt(x(t)) ) d\u2212\u2192 N (0, Cov(\u2207x(t)pt|0(x(t)|x)) pt(x(t))2 ) (8)\nProof. Recall that s\u2217STF(x(t), t) is calculated via Equation 7. The mean of the denominator in the expectation 1n \u2211n j=1 pt|0(x(t)|xj), for large n, approximate 1 n\u22121 \u2211n j=2 pt|0(x(t)|xj), which in turn,\n1 n\u22121 \u2211n j=2 pt|0(x(t)|xj) p\u2212\u2192 pt(x(t)) by WLLN.\nSimilarly, for the mean of the remaining terms in the expectation, by CLT, we have\n\u221a n\n( 1\nn n\u2211 k=1 pt|0(x(t)|xk)\u2207x(t) log pt|0(x(t)|xk)\u2212 pt(x(t))\u2207x(t) log pt(x(t)) ) d\u2212\u2192 N ( 0, Cov(\u2207x(t)pt|0(x(t)|x))\n) Putting them together via Slutsky\u2019s theorem, we conclude the proof.\nB.3 PROOF FOR THEOREM 2\nTheorem 2. Suppose \u2200t \u2208 [0, 1], 0 < \u03c3t <\u221e, then\nVSTF(t) \u2264 1\nn\u2212 1\n( VDSM(t) + \u221a 3d\n\u03c32t\n\u221a Ept(x(t))Df ( p0(x) \u2016 p0|t(x|x(t)) )) +O ( 1\nn2\n) ,\nwhere Df is an f-divergence with f(y) = {\n(1/y \u2212 1)2 (y < 1.5) 8y/27\u2212 1/3 (y \u2265 1.5) . Further, when n d and\np0|t(x|x(t)) \u2248 p0(x) for all x(t), VSTF(t) / VDSM(t)n\u22121 .\nProof. Step 1: Make the likelihood weighting coefficients \u201cindependent\u201d We first apply Hoeffding\u2019s inequality for the set {xi}ni=2 \u223c p n\u22121 0 to make the summation\u2211n\nj=2 pt|0(x(t)|xj) concentrate to its expectation (n \u2212 1)pt(x(t)). Since pt|0(x(t)|xj) \u2208 (0, 1\n( \u221a 2\u03c0\u03c3t)d ), we have\nPr \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 j=2 pt|0(x(t)|xj)\u2212 (n\u2212 1)pt(x(t)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2265 n\u03b31  \u2264 2e\u22122n2\u03b31 (\u221a2\u03c0\u03c3t)d(n\u22121) ,\n\u2200\u03b31 \u2208 ( 12 , 1).\nThus the summation can be re-expressed as:\nn\u2211 j=2 pt|0(x(t)|xj) = (1\u2212O(2e \u22122n2\u03b31 ( \u221a 2\u03c0\u03c3t) d (n\u22121) ))[(n\u2212 1)pt(x(t)) +O(n\u03b31)]\n+O(2e \u22122n2\u03b31 (\n\u221a 2\u03c0\u03c3t) d\n(n\u22121) )O((n\u2212 1) 1 ( \u221a 2\u03c0\u03c3t)d )\n= (n\u2212 1)pt(x(t)) +O(ne\u22122n 2\u03b31\u22121(\n\u221a 2\u03c0\u03c3t) d\n)\nThe coefficient for x1 is then\npt|0(x(t)|x1)\u2211n j=1 pt|0(x(t)|xj) = pt|0(x(t)|x1) pt|0(x(t)|x1) + \u2211n j=2 pt|0(x(t)|xj)\n= pt|0(x(t)|x1)\npt|0(x(t)|x1) + (n\u2212 1)pt(x(t)) +O(ne\u22122n 2\u03b31\u22121( \u221a 2\u03c0\u03c3t)d)\n= O( 1\nn )\nThe coefficient for xk, k \u2208 {2, . . . , n} is:\npt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) = pt|0(x(t)|xk) (n\u2212 1)pt(x(t)) + pt|0(x(t)|x1) +O(ne\u22122n 2\u03b31\u22121( \u221a 2\u03c0\u03c3t)d)\n= 1 (n\u2212 1) pt|0(x(t)|xk) pt(x(t)) +O( 1 n2 ) +O(ne\u22122n 2\u03b31\u22121( \u221a 2\u03c0\u03c3t) d )\nStep 2: Re-express the trace-of-covariance by the \u201cindependent\u201d weights\nPlugging in the above formulation of coefficients, we can rewrite the trace-of-covariance for the new target as:\nVSTF(x(t), t) = Ex1\u223cp0|t(x|x(t))E{xi}ni=2\u223cpn\u22121(x)\u2016 n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk)\n\u2212 Ex1\u223cp0|t(x|x(t))E{xi}ni=2\u223cpn\u22121(x) n\u2211 k=1 pt|0(x(t)|xk)\u2211n j=1 pt|0(x(t)|xj) \u2207x(t) log pt|0(x(t)|xk)\u201622\n\u2264 Ex1\u223cp0|t(x|x(t))E{xi}ni=2\u223cpn\u22121(x)\u2016 n\u2211 k=2\n1 (n\u2212 1) pt|0(x(t)|xk) pt(x(t)) \u2207x(t) log pt|0(x(t)|xk)\n\u2212 Ex1\u223cp0|t(x|x(t))E{xi}ni=2\u223cpn\u22121(x) n\u2211 k=2\n1 (n\u2212 1) pt|0(x(t)|xk) pt(x(t)) \u2207x(t) log pt|0(x(t)|xk)\n+ n\u2211 k=2 O( 1 n2 )\u2207x(t) log pt|0(x(t)|xk)\u2212O( 1 n )Ep(x)[\u2207x(t) log pt|0(x(t)|x)]\u201622 +O( 1 n2 )\n= 1 (n\u2212 1)2 n\u2211 k=2 Tr ( Covxk\u223cp(x)( pt|0(x(t)|xk) pt(x(t)) \u2207x(t) log pt|0(x(t)|xk)) ) +O( 1 n2 )\n= 1\n(n\u2212 1) Tr\n( Covx\u223cp(x)(\npt|0(x(t)|x) pt(x(t))\n\u2207x(t) log pt|0(x(t)|x)) ) +O( 1\nn2 ) (9)\nStep 3: Upper bound the new trace-of-covariance term\nNext, we examine the new trace-of-covariance term:\nTr ( Covx\u223cp0(x)(\npt|0(x(t)|x) pt(x(t))\n\u2207x(t) log pt|0(x(t)|x)) )\n= d\u2211 i=1 Ep0(x) [ ( pt|0(x(t)|x) pt(x(t)) \u2207x(t) log pt|0(x(t)|x))i)2 ]\n\u2212 ( Ep0(x) [ pt|0(x(t)|x) pt(x(t)) \u2207x(t) log pt|0(x(t)|x))i) ])2\n= d\u2211 i=1 Ep0(x) [ ( p0|t(x|x(t)) p0(x) \u2207x(t) log pt|0(x(t)|x))i)2 ]\n\u2212 ( Ep0(x) [ p0|t(x|x(t)) p0(x(t)) \u2207x(t) log pt|0(x(t)|x))i) ])2\n= d\u2211 i=1 Ep0(x) [ ( p0|t(x|x(t)) p0(x) \u2207x(t) log pt|0(x(t)|x))i)2 ] \u2212 (\u2207x(t) log pt|0(x(t))i)2\n= d\u2211 i=1 Ep0(x) [ ( p0|t(x|x(t)) p0(x) \u2207x(t) log pt|0(x(t)|x))i)2 ] \u2212 (\u2207x(t) log pt|0(x(t))i)2\n+ Ep0|t(x|x(t)) [ (\u2207x(t) log pt|0(x(t)|x)i)2 ] \u2212 Ep0|t(x|x(t)) [ (\u2207x(t) log pt|0(x(t)|x)i)2 ] = VDSM(x(t), t) +\nd\u2211 i=1 Ep0|t [ ( p0|t(x|x(t)) p(x) \u2212 1)\u2207x(t) log pt|0(x(t)|x)2i ]\n\u2264 VDSM(x(t), t) + d\u2211 i=1\n\u221a Ep0|t [ ( p0|t(x|x(t))\np(x) \u2212 1)2\n] Ep0|t [ \u2207x(t) log pt|0(x(t)|x)4i ] (10)\nWe can further upper bound the trace-of-covariance term in Equation 9:\nVSTF(x(t), t)\n= 1\n(n\u2212 1) Tr(Covx\u223cp(x)( pt|0(x(t)|x) pt(x(t)) \u2207x(t) log pt|0(x(t)|x))) +O( 1 n2 )\n\u2264 1 n\u2212 1\n( VDSM(x(t), t) +\nd\u2211 i=1\n\u221a Ep0|t [ ( p0|t(x|x(t))\np(x) \u2212 1)2\n] Ep0|t [ \u2207x(t) log pt|0(x(t)|x)4i ])\n+O( 1\nn2 )\nTaking the expectation w.r.t pt(x(t)) for both sides, we get\nVSTF(t) \u2264 Ept(x(t)) [ 1\nn\u2212 1\n( VDSM(x(t), t)\n+ d\u2211 i=1\n\u221a Ep0|t [ ( p0|t(x|x(t))\np(x) \u2212 1)2\n] Ep0|t [ \u2207x(t) log pt|0(x(t)|x)4i ]) +O( 1\nn2 )\n]\n\u2264 1 n\u2212 1\n( VDSM(t) +\nd\u2211 i=1 Ept(x(t))\n\u221a Ep0|t [ ( p0|t(x|x(t))\np(x) \u2212 1)2\n] Ep0|t [ \u2207x(t) log pt|0(x(t)|x)4i ])\n+O( 1\nn2 )\n\u2264 1 n\u2212 1\n( VDSM(t) +\nd\u2211 i=1 \u221a Ept(x(t))Df ( p0(x) \u2016 p0|t(x|x(t)) )\u221a Ep0,t [ \u2207x(t) log pt|0(x(t)|x)4i ])\n+O( 1\nn2 ) (Concavity of x\n1 2 , Cauchy\u2019s inequality)\n\u2264 1 n\u2212 1\n( VDSM(t) + d \u221a Ez\u223cN (0,\u03c32)[ z4 \u03c38t ] \u221a Ept(x(t))Df ( p0(x) \u2016 p0|t(x|x(t)) )) +O( 1 n2 )\n\u2264 1 n\u2212 1\n( VDSM(t) + \u221a 3d\n\u03c32t\n\u221a Ept(x(t))Df ( p0(x) \u2016 p0|t(x|x(t)) )) +O( 1\nn2 )\nwhere Df is an f -divergence with f(y) = {\n(1/y \u2212 1)2 (y < 1.5) 8y/27\u2212 1/3 (y \u2265 1.5) . Note that we choose this\nparticular form of f(y) since it is the convex function with the tightest upper bound on ( 1y \u2212 1) 2.\nC DETAILS FOR THE BEHAVIOR OF VDSM(t)\nIn Section 3, we demonstrate the behavior of VDSM(t) in the three phases on Two Gaussians and a subset of CIFAR-10. Here we provide more details about the two datasets.\nThe distribution of the two Gaussian is 12N (\u00b5, \u03c3\u0302 2I64\u00d764)+ 1 2N (\u2212\u00b5, \u03c3\u0302 2I64\u00d764), where\u00b5 = 0.1\u00b71 \u2208 R64, and \u03c3\u0302 = 1e \u2212 4. We estimate all the integrals in Equation 4 by sampling 1k points from the corresponding distributions. For the subset of CIFAR-10, we uniformly sample 4096 images from CIFAR-10 dataset, and assign uniform distribution on the discrete set. We also approximate VDSM(t) by Monte Carlo estimation and sample 200 perturbations for each t. We use VE SDE for all simulations, and set \u03c3m = 1e\u2212 2, \u03c3M = 50. Interestingly, VDSM(t) is relatively large for the Two Gaussians distribution compared to CIFAR10 (see Figure 2(b)) when t\u2192 0. This can be explained by their continuous and discrete natures. For the two Gaussian distribution, we can rewrite VDSM(t) as\nVDSM(t) = Ex(t)\u223cpt(x(t)) [\n\u03c3\u03022\n\u03c32t (\u03c3 2 t + \u03c3\u0302\n2) + 4 \u03b1(x(t))(1\u2212 \u03b1(x(t)))\u2016\u00b5\u2016\u03c34t \u03c34t + \u03c3\u0302 2 ] where \u03b1(x) = 1\n1+exp\u22124xT\u00b5 can be regarded as the probability that x comes from the Gaussian\ncomponent N (\u00b5, \u03c3\u03022I64\u00d764). When t \u2192 0, obviously \u03b1(x(t))(1 \u2212 \u03b1(x(t))) \u2192 0, and the term \u03b1(x(t))(1\u2212\u03b1(x(t)))\u2016\u00b5\u2016\u03c34t\n\u03c34t+\u03c3\u0302 2 vanishes. Hence limt\u21920 VDSM(t) \u2248 limt\u21920 \u03c3\u0302\n2\n\u03c32t (\u03c3 2 t+\u03c3\u0302\n2) = \u03c3\u0302\n2\n\u03c32m(\u03c3 2 m+\u03c3\u0302 2) , which can not be neglected when \u03c3m is small. On the other hand, we can effectively view the 4069 discrete samples as a mixture of 4096 0-variance Gaussians, i.e., \u03c3\u0302 = 0. Thus by similar reasoning we could see limt\u21920 VDSM(t) \u2248 limt\u21920 \u03c3\u0302 2\n\u03c32t (\u03c3 2 t+\u03c3\u0302\n2) = 0."
        },
        {
            "heading": "D EXPERIMENTAL DETAILS",
            "text": "In this section, we include more details about the training and sampling of score-based models by the STF and DSM objectives. All the experiments are run on two NVIDIA A100 GPUs.\nD.1 TRAINING\nWe consider the CIFAR-10 and CelebA 642 in image generation tasks. Following Song & Ermon (2020), we first center-crop the CelebA images and then resize them to 64\u00d7 64. For VE/VP, we use the same set of hyper-parameters and the NCSN++/DDPM++ backbones and the continuous-time training objectives for forward SDEs in Song et al. (2021b). For EDM, we adopt the improved hyper-parameters and architectures for NCSN++ in Karras et al. (2022). We set the reference batch size n to 1024 on CIFAR-10, 1024 on CelebA 642. The training iteration is 1.3M on CIFAR-10 and 1M on CelebA 642 for VE/VP, and 200M images for EDM (Karras et al., 2022). The small batch size B in Algorithm 1 is the same as the batch size in the baseline score-based methods. For model selection, we pick the checkpoint with the lowest FID per 50k iterations on 10k samples for computing all the scores, as in Song et al. (2021b) for VE/VP, and per 2.5M images on 50k samples as in Karras et al. (2022) for EDM.\nTo measure the stability of converged VE models, we repeat the experiment 3 times on CIFAR-10 for DSM and STF objectives, using different random seeds.\nWe quantitatively study the training overhead of STF. All the numbers are measured on two NVIDIA A100 GPUs. In Table 3 and Table 4, we report the wall-clock training time (s) per 50 iterations/50k images on VE/EDM. We can see that the STF introduces additional overhead after incorporating the large reference batch. Since the calculation of the mini-batch target does not involve neural networks, the STF does not take significantly longer training time. Indeed, in Section 6.3 we show that STF achieves comparable or better performance within a shorter training time.\nWe adopt the RK45 method for the backward ODE sampling of VE, and the DDIM sampler (Song et al., 2021a) for VP. For RK45 sampler of VE, we use the function implemented in scipy.integrate.solve ivp with the tolerances atol=1e\u22125/1e\u22124, rtol=1e\u22125/1e\u22124 for CIFAR-10/CelebA 642. As in Song et al. (2021b), we set the terminal time to 1e\u2212 5/1e\u2212 3 for VE/VP. For EDM, we adopt Heun\u2019s 2nd order method and the discretization scheme in (Karras et al., 2022), with 35 NFE.\nWe use the predictor-corrector (PC) sampler for reverse-time SDE. We follow Song et al. (2021b) to set the Euler-Maruyama method as the predictor and the Langevin dynamics (MCMC) as the corrector.\nD.3 EVALUATIONS\nFor the evaluation, we compute the Fre\u0301chet distance between 50000 samples and the pre-computed statistics of CIFAR-10. For CelebA 642, we adopt the setting in Song & Ermon (2020) where the distance is computed between 10000 samples and the test set."
        },
        {
            "heading": "E EXTRA EXPERIMENTS",
            "text": "E.1 STABILITY OF CONVERGED MODELS\nIn Table 5, we report the sample quality measured by FID/Inception score, and their standard deviations across random seeds on CIFAR-10. We can see that models trained with STF objective have lower variations of their final performances, in most cases. In particular, the standard deviation decreases from 4.41 to 0.06 for RK45 sampler on VE. It suggests that the STF objective can stabilize the performance of converged models.\nE.2 EFFECTS OF STEP SIZE\nIn Figure 6, we show the FID scores with the number of function evaluations of ODE samplers on CIFAR-10 and CelebA 642 . To vary the NFE, we adjust the error tolerance in the RK45 method. The sample quality of the STF objective degrades gracefully when decreasing the NFE. The STF objective consistently outperforms the DSM one for all NFEs on CIFAR-10, and largely improves over the baseline when setting the tolerance to 5e\u2212 3 on CelebA 642 . It suggests that the STF has greater robustness to different step sizes."
        },
        {
            "heading": "F EXTENDED SAMPLES",
            "text": "We provide extended samples from score-based models trained by DSM/STF objective on CIFAR-10 and CelebA 642 by ODE samplers. For systematic comparison, we visualize samples from models trained on different seeds. We also provide samples generated by the state-of-the-art model \u2014 STF with EDM framework.\nF.1 CIFAR-10\nIn Figure 7, we visualize the samples produced by different methods across random seeds for VE. We use the RK45 sampler for sampling. We observe that the model trained by the DSM objective can produce noisy images (in red boxes), and the image quality has great variability across different random seeds. In contrast, models trained by STF objective generate clean and consistent samples with varying random seeds.\nIn Figure 8, we further provide samples from a model trained by STF under the EDM framework (Karras et al., 2022). The model is the current state-of-the-art on the unconditional CIFAR-10 generation task.\nF.2 CELEBA 64\u00d7 64\nIn Figure 9, we provide samples from models trained on DSM and STF objectives with VE."
        }
    ],
    "title": "STABLE TARGET FIELD FOR REDUCED VARIANCE SCORE ESTIMATION IN DIFFUSION MODELS",
    "year": 2023
}