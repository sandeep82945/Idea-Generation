{
    "abstractText": "Understanding and quantifying cause and effect is an important problem in many domains. The generally-agreed solution to this problem is to perform a randomised controlled trial. However, even when randomised controlled trials can be performed, they usually have relatively short duration\u2019s due to cost considerations. This makes learning long-term causal effects a very challenging task in practice, since the long-term outcome is only observed after a long delay. In this paper, we study the identification and estimation of long-term treatment effects when both experimental and observational data are available. Previous work provided an estimation strategy to determine longterm causal effects from such data regimes. However, this strategy only works if one assumes there are no unobserved confounders in the observational data. In this paper, we specifically address the challenging case where unmeasured confounders are present in the observational data. Our long-term causal effect estimator is obtained by combining regression residuals with short-term experimental outcomes in a specific manner to create an instrumental variable, which is then used to quantify the long-term causal effect through instrumental variable regression. We prove this estimator is unbiased, and analytically study its variance. In the context of the front-door causal structure, this provides a new causal estimator, which may be of independent interest. Finally, we empirically test our approach on synthetic-data, as well as real-data from the International Stroke Trial.",
    "authors": [
        {
            "affiliations": [],
            "name": "Graham Van Goffrier"
        },
        {
            "affiliations": [],
            "name": "Lucas Maystre"
        },
        {
            "affiliations": [],
            "name": "Ciar\u00e1n Gilligan-Lee"
        },
        {
            "affiliations": [],
            "name": "Cheng Zhang"
        }
    ],
    "id": "SP:08858f74dbd6efd3ba2a7f1c471109161381c1ed",
    "references": [
        {
            "authors": [
                "Milton Abramowitz",
                "Irene A Stegun",
                "Robert H Romer"
            ],
            "title": "Handbook of mathematical functions with formulas, graphs, and mathematical tables",
            "year": 1988
        },
        {
            "authors": [
                "Susan Athey",
                "Raj Chetty",
                "Guido W. Imbens",
                "Hyunseung Kang"
            ],
            "title": "The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely",
            "venue": "NBER Working Paper",
            "year": 2019
        },
        {
            "authors": [
                "Elias Bareinboim",
                "Judea Pearl"
            ],
            "title": "Causal inference and the data-fusion problem",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2016
        },
        {
            "authors": [],
            "title": "Carolei. The international stroke trial (ist): a randomised trial of aspirin, subcutaneous heparin, both, or neither among 19435 patients with acute ischaemic stroke",
            "venue": "URL https: //www.sciencedirect.com/science/article/pii/S0140673697040117",
            "year": 1997
        },
        {
            "authors": [
                "Praveen Chandar",
                "Brian St. Thomas",
                "Lucas Maystre",
                "Vijay Pappu",
                "Roberto Sanchis-Ojeda",
                "Tiffany Wu",
                "Ben Carterette",
                "Mounia Lalmas",
                "Tony Jebara"
            ],
            "title": "Using survival models to estimate user engagement in online experiments",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Lu Cheng",
                "Ruocheng Guo",
                "Huan Liu"
            ],
            "title": "Long-term effect estimation with surrogate representation",
            "venue": "In Proceedings of the 14th ACM International Conference on Web Search and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Victor Chernozhukov",
                "Denis Chetverikov",
                "Mert Demirer",
                "Esther Duflo",
                "Christian Hansen",
                "Whitney Newey",
                "James Robins"
            ],
            "title": "Double/debiased machine learning for treatment and causal parameters",
            "venue": "arXiv preprint arXiv:1608.00060,",
            "year": 2016
        },
        {
            "authors": [
                "Carlos Cinelli",
                "Daniel Kumor",
                "Bryant Chen",
                "Judea Pearl",
                "Elias Bareinboim"
            ],
            "title": "Sensitivity analysis of linear structural causal models",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Anish Dhir",
                "Ciar\u00e1n M Lee"
            ],
            "title": "Integrating overlapping datasets using bivariate causal discovery",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ciar\u00e1n M Gilligan-Lee",
                "Christopher Hart",
                "Jonathan Richens",
                "Saurabh Johri"
            ],
            "title": "Leveraging directed causal discovery to detect latent common causes in cause-effect pairs",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shantanu Gupta",
                "Zachary C Lipton",
                "David Childers"
            ],
            "title": "Estimating treatment effects with observed confounders and mediators",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Somit Gupta",
                "Ronny Kohavi",
                "Diane Tang",
                "Ya Xu",
                "Reid Andersen",
                "Eytan Bakshy",
                "Niall Cardin",
                "Sumita Chandran",
                "Nanyu Chen",
                "Dominic Coey"
            ],
            "title": "Top challenges from the first practical online controlled experiments summit",
            "venue": "ACM SIGKDD Explorations Newsletter,",
            "year": 2019
        },
        {
            "authors": [
                "Maximilian Ilse",
                "Patrick Forr\u00e9",
                "Max Welling",
                "Joris M Mooij"
            ],
            "title": "Efficient causal inference from combined observational and interventional data through causal reductions",
            "venue": "arXiv preprint arXiv:2103.04786,",
            "year": 2021
        },
        {
            "authors": [
                "Guido Imbens",
                "Nathan Kallus",
                "Xiaojie Mao",
                "Yuhao Wang"
            ],
            "title": "Long-term causal inference under persistent confounding via data combination",
            "venue": "arXiv preprint arXiv:2202.07234,",
            "year": 2022
        },
        {
            "authors": [
                "Olivier Jeunen",
                "Ciar\u00e1n M Gilligan-Lee",
                "Rishabh Mehrotra",
                "Mounia Lalmas"
            ],
            "title": "Disentangling causal effects from sets of interventions in the presence of unobserved confounders",
            "venue": "arXiv preprint arXiv:2210.05446,",
            "year": 2022
        },
        {
            "authors": [
                "Ron Kohavi",
                "Alex Deng",
                "Brian Frasca",
                "Roger Longbotham",
                "Toby Walker",
                "Ya Xu"
            ],
            "title": "Trustworthy online controlled experiments: Five puzzling outcomes explained",
            "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2012
        },
        {
            "authors": [
                "Ciar\u00e1n M Lee",
                "Robert W Spekkens"
            ],
            "title": "Causal inference via algebraic geometry: feasibility tests for functional causal structures with two binary observed variables",
            "venue": "Journal of Causal Inference,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan G Richens",
                "Ciar\u00e1n M Lee",
                "Saurabh Johri"
            ],
            "title": "Improving the accuracy of medical diagnosis with causal machine learning",
            "venue": "Nature communications,",
            "year": 2020
        },
        {
            "authors": [
                "Shohei Shimizu",
                "Patrik O Hoyer",
                "Aapo Hyv\u00e4rinen",
                "Antti Kerminen",
                "Michael Jordan"
            ],
            "title": "A linear non-gaussian acyclic model for causal discovery",
            "venue": "Journal of Machine Learning Research,",
            "year": 2006
        },
        {
            "authors": [
                "Marco Taboga"
            ],
            "title": "Marginal and conditional distributions of a multivariate normal vector",
            "venue": "Available at: https://www.statlect.com/probability-distributions/ multivariate-normal-distribution-partitioning,",
            "year": 2021
        },
        {
            "authors": [
                "Eric J Tchetgen Tchetgen",
                "Andrew Ying",
                "Yifan Cui",
                "Xu Shi",
                "Wang Miao"
            ],
            "title": "An introduction to proximal causal learning",
            "venue": "arXiv preprint arXiv:2009.10982,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Zhang",
                "Karthika Mohan",
                "Judea Pearl"
            ],
            "title": "Causal inference with non-iid data using linear graphical models",
            "venue": "Available at: https://ftp.cs.ucla.edu/pub/stat_ser/r514",
            "year": 2022
        },
        {
            "authors": [
                "A. Appendix"
            ],
            "title": "Covariance Algebra In order to extend the derivations in Gupta et al. (2021) to cases with confounded mediators, multiple mediators, and pre-treatment covariates, it is necessary to introduce some new technology. Many key results including bias and variance for FDC-type estimators and covariance between estimators, all necessary to the estimation",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Quantifying cause and effect relationships is of fundamental importance in many fields, from medicine to economics (Richens et al. (2020); Gilligan-Lee (2020)). The gold standard solution to this problem is to conduct randomised controlled trials, or A/B tests. However, in many situations, such trials cannot be performed; they could be unethical, too expensive, or just technologically infeasible. However, even when randomised controlled trials can be performed, they usually have relatively short durations due to cost considerations. For example, online A/B tests in industry usually last for only a few weeks (Gupta et al., 2019). This makes learning long-term causal effects a very challenging task in practice, since long-term outcomes are often observed only after a long delay. Often short-term outcomes are different to long-term ones (Kohavi et al., 2012), and, as many decision-makers are interested in long-term outcomes, this is a crucial problem to address. For\n* Research was started while this author as an intern at Spotify.\n\u00a9 2023 G.V. Goffrier, L. Maystre & C. Gilligan-Lee.\nar X\niv :2\n30 2.\n10 62\n5v 1\n[ st\nat .M\nL ]\n2 1\ninstance, technology companies are interested in understanding the impact of deploying a feature on long-term retention (Chandar et al., 2022), economists are interested in long-term outcomes of job training programs (Athey et al., 2019), and doctors are interested in the long-term impacts of medical interventions, such as treatments for stroke (Carolei, 1997).\nIn contrast to experimental data, observational data are often easier and cheaper to acquire, so they are more likely to include long-term outcome observations. Previous work by Athey et al. (2019) devised a method to estimate long-term causal effects by combining observational long-term data and short-term experimental data. However, this strategy only works if one assumes there are no unobserved confounders in the observational data. Nevertheless, observational data are very susceptible to unmeasured confounding, which can lead to severely biased treatment effect estimates. Can we combine these short-term experiments with observational data to estimate long-term causal effects when latent confounders are present in observational data?\nIn this paper, we address this problem and study the identification and estimation of longterm treatment effects when both short-term experimental data and observational data with latent confounders are available. We initially work with linear structural equation models. Our long-term causal effect estimator is obtained by combining regression residuals with short-term experimental data in a specific manner to create an instrumental variable, which is then used to quantify the longterm causal effect through instrumental variable regression. We prove that this estimator is unbiased, and analytically study its variance. When applied in the front-door causal structure, this strategy provides a new causal estimator, which may be of independent interest. We extend this estimator from linear structural causal models to the partially linear structural models routinely studied in economics (Chernozhukov et al., 2016) and prove unbiasedness still holds under mild assumptions. Finally, we empirically test our long-term causal effect estimator, demonstrating accurate estimation of long-term effects on synthetic data, as well as real data from the International Stroke Trial.\nAlthough long-term effect estimation is our primary focus, the estimator and methods described can be applied to any single-stage causal effect. In this context, they can be interpreted as a novel strategy that combines Front-Door and Instrument Variables to estimate causal effects in the presence of unobserved confounders.\nIn summary, our main contributions are:\n1. An algorithm for estimating long-term causal effects unbiasedly from both short-term experiments and observational data with latent confounders in linear structural causal models. This approach allows for continuous treatment variables\u2014hence can deal with treatment dosages.\n2. An analytical study of the variance of this estimator.\n3. An extension of our estimator from linear structural causal models to partially linear structural models and a proof that unbiasedness still holds under a weak assumption.\n4. An empirical demonstration of our long-term causal effect estimator on synthetic and real data.\nRelevant source code and documentation has been made freely available in our online repository."
        },
        {
            "heading": "2. Related work",
            "text": "Estimating long-term causal effects The estimation of long-term causal effects from short-term experiments and observational data was initiated by Athey et al. (2019). The authors of that work\ndevised a method to estimate such quantities by making use of short-term mediators, or surrogates, of the treatment. Their estimation strategy was comprised of two parts: first, they use the experimental data to determine the impact of the treatment on the surrogates, and then combined this impact with a predictive causal model that used the observational data to predict the impact of a change in the surrogates on the long-term outcome. This allowed them to predict the impact of the treatment on the long-term outcome directly at the end of the short-term experiment. However, this strategy only works if one assumes there are no unobserved confounders in the observational data. Recent work by Cheng et al. (2021) has expanded this approach with tools from machine learning, by learning efficient representations of the surrogates\u2014again requiring there to be no unobserved confounders. More recent work by Imbens et al. (2022) has explored estimating long-term causal effects when unobserved confounders are present. These authors utilised results from the proximal causal inference literature, see Tchetgen et al. (2020) for an overview of these results, in their estimation strategy. However, to make use of these results, the authors have to assume existence of three sequential mediators between the treatment and long-term outcome, and that these satisfy completeness conditions that, informally, require any variation in the latent confounders is captured by variation in the mediators. Our results, on the other hand, provide long-term treatment effect estimators that are unbiased even in the presence of latent confounders that do not require such sequential mediators that are strong proxies for the latent confounders.\nCombining experimental and observational data Beyond using observational data and shortterm experimental data to estimate long-term causal effects, previous work has explored other advantages of combining observational and experimental data. Indeed, Bareinboim and Pearl (2016) have investigated non-parametric identifiability of causal effects using both observational and experimental data, and how one can utilise such data regimes to transport causal effects learned in one data to another, in a paradigm they refer to as \u201cdata fusion.\u201d Moreover, Jeunen et al. (2022) has shown that one can learn to disentangle the effects of multiple, simultaneously-applied interventions by combining observational data with experimental data from joint interventions. Lastly, Ilse et al. (2021) demonstrated the most efficient way to combine observational and experimental data to learn certain causal effects. They showed they could significantly reduce the number of samples from the experimental data required to achieve a desired estimation accuracy.\nLinear structural causal models Many previous authors have worked in the linear structural causal model formalism. Indeed, Shimizu et al. (2006) has shown that one can recover causal structure given just observational data if one assumes an underlying linear structural causal model with non-Gaussian noise. Gupta et al. (2021) has utilised this formalism to derive closed form expressions for the bias and variance of treatment effect estimators when both observed confounders and mediators are present. Cinelli et al. (2019) has derived closed-form expressions for the treatment effect bias when there are unobserved confounders in the dataset under investigation. Lastly, Zhang et al. (2022) has explored what conditions lead to bias when estimating causal effects from non-IID data, and how can we remove such bias given certain assumptions."
        },
        {
            "heading": "3. Methods",
            "text": "This section is structured as follows. We first define linear structural causal models with Gaussian noise, the class of models we will mainly be working with in this paper. As a warm up to our main problem, we first explore long-term effect estimation when latent confounding influences the\nshort-term treatment and long-term outcome, but does not influence the mediator. We note that this confounding may represent a single cause which persists through both short-term and long-term timescales. The causal structure in this particular case corresponds to the front-door structure studied in Pearl (2009). In this case, we derive\u2014to our knowledge\u2014a novel causal effect estimator for the front-door criterion, which may be of independent interest. This estimator is biased when latent confounding is present between the treatment and long-term outcome. However, the way the bias manifests is instructive, and suggests a way to adapt this estimation strategy to make it unbiased in this case. We prove that the estimator based on this strategy is indeed unbiased in the presence of latent confounding, and analytically study its variance. Finally, we extend this estimator from linear structural causal models to partial linear structural models, and prove that its bias is small in the presence of latent confounding if the treatment is strongly correlated with the latent confounder."
        },
        {
            "heading": "3.1. Setting up the problem",
            "text": "Motivated by the desire to unbiasedly combine short-term experimental data with long-term observational data, we define the following linear Gaussian structural causal model, which we will refer to as the linear confounded-mediator model (CMM):\nWi = u W i , Xi = dWi + u X i , Mi = cXi + Wi + u M i , Yi = aMi + bWi + u Y i , (1)\nwhere index i runs over samples. Here, X ,M ,Y ,W are respectively the treatment, short-term mediator, long-term outcome, and latent confounder. The causal structure for this model is depicted in Figure 1. * For the observed variables X ,M ,Y , the uNi are independent Gaussian noise terms with zero mean: uNi \u223c N (0, \u03c32uN ) for node N \u2208 {X,M, Y }. The term u W i in the latent confounder structural equation is also an independent Gaussian noise term, but it has non-zero mean \u00b5uW 6= 0: uWi \u223c N (\u00b5uW , \u03c32uW ).\nThe framework having been defined, the typically desired treatment effect is ac. But, as we assume that c can be estimated unbiasedly from experimental data, our goal is to estimate a given c and an observational dataset of samples from (X,M, Y ). That is, we ask to what extent it is possible to transfer knowledge of causation before a mediator to knowledge of causation after that mediator, in the presence of unobserved confounding on that mediator. For example, we could take c to have been conclusively estimated via an A/B test, while a is inaccessible to such experimentation due to its long timescale. This question also naturally arises in the context of chains of NM mediator variables,\n*. In this work we assume the causal structure follows Figure 1. To gain confidence in this assumption, one could employ causal discovery algorithms, see Lee and Spekkens (2017); Dhir and Lee (2020); Gilligan-Lee et al. (2022) for more information on these algorithms.\nwhere the statistician hopes to propagate knowledge of an early mediation stage \u2018down the chain\u2019. Although we focus on scalar-valued variables throughout, an extension of this methodology to vectorvalued W and M would be straightforward, only requiring an expansion of the covariance-matrix formalism outlined in Appendix A and interpreting as matrix-valued."
        },
        {
            "heading": "3.2. Warm-up: a mediator without confounding",
            "text": "With = 0 the CMM in Figure 1 is the standard mediator\u2014or front-door\u2014model, treated thoroughly in the linear setting by Gupta et al. (2021). It is well-known that so long as mediator M is not directly confounded, a may be unbiasedly estimated by the front-door criterion estimator (FDC):\na\u0302FDC = P (Y |do(M)) = \u2211 X P (Y |M,X)P (X) = (X.X)(M.Y )\u2212 (X.M)(X.Y ) (X.X)(M.M)\u2212 (X.M)2 , (2)\nwhere we have used A.B as a shorthand for sample-space inner product \u2211\niAi \u00b7 Bi. Note that no knowledge of c is needed. Indeed c can be unbiasedly estimated by regressing M on X here.\nWe now give an alternative derivation of the FDC in terms of instrumental variables, a review of which is given in Pearl (2009). Essentially, an instrument for a causal arrow a : M \u2192 Y is a variable I such that a nonzero arrow f : I \u2192 M exists, and I is uncorrelated with any other causes of Y , such as W or uY in the CMM.\nConsider the ordinary least squares (OLS) regression of M on X , which trivially produces an unbiased estimator c\u0302 = M.XX.X . Naively rearranging the structural equation, the residual of this estimator appears to be noise uM . Constructing the true residual, we see that this still holds once all covariances are accounted for,\nRc \u2261M \u2212 OLS[M |X]X, (3)\nfollowing from independence of uM from uX and uW , the terminal causes of X . For the same reason, this residual Rc = uM is a valid instrument for a : M \u2192 Y , as seen by constructing the relevant instrumental estimator:\na\u0302Rc = OLS[Y |Rc] OLS[M |Rc] . (4)\nThe above expression may be phrased entirely in terms of observed variables by making the substitution uM 7\u2192M \u2212 M.XX.XX . Simplifying, we arrive at a\u0302Rc = a\u0302FDC. Hence, our instrumental-inspired estimator is unbiased and equal to the previously known estimator that follows from the front-door criterion. We will refer to Res[M |X] corresponding to c : X \u2192M more generally as the c-residual Rc. To our knowledge, this construction of the FDC via an instrumental estimator has not appeared in the literature, and we will refer to it as the Instrumental FDC (IFDC)."
        },
        {
            "heading": "3.3. The Instrumental FDC for confounded mediators",
            "text": "A causal arrow : W \u2192 M violates the conditions for the FDC. Our reason for introducing the IFDC is that it facilitates a natural extension of the FDC to the confounded mediator model, and more generally to any model with pathway X \u2192M \u2192 Y as a subgraph. The IFDC estimator can be presumed biased since W and M are no longer d-separated after conditioning on X . Expressions for the IFDC biases on a (and corresponding OLS bias on c) are derived in Appendix B and are given by:\nBias[c\u0302OLS] = d \u03c32uW\nd2\u03c32uW + \u03c3 2 uX\nBias[a\u0302Rc ] = b \u03c32uW \u03c3 2 uX\n2\u03c32uW \u03c3 2 uX + \u03c32uM (\u03c3 2 uX + d2\u03c32uW ) (5)\nThe bias on c vanishes if d or \u03c32uX \u03c3 2 uW , while the bias on a vanishes if b, \u03c32uM b d2 \u03c32uX , or \u03c32uM b \u03c3 2 uW\n. From the structural equations, we might naively expect residual Res[M |X] = uM \u2212 duX , and therefore explain the bias on a\u0302Rc by the lack of independence between uX and X . However, computing the correlations of the residual with X and W in full reveals a surprise:\nE[Cov(Rc, X)] = E[M.X \u2212 M.X\nX.X X.X] = 0 (6)\nE[Cov(Rc,W )] =\nd2(N \u2212 1)\n( \u03c32uX + E [ (X.uX) 2\nX.X\n]) > 0 (7)\nThis is a lesson in not relying too heavily on the intuition of structural equations for confounding variables: the bias on a\u0302Rc in fact arises entirely from correlation between Rc and W . In the following we will see that the residual instrument can be modified to retain unbiasedness if c is known.\n3.4. The /d-improved IFDC\nWe propose that the most direct route to propagate improved knowledge of c forward, in order to improve the IFDC estimator for a, is via intermediate knowledge of the quantity d . Ratios are desirable targets for estimation because they are insensitive to correlated biases on their numerator and denominator, and this particular ratio naively manifests inRc as controlling the size of the biasing uX term. We have identified several strategies for constructing estimators for d , with a ratio estimator based on X = dW + uX and the residual M \u2212 cX \u223c W + uM proving the most successful:(\u0302\nd\n) = M \u2212 cX\nX\u0304 (8)\nwhere A\u0304 denotes the sample mean \u2211\ni(Ai)/N . This estimator is unbiased in the limit of large samples, as \u00b5uW 6= 0 and \u00b5uX = \u00b5uM = 0. It is possible that superior estimators exist, but we find the ratio estimator to be adequate for our purposes.\nThe \u201c d -improved\u201d residual is then defined as the portion of M which is leftover after removing all causal contributions from X , both via direct path c and backdoor path /d:\nRR = Rc \u2212 (\u0302 d ) X = M \u2212 ( c+ (\u0302 d )) X. (9)\nThis construction leaves a door open to joint estimation of c and d from the prior stage in the model, in the sense that only the sum is needed and biases of opposite sign could destructively interfere, but we do not explore this further. The resultant instrumental estimator for a takes the form:\na\u0302RR = RR.Y\nRR.M =\nM.Y \u2212 ( c+ (\u0302 d )) X.Y\nM.M \u2212 ( c+ (\u0302 d )) X.M . (10)\nFor convenience in application by the reader, we express our estimation strategy in algorithmic form: In the next section we show this strategy unbiasedly estimates the causal effect of M on Y .\nAlgorithm 1 d -improved Instrumental FDC Estimator Input: Short-term experimental dataset E = {X,M}, observational dataset O = {X,M, Y } Output: Estimator for causal effect of M on Y .\n1: From E , estimate causal effect of X on M : c. 2: Using samples from O, regress M on X and compute residual: Rc 3: Using samples from O, compute sample mean of M \u2212 cX and X and take their ratio: /d. 4: Compute Rc \u2212 dX and denote it by RR. 5: Use RR in instrumental variable regression to estimate the causal effect of M on Y .\n3.5. Unbiasedness and variance for the /d-improved IFDC\nAlthough we will argue via approximations and simulations that a\u0302RR = RR.Y /RR.M is unbiased (except at its pole), it is more straightforward to show that the ratio of estimators E(RR.Y )/E(RR.M) is unbiased. This uncorrelated-ratio approximation is justified by the fact that it holds exactly for the IFDC, even in the presence of latent confounding, and is further discussed in Appendix B.\nEvaluating algebraically by the methods outlined in Appendix A one obtains:\nE [ M.Y \u2212 ( c+\nd\n) X.Y ] = a ( \u03c32uM \u2212 c \u03c32uX d ) , (11)\nE [ M.M \u2212 ( c+\nd\n) X.M ] = \u03c32uM \u2212 c \u03c32uX d , (12)\nand so we can observe that a\u0302RR is unbiased to the extent that the uncorrelated-ratio approximation holds. There is one exception: a unique value of d = 1 c exists (assuming homoscedasticity of the noise terms for simplicity) for which the numerator and denominator simultaneously approach 0, and at which the bias is therefore unbounded. For finite sample sizes, one expects that this pole will be centered in a region of finite width where the estimator performs poorly, but that this region will contract to a delta function as N \u2192\u221e. In summary, we have the following:\nProposition 1 In linear CMMs, the causal effect a : M \u2192 Y can be unbiasedly estimated by computing the following ratio of expectations:\nE [ Rc.Y \u2212 ( d ) X.Y ] E [ Rc.M \u2212 ( d ) X.M ] = E [M.Y \u2212 (c+ d)X.Y ] E [ M.M \u2212 ( c+ d ) X.M\n] = a. Proof The result follows from application of (11) and (12). Further details appear in Appendix B\nAlthough the presence of this isolated pole in the bias is not an overwhelming obstacle, it is practically inconvenient if samples are limited and one\u2019s system happens to fall in the wrong region of parameter space. Fortunately, there is one more tool at hand. In the case of a longer chain of mediators, more precisely if there exists a prior instrument on arrow a : X \u2192 M (which we will denote g : V \u2192 X), it is no longer necessary for c to be provided by an existing experiment. Instead, it may be estimated instrumentally by c\u0302 = M.VX.V , while the d -improved IFDC can be built from an adjusted prior-instrument residual:\nRV = Res(M |X)\u2212 (\u0302 d ) Res(X|V ). (13)\nThe instrumental estimator a\u0302RR remains unbiased other than at a pole; but this pole is located at d = 1 c(g2+1)\n, again assuming homoscedasticity of the noise terms. The practical consequence is that, if the practitioner has access to both a prior instrument and experimental data (or a low-variance estimation of c from a previous link in the chain), they may choose whichever form of the IFDC is more suited to their value of /d, which will be known. Given sufficiently strong prior causation g, the two poles are well-separated. However, even if only one of these is available, with sufficient samples the bias even arbitrarily near to a pole will approach 0.\nMaking use of the known variance properties of instrumental estimators, we construct an approximate expression for the asymptotic variance of a\u0302RR (details in Appendix B),\nV\u221e(a\u0302RR) = b2\u03c32uW \u03c3 2 uX + \u03c32uY (d 2\u03c32uW + \u03c3 2 uX )\n(d2\u03c32uw + \u03c3 2 uX\n) \u00b7 \u03c32uM +\n2\nd2 \u03c32uX\n(\u03c32uM \u2212 c d \u03c3 2 uX )2\n= V\u221e(a\u0302FDC) \u00b7 1 +\n2 d2 \u03c32uX \u03c32uM(\n1\u2212 c d \u03c32uX \u03c32uM )2 (14) which demonstrates that in general the improved estimator variance need not dramatically exceed that for the typical FDC, except near the bias pole d = 1 c . Similarly, as treatment noise \u03c3 2 uX \u2192 0, V\u221e(a\u0302RR)\u2192 V\u221e(a\u0302FDC); this is equivalent to the situation where d such that the treatment X is very strongly coupled to the confounder W . As mediator noise \u03c32uM \u2192 0, the variance vanishes, for the intuitive reason that weighted confounder W is then exactly known on a per-sample basis."
        },
        {
            "heading": "3.6. Performance of improved estimators in a partial linear CMM",
            "text": "We now assess to what extent the developed estimators remain unbiased when the causal effects d : W \u2192 X and : W \u2192 M are permitted to be nonlinear. That is, we consider update to the confounded mediator model: X = d(W ) + uX ,M = c.X + (W ) + uM . This is an example of a partial linear causal model, which we term the partial linear CMM. We will take functions d(W ) and (W ) to be polynomial-valued, requiring further that d(W ) is invertible such that backdoor path \u25e6 d\u22121 : X \u2192M is well-defined. Let us write:\nd(W ) = \u221e\u2211 k=1 dk W k k! , (W ) = \u221e\u2211 k=1 k W k k! . (15)\nIt is possible to define algebraic conditions on coefficients dk in the form of inequalities between the eigenvalues of the Hermite matrix of d\u2032(W ), such that d\u2032(W ) > 0 \u2200W (permitting d\u2032(W ) = 0 at isolated points) such that d(W ) is invertible if and only if the algebraic conditions are satisfied.\nIt is well-known (Abramowitz et al., 1988) that the power series of an inverse function up to order n may be computed iteratively from the coefficients of the original power series up to order n. We note however that each finite order in the original function induces nonzero terms to infinite polynomial order in the inverse function, which could be included say to orderm to improve precision. Taking m = n, we quote the series expansion for \u25e6 d\u22121,\n\u25e6 d\u22121(X) = 1 d1 X + d1 2 \u2212 d2 1 d31 X2 + d21 3 + 2d 2 2 1 \u2212 d1d3 1 \u2212 2d1d2 2 d51 X3 +O(X4), (16)\nwhich also enjoys the key property that only coefficients order-by-order in d and are needed.\nWe now investigate if the instrumental estimator a\u0302RR , introduced in the linear case in the previous section and defined in the nonlinear case below, is biased:\nRR = M \u2212 cX \u2212 \u25e6 d\u22121(X). (17)\nAgain, the justification for this estimation approach is thatRR should be uncorrelated with confounder W , and therefore a good instrument for a : M \u2192 Y , so long as it is possible to produce unbiased or low-bias estimates of c and of the coefficients of \u25e6 d\u22121.\nIn the non-linear case, how does one compute \u25e6 d\u22121(X)? The residual from regressing M on X is naively given by R = uM + \u25e6 d\u22121(X \u2212 uX) by means of the backdoor path through W . Expanding the series representation from (16), we see that samplewise R\u2192 uM + \u25e6 d\u22121(X) as uX \u2192 0, which corresponds to \u03c32ux \u2192 0. That is, to the case where X is strongly correlated with W . Therefore, in this case, by polynomial regression of R on X , it is theoretically possible to extract all coefficients of \u25e6 d\u22121 to a desired order. We note that this method is much less sample-efficient than the ratio-based estimator for /d which we identified in the linear case. Now, in the case where \u03c32ux \u2192 0, where X is strongly correlated with W , can we prove our estimation approach is unbiased?\nWith RR well-defined, taking advantage of the structural equations, the bias on the instrumental estimator a\u0302RR may then be computed as:\nBias [a\u0302RR ] = E\n[ ( uM + \u25e6 d\u22121(X \u2212 uX)\u2212 \u25e6 d\u22121(X) ) .Y\n(uM + \u25e6 d\u22121(X \u2212 uX)\u2212 \u25e6 d\u22121(X)) .M \u2212 a\n]\n= a\n( \u03c32uM + \u03c3 2 uX P1(\u03c3 2 uX , \u03c32uW )\n\u03c32uM + \u03c3 2 uX P2(\u03c32uX , \u03c3 2 uW )\n) \u2212 a, (18)\nwhere as in previous subsections, we have made use of the uncorrelated-ratio approximation to obtain an asymptotic bias estimate. P1,2 are generic polynomials, and are computed algebraically by Isserlis\u2019 Theorem for higher-order moments. It is clear that this bias approaches 0 as X becomes increasingly correlated with W , yielding:\nProposition 2 In the partial linear CMM (15), Bias [a\u0302RR ]\u2192 0 as \u03c32uX/\u03c3 2 uM \u2192 0.\nProof The result follows from (18).\nThe assumption that X is highly correlated with the latent confounder W is not too strong. Indeed, the fact that X and W are causes of M means that the confounding bias W introduces between M and Y cannot naively be removed using back-door adjustment."
        },
        {
            "heading": "4. Experiments",
            "text": "To empirically test our estimator in linear and partially-linear CMMs, we perform several experiments and measure prediction bias both as a function of confounding and of the noise variances \u03c32 \u2261 {\u03c32uX , \u03c3 2 uM , \u03c32uY , \u03c3 2 uW }. We first test on two synthetic datasets, one with linear data generation functions, and another with nonlinear data generation. To test in a more realistic setting, we create a semi-synthetic experiment using real data from the International Stroke Trial Carolei (1997). Initially, all couplings are assumed linear and are set to 1 unless otherwise specified, and noises assumed zeromean homoscedastic Gaussian, with the exception of \u00b5W = 1. We will then relax the assumption of\nlinearity on d and , and finally relax the assumption of Gaussianity on both W and X by generating semi-synthetic data from the International Stroke Trial dataset Carolei (1997). In all cases, we use the IFDC as baseline.\nRelevant source code and documentation has been made freely available in our online repository."
        },
        {
            "heading": "4.1. Linear synthetic experiments",
            "text": "First, we simulate the CMM and compare the performances of the IFDC and the /d-improved IFDC in estimating a. A 30\u00d7 3 grid over and \u03c32 is specified, and at each point in parameter space, 106 model samples are generated. A sample draw consists of first performing a random Gaussian draw from N (\u00b5, \u03c32) for each noise component uN , where \u00b5 = 1 for N = W and otherwise \u00b5 = 0, and second propagating this data through the structural equations (1) with a = b = c = d = 1. These samples are divided into 100 runs, from which the mean and variance of a\u0302 may be computed for each estimator. The results are shown in Figure 2, with the IFDC shown in the left column and the /d-improved IFDC in the right column. The bias and variances properties for both estimators\nconform to our theoretical expectations. The nonzero bias from (5) is seen in the top left, with bias as for small and as 1/ for large , while vanishingly small variance at this sample quantity is seen in the bottom left. For the improved estimator, the top right plot confirms unbiasedness throughout the -domain except at pole value = 1, as predicted by (12), and reflected in the diverging variance precisely at this value on the bottom right. As mentioned in Section 3, the width of this bias pole can\nbe improved with further samples, or alternatively can be translated by the introduction of a prior instrumental variable to a : X \u2192M"
        },
        {
            "heading": "4.2. Nonlinear synthetic experiments",
            "text": "We now assess our perturbative approach to cubic-order nonlinearities in the coupling functions d and . A 6 \u00d7 5 grid over the quadratic and cubic polynomial coefficients is specified and at each point in parameter space, 105 model samples are generated and divided into 100 runs. We set \u03c32 = 0.3 to ensure convergence, and = 2 to avoid the bias pole for the improved estimator. The results are shown in Figure 3 for cubic-polynomial d and linear , and in Figure 4 for linear d and cubic-polynomial , with the IFDC shown in the left column and the /d-improved IFDC in the right column. For both nonlinear experiments, the > 0.35 bias of the IFDC is drastically\noutperformed by the improved estimator with biases largely of magnitude < 0.05. However, the\nIFDC enjoys significantly more stability against both quadratic and cubic nonlinearities, in fact appearing essentially insensitive to d2 and d3, compared with the improved estimator.\nFor the improved estimators, the dependence on acquired bias on the polynomial coefficients largely agrees with our theoretical analysis in Section 3. Comparing the right plot of Figure 3 with Figure 7, we see confirmation both of the positive bias trend with d2 and of the negative bias trend with d3. There are, however, quantitative differences, where the perturbative approach overpredicts the bias by a factor of 5\u2212 10, suggesting that a more evolved approach than Taylor expansion could be required to fully understand the consequences of nonlinearities in d.\nComparing the right plot of Figure 4 with Figure 8, we again see confirmation both of the weak dependence of bias on e2 and of the signed bias trend with e3. Quantitatively, the match between theory and experiment is much stronger here, confirming the convergence of the polynomial expansion. For large, positive e3, the numerical estimator begins to fail due to large variance, and more samples would be required to resolve this parameter region, but it is clear that beyond e3 \u223c 0.4 the improved estimator bias begins to surpass that of the original IFDC. In general we expect that higher-order nonlinearities would cause the estimator to fail more rapidly, although it is possible it might exceed expectations for specific nonlinear scenarios."
        },
        {
            "heading": "4.3. International Stroke Trial semi-synthetic experiments",
            "text": "To assess the performance of our estimators on more realistic data, we make use of the International Stroke Trial (IST) database (Carolei, 1997), a collection of stroke treatment and 14-day/6-month outcome data for 19, 345 individual patients.\nWe take W = AGE and X = RSBP , the systolic blood pressure at randomisation, both normalized to lie in [0, 1]. We specify linear causal effects for c, a, b, and and construct M and Y by propagation through the structural equations (1) for each IST sample, including Gaussian random noise with variance \u03c32. However, d is not specified as it is manifest in the data with strength and linearity unknown.\nFor simulation, a 20\u00d7 3 grid over and \u03c32 is specified, and at each point in parameter space, 200 runs are generated using the same full set of 19, 345 IST samples, but with independently-sampled noises uM and uY . The bias results are shown in Figure 5, with the IFDC plotted with dashed lines and the /d-improved IFDC with solid. Our improved estimator attains a generic improvement over the original IFDC for all \u2208 [0, 3] and \u03c32 \u2208 [0.1, 1], ranging between 20 \u2212 40% decrease in bias. This application is only a proof of concept, and these positive results indicate that further improvement could likely be achieved by more fully taking account of the non-Gaussianity of X and W and the nonlinearity of d : X \u2192W ."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we studied estimation of long-term treatment effects when both experimental and observational data were available. Specifically, we addressed the case where unmeasured confounders are present in the observational data. Our long-term causal effect estimator was obtained by combining regression residuals with short-term experimental outcomes in a specific manner to create an instrumental variable, which was then used to quantify the long-term causal effect through instrumental variable regression. We initially worked in the linear structural causal model framework, proved this estimator is unbiased, and studied its variance. We then extended this estimator to partially linear structural models and proved unbiasedness still holds under a mild assumption. Finally, we\nempirically tested our long-term causal effect estimator on synthetic data, as well as real data from the International Stroke Trial\u2014demonstrating accurate estimation. Although long-term effect estimation was our primary focus, the estimator and methods described could be applied to any single-stage causal effect with a nonzero-mean confounding variable; we therefore encourage that our results be interpreted within the much broader context of front-door and IV estimation methods."
        },
        {
            "heading": "Acknowledgments",
            "text": "GVG was supported by Spotify and by the STFC UCL Centre for Doctoral Training in Data Intensive Science (grant no. ST/P006736/1), and was funded by the UCL Graduate Research and Overseas Research Scholarships. This research began while GVG was an intern at Spotify. The authors thank Mounia Lalmas for supporting this project, and the anonymous reviewers for their valuable feedback."
        },
        {
            "heading": "Appendix A. Covariance Algebra",
            "text": "In order to extend the derivations in Gupta et al. (2021) to cases with confounded mediators, multiple mediators, and pre-treatment covariates, it is necessary to introduce some new technology. Many key results including bias and variance for FDC-type estimators and covariance between estimators, all necessary to the estimation of the total causal effect, rely on essentially two steps. First, the desired expectation value is expanded using smoothing, also known as the law of total expectation or the tower rule: E[X] = \u2211 x \u2211 y x\u00b7P[X = x, Y = y] = \u2211 y [\u2211 x x \u00b7P[X = x | Y = y] ] \u00b7P[Y = y] = E[E[X | Y ]], (19) where X and Y are random variables (r.v.s) defined on the same probability space, and the expansion may be performed multiple times. In our application, X is replaced by the desired expectation value, and a set of conditioners {Y } are chosen so that the denominator (and as many numerator terms as possible) are fixed under {Y }. These fixed terms simplify by symmetry in some cases, and in more complex cases reduce to known distributions such as the Inverse-Wishart.\nSecond, the unfixed terms must be evaluated. Frequently these are of the form E[u, Y ], where u is some noise r.v. in the causal graph which is neither fixed by Y nor independent from it. Linearity in a Gaussian-noise graphical model implies that any two node or noise r.v.s are bivariate normal, and indeed that any N node or noise r.v.s are N -multivariate normal. This is hugely advantageous, because conditioning acts on a linear projection on a space of multivariate normal r.v.s.\nFor example, suppose X and Y have a bivariate normal distribution:\n(X,Y ) \u223c N ( \u00b5 = ( \u00b5X \u00b5Y ) ,\u03a3 = ( \u03c32X \u03c1\u03c3X\u03c3Y \u03c1\u03c3X\u03c3Y \u03c3 2 Y )) , (20)\nwhere \u03c1 is the correlation between u and Y . Projection implies the following conditional expectations among u and Y :\nE[X | Y ] = \u00b5X + \u03c1 \u03c3X \u03c3Y (Y \u2212 \u00b5Y ), E[Y | X] = \u00b5Y + \u03c1 \u03c3Y \u03c3X (X \u2212 \u00b5X), V[X | Y ] = \u03c32X(1\u2212 \u03c12), V[Y | X] = \u03c32Y (1\u2212 \u03c12). (21)\n(22)\nAs a sanity check, we can see that both variances vanish if \u03c1 = 1, and retain their independent values if \u03c1 = 0. \u03c1 must be evaluated directly, which is straightforward in a linear Gaussian model; for instance, if Y = \u03b1X + U with X \u22a5 U , cov[X,Y ] = \u03b1 \u00b7 cov[X,X] = \u03b1\u03c32X , implying \u03c1 = cov[X,Y ] \u03c3X\u03c3Y\n= \u03b1\u03c3X\u03c3Y . This reproduces the well-known result that the conditional expectation of one of a set of summands on their sum is proportional to the ratio of their variances.\nThe above result is frequently sufficient, however it is too strict for our use case. We will need to be able to compute conditional moments of the form E[ \u220f l ui(l) | Yj ], where the product may include repeated or distinct noises, but the set Yj must be distinct (and sometimes may be reducible). To achieve this, we combine two tools: the general conditional projection for Gaussian families in terms of Schur complements, to easily handle a vector of conditioned r.v.s; and Isserlis\u2019 theorem for higher-order moments to handle arbitrarily complicated products of noises, so long as all r.v.s are zero-mean.\nFollowing Taboga (2021), the multivariate Gaussian conditional moments are: suppose vectorvalued r.v. X is k-multivariate normal with distribution X \u223c N (\u00b5,\u03a3). Then for any partition a+ b = k, where we define\nX = ( Xa Xb ) , \u00b5 = ( \u00b5a \u00b5b ) ,\u03a3 = ( \u03a3a \u03a3 T ab \u03a3ab \u03a3b ) , (23)\nthe vector-valued conditional mean is\nE[Xa | Xb] = \u00b5a + \u03a3Tab\u03a3\u22121b (Xb \u2212 \u00b5b) (24)\nand the matrix-valued conditional variance is\nV[Xa | Xb] = \u03a3a \u2212 \u03a3Tab\u03a3\u22121b \u03a3ab. (25)\nNote that in the above conditional mean, only the bilinear survives if \u00b5a = \u00b5b = 0, as in our applications. Also, the term \u03a3a \u2212 \u03a3Tab\u03a3 \u22121 b \u03a3ab is known as the Schur complement of block \u03a3b in \u03a3. Without needing to rearrange the covariance matrix, V[Xb | Xa] can be found simply by taking the Schur complement of block \u03a3a.\nThe complete partition above is excessive in most cases. If we only desire the expected mean for a single variable Xi \u2208 Xa, for instance, the matrix equation becomes:\nE[Xi | Xb] = [\u00b5a]i + [\u03a3Tab]ij [\u03a3\u22121b ]jk[Xb \u2212 \u00b5b]k (26)\nwhere i, j, k are matrix indices and summations are assumed to be entire. What was a full a \u00d7 a matrix multiplication is now a vector bilinear. Similarly, if we only desire a particular covariance cov[Xi, Xj ] for Xi, Xj \u2208 Xa, the matrix equation becomes:\ncov[Xi, Xj | Xb] = [\u03a3a]ij \u2212 [\u03a3Tab]im[\u03a3\u22121b ]mn[\u03a3ab]nj (27)\nwhere i, j,m, n are matrix indices, and again we have arrived at a vector bilinear."
        },
        {
            "heading": "Appendix B. Proofs of Estimator Biases and Variances",
            "text": "In this section we describe the construction of four residual-based instrumental estimators for a. Two of them will be shown to be unbiased except for some zero-measure choices of structural parameters, which will be characterised.\nB.1. Estimators and their Biases\nFirst let us recall the structural equations for the CM model,\nWi = u W i , Xi = dWi + u X i , Mi = cXi + Wi + u M i , Yi = aMi + bWi + u Y i , (28)\nwhere all variables except the confounder W and noises u are taken to be observable. The motivation\nfor this approach is the observation that noise variable uM , were it measurable, would be an acceptable instrument for a, as shown in Figure 6. The simplest approximation of uM is to regress M on X and take residual Rc as an instrument. As discussed in the text, we could not expect Rc to deliver an unbiased instrumental estimator for a, as the regression of M on X absorbs the backdoor path\nX \u2190W \u2192M . However it is instructive to compute the bias of the estimator by applying the law of total expectation conditioned on X and M :\nBias[a\u0302Rc ] = E[E[a\u0302Rc \u2212 a|X,M ]] = \u2212 b d E [ E [ M.(uX \u2212X)X.X \u2212M.XX.(uX \u2212X) M.MX.X \u2212 (M.X)2 \u2223\u2223\u2223\u2223X,M]] = \u2212 b d E [ M.E[uX |X,M ]X.X \u2212M.XX.E[uX |X,M ] M.MX.X \u2212 (M.X)2\n] = b \u03c32uW \u03c3 2 uX\n2\u03c32uW \u03c3 2 uX + \u03c32uM (\u03c3 2 uX + d2\u03c32uW ) (29)\nIn the first line, the law of total expectation is applied, conditioned on X and M so as to isolate the numerator. In going from the first line to the second line, the independence of uY from X,M has been applied, and in the final expression the conditional expectation E[uX |X,M ] has been calculated as shown in Appendix A. Assuming homoscedasticity of the noise terms, this simplifies to\nBias[a\u0302Rc ] = b\n1 + d2 + 2 . (30)\nOne notable property of this bias is that it vanishes in the limit of both small and large , with global maximum bias of \u00b1 b\u221a\n4+2d2 at = \u00b1 b 2 \u221a 1+d2\nat = \u00b1 \u221a\n1 + d2, as demonstrated in Figure 2. Not all estimators allow for this reduction strategy; in particular, the conditional expectations of the noise terms must combine just such that the denominator is cancelled and the expectation expression becomes that of a scalar. In such cases, we will proceed by estimating the numerator and denominator separately and treating the expectation of the ratio as well-approximated by the ratio of these expectations. For example, a\u0302Rc has the following numerator and denominator expectations, derived by simple independence between noise terms and the fact that E[ui.ui] = \u03c32ui :\nE[X.XM.Y \u2212X.YM.X] = (b+ a )\u03c32uW ( g2\u03c32uV + \u03c3 2 uX ) + a\u03c32uM ( g2\u03c32uV + \u03c3 2 uX + d2\u03c32uW )\n; (31)\nE[X.XM.M \u2212 (X.M)2] = 2\u03c32uW ( g2\u03c32uV + \u03c3 2 uX ) + a\u03c32uM ( g2\u03c32uV + \u03c3 2 uX + d2\u03c32uW ) . (32)\nThe ratio of these expectations, less a, delivers exactly the bias calculated in (29), which tells us that the numerator and denominator r.v.s are independent for this front-door estimator. In general this equivalence will fail due to correlations between the numerator and denominator, but we will assume the correlations to be weak as a useful first approximation.\nWe can now define and analyse the improved residuals which take advantage of prior-stage information about c : X \u2192M and form the key results of this work. First, we take inspiration from the linear structural equations for the confounded mediator model, which suggest that the residual on M after regression on x should have the form uM \u2212 duX . Taking the more general case of a prior instrument g : V \u2192 X in the CM model, we may arrive at this same linear structural quantity by the unique linear combination of residuals between V , X , and M which removes unobserved data W , giving:\nRV = Res[M,X]\u2212 d Res[X,V ] \u223c uM \u2212 d uX (33)\nwhere ratio d is shown in Section 3 to be unbiasedly estimable so long as confounder W acquires some nonzero mean. Subsequently, we construct an instrumental estimator for a:\na\u0302RV = RV .Y\nRV .M =\nV.V V.X(M.Y \u2212 dX.Y ) + (V.X) 2V.Y \u2212 dV.V V.MX.Y\nV.V V.X(M.M \u2212 dX.M) + (V.X)2V.M \u2212 dV.V V.MX.M\n. (34)\nSome simplification is achieved by applying the Law of Total Expectation conditioned over V,X,M , but the result is a nontrivial integral over these three heavily-correlated random vectors (in sample space):\nBias[a\u0302RV ] = E\n[ b\u03c32uW\n\u03c32uM ( d2\u03c32uW + \u03c3 2 uX ) + 2\u03c32uW \u03c3\n2 uX[\nV.V V.X(M.M \u2212 d X.M) + (V.X)2V.M \u2212 d V.V V.MX.M ]\u22121 ( dV.V M.XV.X ( d\u03c32uM \u2212 c \u03c3 2 uX\n) \u2212 V.V V.X ( X.X ( d\u03c32uM \u2212 c \u03c3 2 uX ) + \u03c32uX (dM.M \u2212 X.M)\n) \u2212 dV.V V.M ( X.X ( d\u03c32uM \u2212 c \u03c3 2 uX ) + X.M\u03c32uX\n) + (V.X)3 ( d\u03c32uM \u2212 c \u03c3 2 uX ) + 2V.M\u03c32uX (V.X) 2 )] (35)\nWe do not yet know how to evaluate the above integral, except numerically. Instead, we can evaluate the expectations of the numerator and denominator: E [ V.V V.X(M.Y \u2212\nd X.Y ) + (V.X)2V.Y \u2212 d V.V V.MX.Y\n] = a ( \u03c32uM \u2212 c (g2\u03c32uV + \u03c3 2 uX )\nd\n) ;\n(36) E [ V.V V.X(M.M \u2212\nd X.M) + (V.X)2V.M \u2212 d V.V V.MX.M\n] = \u03c32uM \u2212 c (g2\u03c32uV + \u03c3 2 uX )\nd .\n(37) In contrast to our results on Bias[a\u0302Rc ], the uncorrelated-ratio approximation suggests Bias[a\u0302RV ] ' 0. This only exactly holds if the integral in (35) evaluates to 0, but is promising nonetheless. An intermediate possibility is that (35) approaches 0 as Nsamp \u2192 \u221e, but has a slow dependence on Nsamp.\nIt is worth noting that a\u0302RV could have been constructed another way; naively from the structural equations, Res[M,V ] \u223c W + uM just as Res[M,X] does. We might even expect Res[M,V ] to experience less bias, since V is not confounded by W . However, repeating the above analysis in the uncorrelated-ratio approximation gives a nonzero result,\nBias[a\u0302RV ] ' bcd\u03c32uW\n\u03c32uM + cd(cd+ )\u03c3 2 uW + c(cd\u2212 )d \u03c3 2 uX\n, (38)\nand so we have discarded this route. It is straightforward to simplify estimator a\u0302RV and its corresponding residual to obtain the improved estimator a\u0302RR explored in-depth in the text. One simply sets g = 0 to remove prior\ninstrument V , and redefines the residual with c presumed to be provided from an oracle:\nRR = M \u2212 (c+ d )X \u223c uM . (39)\nImportantly, this construction leaves the door open to joint estimation of c and d from the prior stage in the model, in the sense that only the sum is needed and biases of opposite sign could destructively interfere. The resultant instrumental estimator for a is simple,\na\u0302RR = RR.Y\nRR.M =\nM.Y \u2212 ( c+ d ) X.Y\nM.M \u2212 ( c+ d ) X.M . (40)\nLike a\u0302RV , the full bias is not (yet) reducible beyond a high-dimensional integral,\nBias[a\u0302RR ] = E[ b\u03c32uW \u03c32uM ( d2\u03c32uW + \u03c3 2 uX ) + 2\u03c32uW \u03c3 2 uX\n\u03c32uXM.M + (d\u03c3 2 uM \u2212 (2c+ d)\u03c3 2 uX )X.M + (c+ d)(c \u03c3 2 uX \u2212 d\u03c32uM ) M.M \u2212 ( c+ d ) X.M ] (41)\nbut, also like a\u0302RV , this expectation appears unbiased in the uncorrelated-ratio approximation:\nE [ M.Y \u2212 ( c+\nd\n) X.Y ] = a ( \u03c32uM \u2212 c \u03c32uX d ) ; (42)\nE [ M.M \u2212 ( c+\nd\n) X.M ] = \u03c32uM \u2212 c \u03c32uX d . (43)\nIt is unsurprising that RR is no more biased than RV , and we should expect that evaluation of the integrals in (35) and (41) would show the same or better bias for RR even for finite sample size. In fact, numerical integration of (41) indicates that any nonzero bias terms are proportional to 1/(N+k) for constants k, and therefore asymptotically vanish.\nThere is one crucial difference in the estimation performance of a\u0302RR vs. a\u0302RV , a topological one arising from the presence of prior instrument V . As seen in the uncorrelated-ratio approximation, there are values of d for which the numerator and denominator simultaneously approach 0. Again assuming homoscedasticity of the noise terms for simplicity, this bias pole occurs at d = 1 c for a\u0302RR and at d = 1 c(g2+1) for a\u0302RV . For finite sample sizes, one expects that each pole will be centered in a region of finite width where the estimator performs poorly, but that this bias will contract to a delta function as Nsamp \u2192 \u221e. These poles are connected in the limit as g \u2192 0, although a\u0302RV is not defined at g = 0.\nThe practical consequence of the above analyses is that two instrumental estimators of a, constructed from the /d-improved residual and from the remainder, are essentially unbiased. They each have a pole region of slowly-converging bias, however given sufficiently large g, these regions can be well-separated. In the presence of a prior instrument g, it is therefore possible to construct an unbiased estimator for a throughout ( , d) parameter space. It is for this reason that we illustrate both estimation strategies in full despite their obvious similarities.\nB.2. Variances\nWe refer first to the variance computations in Gupta et al. (2021), where finite-sample and asymptotic variances for c\u0302 and a\u0302 are calculated taking advantage of the asymptotic normality of OLS estimators, and the properties of inverse-Wishart-distributed matrices. For the front-door estimator, the asymptotic variances are quoted as follows:\nV\u221e(a\u0302FDC) = b2\u03c32uw\u03c3 2 ux + \u03c3 2 uy(d 2\u03c32uw + \u03c3 2 ux)\n(d2\u03c32uw + \u03c3 2 ux)\u03c3 2 um\n, (44)\nV\u221e(c\u0302) = \u03c32um\nd2\u03c32uw + \u03c3 2 ux\n. (45)\nVia the Delta method, the asymptotic variance in estimating the total causal effect ac is given by\nV\u221e(a\u0302c) = c 2V\u221e(a\u0302) + a 2V\u221e(c\u0302), (46)\nwhich holds as long as Cov(a\u0302, c\u0302) = 0. Following Corradi; Cameron, the asymptotic variance for a scalar instrumental estimator a\u0302IV = R.Y R.M may be written\nV\u221e(a\u0302R) = E [(R.R) \u00b7 E[u\u0303Y .u\u0303Y |R]]\nCov(R,M)2 (47)\nwhere u\u0303Y denotes all additive contributions to Y besides aM , and we have taken instrument R to have zero mean. Following our claim that the instrumental estimator built from Rc with no confounding on the mediator ( = 0) is simply the FDC estimator, it is instructive to confirm that the IV asymptotic variance agrees with the FDC result from Gupta et al. (2021).\nFor all causal structural models we consider, u\u0303Y = uY +b\u00b7uW . In the = 0 case, no confounding implies Rc \u22a5 u\u0303Y , so that E [(R.R) \u00b7 E[u\u0303Y .u\u0303Y |R]] = E [R.R] \u00b7 E[u\u0303Y .u\u0303Y |Rc]. Further computing E[Rc.Rc] = E[Rc.M ] = \u03c32uM , and evaluating E[u\u0303Y .u\u0303Y |Rc] algebraically via the covariance matrix approach, we arrive at:\nV\u221e(a\u0302Rc, =0) = \u03c32uM \u00b7 (\u03c3 2 uY + b2E[uW .uW |Rc]) (\u03c32uM ) 2 = V\u221e(a\u0302FDC). (48)\nWhen the mediator is permitted to experience some confounding , we should expect some correlation between Rc.Rc and u\u0303Y .u\u0303Y via uW . Separating this term from the product in the numerator, and observing that E[Rc.Rc] = E[Rc.M ] = E[M.MX.X\u2212(M.X) 2\nX.X ], we find\nV\u221e(a\u0302Rc) = \u03c32uY + b 2E[uW .uW |Rc] E[M.MX.X\u2212(M.X) 2 X.X ] +\nO(\u03c34uW )\nE[M.MX.X\u2212(M.X) 2\nX.X ] 2\n(49)\nwhere the quantity in the denominator has the distribution of the marginal from a Wishart-distributed matrix, as the quantity 1D in Gupta et al. (2021). It is possible to simplify this denominator expectation directly to only one non-trivial integral,\nE[Rc.Rc] = \u03c32uM \u00b7 N\nN + 2 + 2\u03c32uW \u2212\n2E[ E[(uW .X)2|X]\nX.X ], (50)\nwhere the final expectation value would reduce to \u03c32uW \u00b7 1 N+2 were uW \u22a5 X , but numerical evaluation via cylindrical coordinates has confirmed that it approaches asymptotic \u03c32uW with strong correlation between uW and X . Thus E[Rc.Rc] is bounded both above and below, with the overall V\u221e(a\u0302Rc) slowly worsening as correlation between uW and X becomes stronger.\nIf we assume that c has been learned through previous experimentation, and that low-variance, unbiased estimation of d has been attained, it is possible to obtain an exact variance result for the /d-improved IFDC estimator. Since Rc = uM \u2212 duX , E[Rc.M ] = \u03c3 2 uM \u2212 c d \u03c3 2 uX\nand E[Rc.Rc] = \u03c32uM + 2 d2 \u03c32uX . Thus,\nV\u221e(a\u0302RR) = b2\u03c32uW \u03c3 2 uX + \u03c32uY (d 2\u03c32uW + \u03c3 2 uX )\n(d2\u03c32uw + \u03c3 2 uX\n) \u00b7 \u03c32uM +\n2\nd2 \u03c32uX\n(\u03c32uM \u2212 c d \u03c3 2 uX\n)2 , (51)\nwhich has the expected property that as \u2192 0, V\u221e(a\u0302RR) \u2192 V\u221e(a\u0302FDC), but with asymptotic confounding \u2192 \u221e, V\u221e(a\u0302RR) \u2192 V\u221e(a\u0302FDC) \u00b7\n\u03c32uM c2\u03c32uX . The variance expression only becomes\nunbounded at the pole d = 1 c , just as expected from our computation of the bias."
        },
        {
            "heading": "Appendix C. Nonlinear Bias Examples",
            "text": "As two practical examples, we demonstrate the computed /d-improved IFDC biases for cubicpolynomial d and linear , and for linear d and cubic-polynomial . Specifically,\nd(W ) = d1W + d2W 2 + d3W 3, (52)\nin which case the invertibility condition simplifies to \u2212 \u221a 3d1d3 \u2264 d2 \u2264 \u221a\n3d1d3, which may only be fulfilled if d1 and d3 have the same (or 0) sign. Setting all variances and b = c = d1 = 1 = 1 for simplicity, we find\nBias [a\u0302RR,nd=3] = 6(2d22 \u2212 d3)(1 + 3d3)\n1 + 72d42 \u2212 30d3 \u2212 108d23 \u2212 180d33 + 18d22(3 + 10d3 + 20d23) , (53)\nBias [a\u0302RR,n =3] = 3 3\n22 + 12 3 + 9 2 3\n. (54)\nVarying the cubic coefficient and plotting curves over the quadratic coefficient, theoretical bias estimates for these two scenarios are presented in Figures 7 and 8, respectively. We have set all noise variances to \u03c32 = 0.2 for these computations, in order to more clearly show trends and to assure convergence. For Figure 7, we have taken terms of d\u22121 up to order m = 10 to demonstrate that at this order in the expansion, the prediction still varies substantially; it is \u201cnon-perturbative\u201d, and so even to order 10 should only be taken as a qualitative estimate. The convergence up to order 3 in Figure 8, however, is taken to be sufficiently precise. To summarise these results, up to non-perturbative effects we expect that nonzero d2 pushes the bias in the positive direction, while nonzero d3 (restricted to be positive by invertibility) pushes the bias in the negative direction. Coordinates in (d2, d3) where unbiasedness is retained or nearly retained should therefore exist. In contrast, nonzero 2 appears to have a much smaller impact on bias, in fact tending towards 0, while nonzero e3 leads to bias in the direction of sign( 3). It is noteworthy that for positive 3, the bias is small and tentatively approaches an asymptote around 0.1, while for negative 3, bias grows rapidly and appears unbounded."
        }
    ],
    "title": "Estimating long-term causal effects from short-term experiments and long-term observational data with unobserved confounding",
    "year": 2023
}