{
    "abstractText": "Several high-resource Text to Speech (TTS) systems currently produce natural, well-established human-like speech. In contrast, low-resource languages, including Arabic, have very limited TTS systems due to the lack of resources. We propose a fully unsupervised method for building TTS, including automatic data selection and pre-training/fine-tuning strategies for TTS training, using broadcast news as a case study. We show how careful selection of data, yet smaller amounts, can improve the efficiency of TTS system in generating more natural speech than a system trained on a bigger dataset. We adopt to propose different approaches for the: 1) data: we applied automatic annotations using DNSMOS, automatic vowelization, and automatic speech recognition (ASR) for fixing transcriptions\u2019 errors; 2) model: we used transfer learning from high-resource language in TTS model and fine-tuned it with one hour broadcast recording then we used this model to guide a FastSpeech2-based Conformer model for duration. Our objective evaluation shows 3.9% character error rate (CER), while the ground truth has 1.3% CER. As for the subjective evaluation, where 1 is bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a mean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness, where many annotators recognized the voice of the broadcaster, which proves the effectiveness of our proposed unsupervised method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tomoki Hayashi"
        },
        {
            "affiliations": [],
            "name": "Hamdy Mubarak"
        },
        {
            "affiliations": [],
            "name": "Soumi Maiti"
        },
        {
            "affiliations": [],
            "name": "Shinji Watanabe"
        },
        {
            "affiliations": [],
            "name": "Wassim El-Hajj"
        },
        {
            "affiliations": [],
            "name": "Ahmed Ali"
        }
    ],
    "id": "SP:58634ad242e5f77896c909a5cce2621dfab11936",
    "references": [
        {
            "authors": [
                "H. Zen",
                "V. Dang",
                "R. Clark",
                "Y. Zhang",
                "Ron J Weiss",
                "Y. Jia",
                "Z. Chen",
                "Y. Wu"
            ],
            "title": "LibriTTS: A corpus derived from Librispeech for text-to-speech",
            "venue": "Proc. Interspeech 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Ren",
                "Y. Ruan",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T. Liu"
            ],
            "title": "Fastspeech: Fast, robust and controllable text to speech",
            "venue": "Advances in neural information processing systems, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Tachibana",
                "K. Uenoyama",
                "S. Aihara"
            ],
            "title": "Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention",
            "venue": "ICASSP, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Je. Donahue",
                "S. Dieleman",
                "M. Bi\u0144kowski",
                "E. Elsen",
                "K. Simonyan"
            ],
            "title": "End-to-end adversarial text-to-speech",
            "venue": "International Conference on Learning Representations, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "R. Skerry-Ryan",
                "D. Stanton",
                "Y. Wu",
                "R. Weiss",
                "N. Jaitly",
                "Z. Yang",
                "Y. Xiao",
                "Z. Chen",
                "S. Bengio"
            ],
            "title": "Tacotron: Towards end-to-end speech synthesis",
            "venue": "Proc. Interspeech 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T. Liu"
            ],
            "title": "Fastspeech 2: Fast and high-quality end-to-end text-to-speech",
            "venue": "International Conference on Learning Representations, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Sonobe",
                "S. Takamichi",
                "H. Saruwatari"
            ],
            "title": "JSUT corpus: free large-scale japanese speech corpus for end-to-end speech synthesis",
            "venue": "arXiv:1711.00354, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Hayashi",
                "R. Yamamoto",
                "K. Inoue",
                "T. Yoshimura",
                "S. Watanabe",
                "T. Toda",
                "K. Takeda",
                "Y. Zhang",
                "X. Tan"
            ],
            "title": "Espnet-tts: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit",
            "venue": "ICASSP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Tu",
                "Y. Chen",
                "C. Yeh",
                "H. Lee"
            ],
            "title": "End-to-end text-to-speech for low-resource languages by cross-lingual transfer learning",
            "venue": "Proc. Interspeech 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Chung",
                "Y. Wang",
                "W. Hsu",
                "Y. Zhang",
                "R. Skerry-Ryan"
            ],
            "title": "Semi-supervised training for improving data efficiency in end-to-end speech synthesis",
            "venue": "ICASSP, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Xu",
                "X. Tan",
                "Y. Ren",
                "T. Qin",
                "J. Li",
                "S. Zhao",
                "T.Y. Liu"
            ],
            "title": "Lrspeech: Extremely low-resource speech synthesis and recognition",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Lin"
            ],
            "title": "Unsupervised learning for sequenceto-sequence text-to-speech for low-resource languages",
            "venue": "Proc. Interspeech 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Gallegos",
                "J. Williams",
                "J. Rownicka",
                "S. King"
            ],
            "title": "An unsupervised method to select a speaker subset from large multispeaker speech synthesis datasets",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Mubarak",
                "A. Hussein",
                "S. Chowdhury",
                "A. Ali"
            ],
            "title": "QASR: QCRI Aljazeera speech resource\u2013a large scale annotated Arabic speech corpus",
            "venue": "ACL, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Bell",
                "M. Gales",
                "T. Hain",
                "J. Kilgour",
                "P. Lanchantin",
                "X. Liu",
                "A. McParland",
                "S. Renals",
                "Os. Saz",
                "M. Wester"
            ],
            "title": "The MGB challenge: Evaluating multi-genre Broadcast media recognition",
            "venue": "ASRU, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Ali",
                "P. Bell",
                "J. Glass",
                "Y. Messaoui",
                "H. Mubarak",
                "S. Renals",
                "Y. Zhang"
            ],
            "title": "The MGB-2 challenge: Arabic multi-dialect broadcast media recognition",
            "venue": "SLT, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C.K. Reddy",
                "V. Gopal",
                "R. Cutler"
            ],
            "title": "Dnsmos: A nonintrusive perceptual objective speech quality metric to evaluate noise suppressors",
            "venue": "ICASSP 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Lo",
                "S. Fu",
                "W. Huang",
                "X. Wang",
                "J. Yamagishi",
                "Y. Tsao",
                "H. Wang"
            ],
            "title": "Mosnet: Deep learning based objective assessment for voice conversion",
            "venue": "Interspeech 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Andreev",
                "A. Alanov",
                "O. Ivanov",
                "D. Vetrov"
            ],
            "title": "Hifi++: a unified framework for neural vocoding, bandwidth extension and speech enhancement",
            "venue": "arXiv:2203.13086, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Naderi",
                "R. Cutler"
            ],
            "title": "An open source implementation of itu-t recommendation p. 808 with validation",
            "venue": "Proc. Interspeech 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C.K. Reddy",
                "V. Gopal",
                "R. Cutler",
                "E. Beyrami",
                "R. Cheng",
                "H. Dubey",
                "S. Matusevych",
                "R. Aichner",
                "A. Aazami",
                "S. Braun"
            ],
            "title": "The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results",
            "venue": "Proc. Interspeech 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Hussein",
                "S. Watanabe",
                "A. Ali"
            ],
            "title": "Arabic speech recognition by end-to-end, modular systems and human",
            "venue": "Computer Speech & Language, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Abdelali",
                "K. Darwish",
                "N. Durrani",
                "H. Mubarak"
            ],
            "title": "Farasa: A fast and furious segmenter for Arabic",
            "venue": "The North American chapter of the association for computational linguistics: Demonstrations, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Shen",
                "R. Pang",
                "R. Weiss",
                "M. Schuster",
                "N. Jaitly",
                "Z. Yang",
                "Z. Chen",
                "Y. Zhang",
                "Y. Wang",
                "R. Skerrv-Ryan"
            ],
            "title": "Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions",
            "venue": "ICASSP, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Li",
                "S. Liu",
                "Y. Liu",
                "S. Zhao",
                "M. Liu"
            ],
            "title": "Neural speech synthesis with transformer network",
            "venue": "AAAI, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Chorowski",
                "D. Bahdanau",
                "D. Serdyuk",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Attention-based models for speech recognition",
            "venue": "NIPS, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. Ito",
                "L. Johnson"
            ],
            "title": "The lj speech dataset",
            "venue": "Online: https://keithito. com/LJ-Speech-Dataset, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "P. Guo",
                "F. Boyer",
                "X. Chang",
                "T. Hayashi",
                "Y. Higuchi",
                "H. Inaguma",
                "N. Kamo",
                "C. Li",
                "D. Garcia-Romero",
                "J. Shi"
            ],
            "title": "Recent developments on ESPnet toolkit boosted by conformer",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. \u0141a\u0144cucki"
            ],
            "title": "Fastpitch: Parallel text-to-speech with pitch prediction",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Perraudin",
                "P. Balazs",
                "P.L. S\u00f8ndergaard"
            ],
            "title": "A fast griffinlim algorithm",
            "venue": "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics.",
            "year": 2013
        },
        {
            "authors": [
                "R. Yamamoto",
                "E. Song",
                "J. Kim"
            ],
            "title": "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
            "venue": "ICASSP, 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 1.\n09 09\n9v 2\n[ cs\n.C L\n] 2\n6 Ja\nduce natural, well-established human-like speech. In contrast, low-resource languages, including Arabic, have very limited TTS systems due to the lack of resources. We propose a fully unsupervised method for building TTS, including automatic data selection and pre-training/fine-tuning strategies for TTS training, using broadcast news as a case study. We show how careful selection of data, yet smaller amounts, can improve the efficiency of TTS system in generating more natural speech than a system trained on a bigger dataset. We adopt to propose different approaches for the: 1) data: we applied automatic annotations using DNSMOS, automatic vowelization, and automatic speech recognition (ASR) for fixing transcriptions\u2019 errors; 2) model: we used transfer learning from high-resource language in TTS model and fine-tuned it with one hour broadcast recording then we used this model to guide a FastSpeech2-based Conformer model for duration. Our objective evaluation shows 3.9% character error rate (CER), while the ground truth has 1.3% CER. As for the subjective evaluation, where 1 is bad and 5 is excellent, our FastSpeech2-based Conformer model achieved a mean opinion score (MOS) of 4.4 for intelligibility and 4.2 for naturalness, where many annotators recognized the voice of the broadcaster, which proves the effectiveness of our proposed unsupervised method.\nIndex Terms\u2014 Text-to-Speech, TTS, low resources, speech recognition"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Great progress in deep learning and neural end-to-end approaches have lowered the barrier to build high quality TTS systems [1, 2, 3]. However, training end-to-end TTS systems requires a large amount of high-quality paired text and audio, which are expensive and time consuming to build. Most of the recent TTS systems require sizable amounts of data; More than 260 hours by professional voice actors [4], 25 hours [5], 585 hours constructed from 2, 456 speakers [1] and 24 hours [2, 6]. Unlike the data collection in building ASR systems, TTS needs careful recording control, since the TTS performance is tied to professional speaker and high-quality sizable recordings. Thus where high-resource languages such as English, Japanese, and Mandarin [1, 7, 8] has reached almost human-like performance in TTS, limited attention is given to under-resource languages [9] or resource efficiency of TTS [10].\nIn [10], they proposed to use semi-supervised training framework by allowing Tacotron to utilize textual and acoustic knowledge contained in large corpora. They condition the Tacotron encoder by embedding input text into word vectors. The Tacotron decoder was then pre-trained in the acoustic domain using an unpaired speech corpus. In [9], authors introduce cross-lingual transfer learning for low-resource languages to build end-to-end TTS. To tackle input space mismatch across languages, authors proposed a phonetic transformation network model by mapping between source and target linguistic symbols according to their pronunciation. Another approach for cross-lingual transfer learning presented by [11] initializes the phoneme embeddings from scratch for low-resource languages and discards the pre-trained phoneme embeddings. There has been some work on data efficiency in TTS, researchers in [12] used vector-quantization variational-Autoencoder to extract the unsupervised linguistic units from large untranscribed speech then finetune it on labeled speech. Others [13] described a new unsupervised speaker selection method based on clustering per-speaker acoustic representations.\nIn this work, we describe a fully unsupervised framework to build a low-resource TTS system in Arabic using broadcast news recordings. This is an interesting case study for low-resource TTS as we use Arabic Aljazeera Broadcast news data from QASR corpus [14] as our data source. Broadcast news recordings are widely available for various languages and possibly for large number of hours [15, 16]. Though such broadcast news data is not very highquality as TTS studio recordings.\nIn this paper, we utilize a different data-selection strategy, text processing and speech synthesis method. First, in data-selection we show that with expert human annotations we can select 1-hour subset of high-quality data. We also show with automatic annotations using DNSMOS [17] we can also achieve very close performance to expert labelled data, character error rate (CER) of 4.0% compared to CER of 3.9% with manual labels. Second, in text-processing, we show that using vowelization we can reduce CER from 32.2% to 3.9%. Finally, in speech synthesis we show that using transfer learning from high-resource language in TTS model and using this autoregressive (AR) TTS model to guide a non-autoregressive (nonAR) TTS model for duration, we can achieve naturalness score of 4.2 and intelligibility score of 4.4 using only 1 hour data. The reproducible recipe have been shared with the community through the ESPnet-TTS project 1.\n1 https://github.com/espnet/espnet/tree/master/egs2/qasr_tts/tts1"
        },
        {
            "heading": "2. BUILDING TTS CORPUS FROM NEWS RECORDING",
            "text": "As per our knowledge there is no standard TTS corpus for our lowresource language Arabic. Hence, we use the MGB-2 corpus [16] obtained from the Aljazeera Broadcast news channel, this data was initially optimized for ASR. Collected data spans over 11 years from 2004 until 2015. It contains more than 4,000 episodes from 19 different programs covering different domains like politics, society, economy, sports, science, etc. For each episode, Aljazeera provided the following: (i) audio sampled at 16KHz; (ii) manual transcription; and (iii) metadata for most of the recordings. Metadata contains following information: program name, episode title and date, speaker names and topics of the episode. Also no alignment information is provided with the textual transcriptions. The quality of the transcription varied significantly; the most challenge is conversational programs with overlapping speech and dialectal usage; and finally metadata is not always available or standardized."
        },
        {
            "heading": "2.1. Speaker Identification and Speaker Linking",
            "text": "Majority of metadata information appears in the beginning of the file. However, some of them are embedded inside the episode transcription. One of the main challenges is the inconsistency in speaker names, e.g. Barack Obama appeared in 9 different forms (Barack Obama, Barack Obama/the US President, Barack Obama/President of USA,etc.). The list of guest speakers and episode topics are not comprehensive, with many spelling mistakes in the majority of metadata field names and attributes. To overcome these challenges, we applied several iterations of automatic parsing and extraction followed by manual verification and standardization. This data is publicly available 2."
        },
        {
            "heading": "2.2. Data Selection for TTS Corpus",
            "text": "Our next step in building the TTS corpus is selecting anchor speakers from the MGB-2 dataset. We picked recordings from two anchors as our main speakers, one male and one female. Intuitively, anchor speakers will be speaking clearly, and most of the time, their recordings will be done in a high-quality studio with less noisy environment. Unlike an interviewee over the phone or an audio reporter covering events outside the studio. By investigating some recordings, we can identify the following six classes and only the last one is our desired data:\n\u2022 Background music normally happens at the beginning or at\nthe end of each episode;\n\u2022 Wrong transcription often happens due to the fact that broad-\ncast human transcription is not verbatim, e.g. sometimes some spoken words were dropped due to repetition, correction or rephrasing;Manual dataset classification\n\u2022 Overlap speech occurs mainly in debates and talk-shows; \u2022 Wrong speaker label when there is a problem in the meta-data\n(speaker linking results);\n\u2022 Bad recording quality happens when the anchor speaker re-\nports from a noisy environment, far-field microphone, or calling over the phone..etc; and\n\u2022 Good recording, none of the above."
        },
        {
            "heading": "2.2.1. Manual dataset classification",
            "text": "To classify the good recordings we hired a professional linguist to listen to all segments from the selected best two anchor speakers and\n2https://arabicspeech.org/qasr_tts\nclassify them into these aforementioned six-classes. The biggest two challenges are bad recordings (40%) and overlapped speech (33%). Out of this task, we were able to manually label about one hour per speaker, which we studied for building a TTS system. We ran experiments on these two different datasets mainly to assess the quality of the proposed recipe in the experimental part."
        },
        {
            "heading": "2.2.2. Automatic dataset classification",
            "text": "The second approach we investigated to choose the good samples is automatic data classification because manual labelling is time consuming. We use three different methods MOSNet, wv-MOS, and DNSMOS.\n\u2022 MOSNet [18] is a deep learning-based assessment model to\npredict human ratings of converted speech. They adopted the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor. \u2022 wv-MOS [19] Due to the limitations of MOSNet\u2019s ability to\ndetect obvious distortions they trained a modern neural network model wav2vec2.0 on the same data as MOSNet. They found that the wv-MOS was an effective predictor of subjective speech quality. \u2022 DNSMOS [17] is a Convolutional Neural Network (CNN)\nbased model that is trained using the ground truth human ratings obtained using ITU-T P.808 [20, 21]. It can be applied to stack rank various Deep Noise Suppression (DNS) methods based on MOS estimates with great accuracy.\nOur input to the proposed methods are the wav files and the output is MOS score ranging from 1 to 5, with lowest score of 1 and highest score of 5. We considered all the MOS scores that are above 4 as excellent samples. In Table 1 we shows various challenges in random 10 hours for the selected two anchor speakers with manual selection. Additionally, we present the labeled good segments from the automatic selection. We investigated the MOS score prediction for each approach by analyzing the common good samples chosen by the manual annotators. We found that DNSMOS & wvMOS achieved high correlation with human annotators ratings for the good segments. However, such MOS rating evaluations solely took into account the quality of the speech, regardless of any transcription errors. In order to consider the good samples with wrong transcriptions and achieve our goal of building a fully unsupervised TTS, we used the Arabic ASR in [22] to fix the spelling mistakes."
        },
        {
            "heading": "2.2.3. Vowelized Text",
            "text": "Our text was without vowelization (aka diacritization) which makes the problem very challenging. In order to diacritize a text, a complex system that considers linguistic rules and statistics is needed [23].\nFurthermore, there are differences between diacritizing a transcribed speech and a normal written text such as correction, hesitation, and repetition. That further needs more attention from the current text diacritizers. Additionally, the diacritized text should match the speakers\u2019 actual pronunciation of words even if they are not grammatically correct. In the experimental part, we will see the impact of the vowelization on affecting the quality of the produced results."
        },
        {
            "heading": "3. ARCHITECTURE",
            "text": "In this section, we will introduce the architecture of text-to-mel models and vocoder models used in the evaluation. We employed AR and non-AR text-to-mel models to compare the performance with a small amount of training data."
        },
        {
            "heading": "3.1. Autoregressive models",
            "text": "We trained Tacotron2 [24] and Transformer-TTS [25] as our AR text-to-mel model. Tacotron2, is a recurrent neural network (RNN)based sequence-to-sequence model. It consists of a bi-directional Long short-term memory (LSTM)-based encoder and a unidirectional LSTM-based decoder with location sensitive attention [26]. After the decoder, the convolutional PostNet refines the predicted sequence by predicting a residual component of the target sequence. Transformer-TTS adopts a multi-headed self-attention mechanism by replacing the RNNs with the parallelizable self-attention structure. This enables faster and more efficient training while maintaining the high perceptual quality comparable to the Tacotron2 [25]. We additionally used the guided attention loss [3] to help the learning of diagonal attention weights for both AR models.\nSince our training data was limited, it was challenging for the model to learn the alignment between the input sequence and the target sequence from one hour of speech. To address this issue, at first, we performed pre-training on the LJSpeech dataset [27], which consists of 24 hours of single-female English speech. After pretraining, we fine-tuned the pre-trained model using a small amount of Arabic training data. We initialized the network parameters except for the token embedding layer because of the mismatch in languages."
        },
        {
            "heading": "3.2. Non-autoregressive models",
            "text": "We trained FastSpeech2 [6] based on Conformer [28] as our non-AR text-to-mel model, which uses the Conformer block instead of the Transformer block. It consists of a Conformer-based encoder, a duration predictor, a pitch predictor, an energy predictor, a length regulator, a Conformer-based decoder, and PostNet. The duration predictor is a convolutional network that predicts the duration of each input token from the hidden representation of the encoder. The pitch and energy predictors are convolutional networks that predict pitch and energy sequences from the encoder hidden representation, respectively. Instead of pitch and energy sequences of the target speech, token-averaged sequences are used to avoid over-fitting [29]. Their embedding is added to the encoder hidden representation. The length regulator replicates each frame of the encoder hidden representation using each input token\u2019s duration to match the time-resolution with the target mel-spectrogram. Finally, the decoder converts the upsampled hidden representation into the target mel-spectrogram, and PostNet refines it.\nWe did not perform pre-training for FastSpeech2 as we found that the non-AR model does not require a large amount of training data. As a result we train it from scratch with much smaller training data compared to the AR model."
        },
        {
            "heading": "3.3. Synthesis",
            "text": "We used the Griffin\u2013Lim algorithm (GL) [30] and Parallel WaveGAN (PWG) [31] to generate speech from the predicted melspectrogram. In the case of GL, the sequence of the predicted mel-spectrogram was converted to a linear spectrogram with the inverse mel-basis, and then GL was applied to the spectrogram. In the case of PWG, we trained the model from scratch with the same training data as the mel-spectrogram generation networks. We used the ground-truth of mel-spectrogram in training while using the predicted one in inference."
        },
        {
            "heading": "4. EXPERIMENTS",
            "text": "We conducted experiments to test our proposed methodologies with LJSpeech English 24-hours of speech from single speaker, then finetuned with one hour in Arabic. The audio quality is evaluated by human testers. One of the common problems of E2E-TTS is that the generated speech sometimes includes the deletion and/or repetition of words in the input text due to alignment errors. To address this issue, we evaluated the word error rate (WER) and CER of generated speech using pre-trained ASR models and then automatically detected the deletion and/or repetition of words. We also experimented with the automatic data selection, as discussed in Section 2.2.2 to construct the TTS corpus."
        },
        {
            "heading": "4.1. Experimental Conditions",
            "text": "Our training process requires first training the text-to-mel prediction network on its own, followed by training a PWG independently on the outputs generated by the first network. We investigated the performance with the combinations of the following three conditions:\nModel architecture: To check the difference among the model architectures, we compared three architectures: Tacotron2, Transformer-TTS, and Fastspeech2, as described in Section 3. The AR models were pre-trained 3 as the character-based model with the LJSpeech dataset. We used 12,600 utterances for the training and 250 utterances for the validation in pre-training. After pre-training, we fine-tuned the model using Arabic corpus as describe in Section 2. We used 25 utterances for development and 25 for testing and the rest for training. For non-AR models, we extracted the groundtruth duration of each input token from the attention weights of the teacher model with teacher forced prediction [2]. We employed both Tacotron2 and Transformer-TTS as the teacher model and compared the performance. Then, we trained the non-AR model from scratch using only Arabic data, with the same split of data as the AR models.\nVowelization: Vowelization of the input transcription is important to solve the mismatch between the input text and the output pronunciation. We performed the vowelization, as described in section 2.2.3. We compared the case of w/ and w/o vowelization to check the effectiveness.\nReduction factor: The reduction factor [5] is a common parameter of text-to-mel models to decide the number of output frames at each time step. This plays an important role in achieving stable training, especially when using a limited amount of training data. We compared the reduction factor 1 and 3 for each condition.\n3https://zenodo.org/record/4925105"
        },
        {
            "heading": "4.2. Results and Analysis",
            "text": ""
        },
        {
            "heading": "4.2.1. Objective Evaluation",
            "text": "For objective evaluation, we reported the Mel-Cepstral Distortion (MCD) [dB]. We also reported Word Error Rate (WER) and CER on a single speaker using the large vocabulary speech recognition Arabic end-to-end transformer system in [22].\nThe objective evaluation in Table 2 shows that vowelization improved results considerably. Table 3 shows the impact of vowelization for the FastSpeech2 models. The generated speech in the unvowelized text compared to the vowelized text includes a lot of deletion of characters in the input text. Usually, the model with a larger reduction factor achieves faster training convergence and easier attention alignment. However, as we can see from the results produced that it reduces the quality of the predicted frames. 1V and 3V showed reasonable performance in this particular task, but the 5V and 7V outperformed the two models even without the pre-training. This result indicated that the amount of training data required for non-AR models is much smaller than AR models.\nWe notice that WER for the ground-truth is very low compared to any other representative test set. This can be owed to the fact that anchor speaker speaks clearly and recording setup is very good.\nTo check the quality of the automatic labeling done by MOSNet, wv-MOS, and DNSMOS (Section 2.2.2) vs. manual labeling provided by the annotators, we trained a FastSpeech2 model with the fine-tuned Transformers as a teacher model. Table 4 shows a comparison between the efficiency of the four models trained on different samples, which shows that the manual labeling slightly outperformed the automatic labeling. However the DNSMOS shows a very promising results as its slightly worse than the manual selection.\nFor checking the efficiency of the data selection and the performance of the mechanism presented, we trained a FastSpeech2 model with the fine-tuned Tacotron2 as a teacher model for both the male and the female speaker dataset. Table 5 shows a comparison between the results of each dataset using the same model. This shows that the male speaker dataset has higher quality."
        },
        {
            "heading": "4.2.2. Subjective Evaluation",
            "text": "Finally, we conducted the subjective evaluation using MOS on naturalness and intelligibility. We compared the following four models trained on the male dataset: 7V, 7, 8V, and 7V with PWG. For a reference, we also used ground-truth samples in the evaluation. We used 100 sentences of evaluation data for the subjective evaluation. Each subject evaluated 100 samples of each model (in total 400 samples) and rated the intelligibility and naturalness of each sample on a 5-point scale: 5 for excellent, 4 for good, 3 for fair, 2 for poor, and 1 for bad. The number of subjects was ten.\nTable 6 shows that with reduction factor 1, vowelization significantly improved both intelligibility and naturalness, revealing the effectiveness of vowelization. We can confirm that a larger reduction factor did not improve the performance, although it accelerated the training speed. Finally, the result with PWG showed that applying the neural vocoder brought a significant improvement of the naturalness and the intelligibility. We can also conclude from the high results presented in the ground-truth that people are very familiar with the tone of the speakers in the broadcast news."
        },
        {
            "heading": "5. CONCLUSION",
            "text": "This paper introduced a method for building TTS in an unsupervised way, including data selection and pre-training/fine-tuning strategies for TTS training, using Arabic broadcast news as a case study. We explained the pipeline that involves data collection, adaptation of AR models, training of non-AR models, and training PWG. Our approach resulted in MOS of 4.4/5 for intelligibility (almost excellent) and 4.2/5 for naturalness (almost very good). For future work, we plan to automate the process for all the speakers in the QASR [14] corpus. Finally, we also consider applying our pipeline on another language, such as the BBC in the MGB-1."
        },
        {
            "heading": "6. REFERENCES",
            "text": "[1] H. Zen, V. Dang, R. Clark, Y. Zhang, Ron J Weiss, Y. Jia,\nZ. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from Librispeech for text-to-speech,\u201d Proc. Interspeech 2019.\n[2] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu,\n\u201cFastspeech: Fast, robust and controllable text to speech,\u201d Advances in neural information processing systems, 2019.\n[3] H. Tachibana, K. Uenoyama, and S. Aihara, \u201cEfficiently train-\nable text-to-speech system based on deep convolutional networks with guided attention,\u201d in ICASSP, 2018.\n[4] Je. Donahue, S. Dieleman, M. Bin\u0301kowski, E. Elsen, and K. Si-\nmonyan, \u201cEnd-to-end adversarial text-to-speech,\u201d International Conference on Learning Representations, 2020.\n[5] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. Weiss,\nN. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al., \u201cTacotron: Towards end-to-end speech synthesis,\u201d Proc. Interspeech 2017.\n[6] Y. Ren, C. Hu, T. Qin, S. Zhao, Z. Zhao, and T. Liu, \u201cFast-\nspeech 2: Fast and high-quality end-to-end text-to-speech,\u201d International Conference on Learning Representations, 2020.\n[7] R. Sonobe, S. Takamichi, and H. Saruwatari, \u201cJSUT corpus:\nfree large-scale japanese speech corpus for end-to-end speech synthesis,\u201d arXiv:1711.00354, 2017.\n[8] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watan-\nabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan, \u201cEspnet-tts: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit,\u201d in ICASSP, 2020.\n[9] T. Tu, Y. Chen, C. Yeh, and H. Lee, \u201cEnd-to-end text-to-speech\nfor low-resource languages by cross-lingual transfer learning,\u201d Proc. Interspeech 2019.\n[10] Y. Chung, Y. Wang, W. Hsu, Y. Zhang, and R. Skerry-Ryan,\n\u201cSemi-supervised training for improving data efficiency in end-to-end speech synthesis,\u201d in ICASSP, 2019.\n[11] J. Xu, X. Tan, Y. Ren, T. Qin, J. Li, S. Zhao, and T. Y. Liu, \u201cLr-\nspeech: Extremely low-resource speech synthesis and recognition,\u201d in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\n[12] H. Zhang and Y. Lin, \u201cUnsupervised learning for sequence-\nto-sequence text-to-speech for low-resource languages,\u201d Proc. Interspeech 2020.\n[13] P. Gallegos, J. Williams, J. Rownicka, and S. King, \u201cAn un-\nsupervised method to select a speaker subset from large multispeaker speech synthesis datasets,\u201d Interspeech, 2020.\n[14] H. Mubarak, A. Hussein, S. Chowdhury, and A. Ali, \u201cQASR:\nQCRI Aljazeera speech resource\u2013a large scale annotated Arabic speech corpus,\u201d ACL, 2021.\n[15] P. Bell, M. Gales, T. Hain, J. Kilgour, P. Lanchantin, X. Liu,\nA. McParland, S. Renals, Os. Saz, M. Wester, et al., \u201cThe MGB challenge: Evaluating multi-genre Broadcast media recognition,\u201d in ASRU, 2015.\n[16] A. Ali, P. Bell, J. Glass, Y. Messaoui, H. Mubarak, S. Renals,\nand Y. Zhang, \u201cThe MGB-2 challenge: Arabic multi-dialect broadcast media recognition,\u201d in SLT, 2016.\n[17] C. K. Reddy, V. Gopal, and R. Cutler, \u201cDnsmos: A non-\nintrusive perceptual objective speech quality metric to evaluate noise suppressors,\u201d in ICASSP 2021.\n[18] C. Lo, S. Fu, W. Huang, X. Wang, J. Yamagishi, Y. Tsao, and\nH. Wang, \u201cMosnet: Deep learning based objective assessment for voice conversion,\u201d in Interspeech 2019.\n[19] P. Andreev, A. Alanov, O. Ivanov, and D. Vetrov, \u201cHifi++: a\nunified framework for neural vocoding, bandwidth extension and speech enhancement,\u201d arXiv:2203.13086, 2022.\n[20] B. Naderi and R. Cutler, \u201cAn open source implementation\nof itu-t recommendation p. 808 with validation,\u201d Proc. Interspeech 2020.\n[21] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng,\nH. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., \u201cThe interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d Proc. Interspeech 2020.\n[22] A. Hussein, S. Watanabe, and A. Ali, \u201cArabic speech recogni-\ntion by end-to-end, modular systems and human,\u201d Computer Speech & Language, 2021.\n[23] A. Abdelali, K. Darwish, N. Durrani, and H. Mubarak,\n\u201cFarasa: A fast and furious segmenter for Arabic,\u201d in The North American chapter of the association for computational linguistics: Demonstrations, 2016.\n[24] J. Shen, R. Pang, R. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, et al., \u201cNatural TTS synthesis by conditioning wavenet on mel spectrogram predictions,\u201d in ICASSP, 2018.\n[25] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, \u201cNeural speech\nsynthesis with transformer network,\u201d in AAAI, 2019.\n[26] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\ngio, \u201cAttention-based models for speech recognition,\u201d NIPS, 2015.\n[27] K. Ito and L. Johnson, \u201cThe lj speech dataset,\u201d Online:\nhttps://keithito. com/LJ-Speech-Dataset, 2017.\n[28] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. In-\naguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi, et al., \u201cRecent developments on ESPnet toolkit boosted by conformer,\u201d ICASSP, 2021.\n[29] A. \u0141an\u0301cucki, \u201cFastpitch: Parallel text-to-speech with pitch pre-\ndiction,\u201d ICASSP, 2021.\n[30] N. Perraudin, P. Balazs, and P. L. S\u00f8ndergaard, \u201cA fast griffin-\nlim algorithm,\u201d in 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics.\n[31] R. Yamamoto, E. Song, and J. Kim, \u201cParallel wavegan: A\nfast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,\u201d in ICASSP, 2020."
        }
    ],
    "year": 2023
}