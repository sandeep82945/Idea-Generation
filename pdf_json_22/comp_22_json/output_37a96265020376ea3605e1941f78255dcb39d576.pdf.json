{
    "abstractText": "Sparse superposition codes were originally proposed as a capacity-achieving communication scheme over the gaussian channel, whose coding matrices were made of i.i.d. gaussian entries [1]. We extend this coding scheme to more generic ensembles of rotational invariant coding matrices with arbitrary spectrum, which include the gaussian ensemble as a special case. We further introduce and analyse a decoder based on vector approximate message-passing (VAMP) [2]. Our main findings, based on both a standard replica symmetric potential theory and state evolution analysis, are the superiority of certain structured ensembles of coding matrices (such as partial row-orthogonal) when compared to i.i.d. matrices, as well as a spectrum-independent upper bound on VAMP\u2019s threshold. Most importantly, we derive a simple \u201cspectral criterion\u201d for the scheme to be at the same time capacity-achieving while having the best possible algorithmic threshold, in the \u201clarge section size\u201d asymptotic limit. Our results therefore provide practical design principles for the coding matrices in this promising communication scheme.",
    "authors": [
        {
            "affiliations": [],
            "name": "TianQi Hou"
        },
        {
            "affiliations": [],
            "name": "YuHao Liu"
        },
        {
            "affiliations": [],
            "name": "Teng Fu"
        }
    ],
    "id": "SP:0a4fdb802825a5fe7b65204bd15717ee74429a83",
    "references": [
        {
            "authors": [
                "A.R. Barron",
                "A. Joseph"
            ],
            "title": "Toward fast reliable communication at rates near capacity with gaussian noise",
            "venue": "IEEE International Symposium on Information Theory,",
            "year": 2010
        },
        {
            "authors": [
                "S. Rangan",
                "P. Schniter",
                "A.K. Fletcher"
            ],
            "title": "Vector approximate message passing",
            "venue": "IEEE Trans. on Information Theory,",
            "year": 2019
        },
        {
            "authors": [
                "C. Rush",
                "A. Greig",
                "R. Venkataramanan"
            ],
            "title": "Capacity-achieving sparse superposition codes via approximate message passing decoding",
            "venue": "IEEE Trans. on Information Theory,",
            "year": 2017
        },
        {
            "authors": [
                "J. Barbier",
                "F. Krzakala"
            ],
            "title": "Approximate message-passing decoder and capacity achieving sparse superposition codes",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2017
        },
        {
            "authors": [
                "J. Barbier",
                "M. Dia",
                "N. Macris"
            ],
            "title": "Proof of threshold saturation for spatially coupled sparse superposition codes",
            "venue": "In 2016 IEEE International Symposium on Info. Theory (ISIT),",
            "year": 2016
        },
        {
            "authors": [
                "E. Biyik",
                "J. Barbier",
                "M. Dia"
            ],
            "title": "Generalized approximate messagepassing decoder for universal sparse superposition codes",
            "venue": "IEEE International Symposium on Information Theory (ISIT),",
            "year": 2017
        },
        {
            "authors": [
                "C. Rush",
                "K. Hsieh",
                "R. Venkataramanan"
            ],
            "title": "Capacity-achieving spatially coupled sparse superposition codes with amp decoding",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2021
        },
        {
            "authors": [
                "J.J Ma",
                "L. Ping"
            ],
            "title": "Orthogonal amp",
            "venue": "IEEE Access,",
            "year": 2017
        },
        {
            "authors": [
                "M. M\u00e9zard",
                "A. Montanari"
            ],
            "title": "Information, physics, and computation",
            "year": 2009
        },
        {
            "authors": [
                "M. Bayati",
                "A. Montanari"
            ],
            "title": "The dynamics of message passing on dense graphs, with applications to compressed sensing",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2011
        },
        {
            "authors": [
                "A. Javanmard",
                "A. Montanari"
            ],
            "title": "State evolution for general approximate message passing algorithms, with applications to spatial coupling",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2013
        },
        {
            "authors": [
                "J. Barbier",
                "N. Macris",
                "M. Dia",
                "F. Krzakala"
            ],
            "title": "Mutual information and optimality of approximate message-passing in random linear estimation",
            "venue": "IEEE Trans. on Information Theory,",
            "year": 2020
        },
        {
            "authors": [
                "G. Reeves",
                "H.D. Pfister"
            ],
            "title": "The replica-symmetric prediction for compressed sensing with gaussian matrices is exact",
            "venue": "In 2016 IEEE International Symposium on Info. Theory (ISIT),",
            "year": 2016
        },
        {
            "authors": [
                "J. Barbier",
                "M. Dia",
                "N. Macris"
            ],
            "title": "Threshold saturation of spatially coupled sparse superposition codes for all memoryless channels",
            "venue": "IEEE Info. Theory Workshop (ITW),",
            "year": 2016
        },
        {
            "authors": [
                "J. Barbier",
                "F. Krzakala",
                "N. Macris",
                "L. Miolane",
                "L. Zdeborov\u00e1"
            ],
            "title": "Optimal errors and phase transitions in high-dimensional generalized linear models",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "J. Barbier",
                "N. Macris",
                "A. Maillard",
                "F. Krzakala"
            ],
            "title": "The mutual information in random linear estimation beyond iid matrices",
            "venue": "IEEE International Symposium on Information Theory (ISIT),",
            "year": 2018
        },
        {
            "authors": [
                "R. Berthier",
                "A. Montanari",
                "P.-M. Nguyen"
            ],
            "title": "State evolution for approximate message passing with non-separable functions",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2020
        },
        {
            "authors": [
                "A. Tulino",
                "S. Verd\u00fa"
            ],
            "title": "Random matrix theory and wireless communications",
            "venue": "Now Publishers Inc,",
            "year": 2004
        },
        {
            "authors": [
                "Marc Potters",
                "Jean-Philippe Bouchaud"
            ],
            "title": "A First Course in Random Matrix Theory: For Physicists, Engineers and Data Scientists",
            "year": 2020
        },
        {
            "authors": [
                "A.M. Tulino",
                "G. Caire",
                "S. Verd\u00fa",
                "S. Shamai"
            ],
            "title": "Support recovery with sparsely sampled free random matrices",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2013
        },
        {
            "authors": [
                "C. Gerbelot",
                "A. Abbara",
                "F. Krzakala"
            ],
            "title": "Asymptotic errors for teacherstudent convex generalized linear models (or: How to prove kabashima\u2019s replica formula)",
            "venue": "arXiv preprint arXiv:2006.06581,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Fan"
            ],
            "title": "Approximate message passing algorithms for rotationally invariant matrices",
            "venue": "The Annals of Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "R. Dudeja",
                "J. Ma",
                "A. Maleki"
            ],
            "title": "Information theoretic limits for phase retrieval with subsampled haar sensing matrices",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2020
        },
        {
            "authors": [
                "R. Venkataramanan",
                "K. K\u00f6gler",
                "M. Mondelli"
            ],
            "title": "Estimation in rotationally invariant generalized linear models via approximate message passing",
            "venue": "arXiv preprint arXiv:2112.04330,",
            "year": 2021
        },
        {
            "authors": [
                "J. Barbier",
                "F. Krzakala"
            ],
            "title": "Replica analysis and approximate message passing decoder for superposition codes",
            "venue": "In 2014 IEEE International Symposium on Information Theory,",
            "year": 2014
        },
        {
            "authors": [
                "J.J. Ma",
                "X.J. Yuan",
                "L. Ping"
            ],
            "title": "Turbo compressed sensing with partial dft sensing matrix",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2014
        },
        {
            "authors": [
                "J.J. Ma",
                "J. Xu",
                "A. Maleki"
            ],
            "title": "Analysis of sensing spectral for signal recovery under a generalized linear model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A. Maillard",
                "F. Krzakala",
                "Y.M. Lu",
                "L. Zdeborov\u00e1"
            ],
            "title": "Construction of optimal spectral methods in phase retrieval",
            "venue": "arXiv preprint arXiv:2012.04524,",
            "year": 2020
        },
        {
            "authors": [
                "B. Aubin",
                "B. Loureiro",
                "A. Baker",
                "F. Krzakala",
                "L. Zdeborov\u00e1"
            ],
            "title": "Exact asymptotics for phase retrieval and compressed sensing with random generative priors",
            "venue": "In Mathematical and Scientific Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "C.K. Wen",
                "K.K. Wong"
            ],
            "title": "Analysis of compressed sensing with spatially-coupled orthogonal matrices",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION AND SETTING\nSparse superposition (SS) codes were introduced for communication over the additive white gaussian noise channel (AWGNC) [1] and proven to achieve the capacity using power allocation [3] or spatial coupling under message-passing based decoding [4], [5], [6], [7]. But the coding matrices were limited to be constructed from independent gaussian entries. In this paper, we extend the coding matrices to a much broader class of matrices beyond the i.i.d. ones, i.e., to rotational invariant matrix ensembles. We deal with several illustrative coding ensembles, both theoretically and practically, by introducing and analyzing a VAMP-based decoding algorithm [2] (which is similar to OAMP [8]). Furthermore, we empirically confirm that a state evolution (SE) recursion accurately tracks VAMP\u2019s performance. By analyzing the fixed points of this SE recursion combined with a replica analysis from statistical mechanics, we precisely quantify computational-to-statistical gaps for several coding matrix ensembles, and demonstrate the superiority of coding matrices whose rows are orthogonal compared to the standard gaussian coding ensemble. Other important contributions come in the form of a simple criterion to select \u201cgood\u201d coding matrices, i.e., for the coding scheme to be capacity-achieving and with the best possible algorithmic threshold, in the large section size limit, but also a spectrumindependent upper bound on VAMP\u2019s algorithmic threshold which sets an absolute limit on its performance for any rotational invariant ensemble.\nLet us emphasize that all our results are at the moment nonrigorous. Our main tools are the replica symmetric method [9] and the state evolution recursion tracking AMP-like algorithms [10], [11], [10]. Concerning the replica method, despite being non-rigorous, a multitude of recent studies prove its exactness in many similar inference problems [3], [12], [13], [14], [15], [16]. This strongly points towards the fact that our replicabased predictions should be exact in a proper asymptotic limit. For the state evolution analysis, it is proven to track VAMP but only for separable denoisers (that would correspond to the trivial B = 1 case of SS codes). Extending the VAMP state evolution to section-wise priors as needed here requires some work, in the spirit of [17] for AMP. Even more care is needed when considering the \u201clarge section size limit\u201d that we are also going to study; see [3] where this was done for the standard SS codes with gaussian coding matrices under AMP decoding. Nevertheless, we conjecture that all the present results can, and will, be proven in the future. We also empirically confirm our predictions through careful numerics. Therefore our results must be considered as numerically-verified conjectures based on by-now well established techniques from statistical physics and the theory of message passing algorithms.\nIn SS codes, the message x = [x1, . . . , xL] is a vector made of L sections, each with B entries. Each section xl, l \u2208 {1, . . . , L} possesses a single non-zero component equal to 1 whose position encodes the symbol to transmit. B is the section size (or alphabet size) and we set N \u00b7\u00b7= LB. We consider random codes generated by a coding matrix A \u2208 RM\u00d7N drawn from a rotational invariant ensemble, i.e., when considering its singular value decomposition A = U \u221a\nDV\u1d40, the orthogonal basis of singular vectors U and V are sampled uniformly in the orthogonal group O(M) of M\u00d7M matrices and O(N), respectively. The diagonal matrix D contains non-negative singular values (Di)i\u2264N on its main diagonal, and whose empirical distribution N\u22121 \u2211 i\u2264N \u03b4Di weakly converges to a well-defined compactly supported probability density function \u03bd(\u03bb) as N,M \u2192 \u221e (not necessarily proportionally). We denote its aspect ratio \u03b1 = M/N . The cardinality of the code is BL. Hence, the (design) rate is R = L log2(B)/M = log2(B)/(\u03b1B) and thus the code is fully specified by (M,R,B). For a message x as before, the codeword is Ax \u2208 RM . We enforce the power constraint \u2016Ax\u201622/M = 1+oL(1) by tuning the spectrum \u03bd(\u03bb) to satisfy\nar X\niv :2\n20 2.\n04 54\n1v 2\n[ cs\n.I T\n] 2\n6 M\nay 2\n02 2\nAlgorithm 1 VAMP-based decoder for SS codes Require: Max iteration K, coding matrix A, observed y\n1: Initialize r1,0 and \u03b31,0 \u2265 0. 2: for k = 0, 1, . . . ,K (or until convergence) do 3: // Denoising 4: x\u03021,k = g1(r1,k, \u03b31,k), \u03b11,k = \u3008g\u20321(r1,k, \u03b31,k)\u3009 5: r2,k = (x\u03021,k \u2212 \u03b11,kr1,k)/(1\u2212 \u03b11,k) 6: \u03b32,k = \u03b31,k(1\u2212 \u03b11,k)/\u03b11,k 7: // LMMSE estimation 8: x\u03022,k = g2(r2,k, \u03b32,k), \u03b12,k = \u3008g\u20322 (r2,k, \u03b32,k)\u3009 9: r1,k+1 = (x\u03022,k \u2212 \u03b12,kr2,k)/(1\u2212 \u03b12,k)\n10: \u03b31,k+1 = \u03b32,k(1\u2212 \u03b12,k)/\u03b12,k 11: end for 12: Return x\u0302 = x\u03021,K .\n\u222b d\u03bb\u03bb\u03bd(\u03bb) = \u03b1B. Codewords are transmitted through an AWGNC, i.e., the received corrupted codeword is y = Ax+z, with i.i.d. z\u00b5 \u223c N ( 0, \u03c32 ) , \u00b5 \u2264M , so that the signal-to-noise ratio is snr = \u03c3\u22122."
        },
        {
            "heading": "II. THE VAMP-BASED DECODER",
            "text": "The VAMP algorithm we propose, see Algorithm 1 below, aims at computing the minimum mean-square (MMSE) estimator given by the expectation of the Bayesian posterior\nP (x | y,A) = 1 Z(y,A)\nexp ( \u2212 snr\n2 \u2016Ax\u2212 y\u201622 )\u220f l\u2264L P0(xl),\nwhere Z(y,A) is a normalization. But as we will see it is successful in doing so only for certain rates R. The hard constraints for the sections of the message are enforced by the prior distribution P0(xl) = B\u22121 \u2211 i\u2208l \u03b4xi,1 \u220f j\u2208l,j 6=i \u03b4xj ,0, where {i \u2208 l} are the B scalar components indices of the section belonging to the section indexed by l.\nVAMP was originally derived for generalized linear estimation [2]. In the present generalization to the vectorial setting of SS codes, only the input non-linear steps differ from canonical VAMP: here the so-called denoiser g1(r, \u03b3) acts section-wise instead of component-wise. In full generality, it is defined as g1(r, \u03b3) \u00b7\u00b7=E[X | R=r] for the random variable R=X+ \u221a \u03b3 Z with X \u223c P\u2297L0 and Z \u223c N (0, IN ). Plugging P0 yields the component-wise expression of the denoiser and its variance:{\n[g1(r, \u03b3)]i = exp(ri/\u03b3)\u2211\nj\u2208li exp(rj/\u03b3)\n,\n[g\u20321(r, \u03b3)]i = \u03b3\u22121[g\u20321(r, \u03b3)]i(1\u2212 [g\u20321(r, \u03b3)]i),\nwhere [g\u20321(r, \u03b3)]i := \u2202xig1(x, \u03b3)|xi=ri , li is the section to which belongs the ith scalar component. g2(r, \u03b3) can be recognized as the MMSE estimate of a random vector x from the data y \u223c N (Ax, snr\u22121IN ) and prior x \u223c N (r, \u03b3\u22121IN ):{\ng2(r, \u03b3) = (snrA\u1d40A + \u03b3IN )\u22121(snrA\u1d40y + \u03b3r), \u3008g\u20322(r, \u03b3)\u3009 = \u03b3N\u22121 Tr[(snrA\u1d40A + \u03b3I)\u22121].\nWe will track two error metrics for the VAMP estimator x\u0302 = (x\u03021, . . . , x\u0302L) = (x\u03021, . . . , x\u0302N ), namely the mean-square error\n(MSE) per section EL and the section error rate SERL (I(\u00b7) is the indicator):\nEL \u00b7\u00b7= 1\nL \u2016x\u2212 x\u0302\u201622, SERL \u00b7\u00b7=\n1\nL \u2211 l\u2264L I (xl 6= x\u0302l) ."
        },
        {
            "heading": "III. STATE EVOLUTION AND REPLICA ANALYSES",
            "text": "We now present the state evolution (SE) and replica symmetric analyses, valid in the asymptotic L\u2192\u221e limit, of the performance of SS codes under MMSE and VAMP decoding, for rotational invariant coding matrices. Both analyses are intimately related, and linked to the estimation problem of a single section S \u223c P0 transmitted through an \u201ceffective gaussian channel\u201d with noise variance\n\u03a3(E)2 \u00b7\u00b7= (BsnrR(\u2212snrE))\u22121.\nHere R(z) \u00b7\u00b7= C\u22121(\u2212z) \u2212 z\u22121 is the R-transform associated to the asymptotic spectral density \u03c1 of B\u22121A\u1d40A, where C\u22121 is the functional inverse of the Cauchy transform C(z) :=\u222b \u03c1(\u03bb) \u03bb\u2212zd\u03bb, see, e.g., [18], [19]. State evolution Let the scalar-valued SE operator\nT (E) \u00b7\u00b7= ES,Z\u2016S\u2212E[S | S+\u03a3(E)Z]\u201622 = EZ[(g(1)(\u03a3(E),Z)\u22121)2+(B\u22121)g(2)(\u03a3(E),Z)2]\nwhere S \u223c P0,Z \u223c N (0, IB) and we define{ g(1)(\u03a3, z) \u00b7\u00b7= [1+e\u2212 1 \u03a32 \u2211B j=2 e 1 \u03a3 (zj\u2212z1)]\u22121,\ng(2)(\u03a3, z) \u00b7\u00b7= [1+e 1 \u03a32 +(z1\u2212z2) 1\u03a3 + \u2211B k=3 e (zk\u2212z2) 1\u03a3 ]\u22121.\nThe SE operator corresponds to the MMSE of S when transmitted over the effective gaussian channel. The SE recursion tracking the asymptotic L \u2192 \u221e limit E(t) of VAMP\u2019s MSE at iteration t is then obtained as a straightforward adaptation of the results of [2] and reads\nE(0) = 1\u2212 1/B, E(t+1) = T (E(t)), t \u2265 0. (1)\nWe refer to [2] for the proof of VAMP\u2019s state evolution in regression. Fig. 1 is a numerical demonstration of the validity of our SE recursion for tracking VAMP for SS codes.\nReplica symmetric analysis A remarkable property of the SE recursion (1) is that its stationary point(s) are in one-toone correspondence with the critical point(s) of the so-called replica symmetric potential (or \u201cfree entropy\u201d) \u03a6B(E) derived from the replica method [9], [20], [4]:\n\u2202E\u03a6B(E)|E\u2217 = 0\u21d4 T (E\u2217) = E\u2217.\nFor the SS codes in the present setting it is: \u03a6B(E) \u00b7\u00b7= SB(\u03a3(E))\u2212UB(E), UB(E) \u00b7\u00b7= B2 \u222b snrE 0 R(x)dx\u2212 E2\u03a32(E) ,\nSB(\u03a3(E)) \u00b7\u00b7= EZ logB ( 1 + \u2211B i=2 ei(Z,\u03a3(E)) ) ,\nwhere ei(Z, x) \u00b7\u00b7= exp((Zi\u2212Z1)/x\u22121/x2) and i.i.d. Zi \u223c N (0, 1). The validity of such replica analysis has by-now been proven in many related settings to ours, such as generalized linear regression and compressive sensing but with gaussian i.i.d. matrices [12], [15], [13]. Apart from few recent works\n[16], [21], [22], [23], [24], the rigorous study of linear regression problems with rotationally invariant matrix ensembles is only at its premises. Thus, proving our present conjectures is an interesting avenue left for future work.\nWe illustrate our results through three coding ensembles: \u2022 (i) As base case we consider the standard gaussian setting where all entries of A are i.i.d. gaussian. The asymptotic results turn out to depend only on the asymptotic eigenvalue distribution \u03c1(\u03bb) of B\u22121ATA, which is given in this case by the Marchenko\u2013Pastur distribution \u03c1(\u03bb) = (1 \u2212 \u03b1)\u03b4(\u03bb) +\u221a\n(\u03bb\u2212 \u03bb\u2212)(\u03bb+ \u2212 \u03bb)/(2\u03c0\u03bb), where \u03bb\u00b1 \u00b7\u00b7= (1 \u00b1 \u221a \u03b1)2. Then R(z) = \u03b1/(1\u2212z) [18]. As it should, the analysis does recover in this case the results of [25], [4], [3]. \u2022 (ii) The row-orthogonal ensemble constructed by randomly selecting M \u2264 N rows from a uniformly sampled N\u00d7 N orthogonal matrix. For this ensemble \u03c1(\u03bb) = (1\u2212\u03b1)\u03b4(\u03bb)+ \u03b1\u03b4(\u03bb\u22121) and R(z) = (1+z+ \u221a (1\u2212 z)2 + 4\u03b1z)/(2z)\u2212z\u22121.\nThis will recover similar results as found in [26]. \u2022 (iii) Finally, a discrete spectrum \u03c1(\u03bb) = (1 \u2212 \u03b1)\u03b4(\u03bb) + \u03b1 2 \u03b4(\u03bb \u2212 1 2 ) + \u03b1 2 \u03b4(\u03bb \u2212 3 2 ). This ensemble is obtained by generating a spectrum with the proper fractions of singular values of A in {0, \u221a 1/2, \u221a 3/2} and then multiplying by\nuniform orthogonal matrices of proper dimensions. The Rtransform is obtained as the solution of a cubic equation solved numerically. This rather artificial case will serve as tractable example of a non capacity-achieving ensemble.\nFig. 1 shows that SE properly tracks VAMP for nonstandard rotational invariant coding matrices. Under VAMP decoding, SS codes exhibit, as L \u2192 \u221e, a sharp phase transition at an algorithmic threshold RVAMP below Shannon\u2019s\ncapacity. RVAMP is defined as the highest rate such that for R \u2264 RVAMP, (1) has a unique fixed point. For the gaussian ensemble RgaussVAMP = 1.52 and for the row-orthogonal one RorthoVAMP = 1.62 and is therefore better compared with the gaussian coding matrices, as noted already for compressive sensing [26]. Whenever R < RVAMP VAMP decodes well (and we conjecture optimally), see red and blue curves. If instead R>RVAMP VAMP fails, see green and black curves.\nFig. 2 depicts the replica potential for the row-orthogonal coding ensemble with snr = 28 and B = 2 (a similar picture would appear for another ensemble). The advantage of the potential when compared to the SE analysis is that, in addition to encode RVAMP as the rate at which an inflexion point appears (which blocks the SE recursion seen as a gradient ascent of \u03a6B(E) initialized at high E), it allows us to also obtain the information-theoretic threshold of the code, defined as the rate where both the \u201cgood\u201d and \u201cbad\u201d maxima of \u03a6B(E) are equal. If R < RVAMP, VAMP\u2019s estimate is conjectured to match the MMSE estimator when initialized randomly, as in standard gaussian SS codes or in linear regression [4], [3], [15], [13]. Instead, if R > RVAMP VAMP is sub-optimal.\nAt finite size VAMP\u2019s performance close its threshold RVAMP, itself extracted from the potential as explained in the caption of Fig. 2, is shown in Fig. 3. This was done using a\nproxy of the row-orthogonal ensemble based on discrete cosine transform matrices. Using these structured matrices dramatically speeds-up the decoding under VAMP while having same performance, which is practically interesting. As predicted by our theory, recovery is good whenever R < RVAMP but poor else. When B increases, the SER changes more drastically as the rate increases. Near its threshold and due to finite size effects, the VAMP performance (averaged over many realizations) has a transient behavior smoothly interpolating between very \u201cpoor\u201d and very \u201cgood\u201d.\nAll codes are made available at [27]. Equipped with these methods, we can therefore completely characterize the performance of SS codes and VAMP with generic rotational invariant coding ensembles as L\u2192\u221e."
        },
        {
            "heading": "A. Analysis in the large section size limit, and main result",
            "text": "The analysis in the large section size limit B \u2192 \u221e requires rescaling the potential \u03a6\u0303(E) \u00b7\u00b7= limB\u2192\u221e \u03a6B(E)/ lnB for it to possess a finite limit. The \u201centropic contribution\u201d limB\u2192\u221e SB(\u03a3)/ lnB = max(1 \u2212 \u03a3\u0303\u22122/2, 0) has been computed in [4] using the replica method, where the effective SNR that needs to be rescaled too is, using R = log2(B)/(\u03b1B), given by \u03a3\u0303(E)\u22122 := lim\u03b1\u21920 snrR(\u2212snrE)/(\u03b1R ln 2); note that for R to remain finite as B \u2192 \u221e then necessarily \u03b1 = \u0398(lnB/B)\u2192 0 and that moreover also R depends on \u03b1. As \u03b1\u2192 0 we Taylor expand C\u22121(\u2212z) = z\u22121 +\u03a8(z)\u03b1+o(\u03b1) and thus R(z)/\u03b1 = \u03a8(z)+o\u03b1(1). So \u03a6\u0303(E) has a well defined expression for small \u03b1 (i.e., large B) given by\nmax ( 1, 1\n2\u03a3\u0303(E)2\n) \u2212 1\u2212 E 2\u03a3\u0303(E)2 \u2212 \u222b snrE\n0\n\u03a8(\u2212u) 2R ln 2 du+ o\u03b1(1).\nThe expressions of \u03a8 for the three spectra we focus on (in order) are worked out easily and read as follows: \u03a8gauss(z) =\n\u03a8ortho(z) = (1 \u2212 z)\u22121, while \u03a8discrete(z) = (4 \u2212 3z)/((z \u2212 2)(3z \u2212 2)). From the analysis of \u03a6\u0303 we can extract our main result stated below (derived in the next section). Notice that rank(A) = \u03b1N with \u03b1 \u2264 1, so we can rewrite the asymptotic spectral density of B\u22121A\u1d40A as \u03c1 = (1\u2212\u03b1)\u03b40 +\u03b1\u03c1supp where \u03c1supp is a p.d.f. of mean 1 due to the power constraint.\nResult 1. Consider SS codes with coding matrix A drawn from a rotational invariant ensemble, whose empirical spectral measure converges to a well defined density with finite support as L \u2192 \u221e. Then the B \u2192 +\u221e limit RVAMP(\u221e) of the VAMP threshold verifies RVAMP(\u221e) \u2264 snr/(2(1 + snr) ln 2). Moreover the code is capacity achieving in the sense that the infinite section size limit RIT(\u221e) of the information-theoretic threshold satisfies RIT(\u221e) = log2(1+snr)/2 := C, with C the Shannon capacity of the AWGNC, if and only if the asymptotic p.d.f. \u03c1supp of the non-zero eigenvalues of B\u22121A\u1d40A verifies \u03c1supp \u2192 \u03b41 in law when B \u2192 \u221e, \u03b1 \u2192 0. Moreover, in that case the algorithmic threshold is as good as it can be, i.e., RVAMP(\u221e) = snr/(2(1 + snr) ln 2).\nAccording to this \u201cspectral criterion\u201d of Result 1, both the gaussian and row-orthogonal coding ensembles are capacityachieving in the large section size limit (taken after the L\u2192\u221e limit), while the discrete spectrum is not. E.g., when snr = 15 we can extract from potential \u03a6\u0303 (see the details in the derivation of Result 1) that RgaussIT (\u221e) = RorthoIT (\u221e) = C = 2 while RdiscreteIT (\u221e) = 1.91. The algorithmic and informationtheoretic thresholds extracted at finite section size B (but infinite L) are shown in Fig. 4, do converge when B increases to their predicted asymptotics. This criterion strongly suggests that the row-orthogonal ensemble is optimal among rotationally invariant ensembles for coding in SS codes, at least information-theoretically, given that \u03c1supp = \u03b41 even for finite B. Fig. 4 also indicates that also the VAMP threshold seems better than with other ensembles."
        },
        {
            "heading": "B. Derivation of Result 1 by a replica analysis",
            "text": "The derivation of Result 1 relies on an auxiliary lemma:\nLemma 1. For z \u2208 R<0: (i) R\u2032(z) > 0; (ii) \u03a8(z) \u2264 11\u2212z . Proof. We start with (i). C\u2032(z) = \u222b d\u03bb \u03c1(\u03bb)(\u03bb\u2212z)2 so C is strictly increasing. Therefore, the inverse C\u22121(z) : R>0 7\u2192 R<0 is well defined. We have\nR\u2032(z) = 1 z2 \u2212 1 C\u2032(C\u22121(\u2212z)) , z < 0.\nFor any z < 0, let t > 0 s.t. z = \u2212C(t) (t exists by monotony of C). Then\nR\u2032(\u2212C(t)) = C \u2032(t)\u2212 C2(t) C\u2032(t)C2(t) = Var((\u03bb\u2212 t)\u22121) C\u2032(t)C2(t) > 0.\nWe have proved (i), and now consider (ii). Recall that \u03c1 = (1 \u2212 \u03b1)\u03b40 + \u03b1\u03c1supp, where \u03c1supp is the asymptotic law of the positive eigenvalues of B\u22121A\u1d40A, and that the power constraint requires \u222b \u03bb\u03c1supp(\u03bb)d\u03bb = 1. From C\u2019s definition we have\n\u2212 z = \u03b1\u2212 1 C\u22121(\u2212z) + \u03b1\n\u222b \u03c1supp(\u03bb)\n\u03bb\u2212 C\u22121(\u2212z) d\u03bb. (2)\nVAMP(\u221e) \u2248 0.67 and\nRdiscreteVAMP (\u221e) \u2248 0.61). Thus, in the large section size limit, row-orthogonal matrices are not better than Gaussian ones; already at B = 256 their thresholds are very similar. However, there may be other benefits to using row-orthogonal matrices, e.g., rate of convergence of VAMP decoding and convergence to lower error (see Fig. 1).\nLet \u03c10 (whose domain is R>0) be the \u03b1 \u2192 0 limit of \u03c1supp, and 0 < \u03bb0 \u223c \u03c10. Note that \u03c10 also satisfies the power constraint E\u03bb0 = 1. Recall C\u22121(\u2212z) = z\u22121 + \u03a8(z)\u03b1+ o(\u03b1), so by multiplying both sides of (2) by C\u22121(\u2212z)/\u03b1 followed by letting \u03b1\u2192 0 yields (we exchange limit and integration by dominated convergence)\n\u03a8(z)z = E ( 1\n1\u2212 z\u03bb0\n) \u2212 1. (3)\nBy Cauchy-Schwarz we have 1 \u2264 E ( 1\n1\u2212 z\u03bb0\n) E(1\u2212 z\u03bb0) = E ( 1\u2212 z 1\u2212 z\u03bb0 ) ,\nwhere the equality holds if and only if \u03c10 = \u03b41. Combining this inequality with (3) proves \u03a8(z) \u2264 11\u2212z for z < 0, with equality if and only if \u03c1supp \u2192 \u03b41 as \u03b1\u2192 0.\nWe are in position to derive our main Result 1.\nDerivation of Result 1. We consider various scenarios for the extrema of potential \u03a6\u0303(E) in order to locate the two thresholds of interest (recall the caption of Fig. 2 for locating the transitions from \u03a6B , or from \u03a6\u0303 at infinite B). Very similar analyses were performed in [4], [14] so we will be brief. Start by noticing that \u03a3\u0303(E)\u22122 := lim\u03b1\u21920 snrR(\u2212snrE)/(\u03b1R ln 2) is a decreasing function from Lemma 1, so E = 1 is its minimum (as E \u2208 [0, 1]).\n\u2022 Case 1: (2\u03a3\u0303(1)2)\u22121 > 1. Recall R(z) = \u03b1\u03a8(z) + o(\u03b1). We have (and using Lemma 1 for the inequality)\n\u03a6\u0303\u2032(E) = \u2212 lim \u03b1\u21920 snr2E 2\u03b1R ln 2 R\u2032(\u2212 snrE) < 0. (4)\nThere is a stable unique maximum at E = 0. \u2022 Case 2: (2\u03a3\u0303(1)2)\u22121 \u2264 1. There exists E1 \u2208 [0, 1] s.t.\n(2\u03a3\u0303(E1) 2)\u22121 = 1. The derivative of \u03a6\u0303(E) is (4) if 0 < E < E1. When E1 < E < 1 it is instead\n\u03a6\u0303\u2032(E) = lim \u03b1\u21920\nsnr2\n2\u03b1R ln 2 (1\u2212 E)R\u2032(\u2212 snrE) > 0. (5)\nThere are thus two maxima at E = 0 and E = 1. Solving 2\u03a3\u0303(1)2 = 1 for R thus gives the algorithmic threshold in the large section limit:\nRVAMP(\u221e) = lim \u03b1\u21920 snrR(\u2212snr) 2\u03b1 ln 2 = snr\u03a8(\u2212snr) 2 ln 2 . (6)\nUnder case 2, the free entropy takes the following values at its maxima:\n\u03a6\u0303(0) = 0, \u03a6\u0303(1) = 1\u2212 lim \u03b1\u21920\n1\n2\u03b1R ln 2 \u222b snr 0 R(\u2212u)du,\nThen setting \u03a6\u0303(0) = \u03a6\u0303(1) gives the information-theoretic threshold in the large section limit:\nRIT(\u221e) = lim \u03b1\u21920\n\u222b snr 0 R(\u2212u)du\n2\u03b1 ln 2 =\n\u222b snr 0\n\u03a8(\u2212u)du 2 ln 2 . (7)\nWith Lemma 1 and (6), (7), we can easily derive an spectrumindependent upper bound on both thresholds:\nRVAMP(\u221e) \u2264 snr\n2(1 + snr) ln 2 ,\nRIT(\u221e) \u2264 1\n2 log2(1 + snr) = C.\nBoth equalities hold if and only if \u03c1supp \u2192 \u03b41 as \u03b1 \u2192 0 as claimed. In this case the scheme is capacity-achieving and VAMP\u2019s algorithmic threshold is as good as it can be."
        },
        {
            "heading": "IV. PERSPECTIVES",
            "text": "There are a number of natural extensions of the present work, in the spirit of recent developments in (generalized) regression with design matrices beyond i.i.d. gaussian [16], [28], [29], [30]. For practical purposes, further studies should concentrate on the influence of the spectra of coding matrices for finite section size SS codes (while our analysis mainly focused on large B). It is also interesting to investigate whether a similar criterion as Result 1 may be extended to SS codes for more generic memoryless channels [14], [6]. Another natural direction to explore concerns the comparison of the \u201cspectral design\u201d we proposed with different types of structures for the coding matrices, in particular power allocation and spatialcoupling [4], [7]. Or to analyze what happens when these structures are combined, e.g., when the blocks of the spatiallycoupled matrices are themselves drawn from a rotational invariant ensemble [31], and see whether threshold saturation [14], [5] (i.e., the \u201cclosing\u201d of the computational-statistical gap of AMP-based decoders) occurs in that setting."
        },
        {
            "heading": "V. ACKNOWLEDGEMENTS",
            "text": "We thank Galen Reeves, Shansuo Liang, Hao Wu and Zhongyi Huang for helpful discussions."
        }
    ],
    "title": "Sparse superposition codes under VAMP decoding with generic rotational invariant coding matrices",
    "year": 2022
}