{
    "abstractText": "State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacrifices completeness due to the inherent limitations of convex relaxations. The latter enables complete verification but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efficient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our verifier achieves a new stateof-the-art on both established benchmarks as well as networks with significantly higher accuracy than previously considered. The latter result (up to 28% certification gains) indicates meaningful progress towards creating verifiers that can handle practically relevant networks.",
    "authors": [
        {
            "affiliations": [],
            "name": "GUIDED BRANCH-AND-BOUND"
        },
        {
            "affiliations": [],
            "name": "Claudio Ferrari"
        },
        {
            "affiliations": [],
            "name": "Mark Niklas M\u00fcller"
        },
        {
            "affiliations": [],
            "name": "Nikola Jovanovi\u0107"
        },
        {
            "affiliations": [],
            "name": "Martin Vechev"
        }
    ],
    "id": "SP:856e01e56720d739d8a1ae3f0bdf5067e945cba7",
    "references": [
        {
            "authors": [
                "David Applegate",
                "Robert Bixby",
                "Va\u0161ek Chv\u00e1tal",
                "William Cook"
            ],
            "title": "Finding cuts in the tsp (a preliminary report)",
            "venue": "Technical report, Citeseer,",
            "year": 1995
        },
        {
            "authors": [
                "Rudy Bunel",
                "P Mudigonda",
                "Ilker Turkaslan",
                "P Torr",
                "Jingyue Lu",
                "Pushmeet Kohli"
            ],
            "title": "Branch and bound for piecewise linear neural network verification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy M. Cohen",
                "Elan Rosenfeld",
                "J. Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In Proc. of ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Krishnamurthy Dvijotham",
                "Alexey Kurakin",
                "Aditi Raghunathan",
                "Jonathan Uesato",
                "Rudy Bunel",
                "Shreya Shankar",
                "Jacob Steinhardt",
                "Ian J. Goodfellow",
                "Percy Liang",
                "Pushmeet Kohli"
            ],
            "title": "Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Rudy Bunel",
                "Alban Desmaison",
                "Krishnamurthy Dvijotham",
                "Pushmeet Kohli",
                "Philip HS Torr",
                "M Pawan Kumar"
            ],
            "title": "Improved branch and bound for neural network verification via lagrangian decomposition",
            "venue": "ArXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Li Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Souradeep Dutta",
                "Susmit Jha",
                "Sriram Sankaranarayanan",
                "Ashish Tiwari"
            ],
            "title": "Output range analysis for deep feedforward neural networks",
            "venue": "In NASA Formal Methods Symposium",
            "year": 2018
        },
        {
            "authors": [
                "Ruediger Ehlers"
            ],
            "title": "Formal verification of piece-wise linear feed-forward neural networks",
            "venue": "In International Symposium on Automated Technology for Verification and Analysis",
            "year": 2017
        },
        {
            "authors": [
                "Timon Gehr",
                "Matthew Mirman",
                "Dana Drachsler-Cohen",
                "Petar Tsankov",
                "Swarat Chaudhuri",
                "Martin Vechev"
            ],
            "title": "Ai2: Safety and robustness certification of neural networks with abstract interpretation",
            "venue": "IEEE Symposium on Security and Privacy (SP)",
            "year": 2018
        },
        {
            "authors": [
                "Sven Gowal",
                "Krishnamurthy Dvijotham",
                "Robert Stanforth",
                "Rudy Bunel",
                "Chongli Qin",
                "Jonathan Uesato",
                "Relja Arandjelovic",
                "Timothy Mann",
                "Pushmeet Kohli"
            ],
            "title": "On the effectiveness of interval bound propagation for training verifiably robust models",
            "venue": "ArXiv preprint,",
            "year": 2018
        },
        {
            "authors": [
                "Patrick Henriksen",
                "Alessio Lomuscio"
            ],
            "title": "Deepsplit: An efficient splitting method for neural network verification via indirect effect analysis",
            "venue": "In Proceedings of the 30th International Joint Conference on Artificial Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "Guy Katz",
                "Clark Barrett",
                "David L Dill",
                "Kyle Julian",
                "Mykel J Kochenderfer"
            ],
            "title": "Reluplex: An efficient smt solver for verifying deep neural networks",
            "venue": "In International Conference on Computer Aided Verification",
            "year": 2017
        },
        {
            "authors": [
                "Panagiotis Kouvaros",
                "Alessio Lomuscio"
            ],
            "title": "Towards scalable complete verification of relu neural networks via dependency-based branching",
            "venue": "In Proceedings of the 30th International Joint Conference on Artificial Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Jingyue Lu",
                "M. Pawan Kumar"
            ],
            "title": "Neural network branching for neural network verification",
            "venue": "In Proc. of ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In Proc. of ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Mark Niklas M\u00fcller",
                "Gleb Makarchuk",
                "Gagandeep Singh",
                "Markus P\u00fcschel",
                "Martin Vechev"
            ],
            "title": "Prima: General and precise neural network certification via scalable convex hull approximations",
            "venue": "Proc. ACM Program. Lang.,",
            "year": 2022
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Harkirat S. Behl",
                "Rudy R. Bunel",
                "Philip H.S. Torr",
                "M. Pawan Kumar"
            ],
            "title": "Scaling the convex barrier with active sets",
            "venue": "In Proc. of ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Hadi Salman",
                "Greg Yang",
                "Huan Zhang",
                "Cho-Jui Hsieh",
                "Pengchuan Zhang"
            ],
            "title": "A convex relaxation barrier to tight robustness verification of neural networks",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Matthew Mirman",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "Fast and effective robustness certification",
            "venue": "In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Rupanshu Ganvir",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "Beyond the single neuron convex barrier for neural network certification",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Markus P\u00fcschel",
                "Martin Vechev"
            ],
            "title": "An abstract domain for certifying neural networks",
            "venue": "Proceedings of the ACM on Programming Languages,",
            "year": 2019
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Markus P\u00fcschel",
                "Martin T. Vechev"
            ],
            "title": "Boosting robustness certification of neural networks",
            "venue": "In Proc. of ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Gaurang Sriramanan",
                "Sravanti Addepalli",
                "Arya Baburaj",
                "Venkatesh Babu R"
            ],
            "title": "Guided adversarial attack for evaluating and enhancing adversarial defenses",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Christian Tjandraatmadja",
                "Ross Anderson",
                "Joey Huchette",
                "Will Ma",
                "Krunal Patel",
                "Juan Pablo Vielma"
            ],
            "title": "The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Vincent Tjeng",
                "Kai Y. Xiao",
                "Russ Tedrake"
            ],
            "title": "Evaluating robustness of neural networks with mixed integer programming",
            "venue": "In Proc. of ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Shiqi Wang",
                "Huan Zhang",
                "Kaidi Xu",
                "Xue Lin",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "J Zico Kolter"
            ],
            "title": "Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification",
            "venue": "In ICML 2021 Workshop on Adversarial Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Eric Wong",
                "J. Zico Kolter"
            ],
            "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
            "venue": "In Proc. of ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Kaidi Xu",
                "Zhouxing Shi",
                "Huan Zhang",
                "Yihan Wang",
                "Kai-Wei Chang",
                "Minlie Huang",
                "Bhavya Kailkhura",
                "Xue Lin",
                "Cho-Jui Hsieh"
            ],
            "title": "Automatic perturbation analysis for scalable certified robustness and beyond",
            "venue": "In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Kaidi Xu",
                "Huan Zhang",
                "Shiqi Wang",
                "Yihan Wang",
                "Suman Jana",
                "Xue Lin",
                "Cho-Jui Hsieh"
            ],
            "title": "Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers",
            "venue": "In Proc. of ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Linjun Zhang",
                "Zhun Deng",
                "Kenji Kawaguchi",
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "How does mixup help with robustness and generalization",
            "venue": "In Proc. of ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "PGD (Madry"
            ],
            "title": "2018) using margin loss (Gowal et al., 2018) and GAMA loss (Sriramanan et al., 2020), both with 5 restarts, 50 steps, and 10 step output diversification",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recent years have witnessed substantial interest in methods for certifying properties of neural networks, ranging from stochastic approaches (Cohen et al., 2019) which construct a robust model from an underlying base classifier to deterministic ones (Gehr et al., 2018; Katz et al., 2017; Xu et al., 2020) that analyze a given network as is (the focus of our work).\nKey Challenge: Scalable and Precise Non-Linearity Handling Deterministic verification methods can be categorized as complete or incomplete. Recent incomplete verification methods based on propagating and refining a single convex region (M\u00fcller et al., 2022; Dathathri et al., 2020; Tjandraatmadja et al., 2020) are limited in precision due to fundamental constraints imposed by convex relaxations. Traditional complete verification approaches based on SMT solvers (Ehlers, 2017) or a single mixed-integer linear programming encoding of a property (Tjeng et al., 2019; Katz et al., 2017) suffer from worst-case exponential complexity and are often unable to compute sound bounds in reasonable time-frames. To address this issue, a Branch-and-Bound approach (Bunel et al., 2020) has been popularized recently: anytime-valid bounds are computed by recursively splitting the problem domain into easier subproblems and deriving bounds on each of these via cheap and less precise methods (Xu et al., 2021; Wang et al., 2021; Palma et al., 2021; Henriksen & Lomuscio, 2021). This approach has proven effective on (smaller) networks where there are relatively few unstable activations and splitting a problem simplifies it substantially. However, for larger networks or those not regularized to be amenable to certification this strategy becomes increasingly ineffective as the larger number of unstable activations makes individual splits less effective, which is exacerbated by the relatively loose underlying bounding methods.\nThis Work: Branch-and-Bound guided by Multi-Neuron Constraints In this work, we propose a novel certification method and verifier, called Multi-Neuron Constraint Guided BaB (MN-BAB), which aims to combine the best of both worlds: it leverages the tight multi-neuron constraints proposed by M\u00fcller et al. (2022) within a BaB framework to yield an efficient GPU-based dual method.\nar X\niv :2\n20 5.\n00 26\n3v 1\n[ cs\n.L G\n] 3\n0 A\npr 2\n02 2\nThe key insight is that the significantly increased precision of the underlying bounding method substantially reduces the number of domain splits (carrying exponential cost) required to certify a property. This improvement is especially pronounced for larger and less regularized networks where additional splits of the problem domain yield diminishing returns. We release all code and scripts to reproduce our experiments at https://github.com/eth-sri/mn-bab.\nMain Contributions:\n\u2022 We present a novel verification framework, MN-BAB, which leverages tight multi-neuron constraints and a GPU-based dual solver in a BaB approach.\n\u2022 We develop a novel branching heuristic, ACS, based on information obtained from analyzing our multi-neuron constraints.\n\u2022 We propose a new class of branching heuristics, CAB, applicable to all BaB-based verifiers, that correct the expected bound improvement of a branching decision for the incurred computational cost.\n\u2022 Our extensive empirical evaluation demonstrates that we improve on the state of the art in terms of certified accuracy by as much as 28% on challenging networks."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "In this section, we review the necessary background for our method (discussed next)."
        },
        {
            "heading": "2.1 NEURAL NETWORK VERIFICATION",
            "text": "The neural network verification problem can be defined as follows: given a network f : X \u2192 Y , an input region D \u2286 X , and a linear property P \u2286 Y over the output neurons y \u2208 Y , prove f(x) \u2208 P, \u2200x \u2208 D. We instantiate this problem with the challenging `\u221e-norm bounded perturbations and set D to the `\u221e ball around an input point x0 of radius : D (x0) = {x \u2208 X | ||x\u2212 x0||\u221e \u2264 }. For ease of presentation, we consider neural networks of L fully-connected layers with ReLU activation functions (we note that MN-BAB can handle a wide range of layers including convolutional, residual, batch-normalization, and average-pooling layers). We focus on ReLU networks as the BaB framework only yields complete verifiers for piecewise linear activation functions but remark that our approach is applicable to a wide class of activations including ReLU, Sigmoid, Tanh, MaxPool, and others. We denote the number of neurons in the ith layer as di and the corresponding weights and biases asW (i) \u2208 Rdi\u00d7di\u22121 and b(i) \u2208 Rdi for i \u2208 {1, ..., L}. Further, the neural network is defined as f(x) := z\u0302(L)(x) where z\u0302(i)(x) :=W (i)z(i\u22121)(x) + b(i) and z(i)(x) := max(0, z\u0302(i)(x)). For readability, we omit the dependency of intermediate activations on x.\nSince we can encode any linear property over output neurons into an additional affine layer, we can simplify the general formulation f(x) \u2208 P to f(x) > 0. The property can now be verified by proving that a lower bound to the following optimization problem is greater 0:\nmin x\u2208D (x0)\nf(x) = z\u0302(L)\ns.t. z\u0302(i) =W (i)z(i\u22121) + b(i)\nz(i) = max(0, z\u0302(i))\n(1)\nA method is called sound if every property it proves actually holds (no false positives). A method is called complete if it can prove every property that actually holds (no false negatives)."
        },
        {
            "heading": "2.2 BRANCH-AND-BOUND FOR VERIFICATION",
            "text": "Bunel et al. (2020) successfully applied the Branch-and-Bound (BaB) approach (lan, 1960) to neural network verification. It consists of a bounding method that computes sound upper and lower bounds on the optimization objective of Eq. (1) and a branching method that recursively splits the problem into subproblems with added constraints, allowing for increasingly tighter bounds. If an upper bound (primal solution) < 0 is found, this represents a counterexample and allows to terminate the procedure. If a lower bound > 0 is obtained, the property is verified on the corresponding (sub-)domain.\nIf a lower bound > 0 is derived on all subdomains, the property is verified. An ideal splitting procedure minimizes the total time required for bounding, which is often well approximated by the number of considered subproblems. A simple approach is to split the input domain, however, this is inefficient for high-dimensional input spaces. Splitting a ReLU activation node into its positive and negative phases has been shown to be far more efficient (Bunel et al., 2020) and ultimately yields a complete verifier (Wang et al., 2021). Hence, we focus solely on ReLU branching strategies.\n2.3 LINEAR CONSTRAINTS\nThe key challenge in neural network verification Eq. (1) is handling the non-linear activations. Stable ReLUs, i.e., those which we can show to be always active (z\u0302 \u2265 0) or inactive (z\u0302 \u2264 0), can be replaced by linear functions. Unstable ReLUs, i.e., those that can be either active or inactive depending on the exact x \u2208 D, have to be approximated using a convex relaxation of their input-output set. We build on the convex relaxation introduced in DEEPPOLY (Singh et al., 2019b) and shown in Fig. 1. Its key property is the single linear upper and lower bound, which allows for efficient bound computation.\n2.4 MULTI-NEURON CONSTRAINTS\nAll convex relaxations that consider ReLU neurons individually are fundamentally limited in their precision by the so-called (single-neuron) convex relaxation barrier (Salman et al., 2019). It can be overcome by considering multiple neurons jointly (Singh et al., 2019a; Tjandraatmadja et al., 2020; M\u00fcller et al., 2022; Palma et al., 2021), thereby capturing interactions between these neurons and obtaining tighter bounds, illustrated in Fig. 2. We leverage the multi-neuron constraints from M\u00fcller et al. (2022), expressed as a conjunction of linear constraints over the joint input and output space of a ReLU layer."
        },
        {
            "heading": "2.5 CONSTRAINED OPTIMIZATION VIA LAGRANGE MULTIPLIERS",
            "text": "To express constraints as part of the optimization problem, we use the technique of Lagrange multipliers. Given a constrained minimization problem minx f(x), s.t. g(x) \u2264 0, we can bound the objective value with:\nmin x f(x) \u2265 min x max \u03bb\u22650 f(x) + \u03bbg(x)\nIf a constraint is satisfied, i.e., g(x)j \u2264 0, \u03bbj = 0 maximizes the (inner) objective, else, i.e., g(x)j > 0, increasing \u03bbj allows the objective to grow unboundedly, shifting the minimum over x until the constraint is satisfied. Hence, if \u03bbj > 0 after optimization, the constraint is active, i.e., it is actively enforced and currently satisfied with equality."
        },
        {
            "heading": "3 A MULTI-NEURON RELAXATION BASED BAB FRAMEWORK",
            "text": "In this section, we describe the two key components of MN-BAB: (i) an efficient bounding method leveraging multi-neuron constraints, as well as constrained optimization via lagrange multipliers (\u00a73.1), and (ii) a branching method tailored to it (\u00a73.2)."
        },
        {
            "heading": "3.1 EFFICIENT MULTI-NEURON BOUNDING",
            "text": "We build on the approach of Singh et al. (2019b), extended by Wang et al. (2021) of deriving a lower bound f as a function of the network inputs and a set of optimizable parameters. Crucially, we tighten these relatively loose bounds significantly by enforcing precise multi-neuron constraints\nvia Lagrange multipliers. To enable this, we develop a method capable of integrating any linear constraint over arbitrary neurons anywhere in the network into the optimization objective. At a high level, we derive linear upper and lower bounds of the form z(i) \u2276\u2212az(i\u22121) + c for every layer\u2019s output in terms of its inputs z(i\u22121). Then, starting with a linear expression in the last layer\u2019s outputs z(L) which we aim to bound, we use the linear bounds derived above, to replace z(L) with symbolic bounds depending only on the previous layer\u2019s values z(L\u22121). We proceed in this manner recursively until we obtain an expression only in terms of the networks inputs. Below, we describe this backsubstitution process for ReLU and affine layers.\nAffine Layer Assume any affine layer z\u0302(i) = W(i)z(i\u22121)+b(i) and a lower bound f = a\u0302(i)z\u0302(i)+ c\u0302(i) with respect to its outputs. We then substitute the affine expression for z\u0302(i) to obtain:\nf = a\u0302(i)W(i)\ufe38 \ufe37\ufe37 \ufe38 a(i\u22121) z(i\u22121) + a\u0302(i)b(i) + c\u0302(i)\ufe38 \ufe37\ufe37 \ufe38 c(i\u22121) = a(i\u22121)z(i\u22121) + c(i\u22121) (2)\nReLU Layer Let f = a(i)z(i) + c(i) be a lower bound with respect to the output of a ReLU layer z(i) = max(0, z\u0302(i)) and l(i) and u(i) bounds on its input s.t. l(i) \u2264 z\u0302(i) \u2264 u(i), obtained by recursively applying this bounding procedure or using a cheaper but less precise bounding method (Singh et al., 2018; Gowal et al., 2018). The backsubstitution through a ReLU layer now consists of three distinct steps: 1) enforcing multi-neuron constraints, 2) enforcing single-neuron constraints to replace the dependence on z(i) by z\u0302(i), and 3) enforcing split constraints, which we describe in detail below.\nEnforcing Multi-Neuron Constraints We compute multi-neuron constraints as described in M\u00fcller et al. (2022), although our approach is applicable to any linear constraints in the input-output space of ReLU activations, written as: [\nP (i) P\u0302 (i) \u2212p(i) ] z(i)z\u0302(i)\n1  \u2264 0. (3) where z\u0302(i) are the pre- and z(i) the post-activation values and P (i), P\u0302 (i), and \u2212p(i) the constraint parameters. We enforce these constraints using Lagrange multipliers (see \u00a72.5), yielding sound lower bounds for all \u03b3(i) \u2208 (R\u22650)ei , where ei denotes the number of multi-neuron constraints in layer i.\na(i)z(i) + c(i) \u2265 max \u03b3(i)\u22650 a(i)z(i) + c(i) + \u03b3(i)>(P (i)z(i) + P\u0302 (i)z\u0302(i) \u2212 p(i))\n= max \u03b3(i)\u22650 (a(i) + \u03b3(i)>P (i))\ufe38 \ufe37\ufe37 \ufe38 a\u2032(i) z(i) + \u03b3(i)>P\u0302 (i)z\u0302(i) + \u03b3(i)>(\u2212p(i)) + c(i)\ufe38 \ufe37\ufe37 \ufe38 c\u2032(i)\nNote that this approach can be easily extended to linear constraints over any activations in arbitrary layers if applied in the last affine layer at the very beginning of the backsubstitution process.\nEnforcing Single-Neuron Constraints We now apply the single-neuron DEEPPOLY relaxation with parametrized slopes \u03b1 collected inD (see below):\nmax \u03b3(i)\u22650 a\u2032(i)z(i) + c\u2032(i) \u2265 max 0\u2264\u03b1(i)\u22641 \u03b3(i)\u22650 a\u2032(i)(D(i)z\u0302(i) + b(i)) + c\u2032(i)\n= max 0\u2264\u03b1(i)\u22641 \u03b3(i)\u22650 a\u2032(i)D(i)\ufe38 \ufe37\ufe37 \ufe38 a\u2032\u2032(i) z\u0302(i) + a\u2032(i)b(i) + c\u2032(i)\ufe38 \ufe37\ufe37 \ufe38 c\u2032\u2032(i)\nThe intercept vector b and the diagonal slope matrixD are defined as:\nDj,j =  1 if lj \u2265 0 or node j is positively split 0 if uj \u2264 0 or node j is negatively split \u03b1j if lj < 0 < uj and aj \u2265 0 uj\nuj\u2212lj if lj < 0 < uj and aj < 0\nbj = { \u2212 uj ljuj\u2212lj if lj < 0 < uj and aj < 0 0 otherwise\nWhere we drop the layer index i for readability and \u03b1j is the lower bound slope parameter illustrated in Fig. 1. Note how, depending on whether the sensitivity a(i)j of f w.r.t. z (i) j has positive or negative sign, we substitute z(i)j for its lower or upper bound, respectively.\nEnforcing Split Constraints We encode split constraints of the form z\u0302(i)j \u2265 0 or z\u0302 (i) j \u2264 0 using the diagonal split matrix S. How the splits are determined will be described in \u00a73.2. For readability, we again dropping the layer index i:\nSj,j =  \u22121 positive split: z\u0302j \u2265 0 1 negative split: z\u0302j < 0 0 no split\n(4)\nS(i)z\u0302(i) \u2264 0 We again enforce these constraints using Lagrange multipliers:\nmax 0\u2264\u03b1(i)\u22641 \u03b3(i)\u22650 a\u2032\u2032(i)z\u0302(i) + c\u2032\u2032(i) \u2265 max 0\u2264\u03b1(i)\u22641 \u03b2(i)\u22650 \u03b3(i)\u22650 (a\u2032\u2032(i) + \u03b2(i)>S(i))\ufe38 \ufe37\ufe37 \ufe38 a\u2032\u2032\u2032(i) z\u0302(i) + c\u2032\u2032(i)\ufe38\ufe37\ufe37\ufe38 c\u2032\u2032\u2032(i)\nPutting everything together, the backsubstitution operation through a ReLU layer is:\nmin x\u2208D a(i)z(i) + c(i) \u2265 min x\u2208D max 0\u2264\u03b1(i)\u22641 \u03b2(i)\u22650 \u03b3(i)\u22650 ((a(i) + \u03b3(i)>P (i))D(i) + \u03b2(i)>S(i) + \u03b3(i)>P\u0302 (i))\ufe38 \ufe37\ufe37 \ufe38 a\u0302(i) z\u0302(i)\n+ (a(i) + \u03b3(i)>P (i))\u2032b(i) + \u03b3(i)>(\u2212p(i)) + c(i)\ufe38 \ufe37\ufe37 \ufe38 c\u0302(i)\n(5)\nFull backsubstitution through all layers leads to an optimizable lower bound on f :\nmin x\u2208D f(x) \u2265 min x\u2208D max 0\u2264\u03b1\u22641 0\u2264\u03b2 0\u2264\u03b3 a(0)x+ c(0) \u2265 max 0\u2264\u03b1\u22641 0\u2264\u03b2 0\u2264\u03b3 min x\u2208D a(0)x+ c(0)\nwhere the second inequality holds due to weak duality. We denote all \u03b1(i)j from every layer of the backsubstitution process with \u03b1 and define \u03b2 and \u03b3 analogously. The inner minimization over x has a closed form solution if D is an lp-ball of radius around x0, given by H\u00f6lder\u2019s inequality:\nmax 0\u2264\u03b1\u22641 0\u2264\u03b2 0\u2264\u03b3 min x\u2208D a(0)x+ c(0) \u2265 max 0\u2264\u03b1\u22641 0\u2264\u03b2 0\u2264\u03b3 a(0)x0 \u2212 ||a(0)>||q + c(0) (6)\nwhere q is defined s.t. 1p + 1 q = 1. Since these bounds are sound for any 0 \u2264 \u03b1 \u2264 1, and \u03b2,\u03b3 \u2265 0, we tighten them by using (projected) gradient ascent to optimize these parameters.\nWe compute all intermediate bounds (l(i), u(i)) using the same bounding procedure, leading to two full parameter sets for every neuron in the network. To reduce memory requirements, we share parameter sets between all neurons in the same layer, but keep separate sets for upper and lower bounds.\nUpper Bounding the Minimum Objective Showing an upper bound on the minimum optimization objective precluding verification (f < 0) allows us to terminate the BaB process early. Propagating any input x \u2208 D through the network yields a valid upper bound, hence, we use the input that minimizes Eq. (6):\nxi =\n{ (x0)i + if a (0) i < 0\n(x0)i \u2212 if a(0)i \u2265 0 ."
        },
        {
            "heading": "3.2 MULTI-NEURON CONSTRAINT GUIDED BRANCHING",
            "text": "Generally, the BaB approach is based on recursively splitting an optimization problem into easier subproblems to derive increasingly tighter bounds. However, the benefit of different splits can vary widely, making an effective branching heuristic which chooses splits that minimize the total number of required subproblems a key component of any BaB framework (Bunel et al., 2020). Typically, a score is assigned based on the expected bound improvement and the most promising split is chosen. Consequently, the better this score captures the actual bound improvement, the better the resulting decision. Both the commonly used BABSR (Bunel et al., 2020) and the more recent FSB (De Palma et al., 2021) approximate bound improvements under a DEEPPOLY style backsubstitution procedure. As neither considers the impact of multi-neuron constraints, the scores they compute might not be suitable proxies for the bound improvements obtained with our method. To overcome this issue, we propose a novel branching heuristic, Active Constraint Score Branching (ACS), considering multi-neuron constraints. Further, we introduce Cost Adjusted Branching (CAB), which corrects the expected bound improvement with the potentially significantly varying expected computational cost.\nActive Constraint Score Branching The value of a Lagrange parameter \u03b3 provides valuable information about the constraint it enforces. Concretely, \u03b3 > 0 indicates that a constraint is active, i.e., the optimal solution fulfills it with equality. Further, for a constraint g(x) \u2264 0, a larger \u2202x\u03b3g(x) indicates a larger sensitivity of the final bound to violations of this constraint. We compute this sensitivity for our multi-neuron constraints with respect to both ReLU outputs and inputs as \u03b3>P and \u03b3>P\u0302 , respectively, where P and P\u0302 are the multi-neuron constraint parameters. We then define our branching score for a neuron j in layer i as the sum over all sensitivities with respect to its input or output:\nsi,j = |\u03b3(i)>P (i)|j + |\u03b3(i)>P\u0302 (i)|j , (7) Intuitively, splitting the node with the highest cumulative sensitivity makes its encoding exact and effectively tightens all of these high sensitivity constraints. We can efficiently compute those scores without an additional backsubstitution pass.\nCost Adjusted Branching Any complete method will decide every property eventually. Hence, its runtime is a key performance metric. Existing branching heuristics ignore this aspect and only consider the expected improvement in bound-tightness, but not the sometimes considerable differences in computational cost. We propose Cost Adjusted Branching, scaling the expected bound improvement (approximated with the branching score) with the inverse of the cost expected for the split, and then picking the branching decision yielding the highest expected bound improvement per cost. The true cost of a split consists of the direct cost of the next branching step and the change of cumulative cost for all consecutive steps. We find a local approximation considering just the former component, similar to only considering the one-step bound improvement, to be a good proxy. We approximate this direct cost by the number of floating-point operations required to compute the new bounds, refer to App. B for a more detailed description. Note that any approximation of the expected cost can be used to instantiate CAB.\nEnforcing Splits Once the ReLU node to split on has been determined, two subproblems for the negative and positive splits of the corresponding node are generated. This is done by setting the corresponding entries in Eq. (4). As the intermediate bounds for all layers up to and including the one that was split remain unchanged, we do not recompute them."
        },
        {
            "heading": "3.3 SOUNDNESS AND COMPLETENESS",
            "text": "The soundness of the BaB approach follows directly from the soundness of the underlying bounding method discussed in Section 3.1. To show completeness, it is sufficient to consider the limit case where all ReLU nodes are split and the network becomes linear, making DEEPPOLY relaxations exact. To also obtain exact bounds, all split constraints have to be enforced. This can be done by computing optimal \u03b2 for the now convex optimization problem (Wang et al., 2021). It follows that a property holds if and only if the exact bounds thus obtained are positive on all subproblems. We conclude that MN-BAB is a complete verifier."
        },
        {
            "heading": "4 EXPERIMENTAL EVALUATION",
            "text": "We now present an extensive evaluation of our method. First, and perhaps surprisingly, we find that existing MILP-based verification tools (Singh et al., 2019c; M\u00fcller et al., 2022) are more effective in verifying robustness on many established benchmark networks than what is considered state-of-theart. This highlights that next-generation verifiers should focus on and be benchmarked using less regularized and more accurate networks. We take a step in this direction by proposing and comparing on such networks, before analyzing the effectiveness of the different components of MN-BAB in an extensive ablation study.\nExperimental Setup We implement a GPU-based version of MN-BAB in PyTorch (Paszke et al., 2019) and evaluate all benchmarks using a single NVIDIA RTX 2080Ti, 64 GB of memory, and 16 CPU cores. We attempt to falsify every property with an adversarial attack, before beginning certification. For every subproblem, we first lower-bound the objective using DEEPPOLY and then compute refined bounds using the method described in \u00a73.\nBenchmarks We benchmark MN-BAB on a wide range of networks (see Table 3 in App. A) on the MNIST (Deng, 2012) and CIFAR10 datasets (Krizhevsky et al., 2009). We also consider three new residual networks, ResNet6-A, ResNet6-B, and ResNet8-A. ResNet6-A and ResNet6-B have the same architecture but differ in regularization strength while ResNet8-A has an additional residual block. All three were trained with adversarial training (Madry et al., 2018) using PGD, the GAMA loss (Sriramanan et al., 2020) and MixUp data augmentation (Zhang et al., 2021). ResNet6-A and ResNet8-A were trained using 8-steps and = 4/255, whereas 20-steps and = 8/255 were used for ResNet6-B.We compare against \u03b2-CROWN (Wang et al., 2021), a BaB-based state-of-the-art complete verifier, OVAL (Palma et al., 2021; De Palma et al., 2021; Bunel et al., 2020), a BaB framework based on a different class of multi-neuron constraints, and ERAN Singh et al. (2019c); M\u00fcller et al. (2022) combining the incomplete verifier PRIMA, whose tight multi-neuron constraints we leverage in our method, and a (partial) MILP encoding.\nComparison to State-of-the-Art Methods In Table 1, we compare the verified accuracy and runtime of MN-BAB with that of state-of-the-art tools ERAN, \u03b2-CROWN, and OVAL. Perhaps surprisingly, we find that both MNIST and the smallest CIFAR10 benchmark network established over the last years (Singh et al., 2019b) can be verified completely or almost completely in less than 50s per sample using ERAN, making them less relevant as benchmarks for new verification tools. On these networks, all three BaB methods (including ours) are outperformed to a similar degree, with the reservation that we could not evaluate OVAL on MNIST ConvBig due to runtime errors. On the remaining unsolved networks where complete verification via a MILP encoding does not scale, MN-BAB consistently obtains the highest certified accuracy and for all but one also the lowest runtime. On ConvSuper we were not able to find configurations for OVAL and \u03b2-CROWN that did not run out of memory. Comparing the BaB methods, we observe an overall trend that more precise but also expensive underlying bounding methods are most effective on larger networks, where additional splits are less efficient, and complex inter-neuron interactions can be captured by the precise multi-neuron constraints. On these networks, the more complex interactions also lead to more active Multi-Neuron Constraints (MNCs) and hence more informative ACS scores.\n0.0 0.5 1.0 Quantile\n100\n101\n102\n103 Subproblem count ratio\nmean\nmean\nResNet6-A ResNet6-B\nFigure 4: Ratio of subproblems required per property with BABSR and ACS.\nAblation Study To analyze the individual components of MN-BaB, we consider the weakly and heavily regularized ResNet6-A and ResNet6-B, respectively, with identical architecture. Concretely, we show the effect different bounding procedures and branching approaches have in the two settings in Table 2. As verified accuracy and mean runtime are very coarse performance metrics, we also analyze the ratio of runtimes and number of subproblems required for verification on a per-property level, filtering out those where both methods verify before any branching occurred (Figs. 3\u20135). Overall, we observe that the number of visited subproblems required for certification can be reduced by two to three orders of magnitude by leveraging precise multi-\nneuron constraints and then again by another one to two orders of magnitude by our novel branching heuristic, ACS. Our cost-adjusted branching yields an additional speed up of around 50%.\nThe trend of MN-BAB succeeding on more challenging networks is confirmed here. Leveraging MNCs enables us to verify 20% more samples while being around 22% faster (see Table 2) on ResNet6-A while on the more heavily regularized ResNet6-B we only verify 6% more samples. When analyzing on a per-property level, shown in Fig. 6, the trend is even more pronounced. For easy problems, leveraging MNCs and ACS has little impact (points in the lower left-hand corner). However, it completely dominates on the harder properties where only using single-neuron constraints and BABSR takes up to 33 times longer (points below the diagonal).\nEffectiveness of Multi-Neuron Constraints In Fig. 3, we show the ratio between the number of subproblems required to prove the same lower bound on the final objective (either 0, if both methods certify, or the smaller of the two lower bounds at termination) with and without MNCs over the quantile of properties for ResNet6-A (blue) and ResNet6-B (orange). We observe that using MNCs reduces the number of subproblems for both networks by between two and three orders of magnitude. Despite the higher per bounding-step cost, this leads to the use of MNCs increasing the number of verified samples by up to 12% while reducing average certification times (see Table 2).\nEffectiveness of ACS Branching In Fig. 4, we show the ratio between the number of subproblems considered when using BABSR vs. ACS over the quantile of properties. We observe that ACS yields significantly fewer subproblems on most (75%) or all properties on ResNet6-A and\nResNet6-B, respectively, leading to an additional reduction by between one and two orders of magnitude and showing the effectiveness of our novel ACS branching heuristic. Average verification times are reduced by 12% and 21% on ResNet6-A and ResNet6-B, respectively. Note that the relatively small improvements in timings are due to timeouts for both methods yielding equally high runtimes which dominate the mean. FSB is consistently outperformed by ACS, certifying 12% less samples on ResNet6-A.\nEffectiveness of Cost Adjusted Branching In Fig. 5, we show the per property verification times with ACS + CAB over those with ACS. Using CAB is faster (points below the dashed line) for all properties, sometimes significantly so, leading to an average speedup of around 50%. Analyzing the results in Table 2, we observe that CAB is particularly effective in combination with the ACS scores and multi-neuron constraints, where bounding costs vary more significantly."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Neural Network Verification Beyond the Single Neuron Convex Barrier After the so-called (Single Neuron) Convex Barrier has been described by Salman et al. (2019) for incomplete relaxation-based methods, a range of approaches has been proposed that consider multiple neurons jointly to obtain tighter relaxations. Singh et al. (2019a) and M\u00fcller et al. (2022) derive joint constraints over the input-output space of groups of neurons and refine their relaxation using the intersection of these constraints. Tjandraatmadja et al. (2020) merge the ReLU and preceding affine layer to consider multiple inputs but only one output at a time. While the two approaches are theoretically incomparable, the former yields empirically better results (M\u00fcller et al., 2022).\nEarly complete verification methods relied on off-the-shelf SMT (Katz et al., 2017; Ehlers, 2017) or MILP solvers (Dutta et al., 2018; Tjeng et al., 2019). However, these methods do not scale beyond small networks. In order to overcome these limitations, Bunel et al. (2020) formulated a BaB style framework for complete verification and showed it contains many previous methods as special cases. The basic idea is to recursively split the verification problem into easier subproblems on which cheap incomplete methods can show robustness. Since then, a range of partially (Xu et al., 2021) or fully (Wang et al., 2021; Palma et al., 2021) GPU-based BaB frameworks have been proposed. The most closely related, Palma et al. (2021), leverages the multi-neuron constraints from Tjandraatmadja et al. (2020) but yields an optimization problem of different structure, as constraints only ever include single output neurons.\nBranching Most ReLU branching methods proposed to date use the bound improvement of the two child subproblems as the metric to decide which node to branch on next. Full strong branching (Applegate et al., 1995) exhaustively evaluates this for all possible branching decisions. However, this is intractable for all but the smallest networks. Lu & Kumar (2020) train a GNN to imitate the behavior of full strong branching at a fraction of the cost, but transferability remains an open question and collecting training data to imitate is costly. Bunel et al. (2020) proposed an efficiently computable heuristic score, locally approximating the bound improvement of a branching decision using the method of Wong & Kolter (2018). Henriksen & Lomuscio (2021) refine this approach by additionally approximating the indirect effect of the branching decision, however, this requires using two different bounding procedures. De Palma et al. (2021) introduced filtered-smart-branching (FSB), using BaBSR to select branching candidates and then computing a more accurate heuristic score only for the selected candidates. Instead of targeting the largest bound improvement, Kouvaros & Lomuscio (2021) aim to minimize the number of unstable neurons by splitting the ReLU node with the most other ReLU nodes depending on it."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose the complete neural network verifier MN-BAB. Building on the Branch-and-Bound methodology, MN-BAB leverages tight multi-neuron constraints, a novel branching heuristic and an efficient dual solver, able to utilize massively parallel hardware accelerators, to enable the verification of particularly challenging networks. Our thorough empirical evaluation shows how MN-BAB is particularly effective in verifying challenging networks with high natural accuracy and practical relevance, reaching a new state-of-the-art in several settings."
        },
        {
            "heading": "7 ETHICS STATEMENT",
            "text": "Most machine learning based systems can be both employed with ethical as well as malicious purposes. Methods such as ours that enable the certification of robustness properties of neural networks are a step towards more safe and trustworthy AI systems and can hence amplify any such usage. Further, malicious actors might aim to convince regulators that the proposed approach is sufficient to show robustness to perturbations encountered during real world application, which could lead to insufficient regulation in safety critical domains."
        },
        {
            "heading": "8 REPRODUCIBILITY STATEMENT",
            "text": "We will make all code and trained networks required to reproduce our experiments available during the review process as supplementary material and provide instructions on how to run them. Upon publication, we will also release them publicly. We explain the basic experimental setup in Section 4 and provide more details in Section A. All datasets used in the experiments are publicly available. Random seeds are fixed where possible and provided in the supplementary material."
        },
        {
            "heading": "A EXPERIMENT DETAILS",
            "text": "In Table 3 we show the per-sample timeout and the batch sizes that were used in the experiments. The timeouts for the first 4 networks were chosen to approximately match the average runtimes reported by Wang et al. (2021), to facilitate comparability.\nSince we keep intermediate bounds of neurons before the split layer fixed, as described in Section 3.2, the memory requirements for splitting at different layers can vary. We exploit this fact and choose batch sizes for our bounding procedure depending on the layer where the split occurred.\nIn order to falsify properties more quickly, we run a strong adversarial attack with the following parameters before attempting certification: We apply two targeted versions (towards all classes) of PGD (Madry et al., 2018) using margin loss (Gowal et al., 2018) and GAMA loss (Sriramanan et al., 2020), both with 5 restarts, 50 steps, and 10 step output diversification (?).\nA.1 ARCHITECTURES\nIn this section, we provide an overview of all the architectures evaluated in Section 4. The architectures of the convolutional networks for MNIST and CIFAR10 are detailed in Table 4. The architectures of both ResNet6-A and ResNet6-B are given in Table 5"
        },
        {
            "heading": "B SPLIT-COST COMPUTATION FOR CAB",
            "text": "Recall that for CAB, we normalize the branching score obtained with an arbitrary branching heuristic with the (approximate) cost of the corresponding split. The true cost of a split consists of the direct cost of the next branching step and the change of cumulative cost for all consecutive steps. As a local approximation, we just consider the former component.\nWe approximate this direct cost by the number of floating-point operations required to compute the new bounds. This is computed as the sum of floating-point operations required for bounding all intermediate bounds that are recomputed. Our bounding approach only enforces constraints\non neurons preceding the neurons included in the bounding objective. Hence, we only recompute intermediate bounds for layers after the layer where the split occurs, as discussed in \u00a73.2.\nTo compute the cost of recomputing the lower (or upper) bound of one intermediate node, we add up all floating point operations needed to perform the backsubstitution described in \u00a73.1. As backsubstitution is just a series of matrix multiplications, the number of required floating point operations can be deduced from the sizes of the multiplied matrices.\nThus if we split on layer k of an L layer network and the cost of backsubstituion from layer i is Ci and the number of nodes is di, the final cost of the split is:\nL\u2211 i=k+1 2diCi\nWhere the factor 2 comes from the fact that for intermediate bounds we need to recompute both lower and upper bounds. The cost Ci of a full backsubstitution from layer i can be computed as the sum over the cost of backsubstituting through all preceding layers Ci = \u2211i\u22121 j=0 cj , where the cost for a single layer can be computed as follows:\n\u2022 ReLU layer: cj = dj + pj , where pj is the number of multi-neuron constraints. \u2022 Linear layer: cj = #Wj , where #Wj is the number of elements in the weight matrix.\n\u2022 Conv layer: cj = djk2j , where kj is the kernel size."
        }
    ],
    "year": 2022
}