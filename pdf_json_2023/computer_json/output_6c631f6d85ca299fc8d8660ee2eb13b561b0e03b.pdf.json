{
    "abstractText": "Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey data collected as a part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Benjamin C. Warner"
        },
        {
            "affiliations": [],
            "name": "Ziqi Xu"
        },
        {
            "affiliations": [],
            "name": "Simon Haroutounian"
        },
        {
            "affiliations": [],
            "name": "Thomas Kannampallil"
        },
        {
            "affiliations": [],
            "name": "Chenyang Lu"
        }
    ],
    "id": "SP:ebcf28b04557673b5d22dbea41e2dbc7ad478a8d",
    "references": [
        {
            "authors": [
                "H. Abbas",
                "F. Garberson",
                "E. Glover",
                "D.P. Wall"
            ],
            "title": "Machine learning approach for early detection of autism by combining questionnaire and home video screening",
            "venue": "Journal of the American Medical Informatics Association, 25(8): 1000\u20131007.",
            "year": 2018
        },
        {
            "authors": [
                "F. Abut",
                "M.F. Akay",
                "J. George"
            ],
            "title": "Developing new VO2max prediction models from maximal, submaximal and questionnaire variables using support vector machines combined with feature selection",
            "venue": "Computers in biology and medicine, 79: 182\u2013192.",
            "year": 2016
        },
        {
            "authors": [
                "E. Alsentzer",
                "J.R. Murphy",
                "W. Boag",
                "W.-H. Weng",
                "D. Jin",
                "T. Naumann",
                "M. McDermott"
            ],
            "title": "Publicly available clinical BERT embeddings",
            "venue": "arXiv preprint arXiv:1904.03323.",
            "year": 2019
        },
        {
            "authors": [
                "K. Bostrom",
                "G. Durrett"
            ],
            "title": "Byte pair encoding is suboptimal for language model pretraining",
            "venue": "arXiv preprint arXiv:2004.03720.",
            "year": 2020
        },
        {
            "authors": [
                "D. Cer",
                "M. Diab",
                "E. Agirre",
                "I. Lopez-Gazpio",
                "L. Specia"
            ],
            "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 1\u201314. Vancouver, Canada:",
            "year": 2017
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, 785\u2013794.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Chen",
                "K. Wang",
                "J.J. Lu"
            ],
            "title": "Feature selection for driving style and skill clustering using naturalistic driving data and driving behavior questionnaire",
            "venue": "Accident Analysis & Prevention, 185: 107022.",
            "year": 2023
        },
        {
            "authors": [
                "B. Chiu",
                "S. Pyysalo",
                "I. Vuli\u0107",
                "A. Korhonen"
            ],
            "title": "BioSimVerb and Bio-SimLex: Wide-coverage evaluation sets of word similarity in biomedicine",
            "venue": "BMC Bioinformatics, 19.",
            "year": 2018
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "D. Erhan",
                "A. Courville",
                "Y. Bengio",
                "P. Vincent"
            ],
            "title": "Why does unsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference on artificial intelligence and statistics, 201\u2013208",
            "venue": "JMLR Workshop and Conference Proceedings.",
            "year": 2010
        },
        {
            "authors": [
                "P. Gage"
            ],
            "title": "A New Algorithm for Data Compression",
            "venue": "C Users J., 12(2): 23\u201338.",
            "year": 1994
        },
        {
            "authors": [
                "Y. Gu",
                "R. Tinn",
                "H. Cheng",
                "M. Lucas",
                "N. Usuyama",
                "X. Liu",
                "T. Naumann",
                "J. Gao",
                "H. Poon"
            ],
            "title": "Domainspecific language model pretraining for biomedical natural",
            "year": 2021
        },
        {
            "authors": [
                "S. Gururangan",
                "A. Marasovi\u0107",
                "S. Swayamdipta",
                "K. Lo",
                "I. Beltagy",
                "D. Downey",
                "N.A. Smith"
            ],
            "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "I. Guyon",
                "S. Gunn",
                "M. Nikravesh",
                "L.A. Zadeh"
            ],
            "title": "Feature extraction: foundations and applications, volume 207",
            "venue": "Springer.",
            "year": 2008
        },
        {
            "authors": [
                "S. Haroutiunian",
                "L. Nikolajsen",
                "N.B. Finnerup",
                "T.S. Jensen"
            ],
            "title": "The neuropathic component in persistent postsurgical pain: a systematic literature review",
            "venue": "PAIN\u00ae, 154(1): 95\u2013102.",
            "year": 2013
        },
        {
            "authors": [
                "S. Hegselmann",
                "A. Buendia",
                "H. Lang",
                "M. Agrawal",
                "X. Jiang",
                "D. Sontag"
            ],
            "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
            "venue": "arXiv preprint arXiv:2210.10723.",
            "year": 2022
        },
        {
            "authors": [
                "A. Joulin",
                "E. Grave",
                "P. Bojanowski",
                "M. Douze",
                "H. J\u00e9gou",
                "T. Mikolov"
            ],
            "title": "FastText.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651",
            "year": 2016
        },
        {
            "authors": [
                "A. Kraskov",
                "H. St\u00f6gbauer",
                "P. Grassberger"
            ],
            "title": "Estimating mutual information",
            "venue": "Physical review E, 69(6): 066138.",
            "year": 2004
        },
        {
            "authors": [
                "T. Kudo"
            ],
            "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 66\u201375. Melbourne, Australia: Association for",
            "year": 2018
        },
        {
            "authors": [
                "V. Lampos",
                "B. Zou",
                "I.J. Cox"
            ],
            "title": "Enhancing feature selection using word embeddings: The case of flu surveillance",
            "venue": "Proceedings of the 26th International Conference on World Wide Web, 695\u2013704.",
            "year": 2017
        },
        {
            "authors": [
                "E. Lehman",
                "E. Hernandez",
                "D. Mahajan",
                "J. Wulff",
                "M.J. Smith",
                "Z. Ziegler",
                "D. Nadler",
                "P. Szolovits",
                "A. Johnson",
                "E. Alsentzer"
            ],
            "title": "Do We Still Need Clinical Language Models? arXiv preprint arXiv:2302.08091",
            "year": 2023
        },
        {
            "authors": [
                "S.M. Lundberg",
                "S.-I. Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "R. Luo",
                "L. Sun",
                "Y. Xia",
                "T. Qin",
                "S. Zhang",
                "H. Poon",
                "T.-Y. Liu"
            ],
            "title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
            "venue": "Briefings in Bioinformatics, 23(6).",
            "year": 2022
        },
        {
            "authors": [
                "C.D. Manning"
            ],
            "title": "Human language understanding & reasoning",
            "venue": "Daedalus, 151(2): 127\u2013138.",
            "year": 2022
        },
        {
            "authors": [
                "P. May"
            ],
            "title": "Machine translated multilingual STS benchmark dataset",
            "year": 2021
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "U. Niemann",
                "P. Brueggemann",
                "B. Boecking",
                "B. Mazurek",
                "M. Spiliopoulou"
            ],
            "title": "Development and internal validation of a depression severity prediction model for tinnitus patients based on questionnaire responses and sociodemographics",
            "venue": "Scientific reports, 10(1): 4664.",
            "year": 2020
        },
        {
            "authors": [
                "D. Oniani",
                "S. Sivarajkumar",
                "Y. Wang"
            ],
            "title": "FewShot Learning for Clinical Natural Language Processing Using Siamese Neural Networks",
            "venue": "arXiv preprint arXiv:2208.14923.",
            "year": 2022
        },
        {
            "authors": [
                "T. Pedersen",
                "S.V. Pakhomov",
                "S. Patwardhan",
                "C.G. Chute"
            ],
            "title": "Measures of semantic similarity and relatedness in the biomedical domain",
            "venue": "Journal of biomedical informatics, 40(3): 288\u2013299.",
            "year": 2007
        },
        {
            "authors": [
                "H. Peng",
                "F. Long",
                "C. Ding"
            ],
            "title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence, 27(8): 1226\u2013 1238.",
            "year": 2005
        },
        {
            "authors": [
                "A. Radford",
                "J. Wu",
                "R. Child",
                "D. Luan",
                "D. Amodei",
                "I Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "S. Ram\u0131\u0301rez-Gallego",
                "I. Lastra",
                "D. Mart\u0131\u0301nez-Rego",
                "V. Bol\u00f3nCanedo",
                "J.M. Ben\u0131\u0301tez",
                "F. Herrera",
                "A. AlonsoBetanzos"
            ],
            "title": "Fast-mRMR: Fast minimum redundancy maximum relevance algorithm for high-dimensional big data",
            "venue": "International Journal of Intelligent Systems,",
            "year": 2017
        },
        {
            "authors": [
                "R. \u0158eh\u016f\u0159ek",
                "P. Sojka"
            ],
            "title": "Software Framework for Topic Modelling with Large Corpora",
            "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 45\u201350. Valletta, Malta: ELRA. http://is.muni.cz/ publication/884893/en.",
            "year": 2010
        },
        {
            "authors": [
                "N. Reimers",
                "I. Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "B. Remeseiro",
                "V. Bolon-Canedo"
            ],
            "title": "A review of feature selection methods in medical applications",
            "venue": "Computers in biology and medicine, 112: 103375.",
            "year": 2019
        },
        {
            "authors": [
                "B.C. Ross"
            ],
            "title": "Mutual information between discrete and continuous data sets",
            "venue": "PloS one, 9(2): e87357.",
            "year": 2014
        },
        {
            "authors": [
                "A. Roy",
                "S. Pan"
            ],
            "title": "Incorporating medical knowledge in BERT for clinical relation extraction",
            "venue": "Proceedings of the 2021 conference on empirical methods in natural language processing, 5357\u20135366.",
            "year": 2021
        },
        {
            "authors": [
                "V.S. Saridewi",
                "R.F. Sari"
            ],
            "title": "Feature selection in the human aspect of information security questionnaires using multicluster feature selection",
            "venue": "International Journal of Advanced Science and Technology, 29(7 Special Issue): 3484\u2013 3493.",
            "year": 2020
        },
        {
            "authors": [
                "R. Sennrich",
                "B. Haddow",
                "A. Birch"
            ],
            "title": "Neural Machine Translation of Rare Words with Subword Units",
            "year": 2016
        },
        {
            "authors": [
                "R.J. Urbanowicz",
                "R.S. Olson",
                "P. Schmitt",
                "M. Meeker",
                "J.H. Moore"
            ],
            "title": "Benchmarking relief-based feature selection methods for bioinformatics data mining",
            "venue": "Journal of biomedical informatics, 85: 168\u2013188.",
            "year": 2018
        },
        {
            "authors": [
                "M.R. Vila",
                "M.S. Todorovic",
                "C. Tang",
                "M. Fisher",
                "A. Steinberg",
                "B. Field",
                "M.M. Bottros",
                "M.S. Avidan",
                "S. Haroutounian"
            ],
            "title": "Cognitive flexibility and persistent post-surgical pain: the FLEXCAPP prospective observational study",
            "venue": "British journal of anaesthesia, 124(5): 614\u2013",
            "year": 2020
        },
        {
            "authors": [
                "J. Wainer",
                "G. Cawley"
            ],
            "title": "Nested cross-validation when selecting classifiers is overzealous for most practical applications",
            "venue": "Expert Systems with Applications, 182: 115222.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "N. Afzal",
                "S. Fu",
                "L. Wang",
                "F. Shen",
                "M. RastegarMojarad",
                "H. Liu"
            ],
            "title": "MedSTS: a resource for clinical semantic textual similarity",
            "venue": "Language Resources and Evaluation, 54: 57\u201372.",
            "year": 2020
        },
        {
            "authors": [
                "P. Washington",
                "K.M. Paskov",
                "H. Kalantarian",
                "N. Stockham",
                "C. Voss",
                "A. Kline",
                "R. Patnaik",
                "B. Chrisman",
                "M. Varma",
                "Q Tariq"
            ],
            "title": "Feature selection and dimension reduction of social autism data",
            "venue": "In Pacific Symposium on Biocomputing",
            "year": 2019
        },
        {
            "authors": [
                "C. Wei",
                "Y.-C. Wang",
                "B. Wang",
                "C.-C.J. Kuo"
            ],
            "title": "An Overview on Language Models: Recent Developments and Outlook",
            "venue": "arXiv preprint arXiv:2303.05759.",
            "year": 2023
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut",
                "V. Sanh",
                "J. Chaumond",
                "C. Delangue",
                "A. Moi",
                "P. Cistac",
                "T. Rault",
                "R. Louf",
                "M Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "arXiv preprint arXiv:1910.03771",
            "year": 2019
        },
        {
            "authors": [
                "M. Wornow",
                "Y. Xu",
                "R. Thapa",
                "B. Patel",
                "E. Steinberg",
                "S. Fleming",
                "M.A. Pfeffer",
                "J. Fries",
                "N.H. Shah"
            ],
            "title": "The shaky foundations of large language models and foundation models for electronic health records",
            "venue": "npj Digital Medicine, 6(1): 135.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Xiong",
                "S. Chen",
                "Q. Chen",
                "J. Yan",
                "B Tang"
            ],
            "title": "Using character-level and entity-level representations to enhance bidirectional encoder representation from transformers-based clinical semantic textual similarity model: ClinicalSTS modeling study",
            "year": 2020
        },
        {
            "authors": [
                "X. Yang",
                "A. Chen",
                "N. PourNejatian",
                "H.C. Shin",
                "K.E. Smith",
                "C. Parisien",
                "C. Compas",
                "C. Martin",
                "A.B. Costa",
                "M. G Flores"
            ],
            "title": "A large language model for electronic health",
            "venue": "records. npj Digital Medicine,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Because of the high cost of collecting surveys in human subjects research, survey data can suffer from high dimensionality and an relatively inadequate number of examples to learn from. One solution is to select a subset of features when training an ML model, but because this relies upon the already small data to learn from, suboptimal selections can be made. This issue is particularly important for clinical data, where the true relationships between features and labels may be too complex or unknown to make optimal decisions regarding feature choices. We explore this problem through a collection of surveys collected to assess persistent postsurgical pain (PPSP), where a patient experiences surgicallyrelated pain for longer than expected (Vila et al. 2020), and whose exact causes are presently unclear (Haroutiunian et al. 2013)."
        },
        {
            "heading": "Proposed Approach",
            "text": "Survey answers are the result of questions that have text that may be semantically related to a target outcome, and in turn\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nsemantically similar or dissimilar to one another. Intuition suggests that STS is a useful analogue to statistical measures that capture relationships between features, such as mutual information (MI). STS may then be useful for determining which questions are relevant to predicting a target question. Moreover, STS scores may be useful in determining which questions are redundant to each other. This may be especially helpful for smaller datasets where there is a limited amount of information immediately available to learn from. To this end, we evaluate the use of STS scores directly to determine the most relevant features, and test STS scores both as a direct replacement and compliment in algorithms utilizing statistical scores, such as minimal-redundancy-maximalrelevance (mRMR).\nThere appears to be nearly no literature examining feature selection with embeddings or STS. The closest match to our knowledge examined the usage of word2vec continuous bag-of-words embeddings (Mikolov et al. 2013) trained upon Twitter data to select Google search query trends matching the embeddings of a target concept (Lampos, Zou, and Cox 2017). Our analysis differs in several key ways, with the principal difference being that we evaluate selection using the top N STS scores and mRMR, in addition to the procedure in Lampos, Zou, and Cox (2017), where they features select beyond a k-standard deviation threshold. Another major difference is the usage of large language models (LLMs) to calculate scores, which is a more recent model than word2vec, and demonstrably perform better with regards to general and clinical tasks (Roy and Pan 2021). Finally, Lampos, Zou, and Cox (2017) apply their filtration method to a dataset with 35,572 different time-series features, whereas we evaluate a smaller tabular dataset in both examples and dimensions.\nWe present several empirical contributions in this paper:\n\u2022 An examination of how STS scores generated by LMs between feature and target questions can help select features, either alone or in combination with statistical measures such as MI.\n\u2022 Evaluating the performance of STS scores produced by different models in feature selection.\n\u2022 A demonstration of how STS-based feature selection algorithms can reduce overfitting.\nWe make our code available at https://github.\nar X\niv :2\n30 8.\n09 89\n2v 1\n[ cs\n.C L\n] 1\n9 A\nug 2\n02 3\ncom/bcwarner/sts-select, and via pip install sts-select."
        },
        {
            "heading": "Related Work",
            "text": ""
        },
        {
            "heading": "Feature Selection",
            "text": "Fitting high-dimensional data is particularly difficult when the number of examples is low\u2014as is with clinical data collected from human subjects\u2014since a model can easily overfit on the training data. To counter this, we can employ the strategy of feature selection, where a subset of the overall features in a dataset are selected for learning.\nFeature selection methods can be divided into three categories: embedded, wrapper, and filter methods. Embedded methods incorporate feature selection as a part of training, while wrapper methods interact in a feedback loop with the learning model. Filter methods select a subset of features based on properties of the dataset before the model is able to learn on the dataset, which differs from embedded and wrapper methods in that they do not form a feedback loop with the model (Guyon et al. 2008). Because of their independence, they tend to generalize well (Remeseiro and BolonCanedo 2019).\nFeature selection methods used in clinical survey data cover a broad range of techniques. A study examining autism spectrum disorder (ASD) survey data, examined feature selection using principal component analysis, tdistributed stochastic neighbor embedding, and denoising autoencoders; and also found that survey features targeting ASD tend to have high levels of redundancy (Washington et al. 2019). Some of the other feature selection methods found for models involve questionnaires include wrapper models based on random forests (Niemann et al. 2020), bootstrapped feature selection (Abbas et al. 2018), principal component analysis, multicluster feature selection (Saridewi and Sari 2020), permutation importance (Chen, Wang, and Lu 2023), and ReliefF (Abut, Akay, and George 2016).\nOne particularly useful feature selection technique for is minimal-redundancy-maximal-relevance (mRMR) , which aims to maximize the relevance of features to the target, while minimizing the redundancy between selected features. This is particularly useful when we have a small number of features that are correlated and want to ensure a model incorporates as broad as a set of information as possible (Peng, Long, and Ding 2005; Ram\u0131\u0301rez-Gallego et al. 2017).\nUnderpinning the mRMR objective function is the mutual information (MI) between classes and features, which measures the amount of shared information between two distributions.\nCalculating true MI between two features is computationally costly, but can be approximated using one of several methods. The MI approximation methods used in this paper are from scikit-learn (Pedregosa et al. 2011), and are a synthesis of the k-nearest neighbors approaches described in (Kraskov, Sto\u0308gbauer, and Grassberger 2004; Ross 2014)."
        },
        {
            "heading": "Semantic textual similarity",
            "text": "Semantic textual similarity (STS) is a task where a LM is used to score the semantic similarity of two sentences,\ngenerally by evaluating the differences between embeddings generated by a model. Cosine similarity is one typical function used to compute the distance between embeddings (Reimers and Gurevych 2019; Oniani, Sivarajkumar, and Wang 2022).\nSTS scores can be produced with traditional LMs such as word2vec (Mikolov et al. 2013) or FastText (Joulin et al. 2016). They may also be produced with large language models (LLMs), which are a class of LM that is broadly defined as having on the order of high millions or more of parameters; and have been typically built with the transformer architecture (Manning 2022).\nLLMs typically employ the pre-training/fine-tuning paradigm, where a model is trained with an unsupervised learning task, and then modified and trained to complete a supervised task (Devlin et al. 2018; Radford et al. 2019). Pre-training is particularly useful since it results in better generalization (Erhan et al. 2010), and because it means that computationally expensive pre-trained models can be reused for different tasks (Wolf et al. 2019). LLMs are distinct from traditional LMs in that they have demonstrated capabilities at many reasoning tasks involving semantic meaning (Singhal et al. 2022; Wei et al. 2023), and are highly applicable since survey features may involve more complex word relationships.\nClinical language involves vocabulary and semantic meaning that is often not present in non-clinical texts, and various pre-trained architectures exist to fill this gap. To date, there are over 80 clinical LMs available (Wornow et al. 2023), with a diverse set of architectural and training designs. Clinical LMs can perform better than general LMs on tasks specific to the clinical domain (Alsentzer et al. 2019), and can do so more efficiently than a general-purpose models (Lehman et al. 2023)."
        },
        {
            "heading": "Methodology",
            "text": ""
        },
        {
            "heading": "Scoring & Selection",
            "text": "A distinction can be made between the scoring of features, where we measure a feature\u2019s relationship to a target or another feature, and the selection of features, where we then use these scores to select the appropriate features. We evaluate the performance of two principal scoring tools, namely MI and STS. In addition, we evaluate the linear combination of them, with the coefficient for STS being a hyperparameter \u03b1, which we test over a logarithmically-spaced range of 30 values from [10\u22122, 102]. For selection methods that employ these scores, we evaluate selecting the top N feature-target scores, selecting feature-target scores above a given standard deviation k, and selection with mRMR using these scores.\nWe restrict our analysis to filter methods because they because they are fit independently from the rest of the training pipeline, and as such do not have a feedback loop from which overfitting could occur (Remeseiro and BolonCanedo 2019), which are appropriate for small datasets. For all of the aforementioned selection algorithms, as well as the baseline algorithms we compare against, we select 20 features since it is approximately an eighth of the features. For selection by standard deviations, all features with scores\nabove \u00b5+ k\u03c3 are selected. We set k = 0.3 for all Skipgram and FastText scorers due to low variance, and use k = 1 for all others."
        },
        {
            "heading": "LMs and Fine-Tuning Datasets",
            "text": "For generating STS scores, we evaluate the performance of several different LLM, as well as Gensim\u2019s (R\u030cehu\u030ar\u030cek and Sojka 2010) implementation of Skipgram (Mikolov et al. 2013) and FastText (Joulin et al. 2016) for baseline comparisons. We use HuggingFace\u2019s transformers (Wolf et al. 2019) with the sentence transformers library (Reimers and Gurevych 2019) to fine-tune and produce STS scores. We evaluate two models trained on general vocabulary, namely all-MiniLM-L12-v2 (Reimers and Gurevych 2019), and bert-base-uncased (Devlin et al. 2018). We also evaluate the performance of models pre-trained on clinical/scientific text, including Bio ClinicalBERT (Alsentzer et al. 2019), BioMed-RoBERTa-base (Gururangan et al. 2020), and PubMedBERT (Gu et al. 2021).\nTo fine-tune these models, we train the model to predict cosine similarity on one of two combined STS datasets, which we group into a general sentence-level and clinical phrase-level set. We also evaluate the performance of finetuning on both sets. Table 1 highlights the composition of the selected fine-tuning datsets.\nClinicalSTS (Xiong et al. 2020) and MedSTS (Wang et al. 2020) are two clinical sentence-pair datasets, but we were unable to obtain them for fine-tuning. To overcome this limitation, we also evaluate the performance of a Bio CinicalBERT model fine-tuned on just the ClinicalSTS dataset (Mulyar, Schumacher, and Dredze 2019).\nExperiments"
        },
        {
            "heading": "Data",
            "text": "The data is collected from a partially complete set of participants from the P5 - Personalized Prediction of Postsurgical Pain study (IRB #202101123) conducted at the Washington University/BJC Healthcare System.\nA total of 12 surveys were assigned to individual users through the Research Electronic Data Capture (REDCap) system. The principal survey is the one described in Vila et al. (2020), which contains the four target outcome questions, and are described in more detail in Table 3. The other surveys include measures of psychological and physical pain, as well as correlated measures.\nThe dataset was assembled from REDCap on February 6th, 2023. It contains 1631 responses from a total 617 participants as a part of a final goal of 2,000 participants from the Anonymous Healthcare System. Table 2 outlines the key characteristics of the dataset, including number of examples and general demographics."
        },
        {
            "heading": "Data & Model Preparation",
            "text": "Several steps are taken to prepare the survey data for fitting upon the candidate models.\nThe first step taken is to prepare the label from this particular survey dataset. The label is derived from the four questions shown in Table 3 is determined using the binary formula y1 = (Q1\u2227Q2)\u2227(Q3 \u2265 3\u2228Q4 \u2265 3). Once the labels are computed, these features are dropped from the dataset. Since we are attempting to predict PPSP, we filter out examples that where a column indicating six-month completion has a null value.\nParticipants may fill out the surveys across several responses, and we combine the responses to contain the last reported non-null value for each question. We then select a subset of questions pre-determined to be clinically relevant at any level to PPSP, which leaves 131 usable features. Features containing references to image data are then filtered out, and columns containing string-type data with more than 5 unique values are filtered out.\nTo deal with missing entries in survey data, several imputation strategies are applied. For columns with numerical types of data, null entries are replaced with the mean value, and then have the L2 norm applied to that column. Date/time types have the median time imputed, and are then be scaled so the minimum and maximum are 0 and 1 respectively. String types\u2014which we are treating as categorical types given the previous filtering of unique values\u2014will be imputed with the most common value, and then split up into one-hot columns. With these steps, this gives 162 features upon which to train a model.\nVarious feature selection and classifier models were tested using the scikit-learn toolkit (Pedregosa et al. 2011). Classsifier models tested include XGBoost (Chen and Guestrin 2016), linear support vector machine (SVM), multilayer perceptron, Gaussian Na\u0131\u0308ve Bayes (Gaussian NB), and knearest neighbors (k-NN). In addition to testing the proposed variations of mRMR, SelectFromModel (which selects based on the weights of a trained model) with linear SVM and XGBoost are tested. The linear SVM model is tested with C over 10 logarithmically spaced values from [10\u22122, 1], while the XGBoost SelectFromModel has the default settings.\nAn 80%/20% train/test split is used for evaluating overall performance, and 5-fold flat cross-validation (CV) is employed to both select hyperparameters and evaluate the overall performance of the dataset. Nested CV is typically employed for evaluating model selection with small datasets, but experimentally may not be necessary with low numbers of hyperparameters while using specific model types, such as gradient boosted trees (Wainer and Cawley 2021). For this reason, and due to the fact that nested CV with K outerfolds would incur a K-fold increase in run-time, flat 5-fold CV is used. For randomization in NumPy and PyTorch and any of their dependencies, we use the seeds 278797835 and 424989."
        },
        {
            "heading": "Feature Selection Implementation",
            "text": "For baseline selection methods, we employ several filter methods, including selecting the best weights from a linear SVM model and a XGBoost model, recursive feature elimination, and an implementation of ReliefF by Urbanowicz\net al. (2018). For ReliefF, we were unable to use 5-fold CV due to a bug and only test 10 neighbors for hyperaprameters.\nMI between features is calculated using the scikit-learn mutual info regression function, while MI between feature and label is calculated using mutual info classif (Pedregosa et al. 2011), which are calculated using the aforementioned k-NN appproach.\nSince Skipgram and FastText are initialized from a blank state and require sentence-level examples to evaluate word similarity, we are only able to evaluate the performance of training upon general sentence-pairs for STS scoring.\nOther than formatting to support the one-hot vectorization of categorical features, the feature names are unmodified. The persistent pain target name is a post-hoc addition, but the other target names remain unmodified, and we average their STS scores together when evaluating a target."
        },
        {
            "heading": "Results",
            "text": ""
        },
        {
            "heading": "Overall Performance",
            "text": "We evaluated 580 different feature selection and classifier pairings. The best performing pairing overall by test area under the receiver-operator curve (AUROC) scores is multilayer perceptron (MLP) selecting the top N scores, using a linear combination of MI scores and STS scores from PubMedBERT fine-tuned on the combined vocabulary. We highlight several MLP baseline selectors as well as those using PubMedBERT in Table 4."
        },
        {
            "heading": "Feature Selector Performance",
            "text": "Algorithm Performance As seen in Figure 1, the three selection algorithms that we evaluate with MI and STS tend to perform better in terms of AUROC than our baselines. When evaluating the test-train difference, we find that these algorithms tend to have lower levels of overfitting than the baselines.\nScoring Performance When evaluating the performance between the STS and MI scorers, as shown in Figure 2, we find that STS tends to perform better at selection than MI alone. When using a linear combination of both, we find the distribution of performance appears to be roughly the"
        },
        {
            "heading": "Test - Train AUROC",
            "text": "same as STS, but can achieve better test scores. For overfitting performance, we find a similar distribution of behavior. Scoring with STS tends to overfit much less than MI alone, and adding MI does not appear to change the distribution of performance as much.\nFor scoring model, shown in Figure 4, we find that the differences between scoring model tend to be smaller. We do note that using a LLM appears to have a much narrower distribution of performance than the LMs tested, and the differences between choice of LLM appears to be smaller.\nFinally, we find that the differences between a clinical and general fine-tuning dataset, shown in Figure 4, appear to negligible overall. This does not mean, however, that finetuning dataset has no overall effect. The best clinical/scientific pre-trained model, PubMedBERT, performed best with the combined fine-tuning dataset on MLP, which we believe is partially attributable to the difference between those scores and scores produced from the general dataset, as seen\nin Figures 5c and 5b, respectively.\nSelected Features When evaluating the selected features, we see a noticeable difference between those selected using MI and STS scores. Figure 5, which shows MI and selected STS scores for the features and targets for the dataset, highlights why this difference exists, as the STS scores are capable of highlighting relationships between features and targets. The differences in the features selected, are shown in the supplemental material, as well as the performance of the model using the SHapley Additive exPlanations (SHAP) (Lundberg and Lee 2017) model when used with Gaussian Na\u0131\u0308ve Bayes."
        },
        {
            "heading": "Discussion",
            "text": ""
        },
        {
            "heading": "Feature Selection Performance",
            "text": "As shown in the previous section, using STS to select features appears to offer significant performance benefits over\ntraditional feature selection methods. One principal reason that the usage of STS appears to work better than statistical measures like MI is that STS scores are not affected by the curse of dimensionality. As can be seen with MI, many overlapping relationships can cause individual features to share little information, and features that we would expect to be more relevant than others will only have slightly more MI than those that would not be relevant. This is particularly evident when looking at mRMR scores calculated with MI scores, seen in the supplementary material, that dip into the negative: the subsequent feature learned is more redundant than it is relevant, yet there are many unselected features that we would consider to be truly relevant.\nSTS also provides a contrasting, non-correlated, source of information compared to statistical measures like MI. This fact can be seen from the strong contrast in highlighted regions between Figures 5a and 5c/5b. When used in mRMR,\nSTS is clearly more able to highlight redundant regions, especially along the diagonal, and is more able to distinguish between relevant and irrelevant features than MI. This is particularly useful for when MI that results from the training dataset fails to match what we might expect from the population sampled, as STS is not vulnerable to differences in the sample and population distributions."
        },
        {
            "heading": "Survey Writing & Feature Naming",
            "text": "The surveys used here were not designed in anticipation of this type of feature selection algorithm, but we anticipate survey designers may want to write questions with consideration to STS-based feature selection. We offer several guidelines for writing surveys or creating other forms of data with descriptive feature names.\nConsiderations should be made with respect to vocabulary. Words should primarily be derived from the vocabulary used to create tokens, and secondarily from the vo-\ncabulary used to fine-tune the pre-trained model to produce STS scores. Even if a ground-truth label is not given for a possible word-level STS pairing, the LM should be able to produce a meaningful STS score, as the LM will have learned word relationships in its vocabulary during pretraining. Acronyms should be expanded where possible, as an acronym may not be in either vocabulary, and thus be semantically meaningless.\nAlthough the evidence presented here suggests that there may be a negligible difference between the performance of tokenizers\u2014as the models tested use different tokenizers\u2014 subtle differences in tokenization may change the way that semantic relationships are captured. For example, Bostrom and Durrett (2020) note that byte-pair encoding (Gage 1994; Sennrich, Haddow, and Birch 2016) has weaker performance than unigram language modeling (Kudo 2018) with respect to morphological segmentations, implying that features tokenized with the latter algorithm may produce more meaningful STS scores."
        },
        {
            "heading": "Future Work",
            "text": "One area of future work is evaluating the performance of other measures, as the scorers evaluated here both have limitations. MI is incapable of measuring the true amount of information a feature contains in context with other features,\nand STS only represent semantic relationships without any regard to their underlying statistical relationship.\nFuture work could also consider the choice of other models for computing STS. We were not able to perform a complete evaluation due to resource limitations, and further analyses could evaluate larger models such as BioGPT (Luo et al. 2022) and GatorTron (Yang et al. 2022). The evidence presented here suggests that there may be marginal benefits to different scoring models.\nFuture work regarding STS-based feature selection could also consider the use of semantic pairs that specifically rate relevancy and redundancy between pairs of question embeddings, rather than similarity. Relevant and redundant questions may not always be semantically similar, and a dataset for fine-tuning upon this type of task may improve the performance of feature selection techniques that use STS.\nAnother potential area of future work would be to serialize the the feature selection task into a text prompt. Serialization of tablular data into a question prompt for a large language model can achieve high performance in a few-shot learning context (Hegselmann et al. 2022), and serialization of a feature selection objective may also be able to capture further semantic relationships between features."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was made possible through the support of the Department of Defense Congressionally Directed Medical Research Programs award W81XWH-21-1-0736. The authors would like to also thank Hanyang Liu for his valuable guidance on the final writing and publishing of this project."
        }
    ],
    "title": "Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection",
    "year": 2023
}