{
    "abstractText": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model\u2019s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark, MQUAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQUAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zexuan Zhong"
        },
        {
            "affiliations": [],
            "name": "Zhengxuan Wu"
        },
        {
            "affiliations": [],
            "name": "Christopher D. Manning"
        },
        {
            "affiliations": [],
            "name": "Christopher Potts"
        },
        {
            "affiliations": [],
            "name": "Danqi Chen"
        },
        {
            "affiliations": [],
            "name": "Boris Johnson"
        },
        {
            "affiliations": [],
            "name": "Rishi Sunak"
        },
        {
            "affiliations": [],
            "name": "Carrie Johnson"
        }
    ],
    "id": "SP:3ee6dcb76ff4c138a83cfadbfa90cca738bd4c85",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing",
            "year": 2023
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "2022a. Knowledge neurons in pretrained transformers. In Association for Computational Linguistics (ACL)",
            "year": 2022
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "2022b. Knowledge neurons in pretrained transformers. In Association for Computational Linguistics (ACL)",
            "year": 2022
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov"
            ],
            "title": "Editing factual knowledge in language models",
            "venue": "In Empirical Methods in Natural Language Processing (EMNLP)",
            "year": 2021
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Damai Dai",
                "Yifan Song",
                "Jingjing Xu",
                "Zhifang Sui",
                "Lei Li"
            ],
            "title": "Calibrating factual knowledge in pretrained language models",
            "venue": "In Findings of Empirical Methods in Natural Language Processing (EMNLP)",
            "year": 2022
        },
        {
            "authors": [
                "Peter Hase",
                "Mona Diab",
                "Asli Celikyilmaz",
                "Xian Li",
                "Zornitsa Kozareva",
                "Veselin Stoyanov",
                "Mohit Bansal",
                "Srinivasan Iyer"
            ],
            "title": "Do language models have beliefs? Methods for detecting, updating, and visualizing model beliefs",
            "year": 2023
        },
        {
            "authors": [
                "Evan Hernandez",
                "Belinda Z Li",
                "Jacob Andreas"
            ],
            "title": "Measuring and manipulating knowledge representations in language models",
            "venue": "arXiv preprint arXiv:2304.00740",
            "year": 2023
        },
        {
            "authors": [
                "Jason Hoelscher-Obermaier",
                "Julia Persson",
                "Esben Kran",
                "Ioannis Konstas",
                "Fazl Barez"
            ],
            "title": "Detecting edit failures in large language models: An improved specificity",
            "year": 2023
        },
        {
            "authors": [
                "Zeyu Huang",
                "Yikang Shen",
                "Xiaofeng Zhang",
                "Jie Zhou",
                "Wenge Rong",
                "Zhang Xiong"
            ],
            "title": "Transformerpatcher: One mistake worth one neuron",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2023
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave"
            ],
            "title": "Towards unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research",
            "year": 2021
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? In Transactions of the Association of Computational Linguistics (TACL)",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei Zaharia"
            ],
            "title": "Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP",
            "year": 2022
        },
        {
            "authors": [
                "Alex Mallen",
                "Akari Asai",
                "Victor Zhong",
                "Rajarshi Das",
                "Hannaneh Hajishirzi",
                "Daniel Khashabi"
            ],
            "title": "When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex Andonian",
                "Yonatan Belinkov",
                "David Bau"
            ],
            "title": "2022b. Massediting memory in a transformer",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D Manning"
            ],
            "title": "2022a. Fast model editing at scale",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Christopher D Manning",
                "Chelsea Finn"
            ],
            "title": "2022b. Memorybased model editing at scale",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Joseph J. Noh",
                "Siyan Li",
                "William S. Armstrong",
                "Ananth Agarwal",
                "Patrick Liu",
                "Chelsea Finn",
                "Christopher D. Manning"
            ],
            "title": "Enhancing selfconsistency and performance of pretrained language models with NLI",
            "year": 2022
        },
        {
            "authors": [
                "Yasumasa Onoe",
                "Michael JQ Zhang",
                "Shankar Padmanabhan",
                "Greg Durrett",
                "Eunsol Choi"
            ],
            "title": "Can lms learn new entities from descriptions? Challenges in propagating injected knowledge",
            "year": 2023
        },
        {
            "authors": [
                "der",
                "Paul Francis Christiano",
                "Jan Leike",
                "Ryan J. Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge bases? In Empirical Methods in Natural Language Processing (EMNLP)",
            "year": 2019
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A Smith",
                "Mike Lewis"
            ],
            "title": "Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog",
            "year": 2019
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Eric Wallace",
                "Sameer Singh"
            ],
            "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "venue": "In Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Anton Sinitsin",
                "Vsevolod Plokhotnyuk",
                "Dmitry Pyrkin",
                "Sergei Popov",
                "Artem Babenko"
            ],
            "title": "Editable neural networks",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Sotoudeh",
                "Aditya V Thakur"
            ],
            "title": "Correcting deep neural networks with small, generalizing patches",
            "venue": "In Workshop on Safety and Robustness in Decision Making",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "LLaMA: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107",
                "Markus Kr\u00f6tzsch"
            ],
            "title": "Wikidata: a free collaborative knowledgebase",
            "venue": "Communications of the ACM,",
            "year": 2014
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS)",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "ReAct: Synergizing reasoning and acting in language models",
            "venue": "In International Conference on Learning Representations (ICLR)",
            "year": 2023
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen"
            ],
            "title": "Factual probing is [MASK]: Learning vs. learning to recall. In North American Chapter of the Association for Computational Linguistics (NAACL)",
            "year": 2021
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi"
            ],
            "title": "2023a. Least-to-most prompting enables complex reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Sheng Zhang",
                "Hoifung Poon",
                "Muhao Chen"
            ],
            "title": "2023b. Context-faithful prompting for large language models",
            "venue": "arXiv preprint arXiv:2303.11315",
            "year": 2023
        },
        {
            "authors": [
                "Chen Zhu",
                "Ankit Singh Rawat",
                "Manzil Zaheer",
                "Srinadh Bhojanapalli",
                "Daliang Li",
                "Felix Yu",
                "Sanjiv Kumar"
            ],
            "title": "Modifying memories in transformer models",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2021
        },
        {
            "authors": [
                "MEMIT",
                "the pre-computed covariance statistics released by Meng"
            ],
            "title": "For Vicuana-7B, we update model weights at layers {5,6,7,8,9} with the default hyperparameters",
            "venue": "Similarly, we compute the covariance statistics for Vicuna-7B on",
            "year": 2022
        },
        {
            "authors": [
                "D Chain-of-thought"
            ],
            "title": "Prompting for Multi-hop Questions We use chain-of-thought (CoT) prompting (Wei et al., 2022) to maximize model performance. Table 9 shows one simplified example of our prompt",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As large language models (LLMs) are deployed widely, the need to keep their knowledge correct and up-to-date without massive retraining costs becomes increasingly important (Sinitsin et al., 2020). Prior work has proposed knowledge editing methods to incrementally inject a set of new facts into a language model (Zhu et al., 2021; De Cao et al., 2021; Meng et al., 2022a,b; Mitchell et al.,\n*Equal contribution. 1Our datasets and code are publicly available at https:\n//github.com/princeton-nlp/MQuAKE.\n2022a,b), but it is not yet clear whether these methods provide a viable solution of updating and maintaining deployed LLMs.\nTo evaluate these methods, existing benchmarks often focus on measuring whether the edited model can recall the newly injected facts and whether unrelated knowledge remains unchanged post-editing. However, a vital unaddressed question is whether the edited model can handle questions where the answer should change as an entailed consequence of edited facts. For example (see Figure 1), if we update the British Prime Minister from Boris Johnson to Rishi Sunak within a model, the answer to Who is married to the British Prime Minister? should also change as a consequence of this edit.\nTherefore, we propose MQUAKE (Multi-hop Question Answering for Knowledge Editing), a new benchmark for a more complete evaluation of knowledge-editing methods. Each example in MQUAKE consists of a multi-hop question (including {2,3,4}-hop questions) which corresponds to a chain of facts. When we edit one or a few facts in a chain, the edited model needs to propagate the change to entailed consequences of the edited facts. MQUAKE includes a dataset MQUAKE-CF\nar X\niv :2\n30 5.\n14 79\n5v 2\n[ cs\n.C L\n] 2\n9 O\nct 2\n02 3\nbased on counterfactual edits, and another dataset MQUAKE-T of temporal knowledge updates to evaluate model editors on real-world changes.\nWe evaluate state-of-the-art knowledge-editing methods on MQUAKE, testing from editing facts mentioned in one question to editing facts mentioned in a large set of questions. The latter setting evaluates approaches that are designed to handle many edits, such as MEMIT (Meng et al., 2022b). Surprisingly, existing knowledge-editing methods often perform well on answering questions that are paraphrases of the edited fact but fail drastically on questions where the answer should change as a consequence of an edited fact. For example, a GPT-J model edited by ROME (Meng et al., 2022a) can only answer 7.6% of multi-hop questions in MQUAKE-CF, even though it could answer 43.4% of the questions before editing.\nTowards faithful knowledge editing, we propose a simple but effective method, MeLLo, that significantly outperforms existing model editors even with a large number of edits. Instead of updating model weights, MeLLo stores edits in an explicit memory inspired by memory-based editing methods (Mitchell et al., 2022b) and prompts the language model iteratively to interact with the edited facts. Specifically, it decomposes a multi-hop question into sub-questions successively, generates tentative answers, and checks whether it is consistent with edited facts before returning the final answer. Such a method does not require any additional training, and can be easily scaled up to large LMs such as GPT-3 (Brown et al., 2020; Ouyang et al., 2022) unlike methods requiring weight updates. We hope that both our benchmark and proposed method provide additional insights into building faithful knowledge-editing methods."
        },
        {
            "heading": "2 Problem Definition",
            "text": "This section introduces our setting and argues that a model edit is only truly successful if the edited model also returns new correct answers for questions that change as a consequence of the edits."
        },
        {
            "heading": "2.1 Querying Factual Knowledge in LLMs",
            "text": "We represent a fact as a triple (s, r, o), consisting of a subject (s), a relation (r), and an object (o), and manually construct a natural language prompt template tr(\u22c5) for each type of relation r as a way of querying knowledge from a language model (Petroni et al., 2019). This template\ntakes a subject s as input and generates a question or a cloze-style statement tr(s). For instance, given the subject United Kingdom and the relation head of government, we can form a cloze sentence The Prime Minister of the United Kingdom is __.\nWe consider an autoregressive language model f \u2236X \u2192 Y , which takes a piece of text x \u2208 X as input and predicts y \u2208 Y , the continuation of x. Given a fact triple (s, r, o), we can query the language model to recall the fact by feeding the prompt tr(s) as input and matching the output f(tr(s)) with the object o. While prior work has studied how to prompt to extract more knowledge (Jiang et al., 2020; Shin et al., 2020; Zhong et al., 2021), we simply use manually-written templates, as enhancing knowledge retrieval is not our focus."
        },
        {
            "heading": "2.2 Knowledge Editing",
            "text": "The knowledge stored in a language model can be incorrect or become outdated over time. One possible solution is to edit the knowledge on the fly without retraining. A fact edit is defined as a pair of fact triples that share the same subject and relation e = ((s, r, o), (s, r, o\u2217)), which represents the associated object is updated from o to o\u2217. For simplicity, we abbreviate the notation for an edit as e = (s, r, o\u2192 o\u2217) throughout the paper.\nGiven a collection of fact edits E = {e1, e2, . . .} and a language model f , knowledge editing involves learning a function K \u2236 F \u00d7 E \u2192 F that yields an edited language model f\u2217 \u2236 X \u2192 Y , K(f,E) = f\u2217. For the methods we assess in Section 4, K modifies the weights of f in an attempt to incorporate E . Our proposed alternative, MeLLo, is much more lightweight: it keeps f frozen and instead uses E as an external knowledge store to guide generation (Section 5).\nIn previous work (De Cao et al., 2021; Mitchell et al., 2022a,c; Meng et al., 2022a,b), the evaluation focuses on assessing whether the edited model recalls the updated knowledge and whether unrelated knowledge remains unchanged post-editing. To evaluate whether a \u201csingle-hop\u201d edit e = (s, r, o\u2192 o\u2217) is successful with an edited model f\u2217(\u22c5), existing paradigms assess whether f\u2217(tr(s)) is equal to o\u2217 (or assigns o\u2217 a high probability). Additionally, they check correctness by varying tr(s) while keeping semantic equivalence (Meng et al., 2022b)."
        },
        {
            "heading": "2.3 Evaluation of Multi-hop Questions",
            "text": "By only evaluating single-hop questions, existing methods were tested for recalling edited facts.\nIt remains unknown whether an edited model can handle a question where the answer should change as an entailed consequence of an edited fact. We propose to evaluate edited models with multi-hop questions by considering a chain of facts C = \u27e8(s1, r1, o1), . . . , (sn, rn, on)\u27e9, where the object of ith fact also serves as the subject of the next fact in the chain, i.e., oi = si+1. We denote R = [r1, . . . , rn] as the relation set and S = [s1, . . . , sn] as the subject set. We then use C to construct a multi-hop question that asks about the head entity s1, with the answer being the tail entity on. Similar to a single-hop question, we generate a question as tR(S). For example, with a chain consisting of two facts (United Kingdom, head of government, Boris Johnson), (Boris Johnson, spouse, Carrie Johnson), one can write a 2- hop question Who is married to the British Prime Minister? Once one or more facts in the chain are edited, e.g., (United Kingdom, head of government, Boris Johnson\u2192 Rishi Sunak), the language model has to leverage the updated knowledge to answer the question, which we posit as a crucial indicator of a model faithfully updating the fact."
        },
        {
            "heading": "3 MQUAKE: Multi-hop Question Answering for Knowledge Editing",
            "text": "We construct the benchmark MQUAKE (Multihop Question Answering for Knowledge Editing), which contains two datasets. The first, MQUAKECF, is designed as a diagnostic dataset for the evaluation of knowledge editing methods on counterfactual edits. The second, MQUAKE-T, comprises temporal-based knowledge updates and is intended to assess the effectiveness of knowledge editing methods in updating outdated information with current, real facts. We first present the data construction process for MQUAKE-CF and MQUAKE-T. Then, we present the data statistics and evaluation settings, followed by evaluation metrics in the end."
        },
        {
            "heading": "3.1 Data Construction of MQUAKE-CF",
            "text": "Sampling chains of facts Our dataset is constructed based on Wikidata (Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014), a knowledge base consisting of fact triples associated with millions of entities. We first sample chains of facts from Wikidata. We manually select 37 common relations and consider a subgraph that solely comprises these relations and the top 20% of common entities based on hyperlink\ncounts in Wikipedia articles.2 We collect chains that contain N = 2,3,4 triples from the Wikidata subgraph. We also adopt heuristics rules to ensure that the sampled fact triples are coherent and lead to natural questions (see Appendix A.1 for details).\nFiltering unrecallable facts As answering multihop questions requires the model to leverage each single-hop fact, we filter out any chain of facts which contain at least one fact that cannot be recalled by GPT-J (Wang and Komatsuzaki, 2021), which we will mainly evaluate on. To recall singlehop facts, we curate a question template for each relation type following prior work (Petroni et al., 2019; Meng et al., 2022a), and query the model using in-context learning with 8 demonstration examples (see Appendix A.2 for more details).\nGenerating multi-hop questions Given C = \u27e8(s1, r1, o1), . . . , (sn, rn, on)\u27e9, we aim to write a set of questionsQ about the head entity s1 with the gold answer a being the tail entity oN . We leverage ChatGPT (gpt-3.5-turbo) to automatically generate questions given a chain of facts C, because (1) this provides us more diverse question formats of good quality; (2) it is challenging to manually write question templates for all the different types. We prompt ChatGPT to generate three questions for each sampled chain of facts. We include the prompt we used and examples of generated multihop questions in Appendix A.3.\nSampling counterfactual edits So far, we have collected \u27e8Q, a,C\u27e9 (questions, answer, fact triples) for each instance in the dataset. Next, we sample counterfactual edits E and collect the corresponding fact triples C\u2217 and answer a\u2217. Given a chain of n factual triples C = \u27e8(s1, r1, o1), . . . , (sn, rn, on)\u27e9, we randomly sample t \u2208 {1, . . . ,N} counterfactual edits in C. For a triple (s, r, o), we sample a counterfactual object o\u2217 from all possible objects that are related to relation r. We replace (s, r, o) with (s, r, o\u2217) in the chain and update other facts accordingly. We make sure that, after injecting counterfactual edits, the new chain still exists so that we are able to find an updated answer a\u2217. We only keep the sampled edits if the corresponding updated answer a\u2217 is not identical to the original one a. We use the same filtering process as Appendix A.2 to make\n2We focus on common entities as LMs can recall facts about common entities more reliably (Mallen et al., 2023).\no\u2217).\nsure GPT-J can recall all unedited single-hop facts in the chains."
        },
        {
            "heading": "3.2 Data Construction of MQUAKE-T",
            "text": "Following a similar procedure to building MQUAKE-CF, we construct the other segment: MQUAKE-T, focusing on temporal-based, realworld fact updates. We take two dumps of Wikidata: 2021-04 and 2023-04, and obtain the differences between the two versions. We find that most changes in Wikidata come from schema changes, i.e., (Encyclop\u00e9die, instance of, encyclopedia \u2192 written work) instead of actual fact updates. We then manually select 6 relations where the changes are most likely to align with real fact changes, e.g., (United Kingdom, head of government, Boris Johnson\u2192 Rishi Sunak). Similarly, we sample chains of facts and filter out unrecallable facts using GPT-J. When we generate edits given a fact chain, instead of sampling artificial counterfactual facts, we require that edits come from the diff set between the two versions of Wikidata. Note that different from\nMQUAKE-CF, each instance in MQUAKE-T relates to only one edit, because all the edits are about position changes (e.g., head of state) and involving two in a question is not coherent. The goal of this dataset is to evaluate how successfully edited language models can answer questions which involve authentic updates to real-world knowledge."
        },
        {
            "heading": "3.3 Dataset Summary",
            "text": "Dataset format As shown in Table 1, each instance in the MQUAKE dataset is denoted by a tuple d = \u27e8E ,Q, a, a\u2217,C,C\u2217\u27e9, where E is a set of edits that we want to inject into the language model, Q represents multi-hop questions we use to evaluate editing methods (we provide three multi-hop questions), a and a\u2217 denote the correct answer before and after edits, and C and C\u2217 correspondingly represent the factual triples associated with this question before and after editing. A desirable knowledge editing method will inject all the edits in E into the model, and enable the model to internally use the edits and answer the questions.\nData statistics Table 2 summarizes the statistics of the MQUAKE-CF and MQUAKE-T datasets. The MQUAKE-CF dataset consists of more than 9K N -hop questions (N \u2208 {2,3,4}), each of which associates with one or more edits.3 We regard it as a diagnostic dataset to study the ability of edited models leveraging newly injected knowledge through editing methods. The MQUAKE-T dataset includes 1.8K instances, each of them associates with one real-world fact change.\nNumber of edited facts We consider two evaluation scenarios: a) First, we perform knowledge editing on only one instance d, which is associated with up to four edited facts. b) Then, we split the dataset into groups of k instances (k \u2208 {1,100,1000,3000} on MQUAKE-CF and k \u2208 {1,100,500,1868} on MQUAKE-T), and consider all instances in a group at the same time and inject all the edited facts of these instances into the model at once. This harder setting is particularly interesting for editing methods such as MEMIT, which can handle large numbers of edits effectively (Meng et al., 2022b).\n3Throughout the paper, our experiments on MQUAKECF are conducted on a randomly sampled subset of the full dataset which includes 3000 instances (1000 instances for each of {2,3,4}-hop questions) due to limited compute resources."
        },
        {
            "heading": "3.4 Evaluation Metrics",
            "text": "We use the following metrics to measure whether the edits are made successfully. We include detailed formal definitions in Appendix B.\n\u2022 Edit-wise success rate measures how many facts can be successfully recalled from the edited language model.\n\u2022 Instance-wise accuracy measures in how many multi-hop instances, the model can recall all the individual single-hop facts. This is a reference metric for multi-hop performance, as the model must encode each individual fact in order to answer the multi-hop question. We measure instance-wise accuracy both before and after editing the model.\n\u2022 Multi-hop accuracy measures the accuracy of the original and edited language models on multi-hop questions. In our datasets, there are three generated multi-hop questions for each instance. If any of the three questions is correctly answered by the model, we regard it as accurate. This is the main metric that we focus on to study models\u2019 ability to use edited knowledge consistently."
        },
        {
            "heading": "4 MQUAKE Challenges Model Editors",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Language models We use GPT-J (6B) and Vicuna-7B (Chiang et al., 2023), which is a finetuned model based on LLaMA-7B (Touvron et al., 2023) as the baseline models to evaluate knowledge editing approaches. It is worth noting that\nexisting parameter-update methods require access to a white-box language model and are very computationally expensive to apply to large models. In Section 5, we propose a lightweight approach, which can be applied to large black-box language models (Ouyang et al., 2022; Brown et al., 2020).\nKnowledge editing approaches We evaluate the following state-of-the-art knowledge editing approaches on our datasets (more details can be found in Appendix C).\n\u2022 Fine-tuning (FT) simply performs gradient descent on the edits to update model parameters. We follow Zhu et al. (2021) and fine-tune one layer in the model with a norm constraint on weight changes.\n\u2022 MEND (Mitchell et al., 2022a) trains a hypernetwork to produce weight updates by transforming the raw fine-tuning gradients given an edited fact.\n\u2022 ROME (Meng et al., 2022a) first localizes the factual knowledge at a certain layer in the Transformer architecture, and then updates the feedforward network in that layer to insert the new facts.\n\u2022 MEMIT (Meng et al., 2022b) extends ROME to edit a large set of facts. It updates feedforward networks in a range of layers to encode all the facts.\nGiven an edited fact (s, r, o \u2192 o\u2217), we convert it to a cloze statement tr(s) and apply knowledge editing approaches with the objective of correctly predicting the edited object o\u2217 given tr(s). We\ninclude the templates of cloze statement tr for each relation type r in Appendix I.\nEvaluation metrics As discussed in Section 3.4, we report edit-wise success rate, instance-wise accuracy, and multi-hop accuracy in our evaluation. We query the model with either manually-written prompt templates (for single-hop facts) or GPTgenerated questions (for multi-hop fact chains). We adapt in-context learning and prompt the model with demonstrations when calculating instancewise and multi-hop accuracy, in order to encourage the language model to recall and output knowledge in the desirable format (Brown et al., 2020).\nWe also consider chain-of-thought prompting (Wei et al., 2022) with in-context demonstrations to ensure the model\u2019s reasoning ability is fully utilized. See Appendix D for detailed prompts that we used to query language models. We denote the multi-hop accuracy with chain-of-thought prompting as multi-hop (CoT)."
        },
        {
            "heading": "4.2 Results on MQUAKE-CF",
            "text": "Table 3 shows the results on MQUAKE-CF when considering each instance individually across different methods with GPT-J and Vicuna-7B as the editing base models. As shown, all of the editing methods perform better than our fine-tuning baseline. In addition, they all gain traction on edit-wise evaluation, with MEMIT and ROME achieving higher than 90% accuracy with GPT-J and Vicuna7B. In other words, when injecting a small number of edits, these techniques successfully inject the edits into language models and have the edited model recall them at inference time, which corroborates previous findings (Zhu et al., 2021; De Cao et al., 2021; Meng et al., 2022a; Mitchell et al., 2022a,b). Subsequently, a low edit-wise success rate entails a worse instance-wise accuracy (e.g., 59.6% for\nMEND vs. 94.0% for MEND), as instance-wise correctness relies on recalling every fact from the model correctly for multi-hop questions.\nSurprisingly, the performance of edited models fails catastrophically at answering multi-hop questions. Even with the strongest baseline approach, MEMIT, multi-hop performance changes from 43.4% \u2192 8.1% with GPT-J and 30.0% \u2192 7.6% with Vicuna-7B. Our results lead to a surprising conclusion that, although these methods act faithfully when evaluating with single-hop questions, all of them fail catastrophically at answering multihop questions that rely on the edited facts. More importantly, compared to the ability to answer multihop questions prior to edits, model performance drops significantly as well. Our findings suggest that current knowledge-editing techniques, instead of integrating new facts into the model as new internal knowledge, are rather hard coding them into the model by updating weights locally. We hope these results can act as a call to the community to rethink the faithfulness of knowledge-editing methods and\nconduct deeper evaluations of edited models. One hypothesis that these edited models cannot answer our multi-hop questions faithfully is that our prompt is not effective enough. Recent works suggest that providing explanations as Chain-ofthought (CoT) can greatly increase model performance even for models at the scale of 6B models (Wei et al., 2022). We further enhance our prompt with explanations and reevaluate all methods. Details about our CoT prompt template can be found in Appendix D. As shown in Table 3, CoT helps slightly across all settings yet still fails catastrophically at answering multi-hop questions. This further suggests that current knowledge-editing methods fail to update knowledge faithfully."
        },
        {
            "heading": "4.3 Results on MQUAKE-T",
            "text": "We evaluate all methods on GPT-J with real-world knowledge edit on MQUAKE.4 The evaluation results are shown in Table 4. We find that in this setting, all methods except fine-tuning achieve near-perfect performance in terms of edit-wise and instance-wise accuracy. However, on multi-hop questions, the performance drops significantly compared to the base model before editing. We find that MEND works surprisingly well with CoT on MQUAKE-T. We hypothesize that this may be due to MEND being particularly effective in editing certain relations (e.g., head of state). On the\n4We exclude Vicuna-7B on MQUAKE-T as it is trained more recently, and is likely to be contaminated with the new knowledge in our temporal questions.\nother hand, our results show that the edited model with CoT can substantially boost multi-hop performance. This suggests that explicit knowledge recall greatly helps the edited models answer multi-hop questions, while these models struggle to utilize the edited knowledge internally."
        },
        {
            "heading": "4.4 Evaluation with Edits at Scale",
            "text": "We extend our evaluation and consider all the edits from a randomly split group of k instances at the same time (k \u2208 {1,100,1000,3000} on MQUAKE-CF and k \u2208 {1,100,500,1868} on MQUAKE-T). The results are shown in Figure 2. We find that, on both MQUAKE-CF and MQUAKE-T, the multi-hop performance of all methods further drops when injecting more edits into language models at the same time."
        },
        {
            "heading": "5 MeLLo: A Proposal for Editing Large Language Models",
            "text": "In Section 4, our evaluation results show that existing knowledge-editing methods fail catastrophically on multi-hop questions of MQUAKE. In this section, we present a simple but effective alternative, MeLLo (Memory-based Editing for Large Language Models)."
        },
        {
            "heading": "5.1 Method",
            "text": "Figure 3 illustrates how MeLLo answers multi-hop questions. Inspired by memory-based knowledgeediting methods (Mitchell et al., 2022b), MeLLo\nkeeps the base language model frozen and maintains all the edits in an explicit memory. During inference, MeLLo (1) decomposes a multi-hop questions into subquestions; (2) prompts the base language model to provide tentative answers to subquestions; and (3) self-checks whether the tentative answers contradict any edited facts in the memory. MeLLo can be applied easily to LLMs such as GPT-3 (Ouyang et al., 2022; Brown et al., 2020).\nEdited fact memory MeLLo stores all the edited facts explicitly in memory. Specifically, all edited facts are first converted into sentence statements through manually-defined templates. Then, an offthe-shelf retrieval model (we use the pretrained Contriever model; Izacard et al. 2021) is used to embed all the edit statements and save them in a retrieval index. The index takes a query as input and returns an edited fact that is the most relevant (i.e., closest in the embedding space) to the query.\nStep-by-step generation and self-checking To answer multi-hop questions with LLMs, we follow previous works and first prompt the model to decompose the multi-hop questions into multiple simple subquestions (Press et al., 2022; Zhou et al., 2023a). For example, in Figure 3, the first subquestion is Who is Ivanka Trump\u2019s spouse? Second, the model generates a tentative answer (e.g., Jared Kushner) to the subquestion based on the (unedited) knowledge stored in the model. Third, to assess whether the generated answer conflicts with any new knowledge edits, the subquestion is used as a query to retrieve a most relevant editing statement from the edited facts saved in memory. Fourth, the model is prompted to self-check if the retrieved fact contradicts the generated answer. If it does, the model adjusts the intermediate answer to this subquestion using the retrieved statement. Note that it is possible that a subquestion does not relate to any edited fact in memory as the corresponding fact is not edited; in this case, the model is prompted to keep the generated answer as the retrieved edit does not cause a contradiction. Finally, the model either generates the next subquestion of the multi-hop question or outputs the final answer."
        },
        {
            "heading": "5.2 Evaluation Results",
            "text": "We apply MeLLo on GPT-J (Wang and Komatsuzaki, 2021), Vicuna-7B (Chiang et al., 2023), and text-davinci-003 (Ouyang et al., 2022; Brown et al., 2020). Table 5 shows performance of MeLLo on MQUAKE-CF and MQUAKE-T.\nWe find that with the same base model (i.e., GPT-J), MeLLo outperforms MEMIT and MEND significantly across all the settings while being more efficient and requiring no training. When incorporating MeLLo with a stronger LLM (text-davinci-003), MeLLo enlarges the performance gap substantially. This suggests that MeLLo works particularly well on strong base language models which can easily follow the instructions in our prompts. Along with its simplicity and efficacy, we think MeLLo can serve as a strong knowledgeediting baseline for future research. First, it does not require access to white-box model weights, so it is very extensible without any adaptation. Second, our base language model remains intact, avoiding the pitfall of overfitting to editing facts or destroying existing capacities due to weight updates. Third, we store edits in an explicit memory component instead of injecting facts into model parameters, which provides greater controllability in removing or adding knowledge on the fly.\nWe note that in order to answer multi-hop questions correctly after editing, the retriever we use in MeLLo needs to retrieve all the associated edited facts from the memory. In Appendix H, we investigate how retrieval accuracy affects the performance of MeLLo when using GPT-3 as the base model."
        },
        {
            "heading": "6 Related Work",
            "text": "Knowledge-editing methods Past work has investigated different approaches in editing LLMs at scale by injecting new knowledge into static model artifacts (Zhu et al., 2021; Sotoudeh and Thakur, 2019; Dai et al., 2022a; Hase et al., 2023; Zhou et al., 2023b; Dong et al., 2022; Huang et al., 2023). Some of these approaches include locating and modifying model weights that are responsible for specific concepts (Meng et al., 2022a,b; Dai et al., 2022b), and fast adaptation through a small auxiliary editing network (Mitchell et al., 2022a; De Cao et al., 2021). Recent work edits knowledge representations during decoding procedures of LLMs (Hernandez et al., 2023). Our proposed approach MeLLo share a similar spirit with SERAC (Mitchell et al., 2022b) where an explicit memory component is used to maintain all the edited facts. Different from SERAC, which trains additional models to incorporate the memory, MeLLo directly uses the base model to self-check whether the model generations need be adjusted. This allows MeLLo to be easily applied to black-\nbox LMs without any extra training.\nKnowledge-editing evaluation The evaluation metrics for knowledge-editing techniques often involve verifying the updated answers by querying the edited facts or related facts (paraphrased or logically-entailed facts), as well as verifying that irrelevant facts are not corrupted (Meng et al., 2022a; Mitchell et al., 2022a; De Cao et al., 2021; Zhu et al., 2021; Hase et al., 2023). More recent work takes a step forward by evaluating LLMs\u2019 abilities to make inferences based on injected facts (Onoe et al., 2023) (e.g., after learning iPhone is a smartphone, the model should also know iPhone can browse the internet), or measuring the absence of unintended side effects of model edits (HoelscherObermaier et al., 2023). Complementary with existing evaluation tools, MQUAKE particularly focuses on assessing whether edited models can answer multi-hop questions where the answer should change as an entailed consequence, showing that existing approaches fail on those questions.\nPrompting methods for multi-hop QA Since the debut of effective base models such as GPT-3, prompt-based methods combined with an optional retrieval module have become a popular approach in handling multi-step QA tasks (Press et al., 2022; Yao et al., 2023; Khattab et al., 2022). Recent work also seeks to combine external NLI modules to justify whether answers to prompt-based queries are able to handle reasoning-based QA questions (Mitchell et al., 2022c). Our method is similar but more generic since we rely on the LLM itself to perform NLI step-by-step before reaching the final answer."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we present a benchmark MQUAKE that assesses knowledge editing methods for language models via multi-hop questions. We find that although edited language models can effectively recall edited facts, they fail on multi-hop questions that are entailed consequences of the edits. We propose a simple but effective alternative, MeLLo, which significantly outperforms existing knowledge editing methods. MeLLo does not require any additional training and can be applied to large LMs such as GPT-3 (Brown et al., 2020). We hope our work can facilitate future research on developing faithful knowledge editing methods.\nLimitations\nThe limitations of our work are as follows.\n\u2022 We mainly evaluate existing knowledge editing methods on GPT-J (Wang and Komatsuzaki, 2021) and Vicuna (Chiang et al., 2023). The efficacy of these methods on other LLMs remains less explored. Note that existing editing methods are very computationally expensive. We leave the evaluation on other models as future work.\n\u2022 We demonstrate that MeLLo outperforms existing knowledge editing methods on models with > 6B parameters. As MeLLo relies on language models for question decomposition and self-checking, future work may study how MeLLo works with smaller models such as GPT-2 (Radford et al., 2019).\n\u2022 Our proposed memory-based approach, MeLLo, while being very effective on the MQUAKE benchmark, requires manually\ndefined prompts to drive language models on new tasks. Although we believe MeLLo is easy to instantiate on different tasks, we acknowledge this limitation and leave the evaluation on other tasks as future work.\n\u2022 The multi-hop questions in MQUAKE are automatically generated by ChatGPT, rather than being crafted by humans. Although MQUAKE-T already involves real knowledge changes, we posit that the use of human-authored questions could further align MQUAKE with the realistic applications of knowledge editing methods."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Dan Friedman, Tianyu Gao, Eric Mitchell, Mengzhou Xia, Howard Yen, Jiayi Geng for providing valuable feedback. This research is partially supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, and Microsoft Azure credits through the \u201cAccelerate Foundation Models Academic Research\u201d Initiative. ZZ is supported by a JP Morgan Ph.D. Fellowship. CM is a CIFAR Fellow."
        },
        {
            "heading": "A Details of Dataset Construction",
            "text": "A.1 Sampling Fact Chains from Wikidata\nWe collect chains of facts that contain N = {2,3,4} triples from Wikidata. We adopt heuristic rules to ensure that the sampled fact triples are coherent and lead to natural questions. Specifically, we apply the following constraints when sampling fact chains from Wikidata. (1) The sampled chain does not involve a circle; (2) The sampled chain does not contain two triples that share the same relation type; (3) The triples with the object being a country can only appear in the last two hops of the chain; (4) The sampled chain contains up to three object types; (5) All triples with a person or location object are consecutive in the chain; (6) The subject entity associated with the relation headquarters location (P159) must be a company or an organization; (7) In all triples with the relation capital (P36), the subject has to be a country. To use these heuristic rules, we manually label the object types for each relation we consider. For example, the relation head of state (P35) corresponds to a person as the object.\nA.2 Filtering Unrecallable Facts with GPT-J\nWe filter out any chain of facts which contain at least one fact that cannot be recalled by GPT-J. For each relation type, we manually define a question template as well as 8 demonstration examples for in-context-learning. We use in-context-learning to ensure the model can capture the answer format from the context. Table 6 shows an example of the prompt we use to recall facts of the relation developer (P178). We include the question templates of all relations on MQUAKE in Appendix I.\nA.3 Generating Questions using ChatGPT\nGiven a chain of facts, we prompt ChatGPT (gpt-3.5-turbo) to automatically generate multihop questions. The prompt we used is shown in Table 7.\nIn Table 8, we show some randomly selected examples of the questions generated by ChatGPT on MQUAKE-CF. We select 3 instances from 2,3,4\u2212hop questions, each of which contains three generated questions. As shown, ChatGPT successfully transforms the chain of triples into grammatically correct questions. Although these multi-hop questions are synthetic, they are logically consistent with the flow of the triple chains. We believe\nthese generated questions are of sufficient quality for assessing the efficacy of knowledge-editing methods."
        },
        {
            "heading": "B Evaluation Metrics",
            "text": "We evaluate the editing results based on three evaluation metrics: edit-wise success rate, instance-wise accuracy, and multi-hop accuracy. Suppose we have edited a language model and obtain the edited model f\u2217(\u22c5).\nEdit-wise success rate measures how many edited facts can be recalled by the edited language model. Given an edit e = (s, r, o\u2192 o\u2217), the editing success is defined as 1[f\u2217(tr(s)) = o\u2217]. We take the averaged value of the all edits in the dataset and refer it as the edit-wise success rate metric.\nInstance-wise accuracy measures how many instances are there where all the associated facts can be recalled by the language model (either the original or edited one). Given an instance d = \u27e8E ,Q, a, a\u2217,C,C\u2217\u27e9, the instance-wise accuracy before editing is defined as\n1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 \u22c0 (s,r,o)\u2208C [f(tr(s)) = o] \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 ,\nand the instance-wise accuracy post editing is defined as\n1 \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 \u22c0 (s,r,o)\u2208C\u2217 [f\u2217(tr(s)) = o\u2217] \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 .\nWe report the averaged instance-wise accuracy in our evaluation.\nMulti-hop accuracy measures the accuracy on multi-hop questions. We regard an instance being predicted correctly if any of the multi-hop questions are answered correctly by the language model. Given an instance d = \u27e8E ,Q, a, a\u2217,C,C\u2217\u27e9, the multi-hop accuracy before editing is defined as\n1 \u23a1\u23a2\u23a2\u23a2\u23a3 \u22c1 q\u2208Q [f(q) = a] \u23a4\u23a5\u23a5\u23a5\u23a6 ,\nand the multi-hop accuracy post editing is defined as\n1 \u23a1\u23a2\u23a2\u23a2\u23a3 \u22c1 q\u2208Q [f\u2217(q) = a\u2217] \u23a4\u23a5\u23a5\u23a5\u23a6 .\nWe report the averaged multi-hop accuracy in our evaluation.\nC Implementation Details for Knowledge Editing Methods\nC.1 Fine-tuning\nOur fine-tuning baseline (FT) performs gradient descent on the edits to update model parameters. We fine-tune layer 21 of GPT-J and layer 31 of Vicuna-7B. We follow Zhu et al. (2021) and use a norm constraint on weight changes with a coefficient 5 \u00d7 10\u22125 in our implementation.\nC.2 MEND\nWe use the GPT-J MEND editor trained by Meng et al. (2022a). For Vicuna-7B, we train our own MEND editor model on the Wikitext generation editing dataset (Mitchell et al., 2022a) with the different hyperparameters. During inference, we set the learning rate scale to be 1.0.\nC.3 ROME\nFor GPT-J, we use the default hyperparameters of ROME and the pre-computed covariance statistics released by Meng et al. (2022a). For Vicuna-7B, we run ROME to update model weights at layer 9 with the default hyperparameters. We compute the covariance statistics for Vicuna-7B on Wikitext using a sample size of 100,000.\nC.4 MEMIT\nFor GPT-J, we use the default hyperparameters of MEMIT and the pre-computed covariance statistics released by Meng et al. (2022b). For Vicuana-7B, we update model weights at layers {5,6,7,8,9} with the default hyperparameters. Similarly, we compute the covariance statistics for Vicuna-7B on Wikitext using a sample size of 100,000."
        },
        {
            "heading": "D Chain-of-thought Prompting for Multi-hop Questions",
            "text": "We use chain-of-thought (CoT) prompting (Wei et al., 2022) to maximize model performance. Table 9 shows one simplified example of our prompt with CoT."
        },
        {
            "heading": "E Extended Golden Labels for MQUAKE-T",
            "text": "Our MQUAKE-T contains limited test cases. To better assess the model\u2019s original performance on multi-hop questions, we extend the possible golden labels for each multi-hop question. Specifically, we allow outdated answers given smaller language\nmodels tend to be less calibrated. To extend the golden labels, we use GPT-3 (text-davinci-003) to query outdated answers. See the prompt we used in Table 10."
        },
        {
            "heading": "F Prompts used in MeLLo",
            "text": "The prompt we used in MeLLo is shown in Table 11. We first prompt the language model to decompose subquestions. Then the language model generates a tentative question to the subquestion (marked in green text); then we use the generated subquestion to retrieve the most relevant edited fact (marked in light blue text) and append it to the prompt. The model self-checks if the retrieved fact contradicts the generated answer. The prompting procedure goes iteratively until the model generates the final answer."
        },
        {
            "heading": "G Breakdown Results on MQUAKE-CF",
            "text": "Table 12 and Table 13 present the breakdown results on MQUAKE-CF when using GPT-J as the base model. We find that, in all editing methods (1) the performance on 2-hop questions is much higher than 3-hop and 4-hop questions; (2) the performance is worse when there are more edits asssociated with the edited instances.\nH Impact of Retrieval Performance\nIn MeLLo, in order to answer multi-hop questions correctly after editing, the retrieval model needs to retrieve all the associated edited facts (each question is associated with 1-4 edited facts) from the memory. Here we investigate how retrieval accuracy affects the performance of MeLLo when using GPT-3 as the base model. We compute the retrieval accuracy (i.e., how many instances where all the as-\nsociated edited facts are correctly retrieved from the memory) when applying MeLLo on MQUAKE-\nCF with GPT-3 by considering different numbers of edited instances at the same time. As Table 14 shows, the performance of MeLLo decreases if the retrieval accuracy is lower (as a result of considering more instances at the same time). Among those questions where all associated facts are successfully retrieved from memory, MeLLo can answer 73.1% of them correctly. This indicates that retrieval performance can significantly impact the model performance. When we consider more irrelevant knowledge edits in the memory, retrieval can be more challenging, and we expect that using more advanced retrieval techniques can improve the performance."
        },
        {
            "heading": "I Question/Cloze Statement Templates used in MQUAKE",
            "text": "Table 15 shows the question templates and the cloze-style statement templates we use in MQUAKE. We use the question templates to query single-hop facts when we use GPT-J for filtering and use the cloze-style statement templates to convert an edited fact to a natural language statement."
        }
    ],
    "title": "MQUAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    "year": 2023
}