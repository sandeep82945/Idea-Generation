{
    "abstractText": "Meta-reinforcement learning (RL) addresses the problem of sample inefficiency in deep RL by using experience obtained in past tasks for solving a new task. However, most existing meta-RL methods require partially or fully on-policy data, which hinders the improvement of sample efficiency. To alleviate this problem, we propose a novel off-policy meta-RL method, embedding learning and uncertainty evaluation (ELUE). An ELUE agent is characterized by the learning of what we call a task embedding space, an embedding space for representing the features of tasks. The agent learns a belief model over the task embedding space and trains a belief-conditional policy and Q-function. The belief model is designed to be agnostic to the order in which task information is obtained, thereby reducing the difficulty of task embedding learning. For a new task, the ELUE agent collects data by the pretrained policy, and updates its belief on the basis of the belief model. Thanks to the belief update, the performance of the agent improves with a small amount of data. In addition, the agent updates the parameters of its policy and Q-function so that it can adjust the pretrained relationships when there are enough data. We demonstrate that ELUE outperforms state-of-the-art meta RL methods through experiments on meta-RL benchmarks. INDEX TERMS Artificial intelligence, inference, meta learning, reinforcement learning, uncertainty.",
    "authors": [
        {
            "affiliations": [],
            "name": "TAKAHISA IMAGAWA"
        },
        {
            "affiliations": [],
            "name": "TAKUYA HIRAOKA"
        },
        {
            "affiliations": [],
            "name": "YOSHIMASA TSURUOKA"
        }
    ],
    "id": "SP:5339e8308df832b0b75da30f94d69a288f1369c8",
    "references": [
        {
            "authors": [
                "D. Silver",
                "J. Schrittwieser",
                "K. Simonyan",
                "I. Antonoglou",
                "A. Huang",
                "A. Guez",
                "T. Hubert",
                "L. Baker",
                "M. Lai",
                "A. Bolton",
                "Y. Chen",
                "T. Lillicrap",
                "F. Hui",
                "L. Sifre",
                "G. van den Driessche",
                "T. Graepel",
                "D. Hassabis"
            ],
            "title": "Mastering the game of go without human knowledge",
            "venue": "Nature, vol. 550, no. 7676, p. 354, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Berner"
            ],
            "title": "Dota 2 with large scale deep reinforcement learning",
            "venue": "2019, arXiv:1912.06680.",
            "year": 2019
        },
        {
            "authors": [
                "D. Li",
                "H. Wu",
                "J. Zhang",
                "K. Huang"
            ],
            "title": "A2-RL: Aesthetics aware reinforcement learning for image cropping",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8193\u20138201.",
            "year": 2018
        },
        {
            "authors": [
                "J. Park",
                "J.-Y. Lee",
                "D. Yoo",
                "I.S. Kweon"
            ],
            "title": "Distort-and-recover: Color enhancement using deep reinforcement learning",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 5928\u20135936.",
            "year": 2018
        },
        {
            "authors": [
                "R. Furuta",
                "N. Inoue",
                "T. Yamasaki"
            ],
            "title": "PixelRL: Fully convolutional network with reinforcement learning for image processing",
            "venue": "IEEE Trans. Multimedia, vol. 22, no. 7, pp. 1704\u20131719, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Dhingra",
                "L. Li",
                "X. Li",
                "J. Gao",
                "Y.-N. Chen",
                "F. Ahmed",
                "L. Deng"
            ],
            "title": "Towards end-to-end reinforcement learning of dialogue agents for information access",
            "venue": "2016, arXiv:1609.00777.",
            "year": 2016
        },
        {
            "authors": [
                "V. Zhong",
                "C. Xiong",
                "R. Socher"
            ],
            "title": "Seq2SQL: Generating structured queries from natural language using reinforcement learning",
            "venue": "2017, arXiv:1709.00103.",
            "year": 2017
        },
        {
            "authors": [
                "S. Levine",
                "C. Finn",
                "T. Darrell",
                "P. Abbeel"
            ],
            "title": "End-to-end training of deep visuomotor policies",
            "venue": "J. Mach. Learn. Res., vol. 17, no. 1, pp. 1334\u20131373, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S.S. Mousavi",
                "M. Schukat",
                "E. Howley"
            ],
            "title": "Traffic light control using deep policy-gradient and value-function-based reinforcement learning",
            "venue": "IET Intell. Transp. Syst., vol. 11, no. 7, pp. 417\u2013423, 2017. 49506 VOLUME 10, 2022 T. Imagawa et al.: Off-Policy Meta-Reinforcement Learning With Belief-Based Task Inference",
            "year": 2017
        },
        {
            "authors": [
                "X. Liang",
                "X. Du",
                "G. Wang",
                "Z. Han"
            ],
            "title": "A deep reinforcement learning network for traffic light cycle control",
            "venue": "IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1243\u20131253, Feb. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Deng",
                "F. Bao",
                "Y. Kong",
                "Z. Ren",
                "Q. Dai"
            ],
            "title": "Deep direct reinforcement learning for financial signal representation and trading",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 28, no. 3, pp. 653\u2013664, Mar. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Schmidhuber",
                "J. Zhao",
                "andM.Wiering"
            ],
            "title": "Simple principles of metalearning",
            "venue": "Tech. Rep. IDSIA, vol. 69, pp. 1\u201323, Jun. 1996.",
            "year": 1996
        },
        {
            "authors": [
                "R. Mendonca",
                "A. Gupta",
                "R. Kralev",
                "P. Abbeel",
                "S. Levine",
                "C. Finn"
            ],
            "title": "Guided meta-policy search",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 9653\u20139664.",
            "year": 2019
        },
        {
            "authors": [
                "K. Rakelly",
                "A. Zhou",
                "D. Quillen",
                "C. Finn",
                "S. Levine"
            ],
            "title": "Efficient offpolicy meta-reinforcement learning via probabilistic context variables",
            "venue": "Proc. Int. Conf. Mach. Learn., 2019, pp. 5331\u20135340.",
            "year": 2019
        },
        {
            "authors": [
                "C. Finn",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "Proc. Int. Conf. Mach. Learn., 2017, pp. 1126\u20131135.",
            "year": 2017
        },
        {
            "authors": [
                "A. Nichol",
                "J. Achiam",
                "J. Schulman"
            ],
            "title": "On first-order meta-learning algorithms",
            "venue": "2018, arXiv:1803.02999.",
            "year": 2018
        },
        {
            "authors": [
                "L. Zintgraf",
                "K. Shiarlis",
                "M. Igl",
                "S. Schulze",
                "Y. Gal",
                "K. Hofmann",
                "S. Whiteson"
            ],
            "title": "VariBAD: A very good method for Bayes-adaptive deep RL via meta-learning",
            "venue": "Proc. Int. Conf. Learn. Represent., 2020, pp. 1\u201320.",
            "year": 2020
        },
        {
            "authors": [
                "J. Humplik",
                "A. Galashov",
                "L. Hasenclever",
                "P.A. Ortega",
                "Y.W. Teh",
                "N. Heess"
            ],
            "title": "Meta reinforcement learning as task inference",
            "venue": "2019, arXiv:1905.06424.",
            "year": 2019
        },
        {
            "authors": [
                "R. Vuorio",
                "S.-H. Sun",
                "H. Hu",
                "J.J. Lim"
            ],
            "title": "Multimodal model-agnostic meta-learning via task-aware modulation",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 1\u201312.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Duan",
                "J. Schulman",
                "X. Chen",
                "P.L. Bartlett",
                "I. Sutskever",
                "P. Abbeel"
            ],
            "title": "RL2: Fast reinforcement learning via slow reinforcement learning",
            "venue": "2016, arXiv:1611.02779.",
            "year": 2016
        },
        {
            "authors": [
                "M.Duff"
            ],
            "title": "Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes",
            "venue": "Doctoral dissertation, Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, 2002. [Online]. Available: https://scholarworks.umass.edu/dissertations/AAI3039353",
            "year": 2002
        },
        {
            "authors": [
                "T. Yu",
                "D. Quillen",
                "Z. He",
                "R. Julian",
                "K. Hausman",
                "C. Finn",
                "S. Levine"
            ],
            "title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "venue": "Proc. Conf. Robot Learn., 2019, pp. 1094\u20131100.",
            "year": 2019
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "year": 2018
        },
        {
            "authors": [
                "L.P. Kaelbling",
                "M.L. Littman",
                "A.R. Cassandra"
            ],
            "title": "Planning and acting in partially observable stochastic domains",
            "venue": "Artif. Intell., vol. 101, nos. 1\u20132, pp. 99\u2013134, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "M. Igl",
                "L. Zintgraf",
                "T.A. Le",
                "F. Wood",
                "S. Whiteson"
            ],
            "title": "Deep variational reinforcement learning for POMDPs",
            "venue": "2018, arXiv:1806.02426.",
            "year": 2018
        },
        {
            "authors": [
                "S. Kapturowski",
                "G. Ostrovski",
                "W. Dabney",
                "J. Quan",
                "R. Munos"
            ],
            "title": "Recurrent experience replay in distributed reinforcement learning",
            "venue": "Proc. Int. Conf. Learn. Represent., 2019, pp. 1\u201319. [Online]. Available: https://openreview.net/forum?id=r1lyTjAqYX",
            "year": 2019
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "Proc. Int. Conf. Mach. Learn., 2018, pp. 1856\u20131865.",
            "year": 2018
        },
        {
            "authors": [
                "K. Sohn",
                "H. Lee",
                "X. Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 3483\u20133491.",
            "year": 2015
        },
        {
            "authors": [
                "M. Zaheer",
                "S. Kottur",
                "S. Ravanbakhsh",
                "B. Poczos",
                "R.R. Salakhutdinov",
                "A.J. Smola"
            ],
            "title": "Deep sets",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3391\u20133401.",
            "year": 2017
        },
        {
            "authors": [
                "M. Rolinek",
                "D. Zietlow",
                "G. Martius"
            ],
            "title": "Variational autoencoders pursue PCA directions (by accident)",
            "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 12406\u201312415.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Bengio",
                "A. Courville",
                "P. Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798\u20131828, Aug. 2013.",
            "year": 1828
        },
        {
            "authors": [
                "T.M. Moerland",
                "J. Broekens",
                "A. Plaat",
                "C.M. Jonker"
            ],
            "title": "Model-based reinforcement learning: A survey",
            "venue": "2020, arXiv:2006.16712.",
            "year": 2020
        },
        {
            "authors": [
                "N. Tishby",
                "F.C. Pereira",
                "W. Bialek"
            ],
            "title": "The information bottleneck method",
            "venue": "Inria, Bordeaux, France, Tech. Rep., 1999, pp. 368\u2013377, doi: 10.48550/arXiv.1904.06979.",
            "year": 1999
        },
        {
            "authors": [
                "C. Zhang",
                "O. Vinyals",
                "R. Munos",
                "S. Bengio"
            ],
            "title": "A study on overfitting in deep reinforcement learning",
            "venue": "2018, arXiv:1804.06893.",
            "year": 2018
        },
        {
            "authors": [
                "C. Zhao",
                "O. Sigaud",
                "F. Stulp",
                "T.M. Hospedales"
            ],
            "title": "Investigating generalisation in continuous deep reinforcement learning",
            "venue": "2019, arXiv:1902.07015.",
            "year": 2019
        },
        {
            "authors": [
                "K. Cobbe",
                "O. Klimov",
                "C. Hesse",
                "T. Kim",
                "J. Schulman"
            ],
            "title": "Quantifying generalization in reinforcement learning",
            "venue": "inProc. Int. Conf.Mach. Learn., 2019, pp. 1282\u20131289.",
            "year": 2019
        },
        {
            "authors": [
                "A. Goyal",
                "R. Islam",
                "D. Strouse",
                "Z. Ahmed",
                "H. Larochelle",
                "M. Botvinick",
                "Y. Bengio",
                "S. Levine"
            ],
            "title": "InfoBot: Transfer and exploration via the information bottleneck",
            "venue": "Proc. Int. Conf. Learn. Represent., 2018, pp. 1\u201321.",
            "year": 2018
        },
        {
            "authors": [
                "M. Igl",
                "K. Ciosek",
                "Y. Li",
                "S. Tschiatschek",
                "C. Zhang",
                "S. Devlin",
                "K. Hofmann"
            ],
            "title": "Generalization in reinforcement learning with selective noise injection and information bottleneck",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 13978\u201313990.",
            "year": 2019
        },
        {
            "authors": [
                "A.A. Alemi",
                "I. Fischer",
                "J.V. Dillon",
                "K. Murphy"
            ],
            "title": "Deep variational information bottleneck",
            "venue": "Proc. Int. Conf. Learn. Represent., 2017, pp. 1\u201319.",
            "year": 2017
        },
        {
            "authors": [
                "T. Haarnoja",
                "A. Zhou",
                "K. Hartikainen",
                "G. Tucker",
                "S. Ha",
                "J. Tan",
                "V. Kumar",
                "H. Zhu",
                "A. Gupta",
                "P. Abbeel",
                "S. Levine"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "2018, arXiv:1812.05905.",
            "year": 2018
        },
        {
            "authors": [
                "R.M. French"
            ],
            "title": "Catastrophic forgetting in connectionist networks",
            "venue": "Trends Cogn. Sci., vol. 3, no. 4, pp. 128\u2013135, Apr. 1999.",
            "year": 1999
        },
        {
            "authors": [
                "Y. Kim",
                "S. Wiseman",
                "A. Miller",
                "D. Sontag",
                "A. Rush"
            ],
            "title": "Semiamortized variational autoencoders",
            "venue": "Proc. Int. Conf. Mach. Learn., 2018, pp. 2683\u20132692.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Lin",
                "G. Thomas",
                "G. Yang",
                "T. Ma"
            ],
            "title": "Model-based adversarial metareinforcement learning",
            "venue": "2020, arXiv:2006.08875.",
            "year": 2020
        },
        {
            "authors": [
                "H. Fu",
                "H. Tang",
                "J. Hao",
                "C. Chen",
                "X. Feng",
                "D. Li",
                "W. Liu"
            ],
            "title": "Towards effective context for meta-reinforcement learning: An approach based on contrastive learning",
            "venue": "2020, arXiv:2009.13891.",
            "year": 2020
        },
        {
            "authors": [
                "C. Colas",
                "O. Sigaud",
                "P.-Y. Oudeyer"
            ],
            "title": "A hitchhiker\u2019s guide to statistical comparisons of reinforcement learning algorithms",
            "venue": "NEC Res. Inst., New Jersey Inst. Comput. Sci., Center Neural Comput. Hebrew Univ., Jerusalem, Israel, Tech. Rep., 2019, doi: 10.48550/arXiv.physics/0004057.",
            "year": 2019
        },
        {
            "authors": [
                "J. Asmuth",
                "M. Littman"
            ],
            "title": "Learning is planning: Near Bayes-optimal reinforcement learning via monte-carlo tree search",
            "venue": "Proc. 27th Conf. Uncertainty Artif. Intell., 2011, pp. 19\u201326.",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Artificial intelligence, inference, meta learning, reinforcement learning, uncertainty.\nI. INTRODUCTION Deep reinforcement learning (RL) has shown superhuman performance in several domains, such as video games and board games [1], [2]. It also has many practical applications such as computer vision [3]\u2013[5], natural language processing [6], [7], robot control [8], traffic control [9], [10], and finance [11]. However, conventional RL focuses on single task learning and does not reuse experience from past tasks, while humans can leverage useful knowledge from past tasks to learn for the new task. This is a major reason why conventional RL is not as sample-efficient as the way humans learn.\nTo overcome this problem, the concept of meta-learning has been proposed [12]. Meta-learning is a class of methods for learning to efficiently learn on a new task by utilizing previous experience. An instance of meta-learning consists of two phases: meta-training and meta-testing. In meta-training, the agent prepares itself for learning in meta-testing via training tasks. In meta-testing, the agent is evaluated on in terms of\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Nagarajan Raghavan .\nits performance on the task to be solved. Note that the sample efficiency in meta-training/meta-testing is an extension of the conventional notion of sample efficiency.1 Although metalearning usually aims to improve the sample efficiency in meta-testing, the sample efficiency in meta-training is also considered important from the perspective of computational cost [13], [14]. Gradient-based meta-learning methods such as MAML [15] and Reptile [16] learn to efficiently reduce the loss of the model after several steps of the model parameter updates (e.g., weights of neural networks). Finn et al. [15] have shown that the sample efficiency of MAML in metatesting is better than that of naive pretraining methods. However, most RL applications of these methods work with on-policy methods [13], which are less sample-efficient than off-policy methods because on-policy methods cannot reuse the data collected by old policies. In addition, the performance of the learned initial parameters in meta-testing can be very poor in some cases until the parameters are updated. Consider, for example, a set of tasks where the agent aims to\n1Namely, the performance improvement in meta-testing per sample in meta-training/meta-testing.\n49494 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 10, 2022\nreach a goal as fast as possible, and the tasks differ in terms of goal positions. Let us assume that there are two tasks whose goals are in opposite directions from the initial position of the agent; then, the well-trained policies for the tasks require contradicting actions. Thus, in this case, the performance before the parameters update is very poor.\nAnother type of meta-learning methods is called contextbased meta-learning. PEARL [14] is a state-of-the-art metaRL method that belongs to this type. PEARL learns how to infer task information in meta-training and uses it in meta-testing. Due to this inference, PEARL generally needs less data to improve its performance in meta-testing than the gradient-based methods when the tasks in meta-training and meta-testing are similar. In addition, the policy and Q-function in PEARL are trained in an off-policy manner, which can improve its sample efficiency in meta-training. Indeed, PEARL was shown to be more sample-efficient in meta-training than MAML [14]. Other context-based metalearning methods have also been proposed, but those methods are on-policy [17]\u2013[20]. A detailed discussion about them is given in Sect. IV. We summarize the features of the existing meta-RL methods in Table 1. In this paper, we extend the idea of PEARL and propose a novel meta-RL method, embedding learning and uncertainty evaluation (ELUE), which has the following features: Off-policy embedding learning\nIn PEARL, policy training is based on an off-policy method, but the training for task embedding, which is used for calculating distributions over tasks, depends on the current policy, i.e., it is on-policy. By removing the dependency on the current policy from the task embedding learning, we propose a fully off-policy method. Thanks to the policyindependent embedding, the data collected by past policies can be reused.\nPolicy and Q-function conditioned by beliefs over tasks PEARL introduces distributions over tasks (i.e., beliefs), but its policy and Q-function depend on a task variable sampled from the distribution. By contrast, ELUE conditions the policy and Q-function on the belief over tasks. The use of the belief conditioned policy and Q-function has been shown to lead to more efficient exploration [17], [18], [21]. Combination of belief and parameter update The original PEARL does not update the parameters of the policy and Q-function in meta-testing. When there are large differences between the tasks in meta-training and in meta-testing, the learned relations between the task information and policy/Q-functionmay no longer be useful, and thus the original PEARL can fail to improve the performance. To alleviate this drawback, our method modifies the relations by updating the parameters.\nOur contributions are as follows: 1) proposing the novel meta-RL method, ELUE, 2) examining the performance of\nthe proposed method and the existing context-based meta-RL methods in experimental settings not examined in the existing studies (e.g., setting where the task inference is not useful) and 3) showing that the proposed method outperforms the existing ones.\nThe paper is organized as follows. In Sect. II we briefly introduce the background of this work, especially, partially observable Markov decision processes (POMDPs). In Sect. III, we introduce how to embed task information and use it for policies and Q-functions in ELUE. In Sect. IV, we introduce several methods related to ours, e.g., PEARL [14], and discuss the differences between ELUE and those methods. In Sect. V, we compare the sample efficiency of state-of-the-art meta-RL methods and ELUE through experiments in theMeta-World [22] environment and show that ELUE performs better than those methods. Finally, some concluding remarks are given in Sect. VI.\nII. PRELIMINARIES Markov decision processes (MDPs) are models for reinforcement learning (RL) tasks. An MDP is defined as a tuple (S,A,T ,R, \u03c1), where S and A are the state and the action spaces respectively, R : S \u00d7 A \u00d7 R \u2192 [0, 1] is a reward function that determines the probability of the amount of reward and T : S \u00d7 A \u00d7 S \u2192 [0, 1] is a transition function that determines the probability of the next state. \u03c1 is the initial state distribution. Let us denote the policy as \u03c0 , which determines the probability of choosing an action at each state. The objective of RL is to maximize the expected cumulative reward by changing the policy. To learn policies, typical RL methods learn a Q-function, Q(s, a) (s \u2208 S, a \u2208 A) and a V-function, V (s) which return the expected cumulative reward of the policy from s, a and s, respectively. The Q/V-function is often trained by minimizing temporal differences [23].\nWe assume that each task in meta-training andmeta-testing can be represented by an MDP, where S and A are the same across tasks. In addition, we consider tasks to be the same when they differ only in \u03c1 because the difference in \u03c1 does not change the optimal policy. Thus, a different task means that T or R is different.\nIn our problem, it is assumed that the tasks are not observable directly. We treat this problem as a partially observable MDP (POMDP) and introduce a probability over R and T , which is called a belief [17], [18]. For clarity, let us assume that R and T are parameterized by \u03d5 and denote them as R\u03d5 and T\u03d5 . It is known that a POMDP can be transformed into a belief MDP whose states are based on beliefs and that the optimal policy of the belief MDP is also optimal in the original POMDP [24]. We denote a history as ht := (s0, a0, r0, s1, . . . , st ), where s\u03c4 \u2208 S, a\u03c4 \u2208 A, r\u03c4 \u2208 R are the state, the action, and the reward at time \u03c4 , respectively. In our problem, a belief at time t is b\u0303t (\u03d5) := P(\u03d5|ht ), and the state of the belief MDP at time t is s+t = (st , b\u0303t ), which is often called a hyper-state. The objective of our problem is maximizing Eh\u221e [ \u2211 \u221e t=0 \u03b3 trt ] by changing a policy that is\nVOLUME 10, 2022 49495\nconditioned on a hyper-state, where \u03b3 is a discount factor. In our problem, the belief is updated by observations:\nb\u0303t+1(\u03d5) = P(\u03d5|ht+1) \u221d P(\u03d5) t\u220f\n\u03c4=0\nR\u03d5(s\u03c4 , a\u03c4 , r\u03c4 )T\u03d5(s\u03c4 , a\u03c4 , s\u03c4+1) (1)\n\u221d P(\u03d5|ht )R\u03d5(st , at , rt )T\u03d5(st , at , st+1). (2)\nHowever, in general, the exact calculation of this belief update is intractable, so the existing methods approximate beliefs and avoid direct calculation [17], [18], [25], [26]. In the next section, we introduce the approximated belief model, its update, and other parts of our method.\nIII. METHOD In this section, we introduce our method, ELUE, which learns how to infer tasks and how to use beliefs based on embeddings of task features. In addition, to alleviate the gap between meta-training and testing, which may prevent improvements from being made if only belief updating is done, it also adapts the learned policy and Q-function to the meta-test task through the updating of their parameters. A sketch of the architecture of our networks is shown in Fig. 1.\nA. LEARNING EMBEDDING In meta-training, the ELUE agent learns embeddings for task features. Through the embedding learning, the agent learns how to extract sufficient information from data to identify the tasks. In this section, we introduce its theoretical background.\nWe formulate the embedding learning problem as follows. There is a latent task variable z \u2208 Rn, whose density is p(z), where n is the dimension of the task embedding space, and let us assume that the reward rt and next state st+1 are sampled from a parameterized model p\u03c61 (rt , st+1|st , at , z), which is shared across tasks. If ht is observed frequently, a reasonablemodel is expected to give ht a high density. Thus, in proportion to the frequency of ht , maximizing the density, log \u222b p(s0) \u220ft\u22121 \u03c4=0 p\u03c61 (r\u03c4 , s\u03c4+1|s\u03c4 , a\u03c4 , z)\u03c0 (a\u03c4 |h\u03c4 )p(z)dz should lead to a reasonable model. This objective depends on the initial distribution and the current policy. However, as shown in Expression (1), they do not contribute to the belief estimation. Thus, instead of the density, we consider the maximization of the evidence lower bound (ELBO) of the following value:\nlog \u222b t\u22121\u220f\n\u03c4=0\np\u03c61 (r\u03c4 , s\u03c4+1|s\u03c4 , a\u03c4 , z)p(z)dz. (3)\nWe introduce a parameterized variational distribution q\u03c62 , and the ELBO is:\nlog \u222b t\u22121\u220f\n\u03c4=0\np\u03c61 (r\u03c4 , s\u03c4+1|s\u03c4 , a\u03c4 , z)p(z)dz (4)\n\u2265 Eq\u03c62 (z|ht )[ \u2211 \u03c4 log p\u03c61 (r\u03c4 , s\u03c4+1|s\u03c4 , a\u03c4 , z)]\n\u2212DKL(q\u03c62 (z|ht )||p(z)). (5)\nWe maximize this ELBO in a similar way to a conditional variational autoencoder (CVAE) [28], i.e., optimizing the parameters of encoder q and decoder p. The sum of loglikelihoods in the ELBO is permutation-invariant in terms of time t of tuple ct := (st , at , rt , st+1).\nThe order of tuples can be ignored in the belief estimation as shown in (1). We introduce the following structure on the basis of [29] so that q\u03c62 (z|ht ) is permutation-invariant. As shown in [29], a function q(X ) is invariant to the permutation of instances in X , if it can be decomposed into the form g( \u2211\nx\u2208X f (x)). We exploit this fact and, instead of historyconditional posterior q\u03c62 (z|ht ), use a posterior conditioned on\n49496 VOLUME 10, 2022\na set of tuples, q\u03c62 (z|c0:t\u22121) := N ( z; g\u03c62 ( t\u22121\u2211 \u03c4=0 f\u03c62 (c\u03c4 ) )) , (6)\nwhereN (\u00b7) is a Gaussian distribution, and g\u03c62 ( \u2211t\u22121 \u03c4=0 f\u03c62 (c\u03c4 )) outputs the parameters of the distribution. Note that q\u03c62 (z|c0:t\u22121) can be used as an approximated belief over z, i.e., bt (z) and that it can be updated with low computational cost.\nLet us denote the replay buffer of tuples of task i asDi and a set of sampled tuples cit1 , c i t2 , . . . c i tk fromDi as c i t1:k .We define the loss of embedding, Lembed (\u03c61, \u03c62), as\nEi,cit1:k [\u2212Eq\u03c62 (z|cit1:k )\n[ \u2211\nc\u03c4\u2208cit1:k\nlog p\u03c61 (r\u03c4 , s\u03c4+1|s\u03c4 , a\u03c4 , z)]\n+DKL(q\u03c62 (z|c i t1:k )||p(z))]. (7)\nNote that this loss function does not depend on the current policy and we can reuse the data in the replay buffer collected by past policies. Moreover, our belief is based on the random sampling of tuples. Therefore, this training depends less on actual trajectories. This can be an advantage, as PEARL with an encoder whose inputs are trajectories (i.e., sequences of states, actions, and rewards in actual order) was not so effective as that with an encoder based on sampled tuples [14]. In addition to these advantages, thanks to CVAE-like training, the agent can learn disentangled representation of the tasks [30]. This disentangled representation is preferable, because it can reduce sensitivity to non-essential task differences [31].\nIn our implementation, we use two decoders whose outputs are the probability of reward, p\u03c61 (rt |st , at , st+1, z), and that of next state, p\u03c61 (st+1|st , at , z). Although the decoders can be used for model-based learning, we trained the embedding, policy and Q-function in a model-free manner. This is because the asymptotic performance of a model-based method is often worse than that of a model-free method [32]. Note that this embedding training is independent from policy and Q-function training and the outputs of the encoder (i.e., beliefs) are detached when the networks for the Q-function and policy are trained.\nB. LEARNING BELIEF-CONDITIONAL POLICY AND Q-FUNCTION In this section, we introduce how the ELUE agent learns a belief-conditional policy and Q-function in meta-training. While the conventional RL agent learns a policy which maximizes the cumulative reward, Eh\u221e [ \u2211 \u221e t=0 \u03b3 trt ], the ELUE agent learns a policy with an additional information bottleneck (IB) objective [33] to improve generalization performance.\nThe IB is a kind of regularization based onmutual information. While conventional deep RL methods face the problem of generalization [34]\u2013[36], it has been shown that the problem can be alleviated by applying the IB [37], [38].\nWe applied the IB loss based on InfoBot [37], which is an application of the IB for reinforcement learning. In their method, it is assumed that goal information is given explicitly for each task. In our case, goal information is not given, so we use beliefs instead. We introduce policy \u03c0 which can be decomposed into the form \u03c0 (at |s + t ) =\u222b\n\u03c01(wt |s + t )\u03c0 2(at |wt , st )dwt , where wt is an additional variable for the IB objective. We add conditional mutual information I (wt ; b\u0303t |st ) as a penalty to the cumulative reward, and the objective is\nE [ \u221e\u2211 t=0 \u03b3 trt ] \u2212 \u03b2 \u221e\u2211 t=0 \u03b3 t I (wt ; b\u0303t |st )\n= E [ \u221e\u2211 t=0 \u03b3 t { rt \u2212 \u03b2Ewt [ log p(wt |s + t ) p(wt |st ) ]}] . (8)\nThe equation is derived from\nI (wt ; b\u0303t |st ) = E [ log p(wt , b\u0303t |st )\np(wt |st )p(b\u0303t |st )\n]\n= E [ log\np(wt |b\u0303t , st ) p(wt |st )\n] . (9)\nThis conditional mutual information penalty is expected to help obtain task-independent yet useful representations as much as possible.\nHowever, p(wt |st ) in (8) is difficult to compute because of the marginalization by b\u0303t (i.e., by ht ). We thus approximate it by a variational distribution q as in previous work [37]\u2013[39]. In general, for random variables X ,Y ,Z , I (X;Y |Z ) \u2264\nE[log p(X |Y ,Z )q(X |Z ) ] (see Sect. H for the details of derivation). By using the fact, a lower bound of (8) is derived as follows:\nE [ \u221e\u2211 t=0 \u03b3 t { rt \u2212 \u03b2Ewt [ log p(wt |s + t ) q(wt |st ) ]}] . (10)\nAlthough any q is allowed (e.g., Alemi et al. [39] used a Gaussian as q), for simplicity, we fix q(wt |s + t ) to be a uniform distribution. By removing the constant part, the objective is\nE [ \u221e\u2211 t=0 \u03b3 t {rt \u2212 \u03b2Ewt [log p(wt |s + t )]} ] , (11)\nand ELUE maximizes this value by changing its policy. This objective is a variant of the objective in soft actor-critic (SAC) [27], which is one of the most sample-efficient offpolicy RLmethods. SAC learns aQ-function and aV-function in addition to the policy. We update the policy, Q-function, and V-function like SAC. Note that although there is a variant of SAC that does not use the V-function [40], the ELUE agent uses a V-function because PEARL also does.\nHowever, it may not be enough to apply the IB to the policy alone because the actor (policy) and the critic (Q/V-function) depend on each other. Hence, in addition to the policy, we also apply the IB to theQ/V-function andwe regularize the outputs of Q/V-function in meta-training and meta-testing. Like the\nVOLUME 10, 2022 49497\npolicy network, our networks for the Q/V-function can be divided into two parts. We introduce additional variables, wQt and wVt for the IB. The former part outputs parameters of distributions for additional variables wQt and w V t for the inputs, (st , at , bt ) and (st , bt ) respectively. The latter part outputs Q-value and V-value for the inputs, (st , at ,w Q t ) and (st ,w V t ) respectively. The outputs of the latter part are denoted as Q(st , at ,w Q t ) and V (st ,w V t ). The corresponding penalties are EwQt [log p(w Q t |st , at , bt )] and EwVt [log p(w V t |st , bt )] as with the loss for the IB in the policy. There is randomness in the outputs of our Q/V-function. We also use additional techniques [38] to stabilize the target values by removing the randomness. Let \u00b5Q(st , bt , at ) and \u00b5V (st , bt ) denote the expectations of w Q t and w V t . In the training, to calculate the Q/V-values in targets values, we use \u00b5Q(st , bt , at ) or \u00b5V (st , bt ) for the inputs of the latter part of the Q/V-networks instead of wQt or w V t .\nLet us denote a belief conditioned on the tuple set cit1:k\u22121 as bi and the belief updated from bi by an additional tuple, citk , as b \u2032i. For simplicity, we abbreviate the subscript tk in citk and denote it as (s i, ai, r i, s\u2032i). ELUE minimizes the losses Lactor (\u03b8\u03c0 ), LQcritic(\u03b8Q), and L V critic(\u03b8V ), where \u03b8\u03c0 , \u03b8Q, and \u03b8V are the parameters of the policy, Q, V networks, respectively. Lactor (\u03b8\u03c0 ), LQcritic(\u03b8Q), and L V critic(\u03b8V ) are\nEi,cit1:k [Ew,a[\u03b2 log\u03c0\u03b8\u03c0 (w|s i, bi)\u2212Q\u03b8Q (s i, a, \u00b5Q(si, bi, a))]],\n(12)\nEi,cit1:k [(EwQ [Q\u03b8Q (s\ni, ai,wQ)]\u2212Q\u0302(si, bi, ai))2 + Q], and\n(13)\nEi,cit1:k\n[ (V\u03b8V (s i)\u2212 V\u0302 (si))2 + V ] , (14)\nwhere\nQ\u0302(si, bi, ai) = r i + \u03b3V\u03b8\u0304V\n( s\u2032i, \u00b5V\u0304 (s\u2032i, b\u2032i) ) , (15)\nV\u0302 (si) = Ew,a[Q\u03b8Q (s i, a, \u00b5Q(si, bi, a))\n\u2212\u03b2 log\u03c0\u03b8\u03c0 (w|s i, bi)]. (16)\nIn the above equations, Ew,a is the expectation over w and a whose probabilities are \u03c01(w|si, bi) and \u03c02(a|si,w), Q is\nEwQt [log p(w Q t |st , at , bt )] and V is EwVt [log p(w V t |st , bt )], \u03b8\u0304V is a parameter vector that is updated by \u03b8\u0304V \u2190 (1\u2212\u03bb)\u03b8\u0304V+\u03bb\u03b8V , and \u00b5V\u0304 (s\u2032i, b\u2032i) is the output of the former part of the target V network. We optimize the parameters by gradient descent. When we calculate the gradients of the losses, the parameters of the corresponding targets, i.e., those of Q in (12), Q\u0302 in (13), and V\u0302 in (14) are fixed. We show the procedures of our method in meta-training in Algorithm 1. In the training, we randomly sample k , the number of tuples in cit1:k\u22121 , to train the agent in a variety of situations in terms of the amount of data necessary to infer the task. For \u03c01(w|si, bi), \u03c02(a|si,w) and the distributions for wQt and w V t , we use Gaussian distributions, and for bounding the range of actions, we use tanh as in SAC [27].\nAlgorithm 1Meta-Training 1: A set of meta-training tasks, T is given 2: while not done do 3: Sample tasks from T 4: Initialize beliefs 5: for i \u2208 the sampled tasks do 6: for step in data collection steps do 7: Gather data from task i by policy \u03c0 (\u00b7|si, bi) 8: Update belief bi and replay buffer Di\n9: end for 10: end for 11: for step in training steps do 12: Make a set of tasks T \u2032 randomly from T 13: Calculate Lembed for T \u2032, as in formula (7), and update parameters to minimize Lembed 14: Calculate Lactor and Lcritic for T \u2032, as in formulae\n(12), (13), and (14), and update parameters to minimize Lactor and Lcritic\n15: end for 16: end while\nAlgorithm 2Meta-Testing 1: A meta-test task is given 2: while not done do 3: for step in data collection steps do 4: Gather data from meta-test task by policy \u03c0 (a|s, b) 5: Update belief b and replay buffer D 6: end for 7: for step in training steps do 8: Calculate Lactor and Lcritic for the task, as in for-\nmulae (12), (13), and (14) and update parameter to minimize modified Lactor and Lcritic\n9: end for 10: end while\nC. ADAPTATION IN META-TEST In meta-testing, the ELUE agent collects data based on the policy conditioned on a belief. The belief is updated at every time step. After updating the belief enough times, our method updates the parameters of neural networks to alleviate the gap between tasks in meta-training and meta-testing. Pseudo code is shown in Algorithm 2. In meta-testing, there are two differences in the parameter update in meta-training:\n1) The parameters for embedding, \u03c61, \u03c62, are fixed to avoid catastrophic forgetting [41] about what it learned in metatraining. Naive updating of the parameters about embedding in meta-testing leads to catastrophic forgetting because the number of tasks in meta-testing is one, which means that the decoder can reconstruct the reward and next state without the latent task variable information, if the decoder is sufficiently trained in meta-testing. If the output of the decoder is independent from the latent task variable, only the second term of (7) is relevant to the learning of the encoder, which means that the encoder loss is minimized when its output is\n49498 VOLUME 10, 2022\nthe same as that of the prior, p(z). On the basis of these considerations, we fix \u03c61, \u03c62 in meta-testing. 2) After updating belief enough times, we fix and copy it for each policy, Q, and, V network and treat the copied beliefs as the parameters of the networks. More precisely, these beliefs are updated with the weights of the networks by minimizing the corresponding losses (12), (13) and (14). This is expected to help adaptation to the meta-test task by giving additional flexibility to task information processing. This updatemethod is inspired by the semi-amortized variational autoencoder [42], whose update method is a combination of inference and gradient descent. By combining gradient descent, it showed better asymptotic performance. Note that the learning of ELUE in the metatesting after the inference (i.e., updating belief enough times) is the same as learning of SAC in a conventional RL setting, except that 1) there are more parameters because of the belief and 2) it is possible that good initial parameters have already been learned thanks to the meta-training.\nIV. RELATED WORK In this section, we review existing methods related to our method and discuss the differences between them. Note that the difference of the methods is summarized in Table 1 in Sect. I. We here provide more detailed discussions about the difference.\nOur method is inspired by PEARL, but there are essential differences. First, PEARL has no decoder, and the encoder is trained to minimize the critic loss. It is a simple approach, but its embedding can change depending on the current policy. The performance of PEARL was shown to degrade when used with off-policy (i.e., not recent) data [14]. Therefore, PEARL uses an additional buffer for recent data to avoid the degradation; in contrast, our method can train the embedding using old data and does not need an additional buffer. Second, PEARL uses an encoder that can be represented as\nq\u03c62 (z|ht ) \u221d \u220ft\u22121 \u03c4=0N (z;\u00b5\u03c4 , \u03c3 2\u03c4 ) \u221d N (z; \u2211t\u22121 \u03c4=0 \u00b5\u03c4\n\u03c32\u03c4\u2211t\u22121 \u03c4=0\n1 \u03c32\u03c4\n, 1\u2211t\u22121 \u03c4=0\n1 \u03c32\u03c4\n),\nwhere \u00b5\u03c4 and \u03c3 2\u03c4 , the mean and variance of a Gaussian distribution, are the outputs of the neural network, f\u03c62 (\u00b7). As discussed around (6), this is not a general form for encoder representation in terms of permutation invariance among c\u03c4 , while our encoder is represented in a general form. Third, PEARL\u2019s policy and Q-function, \u03c0 (s, z) andQ(s, a, z), where z is sampled from q(z|ht ), are z-conditional, and z itself has no uncertainty information. On the other hand, ours are belief-conditional, which has uncertainty information. Fourth, PEARL only considers inference in meta-testing, while our method considers the updating of the parameters of neural networks.\nThere are other meta-learning methods based on inference; however, these methods are on-policy [17]\u2013[20]. Among these on-policy methods, variBAD [17] is the most related to our method. It considers embedding just as ours and beliefs over the embedding space. However, its sample efficiency is not as high as PEARL as shown in [17]. In addition, its encoder is based on recurrent neural networks whose input is\nsimply a history. Moreover, it does not consider updating the parameters of neural networks in meta-testing.\nAs for off-policy approaches, guided meta policy search [13] is introduced as an off-policy meta-learning method. Their method is based on MAML. In meta-training, it updates the parameters to imitate expert trajectories (outer updates) after updates by policy gradients (inner updates), which means that the inner updates and the updates in metatesting need on-policy data. In addition, it is not based on inference. Lin et al. [43] proposed an off-policy method which uses a decoder and policy conditioned the parameters of the decoder. The parameters correspond to a belief in our method and they are adapted to a meta-test task by gradient descent. Their method is also not inference. It is based on MAML-like parameter updates. Fu et al. [44] proposed a different type of encoder which is trained in a contrastive learning manner.\nV. EXPERIMENTS In this section, we compare the sample efficiency of the existing \u2018\u2018context-based\u2019\u2019 methods (PEARL, variBAD, and RL2 [20]) and our method. In the conventional setting of meta-learning experiments, the tasks in meta-testing and meta-trainingwere in the same category and the tasks differed only in goal positions or weights of torso of the agent [14], [15], [17], [19]. Our experimental settings include not only the conventional ones but also settings where the types of tasks in meta-testing and meta-training are different. The environment of the experiments is Meta-World with MuJoCo 2.0. Meta-World is a collection of Sawyer robot arm tasks, and there are 50 types of tasks and several benchmarks. In a task of Meta-World, each episode length is 150. We explain Meta-World in more detail in Sect. A. In this section, we provide five kinds of experimental results. We also provide some additional results of Meta-World in Sect. D, E and conventional benchmarks in Sect. G, ablation studies for embedding in Sect. F. In addition, we perform one-tailed Welch\u2019s t-tests to compare ELUE and PEARL, the competitive existing method. We follow a multiple comparisons scheme introduced in [45], and conclude that method A is better than method B in total when A is better with a confidence level of 0.025 than B in more than half of comparisons. We compare the second half of the data points instead of the entire data because, in general, there is little difference in the data at the beginning of training in RL.\nFirst, to investigate the sample efficiency of our method, we conducted experiments with the conventional setting. We followed the ML1 benchmark scheme of Meta-World, where the differences in tasks mean differences in goals and the goals are sampled from the same distribution in metatraining andmeta-testing.We chose six types of tasks, basketball, dial-turn, pick-place, reach, sweep-into, window-open, from theML10 benchmark2 (descriptions of the types of tasks\n2The names of the types of tasks in the Meta-World paper are different from those in the Meta-World implementation. We refer to the names in the implementation.\nVOLUME 10, 2022 49499\nwe used are shown in Table 2 in appendices). For each run, 20 meta-training tasks were generated. We ran five trials with different random seeds and evaluated the average episode reward at the first episode in meta-testing. The results shown in Fig. 2 indicate that ELUE achieved higher performance than the other methods. In particular, the statistical tests of the results showed that ELUE was superior to PEARL except for \u2018\u2018dial turn\u2019\u2019.\nSecond, to see the merits of our belief updates in metatesting, we examined the performance of ELUE and PEARL in meta-testing with only inferences (i.e., belief updates in ELUE, and posterior updates and sampling the task variable in PEARL) and that of ELUE without any update of a belief, i.e., keeping a belief being a prior (\u2018\u2018NoBelUpdate\u2019\u2019) for comparison. In the experiment, meta-trained networks at about three million time steps in the first experiment of pickplace were used. The results are shown in Fig. 3. The results show that both methods only need small amounts of data to improve the performance. The performance of ELUE was especially high from the first episode while that of PEARL was low at first and improved in the next episode. In the experiments, although ELUE showed better performance on average, ELUE (and the other agents) sometimes failed to complete the tasks, which may be due to the random initialization. In the tasks of Meta-World, there are large reward differences between successes and failures of the tasks. These are the main reasons for the large variances of the results.\nThird, to examine the sample efficiency in a set of diverse tasks, we conducted experiments, following the\nML10 benchmark scheme, where the types of tasks in metatraining and meta-testing were different. In the benchmark, there are 15 types of tasks, ten for meta-training and five for meta-testing. We evaluated the sample efficiency from two perspectives: the performance in meta-testing in the early stages like Fig. 2 and their improvement as the amount of data increases. For the experiment in the latter perspective, we modified PEARL and variBAD to make them update the parameters of neural networks in meta-testing, although the original versions of them do not update the parameters. In the experiment, meta-trainings were executed five times. For each run, 40 tasks were generated. For each\n49500 VOLUME 10, 2022\nmeta-training, meta-tests were executed six times. The metatests were executed for about two million steps. In the metatests, we used meta-trained networks after 600 iterations (25 million time steps) for ELUE and PEARL, 4000 iterations (32 million time steps) for variBAD. To clarify the amount of improvement with our method, we also executed a variant of ELUE which learns policies from scratch in meta-testing without using beliefs. The results are shown in Fig. 4. The performances of PEARL and ELUE slightly improved as time steps in meta-training increased. As for the learning curves of meta-testing, although the variance of the performance of ELUE was large, ELUE outperformed the othermethods on average. FromScratch and variBAD also slightly improved their performance inmeta-testing, although their improvements were much smaller. This indicates that the agents learned to reach the arm to the object but not to move the object to the goal. Although ELUE was better than PEARL on average in Fig. 4, the difference was not statistically significant.\nFourth, to see the effectiveness of applying the IB to our method, we compared the performances of variants of ELUE which were created by naively combining with ELUE and SAC. We changed the coefficients of entropy in SAC. The other hyper parameters were the same across the variants. The results (Fig. 5) show that ELUE was slightly better than the other variants. This implies that the IB objectives helped generalization of learned policies as shown in [38].\nFifth, to examine the effectiveness of our update method in meta-testing, we compared the performances of three update methods (\u2018\u2018BelGrad\u2019\u2019, \u2018\u2018NoBelGrad\u2019\u2019 and \u2018\u2018Inference\u2019\u2019) in the ML10 benchmark. BelGrad means belief updates by gradient descent after inference, which is used in meta-test of ELUE. NoBelGrad means that gradient descent updates are applied to not the belief but the parameters of neural networks. Inference means no gradient updates. NoBelGrad and Inference were executed three times for each meta-training. We used the same settings and the same meta-trained networks in the experiment of Fig. 4. The results (Fig. 6) show that the performance of Inference did not improve as the amount of data increased, and that BelGrad was better than NoBelGrad.\nVI. CONCLUSION We have proposed a novel off-policy meta-learning method, ELUE, which learns the embeddings of task features, beliefs\nVOLUME 10, 2022 49501\nover the embedding space, belief-conditional policies and Q-functions. Owing to the belief-conditional policies, the performance can be improved by updating the beliefs, especially when a meta-test task is similar to meta-training tasks. ELUE also updates the parameters of neural networks in meta-testing, which can alleviate the gap between tasks in meta-testing and those in meta-training. In the experiments with Meta-World benchmarks, we examined the sample efficiency of ELUE and existing methods in the two cases where the tasks in meta-training and meta-testing were similar and diverse. The results show that ELUE outperforms the other methods in both cases.\nThere are topics to be researched in the future. Althoughwe show that ELUE was better than PEARL, both methods were not always successful in solving the task by inference alone. One promising solution to this limitation would be letting the agent create a variety of auxiliary tasks and learn from both the given tasks and those tasks. In addition, we believe that we can improve the performance of inference by using a more sophisticated inference model of the task variables. In ELUE, we use the Gaussian distribution for task inference. We believe that our inference model provides a good starting point for further research.\nAPPENDIX A META-WORLD For convenience, we briefly explain Meta-World. In MetaWorld, there are 50 types of tasks using a Sawyer robot arm. The descriptions of the 15 types of tasks we use are shown in Table 2. The dimensions of the observation and action of each task are six and four, respectively. The amount of reward is determined based on only the center positions of fingers of the arm, an object, and a goal. Here, the goal position cannot be observed by the agent. In the reach tasks, only the center position of the fingers and the goal is related to the reward. In the other 14 types of tasks, the position of the object is also related to the reward. In the tasks, the agent can get only low reward by reaching the fingers to the object, while it can get high reward by moving the object (by the fingers) to the goal. The latter means success of the task.\nAPPENDIX B ADVANTAGE OF BELIEF CONDITIONAL Q-VALUE In this section, we discuss the advantage of Q-value and policy based on a belief (e.g., ELUE and variBAD) over those based on a sampled variable from the belief (e.g., PEARL) in general perspectives.\nLet us assume that the transition and reward functions of an MDP are not known for the agent, as in ordinary reinforcement learning settings. In this case, the agent has to estimate the transition and reward functions, while maximizing the return with limited task knowledge. It is referred to as the exploration-exploitation dilemma. It is known that the Bayes optimal policy is conditioned on a state and a belief [21], [46]. For example, on a locomotion task where the agent have to reach an unknown goal, the Bayes optimal policy may be seeking/going straight to the goal when the\nuncertainty of the goal position is high/low. To learn such a policy by learning methods based on Q-values, e.g., actorcritic, it is needed that the Q-values reflect the uncertainty of the goal position in general. ELUE and variBAD are based on SAC and A2C, respectively, which are variants of actor-critic methods. ELUE and variBAD can learn such a policy based on their Q-values because they are conditioned on the belief.\nPEARL, which is based on SAC, uses a Q-value conditioned on the sampled variable z from the belief. To learn such a policy, z has to be different depending on the uncertainty. However, in general, the beliefs overlap each other and z can be the same value, which means that learning the Bayes optimal policy is difficult.\nAPPENDIX C IMPLEMENTATION DETAILS We provide additional explanations about our implementation. In our implementation, we execute initial sampling at the beginning of meta-training and meta-testing. In the sampling phase, the agent collects data (lines 6 \u2013 9 in Algorithm 1) for all tasks. After this phase, we execute initial embedding training. This is done by a similar way to existing methods for training variational autoencoders [17], [47].\nIn meta-testing, we fix the belief after the initial sampling phase. This is because we found that the improvement by belief updates was very fast and even one episode was enough for the convergence of the performance, as shown in Fig. 3.\nOur policy, Q and V networks are implemented with multilayer perceptrons having two hidden layers whose dimensions are 300. The layers for the IB outputs are inserted between the two hidden layers (five layers in total). The dimension of additional variables for the IB are 256. The dimension of the mean and standard deviation of the beliefs are 15 in ML10 and 5 in ML1 and the other tasks. The coefficients of the KL loss in the embedding loss are 0.1 in ML10 and 1 in ML1 and the other tasks. The coefficients of the IB losses are 0.01 in the policy, Q and V networks. The coefficient of entropy loss in PEARL is also 0.01. We use Adam optimizer. The code is avalable at https://github.com/imagawat/ELUE.git.\nAPPENDIX D ML1 META-TESTING In addition to the ML10 benchmark (Fig. 6), we compare learning methods in meta-testing of pick-place on the ML1 benchmark. The experimental settings were the same as the experiments of Fig. 3 except for the number of time steps in meta-testing. The result is shown in Fig. 7. The results show that the performances of Inference and PEARLOrg were almost the same from the beginning to end, while those of BelGrad, NoBelGrad, and PEARLGrad were improved. In PEARL and ELUE, the performance of methods with the parameter updates were worse than that of methods without the parameter updates at first but gradually improved, and at last became better. In Fig. 7, the difference between BelGrad and NoBelGrad was not so clear as that in Fig. 6.\n49502 VOLUME 10, 2022\nFIGURE 7. The comparison of learning curves in meta-testing. \u2018\u2018PEARLOrg\u2019\u2019 is original PEARL\u2019s update i.e., updating posterior and sampling a task variable from the posterior and \u2018\u2018PEARLGrad\u2019\u2019 is PEARL with updating the parameters of neural networks as \u2018\u2018PEARL\u2019\u2019 in Fig. 4 (c). The other methods are the same as the methods in Fig. 6.\nAPPENDIX E ML1 SUPPLEMENTAL RESULTS We show supplementary results about Fig. 5. The results are shown in Fig. 8. These results are not included in Fig. 5 because of its visibility. The results of Fig. 5 and 8 suggest that the IB objectives help generalization of policies.\nWe also show supplementary results about Fig. 2. Fig. 9 is the third episode rewards of the same experiments of Fig. 2. The results show that the performance of PEARL was better than that in the first episode. However, even in the third episode, ELUE outperformed PEARL in some tasks.\nTo see what happened in meta-testing in pick-place, we analyze the trajectories of the center of the robot arm\u2019s fingers and the objects in the tasks. The results are shown in Fig. 10. The experimental settings were the same as that of Fig. 3. Fig. 10 show that PEARL did not move the object in the first episode in some runs, while ELUE moved the object in all runs. Note that transferring learned policies is difficult when there are only a small number of meta-train tasks. That is because in such a situation, the probability that a meta-test\nFIGURE 8. The comparison of variants of ELUE. The vertical axis is the average episode rewards in meta-testing and the horizontal axis is the number of episodes.\ntask is not similar to any of the meta-train tasks is high. In our ML1 experiment, therewere only 20meta-train tasks, and this may be one of the reasons for the difficulty of transferring policies to the meta-test task."
        },
        {
            "heading": "APPENDIX F LEARNING POLICIES WITHOUT EMBEDDING",
            "text": "For an ablation study, we examine the performance of ELUE without the embedding learning (i.e., SAC with the IB objectives), which is referred to as \u2018\u2018NoEmb\u2019\u2019. NoEmb does not have the ability to identify what the current task is. This can be a drawback, e.g., each task requires contradicting actions at the same state. After meta-training in pick-place and ML10, we executed meta-testing for each benchmark. The experimental settings were the same as those of the former experiments inML1 andML10. The results are shown in Fig. 11. For comparison, the other results are also shown in Fig. 11. To see the performance at the early stage, we show the third episode reward instead of the first one because the performance of PEARL was better as shown in Fig. 2 and 9. The results show that the performance of ELUE improved faster than that of NoEmb. In ML10, the third episode\nVOLUME 10, 2022 49503\nreward of ELUE was better than that of NoEmb, while there was not a clear difference in the performance in ML1. The results imply that a method without task information\nlike NoEmb is one of the competitive baselines in some benchmarks, although it was not analyzed in some existing work [14], [17].\n49504 VOLUME 10, 2022\nAPPENDIX G CONVENTIONAL BENCHMARKS We examine the sample efficiency of our method in the conventional benchmarks, ant-fwd-back and humanoid-dir, which are extensions of MuJoCo robot control tasks in OpenAI gym. In the benchmarks, tasks differ in terms of goal directions of the ant robot or the humanoid robot. In the tasks, their episode rewards are determined by the agent speed in the goal direction, alive bonus, and the other cost e.g., control cost. The alive bonus is the bonus of the agent being \u2018\u2018alive\u2019\u2019 e.g., not falling and the control cost is the cost of the agent to execute actions. In ant-fwd-back, there are only two tasks, tasks with forward and backward directions. In these benchmarks, unlike Meta-World, the episode ends when the agent falls even before 150 steps. The results are shown in Fig. 12. As for the ant benchmark, the performance of NoEmb did not improve more than about 150, while that of PEARL and ELUE gradually improved. Although the learning speed of ELUE was slightly worse than those of PEARL in the third episode reward, the final performance of ELUE was slightly\nbetter. In the first episode, ELUE achieved better results like that in ML1 benchmarks.\nAs for the humanoid benchmark, the first and third episode rewards were almost the same among the methods and their episode rewards were about 750. It seems that the inference was not helpful for improvement of their performances. In this benchmark, the alive bonus was five. Thus, if the agent is alive in a whole episode, its episode reward is 5 \u00d7 150 + goal direction bonus \u2212 control cost. These results imply that most of the reward came from the alive bonus and that the goal direction bonus may be too small to learn different behaviors from task to task.\nAPPENDIX H THEORETICAL SUPPLEMENT For any random variables X ,Y ,Z , (and their realization x, y, z,) and variational distribution q, we show I (X;Y |Z ) \u2264\nE [ log p(X |Y ,Z )q(X |Z ) ] , as with in (14) in [39].\nI (X;Y |Z ) = E [ log\np(X |Y ,Z ) p(X |Z )\n] (17)\nVOLUME 10, 2022 49505\nand KL divergence DKL(p(\u00b7|z)||q(\u00b7|z)) \u2265 0. Thus,\nE[\u2212 log p(X |Z )] \u2264 E[\u2212 log q(X |Z )] (18)\u222b\u222b p(z)p(x|z) log\n1 p(x|z) dxdz\n\u2264 \u222b\u222b p(z)p(x|z) log\n1 q(x|z) . (19)\nTherefore, I (X;Y |Z ) is\nE [ log\np(X |Y ,Z ) p(X |Z )\n] (20)\n= \u222b\u222b\u222b p(z)p(x, y|z) log\np(x|y, z) p(x|z) dxdydz (21)\n= \u222b\u222b\u222b p(z)p(x|z)p(y|x, z) log\np(x|y, z) p(x|z) dxdydz (22)\n\u2264 \u222b\u222b\u222b p(z)p(x|z)p(y|x, z) log\np(x|y, z) q(x|z) dxdydz (23)\n= E [ log\np(X |Y ,Z ) q(X |Z )\n] . (24)\nREFERENCES [1] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, \u2018\u2018Mastering the game of go without human knowledge,\u2019\u2019 Nature, vol. 550, no. 7676, p. 354, 2017. [2] C. Berner et al., \u2018\u2018Dota 2 with large scale deep reinforcement learning,\u2019\u2019 2019, arXiv:1912.06680. [3] D. Li, H. Wu, J. Zhang, and K. Huang, \u2018\u2018A2-RL: Aesthetics aware reinforcement learning for image cropping,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8193\u20138201. [4] J. Park, J.-Y. Lee, D. Yoo, and I. S. Kweon, \u2018\u2018Distort-and-recover: Color enhancement using deep reinforcement learning,\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 5928\u20135936. [5] R. Furuta, N. Inoue, and T. Yamasaki, \u2018\u2018PixelRL: Fully convolutional network with reinforcement learning for image processing,\u2019\u2019 IEEE Trans. Multimedia, vol. 22, no. 7, pp. 1704\u20131719, Jul. 2020. [6] B. Dhingra, L. Li, X. Li, J. Gao, Y.-N. Chen, F. Ahmed, and L. Deng, \u2018\u2018Towards end-to-end reinforcement learning of dialogue agents for information access,\u2019\u2019 2016, arXiv:1609.00777. [7] V. Zhong, C. Xiong, and R. Socher, \u2018\u2018Seq2SQL: Generating structured queries from natural language using reinforcement learning,\u2019\u2019 2017, arXiv:1709.00103. [8] S. Levine, C. Finn, T. Darrell, and P. Abbeel, \u2018\u2018End-to-end training of deep visuomotor policies,\u2019\u2019 J. Mach. Learn. Res., vol. 17, no. 1, pp. 1334\u20131373, 2016. [9] S. S. Mousavi, M. Schukat, and E. Howley, \u2018\u2018Traffic light control using deep policy-gradient and value-function-based reinforcement learning,\u2019\u2019 IET Intell. Transp. Syst., vol. 11, no. 7, pp. 417\u2013423, 2017.\n49506 VOLUME 10, 2022\n[10] X. Liang, X. Du, G. Wang, and Z. Han, \u2018\u2018A deep reinforcement learning network for traffic light cycle control,\u2019\u2019 IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1243\u20131253, Feb. 2019. [11] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, \u2018\u2018Deep direct reinforcement learning for financial signal representation and trading,\u2019\u2019 IEEE Trans. Neural Netw. Learn. Syst., vol. 28, no. 3, pp. 653\u2013664, Mar. 2016. [12] J. Schmidhuber, J. Zhao, andM.Wiering, \u2018\u2018Simple principles of metalearning,\u2019\u2019 Tech. Rep. IDSIA, vol. 69, pp. 1\u201323, Jun. 1996. [13] R. Mendonca, A. Gupta, R. Kralev, P. Abbeel, S. Levine, and C. Finn, \u2018\u2018Guided meta-policy search,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 9653\u20139664. [14] K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, \u2018\u2018Efficient offpolicy meta-reinforcement learning via probabilistic context variables,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2019, pp. 5331\u20135340. [15] C. Finn, P. Abbeel, and S. Levine, \u2018\u2018Model-agnostic meta-learning for fast adaptation of deep networks,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2017, pp. 1126\u20131135. [16] A. Nichol, J. Achiam, and J. Schulman, \u2018\u2018On first-order meta-learning algorithms,\u2019\u2019 2018, arXiv:1803.02999. [17] L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson, \u2018\u2018VariBAD: A very good method for Bayes-adaptive deep RL via meta-learning,\u2019\u2019 in Proc. Int. Conf. Learn. Represent., 2020, pp. 1\u201320. [18] J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. W. Teh, and N. Heess, \u2018\u2018Meta reinforcement learning as task inference,\u2019\u2019 2019, arXiv:1905.06424. [19] R. Vuorio, S.-H. Sun, H. Hu, and J. J. Lim, \u2018\u2018Multimodal model-agnostic meta-learning via task-aware modulation,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 1\u201312. [20] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, \u2018\u2018RL2: Fast reinforcement learning via slow reinforcement learning,\u2019\u2019 2016, arXiv:1611.02779. [21] M.Duff, \u2018\u2018Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes,\u2019\u2019 Doctoral dissertation, Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, 2002. [Online]. Available: https://scholarworks.umass.edu/dissertations/AAI3039353 [22] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, \u2018\u2018Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning,\u2019\u2019 in Proc. Conf. Robot Learn., 2019, pp. 1094\u20131100. [23] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: MIT Press, 2018. [24] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, \u2018\u2018Planning and acting in partially observable stochastic domains,\u2019\u2019 Artif. Intell., vol. 101, nos. 1\u20132, pp. 99\u2013134, 1998. [25] M. Igl, L. Zintgraf, T. A. Le, F. Wood, and S. Whiteson, \u2018\u2018Deep variational reinforcement learning for POMDPs,\u2019\u2019 2018, arXiv:1806.02426. [26] S. Kapturowski, G. Ostrovski, W. Dabney, J. Quan, and R. Munos, \u2018\u2018Recurrent experience replay in distributed reinforcement learning,\u2019\u2019 in Proc. Int. Conf. Learn. Represent., 2019, pp. 1\u201319. [Online]. Available: https://openreview.net/forum?id=r1lyTjAqYX [27] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \u2018\u2018Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2018, pp. 1856\u20131865. [28] K. Sohn, H. Lee, and X. Yan, \u2018\u2018Learning structured output representation using deep conditional generative models,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 3483\u20133491. [29] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola, \u2018\u2018Deep sets,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3391\u20133401. [30] M. Rolinek, D. Zietlow, and G. Martius, \u2018\u2018Variational autoencoders pursue PCA directions (by accident),\u2019\u2019 in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 12406\u201312415. [31] Y. Bengio, A. Courville, and P. Vincent, \u2018\u2018Representation learning: A review and new perspectives,\u2019\u2019 IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798\u20131828, Aug. 2013. [32] T. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker, \u2018\u2018Model-based reinforcement learning: A survey,\u2019\u2019 2020, arXiv:2006.16712. [33] N. Tishby, F. C. Pereira, and W. Bialek, \u2018\u2018The information bottleneck method,\u2019\u2019 Inria, Bordeaux, France, Tech. Rep., 1999, pp. 368\u2013377, doi: 10.48550/arXiv.1904.06979. [34] C. Zhang, O. Vinyals, R. Munos, and S. Bengio, \u2018\u2018A study on overfitting in deep reinforcement learning,\u2019\u2019 2018, arXiv:1804.06893. [35] C. Zhao, O. Sigaud, F. Stulp, and T. M. Hospedales, \u2018\u2018Investigating generalisation in continuous deep reinforcement learning,\u2019\u2019 2019, arXiv:1902.07015.\n[36] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, \u2018\u2018Quantifying generalization in reinforcement learning,\u2019\u2019 inProc. Int. Conf.Mach. Learn., 2019, pp. 1282\u20131289. [37] A. Goyal, R. Islam, D. Strouse, Z. Ahmed, H. Larochelle, M. Botvinick, Y. Bengio, and S. Levine, \u2018\u2018InfoBot: Transfer and exploration via the information bottleneck,\u2019\u2019 in Proc. Int. Conf. Learn. Represent., 2018, pp. 1\u201321. [38] M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann, \u2018\u2018Generalization in reinforcement learning with selective noise injection and information bottleneck,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 13978\u201313990. [39] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, \u2018\u2018Deep variational information bottleneck,\u2019\u2019 in Proc. Int. Conf. Learn. Represent., 2017, pp. 1\u201319. [40] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, and S. Levine, \u2018\u2018Soft actor-critic algorithms and applications,\u2019\u2019 2018, arXiv:1812.05905. [41] R. M. French, \u2018\u2018Catastrophic forgetting in connectionist networks,\u2019\u2019 Trends Cogn. Sci., vol. 3, no. 4, pp. 128\u2013135, Apr. 1999. [42] Y. Kim, S. Wiseman, A. Miller, D. Sontag, and A. Rush, \u2018\u2018Semiamortized variational autoencoders,\u2019\u2019 in Proc. Int. Conf. Mach. Learn., 2018, pp. 2683\u20132692. [43] Z. Lin, G. Thomas, G. Yang, and T. Ma, \u2018\u2018Model-based adversarial metareinforcement learning,\u2019\u2019 2020, arXiv:2006.08875. [44] H. Fu, H. Tang, J. Hao, C. Chen, X. Feng, D. Li, and W. Liu, \u2018\u2018Towards effective context for meta-reinforcement learning: An approach based on contrastive learning,\u2019\u2019 2020, arXiv:2009.13891. [45] C. Colas, O. Sigaud, and P.-Y. Oudeyer, \u2018\u2018A hitchhiker\u2019s guide to statistical comparisons of reinforcement learning algorithms,\u2019\u2019 NEC Res. Inst., New Jersey Inst. Comput. Sci., Center Neural Comput. Hebrew Univ., Jerusalem, Israel, Tech. Rep., 2019, doi: 10.48550/arXiv.physics/0004057. [46] J. Asmuth and M. Littman, \u2018\u2018Learning is planning: Near Bayes-optimal reinforcement learning via monte-carlo tree search,\u2019\u2019 in Proc. 27th Conf. Uncertainty Artif. Intell., 2011, pp. 19\u201326. [47] D. Ha and J. Schmidhuber, \u2018\u2018World models,\u2019\u2019 2018, arXiv:1803.10122.\nTAKAHISA IMAGAWA received the Ph.D. degree in arts and sciences from The University of Tokyo, Japan, in 2018. He is currently a Researcher at the NEC-National Institute of Advanced Industrial Science and Technology (AIST) AI Cooperative Research Laboratory, Japan.\nTAKUYA HIRAOKA received the Ph.D. degree from the Nara Institute of Science and Technology, Japan, in 2016. He is currently a Researcher at NEC Corporation.\nYOSHIMASA TSURUOKA received the Ph.D. degree from The University of Tokyo, in 2002. He worked as a Research Associate at The University of Tokyo and The University of Manchester. Then, he has been working as an Associate Professor at the Japan Advanced Institute of Science and Technology, since 2009. He is currently a Professor at The University of Tokyo. His research interests include natural language processing, artificial intelligence for games, and reinforcement learning.\nVOLUME 10, 2022 49507"
        }
    ],
    "title": "Off-Policy Meta-Reinforcement Learning With Belief-Based Task Inference",
    "year": 2022
}