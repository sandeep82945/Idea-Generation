{
    "abstractText": "Death by suicide is the seventh leading death cause worldwide. The recent advancement in Artificial Intelligence (AI), specifically AI applications in image and voice processing, has created a promising opportunity to revolutionize suicide risk assessment. Subsequently, we have witnessed fast-growing literature of research that applies AI to extract audiovisual non-verbal cues for mental illness assessment. However, the majority of the recent works focus on depression, despite the evident difference between depression symptoms and suicidal behavior non-verbal cues. In this paper, we review the recent works that study suicide ideation and suicide behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues. Automatic suicide assessment is a promising research direction that is still in the early stages. Accordingly, there is a lack of large datasets that can be used to train machine leaning and deep learning models proven to be effective in other, similar tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sahraoui Dhelim"
        },
        {
            "affiliations": [],
            "name": "Liming Chen"
        },
        {
            "affiliations": [],
            "name": "Huansheng Ning"
        },
        {
            "affiliations": [],
            "name": "Chris Nugent"
        }
    ],
    "id": "SP:f77e71fdc776831384dec315bb3b9f4678613a9c",
    "references": [
        {
            "authors": [
                "World Health Organization"
            ],
            "title": "Suicide key facts",
            "venue": "2019. https://www.who.int/news-room/factsheets/detail/suicide (accessed Apr. 14, 2021).",
            "year": 2019
        },
        {
            "authors": [
                "M. Blanchard",
                "B.A. Farber"
            ],
            "title": "It is never okay to talk about suicide\u2019: patients\u2019 reasons for concealing suicidal ideation in psychotherapy",
            "venue": "Psychother. Res., vol. 30, no. 1, pp. 124\u2013136, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C.H. Warner"
            ],
            "title": "Importance of anonymity to encourage honest reporting in mental health screening after combat deployment",
            "venue": "Arch. Gen. Psychiatry, vol. 68, no. 10, pp. 1065\u20131071, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "W. Wang",
                "H. Ning",
                "F. Shi",
                "S. Dhelim",
                "W. Zhang",
                "L. Chen"
            ],
            "title": "A Survey of Hybrid Human- Artificial Intelligence for Social Computing",
            "venue": "IEEE Trans. Human-Machine Syst., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Dhelim",
                "H. Ning",
                "N. Aung"
            ],
            "title": "ComPath: User Interest Mining in Heterogeneous Signed Social Networks for Internet of People",
            "venue": "IEEE Internet Things J., pp. 1\u20131, 2020, doi: 10.1109/JIOT.2020.3037109.",
            "year": 2020
        },
        {
            "authors": [
                "S. Dhelim",
                "L. Chen",
                "N. Aung",
                "W. Zhang",
                "H. Ning"
            ],
            "title": "A hybrid personality-aware recommendation system based on personality traits and types models",
            "venue": "J. Ambient Intell. Humaniz. Comput., pp. 1\u201314, Jul. 2022, doi: 10.1007/s12652-022-04200-5.",
            "year": 2022
        },
        {
            "authors": [
                "A. Pampouchidou"
            ],
            "title": "Automatic assessment of depression based on visual cues: A systematic review",
            "venue": "IEEE Trans. Affect. Comput., vol. 10, no. 4, pp. 445\u2013470, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. He"
            ],
            "title": "Deep learning for depression recognition with audiovisual cues: A review",
            "venue": "Inf. Fusion, vol. 80, pp. 56\u201386, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Zhang",
                "A.M. Schoene",
                "S. Ji",
                "S. Ananiadou"
            ],
            "title": "Natural language processing applied to mental illness detection: a narrative review",
            "venue": "NPJ Digit. Med., vol. 5, no. 1, pp. 1\u201313, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Castillo-S\u00e1nchez",
                "G. Marques",
                "E. Dorronzoro",
                "O. Rivera-Romero",
                "M. Franco-Mart\u00edn",
                "I. De la Torre-D\u00edez"
            ],
            "title": "Suicide Risk Assessment Using Machine Learning and Social Networks: a Scoping Review",
            "venue": "J. Med. Syst., vol. 44, no. 12, p. 205, 2020, doi: 10.1007/s10916-020-01669-5.",
            "year": 2020
        },
        {
            "authors": [
                "N. Cummins",
                "S. Scherer",
                "J. Krajewski",
                "S. Schnieder",
                "J. Epps",
                "T.F. Quatieri"
            ],
            "title": "A review of depression and suicide risk assessment using speech analysis",
            "venue": "Speech Commun., vol. 71, pp. 10\u2013 49, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Scherer",
                "J. Pestian",
                "L.-P. Morency"
            ],
            "title": "Investigating the speech characteristics of suicidal adolescents",
            "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, May 2013, pp. 709\u2013713, doi: 10.1109/ICASSP.2013.6637740.",
            "year": 2013
        },
        {
            "authors": [
                "A. Ozdas",
                "R.G. Shiavi",
                "S.E. Silverman",
                "M.K. Silverman",
                "D.M. Wilkes"
            ],
            "title": "Investigation of Vocal Jitter and Glottal Flow Spectrum as Possible Cues for Depression and Near-Term Suicidal Risk",
            "venue": "IEEE Trans. Biomed. Eng., vol. 51, no. 9, pp. 1530\u20131540, Sep. 2004, doi: 10.1109/TBME.2004.827544.",
            "year": 2004
        },
        {
            "authors": [
                "S. Ji",
                "S. Pan",
                "X. Li",
                "E. Cambria",
                "G. Long",
                "Z. Huang"
            ],
            "title": "Suicidal ideation detection: A review of machine learning methods and applications",
            "venue": "IEEE Trans. Comput. Soc. Syst., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R.T. Liu",
                "A.H. Bettis",
                "T.A. Burke"
            ],
            "title": "Characterizing the phenomenology of passive suicidal ideation: a systematic review and meta-analysis of its prevalence, psychiatric comorbidity, correlates, and comparisons with active suicidal ideation",
            "venue": "Psychol. Med., vol. 50, no. 3, pp. 367\u2013 383, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.M. Silverman",
                "A.L. Berman",
                "N.D. Sanddal",
                "P.W. O\u2019carroll",
                "T.E. Joiner Jr"
            ],
            "title": "Rebuilding the tower of Babel: a revised nomenclature for the study of suicide and suicidal behaviors. Part 2: Suicide-related ideations, communications, and behaviors",
            "venue": "Suicide Life-Threatening Behav., vol. 37, no. 3, pp. 264\u2013277, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "D. Moher",
                "A. Liberati",
                "J. Tetzlaff",
                "D.G. Altman",
                "others"
            ],
            "title": "Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement",
            "venue": "Int J Surg, vol. 8, no. 5, pp. 336\u2013 341, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M.L. Rohlfing",
                "D.P. Buckley",
                "J. Piraquive",
                "C.E. Stepp",
                "L.F. Tracy"
            ],
            "title": "Hey Siri: How Effective are Common Voice Recognition Systems at Recognizing Dysphonic Voices",
            "venue": "Laryngoscope, vol. 131, no. 7, pp. 1599\u20131607, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.N. Chakravarthula"
            ],
            "title": "Automatic prediction of suicidal risk in military couples using multimodal interaction cues from couples conversations",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6539\u2013 6543.",
            "year": 2020
        },
        {
            "authors": [
                "V. Venek",
                "S. Scherer",
                "L.-P. Morency",
                "A.S. Rizzo",
                "J. Pestian"
            ],
            "title": "Adolescent Suicidal Risk Assessment in Clinician-Patient Interaction",
            "venue": "IEEE Trans. Affect. Comput., vol. 8, no. 2, pp. 204\u2013 215, Apr. 2017, doi: 10.1109/TAFFC.2016.2518665.",
            "year": 2017
        },
        {
            "authors": [
                "V. Venek",
                "S. Scherer",
                "L.-P. Morency",
                "A. Rizzo",
                "J. Pestian"
            ],
            "title": "Adolescent suicidal risk assessment in clinician-patient interaction: A study of verbal and acoustic behaviors",
            "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), Dec. 2014, pp. 277\u2013282, doi: 10.1109/SLT.2014.7078587.",
            "year": 2014
        },
        {
            "authors": [
                "A. Belouali"
            ],
            "title": "Acoustic and language analysis of speech for suicidal ideation among US veterans",
            "venue": "BioData Min., vol. 14, no. 1, pp. 1\u201317, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Nasir",
                "B.R. Baucom",
                "C.J. Bryan",
                "S.S. Narayanan",
                "P.G. Georgiou"
            ],
            "title": "Complexity in speech and its relation to emotional bond in therapist-patient interactions during suicide risk assessment interviews",
            "venue": "Interspeech, 2017, pp. 3296\u20133300.",
            "year": 2017
        },
        {
            "authors": [
                "I. Galatzer-Levy"
            ],
            "title": "Validation of Visual and Auditory Digital Markers of Suicidality in Acutely Suicidal Psychiatric Inpatients: Proof-of-Concept Study",
            "venue": "J. Med. Internet Res., vol. 23, no. 6, p. e25199, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Gideon",
                "H.T. Schatten",
                "M.G. McInnis",
                "E.M. Provost"
            ],
            "title": "Emotion recognition from natural phone conversations in individuals with and without recent suicidal ideation",
            "venue": "2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Stasak",
                "J. Epps",
                "H.T. Schatten",
                "I.W. Miller",
                "E.M. Provost",
                "M.F. Armey"
            ],
            "title": "Read speech voice quality and disfluency in individuals with recent suicidal ideation or suicide attempt",
            "venue": "Speech Commun., vol. 132, pp. 10\u201320, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Laksana",
                "T. Baltrusaitis",
                "L.-P. Morency",
                "J.P. Pestian"
            ],
            "title": "Investigating Facial Behavior Indicators of Suicidal Ideation",
            "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), May 2017, pp. 770\u2013777, doi: 10.1109/FG.2017.96.",
            "year": 2017
        },
        {
            "authors": [
                "A.P. Shah",
                "V. Vaibhav",
                "V. Sharma",
                "M. Al Ismail",
                "J. Girard",
                "L.-P. Morency"
            ],
            "title": "Multimodal Behavioral Markers Exploring Suicidal Intent in Social Media Videos",
            "venue": "2019 International Conference on Multimodal Interaction, Oct. 2019, pp. 409\u2013413, doi: 10.1145/3340555.3353718.",
            "year": 2019
        },
        {
            "authors": [
                "S. Kleiman",
                "N.O. Rule"
            ],
            "title": "Detecting Suicidality From Facial Appearance",
            "venue": "Soc. Psychol. Personal. Sci., vol. 4, no. 4, pp. 453\u2013460, Jul. 2013, doi: 10.1177/1948550612466115.",
            "year": 2013
        },
        {
            "authors": [
                "N. Eigbe",
                "T. Baltrusaitis",
                "L.-P. Morency",
                "J. Pestian"
            ],
            "title": "Toward Visual Behavior Markers of Suicidal Ideation",
            "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), May 2018, pp. 530\u2013534, doi: 10.1109/FG.2018.00085.",
            "year": 2018
        },
        {
            "authors": [
                "J. Pestian"
            ],
            "title": "A Machine Learning Approach to Identifying Future Suicide Risk",
            "venue": "SSRN Electron. J., 2018, doi: 10.2139/ssrn.3279211.",
            "year": 2018
        },
        {
            "authors": [
                "D.J. France",
                "R.G. Shiavi",
                "S. Silverman",
                "M. Silverman",
                "M. Wilkes"
            ],
            "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
            "venue": "IEEE Trans. Biomed. Eng., vol. 47, no. 7, pp. 829\u2013837, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "T. Yingthawornsuk",
                "H.K. Keskinpala",
                "D. France",
                "D.M. Wilkes",
                "R.G. Shiavi",
                "R.M. Salomon"
            ],
            "title": "Objective estimation of suicidal risk using vocal output characteristics",
            "venue": "2006.",
            "year": 2006
        },
        {
            "authors": [
                "T. Akkaralaertsest",
                "T. Yingthawornsuk"
            ],
            "title": "Comparative analysis of vocal characteristics in speakers with depression and high-risk suicide",
            "venue": "Int. J. Comput. Theory Eng., vol. 7, no. 6, p. 448, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "P. Anunvrapong",
                "T. Yingthawornsuk"
            ],
            "title": "Characterization of \u0394MFCC in depressed speech sample as assessment of suicidal risk",
            "venue": "International Conference on Advanced Computational Technologies \\& Creative Media (ICACTCM \u20182014), 2014, pp. 119\u2013123.",
            "year": 2014
        },
        {
            "authors": [
                "T. Yingthawornsuk",
                "R.G. Shiavi"
            ],
            "title": "Distinguishing depression and suicidal risk in men using GMM based frequency contents of affective vocal tract response",
            "venue": "2008 International Conference on Control, Automation And Systems, 2008, pp. 901\u2013904.",
            "year": 2008
        },
        {
            "authors": [
                "A. Ozdas",
                "R.G. Shiavi",
                "D.M. Wilkes",
                "M.K. Silverman",
                "S.E. Silverman"
            ],
            "title": "Analysis of vocal tract characteristics for near-term suicidal risk assessment",
            "venue": "Methods Inf. Med., vol. 43, no. 01, pp. 36\u201338, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "H.K. Keskinpala",
                "T. Yingthawornsuk",
                "D.M. Wilkes",
                "R.G. Shiavi",
                "R.M. Salomon"
            ],
            "title": "Screening for high risk suicidal states using mel-cepstral coefficients and energy in frequency bands",
            "venue": "2007 15th European Signal Processing Conference, 2007, pp. 2229\u20132233.",
            "year": 2007
        },
        {
            "authors": [
                "N.N.W. NH",
                "M.D. Wilkes",
                "R.M. Salomon"
            ],
            "title": "Timing Patterns of Speech as Potential Indicators of Near-Term Suicidal Risk",
            "venue": "Int. J. Multidiscip. Curr. Res., vol. 3, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "N.N.W. NH",
                "M.D. Wilkes",
                "R.M. Salomon"
            ],
            "title": "Investigating the Course of Recovery in High Risk Suicide using Power Spectral Density",
            "venue": "Asian J. Appl. Sci., vol. 3, no. 4, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Yingthawornsuk",
                "H.K. Keskinpala",
                "D.M. Wilkes",
                "R.G. Shiavi",
                "R.M. Salomon"
            ],
            "title": "Direct acoustic feature using iterative EM algorithm and spectral energy for classifying suicidal speech",
            "venue": "2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. Gratch"
            ],
            "title": "The distress analysis interview corpus of human and computer interviews",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), 2014, pp. 3123\u20133128.",
            "year": 2014
        },
        {
            "authors": [
                "M. Valstar"
            ],
            "title": "Avec 2013: the continuous audio/visual emotion and depression recognition challenge",
            "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge, 2013, pp. 3\u201310.",
            "year": 2013
        },
        {
            "authors": [
                "F. Ringeval"
            ],
            "title": "AVEC 2019 workshop and challenge: state-of-mind, detecting depression with AI, and cross-cultural affect recognition",
            "venue": "Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop, 2019, pp. 3\u201312.",
            "year": 2019
        },
        {
            "authors": [
                "T. Drugman",
                "B. Bozkurt",
                "T. Dutoit"
            ],
            "title": "A comparative study of glottal source estimation techniques",
            "venue": "Comput. Speech \\& Lang., vol. 26, no. 1, pp. 20\u201334, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "M. Airas"
            ],
            "title": "TKK Aparat: An environment for voice inverse filtering and parameterization",
            "venue": "Logop. Phoniatr. Vocology, vol. 33, no. 1, pp. 49\u201364, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "B. Doval",
                "C. d\u2019Alessandro",
                "N. Henrich"
            ],
            "title": "The spectrum of glottal flow models",
            "venue": "Acta Acust. united with Acust., vol. 92, no. 6, pp. 1026\u20131046, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "J. Walker",
                "P. Murphy"
            ],
            "title": "A review of glottal waveform analysis",
            "venue": "Prog. nonlinear speech Process., pp. 1\u201321, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "T.F. Quatieri",
                "N. Malyska"
            ],
            "title": "Vocal-source biomarkers for depression: A link to psychomotor activity",
            "venue": "2012.",
            "year": 2012
        },
        {
            "authors": [
                "J. Sundberg",
                "S. Patel",
                "E. Bjorkner",
                "K.R. Scherer"
            ],
            "title": "Interdependencies among voice source parameters in emotional speech",
            "venue": "IEEE Trans. Affect. Comput., vol. 2, no. 3, pp. 162\u2013174, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "J. Kane",
                "M. Aylett",
                "I. Yanushevskaya",
                "C. Gobl"
            ],
            "title": "Phonetic feature extraction for contextsensitive glottal source processing",
            "venue": "Speech Commun., vol. 59, pp. 10\u201321, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "H. Kaymaz Keskinpala",
                "T. Yingthawornsuk",
                "R.M. Salomon",
                "R.G. Shiavi",
                "D.M. Wilkes"
            ],
            "title": "Distinguishing High Risk Suicidal Subjects among Depressed Subjects Using Mel-Frequency Cepstrum Coefficients and Cross Validation Technique",
            "venue": "Disting. High Risk Suicidal Subj. among Depress. Subj. Using Mel-Frequency Cepstrum Coefficients Cross Valid. Tech., pp. 1000\u20131004, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "P.P. Reddy",
                "C. Suresh",
                "V.K. Rao",
                "K.S. Chandana",
                "S. Sowkya",
                "R. Akhila"
            ],
            "title": "Vocal Analysis to Predict Suicide Tendency",
            "venue": "Proceedings of International Conference on Advances in Computer Engineering and Communication Systems, 2021, pp. 481\u2013488.",
            "year": 2021
        },
        {
            "authors": [
                "H.R. Venn"
            ],
            "title": "Perception of facial expressions of emotion in bipolar disorder",
            "venue": "Bipolar Disord., vol. 6, no. 4, pp. 286\u2013293, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "P.J. Silvia",
                "W.D. Allan",
                "D.L. Beauchamp",
                "E.L. Maschauer",
                "J.O. Workman"
            ],
            "title": "Biased recognition of happy facial expressions in social anxiety",
            "venue": "J. Soc. Clin. Psychol., vol. 25, no. 6, pp. 585\u2013602, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Y. Wang",
                "N. Guobule",
                "M. Li",
                "J. Li"
            ],
            "title": "The correlation of facial emotion recognition in patients with drug-na{\\\"\\i}ve depression and suicide ideation",
            "venue": "J. Affect. Disord., vol. 295, pp. 250\u2013254, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Scherer"
            ],
            "title": "Automatic audiovisual behavior descriptors for psychological disorder analysis",
            "venue": "Image Vis. Comput., vol. 32, no. 10, pp. 648\u2013658, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J.R. Williamson"
            ],
            "title": "Detecting depression using vocal, facial and semantic communication cues",
            "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, 2016, pp. 11\u201318.",
            "year": 2016
        },
        {
            "authors": [
                "Q. Wang",
                "H. Yang",
                "Y. Yu"
            ],
            "title": "Facial expression video analysis for depression detection in Chinese patients",
            "venue": "J. Vis. Commun. Image Represent., vol. 57, pp. 228\u2013233, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R. Gupta"
            ],
            "title": "Multimodal prediction of affective dimensions and depression in humancomputer interactions",
            "venue": "Proceedings of the 4th international workshop on audio/visual emotion challenge, 2014, pp. 33\u201340.",
            "year": 2014
        },
        {
            "authors": [
                "M. Nasir",
                "A. Jati",
                "P.G. Shivakumar",
                "S. Nallan Chakravarthula",
                "P. Georgiou"
            ],
            "title": "Multimodal and multiresolution depression detection from speech and facial landmark features",
            "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge, 2016, pp. 43\u201350.",
            "year": 2016
        },
        {
            "authors": [
                "S. Alghowinem"
            ],
            "title": "Multimodal depression detection: fusion analysis of paralinguistic, head pose and eye gaze behaviors",
            "venue": "IEEE Trans. Affect. Comput., vol. 9, no. 4, pp. 478\u2013490, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Joshi",
                "R. Goecke",
                "G. Parker",
                "M. Breakspear"
            ],
            "title": "Can body expressions contribute to automatic depression analysis",
            "venue": "2013, doi: 10.1109/FG.2013.6553796.",
            "year": 2013
        },
        {
            "authors": [
                "J. Joshi",
                "A. Dhall",
                "R. Goecke",
                "M. Breakspear",
                "G. Parker"
            ],
            "title": "Neural-net classification for spatiotemporal descriptor based depression analysis",
            "venue": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012), 2012, pp. 2634\u20132638.",
            "year": 2012
        },
        {
            "authors": [
                "L. He",
                "C. Cao"
            ],
            "title": "Automated depression analysis using convolutional neural networks from speech",
            "venue": "J. Biomed. Inform., vol. 83, pp. 103\u2013111, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D.J. Sebald",
                "J.A. Bucklew"
            ],
            "title": "Support vector machine techniques for nonlinear equalization",
            "venue": "IEEE Trans. signal Process., vol. 48, no. 11, pp. 3217\u20133226, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "I. Gratch"
            ],
            "title": "Detecting suicidal thoughts: The power of ecological momentary assessment",
            "venue": "Depress. Anxiety, vol. 38, no. 1, pp. 8\u201316, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Barsties",
                "M. De Bodt"
            ],
            "title": "Assessment of voice quality: current state-of-the-art",
            "venue": "Auris Nasus Larynx, vol. 42, no. 3, pp. 183\u2013188, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Degottex",
                "J. Kane",
                "T. Drugman",
                "T. Raitio",
                "S. Scherer"
            ],
            "title": "COVAREP\u2014A collaborative voice analysis repository for speech technologies",
            "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp), 2014, pp. 960\u2013964.",
            "year": 2014
        },
        {
            "authors": [
                "F. Eyben"
            ],
            "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
            "venue": "IEEE Trans. Affect. Comput., vol. 7, no. 2, pp. 190\u2013202, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Benton",
                "G. Coppersmith",
                "M. Dredze"
            ],
            "title": "Ethical research protocols for social media health research",
            "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017, pp. 94\u2013102.",
            "year": 2017
        },
        {
            "authors": [
                "D.M. Skerrett",
                "K. K\u00f5lves",
                "D. De Leo"
            ],
            "title": "Are LGBT populations at a higher risk for suicidal behaviors in Australia? Research findings and implications",
            "venue": "J. Homosex., vol. 62, no. 7, pp. 883\u2013 901, 2015.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Intelligence (AI), specifically AI applications in image and voice processing, has created a promising opportunity to revolutionize suicide risk assessment. Subsequently, we have witnessed fast-growing literature of research that applies AI to extract audiovisual non-verbal cues for mental illness assessment. However, the majority of the recent works focus on depression, despite the evident difference between depression symptoms and suicidal behavior non-verbal cues. In this paper, we review the recent works that study suicide ideation and suicide behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues. Automatic suicide assessment is a promising research direction that is still in the early stages. Accordingly, there is a lack of large datasets that can be used to train machine leaning and deep learning models proven to be effective in other, similar tasks.\nKeywords: suicide detection; machine learning; speech analysis; visual cues; suicide ideation detection;"
        },
        {
            "heading": "1. Introduction",
            "text": "Suicide is a global mental health problem [1]. More than 800,000 people pass away from suicide, and more than 16 million people attempt suicide every year [1]. Suicide is currently the fourth leading cause of death for people aged between 15 and 29 years [2]. It is still a challenge to assess suicide risks and detect suicide due to the transient and ambivalent nature of severe suicide intent, and the patient's hesitancy. Most suicide risk assessment methods depend on voluntary disclosure of the patient, and most suicidal patients deny suicide ideation during an interview [3][4]. In such cases, the clinician relies on various secondary information sources, such as the patient\u2019s health records, the nurse's observation, and other secondary factors. However, the final judgment about the suicide risk level is based on the clinician\u2019s intuition and observation of the patient\u2019s behaviors during the interview. Although the clinician can observe obvious emotions and body gestures, the clinician cannot observe micro facial expressions and biosignals such as heartbeat rate and brain waves. Therefore, building an Artificial Intelligence (AI) enabled suicide risk assessment tool that observes the patient\u2019s biomarker and audiovisual cues and learns suicide ideation patterns might be a decisive indicator that could help the clinician to reach a clear judgment [5]. With the recent advances in the field of user-centered computing [6], [7], automatic mental disorders detection using audiovisual data has become an active research topic. The patient\u2019s acoustic and visual markers are analysed to detect mental disorders such as depression [8]. The recent advancement in AI, specifically AI applications in image, voice processing and natural language processing [9], [10], has created a promising opportunity to revolutionize suicide risk assessment. Subsequently, we have witnessed fast-growing literature of research that applies AI to extract audiovisual non-verbal cues for mental illness assessment [11]. Although that previous research has found associations between acoustic features and suicide tendency [12][13]; suicidal patients have lower acoustic energy, breathy voice quality, and abnormal glottal control compared to people without suicide ideation history [14]. However, the majority of the recent works focus on depression [15], despite the evident difference between depression signs and suicidal behavior\nnon-verbal cues. In this paper, we review the recent works that study suicide ideation and suicide behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues analysis. Suicidal ideation can vary in presentation and severity, but generally it can be classified as one of the following phases [16]. (1) passive suicide ideation: when the subject has thoughts of suicide or self-harm but no plan to carry it out. (2) active suicide ideation: when the subject has thoughts of engaging in suicide-related behavior and has suicidal intent and/or had developed a plan to carry it out. (3) suicidal attempt: the subject had attempted suicide and/or still attempting to suicide following an unsuccessful suicide attempt [17].\nThe focus of the current review is automatic suicide ideation and suicide behavior detection through audiovisual feature analysis, with a special focus on works that used machine learning and deep learning approaches. We limit the coverage of the review to works published between 2004 and 2021. We have used PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework [18] guidelines to select publications related to automatic suicidal behavior detection. As shown in Figure 1, initially, 251 related papers between January 2004 and November 2021 were identified after searching Google Scholar, Elsevier, Web of Science, ACM Digital Library, IEEE Xplore digital library, Springer and PubMed for articles related to the following research queries: \u201cautomatic suicide detection\u201d, \u201cmachine learning suicide detection\u201d, \u201csuicidal speech analysis\u201d, \u201csuicidal visual cues\u201d, \u201csuicide ideation detection\u201d. The searches were limited to articles written in English. 455 additional articles were identified as related works; these papers were found by tracking the citation map of the searched articles. After removing duplicated articles, a total of 706 papers were collected in the identification phase. In the screening phase, based on the title and abstract screening 551 papers were excluded for not meeting the inclusion criteria. The majority of these papers either studied suicidal behavior from a clinical perspective, without automatic detection, or they use automatic detection for detecting mental disorders such as anxiety and minor depression. 104 articles were excluded in the eligibility phase after full-text reading. Finally, 51 articles were qualified for final inclusion.\nThe suicidal marker that can be extracted from audiovisual data, can be roughly divided into two main categories. Audio markers that convey the suicidal cue can be obtained by analyzing the acoustic features of the speech, and visual markers that can be observed in the suicidal patient\u2019s upper body behaviors during suicidal assessment procedures such as a clinical interview. Figure 2 shows the most used acoustic and visual features used in the literature on suicidal assessment using audiovisual data.\nThe rest of the paper is organized as follows:\nIn Section 2, we discuss data collection scenarios of the reviewed works and summarize some of the most used audiovisual datasets for automatic suicide assessment. In Section 3, we focus on speech analysis and audio-based suicide assessment. In Section 4, we review the works that used upper body features extracted from visual cues for suicide assessment. While Section 5 focuses on the detection models applied to audiovisual data for suicide behavior detection. In Section 6, we summarize the findings and outline limitations and potential future directions. Finally, Section 7 concludes the review."
        },
        {
            "heading": "2. Data collection",
            "text": "Suicide audiovisual data can be collected using three methods. (1) through an open interview between the patient and the clinician; (2) by capturing the audiovisual recording during casual conversations; (3) or by asking patients to read a speech or a set of sentences. Each one of these methods has its advantages and drawbacks. Because the recorded sentence is the same for all participants, the reading task method makes it easy to distinguish a suicidal patient from non-suicidal controls, which enable the machine learning model to focus on the audiovisual feature differences between suicidal patients and the control participants. The second advantage of the reading task method is that the model requires fewer data to learn, as it does not need to learn the context of the spoken sentence from the data, unlike the interview method, where the model needs to deduce the context of the spoken sentences.\nIn the interview method, unlike the reading task, the reason behind sudden changes in one of the acoustic markers must be distinguished from the context of the interview. For example, is the detected change due to grammatical contexts, such as intonation, or a suicidal cue? The second advantage of the reading task is that the clinician can design the reading passage to target specific sounds or syllables that are related to certain suicidal cue patterns. One of the famous reading tasks is known as the \u201crainbow passage\u201d, which is used in speech science because it contains all of the normal sounds in spoken English, and is phonetically balanced [19]. Table 1 presents study samples and data collection scenarios of some reviewed works.\nWhereas Table 2 summarizes some of the most used datasets that contain audiovisual recordings along with ground truth label regarding the mental conditions of the recoded individuals.\n[31] 379 126 mentally ill\npatients, and 130 suicidal patients\nCollected the video recording of every patient when answering interview questions for 8 minutes.\n[32] 253 Suicidal patients Recorded the subjects\u2019 responses to standardized suicide interviews\ndesigned to harvest thought markers.\n[13] 60 teenagers\nbetween the ages of 12 and 17\nCollected the video recording of subjects when answering 5 open-ended questions related to suicide ideation. The recordings were collected in a controlled examination room using a tabletop microphone.\n[33] 22 Suicidal patients Collected the audio of suicidal patients when interviewed by therapists.\n[34] 23 Suicidal patients Collected the audio recording of each patient during the interview with the\ntherapist, as well as when the patient is reading a pre-selected rainbow passage from a book.\n[14] 20 Suicidal patients Collected the audio recording of suicidal patients during real-life\nsituations, the recording was filtered and only high-quality recordings were considered.\n[35] 30 Suicidal and\ndepressed patients\nCollected the audio recording of each patient during the interview with the therapist, as well as when the patient is reading a pre-selected rainbow passage from a book.\n[36] 10 Suicidal and\ndepressed patients\nCollected the audio recording of the subject's speech when answering interview questions in a controlled environment.\n[37] 30 Suicidal and\ndepressed patients\nCollected the audio recording of each patient during the interview with the therapist, as well as when the patient is reading a pre-selected rainbow passage from a book.\n[38] 30 Suicidal and\ndepressed patients\nCollected the audio recording of the subject's speech when answering interview questions in a controlled environment.\n[39] 30 Suicidal and\ndepressed patients\nCollected the audio recording of the patient reading a pre-selected rainbow passage from a book. The rainbow passage is selected to includes all the sounds of spoken English, and phonetically balanced.\n[40][41] 23 Suicidal and\ndepressed patients\nThe patients were recorded during the interview in a soundproof acoustically ideal room, where the patients were asked to take Hamilton Depression Rating Scale (HAMD) and Beck Depression Inventory (BDIII). Following that, the patients were asked to read a rainbow passage.\n[42] 20 Suicidal and\ndepressed patients\nOne is a speech sample recorded from a clinical interview with a therapist and another is a speech sample recorded from a text\u2013reading session."
        },
        {
            "heading": "3. Suicidal audio markers",
            "text": "Audio markers are classified as prosodic features, source features, formant features, and spectral features."
        },
        {
            "heading": "3.1. Source features",
            "text": "Source features are the main influencers of the changes in voice quality, changes in vocal fold vibration, loudness, vocal tract shape and pitch variation.\n3.1.1. Source features and suicidal Source features measure the quality of the voice production process and observe the changes as the airflow from the lungs passes through the glottis. Table 3 lists the most used source features in automatic suicide assessment. When the patient is in a suicidal state, the laryngeal control is affected, such as harshness, breathiness and creakiness, hence this change is reflected in the source feature [46]. The source features are either extracted by measuring the changes of glottal characteristics or changes in vocal fold movements and subsequently the change in voice quality features. The majority of source features are computed by analyzing the time-series data of the glottal flow signal [47], [48]. Nonetheless, it is challenging to determine the critical time points because of the non-uniform vocal fold activity, and signal noise [49]. Source features widely used in suicidal speech analysis include the jitter, which represents the cycle-tocycle fluctuations in glottal pulse timing during a suicidal speech; shimmer, which represents the cycle-tocycle fluctuations in glottal pulse amplitude in voiced regions; and harmonic-to-noise ratio (HNR), which represent the ratio of harmonics to inharmonic. These source features have been proven to be correlated with suicidality [14][50], that is because they are directly related to vocal fold vibration, which influences the vocal fold tension and subglottal pressure [51].\nTable 3 Source feature used in automatic suicide assessment.\nFeature Description Study Significant test\nJitter Deviations in individual consecutive f0 period\nlengths, which indicates irregular closure and asymmetric vocal-fold vibrations\n[20] [23]\n[26][14]\nControl: 0.0165 \u00b1 0.002 (n = 10) Suicide: 0.0217 \u00b1 0.005 (n = 10) \ud835\udc5d \u2264 0.05 (t-test)\nShimmer The difference in the peak amplitudes of\nconsecutive f0 periods, indicates irregularities in voice intensity.\n[20] [23][14]\n[26]\n\ud835\udc5d \u2264 0.05\nQOQ The ratio of the vocal folds' opening time.\nFunctional dysphonias often reduce QOQ range. Speaking loudly requires more effort with a low QOQ and sounds more stalled.\n[21] [23] [22] [13] Control: 0.31 \u00b1 0.13 (n = 8) Suicide: 0.42 \u00b1 0.2 (n = 8)\n\ud835\udc5d \u2264 0.002 (t-test)\nNAQ The ratio between peak-to-peak pulse amplitude\nand the negative peak of the differentiated flow glottogram and normalized with respect to the period time. It can be an estimate of glottal adduction.\n[21] [23][22] [13]\nControl: 0.09 \u00b1 0.04 (n = 8) suicide: 0.12 \u00b1 0.05 (n = 8) \ud835\udc5d \u2264 0.002 (t-test)\nSpectral slope\nMeasures the glottal flow waveform needed for the reconstruction of the excitation signal from the given speech signal by glottal inverse filtering. [14] [26] [14] [13] [35] Control: \u221283.3 \u00b1 5.46 (n = 10) Suicide: \u221275.56 \u00b1 8.53 (n = 10) \ud835\udc5d \u2264 0.05 (t-test)\nVocal tract\nMeasures the change in vocal tract dynamics and constrain articulatory movement [37] [42] N/A\n3.1.2 Source features analysis Due to the difficulties of accurately extracting source features from speech signals, most of the works in automatic suicide detection fail to establish a strong correlation between source features only and suicide, therefore the source features are usually combined with features from other audio categories. Another influencing factor when extracting source features such as shimmer, jitter and HNR is the speech type from which these features were extracted, such as continuous speaking or held vowels context. Generally, the held vowels make the extraction of source features much easier, but they are prone to an error related to individual-related differences in sound pressure levels, which could lead to inaccuracy in feature analysis. Whereas analyzing continuous speech to extract source features is more challenging than extracting them from held vowels, as it is extremely difficult to automatically determine voiced segments in the studied utterance [52]. Identification of speech sections that contain significant source features is an active research direction in the field of suicidal speech analysis [52]. Chakravarthula et al. [20] analyzed source acoustic features in military couples\u2019 conversations and tried to deduce suicidal markers from these acoustic markers. Specifically, they have combined voice quality features (jitter, shimmer) with features from other acoustic categories, mainly prosodic features, and spectral features. They fused these acoustic features with lexical features Linguistic Inquiry and Word Count (LIWC), and behavioral features and applied a support vector machine (SVM) to classify the subjects into various classes (risk vs no-risk classes, degree of risk classes, non-severe vs severe risk classes). Scherer et al. [13] analyzed speech properties of prosodic as well as source features that were extracted from the dyadic interview corpus of both suicidal and non-suicidal adolescents. They have found various statistically significant differences between the speech properties of suicidal and healthy subjects. The source and voice quality features exhibit the strongest differences between the two subject groups. Specifically, NAQ, QOQ and peak features are strongly associated with voice qualities on the tense dimension to breathy, which show that suicidal subjects\u2019 voices are breathier\nthan the voice of healthy people. Venek et al. [21] [7] fused source features (NAQ, QOQ) with prosodic features and formant features to analyse suicidal speech. They applied hierarchical classifier (SVM then AdaBoostM1) on speech of 60 audio-recorded dyadic clinician-patient interviews of 30 suicidal patients. They found that there were 37 statistically significant features that could distinguish suicidal from nonsuicidal subjects, that including: Speech time, Pause time, Personal Pron, 1st person Pron, Impersonal Pron, Past Tense, Negation, Positive emotion, Negative emotion, Tentative, Death, Nonfluencie, Assent, NAQ and QOQ."
        },
        {
            "heading": "3.2. Prosodic features",
            "text": "Prosodic features measure longitudinal variations in a speaker's rhythm, stress level, and intonation of speech. Some of the prominent examples of prosodic features are the speaking rate, the speaker\u2019s pitch and loudness level, and the speaker\u2019s energy dynamics. Among these prosodic features, the energy, and the fundamental frequency (also known as f0, e.g. the rate of vocal fold vibration) and are the most commonly used prosodic features, as they are direct indicators of pitch and loudness.\n3.2.1. Prosodic features and suicidal Assessing depressed and suicidal patients based on prosodic features has been investigated in early paralinguistic studies. Depressed and suicidal patients showed prosodic speech abnormalities such as reduced pitch range, articulation errors, slower speaking rate and reduced pitch. One of the earliest efforts in this regard is the work of Dr. S. Silverman. After analyzing recorded psychiatry sessions of suicidal patients, he discovered that in pre-suicidal phase the patients\u2019 speech can be distinguished through noticeable changes in its quality [14]. Silverman noticed that speech production mechanisms of pre-suicidal patients have altered the acoustic features of speech in measurable ways [53]. Table 4 lists the most used prosodic features in automatic suicide assessment.\nEnergy Measured as the mean-squared central\ndifference across frames and may correlate with motor coordination\n[23] [24][26] [13] [39]\nControl: 2.8806 (n=504) Suicide: 2.3614 (n=84) \ud835\udc5d \u2264 0.001 [23]\nMaximum phonation time The mean of three attempts of the following measure is taken: the\nmaximum time during which phonation of a vowel (usually lal) is sustained as long as possible with an upright position, deep breath, and a comfortable pitch and loudness\n[23] N/A\nSpeech rate Number of speech utterances per\nsecond over the duration of the speech sample (including pauses).\n[21] N/A\nTime talking Sum of the duration of all speech\nsegments.\n[21] N/A\nPause duration mean Mean duration of pause length.\n[21] N/A\nPause variability Measures of dispersion of pause\nduration (variance, standard deviation).\n[21] N/A\nPause rate Total length of pauses divided by the\ntotal length of speech (including pauses).\n[21] N/A\nPauses total Total duration of pauses. [21] N/A\nPitch The pitch level [20]\n[23][24] [26] [29]\nN/A\nPSP Measure is derived by fitting a\nparabolic function to the lower frequencies in the glottal flow spectrum.\n[21] [22] Control : 0.36 (n= 504)\nSuicide : 0.50 (0.09) (n=84)\n\ud835\udc5d \u2264 0.05 [22] MDQ The glottal closure instants (GCI) and\nthe dispersion of peaks in relation to the GCI position is averaged across different frequency bands and then normalized to the local glottal period which outputs the MDQ parameter .\n[22] N/A\n3.2.2 Prosodic features analysis Among prosodic features F0 is the most used one. France et al. [33] studied the prosodic feature (f0), along with other acoustic features extracted from two recording samples, male subjects sample and female subjects samples. They found that formant and spectral features are the best discriminators between male and female samples, and that prosodic feature is the most discriminator within male subjects. Belouali et al. [23] analysed prosodic features to study the statistical significance of the correlation of these features in army veterans' daily conversation and suicidality. A fused set of 15 acoustic features extracted from the speech were measured by the ensemble feature selection. Random Forest (RF) classifier was applied to determine suicidal group, which yielded 86% sensitivity and 70% specificity. Gideon et al. [26] extracted\neGeMAPS features that include prosodic features along with emotional features obtained from the speech from recordings of natural phone conversations to detect suicidal callers. They could extract low-level descriptors (LLDs) for frequency, amplitude, spectral and energy, the parameters in every speech section, which yielded 23 values per frame."
        },
        {
            "heading": "3.3. Formant features",
            "text": "Formant features are also known as filter features, which measure the resonant characteristic of the nasal and vocal tracts that filter the source coming from the vocal folds. The shape of the nasal and vocal tracks are used to reduce some frequencies and increase other frequencies.\n3.3.1. Formant features and suicidal As formant features include vocal tract and acoustic resonance information, hence these features can capture the changes in the suicidal patient\u2019s speech changes, if these changes are reflected in mucus secretion and muscle tension. Early studies reported that suicidal speech can be associated with increase in formant frequency and decrease in its bandwidth. In addition to that, suicidal and depressed patients have different, as suicidal patients had shifts in power from lower to higher frequency compared to depressed patients [33]. Table 5 lists the most used formant features in automatic suicide assessment.\nprosodic inventory used over utterances and the monotonicity of the speech\n3.3.2. Formant features analysis Many studies have found associations between formant feature changes and suicidal state. For instance, the early study by France et al. [33] showed that the increase in formant features, specifically the formant frequencies and the decrease in formant bandwidth are higher in suicidal patients. Stasak et al [27] extracted the formant F1 features, in addition to the disfluency and voice quality features from the speech of 226 psychiatric inpatients with a recorded suicidal behavior. They manually annotated the recorded speeches and converted them to low-dimensional vectors, which helped to identify suicidal patients with more than 73% accuracy. Shah et al. [29] extracted the F0, F1 and F2, spectral flux, loudness and average temporal interval of voiced, as well as silent speech sections. Every feature was averaged over the total duration of the video yielding scalar features. Variance in pitch was measured as the standard deviation in the F0 values in voiced sections."
        },
        {
            "heading": "3.4. Spectral features",
            "text": "Spectral features measure the spectrum of the subject\u2019s speech; which represents the frequency distribution of the speech signal at a given time. Some of the widely used spectral features for suicidal speech analysis include Mel Frequency Cepstral Features (MFCCs) and Power Spectral Density (PSD). Many previous studies have proven the relationship between spectral features, such as energy shift that is measured by PSD, and suicidal speech [42]. Akkaralaertsest et al. [35] extracted and fused spectral features, including the Glottal Spectral Slope (GSS) and MFCC from the voiced section of the speech sample database. Following that, they combined these spectral features with other acoustic features, mainly source and formant features to a classification model. Their results prove that by combining features from the speech production system (source features), along with spectral features can largely increase the classification accuracy. Keskinpala et al [39] extracted and analyzed the spectral features of male and female speech samples from high-risk suicidal patients, specifically, they analyzed MFCC and energy in frequency bands. Their results suggest that mel-cepstral coefficients and energy in frequency band features are highly effective to distinguish between depressed and suicidal patients. Anunvrapong et al. [36] leveraged MFCC and Delta-MFCC (\u0394MFCC) spectral features. They extracted and analyzed \u0394MFCCs which is the sixteen consecutive MFCCs, then trained a classifier using various speech datasets of depressed and suicidal patients. Their proposed classifier can distinguish between suicidal and depression patients with 95% accuracy. Ozdas et al. [38] extracted MFCC features by dividing the voiced speech segments and measuring the logarithm of the discrete Fourier transform (DFT) of every speech segment. Every log spectrum is further filtered with 16 triangular filters. Using only the MFCC features, they could reach 80% accuracy in classifying near-term suicidal patients and non-depressed subjects. Similarly, Wahidah et al. [40] used PSD and MFCC features and achieved 79% accuracy in classifying suicidal patients. Table 6 lists the most used spectral features in automatic suicide assessment.\ntract filter and the higher coefficients\nrepresent periodic vocal fold sources.\n\u0394MFCC Measures rapid temporal information\ncaptured in the MFCC extraction\n[36][23] Control: 0.1893 (n= 504)\nSuicide: 0.1805 (n=84)\n\ud835\udc5d \u2264 0.001 [23] PSD describes how the power of your voice is\ndistributed over frequency\n[34] [39] [40] [41] [42] N/A\nGSS Measures the periodogram of the voiced\nsegments.\n[35] N/A\nPS The feature is essentially an effective\ncorrelate of the spectral slope of the speech signal.\n[15] [22] Control : -0.20 (n= 504)\nSuicide : -0.24 (n=84) \ud835\udc5d \u2264 0.05 [22]"
        },
        {
            "heading": "4. Suicidal visual markers",
            "text": "Visual markers are classified as facial features; eye movement features or posture features."
        },
        {
            "heading": "4.1. Facial features",
            "text": "A growing literature in mental disorder detection has proven that facial appearance and facial expression can carry significant non-verbal cue that can be further interpreted to assess various mental disorders such as depression [8], bipolar [56] and social anxiety [57]. Given that mental disorders are driving factors of suicide, the facial expression is an important non-verbal suicidal cue for distinguishing suicidal patients [58]. Kleiman et al. [30] analyzed the facial feature extracted from photos of 40 people who committed suicide. After performing a t-test, they found that accuracy in discerning whether a subject had committed suicide was significantly higher than chance guessing by participants. Laksana et al. [28] extracted various visual features including frowning, smiling, head movement and eyebrow-raising behaviors. They investigated the occurrence and the frequency of these behaviors. The results suggest that facial expressions such as smiling behaviors are more statistically correlated with suicide than other visual features such as eye movements and head movements. Eigbe et al. [31] studied the smile dynamics (e.g. genuine vs fake smiles) of three groups: people with suicide ideation, people with depression and healthy control subjects. They found that suicidal subjects had the shortest smiling time duration, and the healthy control subject had the longest smiling time. Moreover, the percentage of genuine smiles in suicidal subjects was the lowest, and the healthy control subjects also had a higher percentage of speaking smiles and the longest laughing duration compared to depressed and suicidal groups."
        },
        {
            "heading": "4.2. Eye movement features",
            "text": "Eye movement features have been proven to have a strong correlation with suicidal intent, especially when the suicidal patients are intimidated by a suicide-related question that they often try to avoid answering. In such circumstances, the patients tend to avoid eye contact with the interviewer, with frequent gazing down behaviors. Eigbe et al. [31] studied the gaze aversion (e.g. looking down) of three groups: people with suicide ideation, people with depression and healthy control subjects. They found that the depressed group had significantly less gaze down count than the suicidal group and healthy control group. Additionally, suicide group subjects have higher gazing down, they also found that suicidal patients spent a higher frequency of gazing down than control patients, and their gazes time were longer than other groups as well. Shah et al. [29] investigated the eye gaze movement and concluded that the frequent shift in eye gaze\naversion and eye gaze represents a vital behavioral indicator, as they reflect the subjects\u2019 social avoidance, which is strongly correlated with suicidal ideation."
        },
        {
            "heading": "4.3. Posture features",
            "text": "Although posture features extracted from body movement have been proven to be a strong indicator of depression [8], very few works have studied the relationship between body movement and suicidal behaviors. Galatzer-Levy et al. [25] used OpenFace to extract the angle of the head\u2019s yaw (horizontal movements) and head\u2019s pitch (vertical movement) from a video recording of suicidal patients\u2019 interviews. After analyzing the head movement along with vocal and facial features, and their correlation with suicidality, their results indicate that head yaw and head pitch variability have significant negative linear relationships with suicidality and that low levels of head movement is strongly correlated with suicide severity. Laksana et al. [28] studied head movements that are interpreted as anxious expressions, such as fidgeting, and looking around the room, and their relationship with suicide ideation. They observed that suicidal patients often exhibit anxious expressions and their high head velocity is higher than healthy control subjects that remain relatively stable during the interview. Table 7 summarizes the most used visual feature for automatic suicide detection.\nHead yaw (side to side movement) [25] [64] STIP features (upper body movement) [65][66]\nIn Table 8, we summarize the above-reviewed research in terms of data source, used acoustic and visual features, analysis method, and targeted suicide type. Descriptive statistical methods tried to gain insight into the different audiovisual features and the presence of audiovisual markers in different groups (e.g. suicidal group vs healthy control group). Machine learning methods on the other hand are more focused on the automatic prediction of suicidal patterns in the input features in raw audiovisual data, even in the absence of context (e.g. patient group). Although we cannot rely on statistical analysis alone to draw general conclusions regarding audiovisual features and their relevance to specific suicidal markers, statistical insights have the potential to boost the training process of machine learning models by reducing the training time and increasing predictive performance. For example, this can be accomplished by leveraging the predictions of multiple different models trained on independent datasets from different clinical environments and aggregating them under a single modeling framework.\nTable 8 Audio-visual suicide assessment scheme\nRef Data\nsource\nAcoustic features Visual features Analysis method Targeted\nsuicide type\n[20] Conversati\non\nMFCCs line spectral frequencies jitter shimmer N/A SVM Passive suicide ideation\nSuicidal action\n[21][6][22] Interview NAQ\nQOQ PSP MDQ PS Formants (F1, F2)\nN/A SVM\nAdaBoostM1\nPassive suicide ideation\n[23] Interview MFCC, Energy,\nAmplitude, F0, jitter, shimmer, amplitude perturbation quotient, pitch perturbation quotient, logarithmic energy, QOQ and NAQ\nN/A logistic regression (LR);\nrandom forest (RF);\nSVM\nXGBoost (XGB);\nk-nearest neighbors (KNN);\ndeep neural network (DNN)\nPassive suicide ideation\nActive suicide ideation\nSuicidal action\n[24] Interview pitch\nenergy\nspectral feature\nN/A Lyapunov coefficient Active\nsuicide ideation\nvoice quality Suicidal\naction\n[25] Interview speech prevalence The angle of the head\u2019s pitch\n(up and down movement)\nyaw (side to side movement)\nFacial Action Coding Scheme (FACS)\nmultiple linear regression Suicidal action\n[26] Conversati\non\nEMA\neGeMAPS features Emotions DNN (emotion\nrecognition)\nActive suicide ideation\nSuicidal action\n[27] Reading\ntask\nF1\nDisfluency\nGRBASI voice quality\nN/A SVM\nKNN\nActive suicide ideation\nSuicidal action\n[28] Interview N/A Smiling,\nFrowning Behavior,\nEyebrow Raises,\nHead Motion Velocity\nSVM\nRandom Forest,\nMultinomial Na\u0131ve Bayes.\nActive suicide ideation\nSuicidal action\n[29] Conversati\non\nF0, F1, F2, loudness, spectral flux, pitch variations\nshift in eye gaze\neye gaze aversion\npupil location\ngaze angle\nfrequently shifting gaze\nMultimodal predictive modelling Active suicide\nideation\n[30] Images N/A Facial features Signal detection theory Suicidal\naction\n[31] Interview N/A Smile\nGazing down behavior\nWilcoxon Rank Sum test\nPassive suicide ideation\nActive suicide ideation\nSuicidal action\n[13] Interview Energy\nf0\nPeak slope\nSpectral stationarity\nN/A HMM\nSVM\nPassive suicide ideation\nActive suicide ideation\nSuicidal action\n[33] Interview f0\nAmplitude modulation\nFormants\nPower distribution\nN/A Autoregressive model Passive\nsuicide ideation\n[34] Read\nspeech\ninterview\nPower spectral densities\npeak power, peak location,\nPSD\nN/A Analysis of variance\n(ANOVA)\nPassive suicide ideation\n[14] Natural\nspeech\nJitter\nGlottal flow spectrum\nN/A Maximum Likelihood\nClassifier\nSuicidal action\n[35] Interview\nRead speech\nGlottal Spectral Slope\nMFCC\nN/A PCA\nLeast Squares (LS)\nPassive suicide ideation\n[36] interview MFCC N/A ML and LMS Passive\nsuicide ideation\n[37] Interview Vocal-tract articulation N/A GMM Passive\nsuicide ideation"
        },
        {
            "heading": "5. Artificial intelligent suicide detection models",
            "text": "To detect suicidal signs from audiovisual data, most of the existing works applied either statistical analysis to describe the uniqueness of suicidal patients compared to healthy subjects, or machine learning classifiers to infer the commonalities among suicidal patients. Statistical analysis is usually performed to infer metaknowledge from the extracted audiovisual features. For instance, Doval et al. [24] modeled the speech feature stream as the observed variable of a nonlinear dynamical system, then they calculated the largest Lyapunov coefficient and correlation dimension of the speech series. Gideon et al. [26] investigated the statistical relationship between suicidal ideation and emotion estimated from speech. Specifically, they computed the within-subject standard deviation of each emotion to gauge each emotion\u2019s variability. Similarly, Shah et al. [29] computed GNorm distance to identify the statistical significance between suicidal and non-suicidal groups. ML classification methods are used to categorize the subjects with similar audiovisual features and are expected to distinguish suicidal patients from healthy subjects. Surprisingly, deep-learning models are rarely used in the literature on suicide detection from audiovisual data, despite their popularity in similar tasks such as depression detection [67], as deep-learning models have been proven to achieve good results when applied to audiovisual data. Overall, automatic suicide assessment can reduce diagnosis cost, offer instantaneous detection and avoids human biases."
        },
        {
            "heading": "5.1. Linear classifiers",
            "text": "SVM are supervised learning models with associated learning algorithms that analyze data for classification tasks, as well as regression analysis. Although SVM is most used for linear classification tasks, but it can also deal with non-linear classification using what is called the kernel trick, implicitly mapping their inputs\nRead speech\n[38] Interview MFCC N/A GMM Passive\nsuicide ideation\n[39] Interview\nRead speech\nMFCC,\nEnergy in frequency bands (power spectral density).\nN/A Unimodal Gaussian\nmodelling\nPassive suicide ideation\n[40] [41] Interview\nRead speech\nPSD\nMFCC\nN/A Multiple linear\nregressions\nPassive suicide ideation\n[54] Interview\nRead speech\nMFCC N/A GMM Passive\nsuicide ideation\n[42] Interview\nRead speech\nPSD\nVocal-tract articulation\nN/A GMM Passive\nsuicide ideation\ninto high-dimensional feature spaces [68]. SVM is by far the most used for classifying suicidal patients [13], [20]\u2013[23], [27], [28], [31][55], this is due to the fact that SVM requires just small set of training data, which made it desirable for the aforementioned works. Chakravarthula et al. [20] leveraged SVM classifier to categorize the subjects into various classes (risk vs no-risk, degree of risk, non-severe vs severe risk). They applied sample weighting to mitigate class imbalance and tuned hyperparameters such as feature normalization scheme. Scherer et al. [9] compared SVM with HMM to classify interview recorded audio into suicidal/Non-suicidal. They reported that SVM can achieve accuracy of up to 75%, but HMM could achieve up to 81.25% classification accuracy. Similarly, Stasak et al. [27] used an automatic 2-class classification using SVM and non-linear cosine k-nearest neighbor (KNN) algorithm. While some other works [37], [38], [42] have applied GMM) to model the suicidal patients' class distributions of the extracted audiovisual features."
        },
        {
            "heading": "5.2. Boosting classifiers",
            "text": "Ensemble methods leverage more than one weak learner algorithm to improve generalizability and overcome the drawback of a single weak estimator. Boosting is an ensemble technique primarily used to reduce bias that converts weak learners to strong ones. Boosting classifier gives relatively high accuracy when combined more than weak classifiers such as SVM, for suicide/non suicide groups classification. For example, Venek et al. [21][22] proposed a hierarchical classification method with two layers. In the first layer, the suicidal patients and the non-suicidal patients are distinguished using SVM classifier. In the second layer, the suicidal repeaters and the non-repeaters are classified using the ensemble algorithm AdaBoostM1. Using only the patients\u2019 features, the classification of suicidal vs. non- suicidal patients achieve an accuracy of 85%. However, by adding the clinician\u2019s features the accuracy was improved to 86.7%."
        },
        {
            "heading": "5.3. Decision trees and random forests",
            "text": "Decision trees are non-parametric supervised learning methods that can be used for classification. The rationale behind using tree structure is to create a model that predicts the value of a target variable by learning simple decision rules deduced from the data features. A tree can be viewed as a piecewise constant approximation. A random forest is an ensemble method that combines many decision trees at training time. In the context of classification, the output of the random forest is the class selected by most trees. Belouali et al. [23] applied RF classifier on a fused set of 15 acoustic features extracted from the speech to determine the suicidal group, which yielded 86% sensitivity, 70% specificity. However, Laksana et al. [28] reported that SVM performed better than RF when applied to a single visual feature (smiling)."
        },
        {
            "heading": "5.4. Deep neural networks",
            "text": "Deep neural networks refer to a class of supervised/unsupervised algorithms that are modeled to imitate the human brain, that are designed to recognize patterns. In the context of a classification task, DNN reads the data through the input layer of multiple perceptrons, passes it through stacked hidden layers, and finally the output layer determines the class of the input. DNN performs better than conventional classifiers when the dataset is relatively large. DNN have been reported to achieve high performance when dealing with audiovisual data. Belouali et al. [23] compared six different supervised classification algorithms on audiovisual to classify suicidal/non suicidal patients, namely DNN, logistic regression, RF, SVM, XGB and KNN. When applied to acoustic features only, DNN yielded the highest specificity 70% compared to other classifiers, but the lowest specificity 50% when applied to linguistic features only, which confirms that DNN are more suitable for audiovisual data.\nIn Table 9, we compare the performance of different ML classifiers in the context of suicidal assessment using audiovisual."
        },
        {
            "heading": "6. Discussion, limitations and future directions",
            "text": "In this section, we summarize the finding of this review, and discuss the limitation, as well as potential future directions."
        },
        {
            "heading": "6.1. Discussion",
            "text": "Recording settings: The audiovisual data recording is conducted in a controlled, semi-controlled, or free environment. In controlled settings, the recording is conducted inside an acoustically ideal environment such as an anechoic chamber with a high-quality microphone, and in some cases, the patients were explicitly instructed not to drink caffeinated drinks or alcoholic beverages for several hours before the recording to ensure the use of these did not influence their voice quality. While in semi-controlled environment, the requirement of the soundproof chamber and high-quality microphone is relaxed, therefore the recording can be done remotely, however, the subjects are still instructed to follow some guidelines, such as being in an empty room, or facing a laptop during the recording. Finally, in the free environment, the recording is conducted in any circumstances, usually using smartphones, which gives the advantage of interacting with the patients in natural environments, hence minimizing recall bias and maximizing ecological validity, and allowing the study of micro-processes that influence behavior in real-world contexts [69]. On the other hand, this poses a great challenge in processing the voice in an uncontrolled environment. Which involves extra work on noise removal and acoustic feature pre-processing, due to the low quality of smartphone microphones compared to dedicated microphones [70].\nImportant audiovisual features: Among all the acoustic features, MFCC and F0 is the most used feature that has been strongly correlated with the features and has consistently been observed to change with a speaker\u2019s mental state. And when MFCCs are inputted to GMM as an effective mechanism of speech analysis, this combination has been proven to be ideal for classifying either high-low levels of suicidality. Facial expression is the most effective feature that has been correlated with suicide. Most of the facial features used in the reviewed works are dynamic features, where the changes in the facial feature are observed over a period of time, such as smile intensity or eye movement, which require video recording. Therefore, we clearly observe that works that used video have generally high accuracy compared to works that use only static visual features extracted from images. Furthermore, works that used multimodal features from different categories or even within the same category yield better accuracy compared with single modality schemes, e.g., combining multiple audio features from different categories or visual cues from face and upper body for instance.\nStandardized audiovisual processing framework: Preprocessing, extracting and manipulation of audiovisual features can pose many challenging tasks. However, the usage of collaborative and freely available repositories for speech and video processing frameworks can facilitate boost research in this field. For acoustic feature processing, the collaborative voice analysis repository for speech technologies (COVAREP) [71] and The Geneva minimalistic acoustic parameter set (GeMAPS) [72] are the most popular tools of acoustic feature manipulation. For the visual feature, one of the prominent tools is OpenFace library, which is capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.\nIn Table 10, we summarize the strengths and weaknesses of automatic assessment compared to clinical assessment."
        },
        {
            "heading": "6.2. Ethics and privacy",
            "text": "Developing AI-based suicide detection models that can be applied to real-time systems, such as video streaming and calling apps, raises serious privacy concerns from a research perspective as well as a deployment perspective. From a research point of view, the users\u2019 privacy might be jeopardized throughout the data processing stages. When dealing with publically available datasets, the problem arises when users\u2019 personal attributes can be predicted, and the identity of subjects can be revealed. In this context, many jurisdictions require certain conditions for research that can compromise users\u2019 privacy. The most common\nprocedure is that the researchers must acquire an ethical approval or exemption from their Institutional Review Board (IRB) before the study. In addition to that, they must obtain informed consent from the participant when possible, protect, and anonymize sensitive data during all research stages. Moreover, they need to be careful when linking data across sites is necessary. Finally, when sharing their data, they need to make sure that other researchers also adhere to the same privacy guidelines [73]. Researchers can rely on public social media datasets for AI-based suicidal behavior research as long as they ensure the preservation of the confidentiality of users. The privacy concern is more challenging when applying such AI-based solutions in real-time and large-scale scenarios."
        },
        {
            "heading": "6.3. Limitation",
            "text": "The limitations of reviewed works on audio-visual data analysis for suicidal behavior detection can be summarized as follows:\nLarge-size dataset and population generalization: One of the main limitations of some related works is the relatively small size of the dataset used to train the proposed models, both in terms of the number of patients and recorded duration. E.g. n=20 [25], n=90 [29] , n=97 [24], n=126 [28], n=124 [20]. Morever, many of the previous works have focused their studies on specific high-risk groups, and the dataset that they used to generate the results and draw the conclusion is related only to that specific group. For example, army veterans [23][24], Adolescent [22], LGBT [74]. However, acoustic variability is highly affected by linguistic information and speaker characteristics such as age, gender and cultural background. Therefore, the challenges in finding a speech-based marker for suicide require the study of a large population from different groups to tune and generalize the proposed suicide assessment tool.\nLack of public datasets: Most of the relevant public datasets, including the datasets presented in Tabel 2, are general-purpose datasets that have been collected in the context of detecting abnormal behavior that can be latent symptoms of mental illnesses. The lack of public, or even private, datasets that have been collected specially for suicidal behavior detection, is one of the main limitations in this line of research."
        },
        {
            "heading": "6.4. Future directions",
            "text": "Unlike text-based suicidal assessment that has been widely studied, suicide assessment through audiovisual data analysis is still in early development. There are many potential future research directions in this area:\n Conversational AI for suicide assessment: conversational AI agents can engage in a conversation\nwith a user through written text or voice. Conversational AI is an important technology that has the potential to dominate the area of automatic suicide detection and intervention. As suicidal patients may feel more comfortable expressing their thoughts to AI agents than to a human. The future direction in this area is to train AI chatbots on a large data-set of suicide notes, and use this pretrained model and customize it to the characteristics of each patient.\n Multimodal deep-learning for suicide detection: Deep-learning models have been proven to achieve\ngood results when applied to audiovisual data. Applying deep-learning to multimodal features from different audio-visual categories or even within the same category usually yield better accuracy compared with single modality schemes. For example, conventional neural networks (CNN) are effective when dealing with images, video or audio, while recurrent neural networks (RNN) models such as LSTM and GRU are effective in handling time-series data. One prominent future direction is applying a hybrid CNN-RNN model that has been pre-trained on video interviews of confirmed suicidal patients, such a pre-trained model can capture the time-series markers of suicidal behaviors."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this paper, we have reviewed the recent works that studied suicide ideation and suicidal behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues. Audiovisual features have been proven to convey significant non-verbal cues that can be further interpreted to assess various mental disorders, however, the literature of audiovisual based suicide assessment is still relatively limited to a few small-scale experiments. Automatic suicide assessment is a promising research direction that is still in the early stages. There is a lack of large datasets that can be used to train machine learning and deep learning models that have been proven effective in other similar tasks. Building an AI-enabled suicide risk assessment tool that observes the patient\u2019s biomarker and audiovisual cues and learns suicide ideation patterns might be a decisive indicator that could help the clinician to reach a decisive judgment."
        },
        {
            "heading": "36\u201338, 2004.",
            "text": "[39] H. K. Keskinpala, T. Yingthawornsuk, D. M. Wilkes, R. G. Shiavi, and R. M. Salomon,\n\u201cScreening for high risk suicidal states using mel-cepstral coefficients and energy in frequency bands,\u201d in 2007 15th European Signal Processing Conference, 2007, pp. 2229\u20132233.\n[40] N. N. W. NH, M. D. Wilkes, and R. M. Salomon, \u201cTiming Patterns of Speech as Potential\nIndicators of Near-Term Suicidal Risk,\u201d Int. J. Multidiscip. Curr. Res., vol. 3, 2015.\n[41] N. N. W. NH, M. D. Wilkes, and R. M. Salomon, \u201cInvestigating the Course of Recovery in High\nRisk Suicide using Power Spectral Density,\u201d Asian J. Appl. Sci., vol. 3, no. 4, 2015.\n[42] T. Yingthawornsuk, H. K. Keskinpala, D. M. Wilkes, R. G. Shiavi, and R. M. Salomon, \u201cDirect\nacoustic feature using iterative EM algorithm and spectral energy for classifying suicidal speech,\u201d 2007.\n[43] J. Gratch et al., \u201cThe distress analysis interview corpus of human and computer interviews,\u201d in\nProceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), 2014, pp. 3123\u20133128.\n[44] M. Valstar et al., \u201cAvec 2013: the continuous audio/visual emotion and depression recognition\nchallenge,\u201d in Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge, 2013, pp. 3\u201310.\n[45] F. Ringeval et al., \u201cAVEC 2019 workshop and challenge: state-of-mind, detecting depression with\nAI, and cross-cultural affect recognition,\u201d in Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop, 2019, pp. 3\u201312.\n[46] T. Drugman, B. Bozkurt, and T. Dutoit, \u201cA comparative study of glottal source estimation\ntechniques,\u201d Comput. Speech \\& Lang., vol. 26, no. 1, pp. 20\u201334, 2012.\n[47] M. Airas, \u201cTKK Aparat: An environment for voice inverse filtering and parameterization,\u201d Logop.\nPhoniatr. Vocology, vol. 33, no. 1, pp. 49\u201364, 2008.\n[48] B. Doval, C. d\u2019Alessandro, and N. Henrich, \u201cThe spectrum of glottal flow models,\u201d Acta Acust.\nunited with Acust., vol. 92, no. 6, pp. 1026\u20131046, 2006.\n[49] J. Walker and P. Murphy, \u201cA review of glottal waveform analysis,\u201d Prog. nonlinear speech\nProcess., pp. 1\u201321, 2007.\n[50] T. F. Quatieri and N. Malyska, \u201cVocal-source biomarkers for depression: A link to psychomotor\nactivity,\u201d 2012.\n[51] J. Sundberg, S. Patel, E. Bjorkner, and K. R. Scherer, \u201cInterdependencies among voice source\nparameters in emotional speech,\u201d IEEE Trans. Affect. Comput., vol. 2, no. 3, pp. 162\u2013174, 2011.\n[52] J. Kane, M. Aylett, I. Yanushevskaya, and C. Gobl, \u201cPhonetic feature extraction for context-\nsensitive glottal source processing,\u201d Speech Commun., vol. 59, pp. 10\u201321, 2014.\n[53] S. E. Silverman, M. K. Silverman, and others, \u201cMethods and apparatus for evaluating near-term\nsuicidal risk using vocal parameters.\u201d Google Patents, 2006.\n[54] H. Kaymaz Keskinpala, T. Yingthawornsuk, R. M. Salomon, R. G. Shiavi, and D. M. Wilkes,\n\u201cDistinguishing High Risk Suicidal Subjects among Depressed Subjects Using Mel-Frequency Cepstrum Coefficients and Cross Validation Technique,\u201d Disting. High Risk Suicidal Subj. among Depress. Subj. Using Mel-Frequency Cepstrum Coefficients Cross Valid. Tech., pp. 1000\u20131004, 2007.\n[55] P. P. Reddy, C. Suresh, V. K. Rao, K. S. Chandana, S. Sowkya, and R. Akhila, \u201cVocal Analysis to\nPredict Suicide Tendency,\u201d in Proceedings of International Conference on Advances in Computer Engineering and Communication Systems, 2021, pp. 481\u2013488.\n[56] H. R. Venn et al., \u201cPerception of facial expressions of emotion in bipolar disorder,\u201d Bipolar\nDisord., vol. 6, no. 4, pp. 286\u2013293, 2004.\n[57] P. J. Silvia, W. D. Allan, D. L. Beauchamp, E. L. Maschauer, and J. O. Workman, \u201cBiased\nrecognition of happy facial expressions in social anxiety,\u201d J. Soc. Clin. Psychol., vol. 25, no. 6, pp. 585\u2013602, 2006.\n[58] Y. Wang, N. Guobule, M. Li, and J. Li, \u201cThe correlation of facial emotion recognition in patients\nwith drug-na{\\\"\\i}ve depression and suicide ideation,\u201d J. Affect. Disord., vol. 295, pp. 250\u2013254, 2021.\n[59] S. Scherer et al., \u201cAutomatic audiovisual behavior descriptors for psychological disorder\nanalysis,\u201d Image Vis. Comput., vol. 32, no. 10, pp. 648\u2013658, 2014.\n[60] J. R. Williamson et al., \u201cDetecting depression using vocal, facial and semantic communication\ncues,\u201d in Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, 2016, pp. 11\u201318.\n[61] Q. Wang, H. Yang, and Y. Yu, \u201cFacial expression video analysis for depression detection in\nChinese patients,\u201d J. Vis. Commun. Image Represent., vol. 57, pp. 228\u2013233, 2018.\n[62] R. Gupta et al., \u201cMultimodal prediction of affective dimensions and depression in human-\ncomputer interactions,\u201d in Proceedings of the 4th international workshop on audio/visual emotion challenge, 2014, pp. 33\u201340.\n[63] M. Nasir, A. Jati, P. G. Shivakumar, S. Nallan Chakravarthula, and P. Georgiou, \u201cMultimodal and\nmultiresolution depression detection from speech and facial landmark features,\u201d in Proceedings of the 6th international workshop on audio/visual emotion challenge, 2016, pp. 43\u201350.\n[64] S. Alghowinem et al., \u201cMultimodal depression detection: fusion analysis of paralinguistic, head\npose and eye gaze behaviors,\u201d IEEE Trans. Affect. Comput., vol. 9, no. 4, pp. 478\u2013490, 2016.\n[65] J. Joshi, R. Goecke, G. Parker, and M. Breakspear, \u201cCan body expressions contribute to automatic\ndepression analysis?,\u201d 2013, doi: 10.1109/FG.2013.6553796.\n[66] J. Joshi, A. Dhall, R. Goecke, M. Breakspear, and G. Parker, \u201cNeural-net classification for spatio-\ntemporal descriptor based depression analysis,\u201d in Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012), 2012, pp. 2634\u20132638.\n[67] L. He and C. Cao, \u201cAutomated depression analysis using convolutional neural networks from\nspeech,\u201d J. Biomed. Inform., vol. 83, pp. 103\u2013111, 2018.\n[68] D. J. Sebald and J. A. Bucklew, \u201cSupport vector machine techniques for nonlinear equalization,\u201d\nIEEE Trans. signal Process., vol. 48, no. 11, pp. 3217\u20133226, 2000.\n[69] I. Gratch et al., \u201cDetecting suicidal thoughts: The power of ecological momentary assessment,\u201d\nDepress. Anxiety, vol. 38, no. 1, pp. 8\u201316, 2021.\n[70] B. Barsties and M. De Bodt, \u201cAssessment of voice quality: current state-of-the-art,\u201d Auris Nasus\nLarynx, vol. 42, no. 3, pp. 183\u2013188, 2015.\n[71] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer, \u201cCOVAREP\u2014A collaborative voice\nanalysis repository for speech technologies,\u201d in 2014 ieee international conference on acoustics,\nspeech and signal processing (icassp), 2014, pp. 960\u2013964.\n[72] F. Eyben et al., \u201cThe Geneva minimalistic acoustic parameter set (GeMAPS) for voice research\nand affective computing,\u201d IEEE Trans. Affect. Comput., vol. 7, no. 2, pp. 190\u2013202, 2015.\n[73] A. Benton, G. Coppersmith, and M. Dredze, \u201cEthical research protocols for social media health\nresearch,\u201d in Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017, pp. 94\u2013102.\n[74] D. M. Skerrett, K. K\u00f5lves, and D. De Leo, \u201cAre LGBT populations at a higher risk for suicidal\nbehaviors in Australia? Research findings and implications,\u201d J. Homosex., vol. 62, no. 7, pp. 883\u2013 901, 2015."
        }
    ],
    "title": "Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review",
    "year": 2022
}