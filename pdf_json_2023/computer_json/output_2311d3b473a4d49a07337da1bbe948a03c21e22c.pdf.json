{
    "abstractText": "We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.",
    "authors": [
        {
            "affiliations": [],
            "name": "Felix Biggs"
        },
        {
            "affiliations": [],
            "name": "Antonin Schrab"
        }
    ],
    "id": "SP:6bc7ee5acc4e25f1fffb70304c1cd108be683a7a",
    "references": [
        {
            "authors": [
                "M\u00e9lisande Albert",
                "B\u00e9atrice Laurent",
                "Amandine Marrel",
                "Anouar Meynaoui"
            ],
            "title": "Adaptive test of independence based on HSIC measures",
            "venue": "The Annals of Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Pierre Alquier"
            ],
            "title": "User-friendly introduction to PAC-Bayes bounds",
            "venue": "CoRR, abs/2110.11216,",
            "year": 2021
        },
        {
            "authors": [
                "Gregory Benton",
                "Wesley J Maddox",
                "Jayson Salkey",
                "Julio Albinati",
                "Andrew Gordon Wilson"
            ],
            "title": "Function-space distributions over kernels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Felix Biggs",
                "Benjamin Guedj"
            ],
            "title": "Differentiable PAC-Bayes objectives with partially aggregated neural networks. Entropy",
            "venue": "doi: 10.3390/e23101280. URL https://doi. org/10.3390/e23101280. (Cited on page",
            "year": 2021
        },
        {
            "authors": [
                "Felix Biggs",
                "Benjamin Guedj"
            ],
            "title": "Non-vacuous generalisation bounds for shallow neural networks",
            "venue": "editors, International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Felix Biggs",
                "Benjamin Guedj"
            ],
            "title": "On margins and derandomisation in PAC-Bayes",
            "venue": "Proceedings of The 25th International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Felix Biggs",
                "Benjamin Guedj"
            ],
            "title": "Tighter pac-bayes generalisation bounds by leveraging example difficulty",
            "venue": "Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Felix Biggs",
                "Valentina Zantedeschi",
                "Benjamin Guedj"
            ],
            "title": "On margins and generalisation for voting classifiers",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "St\u00e9phane Boucheron",
                "G\u00e1bor Lugosi",
                "Pascal Massart"
            ],
            "title": "Concentration inequalities: A nonasymptotic theory of independence",
            "venue": "Oxford university press,",
            "year": 2013
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs, 2018",
            "venue": "URL http://github.com/google/jax. (Cited on page",
            "year": 2018
        },
        {
            "authors": [
                "Javier C\u00e1rcamo",
                "Antonio Cuevas",
                "Luis-Alberto Rodr\u00edguez"
            ],
            "title": "A uniform kernel trick for highdimensional two-sample problems",
            "venue": "arXiv preprint arXiv:2210.02171,",
            "year": 2022
        },
        {
            "authors": [
                "Olivier Catoni"
            ],
            "title": "PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning. Institute of Mathematical Statistics lecture notes-monograph series",
            "venue": "Institute of Mathematical Statistics,",
            "year": 2007
        },
        {
            "authors": [
                "Anirban Chatterjee",
                "Bhaswar B. Bhattacharya"
            ],
            "title": "Boosting the power of kernel two-sample tests",
            "venue": "arXiv preprint arXiv:2302.10687,",
            "year": 2023
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey E. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Kevin Swersky",
                "Mohammad Norouzi",
                "Geoffrey E. Hinton"
            ],
            "title": "Big self-supervised models are strong semi-supervised learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross B. Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "CoRR, abs/2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Farah Cherfaoui",
                "Hachem Kadri",
                "Sandrine Anthoine",
                "Liva Ralaivola"
            ],
            "title": "A discrete RKHS standpoint for Nystr\u00f6m MMD",
            "venue": "HAL preprint hal-03651849,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Chugg",
                "Hongjian Wang",
                "Aaditya Ramdas"
            ],
            "title": "A unified recipe for deriving (time-uniform) pac-bayes",
            "venue": "bounds. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Kacper Chwialkowski",
                "Dino Sejdinovic",
                "Arthur Gretton"
            ],
            "title": "A wild bootstrap for degenerate kernel tests. In Advances in neural information processing systems, pages 3608\u20133616",
            "year": 2014
        },
        {
            "authors": [
                "Kacper Chwialkowski",
                "Heiko Strathmann",
                "Arthur Gretton"
            ],
            "title": "A kernel test of goodness of fit",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Kacper P Chwialkowski",
                "Aaditya Ramdas",
                "Dino Sejdinovic",
                "Arthur Gretton"
            ],
            "title": "Fast two-sample testing with analytic representations of probability measures",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Namrata Deka",
                "Danica J. Sutherland"
            ],
            "title": "Mmd-b-fair: Learning fair representations with statistical testing",
            "venue": "CoRR, abs/2211.07907,",
            "year": 2022
        },
        {
            "authors": [
                "Carles Domingo-Enrich",
                "Raaz Dwivedi",
                "Lester Mackey"
            ],
            "title": "Compress then test: Powerful kernel testing in near-linear time",
            "venue": "The 26th International Conference on Artificial Intelligence and Statistics, AISTATS 2023,",
            "year": 2023
        },
        {
            "authors": [
                "M.D. Donsker",
                "S.R.S. Varadhan"
            ],
            "title": "Asymptotic evaluation of certain markov process expectations for large time",
            "venue": "i. Communications on Pure and Applied Mathematics,",
            "year": 1975
        },
        {
            "authors": [
                "A. Dvoretzky",
                "J. Kiefer",
                "J. Wolfowitz"
            ],
            "title": "Asymptotic Minimax Character of the Sample Distribution Function and of the Classical Multinomial Estimator",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1956
        },
        {
            "authors": [
                "Raaz Dwivedi",
                "Lester Mackey"
            ],
            "title": "Kernel thinning",
            "venue": "In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning Theory, COLT 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Daniel M Roy"
            ],
            "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "year": 2017
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Daniel M. Roy"
            ],
            "title": "Data-dependent PAC-Bayes priors via differential privacy",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Kyle Hsu",
                "Waseem Gharbieh",
                "Gabriel Arpino",
                "Daniel Roy"
            ],
            "title": "On the role of data in PAC-Bayes",
            "venue": "The 24th International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Br\u00fcck Florian",
                "Jean-David Fermanian",
                "Aleksey Min"
            ],
            "title": "Distribution free mmd tests for model selection with estimated parameters",
            "venue": "arXiv preprint arXiv:2305.07549,",
            "year": 2023
        },
        {
            "authors": [
                "Magalie Fromont",
                "B\u00e9atrice Laurent",
                "Matthieu Lerasle",
                "Patricia Reynaud-Bouret"
            ],
            "title": "Kernels based tests with non-asymptotic bootstrap approaches for two-sample problems",
            "venue": "In Conference on Learning Theory,",
            "year": 2012
        },
        {
            "authors": [
                "Magalie Fromont",
                "B\u00e9atrice Laurent",
                "Patricia Reynaud-Bouret"
            ],
            "title": "The two-sample problem for Poisson processes: Adaptive tests with a nonasymptotic wild bootstrap approach",
            "venue": "The Annals of Statistics,",
            "year": 2013
        },
        {
            "authors": [
                "Hanjia Gao",
                "Xiaofeng Shao"
            ],
            "title": "Two sample testing in high dimension via maximum mean discrepancy",
            "venue": "arXiv preprint arXiv:2109.14913,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Gretton",
                "Olivier Bousquet",
                "Alex Smola",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Measuring statistical dependence with Hilbert-Schmidt norms",
            "venue": "In International Conference on Algorithmic Learning Theory. Springer,",
            "year": 2005
        },
        {
            "authors": [
                "Arthur Gretton",
                "Kenji Fukumizu",
                "Choon H Teo",
                "Le Song",
                "Bernhard Sch\u00f6lkopf",
                "Alex J Smola"
            ],
            "title": "A kernel statistical test of independence",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "Arthur Gretton",
                "Kenji Fukumizu",
                "Zaid Harchaoui",
                "Bharath K Sriperumbudur"
            ],
            "title": "A fast, consistent kernel two-sample test",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2009
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M. Borgwardt",
                "Malte J. Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander J. Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2012
        },
        {
            "authors": [
                "Arthur Gretton",
                "Dino Sejdinovic",
                "Heiko Strathmann",
                "Sivaraman Balakrishnan",
                "Massimiliano Pontil",
                "Kenji Fukumizu",
                "Bharath K Sriperumbudur"
            ],
            "title": "Optimal kernel choice for large-scale two-sample tests",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Benjamin Guedj"
            ],
            "title": "A primer on PAC-Bayesian learning",
            "venue": "In Proceedings of the second congress of the French Mathematical Society,",
            "year": 1901
        },
        {
            "authors": [
                "Maxime Haddouche",
                "Benjamin Guedj"
            ],
            "title": "Online pac-bayes learning",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Maxime Haddouche",
                "Benjamin Guedj"
            ],
            "title": "Pac-bayes generalisation bounds for heavy-tailed losses through supermartingales",
            "venue": "Trans. Mach. Learn. Res.,",
            "year": 2023
        },
        {
            "authors": [
                "Omar Hagrass",
                "Bharath K. Sriperumbudur",
                "Bing Li"
            ],
            "title": "Spectral regularized kernel two-sample tests",
            "venue": "arXiv preprint arXiv:2212.09201,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross B. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jesse Hemerik",
                "Jelle Goeman"
            ],
            "title": "Exact testing with random permutations",
            "venue": "doi: 10.1007/s11749-017-0571-1. URL https://doi.org/10. 1007/s11749-017-0571-1. (Cited on page",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey E Hinton",
                "Ruslan R Salakhutdinov"
            ],
            "title": "Reducing the dimensionality of data with neural",
            "venue": "networks. science,",
            "year": 2006
        },
        {
            "authors": [
                "Wittawat Jitkrittum",
                "Zolt\u00e1n Szab\u00f3",
                "Kacper P Chwialkowski",
                "Arthur Gretton"
            ],
            "title": "Interpretable distribution features with maximum testing power",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ilmun Kim",
                "Aaditya Ramdas"
            ],
            "title": "Dimension-agnostic inference using cross u-statistics",
            "venue": "(Cited on page",
            "year": 2023
        },
        {
            "authors": [
                "Ilmun Kim",
                "Sivaraman Balakrishnan",
                "Larry Wasserman"
            ],
            "title": "Minimax optimality of permutation tests",
            "venue": "Annals of Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Kirchler",
                "Shahryar Khorasani",
                "Marius Kloft",
                "Christoph Lippert"
            ],
            "title": "Two-sample testing using deep learning",
            "venue": "The 23rd International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "J.M. K\u00fcbler",
                "W. Jitkrittum",
                "B. Sch\u00f6lkopf",
                "K. Muandet"
            ],
            "title": "Learning kernel tests without data splitting",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Jonas M K\u00fcbler",
                "Wittawat Jitkrittum",
                "Bernhard Sch\u00f6lkopf",
                "Krikamol Muandet"
            ],
            "title": "A witness twosample test",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Jonas M K\u00fcbler",
                "Vincent Stimper",
                "Simon Buchholz",
                "Krikamol Muandet",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "AutoML two-sample test",
            "venue": "Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Alexandre Lacasse",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Pascal Germain",
                "Nicolas Usunier"
            ],
            "title": "PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier",
            "venue": "Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems,",
            "year": 2006
        },
        {
            "authors": [
                "Alexandre Lacasse",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Francis Turgeon-Boutin"
            ],
            "title": "Learning with randomized majority votes",
            "venue": "Machine Learning and Knowledge Discovery in Databases, European Conference,",
            "year": 2010
        },
        {
            "authors": [
                "Guy Lever",
                "Fran\u00e7ois Laviolette",
                "John Shawe-Taylor"
            ],
            "title": "Tighter PAC-Bayes bounds through distribution-dependent priors",
            "venue": "Theoretical Computer Science,",
            "year": 2013
        },
        {
            "authors": [
                "Tong Li",
                "Ming Yuan"
            ],
            "title": "On the optimality of gaussian kernel based nonparametric tests against smooth alternatives",
            "venue": "arXiv preprint arXiv:1909.03302,",
            "year": 2019
        },
        {
            "authors": [
                "Yazhe Li",
                "Roman Pogodin",
                "Danica J. Sutherland",
                "Arthur Gretton"
            ],
            "title": "Self-supervised learning with kernel dependence maximization",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Jen Ning Lim",
                "Makoto Yamada",
                "Wittawat Jitkrittum",
                "Yoshikazu Terada",
                "Shigeyuki Matsui",
                "Hidetoshi Shimodaira"
            ],
            "title": "More powerful selective kernel tests for feature selection",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Feng Liu",
                "Wenkai Xu",
                "Jie Lu",
                "Guangquan Zhang",
                "Arthur Gretton",
                "Dougal J Sutherland"
            ],
            "title": "Learning deep kernels for non-parametric two-sample tests",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Feng Liu",
                "Wenkai Xu",
                "Jie Lu",
                "Danica J. Sutherland"
            ],
            "title": "Meta two-sample testing: Learning kernels for testing with limited data",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Liu",
                "Jason Lee",
                "Michael Jordan"
            ],
            "title": "A kernelized Stein discrepancy for goodness-of-fit tests",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Maxime Oquab"
            ],
            "title": "Revisiting classifier two-sample tests",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Andr\u00e9s R. Masegosa",
                "Stephan Sloth Lorenzen",
                "Christian Igel",
                "Yevgeny Seldin"
            ],
            "title": "Second order PAC-Bayesian bounds for the weighted majority vote",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "P. Massart"
            ],
            "title": "The Tight Constant in the Dvoretzky-Kiefer-Wolfowitz Inequality",
            "venue": "The Annals of Probability,",
            "year": 1990
        },
        {
            "authors": [
                "Andreas Maurer"
            ],
            "title": "A note on the PAC-Bayesian theorem",
            "venue": "CoRR, cs.LG/0411099,",
            "year": 2004
        },
        {
            "authors": [
                "David A McAllester"
            ],
            "title": "Some PAC-Bayesian theorems",
            "venue": "In Proceedings of the eleventh annual conference on Computational Learning Theory, pages 230\u2013234",
            "year": 1998
        },
        {
            "authors": [
                "Alfred M\u00fcller"
            ],
            "title": "Integral probability metrics and their generating classes of functions",
            "venue": "Advances in Applied Probability,",
            "year": 1997
        },
        {
            "authors": [
                "Maria Perez-Ortiz",
                "Omar Rivasplata",
                "John Shawe-Taylor",
                "Csaba Szepesvari"
            ],
            "title": "Tighter risk certificates for neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Aaditya Ramdas",
                "Sashank Jakkam Reddi",
                "Barnab\u00e1s P\u00f3czos",
                "Aarti Singh",
                "Larry Wasserman"
            ],
            "title": "On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar"
            ],
            "title": "Do ImageNet classifiers generalize to ImageNet",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sashank Reddi",
                "Aaditya Ramdas",
                "Barnab\u00e1s P\u00f3czos",
                "Aarti Singh",
                "Larry Wasserman"
            ],
            "title": "On the high dimensional power of a linear-time two sample test under mean-shift alternatives",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "David Rindt",
                "Dino Sejdinovic",
                "David Steinsaltz"
            ],
            "title": "Consistency of permutation tests of independence using distance covariance, hsic and dhsic",
            "venue": "Stat, 10(1):e364,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph P Romano",
                "Michael Wolf"
            ],
            "title": "Exact and approximate stepdown methods for multiple hypothesis testing",
            "venue": "Journal of the American Statistical Association,",
            "year": 2005
        },
        {
            "authors": [
                "Mark Rudelson",
                "Roman Vershynin"
            ],
            "title": "Hanson-Wright inequality and sub-gaussian concentration",
            "venue": "Electronic Communications in Probability,",
            "year": 2013
        },
        {
            "authors": [
                "Antonin Schrab",
                "Benjamin Guedj",
                "Arthur Gretton"
            ],
            "title": "KSD aggregated goodness-of-fit test",
            "venue": "Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Antonin Schrab",
                "Ilmun Kim",
                "Benjamin Guedj",
                "Arthur Gretton"
            ],
            "title": "Efficient aggregated kernel tests using incomplete U -statistics",
            "venue": "Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Antonin Schrab",
                "Ilmun Kim",
                "M\u00e9lisande Albert",
                "B\u00e9atrice Laurent",
                "Benjamin Guedj",
                "Arthur Gretton"
            ],
            "title": "MMD aggregated two-sample test",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Matthias Seeger",
                "John Langford",
                "Nimrod Megiddo"
            ],
            "title": "An improved predictive accuracy bound for averaging classifiers",
            "venue": "In Proceedings of the 18th International Conference on Machine Learning, number CONF, pages 290\u2013297,",
            "year": 2001
        },
        {
            "authors": [
                "Yevgeny Seldin",
                "Fran\u00e7ois Laviolette",
                "Nicol\u00f2 Cesa-Bianchi",
                "John Shawe-Taylor",
                "Peter Auer"
            ],
            "title": "PAC-Bayesian inequalities for martingales",
            "venue": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial",
            "year": 2012
        },
        {
            "authors": [
                "Shubhanshu Shekhar",
                "Ilmun Kim",
                "Aaditya Ramdas"
            ],
            "title": "A permutation-free kernel two-sample test",
            "venue": "Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Wojciech Zaremba",
                "Arthur Gretton",
                "Matthew Blaschko"
            ],
            "title": "B-test: A non-parametric, low variance",
            "year": 2021
        },
        {
            "authors": [
                "e.g. Boucheron"
            ],
            "title": "entropy\u201d method instead), and the converse is not true. The fact that we have to prove a variant form of an existing concentration inequality to get concentration for our log-sum-exp statistics is similar to how the same is required in PAC-Bayesian proofs, where e.g. PAC-Bayes Bernstein inequalities also require modified proof techniques that mirror those used to prove the usual Bernstein inequalities",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The fundamental problem of non-parametric two-sample testing consists in detecting the difference between any two distributions having access only to samples from these. Kernel-based tests relying on the Maximum Mean Discrepancy (MMD; Gretton et al., 2012a) as a measure of distance on distributions are well-suited for this framework as they can identify complex non-linear features in the data, and benefit from both strong theoretical guarantees and ease of implementation. This explains their popularity among practitioners and justifies their wide use for real-world applications.\nHowever, the performance of these tests is crucially impacted by the choice of kernel. This is commonly tackled by either: choosing the kernel by some weakly data-dependent heuristic (Gretton et al., 2012a); or splitting off a hold-out set of data for kernel selection, with the other half used for the actual test (Gretton et al., 2012b; Sutherland et al., 2017). This latter method includes training feature extractors such as deep kernels on the first selection half. Both of these methods can incur a significant loss in test power, since heuristics may lead to poor kernel choices, and data splitting reduces the number of data points for the actual test.\nOur contribution is to present MMD-based tests which can strongly adapt to the data without data splitting. This comes in two parallel parts: firstly we show how the kernel can be chosen in an\n\u2217Equal contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 6.\n08 77\n7v 2\n[ st\nat .M\nL ]\nunsupervised fashion using the entire dataset, and secondly we show how multiple such kernels can be adaptively weighted in a single test statistic, optimising test power.\nData Splitting. The data splitting approach selects test parameters on a held-out half of the dataset, and applies the test to the other half. Commonly, this involves optimising a kernel on held-out data in a supervised fashion to distinguish which sample originated from which distribution, either by learning a deep kernel directly (Sutherland et al., 2017; Liu et al., 2020, 2021), or indirectly through the associated witness function (K\u00fcbler et al., 2022a,b). Jitkrittum et al. (2016) propose tests which select witness function features (in either spatial or frequency space) on the held-out data, running the analytic representation test of Chwialkowski et al. (2015) on the remaining data.\nOur first contribution is to show that it is possible to learn a feature extractor for our test (e.g., a deep kernel) on the entirety of the data in an unsupervised manner while retaining the desired non-asymptotic test level. Specifically, any method can be used that is ignorant of which distribution generated which samples. We can thus leverage many feature extraction methods, from auto-encoders (Hinton and Salakhutdinov, 2006) to recent powerful developments in self-supervised learning (He et al., 2020; Chen et al., 2020a,b,c; Chen and He, 2021; Grill et al., 2020; Caron et al., 2020; Zbontar et al., 2021; Li et al., 2021). Remarkably, our method applies to any permutation-based two-sample test, even non-kernel tests, provided the parameters are chosen in this unsupervised fashion. This includes a very wide array of MMD-based tests, and finally provides a formal justification for the commonly-used median heuristic (Gretton et al., 2012b; Ramdas et al., 2015; Reddi et al., 2015).\nAdaptive Kernel Selection. A newer approach originating in Schrab et al. (2023) performs adaptive kernel selection through multiple testing. Aggregating several MMD-based tests with different kernels, each on the whole dataset, results in an overall adaptive test with optimal power guarantees in terms of minimax separation rate over Sobolev balls. Variants of this kernel adaptivity through aggregation have been proposed with linear-time estimators (Schrab et al., 2022b), spectral regularisation (Hagrass et al., 2022), kernel thinning compression (Domingo-Enrich et al., 2023), and in the asymptotic regime (Chatterjee and Bhattacharya, 2023). Another adaptive approach using the entire dataset for both kernel selection and testing is given by K\u00fcbler et al. (2020); this leverages the Post Selection Inference framework, but the resulting test suffers from low power in practice.\nWhile our unsupervised feature extraction method is an extremely general technique and potentially powerful, particularly for high-dimensional structured data like images, it is not always sufficient for data where such feature extraction is difficult. This motivates our second contribution, a method for combining whole-dataset MMD estimates under multiple different kernels into single test statistics, for which we prove exponential concentration. Kernel parameters such as bandwidth can then be chosen in a non-heuristic manner to optimise power, even on data with less structure and varying length scales. Using a single statistic also ensures that a single test can be used, instead of the multiple testing approach outlined above, reducing computational expense.\nMMD-FUSE. By combining these contributions we construct two closely related MMD-FUSE tests. Each chooses a set of kernels based on the whole dataset in an unsupervised fashion, and then adaptively weights and fuses this (potentially infinite) set of kernels through our new statistics; both parts of this procedure are done using the entire dataset without splitting. On the finite sets of kernels we use in practice, the weighting procedure is done in closed form via a weighted soft maximum. We show these new tests to be well-calibrated and give sufficient power conditions which achieve the minimax optimal rate in terms of MMD. In empirical comparisons, our test compares favourably to the state-of-the-art aggregated tests in terms of power and computational cost.\nOutline and Summary of Contributions. In Section 2 we outline our setting, crucial results underlying our work, and alternative existing approaches. Section 3 covers the construction of permutation tests and discusses how we can choose the parameters for any such test in any unsupervised fashion including with deep kernels and by those methods mentioned above. Section 4 introduces and motivates our two proposed tests. Section 5 discusses sufficient conditions for test power (at a minimax optimal rate in MMD), and the exponential concentration of our statistics. Finally, we show that our test compares favourably with a wide variety of competitors in Section 6 and discuss in Section 7."
        },
        {
            "heading": "2 Background",
            "text": "Two-Sample Testing. The two-sample testing problem is to determine whether two distributions p and q are equal or not. In order to test this hypothesis, we are given access to two samples, X := (X1, . . . , Xn)\niid\u223c p and Y := (Y1, . . . , Ym) iid\u223c q as tuples of data points with sizes n and m. We write the combined (ordered) sample as Z := (Z1, . . . , Zn+m) = (X,Y ) = (X1, . . . , Xn, Y1, . . . , Ym).\nWe define the null hypothesis H0 as p = q and the alternative hypothesis H1 as p \u0338= q, usually with a requirement D(p, q) > \u03f5 for a distance D (such as the MMD) and some \u03f5 > 0. A hypothesis test \u2206 is a {0, 1}-valued function of Z, which rejects the null hypothesis if \u2206(Z) = 1 and fails to reject it otherwise. It will usually be formulated to control the probability of a type I error at some level \u03b1 \u2208 (0, 1), so that Pp\u00d7p(\u2206(Z) = 1) \u2264 \u03b1, while simultaneously minimising the probability of a type II error, Pp\u00d7q(\u2206(Z) = 0). In the above we have used the notation Pp\u00d7p and Pp\u00d7q to indicate that the sample Z is either drawn from the null, p = q, or the alternative p \u0338= q. Similar notation will be used for expectations and variances. When a bound \u03b2 \u2208 (0, 1) on the probability of a type II error is given (which may depend on the precise formulation of the alternative), we say the test has power 1\u2212 \u03b2. Maximum Mean Discrepancy. The Maximum Mean Discrepancy (MMD) is a kernel-based measure of distance between two distributions p and q, which is often used for two-sample testing. The MMD quantifies the dissimilarity between these distributions by comparing their mean embeddings in a reproducing kernel Hilbert space (RKHS; Aronszajn, 1950) with kernel k(\u00b7, \u00b7). Formally, if Hk is the RKHS associated with kernel function k, the MMD between distributions p and q is the integral probability metric defined by:\nMMDk(p, q) := sup f\u2208Hk:\u2225f\u2225Hk\u22641\n(EX\u223cp[f(X)]\u2212 EY\u223cq[f(Y )]) .\nThe minimum variance unbiased estimate of MMD2k, is given by the sum of two U-statistics and a sample average as:2\nM\u0302MD2k(Z) := 1 n(n\u2212 1) \u2211\n(i,i\u2032)\u2208[n]2\nk(Xi, Xi\u2032) + 1 m(m\u2212 1) \u2211\n(j,j\u2032)\u2208[m]2\nk(Yj , Yj\u2032)\u2212 2\nmn n\u2211 i=1 m\u2211 j=1 k(Xi, Yj),\nwhere we introduced the notation [n]2 = {(i, i\u2032) \u2208 [n]2 : i \u0338= i\u2032} for the set of all pairs of distinct indices in [n] = {1, . . . , n}. Tests based on the MMD usually reject the null when M\u0302MD2k exceeds some critical threshold, with the resulting power being greatly affected by the kernel choice. For characteristic kernel functions (Sriperumbudur et al., 2011), it can be shown that MMDk(p, q) = 0 if and only if p = q, leading to consistency results. However, on finite sample sizes, convergence rates of MMD estimates typically have strong dependence on the data dimension, so there are settings in which kernels ignoring redundant or unimportant features (i.e. non-characteristic kernels) will give higher test power in practice than characteristic kernels (which can over-weight redundant features).\nDistributions Over Kernels. In our test statistic, we will consider the case where the kernel k \u2208 K is drawn from a distribution \u03c1 \u2208 M1+(K) (with the latter notation denoting a probability measure; c.f. Benton et al., 2019 in the Gaussian Process literature). This distribution will be adaptively chosen based on the data subject to a regularisation term based on a \u201cprior\u201d \u03c0 \u2208 M1+(K) and defined through the Kullback-Liebler divergence: KL(\u03c1\u2225\u03c0) := E\u03c1[log(d\u03c1/d\u03c0)] for \u03c1 \u226a \u03c0 and KL(\u03c1\u2225\u03c0) := \u221e otherwise. When constructing these statistics the Donsker-Varadhan equality (Donsker and Varadhan, 1975), holding for any measurable g : K \u2192 R, will be useful:\nsup \u03c1\u2208M1+(A)\nE\u03c1[g]\u2212KL(\u03c1\u2225\u03c0) = logE\u03c0[exp \u25e6g]. (1)\nThis can be further related to the notion of soft maxima. If \u03c0 is a uniform distribution on finite K with |K| = r, then KL(\u03c1\u2225\u03c0) \u2264 log(r). Setting g = tf for some t > 0, Equation (1) relaxes to\nmax k f(k)\u2212 log(r) t \u2264 1 t log\n( 1\nr \u2211 k etf(k)\n) \u2264 max\nk f(k), (2)\n2Kim et al. (2022) note that M\u0302MD2k can equivalently be written as a two-sample U-statistic with kernel hk(x, x \u2032; y, y\u2032) = k(x, x\u2032) + k(y, y\u2032)\u2212 k(x, y\u2032)\u2212 k(x\u2032, y), which is useful for analysis.\nwhich approximates the maximum with error controlled by t. Our approach in considering these soft maxima is reminiscent of the PAC-Bayesian (McAllester, 1998; Seeger et al., 2001; Maurer, 2004; Catoni, 2007; see Guedj, 2019 or Alquier, 2021 for a survey) approach to capacity control and generalisation. This framework has raised particular attention recently as it has been used to provide the only non-vacuous generalisation bounds for deep neural networks (Dziugaite and Roy, 2017, 2018; Dziugaite et al., 2021; Zhou et al., 2019; Perez-Ortiz et al., 2021; Biggs and Guedj, 2021, 2022a); it has also been fruitfully applied to other varied problems from ensemble methods (Lacasse et al., 2006, 2010; Masegosa et al., 2020; Wu et al., 2021; Zantedeschi et al., 2021; Biggs and Guedj, 2022b; Biggs et al., 2022) to online learning (Haddouche and Guedj, 2022). Our proofs draw on techniques in that literature for U-Statistics (Lever et al., 2013) and martingales (Seldin et al., 2012; Biggs and Guedj, 2023; Haddouche and Guedj, 2023; Chugg et al., 2023). By considering these soft maxima, we gain a major advantage over the standard approaches, as we can obtain concentration and power results for our statistics without incurring Bonferroni-type multiple testing penalties.\nPermutation Tests. The tests we discuss in this paper use permutations of the data Z to approximate the null distribution. We begin our discussion in a fairly general form to include other close settings such as independence testing (Albert et al., 2022; Rindt et al., 2021) or wild bootstrap-based twosample tests (Fromont et al., 2012, 2013; Chwialkowski et al., 2014; Schrab et al., 2023). Let G be a group of transformations on Z; in our setting G = Sn+m, denoting the permutation (or symmetric) group of [N ] by SN and its elements by \u03c3 \u2208 SN , \u03c3 : [N ] \u2192 [N ]. We write gZ for the action of g \u2208 G on Z; e.g. defining \u03c3Z = (Z\u03c3(1), . . . , Z\u03c3(n+m)) for the group elements \u03c3 \u2208 Sn+m. We will suppose Z is invariant under the action of G when the sample is drawn from the null distribution, i.e. if the null is true than gZ =d Z (by which we notate equality of distribution) for all g \u2208 G. This is clearly the case for G = Sn+m in two-sample testing, since under the null Z = (Z1, . . . , Zm+n)\niid\u223c p with p = q, while under the alternative, the permuted sample \u03c3Z for randomised \u03c3 \u223c Uniform(Sn+m) simulates the null distribution. We can use permutations to construct an approximate cumulative distribution function (CDF) of our test statistic under the null, and choose an appropriate quantile of this CDF as our test threshold, which must be exceeded under the null with probability less that level \u03b1. For this we introduce a quantile operator (analogous to the max and min operators) for a finite set {f(a) \u2208 R : a \u2208 A}:\nquantile q, a\u2208A f(a) := inf { r \u2208 R : 1|A| \u2211 a\u2208A 1{f(a) \u2264 r} \u2265 q } . (3)\nVarious different results can be used to choose the threshold giving a correct level; we will here highlight a very general and easy-to-use theorem for constructing permutation tests, and in Section 3 will discuss previously unconsidered implications for using unsupervised feature extraction methods as a part of our tests. Although this result is not new, we believe that its usefulness has been underappreciated in the kernel testing community, and can be more conveniently applied than Romano and Wolf (2005, Lemma 1), which requires exchangeablity and is commonly used, e.g. in Albert et al. (2022, Proposition 1) and Schrab et al. (2023, Proposition 1). Theorem 1 (Hemerik and Goeman, 2018, Theorem 2.). Let G be a vector of elements from G, G = (g1, g2, . . . , gB+1), with gB+1 = id (the identity permutation) for any B \u2265 1. The elements g1, . . . , gB are drawn uniformly from G either i.i.d. or without replacement (which includes the possibility of G = G). If \u03c4(Z) is a statistic of Z and Z =d gZ for all g \u2208 G under the null then\nPp\u00d7p,G ( \u03c4(Z) > quantile\n1\u2212\u03b1,g\u2208G \u03c4(gZ)\n) \u2264 \u03b1.\nIn other words, if we compare \u03c4(Z) with the empirical quantile of \u03c4(gZ) as a test threshold, the type I error rate is no more than \u03b1.3 The potentially complex task of constructing a permutation test reduces to the trivial task of choosing the statistic \u03c4(Z). This result is also true for randomised permutations of any number B \u2265 1 without approximation, so an exact and computationally efficient test can be straightforwardly constructed this way.\nWe finally mention the related approach of the MMD Aggregated test (MMDAgg; Schrab et al., 2023), which combines multiple MMD-based permutation tests with different kernels, and rejects if\n3This test is near-exact. Specifically, if the statistic realisations are distinct (\u03c4(gZ) \u0338= \u03c4(g\u2032Z) for g \u0338= g\u2032) as is common for continuous data, the level is exactly \u230a(B + 1)\u03b1\u230b/(B + 1).\nany of these reject, using distinct quantiles for each kernel. To ensure the overall aggregated test is well-calibrated, these quantiles must be adjusted using a second level of permutations. This incurs additional computational cost, a pitfall avoided by our fused single statistic.\nFaster Sub-Optimal Tests. While the main focus of this paper revolves around the kernel selection problem for optimal, quadratic MMD testing, we also highlight the existence of a rich literature on efficient kernel tests which run in linear (or near-linear) time. These speed improvements are achieved using various tools such as: incomplete U-statistics (Gretton et al., 2012b; Yamada et al., 2019; Lim et al., 2020; K\u00fcbler et al., 2020; Schrab et al., 2022b), block U-statistics (Zaremba et al., 2013; Deka and Sutherland, 2022), eigenspectrum approximations (Gretton et al., 2009), Nystr\u00f6m approximations (Cherfaoui et al., 2022), random Fourier features (Zhao and Meng, 2015), analytic representations (Chwialkowski et al., 2015; Jitkrittum et al., 2016), deep linear kernels (Kirchler et al., 2020), kernel thinning (Dwivedi and Mackey, 2021; Domingo-Enrich et al., 2023), etc.\nThe efficiency of these tests usually entail weaker theoretical power guarantees compared to their quadratic-time counterparts, which are minimax optimal4 (Kim et al., 2022; Li and Yuan, 2019; Fromont et al., 2013; Schrab et al., 2023; Chatterjee and Bhattacharya, 2023). These optimal quadratic tests are either permutation-based non-asymptotic tests (Kim et al., 2022; Schrab et al., 2023) or studentised asymptotic tests (Kim and Ramdas, 2023; Shekhar et al., 2022; Li and Yuan, 2019; Gao and Shao, 2022; Florian et al., 2023). We emphasise that the parameters of any of these permutationbased two-sample tests can be chosen in the unsupervised way we outline in Section 3. We choose to focus in this work on optimal quadratic-time results, but note that our general approach could be extended to sub-optimal faster tests as well."
        },
        {
            "heading": "3 Learning Statistics for Permutation Tests",
            "text": "Here we discuss how we can improve permutation-based tests by learning parameters for them in an unsupervised manner. The reason that we highlighted Theorem 1 in the previous section is that it holds for any \u03c4 . For example, we could use the MMD with a kernel chosen based on the data, \u03c4(Z) = M\u0302MD2k=k(Z)(Z); however, for each permutation \u03c3 we would need to re-compute\n\u03c4(\u03c3Z) = M\u0302MD2k=k(\u03c3Z)(\u03c3Z), so the kernel being used would be different for each permutation. This has two major disadvantages: firstly, it might be computationally expensive to re-compute k for each permutation, especially for a deep kernel5. Secondly, the scale of the resulting MMD could be dramatically different for each permutation, so the empirical quantile might not lead to a powerful test. This second problem is related to the problem of combining multiple different MMD values into a single test which our MMD-FUSE statistics are designed to combat (Section 4; c.f. also MMDAgg, Schrab et al., 2023).\nOur Proposal. In two-sample testing we propose to use a statistic \u03c4\u03b8 parameterised by some \u03b8, where \u03b8 is fixed for all permutations, but depends on the data in an unsupervised or permutation-invariant way. Specifically, we allow such a parameter to depend on \u27e8Z\u27e9 := {Z1, . . . , Zn+m}, the unordered combined sample. Since our tests will not depend on the internal ordering of X and Y (which are assumed i.i.d. under both hypotheses), the only additional information contained in Z over \u27e8Z\u27e9 is the label assigning Zi to its initial sample. This is justified since \u27e8Z\u27e9 = \u27e8\u03c3Z\u27e9 for all \u03c3 \u2208 Sn+m, so setting \u03c4(Z) = \u03c4\u03b8(\u27e8Z\u27e9)(Z) gives a fixed \u03b8 and statistic for all permutations to use in Theorem 1. The information in \u27e8Z\u27e9 can be used to fine-tune test parameters for any test fitting this setup. This solves both the computation and scaling issues mentioned above.\nThe above provides a first formal justification for the use of the median heuristic in Gretton et al. (2012a), since it is a permutation-invariant function of the data. However, a far richer set of possibilities are available even when restricting ourselves to these permutation-free functions of Z. For example, we can use any unsupervised or self-supervised learning method to learn representations to use as the input to an MMD-based test statistic, while paying no cost in terms of calibration and needing to train such methods only once. Given the wide variety of methods dedicated to feature extraction and dimensionality reduction, this opens up a huge range of possibilities for the design of new and principled two-sample tests. The simplicity and generality of our proposal might lead one to\n4With the exception of the near-linear test of Domingo-Enrich et al. (2023) which achieves the same MMD separation rate as the quadratic test but under stronger assumptions on the data distributions.\n5A possibility envisaged by e.g. Liu et al. (2020) and dismissed due to the computational infeasability.\nexpect that this idea has been used before, but it has not to our best knowledge, underlined by the fact that the median heuristic has been widely used without such justification when one follows from this method. This potentially powerful and widely-applicable possibility represents one of our most practical contributions."
        },
        {
            "heading": "4 MMD-FUSE: Fusing U-Statistics by Exponentiation",
            "text": "Say we have computed several MMD values M\u0302MD2k under different kernels k \u2208 K. How might we combine these? One possibility is to perform multiple testing as in the case of MMDAgg. An even simpler approach would simply take the maximum of those values maxk M\u0302MD2k since Theorem 1 shows that this will not prevent us from controlling the level of our test by \u03b1; note though that for each permutation we would take this maximum separately. Indeed, C\u00e1rcamo et al. (2022) show that for certain kernel choices the supremum of the MMD values with respect to the bandwidth is a valid integral probability metric (M\u00fcller, 1997). There are two main problems with this approach: a capacity control issue and a normalisation issue.\nFirstly, if the class over which we are maximising is sufficiently rich (for example a complex neural network), then the maximum may be able to effectively memorise the entire sample for each possible permutation, saturating the statistic for every permutation and limiting test power. Any convergence results would need to hold simultaneously for every k in K, and so power results suffer: for finite |K| we would incur a sub-optimal Bonferroni correction (see Section 5); while for infinite classes, results would need to involve capacity control quantities like Rademacher complexity. Further, only information from a single maximising \u201cbase\u201d kernel can be considered at a time.\nTherefore, in both our statistics, we prefer a \u201csoft\u201d maximum, which considers information from every kernel simultaneously and when more than one of the kernels is well-suited (Section 5) it therefore avoids the Bonferroni correction arising from uniform bounds. From Equation (1), our approach is equivalent to using a KL complexity penalty, and is strongly reminiscent of PAC-Bayesian (Section 2) capacity control. We note that other soft maxima could be considered, but our choice makes obtaining exponential concentration inequalities relatively easy (Appendix C), and the dual formulation (Equation (2)) allows us to derive power results in terms of MMD directly.\nThe second issue is that the MMD estimates might have different scales or variances per kernel which need to be accounted for. In order to be able to meaningfully compare MMD values between each other, these need to be normalised somehow before taking a maximum. We use the common approach of dividing through by a variance-like term (as in \u201cstudentised\u201d tests, see Section 2). The specific normaliser is permutation invariant and gives our statistic tight sub-Gaussian null concentration (for well-chosen regularisation parameter \u03bb; see Appendix C).\nBased on the above we introduce the FUSE-1 statistic which uses a log-sum-exp soft maximum, and the FUSE-N which combines this with normalisation. Both statistics use a \u201cprior\u201d distribution on K, denoted \u03c0(\u27e8Z\u27e9), which is either fixed independently of the data, or is a function of the data which is invariant under permutation (as discussed in Section 3).\nDefinition 1. We define the un-normalised (subscript 1 for normaliser of 1) and normalised (subscript N) test statistics with parameter \u03bb > 0, respectively, as\nF\u0302USE1(Z) := 1 \u03bb log ( Ek\u223c\u03c0(\u27e8Z\u27e9) [ exp ( \u03bbM\u0302MD2k(X,Y ) )]) ,\nF\u0302USEN (Z) := 1\n\u03bb log\nEk\u223c\u03c0(\u27e8Z\u27e9) exp \u03bbM\u0302MD2k(X,Y )\u221a N\u0302k(Z)  , where N\u0302k(Z) := 1n(n\u22121) \u2211 (i,j)\u2208[n+m]2 k(Zi, Zj) 2 is permutation invariant.\nAlthough these statistics appear complex, we note that in the case where \u03c0 has finite support, their calculation reduces to a log-sum-exp of MMD estimates normalised by N\u0302k. This is even clearer when \u03c0 is also uniform on its support, as we consider experimentally; then Equation (2) shows that our statistics reduce to soft maxima, with \u03bb controlling the smoothness or \u201ctemperature\u201d.\nFrom this, we define the FUSE-1 test (with the FUSE-N test \u2206FUSEN defined analogously) as\n\u2206FUSE1(Z) := 1 { F\u0302USE1(Z) > quantile\n1\u2212\u03b1,\u03c3\u2208S F\u0302USE1(\u03c3Z)\n} (4)\nfor sampled set S of permutations as described above. It compares the test statistic F\u0302USE1 with its quantiles under permutation, and rejects if the overall value exceeds a quantile controlled by \u03b1. Note that since N\u0302k(Z) is permutation invariant, it only needs to be calculated once per kernel in \u2206FUSEN (and not separately for each permutation) as per Section 3. See Appendix A.5 for its time complexity.\nComparison with MMDAgg. MMDAgg is a different way to think about combining multiple kernels and MMD values, but it relies on a framework based on multiple testing. This can be problematic in the case where the number of kernels considered is large, since as this number increases in the multiple testing approach MMDAgg, the level needs to be corrected differently, so its theretical power behaviour becomes unclear (though empirically MMDAgg retains its power in this setting). By contrast, our proposed FUSE methods avoid multiple testing as a single statistic and quantile are used, avoiding such issues. The FUSE statistics are even defined in the limit of an infinite number of kernels (continuous/uncountable collection of kernels) by considering distributions on the space of kernels, which is not the case for MMDAgg. Moreover, while the MMDAgg approach is only useful for hypothesis testing, having a quantity like FUSE combining multiple kernel-based measures of distance with exponential concentration bounds could be of interest in a wider range of applications.\nFUSE-1 and the Mean Kernel. Although the un-normalised test has worse performance in practice than our normalised test, the FUSE-1 statistic is interesting in its own right because of various theoretical properties, as we discuss below. Firstly, we introduce the mean kernel K\u03c1(x, y) = Ek\u223c\u03c1k(x, y) under a \u201cposterior\u201d \u03c1 \u2208 M1+(K), which is indeed a reproducing kernel in its own right. In the finite case, this is simply a weighted sum of \u201cbase\u201d kernels.\nNote that the linearity of M\u0302MD2k and MMD 2 k in the kernel k implies that Ek\u223c\u03c1 MMD 2 k(p, q) = MMD2K\u03c1(p, q), and similarly for M\u0302MD 2 k, with these terms appearing in our power results (Section 5).\nCombining this linearity with the dual formulation of F\u0302USE1 via Equation (1) gives\nF\u0302USE1(Z) = sup \u03c1\u2208M1+(K)\nM\u0302MD2K\u03c1(X,Y )\u2212 KL(\u03c1, \u03c0)\n\u03bb .\nThis re-states F\u0302USE1 in terms of \u201cposterior\u201d \u03c1, and makes the interpretation of our statistic as a KL-regularised kernel-learning method clear. In the finite case, our test simply optimises the weightings of the different kernels in a constrained way. We note that for certain infinite kernel sets and choices of prior it is be possible to express the mean kernel in closed form. This happens because, e.g. the expectation of a Gaussian kernel with respect to a Gamma prior over the bandwidth is simply a (closed form) rational quadratic kernel. We discuss this point further in Appendix B."
        },
        {
            "heading": "5 Theoretical Power of MMD-FUSE",
            "text": "In this section we outline possible sufficient conditions for our tests to obtain power at least 1\u2212 \u03b2 at given level \u03b1. The conditions will depend on a fixed data-independent \u201cprior\u201d \u03c0 \u2208 M1+(K) and thus hold even without unsupervised parameter optimisation. They are stated as requirements for the existence of a \u201cposterior\u201d \u03c1 \u2208 M1+(K) with corresponding mean kernel K\u03c1 as defined in Section 4. In the finite case, K\u03c1 is simply a weighted sum of kernels, so these requirements are also satisfied under the same conditions for any single kernel, corresponding to the case where the posterior puts all its weight on a single kernel.\nTechnically, these results require that there is some constant \u03ba upper bounding all of the kernels, and that n and m are within a constant multiple (i.e. n \u2264 m \u2264 cn for some c \u2265 1, notated n \u224d m). They hold when using randomised permutations provided B > c\u2032\u03b1\u22122 log(\u03b2\u22121) for small constant c\u2032 > 0. Theorem 2 (FUSE-1 Power). Fix prior \u03c0 independently of the data. For the un-normalised test FUSE-1 with \u03bb \u224d n/\u03ba and n \u224d m, there exists a universal constant C > 0 such that\n\u2203\u03c1 \u2208 M1+(K) : MMD2K\u03c1(p, q) > C\u03ba\nn\n( 1\n\u03b2 + log\n1 \u03b1 +KL(\u03c1, \u03c0)\n) .\nis sufficient for FUSE-1 to achieve power at least 1\u2212 \u03b2 at level \u03b1.\nA similar result is obtained for FUSE-N under an assumption that the normalising term is well behaved (see assumption in Theorem 3). This requirement will be satisfied for kernels (including most common ones) that tend to zero only in the limit of the data being infinitely far apart. Theorem 3 (FUSE-N Power). Fix prior \u03c0 independently of the data such that for all k \u2208 supp(\u03c0) the expectation EZ\u223cp\u00d7q[N\u0302k(Z)\u22121] < c/\u03ba is bounded for some c < \u221e. For the normalised test FUSE-N with \u03bb \u224d n \u224d m, there exists a universal constant C > 0 such that\n\u2203\u03c1 \u2208 M1+(K) : MMD2K\u03c1(p, q) > C\u03ba\nn\n( 1\n\u03b22 + log\n1 \u03b1 +KL(\u03c1, \u03c0)\n) .\nis sufficient for FUSE-N to achieve power at least 1\u2212 \u03b2 at level \u03b1.\nDiscussion. The conditions in Theorems 2 and 3 give the optimal MMD2 separation rate in n (Domingo-Enrich et al., 2023, Proposition 2). These results also imply consistency if the prior \u03c0 assigns non-zero weight to characteristic kernels.6 Applying these results to uniform priors supported on r points, the KL penalty can be upper bounded as KL(\u03c1, \u03c0) \u2264 log(r). Thus in the worst case, where only a single kernel achieves large MMD2k(p, q), the price paid for adaptivity is only log(r). In many cases, most of the kernels will give large MMD2k(p, q). The posterior will then mirror the prior, and this KL penalty will be even smaller. Thus very large numbers of kernels could be considered, and if all give large MMD values the power would not be greatly affected.\nAdditional Technical Results. Aside from the presentation of our new statistics and tests, we make a number of technical contributions on the way to proving Theorems 2 and 3, as well as proving some additional results. In particular, we give exponential concentration bounds for our statistics under permutations and the null, which do not require bounded kernels. This refined analysis requires the construction of a coupled Rademacher chaos and concentration thereof. We obtain intermediate results using variances from the proofs of Theorems 2 and 3 that could be used in future work to obtain power guarantees under alternative assumptions such as Sobolev spaces (Schrab et al., 2023). Finally, we prove exponential concentration for F\u0302USE1 under the alternative and bounded kernels, requiring the proof of a \u201cPAC-Bayesian\u201d bounded differences-type concentration inequality. See the appendix for a more detailed overview."
        },
        {
            "heading": "6 Experiments",
            "text": "We compare the test power of MMD-FUSE-N (\u03bb = \u221a n(n\u2212 1)) against various MMD-based kernel selective tests (see Section 1 for details) using: the median heuristic (MMD-Median; Gretton et al., 2012a), data splitting (MMD-Split; Sutherland et al., 2017), analytic Mean Embeddings and Smooth Characteristic Functions (ME & SCF; Jitkrittum et al., 2016), the MMD Deep kernel (MMD-D; Liu et al., 2020), Automated Machine Learning (AutoML; K\u00fcbler et al., 2022b), kernel thinning to (Aggregate) Compress Then Test (CTT & ACTT; Domingo-Enrich et al., 2023), and MMD Aggregated (Incomplete) tests (MMDAgg & MMDAggInc; Schrab et al., 2023, 2022b). Additional details and code link for experimental reproducibility are provided in Appendix A.\nDistribution on Kernels. We choose our kernel prior distribution \u03c0 as uniform over a collection of Gaussian, kg\u03b3(x, y) = exp ( \u2212\u2225x\u2212y\u222522 / 2\u03b32 ) , and Laplace, k\u2113\u03b3(x, y) = exp ( \u2212 \u221a 2\u2225x\u2212y\u22251 / \u03b3 ) , kernels with various bandwidths \u03b3 > 0. These bandwidths are chosen as the uniform discretisation of the interval between half the 5% and twice the 95% quantiles (for robustness) of {\u2225z\u2212 z\u2032\u2225r : z, z\u2032 \u2208 Z}, with r \u2208 {1, 2}, respectively. This choice is similar to that of Schrab et al. (2023, Section 5.2), who empirically show that ten points for the discretisation is sufficient (Schrab et al., 2023, Figure 6), which we verify in Appendix A.4. This set of distances is permutation-invariant, so Theorem 1 guarantees a well-calibrated test even though the kernels are data-dependent.\nMixture of Gaussians. Our first experiments (Figure 1) consider multimodal distributions p and q, each a 2-dimensional mixture of four Gaussians with means (\u00b1\u00b5,\u00b1\u00b5) with \u00b5 = 20 and diagonal covariances. For p, the four components all have unit variances, while for q we vary the standard deviation \u03c3 of one of the Gaussians, \u03c3 = 1 corresponds to the null hypothesis p = q. Intuitively, an appropriate kernel bandwidth to distinguish p from q would correspond to that separating Gaussians\n6Under this condition KL(\u03bd, \u03c0)<\u221e with \u03bd the restriction of \u03c0 to characteristic kernels. K\u03bd is characteristic so MMDK\u03bd (p, q)>0 \u21d0\u21d2 p \u0338= q, and our condition lower bound tends to zero with n \u2192 \u221e.\nwith standard deviations 1 and \u03c3. This is significantly smaller than the median bandwidth which scales with the distance \u00b5 between modes.\nTable 1: Test power for detecting the difference between CIFAR-10 and CIFAR-10.1 images with test level \u03b1 = 0.05. The averaged numbers of rejections over 1000 repetitions are reported.\nTests Power\nMMD-FUSE 0.937 MMDAgg 0.883 MMD-D 0.744\nCTT 0.711 MMD-Median 0.678\nACTT 0.652 ME 0.588\nAutoML 0.544 C2ST-L 0.529 C2ST-S 0.452 MMD-O 0.316\nMMDAggInc 0.281 SCF 0.171\nPerturbed Uniform. In Figure 1, we report test power for detecting perturbations on uniform distributions in one and two dimensions. We vary the amplitude a of two perturbations from a = 0 (null) to a = 1 (maximum value for the density to remain non-negative). A similar benchmark was first proposed by Schrab et al. (2023, Section 5.5) and considered in several other works (Schrab et al., 2022b; Hagrass et al., 2022; Chatterjee and Bhattacharya, 2023). Different bandwidths are required to detect different amplitudes of the perturbations.\nGalaxy MNIST. We examine performance on real-world data in Figure 1, through galaxy images (Walmsley et al., 2022) in dimension d = 3\u00d764\u00d764 = 12288 captured by a groundbased telescope. These consist of four classes: \u2018smooth and cigar-shaped\u2019, \u2018edge-on-disk\u2019, \u2018unbarred spiral\u2019, and \u2018smooth and round\u2019. One distribution uniformly samples images from the first three categories, while the other does the same with probability 1\u2212 c and uniformly samples a \u2018smooth and round\u2019 galaxy image with probability of corruption c \u2208 [0, 1]. The null hypothesis corresponds to the case c = 0.\nCIFAR 10 vs 10.1. The aim of this experiment is to detect the difference between images from the CIFAR-10 (Krizhevsky, 2009) and CIFAR-10.1 (Recht et al., 2019) test sets. This is a challenging problem as CIFAR-10.1 was specifically created to consist of new samples from the CIFAR-10 distribution so that it can be used as an alternative test set for models trained on CIFAR-10. Samples from the two distributions are presented in Figure 6 in Appendix A.3 (Liu et al., 2020, Figure 5). This benchmark was proposed by Liu et al. (2020, Table 3) who introduced the deep MMD test MMD-D and the MMD-Split test (here referred to as MMD-O to point out that their implementation has been used rather than ours). They also compare to ME and SCF, as well as to C2ST-L and C2ST-S (Lopez-Paz and Oquab, 2017) which correspond to Classifier Two-Sample Tests based on Sign or\nLinear kernels. For the tests slitting the data, 1000 images from both datasets are used for parameter selection and/or model training, and 1021 other images from each distributions are used for testing. Consequently, tests avoiding data splitting are given the full 2021 images from CIFAR-10.1 and 2021 images sampled from CIFAR-10.\nExperimental Results of Figure 1. We observe similar trends in all eight experiments in Figure 1: MMD-FUSE matches the power of state-of-the-art MMDAgg while being computationally faster (both theoretically and practically). These two tests consistently obtain the highest power in every experiment, except when increasing the number of Galaxy MNIST images where MMD-D obtains higher power. However, we observe in Table 1 that MMD-FUSE outperforms MMD-D on another image data problem, also with large sample size. On synthetic data, the deep kernel test MMD-D surprisingly only obtains power similar to MMD-Split in most experiments (even lower in the 2- dimensional perturbed uniform experiment). The two near-linear aggregated variants ACTT and MMDAggInc trade-off a small portion of test power for computational efficiency, with the former outperforming the latter for large sample sizes. The importance of kernel selection is emphasised by the fact that the two tests using the median bandwidth (MMD-Median and CTT) achieve very low power. While the linear-time SCF test, based in the frequency domain, attains high power in the Mixture of Gaussians experiments, it has low power in the three other experiments. Its spatial domain variant ME performs better on Perturbed Uniform d \u2208 {1, 2} experiments but in general still has reduced power compared to both linear and quadratic time alternatives. Finally, the AutoML test performs well for fixed sample size m = n = 500 (first row of Figure 1), but its power compared to other tests considerably deteriorates as the sample size increases (second row of Figure 1). Overall, MMD-FUSE achieves state-of-the-art performance across all experiments on both low-dimensional synthetic data and high-dimensional real-world data.\nExperimental Results of Table 1. We report in Table 1 the power achieved by each test on the CIFAR 10 vs 10.1 experiment, which is averaged over 1000 repetitions. We observe that MMD-FUSE performs the best and obtains power 0.937, which means that out of 1000 repetitions, it was 937 times able to distinguish between samples from CIFAR-10 and from CIFAR-10.1. This demonstrates that the images in CIFAR-10.1 do not come from the same distribution as those in CIFAR-10.\nExperimental Results of Figure 7. We observe in Figure 7 of Appendix A.4 that MMD-FUSE can achieve higher power than MMDAgg in some additional perturbed uniform experiment. We also note that using a relatively small number of kernels (e.g., 10) is enough to capture all the required information, and that the test power is retained when further increasing the number of kernels."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this work, we propose MMD-FUSE, an MMD-based test which fuses kernels through a soft maximum and a method for learning general two-sample testing parameters in an unsupervised fashion. We demonstrate the empirical performance of MMD-FUSE and show that it achieves the optimal MMD separation rate guaranteeing high test power. This optimality holds with respect to the sample size and likely also for the logarithmic dependence in \u03b1, but we believe the dependence on \u03b2 could be improved in future work; a general question is whether lower bounds in terms of \u03b1 and \u03b2 can be proved. Obtaining separation rates in terms of the L2-norm between the densities (Schrab et al., 2023) may also be possible but challenging since this distance is independent of the kernel.\nAn open question is in explaining the significant empirical power advantage of the normalised test over its un-normalised variant, which is currently not reflected in the derived rates. The importance of this normalisation is clear when considering kernels with different bandwidths, leading to vastly different scaling in the un-normalised permutation distributions. Work here could begin with finitesample concentration guarantees for our normalised statistic or other \u201cstudentised\u201d variants, some of which might obtain better performance.\nFuture work could also examine computationally efficient variants of MMD-FUSE, by either relying on incomplete U -statistics (Schrab et al., 2022b) and leading to suboptimal rates, or by relying on recent ideas of kernel thinning (Domingo-Enrich et al., 2023) which can lead to the same optimal rate under stronger assumptions on the data distributions, or by considering the permutation-free approach of Shekhar et al. (2022). Finally, our two-sample MMD fusing approach could be extended to the HSIC independence framework (Gretton et al., 2005, 2008; Albert et al., 2022) and to the KSD goodness-of-fit setting (Chwialkowski et al., 2016; Liu et al., 2016; Schrab et al., 2022a)."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers and area chair for their thorough reading of our work, for their constructive feedback, and for engaging in the discussions, all of which have helped to improve the paper. Felix Biggs and Antonin Schrab both gratefully acknowledge the support from the U.K. Research and Innovation under the EPSRC grant EP/S021566/1. Arthur Gretton acknowledges support from the Gatsby Charitable Foundation."
        },
        {
            "heading": "Overview of Appendices",
            "text": "Appendix A We give further experimental details, discuss time complexity of our results while graphing run-times.\nAppendix B We discuss how F\u0302USE1 can be expressed in a simple form for certain uncountable priors.\nAppendix C We prove exponential concentration bounds for both our statistics under the null and permutations.\nAppendix D We prove exponential concentration bounds for the F\u0302USE1 statistic under the alternative hypothesis.\nAppendix E We prove the power results stated in Section 5 and other interesting intermediate results."
        },
        {
            "heading": "A Additional Experimental Details",
            "text": ""
        },
        {
            "heading": "A.1 Code and licenses",
            "text": "Our implementation of MMD-FUSE in Jax (Bradbury et al., 2018), as well as the code for the reproducibility of the experiments, are made publicly available at:\nhttps://github.com/antoninschrab/mmdfuse-paper\nOur code is under the MIT License, and we also implement ourselves the MMD-Median (Gretton et al., 2012a) and MMD-Split (Sutherland et al., 2017) tests. For ME & SCF (Jitkrittum et al., 2016), MMD-D (Liu et al., 2020), AutoML (K\u00fcbler et al., 2022b), CTT & ACTT (Domingo-Enrich et al., 2023), MMDAgg & MMDAggInc (Schrab et al., 2023, 2022b), we use the implementations of the respective authors, which are all under the MIT license.\nThe experiments were run on an AMD Ryzen Threadripper 3960X 24 Cores 128Gb RAM CPU at 3.8GHz and on an NVIDIA RTX A5000 24Gb Graphics Card, with a compute time of a couple of hours."
        },
        {
            "heading": "A.2 Test parameters",
            "text": "In general, we use the default test parameters recommended by the authors of the tests (listed above). For the ME and SCF tests, ten test locations are chosen on half of the data. AutoML is run with the recommended training time limit of one minute.\nKernels. As explained in Section 6 (\u00a7Distribution on Kernels), for MMD-Fuse, we use Gaussian and Laplace kernels with bandwidths in {qr5% + i(qr95% \u2212 qr5%)/9 : i = 0, . . . , 9} where qr5% is half the 5% quantile of all the inter-sample distances {\u2225z \u2212 z\u2032\u2225r : z, z\u2032 \u2208 Z} with r = 1 and r = 2 for Laplace and Gaussian kernels, respectively. Similarly, qr95% is twice the 95% quantile.\nMMD-Split selects a Gaussian kernel on half of the data with bandwidth in {qr5%+i(qr95%\u2212qr5%)/99 : i = 0, . . . , 99} by maximizing a proxy for asymptotic power which is the ratio of the estimated MMD with its estimated standard deviation under the alternative (Liu et al., 2020, Equation 3). The MMD test is then run on the other half with the selected kernel.\nCTT and MMD-Median both use a Gaussian kernel with bandwidth the median of {\u2225z \u2212 z\u2032\u22252 : z, z\u2032 \u2208 Z}, while ACTT is run with Gaussian kernels with bandwidths in {q25% + i(q295% \u2212 q25%)/9 : i = 0, . . . , 9}. The MMDAgg and MMDAggInc tests are run with their default implementations, which similarly use collections of 20 kernels split equally between Gaussian and Laplace kernels with ten bandwidths each, but they use a different (non-uniform) discretisation of the intervals [qr95%, q r 5%].\nPermutations. For MMD-Median, MMD-Split, and MMD-FUSE, we use 2000 permutations to estimate the quantiles. MMDAgg and MMDAggInc use 2000 + 2000 and 500 + 500 permutations, respectively, to approximate the quantiles and the multiple testing correction. The CTT and ACTT tests are run with 39 and 299 + 200 permutations, respectively. AutoML uses 10000 permutations\nand MMD-D 100 of them. The ME and SCF tests use asymptotic quantiles. We recall that using a higher number of permutations for the quantile does not necessarily lead to a more powerful test (Rindt et al., 2021, Section 7)."
        },
        {
            "heading": "A.3 Details on the experiments of Section 6",
            "text": "In this section, we present figures illustrating the four experimental settings described in Section 6 with results in Figure 1: Mixture of Gaussians (Figure 2), Perturbed Uniform d = 1 (Figure 3), Perturbed Uniform d = 2 (Figure 4), Galaxy MNIST (Figure 5), and CIFAR 10 vs 10.1 (Figure 6).\nA.4 Experiment varying the number of kernels\nIn Figure 7, we provide an additional perturbed uniform experiment where we vary the number of kernels for MMD-FUSE and for MMDAgg. We consider the task of detecting six one-dimensional perturbations with amplitude a = 0.5 for sample sizes m = n = 500 while varying the number of Gaussian and Laplace kernels from 10 to 1000.\nFirstly, we observe that MMD-FUSE achieves higher power than MMDAgg in this setting. Secondly, both MMD-FUSE and MMDAgg retain their power when increasing the number of bandwidths. This matches the empirical observation of Schrab et al. (2023, Section 5.7) that a discretisation of 10 bandwidths is enough to capture all the information in certain two-sample problems and that using a finer discretisation (which is computationally more expensive) does not improve the test power."
        },
        {
            "heading": "A.5 Time complexity and runtimes",
            "text": "The time complexity of MMD-FUSE is O ( KB ( m2 + n2 )) (5)\nwhere m and n are the sizes of the two samples, K is the number of kernels fused, and B is the number of permutations to estimate the quantile. Note that this is an improvement over the time complexity of MMDAgg (Schrab et al., 2023)\nO ( K ( B +B\u2032 )( m2 + n2 )) (6)\nwhere the extra parameterB\u2032 corresponds to the number of permutations used to estimate the multiple testing correction, often set as B\u2032 = B in practice (Schrab et al., 2023, Section 5.2). We indeed observe in Figure 8 that MMD-FUSE runs twice as fast as MMDAgg.7\nWhile the runtimes of most tests should not depend on the type of data (only on the sample size and on the dimension), we note that the runtimes of tests relying on optimisation (e.g. ME & SCF) can be affected. In the experiment of Figure 8, we consider samples from multivariate Gaussians centred at zero with covariance matrices Id and \u03c3Id with \u03c3 = 1.1. We vary both the sample sizes and the dimensions.\n7The two constants hidden in the O-notations of Equations (5) and (6) are exactly the same, which is indeed verified by our empirical observations."
        },
        {
            "heading": "B Closed Form Mean Kernel",
            "text": "We have observed in the main text that in some cases where the prior has non-finite support, F\u0302USE1 still gives a straightforwardly expressed statistic. This happens because for certain kernel choices, the expectation of the kernel with respect to some parameter is still a closed form kernel. We give one example of such a statistic here.\nTheorem 4. For any fixed \u03b1 > 0, the following is an example of a F\u0302USE1 statistic:\nF\u0302USE rq\n1 (Z) = sup R>0 M\u0302MD2krq(\u03b1,\u221aR\u03b70) (Z)\u2212 \u03b1 \u00b7 logR+ 1/R\u2212 1 \u03bb\nwhere krq(\u03b1,\u03b7) = (1 + \u2225x \u2212 y\u22252/2\u03b72)\u2212\u03b1 is a rational quadratic kernel, and \u03b70 is the \u201cprior\u201d bandwidth (which can essentially be absorbed into the data scaling).\nProof of Theorem 4. Firstly, we define a general rational quadratic kernel\nkrq(\u03b1,\u03b7)(x, y) =\n( 1 +\n\u2225x\u2212 y\u222522 2\u03b72\n)\u2212\u03b1 .\nNote that kg(r) = e\u2212\u03c4r 2/2 with r = \u2225x\u2212 y\u22252 is a bounded kernel with parameter \u03c4 . If we take the expectation of \u03c4 with respect to a Gamma distribution,\nE\u03c4\u223c\u0393(\u03b1,\u03b2)e\u2212\u03c4r 2/2 =\n\u03b2\u03b1\n\u0393(\u03b1) \u222b \u221e \u2212\u221e \u03c4\u03b1\u22121 exp(\u2212\u03c4\u03b2) exp(\u2212\u03c4r2/2) d\u03c4\n= \u03b2\u03b1\n\u0393(\u03b1) \u0393(\u03b1)\n( \u03b2 + r2\n2 )\u2212\u03b1 = ( 1 + r2\n2\u03b2\n)\u2212\u03b1 = krq(\u03b1, \u221a \u03b2)(r)\nwhere \u0393 denotes the gamma function. The KL divergence between Gamma distributions is\nKL(\u0393(\u03b1, \u03b2),\u0393(\u03b10, \u03b20)) = (\u03b1\u2212 \u03b10)\u03c8(\u03b1) + log \u0393(\u03b10)\n\u0393(\u03b1) + \u03b10 logR+ \u03b1\n( 1 R \u2212 1 )\nwhere R = \u03b2/\u03b20 and \u03c8(\u03b1) := \u0393\u2032(\u03b1)/\u0393(\u03b1) denotes the digamma function. Using the dual form of F\u0302USE1 we find that\nF\u0302USE1 = sup \u03b1>0,\u03b2>0\nM\u0302MD2krq(\u03b1,\u221a\u03b2)(Z)\u2212 KL(\u0393(\u03b1, \u03b2),\u0393(\u03b10, \u03b20))\n\u03bb .\nRestricting \u03b1 = \u03b10, prior \u03b20 = \u03b720 and setting \u03b2 = R\u03b7 2 0 gives the result."
        },
        {
            "heading": "C Null and Permutation Concentration Results",
            "text": "In this section we prove the following concentration results under the null (or equivalently, under the permutation distribution). These results show that for appropriate choices of \u03bb, both statistics converge to zero at rate at least O(1/n) under the null or permutations. We recall that without loss of generality n \u2264 m. Theorem 5. For bounded kernels k \u2264 \u03ba <\u221e and parameter \u03bb, with probability at least 1\u2212 \u03b4 over a sample Z from the null,\nF\u0302USE1(Z) \u2264 4\u03ba2\u03bb n(n\u2212 1) + log 1\u03b4 \u03bb\nprovided 0 < \u03bb < \u221a n(n\u2212 1)/8 \u221a 2\u03ba, and\n\u2212F\u0302USE1(Z) \u2264 1 + 32 log 1\u03b4\u221a 2n(n\u2212 1) \u00b7 \u03ba 2 .\nFor potentially unbounded kernels and parameter \u03bb, with probability at least 1\u2212 \u03b4 over a sample Z from the null,\nF\u0302USEN (Z) \u2264 16\u03bb n(n\u2212 1) + log 1\u03b4 \u03bb ,\nprovided 0 < \u03bb < \u221a n(n\u2212 1)/16 \u221a 2, and\n\u2212F\u0302USEN (Z) \u2264 1 + 32 log 1\u03b4\u221a 2n(n\u2212 1) .\nThe above bounds are also valid for F\u0302USE1(\u03c3Z), F\u0302USEN (\u03c3Z) and any fixed Z (potentially nonnull) under permutation by \u03c3 \u223c Uniform(Sn+m).\nWhile the upper bounds depend critically upon our choice of \u03bb, the lower bounds instead hold regardless of \u03bb. Choosing \u03bb \u224d \u221a n(n\u2212 1) \u224d n gives the desired upper bound rates O(1/n)."
        },
        {
            "heading": "C.1 Sub-Gaussian Chaos Theorem",
            "text": "We provided constants for Theorem 5, even though they are not strictly necessary, as we hope they could be useful when adapting FUSE statistics to other settings. In particular, the constants help understand for which values of \u03bb the bounds hold, and to understand the relation between the two different right hand side terms which can be matched by tuning \u03bb. In provide these constants, in this subsection we replicate the proof of a sub-result from Rudelson and Vershynin (2013), but unlike in that paper we explicitly keep track of the constants. To be clear, Rudelson and Vershynin (2013) does not give numerical constants at all (only their existence is proved), and we need then for our statement of Theorem 5. We do not claim this as a contribution, the proof is only included so that we are not stating the numerical constants without justification. Theorem 6 (Sub-Gaussian Chaos; adapted from Rudelson and Vershynin, 2013). Let Xi be meanzero 1-sub-Gaussian variables, such that logEX exp(tX) \u2264 12 t2 for every t \u2208 R. Let A \u2208 Rn\u00d7n be a real symmetric matrix with zeros on the diagonal and we define the sub-Gaussian chaos\nW := n\u2211 i=1 n\u2211 j=1 AijXiXj .\nFor all |t| \u2264 (4 \u221a 2\u2225A\u2225)\u22121,\nEX exp(tW ) \u2264 exp ( 16t2\u2225A\u22252F ) .\nProof. This proof is closely adapted from Rudelson and Vershynin (2013) with some modifications to explicitly track numerical constants.\nLet Xi be mean-zero 1-sub-Gaussian variables, such that logEX exp(tX) \u2264 12 t2 for every t \u2208 R. We are considering\nW := \u2211 i,j AijXiXj\nwhere Aij has zeros on the diagonal. We introduce independent Bernoulli random variables \u03b4i \u2208 {0, 1} with E\u03b4i = 1/2 and define the matrix A\u03b4 with entries (A\u03b4)ij = \u03b4i(1 \u2212 \u03b4j)Aij . Note that E\u03b4A\u03b4 = 14A, since E\u03b4i(1\u2212 \u03b4j) equals 1/4 for i \u0338= j. Writing the set of indices \u039b\u03b4 = {i \u2208 [n] : \u03b4i = 1}, we introduce\nW\u03b4 := \u2211 i,j A\u03b4ijXiXj = \u2211\ni\u2208\u039b\u03b4, j\u2208\u039bc\u03b4 AijXiXj = \u2211 j\u2208\u039bc\u03b4 Xj ( \u2211 i\u2208\u039b\u03b4 AijXi ) .\nBy Jensen\u2019s inequality, EX exp(tW ) \u2264 EX,\u03b4 exp(4tW\u03b4).\nConditioned on \u03b4 and (Xi)i\u2208\u039b\u03b4 , W\u03b4 is a linear combination of the mean-zero sub-gaussian random variables Xj , j \u2208 \u039bc\u03b4 . It follows that this conditional distribution of W\u03b4 is sub-gaussian, and so\nE(Xj)j\u2208\u039bc \u03b4 exp(4tW\u03b4) \u2264 exp 1 2 (4t)2 \u2211 j\u2208\u039bc\u03b4 ( \u2211 i\u2208\u039b\u03b4 AijXi )2 . Now introduce g = (g1, . . . , gn) \u223c N (0, 1) i.i.d. draws from a normal and note that the above can also arise directly as the moment generating function of these Normal variables, so we make the further equality (for fixed X and \u03b4),\nexp 1 2 (4t)2 \u2211 j\u2208\u039bc\u03b4 ( \u2211 i\u2208\u039b\u03b4 AijXi )2 = Eg exp 4t \u2211 j\u2208\u039bc\u03b4 gj ( \u2211 i\u2208\u039b\u03b4 AijXi )2 . Rearranging the terms, and using that the resulting term is a linear combination of the sub-Gaussian Xi, i \u2208 \u039b\u03b4 , we find that\nEX,g exp 4t \u2211 j\u2208\u039bc\u03b4 gj ( \u2211 i\u2208\u039b\u03b4 AijXi )2 = EX,g exp 4t\u2211 i\u2208\u039b\u03b4 Xi ( \u2211 j\u2208\u039bc\u03b4 Aijgj ) \u2264 EX,g exp 1 2 (4t)2 \u2211 i\u2208\u039b\u03b4 ( \u2211 j\u2208\u039bc\u03b4 Aijgj\n)2 \u2264 EX,g exp\n8t2\u2211 i (\u2211 j \u03b4i(1\u2212 \u03b4j)Aijgj )2\n\u2264 EX,g exp ( 8t2\u2225A\u03b4g\u222522 ) .\nNow, by the rotation invariance of the distribution of g, the random variable \u2225A\u03b4g\u222522 is distributed identically with \u2211 i s 2 i g 2 i where si denote the singular values of A \u03b4 , so by independence\nEX,g exp ( 8t2\u2225A\u03b4g\u222522 ) = Eg exp ( 8t2 \u2211 i s2i g 2 i ) = \u220f i Eg exp ( 8t2s2i g 2 i\n) \u2264 \u220f i exp ( 16t2s2i\n) where the last inequality holds if 8t2 maxi s2i \u2264 14 and arises since the MGF of a Chi-squared variable, Eetg2 \u2264 e2t for 0 \u2264 t \u2264 14 . Since maxi si = \u2225A\u03b4\u2225 \u2264 \u2225A\u2225 and \u2211 i s 2 i = \u2225A\u03b4\u22252F \u2264 \u2225A\u2225F , we combine the above steps to find that EX exp(tW ) \u2264 exp ( 16t2\u2225A\u22252F ) for |t| \u2264 (4 \u221a 2\u2225A\u2225)\u22121."
        },
        {
            "heading": "C.2 Permutation Bounds for Two-Sample U-Statistics",
            "text": "The main technical result we use for the null bounds is the following. Here we assume the permutation-invariance property of the U statistic kernel that h(x1, x2; y1, y2) = h(x2, x1; y1, y2) = h(x1, x2; y2, y1). This can always be ensured by symmetrizing the kernel, and is satisfied by the kernel used by M\u0302MD2 which we consider in this paper. Theorem 7. Fix combined sample Z of size n + m with n \u2264 m and let h(x1, x2; y1, y2) be a two-sample U-statistic kernel as described above. Define\nUk(Z) := 1 n(n\u2212 1)m(m\u2212 1) \u2211\n(i,i\u2032)\u2208[n]2 \u2211 (j,j\u2032)\u2208[m]2 h(Zi, Zi\u2032 ;Zn+j , Zn+j\u2032).\nand\nU\u0304k(Z) = 1\nn(n\u2212 1)m(m\u2212 1) sup\u03c3\u2208Sn+m \u221a\u221a\u221a\u221a\u221a \u2211 (i,i\u2032)\u2208[n]2  \u2211 (j,j\u2032)\u2208[m]2 h(Z\u03c3(i), Z\u03c3(i\u2032);Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2. If \u03c3 \u2208 Uniform(Sn+m) is a random permutation of the dataset, then then for all |t| < (4 \u221a 2U\u0304k(Z))\n\u22121, E\u03c3 exp(tUk(\u03c3Z)) \u2264 exp ( t2U\u03042k (Z) ) .\nTheorem 8 (Normaliser Bound). If h(x, x\u2032; y, y\u2032) = k(x, x\u2032) + k(y, y\u2032)\u2212 k(x, y\u2032)\u2212 k(x\u2032, y) then\nU\u0304k(Z) 2 \u2264 16\nn(n\u2212 1)N\u0302k(Z)\nwhere n \u2264 m and N\u0302k(Z) =\n1 n(n\u2212 1) \u2211\n(i,j)\u2208[n+m]2\nk(Zi, Zj) 2.\nAdditionally, if 0 \u2264 k(x, x\u2032) \u2264 \u03ba for all x, x\u2032, then\nU\u03042k (Z) \u2264 4\u03ba2\nn(n\u2212 1) .\nProof of Theorem 7. Firstly, we make the following definitions based on the work of Kim et al. (2022): let L = {l1, . . . , ln} be an n-tuple drawn uniformly without replacement from [m]. Then for any fixed Z EL[U\u0303Lk (Z)] = Uk(Z) (7) where we define\nU\u0303Lk (Z) := 1 n(n\u2212 1) \u2211\n(i,j)\u2208[n]2\nh(Zi, Zj ;Zn+li , Zn+lj ).\nNote that in the above we have used the invariance h(x1, x2; y1, y2) = h(x1, x2; y2, y1).\nNow additionally define \u03b6i as i.i.d. Rademacher variables, and let \u03c3 \u223c Uniform(Sn+m). For any fixed Z\nU\u0303Lk (\u03c3Z) = d U\u0303L,\u03b6k (\u03c3Z) (8)\nwhere we have defined\nU\u0303L,\u03b6k (Z) := 1 n(n\u2212 1) \u2211\n(i,j)\u2208[n]2\n\u03b6i\u03b6jh(Zi, Zj ;Zn+li , Zn+lj ).\nThis works because we can first define Z\u0303i = Zi or Z\u0303i = Zn+li , each with probability 1 2 . The distribution of U\u0303Lk (\u03c3Z\u0303) = d U\u0303Lk (\u03c3Z), and then using the symmetry of k(x, y) in its arguments gives the equivalence Equation (8) (c.f. eq. 28, Kim et al., 2022).\nNow for fixed Z, combining Equations (7) and (8) and Jensen\u2019s inequality we gives\nE\u03c3 exp(tUk(\u03c3Z)) = E\u03c3 exp(tEL[U\u0303Lk (\u03c3Z)|\u03c3])\n= E\u03c3 exp(tEL,\u03b6 [U\u0303L,\u03b6k (\u03c3Z)|\u03c3])\n\u2264 E\u03c3,\u03b6 exp tEL  1 n(n\u2212 1) \u2211 (i,j)\u2208[n]2 \u03b6i\u03b6jh(Z\u03c3(i), Z\u03c3(j);Z\u03c3(n+li), Z\u03c3(n+lj)) \u2223\u2223\u2223\u2223\u2223\u2223\u03c3 \n= E\u03c3,\u03b6 exp t 1 n(n\u2212 1) \u2211 (i,j)\u2208[n]2 \u03b6i\u03b6jEL [ h(Z\u03c3(i), Z\u03c3(j);Z\u03c3(n+li), Z\u03c3(n+lj)) \u2223\u2223\u03c3] \n=: E\u03c3,\u03b6 exp t n\u2211 i=1 n\u2211 j=1 \u03b6i\u03b6jA \u03c3 ij  . In the final step we have defined the matrix A\u03c3, with entries A\u03c3ij = (n(n \u2212 1))\u22121EL [ h(Z\u03c3(i), Z\u03c3(j);Z\u03c3(n+li), Z\u03c3(n+lj))\n\u2223\u2223\u03c3] for i \u0338= j and Aii = 0 for all i. We note that \u03b6 is independent of \u03c3 and satisfies the conditions of Theorem 6, so applying this theorem we obtain that\nE\u03c3,\u03b6 exp t n\u2211 i=1 n\u2211 j=1 \u03b6i\u03b6jA \u03c3 ij  \u2264 E\u03c3 exp ( 16t2\u2225A\u03c3\u22252F ) for |t| < (4 \u221a 2\u2225A\u03c3\u2225)\u22121\n\u2264 E\u03c3 exp ( 16t2\u2225A\u03c3\u22252F ) for |t| < (4 \u221a 2\u2225A\u03c3\u2225F )\u22121\n\u2264 exp ( 16t2 sup\n\u03c3\u2208Sn+m \u2225A\u03c3\u22252F\n) for |t| < (4\n\u221a 2 sup \u03c3\u2208Sn+m \u2225A\u03c3\u2225F )\u22121\nwhere in the second line we have used that \u2225M\u2225 \u2264 \u2225M\u2225F for any matrix M . Now note that\n\u2225A\u03c3\u22252F = n\u2211\ni=1 n\u2211 i\u2032=1 (A\u03c3ii\u2032) 2\n= 1 n2(n\u2212 1)2 \u2211\n(i,i\u2032)\u2208[n]2\n(EL [ h(Z\u03c3(i), Z\u03c3(i\u2032);Z\u03c3(n+li), Z\u03c3(n+li\u2032 )) \u2223\u2223\u03c3])2 = 1 n2(n\u2212 1)2 \u2211\n(i,i\u2032)\u2208[n]2\n 1 m(m\u2212 1) \u2211 (j,j\u2032)\u2208[m]2 h(Z\u03c3(i), Z\u03c3(i\u2032);Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2 . We complete the proof by noting that U\u0304k(Z)2 = sup\u03c3\u2208Sn+m \u2225A\u03c3\u22252F . Proof of Theorem 8. We have\nU\u03042k (Z) = 1 n2(n\u2212 1)2 \u2211\n(i,i\u2032)\u2208[n]2\n 1 m(m\u2212 1) \u2211 (j,j\u2032)\u2208[m]2 h(Z\u03c3(i), Z\u03c3(i\u2032);Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2\n\u2264 1 n2(n\u2212 1)2m(m\u2212 1) \u2211 (i,i\u2032)\u2208[n]2 \u2211 (j,j\u2032)\u2208[m]2 h(Z\u03c3(i), Z\u03c3(i\u2032);Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2\n\u2264 4 n2(n\u2212 1)2m(m\u2212 1) \u2211 (i,i\u2032)\u2208[n]2 \u2211 (j,j\u2032)\u2208[m]2 ( k(Z\u03c3(i), Z\u03c3(i\u2032)) 2 + k(Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2\n+ k(Z\u03c3(i), Z\u03c3(n+j\u2032)) 2 + k(Z\u03c3(i\u2032), Z\u03c3(n+j\u2032))\n2 )\n= 4\nn2(n\u2212 1)2m(m\u2212 1)\n( m(m\u2212 1)\n\u2211 (i,i\u2032)\u2208[n]2 k(Z\u03c3(i), Z\u03c3(i\u2032)) 2\n+ n(n\u2212 1) \u2211\n(j,j\u2032)\u2208[m]2\nk(Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2\n+ (m\u2212 1)(n\u2212 1) \u2211\n1\u2264i\u2264n \u2211 1\u2264j\u2264m k(Z\u03c3(i), Z\u03c3(n+j\u2032)) 2\n+ (m\u2212 1)(n\u2212 1) \u2211\n1\u2264i\u2264n \u2211 1\u2264j\u2264m k(Z\u03c3(i\u2032), Z\u03c3(n+j)) 2\n)\n\u2264 16 n2(n\u2212 1)2 \u2211 (i,j)\u2208[n+m]2 k(Zi, Zj) 2\n= 16\nn(n\u2212 1)N\u0302k(Z)\nwhere the first inequality holds by Jensen\u2019s inequality, the second by convexity (so that (a+ b+ c+ d)2 \u2264 4(a2 + b2 + c2 + d2)) and the third inequality using n \u2264 m to upper bound each of the four scalars inside the parentheses by m(m\u2212 1), and we also upper bounded each of sums over subsets of terms by the sum over all possible terms.\nFor the second part note that since h \u2208 [\u22122\u03ba, 2\u03ba]\nU\u03042k (Z) = 1 n2(n\u2212 1)2 \u2211\n(i,i\u2032)\u2208[n]2\n 1 m(m\u2212 1) \u2211 (j,j\u2032)\u2208[m]2 h(Z\u03c3(i), Z\u03c3(i\u2032);Z\u03c3(n+j), Z\u03c3(n+j\u2032)) 2\n\u2264 1 n2(n\u2212 1)2 \u2211 (i,i\u2032)\u2208[n]2 (2\u03ba)2\n\u2264 4\u03ba 2\nn(n\u2212 1) ."
        },
        {
            "heading": "C.3 Bounds for MMD-FUSE under Null: Proof of Theorem 5",
            "text": "Proof of Theorem 5. Note under the null Z is permutation invariant. We will use Theorem 7 with h as the MMD 2-statistic kernel, so that Uk(Z) = M\u0302MD2(Z) (for fixed k). We will use the notation \u22641\u2212\u03b4 to denote that the inequality holds with probability at least 1 \u2212 \u03b4 over the random variables being considered.\nNote that U\u03042k \u2264 4\u03ba2/n(n\u2212 1) from Theorem 8, so that logEk\u223c\u03c0(\u27e8Z\u27e9) exp ( \u03bbM\u0302MD2k(X,Y ) ) \u22641\u2212\u03b4 logEZEk\u223c\u03c0(\u27e8Z\u27e9) exp ( \u03bbM\u0302MD2k(X,Y ) ) + log 1\n\u03b4 Markov = logE\u03c3EZEk\u223c\u03c0(\u27e8Z\u27e9) exp ( \u03bbM\u0302MD2k(\u03c3Z) ) + log 1\n\u03b4 Null Permutation-Free = logEZEk\u223c\u03c0(\u27e8Z\u27e9)E\u03c3 exp ( \u03bbM\u0302MD2k(\u03c3Z) ) + log 1\n\u03b4 Prior Permutation-Free\n\u2264 logEZEk\u223c\u03c0(\u27e8Z\u27e9)E\u03c3 exp ( \u03bb2U\u03042k ) + log 1\n\u03b4\n\u2264 4\u03ba 2\u03bb2 n(n\u2212 1) + log 1 \u03b4 , (9)\nwhere the penultimate result holds using Theorem 7 provided |\u03bb| < (4 \u221a 2 supk U\u0304k(Z))\n\u22121. Using the bound on U\u0304k from above gives the |\u03bb| < \u221a n(n\u2212 1)/8 \u221a 2\u03ba in the theorem statement.\nBy dividing M\u0302MD2 by the permutation invariant N\u0302k in every one of the above, we also find that\nlogEk\u223c\u03c0(\u27e8Z\u27e9) exp \u03bbM\u0302MD2k(X,Y )\u221a N\u0302k(\u27e8Z\u27e9)  \u22641\u2212\u03b4 logEZEk\u223c\u03c0(\u27e8Z\u27e9)E\u03c3 exp ( \u03bb2U\u03042k N\u0302k(\u27e8Z\u27e9) ) + log 1 \u03b4\n\u2264 16\u03bb 2 n(n\u2212 1) + log 1 \u03b4 (10)\n(11)\nthe first inequality using that N\u0302k is permutation-invariant and holding for |\u03bb| <\u221a supk N\u0302k(4 \u221a 2 supk U\u0304k(Z))\n\u22121. The final step uses that U\u03042k \u2264 16N\u0302k/n(n \u2212 1) from Theorem 8 which also shows that |\u03bb| < \u221a n(n\u2212 1)/16 \u221a 2 is sufficient.\nThe upper tail bounds for F\u0302USEN and F\u0302USE1 immediately follow from the above by dividing through \u03bb > 0 and from their definitions.\nLower Bounds. For the lower tails,\n\u2212F\u0302USE1(Z) = 1\n\u03bb logEk\u223c\u03c0(\u27e8Z\u27e9) exp(\u03bbM\u0302MD2k(Z))\n\u2264 \u2212Ek\u223c\u03c0(\u27e8Z\u27e9)[M\u0302MD2k(Z)] Jensen\n= 1\ns log \u25e6 exp(Ek\u223c\u03c0(\u27e8Z\u27e9)[\u2212sM\u0302MD2k(Z)]) introduce dummy s > 0\n\u2264 1 s log \u25e6 exp(Ek\u223c\u03c0(\u27e8Z\u27e9) \u2212 sM\u0302MD2k(Z)) Jensen on exp \u22641\u2212\u03b4 4\u03ba2s\nn(n\u2212 1) + 1 s log 1 \u03b4 ,\nwhere the last line follows from Equation (9) replacing \u03bb by dummy variable \u2212s for 0 < s <\u221a n(n\u2212 1)/8 \u221a 2\u03ba; we importantly note that this bound held for negative \u03bb by Theorem 7.\nBy following the same process for F\u0302USEN and instead using Equation (10), also holding for potentially negative \u03bb, we obtain\n\u2212F\u0302USEN (Z) \u22641\u2212\u03b4 16s n(n\u2212 1) + 1 s log 1 \u03b4\nfor 0 < s < \u221a n(n\u2212 1)/16 \u221a 2.\nFor the small \u03b4 we generally use, these are tightest when s is at its maximum permissible value, so we substitute these values for the theorem statement.\nUnder Permutation. To prove the equivalent result for \u03c3Z under permutations, we replace Z by \u03c3Z and our application of Markov\u2019s inequality introducing an expectation over Z with one over \u03c3. This changes nothing else in the derivations."
        },
        {
            "heading": "D Concentration under the alternative",
            "text": "We give the exponential convergence bounds for F\u0302USE1 under the alternative and relate it to its mean. In the following we will assume that K is a class of kernels bounded by 0 < \u03ba <\u221e, so that F\u0302USE1 \u2208 [\u22122\u03ba, 2\u03ba]. We also introduce the following quantity (for fixed, data-free prior \u03c0) which is closely related to the expectation of F\u0302USE1,\nFUSE1 := sup \u03c1\u2208M1+(K)\nMMD2K\u03c1(p, q)\u2212 KL(\u03c1, \u03c0)\n\u03bb .\nTheorem 9. FUSE1 is bounded in the following ways: FUSE1 \u2264 EZF\u0302USE1(Z) \u2264 FUSE1 +8\u03ba2\u03bb ( 1\nn +\n1\nm\n) ,\nand\nMMD2K\u03c0 (p, q) \u2264 FUSE1 \u2264 sup \u03c1\u2208M1+(K):KL(\u03c1,\u03c0)<\u221e MMD2K\u03c1(p, q) \u2264 sup k\u2208supp(\u03c0) MMD2k(p, q).\nUnder the null hypothesis FUSE1 = 0.\nWe can now state concentration results for F\u0302USE1 in terms of FUSE1.\nTheorem 10. With probability at least 1\u2212 \u03b4 over the sample F\u0302USE1(Z)\u2212 FUSE1 \u2264 8\u03ba2\u03bb ( 1\nn +\n1\nm\n) + log \u03b4\u22121\n\u03bb\nand with the same probability, FUSE1 \u2212F\u0302USE1(Z) \u2264 2\u03ba \u221a 8 ( 1\nn +\n1\nm\n) log \u03b4\u22121."
        },
        {
            "heading": "D.1 Proofs",
            "text": "Note on the proofs. The \u201cbounded difference lemma\u201d (Theorem 11) we give below is not the same as the bounded difference inequality, though it is closely related. In fact, our result could be used as an intermediate step in proving the latter, but we note that this would not be the usual method (e.g. Boucheron et al., 2013, use an \u201centropy\u201d method instead), and the converse is not true. The fact that we have to prove a variant form of an existing concentration inequality to get concentration for our log-sum-exp statistics is similar to how the same is required in PAC-Bayesian proofs, where e.g. PAC-Bayes Bernstein inequalities also require modified proof techniques that mirror those used to prove the usual Bernstein inequalities.\nThe usual bounded difference inequality cannot be used to prove Theorem 9, since it is not a concentration bound. It also does not give the concentration we need in Theorem 10, since the FUSE statistics do not have the bounded difference property, only M\u0302MD2k does; this is why our proofs cannot use a \u201cplug-in\u201d concentration bound.\nTheorem 11 (Bounded Difference Lemma). A function f has the bounded difference property there exist constants L\u2113 <\u221e, \u2113 \u2208 [n] such that\n|f(z1, . . . , zk, . . . , zn)\u2212 f(z1, . . . , z\u2113, . . . , zn)| \u2264 L\u2113 for any choices of z1, . . . , zn, z\u2032\u2113, and \u2113 \u2208 [n]. For such a function and any independent random variables Z1, . . . , Zn and t \u2208 R (subject to appropriate measurability restrictions)\nE exp(t(f(Z1, . . . , Zn)\u2212 Ef(Z1, . . . , Zn))) \u2264 exp ( 1\n8 t2 n\u2211 \u2113=1 L2\u2113\n) .\nProof of Theorem 11. We introduce the Doob construction, defining\nD\u2113 = E[f(Z1, . . . , Zn)|Z1, . . . , Z\u2113]\u2212 E[f(Z1, . . . , Zn)|Z1, . . . , Z\u2113\u22121] This is a martingale difference sequence with\nn\u2211 \u2113=1 D\u2113 = f(Z1, . . . , Zn)\u2212 Ef(Z1, . . . , Zn).\nIt is shown by (for example) Wainwright (2019, Ch. 2.2, p.37) that D\u2113 lies in an interval of length at most L\u2113 by the bounded differences assumption. Thus, applying iterated expectation and Hoeffding\u2019s lemma for the MGF of bounded random variables,\nE exp(t(f(Z1, . . . , Zn)\u2212 Ef(Z1, . . . , Zn))) = E exp ( \u03bb\nn\u2211 \u2113=1 D\u2113\n)\n= E [ exp ( \u03bb\nn\u22121\u2211 \u2113=1 D\u2113\n) E [exp (\u03bbDn) |Z1, . . . , Zn\u22121] ]\n\u2264 E [ exp ( \u03bb\nn\u22121\u2211 \u2113=1 D\u2113\n)] exp ( 1\n8 \u03bb2L2nDn\n)\n\u2264 exp ( 1\n8 t2 n\u2211 \u2113=1 L2\u2113\n) .\nThe MGF of M\u0302MD2 can then be bounded using the following result, proved via the above. Theorem 12. For bounded kernel k \u2264 \u03ba, t \u2208 R and sample sizes m,n,\nE exp(t(M\u0302MD2k(X,Y )\u2212MMD2k(p, q))) \u2264 exp ( 8t2\u03ba2 ( 1\nm +\n1\nn\n)) .\nProof of Theorem 12. We show that M\u0302MD2k(x, y) has the bounded differences property and then apply Theorem 11. Denote by x\\\u2113 for \u2113 \u2208 [n] that the \u2113-th example in the x sample is changed. Then\n|M\u0302MD2k(x, y)\u2212M\u0302MD2k(x\\\u2113, y)|\n= \u2223\u2223\u2223\u2223\u2223\u2223 2n(n\u2212 1) \u2211\ni\u2208[n]\\{\u2113}\n(k(x\u2113, xi)\u2212 k(x\u2032\u2113, xi))\u2212 2\nmn m\u2211 j=1 (k(x\u2113, yj)\u2212 k(x\u2032\u2113, yj)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 2 n(n\u2212 1) \u2211 i\u2208[n]\\{\u2113} |k(x\u2113, xi)\u2212 k(x\u2032\u2113, xi)|+ 2 mn m\u2211 j=1 |k(x\u2113, yj)\u2212 k(x\u2032\u2113, yj)|\n\u2264 2 n(n\u2212 1)(n\u2212 1) \u00b7 2\u03ba+ 2 mn m \u00b7 2\u03ba\n= 8\u03ba\nn .\nA similar process for the y sample gives bounds of 8\u03ba/m, so that M\u0302MD2 has the bounded differences property with\nn+m\u2211 \u2113=1 L2\u2113 \u2264 n \u00b7 ( 8\u03ba n )2 +m \u00b7 ( 8\u03ba m )2 = 64\u03ba2 ( 1 n + 1 m ) .\nProof of Theorem 9. For the first lower bound, note\nEZF\u0302USE1(Z) = EZ sup \u03c1\nE\u03c1[M\u0302MD2]\u2212 KL\n\u03bb \u2265 sup \u03c1 EZ,\u03c1[M\u0302MD2]\u2212\nKL\n\u03bb = FUSE1 .\nFor the upper bound, note that EZF\u0302USE1(Z) \u2264 [ 1 \u03bb log ( EZEk\u223c\u03c0 [ e\u03bbM\u0302MD 2 k ])] Jensen\n\u2264 [ 1 \u03bb log ( Ek\u223c\u03c0EZ [ e\u03bbM\u0302MD 2 k ])] Independence of \u03c0 from Z\n\u2264 1 \u03bb log ( Ek\u223c\u03c0 [ e\u03bbMMD 2 k +8\u03bb 2\u03ba2(n\u22121+m\u22121) ])\nTheorem 12\n= FUSE1 +8\u03ba 2\u03bb\n( 1\nn +\n1\nm\n) .\nThe lower bound on FUSE1 is obtained by relaxing the supremum with \u03c1 = \u03c0. The upper bounds come since the supremum over \u03c1 \u2208 M1+(K) : KL(\u03c1, \u03c0) <\u221e of MMD2 is clearly greater than the KL-regularised version. Under the null hypothesis, MMDk(p, q) = 0 for every kernel (regardless of them being characteristic or not), so FUSE1 = 0.\nProof of Theorem 10. For the upper bound,\nF\u0302USE1(Z) =\n[ 1 \u03bb log ( Ek\u223c\u03c0 [ e\u03bbM\u0302MD 2 k ])] \u22641\u2212\u03b4 [ 1 \u03bb log ( EZEk\u223c\u03c0 [ e\u03bbM\u0302MD 2 k ])] + log \u03b4\u22121 \u03bb Markov\n\u2264 [ 1 \u03bb log ( Ek\u223c\u03c0EZ [ e\u03bbM\u0302MD 2 k ])] + log \u03b4\u22121 \u03bb Independence of \u03c0 from Z\n\u2264 1 \u03bb log ( Ek\u223c\u03c0 [ e\u03bbMMD 2 k +8\u03bb 2\u03ba2(m\u22121+n\u22121) ]) + log \u03b4\u22121 \u03bb Theorem 12\n= FUSE1 +8\u03ba 2\u03bb\n( 1\nn +\n1\nm\n) + log \u03b4\u22121\n\u03bb .\nFor the lower bound, let \u03c1\u2217 be the value of \u03c1 achieving the supremum in the dual form of FUSE1 (which we note is independent of the sample), so that\nFUSE1 \u2212F\u0302USE1(Z) = sup \u03c1\n{ E\u03c1\u2217 [MMD2k]\u2212 KL(\u03c1, \u03c0)\n\u03bb\n} \u2212 sup\n\u03c1\n{ E\u03c1[M\u0302MD2k]\u2212 KL(\u03c1, \u03c0)\n\u03bb } = E\u03c1\u2217 [MMD2k]\u2212 KL(\u03c1\u2217, \u03c0)\n\u03bb \u2212 sup\n\u03c1\n{ E\u03c1[M\u0302MD2k]\u2212 KL(\u03c1, \u03c0)\n\u03bb } \u2264 E\u03c1\u2217 [MMD2k]\u2212 KL(\u03c1\u2217, \u03c0) \u03bb \u2212 ( E\u03c1\u2217 [M\u0302MD2k]\u2212 KL(\u03c1\u2217, \u03c0) \u03bb\n) = E\u03c1\u2217 [MMD2k \u2212M\u0302MD2k] = MMD2K\u03c1\u2217 (p, q)\u2212 M\u0302MD 2 K\u03c1\u2217 (Z).\nNow the above is for fixed kernel K\u03c1\u2217 independent of the data, so by Markov\u2019s inequality and Theorem 12, for any s > 0 we find\nMMD2K\u03c1\u2217 (p, q)\u2212 M\u0302MD 2 K\u03c1\u2217 (Z) \u22641\u2212\u03b4 s\u22121 logEZ exp ( s ( MMD2K\u03c1\u2217 (p, q)\u2212 M\u0302MD 2 K\u03c1\u2217 (Z) )) + s\u22121 log \u03b4\u22121\n\u2264 8s\u03ba2 ( 1\nm +\n1\nn\n) + s\u22121 log \u03b4\u22121.\nOptimising for s yields s = \u03ba\u22121 \u221a log(1/\u03b4)/ \u221a 8(1/m+ 1/n) which gives the desired result."
        },
        {
            "heading": "E Power Analysis",
            "text": ""
        },
        {
            "heading": "E.1 General Recipe for Power Analysis of Permutation Tests",
            "text": "The general outline for power analysis consists of the following: First we start with the Type II error probability. Then we attempt to upper bound it by iteratively applying the following simple lemma to the different terms.\nMonotonicity in High Probability. Let X,Y be r.v.s, such that X \u2264 Y w.p. \u2265 1 \u2212 \u03b4. Then P(X \u2265 c) \u2264 P(Y \u2265 c) + \u03b4. This result can be applied both when X \u2264 Y a.s. giving \u03b4 = 0, or if Y = a is constant.\nIteratively applying this lemma gives P(Type II) \u2264 N\u03b4, and then we set N\u03b4 = \u03b2. As a first step in the above process we will use the following useful result (adapted from Kim et al., 2022) to convert from the random permutations we use in practice to the full set of permutation. This result shows that when B is taken sufficiently large (roughly \u2126(\u03b1\u22122 log(\u03b2\u22121))), the only changes to the final power results will be in constants multiplying \u03b1 and \u03b2.\nTheorem 13 (From Randomised to Deterministic Permutations). Suppose G = (g1, . . . , gB+1) consists of B uniformly drawn permutations from G, plus the identity permutation as gB+1. Then\nP ( \u03c4(Z) \u2264 quantile\n1\u2212\u03b1,G \u03c4(gZ)\n) \u2264 P ( \u03c4(Z) \u2264 quantile\n1\u2212\u03b1B ,G \u03c4(gZ)\n) + \u03b4\nwhere 1\u2212 \u03b1B = B+1B (1\u2212 \u03b1) + \u221a log(2/\u03b4) 2B .\nWe note that provided B \u2265 8\u03b1\u22122 log(2/\u03b4), then 1\u2212 \u03b1B \u2264 1\u2212 \u03b1/2.\nProof. We note the Dvoretzky\u2013Kiefer\u2013Wolfowitz inequality Dvoretzky et al. (1956); Massart (1990) for empirical CDF Fn of n samples from original CDF F :\nP ( sup x |Fn(x)\u2212 F (x)| \u2265 t ) \u2264 2e\u22122nt2 (12)\nfor every t > 0.\nThe permutation CDF for a group G take the form\nFG(x) = 1 |G| \u2211 g\u2208G 1{\u03c4(gZ) \u2264 x}\nand given sample G\u0303 = (g1, . . . , gB) i.i.d. uniformly from G (this excludes the identity permutation added to G), the empirical CDF is\nF\u0302G\u0303(x) = 1\nB \u2211 g\u2208G\u0303 1{\u03c4(gZ) \u2264 x}\nWe can also write F\u0302G(x) including the identity permutation as gB+1, and note that F\u0302G(x) =\nDefine the good event\nA = { sup x |FG\u0303(x)\u2212 FG(x)| \u2264 \u221a log(2/\u03b4) 2B } which holds with probability at least 1\u2212 \u03b4 by Equation (12). Given A, we have\nqG := quantile 1\u2212\u03b1,g\u2208G \u03c4(gZ) = inf{r \u2208 R : 1 B + 1 \u2211 g\u2208G 1{\u03c4(gZ) \u2264 r} \u2265 1\u2212 \u03b1}\n\u2264 inf{r \u2208 R : 1 B + 1 \u2211 g\u2208G 1{\u03c4(gZ) \u2264 r} \u2265 1\u2212 \u03b1}\n= inf{r \u2208 R : F\u0302G\u0303(r) \u2265 B + 1\nB (1\u2212 \u03b1)} \u2264 inf { r \u2208 R : F\u0302G(r) \u2265 B + 1\nB (1\u2212 \u03b1) +\n\u221a log(2/\u03b4)\n2B } = quantile\n1\u2212\u03b1B ,g\u2208G \u03c4(gZ) =: q\nwhere we have defined \u03b1B as above.\nOverall we find P(\u03c4 \u2264 qG) \u2264 P(\u03c4 \u2264 qG|A) + P(Ac) \u2264 P(\u03c4 \u2264 q) + \u03b4.\nE.2 Variance of M\u0302MD2k\nIn proving our power results it is necessary to upper bound the variance of M\u0302MD2k. Theorem 14. For any kernel k upper bounded by \u03ba, if n \u2264 m \u2264 cn for c \u2265 1, there exists universal constant C > 0 depending only on c, such that\nV[M\u0302MD2k] \u2264 C ( 4\u03baMMD2k\nn + \u03ba2 n2 ) Proof. We define\n\u03c3210 = VX(EX\u2032,Y,Y \u2032 [h(X,X \u2032, Y, Y \u2032)]) \u03c3201 = VY (EX,X\u2032,Y \u2032 [h(X,X \u2032, Y, Y \u2032)]) \u03c3211 = max{E[k2(X,X \u2032)],E[k2(X,Y )],E[k2(Y, Y \u2032)]}.\nFrom a well-known bound (based on Lee, 1990, Equation 2, p.38; see also Kim et al., 2022, Appendix F, Equation 59 or Schrab et al., 2023, Proposition 3),\nV[M\u0302MD2k] \u2264 C ( \u03c3210 m + \u03c3201 n + \u03c3211 ( 1 m + 1 n )2)\n\u2264 C ( \u03c3210 n + \u03c3201 n + \u03c3211 n2 ) where we used that n \u2264 m \u2264 cn. and the result in red above, and the boundedness of the kernel for the final term. This gives the further bound\nV[M\u0302MD2k] \u2264 C ( 4\u03baMMD2k\nn + \u03ba2 n2 ) since\n\u03c3210 = varX(EX\u2032,Y,Y \u2032 [h(X,X \u2032, Y, Y \u2032)]) = EX [( EX\u2032,Y,Y \u2032 [h(X,X \u2032, Y, Y \u2032)] )2] = EX [( EX\u2032,Y,Y \u2032 [\u27e8\u03d5(X)\u2212 \u03d5(Y ), \u03d5(X \u2032)\u2212 \u03d5(Y \u2032)\u27e9]\n)2] = EX [ \u27e8\u03d5(X)\u2212 \u00b5Q, \u00b5P \u2212 \u00b5Q\u27e92\n] \u2264 ( EX [ \u2225\u03d5(X)\u2212 \u00b5Q\u22252 ]) \u2225\u00b5P \u2212 \u00b5Q\u22252\n\u2264 2\u03ba\u2225\u00b5P \u2212 \u00b5Q\u22252\n= 2\u03baMMD2k,\na similar result for \u03c301, and the simple bound \u03c3211 \u2264 \u03ba2."
        },
        {
            "heading": "E.3 Proof of Theorem 2",
            "text": "Proof. Define \u03b1B as in Theorem 13, noting that 1 \u2212 \u03b1B < 1 \u2212 \u03b1/2 under the assumption B \u2265 8\u03b1\u22122 log(4/\u03b2). From Theorem 13 we can consider the full permutation set as\nPp\u00d7q,G ( F\u0302USE1(Z) \u2264 quantile\n1\u2212\u03b1,G F\u0302USE1(\u03c3Z) ) \u2264 Pp\u00d7q ( F\u0302USE1(Z) \u2264 quantile\n1\u2212\u03b1B ,G F\u0302USE1(\u03c3Z)\n) + \u03b2/2.\nWe also recall that from Theorem 5 when \u03bb = cn/\u03ba,\nquantile 1\u2212\u03b1B ,G\nF\u0302USE1(\u03c3Z) \u2264 C1\u03ba(1 + log\u03b1\n\u22121)\nn ,\nso that Pp\u00d7q ( F\u0302USE1(Z) \u2264 quantile\n1\u2212\u03b1B ,G F\u0302USE1(\u03c3Z)\n) \u2264 Pp\u00d7q ( F\u0302USE1(Z) \u2264 C1\u03ba(1 + log\u03b1 \u22121)\nn\n) .\nFor any \u03c1, we define\nS\u03c1 = MMD 2 K\u03c1(p, q)\u2212\nKL(\u03c1, \u03c0) \u03bb \u2212 C1\u03ba(1 + log\u03b1\n\u22121)\nn ,\nwhich we substitute into Pp\u00d7q ( F\u0302USE1(Z) \u2264 C1\u03ba(1 + log\u03b1 \u22121)\nn ) = P ( MMD2K\u03c1(p, q)\u2212 1\n\u03bb KL(\u03c1, \u03c0)\u2212 F\u0302USE1(Z) \u2265 S\u03c1 ) = P ( MMD2K\u03c1(p, q)\u2212 1\n\u03bb KL(\u03c1, \u03c0)\u2212 sup\n\u03c1\u2032\n( M\u0302MD2K\u03c1\u2032 (Z)\u2212 1 \u03bb KL(\u03c1\u2032, \u03c0) ) \u2265 S\u03c1 ) \u2264 P ( MMD2K\u03c1(p, q)\u2212 M\u0302MD2K\u03c1(Z) \u2265 S\u03c1\n) = 1\nS2\u03c1 Vp\u00d7q\n[ M\u0302MD2K\u03c1(Z) ] \u2264 C2 S2\u03c1 ( 4\u03baMMD2 n + \u03ba2 n2 ) .\nAfter substituting S\u03c1, we used the dual form of F\u0302USE1 and the inequalities sup f(\u03c1\u2032) \u2265 f(\u03c1), Chebyshev\u2019s, and Theorem 14. This term is upper bounded by \u03b2/2 if we set\nS2\u03c1 > 2C2 \u03b2\n( 4\u03baMMD2\nn + \u03ba2 n2\n) .\nWe also note that for a, b, x all non-negative, if x2 > a2 + 2b, then x2 > ax+ b. This works because x2 > ax+ b is equivalent to x2 > (\na 2 +\n\u221a a2\n4 + b )2 by taking the positive root, and(\na 2 +\n\u221a a2\n4 + b\n)2 = a2\n2 + b+ 2\na\n2\n\u221a a2\n4 + b \u2264 a2 + 2b\nusing Young\u2019s inequality 2AB \u2264 A2 +B2. Combining the above the Type II error rate is controlled by \u03b2 provided any of the following statements are true for any \u03c1 (with each new result implying the former):\nMMD2K\u03c1(p, q) > KL(\u03c1, \u03c0) \u03bb + C1\u03ba(1 + log\u03b1\n\u22121)\nn + \u221a 2C2 \u03b2 ( 4\u03baMMD2 n + \u03ba2 n2 )\nMMD2K\u03c1(p, q) > KL(\u03c1, \u03c0) \u03bb + C1\u03ba(1 + log\u03b1\n\u22121)\nn +\n\u221a 8C2\u03ba\nn\u03b2 MMD+\n\u221a 2C2\u03ba\nn \u221a \u03b2\nMMD2K\u03c1(p, q) > 2KL(\u03c1, \u03c0)\n\u03bb +\n2C1\u03ba(1 + log\u03b1 \u22121)\nn +\n8C2\u03ba\nn\u03b2 +\n2 \u221a 2C2\u03ba\nn \u221a \u03b2\nMMD2K\u03c1(p, q) > C3\u03ba\nn\n( 1\n\u03b2 + log\n1 \u03b1 +KL(\u03c1, \u03c0) ) where we used that \u221a x+ y \u2264 \u221ax+\u221ay, the result above, and \u03bb = cn/\u03ba."
        },
        {
            "heading": "E.4 Proof of Theorem 3",
            "text": "The proof of this statement proceeds similarly to the proof of Theorem 2. Using Theorem 5 with \u03bb = cn and Theorem 13 we find that\nPp\u00d7q,G ( F\u0302USEN (Z) \u2264 quantile\n1\u2212\u03b1,G F\u0302USEN (\u03c3Z) ) \u2264 Pp\u00d7q ( F\u0302USEN (Z) \u2264 C1(1 + log\u03b1 \u22121)\nn\n) + \u03b2/2.\nFor any \u03c1 such that KL(\u03c1, \u03c0) <\u221e we define\nT\u03c1 = C1 \u03ba \u00b7MMD2K\u03c1\nS\u03c1 = T\u03c1 \u2212 1 \u03bb KL(\u03c1, \u03c0)\u2212 C1\u03ba(1 + log\u03b1\n\u22121)\nn .\nWe assumed that n \u2264 m \u2264 cn for some c \u2265 1. Based on this, note that Nk \u2264 \u03ba2/C21 for C1 depending only on m/n \u2208 [1, c], and (since MMD2 \u2265 0 is strictly non-negative, unlike M\u0302MD2 which can be negative),\n\u2212E\u03c1  MMD2k\u221a N\u0302k(Z)  \u2264 \u2212C1 \u03ba Ek\u223c\u03c1 [ MMD2k ] = \u2212T\u03c1. (13)\nNow we introduce these definitions to bound P ( F\u0302USEN (Z) \u2264 C1(1 + log\u03b1 \u22121)\nn ) = P ( T\u03c1 \u2212 1\n\u03bb KL(\u03c1, \u03c0)\u2212 F\u0302USEN (Z) \u2265 S\u03c1\n)\n= P T\u03c1 \u2212 1 \u03bb KL(\u03c1, \u03c0)\u2212 sup\n\u03c1\u2032\nEk\u223c\u03c1\u2032 M\u0302MD2k(Z)\u221a\nN\u0302k(Z)\n\u2212 1 \u03bb KL(\u03c1\u2032, \u03c0)  \u2265 S\u03c1 \n\u2264 P T\u03c1 \u2212 Ek\u223c\u03c1 M\u0302MD2k(Z)\u221a\nN\u0302k(Z)\n \u2265 S\u03c1  sup f(\u03c1\u2032) \u2265 f(\u03c1)\n= P T\u03c1 \u2212 Ek\u223c\u03c1  MMD2k\u221a\nN\u0302k(Z) \u2212 MMD\n2 k\u221a\nN\u0302k(Z) + M\u0302MD2k(Z)\u221a N\u0302k(Z)\n \u2265 S\u03c1 \n= P T\u03c1 \u2212 Ek\u223c\u03c1  MMD2k\u221a\nN\u0302k(Z)\n+ Ek\u223c\u03c1 MMD2k \u2212M\u0302MD2k(Z)\u221a\nN\u0302k(Z)\n \u2265 S\u03c1 \n= P T\u03c1 \u2212 Ek\u223c\u03c1  MMD2k\u221a\nN\u0302k(Z)\n+ Ek\u223c\u03c1 MMD2k \u2212M\u0302MD2k(Z)\u221a\nN\u0302k(Z)\n \u2265 S\u03c1 \n\u2264 P Ek\u223c\u03c1 MMD2k \u2212M\u0302MD2k(Z)\u221a\nN\u0302k(Z)\n \u2265 S\u03c1  by Equation (13)\n\u2264 P \u2223\u2223\u2223\u2223\u2223\u2223Ek\u223c\u03c1 MMD2k \u2212M\u0302MD2k(Z)\u221a\nN\u0302k(Z)\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2265 S\u03c1  x \u2264 |x|\n\u2264 P Ek\u223c\u03c1 \u2223\u2223\u2223\u2223\u2223\u2223MMD 2 k \u2212M\u0302MD2k(Z)\u221a N\u0302k(Z) \u2223\u2223\u2223\u2223\u2223\u2223  \u2265 S\u03c1  |x| convex, Jensen = P Ek\u223c\u03c1  |MMD2k \u2212M\u0302MD2k(Z)|\u221a\nN\u0302k(Z)\n \u2265 S\u03c1  N\u0302k positive\n\u2264 1 S\u03c1 EZEk\u223c\u03c1  |MMD2k \u2212M\u0302MD2k(Z)|\u221a N\u0302k(Z)  N\u0302k positive, Markov \u2264 1 S\u03c1 \u221a\u221a\u221a\u221aEZEk\u223c\u03c1 [|MMD2k \u2212M\u0302MD2k(Z)|2]EZEk\u223c\u03c1 [ 1\nN\u0302k(Z)\n] Cauchy-Schwarz.\nThus the type II error will be controlled by \u03b2 if for any \u03c1\nS\u03c1 > 2\n\u03b2\n\u221a\u221a\u221a\u221aEZEk\u223c\u03c1 [|MMD2k \u2212M\u0302MD2k(Z)|2]EZEk\u223c\u03c1 [ 1\nN\u0302k(Z)\n] .\nProvided there is a c > 0 with\nEp\u00d7q\n[ 1\nN\u0302k(Z)\n] \u2264 c\nfor all k, this reduces to the condition\nMMD2K\u03c1(p, q) > C2\u03ba\n( log\u03b1\u22121\nn +\n1\n\u03b2\n\u221a E\u03c1Vp\u00d7q [ M\u0302MD2k(Z) ] + KL(\u03c1, \u03c0)\nn\n) .\nApplying Theorem 14, the proof is completed in essentially the same way as in the result for F\u0302USE1 (with slightly different \u03b2 dependence)."
        }
    ],
    "title": "MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting",
    "year": 2023
}