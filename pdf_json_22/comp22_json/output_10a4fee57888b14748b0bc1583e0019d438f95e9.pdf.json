{
    "abstractText": "Mathematical models often aim to describe a complicated mechanism in a cohesive and simple manner. However, reaching perfect balance between being simple enough or overly simplistic is a challenging task. Frequently, game-theoretic models have an underlying assumption that players, whenever they choose to execute a specific action, do so perfectly. In fact, it is rare that action execution perfectly coincides with intentions of individuals, giving rise to behavioural mistakes. The concept of incompetence of players was suggested to address this issue in game-theoretic settings. Under the assumption of incompetence, players have non-zero probabilities of executing a different strategy from the one they chose, leading to stochastic outcomes of the interactions. In this article, we survey results related to the concept of incompetence in classic as well as evolutionary game theory and provide several new results. We also suggest future extensions of the model and argue why it is important to take into account behavioural mistakes when analysing interactions among players in both economic and biological settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Thomas Graham"
        },
        {
            "affiliations": [],
            "name": "\u00b7Maria Kleshnina"
        },
        {
            "affiliations": [],
            "name": "Jerzy A. Filar"
        },
        {
            "affiliations": [],
            "name": "Jayakrishnan Nair"
        }
    ],
    "id": "SP:bb8b3b9799629ebba43ce40190e913b5369fd0bf",
    "references": [
        {
            "authors": [
                "M Abrudan",
                "L You",
                "K Sta\u0148kov\u00e1",
                "F Thuijsman"
            ],
            "title": "2016) A game theoretical approach to microbial coexistence. In advances in dynamic and evolutionary games",
            "year": 2016
        },
        {
            "authors": [
                "C Adami",
                "A Hintze"
            ],
            "title": "Thermodynamics of evolutionary games",
            "venue": "Phys Rev E",
            "year": 2018
        },
        {
            "authors": [
                "E Ak\u00e7ay"
            ],
            "title": "Deconstructing evolutionary game theory: coevolution of social behaviors with their evolutionary setting",
            "venue": "Am Nat",
            "year": 2020
        },
        {
            "authors": [
                "A Albrecht",
                "K Avrachenkov",
                "P Howlett",
                "G Verma"
            ],
            "title": "Evolutionary dynamics in discrete time for the perturbed positive definite replicator equation",
            "venue": "ANZIAM J",
            "year": 2020
        },
        {
            "authors": [
                "K Avrachenkov",
                "VS Borkar"
            ],
            "title": "Metastability in stochastic replicator dynamics",
            "venue": "Dyn Games Appl",
            "year": 2019
        },
        {
            "authors": [
                "J Beck"
            ],
            "title": "Incompetence, training and changing capabilities in game theory",
            "venue": "Ph.D. thesis,",
            "year": 2013
        },
        {
            "authors": [
                "JD Beck"
            ],
            "title": "Game theory implementation of capability investment problem",
            "venue": "Mil Op Res",
            "year": 2011
        },
        {
            "authors": [
                "JD Beck",
                "V Ejov",
                "JA Filar"
            ],
            "title": "Incompetence and impact of training in bimatrix games",
            "year": 2012
        },
        {
            "authors": [
                "JD Beck",
                "JA Filar"
            ],
            "title": "Games, incompetence and training",
            "venue": "Ann ISDG 8:93\u2013110 Dynamic Games and Applications (2023)",
            "year": 2007
        },
        {
            "authors": [
                "I Bomze"
            ],
            "title": "Non-cooperative two-person games in biology: a classification",
            "venue": "Int J Game Theory",
            "year": 1986
        },
        {
            "authors": [
                "I Bomze"
            ],
            "title": "Stability bymutation in evolutionary games.GamesEconBehav",
            "venue": "BurgerR",
            "year": 1995
        },
        {
            "authors": [
                "F Dercole",
                "S Rinaldi"
            ],
            "title": "Analysis of evolutionary processes: the adaptive dynamics approach and its applications",
            "year": 2008
        },
        {
            "authors": [
                "U Dieckmann",
                "P Marrow",
                "R Law"
            ],
            "title": "Evolutionary cycling in predator-prey interactions: population dynamics and the red queen",
            "venue": "J Theor Biol",
            "year": 1995
        },
        {
            "authors": [
                "S Dridi"
            ],
            "title": "Plasticity in evolutionary games",
            "venue": "bioRxiv p",
            "year": 2019
        },
        {
            "authors": [
                "I Eshel"
            ],
            "title": "Evolutionary and continuous stability",
            "venue": "J Theor Biol",
            "year": 1983
        },
        {
            "authors": [
                "JA Filar",
                "K Vrieze"
            ],
            "title": "Competitive Markov Decision Processes",
            "year": 1997
        },
        {
            "authors": [
                "D Foster",
                "P Young"
            ],
            "title": "Stochastic evolutionary game dynamics",
            "venue": "Theor Popul Biol",
            "year": 1990
        },
        {
            "authors": [
                "E Frey"
            ],
            "title": "Evolutionary game theory: theoretical concepts and applications to microbial communities",
            "venue": "Physica A: Statist Mech Appl",
            "year": 2010
        },
        {
            "authors": [
                "FudenbergD",
                "Harris C"
            ],
            "title": "Evolutionary dynamicswith aggregate shocks",
            "venue": "J EconTheor",
            "year": 1992
        },
        {
            "authors": [
                "D Fudenberg",
                "D Levine"
            ],
            "title": "The theory of learning in games",
            "year": 1999
        },
        {
            "authors": [
                "D Harville"
            ],
            "title": "Matrix algebra from a statistician\u2019s perspective, vol 1",
            "year": 1997
        },
        {
            "authors": [
                "C Hilbe",
                "M Abou Chakra",
                "PM Altrock",
                "A Traulsen"
            ],
            "title": "The evolution of strategic timing in collectiverisk dilemmas",
            "venue": "PloS One",
            "year": 2013
        },
        {
            "authors": [
                "C Hilbe",
                "L Schmid",
                "J Tkadlec",
                "K Chatterjee",
                "MA Nowak"
            ],
            "title": "Indirect reciprocity with private, noisy and incomplete information",
            "venue": "Proc Natl Acad Sci",
            "year": 2018
        },
        {
            "authors": [
                "J Hofbauer",
                "P Schuster",
                "K Sigmund",
                "R Wolff"
            ],
            "title": "Dynamical systems under constant organization ii: homogeneous growth functions of degree p=2",
            "venue": "SIAM J Appl Math 38(2):282\u2013304",
            "year": 1980
        },
        {
            "authors": [
                "J Hofbauer",
                "K Sigmund"
            ],
            "title": "Evolutionary game dynamics",
            "venue": "Bullet Am Math Soc",
            "year": 2003
        },
        {
            "authors": [
                "E Hopkins"
            ],
            "title": "Two competing models of how people learn in games",
            "year": 2002
        },
        {
            "authors": [
                "P Hufton",
                "Y Lin",
                "T Galla"
            ],
            "title": "Phenotypic switching of populations of cells in a stochastic environment",
            "venue": "J Statist Mech: Theor Exp",
            "year": 2018
        },
        {
            "authors": [
                "LA Imhof",
                "MA Nowak"
            ],
            "title": "Evolutionary game dynamics in a wright-fisher process",
            "venue": "J Math Biol",
            "year": 2006
        },
        {
            "authors": [
                "LR Izquierdo",
                "SS Izquierdo",
                "WH Sandholm"
            ],
            "title": "Evodyn-3s: a mathematica computable document to analyze evolutionary dynamics in 3-strategy games",
            "venue": "SoftwareX",
            "year": 2018
        },
        {
            "authors": [
                "MJM Jansen"
            ],
            "title": "Regularity and stability of equilibrium points of bimatrix games",
            "venue": "Math Op Res",
            "year": 1981
        },
        {
            "authors": [
                "AP Jurg",
                "MJM Jansen",
                "T Parthasarathy",
                "SH Tijs"
            ],
            "title": "On weakly completely mixed bimatrix games",
            "venue": "Linear Algebra Appl",
            "year": 1990
        },
        {
            "authors": [
                "I Kaplansky"
            ],
            "title": "A contribution to von neumann\u2019s theory of games",
            "venue": "Ann Math 46(3):474\u2013479",
            "year": 1945
        },
        {
            "authors": [
                "M Kleshnina"
            ],
            "title": "Evolutionary games under incompetence and foraging strategies of marine bacteria",
            "venue": "Ph.D. thesis, The University of Queensland",
            "year": 2019
        },
        {
            "authors": [
                "M Kleshnina",
                "JA Filar",
                "V Ejov",
                "JC McKerral"
            ],
            "title": "Evolutionary games under incompetence",
            "venue": "J Math Biol",
            "year": 2018
        },
        {
            "authors": [
                "M Kleshnina",
                "JC McKerral",
                "C Gonzalez-Tokman",
                "JA Filar",
                "JG Mitchell"
            ],
            "title": "Shifts in evolutionary balance of microbial phenotypes under environmental changes",
            "venue": "bioRxiv",
            "year": 2020
        },
        {
            "authors": [
                "M Kleshnina",
                "SS Streipert",
                "JA Filar",
                "K Chatterjee"
            ],
            "title": "Prioritised learning in snowdrift-type games",
            "year": 2020
        },
        {
            "authors": [
                "M Kleshnina",
                "SS Streipert",
                "JA Filar",
                "K Chatterjee"
            ],
            "title": "Mistakes can stabilise the dynamics of rockpaper-scissors games",
            "venue": "PLOS Comput Biol",
            "year": 2021
        },
        {
            "authors": [
                "N Komarova",
                "P Niyogi",
                "M Nowak"
            ],
            "title": "The evolutionary dynamics of grammar acquisition",
            "venue": "J Theor Biol",
            "year": 2001
        },
        {
            "authors": [
                "G Lambert",
                "S Vyawahare",
                "R Austin"
            ],
            "title": "Bacteria and game theory: the rise and fall of cooperation in spatially heterogeneous environments. Interface Focus",
            "year": 2014
        },
        {
            "authors": [
                "Levin S (2003) Complex adaptive systems"
            ],
            "title": "exploring the known, the unknown and the unknowable",
            "venue": "Bullet Am Math Soc 40(1):3\u201319 264 Dynamic Games and Applications",
            "year": 2023
        },
        {
            "authors": [
                "XY Li",
                "C Pietschke",
                "S Fraune",
                "P Altrock",
                "T Bosch",
                "A Traulsen"
            ],
            "title": "Which games are growing bacterial populations playing",
            "venue": "J Royal Soc Interface",
            "year": 2015
        },
        {
            "authors": [
                "LiebermanE",
                "Hauert C",
                "NowakMA"
            ],
            "title": "Evolutionary dynamics on graphs",
            "venue": "Nature",
            "year": 2005
        },
        {
            "authors": [
                "R McKelvey",
                "J Apaloo"
            ],
            "title": "The structure and evolution of competition-organized ecological communities",
            "venue": "Rocky Mt J Math 25(1):417\u2013436",
            "year": 1995
        },
        {
            "authors": [
                "R McKelvey",
                "T Palfrey"
            ],
            "title": "Quantal response equilibria for normal form games",
            "venue": "Games Econ Behav",
            "year": 1995
        },
        {
            "authors": [
                "J Mitchell"
            ],
            "title": "The influence of cell size on marine bacterial motility and energetics",
            "venue": "Microb Ecol",
            "year": 1991
        },
        {
            "authors": [
                "P Moran",
                "P Alfred"
            ],
            "title": "The statistical processes of evolutionary theory. The statistical processes of evolutionary theory",
            "year": 1962
        },
        {
            "authors": [
                "J Nash"
            ],
            "title": "Non-cooperative games",
            "venue": "Annals Math 54(1):286\u2013295",
            "year": 1951
        },
        {
            "authors": [
                "M Nowak"
            ],
            "title": "Evolutionary dynamics: exploring the equations of life",
            "year": 2006
        },
        {
            "authors": [
                "M Nowak",
                "N Komarova",
                "P Niyogi"
            ],
            "title": "Evolution of universal grammar",
            "year": 2001
        },
        {
            "authors": [
                "NowakMA",
                "Sigmund K"
            ],
            "title": "Evolutionary dynamics of biological games",
            "year": 2004
        },
        {
            "authors": [
                "MA Nowak",
                "CE Tarnita",
                "T Antal"
            ],
            "title": "Evolutionary dynamics in structured populations",
            "venue": "Philos Trans Royal Soc B: Biol Sci",
            "year": 2010
        },
        {
            "authors": [
                "M Perc",
                "J G\u00f3mez-Gardenes",
                "A Szolnoki",
                "LM Flor\u00eda",
                "Y Moreno"
            ],
            "title": "Evolutionary dynamics of group interactions on structured populations: a review",
            "venue": "J Royal Soc Interface",
            "year": 2013
        },
        {
            "authors": [
                "R Selten"
            ],
            "title": "Reexamination of the perfectness concept for equilibrium points in extensive games",
            "venue": "Int J Game Theory",
            "year": 1975
        },
        {
            "authors": [
                "R Selten"
            ],
            "title": "Evolution, learning and economic behavior",
            "venue": "Games Econ Behav",
            "year": 1991
        },
        {
            "authors": [
                "LS Shapley",
                "RN Snow"
            ],
            "title": "Basic solutions of discrete games",
            "year": 1952
        },
        {
            "authors": [
                "J Smith"
            ],
            "title": "Evolution and the theory of games",
            "year": 1982
        },
        {
            "authors": [
                "J Smith",
                "G Price"
            ],
            "title": "The logic of animal conflict",
            "venue": "Nature",
            "year": 1973
        },
        {
            "authors": [
                "P Stadler",
                "P Schuster"
            ],
            "title": "Mutation in autocatalytic reaction networks",
            "venue": "J Math Biol",
            "year": 1992
        },
        {
            "authors": [
                "R Stocker"
            ],
            "title": "Marine microbes see a sea of gradients",
            "year": 2012
        },
        {
            "authors": [
                "CE Tarnita",
                "T Antal",
                "MA Nowak"
            ],
            "title": "Mutation-selection equilibrium in games with mixed strategies",
            "venue": "J Theor Biol",
            "year": 2009
        },
        {
            "authors": [
                "C Taylor",
                "D Fudenberg",
                "A Sasaki",
                "MA Nowak"
            ],
            "title": "Evolutionary game dynamics in finite populations",
            "venue": "Bullet Math Biol",
            "year": 2004
        },
        {
            "authors": [
                "P Taylor",
                "L Jonker"
            ],
            "title": "Evolutionary stable strategies and game dynamics",
            "venue": "Math Biosci",
            "year": 1978
        },
        {
            "authors": [
                "TilmanAR",
                "JB Plotkin",
                "E Ak\u00e7ay"
            ],
            "title": "Evolutionary gameswith environmental feedbacks",
            "venue": "Nat Commun",
            "year": 2020
        },
        {
            "authors": [
                "A Traulsen",
                "JC Claussen",
                "C Hauert"
            ],
            "title": "Coevolutionary dynamics: from finite to infinite populations",
            "venue": "Phys Rev Lett",
            "year": 2005
        },
        {
            "authors": [
                "A Traulsen",
                "C Hauert"
            ],
            "title": "Stochastic evolutionary game dynamics",
            "venue": "Rev Nonlin Dyn Complex",
            "year": 2009
        },
        {
            "authors": [
                "A Traulsen",
                "N Shoresh",
                "MA Nowak"
            ],
            "title": "Analytical results for individual and group selection of any intensity",
            "venue": "Bullet Math Biol",
            "year": 2008
        },
        {
            "authors": [
                "F Weissing"
            ],
            "title": "Evolutionary stability and dynamic stability in a class of evolutionary normal form games In Game Equilibrium Models I",
            "year": 1991
        },
        {
            "authors": [
                "JS Weitz",
                "C Eksin",
                "K Paarporn",
                "SP Brown",
                "WC Ratcliff"
            ],
            "title": "An oscillating tragedy of the commons in replicator dynamics with game-environment feedback",
            "venue": "Proc Natl Acad Sci 113(47):E7518\u2013E7525",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Keywords Incompetence \u00b7 Execution errors \u00b7 Learning \u00b7 Game theory \u00b7 Stochastic games \u00b7 Evolutionary games\nThe authors would like to acknowledge partial support from the Australian Research Council under the Discovery grant DP180101602 and support by the European Union\u2019s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie Grant Agreement #754411.\nThis article is part of the topical collection \u201cMulti-agent Dynamic Decision Making and Learning\u201d edited by Konstantin Avrachenkov, Vivek S. Borkar and U. Jayakrishnan Nair.\nB Jerzy A. Filar j.filar@uq.edu.au\nThomas Graham t.graham2@uqconnect.edu.au\nMaria Kleshnina maria.kleshnina@iast.fr\n1 School of Mathematics and Physics, University of Queensland, Brisbane, Australia\n2 Institute of Science and Technology Austria (IST Austria), Klosterneuburg, Austria\n3 Institute for Advanced Studies in Toulouse, Toulouse, France"
        },
        {
            "heading": "1 Introduction",
            "text": "In classical non-cooperative games, the payoffs (or outcomes) are determined directly by the players\u2019 choice of strategies. In reality, however, a player may not be capable of executing his or her chosen strategy due to a lack of skill that we shall refer to as incompetence.\nIn this paper, we survey a relatively recent line of research that is predicated on the assumption that player incompetence is a real and ubiquitous phenomenon that deserves deeper investigation. In the process, we also present a few recent results which, to the best of our knowledge, have not been reported elsewhere. Since we regard the topic of incompetent games as still being in its infancy, the main objective of this paper is to stimulate further research on this subject.\nNaturally, players\u2019 incompetence inherently complicates a game. To prevent the added complexity from becoming unmanageable, we must impose some assumptions on the information domain and the structural form via which incompetence manifests itself. In the development so far, the following key conceptual assumption has been imposed:\n[A1] Incompetencemanifests itself as a set of probability distributions on sets of actions available to one or more players.\nWhile [A1] is restrictive in some ways, it allows us to recover a classical competent game as a special case of an incompetent game. Namely, a game where all of the above incompetence distributions are degenerate and execute the selected actions with certainty.\nThis immediately raises the possibility of parametrising the level of competence or, equivalently, level of skill of a player by the \u2018proximity\u2019 to such a, fully competent, degenerate distribution. It also opens up the possibility of players \u2018learning\u2019 by reducing their levels of incompetence (equivalently, increasing their skill). We note that this kind of learning is essentially different from both the discovery statistical learning and the imitation machine learning.\nTo date, the topic of incompetent games has evolved along two distinct, but conceptually related, directions. The first of these is the study of classical non-cooperative games under the assumption that at least one player is incompetent. The second is the study of evolutionary incompetent games.\nIn the case of classical games under incompetence, existing analyses assumed that all probability distribution capturing incompetence are mutually known by all players. This is plausible in the case of players who are familiar with others\u2019 past performance (e.g. tennis players on the international tour circuit). However, there is certainly scope for relaxing that assumption. Such relaxations may give rise to interesting repeated versions of these games and the natural trade-off between the so-called problem of \u201cexploration versus exploitation\".\nThe preceding \u201cmutually known\" assumption is not explicitly needed in the evolutionary incompetent games. It is also conceptually challenging to ascribe consciousness of such distributions to individual animals or bacteria.Nonetheless,within the evolutionary paradigm, it is reasonable to assume that the emerging equilibrium frequencies of species types have incorporated the mutual incompetence uncertainties in their adaptation to the ecosystem. The uncertainties stemming from the incompetence are thus simply built into the replicator dynamics of the game.\nThis review paper is structured as follows. We introduce incompetence first in classical non-cooperative games and then in evolutionary games. In Sect. 2, specifically in Sects. 2.1- 2.4, we provide an overview of the formal definitions and results on games with incompetent players in classical settings. Then, in Sect. 2.5 we introduce a new concept of incremental\nlearning in the games with incompetent players and derive some of its properties. In Sect. 3, we define evolutionary games with incompetence and provide an overview of results on these games. Additionally, we provide a rationale for the importance of considering these games in biological settings. We conclude by suggesting possible directions for future extensions of games with incompetence."
        },
        {
            "heading": "1.1 Incompetent Classical Non-cooperative Games",
            "text": "Chronologically, Beck and Filar [10] introduced incompetence to matrix games and Beck et al. [9] introduced incompetence to bimatrix games. Essentially, by quantifying a player\u2019s tendency to accidentally deviate from their selected actions, these authors represent incompetence as stochastic matrices that can be used to account for these deviations. The application of incompetent classical games to military planning is discussed in [8] and [7].\nWe note that the notion of incompetence introduced here is superficially similar to several concepts used to measure the sensitivity of equilibria to changes in a game\u2019s parameters. For example, Selten [63] imagines players having \u201ctrembling hands\u201d that cause them to accidentally execute unintended actions with negligible probability. This is used to refine the equilibrium solution concept in extensive-form games by defining trembling hand perfect equilibria. However, unlike the notion of incompetence, a trembling hand is not intended to model players making mistakes with arbitrary or even prescribed probabilities (e.g. a tennis player who routinely places a \u2018passing shot\u2019 in the opponent\u2019s hitting zone).\nWhile the concepts and some of the results are generalisable to N -person non-cooperative games, the setting of classic two-player games\u2014meaning matrix and bimatrix games\u2014is a natural starting point for the introduction of incompetence to game-theoretic models. Larkey et al. [45], when discussing the game \u201cSum Poker\u201d, observe that there are several cognitive and physical limitations that might prevent a player from finding or implementing optimal strategies. Then, seeking to classify the specific difficulties a player encounters, they propose a typology of skills consisting of:\n\u2013 Strategic Skill, the ability to select which games should be played, \u2013 Planning Skill, the ability to develop a desirable strategy within a game, and \u2013 Execution Skill, the ability to execute desired actions throughout a game.\nAlthoughLarkey et al. [45] apply this typology to experimentally compare different strategies in \u201cSum Poker\u201d under different skill limitations, a precise mathematical formulation of skill is not provided. The notion of incompetence mainly addresses the issue of execution skill as it quantifies accidental (ipso facto, unintended) deviations from a player\u2019s chosen strategy.\nThe concept of incompetence is a useful modelling tool in traditional game-theoretic settings because it captures a player\u2019s inability to precisely control the outcome of their actions. Necessarily, real-world strategic interactions are often complicated and a player\u2019s intentions might not be perfectly realised due to noise from their environment. For instance, a tennis player is unable to control the exact trajectory of a shot and might sometimes make consequential mistakes. Similarly, in the economic context, it is conceivable that a firm in the classic Cournot oligopoly model is unable to guarantee the quantity of goods produced, perhaps due to sporadic errors occurring during production. The latter may reflect flaws in the firm\u2019s quality control regime which in itself is related to its level of competence1. The players in these situations must accept some degree of variability in the outcomes of their actions and incompetence is a method capable of accounting for this variability.\n1 Arguably, the famous \u201cToyota Production System (TPS)\" could be seen as that firm\u2019s highly successful effort to reduce its level of incompetence.\nIn this paper, in the context of classical games, wewill review the introduction of incompetence and present several related, unpublished, results. Moreover, we will discuss and solve a simple model for incrementally learning to decrease incompetence during a repeatedlyplayed incompetent matrix game."
        },
        {
            "heading": "1.2 Incompetent Evolutionary Games",
            "text": "Game theory applied to populations of species has branched out from non-cooperative game theory and evolved into an independent field called evolutionary game theory [28,56,68]. The setup of evolutionary games differs from the classical games in the very basic assumption rationality of players. Obviously, one cannot expect rational behaviour from individual bacteria or lions. However, the selection strength still acts rationally, which follows from the classic prediction that the fittest survives [69]. Hence, in evolutionary games players, or animals, do not make conscious choice of strategies to play as humans in economic settings. Instead, individuals are born with a strategy predefined by their parents. The competition then happens at a genetic level, which encodes the strategy. Incidentally, the question of how cooperation evolves in biology is still open and, possibly, related to the coexistence of different strategies at equilibrium. We note that in a recent paper [2] thermodynamics was invoked to shed light on cooperation in evolutionary games.\nThe concept of incompetence was considered in biological settings when studying the evolution of social behaviour [36\u201340]. That is, incompetence now acts at the selection level rather than organisms themselves. Then, incompetence can be seen as behavioural plasticity leading to mixed strategies executed at a genetic level. As a result, we do not require organisms to be aware of probability distributions of all genes as well as their mistakes. Instead, selection forces act upon these distributions driving the competition among types. Such plasticity can be of variable degree, depending on the environmental conditions and adaptations of organisms. The latter will correspond to the level of incompetence discussed above.\nNaturally, the idea of behavioural plasticity or stochasticity is not novel in the field of evolutionary games and recently became one of the foci in the field. There are many approaches considered in biological settings such as genetic mutations [12,57,70,72], learning processes [23,29,41,42,51,57,64], adaptation dynamics [47], phenotypic plasticity [15], noise in continuous and discrete-time replicator dynamics [4,6,19,22] and environmental fluctuations [75,81]. Thus, the notion of incompetence of players merely fills a new niche where behavioural stochasticity is only induced at the moment of interactions.\nLet us demonstrate this concept on a well-studied example of a Hawk-Dove game [67]. Imagine that individuals in a well-mixed large population compete for some resource. Two behavioural strategies are available in a population: a Hawk (aggressive) strategy and a Dove (passive) strategy. That is, Hawks fight for the resource, while Doves prefer to share equally and flee when attacked. This game has a payoff matrix of the same structure as a Chicken or Snowdrift games.As in these classical examples, in an equilibrium, both strategies stably coexist. The game can be illustrated as an interaction between a naturally aggressive person and a naturally passive one. Of course, a counter-attack in response to aggression is not something one naturally expects from a passive person. However, behavioural plasticity induced by incompetence can lead to situations when a passive player responses aggressively or an aggressive player flees instead of fighting. While their strategies did not change as such, the behaviour exhibited by individuals was altered. In [37], it was shown that such an assumption may lead to different evolutionary outcomes and may change the way selection\nis realised. In this survey, we discuss these results and demonstrate them on an example of a biological game."
        },
        {
            "heading": "2 Incompetence in Classical Non-cooperative Games",
            "text": ""
        },
        {
            "heading": "2.1 Games without Incompetence",
            "text": "An m \u00d7 n bimatrix game \u0393 consists of a pair of action sets A = {1, 2, . . . ,m} and B = {1, 2, . . . , n} and a pair of reward matrices R1 = (r1(i, j)) \u2208 Rm\u00d7n and R2 = (r2(i, j)) \u2208 R m\u00d7n . Here, throughout this section only, we use non-standard notation for matrix entries to accommodate additional symbols associated with incompetent games. After an action i \u2208 A is selected by Player 1 and an action j \u2208 B is selected by Player 2, they receive rewards according to the matrix entries r1(i, j) and r2(i, j), respectively. A (mixed) strategy extends this behaviour to allow for randomised action selection. Specifically, Player 1 chooses a strategy from the probability simplex\nX := { x = (xi )mi=1 \u2208 Rm : m\u2211 i=1 xi = 1 and xi \u2265 0,\u2200i = 1, 2, . . . ,m }\n(1)\nover A and Player 2 chooses a strategy from the probability simplex\nY := { y = (y j )nj=1 \u2208 Rn : n\u2211 j=1 y j = 1 and y j \u2265 0,\u2200 j = 1, 2, . . . , n }\n(2)\nover B. The resulting strategy profile (x, y) \u2208 X \u00d7 Y yields an expected reward of vk(x, y) := xRkyT (3)\nto Player k \u2208 {1, 2} where yT is the transpose of y. We want to find the (Nash) equilibria of \u0393 , which capture the notion of a stable strategy profile. Precisely, (x\u2217, y\u2217) \u2208 X \u00d7 Y is an equilibrium whenever it is resilient to unilateral deviations or, equivalently, xR1(y\u2217)T \u2264 x\u2217R1(y\u2217)T and x\u2217R2yT \u2264 x\u2217R2(y\u2217)T (4) for all x \u2208 X and y \u2208 Y. Nash [54], in a seminal contribution to game theory, proves that every game with finitely-many players and actions has an equilibrium point. Vorob\u2019ev [79] shows that, in bimatrix games, the set of equilibria is the union of a finite collection of convex sets. Specifically, these are calledmaximalNash subsets and are the largest equilibrium-containing sets that are closed under interchanging a player\u2019s strategies. The extreme points of amaximal Nash subset are called extreme equilibria and are associated with paired non-singular square submatrices of R1 and R2 called kernels (see Kuhn [43]).\nA bimatrix game satisfying the zero-sum property: r1(i, j) = \u2212r2(i, j) for all i = 1, 2, . . . ,m and j = 1, 2, . . . , n, is called amatrix game and is described by the single matrix R := R1 = \u2212R2. Note that, when \u0393 is a matrix game, every equilibrium (x\u2217, y\u2217) \u2208 X \u00d7 Y achieves the same reward val(\u0393 ) := v1(x\u2217, y\u2217) = \u2212v2(x\u2217, y\u2217), which is called the value of the game \u0393 . Moreover, in recognition of von Neumann\u2019s [55] celebrated minimax theorem showing\nval(\u0393 ) = max x\u2208X miny\u2208Y xRy T = min y\u2208Y maxx\u2208X xRy T , (5)\nthe component strategies x\u2217 and y\u2217 of an equilibrium are often called (minimax) optimal strategies.\nA completely mixed equilibrium (x\u2217, y\u2217) \u2208 X\u00d7Y of a bimatrix game \u0393 is an equilibrium under which every action is playedwith non-zero probability. If\u0393 has only completelymixed equilibria, then it is called a completely mixed game and has only a single (completely mixed) equilibrium (see Kaplansky [35] and Raghavan [62]). Additionally, if \u0393 has a maximal Nash subset containing only completely mixed equilibria, then it is called a weakly completely mixed game. Jurg et al. [34] prove that a weakly completely mixed game contains a unique completely mixed equilibrium."
        },
        {
            "heading": "2.2 Games with Incompetence",
            "text": "Beck et al. [9] introduce incompetence to a bimatrix game \u0393 by allowing players to accidentally deviate from their intended actions. Specifically, after a pair of actions is selected from A \u00d7 B, incompetence randomly determines an executed action profile\u2014also from A \u00d7 B\u2014 according to a predefined probability distribution. This distribution is represented by the incompetence matrices Q1 := (q1(i, i \u2032)) \u2208 Rm\u00d7m and Q2 := (q2( j, j \u2032)) \u2208 Rn\u00d7n . After Player 1 and Player 2 select the actions i \u2208 A and j \u2208 B, they execute the actions i \u2032 \u2208 A and j \u2032 \u2208 B with probability q1(i, i \u2032) and q2( j, j \u2032), respectively. The notation \u0393Q1Q2 denotes the game \u0393 played under incompetence. If the incompetence matrices Q1 and Q2 are unambiguous, we will often replace the subscript \u201cQ1Q2\u201d with simply \u201cQ\u201d (e.g. \u0393Q instead of \u0393Q1Q2 ).\nNote that the original incompetence framework described by Beck et al. [9] allows the players\u2019 sets of selectable and executable actions to differ. This is especially useful for modelling actions that can be executed with variable quality. Beck and Filar [10] give an example of a capability acquisition game in which a defender, after selecting the action \u201cConventional Defence\u201d,may execute either \u201cGoodConventionalDefence\u201d or \u201cBadConventionalDefence\u201d. Here, for the sake of notational simplicity, we assume that a player\u2019s sets of selectable and executable actions coincide.\nSuppose that Player 1 selects the action i \u2208 A and Player 2 selects the action j \u2208 B. The expected reward received by Player k \u2208 {1, 2} under incompetence is\nrkQ(i, j) := m\u2211\ni \u2032=1 n\u2211 j \u2032=1 q1(i, i \u2032)rk(i \u2032, j \u2032)q2( j, j \u2032). (6)\nHence, \u0393Q can be treated as another bimatrix game with the incompetent reward matrix RkQ := (rkQ(i, j)) \u2208 Rm\u00d7n belonging to Player k \u2208 {1, 2}. Clearly, we have\nRkQ = Q1Rk(Q2)T . (7) Note that, as an immediate consequence of (7), an incompetent game derived from a matrix game is also a matrix game. The expected reward granted to Player k \u2208 {1, 2} in \u0393Q is\nvkQ(x, y) := xRkQyT = xQ1RkQ(Q2)T yT (8) for each (x, y) \u2208 X \u00d7 Y.\nBeck et al. [9] are not only interested in games with static incompetence, but also dynamic games wherein players are able to vary their incompetence. This is captured by a pair of learning trajectories Q1 : [0, 1] \u2192 Rm\u00d7m and Q2 : [0, 1] \u2192 Rn\u00d7n . Then, for each pair of learning parameters \u03bb,\u03bc \u2208 [0, 1], the corresponding incompetent game \u0393Q(\u03bb, \u03bc) has Player 1\u2019s incompetencematrix defined as Q1(\u03bb) and Player 2\u2019s incompetencematrix defined as Q2(\u03bc). Equivalently, \u0393Q(\u03bb, \u03bc) is the bimatrix game with reward matrices\nRkQ(\u03bb, \u03bc) := RkQ1(\u03bb)Q2(\u03bc) = Q1(\u03bb)RkQ2(\u03bc)T (9)\nfor each Player k \u2208 {1, 2}. Although the family of parameterised incompetent games \u0393Q(\u03bb, \u03bc) is interesting in its own right (see Sect. 2.4), it is also an essential building-block used to construct dynamic learning games (see Sect. 2.5).\nExample (attack-defence game with incompetence) Consider, as an example, a matrix game\u0393 played between two aeroplane pilots\u2014labelled \u201cAttacker\u201d (Player 1) and \u201cDefender\u201d (Player 2)\u2014competing over three sites. The attacker wants to destroy a site and the defender wants to prevent this from occurring. Precisely, we have the action sets A = {1, 2, 3} and B = {1, 2, 3} where, for each i, j \u2208 {1, 2, 3}, Player 1\u2019s action i means \u201cAttack Site i\u201d and Player 2\u2019s action j means \u201cDefend Site j\u201d. A successful attack occurs if and only if the defending pilot does not anticipate the attacking pilot\u2019s destination or, equivalently, the executed action profile (i, j) \u2208 A\u00d7B has i = j . The attacker receives a reward \u03bd1 = 3 when Site 1 is destroyed, \u03bd2 = 4 when Site 2 is destroyed, and \u03bd3 = 5 when Site 3 is destroyed. The corresponding utility matrix is\nR = \u239b \u239d 0 \u03bd1 \u03bd1\u03bd2 0 \u03bd2\n\u03bd3 \u03bd3 0\n\u239e \u23a0 = \u239b \u239d0 3 34 0 4 5 5 0 \u239e \u23a0 (10)\nwhere R = R1 = \u2212R2. The game value of \u0393 is 120/47 \u2248 2.55 and its (unique) equilibrium has the attacking strategy x\u2217 = 1/47( 20 15 12 ) and the defending strategy y\u2217 = 1/47( 7 17 23 ).\nWeuse incompetence to capture the pilots\u2019 navigation skills and their propensity to arrive at an incorrect site after getting lost. Define their learning trajectories Q1, Q2 : [0, 1] \u2192 R3\u00d73 where, for each \u03bb,\u03bc \u2208 [0, 1], we set\nQ1(\u03bb) = 1 3 J3(1 \u2212 \u03bb) + I3\u03bb and Q2(\u03bc) = 1 3 J3(1 \u2212 \u03bc) + I3\u03bc (11)\nwhere Jn \u2208 Rn\u00d7n is the n\u00d7n all-ones matrix and In \u2208 Rn\u00d7n is the n\u00d7n identity matrix. Note that Qk = 1/n Jn is called uniform incompetence and Qk = In is called complete competence\n[9]. The resulting incompetent reward matrices at \u03bb = \u03bc = 0 and \u03bb = \u03bc = 1/2 are\nRQ(0, 0) = 8 3 \u239b \u239d1 1 11 1 1 1 1 1 \u239e \u23a0 and RQ(1/2, 1/2) = 1 12 \u239b \u239d23 31 3037 24 35 42 41 25 \u239e \u23a0 , (12)\nrespectively. The game value of\u0393Q(0, 0) is 8/3 and the game value of\u0393Q(1/2, 1/2) is 835/324 \u2248 2.58. Moreover, since complete competence is achieved at \u03bb = \u03bc = 1 and \u0393Q(1, 1) = \u0393 , we already know that the game value of \u0393Q(1, 1) is 120/47 \u2248 2.55. Thus, it appears that the parameterised incompetent games \u0393Q(\u03bb, \u03bc) move in the defender\u2019s favour as the learning parameters are increased along \u03bb = \u03bc.\nA clearer picture of the game value\u2019s dependence on these learning parameters is achieved in Fig. 1 by plotting the function (\u03bb, \u03bc) \u2192 val(\u0393Q(\u03bb, \u03bc)) on the domain [0, 1] \u00d7 [0, 1]. Note that, in this specific example, (\u03bb, \u03bc) \u2192 val(\u0393Q(\u03bb, \u03bc)) is piecewise linear and nondecreasing (or non-increasing) in the variable \u03bb (or \u03bc). This means that learning is beneficial for the attacking and defending player when their opponent\u2019s incompetence remains fixed. Furthermore, the game value plateaus on the region [11/47, 1] \u00d7 [26/47, 1] indicating that the attacker reaches their \u201cmaximum useful skill\u2019 at \u03bb = 11/47 \u2248 0.23 and the defender reaches their \u201cmaximum useful skill\u201d at\u03bc = 26/47 \u2248 0.55. Interestingly,\u0393Q(\u03bb, \u03bc) is also completely mixed on (11/47, 1] \u00d7 (26/47, 1], which suggests a connection between complete mixedness and this game value plateau. We will further explore the general properties of parameterised incompetent games in Sect. 2.4."
        },
        {
            "heading": "2.3 Executable Strategies",
            "text": "Although [9] and [10] view incompetence as modifying a player\u2019s reward matrix, it is also possible to view it as modifying their strategy spaces. Here, we return to the setting of a static incompetent game \u0393Q with incompetence matrices Q1 \u2208 Rm\u00d7m and Q2 \u2208 Rn\u00d7n . Note that, after Player 1 selects a strategy x \u2208 X (or Player 2 selects a strategy y \u2208 Y), the resulting executed strategy is xQ1 (or yQ2) after incompetence is included. What strategies are the players able to execute? Well, Player 1 and Player 2 are able to execute the strategies in E1(Q1) := {xQ1 : x \u2208 X} and E2(Q2) := {yQ2 : y \u2208 Y}, (13) respectively.We callEk(Qk) the executable strategy spacebelonging to Player k \u2208 {1, 2} and, when the incompetence matrices are unambiguous, we simply write Ek instead. Importantly, from the perspective of an outside observerwhoonly sees that players have executed strategies from E1 and E2, we would be unable to distinguish whether they are playing the competent game \u0393 or the incompetent game \u0393Q . Figure 2 shows some of the executable strategy spaces within the previously discussed attack-defence game with incompetence. Note that the transition between executable strategy spaces can bemore complicated than the \u201cgrowing\u201d seen in Fig. 2.\nWhat is the connection between equilibria of \u0393Q inX\u00d7Y and equilibria of \u0393 inE1\u00d7E2? Theorem 1 gives conditions under which an equilibrium in the competent game \u0393 can be converted into an equilibrium in the incompetent game \u0393Q , and vice versa. Theorem 1(i) implies that an equilibrium of \u0393 in E1 \u00d7 E2 is always executed by an equilibrium of \u0393Q . Meanwhile, Theorem 1(ii) implies that there exists an equilibrium of \u0393 in the interior of E1 \u00d7 E2 provided that \u0393Q is weakly completely mixed. Lemma 1 If \u0393Q is a weakly completely mixed incompetent bimatrix game, then its incompetence matrices Q1 and Q2 are non-singular.\nProof Let q1(i) denote the i th row of Q1. Assuming that Q1 is singular, there exists I \u2286 {1, 2, . . . ,m} such that \u2211\ni\u2208I \u03b8iq1(i) = 0\nfor some non-zero coefficients \u03b8i \u2208 Rwith i \u2208 I . Then, after right-multiplying by the all-ones row vector 1Tm \u2208 Rm , we have \u2211 i\u2208I \u03b8i = 0. Certainly, since the entries of Q1 are nonnegative, we can partition the index the set I into non-empty subsets I+ = {i \u2208 I : \u03b8i > 0} and I\u2212 = {i \u2208 I : \u03b8i < 0} with \u2211i\u2208I+ \u03b8i = \u2212\u2211i\u2208I\u2212 \u03b8i .\nLet (x\u2217, y\u2217) \u2208 X \u00d7 Y be an equilibrium of \u0393Q . Define a constant \u03b1 = \u2212mini\u2208I\u2212{x\u2217i/\u03b8i} such that x\u2217i + \u03b1\u03b8i \u2265 0 for all i \u2208 I and x\u2217j + \u03b1\u03b8 j = 0 for some j \u2208 I . We construct an alternative strategy x\u2020 \u2208 X, which is also completely mixed, where\nx\u2020i = { x\u2217i + 12\u03b1\u03b8i , i \u2208 I , x\u2217i , i /\u2208 I ,\nfor each i = 1, 2, . . . ,m. Observe that\nx\u2020Q1 = m\u2211 i=1 x\u2020i q 1(i) = \u2211 i /\u2208I x\u2217i q1(i) + \u2211 i\u2208I ( x\u2217i + \u03b1 2 \u03b8i ) q1(i)\n= m\u2211 i=1 x\u2217i q1(i) + \u03b1 2 \u2211 i\u2208I \u03b8iq1(i) = x\u2217Q1,\nso x\u2217 and x\u2020 result in identical expected rewards to Player 1 and Player 2 in \u0393Q and (x\u2020, y\u2217) is an equilibrium of \u0393Q . But, given that \u0393Q contains two distinct completely mixed equilibria (x\u2217, y\u2217) and (x\u2020, y\u2217), it cannot be a weakly completely mixed game. After using a similar argument for the other incompetence matrix Q2, the desired result follows by contraposition.\nTheorem 1 Fix a strategy profile (x\u2217, y\u2217) \u2208 X \u00d7 Y in an incompetent bimatrix game \u0393Q. Then,\n(i) (x\u2217, y\u2217) is an equilibrium in \u0393Q whenever (x\u2217Q1, y\u2217Q2) is an equilibrium in \u0393 , and (ii) (x\u2217Q1, y\u2217Q2) is a (completely mixed) equilibrium in \u0393 whenever \u0393Q is weakly com-\npletely mixed and (x\u2217, y\u2217) is a completely mixed equilibrium in \u0393Q.\nProof (i) Assume that Player 1 possesses a profitable deviation from (x\u2217, y\u2217) in \u0393Q such that v1Q(x, y\n\u2217) > v1Q(x\u2217, y\u2217) for some x \u2208 X. Then, observe that v1 ( x\u2217Q1, y\u2217Q2\n) = x\u2217Q1R1(Q2)T (y\u2217)T = x\u2217R1Q(y\u2217)T < xR1Q(y\n\u2217)T = xQ1R1Q2(y\u2217)T = v1(xQ1, y\u2217Q2) which contradicts the equilibrium inequalities for (x\u2217Q1, y\u2217Q2) in \u0393 . After repeating a similar argument for Player 2, we obtain\nv1Q ( x, y\u2217 ) \u2264 v1Q(x\u2217, y\u2217) and v2Q(x\u2217, y) \u2264 v2Q(x\u2217, y\u2217), meaning that (x\u2217, y\u2217) is an equilibrium of \u0393Q .\n(ii) We know that Player 1\u2019s strategy x\u2217 makes Player 2 indifferent between their actions in \u0393Q , hence\nx\u2217R2Q = x\u2217Q1R2(Q2)T = v2 ( x\u2217, y\u2217 ) 1Tn ,\nwhere 1n \u2208 R1\u00d7n is an all-ones row vector. Clearly, this is solved when x\u2217Q1R2 = v2(x\u2217, y\u2217)1Tn and this solution is unique as Q2 is non-singular (by Lemma 1). This shows that x\u2217Q1 makes Player 2 indifferent between their actions in \u0393 and, by a similar argument, y\u2217Q2 makes Player 1 indifferent between their actions in \u0393 . Note that x\u2217Q1 and y\u2217Q2 are both completely mixed because the entries in x\u2217 and y\u2217 are strictly positive and (by nonsingularity) the columns of Q1 and Q2 cannot contain only zeros. Thus, appealing to the indifference principle, we conclude that (x\u2217Q1, y\u2217Q2) is an equilibrium in \u0393 .\nCorollary 1, which states that a completely mixed matrix game achieves the same game value as its competent counterpart, was originally presented by Beck and Filar [10]. Although they give a utility-centred argument based on Shapley and Snow\u2019s [66] game value formula, we give an alternative strategy-centred argument based on Theorem 1(ii). Note that, a generalisation of this result to bimatrix games is presented in [9]; however, we still choose to highlight the matrix game version for later discussion.\nCorollary 1 [10] If \u0393Q is a completely mixed incompetent matrix game, then \u0393 is a matrix game and val(\u0393 ) = val(\u0393Q). Proof Certainly,\u0393 is also amatrix game because R1 = \u2212R2 directly implies R1Q = \u2212R2Q . If (x\u2217, y\u2217) \u2208 X\u00d7Y is the unique equilibriumof\u0393Q , thenTheorem1(ii) states that (x\u2217Q1, y\u2217Q2) is an equilibrium of \u0393 . So,\nval ( \u0393 ) = v(x\u2217Q1, y\u2217Q2) = x\u2217Q1R(Q2)T (y\u2217)T = x\u2217RQ(y\u2217)T = vQ ( x\u2217, y\u2217\n) = val(\u0393Q), as desired.\nLastly, the result in Theorem 1(ii) can be extended to incompetent bimatrix games that are \u201calmost\u201d weakly completely mixed. Theorem 2 shows that, if \u0393Q can be approximated by a sequence of weakly completely mixed incompetent games, then \u0393 has an equilibrium in E1 \u00d7 E2. Lemma 2 If \u0393Q is a weakly completely mixed incompetent bimatrix game, then \u0393 is also weakly completely mixed.\nProof We know from Theorem 1(ii) that there exists a completely mixed equilibrium (x\u2217, y\u2217) \u2208 E1 \u00d7 E2 of \u0393 , so it lies in the interior of E1 \u00d7 E2. If \u0393 is not weakly completely mixed, then there exists another equilibrium (x\u2020, y\u2020) \u2208 X\u00d7Y such that (x\u2217, y\u2217) and (x\u2020, y\u2020) belong to the same maximal Nash subset. Define the convex combination (x\u03b1, y\u03b1) of these strategy profiles by\nx\u03b1 = \u03b1x\u2217 + (1 \u2212 \u03b1)x\u2020 and y\u03b1 = \u03b1y\u2217 + (1 \u2212 \u03b1)y\u2020\nfor each \u03b1 \u2208 [0, 1]. Note that, because Nash subsets are closed under convex combinations, (x\u03b1, y\u03b1) is an equilibrium of \u0393 for every \u03b1 \u2208 [0, 1]. Moreover, for some \u03b1\u2217 \u2208 (0, 1], the strategy profile (x\u03b1, y\u03b1) lies in the interior of E1 \u00d7 E2 for every \u03b1 \u2208 [0, \u03b1\u2217). But, by Theorem 1(i) and Lemma 1, this means that (x\u03b1(Q1)\u22121, y\u03b1(Q2)\u22121) is a (completely mixed) equilibrium of \u0393Q for each \u03b1 \u2208 [0, \u03b1\u2217). Given that this contradicts the uniqueness of a completely mixed equilibrium in the weakly completely mixed game \u0393Q , we conclude that \u0393 must also be weakly completely mixed. Theorem 2 Let {Q1 }\u221e =1 and {Q2 }\u221e =1 be sequences of incompetence matrices that converge to Q1 and Q2, respectively. Moreover, assume that\u0393Q = \u0393Q1 Q2 is weakly completely mixed for every = 1, 2, . . .. Then, there exists an equilibrium (x\u2217, y\u2217) in \u0393Q = \u0393Q1Q2 such that (x\u2217Q1, y\u2217Q2) is a (completely mixed) equilibrium in \u0393 .\nProof Take the sequences of strategies {x\u2217 }\u221e =1 \u2282 X and {y\u2217 }\u221e =1 \u2282 Y such that, for each = 1, 2, . . ., the strategy profile (x\u2217 , y\u2217 ) is the unique completely mixed equilibrium of\u0393Q . Moreover, let (x\u2020, y\u2020) \u2208 E1 \u00d7 E2 be the unique completely mixed equilibrium of \u0393 , which we know to be weakly completely mixed by Lemma 2. Then, applying Theorem 1, we have x\u2217 Q1 = x\u2020 and y\u2217 Q2 = y\u2020 for each = 1, 2, . . ..\nNote that, because the strategy spaces X and Y are compact, there exists subsequences {x\u2217 s }\u221es=1 and {y\u2217 t }\u221et=1 that converge to some strategies x\u2217 \u2208 X and y\u2217 \u2208 Y, respectively. Clearly,\nx\u2217Q1 = lim s\u2192\u221e x \u2217 s Q1 s = x\u2020 and y\u2217Q2 = limt\u2192\u221e y \u2217 t Q2 t = y\u2020.\nThis shows that (x\u2217Q1, y\u2217Q2) is a completely mixed equilibrium of \u0393 and, by Theorem 1(i), (x\u2217, y\u2217) is an equilibrium of \u0393Q , as required."
        },
        {
            "heading": "2.4 Variational Properties",
            "text": "Now, we return to the dynamic setting where \u0393Q(\u03bb, \u03bc) denotes a family of incompetent games parameterised by a pair of learning trajectories. A central focus in the development of incompetence has been the variational properties of \u0393Q(\u03bb, \u03bc) when \u0393 is a matrix game or a bimatrix game. Here, we will summarise what is known about the behaviour of these incompetent games under variations in the players\u2019 learning parameters.\nBeck et al. [9] study the dependence of equilibrium-induced expected rewards on the players\u2019 learning parameters. They present Theorem 3 and Theorem 4 showing that, under certain conditions on Q1(\u03bb) and Q2(\u03bc), the expected rewards granted by a specific extreme equilibrium have useful representations.\nTheorem 3 [9] Assume that Q1(\u03bb) and Q2(\u03bc) are linear, that is,\nQ1(\u03bb) = (1 \u2212 \u03bb)Q1(0) + \u03bbQ1(1) and Q2(\u03bc) = (1 \u2212 \u03bc)Q2(0) + \u03bcQ2(1) (14)\nfor all \u03bb,\u03bc \u2208 [0, 1]. Fix \u039b, M \u2282 [0, 1] such that \u0393Q(\u03bb, \u03bc) share a h1 \u00d7 h2 Shapley-Snow kernel for all (\u03bb, \u03bc) \u2208 \u039b \u00d7 M. Then, for some constants \u03b1ki j , \u03b2ki j \u2208 R, the expected reward to Player k \u2208 {1, 2} achieved by the kernel\u2019s associated extreme equilibrium in \u0393Q(\u03bb, \u03bc) is\n\u2211hk+1 i=1 \u2211hk+1 j=1 \u03b1ki j\u03bbh\nk\u2212i+1\u03bchk\u2212 j+1\u2211hk i=1 \u2211hk j=1 \u03b2ki j\u03bbh k\u2212i\u03bchk\u2212 j (15)\nfor each (\u03bb, \u03bc) \u2208 \u039b \u00d7 M; a ratio of bivariate polynomials in \u03bb and \u03bc.\nTheorem 4 [9] Assume that Q1(\u03bb) and Q2(\u03bc) are linear with initially uniform incompetence Q1(0) = 1/mJm (or Q2(0) = 1/n Jn). Then, the dependence of an extreme equilibrium\u2019s expected reward in (15) is (at most) linear in \u03bb (or \u03bc).\nFurthermore, in addition to proving specialisations of Theorem3 andTheorem4 formatrix games, Beck and Filar [10] establish several other properties regarding the game value of a parameterised incompetent matrix game \u0393Q(\u03bb, \u03bc). Specifically, they prove that the function (\u03bb, \u03bc) \u2192 val(\u0393Q(\u03bb, \u03bc)) is continuous and not-necessarily monotone in \u03bb and \u03bc. It is also shown that a player can never achieve a greater reward than under complete competence; that is,\nval ( \u0393Q1(\u03bb),In ) \u2264 val(\u0393Q1(\u03bb),Q2(\u03bc)) \u2264 val(\u0393Im ,Q2(\u03bc)) (16) for all \u03bb,\u03bc \u2208 [0, 1]. Beck and Filar [10] also briefly address the plateauing game values of some parameterised incompetent matrix games (see, for example, Fig. 1) by noting that Corollary 1 might apply when a player approaches complete competence. The tools developed in Sect. 2.3 allow us to further explore this observation. Consider the set of learning parameters C := {(\u03bb, \u03bc) \u2208 [0, 1] \u00d7 [0, 1] : \u0393Q(\u03bb, \u03bc) is completely mixed} (17) on which \u0393Q(\u03bb, \u03bc) is completely mixed. Assume that the learning trajectories Q1(\u03bb) and Q2(\u03bc) are continuous. Then, given that the set of reward matrices belonging to completely mixed matrix games is open (see Jansen [33]), the set C is also open. Theorem 2 shows that, for each (\u03bb, \u03bc) \u2208 C, the players are both able to execute a completely competent optimal strategy in \u0393Q(\u03bb, \u03bc). This means that, by an identical argument to Corollary 1, the function (\u03bb, \u03bc) \u2192 val(\u0393Q(\u03bb, \u03bc)) is constant on C. Hence, we expect a game value plateau to emerge whenever \u0393Q(\u03bb, \u03bc) becomes completely mixed."
        },
        {
            "heading": "2.5 Incremental Learning",
            "text": "Next, we will demonstrate a simple model of incremental learning in a parameterised family of incompetent matrix games \u0393Q(\u03bb, \u03bc). This incremental learning game \u0393inc is a stochastic game unfolding over an infinite time horizon T = {0, 1, 2, . . .} in which, between repeated plays of an incompetent game, the playersmay choose to increment their learning parameters through the ordered sets \u039b := {\u03bb1, \u03bb2, . . . , \u03bbM } and M := {\u03bc1, \u03bc2, . . . , \u03bcN }. It is assumed that \u03bbi < \u03bbi+1 and \u03bc j < \u03bc j+1 for each i = 1, 2, . . . , M \u2212 1 and j = 1, 2, . . . , N \u2212 1. This means that a player\u2019s skill parameter can never be decreased or, informally, that a player can halt but never reverse the process of learning. Henceforth, we simplify notation by identifying i with \u03bbi and j with \u03bc j .\nNow, we give a precise description of \u0393inc using the language and notation associated with stochastic games in [18]. The state space S := {(i, j) : i = 1, 2, . . . , M and j = 1, 2, . . . , N} (18) is chosen to index the learning parameters\u039b\u00d7M . Fix a stage t \u2208 T and a state s = (i, j) \u2208 S such that \u03bbi and \u03bc j are the learning parameters belonging to Player 1 and Player 2 at stage t .\nPlayer 1 and Player 2 (optimally) play the incompetent game \u0393Q(i, j) and are given the option to advance their learning parameters to i + 1 and j + 1, respectively. The decision to increment a learning parameter might incur a state-dependent learning cost ck(i, j) to Player k \u2208 {1, 2}. Formally, we say that the actions belonging to Player 1 and Player 2 at state s are\nA(s) := {\n{0, 1}, i = M, {0}, i = M, and B(s) := { {0, 1}, j = N , {0}, j = N , (19)\nwhere \u201c0\u201d means \u201cDon\u2019t Learn\u201d and \u201c1\u201d means \u201cLearn\u201d. If Player 1 selects a \u2208 A(s) and Player 2 selects b \u2208 B(s), then they receive the stage-t immediate rewards\nrk(s, a, b) := { val ( \u0393Q(i, j) ) \u2212 ac1(i, j), k = 1, \u2212val(\u0393Q(i, j)) \u2212 bc2(i, j), k = 2, (20)\nwhere the val(\u0393Q(i, j)) term is the reward received after optimally playing \u0393Q(i, j). Moreover, before the subsequent (t + 1)th stage, the game transition to the state (i + a, j + b) with (degenerate) transition probabilities given by\np(s\u2032|s, a, b) := { 1, s\u2032 = s + (a, b), 0, s\u2032 = s + (a, b), (21)\nfor every s\u2032 \u2208 S. The general transition structure of this game is shown in Fig. 3.\nHere, we will focus on stationary strategies, which are represented as block row vectors f = (f(s))s\u2208S for Player 1 and g = (g(s))s\u2208S for Player 2. The block f(s) = ( f (s, a))a\u2208A(s) stores the probability f (s, a) of choosing action a \u2208 A(s) and the block g(s) = (g(s, b))b\u2208B(s) stores the probability of choosing action b \u2208 B(s). The sets of stationary strategies belonging to Player 1 and Player 2 are denoted by F and G, respectively. The immediate rewards in (20) and the transition probabilities in (21) are extended to F\u00d7G by defining\nrk(s, f, g) := \u2211\na\u2208A(s) \u2211 b\u2208B(s) f (s, a)rk(s, a, b)g(s, b), (22)\nand\np(s\u2032|s, f, g) := \u2211 aA(s) \u2211 b\u2208B(s) f (s, a)p(s\u2032|s, a, b)g(s, b), (23)\nfor each (f, g) \u2208 F\u00d7G. If the stochastic process {St }\u221et=0 stores the state at each stage t \u2208 T , then it becomes a Markov chain under the dynamics induced by a strategy profile (f, g) \u2208 F \u00d7 G. We use Psfg and Esfg to denote probabilities and expectations under these dynamics with the initial state S0 = s \u2208 S. The \u03b2-discounted value (\u03b2 \u2208 [0, 1)) of (f, g) \u2208 F \u00d7 G to Player k \u2208 {1, 2} with the initial state s \u2208 S is\nvk(s, f, g) := \u221e\u2211 t=0 \u03b2 tEsfg [ rk(St , f, g) ] . (24)\nThen, (f\u2217, g\u2217) \u2208 F \u00d7 G is a (Nash) equilibrium of the incremental learning game \u0393inc whenever\nv1(s, f, g\u2217) \u2264 v1(s, f\u2217, g\u2217) and v2(s, f\u2217, g) \u2264 v2(s, f\u2217, g\u2217) (25)\nfor all s \u2208 S, f \u2208 F, and g \u2208 G. Although \u0393inc unfolds over an infinite time horizon, its transition structure admits a specialised backward induction algorithm for computing equilibria. We construct a suitable notion of \u201cpast\u201d and \u201cfuture\u201d states by finding a sequence s1, s2, . . . , sL (where L := MN ) such that \u2032 < implies p(s \u2032 | , a, b) = 0 for all distinct , \u2032 = 1, 2, . . . , L and (a, b) \u2208 A(s ) \u00d7 B(s ).\nIt is straightforward to verify that a suitable ordering exists\u2014for example, the lexicographical ordering. So, we shall assume that an ordering has been fixed and write instead of s . Lemma 3 shows that the discounted value of a strategy profile at a specific state does not depend on the \u201cpast\u201d states. This allows us to restrict the stochastic game \u0393inc to the limited state space { , + 1, . . . , L} while still being able to assess the value of strategies.\nLemma 3 Fix (f, g) \u2208 F \u00d7 G. Then, for any \u2208 {1, 2, . . . , L} and k \u2208 {1, 2}, we have\nvk( , f, g) = r k( , f, g) + \u03b2 \u2211L \u2032= +1 vk( \u2032, f, g)p( \u2032| , f, g)\n1 \u2212 \u03b2 p( | , f, g) . (26)\nProof Observe that, by conditioning on the state S1 after the first transition, the discounted value of (f, g) is\nvk( , f, g) = \u221e\u2211 t=0 \u03b2 tE fg [ rk(St , f, g) ] = E fg[rk(S0, f, g)]\n+ \u221e\u2211 t=1 L\u2211 \u2032=1 \u03b2 tE fg [ rk(St , f, g) \u2223\u2223S1 = \u2032]P fg(S1 = \u2032)\n\u2217= rk( , f, g) + \u03b2 L\u2211\n\u2032=1 p( \u2032 \u2223\u2223 , f, g) \u221e\u2211 t=0 \u03b2 tE \u2032fg [ rk(St , f, g) ]\n\u2217\u2217= rk( , f, g) + \u03b2 L\u2211\n\u2032= p( \u2032| , f, g)vk( \u2032, f, g).\nNote that the above equality \u2217= can be verified by applying the definition of rk( , f, g) and appealing to the fact that {rk(St , f, g)}\u221et=0 is a Markov chain. Similarly, the equality \u2217\u2217= holds by applying the definition of vk( \u2032, f, g). We now easily obtain (26) by rearranging to isolate the vk( , f, g) term on the left-hand side.\nNext,wewill show that a backward induction algorithmcan solve this incremental learning game by working backward through the states 1, 2, . . . , L . Fix a state \u2208 {1, 2, . . . , L \u2212 1} and define F +1 := {(f( \u2032))L \u2032= +1} and G +1 := {(g( \u2032))L \u2032= +1} to be the sets of stationary strategies over the \u201cfuture\u201d states + 1, . . . , L . Assume that we have already found f\u2217 +1 \u2208 F +1 and g\u2217 +1 \u2208 G +1 solving (25) at each state \u2032 = + 1, . . . , L . Can we extend this equilibrium to include the current state? Theorem 5 shows that it is sufficient to consider a simplified version of the equilibrium inequalities that only account for unilateral deviations at state . The sets F (f\u2217 +1) := {(f( ), f\u2217( + 1), . . . , f\u2217(L))} and G (f\u2217 +1) := {(g( ), g\u2217( + 1), . . . , g\u2217(L))} denote the spaces of stationary strategies that extend f\u2217 +1 and g\u2217 +1 to state .\nTheorem 5 The strategy profile (f\u2217 , g\u2217 ) \u2208 F (f\u2217 +1) \u00d7 G (g\u2217 +1) is an equilibrium of \u0393inc restricted to { , . . . , L} whenever\nv1( , f \u2032 , g\u2217 ) \u2264 v1( , f\u2217 , g\u2217 ) and v2( , f\u2217 , g\u2032 ) \u2264 v2( , f\u2217 , g\u2217 ) (27)\nfor all f \u2032 \u2208 F (f\u2217 +1) and g\u2032 \u2208 G (g\u2217 +1).\nProof We need to show that (f\u2217 , g\u2217 ) satisfying (27) also satisfies (25) at state . Take a pair of strategy profiles (f , g ) \u2208 F \u00d7G and (f \u2032 , g\u2032 ) \u2208 F (f\u2217 +1)\u00d7G (g\u2217 +1) such that f( ) = f \u2032( ) and g( ) = g\u2032( ). This means that f \u2032 (or g\u2032 ) is a combination of f (or g ) at and f\u2217 (or g\u2217 ) at + 1, . . . , L . So, (f \u2032 , g\u2032 ) and (f \u2032 , g\u2217 ) satisfy the equilibrium inequalities at the states + 1, . . . , L; that is, we have\nv1( \u2032, f , g\u2032 ) \u2264 v1( \u2032, f \u2032 , g\u2032 ) and v1( \u2032, f , g\u2217 ) \u2264 v1( \u2032, f \u2032 , g\u2217 )\nfor any \u2032 \u2208 { + 1, . . . , L}. Alongside the discounted value representation from Lemma 3, this gives\nv1( , f , g\u2217 ) = r1( , f , g\u2217 ) + \u03b2\n\u2211L \u2032= +1 v1( \u2032, f , g\u2217 )p( \u2032| , f , g\u2217 ) 1 \u2212 \u03b2 p( | , f , g\u2217 )\n\u2264 r 1( , f \u2032 , g\u2217 ) + \u03b2\n\u2211L \u2032= +1 v1( \u2032, f \u2032 , g\u2217 )p( \u2032| , f \u2032 , g\u2217 )\n1 \u2212 \u03b2 p( | , f \u2032 , g\u2217 ) = v1( , f \u2032 , g\u2217 )\nwhere r1( , f, g\u2217 ) = r1( , f \u2032 , g\u2217 ) and p(\u00b7| , f , g\u2217 ) = p(\u00b7| , f \u2032 , g\u2217 ) because f( ) = f \u2032( ). Finally, by (27), we obtain\nv1( , f , g\u2217 ) \u2264 v1( , f \u2032 , g\u2217 ) \u2264 v1( , f\u2217 , g\u2217 ). After repeating an analogous argument for Player 2, we see that the conditions in (27) are sufficient to ensure that that (f\u2217 , g\u2217 ) is an equilibrium of \u0393inc restricted to { , . . . , L}.\nThe useful consequence of Theorem 5 is that, by solving a \u201clocal\u201d problem at \u201cprevious\u201d state , we can extend the equilibrium (f\u2217 +1, g\u2217 +1) to create (f\u2217 , g\u2217 ). This local problem resembles a repeated game with absorbing states. Namely, if the players both choose to forego learning, then the game remains at state . Otherwise, if either of the players choose to learn, then the game transitions into a new state where the expected future rewards are fixed by (f\u2217 +1, g\u2217 +1). The rewards given to Player k \u2208 {1, 2} in this repeated game with absorbing states are\nV kab = { rk ( , a, b ) + \u03b2vk(s + (a, b), f\u2217 +1, g\u2217 +1), (a, b) = (0, 0), rk ( , a, b ) , (a, b) = (0, 0), (28)\nfor each (a, b) \u2208 A( ) \u00d7 B( ). An immediate consequence of Lemma 3 is that, for each k \u2208 {1, 2} and (f , g ) \u2208 F (f\u2217 +1) \u00d7 G (g\u2217 +1), we have\nvk( , f , g ) = 1 1 \u2212 \u03b2 p0q0 \u2211 a\u2208A( ) \u2211 b\u2208B( ) paV k abqb, (29)\nwhere p0 = 1 \u2212 p1 = f ( , 0) and q0 = 1 \u2212 q1 = g( , 0). Hence, to ensure that (f\u2217 , g\u2217 ) \u2208 F (f\u2217 +1) \u00d7 G (g\u2217 +1) satisfies the inequalities in (27), we need to solve the coupled pair of maximisation problems\n\u23a7\u23aa\u23aa\u23aa\u23a8 \u23aa\u23aa\u23aa\u23a9 p\u22170 = arg max p0\u2208[0,1]\n1 1 \u2212 \u03b2 p0q\u22170 \u2211\na\u2208A( ) \u2211 b\u2208B( ) paV 1 abq \u2217 b ,\nq\u22170 = arg max q0\u2208[0,1]\n1 1 \u2212 \u03b2 p\u22170q0 \u2211\na\u2208A( ) \u2211 b\u2208B( ) p\u2217aV 2abqb, (30)\nwhere p\u22170 = 1\u2212 p\u22170 = f \u2217( , 0) and q\u22170 = 1\u2212q\u22171 = g\u2217( , 0). Under the additional assumption that this repeated game with absorbing states is non-degenerate, the solutions are either both pure strategies (p\u22170, q\u22170 \u2208 {0, 1}) or both completely mixed strategies (p\u22170, q\u22170 \u2208 (0, 1)). The pure strategy solutions can be found by imposing restriction p0, p\u22170 , q0, q\u22170 \u2208 {0, 1} in (30); that is, by comparing the payoffs of every possible pure strategy profile. Moreover, by setting the appropriate partial derivatives (with respect to p0 and q0, respectively)\nof the functions being maximised in (30) equal to zero, we obtain\u23a7\u23aa\u23aa\u23a8 \u23aa\u23aa\u23a9 \u2211 a\u2208A( ) p\u2217aV 2a0 \u2212 p\u2217a(1 \u2212 \u03b2 p\u22170)V 2a1 = 0, \u2211\nb\u2208B( ) q\u2217b V 10b \u2212 q\u2217b (1 \u2212 \u03b2q\u22170 )V 11b = 0.\n(31)\nThe solutions to (31) with p\u22170, q\u22170 \u2208 (0, 1) give the completely mixed strategy solutions to \u0393inc. This shows that we are always able to extend (f\u2217 +1, g\u2217 +1) to an equilibrium (f\u2217 , g\u2217 ) of \u0393inc restricted to , . . . , L . Hence, since (f\u2217L , g\u2217L) where f \u2217(L, 0) = g\u2217(L, 0) = 1 is the only strategy profile available at state L , we can work backwards through the states L \u2212 1, L \u2212 2, . . . , 2, 1 and repeatedly extend it until obtaining an equilibrium (f\u2217, g\u2217) of \u0393inc.\nExample (attack-defence game with incremental learning) Lastly, recalling the attackdefence game \u0393Q(\u03bb, \u03bc) previously introduced in Sect. 2.2, suppose that the attacking and defending pilots have the option to undergo navigation training between engagements. We might model this as an incremental learning game \u0393inc in which training allows the pilots to advance their skill parameters through \u039b = {0, 1/5, 2/5, 3/5, 4/5, 1} and M = {0, 1/5, 2/5, 3/5, 4/5, 1} after paying learning costs of c1(i, j) = c2(i, j) = 1/10 at state s = (i, j) \u2208 S. Moreover, assume that the pilots have far-sighted discounted strategy valuations with a discount factor of \u03b2 = 99/100. What are the best strategies to reduce incompetence throughout this game?\nThe aforementioned backward induction algorithm produces a unique equilibrium of \u0393inc showngraphically in Fig. 4.Anode indicates a pair of learning parameters and an arc indicates a transition realised by the equilibrium. So, a vertical arrow means that only Player 1 learns, a horizontal arrow means that only Player 2 learns, a diagonal arrow means that both players learn, and a loop means that neither player learns.\nNote that, under the equilibrium shown in Fig. 4, the attacker learns until their skill reaches the interval [11/47, 1] and the defender learns until they reach the interval [26/47, 1]. We know that the underlying parametrised incompetent game \u0393Q(\u03bb, \u03bc) is completely mixed on (11/47, 1]\u00d7 (26/47, 1]. So, by the observations in Sect. 2.4, both players are able to execute completely competent optimal strategies when (\u03bb, \u03bc) \u2208 [11/47, 1] \u00d7 [26/47, 1]. This means that, once the players have achieved learning parameters within these intervals, the game value plateaus (see Fig. .1) and there is no incentive to learn further. Therefore, it is not always necessary to achieve complete competence so long as the players are able to \u201cmimic\u201d competence by executing an optimal strategy from the completely competent game \u0393 ."
        },
        {
            "heading": "3 Incompetence in Biological Populations",
            "text": "Game theory as a mathematical paradigm found applications not only in economics and behavioural studies, but also in biology. Its first application to biology was driven by the puzzling fact that animal contests rarely result in fights or serious injuries, even though contestants are sufficiently equipped to engage in an open fight [68]. It was suggested that instead of considering individuals as players who may not be rational, the selection itself could be considered as a rational force of evolution, and survival of the entire population is more important than benefits to individual members. Since then, evolutionary game theory emerged as a branch of game theory and ecological sciences studying evolution under selection pressure [28,50,56].\nRecently, the effects of environmental changes on the evolution of biological populations became one of the main foci of the field [3,26,75,81]. Since all organisms on this planet live in a dynamic environment that undergoes changes, the ability to adapt becomes key to survival. Adaptation is a process that improves survival skills and reproductive functions of species, and usually includes two components: genetic adaptation and learning. As a specific example, when a population migrates or their environmental conditions change, their responses to new environmental stimuli may differ, introducing behavioural mistakes in individuals\u2019 interactions. The concept of incompetence was proposed in [37] to address the learning aspect of the evolution of social behaviour. Under the assumption of incompetence of individuals, behaviours that were likely to be observed in the old environment, might not have the same frequency in the new environment, and as organisms adapt, theymight re-learn their previous behaviours."
        },
        {
            "heading": "3.1 Evolutionary Games",
            "text": "Naturally, game assumptions in biological settings differ from the classic games since rationality of each individual behaviour might not always be natural to assume. Consider a population of species consisting of N individual organisms. At every time step, individuals interact in a pair-wise manner, where they have to choose one action out of n distinct available actions. Outcomes of these interactions determine fitness of individuals based on the fitness matrix R \u2208 Rn\u00d7n . In the evolutionary settings, all individuals in the population obtain the same fitness matrix R, however, during the interaction, Player 1\u2019s fitness is determined by R, while Player 2\u2019s fitness is determined by RT . Furthermore, the sets of selectable and executable actions coincide for all players. Let x = (x1, . . . , xn), where xi denotes the frequency of the (pure) strategy i . We assume that in a given population, all individuals have\nthe same set of selectable actions A, fitness matrix R, and the mixed strategy of the entire population x.\nThe main focus of evolutionary games is to predict the strategy x that will be adopted by the population. Since we assume that n actions are available to each individual, the resulting mixed strategies lie in the simplex \u0394n defined by\n\u0394n = { x = (x1, . . . , xn) \u2223\u2223\u2223 n\u2211\ni=1 xi = 1, xi \u2265 0, \u2200i = 1, . . . , n\n} ,\nwhere xi = NiN with Ni being a number of individuals adopting strategy i and N being a total number of individuals in the population. Then, an evolutionary game \u0393 e can be denoted by\n\u0393 e = { R,A, x \u2208 \u0394n } . (32)\nWe say that the population adopts a pure i th strategy if all individuals are behaving as the i th type and, hence, their behavioural frequency vector is the unit basis vector ei . However, this may not always be the case. If not, we are in the case of mixed strategies x, and hence we are interested in finding a mixture x\u2217 which is a stable outcome of the evolution.\nIt was shown, that the concept of Nash equilibria is not sufficient when taking into account the evolution of populations [67]. As a result, a new equilibrium concept was proposed. The evolutionary stable strategy (ESS) ensures that population\u2019s strategy is resistant against random mutations and is defined, more precisely, below.\nDefinition 1 A mixed strategy x\u2217 is called an evolutionary stable strategy if one of the following conditions hold:\n(i) x\u2217R(x\u2217)T > yR(x\u2217)T , \u2200y \u2208 \u0394n ; (ii) if x\u2217R(x\u2217)T = yR(x\u2217)T , then x\u2217RyT > yRyT , \u2200y \u2208 \u0394n .\nHere, x\u2217R(x\u2217)T measures the frequency-dependent fitness of the entire population, given that everyone adopts strategy x, whereas yR(x\u2217)T measure fitness of a population adopting strategy y in a population of individuals using strategy x. In the long run, an ESS guarantees that selection prefers x\u2217 to any other arising strategy. Note that the ESS is a special case of a Nash equilibrium [56].\nHowever, besides equilibria, we are usually interested in how these equilibria can be reached, bringing us to the concept of evolutionary dynamics. Given that biological populations not only interact, but also reproduce, there is a need to take into account the reproduction process. The first classic evolutionary dynamics model was proposed by Taylor and Jonker in [74], and is called replicator dynamics. These dynamics assume well-mixed infinitely large populations which is, of course, a simplification. Subsequently, many new concepts of dynamics were suggested in order to capture mutations [12,57,70,72], finite size of populations and stochasticity [31,53,73,76\u201378], adaptation [13,14,17,25,58], and a population structure [5,49,59,60]. However, to date, the concept of incompetence was only considered in a classic setting of replicator dynamics. In Conclusions section, we discuss possible extensions to other forms of dynamics for incompetent games.\nReplicator dynamics captures a frequency-dependent selection, where the evolution of population\u2019s strategy depends on the current frequencies of all strategies in the population. That is, the fitness of a particular strategy is compared to the mean fitness of the entire population and is determined by the adopted strategies. With respect to a mixed strategy\nx \u2208 \u0394n , the expected fitness of a (pure) strategy i is defined by\nfi = n\u2211 j=1 x jri j = ei RxT = (Rx)i . (33)\nThe mean fitness payoff of the population is then defined by the scalar\n\u03c6 = n\u2211\ni=1 xi fi = xRxT . (34)\nThen, the dynamics of strategy i\u2019s frequency in the population is defined by\nx\u0307i = xi ( fi \u2212 \u03c6), i = 1, . . . , n, or in a matrix form,\nx\u0307i = xi (( RxT ) i \u2212 xRxT ) , i = 1, . . . , n. (35)\nIn the folk theorem of evolutionary game theory, it was shown that any equilibrium of the replicator dynamics is a Nash equilibrium of the game\u0393 e and that a strict Nash equilibrium is asymptotically stable [28]. Moreover, any ESS is an asymptotically stable equilibrium of the replicator dynamics. Hence, when considering evolutionary games, it is frequently sufficient to find equilibria of a static game \u0393 e. This simplification is useful when trying to predict how the behaviour of the game changes under the assumption that interacting individuals are incompetent. We shall next consider how incompetence changes the game setup."
        },
        {
            "heading": "3.2 Evolutionary Games under Incompetence",
            "text": "When introducing an assumption that individuals are prone to making behavioural mistakes in an evolutionary game, one can interpret suchmistakes as a formof behavioural plasticity. In some ways, this can be seen as phenotypic plasticity (for instance, in microbes). However, in application to more sophisticated organisms, behavioural plasticity need not relate to genetic background of the organism. These behavioural mistakes can be driven bymigration to a new environment or any other form of environmental change and are reflected in the incompetence matrix Q analogous to that introduced in Sect. 2.2.\nSince we assume that the entire population obtains only one fitness matrix, we also assume that the incompetence matrix is given for the entire population. Then, a new incompetent fitness matrix is determined in a similar manner to (7) as\nRQ = QRQT . (36) In line with previous sections, we assume that players\u2019 ability for improving their strategy execution is determined by some parameter. Since here we consider one population of players all of whom obtain the same measure of incompetence, we only need one incompetence parameter \u03bb \u2208 [0, 1]. Then, the incompetent fitness matrix is defined as\nR(\u03bb) := RQ(\u03bb) = Q(\u03bb)RQ(\u03bb)T . (37) Throughout this section, wemake a specific assumption on the functional form of learning.\nWe assume that Q(\u03bb) is linear and defined as\nQ(\u03bb) = (1 \u2212 \u03bb)S + \u03bbI , (38)\nwhere S is the staring level of incompetence and I is the identity matrix. When \u03bb = 1, the population does not make any execution errors and has a perfect strategy execution. Now we can define the evolutionary incompetent game as\n\u0393 eQ = { R,A, x \u2208 \u0394n, Q(\u03bb) : \u03bb \u2208 [0, 1] } . (39)\nWe can further simplify the analysis by utilising the property of replicator dynamics that it is invariant under a linear positive transformation [27]. This allows us to reduce the fitness matrix by subtracting diagonal elements of R from the corresponding columns. Mathematically speaking, such transformation can be defined as\nR\u0303 := R \u2212 dR1Tn , (40) where dR is a vector consisting of the diagonal elements of R and 1n is a vector consisting of ones. Throughout the manuscript, we shall denote any matrix R\u0303 as a canonical form of matrix R as in (40). Then, according to (33)-(35), for a new game under incompetence \u0393 eQ , we re-write the expected fitness for strategy i as\nfi (\u03bb) = n\u2211 j=1 r\u0303i j (\u03bb)x j = ei R\u0303(\u03bb)xT , (41)\nand for the mean fitness payoff of the population,\n\u03c6(\u03bb) = n\u2211\ni=1 xi fi (\u03bb) = x R\u0303(\u03bb)xT . (42)\nHence, the incompetent replicator dynamics can be written as\nx\u0307i = xi ( fi (\u03bb) \u2212 \u03c6(\u03bb)), i = 1, . . . , n. (43) In a strict sense, the new system given by (43) is a perturbed evolutionary game, and perturbations depend on the parameter \u03bb. As \u03bb tends to 1 for all i , the game under incompetence approaches the original game given by R. In the following section, we summarise the main results obtained for incompetent evolutionary games."
        },
        {
            "heading": "3.3 Equilibria Transitions",
            "text": "Here,we aremostly interested in behaviours that dynamics exhibit under changes in parameter values \u03bb given the starting level of incompetence S and the fitnessmatrix R. These behaviours may arise for different values of \u03bb and the dynamics change their behaviour at critical levels \u03bbc of \u03bb, referred to as bifurcation points, where equilibria emerge, disappear or change their stability properties.\nDefinition 2 [37] A critical value \u03bbc of the incompetence parameter is the bifurcation point of the replicator dynamics.\nUnder incompetence, behaviour of the game dynamics may exhibit several bifurcations [37]. Since by design the incompetence parameter approaches 1 when incompetence decreases, the incompetent fitness matrix R(\u03bb) is approaching the original fitness matrix R. As a result, in the limit of perfect competence, behaviour of the incompetent game approaches the behaviour of the original game. That is, there exists a maximal critical value of \u03bb, that preserves robust properties of the game. We recall this result in the following theorem.\nTheorem 6 [37] If the game R\u0303 possesses an ESS, x\u2217, and ||Q(\u03bb) \u2212 I || \u2264 \u03b4(\u03bbu), where \u03bbu = max \u03bbc is the maximal critical value of the incompetence parameter for a fixed point x\u2217, then the incompetent game R\u0303(\u03bb), when \u03bb \u2208 (\u03bbu, 1], possesses an ESS, x\u2217(\u03bb), and\nlim \u03bb\u21921\u2212 x\u2217(\u03bb) = x\u2217. (44) A natural question arises of how these bifurcation values of the incompetence parameter can be determined and the behaviour of dynamics. The larger the game (the more available strategies it has), the harder it becomes to define all possible bifurcations. However, even for an arbitrary number of strategies, we can find bifurcations of special equilibria, such as, interior equilibria or pure-strategies equilibria using analysis presented in [11]. Let us first focus on the bifurcations of the interior equilibria. Definition 3 [37] Let x\u2217 be a fixed point and \u03bbc be a bifurcation point that is also a zero of the mean fitness, namely, \u03c6(x\u2217, \u03bbc) = 0. Then, \u03bbc is a balanced bifurcation parameter value.\nThen, the point of bifurcation for an interior equilibrium can be found by considering a determinant of the incompetent fitness matrix. Lemma 4 [37] If x\u2217 is an interior fixed point, that is, x\u2217i > 0,\u2200i . Then every balanced bifurcation parameter value, \u03bbc, is also a singular point of R\u0303(\u03bb) in the sense that det(R\u0303(\u03bb)) = 0.\nNext, we recall the special canonical form of the matrix R\u0303(\u03bb) that is defined through a rank-one transformation of an incompetent fitness matrix R(\u03bb). By [24], its determinant can be written as\ndet(R\u0303(\u03bb)) = det(R(\u03bb) \u2212 dR(\u03bb)1Tn ) = (1 \u2212 1Tn R(\u03bb)\u22121dR(\u03bb)) det(R)[det(Q(\u03bb))]2. (45) Hence, critical values of the incompetence parameter can be found by finding zeroes of either det(Q(\u03bb)), or [1 \u2212 1Tn R(\u03bb)\u22121dR(\u03bb)].\nIn a special case of a rock-paper-scissors game [40], stability of the interior equilibrium is determined by the sign of the determinant of the fitness matrix [80], which gives rise to three cases: (a) if det(R) < 0, then an unstable interior equilibrium exists resulting in a heteroclinic cycle; (b) if det(R) > 0, then such an equilibrium is a stable mixed equilibrium; (c) if det(R) = 0, then there exists a centre and periodic orbits around it.\nHowever, games R\u0303(\u03bb) and R(\u03bb) exhibit the same behaviour [27]. Since the determinant of the fitness matrix R(\u03bb) always preserves the same sign as det(R), then det(R\u0303(\u03bb)) also cannot change its sign while the interior equilibrium exists.\nDeriving a general form of equilibria depending on \u03bb is complex and depends on the form of matrices R and S. For a special case of uniform incompetence, which implies that everybody makes mistakes with the same probability 1/n, we can sometimes find a closedform expression for the interior equilibrium. A uniform incompetence can be interpreted as a form of plasticity in biological populations. For instance, phenotypic plasticity, when different types might have slight variations in the exact degree of each gene expression. We provide this result in the following theorem. Theorem 7 [40] Let x\u2217 be an interior ESS for R. Then, for \u03bb sufficiently close to 0, if the starting level of incompetence, S, is a uniform matrix, that is, si j = 1/n,\u2200i, j = 1, . . . , n, then\nx\u2217(\u03bb) = 1 \u03bb\n( x\u2217 \u2212 1 \u2212 \u03bb\nn 1n\n) (46)\nis an interior ESS for the game R\u0303(\u03bb).\nIn [11], it was shown that pure-strategy equilibrium\u2019s stability properties can be determined from the sign of the j-th column of matrix R\u0303(\u03bb). Hence, given the maximal level of incompetence, we can determine which of the vertices will be stable and when this stability will change.\nTheorem 8 [40] If\n(sl \u2212 s j )T Rs j < 0, \u2200 l = j then vertex j is a stable point of the replicator dynamics with execution errors for \u03bb \u2208 [0, \u03bbc), where \u03bbc is the smallest critical value of the incompetence parameter where r\u0303l j (\u03bbcl j ) changes its sign for some l = j .\nThis result can be generalised for any level of incompetence, where we will have to consider\n(ql \u2212 q j )T Rq j < 0, \u2200 l = j, for all levels of \u03bb, where qi is the i th row of Q(\u03bb). Generally speaking, this condition implies that for a pure strategy to be stable under incompetence, it is necessary for it to be the best response to itself given all other pure strategies.\nAs in [38], in Fig. 5, we provide some cases illustrating how equilibria stability can change as competence level of the population changes, for an example of an unstable rock-paperscissors game. In every panel, the colour-coded bar at the top indicates which of the three vertices is stable, while in the main plot we depict the interior equilibrium components as functions of \u03bb. Even in this simple game, the behaviour exhibited by the replicator dynamics in response to changing \u03bb can be very rich. As shown in these three examples, the interior equilibrium may or may not exist for different values of the incompetence parameter. Similarly, one, two, three or none of the vertices may be stable at the same time.\nAs in the case of classical games, in evolutionary settings it is natural to consider decreasing levels of incompetence, a process we called learning. Note that increasing \u03bb corresponds to greater skill level and decreasing incompetence.\nIn the evolutionary games setup, dynamic incompetencewas interpreted from twodifferent perspectives: as an environmental shift that requires adaptation from organisms before the stable equilibrium is reached and as a learning process designed to maximise the fitness of the population after the population stabilised at some equilibrium.\nBehaviour exhibited by the population dynamics when the process of learning is treated as a function of time, \u03bb(t), was considered in [36,38]. There are many options possible when choosing the functional form. So far, two functional forms of \u03bb(t) were analysed: a sigmoid and a periodic function. The sigmoid form of learning implies that organisms are capable of learning faster in the beginning of the process and slower when they reach sufficient competence. An assumption of a slowing rate for high enough levels of \u03bb is motivated by the absence of necessity to learn fast since the evolutionary stable outcome can already be reached (see Theorem 6).\nAnalysing parameters of\u03bb(t) one can determine how longwill it take for species to be fully recovered in behavioural sense and act as in the environment they are familiar with. However, while the functional form of the adaptation trajectory captures the pace and steepness of the learning process, the starting level of incompetence can be seen as ameasure of themagnitude of the changes in the environment. That is, the further the new habitat is from the previous one, the longer it may take for organisms to fully recover.\nSince natural habitats are prone to some form of regular stochasticity, in [36], it was also consideredhowperiodic environmental fluctuations due to the seasonal or daily changes affect\nthe evolutionary dynamics. It appeared that periodicity of environmental changes leads to periodic behaviour in the evolutionary dynamics as well. Specifically, if the original game possesses a stable equilibrium, then the solution of the incompetent game with periodic form of incompetence will converge to a stable periodic orbit around this stable equilibrium.\nLet us now demonstrate how the concept of incompetence can be applied to amore specific biological setting. In the next section, we formulate a game of two foraging strategies of marine bacteria and try to analyse it from the perspective of incompetence."
        },
        {
            "heading": "3.4 Bacterial Motility Game under Incompetence",
            "text": "Evolutionary game theory has been widely applied to studying the evolution of microbes. Despite their primitivism and small sizes, marine bacteria are among the most ubiquitous forms of marine organisms, playing a central role in governing health of marine ecosystems and regulating global biosphere [52]. Understanding how cells make decisions and interact has implications for both biology of bacterial communities and our exploitation of these communities [1,20,21,44,46,48]. Fundamental to nutrient competition among bacteria is the choice of motility (chemotactic) strategies. Chemotaxis\u2014the ability to sense environmental signals, and react to the stimuli accordingly\u2014has been studied since the late 1800s [16,61].\nHowever, a deterministic game theoretic approach misses an essential feature of the bacterial population dynamics: these populations and their interactions are highly stochastic. For instance, stochastic environmental fluctuations often affect ecological systems [21]. In order\nto at least partially allow for this, the concept of incompetence was applied to study foraging strategies of bacteria in [36] by incorporating behavioural stochasticity in a matrix game that captures interactions between different strategic types of microbes. The aim is to identify the most efficient strategy for given environmental conditions. We consider two possible strategies: nonmotile or chemotactic. Nonmotile bacteria cannot induce active swimming and only drift with the water flow, whereas chemotaxis allows for active choice of direction. The fitness matrix can be constructed as\nNonmotile Chemotactic( )Nonmotile 1 0 Chemotactic 1 \u2212 c + m 12 \u2212 c ,\nwhere c is the cost of swimming andm is the reward for being able to efficiently determine the direction of swimming and both parameters are normalised so that c,m \u2208 [0, 1]. Depending on the exact values of the parameters, the game might exhibit four different behaviours, as proposed in [82] in relation to the signs of matrix elements in a canonical form from (40):\n1. Nonmotile strategy dominates: for c > 1/2 and m < c; 2. Chemotactic strategy dominates: for c < 1/2 and m > c; 3. A stable mixed equilibrium exists: for c > 1/2 and m > c; 4. An unstable mixed equilibrium exists: for c < 1/2 and m < c.\nWe shall focus on two cases: when chemotactic strategy dominates nonmotile strategy and when a stable mixed equilibrium exists. The mixed equilibrium is given by\nxN = 2(m \u2212 c) 2m \u2212 1 and xC = 2c \u2212 1 2m \u2212 1 . (47)\nWhen introducing incompetence in a model, one has to take into account biological limitations of the strategies. For instance, there exist no conditions under which a nonmotile bacterium can exhibit chemotaxis because it lacks receptors and flagella required for such a strategy. However, a chemotactic bacterium can be both nonmotile and chemotactic. Hence, the starting incompetence matrix S for this example, may have the following form\nS = Nonmotile Chemotactic( )Nonmotile 1 0\nChemotactic 12 1 2\n.\nThen, the resulting induced incompetent fitness matrix R\u0303(\u03bb) is given by\nR\u0303(\u03bb) =\nNonmotile Chemotactic\u239b \u239d \u239e \u23a0Nonmotile 0 1 4 ( 4c + 2m \u2212 1 + \u03bb(2m \u2212 1) )\nChemotactic m \u2212 c 0 .\nNote that the relative fitness of chemotactic strategy is not affected by incompetence. However, the advantage of nonmotile strategy depends on the level of incompetence induced by chemotactic bacteria. By using Lemma 4, we can determine the critical value of the\nincompetence parameter, which is determined as the solution of det(R\u0303(\u03bb)) = 0 or r\u030321(\u03bbc) = 0, and given by\n\u03bbc = 1 \u2212 4c + 2m 2m \u2212 1 .\nDepending on the stability properties of the dynamics in the original game, the behaviour of equilibria under incompetence will differ. For instance, if chemotactic strategy was dominating in a fully competent game, then for \u03bb < \u03bbc both strategies will stably co-exist. If a stable mixed equilibrium existed, then for \u03bb < \u03bbc chemotactic strategy will dominate nonmotile strategy.\nAdditionally, turbulence affects life of marine bacteria [71]. Mathematically, this can be modelled via a stochastic adaptation process. Construct a stochastic learning process where each point \u03bb(t) is a random variable with distribution that is determined by the species\u2019 migration process. This assumption provides a more realistic interpretation of the species\u2019 behaviour when we take into account migration and environmental stochasticity. However, games with an ESS are well-known to be robust [11].\nWe shall compare population dynamics for two types of the learning processes: deterministic sigmoid learning and a stochastic migration process. Any learning process, either deterministic or stochastic, will lead to the ESS in terms of a species\u2019 choice (Fig. 6). Observations of the real behaviour depend on the incompetence matrix. Population\u2019s behavioural observations for different learning processes will be different depending on the learning dynamics.\nThe stochastic learning brings us to a situation where the majority of bacteria in a population are able to perform chemotaxis if chemotactic strategy was dominating (Fig. 6, left), as\nin the deterministic case. However, if there exists a stable mixed equilibrium, then dynamics under stochastic incompetence converges to the stable frequency of chemotactic bacteria as well (Fig. 6, right). This is yet different from the equilibrium of the game without incompetence.\nDue to incompetence, extinct strategies may still reappear in the behaviour of individuals as a manifestation of mistakes that cause a revival of the extinct types. This randomisation may become beneficial as a changing environment may require flexibility from individuals in their adaptation.\nEven if the adaptive peak has been reached (i.e. \u03bb = 1), behavioural randomisation may become essential in preparedness to unforeseen changes. This is supported by the existing research in stochastic phenotype switching, when bacteria perform behavioural stochasticity even in stable environments [30].\nWhen considering incompetent games, the main focus of the analysis is on where the dynamics will stabilise and whether a stable equilibrium will be reached. However, what if the change in the environment happened after the stable equilibrium was already reached? Is there an optimal way to re-learn effective strategies that is least costly in terms of fitness losses? We discuss results answering this question in the next section."
        },
        {
            "heading": "3.5 Prioritised Learning",
            "text": "When allowing for learning after the stable equilibrium is reached, the focus of the analysis is the population\u2019s need to re-learn its effective strategies in an optimal manner. Hence, in [39] the learning under incompetence was considered with respect to maximising the fitness over the learning path.\nWhen addressing learning, one needs to distinguish whether the entire population is learning with the rate \u03bb or whether each strategy has its own learning rate \u03bbi \u2208 [0, 1]. This decision depends on the specific situation under consideration. For this section, let us assume a more general case with \u03bb = (\u03bb1, . . . , \u03bbn) to define an evolutionary game under incompetence. Then, a performance measure of the learning path over fitness can be thought of as\n\u03a6C\u2217(\u03bb) = max C\n\u222b \u03c6C (\u03bb)d\u03bb,\nwhere C is a learning path that can be taken and \u03c6C (\u03bb) is the mean-fitness of the population. Note that here it is explicitly assumed that every strategy i has its own incompetence parameter \u03bbi , which implies that \u03bb = (\u03bb1, . . . , \u03bbn). Since the complexity of the problem grows with the number of strategies, this model was considered in its simplest possible setup: when only two strategies compete. That is, the fitness matrix R has the canonical form\nStrategy 1 Strategy 2( )Strategy 1 0 a Strategy 2 b 0\nand the starting level of incompetence S can be denoted as\nS = (\n\u03b7 1 \u2212 \u03b7 1 \u2212 \u03b3 \u03b3\n) .\nIf the initial game has one stable pure equilibrium, then the optimal learning will simply imply reduction of frequency of execution of the unwanted strategy. However, if the game\npossesses a mixed stable equilibrium, it is no longer obvious what the learning path should look like. Note that an interior equilibrium in a 2-strategy game has the following form p\u0302 = (a\u0302, b\u0302), where\na\u0302 := a a + b and b\u0302 := b\na + b . (48) In order to maximise the fitness of the population, it is sufficient to consider the mean\nfitness function [74], which has the following form\n\u03c6 = p\u0302Rp\u0302T = ab a + b ,\nwhich under incompetence is reduced to the analysis of two parameters\na\u0303 := \u03b7 \u2212 a\u0302 1 \u2212 \u03b7 , b\u0303 := \u03b3 \u2212 b\u0302 1 \u2212 \u03b3 . (49)\nAn important aspect stemming from thismodel is the understanding of fitness and learning advantages. Given that relative fitness of each strategy is positive, that is, a, b > 0, we say that the strategy with higher relative fitness obtains a fitness advantage.\nIn addition,we say that strategiesmayobtain a learning advantage. This concept is induced by incompetence and implies that the strategy with more variability in the behaviour, that is, with a higher probability of mistakes, has a higher potential fitness advantage which might be achieved by reducing incompetence. Hence, the lower \u03b7 or \u03b3 , the greater the learning advantage. A new parameter \u03b4 := a\u0303 \u2212 b\u0303 was defined to measure the relative strategic advantage of one strategy over another.We summarise and compare these concepts in Table 1.\nHence, since we allow for one strategy to have a relative strategic advantage over another one, the optimal learning path depends on which strategy is advantageous. This phenomenon was called prioritised learning.\nDefinition 4 We say that there exists prioritised learning for\u03a6C (\u03bb) among stepwise learning paths, if there exists C\u2217 such that one of the directions is preferable over the other. That is, \u03a6C1(\u03bb) = \u03a6C2(\u03bb), where \u03a6(\u03bb) is the fitness-over-learning depending on the direction of learning i and C1, C2 are the learning paths in directions 1 and 2, respectively.\nInterestingly, the sign of \u03b4 fully determines which strategy has to be learnt first. That is, it cannot be determined separately based on either fitness or learning advantages of strategies. The naive suggestion would be that the most advantageous skill in terms of fitness has to be learnt first. However, the strategy with lower relative strategic advantage is learnt first in the optimal learning path.\nTheorem 9 [39] The direction of the optimal learning path is determined by the sign of \u03b4: for \u03b4 > 0 the direction of Strategy 2 is optimal and for \u03b4 < 0 the direction of Strategy 1 is optimal. If \u03b4 = 0, then there is no difference in the direction of optimal learning, that is, \u03a6C1(\u03bb) = \u03a6C2(\u03bb).\nWe suggest that natural selection tries to compensate the most disrupted strategy first even if its fitness is not the highest. Nonetheless, if the fitness difference is high enough to overcome the effect of incompetence, then the optimal learning will demand that the better strategy is learned first. Another possible interpretation would be to consider the mixed equilibrium as mixed strategies used by players. Then, by learning the less-advantageous strategy first, individuals are reaching the nearest optimal mixed strategy.\nIn the next section we shall demonstrate results from three previous sections on a reduced 2-strategy game based on the foraging strategies of marine bacteria as presented in [36]."
        },
        {
            "heading": "3.6 Bacterial Motility Game and Prioritised Learning",
            "text": "Let us now assume that the population has stabilised at the mixed equilibrium defined in (47). Assume that the environmental conditions have changed leading to deviations in strategy executions for both bacterial strategies. For this, assume that the new starting incompetence matrix is defined as\nS\u0302 = Nonmotile Chemotactic( )Nonmotile 1 \u2212 1 1\nChemotactic 2 1 \u2212 2 .\nSince nonmotile bacteria can exhibit chemotactic behaviour only as a random noise, it is natural to assume that 1 \u2264 2. Furthermore, let us allow for each strategy to be learnt at a different pace as in Sect. 3.5. In order to determine the optimal path that maximises the fitness over learning, we first calculate advantages of nonmotile and chemotactic strategies from (49) as\na\u0303 = 1 \u2212 1 1 + 2m \u2212 2c 2(2m \u2212 1) and b\u0303 = 1 \u2212 2 2 + 2c \u2212 1 2(2m \u2212 1) .\nThen, the strategic advantage of nonmotile strategy over chemotactic strategy equals to\n\u03b4 = (1 \u2212 1\n1 \u2212 1 \u2212 2 2\n) + ( 2m \u2212 2c 2(2m \u2212 1) \u2212 2c \u2212 1 2(2m \u2212 1) ) .\nNote that if 1 = 2 = , then \u03b4 = 1/ > 0 and chemotactic strategy has to be learnt first in one step (see Fig. 7 (left)). Generally, chemotactic strategy has to be learnt first whenever \u03b4 > 0 or equivalently\n1 2 > 2m \u2212 2c 2c \u2212 1 ,\nwhich together with the condition 1 < 2 requires that m < 2c \u2212 12 . We plot a special case when \u03b4 = 0 in Fig. 7 (right)."
        },
        {
            "heading": "4 Conclusions and Future Extensions",
            "text": "This paper is predicated on the belief that competitions/games with incompetent agents/ players are ubiquitous in nature. Hence, formalising the notion of incompetence and modelling the impact of the resulting \u201cmistakes\" on the outcomes of games is worthy of detailed analysis. However, we first must recognise that everyday use of the word \u201cincompetence\" carries a very wide range of possible interpretations and hence needs to be narrowed down in order to be rigorously analysed.\nHence, the line of researchwe surveyed is limited to situations where incompetence can be adequately modelled via probability distributions on specified sets of actions available to one or more players (assumption [A1]). This implies that incompetence induced mistakes manifest themselves as random outcomes, different from intended outcomes. The latter certainly captures some essential characteristics of incompetence.\nHowever, in the case of classical incompetent games studied so far, assumption [A1] was augmented by a requirement that players know one another\u2019s propensity to make mistakes. This \u201cmutually known\" aspect concerning the probability distributions ofmistaken executions is certainly restrictive. For instance, it is clear that while it may approximately apply to a match between two professional tennis players at Wimbledon, it would not hold for two children playing one another. We hope that future investigations will relax this restriction."
        },
        {
            "heading": "4.1 Extensions for Classical Games",
            "text": "Currently, in the setting of classical nooncooperative game theory, incompetence has been studied mainly in matrix and bimatrix games. However, there are clearly several other, more general, classes of games to which this approach could be extended. Below, we name just four, out of many possible, generalisations.\na) Continuum of actions. Although we have only dealt with players having finitely-many actions, the concept of incompetence could be extended to games with larger action spaces, for example, games with a continuum of actions. Given a game with a continuum of actions, mixed strategies are represented by cumulative distribution functions and expected utility is computed as a Riemann-Stieltjes integral. This means that, in this context, a general \u201cincompetence-adjusted\u201d utility function (as in (8)) would also need to be expressed in an integral form. b) Incompetence dependent action spaces. While the original incompetence framework described by Beck et al. [9] allows a player\u2019s selectable and executable actions to differ, the theoretical development to date addressed only the case where they coincide. Intuitively, it is clear that there are situations where a player\u2019s incompetence may contract or expand their set of selectable actions. However, this raises the conceptual challenge of dynamically capturing the changes to these sets, as a player reduces his or her incompetence via learning. This would need to be modelled in a sufficiently general and yet technically tractable way. c) Extensions to stochastic games. In stochastic games evolving over discrete time horizon, at each stage players play one of a finite set non-cooperative games called \u201cstates\". The consequence of a single play is an immediate payoff (to each player) and a probabilistic transition to a new state (e.g. see the seminal paper [65]). Clearly, it is possible to replace each state by an incompetent non-cooperative game, thereby inducing an incompetent stochastic game. Such a generalisation would be interesting and, likely, tractable. d) Extensions to incremental learning. The incremental learning games formulated in Sect. 2.5 adopt several simplifying assumptions that could be relaxed to further extend the model. First, the assumption that a player\u2019s learning trajectory can be parameterised by a single learning parameter could be relaxed to allow for \u201cmultidirectional learning\u201d. Second, relaxing the assumption that a player\u2019s level of incompetence can never be decremented would allow the model to describe not only the process of learning, but also the process of forgetting what one has learnt."
        },
        {
            "heading": "4.2 Extensions for Evolutionary Games",
            "text": "It should be clear that the work done so far in studying incompetent evolutionary games constitutes merely a beginning. As above, in this section we briefly describe just three, out of many, possible continuations of this research.\na) Generalisations of population dynamics. First of all, choosing to work within replicator dynamics setting carries with it simplifying assumptions which open it to criticism for oversimplification of natural reproduction processes. While replicator dynamics is a classical approach to modelling the effect of natural selection, over decades of research, these assumptions were relaxed in new approaches to modelling population dynamics.\nIn particular, the effect of the finite population size and inherent stochasticity of the reproduction process were addressed in finite population dynamics likeMoran birth-death process, ability of others to imitate successful behavioural aspects of neighbours were addressed in imitation dynamics and the effect of interaction with neighbours was addressed in many different dynamics on networks. Hence, as a natural extension, one should consider how relaxed assumptions on the population dynamics affect the dynamics of games with incompetence.\nb)Generalising prioritised learning by exploiting the power of simulations. In recent years, evolutionary models adopted methods of computer simulations to facilitate exploration of more realistic complex models that would have been intractable using analytical methods.\nHence, one could extend our prioritised learning of Sect. 3.5 to allowmore than two strategies to compete at the same time. Furthermore, it is tempting to allow every individual organism their own learning parameter so as to closer approximate natural scenarios.\nWhile the complexity of such a complex model will render it intractable analytically, simulating specific setupsmay shed light onmany puzzling biological problems. For instance, the problem of determining how niches emerge and are filled by organisms while interacting under many different environmental conditions with multiple organisms.\nc) Learning as a function of frequency of strategies. One simplifying assumption we made so far in all models of incompetence applied in biology is that of the separation of the learning process from the reproduction process. However, evolution of learning or levels of incompetence might be frequency dependent, which could lead to intricate co-evolution. Setups that follow similar logic were considered in [81] and [75]. While results presented in [75] can be seen as more general, the form of the exact dynamics of the learning process in the settings of incompetence was not addressed in previous works. Hence, we believe it would be worthwhile to consider co-dependence of x(\u03bb) and \u03bb(t, x).\nAcknowledgements The authors would like to acknowledge stimulating email discussions with Dr Wayne Lobb of W.A. Lobb LLC on the topic of evolutionary games. We also thank Dr Thomas Taimre for his input to the material in Sect. 3.\nFunding Open Access funding enabled and organized by CAUL and its Member Institutions.\nData Availability The data sets and simulations discussed in this article are available from the relevant coauthor upon reasonable request.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/."
        }
    ],
    "title": "Where DoMistakes Lead? A Survey of Games with Incompetent Players",
    "year": 2022
}