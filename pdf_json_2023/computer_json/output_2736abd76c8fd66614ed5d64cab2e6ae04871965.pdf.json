{
    "abstractText": "We propose attribute-aware multimodal entity linking, where the input is a mention described with a text and image, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also described with a text description, a visual image and a set of attributes and values. To support this research, we construct AMELI, a large-scale dataset consisting of 18,472 reviews and 35,598 products. To establish baseline performance on AMELI, we experiment with the current state-of-the-art multimodal entity linking approaches and our enhanced attributeaware model and demonstrate the importance of incorporating the attribute information into the entity linking process. To be best of our knowledge, we are the first to build benchmark dataset and solutions for the attribute-aware multimodal entity linking task. Datasets and codes will be made publicly available.",
    "authors": [
        {
            "affiliations": [],
            "name": "Barry Menglong Yao"
        },
        {
            "affiliations": [],
            "name": "Yu Chen"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        },
        {
            "affiliations": [],
            "name": "Sijia Wang"
        },
        {
            "affiliations": [],
            "name": "Minqian Liu"
        },
        {
            "affiliations": [],
            "name": "Zhiyang Xu"
        },
        {
            "affiliations": [],
            "name": "Licheng Yu"
        },
        {
            "affiliations": [],
            "name": "Lifu Huang"
        }
    ],
    "id": "SP:b7edeeab58766e28dbb3223bedd0f74b82ece4a5",
    "references": [
        {
            "authors": [
                "Omar Adjali",
                "Romaric Besan\u00e7on",
                "Olivier Ferret",
                "Herv\u00e9 Le Borgne",
                "Brigitte Grau."
            ],
            "title": "Multimodal entity linking for tweets",
            "venue": "Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14\u201317,",
            "year": 2020
        },
        {
            "authors": [
                "Tom Ayoola",
                "Joseph Fisher",
                "Andrea Pierleoni."
            ],
            "title": "Improving entity disambiguation by reasoning over a knowledge base",
            "venue": "arXiv preprint arXiv:2207.04106.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Ayoola",
                "Shubhi Tyagi",
                "Joseph Fisher",
                "Christos Christodoulopoulos",
                "Andrea Pierleoni."
            ],
            "title": "Refined: An efficient zero-shot-capable approach to end-to-end entity linking",
            "venue": "arXiv preprint arXiv:2207.04108.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Ayoola",
                "Shubhi Tyagi",
                "Joseph Fisher",
                "Christos Christodoulopoulos",
                "Andrea Pierleoni."
            ],
            "title": "ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking",
            "venue": "arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Youngmin Baek",
                "Bado Lee",
                "Dongyoon Han",
                "Sangdoo Yun",
                "Hwalsuk Lee"
            ],
            "title": "Character region awareness for text detection",
            "year": 2019
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Highly Parallel Autoregressive Entity Linking with Discriminative Correction",
            "venue": "arXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Autoregressive Entity Retrieval",
            "venue": "arXiv.",
            "year": 2020
        },
        {
            "authors": [
                "Silviu Cucerzan."
            ],
            "title": "Large-scale named entity disambiguation based on Wikipedia data",
            "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages",
            "year": 2007
        },
        {
            "authors": [
                "Nicola De Cao",
                "Ledell Wu",
                "Kashyap Popat",
                "Mikel Artetxe",
                "Naman Goyal",
                "Mikhail Plekhanov",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Multilingual autoregressive entity linking",
            "venue": "Transactions of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Shahi Dost",
                "Luciano Serafini",
                "Marco Rospocher",
                "Lamberto Ballan",
                "Alessandro Sperduti."
            ],
            "title": "Vtkel: a resource for visual-textual-knowledge entity linking",
            "venue": "Proceedings of the 35th Annual ACM Symposium on Applied Computing, pages 2021\u20132028.",
            "year": 2020
        },
        {
            "authors": [
                "Jingru Gan",
                "Jinchang Luo",
                "Haiwei Wang",
                "Shuhui Wang",
                "Wei He",
                "Qingming Huang."
            ],
            "title": "Multimodal entity linking: a new dataset and a baseline",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pages 993\u20131001.",
            "year": 2021
        },
        {
            "authors": [
                "Octavian-Eugen Ganea",
                "Thomas Hofmann."
            ],
            "title": "Deep joint entity disambiguation with local neural attention",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2619\u20132629, Copenhagen, Denmark. Associa-",
            "year": 2017
        },
        {
            "authors": [
                "Octavian-Eugen Ganea",
                "Thomas Hofmann."
            ],
            "title": "Deep joint entity disambiguation with local neural attention",
            "venue": "arXiv preprint arXiv:1704.04920.",
            "year": 2017
        },
        {
            "authors": [
                "Peng Gao",
                "Shijie Geng",
                "Renrui Zhang",
                "Teli Ma",
                "Rongyao Fang",
                "Yongfeng Zhang",
                "Hongsheng Li",
                "Yu Qiao."
            ],
            "title": "CLIP-Adapter: Better VisionLanguage Models with Feature Adapters",
            "venue": "arXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Yike Guo",
                "Faisal Farooq",
                "Guineng Zheng",
                "Subhabrata Mukherjee",
                "Xin Luna Dong",
                "Feifei Li."
            ],
            "title": "OpenTag",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1049\u20131058.",
            "year": 2018
        },
        {
            "authors": [
                "Zhaochen Guo",
                "Denilson Barbosa."
            ],
            "title": "Robust named entity disambiguation with random walks",
            "venue": "Semantic Web, 9(4):459\u2013479.",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2015
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing",
            "year": 2023
        },
        {
            "authors": [
                "Johannes Hoffart",
                "Mohamed Amir Yosef",
                "Ilaria Bordino",
                "Hagen F\u00fcrstenau",
                "Manfred Pinkal",
                "Marc Spaniol",
                "Bilyana Taneva",
                "Stefan Thater",
                "Gerhard Weikum."
            ],
            "title": "Robust disambiguation of named entities in text",
            "venue": "Proceedings of the 2011 conference on empir-",
            "year": 2011
        },
        {
            "authors": [
                "Hexiang Hu",
                "Yi Luan",
                "Yang Chen",
                "Urvashi Khandelwal",
                "Mandar Joshi",
                "Kenton Lee",
                "Kristina Toutanova",
                "Ming-Wei Chang."
            ],
            "title": "Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Nikolaos Kolitsas",
                "Octavian-Eugen Ganea",
                "Thomas Hofmann."
            ],
            "title": "End-to-end neural entity linking",
            "venue": "arXiv preprint arXiv:1808.07699.",
            "year": 2018
        },
        {
            "authors": [
                "Tuan Manh Lai",
                "Heng Ji",
                "ChengXiang Zhai."
            ],
            "title": "Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking",
            "venue": "arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "PengYuan Li",
                "YongLi Wang."
            ],
            "title": "A multimodal entity linking approach incorporating topic concepts",
            "venue": "2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI), pages 491\u2013494. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Rongmei Lin",
                "Xiang He",
                "Jie Feng",
                "Nasser Zalmout",
                "Yan Liang",
                "Li Xiong",
                "Xin Luna Dong"
            ],
            "title": "PAM: Understanding Product Images in Cross Product Category Attribute Extraction; PAM: Understanding Product Images in Cross Product Category",
            "year": 2021
        },
        {
            "authors": [
                "Shilong Liu",
                "Zhaoyang Zeng",
                "Tianhe Ren",
                "Feng Li",
                "Hao Zhang",
                "Jie Yang",
                "Chunyuan Li",
                "Jianwei Yang",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection",
            "venue": "arXiv preprint arXiv:2303.05499",
            "year": 2023
        },
        {
            "authors": [
                "Lajanugen Logeswaran",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova",
                "Jacob Devlin",
                "Honglak Lee."
            ],
            "title": "Zero-shot entity linking by reading entity descriptions",
            "venue": "arXiv preprint arXiv:1906.07348.",
            "year": 2019
        },
        {
            "authors": [
                "David Milne",
                "Ian H Witten."
            ],
            "title": "Learning to link with wikipedia",
            "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, pages 509\u2013518.",
            "year": 2008
        },
        {
            "authors": [
                "Seungwhan Moon",
                "Leonardo Neves",
                "Vitor Carvalho."
            ],
            "title": "Multimodal named entity disambiguation for noisy social media posts",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2000\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Yasumasa Onoe",
                "Greg Durrett."
            ],
            "title": "Fine-grained entity typing for domain independent entity linking",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8576\u20138583.",
            "year": 2020
        },
        {
            "authors": [
                "Matthew E Peters",
                "Mark Neumann",
                "Robert L Logan IV",
                "Roy Schwartz",
                "Vidur Joshi",
                "Sameer Singh",
                "Noah A Smith."
            ],
            "title": "Knowledge enhanced contextual word representations",
            "venue": "arXiv preprint arXiv:1909.04164.",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Manoj Prabhakar Kannan Ravi",
                "Kuldeep Singh",
                "Isaiah Onando Mulang",
                "Saeedeh Shekarpour",
                "Johannes Hoffart",
                "Jens Lehmann."
            ],
            "title": "Cholan: A modular approach for neural entity linking on wikipedia and wikidata",
            "venue": "arXiv preprint arXiv:2101.09969.",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Kalyani Roy",
                "Pawan Goyal",
                "Manish Pandey."
            ],
            "title": "Attribute value generation from product title using language models",
            "venue": "Proceedings of The 4th Workshop on e-Commerce and NLP, pages 13\u201317.",
            "year": 2021
        },
        {
            "authors": [
                "\u00d6zge Sevgili",
                "Artem Shelmanov",
                "Mikhail Arkhipov",
                "Alexander Panchenko",
                "Chris Biemann."
            ],
            "title": "Neural entity linking: A survey of models based on deep learning",
            "venue": "Semantic Web, 13(3):527\u2013570.",
            "year": 2022
        },
        {
            "authors": [
                "Baoguang Shi",
                "Xiang Bai",
                "Cong Yao"
            ],
            "title": "An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition",
            "year": 2015
        },
        {
            "authors": [
                "Wenxiang Sun",
                "Yixing Fan",
                "Jiafeng Guo",
                "Ruqing Zhang",
                "Xueqi Cheng."
            ],
            "title": "Visual Named Entity Linking: A New Dataset and A Baseline",
            "venue": "arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Wenxiang Sun",
                "Yixing Fan",
                "Jiafeng Guo",
                "Ruqing Zhang",
                "Xueqi Cheng."
            ],
            "title": "Visual named entity linking: A new dataset and a baseline",
            "venue": "arXiv preprint arXiv:2211.04872.",
            "year": 2022
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "Lxmert: Learning cross-modality encoder representations from transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2019
        },
        {
            "authors": [
                "Hongyin Tang",
                "Xingwu Sun",
                "Beihong Jin",
                "Fuzheng Zhang."
            ],
            "title": "A bidirectional multi-paragraph reading model for zero-shot entity linking",
            "venue": "35th AAAI Conference on Artificial Intelligence, AAAI 2021, 15:13889\u201313897.",
            "year": 2021
        },
        {
            "authors": [
                "Aparna Nurani Venkitasubramanian",
                "Tinne Tuytelaars",
                "Marie Francine Moens."
            ],
            "title": "Entity linking across vision and language",
            "venue": "Multimedia Tools and Applications, 76:22599\u201322622.",
            "year": 2017
        },
        {
            "authors": [
                "Qifan Wang",
                "Li Yang",
                "Jingang Wang",
                "Jitin Krishnan",
                "Bo Dai",
                "Sinong Wang",
                "Zenglin Xu",
                "Madian Khabsa",
                "Hao Ma."
            ],
            "title": "Smartave: Structured multimodal transformer for product attribute value extraction",
            "venue": "Findings of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Xuwu Wang",
                "Junfeng Tian",
                "Min Gui",
                "Zhixu Li",
                "Rui Wang",
                "Ming Yan",
                "Lihan Chen",
                "Yanghua Xiao."
            ],
            "title": "WikiDiverse: A multimodal entity linking dataset with diversified contextual topics and entity types",
            "venue": "Proceedings of the 60th Annual Meeting of",
            "year": 2022
        },
        {
            "authors": [
                "Ledell Wu",
                "Fabio Petroni",
                "Martin Josifoski",
                "Sebastian Riedel",
                "Luke Zettlemoyer."
            ],
            "title": "Scalable zeroshot entity linking with dense entity retrieval",
            "venue": "arXiv preprint arXiv:1911.03814.",
            "year": 2019
        },
        {
            "authors": [
                "Huimin Xu",
                "Wenting Wang",
                "Xin Mao",
                "Xinyu Jiang",
                "Man Lan."
            ],
            "title": "Scaling up Open Tagging from Tens to Thousands: Comprehension Empowered Attribute Value Extraction from Product Title",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyang Xu",
                "Ying Shen",
                "Lifu Huang"
            ],
            "title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning",
            "year": 2022
        },
        {
            "authors": [
                "Jun Yan",
                "Nasser Zalmout",
                "Yan Liang",
                "Christan Grant",
                "Xiang Ren",
                "Xin Luna Dong."
            ],
            "title": "AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding",
            "venue": "arXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Li Yang",
                "Qifan Wang",
                "Zac Yu",
                "Anand Kulkarni",
                "Sumit Sanghai."
            ],
            "title": "MAVE: A Product Dataset for Multi-source Attribute Value Extraction",
            "venue": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 1256\u20131265.",
            "year": 2022
        },
        {
            "authors": [
                "Xiyuan Yang",
                "Xiaotao Gu",
                "Sheng Lin",
                "Siliang Tang",
                "Yueting Zhuang",
                "Fei Wu",
                "Zhigang Chen",
                "Guoping Hu",
                "Xiang Ren."
            ],
            "title": "Learning dynamic context augmentation for global entity linking",
            "venue": "arXiv preprint arXiv:1909.02117.",
            "year": 2019
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang"
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "year": 2023
        },
        {
            "authors": [
                "Li Zhang",
                "Zhixu Li",
                "Qiang Yang."
            ],
            "title": "Attentionbased multimodal entity linking with high-quality images",
            "venue": "Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11\u201314, 2021, Proceedings, Part",
            "year": 2021
        },
        {
            "authors": [
                "Renrui Zhang",
                "Wei Zhang",
                "Rongyao Fang",
                "Peng Gao",
                "Kunchang Li",
                "Jifeng Dai",
                "Yu Qiao",
                "Hongsheng Li."
            ],
            "title": "Tip-adapter: Training-free adaption of clip for few-shot classification",
            "venue": "Computer Vision\u2013 ECCV 2022: 17th European Conference, Tel Aviv, Is-",
            "year": 2022
        },
        {
            "authors": [
                "Wenzheng Zhang",
                "Wenyue Hua",
                "Karl Stratos."
            ],
            "title": "Entqa: Entity linking as question answering",
            "venue": "arXiv preprint arXiv:2110.02369.",
            "year": 2021
        },
        {
            "authors": [
                "Wenzheng Zhang",
                "Wenyue Hua",
                "Karl Stratos."
            ],
            "title": "Entqa: Entity linking as question answering",
            "venue": "arXiv preprint arXiv:2110.02369.",
            "year": 2021
        },
        {
            "authors": [
                "Qiushuo Zheng",
                "Hao Wen",
                "Meng Wang",
                "Guilin Qi",
                "Chaoyu Bai."
            ],
            "title": "Faster zero-shot multi-modal entity linking via visual-linguistic representation",
            "venue": "Data Intelligence, 4(3):493\u2013508.",
            "year": 2022
        },
        {
            "authors": [
                "Xingchen Zhou",
                "Peng Wang",
                "Guozheng Li",
                "Jiafeng Xie",
                "Jiangheng Wu."
            ],
            "title": "Weibo-mel, wikidatamel and richpedia-mel: Multimodal entity linking benchmark datasets",
            "venue": "Knowledge Graph and Semantic Computing: Knowledge Graph Empowers",
            "year": 2021
        },
        {
            "authors": [
                "Tiangang Zhu",
                "Yue Wang",
                "Haoran Li",
                "Youzheng Wu",
                "Xiaodong He",
                "Bowen Zhou."
            ],
            "title": "Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Produc",
            "venue": "arXiv.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Entity linking aims to disambiguate and link the entity mentions in text with their corresponding entities in knowledge bases. While previous studies (Onoe and Durrett, 2020; Zhang et al., 2021b; Tan and Bansal, 2019; Tang et al., 2021; Yang et al., 2019; Ganea and Hofmann, 2017a; Ravi et al., 2021; Ayoola et al., 2022a,b) mainly focus on linking entities based on text, recent researchers have started to extend it to multi-modality where both the mentions and entities in knowledge bases are described with text and visual images (Zhang et al., 2021a; Moon et al., 2018; Zhou et al., 2021; Li and Wang, 2021; Venkitasubramanian et al., 2017; Zheng et al., 2022; Dost et al., 2020; Wang et al., 2022b; Adjali et al., 2020). However, all these studies view each entity in the target knowledge base as an atomic symbol described with plain text and images while ignoring the meta-information, such as various attributes of each entity, which, we argue that, is especially important in disambiguating\nentities in a multimodal context. In this work, we focus on multimodal entity linking (MEL) which requires an understanding of finegrained attributes of the mentions from both text and images and linking them to the corresponding entities in the knowledge base that are also illustrated with a set of attributes. Figure 1 shows an example where each entity such as ASUS ROG Laptop - Eclipse Grey in the target knowledge base is described with a set of attributes, such as Screen Size, System Memory, Graphics, and in order to disambiguate and link a particular mention, e.g., ASUS laptop to the target entity, we need to carefully detect the attributes of the mention from its text and image descriptions and compare it against each entity. Such attribute-aware multimodal entity linking is especially important to e-commerce domains, e.g., analyzing user opinions from their social media posts about particular products, and yet, relatively less studied in the prior entity linking literature.\nTo support research toward attribute-aware multimodal entity linking, we introduce AMELI, which consists of (1) a multimodal knowledge base that includes 35,598 product entities collected from Best Buy1 website and each entity is described with a product name, a product description, a set of product attributes and values and several product images; and (2) a multimodal entity linking benchmark dataset that contains 18,472 data instances while each instance contains a text description for a particular entity mention and several images2. The goal is to interpret the multimodal context, especially the various attributes and values, of each entity mention, and map it to a particular entity in the multimodal knowledge base. AMELI is challenging as many entities in the knowledge base are about similar products with subtle difference in a\n1https://www.bestbuy.com/ 2The data described here was accessed, collected, hosted\nand used only by the co-authors at Virginia Tech\nar X\niv :2\n30 5.\n14 72\n5v 1\n[ cs\n.C L\n] 2\n4 M\nay 2\n02 3\nfew attributes and thus the model needs to correctly detect all the attributes from the multimodal context of each mention in order to link it to the target entity.\nWe conduct baseline experiments with several entity linking models and propose a new framework consisting of an NLI-based text model incorporating the entailment between the mention text and the entity attributes and an image model based on contrastive representation learning. Still, experimental results demonstrate a significant gap between machine (33.47%) and human performance (79.73%). The contributions of this work can be summarized as follows:\n\u2022 To the best of our knowledge, AMELI is the first benchmark dataset to support attribute-aware multimodal entity linking and we are the first to integrate attribute features to improve multimodal entity linking task.\n\u2022 We propose a new approach consisting of an NLI-based text model and a contrastiverepresentation-learning-based image model to establish baseline performance on AMELI. A further ablation study shows the benefits of incorporating attribute information in the multimodal entity linking task."
        },
        {
            "heading": "2 Related Work",
            "text": "Textual Entity Linking There is extensive research on textual entity linking (EL), like various benchmark datasets (Guo and Barbosa, 2018; Logeswaran et al., 2019; Hoffart et al., 2011; Cucerzan, 2007; Milne and Witten, 2008) and stateof-the-art neural models (Cao et al., 2021; Lai\net al., 2022; Cao et al., 2020; De Cao et al., 2022; Ayoola et al., 2022c; Zhang et al., 2021c). These methods can be divided into three groups: (Wu et al., 2019; Logeswaran et al., 2019; Ayoola et al., 2022c) train their models with the standard negative log-likelihood objective like in classification tasks. (Peters et al., 2019; Ganea and Hofmann, 2017b; Kolitsas et al., 2018) train their models with the max-margin ranking loss to enforce a positive margin between similarity scores of mentions to positive and negative candidates. And (Cao et al., 2021; Lai et al., 2022; Cao et al., 2020; De Cao et al., 2022) formulize the EL task as the generation task to generate entity name based on the mention context. Since many above methods have achieved high and similar results in the current benchmark datasets, some work (Wu et al., 2019; Logeswaran et al., 2019; Ayoola et al., 2022c) begin to explore zero-shot entity linking. In contrast, we focus on the setting where entity candidates differ in finegrained aspects and force the model to execute fine-grained reasoning in this work.\nMultimodal Entity Linking Multimodal entity linking has been recently explored in various contexts such as social media (Zhang et al., 2021a; Moon et al., 2018; Zhou et al., 2021; Li and Wang, 2021), domain-specific videos (Venkitasubramanian et al., 2017) and general news domains (Wang et al., 2022b). Works in the social media context either focus on the reduction of noise in the abundant social media visual inputs (Zhang et al., 2021a; Li and Wang, 2021). Specifically, Zhang et al. (2021a) proposes utilizing multiple attention mechanisms to overcome the influence of noise through irrelevant images. Li and Wang (2021) uti-\nlize topic clustering mechanisms to coalesce similar concepts from texts and images to filter out noise from irrelevant images. On another note, Moon et al. (2018) throw light on the limited context that the textual counterparts of the social media posts tend to be owing to very short captions associated with their image posts. Given the diverse amounts of multimodal data that need parsing, recent works focus on proposing zero-shot multimodal entity linking (Zheng et al., 2022; Moon et al., 2018). Venkitasubramanian et al. (2017) propose entity disambiguation in a video-textual context, by employing probabilistic graphical models by visualizing the text-video pair as a bipartite graph. With the emergence of the need for entity linking in a multimodal context, a lot of works propose datasets to remedy a host of issues in this space. VTKEL (Dost et al., 2020) is proposed to effectively incorporate the alignment of background knowledge with visual and textual information. Adjali et al. (2020) craft a dataset with the entities grounded in the Twitter Knowledge base. Additionally, they propose a method to jointly model image and text to model representation for entities and mentions. (Zhou et al., 2021) propose WeiboMEL, Wikidata-MEL, and Richpedia-MEL that encompass a diverse range of sources. In a similar vein, Zheng et al. (2022) propose ZEMELD that focuses on zero-shot entity linking capacities. Gan et al. (2021), M3EL along with a bipartite graph matching multimodal entity linking benchmark. Finally, Wang et al. (2022b), in an attempt to counter limited coverage and simplified mention ambiguity, introduce WikiDiverse, which is grounded in Wikipedia.\nOur approach, in contrast to prior works in Multimodal entity linking and disambiguation, takes into consideration unique attribute information along with the visual and textual inputs. Table 1 compares AMELI with mentioned datasets.\nAttribute Value Extraction There is much work on extracting attribute value from product textual titles and descriptions. Several studies (Yan et al., 2021; Guo et al., 2018; Xu et al., 2019) formalize this problem as a sequence tagging task and solve it with LSTM-based methods, and some (Yang et al., 2022; Wang et al., 2020) build a questionanswering model treating each attribute as a question and extracting the answer span corresponding to the attribute value in the product text. Several recent studies (Lin et al., 2021; Zhu et al., 2020;\nWang et al., 2022a) incorporate visual clues into consideration by fusing representations of product text, product images, and/or Optical Character Recognition (OCR) tokens and visual objects detected in the product images and extracting the attribute values based on the fused representations. In this work, we consider utilizing attribute values mentioned in noisy user reviews to boost the multimodal entity linking task by implicitly inferring attribute values based on Natural Language Inference (NLI) models or explicitly extracting attribute values."
        },
        {
            "heading": "3 Dataset Construction",
            "text": ""
        },
        {
            "heading": "3.1 Data Source",
            "text": "Our goal is to build (1) a multimodal knowledge base where each entity is described with both text and images, and (2) an entity linking benchmark dataset where each mention in a given context is also associated with several images and can be linked to a specific entity in the multimodal knowledge base. To construct these two benchmark resources, we use Best Buy3, a popular retailer website for electronics such as computers, cell phones, appliances, toys and so on, given that it consists of both multimodal product descriptions organized in a standard format and user reviews in both text and/or images which can be further used to build the entity linking dataset. As shown in Figure 1, each product in Best Buy is described with a product name, a list of product categories, a paragraph of product description, a set of product attributes and values as well as several images4. In addition, users can post reviews in text and/or images under each product while each review can also be rated as helpful or unhelpful by other users.\nWe develop scripts based on the Requests5\npackage tool to collect all the above information. Each product webpage also requires a button clicking to display the product attributes, so we further utilize the Selenium6 tool to mimic the button clicking and collect all the attributes and values for each product. In this way, we collect 38,329 product entities and 6,500,078 corresponding reviews.\n3https://www.bestbuy.com/ 4For simplicity, we show one image for each review or product in the figure, but there could be multiple associated images for both of them.\n5https://requests.readthedocs.io/en/latest/ 6https://www.selenium.dev/"
        },
        {
            "heading": "3.2 Data Preprocessing",
            "text": "Some reviews only contain text and thus are not suitable for the multimodal entity linking task. Also, some reviews are duplicated or do not contain enough meaningful information to disambiguate the entity mention and link it to the correct target entities. Considering these, we design several rules to further preprocess the collected reviews: (1) Removing the reviews and products without images; (2) Removing the reviews with more than 500 tokens since most of the state-of-the-art pre-trained language models can only deal with 512 tokens; (3) Removing a review if it is only labeled as \u201cunhelpful\u201d by other users since we observe that these reviews normally do not provide much meaningful information; (4) Validating the links between reviews and their corresponding products and removing the invalid links. There are invalid links because Best Buy links each review to all the variants of the target product. For example, for the review about ASUS laptop shown in Figure 1, the target product ASUS ROG Laptop - 1TB SSD has several other variants in terms of color, memory size, processor model, and so on. Best Buy links the review to all the variants of the target product. Since we are taking each product variant as an entity in our multimodal knowledge base, we detect the valid links between reviews and product variants based on a field named productDetails, which reveals the gold target product variant information of the review in Best Buy\u2019s search response. After obtaining the valid link for each review, we remove invalid links between this review and all other products. (5) Removing truncated images uploaded by users since these images cause \u201ctruncated image error\u201d during loading with standard image tools\nlike Pillow7. (6) Removing reviews containing profanity words based on the block word list provided by Free Web Header8. (7) Review images may also contain irrelevant objects or information, for example, a review image for a fridge may also contain much information of the kitchen. We apply the object detection model (Liu et al., 2023) to detect the corresponding object by using the entity name as prompt and save the detected image patch as the cleaned review image. In the case that object can not be detected in some images, we further remove these images. Both original images and cleaned images are included in our dataset."
        },
        {
            "heading": "3.3 Mention Detection",
            "text": "To construct the entity linking benchmark dataset, we further identify entity mentions from the reviews based on their corresponding products. To achieve this, we design a pipelined approach to detect the most plausible product mention from each review. Given a review and its corresponding product, we first extract all the product name candidates from the product title and product category by obtaining their root word and identifying noun chunk of the root word to be product name candidates with spacy9. For each n-gram span (n \u2208 {1, 2, 3, 4, 5, 6}) in the review text, if it or its root form based on lemmatization is matched with any of the product name candidates, we will take it as a candidate mention. Each review text may contain multiple mentions of the target product, thus\n7https://pillow.readthedocs.io/en/stable/ installation.html\n8https://www.freewebheaders.com/bad-wordslist-and-page-moderation-words-list-for-facebook\n9https://spacy.io/usage/ linguistic-features#noun-chunks\nwe compute the similarity between each candidate mention and the title of the target product based on SBERT (Reimers and Gurevych, 2019) and choose the one with the highest similarity as the product mention. We apply this approach to detect product mentions for all the reviews10. Note that, after mention detection, we also remove the reviews that do not contain any product mentions."
        },
        {
            "heading": "3.4 Filtering of Uninformative Reviews",
            "text": "We also notice that many reviews do not contain enough context information from the text and images to correctly link the product mention to the target product entity. For example, in Figure 2, the target product is a Canon camera, however, the review image does not show the camera itself and the review text does not contain any specific information about the camera. To ensure the quality of the entity linking dataset, we further design a validation approach to filter out the reviews that do not contain enough context information.\nFor each review and its corresponding product, we extract four features, including # of mentioned attributes (i.e., the number of product attributes mentioned in the review based on string match), image-based similarity (i.e., the maximum similarity between review images and product images based on CLIP (Radford et al., 2021) image embeddings), description-based similarity (i.e., the similarity between product description and review text based on SBERT (Reimers and Gurevych, 2019)), title-based similarity (i.e., the similarity between the product title and review text using SBERT (Reimers and Gurevych, 2019)). We further manually annotate 500 pairs of review and\n10We manually assess the product mentions for 200 reviews. This approach achieves 91.9% accuracy.\nproduct while each pair is assigned with a label: positive if the review is informative enough to correctly link the mention to the target product, otherwise, negative, and use them to evaluate a thresholdbased approach which predicts the reviews as uninformative reviews if the four extracted feature scores, {# of mentioned attributes, image-based similarity, description-based similarity, title-based similarity}, do not overpass the four corresponding thresholds, which are hyperparameters searched on these examples. The threshold-based method reaches 85% of precision and 82% of recall in predicting informative reviews on these 500 examples11. We further apply it to clean the dataset by removing the reviews predicted as uninformative."
        },
        {
            "heading": "3.5 Train / Dev / Test Split",
            "text": "After all the preprocessing and filtering steps, we finally obtain 35,598 entities for the multimodal knowledge base and 19,241 reviews for the entity linking benchmark dataset. We name it as AMELI. We further split the reviews into training (Train), development (Dev) and test (Test) sets based on the percentages of 75%, 10%, and 15%, respectively.\nNote that since we utilize automatic strategies to detect mentions from reviews and filter out the uninformative reviews, there is still noise remaining in the AMELI though the percentage is low. Thus, we further ask humans to verify the Test set of AMELI. However, it\u2019s not trivial for humans to compare each mention with thousands of entities in the target knowledge base. To facilitate entity disambiguation by humans, for each review, we design two strategies to automatically retrieve the strong negative entity candidates from the knowledge base: (1) as we know the target product of each review, we first retrieve the top-K most similar entities to the target project from the KB as negative candidates. Here, the similarity between two products is computed based on their titles using SBERT (Reimers and Gurevych, 2019); (2) Similarity, we also retrieve the top-K similar entities to the target product based on their images using CLIP (Radford et al., 2021). We combine these 2K12 negative candidates together with the target\n11We compared the threshold-based method with a series of classifiers, like SVM, by training these classifiers on 385 examples and testing on 165 examples. Threshold-based Method reaches the highest accuracy.\n12We set K = 10 as we observe that the top-10 retrieved candidates have covered the most confusing negative entities.\nproduct entity as the set of candidate entities for each review and ask 12 annotators to choose the most likely target entity. Most annotators reach an accuracy of around 80%, while the overall accuracy is 79.73%. We remove the review if the annotator cannot correctly select the target entity. In this way, we finally obtain 3,025 reviews for the test set and name it as Clean-Test set. Table 2 shows the detailed statistics for each split of AMELI. Table 3 shows the category distribution of products in the multimodal knowledge base of AMELI."
        },
        {
            "heading": "4 Approach",
            "text": ""
        },
        {
            "heading": "4.1 Problem Formulation",
            "text": "We first formulate the task as follows: given a user review that consists of a text tr, an image vr13, and a particular product mention tm, we aim to link the mention to a particular product in a target knowledge base (KB) where each entity ej in the KB is described with a text tej , an image vej\n13Following previous studies (Wang et al., 2022b; Sun et al., 2022a), in this work, we also focus on the setting with the first image per review/entity and leave the multi-image setting as the future work\nand a set of attributes a1ej , ..., a j ej . Note that the entity name is also one of the attributes. Following previous work (Sevgili et al., 2022), we solve this task in a two-step pipeline: Candidate Retrieval, retrieving K entity candidates {e0, ..., ek} from the entity KB, followed by Entity Disambiguation, predicting the gold entity e+ from the entity candidates {e0, ..., ek}. Note that e+ may not be in {e0, ..., ek} in the case of candidate retrieval error."
        },
        {
            "heading": "4.2 Preprocessing",
            "text": "Before the retrieval step, we execute the following two preprocessing steps.\nPrior Probability Calculation Previous studies (Ganea and Hofmann, 2017a; Wang et al., 2022b) show that the prior probabilities from mentions to entities (denoted as P (e|m)) contribute a lot in retrieving the correct entity candidates from the target KB. Following similar ideas, we first extract noun chunks from the entity titles and entity categories with spacy14 and calculate the prior probability from noun chunks to the corresponding entity P\u0302 (e|m) and the prior probability from noun chunks to the entity category P\u0302 (c|m).\nAttribute Value Extraction Since there may exist text inside review images, like the brand name shown in a product packaging image, we apply an off-the-shelf OCR tool15 (Shi et al., 2015; Baek et al., 2019) to recognize texts within each review image. If the text is matched with any attribute values of entities in the target KB, we take it as an attribute value for the mention. Additionally, as Roy et al. (2021) shows that language models can achieve around 80% F1 score in attribute value extraction, we further apply GPT2 (Radford et al., 2019) to generate attributes in\n14https://spacy.io/usage/ linguistic-features#noun-chunks\n15https://github.com/JaidedAI/EasyOCR\na zero-shot text completion manner with the input format \u201cAttribute Value Extraction:\\n #Review_text \\n #Attribute_key:\u201d, where \u201cAttribute Value Extraction\u201d is the text prompt, \u201c#Review_text\u201d is the corresponding review text and \u201c#Attribute_key\u201d is the attribute to be extracted. We only keep the generated attributes that can be matched with any attribute values of entities in the target KB."
        },
        {
            "heading": "4.3 Candidate Entity Retrieval",
            "text": "Prior to the disambiguation step, we first retrieve a set of candidate entities from the target KB to condense our search space to the most relevant ones based on textual and visual similarity. Specifically, we apply SBERT (Devlin et al., 2018) to take in the mention text and each entity description and output their representations, which are fed into the cosine similarity function to select the top-1000 entity candidates with the highest similarity scores. We further pair the mention text with each entity description and feed these pairs into a BERT-based (Devlin et al., 2018) cross-encoder model, whose linear classification layer outputs cross-encoder similarity scores. To incorporate visual similarity, we employ CLIP (Radford et al., 2021) to obtain the image\nScore\nrepresentations, followed by a top-J retrieval step using cosine similarity, as previously mentioned. A weighted sum is applied in the textual cosine similarity scores, cross-encoder similarity scores, and visual cosine similarity scores to obtain the merged similarity scores, which are used to choose the top-K (K = 10) entity candidates, followed by filtering the candidates whose P\u0302 (e|m) and P\u0302 (c|m) are equal to 0. Figure 3 illustrates the entity retrieval process employed in this work."
        },
        {
            "heading": "4.4 Entity Disambiguation",
            "text": "As previously mentioned, prior entity disambiguation work only employs descriptive text and image representations for disambiguation. In contrast, our approach takes the structured entity attribute text into consideration. The following sections go through our proposed approach, composed of the image and text modules. Figure 4 illustrates our approach.\nText-based Disambiguation Our text-based disambiguation module is based on Natural Language Inference (NLI), based on the motivation that the review text should imply the product attribute value if it\u2019s mentioned in the review. For example, given the review text \u201cI was hoping it would look more pink than it does, it\u2019s more of a gray-toned light pink. Not a dealbreaker. I still like this bag\u201d, it should imply the attribute value of the target product, e.g., \u201cThe color of this bag is pink\u201d, while contradicting the attribute values of other products, e.g., \u201cThe color of this bag is black\u201d. Thus, for each review with a mention tm and a text tr, given\na set of candidate entities {te0 , ..., tei} and their attribute values {a0e0 , ..., a 0 ei}, ..., {a s e0 , ..., a s ei}, we pair each entity attribute/entity description with the review textual description and feed each pair into a DeBERTa (He et al., 2023) encoder to obtain their contextual representations\nHtm,tei = DeBERTa(tei , tm, tr) Htm,asei = DeBERTa(tm, tr, a s ei)\nFor each entity with multiple attribute values, we concatenate all the contextual representations obtained from DeBERTa and feed it through MLP to predict the final NLI score\nHr,ei = [Htm,a0ei : Htm,a1ei ..., Htm,asei : Htm,tei ]\nst(r, ei) = MLP(Hr,ei)\nwhere : denotes the concatenation operation. During training, we optimize the text-based disambiguation module based on the cross-entropy objective\nLt(r, e+) = \u2212 log( exp(s t(r, e+))\u2211K\u22121\nj=0 exp(s t(r, ej))\n)\nwhere e+ is the gold entity, and K is the number of retrieved candidate entities.\nImage-based Disambiguation Given the review image vr and entity images for a set of entity candidates {ve0 , ..., vei}, we feed them into CLIP to obtain their image representations {Hvr , Hve0 , ...,Hvei}. Inspired by previous studies (Zhang et al., 2022; Gao et al., 2021; Sun et al., 2022a), we feed these through an adapter, which consists of a feed-forward layer and residual connection, to adapt the generic image representations to a task-oriented semantic space\nH\u0302vei = Hvei + ReLU(HveiW e 1 )W e 2\nH\u0302vr = Hvr + ReLU(HvrW r 1 )W r 2\nwhere W r1 and W r 2 are learnable parameters for review representation learning, and W e1 and W e 2 are learnable parameters for entity representation learning.\nDuring training, we apply the following contrastive loss function based on the cosine similarity scores.\nLv(r, e+) = \u2212 log( exp(cosine(H\u0302vr , H\u0302ve+ ))\u2211\nei\u2208B exp(cosine(H\u0302vr , H\u0302vei )) )\nwhere B is the set of all entities in the current batch since we utilize in-batch negatives to improve our model\u2019s ability to distinguish between gold and negative entities.\nInference During inference, we combine the NLI score st(r, ei) from the text-base disambiguation module and the cosine similarity score sv(r, ei) from the image-based disambiguation model, and predict the entity with the highest weighted score s(r, ei) as the target\nsv(r, ei) = cosine(H\u0302vr , H\u0302ve+ )\ns(r, ei) = \u03bb \u00b7 st(r, ei) + (1\u2212 \u03bb) \u00b7 sv(r, ei)\nwhere \u03bb is a coefficient and is tuned on the Dev set. Since we have extracted attributes from each review as detailed in Section 4.2, we further filter the entity candidates whose attribute values do not match with the attribute values extracted from the review."
        },
        {
            "heading": "5 Experiments and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Candidate Retrieval",
            "text": "For each review, we retrieve the top-K entity candidates from the target KB and evaluate the retrieval performance based on Recall@K (K = 1, 10, 20, 50, 100). As shown in Table 4: (1) the multimodal retrieval outperforms the singlemodality retrieval, demonstrating that both text and image information complement each other; (2) The prior probability P (e|r) can boost the performance by a large margin since it can provide statistical information about the overall corpus. (3) Finetuning CLIP with the adapter helps improve the image matching and retrieval performance since the adapter can help CLIP model to fit the specific patterns and characteristics of the target task by transferring the representations to the task representation space."
        },
        {
            "heading": "5.2 Entity Disambiguation",
            "text": "We further evaluate the entity disambiguation performance based on the micro F1-score under the End-to-End setting, where models predict the target entity from the top-K(K = 10) retrieved entities, and Disambiguation setting, where models are evaluated on a subset of testing instances if their gold entities exist in the top-K(K = 10) retrieved candidates, and compare our approach with a Random Baseline which chooses the target product randomly, V2VEL (Sun et al., 2022b), which\nis a visual entity linking model with entity image and mention image as the input, Resnet150 (He et al., 2015) as the image encoder, and one adapter layer to adapt the representation to the task representation space, V2TEL (Sun et al., 2022b), which incorporates CLIP to encode entity text and mention image for prediction, and V2VTEL (Sun et al., 2022b), which combines V2VEL and V2TEL in a two-step retrieval-then-rerank pipeline. As detailed in Section 3.5, the upper bound of Human Performance for entity disambiguation is 79.73% given that the candidate set always contains the target ground truth entity.\nAs shown in Table 5, our approach outperforms V2VEL, V2TEL, V2VTEL and Random Baseline and reaches 33.47% of End-to-End F1 score. One reason for the low performance is the error propagation from the Candidate Retrieval phase to Disambiguation. Our model can reach 52.35% of F1 score under the Disambiguation setting when the gold entity exists in the retrieved candidate set. However, a considerable gap still exists between our model and Human Performance, indicating the complexity of our attribute-aware multimodal entity linking task.\nTo evaluate the impact of each modality on entity disambiguation, we design ablated models of our approach by removing text, image, or attribute from the model input. The results show that each modality can benefit the disambigua-\ntion performance, while the attribute modality contributes the most, which is indicated by the performance gap between the AMELINK model and the AMELINK_w/o_Attribute model. This performance gap may be because attributes provide a strong, direct signal for the coreference between review and gold entity. On the other hand, the attribute information potentially contains traits of the entities that effectively bridge the gap between the visual (i.e., product image) and textual (i.e., description) inputs since the attributes contain visual attributes (i.e., color) as well as descriptive attributes (i.e., system memory)."
        },
        {
            "heading": "6 Conclusion",
            "text": "We explore the attribute-aware multimodal entity linking task, which utilizes three streams of data - image, descriptive text, and structured attribute information - for multimodal entity linking/disambiguation. To support this line of research, we construct AMELI, including a multimodal knowledge base that contains 35,598 product entities with text, images, and fine-grained attributes and a multimodal review dataset that contains 18,472 review instances with text and images. We experiment with designed baseline models and show that adding the attribute information indeed enhances model performance by a considerable amount. Future work could explore more fine-\ngrained multimodal reasoning to take advantage of attribute information to boost entity disambiguation. Incorporating large language models (OpenAI, 2023) with instruction tuning (Yu et al., 2023; Xu et al., 2022) can also be one direction, given their recent promising performance."
        }
    ],
    "title": "AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes",
    "year": 2023
}