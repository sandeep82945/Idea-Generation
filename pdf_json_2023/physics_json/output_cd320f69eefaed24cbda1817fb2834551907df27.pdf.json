{
    "abstractText": "This survey concerns probabilistic models motivated by cooperative sequential adsorption (CSA) models. CSA models are widely used in physics and chemistry for modelling adsorption processes in which adsorption rates depend on the spatial configuration of already adsorbed particles. Corresponding probabilistic models describe random sequential allocation of particles either in a subset of Euclidean space, or at vertices of a graph. Depending on a technical setup these probabilistic models are stated in terms of spatial or integer-valued interacting birth-and-death processes. In this survey we consider several such models that have been studied in recent years.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vadim Shcherbakov"
        }
    ],
    "id": "SP:91128f7b810b0cb1483aa334f04129d267b16194",
    "references": [
        {
            "authors": [
                "M. Costa",
                "M. Menshikov",
                "V. Shcherbakov"
            ],
            "title": "Localisation in a growth model with interaction",
            "venue": "Vachkovskaia",
            "year": 2018
        },
        {
            "authors": [
                "J. Evans"
            ],
            "title": "Random and cooperative sequential adsorption",
            "venue": "Reviews of Modern Physics,",
            "year": 1993
        },
        {
            "authors": [
                "J. Evans"
            ],
            "title": "Random and cooperative sequential adsorption: exactly solvable models on 1d lattices, continuum limits, and 2d extensions. In Nonequilibrium Statistical Mechanics in One Dimension",
            "year": 1997
        },
        {
            "authors": [
                "W. Feller"
            ],
            "title": "An introduction to probability theory and its applications",
            "venue": "Volume II. 2nd Edition. Willey&Sons",
            "year": 1971
        },
        {
            "authors": [
                "S. Freidli",
                "Y. Velenik"
            ],
            "title": "Statistical Mechanics of Lattice Systems: A Concrete Mathematical Introduction",
            "year": 2017
        },
        {
            "authors": [
                "O.-H. Georgii",
                "O. H\u00e4ggstr\u00ebm",
                "C. Maes"
            ],
            "title": "The random geometry of equilibrium phases",
            "venue": "Phase Transitions and Critical Phenomena,",
            "year": 2001
        },
        {
            "authors": [
                "P. Grabarnik",
                "A. S\u00e4rkk\u00e4"
            ],
            "title": "2001).Interacting neighbour point processes: some models for clustering",
            "venue": "Journal of Statistical Computing and Simulations",
            "year": 2001
        },
        {
            "authors": [
                "P. Grabarnik",
                "V. Shcherbakov"
            ],
            "title": "A model of point configurations given by a semi-parametric interaction.Moscow",
            "venue": "University Mathematics Bulletin,",
            "year": 2012
        },
        {
            "authors": [
                "S. Janson",
                "V. Shcherbakov",
                "S. Volkov"
            ],
            "title": "Long term behaviour of a reversible system of interacting random walks",
            "venue": "Journal of Statistical Physics,",
            "year": 2019
        },
        {
            "authors": [
                "E.L. Lehmann",
                "G. Casella"
            ],
            "title": "Theory of point estimation",
            "year": 1998
        },
        {
            "authors": [
                "M. Menshikov",
                "S. Popov",
                "A. Wade"
            ],
            "title": "Non-homogeneous Random Walks",
            "year": 2017
        },
        {
            "authors": [
                "M. Menshikov",
                "V. Shcherbakov"
            ],
            "title": "Long term behaviour of two interacting birthand-death processes",
            "venue": "Markov Processes and Related Fields,",
            "year": 2018
        },
        {
            "authors": [
                "M. Menshikov",
                "V. Shcherbakov"
            ],
            "title": "Localisation in a growth model with interaction",
            "venue": "Arbitrary graphs. ALEA, Latin American Journal in Probability and Mathematical Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "M.D. Penrose"
            ],
            "title": "Existence and spatial limit theorems for lattice and continuum particle systems",
            "venue": "Probability Surveys,",
            "year": 2008
        },
        {
            "authors": [
                "M.D. Penrose"
            ],
            "title": "Growth and roughness of the interface for ballistic deposition",
            "venue": "Journal of Statistical Physics, 131,",
            "year": 2008
        },
        {
            "authors": [
                "M.D. Penrose",
                "J.E. Yukich"
            ],
            "title": "Limit theory for random sequential packing and deposition",
            "venue": "Annals of Applied Probability,",
            "year": 2002
        },
        {
            "authors": [
                "M.D. Penrose",
                "V. Shcherbakov"
            ],
            "title": "Maximum likelihood estimation for cooperative sequential adsorption",
            "venue": "Advances in Applied Probability,",
            "year": 2009
        },
        {
            "authors": [
                "M.D. Penrose",
                "V.V. Shcherbakov"
            ],
            "title": "Asymptotic normality of maximum likelihood estimator for cooperative sequential adsorption",
            "venue": "Advances in Applied Probability,",
            "year": 2011
        },
        {
            "authors": [
                "V. Privman ed"
            ],
            "title": "A special issue of Colloids and Surfaces A, 165",
            "year": 2000
        },
        {
            "authors": [
                "V. Shcherbakov"
            ],
            "title": "Limit theorems for random point measures generated by cooperative sequential adsorption",
            "venue": "Journal of Statistical Physics,",
            "year": 2006
        },
        {
            "authors": [
                "V. Shcherbakov",
                "S. Volkov"
            ],
            "title": "Stability of a growth process generated by monomer filling with nearest-neighbour cooperative effects",
            "venue": "Stochastic Processes and their Applications, 120,",
            "year": 2010
        },
        {
            "authors": [
                "V. Shcherbakov",
                "S. Volkov"
            ],
            "title": "Queueing with neighbours",
            "venue": "Papers in honour of Sir John Kingman. LMS Lecture Notes Series,",
            "year": 2010
        },
        {
            "authors": [
                "V. Shcherbakov",
                "S. Volkov"
            ],
            "title": "Long term behaviour of locally interacting birthand-death processes",
            "venue": "Journal of Statistical Physics, 158,",
            "year": 2015
        },
        {
            "authors": [
                "V. Shcherbakov",
                "A. Yambartsev"
            ],
            "title": "On equilibrium distribution of a reversible growth model",
            "venue": "Journal of Statistical Physics, 148,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "This survey concerns probabilistic models motivated by cooperative sequential adsorption (CSA) models. CSA models are widely used in physics and chemistry for modelling adsorption processes in which adsorption rates depend on the spatial configuration of already adsorbed particles. Corresponding probabilistic models describe random sequential allocation of particles either in a subset of Euclidean space, or at vertices of a graph. Depending on a technical setup these probabilistic models are stated in terms of spatial or integer-valued interacting birth-and-death processes. In this survey we consider several such models that have been studied in recent years.\nKeywords: cooperative sequential adsorption, maximum likelihood estimation, interacting urn model, interacting spin model, Markov chain, reversibility, electric networks, positive recurrence, transience, explosion\nSubject Classification : MSC 60J10, 60J20, 60J27, 60J28, 60K35, 62F10, 62F12, 62M30\nContents"
        },
        {
            "heading": "1 Introduction 2",
            "text": ""
        },
        {
            "heading": "2 Continuous CSA model 3",
            "text": "2.1 The model definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 CSA as a model for time series of spatial locations . . . . . . . . . . . . . . . . . 4 2.3 MLE for CSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.4 Asymptotic properties of MLE estimators . . . . . . . . . . . . . . . . . . . . . 7 2.5 MLE for CSA and the theory of locally determined functionals . . . . . . . . . . 9"
        },
        {
            "heading": "3 CSA growth model on graphs 10",
            "text": "3.1 The model definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.2 Localisation in the growth model with attractive interaction . . . . . . . . . . . 11\n\u2217Department of Mathematics, Royal Holloway, University of London, UK.\nEmail: vadim.shcherbakov@rhul.ac.uk\nar X\niv :2\n31 2.\n13 73\n1v 1\n[ m\nat h.\nPR ]\n2 1\nD ec\n2 02\n3"
        },
        {
            "heading": "4 The reversible model 13",
            "text": "4.1 The model definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Long term behaviour of the model . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.3 Reversibility of the model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4 Recurrent cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.5 Transient cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.6 Phase transition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.7 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.8 The model with finite components . . . . . . . . . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "5 CSA point process 24",
            "text": ""
        },
        {
            "heading": "1 Introduction",
            "text": "This survey concerns probabilistic models motivated by cooperative sequential adsorption (CSA) models. Adsorption is a real life phenomenon which can be thought of as follows. Consider particles (e.g. molecules) diffusing around a surface of a material. When a particle hits the surface, it can be retained (adsorbed) by the latter. CSA describes adsorption process in which adsorption rates depend on the spatial configuration of existing particles. In other words, particles adsorb to a surface subject to interaction with previously adsorbed particles. For example, adsorbed particles can either attract, or repel subsequent arrivals. These types of interactions are common for many physical, chemical and biological processes.\nIn physics and chemistry cooperative effects in adsorption are usually studied by experiments and by computer simulations of an appropriate CSA model. CSA model is a probabilistic model for random sequential deposition of particles (e.g. points or objects of various shape) either in a bounded region of a continuous space, or at vertices (sites) of a graph (e.g. lattice). In such a model a particle is placed at location x with the probability that is proportional to a specified function of the current configuration of existing particles in a neighbourhood of x. Such a construction is technically flexible for modelling both attractive and repulsive interaction between a new particle and previously adsorbed particles, and can be used in modelling CSA like processes in many real-life applications.\nThe paper is organised as follows. In Section 2 we introduce a model for random sequential deposition of particles (points) in a bounded subset of Euclidean space. This continuous model can be naturally interpreted as a model for time series of spatial locations. Fitting the model to data requires estimation of model parameters. We show that statistical inference for the model parameters can be based on maximum likelihood estimation. In particular, we describe the corresponding estimation procedure and discuss asymptotic properties of maximum likelihood estimators. In Section 3 we consider a discrete model random sequential deposition of particles at vertices of a graph (a growth process with graph based interaction). A probabilistic model obtained from the growth process by allowing deposited particles to depart is considered in Section 4. This model is motivated by adsorption processes in which adsorbed particles can be released from the adsorbing substrate. The model is described in terms of a reversible Markov chain and can be regarded as an interacting spin model and closely related to such well known\nmodels of statistical physics and interacting particle systems as the contact process and the Ising model. Finally, in Section 5 we consider a point process motivated by the CSA model. The point process is a probability measure given by a density with respect to Poisson point process (in finite dimensional Euclidean space) and belongs to a class of point processes used in spatial statistics for modelling point patterns."
        },
        {
            "heading": "2 Continuous CSA model",
            "text": "In this section we consider a probabilistic model for sequential deposition of points in a bounded domain of Euclidean space. This model is a continuous analogue of the lattice CSA known as monomer filling with nearest neighbour cooperative effects (see [2] and references therein). This lattice model describes a random sequential deposition of particles on the lattice, where only one particle can be allocated at a site. The probability of allocating a particle at an empty site is proportional to the allocation rate, which depends on the number of existing particles in a neighbourhood of the site. For example, the model on the one-dimensional lattice is specified by parameters ci, i = 1, 2, 3. Namely, a particle is placed at an empty site k with the rate ci, if the total number of existing particles at its nearest neighbours k\u2212 1 and k+1 is equal to i. In the continuous analogue of the lattice CSA model particles (points) are placed sequentially at random into a bounded region of Rd as follows. Given the current configuration of points, the probability of the event that a particle is placed at location x is proportional to a parameter \u03b2k \u2265 0 (called the growth rate), where k is the number of existing points within a given distance R of x. This continuous CSA model was proposed in [21] and was further studied as a model for time series of spatial locations in [18] and [19].\nIn the rest of this section we formally define the model and discuss statistical inference for\nthe model parameters."
        },
        {
            "heading": "2.1 The model definition",
            "text": "Start with some notations. Let N be the set of all positive integers and Z+ = N \u222a {0}. Let R = (\u2212\u221e,\u221e) and R+ = [0,\u221e). By 1A we denote the indicator function of a set or an event A. We assume that all random variables under consideration are defined on a certain probability space with the probability measure P. The expectation with respect to P will be denoted by E.\nGiven points x, y \u2208 Rd we denote by \u2225x\u2212y\u2225 the Euclidean distance between x and y. Given a positive number R points x, y \u2208 Rd are called neighbours, if \u2225x \u2212 y\u2225 \u2264 R, in which case we write x \u223c y. Given a finite set (ordered or unordered) X of points in Rd, define\n\u03bd(x,X ) = \u2211 y\u2208X 1{\u2225x\u2212y\u2225\u2264R} for x \u2208 Rd, (2.1)\nin other words, \u03bd(x,X ) is the number of neighbours of x in the set X . By definition \u03bd(x, \u2205) = 0. The continuous CSA model with the interaction radius R > 0 and parameters (\u03b2k \u2265 0, k \u2208 Z+) is the probabilistic model for random sequential deposition of points in Rd defined as follows. Consider a compact convex set D \u2282 Rd (called the target region, or the observation window). Let X(k) = (X1, ..., Xk), k \u2265 0, be the sequence of locations of first k points allocated\nin D according to the model. By definition, X(0) = \u2205. Given that X(k) = x(k) = (x1, ..., xk) for k \u2265 0 the conditional probability density function of the next point Xk+1 is\n\u03c8k+1(x|x(k)) = \u03b2\u03bd(x,x(k))\u222b\nD \u03b2\u03bd(y,x(k))dy\n, x \u2208 D. (2.2)\nThe joint density of (X1, ..., X\u2113), \u2113 \u2265 1 is given by\np\u2113,\u03b2,D(x1, . . . , x\u2113) = \u2113\u220f\nk=1\n\u03c8k(xk|x(k \u2212 1)) = \u2113\u220f\nk=1\n\u03b2\u03bd(xk,x(k\u22121))\u222b D \u03b2\u03bd(u,x(k\u22121))du , xi \u2208 D, i = 1, . . . , \u2113. (2.3)\nThe described CSA can be regarded as a discrete time spatial birth process with birth rates \u03b2\u03bd(x,x(k)), x \u2208 D, provided that the state of the process at time k is x(k) = (x1, . . . , xk). Alternatively, the model can be described as the acceptance-rejection sampling described below. Namely, let (Yi, i \u2265 1) be a sequence of independent random points uniformly distributed in D, and construct another sequence of random points by accepting each point of the original sequence with a certain probability to be described below, otherwise rejecting that point. Let X(k) = (X1, . . . , Xk) be the sequence of k = kn accepted points from the finite sequence Yi, i = 1, . . . , n. By definition X(0) = \u2205. The point Yn+1 is accepted with probability \u03b2\u03bd(Yn+1,X(k))/C, where C is an arbitrary constant such that max0\u2264i\u2264k \u03b2i \u2264 C. Regardless of the particular choice of C, the next accepted point Xk+1 has the probability density \u03c8k+1(x|X(k)) given by (2.2) In other words, given the sequence X(\u2113) of the first \u2113 accepted points, the next accepted point X\u2113+1 is sampled from a distribution which is specified by the probability density proportional to the function \u03b2\u03bd(x,X(\u2113)), x \u2208 D (the value of C influences only the number of discarded points Yi until the next acceptance).\nRemark 2.1. The defined above continuous CSA model was introduced in [21]. In that paper the asymptotic structure of the model point pattern was studied under the assumption that the sequence (\u03b2n > 0, n \u2265 0) converges to a positive limit as n\u2192 \u221e. This assumption can be interpreted as if \u201cadsorption rates stabilize at saturation\u201d.\nRemark 2.2. A special case of the model, when \u03b2i = 0 for i \u2265 1, is called random sequential adsorption (RSA) model. RSA is also known as the car parking model. In the latter cars are modelled by balls of radius R. Cars sequentially arrive to the target region D and choose a location to park at random. A new arrival is discarded with probability 1, if it overlaps any of previously parked cars. Otherwise it is parked (accepted) with probability 1."
        },
        {
            "heading": "2.2 CSA as a model for time series of spatial locations",
            "text": "It has been noted by physicists (e.g. [2]) that CSA model can be used for modelling sequential point patterns in disciplines such as geophysics, biology and ecology in situations, where a data set is presented by a sequential point pattern, i.e., a collection of spatial events which appear sequentially. In other words, CSA can be used as an approximation of spatial spread dynamics in various applications. This idea was explored in [18] and [19], where the continuous CSA was regarded as a model for time series of spatial locations, which is flexible for modelling both regular and clustered point patterns (e.g. see Figure 1). Note that models for clustered point patterns are of a particular interest in spatial statistics.\nFitting a parametric statistical model to real-life data requires estimation of the model parameters. Statistical inference based on maximum likelihood estimation was developed in [18] and [19] for CSA with a finite number of parameters \u03b2\u2019, which means that there exists a fixed positive integer N such that\n\u03b2k > 0 for 0 \u2264 k \u2264 N and \u03b2k = 0 for k \u2265 N + 1. (2.4)\nIt is easy to see that the density (2.2) (and hence (2.3)) is unaffected by multiplication of all parameters \u03b2k by a constant. Therefore, for identifiability of the model we also assume that \u03b20 = 1, so that the model is parameterised by parameters \u03b2 = (\u03b21, . . . , \u03b2N).\nIn general, both number N and the interaction radius R are also regarded as the model parameters and have to be estimated. The parameter N can be easily estimated by the maximal number of neighbours that a point has in an observed pattern (formal definition is given below). In contrast, estimation of the interaction radius in the general case is an open problem. IfN = 0, i.e. in the case of RSA model, the natural estimator of the interaction radius is the minimal distance between an observed point and those points in the pattern that arrived earlier. In what follows, we assume that the interaction radius R is assumed to be a fixed and known constant."
        },
        {
            "heading": "2.3 MLE for CSA",
            "text": "In this section we explain how to develop statistical inference for parameters of the CSA model by using the method of maximum likelihood estimation (MLE).\nStart with considering the model likelihood. Recall that 1A stands for the indicator of a set\nA. Given an observation x(\u2113) = (x1, ..., x\u2113) \u2208 D\u2113, where \u2113 \u2265 2, define\n\u0393j,k = \u222b D 1{u:\u03bd(u,x(k))=j}du for 0 \u2264 j \u2264 k \u2264 \u2113\u2212 1, j = 0, ..., N, (2.5) \u0393j,k = 0 for k < j, j = 1, ..., N, and \u03930,0 = |D|. (2.6)\nObserve that \u222b D \u03b2\u03bd(u,x(k))du = k\u2211\nj=0 \u03b2j\u0393j,k = \u03930,k + N\u2211 j=1 \u03b2j\u0393j,k.\nFurther, define\ntk,\u2113 = tk(x(\u2113)) := \u2113\u2211\ni=1\n1{\u03bd(xi,x(i\u22121))=k} for k = 0, . . . , N. (2.7)\nIn terms of statistics (2.5) and (2.7) we have the following equation for the model likelihood\np\u2113,\u03b2,D(x1, . . . , x\u2113) =\n\u220fN j=1 \u03b2\ntj(x(\u2113)) j\u220f\u2113\nk=1 \u222b D \u03b2\u03bd(u,x(k\u22121))du =\n\u220fN j=1 \u03b2\ntj,\u2113 j\u220f\u2113\nk=1\n( \u03930,k\u22121 + \u2211N j=1 \u03b2j\u0393j,k\u22121 ) . (2.8) The log likelihood function is therefore given by\nLD(x(\u2113), \u03b2) := log(p\u2113,\u03b2,D(x1, . . . , x\u2113))\n= N\u2211 j=1 tj,\u2113 log(\u03b2j)\u2212 \u2113\u2211 k=1 log\n( \u03930,k\u22121 +\nN\u2211 j=1 \u03b2j\u0393j,k\u22121\n) .\n(2.9)\nRemark 2.3. Note that since X(0) = \u2205 the first point X1 is uniformly distributed in D, and, also, the first term in the sum \u2211\u2113 k=1 ... in the preceding display is just a constant log(|D|).\nMaximum likelihood estimators (MLEs) are defined as usual, i.e. as maximizers of the model likelihood and can be found by solving MLE equations obtained by equating to zero the log-likelihood derivatives.\nIfN is unknown, then it has to be estimated before estimating \u03b2\u2019s. GivenX(\u2113) = (X1, ...X\u2113),\nwhere, as before, we assume that \u2113 \u2265 2, we estimate N by\nN\u0302 = N\u0302(X(\u2113)) := max Xi\u2208X(\u2113)\n\u03bd(Xi, X(i\u2212 1)) = max{j : tj,\u2113 > 0}. (2.10)\nIt is easy to see N\u0302 is the maximum likelihood estimator of the parameter N .\nHaving estimated N by N\u0302 we have that\nLD(X(\u2113), \u03b2) = N\u0302\u2211 j=1 tj,\u2113 log(\u03b2j)\u2212 \u2113\u2211 k=2 log \u03930,k\u22121 + N\u0302\u2211 j=1 \u03b2j\u0393j,k\u22121  . The maximum likelihood estimator \u03b2\u0302(X(\u2113)) = (\u03b2\u03021, . . . , \u03b2\u0302N\u0302 , 0, 0, . . .) of the true parameter vector (\u03b2 (0) 1 , . . . , \u03b2 (0) N ) is defined as the maximizer of the log likelihood LD(X(\u2113), \u03b2) over vectors of the form (\u03b21, . . . , \u03b2N\u0302 , 0, 0 . . .). Since LD(X(\u2113), \u03b2) depends smoothly on (\u03b21, . . . , \u03b2N\u0302); the (\u03b2\u03021, . . . , \u03b2\u0302N\u0302) is a solution to the system of MLE equations\n\u2202LD(X(\u2113), \u03b2)\n\u2202\u03b2j = 0, j = 1, . . . , N\u0302 , (2.11)\nor, equivalently,\ntj,\u2113 \u2212 \u2113\u2211\nk=2\n\u03b2j\u0393j,k\u22121 \u03930,k\u22121 + \u2211N\u0302 i=1 \u03b2i\u0393i,k\u22121 = 0, j = 1, . . . , N\u0302 , (2.12)\nwhere note that the sum \u2211\nk=2 in the right hand side starts from k = 2 because of Remark 2.3.\nIt is obvious that N\u0302 \u2264 N almost surely. If N\u0302 < N , then tj,\u2113 = 0 for N \u2032 + 1 \u2264 j \u2264 N . It is\nalso possible that tj,\u2113 = 0 for some j < N\u0302 . Therefore, if an observed point pattern is not a \u201ctypical\u201d model pattern, then we might not have sufficient information to estimate the full set of parameters. However, if N\u0302 = N and all t\u2212statistics are positive, then there exists a unique positive solution (\u03b2\u03021, . . . , \u03b2\u0302N) of the likelihood equations. It turns out that these conditions hold with probability tending to 1, as the amount of observed information increases in a certain natural sense (to be explained).\nExample 2.1. Suppose that N = 1, i.e., there is one unknown parameter \u03b2 = \u03b21. Assume that an observed sequence of points x(\u2113) = (x1, . . . , x\u2113), \u2113 \u2265 2 is such that N\u0302 = 1 and the statistic t1,\u2113 > 0 (the number of points having 1 neighbour). There is a a single MLE equation in this case,that is\nt1,\u2113 \u2212 \u2113\u2211\nk=2\n\u03b2\u03931,k \u03930,k + \u03b2\u03931,k = 0. (2.13)\nIf 0 < t1,\u2113 < \u2113\u2212 1, then existence and uniqueness of the solution of the MLE equation follows from the fact that the left hand side of equation (2.13) is a strictly monotonic function of \u03b2. If t1,\u2113 = \u2113 \u2212 1, then this suggests that the observed pattern is generated by the model obtained by setting formally \u03b2\u0302 = \u221e. In the corresponding limit model a new point is allocated with probability one in the neighbourhood of existing points subject to the constraint that it cannot have more than one neighbour among those points.\nRemark 2.4. It should be noted that the model log-likelihood, and, hence, MLEs for the CSA model, can be effectively computed numerically by the classical Monte-Carlo (required to compute \u0393\u2212statistics that are given by integrals). We refer to [18] for numerical examples."
        },
        {
            "heading": "2.4 Asymptotic properties of MLE estimators",
            "text": "In this section we briefly discuss asymptotic properties of MLE estimators for CSA in the situation, when the amount of observed information increases. In the classic case of i.i.d. observations this limit regime means that the number of observations tends to infinity. The analogue of this in spatial statistics is known as the increasing domain asymptotic framework, which means the number of observed points tends to infinity, as the target region (observation window) expands to the whole space. We describe below this limit regime in relation to CSA model with a finite number of non-zero parameters \u03b2, where there are natural restrictions on the number of observed points in a given target region.\nLet D1 denote the unit cube centred at the origin (or any compact convex set D1 \u2282 Rd). Consider a sequence of rescaled domains Dm = m\n1/dD1, m \u2208 Z+. Given m, consider the CSA process as the acceptance/rejection sampling with target region D = Dm. Denote by Am(n) the (random) number of points accepted out of the first n incoming points. If N <\u221e, then no particle can be placed at any location x with more than N existing particles within distance R of x. Therefore, the limit \u03b8m = limn\u2192\u221eAm(n) exists almost surely, and is a finite random variable. Further, there exists a finite limit limm\u2192\u221e \u03b8m =: \u03b8\u221e, known as the jamming density (see [18] for more details of this quantity).\nThe increasing domain asymptotic framework in the case of the CSA model can be now\ndefined as follows.\nAssumption 2.1. The number \u2113m of observed points in the domain Dm is asymptotically linear in m with coefficient below the jamming density \u03b8\u221e = \u03b8\u221e(R, \u03b21, ..., \u03b2N), that is\nlim m\u2192\u221e ( \u2113m m ) = \u00b5 \u2208 (0, \u03b8\u221e).\nNote that the above limit is known as the thermodynamic limit in the statistical physics\nliterature.\nAssume in the rest of the section that Assumption 2.1 holds. It turns out that under this assumption the log-likelihood derivatives in the case of CSA model behave asymptotically very similar to those in the i.i.d. case. This fact allows to combine methods of the classic MLE theory for i.i.d. observations (e.g., see [11]) with the modern theory for sums of locally determined functionals (to be explained) to establish consistency and asymptotic normality of MLE estimators for CSA model under assumption 2.1.\nGiven parameters N and \u03b2 = (\u03b21, . . . , \u03b2N) consider the probability measure Pm,\u03b2 on finite point sequences of length \u2113m in Dm specified by the probability density p\u2113,\u03b2,D with \u2113 = \u2113m and D = Dm. Denote \u03b2 (0) = ( \u03b2 (0) 1 , . . . , \u03b2 (0) N ) the true parameter and let P (0) m := Pm,\u03b2(0) . Given observation X(\u2113m) \u2208 Dm define the maximum likelihood estimators\n\u03b2\u0302(m) = \u03b2\u0302(X(\u2113m)) = (\u03b2\u03021,m, . . . , \u03b2\u0302N,m)\nof parameters \u03b2(0) = (\u03b2 (0) 1 , . . . , \u03b2 (0) N ) as those values that maximize the log likelihood function Lm(\u03b2) := LDm(X(\u2113m), \u03b2), as explained in Section 2.3.\nIt was shown in [18, Corollary 2.1] that\nP(0)m\n{ p\u2113m,\u03b2(0),Dm(X1, . . . , X\u2113m)\np\u2113m,\u03b2,Dm(X1, . . . , X\u2113m) > 1\n} \u2192 1, as m\u2192 \u221e,\nfor \u03b2 = (\u03b21, . . . , \u03b2N) \u0338= \u03b2(0) = (\u03b2(0)1 , . . . , \u03b2 (0) N ). This result is analogous to the well known result for the case of i.i.d.observations (e.g., see [11, Chapter, Theorem 2.1]) and justifies why statistical inference for the CSA model can be based on MLE. Furthermore, it was shown in [18, Theorem 2.2 and Lemma 5.2] that with P (0) m \u2212probability tending to 1, asm\u2192 \u221e, the estimator N\u0302 is equal to N and there exists a unique positive solution (\u03b2\u03021,m, . . . , \u03b2\u0302N,m) of the system of MLEs, such that \u03b2\u0302i,m \u2192 \u03b2(0)i for i = 1, . . . , N, in P (0) m \u2212 probability, as m\u2192 \u221e.\nAsymptotic normality of MLE estimator \u03b2\u0302 was established in [19]. Specifically, it was shown that \u221a m(\u03b2\u0302(m) \u2212 \u03b2(0)) \u2192 \u03be(\u00b5) in P(0)m \u2212distribution, as m \u2192 \u221e, where \u03be(\u00b5) is the Gaussian vector with zero mean and covariance matrix given by the inverse matrix of the model limit information matrix. The latter is defined as the limit (in P (0) m \u2212probability, as m \u2192 \u221e) of the observed information matrix \u2212 1 m ( \u22022Lm(\u03b2) \u2202\u03b2i\u2202\u03b2j )N i,j=1 evaluated at the true parameter \u03b2(0). A detailed study of the structure of the information matrix can be found [19] to which we refer for further details.\nUsefulness of showing asymptotic normality of a parameter estimator provides asymptotic justification for creating confidence intervals based on the normal distribution, when a sufficiently large number of points is observed in a sufficiently large region relative to the interaction radius R (see [19] for examples of creation of confidence intervals)."
        },
        {
            "heading": "2.5 MLE for CSA and the theory of locally determined functionals",
            "text": "The asymptotic analysis of MLEs is based on the fact that the model statistics have special structure. Namely, these statistics are sums of so called locally determined functionals over a finite set of points. Below we briefly explain the idea.\nStart with some definitions. A set of points X \u2282 Rd is called locally finite, if its intersection with any ball of a finite radius consists of a finite number of points. A locally determined functional with a given range r > 0 is a measurable real-valued function \u03be(Y,X ) defined for all pairs (Y,X ), where Y \u2208 Rd and X \u2282 Rd is locally finite, with the property that \u03be(Y,X ) is determined only by those points of X that are within distance r of Y . A locally determined functional \u03be(Y,X ) is translation invariant if \u03be(Y,X ) = \u03be(Y + a,X + a) for any a \u2208 Rd. For example, the functional \u03be(x,X ) := 1{\u03bd(x,X )=j}, (2.14) where the quantity \u03bd(x,X ) is defined by (2.1), is a bounded, translation-invariant, locally determined functional with the range equal to the interaction radius R.\nGiven a locally determined functional \u03be, the corresponding additive functional H\u03be on finite\nsequences X(\u2113) = (X1, . . . , X\u2113) \u2208 (Rd)\u2113 is defined as follows\nH\u03be(X(\u2113)) = \u2113\u2211\ni=1\n\u03be(Xi, X(i\u2212 1)). (2.15)\nObserve now that both \u0393\u2212statistics (2.5) and t\u2212statistics (2.7) are sums of locally determined functionals. Indeed, in the case of t\u2212statistics\ntj,\u2113 = \u2113\u2211\ni=1\n1{\u03bd(Xi,X(i\u22121))=j}, 0 \u2264 j \u2264 N, (2.16)\nwe have that the statistic tj,\u2113 is the additive functional corresponding to the locally determined functional (2.14). Representation of \u0393\u2212statistics as sums of locally determined functionals is more technically involved and we refer to [18] for further details.\nThe general limit theory developed in [17]) for additive functionals (2.15) implies that, under Assumption 2.1, there exist strictly positive and continuous in \u00b5 functions (\u03c1j (\u00b5, \u03b2) , 1 \u2264 j \u2264 N and \u03b3j (\u00b5, \u03b2) , 1 \u2264 j \u2264 N , such that\ntj,\u2113m m \u2192 \u03c1j (\u00b5, \u03b2) and \u0393j,\u2113m m \u2192 \u03b3j (\u00b5, \u03b2) , j = 0, . . . , N, (2.17)\nin Pm\u2212probability, as m\u2192 \u221e, and that are related by the system of equations\n\u03c1j (\u00b5, \u03b2) = \u00b5\u222b 0\n\u03b2j\u03b3j (\u03bb, \u03b2) \u03b30 (\u03bb, \u03b2) + \u2211N i=1 \u03b2i\u03b3i (\u03bb, \u03b2) d\u03bb, j = 1, . . . , N.\nFurther, let \u03b3 (0) j (\u03bb) := \u03b3j(\u03bb, \u03b2 (0)) and \u03c1 (0) j (\u00b5) := \u03c1j(\u00b5, \u03b2 (0)) for j = 1, . . . , N . Given \u00b5 \u2208 (0, \u03b8(0)), where \u03b8(0) = \u03b8\u221e(R, \u03b2 (0) 1 , . . . , \u03b2 (0) N ), the vector of true parameters \u03b2 (0) = (\u03b2 (0) 1 , . . . , \u03b2 (0) N ) is a solution of the system of equations\n\u03c1 (0) j (\u00b5)\u2212 \u00b5\u222b 0\n\u03b2j\u03b3 (0) j (\u03bb)\n\u03b3 (0) 0 (\u03bb) + \u2211N i=1 \u03b2i\u03b3 (0) i (\u03bb) d\u03bb = 0, j = 1, . . . , N, (2.18)\nwhich is the infinite-volume limit of the MLE (2.12).\nExample 2.2. Assume, as in Example 2.1, that N = 1. In this case the true single parameter \u03b2(0) = \u03b2 (0) 1 is the unique solution of the limit equation\n\u03c1 (0) 1 (\u00b5)\u2212 \u00b5\u222b 0\n\u03b2\u03b3 (0) 1 (\u03bb)\n\u03b3 (0) 0 (\u03bb) + \u03b2\u03b3 (0) 1 (\u03bb)\nd\u03bb = 0.\nExistence and uniqueness of the solution of this equation follows, similarly to the case of MLE equation in Example 2.1, from monotonicity of the integrand in the parameter \u03b2."
        },
        {
            "heading": "3 CSA growth model on graphs",
            "text": "In this section we consider a probabilistic model for random sequential deposition of particles at vertices of a graph. The model can be regarded as a discrete version of the continuous CSA model considered in the previous section. Recall that in the continuous model the probability of the event that a particle is placed at location x is proportional to the growth rate \u03b2k \u2265 0, where k is the number of existing points in the neighbourhood of x. In the discrete model we assume that the growth rate at a vertex v is equal to e\u03b1k+\u03b2m, where k is the number of existing particles at the vertex v, m is the total number of existing particles in vertices adjacent to v, and \u03b1, \u03b2 are given constants. Thus, in general, we distinguish particles at the vertex itself and in its neighbours.\nThis growth model can be interpreted as an interacting urn model with a graph based loglinear interaction. Similarly to urn models, we are interested in the long term behaviour of the growth process. In particular, we would like to establish in which cases all vertices receive infinitely many particles and in which cases all but finitely particles are allocated at a certain subset of vertices (e.g. at a single vertex)."
        },
        {
            "heading": "3.1 The model definition",
            "text": "The model set up is as follows. Consider an arbitrary finite graph G = (V,E) with the set of vertices V and the set of edge E. If vertices v, u \u2208 V are adjacent, we call them neighbours and write v \u223c u. If vertices v and u are not adjacent, then we write v \u2241 u. By definition, vertex is not a neighbour of itself, i.e. v \u2241 v.\nThe growth process with parameters (\u03b1, \u03b2) \u2208 R2 on the graph G = (V,E) is a discrete time Markov chain X(n) = (Xv(n), v \u2208 V ) \u2208 ZV+ with the following transition probabilities\nP(X(n+ 1) = X(n) + ev|X(n) = x) = e\u03b1xv+\u03b2\n\u2211 u\u223cv xu\n\u0393(x) , x = (xw, w \u2208 V ) \u2208 ZV+, (3.1)\nwhere \u0393(x) = \u2211\nv\u2208V e \u03b1xv+\u03b2 \u2211 u\u223cv xu and ev \u2208 RV is the v-th unit vector, i.e. the vector, the\nv\u2212th coordinate of which is equal to 1, and all other coordinates are zero.\n1\nThe growth process X(n) = (Xv(n), v \u2208 V ) describes a random sequential allocation of particles on the graph, where Xv(n) is interpreted as the number of particles at vertex v at time n. If \u03b2 = 0, then the structure of the underlying graph is irrelevant, and the growth process is a special case of the generalised Po\u0301lya urn (GPU) model. Recall that GPU model is a model for random sequential allocation of particles at a finite number of urns, in which a particle is allocated at an urn v with xv existing particles with probability proportional to the growth rate f(xv), where f is a given positive function. The growth process with parameter \u03b2 = 0 is the GPU model with the exponential growth rate f(k) = e\u03b1k. If \u03b2 \u0338= 0, then the growth process can be regarded as an interacting urn model obtained by adding graph\nbased interaction. Observe that the growth rate e\u03b1xv+\u03b2 \u2211 u\u223cv xu (i.e. the function determining the allocation probability (3.1)) is a monotonically increasing function of the parameter \u03b2. Therefore, if \u03b2 > 0, then interaction between components of the growth process is cooperative in the sense that particles in a neighbourhood of a vertex accelerate the growth rate at the vertex. In contrast, if \u03b2 < 0, then the interaction between process\u2019s components is competitive in the sense that particles in a neighbourhood of a vertex the growth rate slow down the growth at the vertex.\nRemark 3.1. The growth process on arbitrary graph was introduced in [14]. The growth process a single parameter \u03bb := \u03b1 = \u03b2 \u2208 R on a cycle graph was introduced and studied in [22] (Recall that a cycle graph withN \u2265 2 vertices is the graphG = {1 \u223c 2 \u223c ... \u223c N\u22121 \u223c N \u223c 1}). The limit cases of the growth process on a cycle graph obtained by setting \u03bb = \u221e and \u03bb = \u2212\u221e (with convention \u221e\u00b7 0 = 0) were studied in [23] (see an open problem at the end of Section 3.2 for more details). A version of the growth process on a cycle graph, where the parameter \u03bb depends on a vertex (i.e. \u03b1v = \u03b2v = \u03bbv > 0, v \u2208 V ), was studied in [1]."
        },
        {
            "heading": "3.2 Localisation in the growth model with attractive interaction",
            "text": "Recall some known results for GPU models. Consider a GPU model with the growth rate determined by a function f , as described in the preceding section. Assume that f is such that\u2211\u221e k=1 1 f(k)\n< \u221e. It is known that in this case, with probability one, all but a finite number of particles are allocated at a single random urn. In other words, the allocation process localises at a single urn. This result immediately implies the eventual localisation at a single vertex for the growth process with parameters \u03b1 > 0 and \u03b2 = 0 (as it is just a special case of the aforementioned GPU model).\nIt was shown in [14] that a similar localisation effect occurs in the growth process with attractive interaction introduced by a positive parameter \u03b2. It turns out that in this case the growth process localises at special subsets of vertices rather than at a single vertex.\nRecall some definitions from graph theory necessary to state the result. Let G = (V,E) be a finite graph. Then, given a subset of vertices V \u2032 \u2286 V the corresponding induced subgraph is a graph G\u2032 = (V \u2032, E \u2032) whose edge set E \u2032 consists of all of the edges in E that have both endpoints in V \u2032. The induced subgraph G\u2032 is also known as a subgraph induced by the set of vertices V \u2032. A complete induced subgraph is called a clique, and a maximal clique is a clique that is not an induced subgraph of another clique.\nThe localisation result in [14, Theorem 1] is as follows. Consider the growth process X(n) = (Xv(n), v \u2208 V ) \u2208 ZV+ with parameters (\u03b1, \u03b2) on a finite connected graph G = (V,E), and let 0 < \u03b1 \u2264 \u03b2. Then for every initial state X(0) \u2208 ZV+ with probability one there exists a random maximal clique with a vertex set U \u2286 V such that\nlim n\u2192\u221e Xv(n) = \u221e if and only if v \u2208 U, and lim n\u2192\u221e\nXv(n) Xu(n) = eCvu , for v, u \u2208 U,\nwhere\nCvu = \u03bb lim n\u2192\u221e \u2211 w\u2208V Xw(n)[1{w\u223cv,w\u2241u} \u2212 1{w\u223cu,w\u2241v}], if 0 < \u03bb := \u03b1 = \u03b2,\nand Cvu = 0, if 0 < \u03b1 < \u03b2.\nThe above localisation effect was first shown in [22, Theorem 3] (see also [1, Theorem 1]) in the case when \u03b1 = \u03b2 > 0 and the underlying graph is a cyclic graph. In the special case of the cyclic graph any clique is just a pair of neighbouring vertices (assuming that the graph consists of at least three vertices).\nThe proof is based on the following key fact. Namely, given an arbitrary initial configuration the process localises at one of the graph\u2019s clique with probability that is bounded away from zero. This implies that with probability one the process eventually localises at one of the graph\u2019s cliques (the final clique).\nConditioned that particles are allocated only at vertices of a given clique, the numbers of allocated particles at these vertices grow according to a multinomial model in the case when \u03b1 = \u03b2. The allocation probabilities of this multinomial model are determined by the configuration of existing particles in the neighbourhood of the clique which remains unchanged since the start of the localisation. In other words, the multinomial model is determined by the limit quantities Cvu that depend on the state of the process at the time moment, when localisation starts at the final clique. In the case \u03b1 < \u03b2 these quantities irrelevant, and the numbers of particles at vertices of the final clique grow in the same way as in the case of the complete graph described in the example below.\nExample 3.1 (Complete graph). Consider the growth process X(n) = (X1(n), ..., Xm(n)) with parameters 0 < \u03b1 < \u03b2 on a complete graph with m \u2265 2 vertices labeled by 1, ...,m. Let Zi(n) = Xi(n) \u2212 Xm(n), i = 1, ...,m \u2212 1. By [14, Lemma 3.3] the process of differences Z(n) = (Z1(n), ..., Zm\u22121(n)) \u2208 Zm\u22121 is an irreducible positive recurrent Markov chain. Positive recurrence was shown by applying Foster\u2019s criterion (e.g. see [12, Theorem 2.6.4]) with the\nLyapunov function given by\ng(z) = m\u22121\u2211 i=1 z2i , z = (z1, ..., zm\u22121) \u2208 Zm\u22121.\nIt should be noted that exactly the same fact (see [22, ]) is true for the process of differences in the GPU model with the growth rate f(k) = e\u2212\u03bbk, k \u2208 Z+, where \u03bb > 0.\nRemark 3.2. Localisation also occurs if 0 < \u03b2 < \u03b1. In this case with probability one the growth process localises at a single vertex, which is similar to the GPU model with the growth rate give by the function f(k) = e\u03b1k.\nOpen problem: repulsive interaction The long term behaviour of the growth process in the case when \u03bb := \u03b1 = \u03b2 < 0 is largely unknown. In this case the interaction between the process\u2019s components is repulsive, which greatly complicates the study of the process\u2019s behaviour. Below we briefly describe some known results and state an open problem.\nConsider the growth process on a cycle graph G = {1 \u223c 2 \u223c ... \u223c m \u223c 1} (i.e. with m vertices labelled by 1, ...,m) with parameters \u03bb := \u03b1 = \u03b2 < 0. Start with a special semideterministic case of the process obtained by formally setting \u03bb = \u2212\u221e. Namely, in this case given the process\u2019s state x = (x1, ..., xm) (where xi denotes, as before, the number of particles at vertex i) a next particle is allocated to a vertex xi for which the quantity ui = xi\u22121 + xi + xi+1 (with convention 1 \u2212 1 = m and m + 1 = 1) is minimal. If there is more than one such vertex, then one of them is chosen at random. In this semi-deterministic model if the number of vertices of the graph is m = 4, then, given any initial configuration particles will be placed with probability one only at a pair of non-adjacent vertices (i.e. either at vertices {1, 3}, or at vertices {2, 4}). Moreover, after a finite number of steps the numbers of particles at the vertices of the final pair differ by no more than 1 and equal to each other every other step. Similar but much more complicated limit behaviour is observed in the case of the cycle graph with arbitrary number of vertices. We refer to [23] for further details, where complete classification of the long term behaviour of the model with \u03bb = \u2212\u221e on the cycle graph is given. In the case \u2212\u221e < \u03bb < 0 only a partial result is known in the case of the cycle graph with even number of vertices. Namely, it is shown in [22] that if initially there are no particles, then with a positive probability particles can be allocated either only at even, or only at odd vertices. The complete classification of the limit behaviour of the model in the case \u2212\u221e < \u03bb < 0 is an open problem."
        },
        {
            "heading": "4 The reversible model",
            "text": ""
        },
        {
            "heading": "4.1 The model definition",
            "text": "In this section we consider a probabilistic model which is a version of the growth process (defined in Section 3.1) obtained by allowing deposited particles to depart from the graph.\nIt is convenient to define the model in terms of a continuous time Markov chain (CTMC). The model set up is as follows. Let, as before, G = (V,E) be a finite graph with the set of\nvertices V and the set of edges E. Recall that ev \u2208 RV is the vector, the v\u2212th coordinate of which is equal to 1, and all other coordinates are zero. Consider a CTMC X(t) = (Xv(t), v \u2208 V ) \u2208 ZV+ and the transition rates r\u03b1,\u03b2(x,y), x,y \u2208 ZV+ given by\nr\u03b1,\u03b2(x,y) =  e\u03b1xv+\u03b2 \u2211 u\u223cv xu , for y = x+ ev and x = (xv, v \u2208 V ), 1, for y = x\u2212 ev and x = (xv, v \u2208 V ) : xv > 0, 0, otherwise.\n(4.1)\nwhere \u03b1, \u03b2 \u2208 R are given constants. Note that if the death rate was zero, then the CTMC X(t) would be a continuous time version of the growth process. If \u03b2 = 0, then CTMC X(t) is a collection of independent processes labelled by the vertices of graph G. In this case a component of the Markov chain is a continuous time birth-and-death process on Z+ (or, equivalently, a nearest neighbour random walk on Z+) that evolves as follows. Given state k \u2208 Z+ it jumps to k+1 with the rate e\u03b1k and jumps to k \u2212 1 (if k > 0) with the unit rate. Such a process is a special case of the birth-anddeath (BD) process on the set on non-negative integers. The long term behaviour of an integer valued BD process is well known. Namely, given a set of transition characteristics one can, in principle, determine whether the corresponding Markov chain MC is (positive) recurrent or (explosive, if the time is continuous) transient, and compute various other characteristics of the process. In particular, the general theory implies the following long term behaviour of the CTMC X(t) in the independent case (i.e. when \u03b2 = 0).\n\u2022 If \u03b1 < 0, then each component of X(t) is positive recurrent, and, hence, X(t) is positive recurrent.\n\u2022 If \u03b1 = 0, then each component ofX(t) is a reflected symmetric simple random walk on Z+, which is null recurrent. The CTMC X(t) is null recurrent if the number of components\nis either 1, or 2, and it is transient if the number of components is 3 or more.\n\u2022 If \u03b1 > 0, then each component of X(t) is explosive transient, and, hence, the CTMC X(t) is explosive transient.\nIf \u03b2 \u0338= 0, then the CTMC X(t) can be regarded as a system of interacting birth-and-death processes that are labelled by vertices of the graph and evolve subject to interaction determined by the parameter \u03b2. Note that the presence of interaction can significantly affect the collective behaviour of a system and produce effects that might be of interest in modelling the evolution of multicomponent random systems. The model provides a flexible and mathematically tractable choice for modelling various types of interaction. For example, if \u03b2 > 0, then the interaction between components is cooperative meaning that a positive component accelerates growth of its neighbours. In the case \u03b2 < 0 the interaction is competitive, since components suppress growth of each other.\nThe CTMC \u03be(t) was introduced in [24], where its long term behaviour was studied in some special cases. In full generality the long term behaviour of process was studied in [10]. Main results and research methods of these works are explained below."
        },
        {
            "heading": "4.2 Long term behaviour of the model",
            "text": "In this section we review the main results of [10] concerning the long term behaviour of the countable CTMC X(t). Recall countable CTMCs can be non-explosive transient, explosive transient, null recurrent and positive recurrent. It turns out that all these limit behaviours are realised in the case of the CTMC X(t) depending on parameters \u03b1, \u03b2 and on the structure of the underlying graph. In addition, the long term behaviour of the Markov chain is largely determined by a relationship between parameters \u03b1, \u03b2 and the largest eigenvalue of the graph.\nLet us give some definitions. Let A = (avu, v, u \u2208 V ) be the adjacency matrix of the graph G = (V,E), i.e. A is a symmetric matrix such that avu = auv = 0 for u \u2241 v and avu = auv = 1 for u \u223c v. Since A is symmetric, its eigenvalues are real. Denote them by \u03bb1(G) \u2265 \u03bb2(G) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn(G), so that \u03bb1 := \u03bb1(G) is the largest eigenvalue (Perron-Frobenius eigenvalue). It is well known that \u03bb1(G) > 0 (except the case when the graph has no edges). Not that in terms of the adjacency matrix A the birth rate in (4.9) can be written as follows\ne \u03b1xv+\u03b2 \u2211 u\u223cv xu = e\u03b1xv+\u03b2(Ax)v . (4.2)\nFurther, an independent set of vertices in a graph G is a set of the vertices such that no two vertices in the set are adjacent. The independence number \u03ba = \u03ba(G) of a graph G is the cardinality of the largest independent set of vertices.\nTheorem 4.1 below is an extract of [10, Theorem 2.3] that distinguishes between recurrence\nand transience.\nTheorem 4.1. Suppose that the graph G is connected and \u03b2 \u0338= 0.\n1. The CTMC X(t) is recurrent in the following two cases\n(a) \u03b1 < 0 and \u03b1 + \u03b2\u03bb1(G) < 0;\n(b) \u03b1 = 0, \u03b2 < 0 and \u03ba(G) \u2264 2.\n2. The CTMC X(t) is transient in all the cases below\n(a) \u03b1 > 0;\n(b) \u03b1 = 0 and \u03b2 > 0;\n(c) \u03b1 = 0, \u03b2 < 0 and \u03ba(G) \u2265 3;\n(d) \u03b1 < 0 and \u03b1 + \u03b2\u03bb1(G) \u2265 0.\nThe proof of the above result is greatly facilitated by the fact that the CTMC X(t) is reversible, which in turn, allows to apply the method of electric networks. This is explained in the next section."
        },
        {
            "heading": "4.3 Reversibility of the model",
            "text": "Let I be the unit V \u00d7 V matrix, let e \u2208 RV be the vector all components of which are equal to 1, and let (\u00b7, \u00b7) denote the Euclidean scalar product. Define functions\nQ(x) = \u22121 2 ((\u03b1I+ \u03b2A)x,x) = \u2212\u03b1 2 \u2211 v x2v \u2212 \u03b2 \u2211 v\u223cu xvxu, x = (xv, v \u2208 V ) \u2208 RV (4.3)\nS(x) = (x, e) = \u2211 v xv, x = (xv, v \u2208 V ) \u2208 RV , (4.4) W (x) = \u2212Q(x)\u2212 \u03b1 2 S(x), x = (xv, v \u2208 V ) \u2208 RV . (4.5)\nWe claim that the CTMC X(t) is reversible with respect to the invariant measure\neW (x) = e\u2212Q(x)\u2212 \u03b1 2 S(x) = e\n\u03b1 2 \u2211 u xu(xu\u22121)+\u03b2 \u2211 w\u223cu xwxu\n, x \u2208 ZV+, (4.6)\nIndeed, given v \u2208 V and x \u2208 ZV+ we have that\n\u2212Q(x)\u2212 \u03b1 2 S(x) + \u03b1xv + \u03b2(Ax)v = \u2212Q(x+ ev)\u2212 \u03b1 2 S(x+ ev).\nTherefore,\neW (x)e \u03b1xv+\u03b2 \u2211 u\u223cv xu = e\u2212Q(x)\u2212 \u03b1 2 S(x)e\u03b1xv+\u03b2(Ax)v = e\u2212Q(x+ev)\u2212 \u03b1 2 S(x+ev) = eW (x+ev)\nwhich, recalling (4.1), means that the detailed balance equation\neW (x)r\u03b1,\u03b2(x,y) = e W (y)r\u03b1,\u03b2(y,x) for x,y \u2208 ZV+, (4.7)\nholds for CTMC X(t) and the invariant measure (4.6).\nRemark 4.1. It should be noted that rewriting equation (4.8) as follows\ne\u03b1xveW (x) = e\u2212\u03b2 \u2211 u\u223cv xueW (x+ev), (4.8)\nshows that the measure eW (x), x \u2208 ZV+ is also invariant for the CTMC Y (t) = (Yv(t), v \u2208 V ) with the transition rates r\u0302\u03b1,\u03b2(x,y), x,y \u2208 ZV+ given by\nr\u0302\u03b1,\u03b2(x,y) =  e\u03b1xv , for y = x+ ev, and x = (xv, v \u2208 V ), e\u2212\u03b2 \u2211 u\u223cv xu , for y = x\u2212 ev and x = (xv, v \u2208 V ) : xv > 0,\n0, otherwise.\n(4.9)\nIt is shown in [10] that the long term behaviour of CTMC Y\u0302 (t) is largely the same as the long term behaviour of the CTMC X(t).\nReversibility of a Markov chain allows to apply the method of electric networks for determining whether the Markov chain is recurrent or transient. The idea is that recurrence/transience of the reversible Markov chain can be established by analysing the so called effective resistance of a certain electric network. In the case of the CTMC X(t) the corresponding electric network is defined as follows.\nThe CTMC X(t) can be interpreted as a nearest neighbour random walk on the lattice graph ZV+, i.e. the graph with vertices x = (xv, v \u2208 V : xv \u2208 Z+), where vertices x,y \u2208 ZV+ are connected by an edge if the Euclidean distance \u2225x\u2212y\u2225 = 1 (i.e. x and y are nearest neighbours on the lattice).\nThe electric network on this graph is obtained by assigning conductance (resistance\u22121) to each edge, which is done as follows. Given x \u2208 ZV+ and v \u2208 V assign the conductance eW (x), where the function U is defined in (4.5) for each edge (x\u2212 ev,x) with xv \u2265 1. In other words, the edge conductance is equal to the value of the invariant measure of the CTMC X(t) at the state x. The edge (0, ev) v \u2208 V , where 0 is the origin, is assigned the unit conductance. Resistance of an edge is defined the reciprocal of conductance.\nBy the general method, the CTMC X(t) is recurrent (transient), if the so called effective resistance of the described electric network on the graph ZV+ is infinite (finite). The effective resistance in this case if defined, loosely speaking, as the resistance between the origin 0 and \u201cinfinitite\u201d vertex (see [9] for details). It turns out that in the case of the CTMC X(t) the effective resistance of the electric network is relatively easy to estimate (see [10] for further details).\nFurther, the detailed balance equation can be solved for the model providing analytically tractable equation for the invariant measure. This allows to distinguish between null and positive recurrence."
        },
        {
            "heading": "4.4 Recurrent cases",
            "text": "The fact that the invariant measure of the Markov chain is known allows to distinguish between the null and positive recurrence in the recurrent cases of Theorem 4.1. This is done by analysing whether the invariant measure can be normalised to define the stationary distribution. A direct computation gives that in the case 1(a) of the theorem, i.e. when \u03b1 < 0 and \u03b1+ \u03b2\u03bb1(G) < 0, the invariant measure is summable, that is\nZ\u03b1,\u03b2,G := \u2211 x\u2208ZV+ eW (x) <\u221e. (4.10)\nRecalling that an irreducible CTMC is positive recurrent if and only if it has a stationary distribution and is non-explosive. Since a recurrent CTMC is non-explosive we immediately obtain that if \u03b1 < 0 and \u03b1 + \u03b2\u03bb1(G) < 0, then X(t) is positive recurrent with the stationary distribution given by\n\u00b5\u03b1,\u03b2,G(x) = 1\nZ\u03b1,\u03b2,G eW (x) for x = (xv, v \u2208 V ) \u2208 ZV+. (4.11)\nIn contrast, the Markov chain is null recurrent in the case 1(b) of the theorem. Indeed, if \u03b1 = 0, then\nZ\u03b1,\u03b2,G = \u2211 x\u2208ZV+ eW (x) \u2265 \u221e\u2211 k=0 eW (kev) = \u221e\u2211 k=0 1 = \u221e,\nwhere v \u2208 V is any given vertex, i.e. the stationary distribution does not exist in this case (regardless of other characteristics of the model). Therefore, in the case \u03b1 = 0 the Markov\nchain cannot be positive recurrent, and, hence, in the recurrent case when \u03b2 < 0 and \u03ba(G) \u2264 2 the Markov chain is just null recurrent.\nRemark 4.2. In addition, note in [24] positive recurrence of the CTMC X(t) was shown by using the Foster\u2019s criterion for positive recurrence in the case when \u03b1 < 0, \u03b2 > 0 and \u03b1 + \u03b2maxv\u2208V dv(G) < 0, where dv(G) is the number of neighbours of the vertex v \u2208 V , i.e. the number of vertices that are adjacent to v (the degree of the vertex v). The criterion was applied with the Lyapunov function f(x) = (Q(x),x), where Q is the quadratic function defined in (4.3)."
        },
        {
            "heading": "4.5 Transient cases",
            "text": "In the case of a transient CTMC it is natural to ask whether the CTMC is explosive. Note first that in the transient case 2(c) of Theorem 4.1 (i.e. when \u03b1 = 0, \u03b2 < 0 and \u03ba(G) \u2265 3) the CTMC X(t) is non-explosive. Indeed, it is easy to see that if \u03b1 = 0, \u03b2 < 0, then the transition rates are uniformly bounded by 1, and, hence, the process cannot be explosive. In addition, note that in general there are only two possible long term behaviours of the Markov chain if \u03b1 = 0 and \u03b2 < 0. Namely, by the results above CTMC \u03be(t) is either non-explosive transient or null recurrent, and this depends only on the independence number of the graph G.\nFurther, it was shown in [10, Lemma 6.3] that in the transient case \u03b1 < 0 and \u03b2 = \u2212\u03b1 \u03bb1(G) the CTMC X(t) is not explosive. Although the transition rates are unbounded in this transient case, the process tends to infinity by staying in a domain where the rates are bounded, which prevents the explosion. This effect is rather easy to understand in the case when the graph consists of just two adjacent vertices (see [24, Theorems 1 and 4]), when the process is a special case of non-homogeneous random walks in the quarter plane. In this case, if \u03b1 < 0 and \u03b2 = \u2212\u03b1, then the process is pushed away from the boundaries (where the rates can be arbitrarily large) towards the diagonal of the quarter plane, where the rates are bounded (see Figure 3). The same effect of non-explosion takes place in the general case, although its proof is not straightforward (see [10, Lemma 6.3]). It should be noted that the diagonal here is the line determined by\nthe vector (1, 1)T , which is the eigenvector of the adjacency matrix A = [ 0 1\n1 0\n] of the graph\nG = {1 \u223c 2}, that corresponds to the principle eigenvalue 1. If \u03b1 < 0 and \u03b2 = \u2212\u03b1, then the rates around the diagonal are unbounded and the process becomes explosive (see an open problem below concerning explosion in the general case).\nFurther, recall that dv(G) denotes the number of neighbours of a vertex v. It was shown\nin [24, Theorems 1 and 2] that the Markov chain is explosive in the following cases:\n(i) \u03b1 > 0, \u03b2 < 0;\n(ii) \u03b1 + \u03b2min v\u2208V dv(G) > 0 including subcases\n\u2013 \u03b1 > 0 and \u03b2 \u2265 0; \u2013 \u03b1 = 0 and \u03b2 > 0; \u2013 \u03b1 < 0 and \u03b2 > |\u03b1| minv\u2208V dv(G) .\nIn particular, in the cases 2(a), 2(b) of Theorem 4.1 the CTMC X(t) is explosive.\nOpen problem: explosion Recall that in general minv\u2208V dv(G) \u2264 \u03bb1(G), i.e. the maximal eigenvalue of a graph G is not less than the minimal vertex degree of the graph. The above results concerning explosions do not include the transient case when\n\u03b1 < 0 and \u2212 \u03b1 \u03bb1(G) < \u03b2 \u2264 \u2212 \u03b1 minv\u2208V dv(G) ,\nwhich remains unsolved. It was conjectured in [10] that in this case the CTMCX(t) is explosive. The conjecture is based on the intuition explained in the two-dimensional case above. Namely, that in this transient case the process escapes to infinity by \u201cfollowing\u201d the line {sv1 : s \u2208 R}, where v1 is the eigenvector corresponding to the largest eigenvalue \u03bb1(G), and the transition rates grow exponentially along this line."
        },
        {
            "heading": "4.6 Phase transition",
            "text": "There is a phase transition phenomenon in the long term behaviour of CTMC X(t) in the case \u03b1 < 0. Indeed, in this case we have the following classification of the process\u2019s behaviour\n\u2022 Let \u03b1 < 0.\n(i) If \u03b2 < \u2212 \u03b1 \u03bb1(G) , then X(t) is positive recurrent. This includes the case when \u03b2 = 0,\ni.e. when X(t) is formed by a collection of independent positive recurrent reflected\nrandom walks on Z+, and is thus positive recurrent. (ii) If \u03b2 = \u2212 \u03b1\n\u03bb1(G) , then X(t) is non-explosive transient.\n(iii) If \u2212 \u03b1 \u03bb1(G) < \u03b2 < \u2212 \u03b1 minv\u2208V dv(G) , then X(t) is transient. It is conjectured that X(t) is\nexplosive transient (see the open problem in the preceding section).\n(iv) If \u03b2 > \u2212 \u03b1 minv\u2208V dv(G) , then X(t) is explosive transient.\nIf \u03b2 < 0, then interaction in this case is competitive, as neighbours obstruct the growth of each other. The competition implies positive recurrence of the process (which is positive recurrent even without interaction). Suppose now that \u03b2 is positive. One could intuitively expect that if \u03b2 is not large, i.e. the cooperative interaction is not strong, so that the Markov chain is still positive recurrent. On the other hand, if \u03b2 > 0 is sufficiently large, then the intuition suggests that the Markov chain might become transient. It turns out, that \u03b2cr = |\u03b1|\n\u03bb1(G) is the critical\nvalue at which the phase transition occurs. Precisely at this value of \u03b2 the Markov chain is non-explosive transient. Moreover, it is conjectured that given \u03b1 < 0 the corresponding critical value \u03b2cr is the only value of the parameter \u03b2 when the Markov chain is non-explosive transient."
        },
        {
            "heading": "4.7 Examples",
            "text": "The largest eigenvalue of the adjacent matrix of the underlying graph plays essential role is determining the long term behaviour of the CTMC X(t). Estimation of this eigenvalue, and more generally, graph eigenvalues, is a very important problem in many applications. There are well known bounds for the largest eigenvalue \u03bb1, although its explicit value is known only in some special cases. Below we give several simple examples where the largest eigenvalue \u03bb1 can be computed explicitly, which allows to rewrite the conditions of Theorem 4.1 in the case \u03b1 < 0 in more explicit form.\nExample 4.1. IfG = (V,E) is with constant vertex degrees d, then \u03bb1(G) = minv\u2208V dv(G) = d. For example, d = 1 for a graph consisting of two adjacent vertices, and d = 2 in for a cycle graph with at least three vertices. In this case the Markov chain is positive recurrent if and only if \u03b1 < 0 and \u03b1+\u03b2d < 0. If \u03b1 < 0 and \u03b1+\u03b2d = 0, then the Markov chain is non-explosive transient, and if \u03b1+ \u03b2d > 0, then it is explosive transient. Figures 3 and 4 sketch directions of mean jumps of the process in the simplest case of just two interacting components.\nExample 4.2. Assume that the graph G is a star K1,m with m = n \u2212 1 non-central vertices, where m \u2265 1. A direct computation gives that \u03bb1 = \u221a m. Hence, the Markov chain is positive recurrent if and only if \u03b1 < 0 and \u03b1+ \u03b2 \u221a m < 0. If \u03b1 < 0 and \u03b1+ \u03b2 \u221a m \u2265 0, then the Markov chain is transient.\nExample 4.3. Consider a linear graph with n + 2 vertices, where n \u2208 Z+, that is a graph whose vertices can be enumerated by natural numbers 1, . . . , n + 2, and such that 1 \u223c 2 \u223c \u00b7 \u00b7 \u00b7 \u223c n+ 1 \u223c n+ 2. If n = 0, then this is the simplest case of a constant degree graph, and if\nn = 1, then this is the simplest case of a star graph. If n \u2265 2, then the adjacency matrix A of this graph is the tridiagonal matrix given below\nA =  0 1 O 1 0 1 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1\nO 1 0  (n+2)\u00d7(n+2)\nThis is the tridiagonal symmetric Toeplitz matrix which eigenvalues are given by\n\u03bbk = 2 cos\n( k\u03c0\nn+ 3\n) , k = 1, . . . , n+ 2.\nThe maximal eigenvalue is \u03bb1 = 2 cos ( \u03c0 n+3 ) . Thus, the CTMC X(t) is positive recurrent if and\nonly if \u03b1 < 0 and \u03b1 + 2\u03b2 cos (\n\u03c0 n+3\n) < 0. If \u03b1 < 0 and \u03b1 + 2\u03b2 cos ( \u03c0\nn+3\n) \u2265 0, then the Markov\nchain is transient.\nTwo next examples (from [10]) are the cases when the process is null recurrent, and this\nessentially is determined by the independence number \u03ba(G).\nExample 4.4. Let, as in Example 4.2, G be a star K1,m, wherem \u2265 1. Then \u03ba(G) = m = n\u22121. Assume that \u03b1 = 0 and \u03b2 < 0. Then, the Markov chain is null recurrent if n \u2264 3, and transient if n \u2265 4.\nExample 4.5. Let G be a cycle Cn, where n \u2265 3. Then \u03ba(G) = \u230an/2\u230b. Assume that \u03b1 = 0 and \u03b2 < 0. Then, the Markov chain is null recurrent if n \u2264 5, and transient if n \u2265 6."
        },
        {
            "heading": "4.8 The model with finite components",
            "text": "In this section we consider a version of the reversible model obtained by limiting the maximum number of particles at a vertex.\nAs before, let G = (V,E) be a finite connected graph with the adjacency matrix A. Let \u039bN = {0, ..., N}V , where N \u2265 1 is a given natural number. Consider a CTMC X\u0302(t) = (X\u0302v(t), v \u2208 V ) \u2208 \u039bN with transition rates r\u0302\u03b1,\u03b2(x,y), x,y \u2208 ZV+ given by\nr\u0302\u03b1,\u03b2(x,y) =  e\u03b1xv+\u03b2 \u2211 u\u223cv xu , for y = x+ ev and x = (xv, v \u2208 V ) : xv < N, 1, for y = x\u2212 ev and x = (xv, v \u2208 V ) : xv > 0, 0, otherwise.\n(4.12)\nwhere \u03b1, \u03b2 \u2208 R are given constants. In other words, the CTMC X\u0302(t) evolves precisely as the CTMC X(t) transition rates (4.1) subject to the constraint that at most N particles can be placed at a vertex.\nSimilarly to the CTMC X(t), the finite CTMC X\u0302(t) is irreducible and reversible with the\nstationary distribution \u00b5 (N) \u03b1,\u03b2,N(x), x \u2208 \u039bN , given by\n\u00b5 (N) \u03b1,\u03b2,G(x) =\n1\nZ\u03b1,\u03b2,G,N eW (x) for x = (xv, v \u2208 V ) \u2208 \u039bVN , (4.13)\nwhere the function U is defined in (4.5), and Z\u03b1,\u03b2,G,N = \u2211 x\u2208\u039bVN eW (x).\nBy the ergodic theorem for finite irreducible CTMC\u2019s the distribution of X\u0302(t) converges to the stationary distribution (4.13), as t\u2192 \u221e.\nRemark 4.3. Note that if N = 1 (in which case the parameter \u03b1 is redundant) the measure (4.13) is equivalent to a special case of the celebrated Ising model on the graph G. Indeed, the change of variables yv = 2xv \u2212 1 induces a probability measure on {\u22121, 1}V that is proportional to exp ( \u03b2 4 ( \u2211 v\u223cu yvyu + 2 \u2211 v yv) ) . The latter corresponds to the Ising model with the inverse temperature \u03b2/4 and the external field h = \u03b2/2 on the graph G.\nIn the rest of this section we show that, under certain assumptions, the probability measure (4.13) possesses monotonicity properties that are similar to those of the ferromagnetic Ising model.\nWe start with recalling some necessary definitions by adopting those from [6]. Let G = (V,E) be an arbitrary graph (not necessarily finite), and let, as before, \u039bN = {0, ..., N}V . Let FG,N be a standard \u03c3-algebra of subsets of \u039bN generated by cylinder sets (if the graph G = (V,E) is finite, then FG,N is just a set of all subsets of \u039bN). Define a partial order on the set \u039bN . Given x = (xv, v \u2208 V ) \u2208 \u039bN and x\u2032 = (x\u2032v, v \u2208 V ) \u2208 \u039bN we write x \u2264 x\u2032, if xv \u2264 x\u2032v for all v \u2208 G. A probability measure \u00b5 on (\u039bN ,FG,N) is said to be monotone if\n\u00b5(xv \u2265 k|x = z off v) \u2264 \u00b5(xv \u2265 k|x = y off v), (4.14)\nfor all v \u2208 V , k \u2208 {0, ..., N} and z,y \u2208 \u039bV \\{v},N such that z \u2264 y, \u00b5(x = z off v) > 0 and \u00b5(x = y off v) > 0.\nTheorem 4.2. Let \u03b2 > 0. Then the probability measure \u00b5 (N) \u03b1,\u03b2,G is monotone.\nProof of Theorem 4.2. Start with an auxiliary statement (which generalises Lemma 3.1 in [25]).\nProposition 4.1. Let P = (pk, k \u2208 Z+) and Q = (qk, k \u2208 Z+) be discrete probability measures on Z+. If piqj \u2264 pjqi for all 0 \u2264 j < i, then P({i : i \u2265 k}) \u2264 Q({i : i \u2265 k}) for k \u2265 1, i.e. the measure Q stochastically dominates the measure P.\nProof. A direct computation gives that\nP({i : i \u2265 k})\u2212 Q({i : i \u2265 k}) = \u221e\u2211 i=k pi \u2212 \u221e\u2211 i=k qi \u00b1 ( \u221e\u2211 i=k pi )( \u221e\u2211 i=k qi )\n= \u221e\u2211 i=k pi k\u22121\u2211 j=0 qj \u2212 \u221e\u2211 i=k qi k\u22121\u2211 j=0 pj = k\u22121\u2211 j=0 \u221e\u2211 i=k (piqj \u2212 pjqi) \u2264 0,\nfor k \u2265 1, as required.\nGiven a vertex v \u2208 V and configurations y, z \u2208 \u039bN,V \\{v} = {0, 1, ..., N}V \\{v}, such that y \u2264 z, define probability distributions\nP = (pk = \u00b5 (N) \u03b1,\u03b2,G(xv = k|x \u2261 y off v, k = 0, ..., N)\nand\nQ = (qk = \u00b5 (N) \u03b1,\u03b2,G(xv = k|x \u2261 z off v, k = 0, ..., N)\nand show that\nP({k,N}) \u2264 Q({k,N}) for k \u2208 {0, ..., N}. (4.15)\nA direct computation gives that\npk = e\n\u03b1k(k\u22121) 2 +k\u03b2(Ay)v\u2211N i=0 e \u03b1i(i\u22121) 2 +i\u03b2(Ay)v and\ne \u03b1k(k\u22121) 2 +k\u03b2(Az)v\u2211N\ni=0 e \u03b1i(i\u22121) 2 +i\u03b2(Az)v\n. (4.16)\nTherefore\npiqj \u2212 pjqi = e\u03b1\ni(i\u22121)+j(j\u22121) 2 ei\u03b2(Ay)v+j(Az)v ( 1\u2212 e(i\u2212j)\u03b2(A(z\u2212y))v )[\u2211N i=0 e \u03b1i(i\u22121) 2 +i\u03b2(Ay)v ] [\u2211N i=0 e \u03b1i(i\u22121) 2 +i\u03b2(Az)v\n] . Since zu \u2212 yu \u2265 0, we have that\n(A(z\u2212 y))v = \u2211 u\u223cv (zu \u2212 yu) \u2265 0,\nwhich gives 1\u2212e(i\u2212j)\u03b2(A(z\u2212y))v \u2264 0, and, hence, piqj \u2264 pjqi for 0 \u2264 j < i. Equation (4.15) is now follows from Proposition 4.1. Consequently, the measure \u00b5\n(N) \u03b1,\u03b2,G is monotone, as claimed.\nBy [6, Theorem 4.11], a monotone probability measure on (\u039bN ,FG,N) has positive correlations, that is \u00b5\n(N) \u03b1,\u03b2,G(A \u2229 B) \u2265 \u00b5 (N) \u03b1,\u03b2,G(A)\u00b5 (N) \u03b1,\u03b2,G(B) for any increasing events A,B \u2208 FG,N (an\nevent A \u2208 FG,N is said to be increasing if 1{x\u2208A} \u2264 1{x\u2032\u2208A} for x \u2264 x\u2032). It is well known (e.g. see [5], [6]) that positivity of correlations implies existence of the limit for the probability measure (4.13) in the large graph limit, i.e. as the underlying graph G indefinitely expands in an appropriate sense. For example, consider a sequence of graphs Gn given by d-dimensional cubes of volume n centered at the origin. If \u03b2 > 0, then the sequence of corresponding model distributions \u00b5 (N) \u03b1,\u03b2,Gn converges to a limit distribution, as n tends to infinity (convergence is understood in the sense of the weak convergence of finite-dimensional distributions). This limit measure corresponds, in terminology of statistical physics, to the so-called empty (zero) boundary conditions. Existence of a limit measure in the case of other fixed boundary conditions (e.g. when all spins on the boundary of a graph Gn are equal to N) can be shown similarly. Uniqueness of the limit measure, i.e. that the limit measure does not depend on the boundary conditions, is an open problem.\nWe are now going to show that the measure \u00b5 (N) \u03b1,\u03b2,G possesses a monotonicity property in the parameter \u03b2. Recall that given probability measures \u00b5 and \u00b5\u2032 on (\u039bN ,FG,N) the measure \u00b5 is said to be dominated by \u00b5\u2032 (\u00b5 \u2264 \u00b5\u2032), if \u00b5(A) \u2264 \u00b5\u2032(A) for every increasing event A \u2208 FG,N .\nTheorem 4.3. If \u03b21 \u2264 \u03b22, then \u00b5(N)\u03b1,\u03b21,G \u2264 \u00b5 (N) \u03b1,\u03b22,G .\nProof. Given x \u2208 \u039bN let pk = \u00b5\u03b1,\u03b21,N(xv = k|x) and qk = \u00b5\u03b1,\u03b22,N(xv = k|x) for k = 0, ..., N , and consider probability distributions P = (pk, k = 0, ..., N) and Q = (qk, k = 0, ..., N) on {0, ..., N}. By the Holley theorem (e.g. Theorem 4.8 in [6]), it suffices to show that P \u2264 Q, i.e. P({k,N}) \u2264 Q({k,N}) for k = 0, ..., N \u2212 1. Using equation (4.16), as in the proof of Theorem 4.2, we obtain that pk \u223c e \u03b1k(k\u22121) 2 +k\u03b21(Ax)v and qk \u223c e \u03b1k(k\u22121) 2 +k\u03b22(Ax)v , k = 0, ..., N .\nSince \u03b22 \u2212 \u03b21 \u2265 0 we have that \u03b22(Ax)v \u2212 \u03b21(Ax)v = (\u03b22 \u2212 \u03b21) \u2211 u\u2208V xu \u2265 0, and, hence,\npiqj \u2212 pjqi \u223c e\u03b1 i(i\u22121)+j(j\u22121) 2 ei\u03b21(Ax)v+j\u03b22(Ax)v ( 1\u2212 e(i\u2212j)[\u03b22(Ax)v\u2212\u03b21(Ax)v ] ) \u2264 0, if 0 \u2264 j < i.\nBy Proposition 4.1, P \u2264 Q, and the theorem follows."
        },
        {
            "heading": "5 CSA point process",
            "text": "In this section we consider a point process motivated by the CSA model. We call this process by the CSA point process. The construction of the CSA point process is reminiscent to the CSA time series model in Section 2. The key difference between the two models is that the CSA point process is a model for unordered point patterns, while the CSA time series model is a model for sequential point patterns.\nThe CSA point process is defined, similarly to other point processes, as a probability measure on the set of finite point configurations of a subset of Euclidean space. Such a measure is usually specified by a density with respect to the Poisson point process with the unit intensity.\nStart with some notations and definitions. Let D be a compact convex subset of Rd that has a positive Lebesgue measure. For n \u2265 1 define n\u2212point configuration in D as unordered set of points x = {x1, . . . , xn}, xi \u2208 D, i = 1, . . . , n, such that xi \u0338= xj i \u0338= j. Let F be a set of all finite point configurations in D including the empty set \u2205 (the empty set corresponds to n = 0). Let F be a \u03c3\u2212 algebra of subsets of F , such that all maps x \u2192 |x \u2229B|, where B \u2286 D and | \u00b7 | is the cardinality of a discrete set, are measurable with respect to F .\nLet P\u03a0 be the distribution of Poisson point process with the unit intensity on the set D,\ni.e. it is the probability measure on (F,F) given by\nP\u03a0(A) = e \u2212|D| ( 1{\u2205\u2208A} + \u221e\u2211 n=1 1 n! \u222b Dn 1{{x1,...,xn}\u2208A}dx1 . . . dxn ) , A \u2208 F , (5.1)\nwhere 1B denotes the indicator function of set B.\nThe CSA point process with the interaction radius R > 0 and parameters (\u03b2m \u2265 0, m \u2208 Z+) is a probability measure on (F,F) specified by the following density (with respect to measure (5.1))\nf(x) = Z\u22121 \u220f xk\u2208x \u03b2\u03bd(xk,x), (5.2)\nwhere \u03bd(xk,x) is the number of neighbours of a point xk in a finite point configuration x = {x1, ..., } (defined in (2.1)),\nZ = e\u2212|D| ( 1 + \u221e\u2211 n=1 1 n! \u222b Dn n\u220f k=1 \u03b2\u03bd(xk,x)dx1 . . . dxn ) , (5.3)\ni.e. Z is the normalising constant.\nThe process is well defined if Z < \u221e. It was shown in [8, Lemma 1] that, if there exists a constant C > 0, such that \u03b2m \u2264 Cm\u03b1 for all m with some \u03b1 < 1, then the CSA point process (5.2) is well-defined.\nIt turns out that the CSA point process is a special case of the class of interacting neighbours (INP) point process (introduced in [7]). An INP process is specified by a density (with respect to the Poisson point process with the unit intensity) proportional to a function of the form\u220f xk\u2208x g(xk,x), where, in turn, g : D \u00d7 F \u2192 R+ is a non-negative measurable function. The density (5.2) is obtained by setting\ng(x, z) = \u2211 m\u22650 \u03b2m1{|z|=m}.\nIt is easy to see that the construction of the CSA point process is reminiscent to the construction of CSA model for time series of spatial locations in Section 2. By this similarity the CSA point process is also useful for modelling a wide spectrum of point configurations from regular ones to various clustered point patterns.\nSome special cases of the CSA point process are the well known in spatial statistics point\nprocesses. Consider several examples.\n1. A Poisson point process in the domain D with the intensity \u03b2 > 0 is obtained for \u03b2i \u2261 \u03b2, i \u2265 0.\n2. Assume that \u03b20 = \u03b2 and \u03b2i = 0 for i \u2265 1. The corresponding process is the well known process with hard core interaction of intensity \u03b2 and the interaction radius R. Realisations\nof a hard processes are point patterns, in which the distance between any two points is not less than the interaction radius R, i.e. a point has no neighbours.\n3. A natural generalisation of the process with the hard core interaction is the CSA process\nwith a finite number of non zero parameters, that is \u03b2i > 0 for 0 \u2264 i \u2264 N and \u03b2i = 0 for i > N , where N \u2265 0 is a given integer (if N = 0, then we obtain the process with the hard core interaction). Realisations of such a process are point patterns, in which a point can have no more than N neighbours.\n4. The famous in spatial statistics Strauss point process with the parameters \u03b1 > 0 and\n0 < \u03b3 < 1 is obtained by setting \u03b2i = \u03b1\u03b3 i/2, i \u2265 0. Traditionally, its distribution is specified by a density (with respect to Poisson point process with the unit intensity) proportional to the function \u03b1|x|\u03b3s(x), where s(x) is the number of pairs of neighbours in\nthe configuration x. It is easy to see that s(x) = 1/2 \u2211\nxk\u2208x \u03bd(xk,x), i.e. the density (5.2)\nis the density of the Strauss process for the indicated choice of the parameters.\nConsider the CSA point process with a finite number of non zero \u03b2\u2212parameters, i.e. the process in the item 3 above. Parameters of this process can be estimated by adopting the estimation procedure described in the case of the CSA time series model in Section 2.3. Namely, assume, as in Section 2.3 that the interaction radius R is known (or, somehow estimated). Then, given an observation x = {x1, ..., xn} the parameter N (the number of non-zero \u03b2\u2212parameters)\ncan be estimated by N\u0302 = maxx\u2208x \u03bd(x,x). Non-zero parameters \u03b2 can be estimated by using MLE. However, unlike the CSA time series model, the computation of MLE estimators here is not so straightforward. The difficulty is that the computation of the model likelihood in the case of the CSA point process requires the computation of the normalising constant (5.3) which is not analytically tractable. This is the well-known common problem in MLE estimation of parameters of a point process given by a density with respect to the Poisson point process. The normalising constant can be computed/estimated numerically by using the Markov chain Monte-Carlo method. Implementation of the latter is not straightforward for point processes and requires advanced simulation techniques (e.g. the method of perfect simulation). This is in contrast to the case of the CSA time series model, where the classic Monte-Carlo is rather effective (see Remark 2.4)."
        }
    ],
    "title": "Probabilistic models motivated by cooperative sequential adsorption",
    "year": 2023
}