{
    "abstractText": "Code pre-trained models (CodePTMs) have significantly advanced the field of neural code intelligence. Despite their capabilities, these models are susceptible to adversarial attacks that subtly modify the model inputs, resulting in incorrect outputs or predictions. Previous methods of robustness evaluation for CodePTMs primarily stem from a textual perspective, without explicitly taking into account the structure of the code. Furthermore, prior studies fail to encompass a broad enough spectrum of tasks and models. In this paper, we propose a set of novel robustness evaluation methods based on the intrinsic structure of the code. Specifically, we first launch adversarial attacks on crucial identifier tokens and sub-tree structures to explore the impact of imperceptible perturbation. Then, we perform global restructuring of the code using different traversal methods for abstract syntax trees, aiming to explore the model\u2019s sensitivity to input samples with equivalent information. Moreover, for each scenario, we employ adversarial training methods to explore the possibility of restoring the performance of perturbed models. For both code understanding and generation, our proposed method has demonstrated its effectiveness across a wide range of models and tasks, thereby allowing us to make one step forward in our understanding of the inner mechanisms of CodePTMs. Our codes and data are publicly available at https://github.com/nchen909/ CodeRobustness.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nuo Chen"
        },
        {
            "affiliations": [],
            "name": "Qiushi Sun"
        },
        {
            "affiliations": [],
            "name": "Jianing Wang"
        },
        {
            "affiliations": [],
            "name": "Ming Gao"
        },
        {
            "affiliations": [],
            "name": "Xiaoli Li"
        },
        {
            "affiliations": [],
            "name": "Xiang Li"
        }
    ],
    "id": "SP:96fa3a8bd24737a183f28672fa243f64bc9e1c1d",
    "references": [
        {
            "authors": [
                "Wasi Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang."
            ],
            "title": "Unified pre-training for program understanding and generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Uri Alon",
                "Omer Levy",
                "Eran Yahav"
            ],
            "title": "code2seq: Generating sequences from structured representa",
            "year": 2019
        },
        {
            "authors": [
                "Hubert Baniecki",
                "Przemyslaw Biecek"
            ],
            "title": "Adversarial attacks and defenses in explainable artificial intelligence: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Yonatan Bisk."
            ],
            "title": "Synthetic and natural noise both break neural machine translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Anirban Chakraborty",
                "Manaar Alam",
                "Vishal Dey",
                "Anupam Chattopadhyay",
                "Debdeep Mukhopadhyay"
            ],
            "title": "Adversarial attacks and defences: A survey",
            "year": 2018
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde",
                "Jared Kaplan",
                "Harri Edwards",
                "Yura Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374",
            "year": 2021
        },
        {
            "authors": [
                "Nuo Chen",
                "Qiushi Sun",
                "Renyu Zhu",
                "Xiang Li",
                "Xuesong Lu",
                "Ming Gao."
            ],
            "title": "CAT-probing: A metricbased approach to interpret how pre-trained models for programming language attend code structure",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Yangruibo Ding",
                "Luca Buratti",
                "Saurabh Pujar",
                "Alessandro Morari",
                "Baishakhi Ray",
                "Saikat Chakraborty."
            ],
            "title": "Towards learning (dis)-similarity of source code from program contrasts",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "Hsiao-Wuen Hon."
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "Advances in Neural Information Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Zhangyin Feng",
                "Daya Guo",
                "Duyu Tang",
                "Nan Duan",
                "Xiaocheng Feng",
                "Ming Gong",
                "Linjun Shou",
                "Bing Qin",
                "Ting Liu",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "CodeBERT: A pre-trained model for programming and natural languages",
            "venue": "Findings of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "year": 2015
        },
        {
            "authors": [
                "Daya Guo",
                "Shuai Lu",
                "Nan Duan",
                "Yanlin Wang",
                "Ming Zhou",
                "Jian Yin."
            ],
            "title": "UniXcoder: Unified crossmodal pre-training for code representation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Hamel Husain",
                "Ho-Hsiang Wu",
                "Tiferet Gazit",
                "Miltiadis Allamanis",
                "Marc Brockschmidt."
            ],
            "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "venue": "arXiv preprint arXiv:1909.09436.",
            "year": 2019
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Ioannis Konstas",
                "Alvin Cheung",
                "Luke Zettlemoyer."
            ],
            "title": "Mapping language to code in programmatic context",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1643\u20131652, Brussels, Bel-",
            "year": 2018
        },
        {
            "authors": [
                "Mohit Iyyer",
                "John Wieting",
                "Kevin Gimpel",
                "Luke Zettlemoyer."
            ],
            "title": "Adversarial example generation with syntactically controlled paraphrase networks",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Akshita Jha",
                "Chandan K Reddy."
            ],
            "title": "Codeattack: Code-based adversarial attacks for pre-trained programming language models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2023
        },
        {
            "authors": [
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Adversarial examples for evaluating reading comprehension systems",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021\u20132031, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Aditya Kanade",
                "Petros Maniatis",
                "Gogul Balakrishnan",
                "Kensen Shi."
            ],
            "title": "Learning and evaluating contextual embedding of source code",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Dongyeop Kang",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Eduard Hovy."
            ],
            "title": "AdvEntuRe: Adversarial training for textual entailment with knowledge-guided examples",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Ruotian Ma",
                "Qipeng Guo",
                "Xiangyang Xue",
                "Xipeng Qiu."
            ],
            "title": "BERT-ATTACK: Adversarial attack against BERT using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Sen Huang",
                "Johannes Welbl",
                "Sven Gowal",
                "Alexey Cherepanov",
                "James Molloy",
                "Daniel J. Mankowitz",
                "Esme Sutherland Robson",
                "Pushmeet Kohli",
                "Nando de Freitas",
                "Koray Kavukcuoglu",
                "Oriol Vinyals"
            ],
            "title": "Competition-level code generation with alpha",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "ORANGE: a method for evaluating automatic evaluation metrics for machine translation",
            "venue": "COLING, pages 501\u2013507.",
            "year": 2004
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "daresan",
                "Shao Kun Deng",
                "Shengyu Fu",
                "Shujie Liu"
            ],
            "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664",
            "year": 2021
        },
        {
            "authors": [
                "Lili Mou",
                "Ge Li",
                "Lu Zhang",
                "Tao Wang",
                "Zhi Jin"
            ],
            "title": "Convolutional neural networks over tree structures",
            "year": 2016
        },
        {
            "authors": [
                "Anh Tuan Nguyen",
                "Tung Thanh Nguyen",
                "Tien N Nguyen."
            ],
            "title": "Divide-and-conquer approach for multi-phase statistical migration for source code (t)",
            "venue": "ICASE, pages 585\u2013596. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Xipeng Qiu",
                "TianXiang Sun",
                "Yige Xu",
                "Yunfan Shao",
                "Ning Dai",
                "Xuanjing Huang."
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "SCIENCE CHINA Technological Sciences, 63(10):1872\u20131897.",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Ren",
                "Daya Guo",
                "Shuai Lu",
                "Long Zhou",
                "Shujie Liu",
                "Duyu Tang",
                "Ming Zhou",
                "Ambrosio Blanco",
                "Shuai Ma."
            ],
            "title": "Codebleu: a method for automatic evaluation of code synthesis",
            "venue": "arXiv preprint arXiv:2009.10297.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Svajlenko",
                "Judith F Islam",
                "Iman Keivanloo",
                "Chanchal K Roy",
                "Mohammad Mamun Mia."
            ],
            "title": "Towards a big data curated benchmark of inter-project code clones",
            "venue": "2014 IEEE International Conference on Software Maintenance and Evolution, pages 476\u2013",
            "year": 2014
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30, pages 5998\u20136008. Cur-",
            "year": 2017
        },
        {
            "authors": [
                "Yao Wan",
                "Wei Zhao",
                "Hongyu Zhang",
                "Yulei Sui",
                "Guandong Xu",
                "Hai Jin."
            ],
            "title": "What do they capture? a structural analysis of pre-trained language models for source code",
            "venue": "Proceedings of the 44th International Conference on Software Engineering, page",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Haohan Wang",
                "Diyi Yang."
            ],
            "title": "Measure and improve robustness in NLP models: A survey",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Yue Wang",
                "Weishi Wang",
                "Shafiq Joty",
                "Steven C.H. Hoi."
            ],
            "title": "CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Frank F. Xu",
                "Uri Alon",
                "Graham Neubig",
                "Vincent Josua Hellendoorn."
            ],
            "title": "A systematic evaluation of large language models of code",
            "venue": "Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022, page 1\u201310, New",
            "year": 2022
        },
        {
            "authors": [
                "Yichen Xu",
                "Yanqiao Zhu"
            ],
            "title": "A survey on pretrained language models for neural code intelligence",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Yang",
                "Jieke Shi",
                "Junda He",
                "David Lo."
            ],
            "title": "Natural attack for pre-trained models of code",
            "venue": "Proceedings of the 44th International Conference on Software Engineering, ICSE \u201922, page 1482\u20131493, New York, NY, USA. Association for Computing",
            "year": 2022
        },
        {
            "authors": [
                "Noam Yefet",
                "Uri Alon",
                "Eran Yahav."
            ],
            "title": "Adversarial examples for models of code",
            "venue": "Proc. ACM Program. Lang., 4(OOPSLA).",
            "year": 2020
        },
        {
            "authors": [
                "Daoguang Zan",
                "Bei Chen",
                "Fengji Zhang",
                "Dianjie Lu",
                "Bingchao Wu",
                "Bei Guan",
                "Yongji Wang",
                "JianGuang Lou"
            ],
            "title": "Large language models meet nl2code: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Huangzhao Zhang",
                "Zhuo Li",
                "Ge Li",
                "Lei Ma",
                "Yang Liu",
                "Zhi Jin."
            ],
            "title": "Generating adversarial examples for holding robustness of source code processing models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):1169\u20131176.",
            "year": 2020
        },
        {
            "authors": [
                "Jie Zhang",
                "Wei Ma",
                "Xiaofei Xie",
                "Qiang Hu",
                "Yang Liu"
            ],
            "title": "Black-box adversarial attack guided by model behavior for programming pre-trained language models",
            "year": 2023
        },
        {
            "authors": [
                "Yaqin Zhou",
                "Shangqing Liu",
                "Jingkai Siow",
                "Xiaoning Du",
                "Yang Liu."
            ],
            "title": "Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks",
            "venue": "Advances in Neural Information Processing Systems, volume 32.",
            "year": 2019
        },
        {
            "authors": [
                "Yi Zhou",
                "Xiaoqing Zheng",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang",
                "Xuanjing Huang."
            ],
            "title": "Defense against synonym substitution-based adversarial attacks via Dirichlet neighborhood ensemble",
            "venue": "Proceedings",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14857\u201314873 December 6-10, 2023 \u00a92023 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models have revolutionized the landscape of natural language processing (NLP) (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Qiu et al., 2020, inter alia). While these transformer-based models (Vaswani et al.,\n\u2217 Work done while interning at Institute for Infocomm Research, A*STAR.\n\u2020 Equal advising.\n2017) have achieved great success in NLP, their counterparts trained on code (Feng et al., 2020; Guo et al., 2021) have also made remarkable strides in the field of neural code intelligence (Xu and Zhu, 2022; Xu et al., 2022; Zan et al., 2023). Despite their impressive capabilities, CodePTMs still retain prevalent weaknesses inherent in language models: they are sensitive to the input sequence and are susceptible to adversarial attacks. The model\u2019s vulnerability to variations in input can impair its generalization (Wang et al., 2022; Baniecki and Biecek, 2023). Adversarial examples, though imperceptible to humans, can deceive CodePTMs into generating incorrect predictions or code sequences in downstream tasks (e.g., clone detection, code summarization). Unlike similar attacks for images, audio, and natural languages, the structured nature of programming languages introduces distinct and novel challenges. While methods have been proposed by researchers as potential countermeasures against attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Jia and Liang, 2017; Kang et al., 2018; Zhou et al., 2021), the construction of adversarial samples remains an ongoing concern. Adversarial training often involves exposing the model to adversarial examples during training so that the model learns to defend itself against such examples when it encounters them in the future. However, for CodePTMs, an approach that incorporates code structure is yet to be substantially established.\nIn representative works on robustness analysis targeting code scenarios, Yang et al. (2022) pioneer an example generation method that balances both natural semantic and operational semantics. Recently, Jha and Reddy (2023) leverage the structure of code to propose code-specific adversarial samples generation, which can be used to evaluate the vulnerabilities of CodePTMs. While these studies concentrate on generating adversarial code samples, there is an absence of explicit modeling of the structure of code. Furthermore, the existing\n14857\nanalyses remain inadequate in examining different architectures of CodePTM and corresponding downstream tasks, which in turn, hampers the generalizability of the conclusions drawn.\nIn this paper, we propose a comprehensive framework based on the structural information of the code, which integrates sample generation and adversarial training, aiming to conduct a thorough evaluation of the robustness of CodePTMs.\nWe first conduct an assessment through the perturbation of model inputs. Specifically, we propose two strategies: (1) Generating adversarial samples that are imperceptible to humans to launch adversarial attacks on the model. (2) Leveraging the syntax of the code, namely, the structural information of AST to reconstruct the input sequence into a new one that preserves equivalent information to probe models\u2019 sensitivity to input.\nThen, inspired by adversarial training, we trained our model on samples constructed from code structures, significantly enhancing the model\u2019s robustness in the face of structural attacks. Additionally, we conduct an in-depth analysis of the experimental results and validate that the methods to counter various types of attacks are generalizable.\nOur contributions can be summarized as follows: \u2022 By perturbing text while maintaining equiva-\nlent information, and leveraging adversarial attacks, we unveil both the vulnerabilities and sensitivities of various CodePTMs.\n\u2022 We utilize a range of adversarial training approaches to recover the performance of disturbed models and conduct an in-depth examination of the unique attributes displayed by different models across a spectrum of tasks.\n\u2022 Experiments on extensive code-related tasks across different programming languages\ndemonstrate the effectiveness of our method."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Code Pre-trained Models",
            "text": "Following the success of pre-trained language models (Qiu et al., 2020) in NLP, code pre-trained models (CodePTMs) have recently demonstrated remarkable success in a wide range of downstream tasks in the domain of neural code intelligence. By pretraining on massive code-based data (e.g. GitHub repositories), these models can learn rich contextual representations that can be transferred to code-related downstream tasks. Feng et al. (2020) use bimodal data from CodeSearchNet (Husain et al., 2019) to train CodeBERT that shares the same model architecture as RoBERTa (Liu et al., 2019). Then, GraphCodeBERT also uses the same architecture while additionally considering the inherent structure of code, specifically, the data flow graph. There are also models with encoder-decoder architectures, such as CodeT5 (Wang et al., 2021) and PLBART (Ahmad et al., 2021), which inherit the multi-task training strategies of T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). UniXcoder (Guo et al., 2022) harness the UniLM (Dong et al., 2019) architecture and is pre-trained on crossmodal data to support both code understanding and generation. Moreover, decoder-only CodePTMs are crafted to generate high-quality code sequences, which can be utilized for program synthesis (Chen et al., 2021) and even excel in programming competitions (Li et al., 2022)."
        },
        {
            "heading": "2.2 Adversarial Attacks for Language Models",
            "text": "Despite the remarkable achievements of language models existing literature reveals their susceptibility to adversarial samples, which involve subtle perturbations to initial inputs (Chakraborty et al., 2018). In NLP, this technique was initially employed to evaluate the robustness of models across different tasks (Jia and Liang, 2017; Iyyer et al., 2018; Belinkov and Bisk, 2018, inter alia). With the rise of pre-training, BERTAttack (Li et al., 2020) is first proposed that uses pre-trained language models to generate adversarial samples. For the scenario of code-related tasks, methods for AST-based neural networks was first proposed (Yefet et al., 2020; Zhang et al., 2020). Then, Yang et al. (2022) adversarially transform inputs to make victim CodePTMs produce wrong outputs while considering the natural semantics\nof code. Recently, CodeAttack (Jha and Reddy, 2023) utilizes code structure to generate adversarial samples for evaluating the vulnerabilities of CodePTMs. Zhang et al. (2023) harnesses the uncertainty of CodePTMs\u2019 output, utilizing it to guide searching for adversarial examples through variable name substitution. Distinct from these methods, we have adapted various attack strategies to fit the code scenarios. Moreover, we concurrently examine the vulnerabilities of both code generation and understanding tasks for CodePTMs across different architectures."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Adversarial Attacks on Code Pre-trained Models",
            "text": "Subtree Attack. Randomly drop a non-leaf node and its descendants from a code snippet\u2019s parsed AST. In the AST, non-leaf nodes typically represent higher-level constructs of the code, such as conditional statements, function definitions, and so forth. Deleting a non-leaf node along with all its child nodes may imply the disruption of a part of the code, such as a loop, a branch of a conditional statement, or a function call. Specifically, as shown in Algorithm 1, a code snippet is first parsed into an AST with all non-leaf nodes identified. Then, one of them is randomly selected and dropped, along with all of its child nodes. Following this, the modified AST is re-converted back into code snippets. In the rare cases where the AST has no non-leaf nodes, the original code snippet is directly returned.\nSignature Attack. Unlike natural language, the signature of a function often contains more information than other tokens in the sequence. Thus, we\nAlgorithm 1 Subtree Attack Input: Code snippet c Output: Modified code snippet c\u2032\n1: procedure DROPSUBTREE(c) 2: T \u2190 GetAST(c) 3: leaf_parents \u2190 GetLeafnodesParents(T ) 4: if leaf_parents \u2260 None then 5: parent_to_drop \u2190 RandomChoose(leaf_parents) 6: RemoveChildrens(T, parent_to_drop) 7: c\n\u2032 \u2190 ASTtoCode(T ) 8: return c\u2032\n9: else 10: return c 11: end if 12: end procedure\npropose another approach to straightforwardly constructing adversarial samples that involve randomly replacing the signature of an input function with another word from the vocabulary. Although altering the function signature does not change the intrinsic logic of the function, it can modify the code snippets\u2019 context. This subtle change could present challenges to CodePTMs that seek to understand code at a semantic level. For instance, suppose we have a function with the signature add(a, b), which is used in the code to perform additional operations. If we change this function\u2019s signature to subtract(a, b), the intrinsic logic of the function (performing addition operations) remains unchanged. However, the semantics of the function undergoes a significant transformation. Sequential models would typically expect the subtract function to perform subtraction operations, not addition."
        },
        {
            "heading": "3.2 Reconstruction",
            "text": "To investigate the sensitivity of CodePTMs to inputs, we considered a framework for introducing perturbations to the code while maintaining an equivalent amount of information. The primary difference between the sequences generated from AST traversal and the original code lies in their structure and order. In the original code, tokens appear in the order they are found in the source code. However, in the sequence generated by traversing the AST using DFS or BFS, the order of the tokens reflects the structure of the code. In a DFS traversal, the order of the tokens is closer to the actual execution order, while in a BFS traversal, the order mainly reflects the hierarchical structure of the code. Furthermore, although the sequence generated by traversing the AST contains all tokens from the original code, it may not fully retain the semantic information of the original code due to the loss of original structural information. For instance, the condition of a conditional statement and its body might be separated, making their relationship less apparent in the sequence.\nPrevious work has repeatedly demonstrated that code models can capture structural information beneath the textual level (Wan et al., 2022; Chen et al., 2022). Thus, we believe that robust models should be able to extract the necessary information from these \u201creconstructed\u201d sequences. The procedure is demonstrated in Algorithm 2, where m indicates using BFS or DFS for traversal. The details of AST traversal are shown in Algorithm 3."
        },
        {
            "heading": "3.3 Adversarial Training",
            "text": "Following the algorithms proposed in section 3.1, we now introduce adversarial samples during the\nAlgorithm 2 Reconstruction By Traversal Input: Code snippet c, Traversal mode m Output: Modified code snippet c\u2032\n1: procedure TRAVERSAL RECONSTRUCTION(c,m)\n2: T \u2190 GetAST(c) 3: token_list \u2190 list() 4: for node in traverse(c, m) do 5: if node.out_degree = 0 and node.in_degree = 1 then 6: token_list.append(node.value) 7: end if 8: end for 9: c\n\u2032 \u2190 ListtoCode(token_list) 10: return c\u2032 11: end procedure\ntraining process, enabling the model to be robust enough to make reliable predictions or generate correct sequences when confronted with these intentionally designed perturbations.\nWe first denote the CodePTM as M . Given an input x and corresponding target label or sequence y, we define the loss function on original inputs as L(M,x, y) and on adversarial examples as Ls(M,x, y). The construction of adversarial examples, e.g., the use of sub-tree attack to generate samples, is represented by s, which transforms x into a new sample while keeping y unchanged. In the procedure of adversarial training, our goal is to find model parameters that minimize the sum of losses on the original inputs and the structured adversarial examples. This objective can be represented as follows:\nmin M {L(M,x, y) + Ls(M,x, y)} (1) The sum of these two losses signifies the total loss of the model under both normal and adversarial conditions, aiming to enhance the model\u2019s robustness against adversarial attacks. To enhance model robustness more effectively, we shuffle the generated adversarial samples randomly and then proceed with training, preventing them from forming dependencies with the original samples during the training process."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Tasks and Datasets We conduct our experiments on four tasks related to code representation learning, as part of the CodeXGLUE benchmark (Lu et al., 2021). In the realm of code generation, our first trial is on code summarization (Alon et al., 2019), a process aiming to generate natural language comments for a given code snippet. The second one within code generation is code translation (Nguyen et al., 2015), which involves translating a code snippet from one programming language to another. For code understanding, we delve into Clone detection (Svajlenko et al., 2014; Mou et al., 2016), which quantifies the similarity between different code snippets, and defect detection (Zhou et al., 2019), a task focused on predicting the presence of vulnerabilities in the source code, with the potential to pose risks software systems.\nImplementation Details In our experiments, we utilized four representative code training models: GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We employed Tree-sitter1 to parse the source code into ASTs. The training procedure involved using the Adam optimizer (Kingma and Ba, 2015) with a warm-up period of 1,000 steps. Our experiments were conducted using PyTorch 1.5.1 on 4 interconnected NVIDIA RTX 3090 GPUs. The hyperparameters are listed in section B."
        },
        {
            "heading": "4.2 Main Results of Attack",
            "text": "Code Understanding As is shown in Table 1, for both clone detection and defect detection tasks, the models exhibit different behaviors under various scenarios. In clone detection, GraphCodeBERT and CodeT5 demonstrate the highest performance\n1github.com/tree-sitter\nunder full fine-tuning. However, when exposed to various attack strategies, all models experience a decrease in performance. Interestingly, PLBART stands out by showing strong robustness, with its performance being particularly resilient under the bfs attack scenario.\nFrom the perspective of attacking strategies, both two reconstruction attacks significantly impact all models in both tasks, indicating the models are hard to understand the sequences with equivalent information in other formats. While under the subtree attack, all models\u2019 performance in both tasks is negatively impacted but not as severely as the structural attacks. At last, the Signature attack has variable effects on models. For Clone Detection, GraphCodeBERT manages to maintain a high performance. Notably, for defect detection, PLBART significantly outperforms other models under this\ntype of attack, achieving an accuracy of 94.42.\nCode Generation In the task of code translation for Java \u2194 C#, CodeT5 performs best under normal fine-tuning scenarios. While under different attack strategies, the performance of all models drops drastically, with UniXcoder often presents the highest performance, especially under both BFS and DFS attacks. This suggests UniXcoder is relatively more robust against these types of structural attacks for the code translation task.\nStructural attacks of DFS and BFS cause a drastic decline in performance for all models. Despite this, UniXcoder exhibits relative resilience, managing to maintain the highest performance among all models under these attack scenarios. This implicitly suggests that the training procedure of UniXcoder, which models the AST directly is conducive to model robustness. For the subtree and signature attacks, all models see a decrease in performance, but not as drastic as under the aforementioned DFS/BFS attacks. Nevertheless, CodeT5 consistently outperforms other models under both these attack types. Under the subtree attack, CodeT5 achieves the highest performance, indicating strong robust-\nness. Similarly, for the signature attack, CodeT5 maintains a stable performance that holds the highest scores. These results suggest that CodeT5 may have particular resistance to these types of attacks in the context of code translation, likely due to its pre-training with dataflow information.\nIn specific cases, it can be observed that the EM metric in the code translation task drops to near zero. Through case studies, we find that this occurs when the model is confronted with a perturbed sequence, its output can be an empty string.\nThe results of different attack strategies are given in Table 2. Across all programming languages, CodeT5 still consistently outperforms other models in terms of BLEU scores, suggesting its steady performance under various scenarios. Interestingly, the models appear to have difficulties summarizing Ruby code compared to others, which might be attributed to the modest dataset size for this language. On the other hand, summarizing PHP codes is clear to be a task in which all models excel under all kinds of perturbations, likely due to the larger volume of available data, offering models a richer context for capturing the semantics.\nConsidering the viewpoint of attacking strate-\ngies, traversal attacks, namely BFS and DFS, inflict significant damage to the models\u2019 performance, with BFS attacks typically causing a more profound impact than DFS. This could be ascribed to the fact that the sequences obtained through DFS have an expression closer to the actual execution order of the code, which aligns more closely with the semantics of the code. Notwithstanding, the models exhibit increased robustness against subtree and signature attacks, maintaining higher performance under these conditions."
        },
        {
            "heading": "4.3 Main Results of Adversarial Training",
            "text": "The results of adversarial training are demonstrated in Table 3 and Table 4. For code understanding tasks, it is clear that significant robustness enhancement can be observed for the tasks of clone detection and defect detection. The post-training performance displays considerable resilience and recovery after structural attacks across all CodePTMs. CodeT5, GraphCodeBERT, and PLBART notably improved their performance in both tasks, with CodeT5 generally in the leading position. Although UniXcoder trailed behind in terms of performance, it still exhibited some improvement post-training.\nFor the cases of enhancing the robustness of models performing the code translation task, all the models demonstrate substantial improvement after the adversarial training. The performance rebound is especially significant in the signature attack scenario where models like CodeT5 reach near fine-tuning results when being exposed to adversarial examples. Moreover, CodePTMs no longer churn out empty strings when faced with perturbed sequences, preventing the previous catastrophic performance decline.\nFinally, we can also observe notable improvements after comparing Table 2 and Table 4, showcasing the potential efficacy of adversarial training in enhancing model robustness for the code summarization task. All four CodePTMs exhibit enhancement in robustness, and it is noteworthy that the improvements against the dfs and bfs attacks are significant. These two structural attacks can lead to more severe disruptions to the models, causing larger drops in performance. Therefore, when adversarial training is applied, the resulting improvements can appear more noticeable.\nIn a nutshell, adversarial training serves as an effective approach for strengthening model robustness and recovery from adversarial attacks. The\nlevel of improvement, however, varies depending on the model and the specific task. Despite these variations, the trend of enhanced performance posttraining remains consistent across all models and tasks, highlighting the value of our method in the realm of programming language models."
        },
        {
            "heading": "4.4 Analysis",
            "text": "Transferability Adversarial examples constructed through exploiting code structures can effectively enhance the robustness of the model. However, considering that the model may face a variety of adversarial examples concurrently, here we examine its generalization capability. As is demonstrated in Table 5, we evaluate the model trained on adversarial examples constructed via BFS reconstruction on samples reconstructed by the DFS, and vice versa.\nLearning Curve To validate the necessity of using adversarial examples during the training process, in this part, we set different proportions of adversarial training data to observe how the model\nlearns from the adversarial examples.\nWe employ code summarization (Ruby) and defect detection tasks. As clearly shown in Figure 4,\nwith the increase in the number of adversarial examples, the robustness can be significantly improved for both generative and understanding tasks. This further validates the rationality of our approach to constructing samples based on code structure."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose multiple novel attack methods targeting source code from the perspective of code structure. By leveraging the AST of the code, we not only consider constructing adversarial samples that are imperceptible to humans but also create perturbed sequences that preserve the same information as the original samples through their traversal. Then we validate its effectiveness on several mainstream CodePTMs, covering both representative code generation and code understanding tasks. Subsequently, we enhance the model\u2019s robustness using adversarial training and investigate the generalizability of performance recovery under different scenarios. Based on our extensive experiments and observations, we provide a comprehensive analysis of the performance of different CodePTMs across various tasks, considering both the vulnerability to attacks, the potential for performance recovery, and the impact on input sensitivity.\nLimitations\n\u2022 Metrics like BLEU (Papineni et al., 2002; Lin and Och, 2004) and CodeBLEU (Ren et al., 2020) predominantly rely on n-gram matching and hence may not adequately consider semantic similarity. Consequently, when evaluating the code sequences generated by models under attack, these metrics could potentially underestimate their semantic correctness.\n\u2022 Due to the constraints of resources, we confine our backbone models to four representative CodePTMs. While other models (Kanade et al., 2020; Ding et al., 2022) might exhibit slight variance, we hold the view that our current experiments sufficiently encapsulate the most representative scenarios.\nEthics Statement\nThe models and data we utilize are all publicly available; our method will not introduce additional model bias and does not involve misuse of code and natural language comments. By designing various attack methods based on code structure, we quantitatively tested the robustness of CodePTMs. Furthermore, we utilize adversarial training to recover the performance of perturbed models and also investigated the correlation between their performance and the number of adversarial samples. We hold the view that these contributions will benefit the NLP research community."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work has been supported by the National Natural Science Foundation of China under Grant No.U1911203, and the National Natural Science Foundation of China under Grant No.62377012. And the authors would like to thank all the anonymous reviewers for their constructive and insightful comments on this paper."
        },
        {
            "heading": "A Task Overview and Dataset Statistics",
            "text": "A.1 Defect Detection & Clone Detection\nA.2 Code Generation & Translation\nA.3 Code Summarization"
        },
        {
            "heading": "B Hyperparameters for Fine-tuning",
            "text": "The hyperparameters for tuning CodePTMs by both gold data and adversarial examples are listed in Table 9."
        },
        {
            "heading": "C Additional Experimental Results",
            "text": "We include additional experimental results in Table 10 and Table 11."
        },
        {
            "heading": "D Detailed Algorithm for Reconstruction Attack",
            "text": "Due to the space constraints, we put a simplified version of the reconstruction attack in section 3.2. Algorithm 3 is a more detailed version."
        },
        {
            "heading": "E Case Studies",
            "text": "To better understand the effect of adversarial samples generated through exploiting code structure in specific tasks, we present a series of case studies in Table 12, Table 13, and Table 14 for subtree attack, signature attack, and reconstruction attack respectively.\nAlgorithm 3 Traverse Tree Input: Code snippet c, Traversal mode m Output: Modified code snippet c\u2032\n1: procedure TRAVERSAL(c,m) 2: node_type \u2190 list() 3: T \u2190 GetAST(c) 4: queue \u2190 list() 5: queue.append(tree.root) 6: while queue do 7: if m = \"DFS\" then 8: current_node \u2190 queue.pop() 9: else if m = \"BFS\" then\n10: current_node \u2190 queue.pop(0) 11: end if 12: node_type.append(current_node.type) 13: if m = \"DFS\" then 14: for child \u2208 current_node.children[::-1] do 15: queue.append(child) 16: end for 17: else if m = \"BFS\" then 18: for child \u2208 current_node.children\ndo 19: queue.append(child) 20: end for 21: end if 22: end while 23: return queue 24: end procedure\nOriginal After Attack\n1 public NotImplementedFunctionException( String functionName, NotImplementedException cause) { 2 super(functionName, cause); 3 this.functionName = functionName; 4 }\nCode 1: Original input.\n1 NotImplementedFunctionException ( String functionName , NotImplementedException cause ) { 2 super ( functionName , cause ) ; 3 = functionName ; 4 }\nCode 2: Input under subtree attack.\n1 NotImplementedFunctionException ( String functionName , NotImplementedException cause ) { 2 super ( functionName , cause ) ; 3 = functionName ; 4 }\nCode 3: Generated codes based on original input.\n1 FunctionException(string functionName, NotImplementedException cause) 2 : base(functionName, cause) 3 { 4 _functionName = functionName; 5 }\nCode 4: Generated codes based on input under subtree attack.\nCode 7: The summarization for original code.\nCode 8: The summarization for the attacked code.\nTable 13: Case studies of code summarization on Python under signature attack.\nOriginal After Attack\n1 def wix_light_extension(extension) 2 unless extension.is_a?(String) 3 raise InvalidValue.new(:\nwix_light_extension, \"be\u2423an\u2423 String\")\n4 end 5 wix_light_extensions << extension 6 end\nCode 9: Original input.\n1 end 2 wix_light_extension def ) extension ( 3 extension << wix_light_extensions 4 end 5 unless is_a? . extension raise) String ( 6 new . InvalidValue ) , :\nwix_light_extension ( \"\u2423be\u2423an\u2423 String\u2423\"\nCode 10: Input under reconstruction attack.\n1 # Adds a Wix Light Extension .\nCode 11: The summarization for original code.\n1 # wix_light_extension\nCode 12: The summarization for the attacked code.\nTable 14: Case studies of code summarization on Ruby under reconstruction attack (BFS)."
        }
    ],
    "title": "Evaluating and Enhancing the Robustness of Code Pre-trained Models through Structure-Aware Adversarial Samples Generation",
    "year": 2023
}