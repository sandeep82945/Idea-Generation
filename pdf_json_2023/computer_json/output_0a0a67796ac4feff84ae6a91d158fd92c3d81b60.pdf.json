{
    "abstractText": "Most of existing video-language pre-training methods focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information, which is of importance to downstream tasks requiring temporal localization and semantic reasoning. In this work, we propose a simple yet effective video-language pre-training framework, namely GViLM, to learn discriminative spatiotemporal features. Two novel designs involving spatiotemporal grounding and temporal grouping promote learning local region-noun alignment and temporal-aware features simultaneously. Specifically, spatiotemporal grounding aggregates semantically similar video tokens and aligns them with noun phrases extracted from the caption to promote local region-noun correspondences. Moreover, temporal grouping leverages cutand-paste to manually create temporal scene changes and then learns distinguishable features from different scenes. Comprehensive evaluations demonstrate that G-ViLM performs favorably against existing approaches on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition and temporal action localization. G-ViLM performs competitively on all evaluated tasks and in particular achieves R@10 of 65.1 on zero-shot MSR-VTT retrieval, over 9% higher than the state-of-the-art method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuanhao Xiong"
        },
        {
            "affiliations": [],
            "name": "Long Zhao"
        },
        {
            "affiliations": [],
            "name": "Boqing Gong"
        },
        {
            "affiliations": [],
            "name": "Ming-Hsuan Yang"
        },
        {
            "affiliations": [],
            "name": "Florian Schroff"
        },
        {
            "affiliations": [],
            "name": "Ting Liu"
        },
        {
            "affiliations": [],
            "name": "Cho-Jui Hsieh"
        },
        {
            "affiliations": [],
            "name": "Liangzhe Yuan"
        }
    ],
    "id": "SP:256b0334fb86c764b7b09367eb776ada30deaf7b",
    "references": [
        {
            "authors": [
                "Hassan Akbari",
                "Liangzhe Yuan",
                "Rui Qian",
                "Wei-Hong Chuang",
                "Shih-Fu Chang",
                "Yin Cui",
                "Boqing Gong"
            ],
            "title": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Adria Recasens",
                "Rosalia Schneider",
                "Relja Arandjelovi\u0107",
                "Jason Ramapuram",
                "Jeffrey De Fauw",
                "Lucas Smaira",
                "Sander Dieleman",
                "Andrew Zisserman"
            ],
            "title": "Selfsupervised multimodal versatile networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Humam Alwassel",
                "Silvio Giancola",
                "Bernard Ghanem"
            ],
            "title": "Tsp: Temporally-sensitive pretraining of video encoders for localization tasks",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Humam Alwassel",
                "Dhruv Mahajan",
                "Bruno Korbar",
                "Lorenzo Torresani",
                "Bernard Ghanem",
                "Du Tran"
            ],
            "title": "Self-supervised learning by cross-modal audio-video clustering",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Elad Amrani",
                "Rami Ben-Ari",
                "Daniel Rotman",
                "Alex Bronstein"
            ],
            "title": "Noise estimation using density estimation for self-supervised multimodal learning",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang"
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman"
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Sagie Benaim",
                "Ariel Ephrat",
                "Oran Lang",
                "Inbar Mosseri",
                "William T Freeman",
                "Michael Rubinstein",
                "Michal Irani",
                "Tali Dekel"
            ],
            "title": "Speednet: Learning the speediness in videos",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Guanyu Cai",
                "Yixiao Ge",
                "Alex Jinpeng Wang",
                "Rui Yan",
                "Xudong Lin",
                "Ying Shan",
                "Lianghua He",
                "Xiaohu Qie",
                "Jianping Wu",
                "Mike Zheng Shou"
            ],
            "title": "Revitalize region feature for democratizing video-language pre-training",
            "venue": "arXiv preprint arXiv:2203.07720,",
            "year": 2022
        },
        {
            "authors": [
                "Meng Cao",
                "Tianyu Yang",
                "Junwu Weng",
                "Can Zhang",
                "Jue Wang",
                "Yuexian Zou"
            ],
            "title": "Locvtp: Video-text pre-training for temporal localization",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Brian Chen",
                "Andrew Rouditchenko",
                "Kevin Duarte",
                "Hilde Kuehne",
                "Samuel Thomas",
                "Angie Boggust",
                "Rameswar Panda",
                "Brian Kingsbury",
                "Rogerio Feris",
                "David Harwath"
            ],
            "title": "Multimodal clustering networks for self-supervised learning from unlabeled videos",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Peihao Chen",
                "Deng Huang",
                "Dongliang He",
                "Xiang Long",
                "Runhao Zeng",
                "Shilei Wen",
                "Mingkui Tan",
                "Chuang Gan"
            ],
            "title": "Rspnet: Relative speed perception for unsupervised video representation learning",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Chenyou Fan",
                "Xiaofan Zhang",
                "Shu Zhang",
                "Wensheng Wang",
                "Chi Zhang",
                "Heng Huang"
            ],
            "title": "Heterogeneous memory enhanced multimodal attention model for video question answering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Fang",
                "Saurabh Gupta",
                "Forrest Iandola",
                "Rupesh K Srivastava",
                "Li Deng",
                "Piotr Doll\u00e1r",
                "Jianfeng Gao",
                "Xiaodong He",
                "Margaret Mitchell",
                "John C Platt"
            ],
            "title": "From captions to visual concepts and back",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Haoqi Fan",
                "Jitendra Malik",
                "Kaiming He"
            ],
            "title": "Slowfast networks for video recognition",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Linjie Li",
                "Zhe Gan",
                "Kevin Lin",
                "William Yang Wang",
                "Lijuan Wang",
                "Zicheng Liu"
            ],
            "title": "Violet: End-to-end video-language transformers with masked visual-token modeling",
            "venue": "arXiv preprint arXiv:2111.12681,",
            "year": 2021
        },
        {
            "authors": [
                "Akira Fukui",
                "Dong Huk Park",
                "Daylen Yang",
                "Anna Rohrbach",
                "Trevor Darrell",
                "Marcus Rohrbach"
            ],
            "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
            "venue": "arXiv preprint arXiv:1606.01847,",
            "year": 2016
        },
        {
            "authors": [
                "Valentin Gabeur",
                "Chen Sun",
                "Karteek Alahari",
                "Cordelia Schmid"
            ],
            "title": "Multi-modal transformer for video retrieval",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Jiyang Gao",
                "Runzhou Ge",
                "Kan Chen",
                "Ram Nevatia"
            ],
            "title": "Motion-appearance co-memory networks for video question answering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yuying Ge",
                "Yixiao Ge",
                "Xihui Liu",
                "Dian Li",
                "Ying Shan",
                "Xiaohu Qie",
                "Ping Luo"
            ],
            "title": "Bridging video-text retrieval with multiple choice questions",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuying Ge",
                "Yixiao Ge",
                "Xihui Liu",
                "Jinpeng Wang",
                "Jianping Wu",
                "Ying Shan",
                "Xiaohu Qie",
                "Ping Luo"
            ],
            "title": "Miles: visual bert pre-training with injected language semantics for videotext retrieval",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Golnaz Ghiasi",
                "Xiuye Gu",
                "Yin Cui",
                "Tsung-Yi Lin"
            ],
            "title": "Scaling open-vocabulary image segmentation with image-level labels",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Tanmay Gupta",
                "Arash Vahdat",
                "Gal Chechik",
                "Xiaodong Yang",
                "Jan Kautz",
                "Derek Hoiem"
            ],
            "title": "Contrastive learning for weakly supervised phrase grounding",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Tengda Han",
                "Weidi Xie",
                "Andrew Zisserman"
            ],
            "title": "Memoryaugmented dense predictive coding for video representation learning",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Tengda Han",
                "Weidi Xie",
                "Andrew Zisserman"
            ],
            "title": "Selfsupervised co-training for video representation learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Fabian Caba Heilbron",
                "Victor Escorcia",
                "Bernard Ghanem",
                "Juan Carlos Niebles"
            ],
            "title": "Activitynet: A large-scale video benchmark for human activity understanding",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Deng Huang",
                "Peihao Chen",
                "Runhao Zeng",
                "Qing Du",
                "Mingkui Tan",
                "Chuang Gan"
            ],
            "title": "Location-aware graph convolutional networks for video question answering",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yuqi Huo",
                "Mingyu Ding",
                "Haoyu Lu",
                "Nanyi Fei",
                "Zhiwu Lu",
                "Ji-Rong Wen",
                "Ping Luo"
            ],
            "title": "Compressed video contrastive learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yunseok Jang",
                "Yale Song",
                "Youngjae Yu",
                "Youngjin Kim",
                "Gunhee Kim"
            ],
            "title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jianwen Jiang",
                "Ziqiang Chen",
                "Haojie Lin",
                "Xibin Zhao",
                "Yue Gao"
            ],
            "title": "Divide and conquer: Question-guided spatiotemporal contextual attention for video question answering",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Pin Jiang",
                "Yahong Han"
            ],
            "title": "Reasoning with heterogeneous graph alignment for video question answering",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Quan Kong",
                "Wenpeng Wei",
                "Ziwei Deng",
                "Tomoaki Yoshinaga",
                "Tomokazu Murakami"
            ],
            "title": "Cycle-contrast for selfsupervised video representation learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Kenji Hata",
                "Frederic Ren",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Dense-captioning events in videos",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Hildegard Kuehne",
                "Hueihan Jhuang",
                "Est\u0131\u0301baliz Garrote",
                "Tomaso Poggio",
                "Thomas Serre"
            ],
            "title": "Hmdb: a large video database for human motion recognition",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2011
        },
        {
            "authors": [
                "Thao Minh Le",
                "Vuong Le",
                "Svetha Venkatesh",
                "Truyen Tran"
            ],
            "title": "Hierarchical conditional relation networks for video question answering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jie Lei",
                "Linjie Li",
                "Luowei Zhou",
                "Zhe Gan",
                "Tamara L Berg",
                "Mohit Bansal",
                "Jingjing Liu"
            ],
            "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Dongxu Li",
                "Junnan Li",
                "Hongdong Li",
                "Juan Carlos Niebles",
                "Steven CH Hoi"
            ],
            "title": "Align and prompt: Video-and-language pre-training with entity prompts",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Linjie Li",
                "Yen-Chun Chen",
                "Yu Cheng",
                "Zhe Gan",
                "Licheng Yu",
                "Jingjing Liu"
            ],
            "title": "Hero: Hierarchical encoder for video+ language omni-representation pre-training",
            "venue": "In Empirical Methods in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang"
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557,",
            "year": 1908
        },
        {
            "authors": [
                "Yuqi Liu",
                "Pengfei Xiong",
                "Luhui Xu",
                "Shengming Cao",
                "Qin Jin"
            ],
            "title": "Ts2-net: Token shift and selection transformer for text-video retrieval",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Botian Shi",
                "Haoyang Huang",
                "Nan Duan",
                "Tianrui Li",
                "Jason Li",
                "Taroon Bharti",
                "Ming Zhou"
            ],
            "title": "Univl: A unified video and language pre-training model for multimodal understanding and generation",
            "venue": "arXiv preprint arXiv:2002.06353,",
            "year": 2020
        },
        {
            "authors": [
                "Fan Ma",
                "Xiaojie Jin",
                "Heng Wang",
                "Jingjia Huang",
                "Linchao Zhu",
                "Jiashi Feng",
                "Yi Yang"
            ],
            "title": "Temporal perceiving videolanguage pre-training",
            "venue": "arXiv preprint arXiv:2301.07463,",
            "year": 2023
        },
        {
            "authors": [
                "Antoine Miech",
                "Jean-Baptiste Alayrac",
                "Lucas Smaira",
                "Ivan Laptev",
                "Josef Sivic",
                "Andrew Zisserman"
            ],
            "title": "End-to-end learning of visual representations from uncurated instructional videos",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Arsha Nagrani",
                "Paul Hongsuck Seo",
                "Bryan Seybold",
                "Anja Hauth",
                "Santiago Manen",
                "Chen Sun",
                "Cordelia Schmid"
            ],
            "title": "Learning audio-video modalities from image captions",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Tian Pan",
                "Yibing Song",
                "Tianyu Yang",
                "Wenhao Jiang",
                "Wei Liu"
            ],
            "title": "Videomoco: Contrastive video representation learning with temporally adversarial examples",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Mandela Patrick",
                "Po-Yao Huang",
                "Yuki Asano",
                "Florian Metze",
                "Alexander Hauptmann",
                "Joao Henriques",
                "Andrea Vedaldi"
            ],
            "title": "Support-set bottlenecks for video-text representation learning",
            "venue": "arXiv preprint arXiv:2010.02824,",
            "year": 2020
        },
        {
            "authors": [
                "AJ Piergiovanni",
                "Anelia Angelova",
                "Michael S Ryoo"
            ],
            "title": "Evolving losses for unsupervised video representation learning",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Rui Qian",
                "Yeqing Li",
                "Liangzhe Yuan",
                "Boqing Gong",
                "Ting Liu",
                "Matthew Brown",
                "Serge Belongie",
                "Ming-Hsuan Yang",
                "Hartwig Adam",
                "Yin Cui"
            ],
            "title": "Exploring temporal granularity in self-supervised video representation learning",
            "year": 2022
        },
        {
            "authors": [
                "Anna Rohrbach",
                "Marcus Rohrbach",
                "Ronghang Hu",
                "Trevor Darrell",
                "Bernt Schiele"
            ],
            "title": "Grounding of textual phrases in images by reconstruction",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Andrew Rouditchenko",
                "Angie Boggust",
                "David Harwath",
                "Brian Chen",
                "Dhiraj Joshi",
                "Samuel Thomas",
                "Kartik Audhkhasi",
                "Hilde Kuehne",
                "Rameswar Panda",
                "Rogerio Feris"
            ],
            "title": "Avlnet: Learning audio-visual language representations from instructional videos",
            "venue": "In Interspeech,",
            "year": 2021
        },
        {
            "authors": [
                "Paul Hongsuck Seo",
                "Arsha Nagrani",
                "Cordelia Schmid"
            ],
            "title": "Look before you speak: Visually contextualized utterances",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Annual Meeting of the Association for Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Khurram Soomro",
                "Amir Roshan Zamir",
                "Mubarak Shah"
            ],
            "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
            "venue": "arXiv preprint arXiv:1212.0402,",
            "year": 2012
        },
        {
            "authors": [
                "Chen Sun",
                "Fabien Baradel",
                "Kevin Murphy",
                "Cordelia Schmid"
            ],
            "title": "Learning video representations using contrastive bidirectional transformer",
            "venue": "arXiv preprint arXiv:1906.05743,",
            "year": 1906
        },
        {
            "authors": [
                "Chen Sun",
                "Austin Myers",
                "Carl Vondrick",
                "Kevin Murphy",
                "Cordelia Schmid"
            ],
            "title": "Videobert: A joint model for video and language representation learning",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Du Tran",
                "Lubomir D Bourdev",
                "Rob Fergus",
                "Lorenzo Torresani",
                "Manohar Paluri"
            ],
            "title": "C3d: generic features for video analysis",
            "venue": "CoRR, abs/1412.0767,",
            "year": 2014
        },
        {
            "authors": [
                "Donglai Wei",
                "Joseph J Lim",
                "Andrew Zisserman",
                "William T Freeman"
            ],
            "title": "Learning and using the arrow of time",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Dejing Xu",
                "Zhou Zhao",
                "Jun Xiao",
                "Fei Wu",
                "Hanwang Zhang",
                "Xiangnan He",
                "Yueting Zhuang"
            ],
            "title": "Video question answering via gradually refined attention over appearance and motion",
            "venue": "In ACM International Conference on Multimedia,",
            "year": 2017
        },
        {
            "authors": [
                "Hu Xu",
                "Gargi Ghosh",
                "Po-Yao Huang",
                "Prahal Arora",
                "Masoumeh Aminzadeh",
                "Christoph Feichtenhofer",
                "Florian Metze",
                "Luke Zettlemoyer"
            ],
            "title": "Vlm: Task-agnostic videolanguage model pre-training for video understanding",
            "venue": "In Findings of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Hu Xu",
                "Gargi Ghosh",
                "Po-Yao Huang",
                "Dmytro Okhonko",
                "Armen Aghajanyan",
                "Florian Metze",
                "Luke Zettlemoyer",
                "Christoph Feichtenhofer"
            ],
            "title": "Videoclip: Contrastive pre-training for zero-shot video-text understanding",
            "venue": "In Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Jiarui Xu",
                "Shalini De Mello",
                "Sifei Liu",
                "Wonmin Byeon",
                "Thomas Breuel",
                "Jan Kautz",
                "Xiaolong Wang"
            ],
            "title": "Groupvit: Semantic segmentation emerges from text supervision",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Xu",
                "Tao Mei",
                "Ting Yao",
                "Yong Rui"
            ],
            "title": "Msr-vtt: A large video description dataset for bridging video and language",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Mengmeng Xu",
                "Juan-Manuel P\u00e9rez-R\u00faa",
                "Victor Escorcia",
                "Brais Martinez",
                "Xiatian Zhu",
                "Li Zhang",
                "Bernard Ghanem",
                "Tao Xiang"
            ],
            "title": "Boundary-sensitive pre-training for temporal localization in videos",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Mengmeng Xu",
                "Juan Manuel Perez Rua",
                "Xiatian Zhu",
                "Bernard Ghanem",
                "Brais Martinez"
            ],
            "title": "Low-fidelity video encoder optimization for temporal action localization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mengmeng Xu",
                "Chen Zhao",
                "David S Rojas",
                "Ali Thabet",
                "Bernard Ghanem"
            ],
            "title": "G-tad: Sub-graph localization for temporal action detection",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Rui Yan",
                "Mike Zheng Shou",
                "Yixiao Ge",
                "Alex Jinpeng Wang",
                "Xudong Lin",
                "Guanyu Cai",
                "Jinhui Tang"
            ],
            "title": "Videotext pre-training with learned regions",
            "venue": "arXiv preprint arXiv:2112.01194,",
            "year": 2021
        },
        {
            "authors": [
                "Jianwei Yang",
                "Yonatan Bisk",
                "Jianfeng Gao"
            ],
            "title": "Taco: Token-aware cascade contrastive learning for video-text alignment",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Qihang Yu",
                "Huiyu Wang",
                "Siyuan Qiao",
                "Maxwell Collins",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "title": "k-means mask transformer",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Liangzhe Yuan",
                "Rui Qian",
                "Yin Cui",
                "Boqing Gong",
                "Florian Schroff",
                "Ming-Hsuan Yang",
                "Hartwig Adam",
                "Ting Liu"
            ],
            "title": "Contextualized spatio-temporal contrastive learning with self-supervision",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Jiasen Lu",
                "Ximing Lu",
                "Youngjae Yu",
                "Yanpeng Zhao",
                "Mohammadreza Salehi",
                "Aditya Kusupati",
                "Jack Hessel",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Merlot reserve: Neural script knowledge through vision and language and sound",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Can Zhang",
                "Tianyu Yang",
                "Junwu Weng",
                "Meng Cao",
                "Jue Wang",
                "Yuexian Zou"
            ],
            "title": "Unsupervised pre-training for temporal action localization tasks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Video-language pre-training with the goal of learning transferable multi-modal representations has recently attracted much attention that finds numerous applications [1, 7, 23, 39, 40, 41, 44, 47, 64]. Such a model trained on those web-crawled unlabeled noisy data achieves promising performance on various downstream tasks, ranging from single-modal video action recognition to multi-modal text-\n*Work done as a student researcher at Google\nvideo retrieval. Video-language pre-training typically follows the pipeline: a) encoding video and text pairs into latent representations; b) modality fusion and interaction; c) pre-training on specific objectives.\nExisting methods typically optimize these three components in the pre-training pipeline by designing expressive encoders [7,23,40,45,48], fusing two modalities via a crossencoder [23, 39, 40, 41, 44, 63, 77], or adopting a combination of various pre-training tasks such as contrastive learning and masked modeling [11,19,23,24,40,75]. While these modifications benefit the pre-trained model, they lack local discriminative capabilities. Instead, we approach the videolanguage pre-training task from a different perspective with a focus on local fine-grained information.\nIt has been shown that most video-language pre-training methods merely perform well on learning holistic representations to match a \u3008video, caption\u3009 pair and neglect fine-\nar X\niv :2\n30 3.\n16 34\n1v 1\n[ cs\n.C V\n] 2\n8 M\nar 2\ngrained information (e.g., region-noun correspondences, or scene/action changes along the time in a video) [1, 7, 39, 41, 44, 47, 48, 64]. However, such regional or temporal fine-grained information has been demonstrated to play an important role in localization and reasoning tasks [23, 40, 45, 73, 76]. For example, in Figure 1, given a video of a woman applying facial cream, per-frame features from the model pre-trained with only global contrastive loss are relatively hard to distinguish, which makes it challenging to find boundaries in tasks like temporal action localization. Furthermore, region-noun alignment frequently appears in a video and caption pair as shown in Figure 1, where we highlight objects and their associated noun phrases with the same color. Few methods take such alignment into consideration.\nIn this work, we incorporate fine-grained video-caption interactions into the pre-training stage and propose a novel framework, Text-Grounded Video-Language Modeling, named G-ViLM. G-ViLM encourages instance-level videocaption alignment, fine-grained region-noun alignment, and temporal-aware video representations simultaneously. As Figure 2 shows, G-ViLM consists of three primary training objectives: spatiotemporal grounding, temporal grouping, and global contrastive learning. With the video-caption pair as the input, a classical dual-encoder is leveraged to extract the representation for each modality respectively. Note that videos are pre-processed with the cut-and-paste operation [74, 76], i.e., randomly selecting a clip in one video as the foreground and pasting it onto the other background video, to explicitly introduce temporal scene changes, as shown in Figure 2. We first adopt the structure of grouping blocks [65, 72] to aggregate semantically similar video patches to represent objects without off-the-shelf detectors. In spatiotemporal grounding, we align grouped video tokens with noun concepts extracted from the caption via our designed grounding loss. Furthermore, we design a temporal grouping objective to capture temporal information by distinguishing foreground and background representations. Finally, the model is trained by a global video-caption contrastive loss to match instance-level video-caption pairs.\nOur key contributions are summarized as follows: \u2022 We design a spatiotemporal grounding module to cap-\nture fine-grained correspondences by aligning nouns from the caption and regions from the video in a selfsupervised manner. \u2022 We leverage a cut-and-paste operation to introduce temporal scene changes into videos during pretraining, and propose the temporal grouping module to learn more temporal-aware features. \u2022 G-ViLM is evaluated comprehensively on four representative downstream tasks, including text-video retrieval, video question answering, video action recognition and temporal action localization.\n\u2022 Experimental results have shown the effectiveness of G-ViLM and in particular, it outperforms SOTA by 9% in R@10 in zero-shot text-video retrieval and performs competitive on other downstream tasks."
        },
        {
            "heading": "2. Related Work",
            "text": "Video-language pre-training. Video-language pretraining is an emerging research area that aims to develop machine learning models capable of jointly understanding visual and textual content. Representations learned from large scale noisy datasets such as HowTo100M [47], WebVid [7] and VideoCC [48] have demonstrated great potentials in adapting to downstream tasks, including but not limited to text-video retrieval, video question answering and video captioning. Elaborately designed pre-training objectives ranging from generative [15, 19, 42, 43] to discriminative [1,7,23,39,40] have been proposed, among which contrastive learning is prevalent and widely adopted to attract paired video-caption instances and repelling unpaired ones. Early approaches [41, 44, 46, 47, 59, 64, 77] merely leverage offline video features extracted from frozen backbone models, and are less effective in adaptation to various domains. Recently, end-to-end training [1,7,10,23,24,39,40,70] enables video and language features to be learned from raw pixels and captions, respectively. For instance, Frozen [7] adopts a vision transformer as the visual encoder taking both raw images and videos as input and updates the visual and text encoder via contrastive learning. In addition, some methods [10, 19, 70] attempt to make encoders more expressive by introducing richer information from raw data like region features, adding a multi-modal fusion encoder to facilitate modality interaction, or adopting a combination of contrastive learning and masked modeling. However, their primary focus is still on learning holistic global representations to align instance-level \u3008video, caption\u3009 pairs.\nRecently, some approaches have been proposed to leverage finer-grained information such as nouns/verbs phrases from a caption. ALPRO [40] extracts pseudo entity labels by feeding noun prompts into a frozen model and use contrastive objective to align cropped visual regions and the corresponding textual labels. In [23], MCQ recovers randomly masked noun/verbs tokens via resorting to global video features, which implicitly improves text entity association in visual encoding. Despite these efforts, correspondences between visual regions and noun concepts in captions and temporal scene shifts in a video, is still neglected and not modeled explicitly in existing video-language pretraining methods. In this work, we propose two novel designs, spatiotemporal grounding and temporal grouping, to leverage fine-grained information in pre-training stage. Vision language grounding. The goal of Visual Grounding (VG) is to locate the most relevant object or region in a visual input based on a natural language query [17, 20,\n25, 26, 53]. Recently, visual grounding has been adapted to a pre-training task in a self-supervised manner for openvocabulary image segmentation [25, 65]. For example, OpenSeg [25] semantically aligns a caption with extracted image regions via a grounding loss. Moreover, without the off-the-shelf object detectors, GroupViT [65] learns to group together semantic regions from text supervision by contrastive learning. Note that visual grounding is mostly discussed in the image domain and its success motivates us to extend visual-semantic alignment to video-language pre-training. We integrate a novel spatiotemporal grounding module in our framework to promote visual and textual entity correspondences in a self-supervised manner. Video temporal modeling. In contrast to images, videos contain a sequence of dynamic frames and how to model temporal information is critical in video understanding [3, 9, 18, 52, 60, 76]. Specifically, TSP [3] learns temporal information via predicting clips inside or outside the action with substantial annotations while PAL [76] aligns features of pasted pseudo action regions from two synthetic videos. These techniques are elaborately designed for training models on long videos such as movies or TV dramas, which contains natural scene changes. However, few of them have been considered in video-language pre-training since the majority of the dataset contains short videos which feature repeated frames and are lacking in temporal differences. In this work, we develop a temporal grouping method to learn temporal-aware clip features in video-language self-\nsupervised learning. We show that features extracted from explicitly temporal modeling achieve significant improvements in not only temporal action localization tasks, but also coarse-grained reasoning and understanding tasks such as video question answering and video action recognition."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Overview",
            "text": "The framework of G-ViLM is presented in Figure 2. We adopt the dual encoder architecture for video-language pretraining, and there are three primary objectives used in the pre-training stage: 1) spatiotemporal grounding, 2) temporal grouping, and 3) global contrastive learning.\nAs shown in Figure 2, temporal changes are first artificially introduced into training examples through cut-andpaste, and then a set of spatiotemporal video tokens are obtained given the patch size and a linear projection layer. Video tokens are processed in two branches: (1) group tokens aggregate semantically similar video tokens via grouping blocks to promote region-noun groundingness; (2) the output tokens from the last layer of the video encoder are utilized in temporal grouping to improve temporal discriminativeness. In contrast to previous methods where regions are extracted with pre-trained object detectors [10, 40, 70], we leverage the learned group tokens to cluster and organize semantically similar regions in a self-supervised manner, which is more effective and reduces the artifacts of any\ndetectors. For the language branch, the original captions are tokenized into a sequence of text tokens, which are then fed into a text encoder to extract the corresponding representation from the preceding [CLS] token. Noun tokens are extracted in the same way given a set of noun prompts.\nWe model the interaction between region features and noun tokens using spatiotemporal grounding loss. To further promote temporal awareness, we use masks derived from the cut-and-paste operations as the ground-truth for temporal grouping. Finally, a global contrastive loss is computed between the video and the caption representations to match the instance-level \u3008video, caption\u3009 pair."
        },
        {
            "heading": "3.2. Grounding with Group Tokens",
            "text": "Observing the correspondences between visual regions in a video and noun phrases in a caption, as demonstrated in Figure 1, we model such fine-grained alignment for more expressive encoders. In practice, it is infeasible to pool tokens of interest as cluster centers since we do not have ground-truth spatiotemporal segmentation. Thus, we adopt M learnable group tokens to cluster semantic similar regions in a self-supervised manner. Note that group tokens are randomly initialized and shared among different videos. The detailed structure of a grouping block is presented in Appendix A and multiple grouping blocks are placed at different layers of the video encoder to update group tokens progressively. Final group tokens denoted as G = {gmi }Mm=1 aggregate semantically similar voxels and represent different regions in the video vi.\nWe obtain noun tokens as follows: for each caption ci of a video, we extract K noun phrases using noun chunking in spaCy1 and prompt each of them with a set of handcrafted sentence templates, e.g., \u201cA photo of a {noun}\u201d. Such prompted noun phrases are fed into the text encoder to extract noun tokens {nki }Kk=1.\nWe define the notation for softmax on a vector x at the ith element as: \u03c3(x)i = exp(xi)/\u03c4\u2211 j exp(xj)/\u03c4\n, where \u03c4 is the temperature to scale logits. The similarity of all group tokens G to a noun token nk is s(G, nk) = [\u3008g1, nk\u3009, . . . , \u3008gM , nk\u3009] \u2208 RM , where \u3008\u00b7, \u00b7\u3009 is the cosine similarity. Since ground-truth correspondences between regions and nouns are inaccessible, we compute the grounding similarity between all group and noun tokens by:\nG(v, c) = 1\nK K\u2211 k=1\n\u2329 nk, M\u2211 m=1 \u03c3 ( s(G, nk) ) m \u00b7 gm \u232a . (1)\nG(v, c) encourages each noun to be grounded to one or a few regions and avoids penalizing regions that cannot find any relevant nouns.\n1https://spacy.io/\nSimilarity scores over a batch of size B are computed as: G(V, ci) = [G(v1, ci), . . . , G(vB , ci)] \u2208 RB and G(vi, C) = [G(vi, c1), . . . , G(vi, cB ] \u2208 RB , where V = {vi}Bi=1 and C = {ci}Bi=1 denote the set of videos and captions in a batch respectively. Spatiotemporal grounding loss Lg is then defined to enable nouns to be matched with regions for each positive \u3008video, caption\u3009 pair. Lg = Lv\u2192cg +Lc\u2192vg , consists of a video-to-caption grounding loss\nLv\u2192cg = \u2212 1\nB B\u2211 i=1 log \u03c3 (G (vi, C))i , (2)\nand a caption-to-video grounding loss\nLc\u2192vg = \u2212 1\nB B\u2211 i=1 log \u03c3 (G (V, ci))i . (3)"
        },
        {
            "heading": "3.3. Temporal Grouping with Cut-and-Paste",
            "text": "Training data in pre-training stage are usually short video clips with repetitive scenes. To simulate scene shifts, we design a cut-and-paste operation inspired from image augmentations [74,76] to introduce temporal changes manually as augmentation to further improve video representations.\nGiven a target video vi with T frames as the foreground and a randomly sampled video vpi with the index pi as the background from the same batch of size B, we divide each video into Nt = T/t clips with the temporal window size t. We then sample the start and end clip indices s and e from (0, Nt), and paste the corresponding region from vi into the background video vpi to form a blended video v\u0302i. We define the foreground-background mask as mi \u2208 RNt = {1(j \u2208 [s, e])|j \u2208 [0, Nt)}, where 1(\u00b7) is the indicator function. This operation is illustrated in Figure 2.\nA video is first flattened into N non-overlapping voxels. After projected by a linear layer, these voxel tokens are fed into the transformer encoder to obtain transformed tokens zvi \u2208 RN\u00d7d, where d is the feature dimension. To obtain clip-level representations zclipi \u2208 RNt\u00d7d, we average-pool over zvi along spatial dimension after recovering the feature map\u2019s 3D shape. Two cluster centers, zbi for the background and zfi for the foreground, are further computed by averaging features from zvi on the corresponding position based on the mask mi. To assign each clip to background or foreground, we compute ai via dot product with an elementwise softmax function applying on the last dimension:\nai = Softmax(z clip i \u00b7 [z b i ; z f i ] T ) \u2208 RNt\u00d72. (4)\nFinally, the temporal grouping loss can be computed within a batch between ai and the ground-truth maskingmi of onehot version using mean squared error as\nLt = 1\nB B\u2211 i `MSE(ai,One-hot(mi)). (5)\nIn addition, the cut-and-paste operation indicates that v\u0302i has another positive caption cpi apart from its original ci, and the positive indices of captions with weights become W v \u2208 RB\u00d7B = {wvi,j} satisfying\nwvi,j =  \u03b2i, j = i\n1\u2212 \u03b2i, j = pi 0, otherwise , (6)\nwhere \u03b2i = (e\u2212 s)/Nt is the ratio of the foreground in the cut-and-paste video v\u0302i. From the perspective of captions, we can obtainW c = (W v)>. We can derive the augmented grounding loss Lg with the video-to-caption loss as\nLv\u2192cg = \u2212 1\nB B\u2211 i=1 B\u2211 j=1 wvi,j log \u03c3 (G (v\u0302i, C))j , (7)\nand the caption-to-video loss with V\u0302 = {v\u0302i}Bi=1 as\nLc\u2192vg = \u2212 1\nB B\u2211 i=1 B\u2211 j=1 wci,j log \u03c3 ( G ( V\u0302, ci )) j . (8)"
        },
        {
            "heading": "3.4. Overall Pre-training Objective",
            "text": "We also include a global contrastive learning objective for instance-level video-caption alignment. fvi , the video representation of v\u0302i, is extracted from average-pooled group tokens and f ci , the caption representation ci, is selected as [CLS] token from the original caption. Instance similarity scores are defined as: s(V, ci) = [\u3008fv1 , f ci \u3009, . . . , \u3008fvB , fci \u3009] \u2208 RB and s(v\u0302i, C) = [\u3008fvi , f c1\u3009, . . . , \u3008fvi , f cB\u3009] \u2208 RB . The global contrastive loss is defined as Lcontrast = Lv\u2192ccontrast + Lc\u2192vcontrast, with the video-to-caption view of\nLv\u2192ccontrast = \u2212 1\nB B\u2211 i=1 B\u2211 j=1 wvi,j log \u03c3(s(v\u0302i, C))j , (9)\nand the caption-to-video view of\nLc\u2192vcontrast = \u2212 1\nB B\u2211 i=1 B\u2211 j=1 wci,j log \u03c3(s(V, ci))j . (10)\nThe overall pre-training objective is a combination of weighted sum of grouping loss, grounding loss, and global contrastive loss: L = \u03c91Lt + \u03c92Lg + \u03c93Lcontrast. We set three weights equal to one in our experiments for brevity."
        },
        {
            "heading": "4. Experiments",
            "text": "We conduct comprehensive evaluations of G-ViLM against the state-of-the-art methods. First, we introduce the pre-training datasets used in our method and four selected downstream tasks. Next, the implementation details of both\npre-training and fine-tuning procedures are presented. We compare the performance of G-ViLM with existing methods and demonstrate the effectiveness of incorporating finegrained information. In addition, we present ablation results on choices of pre-training datasets and training objectives."
        },
        {
            "heading": "4.1. Pre-training Datasets",
            "text": "We pre-train G-ViLM with VideoCC [48] dataset, which contains about 3.3M video-caption pairs. Specifically, VideoCC is mined online using the Conceptual Captions [56] as a seed dataset, and has shown to be more effective in retrieval and captioning tasks than commonlyadopted datasets such as HowTo100M [47] and WebVid2M [7]. In addition, we include ActivityNet-Caption [36] with 20K well-aligned pairs into the pre-training corpus. Note that for temporal action localization, the model is pretrained on HowTo100M only, which is observed to benefit to TAL compared with VideoCC + ActivityNet."
        },
        {
            "heading": "4.2. Downstream Tasks",
            "text": "Text-Video Retrieval. We adopt the widely used text-video retrieval benchmark MSR-VTT [66] for evaluation. It consists of 10K YouTube video clips with 200K captions. Similar to existing methods [7, 23], we train and test the model on the split of 9K and 1K videos. Video Question Answering (VQA). We consider openended VQA settings with two representative datasets: 1) MSRVTT-QA [62] with 1500 answer candidates; 2) MSVD-QA [62] with 2423 answer candidates. To comply with the data policy, the size of these two datasets that we are using are smaller than the original ones and detailed statistics could be found in Table 4. Video Action Recognition. We select HMDB51 [37] with 6,766 videos from 51 categories and UCF101 [57] with 13,320 videos from 101 categories. Both linear probing and fine-tuning the whole model are explored. Temporal Action Localization (TAL). TAL aims for predicting the temporal extent and the class labels of action instances. We evaluate the performance on ActivityNet [29], an action understanding dataset of 19,994 temporally annotated untrimmed videos with 200 action categories."
        },
        {
            "heading": "4.3. Implementation Details",
            "text": "Input. We sample 32 frames for each video and resize them into 224\u00d7224 as input with the same augmentations in [48]. Each caption is tokenized into 32 tokens including [CLS] during training. K = 2 noun phrases are extracted for each caption and then prompted with a set of manually designed templates such as \u201cIt is a video of {noun}\u201d. Model architecture. We use a 12-layer ViT-base model with the patch size of 2 \u00d7 16 \u00d7 16 as the video encoder and initialize it with weights pre-trained on Kinetics-400. We adopt 32 learnable group tokens and 3 grouping blocks\nfeaturing K-means attention [65, 72]. Grouping blocks are inserted at the 6th, 9th and last layers of the video encoder, following GroupViT [65] and kMaX-DeepLab [72]. The text encoder is initialized from the pre-trained BERT-base model. All representations are projected into the common space with the dimension of 256.\nPre-training and fine-tuning setups. We implement GViLM in JAX and train all models on TPU accelerators. During pre-training, a synchronous SGD with momentum 0.9 and initial learning rate 0.1 is used for optimization. We train G-ViLM for 10 epochs with a batch size 1024 and adopt a cosine learning rate decay schedule with a warmup ratio 0.05. It takes about one day for the whole pre-training stage. We use the same text templates as in [40] to generate text prompts. In terms of fine-tuning, different tasks are trained independently with their own set of hyperparameters\non the target dataset and more details can be found in Appendix B. For temporal action localization, we fix weights of the pre-trained video encoder and its grouping blocks to extract video features, which are then evaluated by GTAD [69], a commonly used method for TAL."
        },
        {
            "heading": "4.4. Evaluation Results",
            "text": ""
        },
        {
            "heading": "4.4.1 Text-Video Retrieval",
            "text": "We evaluate G-ViLM for the task of text-video retrieval on MSR-VTT under both zero-shot and fine-tuning settings, and present detailed results in Table 1 and 2. G-ViLM outperform other methods significantly for zero-shot evaluation with R@10 of 65.1, yielding approximately 9% improvement over the best-performing baseline MCQ. The superior results demonstrate that our pre-trained model builds up a good alignment between video and language and\nhas great generalization to unseen datasets. G-ViLM also achieves performance gain when the model is fine-tuned on the target MSR-VTT dataset, which further validates advantages of the pre-trained model. Note that G-ViLM performs favorably against existing methods despite the much smaller size of the pre-training data used in G-ViLM than those in baselines, such as HowTo100M and WebVid-2M. These results are consistent with the findings in [48] and demonstrate the importance of high-quality video-caption pairs in retrieval tasks. G-ViLM leverages a dual-encoder design and does not include a fusion encoder during the pretraining stage which saves much computation cost. On the other hand, retrieval can be achieved efficiently by computing dot-product between video and caption features without feeding every combination into the fusion model, compared with models such as ClipBERT [39]."
        },
        {
            "heading": "4.4.2 Video Question Answering",
            "text": "VQA results on two open-ended datasets MSRVTT-QA and MSVD-QA are shown in Table 3. To enable G-ViLM to deal with the VQA task, we add a fusion head adapted from BUTD [6] by integrating video and text features with simple linear layers. Then a classifier is inserted after the fusion module to perform question answering as a classification problem. Compared with previous methods which leverage particular architectures for VQA or include a complicated fusion encoder, G-ViLM is the most efficient and flexible for various vision-language tasks. Meanwhile, GViLM achieves on-par or even better performance with selected baselines, with the accuracy of 43.5% (1.4% lift) and 45.2% (0.7% drop) on MSRVTT-QA and MSVD-QA respectively. Note that due to data restrictions shown in Table 4, the training sets are not complete as original ones and we believe the performance of our method can be further improved if trained on more VQA pairs."
        },
        {
            "heading": "4.4.3 Video Action Recognition",
            "text": "For video action recognition, we only keep the video encoder together with its grouping blocks to extract singlemodality video representations for evaluation. Two evaluation settings are considered: (1) linear probing where the backbone encoder is frozen and only the last linear classifier is trained and (2) end-to-end fine-tuning where both the backbone and the classifier are trained. Top-1 accuracy on UCF101 and HMDB51 is reported in Table 5. We can observe that in linear probing, G-ViLM performs well against the other methods, with 3.0% and 2.9% higher than current SOTA, MMV with audio and text on UCF101 and HMDB51. G-ViLM also achieves consistently superior performance under fine-tuning evaluation. Outstanding performance of G-ViLM demonstrates that learning the alignment between videos and captions with fine-grained information\ncontributes to meaningful video representations for tasks involved in a single modality."
        },
        {
            "heading": "4.4.4 Temporal Action Localization",
            "text": "We report mean Average Precision (mAP) values under different temporal Intersection over Union (tIoU) thresholds on ActivityNet in Table 7. As mentioned in Section 4.3, we directly use pre-trained models to extract the video features as the input to G-TAD and do not further train the encoder. G-ViLM consistently exceeds other self-supervised competitors and even fully supervised approaches such as LoFi [68] and BSP [67] in three tIoU thresholds. This observation again consolidates the conclusion that visionlanguage pre-training can not only be applied to specific VL problems like text-video retrieval and VQA, but also benefit single-modal downstream tasks."
        },
        {
            "heading": "4.5. Ablation Study",
            "text": "We analyze the effects of the choice of pre-training datasets and different combinations of three pre-training objectives of G-ViLM in this section. Pre-training datasets. To analyze of effect of the pre-training datasets, we evaluate the performance on selected downstream tasks and present detailed results in Table 8. Four choices of PT datasets are considered, including HowTo100M, ActivityNet, VideoCC, and VideoCC + ActivityNet. G-ViLM performs performs poorly in the zeroshot text-video settings on MSR-VTT when pre-trained on\nHowto100M. On the other hand, G-ViLM performs best on the same task when pre-trained with the combination of VideoCC and ActivityNet. These empirical results coincide with the findings in [48], in which HowTo100M has been pointed out not appropriate for such vision-language tasks requiring strong alignment. It is also worth noting that pre-training on ActivityNet and VideoCC performs consistently better than using only one dataset, which validates our choice for PT dataset. On the other hand, HowTo100M contains a large number of action-related videos, which contributes to learning temporal-aware features and demonstrates better capacities in temporal action localization and thus is used for all TAL experiments. Training objective. We analyze the role of our proposed training objectives and present results of this ablation study in Table 6. We evaluate four scenarios and find that in video-language tasks like zero-shot text-video retrieval and VQA, both temporal grouping loss Lt and spatiotemporal grounding loss Lg contribute to performance gain. And the combination of Lt and Lg can further improve the performance to 28.6 for R@1 on MSRVTT-ZS and 45.2 accuracy on MSVD-QA. For the temporal action localization task, Lg would not significant achieve performance gain in mAP compared to scenario 2 and 4. We hypothesize that HowTo100M is a noisy dataset with weakly-aligned videocaption pairs and grounding in a self-supervised manner might not benefit representation learning on such data."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we present a novel video-language pretraining framework, named G-ViLM, that aims to utilize fine-grained local information to capture region-noun correspondences and temporal-aware features at the same time.\nSpatiotemporal grounding and temporal grouping are introduced to achieve the goal of local region-noun alignment and temporal distinguishment in a self-supervised manner. The proposed framework outperforms existing methods significantly on several downstream tasks, such as text-video retrieval, video question answering, video action recognition, and temporal action localization."
        }
    ],
    "title": "Spatiotemporally Discriminative Video-Language Pre-Training with Text Grounding",
    "year": 2023
}