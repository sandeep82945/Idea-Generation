{
    "abstractText": "In the process of intelligent system operation fault diagnosis and decision making, the multisource, heterogeneous, complex, and fuzzy characteristics of information make the conflict, uncertainty, and validity problems appear in the process of information fusion, which has not been solved. In this study, we analyze the credibility and variation of conflict among evidence from the perspective of conflict credibility weight and propose an improved model of multi-source information fusion based on Dempster-Shafer theory (DST). From the perspectives of the weighting strategy and Euclidean distance strategy, we process the basic probability assignment (BPA) of evidence and assign the credible weight of conflict between evidence to achieve the extraction of credible conflicts and the adoption of credible conflicts in the process of evidence fusion. The improved algorithm weakens the problem of uncertainty and ambiguity caused by conflicts in the information fusion process, and reduces the impact of information complexity on analysis results. And it carries a practical application out with the fault diagnosis of wind turbine system to analyze the operation status of wind turbines in a wind farm to verify the effectiveness of the proposed algorithm. The result shows that under the conditions of improved distance metric evidence discrepancy and credible conflict quantification, the algorithm better shows the conflict and correlation among the evidence. It improves the accuracy of system operation reliability analysis, improves the utilization rate of wind energy resources, and has practical implication value.",
    "authors": [
        {
            "affiliations": [],
            "name": "Liming GouID"
        },
        {
            "affiliations": [],
            "name": "Jian ZhangID"
        },
        {
            "affiliations": [],
            "name": "Naiwen Li"
        },
        {
            "affiliations": [],
            "name": "Zongshui Wang"
        },
        {
            "affiliations": [],
            "name": "Jindong Chen"
        },
        {
            "affiliations": [],
            "name": "Lin Qi"
        }
    ],
    "id": "SP:cf53fd52b1e196a3e139882a72cc3ff241823ca5",
    "references": [
        {
            "authors": [
                "H Paggi",
                "JA Lara",
                "J. Soriano"
            ],
            "title": "Structures generated in a multiagent system performing information fusion in peer-to-peer resource-constrained networks",
            "venue": "Neural Comput & Applic",
            "year": 2020
        },
        {
            "authors": [
                "H Shi",
                "H Zhao",
                "Y Liu",
                "W Gao",
                "S-C. Dou"
            ],
            "title": "Systematic Analysis of a Military Wearable Device Based on a Multi-Level Fusion Framework: Research Directions",
            "venue": "PMID:",
            "year": 1912
        },
        {
            "authors": [
                "L. Fan"
            ],
            "title": "Multiple sensor data fusion algorithm based on fuzzy sets and statistical theory",
            "venue": "Zhang W,",
            "year": 2020
        },
        {
            "authors": [
                "R Cupek",
                "A Ziebinski",
                "M Drewniak",
                "M. Fojcik"
            ],
            "title": "Knowledge integration via the fusion of the data models used in automotive production systems",
            "venue": "Enterprise Information Systems",
            "year": 2019
        },
        {
            "authors": [
                "M Fu",
                "J Liu",
                "H Zhang",
                "S. Lu"
            ],
            "title": "Multisensor Fusion for Magnetic Flux Leakage Defect Characterization Under Information Incompletion",
            "venue": "IEEE Trans Ind Electron",
            "year": 2021
        },
        {
            "authors": [
                "M Kanmani",
                "V. Narasimhan"
            ],
            "title": "An optimal weighted averaging fusion strategy for remotely sensed images",
            "venue": "Multidim Syst Sign Process",
            "year": 2019
        },
        {
            "authors": [
                "M Mokarram",
                "HR Pourghasemi",
                "JP. Tiefenbacher"
            ],
            "title": "Using Dempster\u2013Shafer theory to model earthquake events",
            "venue": "Nat Hazards",
            "year": 2020
        },
        {
            "authors": [
                "Z Wu",
                "Q Zhang",
                "L Cheng",
                "S. Tan"
            ],
            "title": "A New Method of Two-stage Planetary Gearbox Fault Detection Based on Multi-Sensor Information Fusion",
            "venue": "Applied Sciences",
            "year": 2019
        },
        {
            "authors": [
                "P Wen",
                "Y Li",
                "S Chen",
                "S. Zhao"
            ],
            "title": "Remaining Useful Life Prediction of IIoT-Enabled Complex Industrial Systems With Hybrid Fusion of Multiple Information Sources",
            "venue": "IEEE Internet Things J. 2021;",
            "year": 2021
        },
        {
            "authors": [
                "Holst C-A",
                "Lohweg V"
            ],
            "title": "Feature fusion to increase the robustness of machine learners in industrial environments. at\u2014Automatisierungstechnik",
            "year": 2019
        },
        {
            "authors": [
                "M Simjanoska",
                "S Kochev",
                "J Tanevski",
                "AM Bogdanova",
                "G Papa",
                "T. Eftimov"
            ],
            "title": "Multi-level information fusion for learning a blood pressure predictive model using sensor data",
            "venue": "Information Fusion",
            "year": 2020
        },
        {
            "authors": [
                "R Polvara",
                "F Del Duchetto",
                "G Neumann",
                "M. Hanheide"
            ],
            "title": "Navigate-and-Seek: A Robotics Framework for People Localization in Agricultural Environments",
            "venue": "IEEE Robot Autom Lett. 2021;",
            "year": 2021
        },
        {
            "authors": [
                "Mokarram M",
                "Khosravi MR"
            ],
            "title": "A cloud computing framework for analysis of agricultural big data based on Dempster\u2013Shafer theory",
            "venue": "J Supercomput",
            "year": 2021
        },
        {
            "authors": [
                "J Hou",
                "Q Li",
                "Y Liu",
                "S. Zhang"
            ],
            "title": "An Enhanced Cascading Model for E-Commerce Consumer Credit Default Prediction",
            "venue": "Journal of Organizational and End User Computing",
            "year": 2021
        },
        {
            "authors": [
                "Ai Y-T",
                "Guan J-Y",
                "Fei C-W",
                "Tian J",
                "Zhang F-L"
            ],
            "title": "Fusion information entropy method of rolling bearing fault diagnosis based on n-dimensional characteristic parameter distance",
            "venue": "Mechanical Systems and Signal Processing",
            "year": 2017
        },
        {
            "authors": [
                "Y Qin",
                "S Xiang",
                "Y Chai",
                "H. Chen"
            ],
            "title": "Macroscopic\u2013Microscopic Attention in LSTM Networks Based on Fusion Features for Gear Remaining Life Prediction",
            "venue": "IEEE Trans Ind Electron",
            "year": 2020
        },
        {
            "authors": [
                "X Zhao",
                "M Jia",
                "P Ding",
                "C Yang",
                "D She",
                "Z. Liu"
            ],
            "title": "Intelligent Fault Diagnosis of Multichannel Motor\u2013Rotor System Based on Multimanifold Deep Extreme Learning Machine",
            "venue": "IEEE/ASME Trans Mechatron",
            "year": 2020
        },
        {
            "authors": [
                "V Yaghoubi",
                "L Cheng",
                "W Van Paepegem",
                "M. Kersemans"
            ],
            "title": "A novel multi-classifier information fusion based on Dempster\u2013Shafer theory: application to vibration-based fault detection",
            "venue": "Structural Health Monitoring",
            "year": 2021
        },
        {
            "authors": [
                "ADP Dourado",
                "FS Lobato",
                "AA Cavalini",
                "V. Steffen"
            ],
            "title": "Fuzzy Reliability-Based Optimization for Engineering System Design",
            "venue": "Int J Fuzzy Syst",
            "year": 2019
        },
        {
            "authors": [
                "T Xiahou",
                "Y. Liu"
            ],
            "title": "Reliability bounds for multi-state systems by fusing multiple sources of imprecise information",
            "venue": "IISE Transactions",
            "year": 2020
        },
        {
            "authors": [
                "B Suo",
                "L Zhao",
                "Y. Yan"
            ],
            "title": "A novel Dempster-Shafer theory-based approach with weighted average for failure mode and effects analysis under uncertainty",
            "venue": "Journal of Loss Prevention in the Process Industries",
            "year": 2020
        },
        {
            "authors": [
                "W Jiang",
                "S Wang",
                "X Liu",
                "H Zheng",
                "B. Wei"
            ],
            "title": "Evidence conflict measure based on OWA operator in open world",
            "venue": "Deng Y, editor. PLoS ONE",
            "year": 2022
        },
        {
            "authors": [
                "C Brommer",
                "R Jung",
                "J Steinbrener",
                "S. Weiss"
            ],
            "title": "MaRS: A Modular and Robust Sensor-Fusion Framework",
            "venue": "IEEE Robot Autom Lett. 2021;",
            "year": 2020
        },
        {
            "authors": [
                "F. Xiao"
            ],
            "title": "CaFtR: A Fuzzy Complex Event Processing Method",
            "venue": "Int J Fuzzy Syst. 2021 [cited",
            "year": 2021
        },
        {
            "authors": [
                "Y Wang",
                "F Liu",
                "A. Zhu"
            ],
            "title": "Bearing Fault Diagnosis Based on a Hybrid Classifier Ensemble Approach and the Improved Dempster-Shafer Theory",
            "venue": "PMID:",
            "year": 2097
        },
        {
            "authors": [
                "Sarabi-Jamab A",
                "Araabi BN"
            ],
            "title": "An information-based approach to handle various types of uncertainty in fuzzy bodies of evidence. Calcagn\u0131\u0300 A, editor. PLoS ONE",
            "venue": "PMID:",
            "year": 1929
        },
        {
            "authors": [
                "W Ma",
                "Y Jiang",
                "X. Luo"
            ],
            "title": "A flexible rule for evidential combination in Dempster\u2013Shafer theory of evidence",
            "venue": "Applied Soft Computing",
            "year": 2019
        },
        {
            "authors": [
                "CS Lai",
                "Y Tao",
                "F Xu",
                "WWY Ng",
                "Y Jia",
                "H Yuan"
            ],
            "title": "A robust correlation analysis framework for imbalanced and dichotomous data with uncertainty",
            "venue": "Information Sciences",
            "year": 2019
        },
        {
            "authors": [
                "F Xia",
                "H Tang",
                "S. Wang"
            ],
            "title": "Relationships between knowledge bases and their uncertainty measures",
            "venue": "Fuzzy Sets and Systems",
            "year": 2019
        },
        {
            "authors": [
                "F. Xiao"
            ],
            "title": "Multi-sensor data fusion based on the belief divergence measure of evidences and the belief entropy",
            "venue": "Information Fusion",
            "year": 2019
        },
        {
            "authors": [
                "L Zhang",
                "L Ding",
                "X Wu",
                "MJ. Skibniewski"
            ],
            "title": "An improved Dempster\u2013Shafer approach to construction safety risk perception",
            "venue": "Knowledge-Based Systems",
            "year": 2017
        },
        {
            "authors": [
                "F. Xiao"
            ],
            "title": "CEQD: A Complex Mass Function to Predict Interference Effects",
            "venue": "IEEE Trans Cybern. 2021;",
            "year": 2020
        },
        {
            "authors": [
                "X Wu",
                "J Duan",
                "L Zhang",
                "SM. AbouRizk"
            ],
            "title": "A hybrid information fusion approach to safety risk perception using sensor data under uncertainty",
            "venue": "Stoch Environ Res Risk Assess",
            "year": 2018
        },
        {
            "authors": [
                "W Jiang",
                "W Hu",
                "C. Xie"
            ],
            "title": "A New Engine Fault Diagnosis Method Based on Multi-Sensor Data Fusion",
            "venue": "Applied Sciences",
            "year": 2017
        },
        {
            "authors": [
                "J Li",
                "B Xie",
                "Y Jin",
                "Z Hu",
                "L. Zhou"
            ],
            "title": "Weighted Conflict Evidence Combination Method Based on Hellinger Distance and the Belief Entropy",
            "venue": "IEEE Access",
            "year": 2020
        },
        {
            "authors": [
                "Y Tang",
                "D Zhou",
                "S Xu",
                "Z. He"
            ],
            "title": "A Weighted Belief Entropy-Based Uncertainty Measure for Multi-Sensor Data Fusion",
            "venue": "PMID:",
            "year": 2017
        },
        {
            "authors": [
                "I Ullah",
                "J Youn",
                "Y-H. Han"
            ],
            "title": "Multisensor Data Fusion Based on Modified Belief Entropy in Dempster\u2013Shafer Theory for Smart Environment",
            "venue": "IEEE Access",
            "year": 2021
        },
        {
            "authors": [
                "E Brumancia",
                "S Justin Samuel",
                "LM Gladence",
                "K. Rathan"
            ],
            "title": "Hybrid data fusion model for restricted information using Dempster\u2013Shafer and adaptive neuro-fuzzy inference (DSANFI) system",
            "venue": "Soft Comput",
            "year": 2019
        },
        {
            "authors": [
                "F. Xiao"
            ],
            "title": "Generalization of Dempster\u2013Shafer theory: A complex mass function",
            "venue": "Appl Intell",
            "year": 2020
        },
        {
            "authors": [
                "VM Mond\u00e9jar-Guerra",
                "R Mu\u00f1oz-Salinas",
                "MJ Mar\u0131\u0301n-Jim\u00e9nez",
                "A Carmona-Poyato",
                "R. Medina-Carnicer"
            ],
            "title": "Keypoint descriptor fusion with Dempster\u2013Shafer theory",
            "venue": "International Journal of Approximate Reasoning",
            "year": 2015
        },
        {
            "authors": [
                "C Elkin",
                "R Kumarasiri",
                "DB Rawat",
                "V. Devabhaktuni"
            ],
            "title": "Localization in wireless sensor networks: A Dempster-Shafer evidence theoretical approach",
            "venue": "Ad Hoc Networks",
            "year": 2017
        },
        {
            "authors": [
                "S Frittella",
                "K Manoorkar",
                "A Palmigiano",
                "A Tzimoulis",
                "N. Wijnberg"
            ],
            "title": "Toward a Dempster-Shafer theory of concepts",
            "venue": "International Journal of Approximate Reasoning",
            "year": 2020
        },
        {
            "authors": [
                "Y Lin",
                "Y Li",
                "X Yin",
                "Z. Dou"
            ],
            "title": "Multisensor Fault Diagnosis Modeling Based on the Evidence Theory",
            "venue": "IEEE Trans Rel",
            "year": 2018
        },
        {
            "authors": [
                "MN Khan",
                "S. Anwar"
            ],
            "title": "Paradox Elimination in Dempster\u2013Shafer Combination Rule with Novel Entropy Function: Application in Decision-Level Multi-Sensor Fusion",
            "venue": "https://doi.org/",
            "year": 1921
        },
        {
            "authors": [
                "Z Luo",
                "Y. Deng"
            ],
            "title": "A vector and geometry interpretation of basic probability assignment in Dempster-Shafer theory",
            "venue": "Int J Intell Syst",
            "year": 2020
        },
        {
            "authors": [
                "X Zhao",
                "Y Jia",
                "A Li",
                "R Jiang",
                "Y. Song"
            ],
            "title": "Multi-source knowledge fusion: a survey",
            "venue": "World Wide Web",
            "year": 2020
        },
        {
            "authors": [
                "W Xu",
                "J. Yu"
            ],
            "title": "A novel approach to information fusion in multi-source datasets: A granular computing viewpoint",
            "venue": "Information Sciences",
            "year": 2017
        },
        {
            "authors": [
                "C Zhu",
                "B Qin",
                "F Xiao",
                "Z Cao",
                "HM. Pandey"
            ],
            "title": "A fuzzy preference-based Dempster-Shafer evidence theory for decision fusion",
            "venue": "Information Sciences",
            "year": 2021
        },
        {
            "authors": [
                "R Li",
                "Z Chen",
                "H Li",
                "Y. Tang"
            ],
            "title": "A new distance-based total uncertainty measure in Dempster-Shafer evidence theory",
            "venue": "Appl Intell. 2021 [cited",
            "year": 2021
        },
        {
            "authors": [
                "Q Tian",
                "G Huang",
                "K Hu",
                "D. Niyogi"
            ],
            "title": "Observed and global climate model based changes in wind power potential over the Northern Hemisphere during 1979\u20132016",
            "venue": "https://doi",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "In the process of intelligent system operation fault diagnosis and decision making, the multi-\nsource, heterogeneous, complex, and fuzzy characteristics of information make the conflict,\nuncertainty, and validity problems appear in the process of information fusion, which has not\nbeen solved. In this study, we analyze the credibility and variation of conflict among evi-\ndence from the perspective of conflict credibility weight and propose an improved model of\nmulti-source information fusion based on Dempster-Shafer theory (DST). From the perspec-\ntives of the weighting strategy and Euclidean distance strategy, we process the basic proba-\nbility assignment (BPA) of evidence and assign the credible weight of conflict between\nevidence to achieve the extraction of credible conflicts and the adoption of credible conflicts\nin the process of evidence fusion. The improved algorithm weakens the problem of uncer-\ntainty and ambiguity caused by conflicts in the information fusion process, and reduces the\nimpact of information complexity on analysis results. And it carries a practical application out\nwith the fault diagnosis of wind turbine system to analyze the operation status of wind tur-\nbines in a wind farm to verify the effectiveness of the proposed algorithm. The result shows\nthat under the conditions of improved distance metric evidence discrepancy and credible\nconflict quantification, the algorithm better shows the conflict and correlation among the evi-\ndence. It improves the accuracy of system operation reliability analysis, improves the utiliza-\ntion rate of wind energy resources, and has practical implication value."
        },
        {
            "heading": "Introduction",
            "text": "Information fusion technology has solved many troubles [1\u20137] in the military, engineering, and environment since it developed in the 1970s [8]. The application have expanded to much more areas, such as extra energy, new materials, manufacturing, medicine, agriculture, transportation, and economy [9\u201315]. The utilization of information fusion technology enhances the\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 1 / 20\nOPEN ACCESS\nCitation: Gou L, Zhang J, Li N, Wang Z, Chen J, Qi L (2022) Weighted assignment fusion algorithm of evidence conflict based on Euclidean distance and weighting strategy, and application in the wind turbine system. PLoS ONE 17(1): e0262883. https://doi.org/10.1371/journal.pone.0262883\nEditor: Dragan Pamucar, University of Defence in Belgrade, SERBIA\nReceived: August 31, 2021\nAccepted: January 10, 2022\nPublished: January 24, 2022\nCopyright: \u00a9 2022 Gou et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nData Availability Statement: All relevant data are within the paper and its Supporting Information files.\nFunding: The authors received the support of the national key research and development project (2018YFB1403303) and Qinxin Talents Cultivation Program of Beijing Information Science & Technology University (No. QXTCP B201906).\nCompeting interests: The authors have declared that no competing interests exist.\nsystem fault tolerance, self-adaptability, and reduces inference fuzziness. It meets the requirement of traditional algorithms for a priori probability and provides a basis for event decisionmaking. Its typical features make it widely used in fault diagnosis, anomaly detection, reliability, inference, prognosis, and early prediction [16\u201319].\nIn the information explosion era, information presents a massive, multi-source, heteroge-\nneous, multi-dimensional, complex, and fuzzy feature. It has developed rapidly emerging information technology. The development of intelligence has significantly increased the complexity of the various levels of the system, which makes the system faces reliable operation challenges [20, 21]. Under this condition, the Chinese government actively encourages researchers to organize fundamental research on the reliable operation of important equipment and components in key areas, including extra energy, energy conservation, emission reduction, and environmental protection. The data-driven multi-source information fusion technology has become one concern of system operation reliability research.\nWith extended the prior research, the Dempster-Shafer theory (DST) fusion algorithm has\nachieved better performance in comprehensive system state analysis and decision making. However, it has a strong subjective dependence [22] on basic probabilities assignment (BPA) and the independence of evidence, and the correlation relationship between evidence affects the fusion [23]. There are even troubles with distortion and disorder in the practical application process. Thus, based on previous studies, this study argues that quantifying the correlation between evidence and fairly assigning the fusion weights of evidence features is crucial to the fusion results. In response to these questions, researchers have studied the DST fusion algorithm from the perspectives of fusion framework, weight allocation, and method combination.\nIn terms of fusion frameworks, researchers have proposed different framework models,\nwhich improved the algorithm effectiveness. Brommer et al. [24] proposed a modular multisensor fusion framework, which is better efficient in dealing with delayed statistics collection, disordered updates, and monitoring the health of sensors themselves in complex systems. Xiao [25] discussed the modeling of uncertainty based on the framework of Triangular fuzzy numbers for fuzzy complex event processing systems in an uncertain environment, and proposed a fault-tolerant and reliable strategy for scheduling. Wang et al. [26] dealt with evidence conflicts in DST under the framework of fuzzy preference relationships, which improved the diagnostic accuracy of hybrid classifier integration. Prior research improved the idea and effectiveness of the integration to different degrees under the idea of modularity and different attention allocation.\nTo deal with the diversity, uncertainty, and conflict of information, researchers have pro-\nposed ideas of feature correlation, difference, different conflict values, and non-similarity measures. They improved and integrated algorithms [27\u201331] from mathematical perspectives, such as mean, combination, and entropy. Zhang et al. [32] proposed a method incorporating fuzzy object elements, Monte Carlo simulation, and DST, through weighted averaging and data deblurring rules, the result has clear analytical values to represent the final risk level. Xiao [33] combined the complex D-S theory and Quantum mechanics, to express and handle the uncertain information in the framework of the complex plane, and reduce the interference effects caused by uncertainty.\nWu et al. [34] proposed an improved evidence aggregation strategy combining the Demp-\nster-Shafer rule and the weighted average rule. It overcomes the counterintuitive dilemma existing in the high conflict evidence combination by constructing the BPA under relevance metric. Jiang et al. [35] used evidence theory to model uncertainty, adopted a weighted average combination method to merge BPAs. Finally, it validated the method by motor empirical cases under the decision rules. Li et al. [36] proposed a weighted conflicting evidence combination method based on Hellinger distance and belief entropy., and uses distance to measure the\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 2 / 20\nconflict between evidence and applies belief entropy to quantify the uncertainty of basic belief assignments.\nUnder the Dempster-Shafer framework, Tang et al. [37] proposed a weighted belief entropy\nwhich is based on Dunn\u2019s entropy, to quantify the uncertainty in uncertain information and reduce information loss during information processing. Ullah et al. [38] designed a data fusion scheme based on improved BPA belief entropy and quantified the uncertainty in information and transform conflicting data into decision results. The simulation result showed that the proposed scheme had stronger performance in terms of uncertainty, reason, and decision accuracy in an intelligent environment. Brumancia et al. [39] proposed an information fusion algorithm for decision making under different information conditions, which is based on D-S theory and adaptive neuro-fuzzy reasoning (DSANFI) system, it has widely used in robotics, statistics, control, and other fields.\nFrom the researchers\u2019 exploration, the information fusion algorithm based on Dempster-\nShafer has always been a hot focus of research, which has a broad theoretical and practical value. In the current development process, the widespread application of intelligent systems increases the demand for system operation and maintenance. However, the existing algorithms [40\u201342] still have different degrees of information loss, fusion disorder, and low fusion accuracy in practical application, and the algorithms have the problem of universality [43].\nSome studies [44\u201346] suggested that the main problem of the affected fusion results are the\nincomplete identification framework of evidence features, and the basic reliability probability of evidence is difficult to calculate completely and accurately, which lead to information loss and disorder. Therefore, in this research, the DST fusion model is promoted from the perspectives of the knowledge fusion framework, quantification of correlations, and extraction of credible conflicts to overcome the information loss problem."
        },
        {
            "heading": "Propose a fusion framework",
            "text": "The multi-source information fusion problem in this paper refers to integrating multiple sources of information. Multiple sources are information originates from different means of monitoring the same part of the same thing. Therefore, in our proposed fusion framework, the multi-source information fusion problem [47] is summarized as a ternary problem, as shown in Eq (1).\nQ \u00bc fNi;< Ni >;Dg \u00f01\u00de\nWhere, Ni, <Ni>, and D represent data, features, and decisions respectively. The type, state, format, and scenario of data lead to its multi-source heterogeneity and com-\nplexity in the information management. Data set Ni contains an enormous amount of information, and the information is represented in the knowledge form, and the data feature set <Ni> is constructed by mining the information of potential features\u2019 data from the perspective of knowledge management. The knowledge is fused with algorithms to improve the recognition framework, and the accuracy and reliability of algorithms in the fusion process are improved to provide the foundation for management decisions. The relationship between data, features, and decisions is shown in Fig 1.\nData fusion is mainly reflected by the fusion of data features. It fuses the features exhibited\nby multiple homogeneous or heterogeneous data in the time or frequency domain which is beneficial to decision making. Considering the different data exhibits different features, assuming that Vi is different perspectives, then there is some correspondence between the whole process from the mapping of perspective space to data space and then to data feature space, as shown in Eq (2). When the data features or attributes cannot be directly fused, some\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 3 / 20\nkind of consistency processing needs to be performed before fusion.\nV1 V2 . . .\nVi\n0\nB B B B @\n1\nC C C C A . . .) . . .\nN!1 N!2 . . . N!m\n0\nB B B B B @\n1\nC C C C C A . . .) . . .\n< N!1 > < N!2 >\n. . .\n< N!m >\n0\nB B B B B @\n1\nC C C C C A\n\u00f02\u00de\nIt studies the multi-source information fusion analysis framework from three perspectives:\ninformation, algorithm, and decision making, and presents the problems of data ambiguity, conflicting evidence, and low fusion degree in the fusion process. It takes data represented as knowledge and classifies information features from different sources. Considering features similarity and conflict, data features should be quantified and changing rules should be found out, to weaken data ambiguity and keep the potential value of information [48]. Regarding the shortage of algorithms, it deals with the consistency of features and adopts methods of conflict weight assignment to reduce the impact of evidence association and evidence conflict on the fusion results. According to the feature performance, it can make a judgment on the system condition, to rationalize the system failure management in time and effectively reduce the loss. The fusion analysis framework is shown in Fig 2."
        },
        {
            "heading": "Materials and methods",
            "text": "This study divides the algorithm into two stages, including the BPA calculation session and the fusion session. In parts \u201cImproved algorithm under the weighting strategy\u201d and \u201cImproved algorithm under Euclidean distance weighting strategy\u201d, improvements to the BPA calculation process are proposed, in part \u201cFusion algorithm under Improved Euclidean distance weighting strategy\u201d, improvements of the fusion session is proposed. The fusion improvements are based on the BPA calculation.\nhttps://doi.org/10.1371/journal.pone.0262883.g001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 4 / 20\nImproved algorithm under the weighting strategy\nThe feature information in different data sources of the same type has a certain similarity, and the feature information in different heterogeneous data sources also has a certain similarity. Studying the homogeneity and heterogeneity of data, it is necessary to analyze the data similarity when analyzing faults, to reduce the repetitive calculation work. Therefore, it defines a formal concept of data feature similarity, which is the degree of the similarity of the features in the information. As known that the data set is composed of multiple data, so it can be as a matrix.\nTherefore, the features of the data can be denoted as< EiN ! j >, then the similarity between\nthe corresponding features of the two data sets is denoted as Sim\u00f0< EiN ! j >;< EkN ! j >\u00de. According to the data time domain, the data is paired by pair, and the weight of the qth pair of data features is denoted as wq, then the similarity between the features of the two sets can be expressed by Eq (3).\nSim\u00f0< EiN ! j >;< EkN ! j >\u00de \u00bc X\nk\nwqSimk\u00f0< EiN ! j >;< EkN ! j >\u00de \u00f03\u00de\nWhere, Nj2N, i6\u00bck, i, j, k, q is not equal to 0. The weight wq are assigned according to the importance of the features characterized by the data and need to satisfy w1+w2+. . .+wi = 1.\nThere is a similarity between evidence i and j, so it introduces the similarity factor Si. The weighting strategy is used to quantify the similarity between evidence features, then the specific formula for quantifying the similarity between the two evidence is shown in Eq (4).\nSi \u00bc< EiN ! j > \ufffd < EkN ! j > =\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi < EiN ! j> 2\u00fe < EkN ! j> 2 q\n\u00f04\u00de\nLet the similarity of the evidence be Simz, then the similarity of evidence i is shown in\nhttps://doi.org/10.1371/journal.pone.0262883.g002\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 5 / 20\nEq (5).\nSimz \u00bc X \u00f0< EiN ! j > \ufffd < EkN ! j > =\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi < EiN ! j> 2\u00fe < EkN ! j> 2 q \u00de \u00f05\u00de\nThen, it gets a set of similarity sequences of length n\ufffd(n\u22121)/2, where n>1. Each group of evidence that forms a series of similarities with other evidence and the number of similarity data between evidence i and other evidence is (n-1). Therefore, the total similarity between evidence i and other evidence can be expressed by Eq (6).\nSim \u00bc X \u00f0Simzij\u00de\n|fflfflfflfflfflfflffl{zfflfflfflfflfflfflffl} n 1\n\u00f06\u00de\nWhere, i is the specified evidence, and when i is fixed without change, the dynamic value is taken for j, i6\u00bcj.\nThen, the weights of the evidence are assigned as shown in Eq (7).\nwi \u00bc Simi= X Sim \u00f07\u00de\nWhen the similarity of data features is high, the weight of evidence is correspondingly high,\nshows that the supporting evidence for a certain type of event occurrence is high. And it can use more complete evidence data for two types of evidence factors that have high similarities. When the similarity is low, the weight declines, means that the perspective of making a judgment on event occurrence between data may be different, rather than the completely untrustworthy evidence. So it adopts multiple evidence factors to mine valuable conflicting information, to improve the accuracy of system fault diagnosis.\nOnce the similarity of the characteristics of the evidence is mastered, it can perform a new\nfusion of the evidence.\nImproved algorithm under Euclidean distance weighting strategy\nDegree of evidence variation. In practice, there are conflicts among evidence [49]. Con-\nflict is a kind of information related to the similarity of data features and is likely to have some value. The BPA of the evidence shows the credibility level of the evidence and reflects consistency in the assignment of the basic credibility probability of the evidence to the focal elements. Therefore, this study performs dynamic extraction of BPA, and based on this, adjusts the weight of evidence under conflict conditions, assigns conflict coefficients to different focal elements, reduces the weight of evidence with lower confidence in the fusion process, to improve the reliability of fusion results.\nAccording to the relation between the variation in historical data features and the reliability\nof the system, it sets a reasonable threshold value. Dynamically monitor and extract the frequency of data features emerging in different threshold ranges to get the BPA of dynamic changes, as shown in Eq (8).\nP A !\ni\n\ufffd \ufffd \u00bc fiP fi\n\u00f08\u00de\nThe primary methods to measure the correlation between data include distance measure, Pearson relationship coefficient, cosine similarity, and deviation measure. Among them, the Pearson relationship coefficient is usually used to measure the inconsistency of data scale, when there is a subjective judgment standard inconsistency scenario. The cosine similarity\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 6 / 20\ncoefficient is acting on data sparsity. The deviation is to reflect the difference between the basic credible probability distribution of focal elements and the average similarity value, but its use of the average similarity value weakens the measure of the true difference of data. Euclidean distance is a simple method to measure the distance between two points in the m-dimensional space and especially has a significant advantage with integrity data. Therefore, this paper adopts distance [50] to reflect the difference between two sets of data. Assuming that the difference between two pieces of evidence i and j is dij, to ensure that the data is positive, Eq (9) can express the calculation of the difference between two pieces of evidence.\ndij \u00bc\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X\nB\\C\u00bcA\nm1\u00f0A ! i\u00de m2\u00f0A ! j\u00de\n\ufffd \ufffd2 s\n\u00f09\u00de\nWhen the difference between two pieces of evidence is high, the similarity between the evi-\ndence is low and the conflict is high. When the difference is low, the similarity between the evidence is high and the conflict low.\nThe total number of data on the variation among the evidence is n\ufffd(n\u22121)/2, where, n> 1. It aggregates the differences between one evidence and the other to get n sets of variation data.\nThen, Eq (10) can express the difference between evidence i and all others that affects the conclusion.\nd mi\u00f0A ! i\u00de\n\ufffd \ufffd\n\u00bc X\ndij; i\u3001j \u00bc 1; 2; . . . ; n: \u00f010\u00de\nNormalizing the difference between evidence i and others is the difference of evidence i, which can be expressed by Eq (11).\nd\u00f0mi\u00de \u00bc d mi\u00f0A\n! i\u00de\n\ufffd \ufffd\nXn\ni\u00bc1\nd mi\u00f0A ! i\u00de\n\ufffd \ufffd \u00f011\u00de\nWhere, n denotes the number of evidence, and the credibility of evidence i is low when it conflicts with other evidence at a high level.\nCredible weight of evidence. The confidence level of the evidence reflects the credibility\nlevel of the evidence, and the similarity of the focal elements reflects the similarity of the evidence, which reflects the consistency in the assignment of the basic credibility probability of the evidence to the focal elements. So this is an entry point for adjusting the weight under conflict conditions. Assigning the conflict coefficient K to different focal elements Ai reduces the weight of evidence with lower confidence in the fusion process, thus increasing the weight of evidence with high confidence and improving the reliability of fusion results.\nConfidence is the support of data features to the event results, and it is the trustworthiness of the evidence information. The confidence function on the identification framework can be expressed by Eq (12).\nBel\u00f0A ! \u00de \u00bc\nX\nB\ufffdA\nm\u00f0B!\u00de \u00f012\u00de\nThe equation shows that the confidence function is the sum of the probabilities of event\nsupport for all subsets of that event, and B is a subset of A. The confidence function has a certain influence on the reliable transmission of the system.\nThe likelihood function is the degree to which the evidence information does not negate\nthe occurrence of an event, and it shows that the likelihood function is the sum of the\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 7 / 20\nprobabilities that the intersection with that event is not empty. In the identification framework, it can express the likelihood function in Eq (13).\nPl \u00f0A ! \u00de \u00bc\nX\nB\\A6\u00bc\ufffd\nm\u00f0B!\u00de \u00f013\u00de\nThe likelihood function contains both credible and implausible information, as shown in\nEq (14). Therefore, the credibility of the evidence needs to be analyzed.\nPl\u00f0A ! \u00de!\u00f0Cre\u00f0mi\u00de; nCre\u00f0mi\u00de\u00de \u00f014\u00de\nThere is a correlation between the support and the discrepancy of the evidence, as expressed\nin Eq (15).\nSup\u00f0mi\u00de \u00bc 1 d\u00f0mi\u00de \u00f015\u00de\nTherefore, the credible weight of evidence to focal element support can be expressed in Eq (16).\nCre\u00f0mi\u00de \u00bc Sup\u00f0mi\u00de\nXn\ni\u00bc1\nSup\u00f0mi\u00de \u00f016\u00de\nWhere, Cre(mi)2[0,1], \u2211Cre(mi) = 1. When the credibility weight of evidence to focal element support is high, it shows that the\nsupports of other evidence is to a high degree. The credible weight corresponds to the confidence function in the identification framework, and the product of the credible weight and the confidence function is the reliability of that subsystem. Then the reliability transfer function of the entire system is the product of the subsystem reliability.\nFusion algorithm under improved Euclidean distance weighting strategy. The BPA cal-\nculation session introduces evidence similarity, evidence difference, and evidence trustworthiness weights to improve the BPA calculation process of the original algorithm. To fully retain the trustworthy conflicts, this part improves the fusion session of the algorithm based on the improved BPA calculation session.\nThe conflict involvement in fusion directly affects the BPA of the event. Therefore, we con-\nstruct an improved probability assignment model in terms of the credible weight assignment of conflict information, which uses the product of the credibility of evidence to focal element support and the original probability assignment function. Then, it shows the BPA function of evidence under the new probability assignment model calculated through the BPA calculation session in Eq (17).\nm0j\u00f0A ! i\u00de \u00bc Cre\u00f0mi\u00de\ufffdmj\u00f0A ! i\u00de \u00f017\u00de\nBy introducing credible conflict, the sum of the fusion results of the relevance of evidence\nand the fusion results of the credible conflicting evidence makes up a new fusion function. The improved probability assignment function is a fusion calculation of the BPA of the non-conflicting information and the credible conflicting information in the conflict under the new support condition. It means that the improved BPA function is the fusion calculation of the basic probability assignment of the non-conflicting information under the new support condition and the credible conflicting information in the conflict. Thus, the improved probability assignment function is the sum of the BPA function of evidence to a focal element and the support of other evidence to that focal element under the conflict condition, which contains the credible conflict extraction treatment under the new weight for the changed evidence. It shows\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 8 / 20\nthe new probability distribution function in Eq (18).\nm@j \u00f0A ! i\u00de \u00bc m 0 j\u00f0A ! i\u00de \u00fe Cre\u00f0mi\u00de X m0s\u00f0A ! i\u00de \u00f018\u00de\nWhere, s6\u00bcj, i\u3001j\u3001s\ufffdn. Cre\u00f0mi\u00de X m0s\u00f0A ! i\u00de denotes the extent to which other evidence\nagrees with evidence j in support of focal element Ai. Normalizing the improved probability assignment to keep the probabilities are in the same mapping environment. Reassigning the weights of conflict and the sum of all evidence probabilities is 1. Under the new conditions, we classify the features of credible conflicts into the category of trustworthy features; the evidence is independent of each other, and the remaining conflicts that are not considered are discardable.\nTherefore, the evidence under the new BPA is re-fused, and it shows the fusion rule in Eq\n(19).\nE1 \ufffd E2 . . .\ufffd En \u00bc 1 1 K 0 X\n\\Ai\u00bcA\nY\ni\ufffdj\ufffdn\nm@j \u00f0A ! i\u00de \u00f019\u00de\nWhere, K 0 \u00bc X\n\\An\u00bc\ufffd\nY\ni\ufffdj\ufffdn\nm@j \u00f0A ! i\u00de, A6\u00bc\u00d8, indicates that the conflicting factors in the original\nevidence are involved in the fusion by credible weights."
        },
        {
            "heading": "Experiment and analysis",
            "text": "Analysis of improved algorithms in wind turbine operation\nWind power generation technology is mature in renewable energy generation. China has abundant wind energy resources, especially on the southeast coast, Liaotung Peninsula, and northeast. Compared with fossil fuels, the use of clean energy such as wind power can have an effect on reducing carbon dioxide emissions and mitigating global climate change trends. According to a study [51], nearly 80% of power plants in Asia have lost over 30% of their wind energy potential since 1979. Therefore, it takes a wind farm in Jilin province, northeast of China, as an example to analyze the wind turbine operation data, diagnose the fault state, improve system reliability, and increase the efficiency of wind energy utilization.\nAccording to the preliminary analysis, we find that wind speed is one of the key parameters\nof wind turbine operation; some data showed consistency in the variation pattern; parameters such as pressure and temperature are more sensitive to environmental changes; changes in voltage and current are associated with other parameters, and the overall fluctuations of different wind turbine operations have some similarity. Therefore, we organize and analyze data with a tendency, and select a representative wind turbine in the wind farm to study the parameters such as generator speed, gearbox low-speed bearing temperature, gearbox oil pressure, gearbox inlet oil temperature, and grid current in a certain period. And it does not describe the screening process here. Table 1 shows some of the underlying data sets in the experiment.\nWhen the wind speed is in the steady-state range, the wind turbine speed in the normal\noperation state of the system is also in the steady-state range. So we analyze the relative change trends of generator speed, gearbox low-speed bearing temperature, gearbox oil pressure, gearbox inlet oil temperature, and grid current during the operation of the wind turbine at a certain time with the wind turbine speed as the base reference parameter. And we find that there is a correlation between the change patterns of some data; the variation trend of different data is different, and the inconsistency of variation shows that there is a conflict between the evidence. Therefore, according to the difference in the changing pattern of data features, we judge whether there is a credible part of the evidence conflict, dig deeply into the consistency\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 9 / 20\ninformation and conflict information in the data, extract credible fault features, analyze the system operation status, and diagnose the system fault.\nLet the relative change trends of parameters such as generator speed, gearbox low-speed\nbearing temperature, gearbox oil pressure, gearbox inlet oil temperature, and grid current are the evidence E1, E2, E3, E4, and E5, respectively. According to the characterization of distinct features, we excerpt valid and representative data periods from the data set, select the basic feature parameters of the evidence in the operation state, and map them into the [0,1] interval to eliminate the influence of data heterogeneity on feature fusion, as shown in Table 2.\nIn Table 2, we can see that the selected evidence is overall well aggregated. From the vari-\nance and root mean square, the dispersion of evidence E1 and E2 is higher than that of E3, E4, and E5; from the cliff factor, the fluctuation of the evidence is roughly a continuous flat change, showing that the data situation is more stable, and it can select the above parameters for the next analysis of the wind turbine.\nWe divide the mapping of the evidence to system fault support into four types: normal\nstate, implicit fault, explicit fault, and warning. According to the actual occurrence of faults, we identify the points with a more stable change trend in the evidence, define the distribution of the evidence characteristics in the fault characterization, and determine the interval of the fault characterization, as shown in Table 3.\nSince 0 in the mapping interval [0,1] contains the cases of the continuous shutdown without\nstarting and shutdown due to fault, we remove element 0 from the normal state F0, which means that it excludes the status data at the moment of normal wind turbine start-up. While\nTable 1. Partial base dataset.\nTime Generator speed Gearbox low-speed bearing\ntemperature\nGearbox oil pressure Gearbox inlet oil\ntemperature\nGrid current Wind turbine rotation\nspeed\n1 495 105 1 26 64 46 2 11025 155 76 19 730 114 3 11015 257 75 9 626 104 4 10957 308 74 11 650 104 5 10974 335 74 15 653 104 6 10984 352 73 21 626 104 7 11001 363 72 29 645 104 8 10999 372 70 39 672 104 9 11243 379 68 52 818 114 10 12156 387 68 46 1618 119 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nn 11000 497 43 325 825 100\nhttps://doi.org/10.1371/journal.pone.0262883.t001\nprocessing element 0 in the fault state by adding 1 and classifying it into the warning state F3. The fault interval varies for different systems under different climatic conditions and needs to be determined dynamically based on historical state data.\nOrganize the data of wind turbine operation, and analyze the fluctuation of data character-\nistics under different state conditions in the historical data. According to the distribution of the points of the evidence fluctuation interval in different states, such as normal state, hidden fault, explicit fault, and warning, we select a certain moment region with certain credibility and representativeness and calculate the dynamic BPA of each evidence. Depending on the selected interval of the system, it dynamically changes the basic probability distribution. The basic probabilities of selected regions in this paper are calculated and derived, as shown in Table 4.\nFrom Table 4, it shows that there are different levels of conflicting situations among the evi-\ndence, with Evidence E1, E4, and E5 considering the system to have a higher probability of explicit failure, Evidence E2 considering the system to have a higher probability of hidden failure, and Evidence E3 considering the system to have a higher probability of normal state. Exhibit E5 considers that the system also has a higher risk of implicit failure under the high probability of explicit failure.\nFusion results of the classical algorithm\nBased on the typical DST, we fuses the above evidence and it shows the fusion results in Table 5.\nFrom Table 5, we see that after the evidence fused by the original algorithm, the system has\na probability of 69.63% of the occurrence of explicit failure, 18.38% of the occurrence of implicit failure, 11.86% of being in a normal state, and a low probability of 0.12% of the occurrence of warning. If evidence E1, E4, and E5 consider the system to have a higher probability of explicit failure, it significantly weakens the support of evidence E3 for the system to be in a normal state and the support of evidence E2 and E5 for the occurrence of implicit failure to some extent. If evidence E2 considers the system to be in a warning state with a lower probability, it weakens the support of evidence E1 for the system to be in a warning state. The trends of the same features strengthen each other and the trends of distinct features weaken each other.\nhttps://doi.org/10.1371/journal.pone.0262883.t004\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 11 / 20\nFusion results of the algorithm under the improved weighting strategy\nCalculate the similarity of the above evidence using a fusion model improved by the weighting method. Then we can get:\nSi \u00bc\n0:1689 0:1603 0:2473 0:0475\n0:1988 0:1432 0:1967 0:0945\n0:1520 0:1451 0:2972 0:0877\n0:1390 0:1549 0:2877 0:0804\n0:2181 0:2064 0:1831 0:0446\n0:1602 0:2121 0:2557 0:0438\n0:1451 0:2470 0:2495 0:0428\n0:1850 0:1767 0:2008 0:0725\n0:1628 0:1954 0:1978 0:0683\n0:1340 0:2002 0:3010 0:0656\n2\n6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4\n3\n7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5\nSimz \u00bc \u00bd0:6240; 0:6331; 0:6819; 0:6620; 0:6522; 0:6718; 0:6845; 0:6350; 0:6242; 0:7008\ufffdT\nSim \u00bc \u00bd2:6010; 2:6324; 2:5445; 2:6895; 2:6715\ufffdT\nFrom this, calculate the weight of evidence, as 0.1980, 0.2003, 0.1937, 0.2047, and 0.2033,\nrespectively.\nAfter reassigning the weights, calculate the new BPA values for each piece of evidence:\n0:2420 0:2323 0:3788 0:1469\n0:2552 0:3238 0:3395 0:0815\n0:3296 0:2627 0:3000 0:1076\n0:2335 0:2672 0:3960 0:1033\n0:3247 0:2653 0:3377 0:0723\n2\n6 6 6 6 6 6 6 6 4\n3\n7 7 7 7 7 7 7 7 5\n.\nhttps://doi.org/10.1371/journal.pone.0262883.t005\nhttps://doi.org/10.1371/journal.pone.0262883.t006\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 12 / 20\nThe fusion results under the new probability are shown in Table 6.\nFusion results of the algorithm under the improved Euclidean distance weighting strategy\nAnalyze the above evidence using the algorithm under the improved distance strategy of this paper. Calculate the variance of distinct evidence for event support.\ndij \u00bc\n0:0295 0:2059 0:0883 0:1471\n0:1962 0:0686 0:1765 0:0883\n0:0195 0:0784 0:0392 0:0981\n0:0489 0:1470 0:0098 0:1079\n0:1667 0:1373 0:0882 0:0588\n0:0490 0:1275 0:1275 0:0490\n0:0784 0:0589 0:0981 0:0392\n0:2157 0:0098 0:2157 0:0098\n0:2451 0:0784 0:1863 0:0196\n0:0294 0:0686 0:0294 0:0098\n2\n6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4\n3\n7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5\nd\u00f0mi\u00de \u00bc \u00bd0:4708; 0:5296; 0:2352; 0:3136; 0:4510; 0:3530; 0:2746; 0:4510; 0:5294; 0:1372\ufffd T\nThe normalized variance is: d\u00f0mi\u00de= X d\u00f0mi\u00de \u00bc \u00bd0:1257; 0:1414; 0:0628; 0:0837; 0:1204; 0:0942; 0:0733; 0:1204; 0:1413; 0:0366\ufffd T\nCalculate the conflicting credible weights of the evidence, and the credible weights of evidence for event support are 0.1983, 0.1983, 0.1846, 0.2107, and 0.2081, respectively, as shown in Table 7.\nBased on the reassigned weights, calculate the new basic probability function values of the\nevidence, as shown in Table 8.\nhttps://doi.org/10.1371/journal.pone.0262883.t007\nhttps://doi.org/10.1371/journal.pone.0262883.t008\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 13 / 20\nFrom Table 8, we see that it redistributes the probabilities after adopting the trusting atti-\ntude to a part of the inter-evidence conflict, the implicit failure rate of evidence E2 decreases, and the explicit failure rate increases; the probability of the normal state of evidence E3 decreases and the probability of explicit failure increases; the probability of the normal state of evidence E5 increases, and the changes of evidence E1 and E4 are smaller. Re-fused them, and it shows the new fusion results in Table 9.\nComparative analysis of fusion results under different algorithms\nThis paper introduces the improved weighting strategy and distance strategy to quantify the correlation and conflict between evidence features in the research process. Compares the fusion results of the original, and improved algorithms, with the actual situation, as shown in Table 10.\nFrom Table 10, the evidence after improved algorithm fusion under the weighting and distance strategies, reduces the probability of explicit failure of the system by 5.48% ~6.03% compared with the original algorithm; it reduces the probability of implicit failure by 1.12% ~1.15% and increases the probability of being in a normal state by 6.64% ~7.16%; the probability of early warning is 0.12%, which is consistent with the original algorithm fusion.\nThe analysis of the fusion results, as shown in Fig 3, leads to the following conclusions.\n1. The changes in the fusion of evidence E1 and E2 before and after the algorithm improved are small, show that the conflicting nature between the two pieces of evidence is small and\nthe conflict participation in the fusion has little impact on the results, as shown in Fig 3(A).\n2. In the fusion with evidence E3 and E4, there is a significant change in the judgment that the system is in the F0 state and F2 state. The improved algorithm discards part of the worthless conflicting information in the evidence and absorbs part of the conflicting information in\nthe two states of F0 and F2, leading to a large deviation before and after the improvement, as shown in Fig 3(B) and 3(C).\n3. When fused with E5, the probability that the system is in each state shows irregular fluctuations, but overall, the probability that the system is in F3 state has been decreasing, as shown in Fig 3(D).\n4. Fig 3(D) and 3(E) demonstrate the gap between the overall trend of fusion change and the\nactual situation. We can see that the improved fusion algorithm fully considers the conflict\nhttps://doi.org/10.1371/journal.pone.0262883.t009\nhttps://doi.org/10.1371/journal.pone.0262883.t010\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 14 / 20\nfactors between the evidence E2 and E3 and E1, E4 and E5 if the evidence E2 and E3 have fully support for the hidden fault and normal states, respectively.\nThe stability analysis of the changing trend of the fusion results, as shown in Fig 4, reveals\nthat the original algorithm fusion results fluctuate more with the actual value fitting curve, and the fluctuation of the fusion results with the actual value fitting curve under the weighting strategy and the distance strategy are the same, both improvements have a certain effect, but\nhttps://doi.org/10.1371/journal.pone.0262883.g003\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 15 / 20\nthe improved algorithm under the distance strategy is slightly better than the weighting strategy, and the target value of the fit is better. The improved algorithm under the distance strategy improves the fit with the actual situation by 9.47% compared with the original algorithm, and the improved algorithm under the weighting strategy improves the fit with the actual situation by 8.37%. Overall, the improved algorithm under the distance strategy has better results in diagnosing and predicting system faults and it is more effective in improving energy utilization efficiency."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, we propose an improved model of multi-source information fusion under the weighting strategy and distance strategy and check the validity by a case of wind turbine system fault diagnosis in northeastern China. The research results show that the improved algorithm approach under distance strategy has a better adaptability and fits to conflicting information, and quantifies the discrepancy of evidence to event support, credibility, and credible conflict weights considering the fit to reality. The involvement of credible conflicts in the\nhttps://doi.org/10.1371/journal.pone.0262883.g004\nPLOS ONE | https://doi.org/10.1371/journal.pone.0262883 January 24, 2022 16 / 20\nfusion diagnosis solves some uncertainties caused by the loss of credible conflicts and weakens the interference of untrustworthy conflicts on the results.\nThe proposed algorithm in this paper improves the accuracy of the calculation model,\nreduces the relevance and uncertainty in the process of using information features, and interprets the practical application significance of the evidence factors after readjusting the basic probability of the evidence. It also improves the scientific and rational system management, enables managers to have a better understand to the system operation status in time. Effectively reducing the system operation and maintenance costs and losses caused by the faults as well as improves the energy utilization efficiency and it has certain advantages in accuracy and timeliness of fault diagnosis.\nThe method is not only applicable to the wind farm calculations but also to the operational\nreliability analysis of other energy utilization systems that require comprehensive consideration of multiple factors. Considering the resource utilization efficiency in China, and the complexity and uncertainty of the system operational environment, in the future, we will study the complex system operation reliability in information technology developmentto improve the overall accuracy of the model and realize efficient management of system operation."
        },
        {
            "heading": "Supporting information",
            "text": ""
        },
        {
            "heading": "S1 Data.",
            "text": "(XLSX)"
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to acknowledge the editor and reviewers for their valuable comments and suggestions on this paper."
        },
        {
            "heading": "Author Contributions",
            "text": "Conceptualization: Liming Gou.\nData curation: Liming Gou.\nFormal analysis: Jian Zhang, Naiwen Li.\nFunding acquisition: Jian Zhang.\nInvestigation: Liming Gou, Jian Zhang, Jindong Chen.\nMethodology: Liming Gou, Jian Zhang, Jindong Chen.\nSupervision: Jian Zhang, Naiwen Li, Lin Qi.\nValidation: Zongshui Wang.\nVisualization: Lin Qi.\nWriting \u2013 original draft: Liming Gou.\nWriting \u2013 review & editing: Liming Gou, Jian Zhang, Naiwen Li, Zongshui Wang."
        }
    ],
    "title": "Weighted assignment fusion algorithm of evidence conflict based on Euclidean distance and weighting strategy, and application in the wind turbine system",
    "year": 2022
}