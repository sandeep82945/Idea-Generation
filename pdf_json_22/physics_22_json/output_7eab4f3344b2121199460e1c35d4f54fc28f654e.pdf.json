{
    "abstractText": "Many computational problems can be formulated in terms of high-dimensional functions. Simple representations of such functions and resulting computations with them typically suffer from the \u201ccurse of dimensionality\u201d, an exponential cost dependence on dimension. Tensor networks provide a way to represent certain classes of high-dimensional functions with polynomial memory. This results in computations where the exponential cost is ameliorated or in some cases, removed, if the tensor network representation can be obtained. Here, we introduce a direct mapping from the arithmetic circuit of a function to arithmetic circuit tensor networks, avoiding the need to perform any optimization or functional fit. We demonstrate the power of the circuit construction in examples of multivariable integration on the unit hypercube in up to 50 dimensions, where the complexity of integration can be understood from the circuit structure. We find very favourable cost scaling compared to quasi-Monte-Carlo integration for these cases, and further give an example where efficient quasi-Monte-Carlo cannot be theoretically performed without knowledge of the underlying tensor network circuit structure.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruojing Peng"
        },
        {
            "affiliations": [],
            "name": "Johnnie Gray"
        },
        {
            "affiliations": [],
            "name": "Kin-Lic Chan"
        }
    ],
    "id": "SP:1839eeab7d61b50b9f36efdf5153ef5d1fb6794c",
    "references": [],
    "sections": [
        {
            "text": "Arithmetic circuit tensor networks, multivariable function representation, and high-dimensional integration\nRuojing Peng,1 Johnnie Gray,1 and Garnet Kin-Lic Chan1 Division of Chemistry and Chemical Engineering, California Institute of Technology, Pasadena, California 91125, USA\nMany computational problems can be formulated in terms of high-dimensional functions. Simple representations of such functions and resulting computations with them typically suffer from the \u201ccurse of dimensionality\u201d, an exponential cost dependence on dimension. Tensor networks provide a way to represent certain classes of high-dimensional functions with polynomial memory. This results in computations where the exponential cost is ameliorated or in some cases, removed, if the tensor network representation can be obtained. Here, we introduce a direct mapping from the arithmetic circuit of a function to arithmetic circuit tensor networks, avoiding the need to perform any optimization or functional fit. We demonstrate the power of the circuit construction in examples of multivariable integration on the unit hypercube in up to 50 dimensions, where the complexity of integration can be understood from the circuit structure. We find very favourable cost scaling compared to quasi-Monte-Carlo integration for these cases, and further give an example where efficient quasi-Monte-Carlo cannot be theoretically performed without knowledge of the underlying tensor network circuit structure.\nI. INTRODUCTION\nHigh-dimensional multivariable functions (henceforth, multivariable functions) and their integrals appear in a multitude of areas, ranging from statistical and quantum many-body physics1\u20133 to applications in machine learning4\u20139. Simple representations of such functions, for example, on a product grid, require exponential storage, and subsequent manipulation of the functions, e.g. in integration by quadrature, then requires exponential cost in dimension. Many techniques have been introduced to bypass this exponential cost. For example, highdimensional integration is often carried out by Monte Carlo or quasi-Monte Carlo methods, which sample the function at a set of random or pre-selected points10\u201313, thereby exchanging the exponential dependence on dimension for weaker guarantees on error.\nIn the many-body physics community, tensor networks (TN) such as matrix product states, projected entangled pair states, and the multiscale entanglement renormalization ansatz, have long been used to represent multivariable physical quantities, such as quantum states14\u201318 or Boltzmann densities19\u201321. Similar techniques (although mainly for more restricted classes of tensor networks, such as the CANDECOMP/PARAFAC decomposition22\u201326, Hierarchical Tucker decomposition26\u201328, and tensor trains28,29) have appeared in the applied mathematics community as well, and have been used for highdimensional function computation and approximation. In both cases, the idea is to represent a (often discretized) high-dimensional function as a connected network of lowdimensional tensors. This can reveal a non-trivial lowrank structure in the function, thereby achieving a great reduction in memory. Because TN are further embodied with a notion of approximation (through singular value decomposition of pairs of tensors) the use of such approximations can ameliorate, and in some cases remove, the exponential cost with respect to dimension, when com-\nputing with the TN. For multivariable function computation with TN, we must first obtain a TN representation of the function. The manner in which such representations is obtained usually involves problem-specific numerical computation. For instance, if the target function can be efficiently evaluated, then determination of the tensor parameters can be formulated as an optimization problem24,25,30\u201335. Another common scenario is when the function is implicitly defined from a minimization, in which case the tensor representation can be optimized by the variational principle36\u201338. When the function satisfies a differential equation with a known initial condition that is easily expressed as a TN, the TN representation can be propagated39\u201344. In all these cases, therefore, the determination of the tensor network representation of the function involves an associated, potentially large, computational cost. In addition, the tensor networks are generally chosen with a fixed network structure ahead of time (for example, a matrix product state or a tensor train) in order to make the determination of the representation feasible, even though such a structure may not be the most compact38,45,46.\nHere, we introduce an alternative way to construct the TN representation of a function from its arithmetic circuit. As both classical arithmetic circuits and quantum arithmetic circuits can be viewed as TNs, it is immediately clear that multivariable functions can be represented as TNs through these circuits. However, such circuits carry various disadvantages; for example, classical logic gates lead to extremely sparse tensors where the number of indices is proportional to the number of bits of precision47,48, while quantum arithmetic circuits are constrained to unitary tensors, and may be sub-optimal for classical calculations49,50. Consequently, we introduce a tensor network circuit representation that takes advantage of both the ability to store floating point numbers as entries in the tensors, as well as the lack of unitary constraints. This leads to a concise construction that avoids ar X\niv :2\n20 9.\n07 41\n0v 1\n[ m\nat h.\nN A\n] 1\n6 A\nug 2\n02 2\n2 the auxiliary computation involved in obtaining the TN representation itself.\nA byproduct of the arithmetic circuit TN form is that the structure of the tensor network is dictated by the circuit, rather than specified beforehand. Consequently, the TN structure that arises can have a general connectivity, and can in principle look quite different from those typically encountered in quantum many-body physics or applied mathematics settings. Recent developments in the exact and approximate contraction of tensor networks with more unstructured geometries are discussed, for example, in Refs. 51\u201355.\nUsing the arithmetic circuit TN representation we carry out high dimensional quadrature over the unit hypercube (in up to 50 dimensions) for multivariable polynomials and multivariable Gaussians. The use of tensor networks in conjunction with high dimensional integration/summation is hardly new; it is one of the main applications of TNs see, for example, Refs. 33, 34, 56, and 57. However, the availability of the circuit structure of the function leads to new insights into this problem. For example, for the multivariable polynomials, we find exact compressibility of the TN circuit in certain limits of the polynomial parameters. This can then be decoded into an exact integration rule (see Ref. 58 for a related result). Furthermore, away from the exact point, we can relate the difficulty of approximation to the degree of nonlinearity (number of copy operations) in the circuit. In terms of practical performance, because Gaussian quadrature weights can be used in each dimension of the TN quadrature, we find that TN integration converges in accuracy orders of magnitudes more quickly than quasi-Monte Carlo integration for instances of the multivariable polynomial and Gaussian integrals. Finally, we finish with an artificial but instructive case where we construct a function (based on the multi-scale entanglement renormalization ansatz59,60) that can be integrated efficiently when using knowledge of its internal TN circuit structure, but for which function evaluation is hard, thus making quasi-MC hard if the function is treated as a blackbox."
        },
        {
            "heading": "II. TENSOR NETWORK ARITHMETIC CIRCUITS",
            "text": ""
        },
        {
            "heading": "A. Function tensor representation and circuit composition of functions",
            "text": "We first introduce a tensor representation of individual single- and multi-variable functions and how to use the representation to compose complicated functions from simpler ones. We start with a single-variable function f(x). Assuming for the time being that x is a continuous variable, we introduce the continuous function tensor (function matrix) representation F , where the function values are stored in the elements Fx\u03b1, specifically,\nFx0 = 1\nFx1 = f(x) (1)\nFIG. 1: Tensor representation F of the scalar function f(x). The \u201ccontrol\u201d index is represented by the thin leg on the bottom.\nWe refer to x as the variable, and \u03b1 as the control index/leg (the dimension of the control index is 2). We consider here scalar-valued functions, but vector-valued functions can be similarly defined. In numerical applications x will typically be discretized, e.g. on a grid with G points. The control index is used to perform arithmetic and other gate operations on the functions.\nIt will be convenient to use the standard graphical notation of tensor networks. We show a diagram of F in Fig. 1. We use the convention that labelled lines represent indexed elements and unlabelled lines between two tensors are summed over for discrete indices/integrated over for continuous indices.\nThe control index can be thought of as a \u201cqubit\u201d index in the tensor product Hilbert space of functions, analogous to the qubit representation used in quantum mechanics; from Eq. 1, we see that |1\u3009 is associated with a basis function f(x), and |0\u3009 is replaced by the scalar 1. Then given a set of functions {fi(xi)}i=0...N\u22121, we represent a monomial of functions as\nF [1]x0\u03b10F [2] x1\u03b11F [N\u22121] xN\u22121\u03b1N\u22121 = f0(x0) \u03b10f1(x1) \u03b11 . . . fN\u22121(xN\u22121) \u03b1N\u22121\n(2)\nA multivariable function c(x0, x1, . . . xN\u22121) in the product Hilbert space \u220f i{1, f(xi)} takes the form\nc(x0, . . . , xN\u22121) = \u2211 {\u03b1i} \u220f i C\u03b10,...,\u03b1N\u22121F [i] xi\u03b1i (3)\nUsing the qubit analogy, C\u03b10,...,\u03b1N\u22121 may be regarded as a wavefunction amplitude in the computational basis. Note that the above constructs the Hilbert space using single-variable functions, but we could also use more complex building blocks, e.g. a two-variable function tensor, Fxy\u03b1 \u2194 f(x, y). We also note that the use of a product Hilbert space to represent multivariable functions has been considered in other contexts, for instance, for length scale separation in the solution of partial differential equations, or for efficient function parameter storage. For relevant discussions, see e.g. Refs. 35, 43, and 44.\nWe use a tensor network to build a circuit representation of the function c(x0, . . . , xN\u22121) using contractions of the function tensors either with themselves, or other fixed tensors. The simplest example corresponds to performing classical arithmetic and logic to combine single-variable functions, using control tensors. The addition tensor (+)\n3 (a) f(x)g(y) (b) f(x) + g(y)\nFIG. 2: Arithmetic circuit tensor network representation of f(x)g(y) and f(x) + g(y).\nis a three index control tensor, with elements\n(+)\u03b1\u03b2\u03b3 = { 1 \u03b1+ \u03b2 = \u03b3 0 otherwise\n(4)\nNote that unlike usual binary arithmetic, the addition is not modulo 2. The multiplication tensor (\u00d7) is also a three index control tensor,\n(\u00d7)\u03b1\u03b2\u03b3 = {\n1 \u03b1 = \u03b2 = \u03b3 0 otherwise\n(5)\nWith such definitions we can perform classical arithmetic on functions. Contraction of two function tensors F , G with the multiplication or addition tensors yields the higher dimensional objects (shown as a tensor network in Fig. 2) corresponding to functions of two variables, i.e.\nFx\u03b1Gy\u03b2(\u00d7)\u03b1\u03b2\u03b3 \u2194 f(x)g(y) Fx\u03b1Gy\u03b2(+)\u03b1\u03b2\u03b3 \u2194 f(x) + g(y)\nwhere we have assumed summation over repeated indices. The above are simple examples of arithmetic circuit tensor networks. Within these arithmetic circuits, the 0 value of the control index is only needed to perform addition; if we know f(x) only enters the circuit via multiplication, we can always choose to fix the control index \u03b1 = 1 of the corresponding tensor F , and thus omit the control leg entirely. Similarly, scalars have no variable dependence and can thus be specified without their variable leg.\nA second circuit structure involves contraction between the variable legs of the function tensors, which corresponds to integrating a common variable between two functions. For example, given Fx\u03b1 \u2194 f(x) and Gk,x \u2194 g(k, x) (we have dropped the control index on G since we are only using it for multiplication), then\nFx\u03b1Gkx \u2194 \u222b dxf(x)g(k, x) (6)\nwhere we have assumed integration over the repeated continuous index on the left.\nMultivariable functions built from the above arithmetic operations will be multilinear in the underlying functions. To build non-linearity, we use multiple functions of the same variable. To do so, we define a tensor\n(a) f(x)g(x) (b) f(x) + g(x)\nFIG. 3: Arithmetic circuit tensor network representation of f(x)g(x) and f(x) + g(x). The circle represents the COPY tensor.\nFIG. 4: Arithmetic circuit tensor network representation of Eq. (8). The CNOT gate is represented by the dashed box.\nCOPY on the continuous variable legs, with entries\n(COPY)xyz = { 1 x = y = z 0 otherwise\n(7)\nThen, using the copy tensor we can multiply or add two functions of the same variable, as shown in Fig. 3, corresponding to the contractions\n(COPY)xyzFy\u03b1Gz\u03b2(\u00d7)\u03b1\u03b2\u03b3 \u2194 f(x)g(x) (COPY)xyzFy\u03b1Gz\u03b2(+)\u03b1\u03b2\u03b3 \u2194 f(x) + g(x).\nNote that the contraction of the copy tensor is an example of a third type of circuit operation, namely, the contraction over the variable legs of functions with additional variable leg tensors; further examples of this kind are discussed in Sec. II B.\nFinally, we briefly mention that the control legs above are contracted with tensors that implement classical arithmetic/logic; contraction generates a single function output. However, one can also apply more general binary operations, such as quantum logic gates where a single set of control inputs may map to multiple control outputs. For example, we can create a circuit using a quantum CNOT gate (see Fig. 4), giving\nFx\u03b1Gy\u03b2(CNOT)\u03b1\u03b2,\u03b3\u03b4(+)\u03b3\u03b4 \u2194 f(x) + f(x)g(y). (8)\nB. Variable circuits, transformations of variables, and other representations\nThe COPY operation defined above generalizes to other TN circuit representations that use operations only on\n4 (a) \u222b dkf\u0303(k)g\u0303(k)e2\u03c0iky (b) \u222b dxf(x)g(y \u2212 x)\nFIG. 5: Representation of the convolution theorem. The red legs represent position space variables x, y, z, and the green legs represent momentum space variable k. F represents the Fourier transform, and F and Z are both variable tensors.\nthe variable legs of the function tensors. This can be viewed as using a different representation of functions as tensors, and we briefly examine the resulting circuits, here called variable TN circuits. In this case, the function y(x) is represented by the function tensor with two continuous indices,\nF y\u2032x = y(x) if y \u2032 = y(x) (9)\nwhere we have used the underline to emphasize that this is a variable TN circuit. One can e.g. perform arithmetic in this representation, using the addition and multiplication tensors\n(+) xyz if z = x+ y\n(\u00d7) xyz if z = xy (10)\nIn principle, one can represent all operators and all functions from circuits built up this way, however, in a discrete computation, the continuous variables must be discretized in some way. Then, because the precision of the result is stored in the tensor index, rather than the value of a tensor element, the number of bits of precision is limited by the size of the tensor indices.\nIn a circuit that contains both function tensors and variable tensors, the variable part of circuit can be seen as performing a transformation or mapping of variables before entering the function representation. For example, consider the convolution \u222b dxf(x)g(y \u2212 x). This can be written as the combination of a function and a variable circuit, as the network\nFx\u03b1Gz\u03b2(\u00d7)\u03b1\u03b2\u03b3Zxyz (11)\nwith\nZxyz = { 1 z = y \u2212 x 0 otherwise .\nThe convolution theorem \u222b dkf\u0303(k)g\u0303(k)e2\u03c0iky =\u222b\ndxf(x)g(y \u2212 x) can then be expressed as the diagram shown in Fig. 5.\n(a) (b) (c) (d)"
        },
        {
            "heading": "C. Arithmetic circuit tensor networks and integration",
            "text": "The composition of the above elements clearly allows us to construct a general multivariable function by operations on function tensors, or on variable tensors. This yields the arithmetic circuit tensor network representation of the multivariable function. A simple example for the function \u220f3 i=1(fi(x) + gi(y)) is shown in Fig. 6a.\nGiven the TN representation, it is trivial to define integration over the input variables. Assuming a product quadrature for each variable x0, . . . xN\u22121, then each continuous variable is discretized xi[p] \u2194 xi, where [p] denotes the p-th grid point. One can also introduce quadrature weights w[p], giving\nF \u222b \u03b1 = \u2211 p wpFp\u03b1 \u2194 \u222b dxf(x) (12)\nas shown in Figs. 6b, 6c."
        },
        {
            "heading": "D. Contracting the arithmetic circuit",
            "text": "To obtain the value of the function from the arithmetic circuit one needs to contract the tensors. Assuming a scalar output, then\nF\u03b1 = \u2211 {e}/\u03b1 \u220f i f [i] {e} (13)\nwhere e are all the contracted indices, which includes both variable and logical indices in the general case.\nThere are many techniques for contracting tensor networks. Here we will view exact and approximate tensor\n5 network contraction as mainly black box algorithms, thus we do not discuss the details and only give a brief idea of the fundamentals. Further details of techniques for exact contraction are described in Ref. 51, 61\u201369. Additional information on approximate contraction can be found in Refs. 14\u201318, 38, 39, 42, 52\u201355, 68, 70, and 71.\nAlthough a typical arithmetic circuit has a flow from input values to output values, the fact that the arithmetic circuit TN encodes all output values for all inputs simultaneously, removes the directionality of the circuit. In particular, the contractions in the circuit can be evaluated in any order (see Figs. 6b, 6c), interchanging the order of summations and products. This can lead to drastic changes in the complexity of evaluation. Heuristic techniques exist to search and find good orders for exact contraction which have been applied to tensor network and quantum circuit tensor network contraction problems (see Ref. 51, 72\u201378).\nTensor networks without loops (i.e. trees) can be contracted exactly with a cost linear in the number of tensors. For arbitrary connectivity, e.g. with loops, the exact tensor network contraction scales exponentially with the number of tensors. However, tensor networks can also be contracted approximately with an approximation error. Such approximate contraction is widely used in tensor network applications in the simulation of quantum systems, and is closely related to low-rank matrix factorization79\u201381. During the contraction of the tensor network, tensors will be generated which share an increased number of legs with neighbouring tensors (see Fig. 6b, 6c); denote the combined dimension of the legs D. In approximate contraction, we use projectors of dimension D\u00d7 \u03c7 to project the shared leg of dimension D down to a shared leg of dimension \u03c7, thus controlling the cost of further operations. In practice, these isometric matrices are usually determined from an singular value decomposition (SVD).\nA simple example of a compression algorithm is the \u201cboundary\u201d compression algorithm for a regular 2D tensor network. Here, rows of tensors (so-called matrix product states and matrix product operators) are contracted together, to form matrix product states with shared bonds; then a series of SVD\u2019s are applied to reduce the bonds to dimension \u03c7 (see Refs. 14\u201318, 38, 39, 42, 55, 68, and 71). We will deploy the boundary contraction algorithm as the approximation contraction algorithm below. However, just as for exact contraction, the choice of order of when to contract and compress can greatly affect cost and accuracy, and similar to exact contraction, there are heuristics to choose an optimized order of contraction and compression. In this work, we do not encounter sufficiently complicated network structures to use these more sophisticated strategies, but the interested reader is referred to Ref. 55.\nApproximate contraction is critical for applications of arithmetic tensor networks, because it allows subclasses of arithmetic tensor networks to be executed for less than the brute force exponential cost in the number of tensors.\nThis is useful in defining classes of computational problems where the curse of dimensionality is circumvented, as we now examine in our application to multidimensional integration below."
        },
        {
            "heading": "III. APPLICATIONS TO INTEGRATION",
            "text": "We now apply the arithmetic tensor network formalism to the problem of multivariable integration. We start with some basic intuition about complexity, then proceed to progressively more complicated examples of multivariable polynomial and multidimensional Gaussian integration in the hypercube, illustrating the power of the method versus another high dimensional technique, namely quasi-Monte Carlo integration. We finish with a specific circuit function with a complexity-theoretic guarantee of hardness with respect to sampling its values (and thus integration by direct application of quasiMonte Carlo methods) but which can be efficiently integrated if one uses its tensor network structure.\nA. Intuition regarding complexity\nIt is well known that multi-variable functions admitting a separation of variables are easy to integrate over a separable range. The simplest examples are\u222b\n\u21262 dxdyf(x)g(y) = (\u222b \u2126 dxf(x) )(\u222b \u2126 dyg(y) ) (14)\u222b\n\u21262 dxdy (f(x) + g(y)) = \u2126 \u222b \u2126 dxf(x) + \u2126 \u222b \u2126 dyg(y)\n(15)\nwhere \u2126 is the integration range in each variable. In TN language, the separability in the above equations corresponds to the tree structure in the TN diagrams shown in Fig. 2. As discussed, such loop-free tensor networks are easy to contract. In the case of integration, one integrates over the variable legs (numerically, one sums over the discrete variable index with grid weights as in Eq. (12)) and then one repeatedly contracts child tensors into their parents, never creating any shared legs. Note that loop free structures constitute a larger class of functions than separable functions. For example, assuming all operations are adds and multiplies, then the following function\n((f1(x1)f2(x2) + f3(x3))f4(x4) + f5(x5))f6(x6) . . . (16)\nwhere the nested parentheses reflects a binary tree structure, is easily integrated.\nIn contrast, the following multivariable integration, illustrated in Fig. 6d,\u222b\n\u21262 dxdy(f1(x) + g1(y))(f2(x) + g2(y))(f3(x) + g3(y))\n(17)\n6 does not generate a loop-free arithmetic tensor network. In the corresponding TN diagram the COPY tensor for each variable results in the red and green loops, arising from a non-linear dependence of the function on a variable. The contraction of tensors that are part of 2 loops with their neighbors can lead to tensors with more legs/larger size. Similarly the use of quantum gates can result in loops. Thus in our arithmetic TN circuits the use of COPY tensors and quantum gates make the resulting tensor networks increasingly hard to contract, and the resulting functions harder to integrate."
        },
        {
            "heading": "IV. MULTIVARIABLE POLYNOMIAL OF FUNCTIONS",
            "text": "As an instructive example, consider the integral\nZ = \u222b \u2126N dx1 . . . dxNf(x1, . . . xN ) (18)\nwith f a polynomial of the form\nf(x1, ..., xN ) = k\u220f i=1 pi(x1, ..., xN ) (19)\nwith\npi(x1, ..., xN ) = N\u2211 j=1 qji(xj) (20)\nand where qji(xj) are single-variable functions. This functional form was considered in Ref. 82, which showed that integration over the domain [0, 1]N is NP-hard for arbitrary functions qji. We first construct an arithmetic circuit TN representation and then proceed to investigate the complexity of integration for different choices of the functions qji(xj)."
        },
        {
            "heading": "A. Arithmetic tensor network representation",
            "text": "Since the total function is explicitly a product of factors, it is natural to construct a representation for each factor and then multiply them together. Consider a single factor pi(x1, . . . , xN ). This is a sum of terms, and we can use a circuit (shown in Fig. 7a) to implement Eq. (20), where the horizontal bonds of dimension 2 are a manifestation of the low rank structure of p(x1, . . . , xN ). The arithmetic circuit for the full tensor network is shown in Fig. 7b, where the TN for each factor pi(x1, . . . , xN ) becomes a row (with index i) and the set of qji connected by COPY tensors for a given variable xj becomes a column (indexed by j). The COPY tensors make the entire network loopy; as each variable is copied k times, k is a measure of the loopiness of the network. If we first contract the tensors in each dashed box, the resulting circuit has a regular 2D structure, known as a projected entangled pair state (PEPS)14 structure.\n(a) pi(x1, . . . , xN ) (b) f(x1, . . . , xN )\nFIG. 7: Representation of pi(x1, . . . , xN ) and f(x1, . . . , xN ) for N = 3. As discussed in section II, we have omitted the open control leg for each pi(x1, . . . , xN ) since they enter f(x1, . . . , xN ) only through multiplication.\n(a) (b) (c)\nFIG. 8: Boundary row compression of a PEPS with 3 rows, 5 columns. (a) shows the PEPS, where each tensor Ti,j (i/j index row/columns) corresponds to a dashed box in Fig. 7b. (b) For each column j, contract T1,j with T2,j . (c) For each pair of tensors T2,j , T2,j+1 connected by multi-bonds, compress the multi-bonds to a maximum bond dimension \u03c7 (red)."
        },
        {
            "heading": "B. Exact compressibility and an identity",
            "text": "We first consider the case where each factor is identical, i.e. pi(x1, . . . , xN ) = p(x1, . . . , xN ). Then the total function takes the form\nf(x1, ..., xN ) = (q1(x1) + ...+ qN (xN )) k, (21)\nTo perform the integration, we first discretize each variable xi on a grid of G points, and allow each single variable function on the grid to take random values between \u22121, 1 (thus each qi represents a discretized version of a function that is oscillating between \u22121 and 1). We use an equally weighted quadrature.\nThe contraction of the 2D tensor network corresponds to the contraction of the PEPS, thus we use an approximate tensor network contraction strategy commonly used in PEPS, compressing to a finite bond dimension \u03c7. Within the PEPS structure, the horizontal bonds have dimension 2, while the vertical bonds associated with the grid points have dimension G. For G 2, the cost of contracting along the vertical dimension is much cheaper than contracting along the horizontal dimension. Thus we perform the contraction by a boundary row contraction method, contracting rows into rows (see Fig. 8).\nAfter a row is contracted into a row, the dimension of the tensors along the boundary increases; the maximum bond dimension of the boundary tensors after k rows is 2k. However, when performing compression, we immediately find that the boundary tensors are exactly compressible to \u03c7 < 2k after each row contraction. In fact, the entire tensor network can be exactly contracted with bond dimension \u03c7 = k, as illustrated in Fig. 9a, which shows the error in the computed integral (due to compressing to finite bond dimension \u03c7); we see the error of contraction drops to 0 for \u03c7 = k. Overall this means that the integral can then be computed exactly with cost linear in the number of variables N , and polynomial in the function nonlinearity k.\nThe ability to exactly contract the network with small \u03c7 implies the existence of an exact algebraic identity. Each compression corresponds to the insertion of isometries/projectors into the 2D TN. For each row i = 2, . . . , k and column j = 1, . . . , N , we insert left and right projectors PL[i, j] (of dimension i\u00d7 2\u00d7 (i+ 1)) and PR[i, j] (of dimension (i+1)\u00d72\u00d7 i) between row i\u22121, i and column j \u2212 1, j where\n(PL[i, j])ab,c = { 1 a = c, b = 0 1 a = j \u2212 1, c = j, b = 1 0 otherwise\n(22)\n(PR[i, j])c,ab = { 1 a+ b = c 0 otherwise . (23)\nThis is shown step by step in Figs. 9b-9e. For example, in Fig. 9b, we insert a pair of projectors PL[2, 1], PR[2, 1] where PL[2, 1]ab,c takes two input vectors (the a, b indices) [1, q1], [1, q1] and maps them to an output vector (the c index) [1, q1, q 2 1 ]; PR[2, 1]c,ab takes the input vector (c index) [1, q1, q 2 1 ] and maps it to two output vectors (a, b indices) [1, q1], [1, q1]. Similarly, in Fig. 9c, we insert the pair of projectors PL[2, 2], PR[2, 2], where PL[2, 2] takes\ntwo input vectors [1, q1 + q2], [1, q1 + q2] and maps them to an output vector [1, q1 + q2, (q1 + q2)\n2]; PR[2, 2] takes the input vector [1, q1 + q2, (q1 + q2)\n2] and maps it to two output vectors [1, q1 + q2], [1, q1 + q2]. In general,\nPL[i, j] :  1 sj ... si\u22121j \u2297 [ 1sj ] \u2192  1 sj ... sij  (24)\nPR[i, j] :  1 sj ... sij \u2192  1 sj ... si\u22121j \u2297 [ 1sj ] . (25)\nwhere sj = \u2211j j\u2032=1 qj\u2032 is the sum of single variable functions up to the j-th term. Thus, we see the role of the left projector is to retain only the non-redundant polynomials of the single variable functions qj , and the right projector redistributes non-redundant powers into a direct product.\nThe tensor network exact compression then corresponds to computing the integral through a recursive formula (see Ref. 58 for a related constructive approach). For example, we can recursively define a set of integrals over a subset of variables\nIk11 = \u222b dx1q1(x1) k1 (26)\nIk22 = \u222b dx1dx2(q1(x1) + q2(x2)) k2\n= \u222b dx1dx2 k2\u2211 k1=0 ( k2 k1 ) q1(x1) k1q2(x2) k2\u2212k1\n= k2\u2211 k1=0 ( k2 k1 ) Ik11 \u222b dx2q2(x2) k2\u2212k1 (27)\n8 Ik33 = \u222b dx1dx2dx3(q1(x1) + q2(x2) + q3(x3)) k3\n= \u222b dx1dx2dx3 [ k3\u2211 k2=0 ( k3 k2 ) (q1(x1) + q2(x2)) k2q3(x3) k3\u2212k2\n] =\nk3\u2211 k2=0 Ik22 \u222b dx3q3(x3) k3\u2212k2 (28)\nand so on up to\nIkN = \u222b dx1...dxN (q1(x1) + ...+ qN (xN )) k\n= \u222b dx1...dxN [ k\u2211 kN\u22121=0 ( k kN\u22121 )\n(q1(x1) + ...+ qN\u22121(xN\u22121)) kN\u22121qN (xN )\nk\u2212kN\u22121 ]\n= k\u2211 kN\u22121=0 ( k kN\u22121 ) I kN\u22121 N\u22121 \u222b dxNqN (xN ) k\u2212kN\u22121 (29)\nwhere each iterative step involves integration of single variable integrals. The quantities I kj j can be related to the TN with the projectors inserted in each layer. Fig. 9e shows a simple example for N = 3, k = 3. From left to right, the open legs of the first two dashed boxes correspond to Ik11 , I k2 2 respectively for k1, k2 = 0, . . . , 3, and the full contraction of the TN corresponds to Ik3 for k = 3."
        },
        {
            "heading": "C. Perturbations away from exact compressibility",
            "text": "In the above, the compression of the circuit tensor network for the multivariable polynomial function allows us to identify an efficiently integrable case. However, when inserting more general polynomials in Eq. (19), we cannot expect the tensor network to be exactly compressible since we know the general case is NP hard. However, we might expect that functions that are close to the efficiently integrable case remain efficiently integrable up to some accuracy.\nWe thus now consider polynomial functions in Eq. (19) that are obtained by perturbing away from the exactly compressible case. We do this by defining\nqji(xj) = qj1(xj) + \u03b4 \u00b7 rji(xj) (30)\nwhere the single variable functions qj1(xj), rji(xj) are random length-G vectors with values in [-1,1] (recall, G is the number of grid points). For i = 1, we set \u03b4 = 0 for j = 1, . . . , N . Then for i > 1, the difference between qji(xj) and qj1(xj) is controlled by \u03b4. In Fig. 10 we use perturbations of magnitude \u03b4 = 0.01 and \u03b4 = 0.1 for N = 20, G = 10, and we monitor the error in the multivariable integral as a function of \u03c7. In both cases we\n(a) \u03b4 = 0.01\n(b) \u03b4 = 0.1\nFIG. 10: Accuracy of TN integration for the integral of a multivariable function polynomial perturbed away from the exactly compressible point as a function of contraction bond dimension \u03c7. Magnitude of perturbation \u03b4, number of variables, N = 20, number of points per variable, G = 10. k is a measure of the nonlinearity expressed in the TN. Note that there is fast convergence with \u03c7 up to a critical precision, which depends on \u03b4 and k. See Sec. IV C for details.\nsee that the integration error shows two regions of convergence; first there is a rapid convergence to some finite error (around 10\u221210 for \u03b4 = 0.01, and 10\u22124 for \u03b4 = 0.1), followed by a much slower convergence to smaller error. In the rapidly converging regime, the required bond dimension for a given relative error grows as \u03c7 \u223c O(k); in the slow convergence regime, the required bond dimension \u03c7 \u223c exp(k). In all cases, however, the cost to integrate is linear in the number of variables N , and this is independent of the desired precision (not shown for this case, but see next section). Exponential dependence of the complexity appears instead in the cost to improve the precision past a critical threshold."
        },
        {
            "heading": "D. General case",
            "text": "For the general polynomial in Eq. (19), we can expect the bond dimension for a given relative precision to scale \u223c exp(k) as discussed; the tensor network is, in some sense, incompressible. However, in some cases, we may still be able to approximate the integral to good accuracy. This depends strongly on the integrand range, as illustrated in Fig. 11. In Fig. 11a, we take N = 20, G = 10 and single variable function values chosen randomly in the range [\u22121, 1]. Then, the corresponding 2D TN representation of the integral appears almost completely incompressible: only until \u03c7 reaches the bond dimension of the exact contraction \u223c exp(k) do we suddenly see a significant improvement in the integral error (although for rough estimates, e.g. to 10\u22121 precision, it is possible to take \u03c7 orders of magnitude below that required for exact contraction). We can make the single variable functions less oscillatory by increasing the lower bound of the range of the single variable functions, i.e. [\u03bb, 1], where \u03bb is increased from \u22121 to 0. As we do so, the integration problems become easier, as can be seen from Figs. 11a, 11b, 11c. The decrease of relative error with\nincreasing bond dimension is much faster as we raise the lower bound \u03bb of the the single variable functions.\nMonte Carlo integration can face difficulties with oscillatory functions with small or vanishing integrals. In the circuit tensor network representation, there is an exponential dependence for such oscillatory functions, but unlike in quadrature, the exponential cost is in the nonlinearity (parameterized by k), not in the number of variables N . Fig. 11d plots the required bond dimension for a relative error 10\u22124 with N = 20, G = 10 as a function of the non-linearity k for various single variable function ranges qji \u2208 [\u03bb, 1], where \u03bb is increased from \u22121 . . . 0. We see that the required bond dimension increases exponentially in the non-linearity. However, the exponent is much larger for oscillatory integrands (\u03bb = \u22121) than for more positive integrands \u03bb > \u22121.\nWe finally confirm that the error of compressed contraction is essentially independent of the number of variables using boundary contraction as can be seen from Figs. 11e, 11f. Either for the general functional form Eq. (19) with highly oscillatory values from random single variable functions, or for the perturbation from the exact case Eq. (30) with \u03b4 = 0.1, the rate of decrease of relative error with increasing bond dimension is compa-\n10\nrable as we change the number of variables N ."
        },
        {
            "heading": "E. Comparison with quasi-Monte Carlo",
            "text": "To understand the concrete performance of integration using the arithmetic circuit TN, we now compare costs with that of quasi-MC. For this, we construct the polynomial in Eq. (19) with the following multivariable function\nqji(xj) = sin (2\u03c0(xj + aji)) + c (31)\nfor random aji \u2208 [0, 1] and some constant c, and integrate over xj \u2208 [0, 1]. We compare the results from TN contraction to quasi-MC integration in Fig. 12 (we report representative results for random aji) changing both the number of variables N as well as the polynomial power (nonlinearity) k. All calculation are done on Intel(R) Xeon(R) CPU E5-2697 v4 processors. For the quasiMC calculations, the integrand in Eq. (19) was coded in Python then compiled by JAX83, and the sample points were generated using the QMCPY84 package with Sobol generating matrices using a batch size of 107, with the function values for each batch computed on a single core. The TN timings are reported as the runtime for contraction using the Quimb85 package on a single core or two cores, with the time normalized to a single core time. The reference exact result for the integration was taken as the converged TN result w.r.t \u03c7 and G (specified in the caption of Fig. 12).\nFig. 12a plots the convergence of the integral versus time for TN integration and quasi-MC for N = 10, c = 0 and various k. The TN data points represent increasing grid order G using Gauss-Legendre quadrature and bond dimension \u03c7 = 2k. For moderate k, quasi-MC already faces convergence difficulties as a result of the highly oscillatory function values around 0. On the other hand, the TN result converges quickly with G as shown in Fig. 12b. Although details of implementation make it difficult to interpret small differences in absolute timing, the rapid convergence of the TN integration means that it is orders of magnitude more efficient than quasi-MC for high accuracies. However, the TN contraction cost becomes prohibitive at large k, due to using a \u03c7 = 2k bond dimension. For c > 0, the integrand is more positive and we expect the TN to be more compressible, making it possible to integrate the function for larger k. In Fig. 12c, we plot the convergence of the integral versus time for TN integration and quasi-MC for N = 50, k = 30 and various c. The TN data points correspond to G = 12 Gauss-Legendre quadrature for increasing bond dimension \u03c7 \u2208 [60, 560]. (To justify the choice of fixed G = 12, Fig. 12d shows the convergence of the TN contraction result as a function of G for various c at fixed \u03c7 (solid/dashed/dotted lines correspond to \u03c7 = 180/120/60); at G = 12, the quadrature errors are below 10\u22126 for c = 0.3 and below 10\u22128 for c = 0.4 and c = 0.5, and these errors remain essentially constant as \u03c7 increases from 60 to 180). As\nexpected, for a fixed computational time, the integration error decreases for both methods with increasing c. Both methods display relatively quick convergence to a loose threshold, with slower convergence to tighter thresholds. The TN integration converges relatively smoothly to high accuracy, while it appears difficult to converge quasi-MC systematically to high accuracy, which again opens up a significant timing advantage for TN integration over quasi-MC."
        },
        {
            "heading": "V. MULTIVARIABLE GAUSSIAN INTEGRALS IN A HYPERCUBE",
            "text": "As another example, we consider the multivariable Gaussian integral over a finite hypercube\nZ = \u222b \u2126 dx1...dxN exp (\u2212 \u2211 ij Aijxixj) (32)\nwhere A is aN\u00d7N matrix and \u2126 = [\u22121, 1]N . The expression directly corresponds to a tensor network contraction of tensors (Ti) and (Tij) for all i < j where\n(Ti)xi = exp (\u2212Aiix2i ) (33) (Tij)xi,xj = exp (\u2212(Aij +Aji)xixj). (34)\nNote that all tensors enter into the final function via multiplication only, thus there is no need for control legs as discussed in section II, and they are omitted in the figures.\nThe structure of A plays an important role in the the cost of approximability of the integral. In the following we consider band-diagonal A with width W , i.e. Aij 6= 0 only if |i \u2212 j| <= W (dense A corresponds to W = N \u2212 1). In Fig. 13, we show a systematic construction of the TN representation of the integral for N = 5, W = 3, which can easily be generalized to arbitrary N , W . We start with a TN consisting of only T12 (representing exp (\u2212A12x1x2)) as in Fig. 13a. For each j = 2, . . . , 1 + W , we add to the TN the tensor T1j and a COPY tensor to account for the additional occurrence of x1 as in Figs. 13b, 13c. For example, in Fig. 13c, the 4 open legs represent the variables x1, x2, x3, x4, and the overall tensor network represents exp(\u2212(A12x1x2 +A13x1x3 +A14x1x4)).\nNext, for i = 2, and for each j = i + 1, . . . , i + W , we add to the TN the tensor Tij and two COPY tensors, e.g. in Figs. 13d, 13e, the COPY tensors connected by the horizontal bonds correspond to copying the variable xi, and the COPY tensors connected by the vertical bonds correspond to copying xj for each j = 3, 4, .... Iterating the previous step for each i = 3, . . . , N \u2212 1 as in Fig. 13f, one adds all Tij to the TN. Finally we add Ti to the TN for each i = 1, . . . , N as in Fig. 13g. Contracting the tensors in each dashed box, one obtains a 2D-TN shown in Fig. 13h. The TN obtained from the procedure above clearly has a direct correspondence with the structure of A. If A is dense, then the graph is a triangular network, and if A is banded, then the network is also banded.\n11"
        },
        {
            "heading": "A. Fixed width compressibility",
            "text": "For bandedA (fixedW withN), the TN has a quasi-1D structure along the diagonal direction. Thus it is always possible to contract the network exactly with cost linear in N and exponential in W along the diagonal direction. If W is large, it may still be too expensive to use exact contraction, but the regular structure lends itself to a variant of boundary contraction as shown in Fig. 14.\nIn this case we limit the maximum bond dimension \u03c7 during the approximate contraction, with \u03c7 expected to scale like \u223c eW to achieve a fixed relative error. We show the relative contraction error w.r.t. \u03c7 for various W in Fig. 15a, and the bond dimension to achieve various median contraction error as a function of W in Fig. 15b, for a 30\u00d7 30 band-diagonal matrix A with width W and random nonzero elements in [\u22121, 1]. We see that the\ndifficulty of compression indeed scales exponentially with W from the linear trend in the log-linear plot."
        },
        {
            "heading": "B. Approximately banded case",
            "text": "Above, we demonstrate that for moderate width W , the quasi-1D TN can be exactly contracted. As in the perturbed case for the polynomial example, we can also ask if the TN for an approximately band-diagonal A remains compressible. We thus consider A of the form\nA = AW + \u03b4 \u00b7 A\u0303W (35)\nwhere AW is an N\u00d7N band-diagonal matrix with width W such that (AW )ij 6= 0 only if |i \u2212 j| \u2264 W , A\u0303W is an N \u00d7 N matrix such that (A\u0303W )ij 6= 0 only if |i \u2212 j| > W , and non-zero elements of AW , A\u0303W take random\n12\nvalues in [\u22121, 1]. Then the magnitude of the perturbation of A away from AW is controlled by \u03b4. Note that the existence of the perturbation makes the TN a full 2D triangle, rather than quasi-1D.\nWe show the relative contraction error w.r.t \u03c7 for various \u03b4 in Fig. 16a and the bond dimension required to achieve various median errors as a function of \u03b4 in Fig. 16b for A of dimension 20 \u00d7 20. We require increasingly large \u03c7 to reach the same error as \u03b4 increases. However, Fig. 16b is a linear-linear plot, showing the required \u03c7 appears to grow only linearly with the size of the perturbation \u03b4.\nAccuracy v.s. bond dimension \u03c7 for number of variables N = 30, number of points per variable G = 4. (a) Median and interquartile range of error over 20 random instances of the Gaussian matrix A. (b) Bond dimension to achieve various median contraction errors. For details see Sec. V A."
        },
        {
            "heading": "C. Comparison with quasi-Monte Carlo",
            "text": "We now compare the efficiency of TN integration for Gaussian integrals with banded (or approximately banded) A with that of quasi-Monte Carlo. We consider two cases for A of the form Eq. (35) (a) N = 50, W = 5,\n\u03b4 = 0.1 with random nonzero elements of AW , A\u0303W in [\u22121, 1] and case (b) N = 50, W = 6/8/10, \u03b4 = 0 with random nonzero elements of AW in [\u22121, 1].\nIn Fig. 17, we show the relative error of lnZ from TN and quasi-MC integration for cases (a) and (b). For the quasi-MC results, we directly computed the exponent \u2212 \u2211 ij Aijxixj for each sample point x with batched linear algebra code using Python and JAX83, and accumulated the log of the integral using the identity\nln (ea + eb) = a+ ln (1 + eb\u2212a).\nThe sample points were generated using QMCPY84 with\n13\nNiederreiter generating matrices in batches of size 107, and plotted every 20 batches. The TN integration was performed using Quimb85 with Gauss\u2013Legendre quadrature and the exact value of the integral was estimated from TN integration using converged G and \u03c7 (indicated in the caption of Fig. 17). All calculation are done on on a single Intel(R) Xeon(R) CPU E5-2697 v4 processor.\nFor the TN integration data in Fig. 17a (corresponding to case (a)) each line labeled by G is a sequence of TN estimates of lnZ at the corresponding G with increasing \u03c7 = 60, 80, . . . , 140. For G = 4 and G = 6, the value of lnZ plateaus at large \u03c7, corresponding to the intrinsic quadrature error for that G. For G = 8 and G = 10, the TN results are well converged with quadrature, and the small perturbation strength \u03b4 = 0.1 allows for quick convergence with \u03c7. In comparison, quasi-MC struggles with this integrand, and in fact shows little systematic convergence behavior over the \u223c 1600\u00d7 107 samples.\nFor case (b), shown in Fig. 17b, we plot the TN data\nfor lnZ to better distinguish the curves). N is the number of variables, G is number of quadrature points per variable in the TN. (a) Exact result taken as TN partition function at G = 10, bond dimension \u03c7 = 140 (bond dimension error below 5\u00d7 10\u22127). The TN errors\nare plotted for \u03c7 = 60, 80, 100, 120, 140 for each G, which are the labels next to the TN data points. (b)\nExact result is taken at G = 14 for converged \u03c7 = 80/140/200 for W = 6/8/10 with contraction error \u2264 10\u221215/10\u22129/10\u22126. TN/quasi-MC results are\nrepresented by solid/dotted lines. The label next to each data point corresponds to its G value.\npoints for increasing G = 4, 6, . . . , 12 at converged \u03c7 for each W (solid lines). This allows us to examine the speed of convergence of the quadrature, which is very fast. Similar to case (a) quasi-MC struggles to reduce the relative error below 10\u22122 even with more than 1600\u00d7107 samples. Thus, in both cases, we see a substantial advantage of TN integration over quasi-MC for this class of integrands.\n14"
        },
        {
            "heading": "VI. A THEORETICAL EXAMPLE OF SPEEDUP VERSUS QUASI-MONTE CARLO",
            "text": "Here we give an example of a class of integrals that can be computed easily with arithmetic TN methods but for which quasi-MC is hard, because there is a theoretical guarantee that the integrand value cannot be efficiently evaluated without exponential cost in the number of variables N .\nConsider the general functional form\nf(x1, ..., xN ) = N\u220f n=1 g(xn) in(TN)i1,...,iN (36)\nwhere g(xn) are single variable functions and (TN)i1,...,iN is some fixed tensor (possibly represented as a tensor network) for all in = 0, 1. We can choose the structure and the values of the TNi1,...,iN and properties of g(xn) to allow for the easy computation of certain quantities. In our case, we choose the TN to be a 2D multi-scale entanglement renormalization ansatz59,60 (see Fig. 18). In this case, the TN in Eq. (36) is constructed from tensors satisfying certain unitary and isometric properties such that the evaluation of the trace \u2211 {i} |TNi1,...,iN |2 = 1. The unitary and isometric constraints on the tensors and how they lead to the trivial trace are shown in Figs. 18c, 18d.\nNote that it is known to be classically hard (as a function of N) to compute the value TNi1,...,iN for a MERA composed of arbitrary unitaries and isometries86.\nTo extend these properties to the more general functional form, we impose a normalization condition on the single variable functions\u222b 1\n\u22121 dx|g(x)|2 = 2 (37)\u222b 1\n\u22121 dxg(x) = 0 (38)\nThen, suppose we want to integrate the probability of the function over \u2126 = [\u22121, 1]N\nI = \u222b \u2126 |f({xn})|2 \u220f dxn. (39)\nBecause of the additional constraints we have imposed on the single variable functions, contraction of the tensors associated with I can be done efficiently for matching pairs, which then simplify to multiples of the identity, yielding 2N as the value of the integral, even though sampling in the basis of {xn} is computationally hard. This result is clearly contrived because, in addition to requiring a restricted class of tensors and functions, we must also contract the tensor network in a certain order. Furthermore, the structure of the tensors permits sampling in a different basis than {xn}86,87. Nonetheless, this is a concrete example supported by complexity arguments where using the tensor network structure of the function circuit leads to an exponential improvement in cost versus quasi-Monte Carlo methods which assume the function is a blackbox."
        },
        {
            "heading": "VII. CONCLUSIONS",
            "text": "In this work we introduced an arithmetic circuit tensor network representation of multivariable functions and demonstrated its power for high dimensional integration. Compared to existing techniques to represent functions with tensor networks, the ability to use the arithmetic circuit construction removes the need for extra computation to find the tensor representation, while the circuit structure suggests a tensor network connectivity natural to the function. In our examples of high dimensional integration, we find that the tensor network representation allow us to circumvent the curse of dimensionality in many cases, exchanging the exponential cost dependence on dimension for a cost dependence on other circuit characteristics, for example, the number of nonlinear circuit operations. In practice, we find superior performance to quasi-Monte Carlo integration across a range of dimensionalities and accuracies.\nWhile the work here focused on integration as an example, we envision the arithmetic circuit tensor network construction to be powerful also in differential equation\n15\napplications. Here, connections with existing tensor network techniques are intriguing, as are applications of these ideas to many other areas where tensor networks are currently employed, such as for many-body simulations.\n1V. Popov, J. Niederle, and L. Hlavaty\u0300, Functional Integrals in Quantum Field Theory and Statistical Physics, Mathematical Physics and Applied Mathematics (Springer Netherlands, 2001). 2I. M. Gel\u2019fand and A. M. Yaglom, Journal of Mathematical Physics 1, 48 (1960). 3P. Cartier and C. DeWitt-Morette, Journal of Mathematical Physics 41, 4154 (2000). 4D. D. Lewis and J. Catlett, in Machine Learning Proceedings 1994 , edited by W. W. Cohen and H. Hirsh (Morgan Kaufmann, San Francisco (CA), 1994) pp. 148\u2013156. 5E. Liberty, K. Lang, and K. Shmakov, in Proceedings of The 33rd International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 48, edited by M. F. Balcan and K. Q. Weinberger (PMLR, New York, New York, USA, 2016) pp. 2320\u20132329. 6A. ElRafey and J. Wojtusiak, WIREs Computational Statistics 9, e1414 (2017). 7C. A. Ramezan, T. A. Warner, and A. E. Maxwell, Remote Sensing 11 (2019), 10.3390/rs11020185. 8S. Tyagi and S. Mittal, in Proceedings of ICRIC 2019, edited by P. K. Singh, A. K. Kar, Y. Singh, M. H. Kolekar, and S. Tanwar (Springer International Publishing, Cham, 2020) pp. 209\u2013221. 9C. Kaltenecker, A. Grebhahn, N. Siegmund, and S. Apel, IEEE Software 37, 58 (2020). 10W. J. Morokoff and R. E. Caflisch, SIAM Journal on Scientific Computing 15, 1251 (1994). 11B. Tuffin, Monte Carlo Methods and Application 2, 295 (1996). 12F. James, J. Hoogland, and R. Kleiss, Computer Physics Com-\nmunications 99, 180 (1997). 13L. Kocis and W. J. Whiten, ACM Trans. Math. Softw. 23,\n266\u2013294 (1997). 14R. Oru\u0301s, Annals of Physics 349, 117 (2014). 15F. Verstraete, V. Murg, and J. Cirac, Advances in Physics 57,\n143 (2008). 16P. Silvi, F. Tschirsich, M. Gerster, J. Ju\u0308nemann, D. Jaschke,\nM. Rizzi, and S. Montangero, SciPost Phys. Lect. Notes , 8 (2019). 17R. Oru\u0301s, Nature Reviews Physics 1, 538 (2019). 18T. Felser, S. Notarnicola, and S. Montangero, Phys. Rev. Lett. 126, 170603 (2021). 19J. Chen, S. Cheng, H. Xie, L. Wang, and T. Xiang, Phys. Rev. B 97, 085104 (2018). 20L. Pastori, R. Kaubruegger, and J. C. Budich, Phys. Rev. B 99, 165123 (2019). 21S. Li, F. Pan, P. Zhou, and P. Zhang, Phys. Rev. B 104, 075154 (2021). 22R. Harshman, Foundations of the PARAFAC Procedure: Models and Conditions for an \u201dexplanatory\u201d Multi-modal Factor Analysis, UCLA working papers in phonetics (University of California at Los Angeles, 1970). 23J. D. Carroll and J.-J. Chang, Psychometrika 35, 283 (1970). 24G. Beylkin and M. J. Mohlenkamp, Proceedings of the National\nAcademy of Sciences 99, 10246 (2002). 25G. Beylkin and M. J. Mohlenkamp, SIAM Journal on Scientific\nComputing 26, 2133 (2005). 26T. G. Kolda and B. W. Bader, SIAM Review 51, 455 (2009). 27L. Grasedyck, SIAM Journal on Matrix Analysis and Applica-\ntions 31, 2029 (2010). 28L. Grasedyck and W. Hackbusch, Computational Methods in Ap-\nplied Mathematics 11, 291 (2011). 29I. V. Oseledets, SIAM Journal on Scientific Computing 33, 2295\n(2011). 30S. Holtz, T. Rohwedder, and R. Schneider, SIAM Journal on\nScientific Computing 34, A683 (2012).\n31M. Espig, W. Hackbusch, S. Handschuh, and R. Schneider, Computing and Visualization in Science 14, 271 (2011). 32A. Novikov, M. Trofimov, and I. Oseledets, \u201cExponential machines,\u201d (2016). 33I. Oseledets and E. Tyrtyshnikov, Linear Algebra and its Applications 432, 70 (2010). 34S. Dolgov and D. Savostyanov, Computer Physics Communications 246, 106869 (2020). 35J. J. Garc\u0301\u0131a-Ripoll, Quantum 5, 431 (2021). 36G. Evenbly and R. N. C. Pfeifer, Phys. Rev. B 89, 245118 (2014). 37L. Vanderstraeten, J. Haegeman, P. Corboz, and F. Verstraete,\nPhys. Rev. B 94, 155123 (2016). 38R. Haghshenas, J. Gray, A. C. Potter, and G. K.-L. Chan, Phys.\nRev. X 12, 011047 (2022). 39A. J. Daley, C. Kollath, U. Schollwo\u0308ck, and G. Vidal, Journal\nof Statistical Mechanics: Theory and Experiment 2004, P04005 (2004). 40S. Iblisdir, R. Oru\u0301s, and J. I. Latorre, Phys. Rev. B 75, 104305 (2007). 41H. N. Phien, I. P. McCulloch, and G. Vidal, Phys. Rev. B 91, 115137 (2015). 42S.-H. Lin, R. Dilip, A. G. Green, A. Smith, and F. Pollmann, PRX Quantum 2, 010342 (2021). 43M. Lubasch, P. Moinier, and D. Jaksch, Journal of Computational Physics 372, 587 (2018). 44N. Gourianov, M. Lubasch, S. Dolgov, Q. Y. van den Berg, H. Babaee, P. Givi, M. Kiffner, and D. Jaksch, Nature Computational Science 2, 30 (2022). 45I. Glasser, R. Sweke, N. Pancotti, J. Eisert, and J. I. Cirac, in Proceedings of the 33rd International Conference on Neural Information Processing Systems (Curran Associates Inc., Red Hook, NY, USA, 2019) p. 1498\u20131510. 46Y. Levine, O. Sharir, N. Cohen, and A. Shashua, Phys. Rev. Lett. 122, 065301 (2019). 47A. K. Verma, P. Brisk, and P. Ienne, in Proceedings of the Conference on Design, Automation and Test in Europe, DATE \u201908 (Association for Computing Machinery, New York, NY, USA, 2008) p. 1250\u20131255. 48M. Ciesielski, T. Su, A. Yasin, and C. Yu, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 1346 (2020). 49N. Wiebe and M. Roetteler, \u201cQuantum arithmetic and numerical analysis using repeat-until-success circuits,\u201d (2014). 50T. Haener, M. Soeken, M. Roetteler, and K. M. Svore, in Reversible Computation, edited by J. Kari and I. Ulidowski (Springer International Publishing, Cham, 2018) pp. 162\u2013174. 51J. Gray and S. Kourtis, Quantum 5, 410 (2021). 52F. Pan, P. Zhou, S. Li, and P. Zhang, Phys. Rev. Lett. 125,\n060503 (2020). 53A. S. Jermyn, SciPost Phys. 8, 5 (2020). 54C. T. Chubb, \u201cGeneral tensor network decoding of 2d pauli\ncodes,\u201d (2021). 55J. Gray and G. K.-L. Chan, \u201cHyper-optimized compressed con-\ntraction of tensor networks with arbitrary geometry,\u201d (2022). 56L. Vysotsky, A. Smirnov, and E. Tyrtyshnikov, Lobachevskii\nJournal of Mathematics 42, 1608 (2021). 57Y. Nunez-Fernandez, M. Jeannin, P. T. Dumitrescu, T. Kloss,\nJ. Kaye, O. Parcollet, and X. Waintal, \u201cLearning feynman diagrams with tensor trains,\u201d (2022). 58I. Oseledets, Constructive Approximation 37, 1 (2013). 59G. Evenbly and G. Vidal, Phys. Rev. B 79, 144108 (2009). 60G. Evenbly and G. Vidal, Journal of Statistical Physics 157, 931\n(2014). 61S. Bravyi, \u201cContraction of matchgate tensor networks on non-\nplanar graphs,\u201d (2008). 62E. S. Fried, N. P. D. Sawaya, Y. Cao, I. D. Kivlichan, J. Romero,\nand A. Aspuru-Guzik, PLOS ONE 13, 1 (2018). 63M. Levin and C. P. Nave, Phys. Rev. Lett. 99, 120601 (2007). 64G. Evenbly and G. Vidal, Phys. Rev. Lett. 115, 180405 (2015). 65G. Evenbly, Phys. Rev. B 95, 045117 (2017).\n16\n66M. Bal, M. Marie\u0308n, J. Haegeman, and F. Verstraete, Phys. Rev. Lett. 118, 250602 (2017). 67S. Yang, Z.-C. Gu, and X.-G. Wen, Phys. Rev. Lett. 118, 110504 (2017). 68S.-J. Ran, E. Tirrito, C. Peng, X. Chen, L. Tagliacozzo, G. Su, and M. Lewenstein, Tensor Network Contractions, Lecture Notes in Physics (Springer Cham, 2020). 69F. Schindler and A. S. Jermyn, Machine Learning: Science and Technology 1, 035001 (2020). 70S. J. Denny, J. D. Biamonte, D. Jaksch, and S. R. Clark, Journal of Physics A: Mathematical and Theoretical 45, 015309 (2011). 71P. C. G. Vlaar and P. Corboz, Phys. Rev. B 103, 205137 (2021). 72V. Gogate and R. Dechter, \u201cA complete anytime algorithm for\ntreewidth,\u201d (2012). 73R. N. C. Pfeifer, J. Haegeman, and F. Verstraete, Phys. Rev. E 90, 033315 (2014). 74B. Strasser, \u201cComputing tree decompositions with flowcutter: Pace 2017 submission,\u201d (2017). 75Y. Akhremtsev, T. Heuer, P. Sanders, and S. Schlag, in 2017 Proceedings of the Meeting on Algorithm Engineering and Experiments (ALENEX) (2017) pp. 28\u201342. 76S. Boixo, S. V. Isakov, V. N. Smelyanskiy, and H. Neven, \u201cSimulation of low-depth quantum circuits as complex undirected graphical models,\u201d (2017).\n77S. Kourtis, C. Chamon, E. R. Mucciolo, and A. E. Ruckenstein, SciPost Phys. 7, 60 (2019). 78J. M. Dudek, L. Duen\u0303as-Osorio, and M. Y. Vardi, \u201cEfficient contraction of large tensor networks for weighted model counting through graph decompositions,\u201d (2019). 79L. Grasedyck, D. Kressner, and C. Tobler, \u201cA literature survey of low-rank tensor approximation techniques,\u201d (2013). 80N. K. Kumar and J. Schneider, Linear and Multilinear Algebra 65, 2212 (2017). 81B. N. Khoromskij, Chemometrics and Intelligent Laboratory Systems 110, 1 (2012). 82B. Fu, in Frontiers in Algorithmics and Algorithmic Aspects in Information and Management, edited by J. Snoeyink, P. Lu, K. Su, and L. Wang (Springer Berlin Heidelberg, Berlin, Heidelberg, 2012) pp. 182\u2013191. 83J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, \u201cJAX: composable transformations of Python+NumPy programs,\u201d (2018). 84S.-C. T. Choi, F. J. Hickernell, M. McCourt, and A. Sorokin, \u201cQMCPy: A quasi-Monte Carlo Python library,\u201d (2020+). 85J. Gray, Journal of Open Source Software 3, 819 (2018). 86A. J. Ferris and G. Vidal, Phys. Rev. B 85, 165147 (2012). 87A. J. Ferris and G. Vidal, Phys. Rev. B 85, 165146 (2012)."
        }
    ],
    "title": "Arithmetic circuit tensor networks, multivariable function representation, and high-dimensional integration",
    "year": 2022
}