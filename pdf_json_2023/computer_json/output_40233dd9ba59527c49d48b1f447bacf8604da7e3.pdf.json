{
    "abstractText": "Temporal action detection is a very important and challenging task in the field of video understanding, especially for datasets with significant differences in action duration. The temporal relationships between the action instances contained in these datasets are very complex. For such videos, it is necessary to capture information with a richer temporal distribution as much as possible. In this paper, we propose a dual-stream model that can model contextual information at multiple temporal scales. First, the input video is divided into two resolution streams, followed by a Multi-Resolution Context Aggregation module to capture multi-scale temporal information. Additionally, an Information Enhancement module is added after the high-resolution input stream to model both long-range and short-range contexts. Finally, the outputs of the two modules are merged to obtain features with rich temporal information for action localization and classification. We conducted experiments on three datasets to evaluate the proposed approach. On ActivityNet-v1.3, an average mAP (mean Average Precision) of 32.83% was obtained. On Charades, the best performance was obtained, with an average mAP of 27.3%. On TSU (Toyota Smarthome Untrimmed), an average mAP of 33.1% was achieved.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haiping Zhang"
        },
        {
            "affiliations": [],
            "name": "Fuxing Zhou"
        },
        {
            "affiliations": [],
            "name": "Conghao Ma"
        },
        {
            "affiliations": [],
            "name": "Dongjing Wang"
        },
        {
            "affiliations": [],
            "name": "Wanjun Zhang"
        }
    ],
    "id": "SP:a1f461cfeb948b7841003e33f5384604aafdb9e5",
    "references": [
        {
            "authors": [
                "Y. Zhao",
                "Y. Xiong",
                "L. Wang",
                "Z. Wu",
                "X. Tang",
                "D. Lin"
            ],
            "title": "Temporal action detection with structured segment networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Shou",
                "D. Wang",
                "S.F. Chang"
            ],
            "title": "Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs",
            "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "Z. Shou",
                "J. Chan",
                "A. Zareian",
                "K. Miyazawa",
                "S.F. Chang"
            ],
            "title": "CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "H. Xu",
                "A. Das",
                "K. Saenko"
            ],
            "title": "R-c3d: Region convolutional 3d network for temporal activity detection",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "X. Dai",
                "B. Singh",
                "G. Zhang",
                "L.S. Davis",
                "Y.Q. Chen"
            ],
            "title": "Temporal Context Network for Activity Localization in Videos",
            "venue": "In Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Sun",
                "Y. Miao",
                "J. Chen",
                "R. Pajarola"
            ],
            "title": "PGCNet: Patch graph convolutional network for point cloud segmentation of indoor scenes",
            "venue": "Vis. Comput",
            "year": 2020
        },
        {
            "authors": [
                "M. Xu",
                "C. Zhao",
                "D.S. Rojas",
                "A. Thabet",
                "B. Ghanem"
            ],
            "title": "G-tad: Sub-graph localization for temporal action detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "R. Zeng",
                "W. Huang",
                "M. Tan",
                "Y. Rong",
                "P. Zhao",
                "J. Huang",
                "C. Gan"
            ],
            "title": "Graph convolutional networks for temporal action localization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16 \u00d7 16 words: Transformers for image recognition at scale",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "L. Wang",
                "H. Yang",
                "W. Wu",
                "H. Yao",
                "H. Huang"
            ],
            "title": "Temporal Action Proposal Generation with Transformers",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "F. Cheng",
                "G. Bertasius"
            ],
            "title": "TALLFormer: Temporal Action Localization with Long-memory Transformer",
            "venue": "In Proceedings of the European Conference on Computer Vision, Tel Aviv, Israel,",
            "year": 2022
        },
        {
            "authors": [
                "S. Li",
                "F. Zhang",
                "R.W. Zhao",
                "R. Feng",
                "K. Yang",
                "L.N. Liu",
                "J. Hou"
            ],
            "title": "Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation",
            "venue": "In Proceedings of the British Machine Vision Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Qing",
                "H. Su",
                "W. Gan",
                "D. Wang",
                "W. Wu",
                "X. Wang",
                "Y. Qiao",
                "J. Yan",
                "C. Gao",
                "N. Sang"
            ],
            "title": "Temporal Context Aggregation Network for Temporal Action Proposal Refinement",
            "venue": "In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Y. Weng",
                "Z. Pan",
                "M. Han",
                "X. Chang",
                "B. Zhuang"
            ],
            "title": "An efficient spatio-temporal pyramid transformer for action detection",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2022: 17th European Conference, Tel Aviv, Israel,",
            "year": 2022
        },
        {
            "authors": [
                "H. Wang",
                "C. Schmid"
            ],
            "title": "Action recognition with improved trajectories",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Sydney, Australia,",
            "year": 2013
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Two-stream convolutional networks for action recognition in videos",
            "venue": "Adv. Neural Inf. Process. Syst",
            "year": 2014
        },
        {
            "authors": [
                "L. Wang",
                "Y. Xiong",
                "Z. Wang",
                "Y. Qiao"
            ],
            "title": "Towards good practices for very deep two-stream convnets",
            "venue": "arXiv 2015,",
            "year": 2015
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "A. Pinz",
                "A. Zisserman"
            ],
            "title": "Convolutional two-stream network fusion for video action recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "D. Tran",
                "L. Bourdev",
                "R. Fergus",
                "L. Torresani",
                "M. Paluri"
            ],
            "title": "Learning spatiotemporal features with 3d convolutional networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Santiago,",
            "year": 2015
        },
        {
            "authors": [
                "J. Carreira",
                "A. Zisserman"
            ],
            "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "A. Diba",
                "M. Fayyaz",
                "V. Sharma",
                "A.H. Karami",
                "M.M. Arzani",
                "R. Yousefzadeh",
                "L. Van Gool"
            ],
            "title": "Temporal 3d convnets: New architecture and transfer learning for video classification",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "C. Feichtenhofer",
                "H. Fan",
                "J. Malik",
                "K. He"
            ],
            "title": "Slowfast networks for video recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Qiu",
                "T. Yao",
                "T. Mei"
            ],
            "title": "Learning spatio-temporal representation with pseudo-3d residual networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "D. Tran",
                "H. Wang",
                "L. Torresani",
                "J. Ray",
                "Y. LeCun",
                "M. Paluri"
            ],
            "title": "A closer look at spatiotemporal convolutions for action recognition",
            "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA,",
            "year": 2018
        },
        {
            "authors": [
                "S. Xie",
                "C. Sun",
                "J. Huang",
                "Z. Tu",
                "K. Murphy"
            ],
            "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "J. Wu",
                "P. Sun",
                "S. Chen",
                "J. Yang",
                "Z. Qi",
                "L. Ma",
                "P. Luo"
            ],
            "title": "Towards high-quality temporal action detection with sparse proposals",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Y. He",
                "X. Han",
                "Y. Zhong",
                "L. Wang"
            ],
            "title": "Non-Local Temporal Difference Network for Temporal Action Detection",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "T. Lin",
                "X. Zhao",
                "H. Su",
                "C. Wang",
                "M. Yang"
            ],
            "title": "Bsn: Boundary sensitive network for temporal action proposal generation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "J. Gao",
                "Z. Shi",
                "G. Wang",
                "J. Li",
                "Y. Yuan",
                "S. Ge",
                "X. Zhou"
            ],
            "title": "Accurate temporal action proposal generation with relation-aware pyramid network",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Song",
                "I. Kim"
            ],
            "title": "Spatio-Temporal Action Detection in Untrimmed Videos by Using Multimodal Features and Region Proposals",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "J. Gao",
                "Z. Yang",
                "K. Chen",
                "C. Sun",
                "R. Nevatia"
            ],
            "title": "Turn tap: Temporal unit regression network for temporal action proposals",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        },
        {
            "authors": [
                "S. Buch",
                "V. Escorcia",
                "C. Shen",
                "B. Ghanem",
                "J. Carlos Niebles"
            ],
            "title": "Sst: Single-stream temporal action proposals",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "F. Long",
                "T. Yao",
                "Z. Qiu",
                "X. Tian",
                "J. Luo",
                "T. Mei"
            ],
            "title": "Gaussian temporal awareness networks for action localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "T. Lin",
                "X. Liu",
                "X. Li",
                "E. Ding",
                "S. Wen"
            ],
            "title": "Bmn: Boundary-matching network for temporal action proposal generation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Liu",
                "Z. Wang"
            ],
            "title": "Progressive Boundary Refinement Network for Temporal Action Detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence, New York, NY, USA,",
            "year": 2020
        },
        {
            "authors": [
                "H. Su",
                "W. Gan",
                "W. Wu",
                "Y. Qiao",
                "J. Yan"
            ],
            "title": "Bsn++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "G. Chen",
                "Y.D. Zheng",
                "L. Wang",
                "T. Lu"
            ],
            "title": "DCAN: Improving temporal action detection via dual context aggregation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Y.W. Chao",
                "S. Vijayanarasimhan",
                "B. Seybold",
                "D.A. Ross",
                "J. Deng",
                "R. Sukthankar"
            ],
            "title": "Rethinking the faster r-cnn architecture for temporal action localization",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "T. Lin",
                "X. Zhao",
                "Z. Shou"
            ],
            "title": "Single Shot Temporal Action Detection",
            "venue": "In Proceedings of the 25th ACM international conference on Multimedia, Mountain View, CA,",
            "year": 2017
        },
        {
            "authors": [
                "S. Buch",
                "V. Escorcia",
                "B. Ghanem",
                "L. Fei-Fei",
                "J.C. Niebles"
            ],
            "title": "End-to-end, single-stream temporal action detection in untrimmed videos",
            "venue": "In Proceedings of the British Machine Vision Conference 2017, London, UK,",
            "year": 2017
        },
        {
            "authors": [
                "C. Wang",
                "H. Cai",
                "Y. Zou",
                "Y. Xiong"
            ],
            "title": "Rgb stream is enough for temporal action detection",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Tian",
                "C. Shen",
                "H. Chen",
                "T. He"
            ],
            "title": "Fcos: Fully convolutional one-stage object detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "H. Law",
                "J. Deng"
            ],
            "title": "Cornernet: Detecting objects as paired keypoints",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "H. Qiu",
                "Y. Ma",
                "Z. Li",
                "S. Liu",
                "J. Sun"
            ],
            "title": "Borderdet: Border feature for dense object detection",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2020: 16th European Conference, Glasgow, UK,",
            "year": 2020
        },
        {
            "authors": [
                "C. Lin",
                "C. Xu",
                "D. Luo",
                "Y. Wang",
                "Y. Tai",
                "C. Wang",
                "J. Li",
                "F. Huang",
                "Y. Fu"
            ],
            "title": "Learning salient boundary feature for anchor-free temporal action localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "L. Yang",
                "H. Peng",
                "D. Zhang",
                "J. Fu",
                "J. Han"
            ],
            "title": "Revisiting anchor mechanisms for temporal action localization",
            "venue": "IEEE Trans. Image Process",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Adv. Neural Inf. Process. Syst. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "A. Arnab",
                "M. Dehghani",
                "G. Heigold",
                "C. Sun",
                "M. Lu\u010di\u0107",
                "C. Schmid"
            ],
            "title": "Vivit: A video vision transformer",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, BC,",
            "year": 2021
        },
        {
            "authors": [
                "G. Bertasius",
                "H. Wang",
                "L. Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In Proceedings of the ICML, Virtual,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "X. Li",
                "C. Liu",
                "B. Shuai",
                "Y. Zhu",
                "B. Brattoli",
                "H. Chen",
                "I. Marsic",
                "J. Tighe"
            ],
            "title": "Vidtr: Video transformer without convolutions",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Montreal, BC,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "J. Ning",
                "Y. Cao",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "H. Hu"
            ],
            "title": "Video swin transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "X. Liu",
                "Q. Wang",
                "Y. Hu",
                "X. Tang",
                "S. Zhang",
                "S. Bai",
                "X. Bai"
            ],
            "title": "End-to-end temporal action detection with transformer",
            "venue": "IEEE Trans. Image Process",
            "year": 2022
        },
        {
            "authors": [
                "C.L. Zhang",
                "J. Wu",
                "Y. Li"
            ],
            "title": "Actionformer: Localizing moments of actions with transformers",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2022: 17th European Conference, Tel Aviv, Israel,",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "Z. Qing",
                "Z. Huang",
                "Y. Feng",
                "S. Zhang",
                "J. Jiang",
                "M. Tang",
                "C. Gao",
                "N. Sang"
            ],
            "title": "Proposal relation network for temporal action detection",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "D. Shi",
                "Y. Zhong",
                "Q. Cao",
                "J. Zhang",
                "L. Ma",
                "J. Li",
                "D. Tao"
            ],
            "title": "ReAct: Temporal Action Detection with Relational Queries",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Nag",
                "X. Zhu",
                "Y.Z. Song",
                "T. Xiang"
            ],
            "title": "Proposal-free temporal action detection via global segmentation mask learning",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2022: 17th European Conference, Tel Aviv, Israel,",
            "year": 2022
        },
        {
            "authors": [
                "F.C. Heilbron",
                "V. Escorcia",
                "B. Ghanem",
                "J.C. Niebles"
            ],
            "title": "Activitynet: A large-scale video benchmark for human activity understanding",
            "venue": "In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "G.A. Sigurdsson",
                "G. Varol",
                "X. Wang",
                "A. Farhadi",
                "I. Laptev",
                "A. Gupta"
            ],
            "title": "Hollywood in homes: Crowdsourcing data collection for activity understanding",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,",
            "year": 2016
        },
        {
            "authors": [
                "G.A. Sigurdsson",
                "S. Divvala",
                "A. Farhadi",
                "A. Gupta"
            ],
            "title": "Asynchronous temporal fields for action recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "R. Dai",
                "S. Das",
                "S. Sharma",
                "L. Minciullo",
                "L. Garattoni",
                "F. Bremond",
                "G. Francesca"
            ],
            "title": "Toyota smarthome untrimmed: Real-world untrimmed videos for activity detection",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2022
        },
        {
            "authors": [
                "H. Idrees",
                "A.R. Zamir",
                "Y.G. Jiang",
                "A. Gorban",
                "I. Laptev",
                "R. Sukthankar",
                "M. Shah"
            ],
            "title": "The thumos challenge on action recognition for videos \u201cin the wild",
            "venue": "Comput. Vis. Image Underst",
            "year": 2017
        },
        {
            "authors": [
                "H. Zhao",
                "A. Torralba",
                "L. Torresani",
                "Z. Yan"
            ],
            "title": "Hacs: Human action clips and segments dataset for recognition and temporal localization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, Seoul, Republic of Korea,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xiong",
                "L. Wang",
                "Z. Wang",
                "B. Zhang",
                "H. Song",
                "W. Li",
                "D. Lin",
                "Y. Qiao",
                "L. Van Gool",
                "X. Tang"
            ],
            "title": "Cuhk & ethz & siat submission to activitynet challenge 2016",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "F.C. Heilbron",
                "W. Barrios",
                "V. Escorcia",
                "B. Ghanem"
            ],
            "title": "Scc: Semantic context cascade for efficient action detection",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "R. Dai",
                "S. Das",
                "L. Minciullo",
                "L. Garattoni",
                "G. Francesca",
                "F. Bremond"
            ],
            "title": "Pdan: Pyramid dilated attention network for action detection",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Waikoloa, HI, USA,",
            "year": 2021
        },
        {
            "authors": [
                "X. Liu",
                "S. Bai",
                "X. Bai"
            ],
            "title": "An empirical study of end-to-end temporal action detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "A. Piergiovanni",
                "M. Ryoo"
            ],
            "title": "Temporal gaussian mixture layer for videos",
            "venue": "In Proceedings of the International Conference on Machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "R. Dai",
                "S. Das",
                "K. Kahatapitiya",
                "M.S. Ryoo",
                "F. Br\u00e9mond"
            ],
            "title": "MS-TCT: Multi-scale temporal convtransformer for action detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "M.S. Ryoo",
                "K. Gopalakrishnan",
                "K. Kahatapitiya",
                "T. Xiao",
                "K. Rao",
                "A. Stone",
                "Y. Lu",
                "J. Ibarz",
                "A. Arnab"
            ],
            "title": "Token turing machines",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "P. Tirupattur",
                "K. Duarte",
                "Y.S. Rawat",
                "M. Shah"
            ],
            "title": "Modeling multi-label action dependencies for temporal action localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Citation: Zhang, H.; Zhou, F.; Ma, C.;\nWang, D.; Zhang, W. MCMNET:\nMulti-Scale Context Modeling\nNetwork for Temporal Action\nDetection. Sensors 2023, 23, 7563.\nhttps://doi.org/10.3390/s23177563\nAcademic Editors: Antonio\nFern\u00e1ndez-Caballero and\nByung-Gyu Kim\nReceived: 7 August 2023\nRevised: 29 August 2023\nAccepted: 30 August 2023\nPublished: 31 August 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: action detection; multi-scale; self-attention mechanism"
        },
        {
            "heading": "1. Introduction",
            "text": "Due to the rapid growth of online video platforms, video understanding has attracted the attention of a large number of researchers in recent years. Action recognition and action detection are two fundamental tasks in the field of video understanding. Action recognition is the classification of action instances within a pre-edited video. In terms of temporal action detection, it involves the temporal localization and classification of multiple action instances within a raw, unedited video, requiring the detection of the start and end times of a specific action instance and its classification. Compared with action recognition, temporal action localization is closer to realistic scenarios. Therefore, temporal action detection is more challenging than action recognition while also being closer to practical applications. Temporal action detection can be used for video retrieval to filter out some videos containing inappropriate content in a large amount of video data. It can also be applied to security, where violent behavior can be detected by cameras. Especially for videos that contain a large number of action instances, the duration of these actions can vary, and there may be overlapping parts between them. Such videos often better reflect our daily lives. For example, a video of a person working at a table (see Figure 1) may contain multiple different action instances, such as sitting in a chair, reading a book, tidying a table, holding a cup, etc. The duration of these actions vary significantly, and the temporal relationships between actions are also very complex, such as a person holding a cup while tidying a table. Towards modeling such complex temporal information, previous approaches have partly used convolutional networks [1\u20135]. These methods perform well at aggregating\nSensors 2023, 23, 7563. https://doi.org/10.3390/s23177563 https://www.mdpi.com/journal/sensors\nshort-term temporal information. However, they are limited by the size of the receptive field of the convolutional network, which prevents them from capturing the relationships between distant segments in the video. Some researchers subsequently found that constructing a graph structure using each video frame as the node and the temporal relationship between video frames as the edge of the graph convolution [6\u20139] could well model the temporal information between video frames. However, the performance of these methods depends on how the graph structure is constructed and the choice of some hyperparameters. With the advent of vision Transformers (ViTs) [10], transformer-based methods [11\u201315] quickly emerged. The ability of the self-attention mechanism to capture long-term dependencies allows it to model the global temporal context better. However, a pure transformer network requires much more memory than a convolutional network when fed with a large amount of data simultaneously. It is also essential to take into account the local temporal context information.\nTherefore, in order to model contextual information at different temporal scales more effectively, we designed a two-stream network MCMNET, as shown in Figure 2, by first splitting the input data into two temporal resolution streams, with the aim of obtaining a richer representation of information with different coarse and fine granularity by processing more raw feature information with different temporal resolutions. Both streams are fed into the Multi-Resolution Context Aggregation module (MRCA) to obtain multi-scale temporal features. The MRCA module is composed of Attenuation blocks and Aggregation blocks. The Attenuation block mainly consists of a Reduction block, Global block, and Local block which aim to build a pyramid of features with different temporal resolutions. The Reduction block operates on the temporal resolution, reducing the resolution and increasing the dimension of the features; the Global block uses multi-headed selfattention mechanism to model global temporal features; and the Local block uses multiple convolutional layers to model local information. Followed by the modeling of temporal relations at different scales, an Aggregation block is used to fuse the features from each stage to get a unified feature representation. In order to aggregate long-range and short-range contexts more efficiently, we have added an Information Enhancement (IE) module to the back of the high-resolution stream as a complement to the MRCA module. The IE module is a stack of Multi-Path Temporal Convolutions. In each convolution block, there are three paths: the long-range path, to expand the perceptive field and aggregate the long-range context by dilated convolution; the short-range path, to aggregate the short-range context by the ordinary convolution; and the original path, to enhance the representation of features and to solve the problem of network degradation during training. Finally, we combine the\nthree paths and perform element-wise addition to obtain a strong feature with long-range and short-range contexts. We summarize our contributions as follows.\n(1) We propose an effective two-stream network to aggregate the multi-scale temporal context. Using this model, we are able to detect action in some scenarios where the temporal relation of the action is complex. (2) A multi-scale context modeling network is proposed for temporal action detection. MCMNET consists of two main modules: MRCA and IE. MRCA processes the input data in multiple stages with different temporal scales, which allows MCMNET to learn both fine-grained relations in the early stage and coarse relations between composite actions in the latter stage. While IE is used to aggregate long-term and short-term context effectively, which makes the video features richer. (3) The experiments prove the convincing performance of MCMNET on three popular action detection benchmarks: ActivityNet-v1.3, Charades, and TSU."
        },
        {
            "heading": "2. Related Work",
            "text": "In this section, we review the prior work related to action recognition, action detection with CNN, and action detection with transformer."
        },
        {
            "heading": "2.1. Action Recognition",
            "text": "Action recognition is an important task in video understanding. Most of the traditional methods were based on hand-designed visual features [16]. Later on, with the success of deep learning, most of the methods are now based on neural networks. From the beginning, there were dual-stream networks [17\u201319], which used both optical and RGB streams as input and sent to a 2D convolutional neural network for processing. This was followed by the 3D convolutional network [20\u201322], which uses a 3D tensor with two spatial and one time dimension to model spatio-temporal features. To reduce the computational consumption of 3D convolutional networks, some approaches split the 3D convolution into 2D convolution and 1D convolution, becoming (2+1)D convolution [23\u201325]. We are inspired by the dual-stream network and set up a dual-resolution stream input in our network."
        },
        {
            "heading": "2.2. Action Detection with CNN",
            "text": "Action detection aims at localizing the temporal boundaries of human activities in untrimmed videos and classifying the action categories. Most existing work has used CNN-based models [4,26\u201328] to extract spatio-temporal features from the input video frames before feeding the features into the TAD (Temporal Action Detection) network. A common practice is first to generate temporal proposals and then classify each proposal to one of the action categories [3,29\u201331]. For generating proposals, there are anchor-based approaches [32\u201334], which retrieve fine-grained proposals by adjusting a pre-defined multiscale anchor. There are also boundary-based approaches [35\u201338], which predict the start and end confidence of each frame and then match start and end frames to generate the proposals with confidence evaluation. Refs. [30,39] generated proposals based on predefined sliding window anchors and train a classifier to filter anchors. Another practice is the one-stage approach [40\u201342], which performs localization and classification at the same time, and thus, it is more efficient. Ref. [40] presented the first one-stage TAD method using convolutional networks. Later influenced by the anchor-free method [43\u201345] in the object detection task, AFSD, [46] designed a basic anchor-free localizer, along with making full use of the temporal insights of videos to propose novel refinement strategy and consistency learning. Moreover, [47] explored the combination of anchor-based and anchor-free methods. In our work, we define anchor points in the Norm&Location module and combine starting and ending predictions to make training more regular."
        },
        {
            "heading": "2.3. Action Detection with Transformer",
            "text": "The Transformer [48] approach was first applied to NLP, but later, with the advent of ViTs [10], the transformer was formally applied to the image domain. With the success of the transformer in the image domain, researchers began to use the transformer for various tasks in vision, including video understanding. ViViT [49], TimeSformer [50], and VidTr [51] propose to factorize along spatial and temporal dimensions on the granularity of encoder, attention block, or dot-product computation. Because transformer networks require large computational resources when processing larger data such as video, Video Swin Transformer [52] proposes shifted window attention to limit the computation within a small local window. While in the case of the TAD task. TadTR [53] is based on the structure of DETR and uses temporal deformable attention to solve the TAD task as a sequence prediction problem. Actionformer [54] uses an anchor-free approach to design a simple and pure transformer network. Our MRCA module inherits a transformer encoder architecture while gaining benefits from temporal convolution. This enables it to model global and local temporal context at different temporal scales."
        },
        {
            "heading": "3. Proposed Method",
            "text": ""
        },
        {
            "heading": "3.1. Problem Formulation",
            "text": "The input to our pipeline is a raw video that spans varying duration. Following common video action detection methods [55\u201357], we consider feature sequences extracted from video frames by a 3D CNN as input to MCMNET. For each video of length lv, we divide it into T video clips; the length of each video clip is \u03c3, T = lv/\u03c3 , and the feature dimension corresponding to each video clip is C \u00d7 1. In this way, the input feature sequence for the pipeline can be written as X = {xi}Ti=1eRC\u00d7T . Furthermore, for each video sequence, there is a set of labels with number N relative to it: K = {kn = (ts,n, te,n, Cn)}Nn=1, where kn represents the nth action instance, and ts,n, te,n, and Cn are its start time, end time, and action class, respectively. For each input video, temporal action detection model needs\nto predict M possible action instance \u039b = {\n\u03bbm = ( \u2212 t s,m, \u2212 t e,m, \u2212 Cm, Pm )}M m=1 . Here, \u03bbm\nrepresents the mth predicted action in the video, it contains four indicators t\u0304s,m, t\u0304e,m, C\u0304m, and Pm. t\u0304s,m and t\u0304e,m represent the predicted start time and end time for the mth predicted action; C\u0304m and Pm are its predicted action class and confidence score, respectively."
        },
        {
            "heading": "3.2. MCMNET Architecture",
            "text": "The overall architecture of MCMNET is illustrated in Figure 3. We pre-process the video to obtain two feature sequences with different temporal resolutions, which are used as input for the model. The model consists of three main modules: a multi-resolution context aggregation module (MRCA), an information enhancement module (IE), and a post-processing module (Norm&Location).\nFirst, the fragment features are copied twice, and their temporal resolution is adjusted to T and T/2 by convolution, where the feature stream with a temporal resolution of T is called a high-resolution stream and another feature stream is called a low-resolution stream. The two streams will be passed through the MRCA module separately, and for the high-resolution stream, it will pass through the four stages of the MRCA. Each stage the dimensionality of the incoming data from the previous stage is changed, the temporal resolution will be reduced to half of the original one, and the channel size will be expanded to \u03b1 times of the original one accordingly, where \u03b1 is taken as 1.5 in the experiment. The dimensionality-changed data are then passed through a self-attention layer to obtain the global temporal context, after which the standard convolution is used to obtain the local temporal context. In this way, we try to have the model learn fine-grained action representation with more temporal information in the early stages and coarse-grained action reprsentation with less temporal information in the later stages. In order to increase the robustness and diversity of the information contained in the features, the same operation is applied to the low-resolution stream, with the difference that the temporal resolution decays at a rate of 2/3 and the channel size increases at the same multiplier. Next, to improve the model\u2019s ability to aggregate short-range and long-range contexts, we fed high-resolution stream into the IE module, which has eight blocks, each with an Expansion block and a Fixation block. There are three paths in the Expansion block, one of which uses dilated convolution to aggregate long-range context and expand the receptive field. One path uses regular convolution to aggregate short-range temporal context. The last path does not operate on the input features, keeping the original features for fusion with the other paths. However, the method of expanding the receptive field by stacking a large number of dilated convolution layers will cause gridding artifacts, which can lead\nto loss of information, so we effectively avoid this problem by adding a Fixation block after each Expansion block. There are also three path in the Fixation module, except that the dilated convolution in the long-range path is replaced by a convolution with a fixed dilated rate. Finally, after fusing the features obtained above, they are fed into the Norm&Location module for regularization, after which the regression score and classification score are predicted by multiple fully connected layers and their losses are calculated separately, followed by back propagation."
        },
        {
            "heading": "3.3. Multi-Resolution Context Aggregation",
            "text": "The MRCA module is the core of modeling the temporal context, which processes the video sequence features obtained through the I3D network. As depicted in Figure 4, the MRCA contains four Attenuation blocks and four Aggregation blocks. Such multiple blocks are constructed to cope with the complex temporal relationships of the video while building a multi-scale hierarchy of temporal features.\nAttenuation Block. The structure of the Attenuation block is shown in Figure 4, which can be subdivided into three main structures: the Reduction block, the Global block, and the Local block. In this stage, we use a temporal convolutional layer with kernel size and stride of 2 to decay the temporal dimension of the feature to 1/2 of its original size, and accordingly, the channel size is increased to 1.5 times of its original size. In this way, by constructing four stages, each of which processes the dimensionality of the features, a different coarse fine-grained action representation can be obtained. Next, the scale transformed feature token is fed into the Global block, which uses a multi-head self-attention mechanism to integrate global temporal context. Furthermore, its computation process can be described as follows: the input data X = {xi}Ti=1eRC\u00d7T go through eight head self-attention block, for each head j \u2208 {1, . . . , 8}, xi is projected using\nWqji, W k ji, and W v jieR C/8\u00d7C to extract feature representations Qji, Kji, and Vji, referred to as query, key, and value. The outputs Q, K, and V are computed as Qji = W q jixi, Kji = W k jixi, and Vji = Wvjixi. The output of the jth head self-attention is given by:\nAji = So f tmax ( QjikTji/ \u221a C/8 ) Vji (1)\nThen, the combination of multi-head self-attention can be shown as:\nPi = WOi Concat(A1i, . . . , A8i) + xi. (2)\nThe output of multiple headers is concatted and then passed through a linear layer, where WOi eR\nC\u00d7C denotes the weight of the linear layer. After the multi-head attention layer, the output feature size is the same as the input feature size. After that, we use two linear layers and a temporal convolution layer of kernel size 3 as the Local block to obtain the local temporal context. The first linear layer expands the feature dimension, then the convolution layer mixes the neighboring tokens to get local context, and finally, the last linear layer projects the feature dimension back. Each Attenuation block contains L Global and Local blocks, and the final output from each Attenuation block is combined and fed into the Aggregation block. Aggregation Block. After obtaining such multi-scale temporal features, we also need to aggregate the multi-scale features to have a unified video representation in order to facilitate the subsequent detection by the detection head, which requires our Aggregation block. The features obtained from each attenuation module are fed into the Aggregation block, as shown in Figure 4. We will first upsample the output of each Attenuation block Mn, n \u2208 {1, . . . , N}, with different upsampling rates in different block; this operation can be formulated as:\ngn(Mn) = UpSampling(\u2202n Mn). (3)\nwhere \u2202n \u2208 RDo\u00d7\u03b1 n\u22121C denotes the weight of liner layer. Upsampling results in interpolation in the time dimension to the same time dimension as the input features. Since the temporal and semantic information contained in the output features of the N Attenuation blocks varies greatly, in order to balance the temporal and semantic information between each output feature, each upsampled feature performs elementwise addition with the output of the Attenuation block N, which has also undergone the upsampling operation. This is because block N is the deepest layer of the network (N is taken to be 4 in our model), which contains the richest semantic information. This is given by:\nM \u2032 n = gn(Mn)\u2295 g4(M4). (4)\nFinally, all the output of the Aggregation block will be concatenated to a final video representation:\nFHM = Concat ( M \u2032 1, . . . , M \u2032 4 ) . (5)\nwhere FHM represents the features obtained from the high-resolution stream after undergoing MRCA module processing."
        },
        {
            "heading": "3.4. Information Enhancement Module",
            "text": "Because the self-attention mechanism pays more attention to the correlation between positions but ignores the order and distance, we propose an additional convolution module as a complement to the self-attention mechanism: Information Enhancement module aims to aggregate long-range and short-range temporal context for temporal evaluation effectively and increase feature richness. As shown in Figure 5, the IE module can be divided into eight module groups, with two different blocks in each group: the Expansion block and Fixation block. The Expansion block consists of three paths. The first path is a short-range path including an ordinary\nconvolution with a kernel size of 3, which aims to aggregate short-range temporal context. The second path is a long-range path, including a dilated convolution with a kernel size of 3 and a dilation rate of 2k, where k denotes the ordinal number of the current block. The role of this path is to aggregate long-range temporal context and expand the receptive field. The last path leaves the input features untouched in order to preserve the original information and alleviate network degradation during training. Finally, the final feature vector is obtained by fusing the output of the three paths. However, stacking a large number of dilated convolutions in the long-range path of the Expansion block can cause information loss, as not every position in the dilated convolution is involved in the computation, and therefore, information at some positions will be lost. In image processing, the solution to this problem is by stacking dilated convolutions with a jagged dilation rate so that the distribution of convolution kernels covers every position and there are no more omissions. The specific operation is to add a fixation block after each Expansion block, as shown in Figure 5. Compared with the Expansion block, the only change in the Fixation block is that the dilation rate of the expansion convolution in the long-range path is changed from 2k to 3, so that the alternately connected Expansion block and Fixation block form the IE module, which can ensure that the receptive field grows rapidly and there is no information loss."
        },
        {
            "heading": "3.5. Norm and Localization",
            "text": "The input data, after being processed by the MRCA and IE modules, respectively, are then fed into the post-processing module and can be expressed by the following formula:\nFBNL = [(FHM + FLM)/2] + FIE. (6)\nwhere FHM and FIE represent features obtained from high-resolution streams processed by the MRCA and IE modules, respectively, and FLM denotes low-resolution streams\nprocessed by MRCA. The above features are fused to obtain the input FBNL for the postprocessing module. In the post-processing module, we first input features to the Norm module, using pre-defined anchors to generate segments{U \u03b5 j} J j=1, where J is the total number of segments\nand \u03b5 j = { ts,j, te,j }\nrepresents the start moment and end moment of the j-th segment. We sample \u03d1 points (\u03d1: alignment quantity) via interpolation and rescaling as described in Algorithm 1, and generate the segment feature Y = {y\u03b5 j} J j=1.\nAlgorithm 1 Interpolation and Rescaling in Norm&Location.\nInput: The input data {xl}Ll=1; {U\u03b5 j} J j=1, where J is the total number of processed data, \u03b5 j = { ts,j, te,j }\n; alignment quantity \u03d1; 1: for each U\u03b5 j do 2: List all U\u03b5 j in chronological order; 3: Compute sampling interval s = \u2308( ts,j \u2212 te,j ) /\u03d1 \u2309 , interpolation quantity J = \u03d1s; 4: Sample J points based on linear interpolation using the two neighbors of each point\ng = [ ts + k(ts,j\u2212te,j) J f or k in range(J) ] ;\n5: Xin = [ (die \u2212 i)xbic + (i\u2212 bic)xdie f or i in g ] ; 6: y\u03b5 j = [mean(Xin[ks : [k + 1]s]) f or k in range(\u03d1)]; 7: end for\nOutput: Y = {y\u03b5 j} J j=1.\nFor each segment U\u03b5, we calculate its Intersection-over-Union (IoU) with all groundtruth actions Gt, and denote the maximum IoU \u03c8c as the training target. Then, we set three fully connected layers(FC) for U\u03b5, while the last FC layer output two scores pcls and preg. They are trained to match \u03c8c using classification and regression losses, respectively."
        },
        {
            "heading": "3.6. Training and Inference",
            "text": "Training. We train MCMNET by using the classification loss Lc and the localization loss Lr:\nL = Lr + Lc. (7)\nThe loss Lr is used to determine the confidence scores of segments. The loss Lc classifies each feature segment according to its position relative to the action. During model training, the training set in the dataset is processed into two resolutions and then input into the model; the two resolution streams are, respectively, passed through the MRCA module to get the spatio-temporal features with higher-level semantic information, after which the high-resolution streams are passed through the IE module to get the features with long-term and short-term temporal information, and then the two features are united to the same resolution and then fused to get the final video features FBNL. Inputting FBNL into the Norm&Localization module yields two scores, pcls and preg. With these two parameters, the first part of the loss function lr can be constructed. The localization loss Lr is defined as follows:\nLr = Lwce(pcls, Gcls) + \u03c91 \u00b7 Lmse ( preg, \u03c8c ) . (8)\nwhere Lwce is the weighted cross-entropy loss function used to calculate the loss of pcls and Gcls and Gcls is the confidence score obtained by binarizing the IoU map with a threshold 0.5, which is calculated by proposals and ground truth. Furthermore, Lmse is the square error loss. The weight is computed to balance the positive and negative training samples and we set the weighting factor \u03c91 to 10.\nMeanwhile, we input Fbnl directly into FC layers to get the starting and ending probabilities (ps, pe) and corresponding training targets for each feature segment (dss,dse). With these two parameters, it is possible to derive the classification loss function Lc:\nLc = Lwce(ps, dss) + Lwce(pe, dse). (9)\nFinally, we use the weighted cross-entropy loss Lwce to calculate the difference between the prediction and the target. Inference. During inference, the previous method for data processing is the same as in training, and the video data are passed through the trained model to get the final video feature FBNL. After feeding FBNL into the Norm&Localization module, MCMNET outputs the classification and regression scores for each segment U\u03b5. Among J segments, we construct:\n\u039b = {\n\u03bbj = ( \u2212 t s,j, \u2212 t e,j, \u2212 Cj, Pj )}J j=1 . (10)\nwhere ( \u2212 t s,j, \u2212 t e,j ) indicate the beginning and end moment of the predicted action, \u2212 Cj\ndenotes the predicted action class, and Pj denotes the confidence score of the prediction. Pj is obtained by Pj = p\u03b1cls \u00b7 p 1\u2212\u03b1 reg , where \u03b1 is obtained by searching in each setup after optimal value in the experiment. By comparing the predicted segment with the ground-truth, it is possible to obtain the mAP at different tIoUs (temporal Intersection over Union)."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Datasets and Metrics",
            "text": "We perform extensive experiments on the datasets of ActivitiesNet-v1.3, Charades, and TSU to demonstrate the effectiveness of our MCMNET. For comparison with existing models, our work follows the standard evaluation scheme and uses the mAP with intersection over union(IoU) thresholds as the evaluation metric. ActivityNet-v1.3 [58] is a large-scale dataset containing 10,024 training videos, 4926 validation videos, and 5044 test videos belonging to 200 activities covering sports, household, and working actions. ActivitiesNet-v1.3 only contains 1.5 occurrences per video on average, and most videos contain a single action category with 36% background on average. We report the mAP with IoU thresholds [0.5, 0.75, 0.95] on ActivitiyNet-v1.3. Charades [59] is a densely labeled dataset with 9848 videos of 157 daily indoor actions, separated into 7.9 k training and 1.8 k validation clips. Each video may include multiple overlapping activities annotated with frame-level labels. This is in contrast to ActivityNet, which only has one action per time-step. The average length of a video is 30 s. We evaluate the per-frame mAP on these densely labeled datasets following [60,61]. TSU [62] (Toyota Smarthome Untrimmed) is also a densely labeled dataset that contains 536 videos with an average duration of 21 min. Besides, TSU contains some very similar actions, such as eating food and drinking a drink, and some actions with high temporal variance, such as putting on glasses in 5 s, reading for 10 min, or some subtle actions such as stirring coffee. As a result, TSU has longer action durations and more complex temporal relationships than other datasets. We evaluate the per-frame mAP on TSU as Charades. MultiTHUMOS [61] is an extended version of THUMOS\u201914 [63] dataset, which contains dense, multilabel, frame-level action annotations for 30 h across 400 videos in the THUMOS\u201914 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video. This is in contrast to other activity detection datasets, such as ActivityNet and HACS(Human Action Clips and Segments) [64], which only have one activity per time-step."
        },
        {
            "heading": "4.2. Implementation Details",
            "text": "We use pre-extracted features for these three datasets. For ActivityNet-v1.3 and Charades, we used the pre-trained dual-stream network of [65] to extract video feature. For TSU, we use the officially available RGB I3D feacure. In the proposed network, the number of Attenuation blocks and Aggregation blocks is set to B = 4 and the number of Expansion blocks and Fixation blocks is set to N = 8. The number of attention heads for Global block is set to 8. Finally, we implemented and compiled our framework by using PyTorch 1.9, Python 3.7, and CUDA 11.6. For Charades and TSU training, we set the learning rate, batch size, and epoch to 0.0003, 24, and 10, respectively. In ActivityNet-1.3 training, the above parameters are set to 0.00003, 30, and 6, respectively. The learning rate will drop 10-fold every epoch."
        },
        {
            "heading": "4.3. Comparison with State-of-the-Arts Methods",
            "text": "In this subsection, we compare MCMNET with the state-of-the-art action detection method on ActivityNet-v1.3, Charades, MultiTHUMOS, and TSU in Tables 1 and 2.\nIn ActivityNet-v1.3, MCMNET is less advantageous for this dataset, as it contains fewer action instances per video and the temporal relationships are relatively simple, whereas the focus of MCMNET is on modeling multi-scale temporal contextual information. Furthermore, by comparing with the mainstream temporal action location methods, it is found that MCMNET performs significantly better than other methods when the IoU requires medium accuracy. When we focus on densely labeled datasets, we find that MCMNET performs reasonably well compared to other methods for these videos with more complex temporal relationships. Benefiting from MCMNET\u2019s excellent multi-scale temporal information aggregation capability, it performs outstandingly on Charades and MultiTHUMOS. Although MCMNET did not achieve the best performance on the TSU dataset, it still reached the state-of-the-art level. This is probably due to the fact that our\nmodel has a large number of parameters, while the TSU data volume is much smaller than that of Charades, and there are many longer video and action instances in the TSU. Therefore, we also need to pay more attention to modeling temporal information over long distances. At the same time, we compared the computational consumption required by several models, and from the results, we can see that our model requires much less computational resources than the model with pure self-attention mechanism (MLAD), but the computational consumption will still be higher than that of the model with pure CNN models (PDAN, TGM) because our model needs to take care of both the long duration actions and the short duration actions. In the future, we hope to simplify the model as much as possible and reduce the number of parameters so that we can achieve a better balance between effectiveness and efficiency."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "In this subsection, we validate the effectiveness of different components of MCMNET and evaluate the effects of various hyper-parameters. Effectiveness of MRCA. The MRCA module can be divided into two components: Attenuation block and Aggregation block. The Attenuation block can be further divided into three submodules: Reduction block, Global block, and Local block. We ablate the four submodules and study their impact on the final performance. Each submodule is individually enabled and disabled. We conduct ablation experiments on ActivityNet-v1.3, Charades, and TSU, respectively; the results are shown in Tables 3 and 4. It can be seen that overall, the Attenuation block has a significant improvement in the performance on all three datasets, which proves the effectiveness of the module for aggregating local and global temporal contexts. Moreover, the Global block has a more obvious improvement in the two densely labeled datasets, suggesting that self-attention is well suited for handling data with complex temporal relationships.\nEffectiveness of IE. The IE module contains Expansion block and Fixation block, where the Expansion block is used to expand the receptive field and to aggregate longrange temporal context, the Fixation block is used to prevent grid artifact caused by the rapid expansion of receptive field by the Expansion block. The results in Tables 5 and 6 show that when only Expansion block is present, there is a drop in accuracy for all three datasets. This is due to the fact that a large number of stacked dilated convolution layers can cause certain positions in the feature maps to be skipped and fail to participate in the\ncomputation. However, when both the Expansion block and the Fixation block are present, the results are much improved.\nChoice of attenuation factor. In the MACA module, each Attenuation block reduces the time dimension of the input feature to build multi-scale temporal context information. Furthermore, the attenuation factor greatly affects the final feature quality. Therefore, we conducted ablation experiments on the high-resolution stream and the low-resolution attenuation factor, respectively, and the results are shown in Table 7. The results show that the best results can be achieved when the attenuation factor of the high-resolution stream is 2 and that of the low-resolution stream is 1.5. Besides, it can be seen from the results that the effect of the attenuation factor is greater in the high resolution than in the low resolution.\nChoice of the number for IE block. In the IE module, we stack multiple Expansion blocks and Fixation blocks to aggregate long-range and short-range temporal context. We also conducted ablation experiments to determine the optimal number of blocks to use, and the results are shown in Table 8. The results show that as the number of blocks increases from 1 to 7, the performance gradually improves, but when the number of blocks is higher, the accuracy begins to decrease. This is because the kernel size of dilated convolution in the Expansion block is 2k, and as the number of layers increases, the receptive field quickly expands. When it exceeds a certain limit, information loss occurs, which leads to a decrease in accuracy. Therefore, we set the final number of blocks to 7.\nEfficiency Analysis. In this part, we report the effect of each module of MCMNET on the inference time and GFLOPs on ActivityNet-v1.3. Using 2000 proposals as input to the model and processing the video using NVIDIA RTX 3080ti for about 20 min, the results are shown in Table 9. The overall time required for the MRCA module is greater than that for the IE module. This is mainly because the multi-head self-attention in the MRCA module processes features at different time resolutions, which requires a large amount of computation. In addition, the Expansion block in the IE module requires much more time than the Fixation block. This is because in the later blocks of the IE module, the dilated convolution kernel size in the Expansion block is already very large, and the convolution operation at this stage also requires a long time. From the computational consumption results, it can be seen that overall, the computational resources required by the IE module are higher than those of the MRCA, which is due to the fact that the CNN network requires deeper network layers in order to achieve higher perceptive field. Therefore, we also need to work on model simplification in the future so that higher efficiency can be achieved."
        },
        {
            "heading": "4.5. Visualization",
            "text": "We show some qualitative detection results in Figure 6 on TSU (top), ActivityNet-v1.3 (middle) and Charades (bottom). The results showed that the detection performance was very good for some short-duration actions, such as a golf swing. However, the performance still needs to be improved for some longer duration actions, such as brushing teeth and blow drying hair. We also used GradCAM [73] to visualize the class activation map of three models, TSCN, PGCN, and our MCMNET, as shown in Figure 7. From the visualization results, it can be seen that for kicking a football, MCMNET is more precise in locating the key parts compared to the other two models. It focuses mainly on a few key movement parts and rejects the background unrelated to the movement."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this paper, we have proposed a multi-scale context modeling network. First, we extract the feature sequence from the video using a pre-trained model and splitting it into a high-resolution stream and a low-resolution stream. The two streams are then fed into the MRCA module to obtain local and global temporal contexts. The high-resolution streams are additionally fed into the IE module for modeling long-range and short-range temporal relationships. Finally, the three output features are fused and passed through the Norm&location module to regularize the data, which is then input into the classifier to obtain the final score. Extensive experiments conducted on three challenging action detection benchmarks demonstrate that our MCMNET achieves outstanding temporal localization performance. However, there are still some shortcomings in our approach, the main one being that the overall simplicity of the model is neglected in order to achieve better results. On the other hand, since our model operates on pre-extracted features, the whole model cannot be trained end-to-end with raw video data as input. This also leads to the fact that we cannot explore in detail the effect of spatial features of video data on the detection effect. In the future, we will explore in more detail the combination of modules which can take advantage of the strengths of each module while maintaining the simplicity of the model structure. Furthermore, moving closer to an end-to-end training model, it is possible to better model both temporal and spatial features and build a more robust model.\nAuthor Contributions: F.Z. programmed the algorithms, conducted the experiments, and edited the paper; H.Z. and C.M. reviewed the paper and supervised the experiment based on an analysis; D.W. and W.Z. reviewed the paper and designed the algorithm. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by University Research Initiation Fund (URIF), Grant/ Award Number: KYP0222010, KYP0222013.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The datasets generated during and analysed during the current study and source code are all available from the corresponding author on reasonable request.\nConflicts of Interest: The authors declare no conflict of interest."
        }
    ],
    "title": "MCMNET: Multi-Scale Context Modeling Network for Temporal Action Detection",
    "year": 2023
}