{
    "abstractText": "Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation experiments and real-world deployment.",
    "authors": [
        {
            "affiliations": [],
            "name": "Matthew Hanlon"
        },
        {
            "affiliations": [],
            "name": "Boyang Sun"
        },
        {
            "affiliations": [],
            "name": "Marc Pollefeys"
        },
        {
            "affiliations": [],
            "name": "Hermann Blum"
        }
    ],
    "id": "SP:f889a772e11bd7752f8048da20549d37c8afea2a",
    "references": [
        {
            "authors": [
                "I. Abraham",
                "T.D. Murphey"
            ],
            "title": "Active learning of dynamics for data-driven control using koopman operators",
            "venue": "IEEE Transactions on Robotics, 35(5):1071\u20131083",
            "year": 2019
        },
        {
            "authors": [
                "L. Bartolomei",
                "L. Teixeira",
                "M. Chli"
            ],
            "title": "Semantic-aware Active Perception for UAVs using Deep Reinforcement Learning",
            "venue": "IEEE International Conference on Intelligent Robots and Systems, pages 3101\u20133108",
            "year": 2021
        },
        {
            "authors": [
                "L. Bartolomei",
                "L. Teixeira",
                "M. Chli"
            ],
            "title": "Semantic-aware active perception for uavs using deep reinforcement learning",
            "venue": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3101\u20133108",
            "year": 2021
        },
        {
            "authors": [
                "D.S. Chaplot",
                "E. Parisotto",
                "R. Salakhutdinov"
            ],
            "title": "Active neural localization",
            "venue": "arXiv preprint arXiv:1801.08214",
            "year": 2018
        },
        {
            "authors": [
                "J. Chen",
                "B. Sun",
                "M. Pollefeys",
                "H. Blum"
            ],
            "title": "A 3d mixed reality interface for human-robot teaming",
            "venue": "submission",
            "year": 2023
        },
        {
            "authors": [
                "G. Costante",
                "C. Forster",
                "J. Delmerico",
                "P. Valigi",
                "D. Scaramuzza"
            ],
            "title": "Perception-aware path planning",
            "venue": "arXiv preprint arXiv:1605.04151",
            "year": 2016
        },
        {
            "authors": [
                "A.J. Davison",
                "D.W. Murray"
            ],
            "title": "Simultaneous localization and mapbuilding using active vision",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7):865\u2013880,",
            "year": 2002
        },
        {
            "authors": [
                "D. Detone",
                "T. Malisiewicz",
                "A. Rabinovich"
            ],
            "title": "SuperPoint: Self- Supervised Interest Point Detection and Description",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2018-June:337\u2013349,",
            "year": 2018
        },
        {
            "authors": [
                "O. Erat",
                "W.A. Isop",
                "D. Kalkofen",
                "D. Schmalstieg"
            ],
            "title": "Droneaugmented human vision: Exocentric control for drones exploring hidden areas",
            "venue": "IEEE transactions on visualization and computer graphics, 24(4):1437\u20131446",
            "year": 2018
        },
        {
            "authors": [
                "Q. Fang",
                "Y. Yin",
                "Q. Fan",
                "F. Xia",
                "S. Dong",
                "S. Wang",
                "J. Wang",
                "L.J. Guibas",
                "B. Chen"
            ],
            "title": "Towards accurate active camera localization",
            "venue": "European Conference on Computer Vision, pages 122\u2013139. Springer",
            "year": 2022
        },
        {
            "authors": [
                "H.J.S. Feder",
                "J.J. Leonard",
                "C.M. Smith"
            ],
            "title": "Adaptive mobile robot navigation and mapping",
            "venue": "The International Journal of Robotics Research, 18(7):650\u2013668",
            "year": 1999
        },
        {
            "authors": [
                "D. Fontanelli",
                "P. Salaris",
                "F.A. Belo",
                "A. Bicchi"
            ],
            "title": "Visual appearance mapping for optimal vision based servoing",
            "venue": "Experimental Robotics: The Eleventh International Symposium, pages 353\u2013362. Springer",
            "year": 2009
        },
        {
            "authors": [
                "P. H\u00fcbner",
                "K. Clintworth",
                "Q. Liu",
                "M. Weinmann",
                "S. Wursthorn"
            ],
            "title": "Evaluation of HoloLens Tracking and Depth Sensing for Indoor Mapping Applications",
            "venue": "Sensors 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A. Kim",
                "R.M. Eustice"
            ],
            "title": "Perception-driven navigation: Active visual slam for robotic area coverage",
            "venue": "2013 IEEE International Conference on Robotics and Automation, pages 3196\u20133203",
            "year": 2013
        },
        {
            "authors": [
                "A. Kim",
                "R.M. Eustice"
            ],
            "title": "Active visual slam for robotic area coverage: Theory and experiment",
            "venue": "The International Journal of Robotics Research, 34(4-5):457\u2013475",
            "year": 2015
        },
        {
            "authors": [
                "J. Lim",
                "N. Lawrance",
                "F. Achermann",
                "T. Stastny",
                "R. B\u00e4hnemann",
                "R. Siegwart"
            ],
            "title": "Fisher information based active planning for aerial photogrammetry",
            "venue": "2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1249\u20131255",
            "year": 2023
        },
        {
            "authors": [
                "D.G. Lowe"
            ],
            "title": "Object recognition from local scale-invariant features",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2:1150\u20131157",
            "year": 1999
        },
        {
            "authors": [
                "A.A. Makarenko",
                "S.B. Williams",
                "F. Bourgault",
                "H.F. Durrant- Whyte"
            ],
            "title": "An experiment in integrated exploration",
            "venue": "IEEE/RSJ international conference on intelligent robots and systems, volume 1, pages 534\u2013539. IEEE",
            "year": 2002
        },
        {
            "authors": [
                "E. Olson"
            ],
            "title": "Apriltag: A robust and flexible visual fiducial system",
            "venue": "2011 IEEE international conference on robotics and automation, pages 3400\u20133407. IEEE",
            "year": 2011
        },
        {
            "authors": [
                "M. Oquab",
                "T. Darcet",
                "T. Moutakanni",
                "H. Vo",
                "M. Szafraniec",
                "V. Khalidov",
                "P. Fernandez",
                "D. Haziza",
                "F. Massa",
                "A. El-Nouby",
                "M. Assran",
                "N. Ballas",
                "W. Galuba",
                "R. Howes",
                "P.-Y. Huang",
                "S.-W. Li",
                "I. Misra",
                "M. Rabbat",
                "V. Sharma",
                "G. Synnaeve",
                "H. Xu",
                "H. Jegou",
                "J. Mairal",
                "P. Labatut",
                "A. Joulin",
                "P. Bojanowski",
                "M.A. Research"
            ],
            "title": "DINOv2: Learning Robust Visual Features without Supervision",
            "year": 2023
        },
        {
            "authors": [
                "C. Papachristos",
                "S. Khattak",
                "K. Alexis"
            ],
            "title": "Uncertainty-aware receding horizon exploration and mapping using aerial robots",
            "venue": "2017 IEEE International Conference on Robotics and Automation (ICRA), pages 4568\u20134575",
            "year": 2017
        },
        {
            "authors": [
                "S.K. Ramakrishnan",
                "A. Gokaslan",
                "E. Wijmans",
                "O. Maksymets",
                "A. Clegg",
                "J. Turner",
                "E. Undersander",
                "W. Galuba",
                "A. Westbury",
                "A.X. Chang",
                "M. Savva",
                "Y. Zhao",
                "D. Batra",
                "F.A. Research"
            ],
            "title": "Habitat- Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments",
            "venue": "Embodied AI",
            "year": 2021
        },
        {
            "authors": [
                "C. Reardon",
                "K. Lee",
                "J. Fink"
            ],
            "title": "Come see this! augmented reality to enable human-robot cooperative search",
            "venue": "2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR), pages 1\u20137. IEEE",
            "year": 2018
        },
        {
            "authors": [
                "N. Roy",
                "W. Burgard",
                "D. Fox",
                "S. Thrun"
            ],
            "title": "Coastal navigationmobile robot navigation with uncertainty in dynamic environments",
            "venue": "Proceedings 1999 IEEE international conference on robotics and automation (Cat. No. 99CH36288C), volume 1, pages 35\u201340. IEEE",
            "year": 1999
        },
        {
            "authors": [
                "J. R\u00fcckin",
                "F. Magistri",
                "C. Stachniss",
                "M. Popovi\u0107"
            ],
            "title": "An informative path planning framework for active learning in uav-based semantic mapping",
            "venue": "arXiv preprint arXiv:2302.03347",
            "year": 2023
        },
        {
            "authors": [
                "S.A. Sadat",
                "K. Chutskoff",
                "D. Jungic",
                "J. Wawerla",
                "R. Vaughan"
            ],
            "title": "Feature-rich path planning for robust navigation of MAVs with Mono- SLAM",
            "venue": "Proceedings - IEEE International Conference on Robotics and Automation,",
            "year": 2014
        },
        {
            "authors": [
                "P.E. Sarlin",
                "C. Cadena",
                "R. Siegwart"
            ],
            "title": "Dymczyk. From Coarse to Fine: Robust Hierarchical Localization at Large Scale",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June:12708\u201312717,",
            "year": 2019
        },
        {
            "authors": [
                "L. Schmid",
                "M. Pantic",
                "R. Khanna",
                "L. Ott",
                "R. Siegwart",
                "J. Nieto"
            ],
            "title": "An efficient sampling-based method for online informative path planning in unknown environments",
            "venue": "IEEE Robotics and Automation Letters, 5(2):1500\u20131507",
            "year": 2020
        },
        {
            "authors": [
                "J.L. Schonberger",
                "J.M. Frahm"
            ],
            "title": "Structure-from-Motion Revisited",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December:4104\u20134113,",
            "year": 2016
        },
        {
            "authors": [
                "P. Speciale",
                "J.L. Schonberger",
                "S.B. Kang",
                "S.N. Sinha",
                "M. Pollefeys"
            ],
            "title": "Privacy preserving image-based localization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5493\u20135503",
            "year": 2019
        },
        {
            "authors": [
                "Y. Tao",
                "X. Liu",
                "I. Spasojevic",
                "S. Agarwa"
            ],
            "title": "and V",
            "venue": "Kumar. 3d active metric-semantic slam",
            "year": 2023
        },
        {
            "authors": [
                "A. Yamashita",
                "K. Fujita",
                "T. Kaneko",
                "H. Asama"
            ],
            "title": "Path and viewpoint planning of mobile robots with multiple observation strategies",
            "venue": "2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), volume 4, pages 3195\u2013 3200 vol.4",
            "year": 2004
        },
        {
            "authors": [
                "Z. Zhang",
                "D. Scaramuzza"
            ],
            "title": "Perception-aware receding horizon navigation for mavs",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), pages 2534\u20132541. IEEE",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhang",
                "D. Scaramuzza"
            ],
            "title": "Perception-aware receding horizon navigation for mavs",
            "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA), pages 2534\u20132541",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhang",
                "D. Scaramuzza"
            ],
            "title": "Fisher Information Field: an Efficient and Differentiable Map for Perception-aware Planning",
            "year": 2020
        },
        {
            "authors": [
                "R. Zurbr\u00fcgg",
                "H. Blum",
                "C. Cadena",
                "R. Siegwart",
                "L. Schmid"
            ],
            "title": "Embodied active domain adaptation for semantic segmentation via informative path planning",
            "venue": "IEEE Robotics and Automation Letters, 7(4):8691\u20138698",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nVisual localization and mapping systems are by now ubiquitous around humans. They are part of every smartphone, to e.g. improve localization in GPS denied environments, they are used in VR and AR headsets, in cars, and of course in robots. In parallel to this rollout, more and more environments get mapped, and localization becomes an interesting cloud service where any agent can send its observation and gets it localized in a pre-existing map. This can even be achieved without violating privacy [32]. However, it raises the question how well devices and robots can be localized if the map was created from a different kind of device and possibly from quite different points of view, as illustrated in Figure 2.\nAs an example of this larger question, this paper delves into the specific scenario of localizing a ground robot in apre-existing map generated from a mixed-reality headset. This application holds considerable practical value, as it obviates the need to re-map an entire building if a suitable map already exists. Furthermore, for seamless collaboration between human-robot teams, the ability to localize mixedreality devices and robots within a shared map becomes imperative, as highlighted in previous studies [6], [10], [25]. However, accurately localizing a robot in a map created with a head-mounted camera rig introduces its own challenges. These challenges stem from the use of diverse sensor devices and are compounded by significant variations in viewpoint between the mapping trajectory and the operational height of the robot. Such variations result in a reduced visual\n*equal contribution All authors are with the Computer Vision and Geometry Lab at ETH\nZu\u0308rich.\noverlap between the robot\u2019s perspective and the map, subsequently diminishing the localization performance. This issue becomes particularly prominent in the case of ground robots, including quadrupeds, where obstacles such as chairs, tables, and furniture frequently obscure substantial portions of the robot\u2019s environment.\nIn contrast to mixed-reality devices, robots possess the valuable ability to autonomously select their own viewpoints. This leads us to investigate whether active viewpoint selection can effectively address the challenges associated with cross-agent visual localization. In literature, this concept is widely recognized as active perception, and within our specific context, it is referred to as Active Visual Localization. The core objective of Active Visual Localization is to determine the pose of a camera within an existing map representation of the environment. One common approach involves assessing the localization utility of various viewpoints, typically through metrics like the Fisher Information metric or a combination of hand-crafted heuristics.. While extensive work has have been directed towards integrating these utility calculations into planning frameworks, a rela-\nar X\niv :2\n31 0.\n02 65\n0v 1\n[ cs\n.R O\n] 4\nO ct\n2 02\n3\ntively less studied component is how the utility value itself can be calculated, particularly in scenarios where the unique challenges discussed earlier.\nThis work delves into the exploration of a effective utility function to actively enhance visual localization of a robot in a within that was created from a different perspective. Our primary focus lies in evaluating established viewpoint selection and assessment criteria while introducing a novel data-driven approach to viewpoint scoring. The key contributions of this paper are:\n\u2022 a novel data-driven approach to viewpoint scoring resp. selection for active localization \u2022 a comparison and thorough evaluation of viewpoint selection methods for the problem of visual localization between heterogenous agents \u2022 a real-world validation of our findings by integrating the viewpoint selection into an active viewpoint planner for a robot with an arm-mounted camera\nWe further open-source our implementations and collected datasets."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "Active Vision describes the case where an agent has the ability to move visual sensors with the goal of improving the performance of perceptual tasks [2]. This concept finds application across various domains where visual information is utilized, such as exploration [30], data collection [27], or active learning [38]. The case studied in this work focuses on the task of improving visual localization by choosing the most informative viewpoint at a given position, which can be considered a subset of Active Vision: Active Visual Localization. Many existing works in the literature study how informative metrics can be incorporated into motion planning frameworks with the goal of optimizing a motion to maximize localization accuracy [26], [7], [35]. Within the scope of this research, our focus is on the specific task of augmenting visual localization by selecting the most informative viewpoint at a given position.\nRegarding viewpoint selection in the realm of active vision,early approaches primarily relies on handcrafted metrics to gauge the uncertainty or reliability of a given viewpoint in contributing to the system\u2019s state [28], [34], [15], [13], [36], [23]. Some methods take a different path by deriving evaluation metrics from the metric map and constructing a global utility map.. Authors of [37] propose a solution rooted in Fisher information theory [12], [20]. The central task they explore consists of determining the amount of information a viewpoint from a given pose will contribute\nto the localization process. They develop a novel map representation that enables efficient computation of the Fisher information for 6-DoF visual localization, known as the Fisher Information Field. . Similar ideas of using Fisher Information for viewpoint selection have also been explored in recent works[17], [16], [1]. However, it\u2019s important to note that these metrics often rely on heuristics, necessitate the design of specific handcrafted utility functions, and may have limitations in their representation capabilities for diverse and complex scenarios.\nAnother category of works introduces additional vision tasks to enhance the metric extraction process, notably incorporating semantic information [33]. For instance, the work of [3] aims to improve navigation performance by including semantic information, in order to discern perceptually informative areas of the environment. Such kind of work enriches the semantic understanding capability, however highly relies on the performance of the semantic module, and usually requires prior knowledge to link certain semantic classes that clearly correlate to visual informativeness.\nA separate line of research focus on using data-driven approaches to active visual localization. These works mostly choose to formulate the problem as a reinforcement learning task, tightly integrating metric determination with robot execution [4], [5], [11]. As an example, [18] trains an information-aware policy to find traversable paths as well as reduce the uncertainty of the environment. The learningbased model significantly enhance the robot\u2019s comprehension of its surroundings. However, it\u2019s worth noting that these models typically demand substantial computational resources for training and may require the simplification of the environment model to prevent overfitting.\nIn this work, we try to combine the advantages of both the data-driven approach and the viewpoint scoring scheme by formulating viewpoint selection as a classification problem."
        },
        {
            "heading": "III. METHOD",
            "text": "Problem Statement: Let x = (p, q), where p \u2208 R3 and q \u2208 so(3) are the position and orientation of the robot sensor. Given a map representation of the environment M and an initial guess of the robot pose x\u0302, the goal of the active viewpoint planning is to find an orientation for the robot sensor, at a certain position, such that the visual localization method returns the most accurate location estimation x\u0304, i.e.,\nq\u22c6 = argmin q \u2225x\u0304\u2212 x\u2225 (1)\ni = argmin q\n\u2225loc(M,O(x))\u2212 x\u2225 (2)\nHere loc(\u00b7) refers to the localization method, which takes the observation O, captured at the current pose x, and localizes it against the given map representation M.\nIn this paper, we choose M combines the landmark point cloud Ml and the TSDF Mt. Any viewpoint selection policy \u03c0(\u00b7) then takes M = (Ml,Mt) as prior knowledge of the environment, and selects a viewpoint for a certain position:\nq = \u03c0(M, p\u0302) (3)\nFor example, we implement the idea of Fisher Information policy of [37] as:\nqFIM = \u03c0FIM(M, p\u0302) = argmax q \u2211 i\u2208Ml v(x, i)Ii (4)\nwhere v(\u00b7) is the binary visibility of landmark i and Ii is the Fisher Information Metric of observing landmark i. To determine the visibility v(\u00b7), we use the TSDF map Mt. For each sampled viewpoint, a raycasting step is performed, during which we filter out the landmarks that are in collision with the environment, with a small tolerance to account for inaccuracies in the process.\nWe supplement this with two additional simple baselines:\n\u03c0max(M, p\u0302) = argmax q \u2211 i\u2208Ml v(x\u0302, i) (5)\n\u03c0angle(M, p\u0302) = argmax q \u2211 i\u2208Ml vangle(x\u0302, i) (6)\nwhere \u03c0max selects simply the view with the maximum visible landmarks, and \u03c0angle uses a stricter visibility criterion inspired by [8] that only considers a landmark visible if its relative location to x is similar to those poses from which it was seen during mapping. This is illustrated in Figure 3.\nData-driven viewpoint scoring On top of these baselines, we propose a data-driven approach. In this approach, we gather the following essential information for each viewpoint:\n\u2022 distance to every landmark \u2022 minimum and maximum distance per landmark during\nmapping \u2022 viewing angle for every landmark \u2022 minimum and maximum viewing angle per landmark\nduring mapping \u2022 pixel coordinates of every landmark in the camera\nfrustum of x \u2022 number of landmarks in the previously seen angle range \u2022 corresponding DINO [22] appearance features for every\nlandmark in the current viewpoint We build a data collection pipeline in simulated environments. To collect training data, we record for every viewpoint\nthe above information, as well as the ground truth pose and the result of the visual localization method at that viewpoint. Based on this data, we train neural networks to classify whether a viewpoint has been localized within an error threshold. In particular, we distinguish between a MLP-based method \u03c0MLP and a transformer-based method \u03c0V PT . The difference between these is the input encoding of the collected information. We design lightweight models in order to maintain real-time capability. Both models end with a softmax layer to determine the viewpoint score.\nBecause an MLP requires a fixed input dimensionality, but the number of landmarks varies across different viewpoints, we set a feature aggregation step in \u03c0MLP . For each information in the list above, we build a histogram to aggregate the corresponding value of all the filtered landmarks. For the pixel coordinates, we aggregate the information in a 2D heatmap instead of a 1D histogram. DINO features cannot be processed in the MLP.\nOur main motivation to investigate a transformer architecture is its ability to process input data of variable length. Thus, inputs can be provided in a per-landmark fashion, which also allows for the inclusion of features that cannot be easily aggregated, such as DINO appearance per landmark.\nOnce the networks are trained, for a given p, we evaluate all candidate viewpoints, assigning a localization score for each candidate, and then choosing the one with the highest score, e.g.:\n\u03c0VPT(M, p\u0302) = argmax q fV PT (F(x\u0302,M)) (7)\nwhere F are the input features, and fV PT (\u00b7) returns the output score of the transformer-based classifier."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "We build a complete data collection pipeline for training, validation, and testing in both simulated and real-world, and conduct a series of experiments. We also implement several baseline methods for comparison. The data collection and model training is mostly done within the simulated world, details can be found in IV-A and IV-B. All the baseline methods and our evaluation strategy are introduced in IVC. In the end, we show our real-world experiment in IV-D"
        },
        {
            "heading": "A. Data Generation",
            "text": "Simulated Scenario In this work, we focus on indoor scenarios, and we choose a selection of scenes from the HabitatMatterport 3D (HM3D) dataset [24]. HM3D provides highquality 3D reconstructions of real-world indoor environments with textured 3D meshes. We choose 9 scenes, shown in Figure 4, that we import into a simulator. In the simulator, we then manually record controlled trajectories with a simulated camera rig resembling the Microsoft HoloLens 2 [14] at a \u201dhuman-like\u201d height.\nData Collection To obtain Mt, we could use the depth image of the sensors and perform TSDF integration. However, Isaac Sim provides a simpler way to directly obtain occupancy information of a 3D scene using its built-in\nfunction1, which generates accurate occupancy maps for us to perform conclusion and collision checking.\nTo obtain Ml, we use the hloc/COLMAP [29], [31] mapping and localization framework to extract 2D local features from the image and triangulate them into 3D landmarks. We also use hloc as our localization method loc(\u00b7) in eq. 2. The collected images from the simulated HoloLens are then fed into the hloc pipeline to build the landmarks map. The perlandmark features in the feature vector F are also created and attached to each landmark at this stage. Additionally, landmarks that do not fit the following criteria are removed with the aim of improving map quality:\n\u2022 observed in at least 4 mapping images \u2022 observation distance within the range [0.1m, 8.0m] To recover the scale of the map, one option would be to create the reconstruction with known camera poses. However, this would not be representative of how this process could be done using real hardware, where poses would be obtained from some form of VI-SLAM module. Thus, in our experiment, the maps are created without the exact camera poses, instead building the reconstructions from the images alone and then using RANSAC to estimate the scale with respect to the simulator poses.\nTo collect training data, we generate random camera trajectories at a \u201drobot-like\u201d height with a simulated single RGB-D camera, which serves as an onboard sensor of the robot. At each waypoint, we take a group of viewpoint samples, and save its image observations, together with the corresponding ground-truth pose. As shown in Equations 1 and 2, we then run hloc for each viewpoint sample using its image observation and calculate the difference between the estimation pose and the ground truth."
        },
        {
            "heading": "B. Model Training",
            "text": "We train both models on the classification task of predicting whether a viewpoint will result in localization errors smaller than 0.1m and 1 deg. The main pre-processing steps consist of scaling all of the input features F to values between 0 and 1 and balancing the data such that it contains roughly equal numbers of positive and negative samples. Split of the training and validation sets can be found in Figure 4. For each scene, 100 waypoints are randomly generated. At each waypoint, 50 viewpoints are sampled, allowing full rotation in the yaw axis, while limiting the pitch to be within the range of [-10deg, 45deg]. This creates a training+validation set of 25\u2019000 images and a test set of 20\u2019000 images."
        },
        {
            "heading": "C. Evaluation Strategy",
            "text": "As introduced in III, we evaluate three baseline methods \u03c0FIM,\u03c0max,\u03c0angle. We supplement them with two extra naive approaches: \u03c0forward, which simply points the camera towards the next waypoint along the trajectory, and \u03c0random, which randomly selects the viewpoints. Similar to the training process, for each evaluation environment, random waypoints and\n1https://docs.omniverse.nvidia.com/isaacsim/latest/advanced tutorials/ tutorial advanced occupancy map.html\nsamples of viewpoints are generated. All sampled viewpoints are then evaluated by each viewpoint-choosing strategy, saving the localization error of each viewpoint chosen. As an upper bound to the achievable localization accuracy for a given position, the smallest error of the hloc localization for each is also saved and considered the \u201doptimal\u201d viewpointchoosing strategy."
        },
        {
            "heading": "D. Real-world Deployment and Test",
            "text": "Setup To further validate the results obtained from the simulation experiments, the best-performing methods were implemented as a ROS-compatible planning module and\ndeployed on a quadruped robot with a robotic arm that contains a calibrated color camera in the end effector, allowing different viewpoints to be viewed for a given body position. We use a HoloLens 2 to build a map of an indoor environment, as shown in Figure 5.\nData Collection Unlike the simulation, we no longer have the ground-truth poses in a real-world deployment. To evaluate the viewpoint-choosing strategies, we require two types of positional information: Each viewpoint planner needs to get an initial estimate p\u0302 that is used in Equation 3 to select a viewing direction at the current location. Given that viewing direction, the robot moves the arm and takes an image, and localizes this with respect to the existing map. We then need a ground-truth pose to evaluate this result against.\nTo measure accurate poses in the map created from the HoloLens recording, we use a combination of hloc and AprilTag fiducial markers [21]. Once a map of the environment has been created, an AprilTag is affixed to a static location. We capture several high-quality images of the marker, including its surroundings, using a calibrated DSLR camera. The AprilTag marker can be accurately localized within the captured images, and by ensuring that the images contain enough of the surroundings, they can also be localized in the map using hloc. This two-step process provides an estimated location of the AprilTag marker within the map for each image. An average of the estimated locations is taken in order to minimize the effect of errors in the tag or image localizations. This process gives us accurate poses of fiducial markers with respect to the captured landmark map.\nFor any waypoint on which we want to evaluate the viewpoint strategies, we initialize the robot at the location where we placed a fiducial marker and walk the robot to the investigated waypoint. We estimate p\u0302 of that waypoint from observing the fiducial marker with the robot camera and integrating the odometry to the investigated waypoint. This resembles a realistic odometry-based localization prior that can be fed into the viewpoint-choosing strategies.\nA similar approach can then be used to obtain a best-\npossible estimate of the robot body pose at any position in the map by placing a marker on a stable tripod that can be detected in both the robot\u2019s end-effector camera and in a collection of calibrated DSLR images. We use the body pose of the robot estimated in that way, together with the inverse kinematics of the arm, as ground truth that we compare the hloc results to."
        },
        {
            "heading": "V. RESULTS",
            "text": "As introduced in the last section, we evaluate our approaches and the baseline methods in both our simulation pipeline and the real world. Table I shows the quantitative results of the simulation. To demonstrate the generalization ability of our approaches across different feature descriptors, we evaluate all methods using both SuperPoint feature [9] and SIFT feature [19], whereas both of our scoring models are only trained with the SuperPoint feature. The result in the table shows that, with SuperPoint feature, our methods perform the best under every error level, even though our models are only trained to classify the 0.1 m, 1 deg error level. When we switch from SuperPoint to SIFT, recall percentages drop for all the methods, however, our datadriven methods still have the highest recall at almost every error level.\nTo investigate the effectiveness of DINO feature and occlusion handling, we trained several variations of MLPbased and VPT-based scoring models without prefiltering with the occlusion information, as well as VPT-based scoring models without taking DINO features as part of the input. The result shows that, without considering the occlusion, both approaches experience a performance drop, which also occurs in the baseline methods. It is noteworthy that the VPT method can surpass the performance of non-data-driven methods at its trained threshold, even without accounting for occlusion. We accumulate the correctly-localized waypoints along with different distance thresholds and show the result in Figure 6, where it is easier to verify that our learning-based approaches outperform the other baseline methods with both\nSuperPoint and SIFT features. It is also easy to observe the performance improvement by considering the occlusion.\nResults from the 12 waypoints (see Figure 5) in our real-world experiment are shown in Table II. Although the sample size is relatively small, we still see the dominating performance of the VPT-based approach, which shows its generalization ability to the real world. Besides, our approach shows great real-time performance. It achieves to successfully select viewpoints from 100 candidates in less than one second, running on a regular workstation with a NVIDIA GeForce RTX 2080 GPU.\nThe evaluation results highlight the effectiveness of datadriven methods in comparison to hand-crafted information metrics. Learning decision boundaries for landmark features and encoding diverse information enable data-driven approaches to outperform traditional heuristic metrics. Despite being trained with a limited dataset, these scoring models exhibit robustness across various scenarios, different local features, and both simulated and real-world environments. This underscores the potential of data-driven approaches, which adopt a less heuristic approach.\nWe observe differences between the two learning-based approaches. The transformer based model (VLP) generalises worse between different feature descriptors, but transfers best from simulation data to the real-world tests. The MLP, and the VLP with DINO, perform less good on SuperPoint features and the real-world test, but transfer better to SIFT features. This could indicate that in such situation, they get more semantic prior from DINO, that then is more general accross different features\nIn conclusion, our proposed data-driven light transformer exhibits optimal performance when evaluated on the same feature descriptors as those used during training. The inclusion of occlusion filtering based on mesh reconstruction enhances the performance of all approaches. It\u2019s important to note that in cases where geometric data isn\u2019t available,\nthe data-driven approach still produces superior results. This could be attributed to the learning process capturing information about the angle ranges from which landmarks were observed, which indirectly encodes geometric information."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "This work addresses the challenge of localizing ground robots within an existing map constructed from devices with varying perspectives.We introduce a novel data-driven approach to explore effective utility functions for this task and evaluate it alongside diverse viewpoint selection methods in the literature. Experiment results show that our approach greatly improves robot localization ability within a known point-cloud map in the presence of large viewpoint changes and occlusion from ground-level obstacles. These improvements are observed in both simulated and real-world environments.\nFuture endeavors could focus on further enhancing the generalization of our approach across diverse localization methods, map representations, and environmental conditions. Additionally, there is potential for addressing more complex situations, such as dynamic scenarios, to broaden the scope of applicability."
        }
    ],
    "title": "Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach",
    "year": 2023
}