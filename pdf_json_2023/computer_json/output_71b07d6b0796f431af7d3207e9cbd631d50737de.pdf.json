{
    "abstractText": "In this paper, we propose several new stochastic second-order algorithms for policy optimization that only require gradient and Hessian-vector product in each iteration, making them computationally efficient and comparable to policy gradient methods. Specifically, we propose a dimension-reduced second-order method (DR-SOPO) which repeatedly solves a projected two-dimensional trust region subproblem. We show that DR-SOPO obtains an O( \u22123.5) complexity for reaching approximate first-order stationary condition and certain subspace second-order stationary condition. In addition, we present an enhanced algorithm (DVR-SOPO) which further improves the complexity to O( \u22123) based on the variance reduction technique. Preliminary experiments show that our proposed algorithms perform favorably compared with stochastic and variance-reduced policy gradient methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinsong Liu"
        },
        {
            "affiliations": [],
            "name": "Chenghan Xie"
        },
        {
            "affiliations": [],
            "name": "Qi Deng"
        },
        {
            "affiliations": [],
            "name": "Dongdong Ge"
        },
        {
            "affiliations": [],
            "name": "Yinyu Ye"
        }
    ],
    "id": "SP:cf9ba29ee5197e1810e8f104b02a06078059c084",
    "references": [
        {
            "authors": [
                "A. Agarwal",
                "S.M. Kakade",
                "J.D. Lee",
                "G. Mahajan"
            ],
            "title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "N. Agarwal",
                "Z. Allen-Zhu",
                "B. Bullins",
                "E. Hazan",
                "T. Ma"
            ],
            "title": "Finding approximate local minima faster than gradient descent",
            "venue": "In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Arjevani",
                "Y. Carmon",
                "J.C. Duchi",
                "D.J. Foster",
                "A. Sekhari",
                "K. Sridharan"
            ],
            "title": "Second-order information in non-convex stochastic optimization: Power and limitations",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "J. Baxter",
                "P.L. Bartlett"
            ],
            "title": "Infinite-horizon policy-gradient estimation",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2001
        },
        {
            "authors": [
                "J. Bhandari",
                "D. Russo"
            ],
            "title": "Global optimality guarantees for policy gradient methods",
            "venue": "arXiv preprint arXiv:1906.01786,",
            "year": 2019
        },
        {
            "authors": [
                "S. Bhatnagar",
                "M. Ghavamzadeh",
                "M. Lee",
                "R.S. Sutton"
            ],
            "title": "Incremental natural actor-critic algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "C. Cartis",
                "N.I. Gould",
                "P.L. Toint"
            ],
            "title": "Adaptive cubic regularisation methods for unconstrained optimization. part i: motivation, convergence and numerical results",
            "venue": "Mathematical Programming,",
            "year": 2011
        },
        {
            "authors": [
                "S. Cen",
                "C. Cheng",
                "Y. Chen",
                "Y. Wei",
                "Y. Chi"
            ],
            "title": "Fast global convergence of natural policy gradient methods with entropy regularization",
            "venue": "Operations Research,",
            "year": 2022
        },
        {
            "authors": [
                "F.E. Curtis",
                "R. Shi"
            ],
            "title": "A fully stochastic second-order trust region method",
            "venue": "Optimization Methods and Software,",
            "year": 2020
        },
        {
            "authors": [
                "F.E. Curtis",
                "D.P. Robinson",
                "M. Samadi"
            ],
            "title": "A trust region algorithm with a worst-case iteration complexity of O( \u22123/2) for nonconvex optimization",
            "venue": "Mathematical Programming,",
            "year": 2017
        },
        {
            "authors": [
                "A. Cutkosky",
                "F. Orabona"
            ],
            "title": "Momentum-based variance reduction in non-convex sgd",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "H. Daneshmand",
                "J. Kohler",
                "A. Lucchi",
                "T. Hofmann"
            ],
            "title": "Escaping saddles with stochastic gradients",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "J.E. Dennis",
                "Jr.",
                "J.J. Mor\u00e9"
            ],
            "title": "Quasi-newton methods, motivation and theory",
            "venue": "SIAM review,",
            "year": 1977
        },
        {
            "authors": [
                "C. Fang",
                "C.J. Li",
                "Z. Lin",
                "T. Zhang"
            ],
            "title": "Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "T. garage contributors"
            ],
            "title": "Garage: A toolkit for reproducible reinforcement learning research",
            "venue": "https://github. com/rlworkgroup/garage,",
            "year": 2019
        },
        {
            "authors": [
                "S. Ghadimi",
                "G. Lan"
            ],
            "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming",
            "venue": "SIAM Journal on Optimization,",
            "year": 2013
        },
        {
            "authors": [
                "F. Huang",
                "S. Gao",
                "J. Pei",
                "H. Huang"
            ],
            "title": "Momentum-based policy gradient methods",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "D.K. Jha",
                "A.U. Raghunathan",
                "D. Romeres"
            ],
            "title": "Quasi-newton trust region policy optimization",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "C. Jin",
                "R. Ge",
                "P. Netrapalli",
                "S.M. Kakade",
                "M.I. Jordan"
            ],
            "title": "How to escape saddle points efficiently",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "R. Johnson",
                "T. Zhang"
            ],
            "title": "Accelerating stochastic gradient descent using predictive variance reduction",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "S.M. Kakade"
            ],
            "title": "A natural policy gradient",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "J.M. Kohler",
                "A. Lucchi"
            ],
            "title": "Sub-sampled cubic regularization for non-convex optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "V. Konda",
                "J. Tsitsiklis"
            ],
            "title": "Actor-critic algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "G. Lan"
            ],
            "title": "Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes",
            "venue": "Mathematical programming,",
            "year": 2022
        },
        {
            "authors": [
                "T.P. Lillicrap",
                "J.J. Hunt",
                "A. Pritzel",
                "N. Heess",
                "T. Erez",
                "Y. Tassa",
                "D. Silver",
                "D. Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Y. Liu",
                "K. Zhang",
                "T. Basar",
                "W. Yin"
            ],
            "title": "An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "L. Mackey",
                "M.I. Jordan",
                "R.Y. Chen",
                "B. Farrell",
                "J.A. Tropp"
            ],
            "title": "Matrix concentration inequalities via the method of exchangeable pairs",
            "venue": "The Annals of Probability,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Nesterov",
                "B.T. Polyak"
            ],
            "title": "Cubic regularization of newton method and its global performance",
            "venue": "Mathematical Programming,",
            "year": 2006
        },
        {
            "authors": [
                "L.M. Nguyen",
                "J. Liu",
                "K. Scheinberg",
                "M. Tak\u00e1\u010d. Sarah"
            ],
            "title": "A novel method for machine learning problems using stochastic recursive gradient",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "M. Papini",
                "D. Binaghi",
                "G. Canonaco",
                "M. Pirotta",
                "M. Restelli"
            ],
            "title": "Stochastic variance-reduced policy gradient",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga",
                "A. Desmaison",
                "A. Kopf",
                "E. Yang",
                "Z. DeVito",
                "M. Raison",
                "A. Tejani",
                "S. Chilamkurthy",
                "B. Steiner",
                "L. Fang",
                "J. Bai",
                "S. Chintala"
            ],
            "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "N. Pham",
                "L. Nguyen",
                "D. Phan",
                "P.H. Nguyen",
                "M. Dijk",
                "Q. Tran-Dinh"
            ],
            "title": "A hybrid stochastic policy gradient algorithm for reinforcement learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "J. Schulman",
                "S. Levine",
                "P. Abbeel",
                "M. Jordan",
                "P. Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "J. Schulman",
                "P. Moritz",
                "S. Levine",
                "M. Jordan",
                "P. Abbeel"
            ],
            "title": "High-dimensional continuous control using generalized advantage estimation",
            "venue": "arXiv preprint arXiv:1506.02438,",
            "year": 2015
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Z. Shen",
                "A. Ribeiro",
                "H. Hassani",
                "H. Qian",
                "C. Mi"
            ],
            "title": "Hessian aided policy gradient",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "D. Silver",
                "G. Lever",
                "N. Heess",
                "T. Degris",
                "D. Wierstra",
                "M. Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "T. Steihaug"
            ],
            "title": "The conjugate gradient method and trust regions in large scale optimization",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 1983
        },
        {
            "authors": [
                "A.J. Sutton",
                "S.J. Duval",
                "R. Tweedie",
                "K.R. Abrams",
                "D.R. Jones"
            ],
            "title": "Empirical assessment of effect of publication",
            "venue": "bias on meta-analyses. Bmj,",
            "year": 2000
        },
        {
            "authors": [
                "R.S. Sutton",
                "D. McAllester",
                "S. Singh",
                "Y. Mansour"
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "E. Todorov",
                "T. Erez",
                "Y. Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "N. Tripuraneni",
                "M. Stern",
                "C. Jin",
                "J. Regier",
                "M.I. Jordan"
            ],
            "title": "Stochastic cubic regularization for fast nonconvex optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "P. Wang",
                "H. Wang",
                "N. Zheng"
            ],
            "title": "Stochastic cubic-regularized policy gradient method",
            "venue": "Knowledge-Based Systems,",
            "year": 2022
        },
        {
            "authors": [
                "R.J. Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "C. Wu",
                "A. Rajeswaran",
                "Y. Duan",
                "V. Kumar",
                "A.M. Bayen",
                "S. Kakade",
                "I. Mordatch",
                "P. Abbeel"
            ],
            "title": "Variance reduction for policy gradient with action-dependent factorized baselines",
            "venue": "arXiv preprint arXiv:1803.07246,",
            "year": 2018
        },
        {
            "authors": [
                "P. Xu",
                "F. Gao",
                "Q. Gu"
            ],
            "title": "Sample efficient policy gradient methods with recursive variance reduction",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "L. Yang",
                "Q. Zheng",
                "G. Pan"
            ],
            "title": "Sample complexity of policy gradient finding second-order stationary points",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ye"
            ],
            "title": "Second order optimization algorithms",
            "venue": "i. https://web.stanford.edu/class/msande311/lecture12",
            "year": 2022
        },
        {
            "authors": [
                "H. Yuan",
                "X. Lian",
                "J. Liu",
                "Y. Zhou"
            ],
            "title": "Stochastic recursive momentum for policy gradient methods",
            "venue": "arXiv preprint arXiv:2003.04302,",
            "year": 2020
        },
        {
            "authors": [
                "R. Yuan",
                "R.M. Gower",
                "A. Lazaric"
            ],
            "title": "A general sample complexity analysis of vanilla policy gradient",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Y.-x. Yuan"
            ],
            "title": "Recent advances in trust region algorithms",
            "venue": "Mathematical Programming,",
            "year": 2015
        },
        {
            "authors": [
                "C. Zhang",
                "D. Ge",
                "B. Jiang",
                "Y. Ye"
            ],
            "title": "Drsom: A dimension reduced second-order method and preliminary analyses",
            "venue": "arXiv preprint arXiv:2208.00208,",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "C. Ni",
                "C. Szepesvari",
                "M. Wang"
            ],
            "title": "On the convergence and sample efficiency of variance-reduced policy gradient method",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "K. Zhang",
                "A. Koppel",
                "H. Zhu",
                "T. Basar"
            ],
            "title": "Global convergence of policy gradient methods to (almost) locally optimal policies",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 2020
        },
        {
            "authors": [
                "D. Zhou",
                "P. Xu",
                "Q. Gu"
            ],
            "title": "Stochastic nested variance reduction for nonconvex optimization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "B VtVtHtVtVt"
            ],
            "title": "Detailed proofs B.1 Proof of Lemma 3.7 The proof can be found in Yuan et al. (2022), which is the best result though under a weaker expected-LS regularity. We know that our gradient estimator (PGT",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The policy gradient (PG) method, pioneered by Williams (1992), is a widely used approach for finding the optimal policy in reinforcement learning (RL). The main idea behind PG is to directly maximize the total reward by using the (stochastic) gradient of the cumulative rewards. PG is particularly useful for high dimensional continuous state and action spaces due to its ease in implementation. In recent years, the PG method has gained much attention due to its significant empirical successes in a variety of challenging RL applications (Lillicrap et al., 2015; Silver et al., 2014).\nDespite its wide application and popularity, there is a growing concern about the high variance of classical PG method Williams (1992) and its variants Sutton et al. (2000); Baxter and Bartlett (2001). To address this issue, various approaches such as trust region method (Schulman et al., 2015a), proximal policy optimization (Schulman et al., 2017) and natural PG (Kakade, 2001) have been proposed to further improve the algorithm robustness and efficiency. Motivated by the variance reduction technique developed in stochastic optimization, (e.g. Johnson and Zhang (2013); Fang et al. (2018); Zhou et al. (2020)), recent work (Papini et al., 2018; Xu et al., 2020; Shen et al., 2019) have developed new variance-reduced policy gradient\n\u2217Jinsong Liu and Chenghan Xie contribute equally and are listed in alphabetical order \u2020liujinsong@163.sufe.edu.cn \u202120307130043@fudan.edu.cn \u00a7qideng@sufe.edu.cn \u00b6ge.dongdong@mail.shufe.edu.cn \u2016yyye@stanford.edu\nar X\niv :2\n30 1.\n12 17\n4v 1\n[ m\nat h.\nO C\nmethods for model-free RL. In particular, Xu et al. (2020); Shen et al. (2019) obtain O( \u22123) sample complexity1 for finding an -first order stationary point, which improves the O( \u22124) complexity of vanilla PG method (Williams, 1992) by a factor of O( \u22121). We address that all those variance-reduced PG methods only have guaranteed convergence to an first-order stationary point ( -FOSP), which, due to the non-convexity of RL objective, is insufficient to ensure global optimality. By exploiting the objective landscape in policy optimization, much recent efforts have been directed towards finding global solutions beyond first-order stationary convergence. For example, see Agarwal et al. (2021); Bhandari and Russo (2019); Cen et al. (2022); Lan (2022) on global convergence of various PG methods and Zhang et al. (2021); Liu et al. (2020) for the variance-reduced PG methods. Despite much progress, the global optimality can be achieved only for special parameterized settings such as softmax tabular policy and Fisher-non-degenerate policy, or when function approximation error is controllable (Yuan et al., 2022).\nFor the more general setting, Zhang et al. (2020); Yang et al. (2021) have established the convergence of PG methods to second-order stationary point (SOSP), which successfully exclude saddle point solutions that have indefinite Hessian. This research line is driven by the recent progress on developing efficient (stochastic) gradient methods to escape saddle-point solution (Daneshmand et al., 2018; Jin et al., 2017). In addition to the first order methods, second-order methods, such as cubic regularized Newton method (Nesterov and Polyak, 2006; Cartis et al., 2011; Tripuraneni et al., 2018; Kohler and Lucchi, 2017) and trust region method Yuan (2015); Curtis et al. (2017); Curtis and Shi (2020), are known to have provable finite time convergence to SOSP solutions in nonconvex (and stochastic) optimization. However, a challenge to the second-order methods is that they involve some nontrivial subproblems which require either matrix decomposition or external iterative solvers to obtain desired solutions. For example, see (Nesterov and Polyak, 2006; Agarwal et al., 2017; Nocedal and Wright, 1999). To alleviate this issue, Wang et al. (2022) proposed a new stochastic cubic-regularized policy gradient method (SCP-PG) which only involves the cubic regularized Newton subproblem when converging to FOSP. While SCP-PG can save the additional Hessian-vector product from time to time, it has an unsatisfactory sample complexity of O( \u22127/2), which is worse than the optimal rate of O( \u22123) (Arjevani et al., 2020).\nIn this paper, we aim to address some remaining issues in designing efficient second-order method for model-free policy optimization. Our work is motivated by the recent work in trust region optimization, and particularly, the dimension reduced second-order method (DRSOM) for nonconvex optimization (Ye, 2022; Zhang et al., 2022). DRSOM has a significant advantage in that it solves a relatively simple 2-dimensional trust region subproblem without the need of external solvers, resulting a computation overheads on a par with gradient descent methods. Building on this research direction, we develop several new stochastic second-order methods for policy optimizations. Our main contribution can be summarized as the following aspects.\n\u2022 First, we propose a stochastic second-order method for policy optimization (DR-SOPO), based on the recently proposed dimension-reduced trust region method. We show that the proposed method achieves a convergence rate of O( \u22123.5) for approximating a first order stationary point and a second-order stationary point in certain subspace. Our method only involves a cheap dimension reduced subproblem and hence bypasses the computation burden in solving a full dimensional quadratic program for trust region method. As a consequence, the computational overheads of DR-SOPO is comparable to that of policy gradient.\n\u2022 Second, to further enhance the efficiency of DR-SOPO, we propose a dimension and variance-reduced second-order method for policy optimization (DVR-SOPO). DVR-SOPO incorporates a Hessian-aided variance reduction technique (Shen et al., 2019) to DR-SOPO. The variance technique significantly improves the complexity in terms of the stochastic gradient queries, which is the dominating term in the complexity bound. We show that DVR-SOPO further improves the convergence rate of DR-SOPO to O( \u22123).\n\u2022 Finally, we provide some preliminary experiments to demonstrate the empirical advantage of our proposed methods against policy gradient method and variance-reduced policy gradient method. Our\n1For sample complexity, we mean the total number of samples involved in the estimation of gradient and hessian-vector product\ntheoretical analysis and empirical study show the great potential of directly using second-order information in policy optimization and reinforcement learning.\nComparison with TRPO Prior to our work, Schulman et al. (2015a) proposed the trust region policy optimization (TRPO) method, which has received great popularity in RL. However, it is important to note that TRPO is a first-order method that uses KL divergence to enforce optimization stability. This approach fundamentally differs from our work, which draw inspiration from trust region method in numerical optimization. Typically, a classic trust region method involves `p-ball constraints to improve the convergence of second-order method. Recently, Jha et al. (2020) replaced the linear objective in TRPO by quadratic approximation, and proposed to solve the modified TRPO by Quasi-Newton methods. However, their work did not provide any theoretical guarantee for the proposed method."
        },
        {
            "heading": "1.1 More on Related Work",
            "text": "We give a more detailed review over two research directions in policy optimization that are closely related to our work: the study on the convergence to SOSP and the variance reduction technique for the PG methods.\nConvergence to SOSP. Zhang et al. (2020) appears to be the first study showing that PG can escape FOSP and reach high-order stationary solutions. Specifically, they proposed a random horizon rollout for unbiased estimation of policy gradient. Furthermore, equipped with a periodically enlarged stepsize and the correlated negative curvature technique, they show that the a modified PG method an O( \u22129) sample complexity for converging to an ( , \u221a )-SOSP. In a follow-up work Yang et al. (2021), the authors improved the sample complexity to O( \u22129/2) and extended the SOSP analysis to extensive policy optimization algorithms, though under a restrictive objective structure assumption. A more recent work (Wang et al., 2022) proposed to use stochastic cubic Newton method for policy optimization and further improved the sample complexity to O( \u22127/2).\nVariance-reduction. Recent work in policy optimization has made a strong effort to apply variance reduction (VR) techniques, inspired by their success in the oblivious stochastic setting, such as SARAH (Nguyen et al., 2017) and SPIDER (Fang et al., 2018). Hessian-aided VR (Shen et al., 2019) is one of the examples that succeed in non-oblivious setting. Besides, Xu et al. (2020) proposes a SRVR-PG method based on SARAH and use important sampling techniques to deal with the distribution shift, while Pham et al. (2020) provides a single-loop version by a hybrid approach. Based on a novel gradient truncation technique, Zhang et al. (2021) successfully removes a strong assumption on the variance bound of important sampling in earlier work. All of the above algorithms match the sample complexity of O( \u22123). Motivated by the success of stochastic recursive momentum (i.e. STORM, Cutkosky and Orabona (2019)), Huang et al. (2020); Yuan et al. (2020) propose new STORM-like PG methods that blend momentum in the updates. These methods have the same O( \u22123) complexity but do not require alternating between large and small batch sizes.\nPaper structure Our paper proceeds as follows. Section 2 introduces notations and background in policy optimization. Section 3 presents the dimension reduced second-order method and its convergence analysis. Section 4 presents the dimension and variance-reduced method based on the Hessian-aid VR technique. Section 5 elucidates more details about the implementation of trust region procedure. Section 6 conducts empirical study to show the advantage of our proposed methods. We draw conclusion in Section 7 and leave technical proof in the appendix sections."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Notation For a square matrix A \u2208 Rn\u00d7n, we define norm for matrix as \u2016A\u2016 = \u221a \u03bbM , where \u03bbM is the eigenvalue of ATA with biggest absolute value. For a vector v \u2208 Rn, we use \u2016v\u2016 to express the standard Euclidean norm. \u2016v\u2016Q := \u221a vTQv where Q is a positive-definite matrix.\nConsider the Markov decision process M = {S,A,P, r, \u03b3}. Here, S is the state space, A is the action space, P (sh+1 | sh, ah) is the transition kernel, r : S \u00d7 A \u2192 R is the regret/cost function2 and \u03b3 \u2208 [0, 1) is a discount function. The agent\u2019s behavior is modeled by policy function \u03c0, and \u03c0(a | s) is the density of action a \u2208 A given the state s \u2208 S. We describe the policy \u03c0\u03b8 to reflect the fact the policy function is parameterized by a vector \u03b8 \u2208 Rd. Let s0 follow from the initial distribution \u03c1 (\u00b7). Let H be the length of truncated trajectory, and p (\u03c4 ;\u03c0\u03b8) be the density of trajectory \u03c4 = (s0, a0, . . . , sH\u22121, aH\u22121) following from the MDP: p(\u03c4 ;\u03c0\u03b8) := \u03c1(s0) \u220fH\u22121 h=0 P(sh+1 | sh, ah)\u03c0\u03b8(ah | sh). For brevity, we use the notation p(\u03c4 ;\u03c0\u03b8) and p(\u03c4 ; \u03b8) interchangeably. We consider the accumulated discounted regret:\nR(\u03c4) := H\u22121\u2211 h=0 \u03b3hr(sh, ah).\nWe remark that for more general objective with infinite horizon, one can set H = O(log( \u22121)) and view R(\u03c4) as a truncated estimator. See more discussion in Appendix D. Our goal is to solve the following policy optimization problem which minimizes the expected discounted trajectory regret/cost\nmin \u03b8\u2208Rd\nJ(\u03b8) := E\u03c4 [R(\u03c4)] = \u222b R(\u03c4) p(\u03c4 ;\u03c0\u03b8) d\u03c4 (1)\nNote that the above problem can be viewed as nonconvex stochastic optimization (e.g. Ghadimi and Lan (2013)). We say that a solution \u03b8 is an (\u03b51, \u03b52)-second-order stationary point ((\u03b51, \u03b52)-SOSP) if i) \u2016\u2207J(\u03b8)\u2016 \u2264 \u03b51 and ii) \u03bbmin(\u22072J(\u03b8)) \u2265 \u2212\u03b52. Here \u03bbmin denotes the smallest eigenvalue. Moreover, \u03b8 is an \u03b51-first-order stationary point (\u03b51-FOSP) if only condition i) holds. An unbiased estimator of the gradient \u2207J(\u03b8) is given by\ng(\u03b8; \u03c4) := H\u22121\u2211 h=0 \u03a8h(\u03c4)\u2207 log \u03c0\u03b8(ah | sh) (2)\nwhere we denote \u03a8h(\u03c4) = \u2211H\u22121 i=h \u03b3\nir(si, ai). Due to \u2207J(\u03b8) = E\u03c4\u223cp(\u00b7;\u03b8)[g(\u03b8; \u03c4)] we can derive an unbiased estimator of the Hessian \u22072J(\u03b8) by\nH(\u03b8; \u03c4) := \u2207g(\u03b8; \u03c4) + g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T. (3) LetM be a set of i.i.d. trajectories sampled from density p (\u00b7;\u03c0\u03b8). We denote g(\u03b8;M) = 1|M| \u2211 \u03c4\u2208M g(\u03b8; \u03c4)\nand H(\u03b8;M) = 1|M| \u2211 \u03c4\u2208MH(\u03b8; \u03c4)."
        },
        {
            "heading": "3 DR-SOPO",
            "text": ""
        },
        {
            "heading": "3.1 Algorithm",
            "text": "We first present the dimension-reduced second-order method for policy optimization (DR-SOPO), which extends dimension-reduced trust region technique in DRSOM to the non-oblivious stochastic setting. The novel ingredient in DRSOM is to determine the descent step by involving a two-dimensional trust-region subproblem, which is much simpler than the full dimensional quadratic program in standard trust region method. More specifically, let dt = \u03b8t\u2212 \u03b8t\u22121, gt = g(\u03b8t;Mg), Ht = H(\u03b8t;MH), instead of updating \u03b8 by the gradient descent step, we update \u03b8t+1 = \u03b8t \u2212 \u03b11t gt + \u03b12tdt, where the step size \u03b1t = (\u03b11t , \u03b12t )T is determined by solving the following dimension-reduced trust-region (DRTR) problem:\nmin \u03b1\u2208R2\nmt(\u03b1) := J (\u03b8t) + c T t \u03b1+\n1 2 \u03b1TQt\u03b1\ns.t. \u2016\u03b1\u2016Gt \u2264 \u2206, (4)\n2We use regret as opposed to the reward function in standard MDP literature. Our development is more aligned with optimization literature and results in minimization as opposed to maximization problem.\nTable 1: Comparison of different policy optimization algorithms. We ignore the logarithmic terms, for simplicity.\nAlgorithms Conditions Guarantee Sample complexity REINFORCE (Williams, 1992) Assumption 3.2,3.3 FOSP O( \u22124)\nSRVR-PG (Xu et al., 2020) Assumption 3.2,3.3 Var[ \u220f i\u22650 \u03c0\u03b80 (ai|si) \u03c0\u03b8t (ai|si) ] <\u221e FOSP O( \u22123) MBPG (Huang et al., 2020) Assumption 3.2,3.3Bounded variance of IM samplings FOSP O( \u22123) TSIVR-PG (Zhang et al., 2021) Assumption 3.2,3.3 FOSP O( \u22123) HAPG (Shen et al., 2019) Assumption 3.2,3.3 FOSP O( \u22123)\nMRPG (Zhang et al., 2020) Assumption 3.2,3.3, 3.4Positive-definite Fisher information SOSP O( \u22129)\nYang et al. (2021) Assumption 3.2,3.3, 3.4All saddle points are strict SOSP O( \u2212 9 2 )\nSCR-PG Wang et al. (2022) Assumption 3.2,3.3 3.4Bounded variance of IM samplings SOSP O( \u2212 7 2 )\nDR-SOPO Assumption 3.2,3.3 3.4 3.5 SOSPS O( \u2212 7 2 ) DVR-SOPO Assumption 3.2,3.3 3.4 3.5 SOSPS O( \u22123)\nwith\nQt = [ gTt Htgt \u2212dTt Htgt \u2212dTt Htgt dTt Htdt ] \u2208 S2,\nct := ( \u2212\u2016gt\u20162 gTt dt ) , Gt = [ gTt gt \u2212gTt dt \u2212gTt dt dTt dt ] ,\nand \u2016\u03b1\u2016Gt = \u221a \u03b1TGt\u03b1.\nThe two-dimensional subproblem (4) has a closed-form solution, which can be found in Appendix A. Note that while DRTR conceptually utilizes the curvature information, it can be implemented without explicitly computing the Hessian. In fact, we only require two additional Hessian-vector products to formulate (4). Hence, DRTR only incurs a much lower computation cost than standard trust region method Nocedal and Wright (1999).\nMoreover, the next lemma implies that while problem (4) is a two-dimensional trust region model, it can be equivalently transformed into a \u201cfull-scale\u201d trust-region problem associated with projected Hessian H\u0303t, which is pivotal to our theoretical analysis.\nLemma 3.1. The subproblem (4) is equivalent to\nmin d\u2208Rn\nm\u0303t(d) := J (\u03b8t) + g T t d+\n1 2 dTH\u0303td\ns.t. \u2016d\u2016 \u2264 \u2206t, (5)\nwhere H\u0303t = VtV Tt HtVtV Tt and Vt is the orthonormal bases for Lt := span {gt, dt}.\nFor proof, see Zhang et al. (2022). Therefore, H\u0303t can be viewed as an approximated Hessian matrix in the inexact Newton method, and DRTR may similarly be regarded as a cheap quasi-Newton method. For the ease of notation, let \u2207\u0303J(\u03b8t) = VtV Tt \u2207J(\u03b8t)VtV Tt be the projected Hessian onto the subspace Lt.\nWith gradient estimator (2) and Hessian estimator (3), we present the basic DR-SOPO in Algorithm 1, in which we use a fixed trust region radius to make the complexity analysis more concise. A more practical implementation of DR-SOPO will be presented in Algorithm 3, which dynamically adjusts the trust region radius and gets better practical performance.\nAlgorithm 1 Basic DR-SOPO 1: Given T , \u2206 2: for t = 1, . . . , T do 3: Collect sample trajectoriesMg and compute gt 4: Collect sample trajectoriesMH and compute Ht 5: Compute stepsize \u03b11, \u03b12 by solving the DRTR problem (4) 6: Update: \u03b8t+1 \u2190 \u03b8t \u2212 \u03b11gt + \u03b12dt 7: end for"
        },
        {
            "heading": "3.2 Complexity analysis",
            "text": "DR-SOPO can find a second-order stationary point in subspace (( , \u221a )-SOSPS), which is defined as \u03b8 such that \u2016\u2207J(\u03b8)\u2016 \u2264 c1 \u00b7 , \u03bbmin(\u2207\u03032J(\u03b8)) \u2265 \u2212c2 \u00b7 \u221a ,\nwhere \u2207\u03032J(\u03b8) is the projection of \u22072J(\u03b8) in the particular searching space. Next, we will show that DRSOPO converges to a ( , \u221a )-SOSPS with a total sample complexity of O( \u22123.5).\nBefore developing the convergence analysis, we need to make a few assumptions.\nAssumption 3.2. There exists a constant R > 0 such that |r(s, a)| \u2264 R for all a \u2208 A and s \u2208 S.\nAssumption 3.3 (Expected Lipschitzness and smoothness). There exist constants G > 0 and L > 0 such that E\u03c4 [\u2016\u2207 log \u03c0\u03b8(a | s)\u20164] \u2264 G4, E\u03c4 [\u2016\u22072 log \u03c0\u03b8(a | s)\u20162] \u2264 L2, for any choice of parameter \u03b8 and state-action pair (s, a).\nAssumption 3.4. There exists M > 0 such that for any \u03b81, \u03b82 \u2208 Rn\n||\u22072J(\u03b81)\u2212\u22072J(\u03b82)|| \u2264M ||\u03b81 \u2212 \u03b82||\nAssumption 3.5. There exists a constant C > 0 such that\n||(\u22072J(\u03b8t)\u2212 \u2207\u03032J(\u03b8t))dt+1|| \u2264MC\u2016dt+1\u20162\nRemark 3.6. Both Assumption 3.2 and 3.3 are fairly standard in the RL literature. Note that, however, 3.3 is weaker than the standard LS assumptions which are defined for \u2207 log(\u03c0\u03b8(a | s)) and \u22072 log \u03c0\u03b8(a | s) pointwisely. (e.g. Papini et al. (2018); Shen et al. (2019)). More detailed comparison can be found in Yuan et al. (2022). Assumption 3.4 on Lipschitz Hessian is standard for second-order methods, and for RL, M can be deduced by some other regularity conditions. For example, see Zhang et al. (2020). Assumption 3.5 is required for the convergence analysis of DRSOM Zhang et al. (2022), and is commonly used in the literature (Dennis and Mor\u00e9, 1977; Cartis et al., 2011).\nIn view of Assumption 3.3, we characterize some properties of the stochastic gradient and stochastic Hessian, which paves the way for our convergence analysis of the stochastic algorithms.\nLemma 3.7. Under Assumptions 3.2 and 3.3, we have for all \u03b8 \u2208 Rd,\nE\u03c4 [\u2016g(\u03b8; \u03c4)\u2212\u2207J(\u03b8)\u20162] \u2264 G2R2\n(1\u2212 \u03b3)3 := G2g,\nE\u03c4 [\u2016H(\u03b8; \u03c4)\u2212\u22072J(\u03b8)\u20162] \u2264 H4G4R2\n(1\u2212 \u03b3)2 +\nL2R2\n(1\u2212 \u03b3)4 := G2H .\nNote that our bound is slightly better than the one in Shen et al. (2019), though under weaker Assumption 3.3. Moreover, based on the fact that future policy at time t\u2032 can\u2019t affect reward at time t if t < t\u2032, we can construct a variance-reduced unbiased Hessian estimator, whose variance bound doesn\u2019t depend on H. More details can be found in B.2. Next, we derive variance bounds on the sampling gradient and Hessian in Algorithm 1.\nLemma 3.8 (Variance bounds on stochastic estimators). In Algorithm 1, by setting |Mg| = 144G2g 2 , we have\nEt [ \u2016gt \u2212\u2207J (\u03b8t)\u20162 ] \u2264\n2\n144M2 .\nMoreover, by setting |MH | = 22\u00d724 2G2H log(n) , we have\nEt [ \u2225\u2225Ht \u2212\u22072J (\u03b8t)\u2225\u22252 ] \u2264\n242 .\nwhere Et denotes the expectation conditioned on all the randomness before t-th iteration.\nBased on Assumption 3.5, we obtain a regularity condition such that Hessian estimator Ht agrees with H\u0303t along the directions dt+1 formed by past iterates \u03b8t.\nLemma 3.9. Under Assumption 3.5, then we have\nEt [ \u2016(Ht \u2212 H\u0303t)dt+1\u2016 ] \u2264MC\u0303\u22062,\nwhere C\u0303 = C + 124 and dt \u2264 \u2206 = 2 \u221a M .\nThe following lemma implies a sufficient descent property in solving the trust region subproblem.\nLemma 3.10 (Model reduction). At the t-th iteration, let dt+1 and \u03bbt be the optimal primal and dual solution of (5). We have the following amount of decrease on m\u0303t:\nm\u0303t (dt+1)\u2212 m\u0303t(0) = \u2212 1\n2 \u03bbt\u2016dt+1\u20162.\nWith all the tools at our hands, we derive the main convergence property of the DR-SOPO algorithm in the following theorem.\nTheorem 3.11 (Convergence rate of DR-SOPO). Suppose Assumptions 3.2-3.5 hold. Let \u2206t = \u2206 = 2 \u221a\nM\nand \u2206J be a constant number that s.t. \u2206J \u2265 J(\u03b80) \u2212 J\u2217, by setting Mg = 144G2g 2 , |MH | = 22\u00d7242G2H log(d) , T = 24M 2\u2206J\n3 2\nin Algorithm 1, we have\nE[\u2016\u2207J (\u03b8t\u0304)\u2016] \u2264 (3 + 4C\u0303)\nM , E[\u03bbmin(\u2207\u03032J(\u03b8t\u0304))] \u2265 \u22123\n\u221a ,\nwhere t\u0304 is sampled from {1, . . . , T} uniformly at random. Moreover, with probability at least 78 , we have\n\u2016\u2207J (\u03b8t\u0304)\u2016 \u2264 (12 + 16C\u0303)\nM .\nUsing the above result, we develop the sample complexity of Algorithm 1 to approximate second-order stationary condition.\nCorollary 3.12 (Sample complexity of DR-SOPO). Under all the assumptions and parameter settings in Theorem 3.11, Algorithm 1 returns a point x\u0302 such that E[\u2016\u2207J(x\u0302)\u2016] \u2264 (3+4C\u0303)M , E[\u03bbmin(\u2207\u0303 2J(x\u0302)] \u2265 \u22123 \u221a after performing at most\nO\n( \u2206JM\n2G2g 3.5 + \u2206JM 2G2H 2.5\n) (6)\ngradient and Hessian-vector product queries."
        },
        {
            "heading": "4 DVR-SOPO",
            "text": ""
        },
        {
            "heading": "4.1 Algorithm",
            "text": "Note that the sample complexity (6) of Algorithm 1 is dominated by the component contributed by the stochastic gradient queries. In this section, we further improve the complexity of DR-SOPO by variance reduction. By incorporating the Hessian-aided variance reduction technique (HAVR, Shen et al. (2019)), we develop a dimension and variance-reduced second-order method for policy optimization, dubbed DVR-SOPO, which obtains the optimal O( \u22123) worst-case complexity bound.\nVariance reduction We first introduce the variance reduction technique, which helps to construct a more sample-efficient gradient estimator gt. Let {\u03b8s}ts=0 be the solution sequence. The gradient \u2207J(\u03b8t) can be written in a path-integral form: \u2207J (\u03b8t) = \u2207J (\u03b80) + \u2211t s=1 [ \u2207J (\u03b8s) \u2212 \u2207J (\u03b8s\u22121) ] . Let \u03bes be an unbiased estimator for the gradient difference\u2207J (\u03b8s)\u2212\u2207J (\u03b8s\u22121). We can recursively construct the following estimator for \u2207J (\u03b8t) :\ngt = { g (\u03b8t;M0) mod(t, q) = 0, gt\u22121 + \u03bet mod(t, q) 6= 0\nwhereM0 is a mini-batch of trajectories sampled from p (\u00b7 | \u03c0\u03b8) and q is a given epoch length. In other words, we directly estimate the gradient using a mini-batch M0 every p iterations, and we maintain an unbiased estimate by recursively adding the correction term \u03bet to the current estimate gt\u22121 in the other iterations.\nConstruction of \u03bet. We next describe the key idea of HAVR. Due to the non-oblivious stochastic setting of RL (which means that the distribution of random variable \u03c4 is also influenced by independent variable \u03b8), we can\u2019t have access to unbiased samples at \u03b8t and \u03b8t\u22121 simultaneously as classical SVRG does. Therefore, we solve it by utilizing Hessian information: from the Taylor\u2019s expansion, the gradient difference can be written as\n\u2207J (\u03b8t)\u2212\u2207J (\u03b8t\u22121) = \u222b 1\n0\n[ \u22072J(\u03b8(a)) \u00b7 v ] da\n= [\u222b 1 0 \u22072J(\u03b8(a))da ] \u00b7 v,\n(7)\nwhere we denote \u03b8(a) := a\u03b8t+(1\u2212a)\u03b8t\u22121 and v := \u03b8t\u2212\u03b8t\u22121. Note that the integral in (7) can be understood as the expectation Ea [ \u22072J(\u03b8(a)) ] , where a is a random variable uniformly sampled in between [0, 1]. Hence, with our unbiased Hessian estimator (3), we can rewrite the gradient difference as\n\u2207J (\u03b8t)\u2212\u2207J (\u03b8t\u22121) = Ea\u223cU [0,1],\u03c4(a)\u223cp(\u00b7;\u03c0\u03b8(a))[H(\u03b8(a); \u03c4(a))] \u00b7 v\n(8)\nLet M\u0302g = {(a, \u03c4(a))} be set of samples, we can construct the estimator \u03bet by\n\u03bet = 1\n|M\u0302g| \u2211 (a,\u03c4(a))\u2208M\u0302g H(\u03b8(a); \u03c4(a)) \u00b7 v (9)\nComparison with other VR techniques We emphasize that other variance-reduced techniques, such as SRVR-PG, can also be applied in our methods to improve the estimation of \u2207J(\u03b8). However, HAVR appears to have some advantages. In order to construct an unbiased estimator of (9), one relies on the assumption of expected LS, which is weaker than the point-wise LS often imposed on standard VR methods. Moreover, HAVR does not need to assume the bounded variance of importance sampling, which is also a strong assumption imposed by the standard VR techniques.\nBased on the variance reduction technique, we develop the basic DVR-SOPO algorithm in Algorithm 2. A more detailed comparison of the sample complexity with other state-of-the-art policy gradient methods in\nTable 1. Similar to the development of DR-SOPO, we will present a more practical version of DVR-SOPO in Algorithm 4.\nAlgorithm 2 Basic DVR-SOPO 1: Given T , \u2206, q 2: for t = 1, . . . , T do 3: if mod (t, q) = 0 then 4: Compute gt based on sampled trajectoriesM0 5: else 6: Compute \u03bet based on sampled trajectories M\u0302g and update:\ngt = gt\u22121 + \u03bet\n7: end if 8: Collect samples trajectoriesMH and construct Ht 9: Compute stepsize \u03b11, \u03b12 by solving the DRTR problem (4)\n10: Update: \u03b8t+1 \u2190 \u03b8t \u2212 \u03b11gt + \u03b12dt 11: end for"
        },
        {
            "heading": "4.2 Complexity Analysis",
            "text": "DVR-SOPO relies on the same assumptions of DR-SOPO. With the help of HAVR, we can bound the variance of gradient estimator more sample-efficiently.\nLemma 4.1 (Gradient variance bound for DVR-SOPO). Recall the definition of gt in the Algorithm 2. By setting \u2264 G 2 H\n4 , q = 1 8 1/2 , |M\u0302g| = 288G\n2 H\nM2 3/2 , and |M0| =\n288G2g 2 , we have\nE [ \u2016gt \u2212\u2207J (\u03b8t)\u20162 ] \u2264\n2\n144M2 .\nTheorem 4.2 (Convergence rate of DVR-SOPO). Suppose Assumptions 3.2-3.5 hold. Let \u2206t = \u2206 = 2 \u221a\nM\nand \u2206J be a constant number that s.t. \u2206J \u2265 J(\u03b80) \u2212 J\u2217. If we set \u2264 G 2 H 4 , q = 1 8 1/2 , |M\u0302g| = 288G 2 H M2 3/2 , |M0| = 288G2g 2 , |MH | = 22\u00d7242G2H log(d) , and T = 24M2\u2206J\n3 2\nin Algorithm 2, then we have\nE[\u2016\u2207J (\u03b8t\u0304)\u2016] \u2264 (3 + 4C\u0303)\nM , E[\u03bbmin(\u2207\u03032J(\u03b8t\u0304))] \u2265 \u22123\n\u221a ,\nwhere t\u0304 is uniformly sampled from {1, . . . , T}. Moreover, with probability at least 78 , we have\n\u2016\u2207J (\u03b8t\u0304)\u2016 \u2264 (12 + 16C\u0303)\nM .\nCorollary 4.3 (Sample complexity of DVR-SOPO). Under all the assumptions and parameter settings in Theorem 4.2, Algorithm 2 returns a point x\u0302 such that E[\u2016\u2207J(x\u0302)\u2016] \u2264 (3+4C\u0303)M , E[\u03bbmin(\u2207\u0303 2J(x\u0302)] \u2265 \u22123 \u221a after performing at most\nO\n( \u2206J(8M 2G2g +G 2 H)\n3 +\n\u2206JM 2G2H\n2.5 ) gradient and Hessian-vector product queries.\nProof. In view of Lemma 4.1 and 3.8, for each iteration of DVR-SOPO, we need N1 = 288(8G2g+\nG2H M2 )\n3/2 samples\nfor gradient estimator gt and N2 = 22\u00b7576G2H log(d)\nsamples for Hessian estimator Ht. Following Theorem 4.2, the total number of samples required by DVR-SOPO is\n(N1 +N2)T = O (\u2206J(8M2G2g +G2H)\n3 +\n\u2206JM 2G2H log(d)\n2.5\n) ."
        },
        {
            "heading": "5 Practical Implementation",
            "text": "To achieve better empirical performance, we describe more practical versions of DR-SOPO and DVR-SOPO that dynamically adjust the trust region radius \u2206t. Consider the reduction ratio for m\u03b1t (4) at iterate \u03b8t\n\u03c1t := J (\u03b8t)\u2212 J (\u03b8t + dt+1) mt(0)\u2212mt (\u03b1t) . (10)\nIf \u03c1t is too small, our quadratic model is somehow inaccurate, which prompts us to reduce \u2206t. In practice, it is difficult to adjust properly \u2206t in (4). To resolve this issue, we consider the following radius-free problem:\n\u03b2\u03b1(\u03bbt) = min \u03b1\u2208R2\nJ (\u03b8t) + c T t \u03b1+\n1 2 \u03b1TQt\u03b1+ \u03bbt\u2016\u03b1\u20162Gt (11)\nas an alternative to (4). Due to the KKT condition, \u2206t is implicitly defined by \u03bbt, and we can properly adjust \u03bbt to solve \u03b2\u03b1(\u03bbt) and get sensible amount of decrease. This strategy has been proven effective in Zhang et al. (2022) and hence is used in our implementation. The details of practical DR-SOPO and DVR-SOPO are presented in Appendix E.\nA notable portion of the literature has focused on deriving a better choice of \u03a8h(\u03c4) in order to reduce the variance in estimating the policy gradient \u2207J(\u03b8). Conventional approaches include actor-critic algorithms (Bhatnagar et al., 2007; Konda and Tsitsiklis, 1999), and adding baselines (Wu et al., 2018). The Generalized Advantage Estimation (GAE) proposed by Schulman et al. (2015b) is one of the best ways to approximate the advantage function. We emphasize that all these refined advantage estimators can be directly incorporated to our method by placing \u03a8h(\u03c4) correspondingly. Our experiments use GAE(\u03b3, 1), then \u03a8h(\u03c4) becomes \u03a8h(\u03c4) = \u2211H\u22121 i=h \u03b3 ir(si, ai)\u2212 b(sh), where b is the linear baseline."
        },
        {
            "heading": "6 Numerical Results",
            "text": "In this section, we evaluate the performance of DR-SOPO and DVR-SOPO and compare them with REINFORCE (Sutton et al., 1999) and HAPG (Shen et al., 2019). We choose four Mujoco environments (Todorov et al., 2012): Walker2d-v2, Swimmer-v2, HalfCheetah-v2 and Ant-v2. We implement the four algorithms based on the Garage library garage contributors (2019) and PyTorch Paszke et al. (2019).\nFor all the environments, we use deep Gaussian policy, where the mean and variance are parameterized by a fully-connected neural network. We normalize the gradient estimator of all four algorithms for fair comparison. We also initialize all algorithms with the same random policy parameters. We repeat the experiment 10 times to reduce the impact of randomness and plot the mean and variance, using the same batch size for all algorithms. Note that the two variance-reduced algorithms DVR-SOPO and HAPG are double-loop procedures. We set the period and batch size of inner loop to be the same. For the two secondorder algorithms DR-SOPO and DVR-SOPO, we set the additional hyper-parameters to be the same. More details of model architecture and hyper-parameter settings are shown in Appendix F.\nWe use the number of system probes (the number of state transitions) instead of the number of trajectories to measure the sample complexity. System probes is a better criterion since different trajectories might have different number of system probes when a failure flag returned from the environment. To be consistent with\nprevious empirical study, we measure the algorithm performance using the average episode return, which is the negation of the cost/regret. Hence the higher return indicates better result.\nThe experiment results are shown in Fig. 1. First, we observe that DR-SOPO consistently outperform REINFORCE in all four environments, and achieving significantly better final average return in Walker2d, HalfCheetah and Ant. Moreover, we observe similar comparison results between DVR-SOPO and HAPG, as DVR-SOPO converges faster in all four environments and achieves significantly better final average return on Walker2d and HalfCheetah. These experiment results indeed confirm the advantage of using second-order information. Next, we compare the performance of two second-order algorithms DR-SOPO and DVR-SOPO. In Swimmer, their convergence rates are similar, but DVR-SOPO yields better final average reward. In Walker2d, they yield similar final average reward but DVR-SOPO converge faster than DR-SOPO at the beginning. In HalfCheetah and Ant, DVR-SOPO not only converge faster but yield better final average reward than DR-SOPO. The experiment results suggest that variance reduction does improve the performance of second-order algorithms, which align with our theoretical analysis."
        },
        {
            "heading": "7 Discussion",
            "text": "In this paper, we propose several efficient second-order methods, namely DR-SOPO and DVR-SOPO, for policy optimization. We show that DR-SOPO obtains an O( \u22123.5) complexity for reaching first order stationary condition and a subspace second-order stationary condition. DVR-SOPO further improves the rate to O( \u22123) based on the recently proposed variance reduction technique. Our methods have the distinct advantage of only requiring the computation of gradient and Hessian-vector product in each iteration, without the need to solve much complicated quadratic subproblems. As a result, they can be implemented with efficiency comparable to standard PG methods.\nIt is worth noting that while we mainly focus on the dimension-reduced model, it is easy to extend our analysis to develop a \"full-dimension\u201d stochastic trust region algorithm that solves the standard trust region subproblem instead of the reduced problem (4). Due to the page limits, we give more detail analysis in Appendix section C.\nOne interesting direction is to further exploit the connection between our methods and TRPO. It would be interesting to see how to apply our algorithm to TRPO if the objective is replaced by a more accurate\nquadratic approximation. Additionally, it would be valuable to investigate the performance of our methods for certain parameterized policy when global optimality is guaranteed."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is partially supported by the National Natural Science Foundation of China (NSFC) [Grant NSFC72150001, 72225009, 11831002]. The authors would like to thank Chuwen Zhang for the discussion on DRSOM and suggestions for the experiments."
        },
        {
            "heading": "A Optimal condition and solution for trust region problems",
            "text": "We introduce the widely known optimal conditions for trust region methods. The vector \u03b1t is the global solution to DRTR problem (4) if it is feasible and there exists a Lagrange multiplier \u03bbt \u2265 0 such that (\u03b1t, \u03bbt) is the solution to the following equations:\n(Qt + \u03bbGt)\u03b1+ ct = 0, Qt + \u03bbGt 0, \u03bb (\u2206\u2212 \u2016\u03b1\u2016Gt) = 0. (12)\nThen by the optimal condition (12), we have the closed form solution of \u03b1t:\n\u03b1t = \u2212(Qt + \u03bbtGt)\u22121ct.\nSince \u03b1 only has two dimensions, it can be easily solved numerically. Furthermore, recalling Lemma 3.1, by constructing dt+1 = Vt\u03b1t, we can prove that dt+1 is the solution to the full-scale problem (5) such that(\nH\u0303t + \u03bbtI ) dt+1 +Gt = 0, H\u0303t + \u03bbtI 0, \u03bbt (\u2016dt+1\u2016 \u2212\u2206t) = 0, (13)\nwhere H\u0303t = VtVtTHtVtVtT."
        },
        {
            "heading": "B Detailed proofs",
            "text": ""
        },
        {
            "heading": "B.1 Proof of Lemma 3.7",
            "text": "The proof can be found in Yuan et al. (2022), which is the best result though under a weaker expected-LS regularity. We know that our gradient estimator (PGT estimator, (Sutton et al., 2000)) is equivalent to GPOMDP estimator Baxter and Bartlett (2001):\ng(\u03b8; \u03c4) = H\u22121\u2211 h=0 ( h\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8 (at | st) )( \u03b3hr (sh, ah) ) So we will also use the GPOMDP estimator definition if needed."
        },
        {
            "heading": "Proof.",
            "text": "E\u03c4 [ \u2016g(\u03b8; \u03c4)\u20162 ] =E\u03c4 \u2225\u2225\u2225\u2225\u2225 H\u22121\u2211 t=0 \u03b3t/2r(st, at)\u03b3 t/2 ( t\u2211 k=0 \u2207\u03b8 log \u03c0\u03b8 (ak | sk) )\u2225\u2225\u2225\u2225\u2225 2 \n\u2264 E\u03c4 (H\u22121\u2211 t=0 \u03b3tr(st, at) 2 )H\u22121\u2211 k=0 \u03b3k \u2225\u2225\u2225\u2225\u2225 k\u2211 k\u2032=0 \u2207\u03b8 log \u03c0\u03b8 (ak\u2032 | sk\u2032) \u2225\u2225\u2225\u2225\u2225 2 \n\u2663 \u2264 R\n2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 k=0 \u03b3k E\u03c4 \u2225\u2225\u2225\u2225\u2225 k\u2211 k\u2032=0 \u2207\u03b8 log \u03c0\u03b8 (ak\u2032 | sk\u2032) \u2225\u2225\u2225\u2225\u2225 2 \n(\u2666) =\nR2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 k=0 \u03b3k k\u2211 k\u2032=0 E\u03c4 [ \u2016\u2207\u03b8 log \u03c0\u03b8 (ak\u2032 | sk\u2032)\u20162 ] \u2264 G 2R2\n1\u2212 \u03b3 \u00b7 H\u22121\u2211 k=0 \u03b3k(k + 1)\n\u2264 G 2R2\n(1\u2212 \u03b3)3 := G2g,\nwhere (\u2663) uses the Assumption 3.2, and (\u2666) is due to the fact that for any h 6= h\u2032: E\u03c4 [ \u2207\u03b8 log \u03c0\u03b8 (ah | sh))T(\u2207\u03b8 log \u03c0\u03b8 (ah\u2032 | sh\u2032)) ] = 0. (14)\nTo derive the variance-reduced Hessian estimator and bound its variance, note that\nE\u03c4 [\u2016H(\u03b8; \u03c4)\u20162] \u2264 2E[\u2016\u2207\u03b8g(\u03b8; \u03c4)\u20162] + 2E\u03c4 [\u2016g(\u03b8; \u03c4)\u2207\u03b8 log p(\u03c4 | \u03b8)T\u20162].\nPlugging the GPOMDP definition of g(\u03b8; \u03c4) in this term yields\nE\u03c4 [ \u2016\u2207g(\u03b8; \u03c4)\u20162 ] \u2264 E\u03c4 (H\u22121\u2211 h=0 \u03b3hR2) H\u22121\u2211 h=0 \u03b3h \u2225\u2225\u2225\u2225\u2225 h\u2211 t\u2032=0 \u22072\u03b8 log \u03c0\u03b8 (at\u2032 | st\u2032) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 R 2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 h=0 \u03b3h E\u03c4 \u2225\u2225\u2225\u2225\u2225 h\u2211 t\u2032=0 \u22072\u03b8 log \u03c0\u03b8 (at\u2032 | st\u2032) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 R 2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 h=0 \u03b3h(h+ 1) h\u2211 t\u2032=0 E\u03c4 [\u2225\u2225\u22072\u03b8 log \u03c0\u03b8 (at\u2032 | st\u2032)\u2225\u22252]\n\u2264 L 2R2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 h=0 \u03b3h(h+ 1)2 \u2264 2L 2R2\n(1\u2212 \u03b3)4 .\nFor the ease in notation, let us denote zt = \u2207\u03b8 log \u03c0\u03b8 (at | st). It follows that\nE\u03c4 [\u2225\u2225g(\u03b8; \u03c4)(\u2207\u03b8 log p(\u03c4 | \u03b8))T\u2225\u22252]\n= E\u03c4 \u2225\u2225\u2225\u2225\u2225 H\u22121\u2211 h=0 \u03b3hr (sh, ah) ( h\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8 (at | st) )( H\u22121\u2211 t\u2032=0 (\u2207\u03b8 log \u03c0\u03b8 (at\u2032 | st\u2032))T )\u2225\u2225\u2225\u2225\u2225 2 \n\u2666 \u2264 E\u03c4 [( H\u22121\u2211 h=0 \u03b3hR2 )( H\u22121\u2211 h=0 \u03b3h \u2225\u2225\u2225 h\u2211 t=0 zt H\u22121\u2211 t\u2032=0 zTt \u2225\u2225\u22252)]\n\u2264 E\u03c4 [( H\u22121\u2211 h=0 \u03b3hR2 )( H\u22121\u2211 h=0 \u03b3h ( h\u2211 t=0 \u2016zt\u2016 H\u22121\u2211 t\u2032=0 \u2016zt\u2032\u2016T )2)]\n\u2264 R 2\n1\u2212 \u03b3 E\u03c4 [ H\u22121\u2211 h=0 \u03b3h (H\u22121\u2211 t=0 \u2016zt\u2016 H\u22121\u2211 t\u2032=0 \u2016zt\u2032\u2016T )2]\n] \u2264 R 2\n1\u2212 \u03b3 H\u22121\u2211 h=0 \u03b3hH4 E\u03c4 [\u2016zt\u20164]\n\u2264 G 4R2\n1\u2212 \u03b3 H\u22121\u2211 h=0 \u03b3hH4 \u2264 G 4H4R2 (1\u2212 \u03b3)2\nwhere (\u2666) is due to Cauchy-Schwarz inequality, and (]) is due to the fact that for any matrix (or scalar)\nA0, A1, . . . An with dimension d1 \u00b7 d2, we have\nE[\u2016( n\u2211 i=0 Ai n\u2211 i=0 ATi )\u20162] = E[\u2016 n\u2211 i=0 AiA T i + 2 \u2211 i6=j AiA T j \u20162]\n\u2264 E[( n\u2211 i=0 \u2016Ai\u20162 + 2 \u2211 i6=j \u2016Ai\u2016\u2016Aj\u2016)2]\n\u2264 E[ ( (n+ 1) n\u2211 i=0 \u2016Ai\u20162 )2 ]\n= (n+ 1)2 \u00b7 E[( n\u2211 i=0 \u2016Ai\u20162)2] = (n+ 1)3 \u00b7 E[ n\u2211 i=0 \u2016Ai\u20164].\n(15)\nHence, we have E\u03c4 \u2016H(\u03b8; \u03c4)\u20162 \u2264 2E\u03c4 [\u2225\u2225g(\u03b8; \u03c4)(\u2207\u03b8 log p(\u03c4 | \u03b8))T\u2225\u22252]+2E\u03c4 [\u2016\u2207g(\u03b8; \u03c4)\u20162] \u2264 2H4G4R2(1\u2212 \u03b3)2 + 4L2R2\n(1\u2212 \u03b3)4 := G2H .\nFinally, using E\u03c4 [(X \u2212 E\u03c4 [X])2] \u2264 E\u03c4 [X2] for all random variable X, we have\nE\u03c4 \u2016g(\u03b8; \u03c4)\u2212\u2207J(\u03b8)\u20162 \u2264 E\u03c4 \u2016g(\u03b8; \u03c4)\u20162 \u2264 G2g, E\u03c4 \u2016H(\u03b8; \u03c4)\u2212\u22072J(\u03b8)\u20162 \u2264 E\u03c4 \u2016H(\u03b8; \u03c4)\u20162 \u2264 G2H .\nB.2 More discussion about Hessian estimator\n1. A Variance-reduced unbiased estimator: We claim here that by constructing a variance reduced Hessian estimator, the variance of H(\u03b8; \u03c4) can actually by bounded more tightly, which is without parameter H. To do so, note that\nE\u03c4 [ g(\u03b8; \u03c4)(\u2207\u03b8 log p(\u03c4 | \u03b8))T ] = E\u03c4\n[ H\u22121\u2211 h=0 ( h\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8 (at | st) ) \u03b3hr (sh, ah) (\u2207\u03b8 log p(\u03c4 | \u03b8))T ]\n\u2666 = E\u03c4 [ H\u22121\u2211 h=0 ( h\u2211 t=0 \u2207\u03b8 log \u03c0\u03b8 (at | st) ) \u03b3hr (sh, ah) ( H\u22121\u2211 t\u2032=0 (\u2207\u03b8 log \u03c0\u03b8 (at\u2032 | st\u2032))T )]\n\u2665 = E\u03c4 [ H\u22121\u2211 h=0 uh \u00b7 uTh\u03b3hr (sh, ah)] ] ,\nwhere uh = \u2211h t=0\u2207\u03b8 log \u03c0\u03b8 (at | st), (\u2666) is due to \u2207\u03b8P (st\u2032+1 | st\u2032 , at\u2032) = 0 and (\u2665) uses property (14).\nLet H \u2032(\u03b8; \u03c4) = \u2211H\u22121 h=0 \u03b3 hr (sh, ah)uh \u00b7uTh +\u2207g(\u03b8; \u03c4), it is also an unbiased estimator of \u22072J(\u03b8). Moreover,\nwe have\nE\u03c4 [\u2225\u2225\u2225H\u22121\u2211 h=0 \u03b3hr (sh, ah) (uh \u00b7 uTh ) \u2225\u2225\u22252]\n\u2666 \u2264 E\u03c4 [( H\u22121\u2211 h=0 \u03b3hR2 )( H\u22121\u2211 h=0 \u03b3h \u2225\u2225\u2225uh \u00b7 uTh\u2225\u2225\u22252 )]\n\u2264 R 2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 h=0 \u03b3h E\u03c4 [\u2225\u2225\u2225uh \u00b7 uTh\u2225\u2225\u22252]\n] \u2264 R 2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 h=0 \u03b3h(h+ 1)3 h\u2211 t=0 E\u03c4 [\u2225\u2225\u2207\u03b8 log \u03c0\u03b8 (at | st)\u2225\u22252]\n\u2264 G 4R2 1\u2212 \u03b3 \u00b7 H\u22121\u2211 h=0 \u03b3h(h+ 1)4 \u2264 24G 4R2\n(1\u2212 \u03b3)6 ,\nwhere (\u2666) is due to Cauchy-Schwarz inequality, and (]) is due to (15). Hence, we have\nE\u03c4 \u2016H \u2032(\u03b8; \u03c4)\u20162 \u2264 2E\u03c4 [\u2225\u2225\u2225H\u22121\u2211 h=0 \u03b3hr (sh, ah)utu T t \u2225\u2225\u22252]+ 2E\u03c4 [\u2016\u2207g(\u03b8; \u03c4)\u20162] \u2264 48G4R2 + 4L2R2(1\u2212 \u03b3)2 (1\u2212 \u03b3)6 := G\u2032H 2\nIn practice, this method requires parallelism and a significant amount of computing resources as it needs to be backpropagated multiple times (H times).\n2. Biased Hessian Estimation: Consider the biased estimator of \u22072J(\u03b8):\nH\u00b5(\u03b8; \u03c4) = \u2207g(\u03b8; \u03c4) + \u00b5g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T (16)\nNote that when \u00b5 = 1, this bound becomes the unbiased Hessian estimator. However, a biased Hessian estimator which has better practical variance property. We can further derive its MSE bound:\nE\u03c4 [\u2016H\u00b5(\u03b8; \u03c4)\u2212\u22072J(\u03b8)\u20162] = Var(H\u00b5(\u03b8; \u03c4)) + \u2016E\u03c4 [H\u00b5(\u03b8; \u03c4)]\u2212\u22072J(\u03b8)\u20162\n= Var(\u2207g(\u03b8; \u03c4) + \u00b5g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T) + \u2016E\u03c4 [(\u00b5\u2212 1)g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T]\u20162 = (\u00b5\u2212 1)2\u2016E\u03c4 [g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T]\u20162 + Var(\u2207g(\u03b8; \u03c4)) + \u00b52 Var [ g(\u03b8; \u03c4)(\u2207\u03b8 log p(\u03c4 | \u03b8))T ] + 2\u00b5E\u03c4 [ [\u2207g(\u03b8; \u03c4)\u2212 E\u03c4 [\u2207g(\u03b8; \u03c4)]] [ g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T \u2212 E\u03c4 [ g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T\n]]] = (1\u2212 2\u00b5)\u2016E\u03c4 [g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T]\u20162 + \u00b52 E\u03c4 [\u2016g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T\u20162] + Var(\u2207g(\u03b8; \u03c4))\n+ 2\u00b5E\u03c4 [ [\u2207g(\u03b8; \u03c4)\u2212 E\u03c4 [\u2207g(\u03b8; \u03c4)]] [ g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T \u2212 E\u03c4 [ g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T ]]] \u2264 2\u00b5 \u221a E\u03c4 [\u2016\u2207g(\u03b8; \u03c4)\u2212 E\u03c4 [\u2207g(\u03b8; \u03c4)] \u20162]E\u03c4 [\u2016g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T \u2212 E\u03c4 [g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T] \u20162]\n+ (\u00b5\u2212 1)2 E\u03c4 [\u2016g(\u03b8; \u03c4)\u2207 log p(\u03c4 ; \u03b8)T\u20162] + Var(\u2207g(\u03b8; \u03c4)) \u2264 E\u03c4 [ \u2016\u2207g(\u03b8; \u03c4)\u20162 ] + (\u00b5\u2212 1)2 E\u03c4 [\u2225\u2225g(\u03b8; \u03c4)(\u2207\u03b8 log p(\u03c4 | \u03b8))T\u2225\u22252] + 2\u00b5 \u221a E\u03c4 [\u2016\u2207g(\u03b8; \u03c4)\u20162]E\u03c4 [ \u2016g(\u03b8; \u03c4)(\u2207\u03b8 log p(\u03c4 | \u03b8))T\u20162\n] \u2264 2L 2R2\n(1\u2212 \u03b3)4 + (\u00b5\u2212 1)2G\n4H4R2\n(1\u2212 \u03b3)2 + 2\u00b5\n\u221a 2L2R2\n(1\u2212 \u03b3)4 G4H4R2 (1\u2212 \u03b3)2 .\nBy setting the value \u00b5 = 1\u2212 \u221a\n2L (1\u2212\u03b3)G2H2 , we obtain the tightest bound:\n2 \u221a 2LR2G2H2\n(1\u2212 \u03b3)3 .\nIn experiment, we find that using a biased estimator often leads to better performance, and hence adjust \u00b5 as a hyper-parameter."
        },
        {
            "heading": "B.3 Proof of Lemma 3.8",
            "text": "Proof.\nEt [ \u2016gt \u2212\u2207J (\u03b8t)\u20162 ] = Et [\u2225\u2225\u2225 1|Mg| \u2211 \u03c4\u2208Mg g(\u03b8, \u03c4)\u2212\u2207J(\u03b8t) \u2225\u2225\u22252]\n= Et [ \u2016 \u2211 \u03c4\u2208Mg g(\u03b8, \u03c4)\u2212 |Mg| \u00b7 \u2207 2J(\u03b8t)\u2016 ]\n|Mg|2 = Et [ \u2016g(\u03b8, \u03c4)\u2212\u2207J(\u03b8t)\u20162 ] |Mg|\n\u2264 2\n144M2 ,\nwhere the last equality is due to the fact that samples are independent and Mg = 144G2g 2 .\nAs for the variance bound of Hessian, it is a direct result of the following auxiliary lemma (whose proof can be found in Arjevani et al. (2020). We derive it here just for completeness) by setting Ai = H(\u03b8; \u03c4), B = \u22072J(\u03b8).\nLemma B.1. Let (Ai) m i=1 be a collection of i.i.d. matrices in Sn, with E [Ai] = B and E \u2016Ai \u2212B\u2016 2 \u2264 \u03c32. Then it holds that\nE \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 Ai \u2212B \u2225\u2225\u2225\u2225\u2225 2 \u2264 22\u03c3 2 log n m .\nProof: We drop the normalization by m throughout this proof. We first symmetrize. Observe that by Jensen\u2019s inequality we have\nE \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 Ai \u2212B \u2225\u2225\u2225\u2225\u2225 2 \u2264 EA EA\u2032 \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 Ai \u2212A\u2032i \u2225\u2225\u2225\u2225\u2225 2\n= EA EA\u2032 \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 (Ai \u2212B)\u2212 (A\u2032i \u2212B) \u2225\u2225\u2225\u2225\u2225 2\n= EA EA E \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 i ((Ai \u2212B)\u2212 (A\u2032i \u2212B)) \u2225\u2225\u2225\u2225\u2225 2 \u2264 4EA E \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 i (Ai \u2212B) \u2225\u2225\u2225\u2225\u2225 2 ,\nwhere (A\u2032)mi=1 is a sequence of independent copies of (Ai) m i=1 and ( i) m i=1 are Rademacher random variables. Henceforth we condition on A. Let p = log n, and let \u2016 \u00b7 \u2016Sp denote the Schatten p-norm. In what follows, we will use that for any matrix X, \u2016X\u2016 \u2264 \u2016X\u2016S2p \u2264 e1/2\u2016X\u2016. To begin, we have\nE \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 i (Ai \u2212B) \u2225\u2225\u2225\u2225\u2225 2 \u2264 E \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 i (Ai \u2212B) \u2225\u2225\u2225\u2225\u2225 2\nS2p\n\u2264 E \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 i (Ai \u2212B) \u2225\u2225\u2225\u2225\u2225 2p\nS2p\n1/p ,\nwhere the second inequality follows by Jensen. We now apply the matrix Khintchine inequality (Mackey et al. (2014), Corollary 7.4), which implies thatE\n\u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 i (Ai \u2212B) \u2225\u2225\u2225\u2225\u2225 2p\nS2p\n1/p \u2264 (2p\u2212 1) \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 (Ai \u2212B)2 \u2225\u2225\u2225\u2225\u2225 S2p \u2264 (2p\u2212 1) m\u2211 i=1 \u2016(Ai \u2212B)\u20162S2p\n\u2264 e(2p\u2212 1) m\u2211 i=1 \u2016(Ai \u2212B)\u20162 .\nPutting all the developments so far together and taking expectation with respect to A, we have\nE \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 Ai \u2212B \u2225\u2225\u2225\u2225\u2225 2 \u2264 4e(2p\u2212 1) m\u2211 i=1 EAi \u2016(Ai \u2212B)\u2016 2 \u2264 4e(2p\u2212 1)m\u03c32.\nTo obtain the final result we normalize by m2"
        },
        {
            "heading": "B.4 Proof of Lemma 3.9",
            "text": "Recalling \u2207\u03032J(\u03b8t) = VtVtT\u22072J(\u03b8t)VtVtT, we have"
        },
        {
            "heading": "Proof.",
            "text": "Et ||(Ht \u2212 H\u0303t)dt+1||\n\u2264 Et[||(\u22072J(\u03b8t)\u2212 \u2207\u03032J(\u03b8t))dt+1||+ ||(Ht \u2212\u22072J(\u03b8t))dt+1||+ ||VtVtT(\u22072J(\u03b8t)\u2212Ht)VtVtTdt+1||] (\u2666) \u2264 MC\u22062 + Et[||(Ht \u2212\u22072J(\u03b8t))dt+1||] + Et[\u2016VtVtT\u2016 \u00b7 \u2016(\u22072J(\u03b8t)\u2212Ht)dt+1||] (]) \u2264 MC\u22062 + 2Et[\u2016(\u22072J(\u03b8t)\u2212Ht)dt+1\u2016] (\\)\n\u2264 MC\u22062 + 2 \u00b7 \u221a\n24 \u2206 = MC\u0303\u22062,\nwhere (\u2666) is due to Assumption 3.5 and the fact that VtVtTdt+1 = dt+1; (]) is due to the fact that VtTVt = I and VtVtT has the same non-zero eigenvalue with VtTVt ; (\\) is due to Lemma 3.8 and the fact that dt+1 \u2264 \u2206t = \u2206 = 2 \u221a M . Also, C\u0303 = C + 1 24 ."
        },
        {
            "heading": "B.5 Proof of Theorem 3.11",
            "text": ""
        },
        {
            "heading": "Proof.",
            "text": "J(\u03b8t+1)\u2212 J(\u03b8t)\n\u2264 \u2207J(\u03b8t)Tdt+1 + 1\n2 dTt+1\u22072J(\u03b8t)dt+1 +\nM\n6 ||dt+1||3\n= gTt dt+1 + 1\n2 (dt+1)\nTHtdt+1 + (\u2207J(\u03b8t)\u2212 gt)Tdt+1 + 1\n2 (dt+1)\nT(\u22072J(\u03b8t)\u2212Ht)dt+1 + M\n6 ||dt+1||3\n(\u2666) \u2264 \u22121\n2 \u03bbt\u2016dt+1\u20162 + ||\u2207J(\u03b8t)\u2212 gt||\u2206t +\n1 2 \u2016(\u22072J(\u03b8t)\u2212Ht)\u2016\u22062t + M 6 ||dt+1||3,\n(17)\nwhere (\u2666) is due to Cauchy-Schwarz inequality and Lemma 3.10. At t-th iteration, if \u2016dt+1\u2016 = \u2206t = 2 \u221a\nM , then we can bound \u2016\u03bbtdt+1\u2016 by\n\u2016\u03bbtdt+1\u2016 \u2264 M\u221a (J(\u03b8t)\u2212 J(\u03b8t+1)) + \u2016\u2207J(\u03b8t)\u2212 gt\u2016+ 1\n2 \u2016\u22072J(\u03b8t)\u2212Ht\u2016\u2206t +\nM\n6 ||dt+1||2, (18)\nOtherwise, if dt+1 < \u2206t , then by the optimal condition of (4), \u03bbt = 0, so the upper bound (18) still holds. Therefore, we have\n||\u2207J(\u03b8t+1)|| (\\) \u2264 ||\u2207J(\u03b8t+1)\u2212\u2207J(\u03b8t)\u2212\u22072J(\u03b8t)dt+1||+ ||\u2207J(\u03b8t)\u2212 gt||\n+ ||(\u22072J(\u03b8t)\u2212Ht)dt+1||+ ||gt + H\u0303kdt+1||+ ||(H\u0303k \u2212Ht)dt+1|| (\u2666) \u2264 M\u2206 2\n2 + ||\u2207J(\u03b8t)\u2212 gt||+ \u2206||\u22072J(\u03b8t)\u2212Ht||+ \u2016gt + H\u0303kdt+1\u2016+MC\u0303\u22062\n(]) \u2264 M\u2206 2\n2 + ||\u2207J(\u03b8t)\u2212 gt||+ \u2206||\u22072J(\u03b8t)\u2212Ht||+ \u2016\u03bbtdt+1\u2016+MC\u0303\u22062,\nwhere (\\) is due to the triangle inequality, (\u2666) is due to Taylor approximation, Cauchy-Schwarz inequality and Lemma 3.9, (]) is due to the optimal condition of DRTR (13). Plugging the upper bound of \u2016\u03bbtdt+1\u2016 into the above equation, summing from t = 0 to T \u2212 1 and then taking the expectation on both sides, we obtain\n1\nT T\u22121\u2211 t=0 E[\u2016\u2207J(\u03b8t+1)\u2016]\n\u2264 M\u221a T E [ T\u22121\u2211 t=0 (J(\u03b8t)\u2212 J(\u03b8t+1)) ] + 2 T T\u22121\u2211 t=0 E[||\u2207J(\u03b8t)\u2212 gt||] + 3 2T T\u22121\u2211 t=0 \u2206E[||\u22072J(\u03b8t)\u2212Ht||] + 2 + 3C\u0303 3 M\u22062\n\u2264 M\u221a T (J(\u03b80)\u2212 J\u2217) + (71 + 96C\u0303) 24M ,\nwhere the last inequality is due to Lemma 3.8 and \u2206 = 2 \u221a\nM . Then we have the desired result by taking T = 24M 2\u2206J\n3 2\nand t\u0304 be uniformly sampled from 0, . . . , T \u2212 1.\nMoreover, by Markov inequality (Pr(X \u2265 a) \u2264 E[X]a for any non-negative random variable X), with probability at least 78 , we have\n\u2016\u2207J (\u03b8t\u0304+1)\u2016 \u2264 (12 + 16C\u0303)\nM .\nTo derive the second-order condition, from (17) and the optimal condition (12) of DRTR, we have\nE[J(\u03b8t+1)\u2212 J(\u03b8t)] \u2264 E [ \u22121\n2 \u03bbt\u2016dt+1\u20162\n] + E [ ||\u2207J(\u03b8t)\u2212 gt||\u2206t + 1\n2 \u2016(\u22072J(\u03b8t)\u2212Ht)\u2016\u22062t +\nM\n6 ||dt+1||3 ] \u2264 E[\u03bbt|\u03bbt > 0] \u00b7 Pr(\u2016\u03bbt\u2016 > 0) \u00b7 (\u2212 2\nM2 ) + E[\u03bbt|\u03bbt = 0] \u00b7 Pr(\u2016\u03bbt\u2016 = 0) \u00b7 (\u2212\n1 2 d2k) +\n5 3/2\n3M2\n= E[\u03bbt|\u03bbt > 0] \u00b7 Pr(\u2016\u03bbt\u2016 > 0) \u00b7 (\u2212 2\nM2 ) +\n5 3/2\n3M2\n= E[\u03bbt] \u00b7 (\u2212 2\nM2 ) +\n5 3/2 3M2 .\n(19)\nSumming from t = 0 to T \u2212 1, and plugging T into the equation, we have\n1 T E [ T\u2211 t=1 (J(\u03b8t)\u2212 J(\u03b8t\u22121)) ] \u2265 1 T \u00b7 (J\u2217 \u2212 J(\u03b80)) \u2265 \u2212 3/2 24M2 . (20)\nCombining (19) and (20), we have\nE [\u03bbt\u0304] \u2264 41\n48\n\u221a , (21)\nwhere t\u0304 is uniformly sampled from 0, . . . , T \u2212 1. By Assumption 3.4 and Lemma 3.8, we have\n\u221241 48 \u221a I \u2212E[\u03bbt\u0304]I E[H\u0303t\u0304] = E[Vt\u0304Vt\u0304THt\u0304+1Vt\u0304Vt\u0304T + H\u0303t\u0304 \u2212 Vt\u0304Vt\u0304THt\u0304+1Vt\u0304Vt\u0304T]\n= E[Vt\u0304Vt\u0304THt\u0304+1Vt\u0304Vt\u0304T + Vt\u0304Vt\u0304T (Ht\u0304 \u2212Ht\u0304+1)Vt\u0304Vt\u0304T] E[Vt\u0304Vt\u0304THt\u0304+1Vt\u0304Vt\u0304T + \u2225\u2225Vt\u0304Vt\u0304T (Ht\u0304 \u2212Ht\u0304+1)Vt\u0304Vt\u0304T\u2225\u2225 I] E[Vt\u0304Vt\u0304THt\u0304+1Vt\u0304Vt\u0304T] + E[\n\u2225\u2225Vt\u0304Vt\u0304T\u2225\u2225 \u2016Ht\u0304+1 \u2212Ht\u0304\u2016 \u2225\u2225Vt\u0304Vt\u0304T\u2225\u2225 I] = E[H\u0303t\u0304+1] + E[\u2016Ht\u0304+1 \u2212Ht\u0304\u2016 I] E[H\u0303t\u0304+1] + E[(\u2016Ht\u0304+1 \u2212\u22072J(\u03b8t\u0304+1)\u2016+M \u2016dt\u0304+1\u2016+ \u2016\u22072J(\u03b8t\u0304)\u2212Ht\u0304\u2016)I]\nE[H\u0303t\u0304+1] + 25\n12\n\u221a I,\nwhich indicates that E[\u03bbt\u0304+1] \u2264 14148 . By Lemma 3.8, we have\nE[\u2016\u2207\u03032J(\u03b8t\u0304+1)\u2212 H\u0303t\u0304+1\u2016] \u2264 E[\u2016Vt\u0304+1V Tt\u0304+1\u2016 \u00b7 \u2016\u2207 2J(\u03b8t\u0304+1)\u2212Ht\u0304+1\u2016 \u00b7 \u2016Vt\u0304V Tt\u0304+1\u2016] = E[\u2016\u2207 2J(\u03b8t\u0304+1)\u2212Ht\u0304+1\u2016] \u2264 1\n24\n\u221a\n\u21d0\u21d2 E[\u2207\u03032J(\u03b8t\u0304+1)] E[H\u0303t\u0304]\u2212 1\n24\n\u221a I\n\u21d0\u21d2 E[\u03bbmin(\u2207\u03032J(\u03b8t\u0304+1))] \u2265 E[\u03bbmin(H\u0303t\u0304+1)]\u2212 1\n24\n\u221a = \u2212E[\u03bbt\u0304+1]\u2212 1\n24\n\u221a \u2265 \u2212143\n48\n\u221a \u2265 \u22123 \u221a ."
        },
        {
            "heading": "B.6 Proof of Lemma 4.1",
            "text": "Proof. For ease of presentation, we consider t < q. The general case is a straightforward extension. Recall \u03b8(a) := a\u03b8t + (1\u2212 a)\u03b8t\u22121.\nEt \u2016gt \u2212\u2207J(\u03b8t)\u20162 = Et \u2016gt\u22121 + \u03bet \u2212\u2207J(\u03b8t)\u20162\n= Et \u2016\u03bet \u2212 (\u2207J(\u03b8t)\u2212\u2207J(\u03b8t\u22121)) + gt\u22121 \u2212\u2207J(\u03b8t\u22121)\u20162\n(\u2663) = Et \u2016\u03bet \u2212 (\u2207J(\u03b8t)\u2212\u2207J(\u03b8t\u22121))\u20162 + Et \u2016gt\u22121 \u2212\u2207J(\u03b8t\u22121)\u20162\n(22)\nwhere (\u2663) is due to the fact that \u03bet is conditionally independent of gt\u22121, \u03b8t\u22121.\nBy our construction (9), \u03bet is an unbiased estimator of \u2207J(\u03b8t)\u2212\u2207J(\u03b8t\u22121). Next we bound the variance\nof estimator \u03bet. Let us denote m = |M\u0302g| and consider samples a1, \u03c41, . . . , am, \u03c4m. Then we have\nEt \u2016\u03bet \u2212 (\u2207J(\u03b8t)\u2212\u2207J(\u03b8t\u22121))\u20162\n= E \u2225\u2225\u2225\u2225\u2225 ( 1 m m\u2211 i=1 H(\u03b8(ai), \u03c4(ai))\u2212 \u222b 1 0 \u22072J(\u03b8(a)) da ) \u00b7 v \u2225\u2225\u2225\u2225\u2225 2 \n= 1\nm2 m\u2211 i=1 E [\u2225\u2225\u2225\u2225(H(\u03b8(ai), \u03c4(ai))\u2212 \u222b 1 0 \u22072J(\u03b8(a)) da ) \u00b7 v \u2225\u2225\u2225\u22252 ]\n= 1\nm2 m\u2211 i=1 E [\u2225\u2225\u2225\u2225(H(\u03b8(ai), \u03c4(ai))\u2212\u22072J(\u03b8(ai)) + \u222b 1 0 ( \u22072J(\u03b8(ai))\u2212\u22072J(\u03b8(a)) ) da ) \u00b7 v \u2225\u2225\u2225\u22252 ]\n= 1\nm2 m\u2211 i=1 E [\u2225\u2225(H(\u03b8(ai), \u03c4(ai))\u2212\u22072J(\u03b8(ai))) \u00b7 v\u2225\u22252]+ 1 m2 m\u2211 i=1 E [\u2225\u2225\u2225\u2225\u222b 1 0 ( \u22072J(\u03b8(ai))\u2212\u22072J(\u03b8(a)) ) da \u00b7 v \u2225\u2225\u2225\u22252 ]\n= 1\nm2 m\u2211 i=1 E [\u2225\u2225H(\u03b8(ai), \u03c4(ai))\u2212\u22072J(\u03b8(ai))\u2225\u22252] \u2016v\u20162 + 1 m2 m\u2211 i=1 E [\u222b 1 0 \u2225\u2225\u22072J(\u03b8(ai))\u2212\u22072J(\u03b8(a))\u2225\u22252 da] \u2016v\u20162 \u2264 G 2 H\nm \u2016v\u20162 + M\n2\nm \u2016v\u20164,\nwhere the inequality uses the Hessian variance bound in Lemma 3.7 and the Hessian Lipschitzness (Assumption 3.4) to bound the two sums, respectively. Due to the trust region choice, we have \u2016v\u2016 = \u2016\u03b8t+1 \u2212 \u03b8t\u2016 \u2264 2 \u221a\nM . It follows that\nEt \u2016\u03bet \u2212 (\u2207J(\u03b8t)\u2212\u2207J(\u03b8t\u22121))\u20162 \u2264 4G2H\nmM2 +\n16 2\nmM2 .\nSince \u2264 G 2 H\n4 , by telescoping (22) and taking the expectation over all the randomness, we obtain\nE \u2016gt \u2212\u2207J (\u03b8t)\u20162 \u2264 8t \u00b7G2H \u2223\u2223\u2223M\u0302g\u2223\u2223\u2223M2 + E \u2016g0 \u2212\u2207J (\u03b80)\u20162 \u2264 8q \u00b7G2H \u2223\u2223\u2223M\u0302g\u2223\u2223\u2223M2 + G2g |M0| .\nBy setting q = 1 8 1/2 , \u2223\u2223\u2223M\u0302g\u2223\u2223\u2223 = 288G2HM2 3/2 , and |M0| = 288G2g 2 , the result of the lemma follows."
        },
        {
            "heading": "B.7 Proof of Theorem 4.2",
            "text": "The proof of Theorem 4.2 is nearly identical to that of Theorem 3.11. The only difference is that the bound for the variance of gradient estimator is based on Lemma 4.1."
        },
        {
            "heading": "C Full dimension trust region method for policy optimization",
            "text": "Our proposed dimension-reduced method aims to alleviate the high computation cost associated with solving the trust region subproblem in the whole space. However, it should be noted that by invoking the full dimension trust region subproblem, we can achieve a second-order stationary point (SOSP) in the whole space with a sample complexity of O( \u22123), although this method requires more Hessian-vector products in the subsolver.\nSpecifically, we need to solve the full-dimension trust region (FDTR) problem:\nmin \u03b1\u2208Rn\nmt(d) := J (\u03b8t) + g T t d+\n1 2 dTHtd\ns.t. \u2016d\u2016 \u2264 \u2206, (23)\nand the vector dt is the global solution to FDTR problem (23) if it is feasible and there exists a Lagrange multiplier xt \u2265 0 such that (dt, xt) is the solution to the following equations:\n(Ht + \u03bb) d+ gt = 0, Qt + xI 0, x (\u2206\u2212 \u2016d\u2016) = 0. (24)\nTo solve the full-dimension trust-region subproblem, the Steihaug-CG method (Steihaug, 1983) and dogleg method can be used. If we can solve the subproblem efficiently, by substituting \u03bbt of in the proof of Theorem 3.11 with xt and following a similar procedure, we can prove the following theorem:\nTheorem C.1 (Convergence rate of full-dimension SOPO). Suppose Assumptions 3.2-3.4 hold. Let \u2206t = \u2206 = 2 \u221a\nM and \u2206J be a constant number that s.t. \u2206J \u2265 J(\u03b80) \u2212 J \u2217, by setting Mg = 144G2g 2 , |MH | =\n22\u00d7242G2H log(d) , T = 24M2\u2206J\n3 2\nin Algorithm 1, we have\nE[\u2016\u2207J (\u03b8t\u0304)\u2016] \u2264 3\nM , E[\u03bbmin(\u22072J(\u03b8t\u0304))] \u2265 \u22122\n\u221a ,\nwhere t\u0304 is sampled from {1, . . . , T} uniformly at random. Moreover, with probability at least 78 , we have\n\u2016\u2207J (\u03b8t\u0304)\u2016 \u2264 12\nM .\nTheorem C.2 (Convergence rate of full-dimension VRSOPO). Suppose Assumptions 3.2-3.4 hold. Let \u2206t = \u2206 = 2 \u221a\nM and \u2206J be a constant number that s.t. \u2206J \u2265 J(\u03b80)\u2212J \u2217. If we set \u2264 G\n2 H\n4 , q = 1 8 1/2 , |M\u0302g| = 288G\n2 H\nM2 3/2 ,\n|M0| = 288G2g 2 , |MH | = 22\u00d7242G2H log(d) , and T = 24M2\u2206J\n3 2\nin Algorithm 2, then we have\nE[\u2016\u2207J (\u03b8t\u0304)\u2016] \u2264 3\nM , E[\u03bbmin(\u2207\u03032J(\u03b8t\u0304))] \u2265 \u22122\n\u221a ,\nwhere t\u0304 is uniformly sampled from {1, . . . , T}. Moreover, with probability at least 78 , we have\n\u2016\u2207J (\u03b8t\u0304)\u2016 \u2264 12\nM .\nD Infinite setting In the infinite setting, where the objective function J(\u03b8) is actually a truncated objective, and J\u221e(\u03b8) denotes the infinite setting objective. We claim that if we set H = O(log( \u22121)), then finding an SOSP for the truncated objective is sufficient for finding an SOSP for the infinite setting objective.\nLemma D.1. There exists D, D \u2032 such that for all \u03b8 \u2208 Rn, we have\n\u2016J\u221e(\u03b8)\u2212 J(\u03b8)\u2016 \u2264 R\n1\u2212 \u03b3 \u03b3H , \u2016\u2207J\u221e(\u03b8)\u2212\u2207J(\u03b8)\u2016 \u2264 D\u03b3H , \u2016\u22072J\u221e(\u03b8)\u2212\u22072J(\u03b8)\u2016 \u2264 D\n\u2032 \u03b3H .\nProof. The first two inequality can be found in Yuan et al. (2022), where D = GR1\u2212\u03b3 \u221a 1 1\u2212\u03b3 +H. For the last inequality, we have\n\u2016\u22072J\u221e(\u03b8)\u2212\u22072J(\u03b8)\u20162\n= \u2225\u2225\u2225\u2225\u2225E\u03c4 [ \u221e\u2211 h=H \u03b3hr (sh, ah)uh \u00b7 uTh + \u221e\u2211 h=H \u03b3hr (sh, ah) h\u2211 t\u2032=0 \u22072\u03b8 log \u03c0\u03b8(at\u2032 | st\u2032) ]\u2225\u2225\u2225\u2225\u2225 2\n\u2264 E\u03c4 \u2225\u2225\u2225\u2225\u2225 \u221e\u2211 h=H \u03b3hr (sh, ah)uh \u00b7 uTh \u2225\u2225\u2225\u2225\u2225 2 + E\u03c4 \u2225\u2225\u2225\u2225\u2225 \u221e\u2211 h=H \u03b3hr (sh, ah) h\u2211 t\u2032=0 \u22072\u03b8 log \u03c0\u03b8(at\u2032 | st\u2032) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 E\u03c4 [( \u221e\u2211 h=H \u03b3hR2 )( \u221e\u2211 h=H \u03b3h\u2016uh \u00b7 uTh \u20162 )] + E\u03c4 ( \u221e\u2211 h=H \u03b3hR2 ) \u221e\u2211 h=H \u03b3h \u2225\u2225\u2225\u2225\u2225 h\u2211 t\u2032=0 \u22072\u03b8 log \u03c0\u03b8(at\u2032 | st\u2032) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 \u03b3 HR2\n1\u2212 \u03b3 \u221e\u2211 h=H \u03b3h(h+ 1)3 h\u2211 t\u2032=0 E\u03c4 [ \u2016\u2207\u03b8 log \u03c0\u03b8 (at | st) \u20164 ] + \u03b3HR2 1\u2212 \u03b3 \u221e\u2211 h=H \u03b3h(h+ 1) h\u2211 t\u2032=0 E\u03c4 [ \u2016\u22072\u03b8 log \u03c0\u03b8(at\u2032 | st\u2032)\u20162 ] = \u03b32HR2G4\n1\u2212 \u03b3 \u221e\u2211 h=0 \u03b3h(h+ 1 +H)4 + \u03b32HR2L2 1\u2212 \u03b3 \u221e\u2211 h=0 \u03b3h(h+ 1 +H)2\n\u2264 \u03b3 2HR2G4\n(1\u2212 \u03b3)2\n[ 24\n(1\u2212 \u03b3)4 +\n24H\n(1\u2212 \u03b3)3 +\n12H2\n(1\u2212 \u03b3)2 +\n4H3\n(1\u2212 \u03b3) +H4\n] + \u03b32HR2L2\n(1\u2212 \u03b3)2\n[ 2\n(1\u2212 \u03b3)2 +\n2H\n(1\u2212 \u03b3) +H2\n] ,\nwhere the first inequality is due to Jensen\u2019s inequality, the second inequality is due to Cauchy-Schwarz inequality and the last inequality is due to Inequality of arithmetic and geometric means. It follows that\nD \u2032 = RG2\n1\u2212 \u03b3\n\u221a 24\n(1\u2212 \u03b3)4 +\n24H\n(1\u2212 \u03b3)3 +\n12H2\n(1\u2212 \u03b3)2 +\n4H3\n(1\u2212 \u03b3) +H4 +\nRL\n1\u2212 \u03b3\n\u221a 2\n(1\u2212 \u03b3)2 +\n2H\n(1\u2212 \u03b3) +H2."
        },
        {
            "heading": "E Practical versions of DR-SOPO and DVR-SOPO",
            "text": "Algorithm 3 Practical DR-SOPO algorithm 1: Given T , \u22061, \u2206max, \u03b7 2: for t = 1, . . . , T do 3: Collect sample trajectoriesMg and compute gt 4: Collect sample trajectoriesMH and compute Ht 5: Compute stepsize \u03b1 = (\u03b11, \u03b12) by solving the radius-free problem (11) 6: if \u2016\u03b1\u2016 > \u2206max then 7: \u03b1 = \u03b1/\u2016\u03b1\u2016 \u2217\u2206max 8: end if 9: Calculate \u03c1t by 10 10: if \u03c1k > \u03b7 then 11: Update: \u03b8t+1 \u2190 \u03b8t \u2212 \u03b11gt + \u03b12dt 12: else 13: Adjust the Lagrange multiplier \u03bbt 14: end if 15: end for 16: return \u03b8t\u0304, which is uniformly picked from {\u03b8t}t=1,\u00b7\u00b7\u00b7 ,T\nAlgorithm 4 Practical DVR-SOPO algorithm 1: Given T , \u22061, \u2206max, \u03b7, q 2: for t = 1, . . . , T do 3: if mod (t, q) = 0 then 4: Collect sample trajectoriesM0 and compute gt 5: else 6: Collect sample trajectories M\u0302g and compute \u03bet:\ngt = gt\u22121 + \u2206t\n7: end if 8: sample |MH | trajectories to construct Ht 9: Compute stepsize \u03b1 = (\u03b11, \u03b12) by solving the radius-free problem (11)\n10: if \u2016\u03b1\u2016 > \u2206max then 11: \u03b1 = \u03b1/\u2016\u03b1\u2016 \u2217\u2206max 12: end if 13: Calculate \u03c1t by 10 14: if \u03c1k > \u03b7 then 15: Update: \u03b8t+1 \u2190 \u03b8t \u2212 \u03b11gt + \u03b12dt 16: else 17: Adjust the lagrange multiplier \u03bbt 18: end if 19: end for 20: return \u03b8t\u0304, which is uniformly picked from {\u03b8t}t=1,\u00b7\u00b7\u00b7 ,T"
        },
        {
            "heading": "F Hyper-parameter Settings",
            "text": ""
        }
    ],
    "title": "Stochastic Dimension-reduced Second-order Methods for Policy Optimization\u2217",
    "year": 2023
}