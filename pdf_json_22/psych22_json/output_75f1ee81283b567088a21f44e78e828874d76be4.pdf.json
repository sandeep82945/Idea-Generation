{
    "abstractText": "Objective: This study sought to integrate the NIH Science of Behavior Change (SOBC) measures repository comprising measures of putative mechanisms with mechanisms of action (MoA) identified by the Human Behaviour-Change Project (HBCP). Design: Participants were 30 international experts recruited from professional networks and societies. In three anonymous virtual rounds, experts established consensus on hypothesized links between 26 MoAs and 44 self-report measures. Methods: In Round 1, experts completed a survey rating agreement with 84 pre-identified measure-MoA links and suggested new links. In Round 2, experts discussed 10 links in an online forum, including pre-identified links with <50% agreement and new links suggested by 20-50% of experts. In Round 3, experts completed a survey rating all links eligible for discussion in Round 2. Results: Twenty-seven experts completed Round 1, 23 completed Round 2, and 18 completed Round 3. In Round 1, 82 of 84 pre-identified links reached >50% agreement and 14 new links were suggested by >50% of experts. In Round 2, experts discussed measure-MoA links and measurement quality. In Round 3, 71 of 96 links reached >50% agreement. A total of 167 links reached >50% expert agreement, 33 of which reached >90% agreement. Conclusion: By identifying putative mechanisms (HBCP) for the 44 self-report measures (SOBC), this study advances the cumulation of scientific results and interoperability of resources to facilitate process research.",
    "authors": [],
    "id": "SP:e8f260c6f2db575aef57a421e9d00dafdfd5a826",
    "references": [
        {
            "authors": [
                "M. Allemand",
                "C. Fl\u00fcckiger"
            ],
            "title": "Personality change through digital-coaching interventions",
            "venue": "Current Directions in Psychological Science,",
            "year": 2022
        },
        {
            "authors": [
                "A.M. Burrell",
                "J.L. Allan",
                "D.M. Williams",
                "M. Johnston"
            ],
            "title": "What do self\u2010efficacy items measure? Examining the discriminant content validity of self\u2010efficacy items",
            "venue": "British Journal of Health Psychology,",
            "year": 2018
        },
        {
            "authors": [
                "R.N. Carey",
                "L.E. Connell",
                "M. Johnston",
                "A.J. Rothman",
                "M. De Bruin",
                "M.P. Kelly",
                "S. Michie"
            ],
            "title": "Behavior change techniques and their mechanisms of action: a synthesis of links described in published intervention literature",
            "venue": "Annals of Behavioral Medicine,",
            "year": 2019
        },
        {
            "authors": [
                "J.K. Flake",
                "E.I. Fried"
            ],
            "title": "Measurement schmeasurement: Questionable measurement practices and how to avoid them",
            "venue": "Advances in Methods and Practices in Psychological Science,",
            "year": 2020
        },
        {
            "authors": [
                "M. Johnston",
                "R.N. Carey",
                "L.E. Connell Bohlen",
                "D.W. Johnston",
                "A.J. Rothman",
                "M. de Bruin",
                "S. . . Michie"
            ],
            "title": "Development of an online tool for linking behavior change techniques and mechanisms of action based on triangulation of findings from literature synthesis and expert consensus. Translational Behavioral Medicine",
            "year": 2020
        },
        {
            "authors": [
                "M. Johnston",
                "R.N. Carey",
                "L.E. Connell Bohlen",
                "D.W. Johnston",
                "A.J. Rothman",
                "M. de Bruin",
                "S. . . Michie"
            ],
            "title": "Development of an online tool for linking behavior change techniques and mechanisms of action based on triangulation of findings from literature synthesis and expert consensus",
            "venue": "Translational Behavioral Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "M. Johnston",
                "D. Dixon",
                "J. Hart",
                "L. Glidewell",
                "C. Schr\u00f6der",
                "B. Pollard"
            ],
            "title": "Discriminant content validity: A quantitative methodology for assessing content of theory\u2010based measures, with illustrative applications",
            "venue": "British Journal of Health Psychology,",
            "year": 2014
        },
        {
            "authors": [
                "H.W. Marsh",
                "R. Pekrun",
                "P.D. Parker",
                "K. Murayama",
                "J. Guo",
                "T. Dicke",
                "A.K. Arens"
            ],
            "title": "The murky distinction between self-concept and self-efficacy: Beware of lurking jingle-jangle fallacies",
            "venue": "Journal of educational psychology,",
            "year": 2019
        },
        {
            "authors": [
                "S. Michie",
                "R. West",
                "A.N. Finnerty",
                "E. Norris",
                "A.J. Wright",
                "M.M. Marques",
                "J. . . Hastings"
            ],
            "title": "Representation of behaviour change interventions and their evaluation: Development of the Upper Level of the Behaviour Change Intervention Ontology",
            "venue": "Wellcome Open Research,",
            "year": 2020
        },
        {
            "authors": [
                "S. Michie",
                "C.E. Wood",
                "M. Johnston",
                "C. Abraham",
                "J. Francis",
                "W. Hardeman"
            ],
            "title": "Behaviour change techniques: the development and evaluation of a taxonomic method for reporting and describing behaviour change interventions (a suite of five studies involving consensus methods, randomised controlled trials and analysis of qualitative data)",
            "venue": "Health technology assessment,",
            "year": 2015
        },
        {
            "authors": [
                "T.L. Milfont",
                "R. Fischer"
            ],
            "title": "Testing measurement invariance across groups: Applications in crosscultural research",
            "venue": "International Journal of psychological research,",
            "year": 2010
        },
        {
            "authors": [
                "R. Nair",
                "R. Aggarwal",
                "D. Khanna"
            ],
            "title": "Methods of formal consensus in classification/diagnostic criteria and guideline development. Paper presented at the Seminars in arthritis and rheumatism",
            "year": 2011
        },
        {
            "authors": [
                "M. Riddle"
            ],
            "title": "News from the NIH: using an experimental medicine approach to facilitate translational research",
            "venue": "Science of Behavior Change (SOBC) Working Group",
            "year": 2015
        },
        {
            "authors": [
                "B.W. Roberts",
                "J. Luo",
                "D.A. Briley",
                "P.I. Chow",
                "R. Su",
                "P.L. Hill"
            ],
            "title": "A systematic review of personality trait change through intervention",
            "year": 2017
        },
        {
            "authors": [
                "P. Sheeran",
                "W. Klein",
                "A.J. Rothman"
            ],
            "title": "Health behavior change: Moving from observation to intervention",
            "venue": "Annual review of psychology,",
            "year": 2017
        },
        {
            "authors": [
                "I.P. Sinha",
                "R.L. Smyth",
                "P.R. Williamson"
            ],
            "title": "Using the Delphi technique to determine which outcomes to measure in clinical trials: recommendations for the future based on a systematic review of existing studies",
            "venue": "PLoS Med,",
            "year": 2011
        },
        {
            "authors": [
                "J.A. Sumner",
                "J.L. Birk",
                "T. Cornelius",
                "L. Derby",
                "D. Edmondson",
                "K.W. Davidson"
            ],
            "title": "The NIH Science of Behavior Change Mechanism-Focused Approach to Behavior Change Research. Paper presented at the PSYCHOSOMATIC MEDICINE",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Sumner",
                "R.N. Carey",
                "S. Michie",
                "M. Johnston",
                "D. Edmondson",
                "K.W. Davidson"
            ],
            "title": "Using rigorous methods to advance behaviour change science",
            "venue": "Nature human behaviour,",
            "year": 2018
        },
        {
            "authors": [
                "A.H. Van de Ven",
                "A.L. Delbecq"
            ],
            "title": "The nominal group as a research instrument for exploratory health studies",
            "venue": "American journal of public health,",
            "year": 1972
        },
        {
            "authors": [
                "J. Waggoner",
                "J.D. Carline",
                "S.J. Durning"
            ],
            "title": "Is there a consensus on consensus methodology? Descriptions and recommendations for future consensus research",
            "venue": "Academic Medicine,",
            "year": 2016
        },
        {
            "authors": [
                "H. Wickham",
                "W. Chang",
                "M.H. Wickham"
            ],
            "title": "Package \u2018ggplot2",
            "venue": "Create elegant data visualisations using the grammar of graphics. Version,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Objective: This study sought to integrate the NIH Science of Behavior Change (SOBC) measures repository comprising measures of putative mechanisms with mechanisms of action (MoA) identified by the Human Behaviour-Change Project (HBCP). Design: Participants were 30 international experts recruited from professional networks and societies. In three anonymous virtual rounds, experts established consensus on hypothesized links between 26 MoAs and 44 self-report measures. Methods: In Round 1, experts completed a survey rating agreement with 84 pre-identified measure-MoA links and suggested new links. In Round 2, experts discussed 10 links in an online forum, including pre-identified links with <50% agreement and new links suggested by 20-50% of experts. In Round 3, experts completed a survey rating all links eligible for discussion in Round 2. Results: Twenty-seven experts completed Round 1, 23 completed Round 2, and 18 completed Round 3. In Round 1, 82 of 84 pre-identified links reached >50% agreement and 14 new links were suggested by >50% of experts. In Round 2, experts discussed measure-MoA links and measurement quality. In Round 3, 71 of 96 links reached >50% agreement. A total of 167 links reached >50% expert agreement, 33 of which reached >90% agreement. Conclusion: By identifying putative mechanisms (HBCP) for the 44 self-report measures (SOBC), this study advances the cumulation of scientific results and interoperability of resources to facilitate process research. Keywords: behaviour, behaviour change, mechanisms, measurement\nBackground\nTo improve the rigor, reproducibility, and impact of behavioural interventions, research\nhas been conducted into the hypothesized mechanisms that may underlie intervention success. This includes two groups of researchers working in complementary areas, the US-based NIH Science of Behavior Change (SOBC) initiative and the UK-based Human Behaviour-Change Project (HBCP). This paper reports a collaboration between the two groups to identify measures associated with behavioural mechanisms of action.\nThe SOBC has adopted a mechanism-focused, experimental medicine approach to\nbehavioural research (Riddle & Science of Behavior Change (SOBC) Working Group, 2015; Sheeran, Klein, & Rothman, 2017; Sumner et al., 2019; Sumner et al., 2018), which emphasizes evaluating how changes in a mechanism lead to changes in the target behaviour. For example, a researcher might hypothesize a goal-setting intervention promotes physical activity by increasing self-efficacy and test whether increases in self-efficacy attributable to the intervention lead to increases in physical activity. One of the core pillars of this initiative was to develop an online, publicly available Measures Repository (Science Of Behavior Change (SOBC)) where researchers could upload candidate measures of hypothesized mechanisms that underlie behaviour change and share data on behavioural validation (e.g., does a change in the mechanism lead to a change in behaviour?). The first group of measures uploaded were focused on three, broad mechanistic domains: stress, self-regulation, and interpersonal processes (though the measures themselves were not necessarily developed to assess these specific mechanisms). Each uploaded measure includes supporting information (e.g., on scale development and previous validation), scoring procedures, Open Science Framework (OSF) documentation, and links to\nrelevant citations in multiple reference management software platforms via Google scholar (Science Of Behavior Change (SOBC)).\nWhereas SOBC researchers aimed to curate measures of mechanisms as one scientific\nresource for behaviour change research, researchers at the Human Behaviour-Change Project (HBCP) aimed to link behaviour change techniques (BCTs) used in intervention research with the mechanisms through which these techniques are hypothesized to operate (mechanisms of action; MoA) (Johnston et al., 2021). To achieve this, researchers conducted literature analysis (Carey et al., 2019), expert consensus (Bohlen et al.), and triangulated studies to systematically develop the putative MoAs for BCTs. This work was conducted using the most frequently used 74 BCTs within the 93-item Behaviour Change Technique Taxonomy v1 (Michie et al., 2015), such as Goal-Setting or Salience of Consequences, and 26 coded MoAs (e.g., Beliefs About Capabilities, Self-Image; Supplemental Appendix A) (Carey et al., 2019; Johnston et al., 2020). The MoAs were developed to represent the most commonly occurring MoAs across 83 theories of behaviour and behaviour change. Based on this work, HBCP scientists developed an interactive online resource, the Theory and Techniques Tool (TaTT), which details the strength of the support for (or against) a given hypothesized BCT-MoA link (Human Behaviour Change Project (HBCP)). The TaTT includes functionality for users to comment and provide additional information regarding each link.\nAdvancing science cumulatively is facilitated by compiling systematic knowledge about\nall parts of the causal chain linking interventions, mechanisms, and behaviours. Lack of attention to measurement of hypothesized mechanisms of change presents a significant barrier to rigorous and reproducible science. Rather than selecting measures on the basis of scientific properties (e.g., content validity, sensitivity to change, appropriate timescale), researchers often choose\nmeasures based on convenience, historical use, or face validity. Lack of attention to the correspondence between measures and mechanisms is a barrier to advancing behavioural science.\nCreating a systematic set of links between the SOBC repository\u2019s measures and the\nHBCP\u2019s ontology of hypothesized mechanisms of behaviour change helps to address this gap between measures and mechanisms. However, the efforts of SOBC and HBCP researchers do not correspond perfectly to each other. The SOBC Measures Repository was initially designed to address three, broad mechanisms \u2013 stress, self-regulation, and interpersonal processes \u2013 with a goal of determining whether a given measure could capture change in hypothesized mechanisms through a rigorous behavioural validation process (Science Of Behavior Change (SOBC)). The measures uploaded to the repository did not have to correspond perfectly to any one category. They might fit well under one mechanistic domain, two, or none of these three domains. The true correspondence of these measures to any specific mechanism(s) is thus untested. The HBCP aims to develop a comprehensive Behaviour Change Intervention Ontology (BCIO), including ontologies of BCTs and MoAs. The BCIO intends to encompass the current state of knowledge on behaviour change, including frequently occurring concepts (i.e., MoAs) across theories of behaviour and behaviour change. Thus, it follows the measures should conceptually map to at least one of the HBCP mechanistic domains (Human Behaviour Change Project (HBCP)).\nTo build upon the work already completed within each project, meant the collaboration\nneither started from a bottom-up (e.g., creating measures from scratch to correspond to each HBCP mechanisms) nor top-down (e.g., creating a novel ontology of mechanisms assessed in the SOBC repository) approach. To bring the project data together, this study used a modified expert consensus methodology to compile preliminary evidence for potential linkages between 44 self-\nreport measures of mechanisms from the SOBC Measures Repository and 26 MoAs identified by the HBCP.\nWe aimed to identify (i) MoAs that best represent measures in the SOBC Measures\nRepository and (ii) potential measures of the MoAs that are targeted by BCTs in the TaTT.\nMethods"
        },
        {
            "heading": "Participants",
            "text": "Participants were interdisciplinary experts recruited via listservs of professional and\nscientific networks and societies that focus on behaviour change intervention development, including Society of Behavioral Medicine (SBM), European Health Psychology Society (EHPS), the SOBC listserv, and the HBCP listserv. Interested participants were directed to an online, Qualtrics-based informed consent procedure, followed by a brief screening survey that asked about professional discipline, country, and self-rated expertise in theories of behaviour change.\nA total of 80 individuals consented to be screened and completed the screening\nquestionnaire. We selected 30 experts\u201415 based in the United States, 15 non-US-based\u2014using criteria that maximized self-rated expertise in theories of behaviour change, and ensured a broad range of represented disciplines, and geographical regions. Selections based on expertise were made according to two self-ratings: (i) a response of Extensively for at least one of: use of behaviour change theory, publications on the topic, reviews on the topic, or design of a behaviour change intervention, where the scale was Extensively to Not at all; (ii) a response of 5 or over on knowledge of behaviour change theories, mechanisms, or intervention evaluation/design, where the scale was 1 [No knowledge/expertise] to 7 [Profound\nknowledge/expertise]. Experts who met both of these criteria and elected to participate in the study could earn up to $200 ($100 for Round 1, $50 for Round 2, and $50 for Round 3). Procedure\nAll procedures were virtual, and experts remained anonymous to each other. This study\nfollowed similar procedures to those conducted by HBCP researchers in which experts identified links between BCTs and MoAs (Bohlen et al.). The procedures in the HBCP studies were based on a modified Nominal Group Technique (NGT) (Van de Ven & Delbecq, 1972) spread over three rounds and two weeks to complete each round. Participating experts received an introductory email and two emails prior to each round. One email was a reminder, and the other notified experts the round had begun. Three further modifications to the NGT were made. First, a consensus benchmark of 50% was selected, which is lower than the traditional cutpoint of 70 to 80%. This was determined a priori as this is an exploratory developmental study intended to capture a large number of potential measure-MoA links for further study (Nair, Aggarwal, & Khanna, 2011). It also allows flexibility due to the lack of perfect correspondence between the labels given to mechanisms targeted by SOBC and the comprehensive MoAs identified by the HBCP. Second, to reduce participant burden, not all links were re-rated in Round 3 (i.e., those that reached 50% consensus in Round 1 were not presented again in Round 3). Third, summary statistics were not provided for participants after Round 1.\nStudy procedures were approved by the [BLINDED] Institutional Review Board, and\nparticipants provided electronic informed consent prior to completing study procedures. Preliminary Work\nTwo members of the SOBC Resource and Coordinating Center ([BLINDED] and\n[BLINDED]) independently coded links between 44 self-report measures from the SOBC\nMeasures Repository and the 26 MoAs used in the HBCP studies. Each rater considered all possible links between each measure and MoA, then selected those they considered to be \u201crelated.\u201d Some examples of measure-MoA links easily excluded at this stage are 10-Item Personality Inventory-Knowledge and Zimbardo Time Perspective Inventory-Social/Professional Role & Identity. An example that was more \u201con the fence\u201d is Domain Specific Risk Taking Survey - Expected Benefits-Behaviour Regulation. Although the scale does concern behavioural choices, coders reasoned that expecting a beneficial outcome for a given risky behaviour did not correspond to the MoA definition of \u201cBehavioural, cognitive and/or emotional skills for managing or changing behaviour\u201d (Human Behaviour Change Project (HBCP)).\n[BLINDED] and [BLINDED] met to discuss preliminary codes. Of the 44 self-report\nmeasures, seven were flagged for discussion with [BLINDED] to provide additional expertise on the MoA definitions. After this discussion, codes were reconciled. To reduce participant burden, we pre-identified links rather than asking experts to rate all 1,144 possible measure-MoA links. Because decisions to exclude certain measure-MoA links may have differed with a larger group of coders during this preliminary round, experts were provided the opportunity to suggest any missed measure-MoA links that were not pre-identified and presented in Round 1. This ensured no widely suggested measures were missed, because all links suggested by 50% or more of experts were rated by all participants in Round 3. Round 1\nIn Round 1, experts completed an online survey via Qualtrics, rating each of the 84 pre-\nidentified measure-MoA links based on whether each measure was related to the coded MoA(s). MoAs were defined as \u201cthe processes through which a behaviour change technique (BCT) affects behaviour\u201d and definitions were provided for each (see Supplemental Appendix A).\nExperts were informed that each measure may be linked to more than one MoA, some MoAs may be linked to no measures at all, and that these links were intended to be broad and useful. An email prior to the launch of Round 1 was sent to provide experts with MoA definitions, a link to the Measures Repository, general instructions (i.e., they would be asked to rate agreement with measure-MoA links and have the opportunity to suggest new links between measures and MoAs that were not pre-identified by the research team), and the study timeline (see Supplemental Appendix B).\nEach measure-MoA link was presented as an individual question to rate, and experts were\nprovided with the measure name, measure description, and a link to the Measures Repository to access all measure items (see Supplemental Appendix C). Each question was phrased: \u201cIs this measure related to [MoA] as a MoA?\u201d Response options ranged from 1 (Definitely no) to 5 (Definitely yes). The consensus threshold was set low to be highly inclusive of potential measure-MoA links because this study constitutes a preliminary step toward the larger goal of establishing measure-MoA links in future research and because measures were not explicitly designed to assess specific MoAs. Specifically, the threshold was set such that at least 50% of experts must agree the measure and MoA were related (rating of 4 [Probably yes] or 5 [Definitely yes]) to be included for further consideration as a potential link. Using the term \u201crelated\u201d was selected for similar reasons (i.e., to allow for a broad capture of potential measureMoA links). Experts were given the opportunity to suggest any additional links between measures and MoAs that had not been pre-identified. Round 2\nResults from Round 1 were tabulated and used to inform Round 2. In Round 2, experts\nwere invited to participate in an anonymous, asynchronous discussion occurring over a two-week\nperiod via an online platform (\u2018Courseworks\u2019) to share feedback regarding their experiences in Round 1, and to discuss a selection of 10 measures that did not reach consensus in Round 1. This number was selected to reduce participant burden and allow deeper conversation about a smaller number of measures rather than only a few comments on a much larger number. Measures eligible for discussion were those pre-identified links with less than 50% agreement (i.e., a response of 4 [Probably yes] or 5 [Definitely yes]) as well as new links suggested by 20-50% of respondents. Selections were determined to ensure representation of a range of different measures and MoAs. Round 3\nIn Round 3, experts rated the links discussed in Round 2, plus all other measure-MoA\nlinks that met criteria for discussion in Round 1 (see Supplemental Appendix C). As in Round 1, experts were asked to rate agreement for each measure-MoA link. Experts were provided with descriptions of each MoA and the measure name, description, and a link to the measure on the Measures Repository. Each question was phrased thus: \u201cIs this measure related to [MoA] as a MoA?\u201d Response options ranged from 1 (Definitely no) to 5 (Definitely yes). Data Analysis Strategy\nIn Round 1, experts who did not answer any questions (n = 2), or who began but did not\ncomplete the survey (n = 1) were excluded from further analysis. Consensus with measure-MoA links was calculated as percent agreement that the measure and MoA were related (rating of 4 [Probably yes] or 5 [Definitely yes]) using pairwise deletion (i.e., the denominator for 50% consensus was always computed out of the total number of experts responding to a given question). This technique allowed for accidental missingness on a given question. Newly suggested measure-MoA links were considered to meet consensus if more than 50% of experts\nsuggested a given link; the denominator for this calculation was 27 experts. If links were suggested by at least 20% of experts but were below the 50% benchmark for agreement, then these were considered eligible for discussion and for rating in Round 3. Round 3 employed identical criteria to Round 1. In Round 3, all 18 remaining experts began and completed the survey.\nHeat Maps. All results which reached at least 50% consensus are represented in \u201cheat\nmaps.\u201d Heat maps present data values as color gradients within a matrix to provide an additional means of interpreting the results. The cells within the heat map reflect the percentage of agreement among experts and shaded to reflect the relative strength or \u201cheat\u201d of that value (in this case, the extent of agreement for that particular link). All heat maps were produced using ggplot2 (Wickham, Chang, & Wickham, 2016) in R version 4.1.2. The heat map groups the rows (i.e., measures) and columns (i.e., MoAs) by similarity, such that measures linked to similar MoAs are closer together, and MoAs linked to similar numbers of measures are closer together.\nData Availability Statement. Data tables with all percent consensus ratings are available\nin the Supplemental Appendix D, and raw data are available upon reasonable request from the first author ([BLINDED]).\nResults\nMost experts were from the United States (15; 50%), followed by Canada (6; 20%), and\nIreland and the United Kingdom (5; 16.7%). Others (4; 13.3%) lived in Australia, Brazil, Israel, or the Netherlands. Most (25; 83.3%) worked in a university setting, followed by the public sector, charity/voluntary sector, or academic medical center. Most (13; 43.3%) described their discipline as psychology, 7 as public health (23.3%), and the remaining as other (e.g., nursing,\nkinesiology; 10; 30.0%). All experts had extensive experience in behaviour change interventions, theories, and mechanisms of behaviour change (Figure 1).\nOf the 30 experts, 27 completed Round 1, 23 completed Round 2, and 18 completed\nRound 3. Dropout was monotonic (i.e., no experts who missed Round 2 returned for Round 3). Only one participant formally withdrew due to external circumstances. There were no differences between the 18 experts completing all 3 rounds and those who dropped out in terms of location (US v. non-US) or self-rated expertise. Sensitivity analyses examining agreement with pre-identified measure-MoA links data from only the 18 experts completing all three rounds did not substantively impact consensus ratings. In Round 1, 13 items had one skipped response, and two items had two skipped responses. In Round 3, only one item had a single skipped response. The median time spent on the survey for experts who completed Round 1 was 6.30 hours (IQR 89.97, Range 0.54, 323.48), and for Round 3, the median was 30.95 hours (IQR 112.47, Range 0.16, 225.41). Time spent completing each survey varied widely because experts were able to complete the survey in multiple rounds, meaning they could start and stop the survey as many times as necessary over a two-week period. Thus, hours spent reflects the total amount of time elapsed from when an expert first started the survey, and the time it was completed. Round 1\nOf 84 pre-identified measure-MoA links presented to the expert panel, 82 reached at least\n50% agreement, and only two did not. Experts suggested 109 new links, 14 met the 50% threshold level of consensus (i.e., were suggested by 50% or more participants) and 95 of which had 20-50% agreement. All links are displayed in Supplemental Appendix D, supplemental Table 1 and are separated by type (i.e., pre-identified v. suggested).\nRound 21\nThere were 16 posts in the general discussion thread, and the 10 measure-MoA-specific\nposts had a mean of 17 replies (range: 12-19). There was debate over the MoAs themselves, including whether MoAs must be modifiable, by definition, along with some acknowledgement that some theoretically stable MoAs may be modifiable after all (e.g., Optimism). There was also discussion of whether a MoA could be a moderator or a tailoring variable rather than exclusively a mediator. Experts raised important points about the boundaries of these measures (e.g., invariance across cultures).\nIn terms of the task, there were some discrepancies in how experts interpreted \u201crelated\nto\u201d when assessing measure-MoA links. As one expert stated, \u201cI also found that it was not clear what was meant by \u2018related\u2019. Some experts interpreted the task as judging to which degree the measure assesses the MOA, others did not.\u201d This theme recurred in the measure-MoA-specific boards, and some experts cautioned against overstating the measure-MoA links, drawing attention to the need for further examination of validity. Specifically, as one expert pointed out, \u201cAlthough a \u201clink\u201d between a measure and MOA exists (the links we are drawing, in this consensus exercise), this does not mean that that measure assesses the entire construct of interest and ONLY that construct of interest, for the intended population, context, behavior of interest, etc. Therefore, regardless of a conceptual \"link\", the measures are not valid for use as measures of the MOAs.\u201d\n1 Qualitative data will be comprehensively coded and analyzed, and results will be published in a future manuscript.\nThe primary concern was over the quality of the measures (construct validity,\nconvergent/divergent validity, double-barrel items, etc.). One expert suggested that linking measures-MoAs would necessitate developing measures from scratch for the sole purpose of measuring a specific MoA, and another pointed out that these flaws in the measures themselves would hinder the ability to detect meaningful associations (e.g., due to measurement error). Experts additionally acknowledged lack of expertise in a specific theoretical domain for a given measure, recommended a focus on the wording of measure items (rather than measure titles alone), and drew attention to measures-outcome correspondence (e.g., to measure a MoA such as Intention, Skills, or Knowledge, a measure must be content-specific; the target in dyadic scales may focus on only one individual or on both). Experts also noted some of the MoA definitions were quite broad (e.g., Self-Image), such that almost any measure might be \u201crelated.\u201d Round 3\nThe two pre-identified links that did not reach agreement and the 94 new links with 20-\n50% agreement were rated in Round 3 (note: one newly suggested link was left out of the survey in error). Of the two pre-identified links, one reached 72.2% consensus after Round 3. Of the 94 newly suggested links, 70 reached at least 50% consensus. All ratings are displayed in Supplemental Appendix D, Supplemental Table 2.\nCombining results from Rounds 1 and 3, a total of 167 measure-MoA links reached\nconsensus: 82 pre-identified links from Round 1, 14 newly suggested links from Round 1, and 71 from Round 3. These links included all 44 self-report measures from the SOBC Measures Repository and 20 of the 26 HBCP MoAs. The MoAs Knowledge, Norms, Subjective Norms, Needs, Feedback Processes, Social Learning/Imitation, and Perceived\nSusceptibility/Vulnerability were not linked to any measures. All links meeting consensus are displayed in Table 1.\nOut of the 167 links meeting the 50% threshold, 33 had consensus between 90-100%.\nMean agreement was 74.21% (SD = 15.62%; Range: 50%-100%). Across the 44 measures, a single measure was linked to a mean of 3.80 MoAs (SD = 1.36; Median = 4.0; Range: 1-6), and the mode was 5 MoA linkages for a single measure (15; 34.1%). Across the 20 MoAs, a single MoA was linked to a mean of 8.35 measures (SD = 9.18; Median = 3.5; Range: 1-33). Of all MoAs, Behavioural Regulation had the greatest number of linked measures (n = 33). Figure 2 depicts all final links between measures and MoAs, and Figure 3 shows links with 90% or more agreement.\nDiscussion\nThis study aimed to link measures from the Science of Behavior Change (SOBC)\nMeasures Repository to hypothesized mechanisms of action (MoAs) identified by the Human Behaviour-Change Project (HBCP). A consensus exercise involving three rounds generated 167 measure-MoA links that reached at least 50% agreement (i.e., experts agreed these measures were \u201crelated\u201d), 33 of which had 90% or greater agreement. This is a first step in obtaining data to increase interoperability of scientific resources designed to facilitate process research. By linking measures to a comprehensive ontology of MoAs, this study also enables consistency of measurement of MoAs, so that research on how interventions exert their effects on behavioural outcomes can accumulate systematically. Compiling evidence for or against these measure-MoA links can inform researchers in selecting optimal measures from the SOBC repository when designing studies that utilize the experimental medicine approach. Further, for example, just because a measure is purportedly a stress measure, does not mean it truly measures stress, or\npurely measures stress, and it may be associated with other MoAs. This study allows us to tease apart this conceptual issue for measures currently housed within the SOBC measures repository.\nResearch building on this preliminary study should further investigate these putative\nmeasure-MoA links and test the utility of these links for use in behavioural research. For example, is a measure valid and reliable for capturing change in the hypothesized MoA? Is one measure more readily influenced than another measure of that same MoA? If so, does this depend on choice of BCT? On what timescale can these measures capture change? Evidence to this effect may benefit research by highlighting the differences in performance of measures of MoAs across studies. This line of research could shed valuable light on a variety of measurement issues, including measurement quality, timescale of measurement (e.g., short-term v. long-term change), jingle-jangle fallacies (Marsh et al., 2019), and nuances in the operationalization of MoAs. Evidence would also accumulate in support of BCT-MoA linkages. As an example, one may find that all of the SOBC measures deemed to be of Beliefs About Capabilities are modified by a Goal-Setting intervention. It is also important to acknowledge that poor measurement practices remain a serious issue in the broader field of psychological science, and there have been other calls to action for more attention to the validity of measures. A recent manuscript (Flake & Fried, 2020) highlights a number of these issues, including differing results across studies that utilize different measures of the same ostensible construct. Flake and Fried (2020) additionally provide some helpful guidance on considerations for researchers when selecting measures to optimize the validity of their study.\nThis study had several strengths. First, the broad expertise of the participants allowed for\ninformed ratings, fruitful consensus discussions, and valuable study feedback. Second, the multiple rounds allowed for correction and refinement after each round. Third, selecting a low\nthreshold for consensus (50%) and using a general term for potential linkage (\u201crelated\u201d) is appropriate for exploratory developmental work in this new area, allowing us to capture a large number of potential measure-MoA links and reducing the possibility that important links were missed.\nSeveral challenges arose, and expert feedback highlighted important areas for\nimprovement in the field. The lack of a one-to-one correspondence between an operational measure and a conceptual mechanism can be attributed to multiple factors, including poor practices in measurement, lack of shared definitions of constructs between different research groups or fields, or different levels of specificity in a measure or mechanism, such as measuring the construct of self-regulation as the tendency to engage in self-regulation generally v. the tendency to engage in self-regulatory abilities around a specific behaviour. When designing behavioural research studies, rather than starting investigations with a measure, one often begins with the conceptual structure (mechanisms). Measures that are conceptually appropriate for a specific study are critical to test these mechanistic hypotheses, but appropriate measures may not exist. Barriers to the creation of new measures, or comprehensive validation of previously created measures, include continuing with past practice, poor uptake of new measures, and lack of resources (e.g., time, funding), among others.\nIn the present study, the 44 self-report measures from the SOBC repository were not\ndesigned to measure the coded MoAs. Rather, they were originally created to assess specific conceptual constructs that each can be categorized as pertaining to one or more of the broad domains of stress, self-regulation, and interpersonal processes. Similarly, the MoA definitions were not developed in reference to the repository measures. Despite this inherent challenge, we were able to bridge two research approaches by forming initial links between the useful\nrepository resource and the valuable MoA definitions. The experts reached consensus on many measure-MoA links, some of which reached 100% agreement. Future work should consider how to bring entity definitions together with measures of constructs across all the entities in the Behaviour Change Intervention Ontology (Michie et al., 2020), especially behaviour itself, and should incorporate non-self-report measures, such as tasks or observational coding. This would help in the design of future research as well as the curation of extant knowledge. The SOBC repository is not a comprehensive collection of measures, whereas the HBCP MoAs were developed to represent the most commonly occurring constructs across theories of behaviour and behaviour change. Because the goal was to link the SOBC repository and the HBCP TaTT tool, this study was limited to the available measures; however, the SOBC repository is evolving and growing, and there are plans to incorporate methods to allow for linkages between newly uploaded measures and the HBCP MoAs, as well as methods for the refinement of existing links.\nThe measure-MoA links were intended to be useful for researchers who want to identify\nmeasures of MoAs that can be targeted for change in behavioural interventions. The heat maps and associated consensus data can be viewed as a summary of behavioural health researchers\u2019 beliefs about measure\u2013MoA links and can be used as a starting point for intervention designers and evaluators. These data have been integrated into the user interface for both the SOBC Measures Repository and the Theory and Techniques Tool (TaTT) (Human Behaviour Change Project (HBCP)), allowing users to access these data and navigate between the TATT and the Measures Repository with ease. However, knowing that a measure and MoA are \u201crelated\u201d should not be understood as a recommendation to use a specific measure for a specific MoA. Experts may have operationalized \u201crelated\u201d in different ways when considering measure-MoA links. Much like the TaTT (Human Behaviour Change Project (HBCP)), the current set of links\nbetween MoAs and measures represents only hypothesized associations that should be evaluated further before use. Furthermore, experts may have focused on MoA labels rather than definitions (or measure titles rather than item content). Future research should investigate and assess the utility of these links, the strength of these links, and whether better alternatives exist for measurement, particularly given the varied quality of the self-report measures in terms of validity and reliability. This could be accomplished by seeking feedback from a wide range of researchers interested in the development of behavioural interventions, for example, users of the SOBC Measures Repository or the HBCP TaTT. In fact, the TaTT already includes a feature such that users can comment and discuss the different BCT-MoA links (Human Behaviour Change Project (HBCP)), and a similar feature may be useful for the measure-MoA links identified in the present study. Additional studies, using different study designs to assess this information, as well as triangulation techniques, may also be a fruitful next step (e.g., gamification techniques that allow \u201cnon-expert\u201d repository users to assess the strength of various measure-MoA links, including links and non-links identified by the experts in the present study). Multiple replicating methodologies and triangulation efforts were used to generate the results of BCT-MoA links in the TaTT project (Bohlen et al.; Carey et al., 2019; Johnston et al., 2020). A similar approach, and/or data addressing the validity and reliability of new measures uploaded to the SOBC Measures Repository could also be considered.\nIn the long-term, it will be important to identify measure-MoA linkages not just for self-\nreport measures but also behavioural measures (e.g., Stroop task). Research should address not only whether or not a measure taps into a putative mechanism but also whether it taps that specific mechanism and not other similar mechanisms (i.e., content validity and discriminant content validity (Burrell, Allan, Williams, & Johnston, 2018; Johnston et al., 2014)). Moreover, a\ncritical next step will be to link measures of mechanisms to BCTs that are associated with particular interventions (i.e., the early-stage steps of the causal chain) and particular behavioural outcomes (i.e., the late-stage steps of the causal chain). This latter research direction is currently being addressed in the HBCP (Michie et al., 2020).\nMeasures may not generalize to certain populations or settings, and data on measurement\ninvariance for many of these measures are lacking (Milfont & Fischer, 2010). There may be limits for certain measures when considering behavioural domains (e.g., the measure may be too specific, or not specific enough) or target individuals (e.g., dyadic scales may concern behavioural regulation in one person, the other, or both). Some measures may not demonstrate strong construct validity, convergent validity, or divergent validity (e.g., a measure may be equally related to two different MoAs, but a researcher may desire a pure measure of only one of these MoAs). Indeed, construct validity was a major concern raised by experts in Round 2.\nAdditional features of the study design are important to consider when evaluating the\nfindings. Participants may have been biased to agree with the pre-identified measure-MoA links. Because no questions with non-links were included as attention checks, this possibility cannot be tested. That said, reducing participant burden from rating over 1,000 links to only 84 outweighs the limitations of this approach. The consensus ratings of \u201crelated\u201d in Rounds 1 and 3 are not directly comparable to those links suggested by more than 50% of experts in Round 1. Not all links rated in Round 1 were re-rated in Round 3. Summaries of Round 1 results were also not provided prior to Rounds 2 and 3, which could have reduced the ability of experts to reach consensus. Another question raised in this study included whether the MoAs were believed to be modifiable or stable personality traits (e.g., Optimism) and whether some MoAs might serve as moderators rather than mediators (i.e., mechanisms) of intervention effects on behavioural\noutcomes. However, recent evidence indicates that personality may prove to be more malleable to behavioural intervention than previously considered (Allemand & Fl\u00fcckiger, 2022; Roberts et al., 2017). As such, there may be value in examining personality as both a mediator and moderator of behaviour change. Finally, there was substantial participant attrition of 33.3% from Round 1 to Round 3, which may have increased bias in consensus ratings. Although this reduced the sample size from 30 to 18, a panel size of 5-11 is considered more than sufficient for a consensus study (Sinha, Smyth, & Williamson, 2011; Waggoner, Carline, & Durning, 2016). Furthermore, sensitivity analyses comprising only the 18 participants who completed Round 3 did not alter the Round 1 study conclusions.\nConclusion\nThis study contributes to behaviour change research through a preliminary integration of\nputative measures of mechanisms from the SOBC Measures Repository with the MoAs used within the HBCP. A total of 167 potential measure-MoA links were identified for refinement in future studies, with agreed upon links based on expert consensus rather than assumptions about the measure, or traditions for using the measure within the field. These results can be used to facilitate the accumulation of scientific evidence for process pathways and can also be used to increase the interoperability of scientific resources in the larger mission of understanding how interventions can effectively change behaviours.\nAcknowledgements\nDeclaration of Interest\nThe authors declare that they have no conflicts of interest.\nReferences\nAllemand, M., & Fl\u00fcckiger, C. (2022). Personality change through digital-coaching interventions. Current Directions in Psychological Science, 09637214211067782. Bohlen, L. C., Carey, R., de Bruin, M., Rothman, A., Johnston, M., Kelly, M. P., & Michie, S. Links between behaviour change techniques and mechanisms of action: an expert consensus study. Annals of Behavioral Medicine, 53(8), 708-720. Burrell, A. M., Allan, J. L., Williams, D. M., & Johnston, M. (2018). What do self\u2010efficacy items measure? Examining the discriminant content validity of self\u2010efficacy items. British Journal of Health Psychology, 23(3), 597-611. Carey, R. N., Connell, L. E., Johnston, M., Rothman, A. J., De Bruin, M., Kelly, M. P., & Michie, S. (2019). Behavior change techniques and their mechanisms of action: a synthesis of links described in published intervention literature. Annals of Behavioral Medicine, 53(8), 693-707. Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456- 465. Human Behaviour Change Project (HBCP). Theory and Techniques Tool. Retrieved from https://theoryandtechniquetool.humanbehaviourchange.org/ Johnston, M., Carey, R. N., Connell Bohlen, L. E., Johnston, D. W., Rothman, A. J., de Bruin, M., . . . Michie, S. (2020). Development of an online tool for linking behavior change techniques and mechanisms of action based on triangulation of findings from literature synthesis and expert consensus. Translational Behavioral Medicine. Johnston, M., Carey, R. N., Connell Bohlen, L. E., Johnston, D. W., Rothman, A. J., de Bruin, M., . . . Michie, S. (2021). Development of an online tool for linking behavior change techniques and mechanisms of action based on triangulation of findings from literature synthesis and expert consensus. Translational Behavioral Medicine, 11(5), 1049-1065. Johnston, M., Dixon, D., Hart, J., Glidewell, L., Schr\u00f6der, C., & Pollard, B. (2014). Discriminant content validity: A quantitative methodology for assessing content of theory\u2010based measures, with illustrative applications. British Journal of Health Psychology, 19(2), 240-257. Marsh, H. W., Pekrun, R., Parker, P. D., Murayama, K., Guo, J., Dicke, T., & Arens, A. K. (2019). The murky distinction between self-concept and self-efficacy: Beware of lurking jingle-jangle fallacies. Journal of educational psychology, 111(2), 331. Michie, S., West, R., Finnerty, A. N., Norris, E., Wright, A. J., Marques, M. M., . . . Hastings, J. (2020). Representation of behaviour change interventions and their evaluation: Development of the Upper Level of the Behaviour Change Intervention Ontology. Wellcome Open Research, 5. Michie, S., Wood, C. E., Johnston, M., Abraham, C., Francis, J., & Hardeman, W. (2015). Behaviour change techniques: the development and evaluation of a taxonomic method for reporting and describing behaviour change interventions (a suite of five studies involving consensus methods, randomised controlled trials and analysis of qualitative data). Health technology assessment, 19(99). Milfont, T. L., & Fischer, R. (2010). Testing measurement invariance across groups: Applications in crosscultural research. International Journal of psychological research, 3(1), 111-130. Nair, R., Aggarwal, R., & Khanna, D. (2011). Methods of formal consensus in classification/diagnostic criteria and guideline development. Paper presented at the Seminars in arthritis and rheumatism. Riddle, M., & Science of Behavior Change (SOBC) Working Group. (2015). News from the NIH: using an experimental medicine approach to facilitate translational research. In: Oxford University Press.\nRoberts, B. W., Luo, J., Briley, D. A., Chow, P. I., Su, R., & Hill, P. L. (2017). A systematic review of personality trait change through intervention. Psychol Bull, 143(2), 117. Science Of Behavior Change (SOBC). The Measures. Retrieved from https://measures.scienceofbehaviorchange.org/ Sheeran, P., Klein, W., & Rothman, A. J. (2017). Health behavior change: Moving from observation to intervention. Annual review of psychology, 68(1), 573-600. Sinha, I. P., Smyth, R. L., & Williamson, P. R. (2011). Using the Delphi technique to determine which outcomes to measure in clinical trials: recommendations for the future based on a systematic review of existing studies. PLoS Med, 8(1), e1000393. Sumner, J. A., Birk, J. L., Cornelius, T., Derby, L., Edmondson, D., & Davidson, K. W. (2019). The NIH Science of Behavior Change Mechanism-Focused Approach to Behavior Change Research. Paper presented at the PSYCHOSOMATIC MEDICINE. Sumner, J. A., Carey, R. N., Michie, S., Johnston, M., Edmondson, D., & Davidson, K. W. (2018). Using rigorous methods to advance behaviour change science. Nature human behaviour, 2(11), 797. Van de Ven, A. H., & Delbecq, A. L. (1972). The nominal group as a research instrument for exploratory health studies. American journal of public health, 62(3), 337-342. Waggoner, J., Carline, J. D., & Durning, S. J. (2016). Is there a consensus on consensus methodology? Descriptions and recommendations for future consensus research. Academic Medicine, 91(5), 663-668. Wickham, H., Chang, W., & Wickham, M. H. (2016). Package \u2018ggplot2\u2019. Create elegant data visualisations using the grammar of graphics. Version, 2(1), 1-189.\nBehavioral Regulation Generalized Self-Efficacy Scale 72.2\nBehavioral Regulation Grit-S 73.1\nBehavioral Regulation I-7: Impulsiveness and Venturesomeness\nQuestionnaire\n81.5\nBehavioral Regulation Mindful Attention Awareness Scale 92.3\nBehavioral Regulation Multidimensional Assessment of Interoceptive\nAwareness\n76.9\nBehavioral Regulation Multidimensional Personality Questionnaire:\nControl vs. Impulsivity Scale\n96.3\nBehavioral Regulation NIH Self-Efficacy Scale 81.5\nBehavioral Regulation Parent-rated Stress (NIH Perceived Stress\nScale)\n55.6\nBehavioral Regulation Pearlin Mastery Scale 50.0\nBehavioral Regulation Selection-Optimization-Compensation\nQuestionnaire\n92.0\nBehavioral Regulation Short Self-Regulation Questionnaire 100.0\nBehavioral Regulation SIDES Affect Dysregulation Scale (Child-\nReported)\n88.9\nBehavioral Regulation Theories of Willpower Scale 88.5\nBehavioral Regulation Three Factor Eating Questionnaire R-18 96.3\nBehavioral Regulation UPPS-P Impulsivity Scale Survey 96.3\nBehavioral Regulation Zimbardo Time Perspective Inventory 69.2\nBehavioral Regulation Zuckerman Sensation Seeking Survey-V 63.0\nBeliefs about Capabilities Barratt Impulsiveness 50.0\nBeliefs about Capabilities Behavior Rating Inventory of Executive\nFunction (BRIEF) - Adults\n51.9\nBeliefs about Capabilities Brief COPE 72.2\nBeliefs about Capabilities Brief Risk-Resilience 72.2\nBeliefs about Capabilities Brief Self-Control 100.0\nBeliefs about Capabilities Dickman Functional and Dysfunctional\nImpulsivity Survey\n50.0\nBeliefs about Capabilities Emotion Regulation Questionnaire 50.0\nBeliefs about Capabilities Generalized Self-Efficacy Scale 100.0\nBeliefs about Capabilities Grit-S 72.2\nBeliefs about Capabilities NIH Self-Efficacy Scale 74.1\nBeliefs about Capabilities Pearlin Mastery Scale 96.3\nBeliefs about Capabilities Short Self-Regulation Questionnaire 51.9\nBeliefs about Capabilities SIDES Affect Dysregulation Scale (Child-\nReported)\n70.4\nBeliefs about Capabilities Theories of Willpower Scale 55.6\nBeliefs about Capabilities Three Factor Eating Questionnaire R-18 72.2\nBeliefs about Capabilities UPPS-P Impulsivity Scale Survey 55.6\nBeliefs about Consequences\nConsideration of Future Consequences Scale 92.6\nBeliefs about Consequences\nDomain Specific Risk Taking Survey - Expected Benefits\n92.3\nBeliefs about Consequences\nDomain Specific Risk Taking Survey - Risk Perceptions\n100.0\nBeliefs about Consequences\nDomain Specific Risk Taking Survey - Risk Taking\n81.5\nBeliefs about Consequences\nSelection-Optimization-Compensation Questionnaire\n73.1\nBeliefs about Consequences\nTheories of Willpower Scale 76.9\nBeliefs about Consequences\nThree Factor Eating Questionnaire R-18 63.0\nBeliefs about Consequences\nZimbardo Time Perspective Inventory 76.9\nEmotion 10-Item Personality 55.6\nEmotion Barratt Impulsiveness 50.0\nEmotion Brief COPE 88.9\nEmotion Brief Risk-Resilience 95.8\nEmotion Couple Coercion Scale 66.7\nEmotion Daily Inventory of Stressful Events (DISE) 63.0\nEmotion Ecological Momentary Assessment of Stressful\nEvents\n81.5\nEmotion Emotion Regulation Questionnaire 92.6\nEmotion Emotion Regulation Strategies Scale 96.3\nEmotion Five Facts of Mindfulness Survey 81.5\nEmotion I-7: Impulsiveness and Venturesomeness\nQuestionnaire\n59.3\nEmotion Kessler Psychological Distress Scale (K6+) 96.3\nEmotion Multidimensional Assessment of Interoceptive\nAwareness\n81.5\nEmotion Parent-rated Stress (NIH Perceived Stress\nScale)\n96.3\nEmotion Positive and Negative Affect Scheduled\n(PANAS)\n92.6\nEmotion Positive and Negative Affect Scheduled\n(PANAS)- Child\n100.0\nEmotion SIDES Affect Dysregulation Scale (Child-\nReported)\n96.3\nEmotion Three Factor Eating Questionnaire R-18 61.1\nEnvironmental Context & Resources\nDaily Inventory of Stressful Events (DISE) 88.9\nEnvironmental Context & Resources\nEcological Momentary Assessment of Stressful Events\n88.9\nGeneral Attitudes/Beliefs Future Time Perspective Scale 70.4\nGeneral Attitudes/Beliefs Theories of Willpower Scale 73.1\nGoals BIS/BAS 88.9\nGoals Consideration of Future Consequences Scale 72.2\nGoals Future Orientation Scale of Time Perspective\nStudy\n55.6\nGoals Grit-S 76.9\nGoals Selection-Optimization-Compensation\nQuestionnaire\n51.9\nGoals Short Self-Regulation Questionnaire 88.9\nIntentions Domain Specific Risk Taking Survey - Risk\nTaking\n55.6\nMemory, Attention, & Decision Processes\nBarratt Impulsiveness 96.3\nMemory, Attention, & Decision Processes\nBehavior Rating Inventory of Executive Function (BRIEF) - Adults\n100.0\nMemory, Attention, & Decision Processes\nCognitive Reflection Test 96.3\nMemory, Attention, & Decision Processes\nConsideration of Future Consequences Scale 74.1\nMemory, Attention, & Decision Processes\nDeferment of Gratification 70.4\nMemory, Attention, & Decision Processes\nDickman Functional and Dysfunctional Impulsivity Survey\n70.4\nMemory, Attention, & Decision Processes\nDomain Specific Risk Taking Survey - Expected Benefits\n55.6\nMemory, Attention, & Decision Processes\nDomain Specific Risk Taking Survey - Risk Perceptions\n50.0\nMemory, Attention, & Decision Processes\nEmotion Regulation Questionnaire 61.1\nMemory, Attention, & Decision Processes\nFive Facts of Mindfulness Survey 77.8\nMemory, Attention, & Decision Processes\nFuture Orientation Scale of Time Perspective Study\n77.8\nMemory, Attention, & Decision Processes\nI-7: Impulsiveness and Venturesomeness Questionnaire\n61.1\nMemory, Attention, & Decision Processes\nMindful Attention Awareness Scale 96.3\nMemory, Attention, & Decision Processes\nMultidimensional Assessment of Interoceptive Awareness\n85.2\nMemory, Attention, & Decision Processes\nMultidimensional Personality Questionnaire: Control vs. Impulsivity Scale\n70.4\nMemory, Attention, & Decision Processes\nSelection-Optimization-Compensation Questionnaire\n84.6\nMemory, Attention, & Decision Processes\nUPPS-P Impulsivity Scale Survey 74.1\nMotivation BIS/BAS 63.0\nMotivation Consideration of Future Consequences Scale 61.1\nMotivation Deferment of Gratification 50.0\nMotivation Grit-S 55.6\nOptimism Future Time Perspective Scale 63.0\nOptimism Generalized Self-Efficacy Scale 66.7\nOptimism NIH Self-Efficacy Scale 66.7\nOptimism Pearlin Mastery Scale 50.0\nOptimism Zimbardo Time Perspective Inventory 50.0\nPerceived Susceptibility /Vulnerability\nDaily Inventory of Stressful Events (DISE) 50.0\nPerceived Susceptibility /Vulnerability\nDomain Specific Risk Taking Survey - Risk Taking\n66.7\nPerceived Susceptibility /Vulnerability\nDomain Specific Risk Taking Survey - Risk Perceptions\n55.6\nReinforcement BIS/BAS 63.0\nReinforcement Deferment of Gratification 70.4\nSelf-Image 10-Item Personality 88.9\nSelf-Image Barratt Impulsiveness 88.9\nSelf-Image Behavior Rating Inventory of Executive\nFunction (BRIEF) - Adults\n88.9\nSelf-Image BIS/BAS 83.3\nSelf-Image Brief COPE 72.2\nSelf-Image Brief Risk-Resilience 85.2\nSelf-Image Brief Self-Control 77.8\nSelf-Image Dickman Functional and Dysfunctional\nImpulsivity Survey\n77.8\nSelf-Image Domain Specific Risk Taking Survey - Risk\nTaking\n77.8\nSelf-Image Five Facts of Mindfulness Survey 77.8\nSelf-Image Generalized Self-Efficacy Scale 96.3\nSelf-Image Grit-S 59.3\nSelf-Image I-7: Impulsiveness and Venturesomeness\nQuestionnaire\n88.9\nSelf-Image Kessler Psychological Distress Scale (K6+) 61.1\nSelf-Image Multidimensional Assessment of Interoceptive\nAwareness\n55.6\nSelf-Image Multidimensional Personality Questionnaire:\nControl vs. Impulsivity Scale\n70.4\nSelf-Image NIH Self-Efficacy Scale 92.3\nSelf-Image Pearlin Mastery Scale 88.9\nSelf-Image Selection-Optimization-Compensation\nQuestionnaire\n56.0\nSelf-Image Short Self-Regulation Questionnaire 88.9\nSelf-Image SIDES Affect Dysregulation Scale (Child-\nReported)\n72.2\nSelf-Image Three Factor Eating Questionnaire R-18 77.8\nSelf-Image UPPS-P Impulsivity Scale Survey 55.6\nSelf-Image Zimbardo Time Perspective Inventory 72.2\nSelf-Image Zuckerman Sensation Seeking Survey-V 88.9\nSkills Behavior Rating Inventory of Executive\nFunction (BRIEF) - Adults\n66.7\nSkills Brief COPE 55.6\nSkills Brief Risk-Resilience 55.6\nSkills Brief Self-Control 55.6\nSkills Cognitive Reflection Test 61.1\nSkills Emotion Regulation Questionnaire 72.2\nSkills Emotion Regulation Strategies Scale 61.1\nSkills Five Facts of Mindfulness Survey 72.2\nSkills Future Orientation Scale of Time Perspective\nStudy\n55.6\nSkills Generalized Self-Efficacy Scale 66.7\nSkills Mindful Attention Awareness Scale 55.6\nSkills Multidimensional Assessment of Interoceptive\nAwareness\n66.7\nSkills NIH Self-Efficacy Scale 72.2\nSkills Short Self-Regulation Questionnaire 61.1\nSkills SIDES Affect Dysregulation Scale (Child-\nReported)\n50.0\nSocial Influences Couple Coercion Scale 77.8\nSocial Influences Parent-Child Coercion Scale 66.7\nSocial Influences Parent Cognition Scale 81.5\nSocial/Professional Role & Identity 10-Item Personality 55.6 Social/Professional Role & Identity Parent-Child Coercion Scale 50.0 Values Consideration of Future Consequences Scale 77.8"
        }
    ],
    "title": "LINKING MEASURES TO MECHANISMS OF ACTION 1 Linking Measures to Mechanisms of Action: An Expert Opinion Study LINKING MEASURES TO MECHANISMS OF ACTION",
    "year": 2022
}