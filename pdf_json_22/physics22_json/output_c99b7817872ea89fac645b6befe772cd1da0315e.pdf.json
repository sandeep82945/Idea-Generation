{
    "abstractText": "The SKA pulsar search pipeline will be used for real time detection of pulsars. Modern radio telescopes such as SKA will be generating petabytes of data in their full scale of operation. Hence experience-based and datadriven algorithms are being investigated for applications such as candidate detection. Here we describe our findings from testing a state of the art object detection algorithm called Mask R-CNN to detect candidate signatures in the SKA pulsar search pipeline. We have trained the Mask R-CNN model to detect candidate images. A custom semi-auto annotation tool was developed and investigated to rapidly mark the regions of interest in large datasets. We have used a simulation dataset to train and build the candidate detection algorithm. A more detailed analysis is planned. The paper presents details of this initial investigation highlighting the future prospects.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shashank Sanjay Bhat"
        },
        {
            "affiliations": [],
            "name": "Thiagaraj Prabu"
        },
        {
            "affiliations": [],
            "name": "Ben Stappers"
        },
        {
            "affiliations": [],
            "name": "Atul Ghalame"
        },
        {
            "affiliations": [],
            "name": "Snehanshu Saha"
        },
        {
            "affiliations": [],
            "name": "T.S.B Sudarshan"
        },
        {
            "affiliations": [],
            "name": "Zafiirah Hosenie"
        }
    ],
    "id": "SP:2feb44e91d83c952eb4c5d35a0aff0cc5e165745",
    "references": [
        {
            "authors": [
                "P. S"
            ],
            "title": "SKA PHASE 1 EXECUTIVE SUMMARY, https://www.skatelescope.org/ wp-content/uploads/2021/03/22380_SKA_ Project-Summary_v4_single-pages.pdf",
            "year": 2015
        },
        {
            "authors": [
                "P. Dewdney",
                "W. Turner",
                "R Braun"
            ],
            "title": "SKA1 SYSTEM BASELINEV2 DESCRIPTION, https: //www.skatelescope.org/wp-content/ uploads/2014/03/SKA-TEL-SKO-0000308_ SKA1_System_Baseline_v2",
            "year": 2015
        },
        {
            "authors": [
                "K. Gurney"
            ],
            "title": "An Introduction to Neural Networks (USA: Taylor and Francis",
            "year": 1997
        },
        {
            "authors": [
                "A.K. Jain"
            ],
            "title": "Machine Learning and Knowledge Discovery in Databases, ed",
            "year": 2008
        },
        {
            "authors": [
                "Lin",
                "T.-Y",
                "P. Doll\u00e1r",
                "R Girshick"
            ],
            "title": "Feature Pyramid Networks for Object Detection, doi:10.48550/ARXIV.1612.03144",
            "year": 2016
        },
        {
            "authors": [
                "R.J. Lyon",
                "B.W. Stappers",
                "L. Levin",
                "M.B. Mickaliger",
                "A. Scaife"
            ],
            "title": "2018, A Processing Pipeline for High Volume Pulsar Data Streams, doi:10.48550/ARXIV.1810.06012",
            "year": 2018
        },
        {
            "authors": [
                "S.M. Ransom"
            ],
            "title": "Fast Search Techniques for High Energy Pulsars, doi:10.48550/ARXIV.ASTROPH/0112006",
            "year": 2001
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, doi:10.48550/ARXIV.1506.01497",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Keywords. Modern Radio Telescopes\u2014Anomaly Detection\u2014Time Series\u2014Mask R-CNN\u2014Binary pulsars."
        },
        {
            "heading": "1. Introduction",
            "text": "Modern Radio Telescopes, such as the Square Kilometre Array (SKA), generate data in the order of petabytes in their full scale of operation. Data volumes arise due to the extensive array size (197 antennas in South Africa and 1,31,072 antennas in Western Australia), multiple beams (up to about 1500), broadband digitisation (about 800 Msps), and from the demanding science requirements (1, 2015; Dewdney et al., 2015). Correspondingly there is a considerable high-speed signal processing: FFT, filtering, cross-correlations, beamforming, etc., employed using dedicated hardware components. Subsequent processing uses a mix of application-specific and non-deterministic algorithms, where statistical and experience-based decision making is indispensable.\nManual or semi-automated approaches will not be feasible when handling such volumes of data. We will have to employ automated and heuristic algorithms that can ease the classification of data, detection of patterns/objects in the data streams in real-time. Machine Learning, which is an actively evolving field, brings in a host of valuable practical techniques to efficiently han-\ndle the radio-telescope large volume data processing. This paper investigates the use of a state of the art object detection algorithm called Mask R-CNN in order to detect accelerated binary pulsar\u2019s signatures in a pulsar search pipeline data stream. In a related work by (Lyon et al., 2016; Keith et al., 2010; Bates et al., 2012) candidate detection was performed on pulsar profiles. We demonstrate a novel approach of detecting candidate signatures in the middle of the processing pipeline. Additionally our effort for detecting binary pulsar signatures involving machine learning based approaches is novel. By determining the location, co-ordinates of the candidate we will be able to extract useful information for the sifting needed to sort the candidates.\nThis paper is outlined as follows: Section 2. gives a brief background on machine learning. Section 3. introduces the SKA pulsar search pipeline (Levin et al., 2017), the Fourier Domain Acceleration Search (FDAS)(Ransom, 2001) and the machine learning pipeline which was investigated. Section 4. provides a discussion about our main results. Section 5. talks about our future propects and Section 6. concludes the work.\n\u00a9 Indian Academy of Sciences 1\nar X\niv :2\n20 9.\n04 43\n0v 3\n[ as\ntr o-\nph .I\nM ]\n1 7"
        },
        {
            "heading": "2. Background on Machine Learning",
            "text": "Machine Learning is the study of algorithms which can improve over time with experience and data. Machine Learning is classified into three categories : 1. Supervised Learning, 2. Unsupervised Learning and 3. Reinforcement Learning.\nThe term Supervised Learning implies that the data is labeled. An Artificial Intelligence (AI) model creates a mapping between the features and output and uses this mapping function to predict values for new datasets. Support Vector Machines (Hearst et al., 1998), Decision Trees (Quinlan, 1986), K-Nearest Neighbors (Cunningham & Delany, 2022) are some of the widely used supervised machine learning algorithms.\nUnsupervised machine learning signifies AI models which creates a mapping function by clustering and grouping similar data. K-means clustering (Jain, 2008), Expectation Maximization clustering (Moon, 1996), Auto encoders (Bank et al., 2020) are some of the widely used unsupervised machine learning algorithms.\nReinforcement Learning works on the concept of AI agent which observes its surroundings and performs an action. Based on the action taken it is either rewarded or penalized (Sutton & Barto, 1998). Over time the AI model will aim towards maximizing the reward function.\nIn most of the above three categories of machine learning the underlying workhorse is a neural network. A neural network is primarily composed of neurons (Gurney, 1997). A neuron takes a sum of weighted inputs along with a bias and subjects this output to an activation function. The activation function determines if a neuron has to switch on or off. Figure 1 shows a general structure of a neuron with two inputs and a bias. Neural Network based approaches in some cases create the mapping between the input data and its corresponding labels.\nNeural networks can be effectively used for modeling the behaviour of a function. But in case of complex functions more number of neurons in the form of layers will have to be added. A neural network with multiple hidden layers is called a deep neural network. Figure 2 shows an artificial neural network with one hidden layer.\nIn case of pulsar searches we will have to deal with large amounts of image based data and the processing will have to be done in real time (Lyon et al., 2018). Lyon et al, 2018 in their paper introduces a real time pulsar candidate detection pipeline.\nWhen dealing with image data, we use a special case of an Artificial Neural Network called the Convolutional Neural Network (CNN) (Krizhevsky et al.,\n2012). The feature extraction in a CNN happens with a repeated set of convolutions and pooling. Convolutions are performed via filters to produce feature-maps. The pooling layer is responsible for reducing the spatial size of the feature maps. The feature map, additionally helps in reducing subsequent computation required."
        },
        {
            "heading": "3. SKA Pulsar Search pipeline",
            "text": "One of the complex tasks of the SKA pulsar engine shown in Fig 4 is to search for pulsars in binary systems where the apparent frequency of the pulsar is changed significantly during the observation.\nA Fourier domain acceleration search (FDAS) algorithm (Ransom et al., 2002) is being tested for the SKA application. The frequency changes manifest in the Fourier space as sinc functions convolved with an FIR response. In order to deconvolve these signals, a set of optimised matched filters are constructed and used in the FDAS module (Fig 3).\nThe FDAS module receives the RFI mitigated complex spectra generated from the dedispersed time series. Each spectrum is passed through a set of 85 matched filters, each analysing a unique acceleration-period range in the search. Outputs from the filters are detected to obtain the power series and saved in a 2-dimensional data array known as a filter-output-plane (FOP) (Fig 5).\nFor SNR optimization across the search parameter space, 84 filters are required to search across pulsar periods up to 500 Hz with +/-350 ms\u22122 acceleration range. A typical data size of 4 M samples, which corresponds to an observation time of about 10 minutes (Thiagaraj et al., 2020).\nOur focus in the work is on the Fourier Domain Acceleration Search module. We have investigated the use of Mask R-CNN to detect candidates of accelerated binary pulsar signatures.\n3.1 Fourier Domain Acceleration Search\nThe intermediate output product of the Fourier Domain Acceleration Search (FDAS) processing when viewed as a particular image intensity mode a distinct shape (Hourglass, Butterfly) is observed. These butterfly shapes comes from the particular arrangement of the filters, and when an accelerated binary pulsar fundamental or harmonics signals gets deconvolved through them. The intensity, location and inclination of these shape / patterns give us information about these candidates.\nConventional processing in this stage, involves sift-\ning through the FOP array and performing a harmonic summing And thresholding. Additionally, basic CNNs (Convolutional Neural Network) perform classification, but we investigate an improved version of CNN, known as the Mask-RCNN that can identify the patterns and provide a mark the regions (bounding box) where pattern features are identified, Typically the CNN is computationally expensive to implement. In our case due to the use of accelerators additional computation steps for implementing convolutions is less significant.\nThe R-CNN and Fast R-CNN algorithm (Girshick, 2015) which uses a selective search algorithm could not be used for real life deployments as there was a performance bottleneck. The Faster R-CNN algorithm (Ren et al., 2015) could not be used as we needed a mask to be drawn for every detected object. Due to these reasons we chose Mask R-CNN to perform candidate signature detection. Our investigation is to see if the Mask R-CNN suite can be used to train and recognize these image patterns. We have identified the public domain availability of Mask R-CNN from matterport github repository.(Repository Link)1\n3.2 Mask R-CNN\nMask R-CNN (He et al., 2017) is a small, flexible generic object instance segmentation framework. It not only detects targets in the image, but also gives a high-quality segmentation result for each target. It is extended on the basis of Faster R-CNN, and adds a new branch for predicting an object mask which is parallel with bounding box recognition branch. For object classification we have used the default ResNet101 (He et al., 2015) as our classfication network. For the Detection network, we have used a Region Proposal Network which was proposed with the Faster R-CNN algorithm. Feature Extraction in the Mask R-CNN happens via the Feature Pyramid Network (FPN) algorithm. The FPN algorithm was designed to handle images of various sizes and shapes by keeping speed, memory and accuracy in mind. The algorithm consists of both a top down and bottom up approach for producing feature maps. The bottom up network features a regular convolutional neural network for extracting features from images. Every layer in the bottom up pyramid reduces spatially allowing for high level structures to be determined, thereby increasing the semantic value. The top down network provides a pathway to construct high resolution layers from the semantic rich layer. Additionally there are lateral connections provided to help the detector better detect the object loca-\n1https://github.com/matterport/Mask_RCNN\ntions. This algorithm has shown significant improvements over the other state-of-the-art approaches (Lin et al., 2016). Mask R-CNN defines the loss function 2 as follows:\nL = Lcls + Lbox + Lmask (1)\nWhere Lcls denotes the classification loss, Lbox denotes the bounding box loss and Lmask is the average binary cross entropy loss, only including k-th mask if the region is associated with the ground truth class k.\n3.3 Basic Tests with Mask R-CNN\nMask R-CNN is available as a github repository from matterport. The github repository is cloned onto our local system following the setup procedure given in the github repository\nAt the time of this work, the default Mask R-CNN available from the repository has been trained on 80 different objects from the MS COCO (Lin et al., 2014) dataset. Figure 6 is an example illustration to show the default detection capability of Mask R-CNN. Image shown is a city street and the detections are highlighted with bounding boxes.\nIn addition, in order to understand the training procedure of Mask R-CNN we have experimented it to detect damages in a set of car images. For this purpose we collected sample images from an external github repository (Repository Link)3. It was required to annotate the images using Visual Geometry Group (VGG) annotator. We then trained the Mask R-CNN suite using the standard procedures described in github. The loss function for this experiment remains the same as the standard loss function described in section 3.2. For this experiment we havent performed any predictive success analysis 4. Figure 7 shows the result from training Mask R-CNN on the car damage dataset.\nThe training of the neural network is an important task and this procedure is described in the next section.\n3.4 Training a neural network\nEvery neural network requires a good dataset (both qualitatively and quantitatively). A dataset is split into three parts i.e., a training set which is used in the neural network, a validation set which is used for preventing overfitting and a testing set to evaluate the performance of a neural network. During the training of the neural network there is a specific loss function which is de-\n2https://en.wikipedia.org/wiki/Loss_function 3https://github.com/priya-dwivedi/Deep-Learning/\ntree/master/mask_rcnn_damage_detection/customImages 4https://en.wikipedia.org/wiki/Predictive_analytics\nfined. A loss function can either be predetermined or a custom user defined. The loss function gives us an approximate idea about how well an algorithm models a given data. A neural network updates its weights with the backpropagation algorithm. A forward pass in a neural network is defined as one run starting from the input layer until the output layer. A combination of one forward propagation and one backward propagation is called an epoch. During the training cycle the algorithm is subjected to the training data and is validated with the validation dataset. When the validation loss begins exceeding the training loss we can safely conclude the training process and extract the weights. The weights are a crucial database produced at the end of the training process which will help in making appropriate connection between the neurons. The weights will be provided to the inferencing logic.\n3.5 Adapting Mask R-CNN for candidate detection\nAs discussed before, The default Mask R-CNN is used for detecting generic objects. We will have to train the Mask R-CNN to produce a new set of weights for detecting the candidate signatures (hourglass/butterfly patterns). This process involves four steps :\n\u2022 Preparing the dataset\n\u2022 Image Annotation\n\u2022 Training\n\u2022 Testing\n3.6 Dataset Preparation\nIn order to train the neural network, we have used different sets of binary pulsar signal data. We had a choice of using real observational data from a telescope or using simulated data. A telescope data will have the presence of interfering signals which is undesirable for initial training of the machine learning (ML) suite. In order to maintain a controlled test/training environment we have considered to use simulated datasets. Such simulated data are produced using a mock pulsar signal generation tool called SIGPROC5 fake utility. Fake is a command line program written to create test data sets containing pulses hidden in Gaussian noise background. Various (38 different) datasets for the training purpose were produced by modifying the -period, -width, -snrpeak, -binary, -bper parameters of the fake utility. We have used data files produced with accelerations (+/- 500ms\u22122, +/- 250ms\u22122, +/- 25ms\u22122 and 0\n5https://sigproc.sourceforge.net/sigproc.pdf\nms\u22122 ), periods (2 s and 0.002 s), pulse duty cycle ratios (0.4, 0.2, 0.1 and 0.05), a constant SNR value of 80 and dispersion measure of 1.0. The simulated data obtained are in a filterbank format. The sampling period was set to 64 microseconds and the observation period was set to 536 seconds (close to real observation parameters). The data files are first dedispersed and channel collapsed to form a time series of 8 million samples.6\nThe time series data is subjected to 8 million point Fast Fourier Transform (real to complex). The resulting\n6https://www.jb.man.ac.uk/research/pulsar/\nEducation/Tutorial/tut/tut.html\n4 million points after the transform are convolved with 85 templates to get a filter output plane (FOP), which is an array of dimension 85 rows by 4 million columns. A schematic of the data preparation flow is shown in Fig 9.\nThe FOP array is subjected to a sliding window approach to extract images used for training the Mask R-CNN model. The images are of size 85 rows by 256 columns (similar to an image having 85x256 pixel size). Since the FOP is a very large array, there are many regions in the array without any significantly useful data for the training purpose. So we have selected regions where there are more likelihood presence of\nsignals and noise combinations (starting from fundamental frequency locations to their multiple harmonic positions). This sliding window approach helped us to get 886 image segments (85x256 arrays) from the original FOP. The number 886 is an arbitrary choice and adequate to select a smaller training dataset. From this dataset we have picked up 50 good quality images for the training purposes and 34 random set of images for testing. Figure 10 shows the images obtained by processing different datasets that are having varying signal strengths (Signal to Noise Ratio). After the images are obtained they are subjected to image boundary annotation, as discussed in the next section.\n3.7 Image Annotation\nThe Mask R-CNN model requires annotating the images using an image boundary annotation tool. The developers recommend using a standard VGG annotator. Initially we have used this standard tool for annotating the images however we found this tool required more human involvement to edit the boundaries and appeared cumbersome to annotate large number of images . Hence we explored developing our own custom annotator tool with features to annotate the images a little faster than the VGG tool. The annotation speed performance was achieved by automatically drawing the six sides (edges) needed for bounding the butterfly image boundaries. As mentioned earlier,for VGG it was required to draw each of the six sides manually which was consuming more time. The custom annotator was made use of to test short bounding box based annotations. The working principles of the annotation tools are presented below for a comparison.\nVGG Annotator developed by the visual geometry group7 is very useful to draw multiple segment annotation boxes which are required for complex images such as humans, animals, buildings, cars etc (Dutta & Zisserman, 2019). After the manual annotation, the coordinates of the bounding box are saved in a JSON 8 file format. We have produced one set of (50 images) training image dataset using the VGG annotator. We annotated our training data set (consisting of 50 images) manually using a six point reference for the mask and saved the co-ordinates in a json format (Fig.11a-d).\nThe custom annotator was developed to speed up the annotation process9. It is based on python and javascript. The code can be executed in a web browser. On loading the image the user needs to select the mid\n7https://www.robots.ox.ac.uk/\u02dcvgg/software/via/ 8https://en.wikipedia.org/wiki/JSON 9https://drive.google.com/drive/folders/\n1JOeRlDB4ckhhG9QeNID1qK0eMF19Wbo8\npoint of the butterfly image and the tool will automatically draw the six edges of the bounding boxes. We have pre-estimated and fixed upper limits for the dimension of the bounding box and thereby the code could automatically complete the annotation process. Upon selecting (with a mouse-click) the butterfly image\u2019s center, the annotator calculates the six nodal points of the bounding box with respect to the center point chosen by the user. We have used this tool to create a second set of training dataset which is essentially the same as the one produced by the VGG annotator but with shorter wings (Fig.11e-f).\nAfter the annotation was completed we pass the dataset for training. The training process is explained in the next session.\n3.8 Training\nThe Mask R-CNN suite comes with a toolflow for training. It is a computationally intensive task and we have installed the required programs in the Google Colab 10 cloud environment with GPU-based acceleration. While performing the training we have saved the weights-file produced after every epoch. The number of epochs required is not known apriori. So we have arbitrarily started the training process with the limit set to six epochs. But we found that the training was converging much before the third epoch.\nThe loss curves is a progress indicator of the train-\n10https://colab.research.google.com/\ning process. Typically the flattening of the loss curve around lower values indicates that the training is complete and it can be terminated. At each new epoch the same dataset is fed, but in a shuffled order to the network in order to build an effective mapping function for the weights.\nFigure 12 shows the loss curves obtained for the training process. We can observe that after epoch two the loss curves are flattening. It can also be observed that two of our datasets (full extent annotated by VGG and short extent annotation by custom annotator) have shown similar trends. Since the loss curves started flattening at the second epoch we terminated our training process at the next (3rd) epoch. We utilized the transfer learning 11 method to train our Mask R-CNN suite between each of the epochs. The pretrained COCO weights (Lin et al., 2014) were used as the initial base weights file for the training. The model was trained with a learning rate of 0.001 on both datasets. The learning rate is a tuning parameter that determines the step size at each iteration while moving toward a minimum of a loss function. A larger learning rate signifies a higher chance of convergence at a local minima. A smaller learning rate signifies a higher chance of convergence at a global minima. The local and the global minima are referred to the loss function.\nAs mentioned before the training is computationally intensive and we have used Google Colab which is equipped with a Nvidia T4 GPU. Mask R-CNN has a specific configurable parameter which is known as IMAGES PER GPU. We found that by limiting the number of images to be processed by the GPU as two, the computation went faster. Figure 12 shows the model loss curves produced during the training process of the two datasets.\n3.9 Results from our experiment\nThe training process generates the weights file for inferencing. The weight file is collected and passed onto the Mask R-CNN inferencing logic. We have considered 34 images for testing purposes. Out of the 34 images, four images had a high SNR, 13 images had a medium SNR and 17 images had very low SNR. Figure 13 shows a subset of the images passed through the Mask R-CNN inferencing logic. As these were the first experimental results elaborate statistics were not collected. However we have the following qualitative analysis:\n\u2022 The full length annotation based training was able to identify the regions and mark them with\n11https://en.wikipedia.org/wiki/Transfer_learning\nhigh confidence scores. The masks appeared only for very high SNR cases.(Fig 13 a,b)\n\u2022 When the butterfly pattern is located around the edges of the image, we observed multiple detections having overlapped regions. (Fig 13 e,g,i)\n\u2022 With low SNR cases false detections appeared at multiple places. (Fig 13 k,l)\n\u2022 Short length annotation based training resulted in higher false detections in the high SNR cases.\n\u2022 The medium SNR images also gave multiple detected objects with overlapped regions. (Fig 13 h,i,j)\n\u2022 The low SNR cases also gave multiple false detections. (Fig 13 m,n)\n\u2022 In each of the above three cases the mask was not drawn over the detected objects.\n\u2022 Image pixel size of the training region seemed to play a major role in the detection and loss function estimation."
        },
        {
            "heading": "4. Discussion",
            "text": "The current design of SKA pulsar search is likely to use machine learning approaches in a variety of places, for example, for RFI detection and pulsar candidate identification (Lyon et al., 2016). We have investigated a machine learning approach for a new application in the search processing (Fourier Domain Acceleration Search (FDAS)) where conventional thresholding based approaches are usually followed. We have trained the network for a single feature detection. Usually the Mask R-CNN applications detect objects with well-defined boundaries where the object edges are well defined (sharp) with regard to the background. In our case the object has a fuzzy boundary but with a recognizable butterfly pattern for the human eyes. We have carried out this investigation to detect such fuzzy objects as a research work within the SKA pulsar search activity. Our work demonstrated the ability to train a network to detect such fuzzy objects. It has opened up further possible studies where we can do more quantitative studies on the algorithm and its computational performance improvements for the pulsar search work in general. In addition we have also looked at the limitations of the standard annotation tools and also studied the extent of annotation required using a semiautomated custom annotation tool. There is scope for further enhancement of this custom annotation tool. The present training method used a single feature, by including a few extended features to the training (some noise patterns) and code enhancements, we will be able to determine the location, inclination and intensity of the candidates more precisely. Such information will enable simplifying usual complexities associated with subsequent candidate sifting (Sorting) and related processing. Thus our detection pipeline can be further improved to provide higher level of information during the search."
        },
        {
            "heading": "5. Future Prospects",
            "text": "The practical implementation of this new scheme and its interfaces with the processing pipeline needs to be further explored. For this purpose we have investigated the use of FPGA platform and OpenCL languages. We have carried out a study for hardware acceleration of Mask R-CNN for a future implementation. Since the pulsar search pipeline runs on an accelerator,we have studied the possibility of using an FPGA platform. In our early investigation we found the OpenCL implementation of CNN available for DE-10 Nano FPGA\nboard 12. However more work needs to be done to map\n12https://www.intel.com/content/www/us/en/developer/\ntopic-technology/edge-5g/hardware/fpga-de10-nano\nthe different libraries and enhance it for Mask R-CNN. A proposed implementation of Mask R-CNN on hardware platform is shown in Figure 14. We have also made a docker image of the basic Mask R-CNN software and will be enhancing it to deploy it over kuber-\nnetes clusters. More investigation will have to be performed with a larger dataset. In the future study we will have more quantitative study with multiple classes. We will also be investigating the image pixelization aspects in the future work.\nWe would like to use real telescope data for our further investigations. In our future work, we will be investigating the Masking logic for identifying overlapping instances of butterfly pattern in an RFI background. Modern radio telescope data analysis faces challenges in the form of Radio Frequency Interference (RFI). The effect of RFI on our scheme requires future investigation. Normally RFI can be dealt with by capturing data in an RFI free observatory sites, removing the RFI prone datasets or by replacing the RFI affected set by benign random or median values (Buch et al., 2018, 2022; Buch et al., 2016, 2019). We have also proposed in our earlier work (Bhat et al., 2020) a novel method to identify observation slots that are likely to be free from RFI. It proposes a routine analysis of the radio telescope incoming data streams in order to identify RFI free observation slots using machine learning techniques. The methodology basically makes use of statistical analysis and detecting outliers in the data to build a comprehensive database. This database can be analyzed to view the RFI trends with time. The potential RFI free slots can be predicted using LSTM (Long Short Term Memory) techniques (Misra et al., 2007, 2008).\nThe current work was taken up as a research activity towards identifying new algorithms and techniques for future implementations."
        },
        {
            "heading": "6. Conclusion",
            "text": "The paper showed the potential of a machine learning algorithm Mask R-CNN for detecting candidate images in a pulsar search pipeline. We have tested this concept with a set of simulated data and shown the results here. We have also investigated the aspects of annotation for the training and presented a brief discussion. We have presented these details from the perspective of future improvements and hardware implementation. Our training made use of the cloud based computational infrastructure provided by Google. The entire work details, codes developed, data images used, output products including the neural network weight files are documented and available via github13. We anticipate the future pulsar search and similar candidate detection will benefit from this work."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank the anonymous referee whose comments have vastly enhanced the quality of this manuscript. The authors would also like to thank Raman Research Institute, University of Manchester, PES University and BITS Goa for their support."
        }
    ],
    "title": "Investigation of a Machine learning methodology for the SKA pulsar search pipeline",
    "year": 2023
}