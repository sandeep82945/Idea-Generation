{
    "abstractText": "ABSTRACT Recent research on vulnerabilities of deep reinforcement learning (RL) has shown that adversarial policies adopted by an adversary agent can influence a target RL agent (victim agent) to perform poorly in a multi-agent environment. In existing studies, adversarial policies are directly trained based on experiences of interacting with the victim agent. There is a key shortcoming of this approach \u2014 knowledge derived from historical interactions may not be properly generalized to unexplored policy regions of the victim agent, making the trained adversarial policy significantly less effective. In this work, we design a new effective adversarial policy learning algorithm that overcomes this shortcoming. The core idea of our new algorithm is to create a new imitator \u2014 the imitator will learn to imitate the victim agent\u2019s policy while the adversarial policy will be trained not only based on interactions with the victim agent but also based on feedback from the imitator to forecast victim\u2019s intention. By doing so, we can leverage the capability of imitation learning in well capturing underlying characteristics of the victim policy only based on sample trajectories of the victim. Our victim imitation learning model differs from prior models as the environment\u2019s dynamics are driven by adversary\u2019s policy and will keep changing during the adversarial policy training. We provide a provable bound to guarantee a desired imitating policy when the adversary\u2019s policy becomes stable. We further strengthen our adversarial policy learning by making our imitator a stronger version of the victim. That is, we incorporate the opposite of the adversary\u2019s value function to the imitation objective, leading the imitator not only to learn the victim policy but also to be adversarial to the adversary. Finally, our extensive experiments using four competitive MuJoCo game environments show that our proposed adversarial policy learning algorithm outperforms state-of-the-art algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "The Viet Bui"
        },
        {
            "affiliations": [],
            "name": "Tien Mai"
        },
        {
            "affiliations": [],
            "name": "Thanh H.Nguyen"
        }
    ],
    "id": "SP:144af3c708c4c3857be013bc119e3cd4dd50654e",
    "references": [
        {
            "authors": [
                "Trapit Bansal",
                "Jakub W. Pachocki",
                "Szymon Sidor",
                "Ilya Sutskever",
                "Igor Mordatch"
            ],
            "title": "Emergent Complexity via Multi-Agent Competition",
            "venue": "ArXiv abs/1710.03748",
            "year": 2018
        },
        {
            "authors": [
                "Vahid Behzadan",
                "Arslan Munir"
            ],
            "title": "Vulnerability of deep reinforcement learning to policy induction attacks",
            "venue": "In International Conference on Machine Learning and Data Mining in Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "German Ros",
                "Felipe Codevilla",
                "Antonio Lopez",
                "Vladlen Koltun"
            ],
            "title": "CARLA: An open urban driving simulator",
            "venue": "In Conference on robot learning. PMLR,",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Paul Christiano",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models",
            "year": 2016
        },
        {
            "authors": [
                "Chelsea Finn",
                "Sergey Levine",
                "Pieter Abbeel"
            ],
            "title": "Guided cost learning: Deep inverse optimal control via policy optimization",
            "venue": "In International conference on machine learning",
            "year": 2016
        },
        {
            "authors": [
                "Justin Fu",
                "Katie Luo",
                "Sergey Levine"
            ],
            "title": "Learning robust rewards with adversarial inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:1710.11248",
            "year": 2017
        },
        {
            "authors": [
                "Adam Gleave",
                "Michael Dennis",
                "Neel Kant",
                "Cody Wild",
                "Sergey Levine",
                "Stuart J. Russell"
            ],
            "title": "Adversarial Policies: Attacking Deep Reinforcement Learning",
            "year": 2020
        },
        {
            "authors": [
                "Wenbo Guo",
                "Xian Wu",
                "Sui Huang",
                "Xinyu Xing"
            ],
            "title": "Adversarial Policy Learning in Two-player Competitive Games",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in neural information processing systems",
            "year": 2016
        },
        {
            "authors": [
                "Sham Kakade",
                "John Langford"
            ],
            "title": "Approximately optimal approximate reinforcement learning",
            "venue": "In In Proc. 19th International Conference on Machine Learning",
            "year": 2002
        },
        {
            "authors": [
                "Jernej Kos",
                "Dawn Song"
            ],
            "title": "Delving into adversarial attacks on deep policies",
            "venue": "arXiv preprint arXiv:1705.06452",
            "year": 2017
        },
        {
            "authors": [
                "Xian Yeow Lee",
                "Sambit Ghadai",
                "Kai Liang Tan",
                "Chinmay Hegde",
                "Soumik Sarkar"
            ],
            "title": "Spatiotemporally constrained action space attacks on deep reinforcement learning agents",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Denis Yarats",
                "Yann N Dauphin",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Deal or no deal? end-to-end learning for negotiation dialogues",
            "year": 2017
        },
        {
            "authors": [
                "Jieyu Lin",
                "Kristina Dzeparoska",
                "Sai Qian Zhang",
                "Alberto Leon-Garcia",
                "Nicolas Papernot"
            ],
            "title": "On the robustness of cooperative multi-agent reinforcement learning",
            "venue": "IEEE Security and Privacy Workshops (SPW)",
            "year": 2020
        },
        {
            "authors": [
                "Yen-Chen Lin",
                "Zhang-Wei Hong",
                "Yuan-Hong Liao",
                "Meng-Li Shih",
                "Ming-Yu Liu",
                "Min Sun"
            ],
            "title": "Tactics of adversarial attack on deep reinforcement learning agents",
            "year": 2017
        },
        {
            "authors": [
                "Yuzhe Ma",
                "Xuezhou Zhang",
                "Wen Sun",
                "Jerry Zhu"
            ],
            "title": "Policy poisoning in batch reinforcement learning and control",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Mohammadreza Nazari",
                "Afshin Oroojlooy",
                "Lawrence Snyder",
                "Martin Tak\u00e1c"
            ],
            "title": "Reinforcement learning for solving the vehicle routing problem",
            "venue": "Advances in neural information processing systems",
            "year": 2018
        },
        {
            "authors": [
                "Laura Noonan"
            ],
            "title": "JPMorgan develops robot to execute trades",
            "venue": "Financial Times",
            "year": 2017
        },
        {
            "authors": [
                "Amin Rakhsha",
                "Goran Radanovic",
                "Rati Devidze",
                "Xiaojin Zhu",
                "Adish Singla"
            ],
            "title": "Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning",
            "venue": "In International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Amin Rakhsha",
                "Xuezhou Zhang",
                "Xiaojin Zhu",
                "Adish Singla"
            ],
            "title": "Reward poisoning in reinforcement learning: Attacks against unknown learners in unknown environments",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Alessio Russo",
                "Alexandre Proutiere"
            ],
            "title": "Optimal attacks on reinforcement learning policies",
            "venue": "arXiv preprint arXiv:1907.13548",
            "year": 2019
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In International conference on machine learning",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347",
            "year": 2017
        },
        {
            "authors": [
                "Jiaming Song",
                "Hongyu Ren",
                "Dorsa Sadigh",
                "Stefano Ermon"
            ],
            "title": "Multiagent generative adversarial imitation learning",
            "venue": "Advances in neural information processing systems",
            "year": 2018
        },
        {
            "authors": [
                "Jianwen Sun",
                "Tianwei Zhang",
                "Xiaofei Xie",
                "Lei Ma",
                "Yan Zheng",
                "Kangjie Chen",
                "Yang Liu"
            ],
            "title": "Stealthy and efficient adversarial attacks against deep reinforcement learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "MTCAJ Thomas",
                "A Thomas Joy"
            ],
            "title": "Elements of information theory. Wiley- Interscience",
            "year": 2006
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ international conference on intelligent robots and systems",
            "year": 2012
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of machine learning research 9,",
            "year": 2008
        },
        {
            "authors": [
                "Chaowei Xiao",
                "Xinlei Pan",
                "WarrenHe",
                "Jian Peng",
                "Mingjie Sun",
                "Jinfeng Yi",
                "Mingyan Liu",
                "Bo Li",
                "Dawn Song"
            ],
            "title": "Characterizing attacks on deep reinforcement learning",
            "year": 2019
        },
        {
            "authors": [
                "Lantao Yu",
                "Jiaming Song",
                "Stefano Ermon"
            ],
            "title": "Multi-agent adversarial inverse reinforcement learning",
            "venue": "In International Conference on Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Robust reinforcement learning on state observations with learned optimal adversary",
            "venue": "arXiv preprint",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "KEYWORDS",
            "text": "Reinforcement Learning, Non-zero-sum Multi-agent Competition, Adversarial Policy, Imitation Learning"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Exploring vulnerabilities of deep reinforcement learning has drawn a lot of interests from the AI research community [2, 17, 20, 21], given recent successes of deep RL in accomplishing a variety of interesting multi-agent learning tasks [3, 14, 18, 19]. Most of the existing work follows the traditional adversarial learning framework which often makes strong assumptions about the adversary\u2019s capabilities. Typically, the attacker is assumed to be able to manipulate input image observations or even interfere with the learning\nPreprint, , , . \u00a9 2022\nprocess of the victim agent. As pointed out in some recent work [7], these assumptions are not practical, especially in real-world domains such as autonomous driving in which the attacker cannot easily modify the input of the victim policy.\nA recent alternative attack approach to the victim policy was introduced in [7] which presents the idea of adversarial policy. Essentially, an attacker can build an adversarial policy for an opponent agent (this agent is under control by the attacker, thus is called adversary agent in our paper) that takes actions in a shared environment with the victim. This adversarial policy can weaken the outcome of the victim\u2019s policy, not by making the opponent choose stronger actions, but instead by inducing natural observations that can lead the victim to behave in an undesired way. Following this interesting approach, Guo et al. [8] extended the two-player zero-sum Markov game model used in [7] for the general non-zero-sum game setting and introduced a more effective adversarial policy learning algorithm based on a surrogate learning objective function.\nMotivated by these initial successes which demonstrate negative effects of adversarial policy on the victim\u2019s policy, our paper focuses on designing new stronger adversarial policies in the non-zerosum Markov game setting. Our key contribution is to introduce an imitator of which goal is to discover intrinsic properties of the victim\u2019s policy from historical interactions between the victim and adversary agents \u2014 this knowledge is then transferred to the attacker to improve the adversarial policy. By following this idea, we are able to exploit the advantage of imitation learning [9] to anticipate the victim agent\u2019s moves in unexplored policy regions, allowing us to strengthen the generated adversarial policy. This is a significant advancement compared to previous work [7, 8] which learns an adversarial policy directly from past interactions with the victim agent, substantially limiting the impact of the trained adversarial policy on unseen policy regions of the victim agent.\nA key challenge in incorporating our new imitator into the adversarial policy learning framework is that the imitator is trained simultaneously with the adversarial policy learning \u2014 the interdependency between the imitator and the adversary agent complicates the entire training process. Note that in traditional imitation learning, the set of the victim\u2019s policy trajectories used for training the imitator is fixed during the training process. On the other hand, in the context of adversarial policy learning, the environment\u2019s dynamics (and as a result, the victim trajectories obtained for training the imitator) are driven by the adversarial policy. The adversarial policy, in turn, is trained based on the policy output of the imitator. This learning inter-dependency complication requires a carefully designed training process to ensure a convergence to high-quality outcomes for both the imitation policy and the adversarial policy.\nar X\niv :2\n21 0.\n16 91\n5v 1\n[ cs\n.L G\n] 3\n0 O\nct 2\n02 2\nTo address this learning challenge, we provide the following contributions: (i) we theoretically characterize the dependency between the adversarial policy and the imitation policy; (ii) we provide a provable bound to guarantee a desired imitating policy when the adversary\u2019s policy becomes stable; and (iii) we further strengthen our adversarial policy learning by enhancing our imitator \u2014 we incorporate the negation of the adversary\u2019s goal into the imitator\u2019s objective function, making the imitator to both learn the victim policy and be adversarial to the adversary agent.\nLastly, we conduct extensive experiments on four competitive MuJoCo game environments introduced by Emergent Complexity [1] (including Kick And Defend, You Shall Not Pass, Sumo Humans, and Sumo Ants) to evaluate our proposed adversarial policy learning algorithms. We empirically show that our generated adversarial policies obtained a significantly higher winning (plus tie) rates against the victim agent in these game environments, in comparison with state-of-the-art adversarial policy methods. We further show that we can make the victim agent substantially more resilient to any adversarial policies (generated by different algorithms) by retraining the victim policy against our adversarial policies."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Attacks on deep RL. Existing works have focused on creating attacks by manipulating victim\u2019s observations or victim\u2019s actions. For example, [10] and [2] propose to perturb victim\u2019s observations to force the victim to make sub-optimal actions, thus fails the task. Later works [12, 16, 22, 26, 32] extend this approach by proposing to manipulate victim\u2019s observations at some selected time steps, instead of the whole trajectories. Other papers [2, 10, 15, 30, 33] focus on attacks through observation manipulation but in black-box settings, i.e., the adversary does not have the power to manipulate victim\u2019s observations, but can access the input and output of the victim\u2019s policy or deep-Q networks. Besides, there are works that propose to directly perturb actions taken by victim agents in both white-box and black-box settings [13, 30]. There is another emerging line of approaches that view the attacks as a two-player competitive game, i.e., focusing on training an adversarial agent to play with the victim. For example, [7] train an adversarial agent using Proximal Policy Optimization (PPO) [24] for a set of MuJoCo game environments [28], and [8] propose to break the zero-sum-game setting and redesign the adversary\u2019s reward function to achieve better adversary agents. Our methods belong to this direction, but differ from prior methods as we create an imitator that uses imitation learning to mimic and predict victim\u2019s intention \u2014 this prediction can be used to further strengthen the adversarial agent training.\nImitation learning. A core component of our algorithms is an imitation learning model trained to mimic and product similar victim\u2019s actions. We employed an extended version of the Generative Adversarial Imitation Learning (GAIL) algorithm [9, 25], a state-of-the-art imitation leaning algorithm that is highly scalable for continuous domains such as the MuJoCo ones. The literature on learning from expert demonstrations covers both imitation learning and inverse reinforcement learning (IRL) works. While imitation learning presents a direct approach to imitate expert\u2019s policies, IRL [4\u20136, 31] assumes that expert\u2019s policy is driven by an expert\u2019s reward function, thus propose to infer this function\nfrom demonstrations. Even-though IRL is more transferable for changing environments, it is often less effective in mimicking expert\u2019s demonstrations [9]. On the other hand, for the MuJoCo game environments, since the rewards are obvious, imitation learning (or specifically GAIL) provides us with a direct and suitable algorithm for imitating the victim. Note that, in our settings, the victim\u2019s environmental dynamics keep changing during the adversarial training, raising a need for redesigning the GAIL, in both theoretical and practical aspects. We address this issue later in this paper."
        },
        {
            "heading": "3 ADVERSARIAL POLICY FRAMEWORK",
            "text": "Following the general framework introduced in [7], we consider a two-player Markov game in which the victim plays against an opponent which is under control by the adversary. We thus name these two players the victim agent and adversary agent. We represent the two-player non-zero-sum Markov game as a tuple:\n(S,A\ud835\udefc ,A\ud835\udf08 , q\ud835\udefc , q\ud835\udf08 , r\ud835\udefc , r\ud835\udf08 , \ud835\udefe), where\ud835\udefc refers to the adversary and\ud835\udf08 refers to the victim. In addition, S is the set of the states, (A\ud835\udefc ,A\ud835\udf08 ) are sets of actions, (q\ud835\udefc , q\ud835\udf08 ) are transition probabilities and (r\ud835\udefc , r\ud835\udf08 ) are reward functions of the two players, respectively. Finally, \ud835\udefe \u2208 [0, 1] is a discount factor. The objective of each player is to maximize his/her long-term expected reward. Essentially, given a policy of the victim, denoted by \ud835\udf0b\ud835\udf08 , the adversary aims at finding an optimal policy \ud835\udf0b\ud835\udefc that maximizes their long-term reward, formulated as follows:\nmax \ud835\udf0b\ud835\udefc\n{ \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08 )) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08 )]} ,\nwhere \ud835\udc600 is the initial state and \ud835\udf0f denotes a trajectory sampled from executing the adversary policy \ud835\udf0b\ud835\udefc in the environment and \ud835\udc60\ud835\udc61 \u2208 \ud835\udf0f for all \ud835\udc61 . Transition probabilities of the adversary, denoted by q\ud835\udefc (\ud835\udf0b\ud835\udf08 ), depends on the policy of the victim \ud835\udf0b\ud835\udf08 . Specifically, the transition probability \ud835\udc5e\ud835\udefc (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 ) can be generally computed as follows:\n\ud835\udc5e\ud835\udefc (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 ) = \u2211\ufe01\n\ud835\udc4e\ud835\udf08 \u2208A\ud835\udf08 \ud835\udf0b\ud835\udf08 (\ud835\udc4e\ud835\udf08\ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ),\nwhere \ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc , \ud835\udc4e\ud835\udf08\ud835\udc61 ) is the probability of reaching state \ud835\udc60\ud835\udc61+1 if the adversary and victim agents take action \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e \ud835\udf08 \ud835\udc61 , respectively, at state \ud835\udc60\ud835\udc61 . In other words, if the policy of the victim is fixed, then the transition probabilities q\ud835\udefc are also fixed. Similarly, the objective of the victim is to maximize the expected long-term rewards of the victim, formulated as follows:\nmax \ud835\udf0b\ud835\udf08\n{ \ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )]} .\nIntuitively, if the policy of one player is fixed, then the transition probabilities (or dynamics) of the other player\u2019s environment is also fixed; thus the two-player game becomes a standard RL task.\nIn this paper, we assume the victim follows a fixed policy (which was pre-trained). This is a common assumption in adversarial policy learning research, motivated by real-world settings such as autonomous vehicles in which RL-trained policies might be deployed [8]. We will later discuss the effect of unfixed victim\u2019s policies on the adversarial policy training. As mentioned previously, existing work directly trains adversarial policies based on interactions with the victim. We instead create an imitator who follows\nimitation learning to discover underlying characteristics of the victim policy and transfers that knowledge to the adversary, helping the adversary produces a better adversarial policy. Both the imitator and the adversary policies will be trained simultaneously based on interactions between the adversary and the victim.\nNext, wewill first present our imitation learning of the victim policy. We then follow with the elaboration on our adversarial policy learning that incorporates the victim imitation learning component."
        },
        {
            "heading": "4 VICTIM IMITATION LEARNING",
            "text": "In standard imitation learning, we learn to imitate an expert (which is the victim in our study) based on a fixed set of trajectories sampled from the expert\u2019s policy. On the other hand, in our problem, learning the victim policy is more challenging since it involves the adversarial policy which is also being trained at the same time (our observations of the victim policy depend on what policy the adversary is playing). In the following, we first introduce our advanced imitation model and algorithm given a fixed adversary policy. We then present our theoretical results on the impact of the adversary policy (during the training process) on our imitation learning."
        },
        {
            "heading": "4.1 Enhanced Imitation Learning Model",
            "text": "Our objective is to build an imitation learning model to imitate the victim\u2019s policy through observing victim trajectories. Our model is essentially an enhanced version of the GAIL algorithm [9]. Overall, following GAIL, given an attacker policy \ud835\udf0b\ud835\udefc , an imitation policy can be learned by solving the following saddle point problem:\nmax \ud835\udf0b\ud835\udf08 \ud835\udf13 min \ud835\udc37\ud835\udc64\n{ \ud835\udf19 (\ud835\udf0b\ud835\udf08\n\ud835\udf13 , \ud835\udc37\ud835\udc64) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08\n\ud835\udf13 [\u2211\ufe01 \ud835\udc61 log(\ud835\udc37\ud835\udc64 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] + E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 log(1 \u2212 \ud835\udc37\ud835\udc64 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212 \ud835\udf06\ud835\udc3b (\ud835\udf0b\ud835\udf08 \ud835\udf13 ) } (1)\nwhere\ud835\udc3b (\u00b7) is the entropy function, \ud835\udf0b\ud835\udf08 \ud835\udf13 refers to the imitating policy which is an output of a neural net with parameter\ud835\udf13 , and \ud835\udf0b\ud835\udf08 is the victim\u2019s policy to be imitated. In addition, \ud835\udc37 is a discriminative neural net model with parameter\ud835\udc64 to distinguish between trajectories generated by \ud835\udf0b\ud835\udf08\n\ud835\udf13 and those from the victim\u2019s policy. Normally, GAIL\nwould require a large amount of victim\u2019s trajectories to provide a good imitation policy. Since we want the imitation model to work with the adversary\u2019s policy optimization simultaneously, this is difficult to achieve at early episodes when the set of demonstrations from the victim is limited. Therefore, we propose to robustify GAIL by adding the adversary\u2019s value function to the objective:\nmax \ud835\udf0b\ud835\udf08 \ud835\udf13 min \ud835\udc37\ud835\udc64\n{ \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\n\ud835\udf13 , \ud835\udc37\ud835\udc64 \ud835\udf0b\ud835\udefc ) = \ud835\udf19 (\ud835\udf0b\ud835\udf08 \ud835\udf13 , \ud835\udc37\ud835\udc64) \u2212\ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 ))\n} (2)\nIn this enhanced model (2), the aim to train a policy that both mimics the victim and minimizes the adversary\u2019s long-term reward. The value function \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )) is the adversary\u2019s expected reward, but defined as a function of imitator\u2019s policy. The inclusion of the opponent\u2019s value function makes (2) not straightforward to handle and would require a redesign of the objective function to make it practical, as stated in [8]. Despite of that, we can provide, in the following, a simple formulation for the gradient of\ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )),\nmaking it convenient to be handled by a standard policy optimization algorithm, e.g., TRPO or PPO [23, 24].\nLemma 4.1. The gradient of the value function for the adversary \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )) w.r.t\ud835\udf13 can be computed as follows:\n\u2207\ud835\udf13 ( \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )) ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08\n\ud835\udf13\n[ \ud835\udc45\ud835\udefc (\ud835\udf0f) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc ))] , where \ud835\udc45\ud835\udefc (\ud835\udf0f) = \u2211\ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) with \ud835\udc60\ud835\udc61 \u2208 \ud835\udf0f .\nAs a result, at each imitation learning step, after updating the discriminator \ud835\udc37\ud835\udc64 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ), one can update the imitating policy \ud835\udf0b\ud835\udf08\ud835\udf13 using the gradient given in Proposition (4.2) below.1\nProposition 4.2. The gradient of the objective (2) w.r.t.\ud835\udf13 can be computed as follows:\nE\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 \ud835\udf13 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] \u2212\ud835\udf06\u2207\ud835\udf13\ud835\udc3b (\ud835\udf0b\ud835\udf08\ud835\udf13 )\nwhere \ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) = log(\ud835\udc37\ud835\udc64 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )) \u2212 \ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) .\nWith all the findings above, we can show that the enhanced imitation learning model (2) can be converted into a standard GAIL with a modified objective function, with a note that the imitation learning model depends on the adversary\u2019s policy \ud835\udf0b\ud835\udefc , which dictates the dynamics of the victim\u2019s environment.\nCorollary 4.3. The enhanced imitation learning model (2) is equivalent to GAIL with the modified objective:\nmax \ud835\udf0b\ud835\udf08 min \ud835\udc37\ud835\udc64\n{ \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\n\ud835\udf13 , \ud835\udc37\ud835\udc64 |\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08\n\ud835\udf13 [\u2211\ufe01 \ud835\udc61 \ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )]\n+ E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01\n\ud835\udc61\nlog(1 \u2212 \ud835\udc37\ud835\udc64 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212 \ud835\udf06\ud835\udc3b (\ud835\udf0b\ud835\udf08 \ud835\udf13 ) } (3)"
        },
        {
            "heading": "4.2 Imitation Learning Algorithm",
            "text": "With all the theoretical results on gradient computation developed in the previous section, we are now ready for the victim imitation learning algorithm. As shown in Corollary 4.3, the enhanced imitation learning model can be converted to a standard one, implying that the same optimization steps in [9] can be used with the the modified discriminator\u2019s objective. That is, at each iteration of the adversarial policy optimization, one can follow the following three steps to update the imitating policy \ud835\udf0b\ud835\udf08\n\ud835\udf13 :\n(i) Sample imitation trajectories \ud835\udf0f\ud835\udf08 \ud835\udc56 \u223c (\ud835\udf0b\ud835\udf08 \ud835\udf13 , \ud835\udf0b\ud835\udefc ).\n(ii) Update the discriminator \ud835\udc37\ud835\udc64 (\ud835\udc60, \ud835\udc4e\ud835\udf08 ) with the gradients:\nE\ud835\udf0f\ud835\udf08 \ud835\udc56 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\u2207\ud835\udc64 log(\ud835\udc37\ud835\udc64 (\u00b7)) ] + E\ud835\udf0f\ud835\udf08 \ud835\udc38 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\u2207\ud835\udc64 log(1 \u2212 \ud835\udc37\ud835\udc64 (\u00b7)) ] (4)\nwhere \ud835\udf0f\ud835\udf08 \ud835\udc38 are historical trajectories of the victim collected from interactions between the adversary and the victim.\n1Detailed proofs of all theoretical results are in the appendix.\n(iii) Update\ud835\udf13 with the gradients:\nE\ud835\udf0f\ud835\udf08 \ud835\udc56 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] \u2212\ud835\udf06\u2207\ud835\udf13\ud835\udc3b (\ud835\udf0b\ud835\udf08\ud835\udf13 ), (5)\nwhich is a standard policy gradient update, for which one can use TRPO [23] or PPO [24].\nFor all the updates above, we interact with an adversary of policy \ud835\udf0b\ud835\udefc . This policy will keep changing during the adversarial policy learning and affect the victim\u2019s environmental dynamics. This would make the imitation learning process unstable and challenging to handle. We analyze the effect of the adversary\u2019s policy on the imitating policy in Section 4.3."
        },
        {
            "heading": "4.3 Effects of the Adversary\u2019s Policy on the",
            "text": "Victim Imitation Policy\nIt is important to see that our imitation learning differs from the standard GAIL as the dynamics q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) are dependent of the adversary policy \ud835\udf0b\ud835\udefc and our imitation learning model will be trained simultaneously with the adversary\u2019s policy. This raises questions of how the learning of the imitating policy is affected by such changing dynamics, and whether one can get a desired imitating policy when the adversary\u2019s policy gets stable. To answer these questions, let use consider the following victim\u2019s expected reward as a function of adversary\u2019s policy.\n\u0393(\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] .\nThat is, \u0393(\ud835\udf0b\ud835\udefc ) is the expected reward that the victim can get by running a fixed policy \ud835\udf0b\ud835\udf08 when the adversary policy is \ud835\udf0b\ud835\udefc . Lemma 4.4 establishes a bound for the gap |\u0393(\ud835\udf0b\ud835\udefc ) \u2212\u0393(\ud835\udf0b\ud835\udefc ) |, which implies that \u0393(\ud835\udf0b\ud835\udefc ) will converge to \u0393(\ud835\udf0b\ud835\udefc ) if \ud835\udf0b\ud835\udefc gets close to \ud835\udf0b\ud835\udefc .\nLemma 4.4. Given two adversary policies \ud835\udf0b\ud835\udefc and \ud835\udf0b\ud835\udefc , let H = max\ud835\udc60 {|\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) |}. We obtain the following bound: \u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) \u2264 \ud835\udefeH\u221a2 ln 2\n1 \u2212 \ud835\udefe max\ud835\udc60\u2208\ud835\udc46\n{\u221a\ufe01 \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } where \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) is the KL divergence between the two adversary policies \ud835\udf0b\ud835\udefc and \ud835\udf0b\ud835\udefc .\nTo prove the above lemma, we extend the concept of the advantage function popularly used in single-agent RL [11, 23] to introduce the following victim\u2019s competitive advantage function, for any two states \ud835\udc60, \ud835\udc60 , conditional on adversary\u2019s policy \ud835\udf0b\ud835\udefc ,\n\ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60) = \ud835\udc5f \ud835\udf08 (\ud835\udc60) + \ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) . This allow use to write the expected reward \u0393(\ud835\udf0b\ud835\udefc ) in terms of the expected long-term competitive advantage function over another adversary policy \ud835\udf0b\ud835\udefc .\n\u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61 ( \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60\ud835\udc61 , \ud835\udc60\ud835\udc61+1) ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] with a note that E\ud835\udc60\ud835\udc61+1\u223cq(\ud835\udf0b\ud835\udefc ) [\ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60\ud835\udc61 , \ud835\udc60\ud835\udc61+1)] = 0. This identity expresses the expected return of the adversary policy \ud835\udf0b\ud835\udefc over another policy \ud835\udf0b\ud835\udefc in terms of victim\u2019s expected rewards. The competitive advantage function can be further bounded as\nE\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [\ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60)] \u2264 \ud835\udefeH max \ud835\udc60 | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) \u2212 \ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |1,\nwhich can further bounded by\n\ud835\udefeH max \ud835\udc60\n{\u221a\ufe01 2 ln 2\ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } .\nThe full proof is given in the appendix. The proof of Lemma 4.4 also reveals a bound based on maximum norm |\u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) | \u2264 H\ud835\udefe 1\u2212\ud835\udefe | |\ud835\udf0b\n\ud835\udefc \u2212 \ud835\udf0b\ud835\udefc | |\u221e, implying that \u0393(\ud835\udf0b\ud835\udefc ) is Lipschitz continuous in \ud835\udf0b\ud835\udefc with Lipschitz constant H\ud835\udefe1\u2212\ud835\udefe .\nNow, let \ud835\udf0b\ud835\udefc\u2217 be a target adversary\u2019s policy that the imitator should be trained with. This would be a trained adversary\u2019s policy after the adversarial policy learning. If the imitating policy is trained with another adversary\u2019s policy, we aim to explore how this imitating policy performs under the target policy \ud835\udf0b\ud835\udefc\u2217. Theorem 5.3 below establishes a performance guarantee for the imitating policy if it is trained with a different adversary\u2019s policy.\nTheorem 4.5. Suppose that discriminator\u2019s network model \ud835\udc37 of (2) varies within [\ud835\udc37\ud835\udc3f, \ud835\udc37\ud835\udc48 ] \u2282 [0, 1]. Let \ud835\udf0b\ud835\udefc\u2217 be the target adversary policy that we want to train the imitation policy with, and let (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217) be the imitation policy and the imitator\u2019s discriminator trained with another adversary \ud835\udf0b\ud835\udefc , we have the following performance guarantee for \ud835\udf0b\ud835\udf08\u2217. \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212max\n\ud835\udf0b\ud835\udf08 min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)}\n\u2264 2\ud835\udc3e max\n\ud835\udc60\u2208\ud835\udc46\n{\u221a\ufe01 \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc\u2217 (\u00b7|\ud835\udc60)) } ,\nwhere\n\ud835\udc3e = \ud835\udefe \u221a 2 ln 2\n( max\ud835\udc60 {\ud835\udc5f \ud835\udf08 (\ud835\udc60)} \u2212 log(\ud835\udc37\ud835\udc3f \u2212 \ud835\udc37\ud835\udc3f\ud835\udc37\ud835\udc48 ) ) (1 \u2212 \ud835\udefe)2 .\nSince the adversary policy \ud835\udf0b\ud835\udefc will keep changing during our adversarial policy optimization, Theorem 4.5 implies that the imitating policy will be stable if \ud835\udf0b\ud835\udefc becomes stable, and if \ud835\udf0b\ud835\udefc is approaching the target adversary\u2019s policy, the imitator\u2019s policy also converges to the one that is trained with the target adversary policy with rate O (\u221a\ufe01 \ud835\udc37KL (\ud835\udf0b\ud835\udefc | |\ud835\udf0b\ud835\udefc\u2217) ) . In other words, if the actual policy \ud835\udf0b\ud835\udefc is not not too far from the target \ud835\udf0b\ud835\udefc\u2217 such that \ud835\udc37KL (\ud835\udf0b\ud835\udefc | |\ud835\udf0b\ud835\udefc\u2217) \u2264 \ud835\udf16 , then the expected return of the imitating policy is within a O( \u221a \ud835\udf16) neighbourhood of the desired \u201cexpected return\u201d."
        },
        {
            "heading": "5 ADVERSARIAL POLICY TRAINING",
            "text": "We now discuss our main adversarial policy learning algorithm. We start by explaining the adversarial policy model introduced in [8], upon which we build our new adversarial policy learning algorithm. We then introduce our integration of the victim imitator and our new corresponding main learning algorithm. Finally, we provide our theoretical analysis on the worst-case performance of our learning algorithm when the victim\u2019s policy is not fixed."
        },
        {
            "heading": "5.1 Adversarial Policy Learning with Integration of Victim Imitator",
            "text": "Similar to prior works, we assume that the policy of victim is fixed and the aim is to learn an adversary policy to maximize the chances of winning (or win and tie). Similarly to [8], we train the policy by\nmaximizing the following enhanced objective\nmax \ud835\udf0b\ud835\udefc\n{ \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) } (6)\nwhere\ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600) is the expected long-term reward of the adversary by following the policy \ud835\udf0b\ud835\udefc but the transition probabilities are affected by the victim policy \ud835\udf0b\ud835\udf08 . The objective in (6) involves both the value functions of the adversary and victim, in which the value function of the victim depends on the adversary\u2019s policy though the environment dynamics. This complication makes (6) not straightforward to solve. In Proposition 5.1 below we show how to compute the policy gradient of the enhanced adversarial training model in (6), based on which we can show the RL problem in (6) can be converted into a standard competitive game.\nProposition 5.1. The gradient of (6) w.r.t adversary\u2019s policy can be computed as follows:\n\u2207\ud835\udf03 ( \ud835\udc49\ud835\udf0b\ud835\udefc\n\ud835\udf03 (\ud835\udc600) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc\ud835\udf03 )) ) = E\ud835\udf0f\u223c(\ud835\udf0b\ud835\udf08 ,\ud835\udf0b\ud835\udefc ) [ \u0394\ud835\udc45 (\ud835\udf0f)\n\u2211\ufe01 \ud835\udc61 \u2207\ud835\udf03 log\ud835\udf0b\ud835\udefc\ud835\udf03 (\ud835\udc4e \ud835\udefc \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] .\nwhere \u0394\ud835\udc45 (\ud835\udf0f) = \u2211\ud835\udc61 \ud835\udefe\ud835\udc61 [\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) \u2212 \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 )] Similarly to Corollary 4.3, the RL problem in (6) can be converted into a standard competitive gamewith differentiated rewards \ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) \u2212 \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) and fixed victim\u2019s policy \ud835\udf0b\ud835\udf08 . Thus, the environmental dynamics are fixed and a standard RL algorithm can apply.\nCorollary 5.2. (6) is equivalent to\nmax \ud835\udf0b\ud835\udefc\n{ E\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08 )]} (7)\nwhere \u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) = \ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) \u2212 \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ).\nIntegrating the victim imitator. To integrate the imitation learning model to the adversarial policy optimization, we use the imitating policy \ud835\udf0b\ud835\udf08 to predict victim\u2019s intention (i.e., next actions) and include this information into the state space of the adversary. Intuitively, we support the adversary by providing it more information about the victim\u2019s next moves. Based on Corollary 5.2, we train the adversary\u2019s policy by solving the optimization problem (7) where the adversary\u2019s policy is now of the form \ud835\udf0b\ud835\udefc ( \ud835\udc4e\ud835\udefc\ud835\udc61 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) . That is, the adversary\u2019s policy now is conditional on current state \ud835\udc60\ud835\udc61 as well as a predicted next victim action \ud835\udc4e\ud835\udf08\ud835\udc61 provided by the imitating policy model \ud835\udf0b\ud835\udf08 , and the imitating policy model \ud835\udf0b\ud835\udf08 takes the state \ud835\udc60\ud835\udc61 to predict the next victim\u2019s actions. Finally, (7) can be solved using standard policy gradient algorithms such as PPO.\nMain learning algorithm. Putting all the results developed above together, we present our adversarial policy learning in Algorithm 1. Figure 8 illustrates the three components of our algorithm, including the adversary, the victim and the imitator, and connections between these three components. In short, both the adversary policy and imitator policy are simultaneously updated during interactions between the adversary agent and the victim agent. Observed trajectories are transferred to the imitator to update the imitating policy, following steps in Section 4.2. Simultaneously, our algorithm provides the imitator with the victim\u2019s current state to ask\nAlgorithm 1 Adversarial Policy Imitation Learning (brief version)\nInput:Adversary\u2019s policy network \ud835\udf0b\ud835\udefc \ud835\udf03 ; imitator\u2019s policy network \ud835\udf0b\ud835\udf08 \ud835\udf13 ; imitator\u2019s discriminator \ud835\udc37\ud835\udc64 ; initial parameters \ud835\udf030,\ud835\udf130,\ud835\udc640. for \ud835\udc56 = 0, 1, 2, ... do # Updating imitator\u2019s policy Sample trajectories \ud835\udf0f\ud835\udc56 \u223c \ud835\udf0b\ud835\udefc\ud835\udf03\ud835\udc56 , \ud835\udf0b\n\ud835\udf08 . Update discriminator \ud835\udc37\ud835\udc64 network from\ud835\udc64\ud835\udc56 to\ud835\udc64\ud835\udc56+1 using (4). Update imitator\u2019s policy network from \ud835\udf13\ud835\udc56 to \ud835\udf13\ud835\udc56+1 based on\nTRPO or PPO using (5). # Updating adversary\u2019s policy Generate imitator\u2019s predicted actions \ud835\udc4e\ud835\udf08 \u223c \ud835\udf0b\ud835\udefc\n\ud835\udf13\ud835\udc56+1 Update the adversary policy from \ud835\udf03\ud835\udc56 to \ud835\udf03\ud835\udc56+1 based on TRPO\nor PPO, using the gradients in (5.1). end for\nfor victim\u2019s intention. This information will be passed to the adversary\u2019s policy network to update the policy adversarial learning. It is expected that when the adversary\u2019s policy gets stable and demonstrations from the victim agent are sufficient, the imitation policy also gets close to a desired one (as shown in Section 4.3) and the imitator is able to accurately predict victim agent\u2019s next actions. Algorithm 1 shows the main steps of our adversarial policy training algorithm and we give a more detailed version in the appendix."
        },
        {
            "heading": "5.2 Worst-case Performance When Training the Adversary with Unfixed Victim\u2019s Policy",
            "text": "So far we train the adversary policy by assuming that the victim agent always follows a fixed policy. We explore, in this section, the question that, if the victim\u2019s policy is not fixed, how the victim\u2019s unstable policy would affect the adversarial policy learning. To start our analysis, let \ud835\udf0b\ud835\udf080 be a \u201ctrue\u201d victim policy that the adversary agent should be trained with and suppose that, due to external causes, the adversary agent is only trained with victim policies that vary within the following set:\n\u03a9(\ud835\udf16) = {\ud835\udf0b\ud835\udf08 max \ud835\udc60\u2208S \ud835\udc37KL (\ud835\udf0b\ud835\udf08 (\ud835\udc60) | |\ud835\udf0b\ud835\udf080 (\ud835\udc60)) \u2264 \ud835\udf16}.\nWe define the worst-case expected return of the adversary agent when being trained with such varying victim policies \ud835\udc4c (\ud835\udf16) = min\ud835\udf0b\ud835\udf08 \u2208\u03a9 (\ud835\udf16) max\ud835\udf0b\ud835\udefc { E\ud835\udf0f\u223c(\ud835\udf0b\ud835\udefc ) [\u2211\u221e \ud835\udc61=0 \ud835\udefe \ud835\udc61\u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) q(\ud835\udf0b\ud835\udf08 )]} . The following theorem gives a bound for the gap between the worst-case and the desired expected return obtained from training with the \"true\" victim policy \ud835\udc4c \u2217 = max\ud835\udf0b\ud835\udefc { E\ud835\udf0f [ \u2211 \ud835\udc61 \ud835\udefe \ud835\udc61\u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf080 )]}.\nTheorem 5.3. For any \ud835\udf16 > 0, we have the following bound\n\ud835\udc4c (\ud835\udf16) \u2212 \ud835\udc4c \u2217 \u2264 \ud835\udefe\u221a2 ln 2max\ud835\udc60 {|\u0394\ud835\udc5f (\ud835\udc60) |} (1 \u2212 \ud835\udefe)2 \u221a \ud835\udf16.\nThe above bound implies that the worst-case performance of the adversarial training would not too bad (i.e., within a neighbourhood O( \u221a \ud835\udf16)) if the victim policy that the adversary is trained with is not too far from the \u201ctrue\u201d victim policy. On the other hand, if the adversary is trained with an arbitrary victim policy, the training outcomes would be very bad. Let us use the Rock-paper-scissors game to illustrate this. If the victim always plays \u201crock\u201d during the adversary\u2019s training, it will not take long for the adversary agent to see that playing \u201cpaper\u201d always gives a 100% winning rate. But if the victim change their policy to playing \u201cscissors\u201d, then that trained adversary\u2019s policy will always yield a 0% winning rate."
        },
        {
            "heading": "6 EVALUATION",
            "text": "We evaluate our proposed Enhanced Adversarial Policy Imitation Learning (E-APIL) algorithm, i.e., the adversarial policy learning (7) with an enhanced imitator (2), and the non-enhanced imitation version of our algorithm (named, APIL), i.e., the adversarial policy learning (7) with a non-enhanced imitator (1). As to do so, we use four competitive MuJoCo game environments introduced by Emergent Complexity (EC) [1], including Kick And Defend, You Shall Not Pass, Sumo Humans, and Sumo Ants. We compare the performance of our algorithms with: (i) the well-trained adversary/victim agents in [1] which we consider as Baseline agents; (ii) Attacking Deep Reinforcement Learning (ADRL) [7]; and (iii) Adversarial Policy Learning (APL) [8]. The two methods, ADRL and APL, are the state-of-the-art methods in adversarial policy learning. For fair comparisons, we use the same experiment settings (i.e., pre-trained parameters, hyperparameters, and evaluation metrics) as in [7, 8]. Implementation details are specified in supplementary section."
        },
        {
            "heading": "6.1 Adversarial Policy Performance: Training Adversary against Baseline Victim",
            "text": "In this experiment, we train our adversary agent using our proposed algorithms to play against the baseline victim agent [1]. We aim to examine if our generated adversarial policy can trigger the victim agent to perform poorly. Table 1 shows the winning rate (i.e., numbers in white cells) and the winning plus tie rate (numbers in gray cells) of our trained adversary agents playing against the baseline victim agent, compared to those trained by other adversarial policy algorithms (also against the baseline victim). Each reported value is calculated based on 1000 different rounds of game playing.\nOverall, our APIL and E-APIL methods achieve significantly higher winning rates for Kick-And-Defend and Sumo-Humans compared to all existing methods. In You-Shall-Not-Pass, our method E-APIL achieves a winning rate which is only 1% less than the best-performed method (APL) in this game environment, while significantly outperforming the others. On the other hand, in SumoAnts, we observe an interesting phenomenon. While we obtain the best winning-plus-tie rates in Sumo-Ants, we obtain a lower winning rate compared to the baseline adversary. This phenomenon also holds true for existing algorithms (ADRL and APL). The cause of this phenomenon comes from a unique underlying characteristic of Sumo-Ants, i.e., it is very challenging to reach the win outcome \u2014 the victim has a high chance to reach a draw outcome by just jumping to the ground without touching opponent. As a result, our adversary was essentially trained to optimize the policy towards draw outcomes in Sumo-Ants, at the sacrifice of the win rate.\nWe now seek to better understand why our methods get higher winning rates than other algorithms. Figure 4 shows t-SNE visualization [29] of the trained adversary against the baseline victim by recording victim\u2019s policy activations. The t-SNE visualizations for"
        },
        {
            "heading": "Kick And Defend You Shall Not Pass",
            "text": "all four game environments indicates that our algorithms APIL/EAPIL seek to activate different policy distribution regions of the victim (the orange and red regions) compared to existing algorithms, allowing our policy learning to converge to a better optimum.\nTable 1 also shows that ADRL and APL are more focused on getting draw in the Sumo games than learning how to win. We plot in Figure 6 the training performance of our algorithms for the four game environments, which show that the tie rates are already high during early episodes. As mentioned, the victim in these games can easily get a draw by just jumping to the ground without touching the opponent, which makes the tie rates very high.\nFinally, Table 1 shows that our E-APIL with an enhanced imitator is significant better than APIL in both Kick-and-Defend and You-Shall-Not-Pass. This result implies that incorporating the adversary\u2019s expected rewards into the imitator\u2019s value function definitely helps improve the quality of the generated adversarial policy. In Sumo-Humans and Sumo-Ants where the tie rates account for a\nlarge proportion of the outcomes, the performance of E-APIL and APIL are not substantially disparate."
        },
        {
            "heading": "6.2 Blinding the Trained Adversary",
            "text": "To further understand the role of the imitator behind the efficiency of our adversarial training algorithms, we conduct the following experiment. First, we take the trained adversary agents and let them play with the baseline victim, but now we blind the adversary\u2019s observation on the victim or, in other words, zero out the adversary observation pertaining to the victim. By blinding the adversary agents, we aim to demonstrate that the trained adversary still manages to make the victim to perform poorly just based on the imitator\u2019s policy output, despite of the blinded disadvantage.\nTables 2 reports our experiments with blinded trained adversary against the baseline victim. In general, our APIL and E-APIL adversary agents outperform those trained by the other methods when playing against the baseline victim. Intuitively, even when being blinded, by taking feedback from the trained imitator, our trained adversary agents would still partially predict victim\u2019s intention to make better actions, compared to those trained by other methods.\nIn summary, our methods works well in interactive environments, even in the blinding setting, thanks to the capability of predicting opponent\u2019s intention through the trained imitator."
        },
        {
            "heading": "6.3 Improving Victim Resiliency: Retraining Victim against New Adversary",
            "text": "Previous studies demonstrate that one could retrain the victim and thus improve its adversary resistance [7, 8]. In this experiment, we also retrain the victim agent against the newly trained adversary agent to examine the resistance of the retrained victim agent against adversarial policies. We further explore the resilience transferability of the retrained victim agents. Specifically, similar to previous work, we retrain the victim agent against a mixed adversary agent of the new adversary (whose policy is trained based on one of the evaluated adversarial training algorithms (i.e., baseline, ADRL, APL and ours) and the baseline adversary. We then have the retrained victim agent play against the baseline adversary for 1000 rounds and report its winning as well as winning plus tie rates.\nTable 3 shows the winning and winning plus tie rates of the retrained victim agents playing against the baseline adversary. For example, the E-APIL column shows the game results between\nthe victim agent (retrained based on interactions with an E-APIL adversary) and the Baseline adversary agent. Except for the SumoAnts game, our methods outperform other algorithms in terms of retraining the victim agent to be stronger. For all the games, the baseline victim generally yields good results, which is not a surprising observation as the OpenAI\u2019s baseline agents [1] are well trained against their opponents with around 1B steps and 4 GPUs. In our experiments, with only 35M + 10M steps and 1 GPU, we are able to make the victim agent significantly stronger. Moreover, despite the fact that the APIL/E-APIL based victim agents are retrained against our APIL/E-APIL adversary agents, these retrained victim agents still manage to perform well against the baseline adversary as show in the last two columns of Table 3. This result clearly shows that the strong resilience of APIL/E-APIL based victim agents can be transferred to other game settings with different types of adversary agents (e.g., the baseline adversary in this experiment).\nWe further test the performance of each retrained victim agent against our E-APIL adversary and report the winning and winning plus tie rates in Table 4. For the two non-sumo games, the winning\nrates of ADRL/APL retrained victims are less than 60%, which are significantly smaller than our rates. Obviously, our E-APIL retrained victim achieves better results because it\u2019s trained against our EAPIL adversary, but our APIL also gets much better winning rates than ADRL/APL methods. It generally indicates the robustness and efficiency of our algorithms, compared to other approaches, in terms of retraining the victim agent to have better versions of it."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper introduces a new effective adversarial policy learning algorithm based on a novel integration of a new victim-imitation learning into the adversarial policy training process. Our victimimitation component (which is an enhanced version of the stateof-the-art imitation method GAIL) discovers underlying characteristics of the victim agent, enabling the prediction of the victim\u2019s next moves which can be leveraged to strengthen the adversarial policy generation. We present important theoretical results on the inter-dependency between the victim-imitation learning and the adversarial policy learning, showing the convergence of our learning\nalgorithm. We demonstrate the superiority of our proposed algorithm compared to existing adversarial policy learning algorithms through extensive experiments on various game environments."
        },
        {
            "heading": "A MISSING PROOFS A.1 Proof of Lemma 4.1",
            "text": "Lemma A.1. The gradient of \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )) w.r.t\ud835\udf13 can be computed as follows:\n\u2207\ud835\udf13 ( \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )) ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08\n\ud835\udf13\n[ \ud835\udc45\ud835\udefc (\ud835\udf0f) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] . (8) Proof. We write the adversary\u2019s expected reward as\nE\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )] = \u2211\ufe01 \ud835\udf0f \ud835\udc45\ud835\udefc (\ud835\udf0f) \u220f \ud835\udc61 \ud835\udf0b\ud835\udefc (\ud835\udc4e\ud835\udefc\ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc )\n= \u2211\ufe01\n\ud835\udf0f={(\ud835\udc60\ud835\udc61 ,\ud835\udc4e\ud835\udefc\ud835\udc61 ,\ud835\udc4e\ud835\udf08\ud835\udc61 ) } \ud835\udc45\ud835\udefc (\ud835\udf0f) \u220f \ud835\udc61 \ud835\udf0b\ud835\udefc (\ud835\udc4e\ud835\udefc\ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) (9)\nTaking the derivative of the above expected value w.r.t.\ud835\udf13 we get\n\u2207\ud835\udf13 ( E\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )]) = \u2211\ufe01 \ud835\udf0f={(\ud835\udc60\ud835\udc61 ,\ud835\udc4e\ud835\udefc\ud835\udc61 ,\ud835\udc4e\ud835\udf08\ud835\udc61 ) } \ud835\udc45\ud835\udefc (\ud835\udf0f)\ud835\udc43 (\ud835\udf0f)\u2207\ud835\udf13 log (\u220f \ud835\udc61 \ud835\udf0b\ud835\udefc (\ud835\udc4e\ud835\udefc\ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) ) =\n\u2211\ufe01 \ud835\udf0f={(\ud835\udc60\ud835\udc61 ,\ud835\udc4e\ud835\udefc\ud835\udc61 ,\ud835\udc4e\ud835\udf08\ud835\udc61 ) } \ud835\udc45\ud835\udefc (\ud835\udf0f)\ud835\udc43 (\ud835\udf0f) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 )\n= E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 \ud835\udf13\n[ \ud835\udc45\ud835\udefc (\ud835\udf0f) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] , which is the desired equality. \u25a1"
        },
        {
            "heading": "A.2 Proof of Proposition 4.2",
            "text": "Proposition A.2. The gradient of the objective (2) w.r.t.\ud835\udf13 can be computed as follows:\nE\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 \ud835\udf13 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] \u2212 \ud835\udf06\u2207\ud835\udf13\ud835\udc3b (\ud835\udf0b\ud835\udf08\ud835\udf13 )\nwhere \ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) = log(\ud835\udc37 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )) \u2212 \ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) .\nProof. The first part of (2) is a standard long-term reward whose gradients can be computed as\n\u2207\ud835\udf13 ( \ud835\udf19 (\ud835\udf0b\ud835\udf08 \ud835\udf13 , \ud835\udc37) ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08\n\ud835\udf13 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 log(\ud835\udc37 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 )) \u2211\ufe01 \ud835\udc61 log\ud835\udf0b\ud835\udf08 \ud835\udf13 (\ud835\udc4e\ud835\udf08\ud835\udc61 |\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] Combine this with the derivation in Lemma 4.1 we get\n\u2207\ud835\udf13 ( \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf13 , \ud835\udc37) ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf13 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 ( log(\ud835\udc37 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 )) \u2212 \ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) ) \u2211\ufe01 \ud835\udc61 log\ud835\udf0b\ud835\udf08 \ud835\udf13 (\ud835\udc4e\ud835\udf08\ud835\udc61 |\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] , as desired. \u25a1"
        },
        {
            "heading": "A.3 Proof of Corollary 4.3",
            "text": "Corollary A.3. The enhanced imitation learning model (2) is equivalent to GAIL with the modified discriminator objective:\nmax \ud835\udf0b\ud835\udf08 min \ud835\udc37\u2208[0,1]\n{ \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 \ud835\udf02 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] + E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 log(1 \u2212 \ud835\udc37 (\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212 \ud835\udf06\ud835\udc3b (\ud835\udf0b\ud835\udf08 )} (10)\nProof. The corollary can be deduced from Proposition 4.2, or one can write \ud835\udc49\ud835\udf0b\ud835\udefc (\ud835\udc600 |q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08\ud835\udf13 )] = \u2211\ufe01\n\ud835\udf0f \ud835\udc45\ud835\udefc (\ud835\udf0f) \u220f \ud835\udc61 \ud835\udf0b\ud835\udefc (\ud835\udc4e\ud835\udefc\ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc )\n= \u2211\ufe01\n\ud835\udf0f={(\ud835\udc60\ud835\udc61 ,\ud835\udc4e\ud835\udefc\ud835\udc61 ,\ud835\udc4e\ud835\udf08\ud835\udc61 ) } \ud835\udc45\ud835\udefc (\ud835\udf0f) \u220f \ud835\udc61 \ud835\udf0b\ud835\udefc (\ud835\udc4e\ud835\udefc\ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udf0b\ud835\udf08\ud835\udf13 (\ud835\udc4e \ud835\udf08 \ud835\udc61 |\ud835\udc60\ud835\udc61 )\ud835\udc43 (\ud835\udc60\ud835\udc61+1 |\ud835\udc60\ud835\udc61 , \ud835\udc4e\ud835\udefc\ud835\udc61 , \ud835\udc4e\ud835\udf08\ud835\udc61 )\n= E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] ,\nwhich directly leads the desired equivalence. \u25a1"
        },
        {
            "heading": "A.4 Proof of Lemma 4.4",
            "text": "Lemma A.4. Given two adversary policies \ud835\udf0b\ud835\udefc and \ud835\udf0b\ud835\udefc , letH = max\ud835\udc60 {|\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) |} \u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) \u2264 \ud835\udefeH\u221a2 ln 2\n1 \u2212 \ud835\udefe max\ud835\udc60\u2208\ud835\udc46\n{\u221a\ufe01 \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } where \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) is the KL divergence between \ud835\udf0b\ud835\udefc and \ud835\udf0b\ud835\udefc .\nProof. Recall that we define \u0393(\ud835\udf0b\ud835\udefc ) as the expected reward of the victim with policy \ud835\udf0b\ud835\udf08 when the adversary follows policy \ud835\udf0b\ud835\udefc \u0393(\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] .\nGiven two adversary policies \ud835\udf0b\ud835\udefc and \ud835\udf0b\ud835\udefc , we define the following victim\u2019s competitive advantage function \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60) for two states \ud835\udc60, \ud835\udc60 \u2208 S\n\ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60) = \ud835\udc5f \ud835\udf08 (\ud835\udc60) + \ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )), where \ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211 \ud835\udc61 \ud835\udefe \ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q(\ud835\udf0b\ud835\udefc ), \ud835\udc600 = \ud835\udc60] . We then compute the advantage of the adversary policy \ud835\udf0b\ud835\udefc over \ud835\udf0b\ud835\udefc but in terms of victim\u2019s expected rewards as follows\n\u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc ))\n(\ud835\udc4e) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) ] + E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 (\ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udc61+1 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udc61 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc ))) ]\n= E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 ( \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) + \ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udc61+1 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udc61 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) )] (\ud835\udc4f) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 ( \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60\ud835\udc61 , \ud835\udc60\ud835\udc61+1) )] , (11)\nwhere (\ud835\udc4e) is due to the fact that \u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 (\ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udc61+1 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udc61 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc ))) = \ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )),\nand (\ud835\udc4f) is due to the definition of the competitive advantage function \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60\ud835\udc61 , \ud835\udc60\ud835\udc61+1). Here we note that E\ud835\udc60\ud835\udf08\u223c\ud835\udf0b\ud835\udf08 ,q(\ud835\udf0b\ud835\udefc ) |\ud835\udc60\ud835\udf08 [ \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60\ud835\udf08 , \ud835\udc60\ud835\udf08 ) ] = E\ud835\udc60\ud835\udf08\u223c\ud835\udf0b\ud835\udf08 ,q(\ud835\udf0b\ud835\udefc ) |\ud835\udc60\ud835\udf08 [ \ud835\udc5f (\ud835\udc60\ud835\udf08 ) + \ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udf08 | q(\ud835\udf0b\ud835\udefc )) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60\ud835\udf08 | q(\ud835\udf0b\ud835\udefc )) ] = 0.\nWe further have the following bound for the competitive advantage function. E\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) |\ud835\udc60 [ \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60) ] = E\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [ \ud835\udc5f \ud835\udf08 (\ud835\udc60) + \ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) ] \u2212 E\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [ \ud835\udc5f \ud835\udf08 (\ud835\udc60) + \ud835\udefe\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) ] = \ud835\udefe\n(\u2211\ufe01 \ud835\udc60 \ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 | q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) ( \ud835\udc43 (\ud835\udc60 |\ud835\udc60, \ud835\udf0b\ud835\udf08 , \ud835\udf0b\ud835\udefc ) \u2212 \ud835\udc43 (\ud835\udc60 |\ud835\udc60, \ud835\udf0b\ud835\udf08 , \ud835\udf0b\ud835\udefc ) )) \u2264 \ud835\udefeHE\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 |\ud835\udc60 [ | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) \u2212 \ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |1\n] (\ud835\udc50) \u2264 \ud835\udefeH max\n\ud835\udc60\u2208S\n{\u221a\ufe01 2 ln 2KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } , (12)\nwhereH = max\ud835\udc60 {|\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc60 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) |} and (\ud835\udc50) is due to the inequality | |\ud835\udc5d \u2212 \ud835\udc5e | |1 \u2264 \u221a\ufe01 2 ln 2\ud835\udc37KL (\ud835\udc5d | |\ud835\udc5e) for two distributions \ud835\udc5d, \ud835\udc5e [27]. Moreover, if we define \ud835\udf16 = max\n\ud835\udc60 { E\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 ,q(\ud835\udf0b\ud835\udefc ) |\ud835\udc60 [\ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60)] } , then \ud835\udf16 \u2192 0 if \ud835\udf0b\ud835\udefc \u2192 \ud835\udf0b\ud835\udefc . Moreover, we can see from (11) that \u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) = E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 ,q\ud835\udf08 (\ud835\udf0b\ud835\udefc ) [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61 ( \ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60\ud835\udc61 , \ud835\udc60\ud835\udc61+1) )] \u2264 E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 ,q(\ud835\udf0b\ud835\udefc ) [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\ud835\udf16 ] = \ud835\udf16 1 \u2212 \ud835\udefe . (13)\nPutting (12) and (13) together, we can bound the gap |\u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) | as \u0393(\ud835\udf0b\ud835\udefc ) \u2212 \u0393(\ud835\udf0b\ud835\udefc ) \u2264 max\ud835\udc60 { E\ud835\udc60\u223c\ud835\udf0b\ud835\udf08 ,q(\ud835\udf0b\ud835\udefc ) |\ud835\udc60 [\ud835\udc34\ud835\udf0b\ud835\udefc (\ud835\udc60, \ud835\udc60)] }\n1 \u2212 \ud835\udefe\n\u2264 \ud835\udefeH 1 \u2212 \ud835\udefe max\ud835\udc60\u2208S\n{\u221a\ufe01 2 ln 2KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } ,\nas desired. \u25a1"
        },
        {
            "heading": "A.5 Proof of Theorem 4.5",
            "text": "Theorem A.5. Suppose that discriminator\u2019s network model \ud835\udc37 of (2) varies within [\ud835\udc37\ud835\udc3f, \ud835\udc37\ud835\udc48 ] \u2282 [0, 1]. Let \ud835\udf0b\ud835\udefc\u2217 be the target adversary policy that we want to train the imitation policy with, and let (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217) be the imitation policy and the imitator\u2019s discriminator trained with another adversary \ud835\udf0b\ud835\udefc , we have the following performance guarantee for \ud835\udf0b\ud835\udf08\u2217. \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212max\n\ud835\udf0b\ud835\udf08 min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)} \u2264 2\ud835\udc3e max \ud835\udc60\u2208\ud835\udc46\n{\u221a\ufe01 \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc\u2217 (\u00b7|\ud835\udc60)) } , (14)\nwhere\n\ud835\udc3e = \ud835\udefe \u221a 2 ln 2\n( max\ud835\udc60 {\ud835\udc5f \ud835\udf08 (\ud835\udc60)} \u2212 log(\ud835\udc37\ud835\udc3f \u2212 \ud835\udc37\ud835\udc3f\ud835\udc37\ud835\udc48 ) ) (1 \u2212 \ud835\udefe)2 .\nProof. From the proof of Lemma 4.4 we can deduce the following, for any policies \ud835\udf0b\ud835\udefc , \ud835\udf0b\ud835\udefc , and any reward function \ud835\udc5f \ud835\udf08 (\ud835\udc4e), E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212 E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01\n\ud835\udc61\n\ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )]\n\u2264 \ud835\udefe 1 \u2212 \ud835\udefe max\ud835\udc60 {|\ud835\udc49\ud835\udf0b \ud835\udf08 (\ud835\udc60 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc )) |}max \ud835\udc60\u2208S\n{\u221a\ufe01 2 ln 2KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } \u2264 \ud835\udefe max\ud835\udc60 |\ud835\udc5f\n\ud835\udf08 (\ud835\udc60) | (1 \u2212 \ud835\udefe)2\n{\u221a\ufe01 2 ln 2KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } . (15)\nWe now using this to bound the gap \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc ) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc ) as follows. We first write \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc ) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc ) \u2264 E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 (log(\ud835\udc37) \u2212 \ud835\udc5f (\ud835\udc60\ud835\udc61 )) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212 E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 (log(\ud835\udc37) \u2212 \ud835\udc5f (\ud835\udc60\ud835\udc61 )) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] +\n+ E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 log(1 \u2212 \ud835\udc37) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )] \u2212 E\ud835\udf0f\u223c\ud835\udf0b\ud835\udf08 [\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 log(1 \u2212 \ud835\udc37) q\ud835\udf08 (\ud835\udf0b\ud835\udefc )]\n\u2264 \ud835\udefe \u221a 2 ln 2\n(1 \u2212 \ud835\udefe)2 max \ud835\udc60\n{\u221a\ufe01 KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } ( max \ud835\udc60,\ud835\udc37 {|\ud835\udc5f\ud835\udefc (\ud835\udc60) \u2212 log(\ud835\udc37)} +max \ud835\udc37 | log(1 \u2212 \ud835\udc37) | ) (\ud835\udc51) \u2264 \ud835\udefe \u221a 2 ln 2\n(1 \u2212 \ud835\udefe)2 max \ud835\udc60\n{\u221a\ufe01 KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } ( max \ud835\udc60 {\ud835\udc5f (\ud835\udc60\ud835\udefc )} \u2212 log(\ud835\udc37\ud835\udc3f) \u2212 log(1 \u2212 \ud835\udc37\ud835\udc48 ) ) , (16)\nwhere (\ud835\udc51) is because \ud835\udc37 \u2208 [\ud835\udc37\ud835\udc3f, \ud835\udc37\ud835\udc48 ]. For ease of notation, let\n\ud835\udc3e = \ud835\udefe \u221a 2 ln 2\n( max\ud835\udc60\ud835\udefc {\ud835\udc5f (\ud835\udc60\ud835\udefc )} \u2212 log(\ud835\udc37\ud835\udc3f) \u2212 log(1 \u2212 \ud835\udc37\ud835\udc48 ) ) (1 \u2212 \ud835\udefe)2 ; \ud835\udf16 = \u221a\ufe01 \ud835\udc37KL (\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) .\nWe first try to bound min\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2212min\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)} as follows.\n\u2022 If min\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2265 min\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)}, then we let \ud835\udc37\u2217 = argmin\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)} to have min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2212min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)} = min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2212min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)}\n\u2264 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37\u2217 |\ud835\udf0b\ud835\udefc ) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2264 \ud835\udc3e\ud835\udf16 (17)\n\u2022 If min\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2264 min\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)}, then we let \ud835\udc37\u2217 = argmin\ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} to have a similar evaluation min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2212min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)} = min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)} \u2212min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )}\n\u2264 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37\u2217 |\ud835\udf0b\ud835\udefc ) \u2264 \ud835\udc3e\ud835\udf16. (18)\nSo we always have min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2212min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)}\n\u2264 \ud835\udc3e\ud835\udf16. (19) Now, suppose \ud835\udf0b\ud835\udefc\u2217 is a target adversary policy that we want the imitation learning model to train with, and let (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217) be an imitation learning policy and the discriminator network that are trained with adversary policy \ud835\udf0b\ud835\udefc . To bound the gap between |\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212 max\ud835\udf0b\ud835\udf08 min\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) |, we consider the following two cases \u2022 If \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2265 max\ud835\udf0b\ud835\udf08 min\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217), then \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212max\n\ud835\udf0b\ud835\udf08 min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) = \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212max \ud835\udf0b\ud835\udf08 min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)\n(\ud835\udc52) \u2264 \ud835\udc3e\ud835\udf16 + \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc ) \u2212min\n\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)\n(\ud835\udc53 ) = \ud835\udc3e\ud835\udf16 +min\n\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc ) \u2212min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)\n\u2264\ud835\udc3e\ud835\udf16 + min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc )} \u2212min \ud835\udc37 {\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217)}\n(\ud835\udc54) \u2264 2\ud835\udc3e\ud835\udf16, (20)\nwhere (\ud835\udc52) is because |\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc ) | \u2264 \ud835\udc3e\ud835\udf16 (according to (16)), (\ud835\udc53 ) is due to \ud835\udc37\ud835\udf08\u2217 = argmax\ud835\udc37\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc ) and (\ud835\udc54) is because of (19). \u2022 If \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2265 max\ud835\udf0b\ud835\udf08 min\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217), we let \ud835\udf0b\ud835\udf08\u2217\u2217 = argmax\ud835\udf0b\ud835\udf08 min\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) and write \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212max\n\ud835\udf0b\ud835\udf08 min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) = max \ud835\udf0b\ud835\udf08 min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217)\n(\u210e) \u2264 min\n\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) \u2212 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc ) + \ud835\udc3e\ud835\udf16\n(\ud835\udc56) \u2264 \ud835\udc3e\ud835\udf16 +min\n\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) \u2212min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc )\n\u2264\ud835\udc3e\ud835\udf16 + min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) \u2212min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc )\n( \ud835\udc57) \u2264 2\ud835\udc3e\ud835\udf16, (21)\nwhere (\u210e) is due to\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc )\u2212\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2264 \ud835\udc3e\ud835\udf16 (see (16)), (\ud835\udc56) is due to\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc ) = max\ud835\udf0b\ud835\udf08 min\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc ) \u2265 min\ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217\u2217, \ud835\udc37 |\ud835\udf0b\ud835\udefc ), and ( \ud835\udc57) is because of (19).\nCombine the two cases above we obtain he desired bound.\n|\ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08\u2217, \ud835\udc37\ud835\udf08\u2217 |\ud835\udf0b\ud835\udefc\u2217) \u2212max \ud835\udf0b\ud835\udf08 min \ud835\udc37 \ud835\udf19\ud835\udc38 (\ud835\udf0b\ud835\udf08 , \ud835\udc37 |\ud835\udf0b\ud835\udefc\u2217) | \u2264 2\ud835\udc3e max \ud835\udc60\u2208S\n{\u221a\ufe01 KL(\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udefc (\u00b7|\ud835\udc60)) } . \u25a1"
        },
        {
            "heading": "A.6 Proof of Proposition 5.1",
            "text": "Proposition A.6. The gradient of (6) w.r.t adversary\u2019s policy can be computed as follows:\n\u2207\ud835\udf03 ( \ud835\udc49\ud835\udf0b\ud835\udefc\n\ud835\udf03 (\ud835\udc600) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc\ud835\udf03 ))\n) = E\ud835\udf0f\u223c(\ud835\udf0b\ud835\udf08 ,\ud835\udf0b\ud835\udefc ) [ \u0394\ud835\udc45 (\ud835\udf0f) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf03 log\ud835\udf0b\ud835\udefc\ud835\udf03 (\ud835\udc4e \ud835\udefc \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] where \u0394\ud835\udc45 (\ud835\udf0f) = \u2211\ud835\udc61 \ud835\udefe\ud835\udc61 (\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) \u2212 \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 )).\nProof. Similarly to the proof of Lemma 4.1, we compute the gradient of \ud835\udc49\ud835\udf0b\ud835\udf08 (\u00b7) as \u2207\ud835\udf03 ( \ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc\ud835\udf03 )) ) = \u2211\ufe01 \ud835\udf0f={(\ud835\udc60\ud835\udc61 ,\ud835\udc4e\ud835\udefc\ud835\udc61 ,\ud835\udc4e\ud835\udf08\ud835\udc61 ) }\u223c\ud835\udf0b\ud835\udefc ,\ud835\udf0b\ud835\udf08 (\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) ) \ud835\udc43 (\ud835\udf0f) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf03 log\ud835\udf0b\ud835\udefc\ud835\udf03 (\ud835\udc4e \ud835\udefc \ud835\udc61 |\ud835\udc60\ud835\udc61 )\n= E\ud835\udf0f={(\ud835\udc60\ud835\udc61 ,\ud835\udc4e\ud835\udefc\ud835\udc61 ,\ud835\udc4e\ud835\udf08\ud835\udc61 ) }\u223c\ud835\udf0b\ud835\udefc ,\ud835\udf0b\ud835\udf08 [(\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ) ) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf13 log\ud835\udf0b\ud835\udefc\ud835\udf03 (\ud835\udc4e \ud835\udefc \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] ,\nThus, we can write the gradient of (6) as \u2207\ud835\udf03 ( \ud835\udc49\ud835\udf0b\ud835\udefc\n\ud835\udf03 (\ud835\udc600) \u2212\ud835\udc49\ud835\udf0b\ud835\udf08 (\ud835\udc600 |q\ud835\udf08 (\ud835\udf0b\ud835\udefc\ud835\udf03 ))\n) = E\ud835\udf0f\u223c(\ud835\udf0b\ud835\udf08 ,\ud835\udf0b\ud835\udefc ) [(\u2211\ufe01 \ud835\udc61 \ud835\udefe\ud835\udc61 (\ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) \u2212 \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 )) ) \u2211\ufe01 \ud835\udc61 \u2207\ud835\udf03 log\ud835\udf0b\ud835\udefc\ud835\udf03 (\ud835\udc4e \ud835\udefc \ud835\udc61 |\ud835\udc60\ud835\udc61 ) ] ,\nwhich concludes the proof. \u25a1"
        },
        {
            "heading": "A.7 Proof of Corollary 5.2",
            "text": "Corollary A.7. (6) is equivalent to\nmax \ud835\udf0b\ud835\udefc\n{ E\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08 )]} ,\nwhere \u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) = \ud835\udc5f\ud835\udefc (\ud835\udc60\ud835\udc61 ) \u2212 \ud835\udc5f \ud835\udf08 (\ud835\udc60\ud835\udc61 ).\nProof. The equivalence can be straightforwardly deduced from Proposition 5.1. \u25a1"
        },
        {
            "heading": "A.8 Proof of Theorem 5.3",
            "text": "Theorem A.8. For any \ud835\udf16 > 0, we have the following bound \ud835\udc4c (\ud835\udf16) \u2212 \ud835\udc4c \u2217 \u2264 \ud835\udefe\u221a2 ln 2max\ud835\udc60 {|\u0394\ud835\udc5f (\ud835\udc60) |}\n(1 \u2212 \ud835\udefe)2 \u221a \ud835\udf16.\nProof. In analogy to the proofs of Lemma 4.4 and Theorem 4.5, if we define \u039b(\ud835\udf0b\ud835\udf08 ) as the adversary\u2019s expected return if the victim\u2019s policy is \ud835\udf0b\ud835\udf08\n\u039b(\ud835\udf0b\ud835\udf08 ) = max \ud835\udf0b\ud835\udefc\n{ E\ud835\udf0f\u223c\ud835\udf0b\ud835\udefc [ \u221e\u2211\ufe01 \ud835\udc61=0 \ud835\udefe\ud835\udc61\u0394\ud835\udc5f (\ud835\udc60\ud835\udc61 ) q\ud835\udefc (\ud835\udf0b\ud835\udf08 )]}\nthen, similarly to the derivation in (15), we can get the following bound for any \ud835\udf0b\ud835\udf08 \u2208 \u03a9(\ud835\udf16) \u039b(\ud835\udf0b\ud835\udf08 ) \u2212 \u039b(\ud835\udf0b\ud835\udf080 ) \u2264 \ud835\udefe max\ud835\udc60 |\u0394\ud835\udc5f (\ud835\udc60) |(1 \u2212 \ud835\udefe)2 {\u221a\ufe012 ln 2KL(\ud835\udf0b\ud835\udf08 (\u00b7|\ud835\udc60) | |\ud835\udf0b\ud835\udf08 (\u00b7|\ud835\udc60))} \u2264 \u221a 2 ln 2\ud835\udf16\ud835\udefe max\ud835\udc60 |\u0394\ud835\udc5f (\ud835\udc60) |\n(1 \u2212 \ud835\udefe)2 (22)\nwhich also implies that min \ud835\udf0b\ud835\udf08 \u2208\u03a9 (\ud835\udf16) {\u039b(\ud835\udf0b\ud835\udf08 )} \u2212 \ud835\udc4c \u2217 \u2264 \u221a2 ln 2\ud835\udf16\ud835\udefe max\ud835\udc60 |\u0394\ud835\udc5f (\ud835\udc60) |(1 \u2212 \ud835\udefe)2 ,\nwhich is also the desired result. \u25a1"
        },
        {
            "heading": "B ADVERSARIAL POLICY IMITATION LEARNING ALGORITHM",
            "text": "Algorithm 2 shows the detailed steps of our Adversarial Policy Imitation Learning algorithms and Figure 8 provides a more detailed overview of our framework.\nAlgorithm 2 Adversarial Policy Imitation Learning\nupdate policy\nEnvironment\nAdversary VictimGenerator\nstate\naction\nadv reward\nadv action imi action vic action\nadv replay buffer\nimi replay buffer\nadv state\nimi state\nvic state\nDiscriminator\nimi reward\ntrajectory\ntrajectory\nupdate disc\nstudent buffer\nexpert buffer\nupdate policy\nImitator"
        },
        {
            "heading": "C EXPERIMENTAL SETTINGS AND ADDITIONAL EXPERIMENTS",
            "text": "Similar to APL\u2019s settings [8], we train our adversary agents with 35M steps and retrain victim with 10M steps. In addition, we use the same model architectures, hidden dimensions and hyperparameters with 1 GPU Nvidia GeForce RTX 2080 Ti, 24-core CPU Intel Xeon 4116 @ 2.1GHz and 64G RAM. All evaluation are tested individually with different seeds over 1000 episodes.\nWhen retraining the victim, we let the victim agent play with a mixing adversary which is a combination of the newly trained adversary and the baseline one. That is, we randomly taking actions from both adversaries. In Figures 9 and 11, we report the performance of the retraining victim, but with each adversary separately. It is clear that the victim retraining performance improves over episodes for both adversaries for Kick-and-Defend, You-Shall-Not-Pass and Sumo-Humans. However, for Sumo-Ants, the performance from retraining with our trained adversary improves, but that from retraining with the baseline adversary degrades. This also indicates an advantage of our trained adversary agent in terms of retraining the victim."
        }
    ],
    "title": "Imitating Opponent to Win: Adversarial Policy Imitation Learning in Two-player Competitive Games",
    "year": 2022
}