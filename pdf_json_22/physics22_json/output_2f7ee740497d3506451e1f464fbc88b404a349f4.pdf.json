{
    "abstractText": "Full waveform inversion (FWI) aims at reconstructing unknown physical coefficients in wave equations using the wave field data generated from multiple incoming sources. In this work, we propose an offline-online computational strategy for coupling classical least-squares based computational inversion with modern deep learning based approaches for FWI to achieve advantages that can not be achieved with only one of the components. In a nutshell, we develop an offline learning strategy to construct a robust approximation to the inverse operator and utilize it to design a new objective function for the online inversion with new datasets. We demonstrate through numerical simulations that our coupling strategy improves the computational efficiency of FWI with reliable offline training on moderate computational resources (in terms of both the size of the training dataset and the computational cost needed).",
    "authors": [
        {
            "affiliations": [],
            "name": "Wen Ding"
        },
        {
            "affiliations": [],
            "name": "Kui Ren"
        },
        {
            "affiliations": [],
            "name": "Lu Zhang"
        }
    ],
    "id": "SP:a2663cddc2fa54ed1d2f8d211a3c6d265f26d6cb",
    "references": [
        {
            "authors": [
                "A. Adler",
                "M. Araya-Polo",
                "T. Poggio"
            ],
            "title": "Deep learning for seismic inverse problems: Toward the acceleration of geophysical analysis workflows",
            "venue": "IEEE Signal Processing Magazine, 38 ",
            "year": 2021
        },
        {
            "authors": [
                "J. Adler",
                "O. \u00d6ktem"
            ],
            "title": "Solving ill-posed inverse problems using iterative deep neural networks",
            "venue": "Inverse Problems, 33 ",
            "year": 2017
        },
        {
            "authors": [
                "V. Ak\u00e7elik",
                "G. Biros",
                "O. Ghattas"
            ],
            "title": "Parallel multiscale Gauss-Newton-Krylov methods for inverse wave propagation",
            "venue": "Proceedings of the 2002 ACM/IEEE Conference on Supercomputing",
            "year": 2002
        },
        {
            "authors": [
                "M. Araya-Polo",
                "A. Adler",
                "S. Farris",
                "J. Jennings"
            ],
            "title": "Fast and accurate seismic tomography via deep learning",
            "venue": "Deep Learning: Algorithms and Applications, Springer",
            "year": 2020
        },
        {
            "authors": [
                "E. Bachmann",
                "J. Tromp"
            ],
            "title": "Source encoding for viscoacoustic ultrasound computed tomography",
            "venue": "J. Acoust. Soc. Am., 147 ",
            "year": 2020
        },
        {
            "authors": [
                "G. Bao",
                "W.W. Symes"
            ],
            "title": "On the sensitivity of hyperbolic equation to the coefficient",
            "venue": "Comm. in P.D.E., 21 ",
            "year": 1996
        },
        {
            "authors": [
                "G. Bao",
                "X. Ye",
                "Y. Zang",
                "H. Zhou"
            ],
            "title": "Numerical solution of inverse problems by weak adversarial networks",
            "venue": "Inverse Problems, 36 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Bernard",
                "V. Monteiller",
                "D. Komatitsch",
                "P. Lasaygues"
            ],
            "title": "Ultrasonic computed tomography based on full-waveform inversion for bone quantitative imaging",
            "venue": "Phys. Med. Bio., 62 ",
            "year": 2017
        },
        {
            "authors": [
                "L. Borcea",
                "V. Druskin",
                "A. Mamonov",
                "M. Zaslavsky"
            ],
            "title": "Untangling the nonlinearity in inverse scattering with data-driven reduced order models",
            "venue": "Inverse Problems, 34 ",
            "year": 2018
        },
        {
            "authors": [
                "R. Brossier",
                "S. Operto",
                "J. Virieux"
            ],
            "title": "Which data residual norm for robust elastic frequency-domain full waveform inversion",
            "venue": "Geophysics, 75 ",
            "year": 2010
        },
        {
            "authors": [
                "T.A. Bubba",
                "M. Galinier",
                "M. Lassas",
                "M. Prato",
                "L. Ratti",
                "S. Siltanen"
            ],
            "title": "Deep neural networks for inverse problems with pseudodifferential operators: An application to limited-angle tomography",
            "venue": "SIAM J. Imaging Sci., 14 ",
            "year": 2021
        },
        {
            "authors": [
                "C. Bunks",
                "F.M. Saleck",
                "S. Zaleski",
                "G. Chavent"
            ],
            "title": "Multiscale seismic waveform inversion",
            "venue": "Geophysics, 50 ",
            "year": 1995
        },
        {
            "authors": [
                "C. Burstedde",
                "O. Ghattas"
            ],
            "title": "Algorithmic strategies for full waveform inversion: 1D experiments",
            "venue": "Geophysics, 74 ",
            "year": 2009
        },
        {
            "authors": [
                "J. Chen",
                "Y. Chen",
                "H. Wu",
                "D. Yang"
            ],
            "title": "The quadratic Wasserstein metric for earthquake location",
            "venue": "J. Comput. Phys., 373 ",
            "year": 2018
        },
        {
            "authors": [
                "K. Chen",
                "M.D. Sacchi"
            ],
            "title": "Time-domain elastic Gauss-Newton full-waveform inversion: a matrix-free approach",
            "venue": "Geophys. J. Int., 223 ",
            "year": 2020
        },
        {
            "authors": [
                "G. C\u00f4rte",
                "J. Dramsch",
                "H. Amini",
                "C. MacBeth"
            ],
            "title": "Deep neural network application for 4D seismic inversion to changes in pressure and saturation: Optimizing the use of synthetic training datasets",
            "venue": "Geophysical Prospecting, 68 ",
            "year": 2020
        },
        {
            "authors": [
                "T. Dierkes",
                "O. Dorn",
                "F. Natterer",
                "V. Palamodov",
                "H. Sieschott"
            ],
            "title": "Fr\u00e9chet derivatives for some bilinear inverse problems",
            "venue": "SIAM J. Appl. Math., 62 ",
            "year": 2002
        },
        {
            "authors": [
                "B. Engquist",
                "B.D. Froese",
                "Y. Yang"
            ],
            "title": "Optimal transport for seismic full waveform inversion",
            "venue": "Commun. Math. Sci., 14 ",
            "year": 2016
        },
        {
            "authors": [
                "B. Engquist",
                "K. Ren",
                "Y. Yang"
            ],
            "title": "The quadratic Wasserstein metric for inverse data matching",
            "venue": "Inverse Problems, 36 ",
            "year": 2020
        },
        {
            "authors": [
                "I. Epanomeritakis",
                "V. Akcelik",
                "O. Ghattas",
                "J. Bielak"
            ],
            "title": "A Newton-CG method for large-scale three-dimensional elastic full-waveform seismic inversion",
            "venue": "Inverse Problems, 24 ",
            "year": 2008
        },
        {
            "authors": [
                "J. Fang",
                "H. Zhou",
                "Y.E. Li",
                "Q. Zhang",
                "L. Wang",
                "P. Sun",
                "J. Zhang"
            ],
            "title": "Data-driven low-frequency signal recovery using deep-learning predictions in full-waveform inversion",
            "venue": "Geophysics, 85 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Farris",
                "M. Araya-Polo",
                "J. Jennings",
                "B. Clapp",
                "B. Biondi"
            ],
            "title": "Tomography: a deep learning vs full-waveform inversion comparison",
            "venue": "Proceedings, First EAGE Workshop on High Performance Computing for Upstream in Latin America, vol. 2018",
            "year": 2018
        },
        {
            "authors": [
                "J. Feliu-Fab\u00e0",
                "Y. Fan",
                "L. Ying"
            ],
            "title": "Meta-learning pseudo-differential operators with deep neural networks",
            "venue": "J. Comput. Phys., 404 ",
            "year": 2020
        },
        {
            "authors": [
                "A. Fichtner"
            ],
            "title": "Full Seismic Waveform Modelling and Inversion",
            "venue": "Springer-Verlag, Berlin",
            "year": 2011
        },
        {
            "authors": [
                "R. Fletcher"
            ],
            "title": "Practical methods of optimization",
            "venue": "John Wiley & Sons",
            "year": 2013
        },
        {
            "authors": [
                "L. Guasch",
                "O. Calder\u00f3n Agudo",
                "M.X. Tang",
                "P. Nachev",
                "M. Warner"
            ],
            "title": "Fullwaveform inversion imaging of the human brain",
            "venue": "Digit. Med., 3 ",
            "year": 2000
        },
        {
            "authors": [
                "W. Hu",
                "Y. Jin",
                "X. Wu",
                "J. Chen"
            ],
            "title": "Progressive transfer learning for low-frequency data prediction in full-waveform inversion",
            "venue": "Geophysics, 86 ",
            "year": 2021
        },
        {
            "authors": [
                "V. Isakov"
            ],
            "title": "Inverse Problems for Partial Differential Equations",
            "venue": "Springer-Verlag, New York, second ed.",
            "year": 2006
        },
        {
            "authors": [
                "A. Javaherian",
                "F. Lucka",
                "B.T. Cox"
            ],
            "title": "Refraction-corrected ray-based inversion for three-dimensional ultrasound tomography of the breast",
            "venue": "Inverse Problems, 36 ",
            "year": 2020
        },
        {
            "authors": [
                "Z. Jia",
                "R. Guo",
                "M. Li",
                "G. Wang",
                "Z. Liu",
                "Y. Shao"
            ],
            "title": "3-d model-based inversion using supervised descent method for aspect-limited microwave data of metallic targets",
            "venue": "IEEE Trans. Geosci. Remote Sens., ",
            "year": 2021
        },
        {
            "authors": [
                "V. Kazei",
                "O. Ovcharenko",
                "P. Plotnitskii",
                "D. Peter",
                "X. Zhang",
                "T. Alkhalifah"
            ],
            "title": "Mapping full seismic waveforms to vertical velocity profiles by deep learning",
            "venue": "Geophysics, 86 ",
            "year": 2021
        },
        {
            "authors": [
                "D. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv:1412.6980, ",
            "year": 2014
        },
        {
            "authors": [
                "F. Li",
                "U. Villa",
                "S. Park",
                "M.A. Anastasio"
            ],
            "title": "Three-dimensional stochastic numerical breast phantoms for enabling virtual imaging trials of ultrasound computed tomography",
            "venue": "arXiv:2106.02744, ",
            "year": 2021
        },
        {
            "authors": [
                "Z. Lin",
                "R. Guo",
                "M. Li",
                "A. Abubakar",
                "T. Zhao",
                "F. Yang",
                "S. Xu"
            ],
            "title": "Low-frequency data prediction with iterative learning for highly nonlinear inverse scattering problems",
            "venue": "IEEE Trans. Microw. Theory Tech., ",
            "year": 2021
        },
        {
            "authors": [
                "B. Liu",
                "S. Yang",
                "Y. Ren",
                "X. Xu",
                "P. Jiang",
                "Y. Chen"
            ],
            "title": "Deep-learning seismic fullwaveform inversion for realistic structural models",
            "venue": "Geophysics, 86 ",
            "year": 2021
        },
        {
            "authors": [
                "Q. Liu",
                "S. Beller",
                "W. Lei",
                "D. Peter",
                "J. Tromp"
            ],
            "title": "Preconditioned BFGS-based uncertainty quantification in elastic full waveform inversion",
            "venue": "arXiv:2009.12663, ",
            "year": 2020
        },
        {
            "authors": [
                "F. Lucka",
                "M. P\u00e9rez-Liva",
                "B.E. Treeby",
                "B.T. Cox"
            ],
            "title": "High resolution 3D ultrasonic breast imaging by time-domain full waveform inversion",
            "venue": "arXiv:2102.00755, ",
            "year": 2021
        },
        {
            "authors": [
                "S. Mache",
                "P.K. Pokala",
                "K. Rajendran",
                "C.S. Seelamantula"
            ],
            "title": "DuRIN: A deepunfolded sparse seismic reflectivity inversion network",
            "venue": "arXiv:2104.04704, ",
            "year": 2021
        },
        {
            "authors": [
                "T.P. Matthews",
                "J. Poudel",
                "L. Li",
                "L.V. Wang",
                "M.A. Anastasio"
            ],
            "title": "Parameterized joint reconstruction of the initial pressure and sound speed distributions for photoacoustic computed tomography",
            "venue": "SIAM J. Imaging Sci., 11 ",
            "year": 2018
        },
        {
            "authors": [
                "L. M\u00e9tivier",
                "A. Allain",
                "R. Brossier",
                "Q. M\u00e9rigot",
                "E. Oudet",
                "J. Virieux"
            ],
            "title": "Optimal transport for mitigating cycle skipping in full waveform inversion: a graph space transform approach",
            "venue": "Geophysics, 83 ",
            "year": 2018
        },
        {
            "authors": [
                "R. Modrak",
                "J. Tromp"
            ],
            "title": "Seismic waveform inversion best practices: regional",
            "venue": "global and exploration test cases, Geophys. J. Int., 206 ",
            "year": 2016
        },
        {
            "authors": [
                "R.-E. Plessix"
            ],
            "title": "A review of the adjoint-state method for computing the gradient of a functional with geophysical applications",
            "venue": "Geophys. J. Int., 167 ",
            "year": 2006
        },
        {
            "authors": [
                "R.G. Pratt"
            ],
            "title": "Seismic waveform inversion in the frequency domain",
            "venue": "Part 1: Theory and verification in a physical scale model, Geophysics, 64 ",
            "year": 1999
        },
        {
            "authors": [
                "R.G. Pratt",
                "C. Shin",
                "G.J. Hicks"
            ],
            "title": "Gauss-Newton and full Newton methods in frequency-space seismic waveform inversion",
            "venue": "Geophys. J. Int., 133 ",
            "year": 1998
        },
        {
            "authors": [
                "N. Rahaman",
                "A. Baratin",
                "D. Arpit",
                "F. Draxler",
                "M. Lin",
                "F. Hamprecht",
                "Y. Bengio",
                "A. Courville"
            ],
            "title": "On the spectral bias of neural networks",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, vol. 97",
            "year": 2019
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "J. Comput. Phys., 378 ",
            "year": 2019
        },
        {
            "authors": [
                "Y. Ren",
                "L. Nie",
                "S. Yang",
                "P. Jiang",
                "Y. Chen"
            ],
            "title": "Building complex seismic velocity models for deep learning inversion",
            "venue": "IEEE Access, 9 ",
            "year": 2021
        },
        {
            "authors": [
                "R. Rojas-Gomez",
                "J. Yang",
                "Y. Lin",
                "J. Theiler",
                "B. Wohlberg"
            ],
            "title": "Physics-consistent data-driven waveform inversion with adaptive data augmentation",
            "venue": "IEEE Geosci. Remote. Sens. Lett., ",
            "year": 2020
        },
        {
            "authors": [
                "B. Ronen",
                "D. Jacobs",
                "Y. Kasten",
                "S. Kritchman"
            ],
            "title": "The convergence rate of neural networks for learned functions of different frequencies",
            "venue": "Advances in Neural Information Processing Systems, vol. 32",
            "year": 2019
        },
        {
            "authors": [
                "F. Santosa",
                "W.W. Symes"
            ],
            "title": "Analysis of Least-squares velocity inversion",
            "venue": "Society of Exploration Geophysicists",
            "year": 1989
        },
        {
            "authors": [
                "L. Sirgue",
                "R.G. Pratt"
            ],
            "title": "Efficient waveform inversion and imaging: A strategy for selecting temporal frequencies",
            "venue": "Geophysics, 69 ",
            "year": 2004
        },
        {
            "authors": [
                "J.D. Smith",
                "K. Azizzadenesheli",
                "Z.E. Ross"
            ],
            "title": "Eikonet: Solving the eikonal equation with deep neural networks",
            "venue": "arXiv:2004.00361, ",
            "year": 2020
        },
        {
            "authors": [
                "C. Song",
                "T. Alkhalifah"
            ],
            "title": "Wavefield reconstruction inversion via physics-informed neural networks",
            "venue": "arXiv:2104.06897, ",
            "year": 2021
        },
        {
            "authors": [
                "B. Sun",
                "T. Alkhalifah"
            ],
            "title": "ML-misfit: learn a robust misfit function for full-waveform inversion using machine learning",
            "venue": "arXiv:2002.03163v2, ",
            "year": 2020
        },
        {
            "authors": [
                "H. Sun",
                "L. Demanet"
            ],
            "title": "Extrapolated full-waveform inversion with deep learning",
            "venue": "Geophysics, 85 ",
            "year": 2020
        },
        {
            "authors": [
                "J. Sun",
                "K.A. Innanen",
                "C. Huang"
            ],
            "title": "Physics-guided deep learning for seismic inversion with hybrid training and uncertainty analysis",
            "venue": "Geophysics, 86 ",
            "year": 2021
        },
        {
            "authors": [
                "W.W. Symes"
            ],
            "title": "Migration velocity analysis and waveform inversion",
            "venue": "Geophysical Prospecting, 56 ",
            "year": 2008
        },
        {
            "authors": [
                "J. Tromp",
                "C. Tape",
                "Q. Liu"
            ],
            "title": "Seismic tomography",
            "venue": "adjoint methods, time reversal and banana-doughnut kernels, Geophys. J. Int., 160 ",
            "year": 2005
        },
        {
            "authors": [
                "J. Virieux",
                "A. Asnaashari",
                "R. Brossier",
                "L. M\u00e9tivier",
                "A. Ribodetti",
                "W. Zhou"
            ],
            "title": "An introduction to full waveform inversion",
            "venue": "Encyclopedia of Exploration Geophysics",
            "year": 2014
        },
        {
            "authors": [
                "J. Virieux",
                "S. Operto"
            ],
            "title": "An overview of full-waveform inversion in exploration geophysics",
            "venue": "Geophysics, 74 ",
            "year": 2009
        },
        {
            "authors": [
                "J. Wiskin",
                "B. Malik",
                "D. Borup",
                "N. Pirshafiey",
                "J. Klock"
            ],
            "title": "Full wave 3D inverse scattering transmission ultrasound tomography in the presence of high contrast",
            "venue": "Scientific Reports, 10 ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wu",
                "Y. Lin"
            ],
            "title": "InversionNet: An efficient and accurate data-driven full waveform inversion",
            "venue": "IEEE Transactions on Computational Imaging, 6 ",
            "year": 2020
        },
        {
            "authors": [
                "Z.J. Xu",
                "Y. Zhang",
                "Y. Xiao"
            ],
            "title": "Training behavior of deep neural network in frequency domain",
            "venue": "arXiv:1807.01251v3, ",
            "year": 2018
        },
        {
            "authors": [
                "F. Yang",
                "J. Ma"
            ],
            "title": "Deep-learning inversion: A next-generation seismic velocity model building method",
            "venue": "Geophysics, 84 ",
            "year": 2019
        },
        {
            "authors": [
                "S. Yu",
                "J. Ma"
            ],
            "title": "Data-driven geophysics: from dictionary learning to deep learning",
            "venue": "arXiv:2007.06183, ",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhang",
                "J. Gao"
            ],
            "title": "Deep-learning full-waveform inversion using seismic migration images",
            "venue": "IEEE Trans. Geosci. Remote Sens., ",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "Y. Lin"
            ],
            "title": "Data-driven seismic waveform inversion: A study on the robustness and generalization",
            "venue": "IEEE Trans. Geosci. Remote Sens., 58 ",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Key words. full waveform inversion, computational inverse problem, deep learning, neural networks, preconditioning, data-driven inversion\nAMS subject classifications 2020. 35R30, 49N45, 65M32, 74J25, 78A46, 86A22"
        },
        {
            "heading": "1 Introduction",
            "text": "Full waveform inversion (FWI) refers to the process of extracting information on physical parameters of wave equations from data related to the solutions to the wave equations [3, 9, 10, 12, 15, 18, 21, 37, 41, 43, 44, 45, 52, 58, 59, 60]. In seismic imaging, this is manifested as the problem of reconstructing the speed distribution of seismic waves in the interior of the Earth from measured wave field data on the Earth surface. The sources of the measured waves could come either from nature, such as earthquakes, or from geophysical exploration activities by humankind, such as air guns and seismic vibrators. We refer interested readers\n\u2217Department of Applied Physics and Applied Mathematics, Columbia University, New York, NY 10027; wd2288@columbia.edu\n\u2020Department of Applied Physics and Applied Mathematics, Columbia University, New York, NY 10027; kr2002@columbia.edu\n\u2021Department of Applied Physics and Applied Mathematics, Columbia University, New York, NY 10027; lz2784@columbia.edu\nar X\niv :2\n20 3.\n01 79\n9v 1\n[ m\nat h.\nN A\n] 3\nM ar\n2 02\nto [13, 25, 42, 61] and references therein for overviews on the recent development in the field of FWI for geophysical applications. While the term FWI was mainly coined in the seismic imaging community, FWI also has a wide range of applications in other imaging applications, such as in medical ultrasound imaging [5, 8, 27, 30, 34, 38, 40, 62]. From the practical point of view, the main difference between geophysical and medical FWI is that the quality of the dataset collected in medical applications, both in terms of the variety of source-detector configurations can be arranged and in terms of the frequency contents of the incident sources, is much richer than that of the geophysical FWI dataset.\nFor the sake of concreteness, let us consider the simplest model of acoustic wave propagation in a heterogeneous medium \u2126 with wave speed field m(x) > 0. The wave field u solves\n1\nm2(x)\n\u22022u \u22022t \u2212\u2206u = 0, in (0,+\u221e)\u00d7 \u2126 \u2202u\n\u2202n = h(t,x), on (0,+\u221e)\u00d7 \u2202\u2126\n(1)\nwith an appropriate initial condition. Here, n is the unit outward normal vector of the domain boundary at x \u2208 \u2202\u2126. The data that we measure is time traces of the solution to the wave equation (1) at a set of detector locations, say \u0393 \u2282 Rd, for a period of time, say T , that is,\ng := u(t,x)|(0,T ]\u00d7\u0393 . (2)\nThe objective of FWI in this setting is to recover the unknown wave speed field m in the wave equation (1) from the measured data u(t,x)|(0,T ]\u00d7\u0393 collected in a multi-source multidetector configuration. This is a challenging inverse problem that has rich mathematical and computational content. The main computational strategy, due to the lack of explicit/semiexplicit reconstruction methods, in solving the FWI inverse problem as well as many other model-based inverse problems, is the classical L2 least-squares formulations where we search for the inverse solution by minimizing the L2 mismatch between model predictions and observed data. To formulate this more precisely, we assume that we collect data from Ns acoustic sources {hs}Nss=1, and we denote by f(m;hs) the forward model that takes m to the corresponding wave field data gs (1 \u2264 s \u2264 Ns). Then the inverse problem of reconstructing m from measured data g\u03b4s aims at solving the following operator equation:\nf(m) = g\u03b4 (3)\nwhere\nf(m) :=  f(m;h1) ... f(m;hs)\n... f(m;hNs)  and g\u03b4 :=  g\u03b41 ... g\u03b4s ... g\u03b4Ns  . The superscript \u03b4 denotes the fact that the datum g is polluted by measurement noise. The classical L2 least-squares method performs the reconstruction by searching for m that\nminimizes the mismatch functional (with the possibility of adding a regularization term):\n\u03a8(m) := 1\n2 \u2016f(m)\u2212 g\u03b4\u20162[L2((0,T ]\u00d7\u0393)]Ns . (4)\nThis is a challenging numerical optimization problem that has been extensively studied in the past three decades. Many novel methods have been developed to address two of the main challenges: (i) the high computational cost needed to reconstruct high-resolution images of m, and (ii) the abundance of local minimizers (due to the non-convexity of the least-squares functional) that trap iterative reconstruction algorithms; see for instance [13, 25, 51] for a detailed explanation of those challenges among others.\nIn recent years, there are great interest in the FWI community to use deep learning techniques, based on neural networks, to replace the classical least-squares based inversion methods [1, 4, 16, 22, 23, 28, 31, 32, 36, 39, 49, 54, 55, 56, 57, 63, 65, 66, 67, 68]. Assume that we are given a set of sampled data\n{gj := (gj1, \u00b7 \u00b7 \u00b7 , gjs, \u00b7 \u00b7 \u00b7 , gjNs)T,mj}Nj=1 , (5)\nwhere {mj}Nj=1 are a set of N velocity profiles sampled from a given distribution and {gj}Nj=1 are the corresponding wave field predictions generated from Ns sources {hs}Nss=1 with the model g = f(m). Deep learning methods try to train a neural network, denote by f\u22121\u03b8 (g), with \u03b8 denoting the set of parameters (that is, the weight matrices and the bias vectors) of the neural networks, that represents the inverse operator f\u22121. A training process based on the L2 loss functional can be formulated as:\n\u03b8\u0302 = arg min \u03b8\u2208\u0398\nL(\u03b8) with L(\u03b8) := 1\n2N N\u2211 j=1 \u2016mj \u2212 f\u22121\u03b8 (gj)\u2016 2 L2(\u2126)\nwhere \u0398 represent the space of parameters of the network, and a regularization term can be added to the loss function L(\u03b8) to help stabilize the training process. The number of samples N needs to be large enough in order for L(\u03b8) to be a good approximation to the expectation of the mismatch over the distribution: Em[\u2016m\u2212 f\u22121\u03b8 (g(m))\u20162L2(\u2126)]. Many other types of loss functions can be used, but we will not dive into this direction. Note that since we know the forward operator f and are only interested in learning its inverse operator, the datasets used in the training process are synthetic: for each data point (gj,mj), gj is constructed by solving the wave equation (1) with the given speed field mj and evaluate (2).\nNumerical experiments, such as those documented in [4, 32, 36, 54, 63, 65, 66, 67, 68], showed that, with sufficiently large training datasets, it is possible to train highly accurate inverse operators that can be used to directly map measured wave field data into the velocity field. This, together with the recent success in learning inverse operators for other inverse problems (see for instance [2, 7, 11, 24, 47, 53] for some examples) has led many to believe, probably overly optimistically, that one can completely replace classical computational inversion with offline deep learning.\nDespite the tremendous success in deep learning for FWI, it is still computational challenging to train a once-for-all inverse machine f\u22121\u03b8 . First, with the aim of reconstructing\nhigh-resolution images of the velocity field m(x), the size of the neural networks to be constructed as a discrete representation of f\u22121\u03b8 is prohibitively large. Second, it is well known that f : m 7\u2192 g is a smoothing operator (between appropriate spaces; see for instance [29] and references therein for more precise mathematical characterization of the statement). The inverse operator is therefore de-smoothing. Learning such operators requires the ability to capture precisely high-frequency information in the training data, and this is very hard to do in the training process as deep neural networks tend to capture low-frequency components of the data much more efficiently than the high-frequency components [46, 50, 64]. On top of the above, the inverse operator f\u22121\u03b8 we learned from model-generated data very often has limited generalization, making it challenging to apply the operator to new measured datasets.\nIn this work, we propose an offline-online computational strategy for coupling classical least-squares based computational inversion with deep learning based approaches for FWI to achieve advantages that can not be achieved with only one of the components. Roughly speaking, we utilize offline trained approximate inverse of the operator f to precondition online least-squares based numerical reconstructions. Instead of pursuing high-quality training of highly accurate inverse operator, we train neural networks that only capture the main features in the velocity field. This relaxes dramatically the requirement on both the size of the dataset and the computational resources needed in the training process, and the trained model is more generalizable to other classes of velocity models. Meanwhile, the offline trained approximate inverse is sufficient as a nonlinear preconditioner to improve the speed of convergence of the classical least-squares based FWI numerical reconstruction in the online stage of the inversion.\nThe rest of the paper is organized as follows. We first describe the proposed coupling strategy in Section 2 in the abstract setting. We then present some preliminary understanding on the training and reconstruction stage of the method in Section 3. In Section 4 we discuss the details of the implementation of the strategy. Extensive numerical simulations are presented in Section 5 to demonstrate the performance of the learning-inversion coupling. Concluding remarks are offered in Section 6."
        },
        {
            "heading": "2 Coupling learning with FWI",
            "text": "Our main objective here is to couple the deep learning based image reconstruction approach with the classical least-squares based image reconstruction method for FWI. More precisely, we utilize the approximate inverse we learned with neural networks to construct a new objective function for least-squares based FWI reconstruction from measured data."
        },
        {
            "heading": "2.1 Robust offline learning of main features",
            "text": "In the offline learning stage, we use deep learning to train an approximate inverse of the operator f . As we outlined in the previous section, our main argument is that the learning process can only be performed reliably on a small amount of dominant features of the velocity\nfield. First, resolving all details of the velocity field requires over-sized neural networks that demand an exceedingly large amount of training data, not to mention that such networks are computationally formidable to train reliably. Second, large neural networks or large sizes display serious frequency bias in picking up frequency contents in the training datasets [64], making it inefficient in fitting high-frequency components of the velocity field. Despite all the challenges in resolving high-frequency features, it has been shown in different scenarios that learning low-frequency components of the velocity profile can be done in a robust manner [35, 48, 56]. This means that if we take the Fourier representation, the lower Fourier modes of the inverse operator can be learned stably. This good low-frequency approximate inverse is our main interest in the learning stage (even though an accurate inverse itself would be better if one can realistically have it).\nLet M be the feature map we selected, and m the corresponding feature vector, that is,\nM : m(x) \u2208M 7\u2212\u2192 m \u2208M ,\nwhere M\u2286 L2(\u2126) is the class of velocity field that we are interested in and M the space of the feature vectors. Motivated by the analysis of weighted optimization in [19, 20], we train a network, which we still denote as f\u22121\u03b8 : g 7\u2192 m, using the synthetic dataset (5), through the optimization problem\n\u03b8\u0302 = arg min \u03b8\u2208\u0398 L(\u03b8) with L(\u03b8) := 1 2N N\u2211 j=1 \u2225\u2225\u2225\u00b5~ (mj \u2212 f\u22121\u03b8 (gj))\u2225\u2225\u22252M (6) where the weight vector \u00b5 is selected to weight the loss heavily on the features we are interested in while damping the features that are hard to learn stably. The is used to denote the componentwise product between the vectors involved. The selection of the feature vectors as well as the weighting vector \u00b5 will be discussed in Section 5 in more detail. For the purpose of illustrating the main idea, let us point out that one example is to think of (6) as the equivalence of\n\u03b8\u0302 = arg min \u03b8\u2208\u0398\n1\n2N N\u2211 j=1 \u222b \u2126 (\u222b \u2126 \u00b5(x\u2212 y) ( mj(y)\u2212 f\u22121\u03b8 (gj)(y) ) dy )2 dx\nin the Fourier domain, i.e. when the features we use are Fourier modes, with \u00b5 the Fourier transform of the kernel \u00b5(x). If we take \u00b5 to be a smoothing kernel, such as a Gaussian kernel, \u00b5 will decay fast with the increase of the frequency. In such a case, the learning problem (6) focuses on the lower Fourier modes of the velocity field m.\nWeighted optimization schemes of the form (6) with weight \u00b5 to emphasize dominant features in the learning problems have been extensively studied in the learning and inverse problems community; see [20] and references therein. When the feature we selected are Fourier basis, it has been shown that correct selecting of the weight \u00b5 in the training scheme can lead to more robust learning results for a class of models f\u22121\u03b8 following certain distributions, sometimes at the expense of learning accuracy, with better generalization capabilities [20]. This is the main motivation for us to adopt this strategy for our purpose in this research."
        },
        {
            "heading": "2.2 New objective function for online inversion",
            "text": "In the online reconstructions stage, we utilize the approximate inverse we trained to construct a new objective function for FWI image reconstruction from given noisy data g\u03b4. More precisely, instead of solving the model (3), we aim at solving the modified model\nf\u0302\u22121 \u03b8\u0302\n( f(m) ) = f\u0302\u22121\n\u03b8\u0302 (g\u03b4) (7)\nwhere f\u0302\u22121 \u03b8\u0302 := M\u22121 \u25e6 f\u22121 \u03b8\u0302 : g 7\u2192 m is the learned approximate to f\u22121 (while f\u22121 \u03b8\u0302\n: g 7\u2192 m is the learned representation in M). The least-squares formulation for the reconstruction problem now takes the form\nm\u0302 = arg min m\u2208M \u03a6(m) , (8)\nwith\n\u03a6(m) := 1\n2 \u2016f\u0302\u22121 \u03b8\u0302\n( f(m) ) \u2212 f\u0302\u22121\n\u03b8\u0302 (g\u03b4)\u20162L2(\u2126) +\n\u03b3 2 \u2016M\u22121(\u00b5\u22121 ~m)\u20162L2(\u2126) . (9)\nThe last term in the objective functional is a Tikhonov regularization functional that imposes a smoothness constraint on the target velocity field. This smoothness constraint is selected such that it is consistent with the training process. The natural initial guess for any iterative solution scheme for this minimization problem is m0 := f\u0302\n\u22121 \u03b8\u0302 (g\u03b4).\nLet us emphasize that there is a significant difference between the L2 objective function \u03a6(m) we introduced in (9), ignoring the regularization term, and the standard L2 objective function \u03a8(m) defined in (4). Our objective function \u03a6(m) measures the mismatch between the approximations of predicted velocity field and the true velocity field corresponding to the measured data, while the standard objective function \u03a8(m) measures the mismatch between predicted wave field data with measured wave field data. In other words, our objective function works on the parameter space (also called the model space in the FWI literature, that is, the space of the velocity field) while the standard objective function is defined on the signal space (that is the space of wave field signals at the detectors). With reasonably-trained f\u0302\u22121 \u03b8\u0302\n, the functional \u03a6(m) has advantageous landscape for optimization purpose as we will demonstrate in the numerical simulations in Section 5."
        },
        {
            "heading": "2.3 The benefits of the coupling approach",
            "text": "The offline-online coupling scheme we proposed allowed us to focus on training a robust approximate inverse instead of the exact inverse. This makes the learning process more stable and also requires less computational resources (in terms of the amount of data, the size of the network, and the computational cost for optimization) than training an accurate inverse. Moreover, the sacrifice in accuracy brings better generalizability for the learned approximate inverse. On the computational side, the trained approximate inverse serves as a \u201cpreconditioner\u201d for the inversion process. It can not only provide a good initial guess for the reconstruction but also simplify the landscape of the optimization problem.\nWe finish this section with the following remark. In the ideal case when all the operators involved are invertible as they should be, the solution to (7) is identical to the solution to (3), assuming that g indeed lives in the range of f . Therefore, our formulation does not change the true solution to the original inverse problem. However, as we will see, the new formulation utilizes the result of learning to facilitate the FWI reconstruction in terms of saving computational cost as well as making the optimization landscape more desirable."
        },
        {
            "heading": "3 Formal understanding of the coupling",
            "text": "We now attempt to gain a more systematic understanding of the coupling strategy. As we have argued in the previous sections, it is computationally challenging to train neural networks that are accurate approximations of the inverse operator and are very generalizable at the same time. However, there is certainly some dominant information in the inverse operator that we could extract with learning and this is the approximate inverse that we are interested in constructing."
        },
        {
            "heading": "3.1 Elements of network training",
            "text": "Due to the fact that the training data we have are generated from exactly the same operator we are trying to represent with the neural network, the learning process we have is much more under control than those purely data-driven learning problems in applications. Here we highlight a few critical issues in the learning process without getting into the details of the implementation of the learning algorithm.\nSampling training data. To learn the inverse operator, we need to pay attention to both its input space and its output space. While our focus will be to learn the low-frequency component of the inverse operator, we want the training data to include as much highfrequency information as possible to gain generalization capability in the input space. Let Kout be the frequency range for the network output that we are interested in recovering and Kin the frequency range of the velocity fields that generated the wavefield data. We construct the training dataset as\n{mj(x),g := f(mj(x) + m\u0303j(x))}Nj=1\nwhere m\u0303j(x) are selected such that F(m\u0303j)(k) = 0 \u2200k \u2208 Kout, and F(m\u0303j)(k) 6= 0 \u2200k \u2208 Kin\\Kout (F(m) denoting the Fourier transform of m). In other words, we train the network with input wavefield data having richer frequency content of the velocity field than the output velocity field. This constructing enrich the frequency content of the input data but does not increase the computational cost of the training process.\nThe well-known result on the differentiability of the data g with respect to m, quoted in the proposition below, indicates that the input space of the inverse operator, i.e. the range of the forward operator, is quantitatively smoother than the output space (the velocity space)\nthat we are working with. Therefore, a well-trained network approximation should have good interpolation ability in applications when the space of velocity field we are interested in working with is sufficiently smooth.\nProposition 3.1 ( [6, 17, 29]). Let \u2126 be a smooth domain and h(t,x) be the restriction of a C1 function on \u2202\u2126. Assume further that m \u2208 C2(\u2126\u0304)\u2229 [m,m] for some 0 < m < m < +\u221e. Then the map: f(m) : m 7\u2212\u2192 g is Fre\u0301chet differentiable at any m \u2208 C2(\u2126\u0304) \u2229 [m,m].\nThe result is standard. We refer interested reader to [6, 17, 29] and references therein for more precise formulations of it in different scenarios. This result also ensures that if we can train a stable network, then the learning quality is guaranteed; see Lemma 3.3 below.\nNetwork training error. Our main objective of this work is to focus the learning process on the low-frequency content of the output of the inverse operator. We do this with the weighted optimization scheme (6) by selecting weight \u00b5 that penalizes heavily the lowfrequency component of the mismatch of true data and the network prediction. The impact of such weighting schemes on the learning results have been analyzed extensively; see [19, 20] and reference therein. We illustrate this in an extremely simplified setting. Let F := (f\u22121)\u2032(m0) be the linearization of f\n\u22121 at m0 for a one-dimensional medium. Assume that the learning loss function L(\u03b8) in (6) is minimized to the order of \u03b52 in the training process. Then on the leading order, the trained F satisfies\n\u00b5~ (m\u2212 FG) \u223c O(\u03b5),\nwhere m = [m1, \u00b7 \u00b7 \u00b7 ,mN ] is the matrix whose columns are vectors of the Fourier coefficients of the training velocity samples {mj}Nj=1, G = [g1, \u00b7 \u00b7 \u00b7 ,gN ] is a matrix whose columns are vectors of the input data, and O(\u03b5) is a diagonal matrix of size order \u03b5. The trained linearized inverse operator, when applied to a new input data g\u03b4, gives the result\nFg\u03b4 \u223c ( m\u2212 \u00b5\u22121 ~O(\u03b5) ) GT(GGT)\u22121g\u03b4 .\nThe nature of \u00b5 clearly indicates that the relative error in the learned output is larger in the high-frequency Fourier modes."
        },
        {
            "heading": "3.2 Inversion with accurate training",
            "text": "As we discussed in the previous section, when the network is trained so that f\u0302\u22121 \u03b8\u0302 = f\u22121, the objective function \u03a6(m), defined in (9), in the reconstruction stage is a convex functional of m. When the learning is not perfect but sufficiently accurate, the functional \u03a6(m) still has an advantageous landscape. This is given in the following result.\nLemma 3.2. Let f\u0302\u22121 \u03b8\u0302 : g \u2208 [L2([0, T ]\u00d7\u0393)]Ns 7\u2192 m \u2208 L2(\u2126) be an approximation to f\u22121 with Fre\u0301chet derivative at g given as df\u0302\u22121\n\u03b8\u0302 [g]. Assume that\nsup m \u2016f\u0302\u22121 \u03b8\u0302\n( f(m) ) \u2212m\u2016L2(\u2126) \u2264 (10)\nand A := 1 + sup\ng \u2016df\u0302\u22121 \u03b8\u0302 [g]\u2016L([L2([0,T ]\u00d7\u0393)]Ns ;L2(\u2126)) < +\u221e (11)\nfor some > 0 and g\u03b4 = f(m) + \u03b4 for some \u03b4 with \u2016\u03b4\u2016[L2([0,T ]\u00d7\u0393)]Ns sufficiently small. Then we have that\u2223\u2223\u2223\u2016f\u0302\u22121\n\u03b8\u0302\n( f(m) ) \u2212 f\u0302\u22121\n\u03b8\u0302 (g\u03b4)\u2016L2(\u2126) \u2212 \u2016m\u2212m0\u2016L2(\u2126) \u2223\u2223\u2223 \u2264 2 + A\u2016\u03b4\u2016[L2([0,T ]\u00d7\u0393)]Ns . (12) Proof. We denote by r(m) = f\u0302\u22121\n\u03b8\u0302\n( f(m) ) \u2212m. We then have, by Taylor\u2019s theorem, that\nf\u0302\u22121 \u03b8\u0302 (g\u03b4) = f\u0302\u22121 \u03b8\u0302 (f(m0) + \u03b4) = m0 + r(m0) + df\u0302 \u22121 \u03b8\u0302\n[f(m0)] ( \u03b4 ) + o(\u03b4),\nwhere lim \u03b4\u21920\n\u2016o(\u03b4)\u2016L2(\u2126) \u2016\u03b4\u2016\n[L2([0,T ]\u00d7\u0393)]Ns = 0. We therefore have\nf\u0302\u22121 \u03b8\u0302\n( f(m) ) \u2212 f\u0302\u22121\n\u03b8\u0302 (g\u03b4) = m\u2212m0 + r(m)\u2212 r(m0)\u2212 df\u0302\u22121\u03b8\u0302 [f(m0)]\n( \u03b4 ) + o(\u03b4) .\nWe can now use the triangle inequality to conclude that\u2223\u2223\u2223\u2016f\u0302\u22121 \u03b8\u0302 ( f(m) ) \u2212 f\u0302\u22121 \u03b8\u0302 (g\u03b4)\u2016L2(\u2126) \u2212 \u2016m\u2212m0\u2016L2(\u2126) \u2223\u2223\u2223 \u2264 \u2016r(m)\u2212 r(m0)\u2212 df\u0302\u22121\u03b8\u0302 [f(m0)] ( \u03b4 )\n+ o(\u03b4)\u2016L2(\u2126) \u2264 \u2016r(m)\u2016L2(\u2126) + \u2016r(m0)\u2016L2(\u2126) + \u2016df\u0302\u22121\u03b8\u0302 [f(m0)] ( \u03b4 ) \u2016L2(\u2126) + \u2016o(\u03b4)\u2016L2(\u2126) \u2264 2 + A\u2016\u03b4\u2016[L2([0,T ]\u00d7\u0393)]Ns\nwhere the last step comes from the assumptions in (10) and (11). The proof is complete.\nThis result says that the new objective function \u03a6(m) in (9) behaves similarly to the\nquadratic functional \u2016m\u2212 f\u0302\u22121 \u03b8\u0302 (g)\u20162L2(\u2126) provided that the trained f\u0302 \u22121 \u03b8\u0302 is accurate enough. It is clear that we can replace the strong assumption on the accuracy of f\u0302\u22121 \u03b8\u0302 , sup m \u2016f\u0302\u22121 \u03b8\u0302 ( f(m) ) \u2212\nm\u2016L2(\u2126) \u2264 , with the weaker assumption \u2016f\u0302\u22121\u03b8\u0302 ( f(m) ) \u2212m\u2016L2(\u2126) \u2264 \u2016m\u2016L2(\u2126), in which case the 2 term in the bound (12) will be replaced by (\u2016m\u2016L2(\u2126) + \u2016m0\u2016L2(\u2126)). The conclusion still holds.\nDue to the smoothing property of the forward operator as given in Proposition 3.1, the stability of the trained inverse operator, measured by the boundedness of its Fre\u0301chet derivative, is enough to ensure the accuracy of the neural network reconstruction. Therefore, if we could train network approximations with such stability property, they have good generalization capabilities in the output space.\nLemma 3.3. Let m,m0 \u2208 C2(\u2126) \u2229 [m,m] for some 0 < m < m < +\u221e. Then, when \u2016m\u2212m0\u2016L2(\u2126) is sufficiently small, there exists a constant c such that\n\u2016f\u0302\u22121 \u03b8\u0302\n( f(m) ) \u2212 f\u0302\u22121\n\u03b8\u0302\n( f(m0) ) \u2016L2(\u2126) \u2264 c\u2016m\u2212m0\u2016L2(\u2126) (13)\nProof. By Proposition 3.1, the map m 7\u2192 g := f(m) is Fre\u0301chet differentiable with the derivative at m in direction m\u0303 denoted as df [m](m\u0303). By Taylor\u2019s theorem, we have\nf\u0302\u22121 \u03b8\u0302\n( f(m) ) = f\u0302\u22121\n\u03b8\u0302\n( f(m0) + df [m0](m\u2212m0) + o(m\u2212m0) ) = f\u0302\u22121\n\u03b8\u0302\n( f(m0) ) + df\u0302\u22121\n\u03b8\u0302 [f(m0)]\n( df [m0](m\u2212m0) ) + o\u0303(m\u2212m0),\nwhere lim m\u2192m0 \u2016o(m\u2212m0)\u2016[L2([0,T ]\u00d7\u0393)]Ns \u2016m\u2212m0\u2016[L2(\u2126) = lim m\u2192m0 \u2016o\u0303(m\u2212m0)\u2016L2(\u2126) \u2016m\u2212m0\u2016[L2(\u2126)] = 0. We therefore have\nf\u0302\u22121 \u03b8\u0302\n( f(m) ) \u2212 f\u0302\u22121\n\u03b8\u0302 (f(m0)) = df\u0302 \u22121 \u03b8\u0302\n[f(m0)] ( df [m0](m\u2212m0) ) + o\u0303(m\u2212m0) .\nThe bound in (13) then follows from the assumption (11).\nWhen the class of velocity models is sufficiently nice, for instance, when each m(x) can be represented with a small number of Fourier coefficients in a narrow frequency band, one can hope that accurate training is achievable. When this is the case, Lemma 3.2 and Lemma 3.3 ensure that the learned model can be utilized to facilitate the FWI reconstruction with the new dataset."
        },
        {
            "heading": "3.3 Computational simplifications",
            "text": "The reconstruction stage of the coupling can be greatly simplified when the training of the neural network approximation is sufficiently accurate.\nFirst, the coupling method will degenerate to a deep learning based method when we have confidence in our ability in training an accurate deep neural network representation of the inverse operator in FWI. Indeed, when f\u0302\u22121\n\u03b8\u0302 = f\u22121, that is, f\u0302\u22121 \u03b8\u0302 is exactly the inverse, the\nreconstruction step (8) simplifies to\nm\u0302 = arg min m\u2208M\n1 2 \u2016m\u2212 f\u0302\u22121 \u03b8\u0302 (g\u03b4)\u20162L2(\u2126) + \u03b3 2 \u2016\u2207m\u20162L2(\u2126) ,\nassuming, only for the sake of simplifying the notation, that the weighting operator \u00b5(x\u2212y) is taken as an integral operator such that \u00b5\u22121(k) = k. This gives a fast inversion for the new data and immediately leads to the optimal selection of the regularization parameter when the regularization term is not too complicated. In this case, we simply did a post-process on the deep learning reconstruction given by the operator f\u0302\u22121\n\u03b8\u0302 . The solutions to this are\nexplicitly given as m\u0302 = (I + \u03b3\u2206)\u22121f\u0302\u22121\n\u03b8\u0302 (g\u03b4) ,\nwhere I is the identity and \u2206 is the Laplacian operator. Therefore, m is simply a smoothed version of the result produced by the trained neural network, f\u0302\u22121\n\u03b8\u0302 (g\u03b4). The exact form of\nthe smoothing effect depends on the selection of \u00b5.\nSecond, when we can not train an accurate f\u22121, but can train a good approximation to the inverse, that is, when the operator I \u2212 f\u0302\u22121\n\u03b8\u0302 \u25e6 f is not zero but small in an appropriate\noperator norm, the FWI problem (7) can be solved by using Neumann series. More precisely, we can rewrite (7) as\nm\u2212K(m) = f\u0302\u22121 \u03b8\u0302 (g\u03b4), K := I \u2212 f\u0302\u22121 \u03b8\u0302 \u25e6 f\nwhose solution can be expressed in a Neumann series as\nm\u0302 = (I \u2212K)\u22121(f\u0302\u22121 \u03b8\u0302 (g\u03b4)) = \u221e\u2211 j=0 Kj ( f\u0302\u22121 \u03b8\u0302 (g\u03b4) ) . (14)\nThe better the approximation f\u0302\u22121 \u03b8\u0302 is to f\u22121, the faster the series converges. For the training we had, see more discussion in Section 5, a few terms of the Neumann series often provide sufficient accuracy for the reconstruction.\nLet us emphasize that by the informal analysis in Section 3.1, the error in the learning implies roughly that |F ( m\u2212K(m) ) (k)| \u223c \u03b6(k)\u2016m\u2016L2(\u2126) with \u03b6(k) large for large |k|. Due to the fact that the operator norm of I \u2212 K is bounded below by maxk \u03b6(k), this means that the convergence speed of the Neumann series is controlled by the worst training error in the (high-frequency) Fourier modes."
        },
        {
            "heading": "3.4 Utilizing learning outside of training domain",
            "text": "It is important to point out that the weight \u00b5 in the weighted training scheme (6) should be selected to emphasize the low-frequency components of the output and penalize the highfrequency components. It should not completely remove the high-frequency components. If it does, then the high-frequency components of the velocity field in the reconstruction stage can not be recovered with the optimization problem (8). This is an obvious yet important observation that we summarize as a lemma to emphasize it.\nLemma 3.4. Let f\u0302\u22121 \u03b8\u0302 be such that for any m, F [\u0302f\u22121 \u03b8\u0302 (f(m))](k) = 0 \u2200|k| > k0, and m\u0302 be reconstructed from (8) with a gradient-based iterative scheme or the Neumann series method in (14). Then F [m\u0302](k) = 0 \u2200|k| > k0.\nProof. Under the assumption on f\u0302\u22121 \u03b8\u0302 , it is straightforward to check that F(m0)(k) = 0 (m0 := f\u0302\n\u22121 \u03b8\u0302 (g\u03b4)) \u2200|k| > k0, and F(Kjm0)(k) = 0 \u2200|k| > k0, for any j \u2265 1. Therefore F(m\u0302)(k) = 0 \u2200|k| > k0. Let m` be the `-th iteration of a gradient based iterative scheme, then F(r(m`))(k) = 0 (r(m) := f\u0302\u22121\u03b8\u0302 ( f(m) ) \u2212 f\u0302\u22121 \u03b8\u0302 (g\u03b4)) \u2200|k| > k0. This leads to the fact that\nF ( d\u03a6[m`](\u03b4m) ) (k) = 0 for any \u03b4m. Therefore, F(m`+1)(k) = 0 \u2200|k| > k0. The rest of the\nproof follows from an induction.\nFor any velocity field that can be written as mb + \u03b4m with mb the prediction of the trained neural network and \u03b4m outside of the range of the neural network but either has small amplitude (compared to that of m) or has large amplitude by small support compared\nto the size of the domain (in which case \u03b4m is very localized), we can recover \u03b4m with an additional linearized reconstruction step. We linearize the inverse problem around the network prediction f\u22121\n\u03b8\u0302 (g\u03b4). The reconstruction can be performed with a classical migration scheme,\nor equivalently by minimizing the following quadratic approximation to the functional (9):\n\u03a8Q(m) = 1\n2 \u2225\u2225\u2225f(mb) + df [mb](m\u2212mb)\u2212 g\u03b4\u2225\u2225\u22252 [L2((0,T ]\u00d7\u0393)]Ns + \u03b3 2 \u2016\u2207m\u20162L2(\u2126) , (15)\nwhere mb := f \u22121 \u03b8\u0302 (g\u03b4)."
        },
        {
            "heading": "4 Computational implementation",
            "text": "We now provide some details on the implementation of the coupling framework we outlined in the previous section. For computational simplicity, we focus on the implementation in two spatial dimensions even though the methodology itself is independent of the dimension of the problem."
        },
        {
            "heading": "4.1 Computational setup",
            "text": "For the purpose of being concrete, we first describe briefly the geometrical setting under which we implement the learning and reconstruction algorithms. Let x = (x, z). The computational domain of interests is \u2126 = (0, L) \u00d7 (\u2212H, 0). We impose periodic boundary conditions on the left and right boundaries of the domain. Probing sources and detectors are placed on the top and bottom boundaries \u0393t = (0, L) \u00d7 {0} and \u0393b = (0, L) \u00d7 {\u2212H}, depending on the exact applications we have in mind. In geophysical applications, source and detectors are both placed on the top boundary while in medical ultrasound type of applications, source and detectors could be placed on the opposite sides; see Figure 1 for\nan illustration. Under this setup, the wave equation (1) with a source h(t, x) on the top boundary and a reflective bottom boundary takes the form\n1 m2 \u22022u \u22022t \u2212\u2206u = 0, in (0, T ]\u00d7 \u2126,\nu(0, x, z) = \u2202u\n\u2202t (0, x, z) = 0, (x, z) \u2208 (0, L)\u00d7 (\u2212H, 0), u(t, 0, z) = u(t, L, z), (t, z) \u2208 (0, T ]\u00d7 (\u2212H, 0),\n\u2202u \u2202z (t, x,\u2212H) = 0, (t, x) \u2208 (0, T ]\u00d7 (0, L), \u2202u\n\u2202z (t, x, 0) = h(t, x), (t, x) \u2208 (0, T ]\u00d7 (0, L).\n(16)\nSimilar equations can be written down for other types of source-detector configurations."
        },
        {
            "heading": "4.2 The neural network for learning",
            "text": "With the above computational setup, we can generate the training dataset (5) by solving the wave equation (16) with given source functions. We will describe in detail how the training dataset is generated, including the spatial-temporal discretization of the wave equation (16).\nWe construct an autoencoder network scheme to represent the inverse operator. The learning architecture contains three major substructures: an encoder network E\u03b8, a decoder network D\u03b8, and an additional predictor network P\u03b8; see Figure 2 for an illustration of the network flow. More information on the construction of the encoder, the decoder and the predictor is documented in Appendix C. The encoder-decoder substructure is trained to regenerate the input data, while the predictor reads the latent variable to predict velocity field m. In terms of the input-output data, the network training aims at finding the network parameter \u03b8 such that\ngj = D\u03b8(E\u03b8(gj)) and mj = P\u03b8(E\u03b8(gj)), 1 \u2264 j \u2264 N . (17)\nThis is done by a minimization algorithm that minimizes a combined `1-`2 loss function with the `1 loss for the encoder-decoder substructure while `2 for the encoder-predictor substructure. More precisely, we train the network by solving\n\u03b8\u0302 = arg min \u03b8\u2208\u0398\nL\u0303(\u03b8),\nwith\nL\u0303(\u03b8) := 1 N N\u2211 j=1 \u2016gj \u2212D\u03b8(E\u03b8(gj))\u2016`1 + 1 2N N\u2211 j=1 \u2016\u00b5~ ( mj \u2212 P\u03b8(E\u03b8(gj)) ) \u20162M .\n(18)\nWhile the `1 loss for the encoder-decoder substructure is standard in the learning literature, the second part of the loss function is simply what we introduced in (6). Once the training is performed, the approximated inverse is taken as\nf\u22121 \u03b8\u0302 := P\u03b8\u0302 \u25e6 E\u03b8\u0302 .\nLet us emphasize that the main motivation for us to adopt this autoencoder framework, instead of directly training a network for f\u22121\n\u03b8\u0302 , is to take advantage of the commonly observed\ncapability of autoencoders to identify lower dimension features from high-dimensional input data. That is, very often, one can train the autoencoder such that the latent variable E\u03b8(g) contains most of useful information in g but has much lower dimension than g. This lowers the dimension of the predictor network and therefore makes it easier to train the overall network. Moreover, the weighted optimization we used in the encoder-predictor substructure further stabilizes the learning process by focusing on matching the lower-frequency components of the output."
        },
        {
            "heading": "4.3 Learning-assisted FWI inversion",
            "text": "To implement the preconditioned FWI reconstruction method, that is, the solution to the least-squares optimization problem (8), we tested two different algorithms.\nQuasi-Newton method with adjoint state. We implemented a quasi-Newton method based on the BFGS gradient update rule [26] for the numerical reconstruction. This BFGS optimization algorithm itself is standard, so we will not describe it in details here. The algorithm requires the gradient of the objective function \u03a6(m) defined in (9). We evaluate the gradient with a standard adjoint state method. The procedure is documented in Algorithm 1 of Appendix A. The main complication that the learning stage brings into the adjoint state calculation is that we will need the transpose of the gradient of the neural network with respect to its input. This imposes restrictive accuracy requirements on the training of the neural network in the sense that we need the network to learn not only the map from measurement to the velocity field but also the derivative of the operator.\nNeumann series method. The Neumann series method based on (14) is more training friendly since it does not require the adjoint operator of the learned approximate inverse f\u0302\u22121 \u03b8\u0302 . We implemented a J-term truncated Neumann series approximation\nm\u0302 = J\u22121\u2211 j=0 Kj ( f\u0302\u22121 \u03b8\u0302 (g\u03b4) ) . (19)\nThe computational procedure is summarized in Algorithm 2 of Appendix B."
        },
        {
            "heading": "5 Numerical experiments",
            "text": "We now present some numerical simulations to illustrate some of the main characters of the proposed framework of coupling deep learning with model-based FWI reconstruction. We fix the computational domain to be \u2126 = [0, 1] \u00d7 [\u22121, 0], that is, L = H = 1. In this proofof-concept study, we use acoustic source functions that can generate data at all frequencies. We leave it as future work to consider the situation where low-frequency wavefield data are impossible to measure, in applications such as seismic imaging."
        },
        {
            "heading": "5.1 Velocity feature models",
            "text": "In this work, we consider two different feature models for the output velocity field of the neural network.\nGeneralized Fourier feature model. In the first model, we represent m(x) as linear combinations of the Laplace-Neumann eigenfunctions on the computational domain \u2126. To be precise, let (\u03bbk, \u03d5k) (k = (kx, kz) \u2208 N0 \u00d7N0) be the eigenpair of the eigenvalue problem:\n\u2212\u2206\u03d5 = \u03bb\u03d5, in \u2126, n \u00b7 \u2207\u03d5 = 0, on \u2202\u2126 .\nwhere n(x) is the unit outward normal vector of the domain boundary at x \u2208 \u2202\u2126. Then \u03bbk = (kx\u03c0) 2 + (kz\u03c0) 2, and\n\u03d5k(x, z) = cos(kx\u03c0x) cos(kz\u03c0z) .\nIn our numerical simulations, we take\nm(x) = M\u2211\nkx,kz=0\nm(k) \u03d5k(x, z) , (20)\nfor some given M . The generation of the random coefficients m(k) will be described in detail in the next section.\nGaussian mixture model. The second feature model we take is the Gaussian mixture model. More precisely, we represent m(x) as a superposition of Gaussian functions:\nm(x) = m0 + M\u2211 k=1 cke \u2212 1 2 (x\u2212xk0)T\u03a3 \u22121 k (x\u2212x k 0) . (21)\nWith a small number of highly localized Gaussians, successful reconstruction of such a model could provide inside on source locating problems in seismic applications [14]. This is the main motivation for us to consider this model."
        },
        {
            "heading": "5.2 Learning dataset generation",
            "text": "To generate training data, we generate a set of velocity fields and then solve the wave equation model (16) with source functions {hs}Nss=1 to get the corresponding wave field data at the detectors.\nGenerating velocity fields. We first construct a set of N random velocity fields {mj}Nj=1 using the representation (20) or (21). We do this by randomly choosing the coefficients {m(k)}k\u2208N0\u00d7N0 from the uniform distribution U [\u22120.5, 0.5] when considering the model (20) and the coefficients ck from U [0, 5], xk0 from U(\u2212H, 0)\u00d7 U(0, L), (\u03a3k)ij from U [0, 0.2] + 0.1 and m0 = 10 when using the model (21). To mimic frequency content of realistic velocity fields, we force the coefficient m(k) in the random Fourier model (20) to decay asymptotically as\nm(k) \u223c m(k)[(kx + 1)(kz + 1)]\u2212\u03b1, for large |k| = \u221a k2x + k 2 z (22)\nwith \u03b1 \u2265 0 given in the concrete examples later. To make sure that the velocity fields we generated are physically meaningful, we rescale them so that the velocity lives in a range [m,m] (0 < m < m < +\u221e). The linear rescaling is done through the operation\nm(x)\u2190 m\u2212m m\u2217 \u2212m\u2217 m(x) + mm\u2217 \u2212mm\u2217 m\u2217 \u2212m\u2217 , (23)\nwhere m\u2217 := max x m(x) and m\u2217 := min x m(x).\nIn Figure 3 we show some typical samples of the velocity field generated from the aforementioned process. The top panel of Figure 3 shows the surface plots of 4 different randomly generated velocity fields using the model (20) with M = 4. The bottom panel presents the surface plots of 4 random realizations of the velocity field given by the model (21) with M = 2. Random noise at different levels will be added to the sampled velocity fields to study the generalization of the learning scheme we have. The exact level of noise will be given later in concrete examples.\nFinite difference scheme for the wave equation. We use the time-domain staggergrid finite difference scheme that adopts a second-order in both the time and the spatial directions to solve the wave equation (16). Precisely, the discretization is performed with elements over the Cartesian grids formed by (xk, zl) = (k\u2206x, l\u2206z), k, l = 0, 1, ..., K with \u2206x = L/K and \u2206z = H/K. The receivers are equally placed at the bottom surface, coinciding with the grid points, as documented in the right panel of Figure 1, namely, there are K + 1 receivers for each velocity model. We then record the wave signal starting at time t0 and take another shot every j\u2206t until the termination time T , here, j is a positive integer and \u2206t is the uniform time step size for the forward wave solver. As an example for illustration, we take\nh(t, x) = e \u2212(x\u22120.6)2 0.01 \u2212 e \u2212(x\u22120.3)2 0.01 (24)\nto be the top source in (16) and present the recorded time series wave signals in Figure 4. Table 1 summarizes the parameters we used to generate these wavefield signals.\nFigure 4, from the left panel to the right panel, shows the time series wave signals at the bottom surface generated from the velocity model satisfying (21) with M = 2, and the velocity model satisfying (20) with M = 4, respectively; from the top panel to the bottom panel are the wave signals without noise, with 10% multiplication Gaussian noise, and with 10% additive Gaussian noise, respectively.\nLast, we note that to obtain a reliable learning dataset, one needs to guarantee the stability of the time integrator when solving (16). Recall that the second order time-domain\nstagger-grid finite difference forward wave solver is stable under the following CFL condition\n\u2206t \u2264 min{\u2206x,\u2206z}\u221a 2 maxx{m(x)} . (25)\nTo guarantee the stability of the forward solver for all velocity samples, we force\n\u2206t = \u2206t\u2217 < min{\u2206x,\u2206z}\u221a 2 maxx{m(x)} ,\nwhere m is used in the scaling (23), for the data generation of the offline training stage. In this work, we set \u2206t\u2217 = 0.0005 as shown in Table 1 based on our setting."
        },
        {
            "heading": "5.3 Training and testing performance",
            "text": "We now present a systematic numerical exploration on the training and testing performance of the offline training stage. Given that the training and application of the Gaussian mixture\nvelocity model (21) with a small amount of Gaussians functions is extremely successful (due to the smallness of the parameter space) according to our numerical experience, we will focus on the training of the generalized Fourier velocity model (20).\nTraining dataset size. We first emphasize that the training results we show in this section are obtained on a very small dataset in the following sense. The number of data points in the artificial dataset {gj,mj}Nj=1 is small with N = 106. Moreover, for each mj, we collect the wavefield from Ns = 3 illumination sources and Nd = 51 detectors. Those source-detector pairs are a subset of the source-detector pairs for the dataset we used in the reconstruction step. Moreover, at each detector, we use only data at 51 time steps out of the 1000 time steps in the numerical solutions. This small dataset is used so that we can handle the computational cost of the training process with our limited computing resources. It is also intentionally done to demonstrate that one can train reasonable approximate inverse with a significantly smaller dataset if one is willing to sacrifice a little of the training accuracy.\nTraining-testing dataset split. We perform a standard training-validation cycle on the neural network approximate inverse. Before the training process starts, we randomly split the artificial dataset of N = 106 data points into a training dataset and a testing dataset. The training dataset takes 80% of the original dataset, while the test dataset takes the rest 20% of the data points. The training dataset and the validation dataset have no intersection, namely, no data points in the validation set are present in the training dataset."
        },
        {
            "heading": "5.3.1 Random Fourier velocity model: case of non-decaying coefficients",
            "text": "We start with the most challenging scenario where we train the neural network to approximate the inverse operator for the velocity model (20) with randomly generated Fourier coefficients without any decay requirement on the coefficients, that is, we set the decay rate \u03b1 = 0 in (22). This is an extremely challenging case because the effective parameter space of this class of velocity models grows exponentially with respect to the number of Fourier models we have in the model. Ideally, one would need an exponentially large training dataset in order to have reasonable training results. However, due to the smooth property of the map f : m 7\u2192 g, we demonstrate below that with a relatively small dataset, and a very limited number of source detector pairs and time shots, our training result is fairly encouraging.\nIn Figure 5, we show three randomly selected velocity fields (m) from the testing dataset,\nthe corresponding neural network predictions (m\u0303 = f\u0302\u22121 \u03b8\u0302 (f(m))), and the error in the prediction (m \u2212 m\u0303). The largest number of Fourier modes allowed in these learning processes is 10, meaning that 0 \u2264 kx, kz \u2264 9 in the velocity model (20). The training output is a 10 \u00d7 10 matrix containing the content of m(k) in (20). The output space is therefore 100-dimensional. A naive visual inspection of the results in Figure 5 shows that the training process is quite successful as the testing errors seem to be pretty reasonable, especially given that our training dataset is fairly small (0.8 \u00d7 106 data points to be precise). While it is expected that when the number of Fourier modes allowed in the velocity model is very\nlarge, the validation error will be sufficiently large if we keep the training sample size, we do observe that validation error is quite small in general for cases when less than 10 \u00d7 10 Fourier modes are pursued in the learning process. Increasing computational power would certainly improve training quality.\nLet us remark that our training results indeed show that we have better accuracy in learning the low-frequency components of the inverse operator as we discussed in the previous sections of the work. In the right column of Figure 5, we provide the Fourier coefficients of the errors in the network prediction. In all velocity fields, we see clearly larger errors in the higher-frequency components of the network velocity recovery. This is a universal phenomenon that we observed over the testing dataset.\nTo dive a little more into the training quality and the optimization landscape after applying our neural network preconditioner, we offer in Figure 6 the training-validation loss curves for a typical learning experiment. We observe very similar curves for training and validating with the velocity model (20) with different total numbers of Fourier modes. We measure the training accuracy quantitatively with the size of the operator I \u2212 f\u0302\u22121 \u03b8\u0302 \u25e6 f . More precisely, we evaluate the three main quantities for a data point (g,m) in the testing dataset:\n(i) The error in the network prediction of Fourier modes of m:\n\u2206m(k) := m(k)\u2212 f\u22121 \u03b8\u0302 \u25e6 f(m) .\n(ii) The landscape of the classical functional \u03a8(m) evaluated along a line in the direction of a given Fourier mode of m, \u03d5k, passing through two different points m = f \u22121(g) and mnet = f\u0302 \u22121 \u03b8\u0302 (g):\n\u03a8O(h; k) := \u2016g \u2212 f(m0 + h\u03d5k)\u20162[L2((0,T ]\u00d7\u0393)]Ns , m0 = O(m), O \u2208 {I, f\u0302 \u22121 \u03b8\u0302 \u25e6 f} .\n(iii) The landscape under our preconditioner, the new mismatch function \u03a6(m) evaluated as in (ii):\n\u03a6O(h; k) := \u2016f\u0302\u22121\u03b8\u0302 (g)\u2212 f\u0302 \u22121 \u03b8\u0302 \u25e6 f(m0 + h\u03d5k)\u20162L2(\u2126), m0 = O(m), O \u2208 {I, f\u0302\u22121\u03b8\u0302 \u25e6 f} .\nWhen a perfect learning is performed, we would have \u2206m(k) = 0, \u03a8I(h; k) = \u03a8f\u0302\u22121 \u03b8\u0302 \u25e6f (h; k), and \u03a6I(h; k) = \u03a6f\u0302\u22121 \u03b8\u0302 \u25e6f (h; k) for any (g,m) in the training dataset, and \u2206m(k) small, \u03a8I(h; k) \u2248 \u03a8f\u0302\u22121 \u03b8\u0302 \u25e6f (h; k), and \u03a6I(h; k) \u2248 \u03a6f\u0302\u22121 \u03b8\u0302 \u25e6f (h; k) for any (g,m) in the testing dataset.\nIn Figure 7, we show plots of \u2206m(k) (left column), \u03a8I(h; k) (red line) and \u03a8f\u0302\u22121 \u03b8\u0302 \u25e6f (k) (blue line) (middle column), and \u03a6I(h; k) (red line) and \u03a6f\u0302\u22121 \u03b8\u0302 \u25e6f (k) (blue line) (right column), for four randomly selected (g,m) pairs in the testing dataset. Shown are results for k = (2, 3) and k = (1, 1). Very similar behavior are observed along other coordinates \u03d5k.\nThe plots in Figure 7 provide a quantitative description of the accuracy of the trained network. They clearly indicates that the trained f\u0302\u22121\n\u03b8\u0302 is indeed a good approximation to f\u22121.\nMoreover, a comparison of the second column and the third column gives the impression that along with the coordinates we plotted, the new objective functional \u03a6 in (9) has a much better landscape than the classical \u03a8 in (4). This is what we observed in other coordinates that are not shown here as well. Therefore, the trained neural network f\u0302\u22121 \u03b8\u0302\ncan work as a nonlinear preconditioner to improve convexify of the optimization landscape. Moreover, the plots provided a good indication that the trained network is fairly generalizable in the\nfollowing sense. The Fourier coefficients (including m(2,3) and m(1,1) shown in the plots) in the training dataset are all randomly drawn in the interval [\u22120.5, 0.5]. Here in the plots, we consider the coefficient values in the range [\u22121, 1]. The agreement of the red and blue\nlines outside of the training value range [\u22120.5, 0.5], that is, in the range [\u22121,\u22120.5)\u222a (0.5, 1], suggests that the trained neural network can be used in a region of coefficient values that is far larger than its training domain."
        },
        {
            "heading": "5.3.2 Random Fourier velocity model: case of decaying coefficients",
            "text": "While training the neural network for an approximate inverse f\u0302\u22121 \u03b8\u0302\non a large space of velocity field is extremely useful for generalization purpose, it also poses great challenges when the number of Fourier modes included in the model gets very large. Not only will we need an exponentially larger training dataset, but also the training process takes exponentially growing computational power. This is what we observed in our numerical experiments. In this section, we show some training-validation results for the velocity model (20) with decaying Fourier coefficients following the pattern we imposed in (22). We present results from two different cases: the slow decay case with \u03b1 = 1/2 and the fast decay case with \u03b1 = 1.\nIn Figure 8, we show typical validation results on five randomly selected velocity profiles in the testing dataset. The top two rows are the results for the training of velocity model with \u03b1 = 0, the third row is the case of \u03b1 = 1/2 while the bottom two rows are for the case of \u03b1 = 1. In both cases, the training is successful as can be seen from the relatively small errors in the predictions. Plots of the functionals \u03a8I and \u03a8f\u0302\u22121\n\u03b8\u0302 \u25e6f show similar patterns\nas those in Figure 7. We omit those to space. Moreover, prediction errors in the Fourier domain display very similar behavior as observed in the previous subsection: the error is higher for high-frequency components and lower for low-frequency components.\nTo study the generalization capability of the learned network, we validate the learning with on dataset generated from a different velocity model, that is consider the case where training and testing data samples are from different classes. In Figure 9, we train a neural network to recover the first 10\u00d7 10 Fourier coefficient of the velocity field and validate the trained neural network on a dataset generated from velocity models that contains 20 \u00d7 20 random Fourier modes. The decay rate in this particular case is \u03b1 = 1 but similar results are observed for \u03b1 = 1/2 as well. The validation results demonstrate that the trained network is reasonably generalizable in the setting that we considered."
        },
        {
            "heading": "5.3.3 Mesh-based velocity model",
            "text": "In the last training-validation numerical experiment, we demonstrate that the phenomena observed in the previous subsections are not particularly due to the Fourier parameterization of the velocity field in (20) that we used. Indeed, the results are more related to our method of training. Here we perform the same type of training on a neural network whose output is the velocity field represented on a 51\u00d751 uniform mesh on the domain \u2126. The output space is therefore much larger compared to the training in the case of the random Fourier velocity model. However, the training result, after projecting into the Fourier space, has almost identical properties as what we observed in the random Fourier model. In Figure 10, we show the out-of-domain validation result for the training. The velocity fields that generated\nthe training dataset have 10 \u00d7 10 Fourier modes while the velocity fields in the validation dataset have 20 \u00d7 20 Fourier modes (but represented on a 51 \u00d7 51 uniform mesh), both generated with \u03b1 = 1. The relatively small validation errors again indicate that the training is fairly successful and reasonably generalizable. The computational cost, in this case, is much larger than those in the previous subsections since the neural network has a larger size due to the increased size of the network output."
        },
        {
            "heading": "5.4 Learning-assisted FWI reconstruction",
            "text": "In this section, we present inversion results for some simulated datasets to verify the efficiency and stability of the proposed coupling method. All simulations on the inversion stage are conducted on a quadcore Intel Core i7 with 16 GB RAM."
        },
        {
            "heading": "5.4.1 Convexity of the new objective function",
            "text": "Lemma 3.2 indicates that if we have relatively accurate training, the new objective function for our coupled reconstruction scheme behaves similarly to the functional \u2016m \u2212 m0\u20162L2(\u2126), m0 being the true solution. Figure 7 provided some evidence of this in the training of the random Fourier model. In the one coefficient case, plots in Figure 7 clearly show that the new objective function is almost convex. We now present some numerical evidence in the case of the Gaussian mixture velocity model. In particular, we are interested in seeking convexity with respect to the location of a Gaussian perturbation. More precisely, the velocity field m(x) is set to be a single Gaussian model with M = 1 in (21), that is,\nm(x) = m0 + c1e \u2212 1 2 (x\u2212x10)T\u03a3 \u22121 1 (x\u2212x10), x10 = (x 1 0, z 1 0), \u03a31 = ( \u03c321 0 0 \u03c321 ) .\nwhere the background velocity m0, the amplitude c1, and the variance \u03c31 are fixed to be (m0, c1, \u03c31) = (10, 5, 0.1). We then present the objective functions \u03a8(m) and \u03a6(m) (\u03b3 = 0)\nwith respect to the location (x10, z 1 0) in Figure 11. The setting of the offline training stage for generating f\u0302\u22121 \u03b8\u0302 is the same as those in Section 5.2.\nFigure 11 presents the landscapes of objective functions \u03a8(m) and \u03a6(m) (\u03b3 = 0) with fixed (m0, c1, \u03c31). In particular, we set (x 1 0, z 1 0) = (0.5,\u22120.5) as the ground true velocity model which generates the wave signal g. From Figure 11, we observe that, (i) the classical objective function \u03a8(m) is not a convex function, and its landscape shows that the optimization can be easily trapped into a local minimum if the initial model is not carefully chosen; (ii) the new objective function \u03a6(m) (\u03b3 = 0) for the proposed coupling method becomes more convex which is consistent with Lemma 3.2. In addition, we note that when the initial model is close enough to the exact model (located at the convex region of the misfit function), the global minimum is guaranteed and one can also expect a fast convergence. In fact, a good initial model under the setting of the proposed coupling scheme can be easily obtained by adding a small perturbation to f\u0302\u22121\n\u03b8\u0302 (g) as indicated by Neumann series (14).\n5.4.2 Inversion for the velocity model (21) with M = 2\nThe first inversion example was performed to recover the following mixed Gaussian velocity model (21) with M = 2 and m0 = 10,\nm(x) = 10 + 2\u2211\nk=1\ncke \u2212 1 2 (x\u2212xk0)T\u03a3 \u22121 k (x\u2212x k 0), xk0 = (x k 0, z k 0 ), \u03a3k = ( \u03c32k 0 0 \u03c32k ) . (26)\nWe use the same offline training settings as those for the mixed Gaussian wave signal generation in Section 5.2 to construct f\u0302\u22121\n\u03b8\u0302 for the online inversion stage. However, to generate ver-\nsatile wave signals at the bottom surface to recover the features {c1, c2, \u03c31, \u03c32, x10, x20, z10 , z20}, we enforce three different top sources hi(x), i = 1, 2, 3 with\nh1(x) = e \u2212 (x\u22120.8)\n2 0.01 \u2212 e\u2212 (x\u22120.2)2 0.01 , h2(x) = e \u2212 (x\u22120.4) 2 0.01 \u2212 e\u2212 (x\u22120.7)2 0.01 ,\nand\nh3(x) = e \u2212 (x\u22120.6)\n2 0.01 \u2212 e\u2212 (x\u22120.3)2 0.01 ,\nrather than one single external top source in Section 5.2.\nFor the inversion stage, we implement a J-term truncated Neumann series approximation (19) to obtain the reconstructed velocity image. Note that J = 1 corresponds to the reconstructed velocity image from the offline training stage. We also add the Gaussian noise with zeros mean and 10% standard derivation to test the stability of the proposed coupling scheme. Figure 12 presents the reconstructed images. Precisely, the first three columns show the surface plots of the exact velocity field, the neural network prediction velocity field from the offline training stage, and the reconstructed velocity field with J = 20 from the online inversion stage, while the last column displays the difference between the exact velocity field (first column) and the reconstructed velocity field (third column). From the top row to the bottom row of Figure 12, we present the results from the noise-free wave signal, the wave signal with 10% multiplication Gaussian noise, and the wave signal with 10% additive Gaussian noise, respectively. We see that the online inversion stage improves the accuracy of the reconstructions for all cases. Table 2 lists the L2/L\u221e errors on the velocity field for the entire computational domain, as well as the CPU time for various implementations with different values of J . There, we note that for the wave signals without noise and with 10% multiplication Gaussian noise, both L2 and L\u221e reconstruction errors dropped by a factor \u223c 104 within 30 seconds; for the wave signal with 10% additive Gaussian noise, it seems that there is no improvement to add more Neumann terms in (19) once the L2 error reduces to 5.89 \u00d7 10\u22123 and L\u221e error reduces to 4.28 \u00d7 10\u22122; for this type of the situation, we can use the reconstruction from adding Neumann terms as an initial guess for a gradient-based optimization scheme to further improve the resolution of the reconstruction, see Section 4.3.\n5.4.3 Inversion for the velocity model (20) with M = 4\nFor the second inversion example, we work on reconstructing the features of the following velocity model\nm(x) = 4\u2211\nkx,kz=0\nm(k) cos(kx\u03c0x) cos(kz\u03c0z), k = (kx, kz) (27)\nwith \u03b1 = 0 in (22), namely, we don\u2019t consider any decay on the coefficients for this example. In addition, we use the same training settings as those in Section 5.2 for the Fourier wave signal generation. But for the external top sources hi(x), we choose them to be the same as the sources in Section 5.4.2 to generate resourceful training samples for the construction of f\u0302\u22121 \u03b8\u0302 .\nFor the online inversion stage, we also implement a J-term truncated Neumann series approximation (19) to recover the velocity model. To test the stability of the proposed coupled scheme, as in Section 5.4.2, we add the Gaussian noise with zeros mean and 10% standard derivation to the wave signals. Figure 13 presents the surface plots of the reconstructed velocity images with J = 20, as well as the surface plots for the difference between the reconstructed image and the ground true velocity model. The layout of Figure 13 is the same as the one in Figure 12. We observe that the training prediction is stable with respect to the noise, see the second column of Figure 13 and L2/L\u221e errors when J = 1 in Table 3. In addition, we note that the inversion stage can significantly improve the accuracy of the reconstruction. For the data without noise, the errors dropped by a factor \u223c 107 within 30 seconds; even for the data with 10% Gaussian noise, the errors also dropped by a factor \u223c 103 within 30 seconds.\n5.4.4 Inversion for the velocity model (20) with M = 7\nFor the third example, we consider a velocity model which contains 8 Fourier modes in each direction, namely,\nm(x, z) = 7\u2211\nkx,kz=0\nm(k) cos(kx\u03c0x) cos(kz\u03c0z). (28)\nThe spatial and temporal discretization, as well as the rules for data generation, the choice of the top source hi(x) are the same as the example in Section 5.4.3.\nFor the inversion stage, we again implement a J-term truncated Neumann series approximation (19) to obtain the reconstructed velocity image. Figure 14 presents the surface plots of the reconstructed velocity images with various values of J in the online inversion stage. Precisely, each row of Figure 14 corresponds to one velocity model; from the left to the right are the ground true velocity field, the reconstructed velocity image with J = 1, the reconstructed velocity image with J = 20, and the reconstructed velocity image with J = 50, respectively. We note that the online inversion stage improves the accuracy of the reconstruction for all cases which verifies the effectiveness of the proposed coupling scheme."
        },
        {
            "heading": "5.4.5 Inversion for the velocity model outside of the training domain",
            "text": "In the last example, we test the proposed coupling scheme on a velocity model which is outside of the training domain. Precisely, the design of the offline training stage is the same as the one in Section 5.4.3, namely, we focus on learning the first 5 Fourier modes along each direction during the training. However, our goal in this example is to reconstruct the following velocity model,\nm(x, z) = { 8.4, (x, z) \u2208 [0.22, 0.74]\u00d7 [\u22120.52,\u22120.5], 7.6, otherwise, , (x, z) \u2208 [0, 1]\u00d7 [\u22121, 0], (29)\nwhich is apparently outside of the training domain containing many high frequency components. To reconstruct (29), we first implement the J-term truncated Neumann series approximation (14) with J = 20 to obtain the low frequency part of the velocity model (29), then use it as the initial guess of a quasi-Newton algorithm based on the BFGS gradient update rule to minimize (4). In addition, to recover the high-frequency components of the velocity field, except the 51 receivers at the bottom surface in the training stage, we place another 51 receivers at the top surface when minimizing (4), and enforce 7 different top sources hi(x), i = 1, 2 \u00b7 \u00b7 \u00b7 , 7 with h1, h2, h3 being the same as the top sources in the training stage, and\nh4(x) = e \u2212 (x\u22120.7)\n2 0.01 \u2212 e\u2212 (x\u22120.2)2 0.01 , h5(x) = e \u2212 (x\u22120.3) 2 0.01 \u2212 e\u2212 (x\u22120.9)2 0.01 ,\nh6(x) = e \u2212 (x\u22120.2)\n2 0.01 \u2212 e\u2212 (x\u22120.5)2 0.01 , h7(x) = e \u2212 (x\u22120.1) 2 0.01 \u2212 e\u2212 (x\u22120.6)2 0.01 .\nFigure 15 presents the surface plots of the reconstructed velocity images with both noisefree data and the data with Gaussian noises. Precisely, the top row shows the reconstructed velocity from noise-free data, the middle row displays the reconstructed velocity from the data with 10% multiplication Gaussian noise, and the bottom row presents the reconstructed velocity from the data with 10% additive Gaussian noise; while from the left to the right columns are the ground true velocity field, the reconstructed velocity image with J = 1, the reconstructed velocity image with J = 20 (initial guess), and the reconstructed velocity image by minimizing (4), respectively. We note that adding several terms to the Neumann series approximation can lead to a relatively good reconstruction for the low-frequency components of the velocity field (29) for all cases (noise-free data and the data with Gaussian noise) by comparing the reconstruction results in column 2 and column 3. Then solving an extra classical minimization problem as documented in Section 3.4 helps grab the highfrequency components of the velocity field as shown in the last column."
        },
        {
            "heading": "6 Concluding remarks",
            "text": "We presented in this work an offline-online computational strategy for coupling deep learning methods with classical model-based iterative reconstruction schemes for the FWI problem. The main advantage of the coupling lies in two aspects. First, the coupling requires much less rigorous training for the learning part than a purely learning based approach. This makes the learning of the approximate inverse operator much more realistic with limited computational resources. Second, the offline learning can still significantly reduce the online reconstruction with new datasets when used as a nonlinear preconditioner. The numerical simulations we performed demonstrated the feasibility of such a coupled approach.\nThere are many important issues in the current direction that need to be more rigorously investigated. One particular aspect is to develop a mathematical characterization of the training error in the learning process and study its impact on the reconstruction step. A second aspect is to improve the learning algorithm to learn more features in the inverse operator. As we reasoned in the paper, it is extremely challenging to learn all the details in the inverse operator. However, we believe that one could do much better than the numerical\nexperiments in this paper where we pursue only a very small amount of features in the learning process. Searching for better feature models for the velocity field as well as the time traces of the wavefield is also an important task with the potential to significantly improve the performance of the learning procedure."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is partially supported by the National Science Foundation through grants DMS1913309 and DMS-1937254."
        },
        {
            "heading": "A Adjoint state gradient calculation",
            "text": "We summarize here the calculation of the Fre\u0301chet derive of the objective function \u03a6(m) defined in (9) with respect to the velocity field m.\nFollowing Proposition 3.1, the Fre\u0301chet differentiability of the map f(m) with respect to m is well-established under reasonable assumptions on the smoothness of the domain, the regularity of the incident wave source h and the regularity of the velocity field m. With the assumption we have on the differentiability of the trained network f\u0302\u22121\n\u03b8\u0302 , the Fre\u0301chet\ndifferentiability of \u03a6(m) in (9) is ensured.\nTo simplify the notation, we denote by\nr(m) := f\u0302\u22121 \u03b8\u0302 (f(m))\u2212 f\u0302\u22121 \u03b8\u0302 (g\u03b4) (30)\nthe data residual and \u0393 \u2282 \u2126 the subset of the boundary of the domain where acoustic data are measured. To be concrete, we take the regularization functional to be the H1 semi-norm of the unknown m. We also assume that the velocity is known on the boundary of the domain so that the perturbation \u03b4m|\u2202\u2126 = 0. This assumption simplified the calculations below but is by no means essential.\nTaking the derivative of \u03a6(m) with respect to m in the direction \u03b4m, we have\n\u03a6\u2032(m)[\u03b4m] = \u222b \u2126 r(m) ( f\u0302\u22121 \u03b8\u0302 )\u2032 (f(m)) [ f \u2032(m)[\u03b4m] ] dx + \u03b3 \u222b \u2126 \u2207m \u00b7 \u2207\u03b4mdx . (31)\nLet ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 : L2(\u2126) 7\u2192 L2((0, T ]\u00d7\u0393) be the adjoint of the operator ( f\u0302\u22121 \u03b8\u0302 )\u2032 (f(m)). Using the assumption that \u03b4m|\u2202\u2126 = 0, we can then write the above result as\n\u03a6\u2032(m)[\u03b4m] = \u222b T 0 \u222b \u0393 ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 [r(m)]f \u2032(m)[\u03b4m] dS(x)dt\u2212 \u03b3 \u222b \u2126 ( \u2206m ) \u03b4mdx . (32)\nThis can be written into the following form with the adjoint operator of f \u2032(m), f \u2032\u2217 : L2((0, T ]\u00d7 \u0393) 7\u2192 L2(\u2126):\n\u03a6\u2032(m)[\u03b4m] = \u222b \u2126 f \u2032\u2217 [( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 [r(m)] ] \u03b4mdx\u2212 \u03b3 \u222b \u2126 ( \u2206m ) \u03b4mdx . (33)\nThe adjoint operator f \u2032\u2217 can be found in the standard way. We document the calculation for the specific two-dimensional setup we have as follows.\nFor the wave equation (16), we can formally differentiate u with respect to m to have that u\u2032 solves\n1 m2 \u22022u\u2032 \u22022t \u2212\u2206u\u2032 = 2\u03b4m m3 \u22022u \u22022t , in (0, T ]\u00d7 \u2126\nu\u2032(0, x, z) = \u2202u\u2032\n\u2202t (0, x, z) = 0, (x, z) \u2208 (0, L)\u00d7 (\u2212H, 0) u\u2032(t, 0, z) = u\u2032(t, L, z), (t, z) \u2208 (0, T ]\u00d7 (\u2212H, 0)\n\u2202u\u2032 \u2202z (t, x,\u2212H) = 0, (t, x) \u2208 (0, T ]\u00d7 (0, L) \u2202u\u2032\n\u2202z (t, x, 0) = 0, (t, x) \u2208 (0, T ]\u00d7 (0, L)\n(34)\nLet us define the adjoint problem\n1 m2 \u22022w \u22022t \u2212\u2206w = 0, in (0, T ]\u00d7 \u2126\nw(T, x, z) = \u2202w\n\u2202t (T, x, z) = 0, (x, z) \u2208 (0, L)\u00d7 (\u2212H, 0)\nw(t, 0, z) = w(t, L, z) = 0, (t, z) \u2208 (0, T ]\u00d7 (\u2212H, 0) \u2202w \u2202x (t, 0, z) + \u2202w \u2202x (t, L, z) = 0, (t, z) \u2208 (0, T ]\u00d7 (\u2212H, 0)\n\u2202w \u2202z (t, x,\u2212H) = 0, (t, x) \u2208 (0, T ]\u00d7 (0, L) \u2202w\n\u2202z (t, x, 0) = ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 [r(m)], (t, x) \u2208 (0, T ]\u00d7 (0, L)\n(35)\nWe can then multiply the equation for u\u2032 by w and the equation for w by u\u2032 and use integration by part to show that\n\u03a6\u2032(m)[\u03b4m] = \u2212 \u222b\n\u2126\n2\nm3 (\u222b T 0 \u2202w \u2202t \u2202u \u2202t dt ) \u03b4mdx\u2212 \u03b3 \u222b \u2126 ( \u2206m ) \u03b4mdx . (36)\nWhen the data in the inversion are collected from Ns different incoming sources {hs}Nss=1, the forward map f(m) and the data g\u03b4 defined in (3). Let us (1 \u2264 s \u2264 Ns) be solution to (16) with source hs, and ws be the solution to the adjoint equation (35) with the s-th\ncomponent of ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 [r(m)], here\nr(m) = f\u0302\u22121 \u03b8\u0302 (f(m))\u2212 f\u0302\u22121 \u03b8\u0302 (g\u03b4), (37)\nthen derivative of \u03a6(m) can be computed as\n\u03a6\u2032(m)[\u03b4m] = \u2212 \u222b\n\u2126\n2\nm3 ( Ns\u2211 s=1 \u222b T 0 \u2202ws \u2202t \u2202us \u2202t dt ) \u03b4mdx\u2212 \u03b3 \u222b \u2126 ( \u2206m ) \u03b4mdx . (38)\nAlgorithm 1 Gradient Calculation with Adjoint State\n1: for s = 1 to Ns do 2: Solve (16) with hs for us 3: Evaluate the f(m;hs) component of f(m) 4: end for 5: Evaluate r(m) according to (37) with the network f\u0302\u22121\n\u03b8\u0302 6: Evaluate ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 [r(m)] with the neural network 7: for s = 1 to Ns do 8: Solve (35) with the s-th component of ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 [r(m)] as the source term for ws\n9: end for 10: Evaluate \u03a6\u2032(m) according to (38)\nThe computational procedure is summarized in Algorithm 1. The main difference between the calculation here and the adjoint calculation for a standard FWI gradient calculation is that we need to use the network f\u0302\u22121\n\u03b8\u0302 to backpropagate the data into the velocity field\nin Line 5 of Algorithm 1 to compute the residual, and then use the adjoint of the network operator (transpose of the gradient in the discrete case), ( f\u0302\u22121 \u03b8\u0302 )\u2032\u2217 , to map the residual r(m) to the source of the adjoint wave equation in Line 6.\nB Inversion with truncated Neumann series\nThe truncated Neumann series reconstruction (14) can be implemented with only the forward wave simulation and the learned neural network (without the need for the gradient of the learned operator). Let us define\nm0 := f\u0302 \u22121 \u03b8\u0302 (g\u03b4), RJ := J\u22121\u2211 j=0 Kj ( m0) ,\nwith K defined in (14). We can then verify that\nRJ+1(m0) = (I+K J\u22121\u2211 j=0 Kj) ( m0) = m0 +KRJ(m0) = m0 +RJ(m0)\u2212 f\u0302\u22121\u03b8\u0302 (f(RJ(m0))) . (39)\nThis leads to the computational procedure summarized in Algorithm 2. The main differ-\nAlgorithm 2 Reconstruction with J-Term Truncated Neumann Series\n1: Evaluate m0 := f\u0302 \u22121 \u03b8\u0302 (g\u03b4) with the learned neural network 2: Set m\u2190 m0; 3: for j = 1 to j = J \u2212 1 do 4: for s = 1 to Ns do 5: Solve (16) with (m,hs) for us 6: Evaluate the f(m;hs) component of f(m) 7: end for 8: Update m\u2190 m0 +m\u2212 f\u0302\u22121\u03b8\u0302 (f(m)) 9: end for\nence between the calculation here and the adjoint calculation for a standard FWI gradient calculation is that we only need to evaluate the network f\u0302\u22121\n\u03b8\u0302 to project the data back into\nthe velocity field to compute the residual m0\u2212 f\u0302\u22121\u03b8\u0302 (f(m)), and update the current result m."
        },
        {
            "heading": "C Network structure and training",
            "text": "For the sake of reproducibility of our research, we provide here the structures of the encoder, decoder and predictor networks we used in the encoder-decoder-predictor training framework described in Section 4.2; see Figure 16.\nThe different layers of the networks are all standard as indicated by their names. In our implementation, the input of the neural network is the a Nt \u00d7Nd \u00d7Ns tensor representing the solution of (16) for Ns sources, at Nd detector points {xd}Ndd=1, and on Nt time instances {tj}Ntj=1: u(ti,xj;hs), i = 1, \u00b7 \u00b7 \u00b7 , Nt, j = 1, \u00b7 \u00b7 \u00b7 , Nd, and s = 1, \u00b7 \u00b7 \u00b7 , Ns. The network outputs the recovered input (from the decoder and the reconstructed velocity field from the predictor. When the output velocity field is represented with the Fourier basis, the output of the predictor is an M \u00d7M matrix whose ij-element is m(ki, kj) (0 \u2264 ki, kj \u2264M).\nBesides the sizes of the network input (that is, the input of the encoder) and the network output (that is, the output of the predictor), the key parameters of the overall network are: (i) the size of the latent variables, and (ii) the number of ResNet blocks in each of the sub-networks (n1, n2 and n3). In our implementation, we tested the network structure with different numbers of ResNet blocks. The training results are not sensitive to the selection of such numbers (which controls the size of the overall network). In the numerical simulations we presented in the paper, we use n1 = 10, n2 = 5 and n3 = 10. The computational code we used for the numerical simulations in this paper, implemented using Python, are deposited at https://github.com/wending1/FWI Deep Learning.\nThe network training is achieved with the Adam optimizer [33]. The learning rate is initially set to be 5\u00d7 10\u22124, and decays by a factor of 1.2 for every 5 epoch. The batch size is chosen to be 128. We stop the training after 50 epoch."
        }
    ],
    "title": "Coupling Deep Learning with Full Waveform Inversion",
    "year": 2022
}