{
    "abstractText": "Recent progress in music generation has been remarkably advanced by the stateof-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a realtime generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Max W. Y. Lam"
        },
        {
            "affiliations": [],
            "name": "Qiao Tian"
        },
        {
            "affiliations": [],
            "name": "Tang Li"
        },
        {
            "affiliations": [],
            "name": "Zongyu Yin"
        },
        {
            "affiliations": [],
            "name": "Siyuan Feng"
        },
        {
            "affiliations": [],
            "name": "Ming Tu"
        },
        {
            "affiliations": [],
            "name": "Yuliang Ji"
        },
        {
            "affiliations": [],
            "name": "Rui Xia"
        },
        {
            "affiliations": [],
            "name": "Mingbo Ma"
        },
        {
            "affiliations": [],
            "name": "Xuchen Song"
        },
        {
            "affiliations": [],
            "name": "Jitong Chen"
        },
        {
            "affiliations": [],
            "name": "Yuping Wang"
        },
        {
            "affiliations": [],
            "name": "Yuxuan Wang"
        }
    ],
    "id": "SP:d782b6f1e998c6575e719d3f358abfef00023dea",
    "references": [
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Olivier Teboul",
                "David Grangier",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "Audiolm: a language modeling approach to audio generation",
            "venue": "arXiv preprint arXiv:2209.03143,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Heewoo Jun",
                "Christine Payne",
                "Jong Wook Kim",
                "Alec Radford",
                "Ilya Sutskever"
            ],
            "title": "Jukebox: A generative model for music",
            "venue": "arXiv preprint arXiv:2005.00341,",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Agostinelli",
                "Timo I Denk",
                "Zal\u00e1n Borsos",
                "Jesse Engel",
                "Mauro Verzetti",
                "Antoine Caillon",
                "Qingqing Huang",
                "Aren Jansen",
                "Adam Roberts",
                "Marco Tagliasacchi"
            ],
            "title": "Musiclm: Generating music from text",
            "venue": "arXiv preprint arXiv:2301.11325,",
            "year": 2023
        },
        {
            "authors": [
                "Qingqing Huang",
                "Daniel S Park",
                "Tao Wang",
                "Timo I Denk",
                "Andy Ly",
                "Nanxin Chen",
                "Zhengdong Zhang",
                "Zhishuai Zhang",
                "Jiahui Yu",
                "Christian Frank"
            ],
            "title": "Noise2music: Text-conditioned music generation with diffusion models",
            "venue": "arXiv preprint arXiv:2302.03917,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng- Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Felix Kreuk",
                "Gabriel Synnaeve",
                "Adam Polyak",
                "Uriel Singer",
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Devi Parikh",
                "Yaniv Taigman",
                "Yossi Adi"
            ],
            "title": "Audiogen: Textually guided audio generation",
            "venue": "arXiv preprint arXiv:2209.15352,",
            "year": 2022
        },
        {
            "authors": [
                "Chengyi Wang",
                "Sanyuan Chen",
                "Yu Wu",
                "Ziqiang Zhang",
                "Long Zhou",
                "Shujie Liu",
                "Zhuo Chen",
                "Yanqing Liu",
                "Huaming Wang",
                "Jinyu Li"
            ],
            "title": "Neural codec language models are zero-shot text to speech synthesizers",
            "venue": "arXiv preprint arXiv:2301.02111,",
            "year": 2023
        },
        {
            "authors": [
                "Eugene Kharitonov",
                "Damien Vincent",
                "Zal\u00e1n Borsos",
                "Rapha\u00ebl Marinier",
                "Sertan Girgin",
                "Olivier Pietquin",
                "Matt Sharifi",
                "Marco Tagliasacchi",
                "Neil Zeghidour"
            ],
            "title": "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision",
            "venue": "arXiv preprint arXiv:2302.03540,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Rongjie Huang",
                "Max WY Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu",
                "Yi Ren",
                "Zhou Zhao"
            ],
            "title": "Fastdiff: A fast conditional diffusion model for high-quality speech synthesis",
            "venue": "arXiv preprint arXiv:2204.09934,",
            "year": 2022
        },
        {
            "authors": [
                "Sungwon Kim",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "title": "Guided-tts 2: A diffusion model for high-quality adaptive text-to-speech with untranscribed data",
            "venue": "arXiv preprint arXiv:2205.15370,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Shen",
                "Zeqian Ju",
                "Xu Tan",
                "Yanqing Liu",
                "Yichong Leng",
                "Lei He",
                "Tao Qin",
                "Sheng Zhao",
                "Jiang Bian"
            ],
            "title": "Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers",
            "venue": "arXiv preprint arXiv:2304.09116,",
            "year": 2023
        },
        {
            "authors": [
                "Haohe Liu",
                "Zehua Chen",
                "Yi Yuan",
                "Xinhao Mei",
                "Xubo Liu",
                "Danilo Mandic",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Audioldm: Text-to-audio generation with latent diffusion models",
            "venue": "arXiv preprint arXiv:2301.12503,",
            "year": 2023
        },
        {
            "authors": [
                "Rongjie Huang",
                "Jiawei Huang",
                "Dongchao Yang",
                "Yi Ren",
                "Luping Liu",
                "Mingze Li",
                "Zhenhui Ye",
                "Jinglin Liu",
                "Xiang Yin",
                "Zhou Zhao"
            ],
            "title": "Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models",
            "venue": "arXiv preprint arXiv:2301.12661,",
            "year": 2023
        },
        {
            "authors": [
                "Flavio Schneider",
                "Zhijing Jin",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Mo\u00fbsai: Text-to-music generation with long-context latent diffusion",
            "venue": "arXiv preprint arXiv:2301.11757,",
            "year": 2023
        },
        {
            "authors": [
                "David Holz"
            ],
            "title": "Midjourney. Artificial Intelligence platform",
            "venue": "Accessible at https://www.midjourney.com/Accessed November",
            "year": 2023
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion gans",
            "venue": "arXiv preprint arXiv:2112.07804,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Qiuqiang Kong",
                "Yin Cao",
                "Turab Iqbal",
                "Yuxuan Wang",
                "Wenwu Wang",
                "Mark D Plumbley"
            ],
            "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Antoine Caillon",
                "Philippe Esling"
            ],
            "title": "Rave: A variational autoencoder for fast and high-quality neural audio synthesis",
            "venue": "arXiv preprint arXiv:2111.05011,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Pasini",
                "Jan Schl\u00fcter"
            ],
            "title": "Musika! fast infinite waveform music generation",
            "venue": "arXiv preprint arXiv:2208.08706,",
            "year": 2022
        },
        {
            "authors": [
                "S Forsgren",
                "H Martiros"
            ],
            "title": "Riffusion - stable diffusion for real-time music generation",
            "venue": "URL https://riffusion.com/,",
            "year": 2023
        },
        {
            "authors": [
                "Jort F Gemmeke",
                "Daniel PW Ellis",
                "Dylan Freedman",
                "Aren Jansen",
                "Wade Lawrence",
                "R Channing Moore",
                "Manoj Plakal",
                "Marvin Ritter"
            ],
            "title": "Audio set: An ontology and human-labeled dataset for audio events",
            "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2017
        },
        {
            "authors": [
                "Zal\u00e2n Borsos",
                "Matt Sharifi",
                "Damien Vincent",
                "Eugene Kharitonov",
                "Neil Zeghidour",
                "Marco Tagliasacchi"
            ],
            "title": "Soundstorm: Efficient parallel audio generation",
            "venue": "arXiv preprint arXiv:2305.09636,",
            "year": 2023
        },
        {
            "authors": [
                "Huiwen Chang",
                "Han Zhang",
                "Lu Jiang",
                "Ce Liu",
                "William T Freeman"
            ],
            "title": "Maskgit: Masked generative image transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Luo",
                "Zhuo Chen",
                "Takuya Yoshioka"
            ],
            "title": "Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Jingjing Chen",
                "Qirong Mao",
                "Dong Liu"
            ],
            "title": "Dual-path transformer network: Direct contextaware modeling for end-to-end monaural speech separation",
            "venue": "arXiv preprint arXiv:2007.13975,",
            "year": 2020
        },
        {
            "authors": [
                "Max WY Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu"
            ],
            "title": "Effective low-cost time-domain audio separation using globally attentive locally recurrent networks",
            "venue": "IEEE Spoken Language Technology Workshop (SLT),",
            "year": 2021
        },
        {
            "authors": [
                "Max WY Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu"
            ],
            "title": "Sandglasset: A light multi-granularity selfattentive network for time-domain speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Cem Subakan",
                "Mirco Ravanelli",
                "Samuele Cornell",
                "Mirko Bronzi",
                "Jianyuan Zhong"
            ],
            "title": "Attention is all you need in speech separation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Shengkui Zhao",
                "Bin Ma"
            ],
            "title": "Mossformer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Yu-An Chung",
                "Yu Zhang",
                "Wei Han",
                "Chung-Cheng Chiu",
                "James Qin",
                "Ruoming Pang",
                "Yonghui Wu"
            ],
            "title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2021
        },
        {
            "authors": [
                "Neil Zeghidour",
                "Alejandro Luebs",
                "Ahmed Omran",
                "Jan Skoglund",
                "Marco Tagliasacchi"
            ],
            "title": "Soundstream: An end-to-end neural audio codec",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Qingqing Huang",
                "Aren Jansen",
                "Joonseok Lee",
                "Ravi Ganti",
                "Judith Yue Li",
                "Daniel PW Ellis"
            ],
            "title": "Mulan: A joint embedding of music audio and natural language",
            "venue": "arXiv preprint arXiv:2208.12415,",
            "year": 2022
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu"
            ],
            "title": "Conformer: Convolution-augmented transformer for speech recognition",
            "year": 2005
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "arXiv preprint arXiv:2202.00512,",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusion-based generative models",
            "venue": "arXiv preprint arXiv:2206.00364,",
            "year": 2022
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ],
            "title": "Wavegrad: Estimating gradients for waveform generation",
            "venue": "In International conference on learning representations,",
            "year": 2020
        },
        {
            "authors": [
                "Max WY Lam",
                "Jun Wang",
                "Dan Su",
                "Dong Yu"
            ],
            "title": "Bddm: Bilateral denoising diffusion models for fast and high-quality speech synthesis",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ken Shoemake"
            ],
            "title": "Animating rotation with quaternion curves",
            "venue": "In Proceedings of the 12th annual conference on Computer graphics and interactive techniques,",
            "year": 1985
        },
        {
            "authors": [
                "Biao Zhang",
                "Rico Sennrich"
            ],
            "title": "Root mean square layer normalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Sander Dieleman",
                "Heiga Zen",
                "Karen Simonyan",
                "Oriol Vinyals",
                "Alex Graves",
                "Nal Kalchbrenner",
                "Andrew Senior",
                "Koray Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "arXiv preprint arXiv:1609.03499,",
            "year": 2016
        },
        {
            "authors": [
                "Nal Kalchbrenner",
                "Erich Elsen",
                "Karen Simonyan",
                "Seb Noury",
                "Norman Casagrande",
                "Edward Lockhart",
                "Florian Stimberg",
                "Aaron Oord",
                "Sander Dieleman",
                "Koray Kavukcuoglu"
            ],
            "title": "Efficient neural audio synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "venue": "arXiv preprint arXiv:2104.09864,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Lei",
                "Yu Zhang",
                "Sida I Wang",
                "Hui Dai",
                "Yoav Artzi"
            ],
            "title": "Simple recurrent units for highly parallelizable recurrence",
            "venue": "arXiv preprint arXiv:1709.02755,",
            "year": 2017
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm De Vries",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Robin Rombach",
                "Patrick Esser"
            ],
            "title": "Stable diffusion v2-1",
            "venue": "URL https://huggingface.co/stabilityai/stable-diffusion-2-1,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Antonia Creswell",
                "Tom White",
                "Vincent Dumoulin",
                "Kai Arulkumaran",
                "Biswa Sengupta",
                "Anil A Bharath"
            ],
            "title": "Generative adversarial networks: An overview",
            "venue": "IEEE signal processing magazine,",
            "year": 2018
        },
        {
            "authors": [
                "Dongchao Yang",
                "Jianwei Yu",
                "Helin Wang",
                "Wen Wang",
                "Chao Weng",
                "Yuexian Zou",
                "Dong Yu"
            ],
            "title": "Diffsound: Discrete diffusion model for text-to-sound generation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Jungil Kong",
                "Jaehyeon Kim",
                "Jaekyoung Bae"
            ],
            "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Won Jang",
                "Dan Lim",
                "Jaesam Yoon",
                "Bongwan Kim",
                "Juntae Kim"
            ],
            "title": "Univnet: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation",
            "venue": "arXiv preprint arXiv:2106.07889,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Shawn Hershey",
                "Sourish Chaudhuri",
                "Daniel PW Ellis",
                "Jort F Gemmeke",
                "Aren Jansen",
                "R Channing Moore",
                "Manoj Plakal",
                "Devin Platt",
                "Rif A Saurous",
                "Bryan Seybold"
            ],
            "title": "Cnn architectures for large-scale audio classification",
            "venue": "In 2017 ieee international conference on acoustics, speech and signal processing (icassp),",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Kilgour",
                "Mauricio Zuluaga",
                "Dominik Roblek",
                "Matthew Sharifi"
            ],
            "title": "Fr\u00e9chet audio distance: A reference-free metric for evaluating music enhancement algorithms",
            "venue": "In INTERSPEECH,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Music is an art composed of harmony, melody, and rhythm that permeates every aspect of human life. With the blossoming of deep generative models [1\u20133], music generation has drawn much attention in recent years [4\u20136]. As a prominent class of generative models, language models (LMs) [7, 8] showed extraordinary modeling capability in modeling complex relationships across long-term contexts [9\u201311]. In light of this, AudioLM [3] and many follow-up works [5, 12\u201314] successfully applied LMs to audio synthesis. Concurrent to the LM-based approaches, diffusion probabilistic models (DPMs) [1, 15, 16], as another competitive class of generative models [2, 17], have also demonstrated exceptional abilities in synthesizing speech [18\u201320], sounds [21, 22] and music [6, 23].\nHowever, generating music from free-form text remains challenging as the permissible music descriptions can be very diverse and relate to any of the genres, instruments, tempo, scenarios, or even some subjective feelings. Conventional text-to-music generation models are listed in Table 1, where both MusicLM [5] and Noise2Music [6] were trained on large-scale music datasets and demonstrated the state-of-the-art (SOTA) generative performances with high fidelity and adherence to various aspects of text prompts. Yet, the success of these two methods comes with large computational costs, which would be a serious impediment to their practicalities. In comparison, Mo\u00fbsai [23] building upon DPMs made efficient samplings of high-quality music possible. Nevertheless, the number of their demonstrated cases was comparatively small and showed limited in-sample dynamics. Aiming for a feasible music creation tool, a high efficiency of the generative model is essential since it facilitates interactive creation with human feedback being taken into account as in [24].\nPreprint. Under review.\nar X\niv :2\n30 5.\n15 71\n9v 1\n[ cs\n.S D\n] 2\n5 M\nay 2\n02 3\nWhile LMs and DPMs both showed promising results, we believe the relevant question is not whether one should be preferred over another but whether we can leverage both approaches with respect to their individual advantages, e.g., [25]. After analyzing the success of MusicLM, we leverage the highest-level LM in MusicLM, termed as semantic LM, to model the semantic structure of music, determining the overall arrangement of melody, rhythm, dynamics, timbre, and tempo. Conditional on this semantic LM, we exploit the non-autoregressive nature of DPMs to model the acoustics efficiently and effectively with the help of a successful sampling acceleration technique [26]. All in all, in this paper, we introduce several novelties that constitute our main contributions:\n1. We present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music of competitive quality while reducing 95.7% and 99.6% iterations of MusicLM to sample 10s and 30s music, being faster than real-time on a V100 GPU.\n2. We propose the novel dual-path diffusion (DPD) models to efficiently model coarse and fine acoustic information simultaneously with a particular semantic conditioning strategy.\n3. We design an effective sampling scheme for DPD, which improves the generation quality over the previous sampling method in [23] proposed for this class of LDMs.\n4. We reveal a successful audio VAE-GAN that effectively learns continuous latent representations, and is capable of synthesizing audios of competitive quality together with DPD."
        },
        {
            "heading": "2 Related Work",
            "text": "Audio Generation Apart from the generation models shown in Table 1, there are also music generation models [28, 29] that can generate high-quality music samples at high speed, yet they cannot accept free-form text conditions and can only be trained to specialize in single-genre music, e.g., techno music in [29]. There also are some successful music generators in the industry, e.g. Mubert [30] and Riffusion [31], yet, as analyzed in [5], they struggled to compete with MusicLM in handling free-form text prompts. In a more general scope of audio synthesis, some promising text-to-audio synthesizers [12, 21, 22] trained with AudioSet [32] also demonstrated to be able to generate music from free-form text, but the musicality is limited. AudioLM [3] unconditionally continued piano audios with promising fidelity. Parallel to this work, SoundStorm [33] exceedingly accelerated the AudioLM with a non-autoregressive decoding scheme [34], such that the acoustic LM can be decoded in 27 forward passes. In comparison, neglecting the individual cost of networks, MeLoDy takes 5 to 20 forward passes to generate acoustics of high fidelity, as discussed in Section 5.\nNetwork Architecture The architecture designed for our proposed DPD was inspired by the dual-path networks used in the context of audio separation, where Luo et al. [35] initiated the idea of segmentation-based dual-path processing, and triggered a number of follow-up works achieving the state-of-the-art results [36\u201340]. Noticing that the objective in diffusion models indeed can be viewed as a special case of source separation, this kind of dual-path architecture effectually provides us a basis for simultaneous coarse-and-fine acoustic modeling.\n1We focus on non-vocal music data by using an audio classifier [27] to filter out in-house music data with vocals. Noticeably, generating vocals and instrumental music simultaneously in one model is defective even in the SOTA works [5, 6] because of the unnaturally sound vocals. While this work aims for generating production-level music, we improve the fidelity by reducing the tendency of generating vocals."
        },
        {
            "heading": "3 Background on Audio Language Modeling",
            "text": "This section provides the preliminaries that serve as the basis for our model. In particular, we briefly describe the audio language modeling framework used in MusicLM."
        },
        {
            "heading": "3.1 Audio Language Modeling with MusicLM",
            "text": "MusicLM [5] mainly follows the audio language modeling framework presented in AudioLM [3], where audio synthesis is viewed as a language modeling task over a hierarchy of coarse-to-fine audio tokens. In AudioLM, there are two kinds of tokenization for representing different scopes of audio:\n\u2022 Semantic Tokenization: K-means over representations from SSL, e.g., w2v-BERT [41]; \u2022 Acoustic Tokenization: Neural audio codec, e.g., SoundStream [42].\nTo better handle the hierarchical structure of the acoustic tokens, AudioLM further separates the modeling of acoustic tokens into coarse and fine stages. In total, AudioLM defines three LM tasks: (1) semantic modeling, (2) coarse acoustic modeling, and (3) fine acoustic modeling.\nWe generally define the sequence of conditioning tokens as c1:Tcnd := [c1, . . . , cTcnd ] and the sequence of target tokens as u1:Ttgt := [u1, . . . ,uTtgt ]. In each modeling task, a Transformer-decoder language model parameterized by \u03b8 is tasked to solve the following autoregressive modeling problem:\np\u03b8(u1:Ttgt |c1:Tcnd) = Ttgt\u220f j=1 p\u03b8(uj |[c1, . . . , cTcnd ,u1, . . . ,uj\u22121]), (1)\nwhere the conditioning tokens are concatenated to the target tokens as prefixes. In AudioLM, semantic modeling takes no condition; coarse acoustic modeling takes the semantic tokens as conditions; fine acoustic modeling takes the coarse acoustic tokens as conditions. The three corresponding LMs can be trained in parallel with the ground-truth tokens, but need to be sampled sequentially for inference."
        },
        {
            "heading": "3.1.1 Joint Tokenization of Music and Text with MuLan and RVQ",
            "text": "To maintain the merit of audio-only training, MusicLM relies on MuLan [43], which is a two-tower, joint audio-text embedding model that can be individually trained with large-scale music data and weakly-associated, free-form text annotations. The MuLan model is pre-trained to project the music audio and its corresponding text description into the same embedding space such that the associated embeddings can be close to each other. In MusicLM, the MuLan embeddings of music and text are tokenized using a separately learned residual vector quantization (RVQ) [42].\nDifferent from AudioLM, MusicLM employs the MuLan tokens as the additional prefixing tokens, as in Eq. (1), for the semantic modeling and the coarse acoustic modeling. During training, the audio is first fed to the MuLan music tower to obtain the music embedding. Then, an RVQ is applied to the music embedding, resulting in the ground-truth MuLan tokens for conditioning the semantic LM and the coarse acoustic LM. To generate music from a text prompt, the text embedding obtained from the MuLan text tower is passed to the same RVQ and is discretized into the inference-time MuLan tokens. Based on the prefixing MuLan tokens, the semantic tokens, coarse acoustic tokens, and fine acoustic tokens are subsequently computed to generate high-fidelity music audio adhering to the text prompt."
        },
        {
            "heading": "4 Model Description",
            "text": "The overall training and sampling pipelines of MeLoDy are shown in Figure 1, where, we have three modules for representation learning: (1) MuLan, (2) Wav2Vec2-Conformer, and (3) audio VAE, and two generative models: a language model (LM) and a dual-path diffusion (DPD) model, respectively, for semantic modeling and acoustic modeling. In the same spirit as MusicLM, we leverage LM to model the semantic structure of music for its promising capability of modeling complex relationships across long-term contexts [9\u201311]. Similar to MusicLM, we pre-train a MuLan model to obtain the conditioning tokens. For semantic tokenization, we opt to use the Wav2Vec2-Conformer model, which follows the same architecture as Wav2Vec2 [44] but employs the Conformer blocks [45] in place of the Transformer blocks. The remainder of this section presents our newly proposed DPD model and the audio VAE-GAN used for DPD model, while other modules overlapped with MusicLM are described in Appendix B regarding the training and implementation details."
        },
        {
            "heading": "4.1 Dual-Dath Diffusion: Angle-Parameterized Continuous-Time Latent Diffusion Models",
            "text": "The proposed dual-path diffusion (DPD) model is a variant of diffusion probabilistic models (DPMs) [1, 15, 46] in continuous-time [16, 47\u201349]. Instead of directly operating on the raw data x \u223c pdata(x), with reference to the latent diffusion models (LDMs) [2], we consider a low-dimensional latent representation z = E\u03d5(x), where \u03d5 is a pre-trained autoencoder that enables reconstruction of the raw data from the latent: x \u2248 D\u03d5(z). Here, we use E\u03d5 to denote the encoder, and D\u03d5 to denote the decoder. By working on a low-dimensional latent space, the computational burden of DPMs can be significantly relieved [2]. We present our audio autoencoder in Section 4.2, which is tailored for DPMs and performed the stablest in our experiments.\nIn DPD, we consider a Gaussian diffusion process zt that is fully specified by two strictly positive scalar-valued, continuously differentiable functions \u03b1t, \u03c3t [16]: q(zt|z) = N (zt;\u03b1tz, \u03c32t I) for any t \u2208 [0, 1]. In the light of [48], we define \u03b1t := cos(\u03c0t/2) and \u03c3t := sin(\u03c0t/2) to benefit from some nice trigonometric properties, i.e., \u03c3t = \u221a 1\u2212 \u03b12t (a.k.a. variance-preserving [16]). By this definition, zt can be elegantly re-parameterized in terms of angles \u03b4:\nz\u03b4 = cos(\u03b4)z+ sin(\u03b4)\u03f5 for any \u03b4 \u2208 [0, \u03c0/2], \u03f5 \u223c N (0, I). (2) Note that z\u03b4 gets noisier as \u03b4 increases from 0 to \u03c0/2, which defines the forward diffusion process.\nTo generate samples, we use a \u03b8-parameterized variational model p\u03b8(z\u03b4\u2212\u03c9|z\u03b4) to invert the diffusion process by enabling running backward in angle with 0 < \u03c9 \u2264 \u03b4. Based on this model, we can sample z from z\u03c0/2 \u223c N (0, I) with T sampling steps, by discretizing \u03c0/2 into T segments as follows:\np\u03b8(z|z\u03c0/2) = \u222b z\u03b41:T\u22121 T\u220f t=1 p\u03b8(z\u03b4t\u2212\u03c9t |z\u03b4t) dz\u03b41:T\u22121 , \u03b4t = { \u03c0 2 \u2212 \u2211T i=t+1 \u03c9i, 1 \u2264 t < T ; \u03c0 2 , t = T, (3)\nwhere the angle schedule, denoted by \u03c91, . . . , \u03c9T , satisfies \u2211T\nt=1 \u03c9t = \u03c0/2. Schneider et al. [23] proposed a uniform angle schedule: \u03c9t = \u03c02T for all t. As revealed in previous scheduling methods [50, 51] for DPMs, taking larger steps at the beginning of the sampling followed by smaller steps could improve the quality of samples. Following this strategy, we design a new linear angle schedule, which empirically gives more stable and higher-quality results, and is written as\n\u03c9t = \u03c0\n6T +\n2\u03c0t\n3T (T + 1) . (4)\nWe extensively compare this linear angle schedule with the uniform one in [23] in Appendix D."
        },
        {
            "heading": "4.1.1 Multi-Chunk Velocity Prediction for Long-Context Generation",
            "text": "For model training, similar to the setting in [23] for long-context generation, the neural network is tasked to predict a multi-chunk target vtgt that comprises M chunks of velocities, each having a different noise scale. Formally speaking, given that z, z\u03b4, \u03f5 \u2208 RL\u00d7D with L representing the length of audio latents and D representing the latent dimensions, we define vtgt := v1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 vM , where\nvm := cos(\u03b4m)\u03f5[Lm\u22121 : Lm, :]\u2212 sin(\u03b4m)z[Lm\u22121 : Lm, :], Lm := \u230a mL\nM\n\u230b . (5)\nHere, we use the NumPy slicing syntax (0 as the first index) to locate the m-th chunk, and we draw \u03b4m \u223c Uniform[0, \u03c0/2] for each chunk at each training step to determine the noise scale. To learn \u03b8, we use the mean squared error (MSE) loss in [1, 48]:\nLdiff := Ez,\u03f5,\u03b41,...,\u03b4M [ \u2225vtgt \u2212 v\u0302\u03b8(znoisy; c)\u222522 ] , (6)\nznoisy := cos(\u03b4m)z[Lm\u22121 : Lm, :] + sin(\u03b4m)\u03f5[Lm\u22121 : Lm, :], (7)\nwhere c generally denotes the collection of conditions used for the velocity prediction. In MeLoDy, as illustrated in Figure 1, we propose to use the semantic tokens u1, . . . ,uTST , which are obtained from the SSL model during training and generated by the LM at inference time, to condition the DPD model. In our experiments, we find that the stability of generation can be significantly improved if we use token-based discrete conditions to control the semantics of the music and let the diffusion model learn the embedding vector for each token itself. Additionally, to assist the multi-chunk prediction, we append an angle vector to the condition that represents the angles drawn in the M chunks:\nc := {u1, . . . ,uTST , \u03b4} , \u03b4 := [\u03b41] L1 r=1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 [\u03b4M ] LM r=1 \u2208 R L (8)\nwhere [a]Br=1 denotes the operation of repeating a scalar a for B times to make a B-length vector. Suppose we have a well-trained velocity model, for sampling, we apply the trigonometric identities to the DDIM sampling algorithm [26] (see Appendix A) and obtain a simplified update rule:\nz\u03b4t\u2212\u03c9t = cos(\u03c9t)z\u03b4t \u2212 sin(\u03c9t)v\u0302\u03b8(z\u03b4t ; c), (9)\nby which, using the angle schedule in Eq. (4) and running from t = T to t = 1, we get a sample of z."
        },
        {
            "heading": "4.1.2 Dual-Path Modeling for Efficient and Effective Velocity Prediction",
            "text": "Next, we present how v\u0302\u03b8 takes in the noisy latent and the conditions and efficiently incorporates the semantic tokens into the coarse processing path for effective velocity prediction. As a highlight of this work, we modify the dual-path technique borrowed from audio separation [35, 37, 38], and propose a novel architecture for efficient, simultaneous coarse and fine acoustic modeling, as shown in Figure 2. This architecture comprises several critical modules, which we present one by one below.\nTo begin with, we describe how the conditions are processed in DPD (the middle part in Figure 2):\nEncoding Angle Vector First, we encode \u03b4 \u2208 RL, which records the frame-level noise scales of latents. Instead of using the classical positional encoding [1], we use a Slerp-alike spherical interpolation [52] to two learnable vectors estart, eend \u2208 R256 based on broadcast multiplications \u2297:\nE\u03b4 := MLP (sin(\u03b4)\u2297 estart + sin(\u03b4)\u2297 eend) \u2208 RL\u00d7Dhid , (10)\nwhere MLP(x) := RMSNorm(W2GELU(xW1 + b1) + b2) projects an arbitrary input x \u2208 RDin to RDhid using RMSNorm [53] and GELU activation [54]. Here, Dhid is hidden dimension defined for the model, and W1 \u2208 RDin\u00d7Dhid , W2 \u2208 RDhid\u00d7Dhid , b1,b2 \u2208 RDhid are the learnable parameters.\nEncoding Semantic Tokens The remaining conditions are the discrete tokens representing semantic information u1, . . . ,uTST . Following the typical approach for embedding natural languages [8], we directly use a lookup table of vectors to map any token ut \u2208 {1, . . . , VST} into a real-valued vector E(ut) \u2208 RDhid , where VST denotes the vocabulary size of the semantic tokens, i.e., the number of clusters in k-means for Wav2Vec2-Conformer. By stacking the vectors along the time axis and applying an MLP block, we obtain EST := MLP ([E(u1), . . . , E(uTST)]) \u2208 RTST\u00d7Dhid . Next, we show how the network input (i.e., znoisy at training time, or z\u03b4t at inference time) is processed given the condition embeddings. We use znoisy as input for our explanation below, since z\u03b4t is only its special case with all chunks having the same noise scale. The input znoisy is first linearly transformed and added up with the angle embedding of the same shape: H := RMSNorm (znoisyWin +E\u03b4) , where Win \u2208 RD\u00d7Dhid is learnable. We then perform segmentation for dual-path processing.\nSegmentation As shown in Figure 3a, the segmentation module divides a 2-D input into S halfoverlapping segments each of length K, represented by a 3-D tensor H := [0,H1, . . . ,HS ,0] \u2208 RS\u00d7K\u00d7Dhid , where Hs := H [ (s\u22121)K 2 : (s\u22121)K 2 +K, : ] , and H is zero-padded such that we have\nS = \u2308 2L K \u2309 +1. With a segment size K \u2248 \u221a L, the length for sequence processing becomes sub-linear\n(O( \u221a L)) as opposed to tackling the whole sequence (O(L)). This greatly reduces the difficulty of learning a very long sequence and permits MeLoDy to use higher-frequency latents.\nDual-Path Blocks After the segmentation, we obtain a 3-D tensor input for N dual-path blocks, each block exhibits an architecture shown on the rightmost of Figure 2. The input to the i-th dual-path block is denoted as H(i), and we have H(1) := H. Each block contains two stages corresponding to coarse-path (i.e., inter-segment) and fine-path (i.e., intra-segment) processing, respectively. Similar to the observations in [37, 38], we find it superior to use an attention-based network for coarse-path processing and to use a bi-directional RNN for fine-path processing. The goal of fine acoustic modeling is to better reconstruct the fine details from the roughly determined audio structure [3]. At a finer scope, only the nearby elements matter and contain the most information needed for refinement, as supported by the modeling perspectives in neural vocoding [55, 56]. Specifically, we employ the Roformer network [57] for coarse-path processing, where we use a self-attention layer followed by a cross-attention layer to be conditional on EST with rotary positional embedding. On the other hand, we use a stack of 2-layer simple recurrent units (SRUs) [58] for fine-path processing. The feature-wise linear modulation (FiLM) [59] is applied to the output of SRUs to assist the denoising with the angle embedding E\u03b4 and the pooled EST. Each of these processing stages is detailed below.\nCoarse-Path Processing In a dual-path block, we first process the coarse path corresponding to the vertical axis shown in Figure 3a, in which the columns are processed in parallel:\nH(i)c-out := RepeatSegments ([ Roformer ( MergeSegments ( H(i) ) [:, k, :] ) , k = 0, . . . ,K (i) MS \u2212 1 ]) ,\n(11)\nwhere the coarse-path output H(i)c-out \u2208 RS\u00d7K\u00d7Dhid has the same shape as H(i), and MergeSegments(\u00b7) and RepeatSegments(\u00b7) are the operations that, respectively, compress and expand the segments\nhorizontally to aggregate the information within a segment for a coarser scale of inter-segment processing. Note that, without taking the merging and repeating operations, the vertical axis is simply a sequence formed by skipping K/2 elements in H, which does not really capture the desired coarse information. The merging is done by averaging every pair of 2min{i,N\u2212i+1} columns with zero paddings and a half stride such that K(i)MS = \u2308 K 2min{i,N\u2212i+1}\u22121 \u2309 . The upper part of Figure 3b illustrates the case of i = 2. Similar to [38], our definition of K(i)MS changes the width of the 3d tensor with the block index i in a sandglass style, as we have the shortest segment at the middle block and the longest segment at the first and the last block. To match with the original length, a repeating operation following from the Roformer is performed, as shown in the lower part of Figure 3b. Fine-Path Processing We then obtain the fine-path input: H(i)f-in := RMSNorm ( H(i) +H(i)c-out ) , which is fed to a two-layer SRU by parallelly processing the rows illustrated in Figure 3a:\nH(i)f-out :=\n[ FiLM ( SRU ( H(i)f-in[s, :, :] ) ,E\u03b4 [ sL\nS , :\n] + 1\nTST TST\u22121\u2211 t=0 EST[t, :]\n) , s = 0, . . . , S \u2212 1 ] ,\n(12) where FiLM(x,m) := MLP3 ((x\u2297 MLP1(m)) + MLP2(m)) for an arbitrary input x and modulation condition m, and \u2297 is the operations of broadcast multiplication. Followed from this, we have the input for the next dual-path block: H(i+1) := RMSNorm ( H(i)f-in +H (i) f-out ) . After recursively processing through N dual-path blocks, the 3-D tensor is transformed back to a 2-D matrix using an overlap-and-add method [35]. Finally, the predicted velocity is obtained as follows:\nv\u0302\u03b8(znoisy; c) := RMSNorm ( OverlapAdd ( H(N+1) )) Wout, (13)\nwhere Wout \u2208 RDhid\u00d7D is learnable. We present more details of our implementation in Appendix B."
        },
        {
            "heading": "4.2 Audio VAE-GANs for Latent Representation Learning",
            "text": "To avoid learning arbitrarily high-variance latent representations, Rombach et al. [2] examined a KLregularized image autoencoder for latent diffusion models (LDMs) and demonstrated extraordinary stability in generating high-quality image [60], igniting a series of follow-up works [61]. Such an autoencoder imposes a KL penalty on the encoder outputs in a way similar to VAEs [62, 63], but, different from the classical VAEs, it is adversarially trained as in the generative adversarial networks (GANs) [64]. In this paper, this class of autoencoders is referred to as the VAE-GAN. Although VAE-GANs are promisingly applied to image generation, there is still a lack of comparable successful methods for the autoencoding of audio waveforms. In this work, we propose a similarly trained audio VAE-GAN, which empirically showed remarkable stability when applied to our DPD model in comparison to other commonly used VQ-VAE used in [12, 21, 65].\nSpecifically, the audio VAE-GAN is trained to reconstruct 24kHz audio with a striding factor of 96, resulting in a 250Hz latent sequence. The architecture of the decoder is the same as that in HiFi-GAN [66]. For the encoder, we basically replace the up-sampling modules in the decoder with convolutionbased down-sampling modules while other modules stay the same. For adversarial training, we use the multi-period discriminators in [66] and the multi-resolution spectrogram discriminators in [67]. The training details are further discussed in Appendix B. To match the normal range of targets for diffusion models [1, 2], we map the encoder outputs to [\u22121, 1] by z(i,j) := min { max { z\u0304(i,j)/3,\u22121 } , 1 } \u2200i, j, where the subscript (i, j) denotes the value on the i-th row and j-th column, and the choice of 3 in practice would sieve extreme values occupying < 0.1%."
        },
        {
            "heading": "4.3 Music Inpainting, Music Continuation and Music Prompting with MeLoDy",
            "text": "We show that the proposed MeLoDy supports interpolation (i.e., audio inpainting) and extrapolation (i.e., audio continuation) with tricks of manipulating random noises. Noticeably, diffusion models have been successfully used for effective audio inpainting [21, 22]. Yet, audio continuation has been an obstacle for diffusion models due to their non-autoregressive nature. Besides audio continuation, based on MuLan, MeLoDy also supports music prompts to generate music of a similar style, as shown in Figure 1. Examples of music inpainting, music continuation, and music prompting are shown on our demo page. We present the algorithms of these functionalities in Appendix C."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Data Preparation As shown in Table 1, MeLoDy was trained on 257k hours of music data (6.4M 24kHz audios), which were filtered with [27] to focus on non-vocal music. Additionally, inspired by the text augmentation in [6], we enriched the tag-based texts to generate music captions by asking ChatGPT [68]. This music description pool is used for the training of our 195.3M MuLan, where we randomly paired each audio with either the generated caption or its respective tags. In this way, we robustly improve the model\u2019s capability of handling free-form text.\nSemantic LM For semantic modeling, we trained a 429.5M LLaMA [69] with 24 layers, 8 heads, and 2048 hidden dimensions, which has a comparable number of parameters to that of the MusicLM [5]. For conditioning, we set up the MuLan RVQ using 12 1024-sized codebooks, resulting in 12 prefixing tokens. The training targets were 10s semantic tokens, which are obtained from discretizing the 25Hz embeddings from a 199.5M Wav2Vec2-Conformer with 1024-center k-means.\nDual-Path Diffusion For the DPD model, we set the hidden dimension to Dhid = 768, and block number to N = 8, resulting in 296.6M parameters. For the input chunking strategy, we divide the 10s training inputs in a fixed length of L = 2500 into M = 4 parts. For segmentation, we used a segment size of K = 64 (i.e., each segment is 256ms long), leading to S = 80 segments. In addition, we applied the classifier-free guidance [70] to DPD for improving the correspondence between samples and conditions. During training, the cross-attention to semantic tokens is randomly replaced by self-attention with a probability of 0.1. For sampling, the predicted velocity is linearly combined as . For all of our generations, a scale of 2.5 was used for classifier-free guidance.\nAudio VAE-GAN For audio VAE-GAN, we used a hop size of 96, resulting in 250Hz latent sequences for encoding 24kHz music audio. The latent dimension D = 16, thus we have a total compression rate of 6\u00d7. The hidden channels used in the encoder were 256, whereas that used in the decoder were 768. The audio VAE-GAN in total contains 100.1M parameters."
        },
        {
            "heading": "5.2 Performance Analysis",
            "text": "Objective Metrics We use the VGGish-based [71] Frec\u0301het audio distance (FAD) [72] between the generated audios and the reference audios from MusicCaps [5] as a rough measure of generation fidelity.2 To measure text correlation, we use the MuLan cycle consistency (MCC) [5], which calculates the cosine similarity between text and audio embeddings using a pre-trained MuLan.3\nInference Speed We first evaluate the sampling efficiency of our proposed MeLoDy. As DPD permits using different numbers of sampling steps depending on our needs, we report its generation speed in Table 2. Surprisingly, MeLoDy steadily achieved a higher MCC score than that of the reference set, even taking only 5 sampling steps. This means that (i) the MuLan model determined that our generated samples were more correlated to MusicCaps captions than reference audios, and (ii) the proposed DPD is capable of consistently completing the MuLan cycle at significantly lower costs than the nested LMs in [5].\n2Note that MeLoDy was mainly trained with non-vocal music data, its sample distribution could not fit the reference one as well as in [5, 6], since about 76% audios in MusicCaps contain either vocals or speech.\n3Since our MuLan model was trained with a different dataset, our MCC results cannot be compared to [5, 6].\nComparisons with SOTA models We evaluate the performance of MeLoDy by comparing it to MusicLM [5] and Noise2Music [6], which both were trained large-scale music datasets and demonstrated SOTA results for a wide range of text prompts. To conduct fair comparisons, we used the same text prompts in their demos (70 samples from MusicLM; 41 samples from Noise2Music),4 and asked seven music producers to select the best out of a pair of samples or voting for a tie (both win) in terms of musicality, audio quality, and text correlation. In total, we conducted 777 comparisons and collected 1,554 ratings. We detail the evaluation protocol in Appendix F. Table 3 shows the comparison results, where each category of ratings is separated into two columns, representing the comparison against MusicLM (MLM) or Noise2Music (N2M), respectively. Finally, MeLoDy consistently achieved comparable performances (all winning proportions fall into [0.4, 0.6]) in musicality and text correlation to MusicLM and Noise2Music. Regarding audio quality, MeLoDy outperformed MusicLM (p < 0.05) and Noise2Music (p < 0.01), where the p-values were calculated using the Wilcoxon signed-rank test. We note that, to sample 10s and 30s music, MeLoDy only takes 4.32% and 0.41% NFEs of MusicLM, and 10.4% and 29.6% NFEs of Noise2Music, respectively.\nDiversity Analysis Diffusion models are distinguished for its high diversity [25]. We conduct an additional experiment to study the diversity and validity of MeLoDy\u2019s generation given the same text prompt of open description, e.g., feelings or scenarios. The sampled results were shown on our demo page, in which we obtained samples with diverse combinations of instruments and textures.\nAblation Studies We also study the ablation on two aspects of the proposed method. In Appendix D, we compared the uniform angle schedule in [23] and the linear one proposed in DPD using the MCC metric and case-by-case qualitative analysis. It turns out that our proposed schedule tends to induce fewer acoustic issues when taking a small number of sampling steps. In Appendix E, we showed that the proposed dual-path architecture outperformed other architectures [23, 31] used for LDMs in terms of the signal-to-noise ratio (SNR) improvements using a subset of the training data."
        },
        {
            "heading": "6 Discussion",
            "text": "Limitation We acknowledge the limitations of our proposed MeLoDy. To prevent from having any disruption caused by unnaturally sound vocals, our training data was prepared to mostly contain non-vocal music only, which may limit the range of effective prompts for MeLoDy. Besides, the training corpus we used was unbalanced and slightly biased towards pop and classical music. Lastly, as we trained the LM and DPD on 10s segments, the dynamics of a long generation may be limited.\nBroader Impact We believe our work has a huge potential to grow into a music creation tool for music producers, content creators, or even normal users to seamlessly express their creative pursuits with a low entry barrier. MeLoDy also facilitates an interactive creation process, as in Midjourney [24], to take human feedback into account. For a more precise tune of MeLoDy on a musical style, the LoRA technique [73] can be potentially applied to MeLoDy, as in Stable Diffusion [60].\n4All samples for evaluation are available at https://Efficient-MeLoDy.github.io/. Note that our samples were not cherry-picked, whereas the samples we compared were cherry-picked [6], constituting very strong baselines.\n5We use + to separate the counts for the iterative modules, i.e., LM or DPM. Suppose the cost of each module is comparable, then the time steps taken by LM and the diffusion steps taken by DPM can be fairly compared."
        },
        {
            "heading": "A Mathematical Background for Dual-Path Diffusion",
            "text": "A.1 Forward Diffusion Process\nIn dual-path diffusion (DPD), we consider a Gaussian diffusion process [16] that continuously diffuses our generation target z into increasingly noisy versions of z, denoted as zt with t \u2208 [0, 1] running from t = 0 (least noisy) to t = 1 (most noisy). This forward diffusion process is formally defined as\nq(zt|z) = N (zt;\u03b1tz, \u03c32t I), (14)\nwhere two strictly positive scalar-valued, continuously differentiable functions \u03b1t, \u03c3t define the noise schedule [1] of this forward diffusion process. Building upon the nice properties of Gaussian distributions, we can express q(zt|zs), for any 0 \u2264 s < t \u2264 1, as another Gaussian distribution:\nq(zt|zs) = N ( zt;\n\u03b1t \u03b1s zs,\n( \u03c32t \u2212\n\u03b1t \u03b1s \u03c32s\n) I ) . (15)\nRegarding the choice of noise scheduling functions, we consider the typical setting used in [1, 15]: \u03b1t = \u221a 1\u2212 \u03c32t , which gives rise to a variance-preserving diffusion process [16]. Specifically, we employ the trigonometric functions in [48], defined as follows:\n\u03b1t := cos(\u03c0t/2) \u03c3t := sin(\u03c0t/2) \u2200t \u2208 [0, 1] (16) \u21d4 \u03b1\u03b4 := cos(\u03b4) \u03c3\u03b4 := sin(\u03b4) \u2200\u03b4 \u2208 [0, \u03c0/2]. (17)\nWith this re-parameterization, the diffusion process can now be defined in terms of angle \u03b4 \u2208 [0, \u03c0/2]:\nz\u03b4 = cos(\u03b4)z+ sin(\u03b4)\u03f5, \u03f5 \u223c N (0, I), (18)\nwhere z\u03b4 gets noisier as \u03b4 increases from 0 to \u03c0/2.\nA.2 Prediction of Diffusion Velocity\nThe diffusion velocity of z\u03b4 at \u03b4 [48] is defined as:\nv\u03b4 := dz\u03b4 d\u03b4 = d cos(\u03b4) d\u03b4 z+ d sin(\u03b4) d\u03b4 \u03f5 = cos(\u03b4)\u03f5\u2212 sin(\u03b4)z. (19)\nBased on v\u03b4 , we can compute z and \u03f5 from a noisy latent z\u03b4:\nz = cos (\u03b4)z\u03b4 \u2212 sin(\u03b4)v\u03b4 = \u03b1\u03b4z\u03b4 \u2212 \u03c3\u03b4v\u03b4; (20) \u03f5 = sin (\u03b4)z\u03b4 + cos(\u03b4)v\u03b4 = \u03c3\u03b4z\u03b4 + \u03b1\u03b4v\u03b4, (21)\nwhich suggests v\u03b4 a feasible target for network prediction v\u0302\u03b8(z\u03b4; c) given a collection of conditions c, as an alternative to the z prediction (z\u0302\u03b8(z\u03b4; c)), e.g., in [16], and the \u03f5 prediction (\u03f5\u0302\u03b8(z\u03b4; c)), e.g., in [1, 50, 74]. As reported by Salimans and Ho [48] and Schneider et al. [23], training the neural network \u03b8 with a mean squared error (MSE) loss as in the pioneering work [1] remains effective:\nL := Ez\u223cpdata(z),\u03f5\u223cN (0,I),\u03b4\u223cUniform[0,1] [ \u2225cos(\u03b4)\u03f5\u2212 sin(\u03b4)z\u2212 v\u0302\u03b8(cos(\u03b4)z+ sin(\u03b4)\u03f5; c)\u222522 ] , (22)\nwhich forms the basis of DPD\u2019s training loss, i.e., the simplest case of considering only a single chunk per input (M = 1) in Eq. (7). We can easily extend this to a multi-chunk version by sampling M different angles \u03b41, . . . , \u03b4M \u223c Uniform[0, 1], where the m-th sampled angle is applied to the corresponding chunk of the latent, i.e., z[(m\u2212 1)L/M : mL/M ].\nA.3 Generative Diffusion Process\nGeneration is done by inverting the forward process from a noise vector randomly drawn from N (0, I). One efficient way to accomplish this is to take advantage of DDIM [26], which enables running backward from angle \u03b4 to angle \u03b4 \u2212 \u03c9, for any step size 0 < \u03c9 < \u03b4:\np\u03b8(z\u03b4\u2212\u03c9|z\u03b4) := q ( z\u03b4\u2212\u03c9 \u2223\u2223\u2223\u2223z = z\u03b4 \u2212 \u03c3\u03b4 \u03f5\u0302\u03b8(z\u03b4; c)\u03b1\u03b4 ) = \u03b1\u03b4\u2212\u03c9 ( z\u03b4 \u2212 \u03c3\u03b4 \u03f5\u0302\u03b8(z\u03b4; c) \u03b1\u03b4 ) + \u03c3\u03b4\u2212\u03c9\u03f5, (23)\nwhere \u03f5 \u223c N (0, I). Song et al. [26] considered \u03f5 \u2261 \u03f5\u0302\u03b8(z\u03b4; c), leading to a deterministic update rule:\nz\u03b4\u2212\u03c9 = \u03b1\u03b4\u2212\u03c9 \u03b1\u03b4 z\u03b4 +\n( \u03c3\u03b4\u2212\u03c9 \u2212\n\u03b1\u03b4\u2212\u03c9\u03c3\u03b4 \u03b1\u03b4\n) \u03f5\u0302\u03b8(z\u03b4; c). (24)\nBuilding upon the diffusion velocity, Salimans and Ho [48] re-parameterized DDIM as\np\u03b8(z\u03b4\u2212\u03c9|z\u03b4) :=q (z\u03b4\u2212\u03c9 |z = \u03b1\u03b4z\u03b4 \u2212 \u03c3\u03b4v\u0302\u03b8(z\u03b4; c) ) (25) =\u03b1\u03b4\u2212\u03c9 (\u03b1\u03b4z\u03b4 \u2212 \u03c3\u03b4v\u0302\u03b8(z\u03b4; c)) + \u03c3\u03b4\u2212\u03c9\u03f5, (26)\nwhere \u03f5 \u223c N (0, I). Here, we can similarly consider a parameterized noise vector \u03f5 \u2261 \u03c3\u03b4z\u03b4 + \u03b1\u03b4v\u0302\u03b8(z\u03b4; c) based on Eq. (21), yielding a simplified deterministic update rule:\nz\u03b4\u2212\u03c9 =\u03b1\u03b4\u2212\u03c9 (\u03b1\u03b4z\u03b4 \u2212 \u03c3\u03b4v\u0302\u03b8(z\u03b4; c)) + \u03c3\u03b4\u2212\u03c9 (\u03c3\u03b4z\u03b4 + \u03b1\u03b4v\u0302\u03b8(z\u03b4; c)) (27) =(\u03b1\u03b4\u2212\u03c9\u03b1\u03b4 \u2212 \u03c3\u03b4\u2212\u03c9\u03c3\u03b4) z\u03b4 + (\u03c3\u03b4\u2212\u03c9\u03b1\u03b4 \u2212 \u03b1\u03b4\u2212\u03c9\u03c3\u03b4) v\u0302\u03b8(z\u03b4; c) (28) =cos(\u03c9)z\u03b4 \u2212 sin(\u03c9)v\u0302\u03b8(z\u03b4; c) (29)\nwhere the last equation is obtained by applying the trigonometric identities:\n\u03b1\u03b4\u2212\u03c9\u03b1\u03b4 \u2212 \u03c3\u03b4\u2212\u03c9\u03c3\u03b4 = cos(\u03b4 \u2212 \u03c9) cos(\u03b4)\u2212 sin(\u03b4 \u2212 \u03c9) sin(\u03b4) = cos(\u03c9); (30) \u03c3\u03b4\u2212\u03c9\u03b1\u03b4 \u2212 \u03b1\u03b4\u2212\u03c9\u03c3\u03b4 = sin(\u03b4 \u2212 \u03c9) cos(\u03b4)\u2212 cos(\u03b4 \u2212 \u03c9) sin(\u03b4) = sin(\u03c9). (31)\nBuilding upon this angular update rule and having specified the angle step sizes \u03c91, . . . , \u03c9T with\u2211T t=1 \u03c9t = \u03c0/2, we can generate samples from z\u03c0/2 \u223c N (0, I) after T steps of sampling:\nz\u03b4t\u2212\u03c9t = cos(\u03c9t)z\u03b4t \u2212 sin(\u03c9t)v\u0302\u03b8(z\u03b4t ; c), \u03b4t =\n{ \u03c0 2 \u2212 \u2211T i=t+1 \u03c9i, 1 \u2264 t < T ;\n\u03c0 2 , t = T,\n(32)\nrunning from t = T to t = 1."
        },
        {
            "heading": "B Training and Implementation Details",
            "text": "B.1 Audio VAE-GAN\nAs shown in Figure 4, we train a VAE-GAN to extract 250Hz 16-dimensional latent z \u2208 RL\u00d716 from a 24kHz audio x \u2208 RTwav with L = \u2308Twav/96\u2309. The audio VAE-GAN mainly comprises three trainable modules: (i) a variational Gaussian encoder E\u03d5(x) \u2261 N (\u00b5\u03d5(x), \u03c3\u03d5(x)I), (ii) a decoder D\u03d5(z), and (iii) a set of n discriminators {D(i)}ni=1. Regarding network architecture, we use the ResNet-style convolutional neural networks (CNNs) in HiFi-GAN [66] as the backbone.6 For the encoder, we replace the up-sampling blocks in HiFi-GAN with convolution-based down-sampling blocks, with down-sampling rates of [2, 3, 4, 4], output dimensions of [32, 64, 128, 256] and kernel sizes of [5, 7, 9, 9] in four down-sampling blocks, giving 40M\n6Our implementation is similar to that in https://github.com/jik876/hifi-gan.\nparameters. The final layer of the encoder maps the 256-dimensional output to two 16-dimensional latent sequences, respectively for the mean and variance of diagonal Gaussian sampling.7 As shown in Figure 4, to match the normal range of targets for diffusion models [1, 2], we map the sampled outputs to [\u22121, 1] by z(i,j) := min { max { z\u0304(i,j)/3,\u22121 } , 1 } \u2200i, j, where the subscript (i, j) denotes the value on the i-th row and j-th column, and the choice of 3 in practice would sieve extreme values occupying < 0.1%. For the architecture setting of the decoder, it inherits the same architecture of HiFi-GAN, and uses up-sampling rates of [4, 4, 3, 2], kernel sizes of [9, 9, 5, 7] and larger number of output channels ([768, 384, 192, 96]) for four up-sampling blocks, taking 60.1M parameters.\nFor adversarial training, we use the multi-period discriminators in [66] and the multi-resolution spectrogram discriminators in [67]. The training scheme is similar to that in [66]. The training loss for the encoder and the decoder comprises four components:\nLvae-gan(\u03d5) :=Ex\u223cpdata(x) [ Ez\u223cE\u03d5(x) [\u03bbmr-stftLmr-stft + \u03bbfmLfm + \u03bbganLgan + \u03bbklLkl] ] (33)\nLmr-stft := R\u2211\nr=1\n\u2225STFTr(x)\u2212 STFTr (D\u03d5 (z))\u22251 (34)\nLfm := n\u2211\ni=1\n1\n|D(i)| |D(i)|\u2211 l=1 \u2225\u2225\u2225D(i)l (x)\u2212D(i)l (D\u03d5 (z))\u2225\u2225\u2225 1\n(35)\nLgan := n\u2211\ni=1\n( D(i) (D\u03d5 (z))\u2212 1 )2 (36)\nLkl :=KL (E\u03d5(x)||N (0, I)) , (37)\nwhere STFTr computes the magnitudes after the r-th short-time Fourier transform (STFT) out of R = 7 STFTs (the number of FFTs = [8192, 4096, 2048, 512, 128, 64, 32]; the window sizes = [4096, 2048, 1024, 256, 64, 32, 16]; the hop sizes = [2048, 1024, 512, 128, 32, 16, 8]), |D(i)| denotes the number of hidden layers used for feature matching in discriminator D(i), D(i)l denotes the outputs of the l-th hidden layers in discriminator D(i), and \u03bbmr-stft, \u03bbfm, \u03bbgan, \u03bbkl are the weights, respectively, for the multi-resolution STFT loss Lmr-stft, the feature matching loss Lfm, the GAN\u2019s generator loss Lgan, and the Kullback\u2013Leibler divergence based regularization loss Lkl. To balance the scale of different losses, we set \u03bbmr-stft = 50, \u03bbfm = 20, Lgan = 1, and \u03bbkl = 5 \u00d7 10\u22123 in our training. In practice, we find it critical to lower the scale of the KL loss for a better reconstruction, though the distribution of the latents can still be close to zero mean and unit variance.\nB.2 Wav2Vec2-Conformer\nOur implementation of Wav2Vec2-Conformer was based on an open-source library.8 In particular, Wav2Vec2-Conformer follows the same architecture as Wav2Vec2 [44], but replaces the Transformer structure with the Conformer [45]. This model with 199.5M parameters was trained in self-supervised learning (SSL) manner similar to [44] using our prepared 257k hours of music data.\nB.3 MuLan\nOur reproduced MuLan [43] is composed of a music encoder and a text encoder. For music encoding, we rely on a publicly accessible Audio Spectrogram Transformer (AST) model pre-trained on AudioSet,9 which gives promising results on various audio classification benchmarks. For text encoding, we employ the BERT [8] base model pre-trained on a large corpus of English data using a masked language modeling (MLM) objective.10 These two pre-trained encoders, together having 195.3M parameters, were subsequently fine-tuned on the 257k hours of music data with a text augmentation technique similar to [6]. In particular, we enriched the tag-based texts to generate music captions by asking ChatGPT [68]. At training time, we randomly paired each audio with either\n7The Gaussian sampling is referred to LDMs\u2019 implementation at https://github.com/CompVis/latentdiffusion/blob/main/ldm/modules/distributions/distributions.py\n8https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer 9https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593\n10https://huggingface.co/bert-base-uncased\nAlgorithm 1 Music Generation 1: given D\u03d5, v\u0302\u03b8 , T , \u03c91, . . . , \u03c9T 2: input Music/text prompt P 3: 4: Initialize \u03b4T = \u03c0/2 5: Compute the MuLan tokens for P: c1:Tcnd 6: Generate u1:TST from c1:Tcnd with LM \u25b7 (1) 7: Sample z\u03b4T \u223c N (0, I) 8: for t = T to 1 do 9: Prepare condition: c = {u1:TST , [\u03b4t] L r=1} \u25b7 (8) 10: Update angle: \u03b4t\u22121 = \u03b4t \u2212 \u03c9t \u25b7 (3) 11: z\u03b4t\u22121 = cos(\u03c9t)z\u03b4t \u2212 sin(\u03c9t)v\u0302\u03b8(z\u03b4t ; c) \u25b7 (9) 12: end for 13: repeat 14: pass c1:Tcnd , u1:TST and z0 to Algorithm 2 15: until z0 reaches the desired length 16: return D\u03d5(z0) Algorithm 2 Music Continuation 1: given D\u03d5, v\u0302\u03b8 , T , M , \u03c91, . . . , \u03c9T 2: input Music z0 and c1:Tcnd , u1:TST (if provided) 3: 4: Denote MST = \u2308TST/M\u2309, LM = \u2308L/M\u2309 5: Initialize \u03b4T = \u03c0/2 6: Generate uTST:TST+MST from c1:Tcnd \u2295 uMST:TST 7: Sample znew \u223c N (0, I) \u2208 RLM 8: Save first chunk: zsave = z0[: LM ] 9: z\u03b4T = z0[LM :]\u2295 znew 10: for t = T to 1 do 11: Update \u03b4new = [0] L\u2212LM r=1 \u2295 [\u03b4t] LM r=1 12: Prepare condition: c = {uMST:TST+MST , \u03b4new} 13: Update angle: \u03b4t\u22121 = \u03b4t \u2212 \u03c9t 14: z\u03b4t\u22121 = cos(\u03c9t)z\u03b4t \u2212 sin(\u03c9t)v\u0302\u03b8(z\u03b4t ; c) 15: end for 16: return zsave \u2295 z0\nthe generated caption or its respective tags. In practice, this could robustly improve the model\u2019s capability of handling free-form text."
        },
        {
            "heading": "C Algorithms for MeLoDy",
            "text": "MeLoDy supports music or text prompting for music generation, as illustrated in Figure 1. We concretely detail the sampling procedures in Algorithm 1, where the algorithm starts by generating the latent sequence of length L and then recursively prolongs the latent sequence using Algorithm 2 until it reaches the desired length.\nWe further explain how music continuation can be effectively done in DPD. Recall that the inputs for training DPD are the concatenated chunks of noisy latents in different noise scales. To continue a given music audio, we can add a new chunk composed of random noises and drop the first chunk. This is feasible since the conditions (i.e., the semantic tokens and the angles) defined for DPD have an autoregressive nature. Based on the semantic LM, we can continue the generation of \u2308TST/M\u2309 semantic tokens for the new chunk. Besides, it is sensible to keep the chunks other than the new chunk to have zero angles: \u03b4new := [0] L\u2212\u2308L/M\u2309 r=1 \u2295 [\u03b4t] \u2308L/M\u2309 r=1 , as shown in Algorithm 2.\nIn addition, music inpainting can be done in a similar way. We replace the inpainting partition of the input audio with random noise and partially set the angle vector to zeros to mark the positions where the denoising operations are not needed. Yet, in this case, the semantic tokens can only be roughly estimated using the remaining part of the music audio."
        },
        {
            "heading": "D Ablation Study on Angle Schedules",
            "text": "We conduct an ablation study on angle schedules to validate the effectiveness of our proposed angle schedule \u03c91, . . . , \u03c9T in Eq. (4) in comparison to the previous uniform angle schedule [23] also used for angle-parameterized continuous-time diffusion models. In particular, the same pre-trained DPD model v\u0302\u03b8 and was used to sample with two different angle schedules with 10 steps and 20 steps,\nrespectively, conditional on the same semantic tokens generated for the text prompts in MusicCaps. Table 4 shows their corresponding objective measures in terms of FAD and MCC. We observe a significant improvement, especially when taking a small number of sampling steps, by using the proposed sampling method. This is aligned with our expectations that taking larger steps at the beginning of the sampling followed by smaller steps could improve the quality of samples, similar to the findings in previous diffusion scheduling methods [50, 51].\nWe further qualitatively analyze the quality of the generated samples using some simple text prompts of instruments, i.e., flute, saxophone, and acoustic guitar, by pair-wise comparing their spectrograms as illustrated in Figure 5. In the case of \u201cflute\u201d, sampling with the proposed angle schedule results in a piece of naturally sound music, being more saturated in high-frequency bands and even remedying the breathiness of flute playing, as shown in Figure 5b. On the contrary, we can observe from the spectrogram in Figure 5a that the sample generated with a uniform angle schedule is comparatively monotonous. In the case of \u201csaxophone\u201d, the uniform angle schedule leads to metallic sounds that are dissonant, as revealed by the higher energy in 3kHz to 6kHz frequency bands shown in Figure 5c. In comparison, the frequency bands are more consistent in Figure 5d, when the proposed schedule is used. While the comparatively poorer sample quality using the uniform schedule could be caused by the limited number of sampling steps, we also show the spectrograms after increasing the sampling steps from 10 to 20. In the case of \u201cacoustic guitar\u201d, when taking 20 sampling steps, the samples generated with both angle schedules sound more natural. However, in Figure 5e, we witness a horizontal line around the 4.4kHz frequency band, which is unpleasant to hear. Whereas, the sample generated by our proposed schedule escaped such an acoustic issue, as presented in Figure 5f.\n(a) 10-step sampling with uniform angle schedule for text prompt: \u201cflute\u201d (b) 10-step sampling with our proposed angle schedule for text prompt: \u201cflute\u201d\n(c) 10-step sampling with uniform angle schedule for text prompt: \u201csaxophone\u201d (d) 10-step sampling with our proposed angle schedule for text prompt: \u201csaxophone\u201d\n(e) 10-step sampling with uniform angle schedule for text prompt: \u201cacoustic guitar\u201d (f) 10-step sampling with our proposed angle schedule for text prompt: \u201cacoustic guitar\u201d\nFigure 5: Spectrograms of generated samples with uniform (left) and our proposed (right) angle schedules"
        },
        {
            "heading": "E Ablation Study on Architectures",
            "text": "To examine the superiority of our proposed dual-path model in Figure 2, we also study the ablation of network architectures. In particular, to focus on the denoising capability of different architectures, we only take a subset of the training data (approximately 5k hours of music data) to train different networks with the same optimization configurations \u2013 100k training steps using AdamW optimizer with a learning rate of 5 \u00d7 10\u22124 and a batch size of 96 on 8 NVIDIA V100 GPUs. For a fair comparison, we train the UNet-1D11 and the UNet-2D12 with comparable numbers of parameters (approximately 300M). Note that the FAD and MCC measures are not suitable for evaluating the performance of each forward pass of the trained network for denoising. In addition to the training objective, i.e., the velocity MSE, we use the scale-invariant signal-to-noise ratio (SNR) improvements (SI-SNRi) [35, 37] between the true latent z and the predicted latent z\u0302 = \u03b1\u03b4z\u03b4 \u2212 \u03c3\u03b4v\u0302\u03b8(z\u03b4; c). The results are shown in Table 5, where our proposed dual-path architecture outperforms the other two widely used UNet-style architectures in terms of both the velocity MSE and SI-SNRi."
        },
        {
            "heading": "F Qualitative Evaluation",
            "text": "To conduct a pair-wise comparison, each music producer is asked to fill in the form composed of three questions. Specifically, we present the user interface for each pair-wise comparison in Figure 6.\n11Our implementation of UNet-1D relied on https://github.com/archinetai/a-unet. 12Our implementation of UNet-2D relied on https://huggingface.co/riffusion/riffusion-model-v1."
        }
    ],
    "title": "Efficient Neural Music Generation",
    "year": 2023
}