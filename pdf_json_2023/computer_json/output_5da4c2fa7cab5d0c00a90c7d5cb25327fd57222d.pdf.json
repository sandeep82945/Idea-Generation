{
    "abstractText": "In this paper, we propose a joint generative and contrastive representation learning method (GeCo) for anomalous sound detection (ASD). GeCo exploits a Predictive AutoEncoder (PAE) equipped with self-attention as a generative model to perform frame-level prediction. The output of the PAE together with original normal samples, are used for supervised contrastive representative learning in a multi-task framework. Besides cross-entropy loss between classes, contrastive loss is used to separate PAE output and original samples within each class. GeCo aims to better capture context information among frames, thanks to the self-attention mechanism for PAE model. Furthermore, GeCo combines generative and contrastive learning from which we aim to yield more effective and informative representations, compared to existing methods. Extensive experiments have been conducted on the DCASE2020 Task2 development dataset, showing that GeCo outperforms state-of-the-art generative and discriminative methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiao-Min Zeng"
        },
        {
            "affiliations": [],
            "name": "Yan"
        },
        {
            "affiliations": [],
            "name": "Zhuo"
        },
        {
            "affiliations": [],
            "name": "Yu Zhou"
        },
        {
            "affiliations": [],
            "name": "Yu-Hong"
        },
        {
            "affiliations": [],
            "name": "Li"
        },
        {
            "affiliations": [],
            "name": "Hui Xue"
        },
        {
            "affiliations": [],
            "name": "Li-Rong"
        },
        {
            "affiliations": [],
            "name": "Dai"
        },
        {
            "affiliations": [],
            "name": "Ian McLoughlin"
        }
    ],
    "id": "SP:6df0ab502ebc71beb7fc91276dad86567cbb6b94",
    "references": [
        {
            "authors": [
                "Yuma Koizumi",
                "Yohei Kawaguchi",
                "Keisuke Imoto",
                "Toshiki Nakamura",
                "Yuki Nikaido",
                "Ryo Tanabe",
                "Harsh Purohit",
                "Kaori Suefusa",
                "Takashi Endo",
                "Masahiro Yasuda",
                "Noboru Harada"
            ],
            "title": "Description and discussion on dcase2020 challenge task2: Unsupervised anomalous sound detection for machine condition monitoring",
            "venue": "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020), Tokyo, Japan, November 2020, pp. 81\u2013 85.",
            "year": 2020
        },
        {
            "authors": [
                "Shouichi Hatanaka",
                "Hiroaki Nishi"
            ],
            "title": "Efficient GANbased unsupervised anomaly sound detection for refrigeration units",
            "venue": "2021 IEEE 30th International Symposium on Industrial Electronics (ISIE), 2021, pp. 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "Kota Dohi",
                "Takashi Endo",
                "Harsh Purohit",
                "Ryo Tanabe",
                "Yohei Kawaguchi"
            ],
            "title": "Flow-based self-supervised density estimation for anomalous sound detection",
            "venue": "IEEE ICASSP 2021, pp. 336\u2013340.",
            "year": 2021
        },
        {
            "authors": [
                "Kaori Suefusa",
                "Tomoya Nishida",
                "Harsh Purohit",
                "Ryo Tanabe",
                "Takashi Endo",
                "Yohei Kawaguchi"
            ],
            "title": "Anomalous sound detection based on interpolation deep neural network",
            "venue": "IEEE ICASSP 2020, pp. 271\u2013275.",
            "year": 2020
        },
        {
            "authors": [
                "Tomoki Hayashi",
                "Takenori Yoshimura",
                "Yusuke Adachi"
            ],
            "title": "Conformer-based ID-aware autoencoder for unsupervised anomalous sound detection",
            "venue": "Tech. Rep., DCASE, July 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Hadi Hojjati",
                "Narges Armanfard"
            ],
            "title": "Self-supervised acoustic anomaly detection via contrastive learning",
            "venue": "IEEE ICASSP 2022, pp. 3253\u20133257.",
            "year": 2022
        },
        {
            "authors": [
                "Han Chen",
                "Yan Song",
                "Li-Rong Dai",
                "Ian McLoughlin",
                "Lin Liu"
            ],
            "title": "Self-supervised representation learning for unsupervised anomalous sound detection under domain shift",
            "venue": "IEEE ICASSP 2022, pp. 471\u2013475.",
            "year": 2022
        },
        {
            "authors": [
                "Ritwik Giri",
                "Srikanth V. Tenneti",
                "Fangzhou Cheng",
                "Karim Helwani",
                "Umut Isik",
                "Arvindh Krishnaswamy"
            ],
            "title": "Self-supervised classification for detecting anomalous sounds",
            "venue": "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020), Tokyo, Japan, November 2020, pp. 46\u201350.",
            "year": 2020
        },
        {
            "authors": [
                "Youde Liu",
                "Jian Guan",
                "Qiaoxi Zhu",
                "Wenwu Wang"
            ],
            "title": "Anomalous sound detection using spectral-temporal information fusion",
            "venue": "IEEE ICASSP 2022, pp. 816\u2013820.",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Mantas Mazeika",
                "Thomas Dietterich"
            ],
            "title": "Deep anomaly detection with outlier exposure",
            "venue": "Int. Conf. on Learning Representations, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Gordon Wichern",
                "Ankush Chakrabarty",
                "Zhong-Qiu Wang",
                "Jonathan Le Roux"
            ],
            "title": "Anomalous sound detection using attentive neural processes",
            "venue": "IEEE WAS- PAA, 2021, pp. 186\u2013190.",
            "year": 2021
        },
        {
            "authors": [
                "Pramuditha Perera",
                "Vlad I Morariu",
                "Rajiv Jain",
                "Varun Manjunatha",
                "Curtis Wigington",
                "Vicente Ordonez",
                "Vishal M Patel"
            ],
            "title": "Generative-discriminative feature representations for open-set recognition",
            "venue": "Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11814\u201311823.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16000\u201316009.",
            "year": 2022
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 18661\u201318673, 2020.",
            "year": 1866
        },
        {
            "authors": [
                "Yifan Sun",
                "Changmao Cheng",
                "Yuhan Zhang",
                "Chi Zhang",
                "Liang Zheng",
                "Zhongdao Wang",
                "Yichen Wei"
            ],
            "title": "Circle loss: A unified perspective of pair similarity optimization",
            "venue": "Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6398\u20136407.",
            "year": 2020
        },
        {
            "authors": [
                "Yuma Koizumi",
                "Shoichiro Saito",
                "Hisashi Uematsu",
                "Noboru Harada",
                "Keisuke Imoto"
            ],
            "title": "ToyADMOS: A dataset of miniature-machine operating sounds for anomalous sound detection",
            "venue": "IEEE WASPAA, November 2019, pp. 308\u2013312.",
            "year": 2019
        },
        {
            "authors": [
                "Harsh Purohit",
                "Ryo Tanabe",
                "Takeshi Ichige",
                "Takashi Endo",
                "Yuki Nikaido",
                "Kaori Suefusa",
                "Yohei Kawaguchi"
            ],
            "title": "MIMII Dataset: Sound dataset for malfunctioning industrial machine investigation and inspection",
            "venue": "DCASE, November 2019, pp. 209\u2013213.",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Kevin Wilkinghoff"
            ],
            "title": "Sub-cluster adacos: Learning representations for anomalous sound detection",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN), 2021, pp. 1\u20138.",
            "year": 2021
        },
        {
            "authors": [
                "Haohang Xu",
                "Shuangrui Ding",
                "Xiaopeng Zhang",
                "Hongkai Xiong",
                "Qi Tian"
            ],
            "title": "Masked autoencoders are robust data augmentors",
            "venue": "arXiv preprint arXiv:2206.04846, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 predictive autoencoder, contrastive learning, representation learning, anomalous sound detection"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "The task of analysing sound recordings to automatically detect whether a given clip is abnormal or not is referred to as anomalous sound detection (ASD). It has wide application in areas such as automatic surveillance and machine condition monitoring, and has received increasing research attention thanks to DCASE1 challenges. The task is significantly complicated by the difficulty of collecting and labeling anomalous sounds [1] which often occur sporadically.\nExisting systems from recent DCASE challenges that have achieved promising performance, mainly focus on de-\nYan Song is the corresponding author. This work was supported by the Leading Plan of CAS (XDC08030200) and The National Kay Research and Development Program of China under Grant 2020AAA0107705.\n1DCASE: Detection and Classification of Acoustic Scenes and Events, https://dcase.community.\nscribing the distribution of normal data using machine learning methods, for sounds emitted from target machines [1]. Conventional ASD systems exploit generative models [1, 2, 3], which detect anomalies based on the reconstruction error learned from the normal sounds. In 2020, Interpolation DNN (IDNN) was proposed [4], where a model is trained to predict the removed centre frame of a spectrogram from the surrounding frames, leading to improved frame-wise representation, especially for non-stationary sounds. ID-aware autoencoders can also utilize class information (e.g., machine class or attribute) to assist the model in avoiding confusion from anomalous or normal sounds in different classes [5]. Many discriminative CNN-based methods (e.g. ResNet [6, 7], MobileNetV2 [8], and STgram [9]), have also been applied to the ASD task. Such methods mainly focus on learning representations from the perspective of outlier exposure (OE) [10] or self-supervised learning [7], where sounds from different machines are considered to be pseudo anomalies. ASD performance can be further improved by fusing anomaly scores from generative and discriminative systems [8].\nIn general, the key to improving ASD performance is to learn a compact and discriminative feature representation from normal sound data. However, in AE-based methods [1, 4], multiple frames of a spectrogram are reshaped to a one-dimension input feature, which fails to take full advantage of temporal context information. Wichern et al. [11] proposed attentive neural processes (ANP) to improve performance by introducing an attention mechanism between coordinates and magnitude. Meanwhile, discriminative learning methods [7, 8, 9] generally learn feature representations to discriminate known classes in the ideal case [12]. However, since actual anomalous sounds occur only sporadically and are naturally highly diverse, a learned feature representation is unlikely to be sufficient for detecting unseen anomalies having a distribution different from that of seen data.\nIn this paper, we propose a joint generative-contrastive method (GeCo) to learn more effective representations, as shown in Fig. 1. A Predictive AutoEncoder (PAE) consisting of transformer blocks, is proposed to employ a mask-\nar X\niv :2\n30 5.\n12 11\n1v 1\n[ ee\nss .A\nS] 2\n0 M\nay 2\ning and prediction strategy to learn a frame-wise representation. Its self-attention mechanism is exploited to model relationships across neighbouring frames as in [13, 14]. PAEreconstructed samples, together with original normal data, are used for joint generative and contrastive learning under the multi-task framework. Cross-Entropy (CE) loss is used to discriminate known classes while supervised contrastive loss helps to discriminate the reconstruction and original samples within a class. GeCo aims to build a model which learns a more informative representation with the help of PAE. The supervised contrastive loss can further enhance the compactness and discrimination of the learned representations.\nWe evaluate the effectiveness of the GeCo learned representation on the DCASE2020 Task2 development dataset, revealing a significant improvement over the state-of-the-art systems. GeCo achieves 93.97% AUC and 87.34% pAUC comfortably outperforming the 90.47% AUC and 83.61% pAUC of the DCASE2020 winning system."
        },
        {
            "heading": "2. METHOD",
            "text": "In this section, we will introduce the proposed GeCo in detail, including the generative model PAE, and joint generative and contrastive learning procedure employing a multi-task framework, as shown in Fig. 1."
        },
        {
            "heading": "2.1. Predictive AutoEncoder (PAE)",
            "text": "The Predictive AutoEncoder (PAE) uses an encoder-decoder architecture similar to [4], to predict unseen frames based on the self-attention mechanism [13, 14]. Given an acoustic feature x with n frames, input feature xm is obtained by randomly masking part of the frames of x. An encoder E is trained to learn latent representations from the input sequence, while a decoder D is trained to predict the masked features from latent representations. Both encoder E and decoder D comprise several transformer blocks. Due to the introduction of a self-attention mechanism, context information\nis obtained across frames, which is beneficial for predicting the masked frames. Furthermore, an additional bottleneck structure L is set to perform dimension reduction, which consists of a Multiple Layer Perceptron (MLP) between E and D.\nFollowing conventional AE-based approaches [1, 4], Mean Squared Error (MSE) is adopted as an objective,\nJ = ||M x\u2212M D (L (E (xm)))||22 (1)\nwhere M is a binary mask matrix representing the position of randomly masked frames and is an element-wise multiplication. The anomaly score is calculated using the MSE between predicted and target frames.\nPAE is pre-trained using the normal sounds from each machine type in an end-to-end manner. In GeCo, PAE is further applied for the classification based method, where reconstructed output is used with the original normal data together, under the multi-task learning framework."
        },
        {
            "heading": "2.2. Joint Generative-Contrastive Learning (GeCo)",
            "text": "The GeCo follows the unified self-supervised representation learning method of [7], where sounds from different machine IDs are used as pseudo anomalies. Let x\u0303 denote the reconstruction from original normal sample x, and f\u03b8 be the feature extractor parameterized with \u03b8. Given a batch of N samples, B = {xi,yi}, i = 1, \u00b7 \u00b7 \u00b7 , N , the corresponding reconstructions B\u0303 = {x\u0303i,yi}, i = 1, \u00b7 \u00b7 \u00b7 , N , where yi is machine ID. The minibatch for training GeCo is B \u222a B\u0303. In operation, only the masked frames in x are reconstructed by their neighbourhood. Sample x\u0303 can also be considered as a type of data augmentation from x, and shares the same machine ID as x. Learning from both x\u0303 and x aims to improve the generalization and robustness of the representation.\nFrom the perspective of contrastive learning [6], discriminating x\u0303 and x may enforce the learned representation to be compact. We further train GeCo under the multi-task learning framework, as shown in Fig. 1. Given the original x and\nreconstruction x\u0303, the Cross-Entropy (CE) loss LCE is utilized, which results in machine ID related clusters. The supervised contrastive learning [15] then separates the reconstruction x\u0303 and original x points within each class. Specifically, for a semantic cluster related to the c-th class in each mini-batch, given embedding zi = f\u03b8(xi|yi = c), the positive samples are Cp = {zp, p 6= i|yp = c}, and the negative samples are Cn = {z\u0303i = f\u03b8(x\u0303i|yi = c)}. The supervised contrastive loss is defined as follows,\nLCon = \u2212 1 |Cp| \u2211\nzp\u2208Cp\nlog exp(zi \u00b7 zp)\u2211\nza\u2208Cp\u222aCn exp(zi \u00b7 za) (2)\nwhere |Cp| is the cardinality of positive samples. By replacing zp and z\u0303i with the corresponding weight vectors wp and wn, we can express the contrastive loss in Eqn.(2) as a Binary Cross-Entropy (BCE) loss, as discussed in [16]. The total loss in GeCo for multi-task learning is the weighted sum,\nLtotal = LCE + \u03bbLCon (3)\nwhere \u03bb is the balance factor in total loss. When \u03bb is set to 0, using PAE to generate samples can be considered a kind of data augmentation.\nTo further balance the CE and BCE loss, a ramp-up strategy is proposed for adjusting \u03bb. During the early stages of training, \u03bb is set to 0, to enable the model to focus on learning robust and generalizable embeddings relevant to the machine IDs. Due to the CE loss, the embedding space will begin to contain ID-related clusters. \u03bb is then linearly increased from 0 to \u03bbmax, as the training proceeds. This enables the model to gradually pay attention to classifying the reconstruction and original data, without influencing different machine ID classes. With the help of this ramp-up strategy, the model can not only maintain its discriminative capabilities according to machine IDs, but also learn a compact representation that is beneficial for the ASD task."
        },
        {
            "heading": "2.3. Anomaly Score Fusion",
            "text": "As aforementioned, GeCo integrates generative and contrastive representation learning. The former exploits framelevel prediction error as an anomaly score, whereas the latter utilizes clip-level embeddings to calculate a similarity with the ID-related cluster centres. A weighted fusion is then computed of frame-level and clip-level scores:\nAnomalyScore (x) = MSE + \u03b3 (1\u2212 Cos-Simi) (4)\nwhere \u03b3 is the weight hyper-parameter. MSE represents the frame-level anomaly score, as described in section 2.1 in detail. Cos-Simi is the cosine-similarity between the embedding of a test clip and the centre of the machine ID clusters, which is applied to calculate the clip-level anomaly score. This simple score fusion can effectively make use of the complementarity of the frame-level and clip-level scores to improve overall ASD performance."
        },
        {
            "heading": "3. EXPERIMENTS AND RESULTS",
            "text": ""
        },
        {
            "heading": "3.1. Dataset",
            "text": "We experimentally evaluate GeCo on the DCASE2020 Challenge Task2 development dataset, which is composed of a subset of ToyADMOS [17] and MIMII [18] datasets. The development dataset has six machine types (ToyCar, ToyConveyor, Fan, Pump, Slider, and Valve). Each machine type consists of clips from three or four machines. The real normal sounds from different machines are labeled with machine IDs, leading to a training set having 23 classes in total."
        },
        {
            "heading": "3.2. Implementation Details",
            "text": "We extract the log-Mel spectrogram as the input of our system with 128 Mel filters, a window size of 1024 and hop size of 512, for all 16kHz sample rate input clips.\nPAE is first used to model the input of 5 frames for every machine type. To do this, a random frame is selected as the target frame and is replaced with a learnable mask token. Both encoder E and decoder D consist of two transformer blocks having dimensions of 512 and 256, respectively, with the dimension of the sandwiched bottleneck structure L being 64. PAE is trained using the ADAM optimizer [19] over 60 epochs with a batchsize set to 512. An initial learning rate of 1e-3 is used for training during the first 30 epochs, declining to 1e-4 for the remaining 30 epochs.\nWe utilize all of the training data in the development dataset to train GeCo. 65 frames are randomly cropped as the input, and then one random frame out of every 5 frames within the input features is substituted by the output of PAE. The ResNet18 [7] operates as a feature extractor, with a minibatch size of 32, which would expand to 64 after generation of the PAE output. A SGD algorithm is employed with momentum of 0.9 and weight decay of 1e-4 to optimize this model over 120 epochs, with an initial learning rate of 0.1. This is multiplied by 0.1 at the 50th and 90th epochs. As for ramp-up strategy, the value of \u03bb is set to 0 in the first 30 epochs and gradually increases to 10 from the 30th epoch to the 90th epoch. \u03bb stays at \u03bbmax = 10 for the last 30 epochs.\nPerformance is evaluated by using the area under the receiver operating characteristic curve (AUC) and the partialAUC (pAUC), which is calculated as the AUC over a low false-positive-rate (FPR) range which is set to [0, 0.1] for DCASE2020 Challenge Task2."
        },
        {
            "heading": "3.3. Results",
            "text": "We compare the proposed methods with previous generative and discriminative architectures and present the performance metrics obtained for each machine type in Table 1. In discriminative methods, the baseline system is a vanilla ResNet18 [7] that uses all training data to recognize 23 machine IDs. Besides, we compare our anomaly score fusion results with the\nhighest ranked ensemble systems in the DCASE2020 Challenge Task2.\nFor generative methods, the performance of PAE is superior to other generative models for most machine types, especially the Valve, proving the validity of exploiting context information for ASD. For discriminative methods, it is evident that GeCo achieves significant improvement over the baseline and other methods. We believe the primary reason is that the representations learned by our proposed method can more effectively describe a compact distribution of normal samples. Meanwhile, due to the constraint of LCE and LCon, slight anomalous discrepancies can be reflected in the feature representations, which enables hard anomalies to be detected more efficiently, resulting in a higher pAUC. Employing anomaly score fusion (\u2018Ensembles\u2019), we fix \u03b3 in Eqn (4) to 200 and present the results in Table 1. In addition, we achieve the best performance by grid searching the optimal value of \u03b3 for each machine type. The optimal values are presented as machine type(\u03b3) as follows: ToyCar(125), ToyConveyor(135), Fan(495), Pump(225), Slider(110), Valve(125). It is not hard to observe that our method significantly outperforms the state-of-the-art ensemble method [8]. We believe this is due to the extraordinary fact that our method introduces a generative model into a discriminative framework, which allows frame-level and clip-level anomaly scores to be calcu-\nlated together. Moreover, we also validate the effectiveness of our method on the evaluation set. We achieve 96.67% AUC and 90.61% pAUC, which outperforms the 96.42% AUC and 89.24% pAUC achieved by the state-of-the-art system[20].\nIn Table 2, we study the influences of the balance factor \u03bb on performance. When \u03bb = 0, the reconstruction samples effectively act as augmented samples for training the model, which achieves noticeable improvement over the baseline. This result indicates reconstruction samples are robust data augmentation, which is consistent with [21]. However, if the value of \u03bb is unreasonable (e.g. \u03bb = 10), the result performs worse than the baseline. That is because the model focuses more on detecting generative frames, resulting in loose machine ID related clusters and imprecise representations of normal samples. The ramp-up strategy yields the best performance, suggesting its effectiveness in solving the sensitivity problem towards \u03bb."
        },
        {
            "heading": "4. CONCLUSION",
            "text": "In this paper, we propose a joint GeCo method for ASD tasks. The method includes a PAE with self-attention mechanism which is exploited to perform frame-level prediction. With the help of a self-attention mechanism, context information from neighbouring frames to the masked ones is effectively captured to improve ASD performance, based on generative learning methods. The output of the PAE plays several crucial roles under a multi-task framework. As augmented samples, the reconstructions improve the generalization capability and robustness of the representation obtained by self-supervised learning. Meanwhile, as negative samples from supervised contrastive learning, they act to make the embedding space of normal data more compact. Extensive experimental results demonstrate the superiority of our proposed methods, and we achieve state-of-the-art performance on the evaluation set."
        },
        {
            "heading": "5. REFERENCES",
            "text": "[1] Yuma Koizumi, Yohei Kawaguchi, Keisuke Imoto, Toshiki Nakamura, Yuki Nikaido, Ryo Tanabe, Harsh Purohit, Kaori Suefusa, Takashi Endo, Masahiro Yasuda, and Noboru Harada, \u201cDescription and discussion on dcase2020 challenge task2: Unsupervised anomalous sound detection for machine condition monitoring,\u201d in Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020), Tokyo, Japan, November 2020, pp. 81\u2013 85.\n[2] Shouichi Hatanaka and Hiroaki Nishi, \u201cEfficient GANbased unsupervised anomaly sound detection for refrigeration units,\u201d in 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE), 2021, pp. 1\u20137.\n[3] Kota Dohi, Takashi Endo, Harsh Purohit, Ryo Tanabe, and Yohei Kawaguchi, \u201cFlow-based self-supervised density estimation for anomalous sound detection,\u201d in IEEE ICASSP 2021, pp. 336\u2013340.\n[4] Kaori Suefusa, Tomoya Nishida, Harsh Purohit, Ryo Tanabe, Takashi Endo, and Yohei Kawaguchi, \u201cAnomalous sound detection based on interpolation deep neural network,\u201d in IEEE ICASSP 2020, pp. 271\u2013275.\n[5] Tomoki Hayashi, Takenori Yoshimura, and Yusuke Adachi, \u201cConformer-based ID-aware autoencoder for unsupervised anomalous sound detection,\u201d Tech. Rep., DCASE, July 2020.\n[6] Hadi Hojjati and Narges Armanfard, \u201cSelf-supervised acoustic anomaly detection via contrastive learning,\u201d in IEEE ICASSP 2022, pp. 3253\u20133257.\n[7] Han Chen, Yan Song, Li-Rong Dai, Ian McLoughlin, and Lin Liu, \u201cSelf-supervised representation learning for unsupervised anomalous sound detection under domain shift,\u201d in IEEE ICASSP 2022, pp. 471\u2013475.\n[8] Ritwik Giri, Srikanth V. Tenneti, Fangzhou Cheng, Karim Helwani, Umut Isik, and Arvindh Krishnaswamy, \u201cSelf-supervised classification for detecting anomalous sounds,\u201d in Proceedings of the Detection and Classification of Acoustic Scenes and Events 2020 Workshop (DCASE2020), Tokyo, Japan, November 2020, pp. 46\u201350.\n[9] Youde Liu, Jian Guan, Qiaoxi Zhu, and Wenwu Wang, \u201cAnomalous sound detection using spectral-temporal information fusion,\u201d in IEEE ICASSP 2022, pp. 816\u2013820.\n[10] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich, \u201cDeep anomaly detection with outlier exposure,\u201d in Int. Conf. on Learning Representations, 2019.\n[11] Gordon Wichern, Ankush Chakrabarty, Zhong-Qiu Wang, and Jonathan Le Roux, \u201cAnomalous sound detection using attentive neural processes,\u201d in IEEE WASPAA, 2021, pp. 186\u2013190.\n[12] Pramuditha Perera, Vlad I Morariu, Rajiv Jain, Varun Manjunatha, Curtis Wigington, Vicente Ordonez, and Vishal M Patel, \u201cGenerative-discriminative feature representations for open-set recognition,\u201d in Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11814\u201311823.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, \u201cAttention is all you need,\u201d Advances in Neural Information Processing Systems, vol. 30, 2017.\n[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dolla\u0301r, and Ross Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16000\u201316009.\n[15] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan, \u201cSupervised contrastive learning,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 18661\u201318673, 2020.\n[16] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen Wei, \u201cCircle loss: A unified perspective of pair similarity optimization,\u201d in Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6398\u20136407.\n[17] Yuma Koizumi, Shoichiro Saito, Hisashi Uematsu, Noboru Harada, and Keisuke Imoto, \u201cToyADMOS: A dataset of miniature-machine operating sounds for anomalous sound detection,\u201d in IEEE WASPAA, November 2019, pp. 308\u2013312.\n[18] Harsh Purohit, Ryo Tanabe, Takeshi Ichige, Takashi Endo, Yuki Nikaido, Kaori Suefusa, and Yohei Kawaguchi, \u201cMIMII Dataset: Sound dataset for malfunctioning industrial machine investigation and inspection,\u201d in DCASE, November 2019, pp. 209\u2013213.\n[19] Diederik P Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[20] Kevin Wilkinghoff, \u201cSub-cluster adacos: Learning representations for anomalous sound detection,\u201d in 2021 International Joint Conference on Neural Networks (IJCNN), 2021, pp. 1\u20138.\n[21] Haohang Xu, Shuangrui Ding, Xiaopeng Zhang, Hongkai Xiong, and Qi Tian, \u201cMasked autoencoders are robust data augmentors,\u201d arXiv preprint arXiv:2206.04846, 2022."
        }
    ],
    "title": "JOINT GENERATIVE-CONTRASTIVE REPRESENTATION LEARNING FOR ANOMALOUS SOUND DETECTION",
    "year": 2023
}