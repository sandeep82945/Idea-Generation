{
    "abstractText": "Prompt tuning provides an efficient way for users to customize Large Language Models (LLMs) with their private data in the emerging LLM service scenario. However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization. Based on prompt tuning, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy guarantees for LLM services. RAPT adopts a local privacy setting, allowing users to privatize their data locally with local differential privacy. As prompt tuning performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yansong Li"
        },
        {
            "affiliations": [],
            "name": "Zhixing Tan"
        },
        {
            "affiliations": [],
            "name": "Yang Liu"
        }
    ],
    "id": "SP:634ca5476346017c3e670ce9818ae946ee2c9492",
    "references": [
        {
            "authors": [
                "Faiyaz Al Zamal",
                "Wendy Liu",
                "Derek Ruths."
            ],
            "title": "Homophily and latent attribute inference: Inferring latent attributes of twitter users from neighbors",
            "venue": "Proc. of AAAI, volume 6, pages 387\u2013390.",
            "year": 2012
        },
        {
            "authors": [
                "Rohan Anil",
                "Badih Ghazi",
                "Vineet Gupta",
                "Ravi Kumar",
                "Pasin Manurangsi."
            ],
            "title": "Large-scale differentially private bert",
            "venue": "arXiv preprint arXiv:2108.01624.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom Brown",
                "Dawn Song",
                "Ulfar Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "venue": "In USENIX Security,",
            "year": 2021
        },
        {
            "authors": [
                "Konstantinos Chatzikokolakis",
                "Miguel E Andr\u00e9s",
                "Nicol\u00e1s Emilio Bordenabe",
                "Catuscia Palamidessi."
            ],
            "title": "Broadening the scope of differential privacy using metrics",
            "venue": "Proc. of PETS, pages 82\u2013102. Springer.",
            "year": 2013
        },
        {
            "authors": [
                "Zihan Chen",
                "Hongbo Zhang",
                "Xiaoji Zhang",
                "Leqi Zhao."
            ],
            "title": "Quora question pairs",
            "venue": "University of Waterloo, pages 1\u20137.",
            "year": 2018
        },
        {
            "authors": [
                "Maximin Coavoux",
                "Shashi Narayan",
                "Shay B Cohen."
            ],
            "title": "Privacy-preserving neural representations of text",
            "venue": "arXiv preprint arXiv:1808.09408.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre Evfimievski",
                "Johannes Gehrke",
                "Ramakrishnan Srikant."
            ],
            "title": "Limiting privacy breaches in privacy preserving data mining",
            "venue": "Proc of SIGMOD-SIGACT-SIGART, pages 211\u2013222.",
            "year": 2003
        },
        {
            "authors": [
                "Oluwaseyi Feyisetan",
                "Borja Balle",
                "Thomas Drake",
                "Tom Diethe."
            ],
            "title": "Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations",
            "venue": "Proc. of WSDM, pages 178\u2013186.",
            "year": 2020
        },
        {
            "authors": [
                "Shlomo Hoory",
                "Amir Feder",
                "Avichai Tendler",
                "Sofia Erell",
                "Alon Peled-Cohen",
                "Itay Laish",
                "Hootan Nakhost",
                "Uri Stemmer",
                "Ayelet Benjamini",
                "Avinatan Hassidim"
            ],
            "title": "Learning and evaluating a differentially private pre-trained language model",
            "year": 2021
        },
        {
            "authors": [
                "Dirk Hovy",
                "Anders Johannsen",
                "Anders S\u00f8gaard."
            ],
            "title": "User review sites as a resource for largescale sociolinguistic studies",
            "venue": "Proc. of TheWebConf, pages 452\u2013461.",
            "year": 2015
        },
        {
            "authors": [
                "Gavin Kerrigan",
                "Dylan Slack",
                "Jens Tuyls."
            ],
            "title": "Differentially private language models benefit from public pre-training",
            "venue": "arXiv preprint arXiv:2009.05886.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proc. of ACL, pages 4582\u20134597.",
            "year": 2021
        },
        {
            "authors": [
                "Xuechen Li",
                "Florian Tramer",
                "Percy Liang",
                "Tatsunori Hashimoto."
            ],
            "title": "Large language models can be strong differentially private learners",
            "venue": "arXiv preprint arXiv:2110.05679.",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602.",
            "year": 2021
        },
        {
            "authors": [
                "Lingjuan Lyu",
                "Xuanli He",
                "Yitong Li."
            ],
            "title": "Differentially private representation for NLP: Formal guarantee and an empirical study on privacy and fairness",
            "venue": "Proc. of EMNLP, pages 2355\u20132365.",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Mamou",
                "Hang Le",
                "Miguel Del Rio",
                "Cory Stephenson",
                "Hanlin Tang",
                "Yoon Kim",
                "SueYeon Chung."
            ],
            "title": "Emergence of separable manifolds in deep language representations",
            "venue": "37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Plant",
                "Dimitra Gkatzia",
                "Valerio Giuffrida."
            ],
            "title": "CAPE: Context-aware private embeddings for private language learning",
            "venue": "Proc. of EMNLP, pages 7970\u20137978.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Qu",
                "Weize Kong",
                "Liu Yang",
                "Mingyang Zhang",
                "Michael Bendersky",
                "Marc Najork"
            ],
            "title": "Natural Language Understanding with PrivacyPreserving BERT",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Weiyan Shi",
                "Si Chen",
                "Chiyuan Zhang",
                "Ruoxi Jia",
                "Zhou Yu."
            ],
            "title": "Just fine-tune twice: Selective differential privacy for large language models",
            "venue": "arXiv preprint arXiv:2204.07667.",
            "year": 2022
        },
        {
            "authors": [
                "Weiyan Shi",
                "Aiqi Cui",
                "Evan Li",
                "Ruoxi Jia",
                "Zhou Yu."
            ],
            "title": "Selective differential privacy for language modeling",
            "venue": "arXiv preprint arXiv:2108.12944.",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proc. of EMNLP, pages 1631\u20131642.",
            "year": 2013
        },
        {
            "authors": [
                "Congzheng Song",
                "Ananth Raghunathan."
            ],
            "title": "Information leakage in embedding models",
            "venue": "Proc. of ACM SIGSAC, pages 377\u2013390.",
            "year": 2020
        },
        {
            "authors": [
                "Elena Voita",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives",
            "venue": "Proc. of ACL, pages 4396\u20134406.",
            "year": 2019
        },
        {
            "authors": [
                "Xi Wu",
                "Fengan Li",
                "Arun Kumar",
                "Kamalika Chaudhuri",
                "Somesh Jha",
                "Jeffrey Naughton."
            ],
            "title": "Bolt-on differential privacy for scalable stochastic gradient descent-based analytics",
            "venue": "Proc. of the SIGMOD, pages 1307\u20131322.",
            "year": 2017
        },
        {
            "authors": [
                "Zonghan Yang",
                "Yang Liu."
            ],
            "title": "On robust prefix-tuning for text classification",
            "venue": "arXiv preprint arXiv:2203.10378.",
            "year": 2022
        },
        {
            "authors": [
                "Da Yu",
                "Saurabh Naik",
                "Arturs Backurs",
                "Sivakanth Gopi",
                "Huseyin A Inan",
                "Gautam Kamath",
                "Janardhan Kulkarni",
                "Yin Tat Lee",
                "Andre Manoel",
                "Lukas Wutschitz"
            ],
            "title": "Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500",
            "year": 2021
        },
        {
            "authors": [
                "Da Yu",
                "Huishuai Zhang",
                "Wei Chen",
                "Jian Yin",
                "TieYan Liu."
            ],
            "title": "Large scale private learning via lowrank reparametrization",
            "venue": "Proc. of ICML, pages 12208\u201312218. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent years have witnessed the tremendous success of Large Language Models (LLMs) (Devlin et al., 2018; Brown et al., 2020). With the growing size of LLMs, prompt tuning methods (Li and Liang, 2021; Lester et al., 2021) have emerged as a simple and parameter-efficient solution for steering LLMs to downstream tasks, which not only achieve competitive performance but also enable mixed-task inference (Li and Liang, 2021). The large size of LLMs also makes them expensive to deploy and run for individual users. As a result, recent LLMs are usually released as cloud services and often along with a prompt tuning API for users to customize the LLM, which is exemplified by the recent NVIDIA NeMo LLM service.1\n\u2217 Corresponding authors: Z. Tan (zxtan@zgclab.edu. cn) and Y. Liu (liuyang2011@tsinghua.edu.cn).\n1https://www.nvidia.com/en-us/gpu-cloud/ nemo-llm-service\nAlthough prompt tuning APIs provide an efficient way for users to customize and utilize LLM services, the sensitive nature of private data brings concerns about private data leakage when using LLM services. To customize and utilize the LLM service with prompt tuning, the users need to upload their data to the service provider. However, it is well known that the input text or even the embedding representations may leak private information to various adversaries (Coavoux et al., 2018). Therefore, it is important to provide privacy preservation while using and customizing LLMs to make LLM services trustworthy.\nHowever, privacy preservation in the context of LLM services is challenging. First, it is not trivial to provide privacy guarantees to private data in the context of LLM service. Existing works on LLM privacy preservation (Yu et al., 2021a; Shi et al., 2021; Anil et al., 2021; Hoory et al., 2021; Li et al., 2021; Shi et al., 2022) focus on centralized privacy settings, which rely on the service provider to protect users from privacy leakage. Such settings may not be satisfactory in the case of an honest-but-curious service provider or middle eavesdropper (Lyu et al., 2020). Second, imposing privacy protection inevitably degrades the performance of downstream tasks, known as the privacy-utility trade-off. Yang and Liu (2022) find that prompt tuning lacks robustness, therefore it is likely that customizing LLMs with prompt tuning will suffer from unaffordable performance degradation when providing privacy protection.\nTo address the above challenges, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework for customizing and utilizing LLM service with privacy preservation. For privacy protection, RAPT applies a local privacy setting (Lyu et al., 2020; Qu et al., 2021), where users apply a privacy mechanism on data locally before publishing data. Specifically, RAPT uses text-to-text privatization (Feyisetan et al., 2020), which is based on\nar X\niv :2\n30 5.\n06 21\n2v 1\n[ cs\n.C L\n] 1\n0 M\nay 2\n02 3\nlocal differential privacy (LDP) (Evfimievski et al., 2003). To mitigate the performance degradation when imposing privacy protection, we propose a novel privatized token reconstruction task based on the recent findings that the masked language modeling objective can learn separable deep representations (Voita et al., 2019; Mamou et al., 2020). The objective of privatized token reconstruction is to recover the original content of a privatized special token sequence from LLM representations, which is inspired by the success of masked language modeling (Devlin et al., 2018). Different from standard prompt tuning, RAPT trains prompt jointly on the downstream and privatized token reconstruction tasks. In this way, RAPT combines the benefit of being lightweight and modular of prompt tuning while providing improved utility (i.e., performance) by privatized token reconstruction. Figure 1 gives an overview of our RAPT framework.\nWe carry out simulated privacy attacks and conduct experiments on three natural language understanding tasks with different LLMs to study the effectiveness of our proposed framework. Experiments show that our RAPT achieves competitive performance while providing privacy protection across different tasks and LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": "Recent studies on LLM privacy protection can be roughly divided into centralized and local approaches.\nCentralized Approaches. Most existing works focus on centralized privacy setting, which relies on a central data curator to protect data from privacy leakage. There are many works to study how to train a privacy-preserving LLM (Carlini et al., 2021; Hoory et al., 2021; Li et al., 2021; Yu et al., 2021b), which is beyond the scope of this work. Kerrigan et al. (2020) use deferentially-private finetuning to protect private data used to fine-tune a public language model. Yu et al. (2021a) study protecting privacy data during fine-tuning stage with lightweight fine-tuning methods such as adapters and PREFIX-TUNING. Our setting is different from all these works. We study to protect the private data of users when using LLM services and do not assume the existence of a central data curator.\nLocal Approaches. Local approaches impose a stronger degree of privacy protection but at the cost of loss in utility (i.e., the performance on the downstream task). Lyu et al. (2020) study maintaining the utility of the model under local privacy protection with a differentially private neural representation method. They only consider privacy protection during the inference stage but not the finetuning stage. Qu et al. (2021) propose a privacyconstrained fine-tuning (PCF) method for privacy protection during both fine-tuning and inference stages. Their method needs to fine-tune the entire model on privatized data, which is costly for large language models. Our approach also adopts a local privacy setting and supports privacy protection for\nfine-tuning and inference stages but is lightweight. We also introduce a novel privatized token reconstruction task for improving the performance of prompt tuning methods when training and inference with privatized data."
        },
        {
            "heading": "3 Approach",
            "text": "In this section, we describe RAPT in detail. We first introduce text-to-text privatization, which is used for users to privatize their data locally. We then describe how to customize and utilize LLM services with privatized data using our prompting method."
        },
        {
            "heading": "3.1 Text-to-Text Privatization",
            "text": "Using LLM services may face attacks such as eavesdropping attacks. Therefore, it is favorable to adopt a local privacy setting where users perform data privatization locally on their devices.\nAs most LLMs employ a text-to-text interface, we use text-to-text privatization in our framework following Feyisetan et al. (2020). Text-to-text privatization is based on the dX -privacy (Chatzikokolakis et al., 2013), a distance-based relaxation form of local differential privacy, which is widely used for protecting the privacy of textual content (Feyisetan et al., 2020; Qu et al., 2021). Formally speaking, for a given input set X and output set Y , dX is a distance function defined on X . A randomized mechanism M : X \u2192 Y is said to satisfies dX - privacy if and only if for any x \u2208 X and x\u2032 \u2208 X , the distribution over outputs of M(x) and M(x\u2032) are bounded by:\nP (M(x) = y)\nP (M(x\u2032) = y) \u2264 e\u03b7dX (x,x\u2032), \u2200y \u2208 Y, (1)\nwhere \u03b7 \u2265 0 is the privacy parameter, which controls the degree of privacy protection.2\nFor applying text-to-text privatization, we first need an embedding model to map words to vectors. Given a sequence x = [x1, . . . , xn] and an embedding model E \u2208 R|V |\u00d7d, where |V | and d are the vocabulary size and dimension of embeddings respectively, the embedding model maps x into a sequence of vectors [x1, . . . ,xn]. Assume using L2 distance as the distance function, applying dX with privacy parameter \u03b7 to a word embedding xt \u2208 Rd can be through adding random noise\n2To avoid confusion, we follow Qu et al. (2021) and use \u03b7 to distinguish , which is a commonly used parameter in differential privacy literature.\nz \u223c exp(\u2212\u03b7\u2016z\u2016) to xt (Wu et al., 2017). Let z = lv, sampling z is equivalent to first sampling the scalar l \u223c \u0393(d, 1\u03b7 ) and then sampling the vector v uniformly from the unit ball space Bd. Therefore, the privatized representation M(xt) of xt can be formally described as\nM(xt) = xt + z. (2)\nNext, we replace xt with a word x\u2032t that is closest to xt in the embedding space:\nx\u2032t = argmink\u2016wk \u2212M(wt)\u2016. (3)\nBy repetitively replacing words in a sequence x, we obtain a privatized textual version M(x) of x."
        },
        {
            "heading": "3.2 LLM Customization",
            "text": "Being lightweight and modular, prompt tuning is a suitable choice for customizing LLM services. We shall assume the users customize LLM services through prompt tuning APIs by uploading their privatized training data to service providers."
        },
        {
            "heading": "3.2.1 Prompt Tuning",
            "text": "Prompt tuning uses continuous vectors to adapt LLMs and is learned through gradient descent (Li and Liang, 2021; Lester et al., 2021). Prompt tuning has many variants, such as PREFIX-TUNING (Li and Liang, 2021), PROMPT TUNING (Lester et al., 2021), and P-TUNING (Liu et al., 2021). We follow the notation of Lester et al. (2021) for ease of description, but other variants are also applicable to our approach.\nLet f denote the backbone LLM model for the LLM service. A prompt P \u2208 RN\u00d7h of length N is a sequence of continuous vectors [p1, . . . ,pN ], where pi \u2208 Rh is a tunable virtual token embedding and h is the hidden size of the backbone LLM. Given an input sequence x = [xi, . . . , xn], the prompt is prepended to X \u2208 Rn\u00d7h, where X = [x1, . . . ,xn] is the sequence of word embeddings and x \u2208 Rh. Then we obtain a sequence of activation by using the LLM:\nH = f(JP ;XK), (4)\nwhere JP ;XK dentoes the concatenation of two sequences P and X , and H \u2208 Rn\u00d7h is the sequence of activations. H is used to predict taskdependent labels, typically through a language modeling head (Liu et al., 2021)."
        },
        {
            "heading": "3.2.2 Privatized Token Reconstruction",
            "text": "Unfortunately, directly tuning prompts on privatized data significantly degrades the performance of LLM on downstream tasks, even with weak privacy protections, which implies that prompt tuning is sensitive to random perturbations imposed by text-to-text privatization.\nRecent studies on language model representations (Voita et al., 2019; Mamou et al., 2020) found that the masked language modeling objective (Devlin et al., 2018) is helpful in learning separable deep representations. Inspired by their findings, we propose a novel privatized token reconstruction task, which is trained jointly with the downstream task. We expect training with privatized token reconstruction can help LLMs learn better task-dependent representations, therefore improving the performance of LLMs on downstream tasks with privacy preservation.\nPrivatized token reconstruction is similar to masked language modeling. However, reconstructing a word in a privatized input is infeasible because the word may contain sensitive information. To alleviates this issue, we prepend a sequence of tokens, which we refer to as \u201cplain tokens\u201d, to each input in the training data. Plain tokens consist of arbitrarily chosen tokens, therefore, can be safely sent to the service provider without worrying about privacy leakage. Figure 2 gives an overview of RAPT during LLM customization.\nFormally, let k = [k1, . . . , km] denote the plain tokens. Given a set of training examples D = {\u3008xi,yi\u3009|i = 1, . . . , |D|}, where |D| denotes the size of the training data. The users first prepend k to each input xi and then obtain a privatized version M(Jk;xiK) = [k\u20321, . . . , k\u2032m, x\u20321, . . . , x\u2032n]\nof Jk;xiK through text-to-text privatization. Let z = M(Jk;xiK), the LLM produces a sequence of activationsG with the input z and the prompt P :\nG = f(JP ;ZK), (5)\nwhere Z is the sequence of word embeddings of z, andG \u2208 R(m+n)\u00d7h.\nAfter obtaining G, we use the first m vectors G1:m = [g1, . . . , gm] in G to reconstruct plain tokens. To achieve this, we introduce an additional reconstruction head consisting of two linear layers. The probability distribution for predicting the i-th token ki in plain tokens is\npi = softmax(W1W2gi), (6)\nwhere W1 \u2208 R|T |\u00d7c and W2 \u2208 Rc\u00d7h are the parameters of reconstruction head, c is the hidden size of the reconstruction head, and |T | denotes the vocabulary size of the reconstruction head. The loss function for the privatized token reconstruction task is\nLrec = \u2212 m\u2211 i=1 log pi[ji], (7)\nwhere ji denotes the index of ki in the reconstruction head vocabulary, pi[ji] denotes the ji-th scalar in the vector pi. Please note that the vocabulary of the reconstruction head does not need to be the same as the vocabulary of the LLM.\nThe remaining n activationsGn:m+n are used to predict task-dependent labels. Assume the downstream task uses cross-entropy loss, yi \u2208 {0, 1}|C| is the associated label with yj denotes the label for the j-th class, and |C| is the number of classes. The objective function of the downstream task is formally described as\np = g(Gm:m+n), (8)\nLtask = \u2212 |C|\u2211 i=1 yi log p[i], (9)\nwhere g is the function of language modeling head. As a result, the loss function for training RAPT is\nL = Ltask + Lrec . (10)\nDuring the inference stage, the user side also applies text-to-text privatization to protect private information and obtain the prediction from the LLM service. We note that the reconstruction head is not used in the inference stage, and therefore {W1,W2} can be discarded after training."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Setup",
            "text": "Baselines. We build RAPT on top of two representative prompt tuning methods: PROMPT TUNING (Lester et al., 2021) and PREFIX-TUNING (Li and Liang, 2021). Both methods introduce a learnable continuous prompt for steering LLMs to downstream tasks. Compared with PROMPT TUNING, PREFIX-TUNING introduces more parameters by using deep prompts, which prepends continuous vectors to activations in all LLM layers.\nImplementation Details. To demonstrate the versatility of RAPT across LLMs, we use BERTBASE (Devlin et al., 2018), T5BASE (Raffel et al., 2020) as the backbone LLMs. Experiments are performed on a machine with 8 RTX 2080Ti GPUs. We adopt Adam (Kingma and Ba, 2014) as the optimizer. For the training, we set the learning rate to 6e-5 for all methods. The batch size is set to 128, and we train all methods for a total of 4 epochs. We set the maximum sequence length to 128 tokens. For PROMPT TUNING, the prompt length is set to 150, and for PREFIX-TUNING, we set the prompt length to 10. We set the hidden size c and the vocabulary size of the reconstruction head |T | to 96 and 7,630, respectively. For the inference, we report the average score across 5 independent runs. Unless otherwise noted, the length of plain tokens is set to 40 in all our experiments."
        },
        {
            "heading": "4.2 Privacy Experiments",
            "text": "We first study the effectiveness of RAPT against adversaries on simulated attacks following Song and Raghunathan (2020). We use BERTBASE as the backbone LLM and conduct experiments on the TP-UK dataset.\nAttacks. We use the following simulated attacks to study the effectiveness of RAPT for privacy protection. A detailed description of the two attacks is given in Appendix A.3.\n\u2022 Embedding Inversion Attack. A token-level attack (Song and Raghunathan, 2020) where the user input text representations can be reverted to the original input text through a distance-based or MLP-based search algorithm within the existing embedding space.\n\u2022 Attribute Inference Attack. An attack that attempts to infer private attributes of users from hidden representations (Al Zamal et al., 2012). TP-UK includes demographic attributes of users: gender and age. We follow the setting of Plant et al. (2021) to divide age properties into six equal-sized age range bins3 with assigned unique labels and gender into binary categorical variables represented by 0 and 1.\nMetrics. We use 1\u2212X as the evaluation metric for all attacks where X denotes the success rate of the simulated attack. We refer to this metric as empirical privacy.\nResults. Figure 3a demonstrates the results with text-to-text privatization against embedding inversion attacks. With privacy protection, the attacker can easily recover the original input. With textto-text privatization and by decreasing the privacy parameter \u03b7, we achieve better empirical privacy. These empirical privacy results can be used as the basis for the choice of the privacy parameter \u03b7.\nFigure 3b and 3c demonstrate the results of different prompt tuning methods with text-to-text pri-\n3The bins include [\u22641955, 1955-1963, 1964-1971, 1972- 1978, 1979-1985, \u22651986].\nvatization against attribute inference attacks. Attribute inference attacks are much harder than embedding inversion attacks because they need to infer information from deep representations. With text-to-text privatization, we observe improved empirical privacy for all methods. By introducing privatized token reconstruction, we can see that both PROMPT TUNING and PREFIX-TUNING achieve better empirical privacy regardless of the value of \u03b7. These results suggest that our RAPT can provide privacy protection against attribute inference attacks."
        },
        {
            "heading": "4.3 Utility Experiments",
            "text": "We study the effectiveness of RAPT in improving utility. We shall use embedding of the backbone\nLLM as the embedding model to perform text-totext privatization.\nDatasets. We evaluate our approach on the following datasets: Stanford Sentiment Treebank (SST) (Socher et al., 2013), Quora Question Pairs (QQP) (Chen et al., 2018), and the UK section of the Trustpilot Sentiment (TP-UK) (Hovy et al., 2015). We use accuracy scores as evaluation metrics for the three tasks.\nResults. Table 1 shows the results of PROMPT TUNING and PREFIX-TUNING with privacy protection on three downstream tasks. With smaller \u03b7, text-to-text privatization offers stronger privacy guarantees but inevitably hurts the performance of downstream tasks.4 Without using privatized token reconstruction, we can see that both PROMPT TUNING and PREFIX-TUNING perform poorly across all tasks and LLMs even with the weakest privacy protection (the largest \u03b7), which indicates that prompt tuning methods are sensitive to random perturbations imposed by text-to-text privatization. By introducing the privatized token reconstruction task, the performance of two prompt tuning methods is significantly improved across tasks and LLMs. The results suggest that privatized token reconstruction\n4Please note that the value of privacy parameter \u03b7 is not directly comparable for different LLMs because the average distance between two words in the embedding space is different for different LLMs."
        },
        {
            "heading": "Method Params. ACC EMP",
            "text": "is very effective in improving the performance of prompt tuning methods when trained on privatized data. The results coincide with our intuition that using reconstruction can help LLMs learn better representations.\nTo investigate whether our approach is still effective on a larger language model, we further conduct experiments on the SST dataset using the T53B LLM. These experiments are conducted on a machine with one NVIDIA A40 GPU. Table 2 shows the results. We also observe significant improvements when introducing privatized token reconstruction. The results confirm that our approach is effective regardless of the size of the LLM."
        },
        {
            "heading": "4.4 Comparison with Other Methods",
            "text": "We compare our RAPT with existing privacypreserving methods for LLMs, including CAPE (Plant et al., 2021), DPNR (Lyu et al., 2020), and Fine-tuning (Qu et al., 2021) methods on the SST-2 dataset. We set \u03b7 = 100 for fine-tuning and RAPT. Due to the differences in privacy-preserving mechanism between RAPT, CAPE and DPNR, to make a fair comparison, we adopt the settings corresponding to the best results in CAPE and DPNR. Details of these methods are given in Appendix A.4.\nTable 3 show the results. We found RAPT achieves competitive performance compared with other privacy-preserving methods in terms of empirical privacy protection and downstream task performance. Importantly, RAPT requires much less parameters to customize LLMs, which is crucial for LLM services."
        },
        {
            "heading": "4.5 Intrinsic Evaluation",
            "text": "We compare different variants of RAPT on the SST-2 task using the BERTBASE model. We assume RAPT uses PREFIX-TUNING for steering"
        },
        {
            "heading": "Embedding P \u03b7 ACC",
            "text": "BERTBASE to the SST-2 task."
        },
        {
            "heading": "4.5.1 Embedding Models",
            "text": "We investigate the effect of using different embedding models in text-to-text privatization. Specifically, the user side uses embedding models from GPT-2, RoBERTa, BioBERT (Lee et al., 2020), or T5 to map the input text to the privatized text. We adjust the privacy parameter \u03b7 for using different embedding models to match the level of probability for replacing a token in text. Table 4 shows the results of our experiments. We find that using different embedding models does not significantly affect the performance on the downstream task, even with embedding models trained on different domains (e.g., BERT vs. BioBERT). This is possible because recent embedding models are similar to each other by being trained on massive data."
        },
        {
            "heading": "4.5.2 Privatized Token Reconstruction",
            "text": "Content of Plain Tokens. We first show that the content of plain tokens can be chosen arbitrarily. We randomly generate 5 plain tokens with a length of 40 and compare the performance on the SST-2 task. Table 5 show the results. We can see that the performances with different plain tokens of the same length are comparable. The results suggest we can arbitrarily choose the plain tokens used in the privatized token reconstruction task.\nNumber of Plain Tokens. We study the effect of using diverse plain tokens in the privatized token reconstruction task. We first construct a set of plain tokens with the same length. Then during training, for each training input, we randomly choose plain tokens and prepend the plain tokens to the\ninput. From Figure 4a, we found that using more plain tokens during training slightly improves the performance on SST-2. However, using one plain token during privatized token reconstruction suffices to improve the performance of the LLM on downstream tasks.\nLength of Plain Tokens. We study the effect of using plain tokens with different lengths. Intuitively, the LLM needs to learn better representations to reconstruct longer plain tokens. Therefore using longer plain tokens may benefit the corresponding performance on downstream tasks. As shown in Figure 4b, we found that the results coincide with our intuition. Using longer plain tokens performs significantly better than using shorter plain tokens.\nReconstruction Head. We study the hidden size and vocabulary size of the reconstruction head. Figure 4c and 4d show the results. Using a larger hidden size generally performs better but introduces\nmore parameters during training. As for the vocabulary size, we found it is necessary to use a moderately large vocabulary. A small vocabulary makes the prediction of plain tokens much easier, therefore degrading the benefits of privatized token reconstruction task."
        },
        {
            "heading": "5 Analysis",
            "text": "In this section, we analyze the geometry of LLM representations to study why RAPT improves the performance of downstream tasks when trained on locally privatized data. We use PREFIX TUNING as the prompt tuning method for RAPT and set \u03b7 = 100. We use BERTBASE as the backbone LLM.\nWe first compute representations in each BERT layer for PREFIX-TUNING without privatization, PREFIX-TUNING with text-to-text privatization, and RAPT using data from SST-2. For each layer, we project the representations of different methods into 2D space using principal component analysis (PCA). Then we compute the average distance of representations between different methods. Figure 5 shows the results. We found learned deep representations that are similar to the representations learned without using privacy protection. It is clear that the learned representations for RAPT become closer to the representations without privacy protection as the layer number increases. Therefore, we confirm that RAPT can learn better representations."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we present RAPT for privacy preservation for LLM services. RAPT offers stronger privacy protection with local differential privacy while providing competitive performance with privatized token reconstruction. Experiments verify the effectiveness of RAPT across tasks and LLMs."
        },
        {
            "heading": "Limitations",
            "text": "Due to limited computational resources, we have yet to verify the effectiveness of our method on very large language models like GPT-3. We mainly conduct experiments on natural language understanding tasks. Besides that, we only evaluate empirical privacy level under embedding inversion and attribute inference attack scenarios, while it remains to be explored whether RAPT can achieve comparable performance with other privacy-preserving methods in membership inference attack scenario, which is to identify whether a sample is used to train the LLM."
        },
        {
            "heading": "Method SST QQP TP-UK",
            "text": ""
        },
        {
            "heading": "BERTBASE",
            "text": ""
        },
        {
            "heading": "Method SST QQP TP-UK",
            "text": ""
        },
        {
            "heading": "T5BASE",
            "text": ""
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Results without Privacy Protection",
            "text": "Table 6 show the results of PROMPT TUNING and PREFIX-TUNING without privacy protection. Without privacy protection, reconstructing plain tokens becomes trivial. As a result, adding token reconstruction cannot bring further improvements."
        },
        {
            "heading": "A.2 Other Fine-tuning Methods",
            "text": "Our privacy mechanism and privatized reconstruction task can also applicable to other fine-tuning methods. Table 9 shows the results of using prompt tuning, PREFIX-TUNING, ADAPTER, and FINE-TUNING for steering LLMs to downstream tasks. No matter which fine-tuning method used, privatized token reconstruction improves the performance on downstream tasks significantly."
        },
        {
            "heading": "A.3 Detailed Descriptions of Simulated Attacks",
            "text": ""
        },
        {
            "heading": "A.3.1 Attribute Inference Attack",
            "text": "An attacker a can infer user\u2019s private attribute ti \u2208 {0, 1}|C| from the hidden representations Z = {z1, ..., zn}, |C| is the number of private classes. Z can either be the mean vector of perturbed word embeddingsM(x) or the output contextualized representations H of LLM. Specifi-\ncally, the attacker\u2019s objective is:\np = a( 1\nn n\u2211 i=1 zi;\u03b8a), (11)\nLattack = \u2212 |C|\u2211 i=1 ti log pi, (12)\nwhere \u03b8a is set of trainable parameters in attacker. We utilize a 2-layer MLP with 768 hidden units and ReLU activation function as the attribute inference attacker based on Plant et al. (2021)\u2019s setting."
        },
        {
            "heading": "A.3.2 Embedding Inversion Attack",
            "text": "We follow Qu et al. (2021) white-box inversion setting. Given Rd is the general embedding space, for any privatized token embedding vt, we could recover the original token embedding through L2 distance\nmin\u2016v \u2212 vt\u20162, v \u2208 Rd. (13)\nWe leverage the nearest neighbour search as the embedding inversion attacker to search the nearest token embedding within the original embedding space for each perturbed word embedding."
        },
        {
            "heading": "A.4 Detailed Descriptions of Other Methods",
            "text": "\u2022 CAPE (Plant et al., 2021): applying both\nLaplace-based embedding perturbation and an adversarial learning objective to privatize the representation outputs of LLM.\n\u2022 DPNR (Lyu et al., 2020): incorporating word dropout and Laplace-based embedding perturbation to privatize the representation outputs of LLM.\n\u2022 Finetuning (Qu et al., 2021): using dX -privacy to perturb user input embedding and updating the entire LLM to adopt privatized word embeddings.\nAs for CAPE, we applied BERTBASE model with privacy parameters = 0.01 and \u03bb = 1.5 and set learning rate to 1e\u2212 3 in adversarial training process. As for DPNR, we applied BERTBASE model and set word dropout rate \u00b5 = 0.1 and privacy parameters = 5. We set \u03b7 = 100 for our RAPT."
        },
        {
            "heading": "A.5 Effect of Prompt Length",
            "text": "Table 7 shows the performance of RAPT using different prompt length. We can see that RAPT typically performs better as the length of the prompt\nincreases because the number of trainable parameters is proportional to the length of the prompt. Adding privatized token construction task does not affect this trend."
        },
        {
            "heading": "A.6 Plain Tokens",
            "text": "Table 8 shows the plain tokens used in our experiments for different tasks.\nA.7 Visualization Figure 6 gives a visualization of distributions of private attribute representations for different methods. It is clear that with text-to-text privatization, it is hard to infer private attribute based on the distribution for PREFIX-TUNING and RAPT."
        }
    ],
    "title": "Privacy-Preserving Prompt Tuning for Large Language Model Services",
    "year": 2023
}