{
    "abstractText": "Human visual attention is a complex phenomenon that has been studied for decades. Within it, the particular problem of scanpath prediction poses a challenge, particularly due to the interand intra-observer variability, among other reasons. Besides, most existing approaches to scanpath prediction have focused on optimizing the prediction of a gaze point given the previous ones. In this work, we present a probabilistic time-evolving approach to scanpath prediction, based on Bayesian deep learning. We optimize our model using a novel spatio-temporal loss function based on a combination of Kullback-Leibler divergence and dynamic time warping, jointly considering the spatial and temporal dimensions of scanpaths. Our scanpath prediction framework yields results that outperform those of current state-of-the-art approaches, and are almost on par with the human baseline, suggesting that our model is able to generate scanpaths whose behavior closely resembles those of the real ones.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel Martin"
        },
        {
            "affiliations": [],
            "name": "Diego Gutierrez"
        },
        {
            "affiliations": [],
            "name": "Belen Masia"
        }
    ],
    "id": "SP:1619aa38eff9db41da1a5e52476d9be623d1811c",
    "references": [
        {
            "authors": [
                "L. Itti",
                "C. Koch",
                "E. Niebur"
            ],
            "title": "A model of saliency-based visual attention for rapid scene analysis",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence, vol. 20, no. 11, pp. 1254\u20131259, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "L. Itti",
                "C. Koch"
            ],
            "title": "Computational modelling of visual attention",
            "venue": "Nature reviews neuroscience, vol. 2, no. 3, pp. 194\u2013203, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "M. K\u00fcmmerer",
                "M. Bethge"
            ],
            "title": "State-of-the-art in human scanpath prediction",
            "venue": "arXiv preprint arXiv:2102.12239, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J.H. Goldberg",
                "J.I. Helfman"
            ],
            "title": "Visual scanpath representation",
            "venue": "Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications, 2010, pp. 203\u2013210.",
            "year": 2010
        },
        {
            "authors": [
                "Z. Kapoula"
            ],
            "title": "The influence of peripheral preprocessing on oculomotor programming in a scanning task",
            "venue": "Eye movements and psychological functions. Routledge, 2021, pp. 101\u2013114.",
            "year": 2021
        },
        {
            "authors": [
                "D. Martin",
                "A. Serrano",
                "A.W. Bergman",
                "G. Wetzstein",
                "B. Masia"
            ],
            "title": "Scangan360: A generative model of realistic scanpaths for 360\u25e6 images",
            "venue": "IEEE Transactions on Visualization and Computer Graphics, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "O. Le Meur",
                "A. Coutrot"
            ],
            "title": "Introducing context-dependent and spatially-variant viewing biases in saccadic models",
            "venue": "Vision research, vol. 121, pp. 72\u201384, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T. Judd",
                "K. Ehinger",
                "F. Durand",
                "A. Torralba"
            ],
            "title": "Learning to predict where humans look",
            "venue": "IEEE ICCV, 2009, pp. 2106\u20132113.",
            "year": 2009
        },
        {
            "authors": [
                "O. Le Meur",
                "Z. Liu"
            ],
            "title": "Saccadic model of eye movements for free-viewing condition",
            "venue": "Vision Research, vol. 116, pp. 152 \u2013 164, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "B.W. Tatler",
                "B.T. Vincent"
            ],
            "title": "The prominence of behavioural biases in eye guidance",
            "venue": "Visual Cognition, vol. 17, no. 6-7, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "W. Sun",
                "Z. Chen",
                "F. Wu"
            ],
            "title": "Visual scanpath prediction using iorroi recurrent mixture density network",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 6, pp. 2101\u20132118, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Bao",
                "Z. Chen"
            ],
            "title": "Human scanpath prediction based on deep convolutional saccadic model",
            "venue": "Neurocomputing, vol. 404, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Arabadzhiyska",
                "O.T. Tursun",
                "K. Myszkowski",
                "H.-P. Seidel",
                "P. Didyk"
            ],
            "title": "Saccade landing position prediction for gaze-contingent rendering",
            "venue": "ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp. 1\u201312, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Nguyen",
                "Z. Yan",
                "K. Nahrstedt"
            ],
            "title": "Your attention is unique: Detecting 360-degree video saliency in head-mounted display for head movement prediction",
            "venue": "Proceedings of the 26th ACM international conference on Multimedia, 2018, pp. 1190\u20131198.",
            "year": 2018
        },
        {
            "authors": [
                "R. Fahimi",
                "N.D. Bruce"
            ],
            "title": "On metrics for measuring scanpath similarity",
            "venue": "Behavior Research Methods, pp. 1\u201320, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S.R. Ellis",
                "J.D. Smith"
            ],
            "title": "Patterns of statistical dependency in visual scanning",
            "venue": "Eye movements and human information processing, pp. 221\u2013238, 1985.",
            "year": 1985
        },
        {
            "authors": [
                "S. Xingjian",
                "Z. Chen",
                "H. Wang",
                "D.-Y. Yeung",
                "W.-K. Wong",
                "W.-c. Woo"
            ],
            "title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
            "venue": "Advances in neural information processing systems, 2015, pp. 802\u2013810.",
            "year": 2015
        },
        {
            "authors": [
                "D. Walther",
                "C. Koch"
            ],
            "title": "Modeling attention to salient protoobjects",
            "venue": "Neural Networks, vol. 19, pp. 1395\u20131407, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "Q. Zhao",
                "C. Koch"
            ],
            "title": "Learning a saliency map using fixated locations in natural scenes",
            "venue": "Journal of Vision, vol. 11, p. 9, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "Y. Lu",
                "W. Zhang",
                "C. Jin",
                "X. Xue"
            ],
            "title": "Learning attention map from images",
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "A. Borji"
            ],
            "title": "Boosting bottom-up and top-down visual features for saliency estimation",
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Z. Bylinskii",
                "T. Judd",
                "A. Borji",
                "L. Itti",
                "F. Durand",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Mit saliency benchmark",
            "venue": "http://saliency.mit.edu/, 2019. A PROBABILISTIC TIME-EVOLVING APPROACH TO SCANPATH PREDICTION 10",
            "year": 2019
        },
        {
            "authors": [
                "C. Yang",
                "L. Zhang",
                "R. Lu",
                "Huchuan",
                "Xiang",
                "M.-H. Yang"
            ],
            "title": "Saliency detection via graph-based manifold ranking",
            "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. IEEE, 2013, pp. 3166\u20133173.",
            "year": 2013
        },
        {
            "authors": [
                "E. Vig",
                "M. Dorr",
                "D. Cox"
            ],
            "title": "Large-scale optimization of hierarchical features for saliency prediction in natural images",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. K\u00fcmmerer",
                "T.S.A. Wallis",
                "M. Bethge"
            ],
            "title": "Deepgaze ii: Reading fixations from deep features trained on object recognition",
            "venue": "arXiv preprint arXiv:1610.01563, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Pan",
                "E. Sayrol",
                "X. Giro-i Nieto",
                "K. McGuinness",
                "N.E. O\u2019Connor"
            ],
            "title": "Shallow and deep convolutional networks for saliency prediction",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.",
            "year": 2016
        },
        {
            "authors": [
                "D. Martin",
                "A. Serrano",
                "B. Masia"
            ],
            "title": "Panoramic convolutions for 360\u25e6 single-image saliency prediction",
            "venue": "CVPR Workshop on Computer Vision for Augmented and Virtual Reality, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Pan",
                "C. Canton",
                "K. McGuinness",
                "N.E. O\u2019Connor",
                "J. Torres",
                "E. Sayrol",
                "X. a. Giro-i Nieto"
            ],
            "title": "Salgan: Visual saliency prediction with generative adversarial networks",
            "venue": "arXiv preprint arXiv:1701.01081, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C. Xia",
                "J. Han",
                "F. Qi",
                "G. Shi"
            ],
            "title": "Predicting human saccadic scanpaths based on iterative representation learning",
            "venue": "IEEE Transactions on Image Processing, vol. 28, no. 7, pp. 3502\u20133515, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Cornia",
                "L. Baraldi",
                "G. Serra",
                "R. Cucchiara"
            ],
            "title": "Predicting human eye fixations via an lstm-based saliency attentive model",
            "venue": "IEEE Transactions on Image Processing, vol. 27, no. 10, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Wang",
                "J. Shen",
                "X. Dong",
                "A. Borji"
            ],
            "title": "Salient object detection driven by fixation prediction",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Liu",
                "D. Xu",
                "Q. Huang",
                "W. Li",
                "M. Xu",
                "S. Lin"
            ],
            "title": "Semanticallybased human scanpath estimation with hmms",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 3232\u2013 3239.",
            "year": 2013
        },
        {
            "authors": [
                "H.R. Tavakoli",
                "E. Rahtu",
                "J. Heikkil\u00e4"
            ],
            "title": "Stochastic bottom\u2013 up fixation prediction and saccade generation",
            "venue": "Image and Vision Computing, vol. 31, no. 9, pp. 686\u2013693, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Assens Reina",
                "X. Giro-i Nieto",
                "K. McGuinness",
                "N.E. O\u2019Connor"
            ],
            "title": "Saltinet: Scan-path prediction on 360 degree images using saliency volumes",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, 2017, pp. 2331\u20132338.",
            "year": 2017
        },
        {
            "authors": [
                "B.W. Tatler",
                "J.R. Brockmole",
                "R.H. Carpenter"
            ],
            "title": "Latest: A model of saccadic decisions in space and time.",
            "venue": "Psychological review,",
            "year": 2017
        },
        {
            "authors": [
                "D. Zanca",
                "S. Melacci",
                "M. Gori"
            ],
            "title": "Gravitational laws of focus of attention",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 42, no. 12, pp. 2983\u20132995, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W. Wang",
                "C. Chen",
                "Y. Wang",
                "T. Jiang",
                "F. Fang",
                "Y. Yao"
            ],
            "title": "Simulating human saccadic scanpaths on natural images",
            "venue": "CVPR 2011. IEEE, 2011, pp. 441\u2013448.",
            "year": 2011
        },
        {
            "authors": [
                "R. Engbert",
                "H.A. Trukenbrod",
                "S. Barthelm\u00e9",
                "F.A. Wichmann"
            ],
            "title": "Spatial statistics and attentional dynamics in scene viewing",
            "venue": "Journal of vision, vol. 15, no. 1, pp. 14\u201314, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Adeli",
                "F. Vitu",
                "G.J. Zelinsky"
            ],
            "title": "A model of the superior colliculus predicts fixation locations during scene viewing and visual search",
            "venue": "Journal of Neuroscience, vol. 37, no. 6, pp. 1453\u20131467, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. Boccignone",
                "M. Ferraro"
            ],
            "title": "Modelling gaze shift as a constrained random walk",
            "venue": "Physica A: Statistical Mechanics and its Applications, vol. 331, no. 1-2, pp. 207\u2013218, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "X. Sun",
                "H. Yao",
                "R. Ji",
                "X.-M. Liu"
            ],
            "title": "Toward statistical modeling of saccadic eye-movement and visual saliency",
            "venue": "IEEE Transactions on Image Processing, vol. 23, no. 11, pp. 4649\u20134662, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A.D. Clarke",
                "M.J. Stainer",
                "B.W. Tatler",
                "A.R. Hunt"
            ],
            "title": "The saccadic flow baseline: Accounting for image-independent biases in fixation behavior",
            "venue": "Journal of vision, vol. 17, no. 11, pp. 12\u201312, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Chen",
                "M. Jiang",
                "Q. Zhao"
            ],
            "title": "Predicting human scanpaths in visual question answering",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10 876\u201310 885.",
            "year": 2021
        },
        {
            "authors": [
                "M. Assens",
                "X. Giro-i Nieto",
                "K. McGuinness",
                "N.E. O\u2019Connor"
            ],
            "title": "Pathgan: Visual scanpath prediction with generative adversarial networks",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops, 2018, pp. 0\u20130.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Hu",
                "S. Li",
                "C. Zhang",
                "K. Yi",
                "G. Wang",
                "D. Manocha"
            ],
            "title": "Dgaze: Cnn-based gaze prediction in dynamic scenes",
            "venue": "IEEE transactions on visualization and computer graphics, vol. 26, no. 5, pp. 1902\u20131911, 2020.",
            "year": 1902
        },
        {
            "authors": [
                "M. Qiao",
                "M. Xu",
                "Z. Wang",
                "A. Borji"
            ],
            "title": "Viewport-dependent saliency prediction in 360\u00b0 video",
            "venue": "IEEE Transactions on Multimedia, vol. 23, pp. 748\u2013760, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "W. Zhang",
                "Y. Liu",
                "Y. Wang"
            ],
            "title": "Very long term field of view prediction for 360-degree video streaming",
            "venue": "2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2019, pp. 297\u2013302.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xu",
                "Z. Zhang",
                "S. Gao"
            ],
            "title": "Spherical dnns and their applications in 360\u25e6 images and videos",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C.-L. Fan",
                "J. Lee",
                "W.-C. Lo",
                "C.-Y. Huang",
                "K.-T. Chen",
                "C.-H. Hsu"
            ],
            "title": "Fixation prediction for 360 video streaming in head-mounted virtual reality",
            "venue": "Proceedings of the 27th Workshop on Network and Operating Systems Support for Digital Audio and Video, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Zemblys",
                "D.C. Niehorster",
                "K. Holmqvist"
            ],
            "title": "gazenet: End-toend eye-movement event detection with deep neural networks",
            "venue": "Behavior research methods, vol. 51, no. 2, pp. 840\u2013864, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Liu",
                "J. Lehman",
                "P. Molino",
                "F.P. Such",
                "E. Frank",
                "A. Sergeev",
                "J. Yosinski"
            ],
            "title": "An intriguing failing of convolutional neural networks and the coordconv solution",
            "venue": "Neural information processing systems, 2018, pp. 9605\u20139616.",
            "year": 2018
        },
        {
            "authors": [
                "L. Larsson",
                "M. Nystr\u00f6m",
                "M. Stridh"
            ],
            "title": "Detection of saccades and postsaccadic oscillations in the presence of smooth pursuit",
            "venue": "IEEE Transactions on biomedical engineering, vol. 60, no. 9, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "K. Zhang",
                "Z. Chen",
                "S. Liu"
            ],
            "title": "A spatial-temporal recurrent neural network for video saliency prediction",
            "venue": "IEEE Transactions on Image Processing, vol. 30, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Palazzi",
                "D. Abati",
                "F. Solera",
                "R. Cucchiara"
            ],
            "title": "Predicting the driver\u2019s focus of attention: the dr (eye) ve project",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 7, pp. 1720\u20131733, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X. Wu",
                "Z. Wu",
                "J. Zhang",
                "L. Ju",
                "S. Wang"
            ],
            "title": "Salsac: A video saliency prediction model with shuffled attentions and correlationbased convlstm",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 2020, pp. 12 410\u201312 417.",
            "year": 2020
        },
        {
            "authors": [
                "M. M\u00fcller"
            ],
            "title": "Dynamic time warping",
            "venue": "Information retrieval for music and motion, pp. 69\u201384, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "M. Cuturi",
                "M. Blondel"
            ],
            "title": "Soft-dtw: a differentiable loss function for time-series",
            "venue": "arXiv preprint arXiv:1703.01541, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.-Y. Chang",
                "D.-A. Huang",
                "Y. Sui",
                "L. Fei-Fei",
                "J.C. Niebles"
            ],
            "title": "D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Song",
                "W. Wang",
                "S. Zhao",
                "J. Shen",
                "K.-M. Lam"
            ],
            "title": "Pyramid dilated deeper convlstm for video salient object detection",
            "venue": "Proceedings of the European conference on computer vision (ECCV), 2018, pp. 715\u2013731.",
            "year": 2018
        },
        {
            "authors": [
                "R. Azad",
                "M. Asadi-Aghbolaghi",
                "M. Fathy",
                "S. Escalera"
            ],
            "title": "Bidirectional convlstm u-net with densley connected convolutions",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0\u20130.",
            "year": 2019
        },
        {
            "authors": [
                "H. Wang",
                "D.-Y. Yeung"
            ],
            "title": "Towards bayesian deep learning: A framework and some existing methods",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 12, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Kendall",
                "Y. Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision?",
            "venue": "arXiv preprint arXiv:1703.04977,",
            "year": 2017
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision, vol. 115, no. 3, pp. 211\u2013252, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "J. Xu",
                "M. Jiang",
                "S. Wang",
                "M.S. Kankanhalli",
                "Q. Zhao"
            ],
            "title": "Predicting human gaze beyond pixels",
            "venue": "Journal of Vision, vol. 14, no. 1, pp. 1\u201320, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "T. Judd",
                "F. Durand",
                "A. Torralba"
            ],
            "title": "Fixations on low-resolution images",
            "venue": "Journal of Vision, vol. 11, no. 4, pp. 14\u201314, 2011. A PROBABILISTIC TIME-EVOLVING APPROACH TO SCANPATH PREDICTION 11",
            "year": 2011
        },
        {
            "authors": [
                "O. Yadan"
            ],
            "title": "Hydra - a framework for elegantly configuring complex applications",
            "venue": "Github, 2019. [Online]. Available: https: //github.com/facebookresearch/hydra",
            "year": 2019
        },
        {
            "authors": [
                "F. e. a. Wa"
            ],
            "title": "Pytorch lighning",
            "venue": "Github, 2019. [Online]. Available: https://github.com/PyTorchLightning/pytorch-lightning",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Scanpath prediction, convolutional recurrent networks, saliency, machine learning\nF"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "UNDERSTANDING human visual attention has been anactive research area for decades. A plethora of works have been devoted to analyzing human attention when viewing content in different disciplines, including computer vision, graphics, neuroscience, or psychology.\nHowever, and regardless of the medium, gathering sufficiently large amounts of data to perform behavioral studies is a cumbersome and time-consuming task. Being able to generate virtual observers mimicking such attention process would greatly facilitate the process, thus helping achieve more significant advances in the fields.\nMany works have focused on, given an image, predicting where the attention of the observer is going to be directed to. Traditionally, the problem has been tackled through spatial, bottom-up analyses of the image, leading to the determination of salient areas represented as saliency maps, which are topographical representations by a scalar quantity of the conspicuity (i.e., saliency) at every location in the visual field [1], [2].\nAlthough this may suffice for certain applications, saliency maps fail to capture the temporal dimension of gaze. This temporal information is relevant in a varied number of scenarios: How long does it take for an observer to find a specific object in an image? How should one design the layout of a 360\u00ba environment or scene? Will some distractor drive attention away from the main focal point, and if so, how and to what extent? Current application areas where this temporal dimension prediction is relevant range from marketing and product placement, webpage design or scene design, to analysis of visual pathologies or realistic eye motion simulation (e.g., for avatar animation).\nTo take into account this temporal information, a number\n\u2022 Daniel Martin, Diego Gutierrez, and Belen Masia are with Universidad de Zaragoza, I3A.\n\u2022 Correspondence to: danims@unizar.es. Manuscript received April 2022.\nof works have tackled the problem of scanpath prediction [3]. A scanpath can be defined as a sequence of consecutive eye movements (i.e., fixations and saccades) through time and space [4]. Gaze behavior is a complex phenomenon which involves spatio-temporal dependencies [5], [6], as well as a large inter- and intra-observer variability [7], [8].\nWhen attempting to model the temporal dimension of gaze, the problem is often posed as follows: given an input image I , and a sequence of gaze points {s0, ..., st\u22121}, the goal is to predict the next gaze point in the scanpath, st. Previous works either resort to heuristics and hand-crafted features [9], [10], or to data-driven methods [11], [12] to do this. However, most of the existing methods are designed to optimize the prediction of a single fixation point, given the previous points; thus the scanpath is progressively built by concatenating successive single-point solutions. While this strategy is useful for several applications such as foveated rendering [13], [14], it may lead to increasing deviations from actual human viewing behavior and scanpath plausibility [15].\nIn this paper we present a method to predict full, plausible scanpaths given an input image I (see Figure 1). We leverage the fact that, despite the inter- and intra-observer variability, common patterns and behaviors do emerge when humans observe certain content [16]. This allows us to obtain not a single scanpath, but a distribution of scanpaths within this common behavioral space. This distribution can then be sampled to generate individual scanpaths.\nWe rely on convolutional long-short term memory networks (ConvLSTM) [17], since their recurrent architecture is well suited to capture the temporal dependency of each predicted point in a scanpath, while their convolutional nature has proven to be successful handling problems with both spatial and temporal dependencies. To obtain the distribution of plausible scanpaths we explicitly incorporate the inherent uncertainty of the problem into our model: Our ConvLSTM module is, for the first time, based on Bayesian deep learning, so that its weights are not deterministic, but\nar X\niv :2\n20 4.\n09 40\n4v 1\n[ cs\n.C V\n] 2\n0 A\npr 2\n02 2\nsampled from a learned distribution instead. In addition, our network is trained using a novel spatio-temporal loss function that combines the benefits of the Kullback-Leibler divergence and dynamic time warping (DTW) for joint spatio-temporal optimization.\nOur resulting trained model is able to generate a distribution of plausible scanpaths for a given input image, where each scanpath mimics the visual behavior of a human observer and takes less than one second to generate. We have validated our model both qualitatively and quantitatively, including an exhaustive set of existing metrics accounting for different scanpath characteristics [15]. Our model outperforms the state of the art, being almost on par with the human baseline. We will make our code and model publicly available to encourage future research."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Saliency prediction",
            "text": "First approaches towards modeling human attention were based on saliency, as a measure of how much each part of a scene attracts human attention. The seminal work by Itti et al. [1] established the basis of visual attention prediction in images, by extracting hand-crafted features to generate a saliency map. This work inspired many posterior approaches (e.g., [18], [19]) which were also based on the computation of conspicuity maps for different visual features (such as color, intensity, orientation of edges, or faces), which were then combined into a final saliency map. Other approaches included multiple semantic segmentation and surroundness analysis [20], or known human priors such as center bias or horizon line detectors [21], to improve saliency prediction.\nWith the proliferation of deep learning techniques and the appearance of public datasets [8], [22], [23], data-driven methods emerged, yielding impressive results. These methods were mostly based on convolutional neural networks (CNN) that extract latent features from which to infer saliency [24], [25], [26], [27]. Other approaches also leveraged the advances of generative networks [28], [29] and recurrent neural networks [30], [31]. None of these works, however, take into account the dynamic nature of gaze behavior, not being able to model the temporal dimension of human attention."
        },
        {
            "heading": "2.2 Scanpath prediction",
            "text": "Scanpath models usually aim to progressively build a scanpath by concatenating single-point predictions, which may be partially based on the previous points of the path. Ellis and Smith [16] presented a framework based on Markov stochastic processes. Later, other works proposed approaches that included known human biases, (such as the center bias, or human oculomotor constraints) [9], [10], [32], [33]. Data-driven methods provide faster and more precise approaches, for instance using existing saliency prediction methods as a proxy to scanpath prediction, by means of winner-takes-all and inhibition-of-return strategies, by sampling heuristics [34], or simply leveraging deep features from neural networks [25].\nScanpath prediction methods can be roughly categorized into (i) biologically inspired, (ii) statistically inspired, (iii) cognitively inspired, and (iv) engineered models [3]. Biologically inspired models take into account the importance of low-level features [1], [35], [36], visual working memory [37], attention and inhibition-of-return [38], or neuropsychology [39]. Statistically inspired models try to mimic certain statistical properties of human scanpaths [7], [29], [40], [41], [42]. Cognitively inspired models assume that other cognitive processes besides low-level features can drive observers\u2019 attention, and therefore implement different human mechanisms such as low-level saliency, semantic and spatial effects [32] or region-of-interest and inhibitionof-return [11]. Finally, engineered models just exploit the ability of data-driven techniques to fit to given data [12], [34], [43], [44], [45].\nWith this surge of data-driven approaches, and motivated by the temporal dependencies that human viewing behavior presents, some works have resorted to recurrent neural networks (RNN), which are capable of encoding previous information, and leveraging it to formulate a prediction [14]. However, scanpath prediction requires handling temporal and spatial information. To account for both, some recent approaches have built their models following ConvLSTM strategies [11], [46], [47], [48], where convolutional operators handle spatial features while LSTM architectures enable learning temporal information.\nHowever, all the aforementioned works are trained to optimize single-point predictions, by means of direct losses such as MSE [44] or BCE [11], and thus do not concern themselves with the plausibility of the scanpath as a whole. Recently, the work of Martin et al. [6] presented a scanpath\ngeneration method for 360\u25e6 content, where the model was optimized by means of a dynamic time warping loss function on the whole distribution of ground-truth scanpaths, rather than on a single-point solution, and was hence able to learn and mimic latent behaviors in its predictions.\nIn this work, and endorsed by previous literature, we resort to convolutional recurrent networks, but overcome the limitations of single-point prediction approaches by combining a novel loss function that combines dynamic time warping and Kullback-Leibler divergence, and a probabilistic approach. The loss function enables focusing on both the temporal and spatial aspects of the whole scanpaths, and optimizes our model over the whole distribution of real scanpaths, while our probabilistic approach accounts for the inherent human variability."
        },
        {
            "heading": "3 OUR MODEL",
            "text": "Our model performs probabilistic scanpath prediction given a single 2D image as input. The model, based on recurrent neural networks, is described in detail in this section: we introduce the representation we employ for the scanpaths (Section 3.1), a novel loss function that is able to optimize our scanpaths in a joint spatio-temporal fashion (Section 3.3), our model architecture in depth (Section 3.4), and additional details on our training data and procedure (Section 3.5)."
        },
        {
            "heading": "3.1 Scanpath Representation",
            "text": "Scanpaths are commonly defined as a sequence s = {s0, s1, .., sN\u22121} of gaze points1, where si = (xi, yi), and (xi, yi) are the image coordinates of that particular gaze point. While this representation may suffice in some cases [6], [44], [49], [50], it usually falls short for problems where it is necessary to establish a relationship between those coordinates and the position of features within an image. Indeed, convolutional networks are trained to be shift-invariant [51], and forcing them to explicitly learn\n1. In our case, and following common practice [11], [12], [25], the points in a scanpath correspond to fixation points (i.e., we do not attempt to predict saccades and other ocular movements).\nthe relation between gaze point coordinates and the actual positions of image features is challenging and hinders the training process.\nAdditionally, scanpaths for a given image exhibit both inter- and intra-observer variability. Not all observers will explore the image in exactly the same way, resulting in interobserver variability. Besides, an observer watching the same image twice may follow slightly different scanpaths, and, even if asked to follow a certain path, there is a ballistic or noisy component in ocular movements (e.g., saccades or post-saccadic oscillations [52]), that results in different gaze points. As a result, scanpaths are non-deterministic. However, they do exhibit clear patterns across and within observers, as multiple works have shown [9].\nGiven this variability, and in order to facilitate the spatial learning of the network, instead of representing each gaze point with its coordinates, si = (xi, yi), a more adequate representation for si is a Gaussian distribution gsi centered in (xi, yi), and defined over the whole image. In each distribution gi, there is thus a value gi(x, y) per pixel (x, y), which represents the probability of a gaze point falling at pixel (x, y) at time step i. A scanpath s is therefore represented as a sequence gs = {gs0, gs1, .., gsN\u22121} of Gaussian maps gsi (see Figure 2); we term a scanpath represented in this way a spatialized scanpath. This representation facilitates spatial learning by providing a direct correlation between a scanpath and its corresponding image."
        },
        {
            "heading": "3.2 Overview of the Model",
            "text": "Our model, illustrated in Figure 3, is based on the recently presented ConvLSTM [17], a type of recurrent neural network. ConvLSTMs maintain the recurrent nature of traditional LSTMs, processing data in a sequential manner, thus being able to learn the temporal features of the data. Additionally, ConvLSTMs are provided with convolutional operators that handle visual information and facilitate learning spatial features in the input sequence.\nFurther, we resort to a Bayesian approach when modeling the ConvLSTM module, in order to better incorporate the uncertainty driven by inter- and intra-observer variability: The output of the ConvLSTM module is not a point, but rather a probability map (see Figure 3). Our whole model therefore predicts, given an input image, a sequence of timeevolving scanpath probabilistic maps (tSPM) (see Figure 4). Each tSPM represents the probabilities of the next gaze fixation point falling on each pixel of the image at a certain time instant.\nWe additionally leverage pretrained neural networks on image classification tasks to facilitate feature extraction, and CoordConv layers to improve learning of spatial features. The details of our model architecture are described in Section 3.4."
        },
        {
            "heading": "3.3 Loss Function",
            "text": "Our spatialized scanpath representation facilitates working over the spatial component of the scanpaths, as explained in Section 3.1. Both the spatial and temporal domains are critical when predicting gaze points. Recurrent neural networks (RNN) have proven to be powerful tools able to handle time dependencies in data, being able to extract, maintain and\neven infer patterns through time, and thus have been successfully used in some approaches for gaze prediction (see Section 2). However, all those approaches have designed their RNN-based models to optimize the prediction at each time step, with element-wise loss functions, such as mean squared error (MSE) or binary cross-entropy (BCE), that penalize the prediction for each point in isolation.\nIn contrast, we propose a novel loss function based on the Kullback-Leibler divergence and dynamic time warping, computed over the whole scanpath. The former allows our model to account for the spatial relations between gaze points, while the latter ensures a realistic and plausible temporal behavior of the predicted scanpaths.\nKullback-Leibler Divergence (KL-Div) KL-Div is a measure of how different a probability distribution is from another one, and is one of the most commonly used metrics and losses in saliency prediction problems [46], [53], [54], [55]. The Kullback-Leibler divergence (DKL) is defined as:\nDKL(P ||Q) = \u2211 j P (j)ln P (j) Q(j) , (1)\nwhere P and Q are the probability distributions to be compared, and j refers to each point of the distribution. In our particular case, each gaze point is represented in a spatialized manner, hence KL-Div is able to give a qualitative measurement on how different two points are based on their probability distributions P and Q.\nDynamic Time Warping (DTW) DTW is a measure of similarity between two time series that may differ in length or speed [56]. The DTW algorithm attempts to find the optimal match between the points of two temporal sequences, r and s, by matching each point in one of them with at least one\npoint in the other, without forcing a one-to-one correspondence between both sequences. The optimal match is found by minimizing a cost function: a distance matrix \u2206 stores the cost (Euclidean distance) for each possible pair of points, and the optimization searches for the matching (alignment) between r and s such that the total cost is minimized. This can be written as:\nDTW (r, s) = min A \u3008A,\u2206(r, s)\u3009, (2)\nwhere A is a binary alignment matrix between two time series r and s, \u2206(r, s) = [\u03b4(ri, sj)]i,j is a matrix containing the distances \u03b4(\u00b7, \u00b7) between each pair of points in r and s, and \u3008\u00b7, \u00b7\u3009 denotes the inner product between both matrices. Since the minimum function is not differentiable, a soft version has been proposed [57]:\nDTW \u03b3(r, s) = min A \u03b3\u3008A,\u2206(r, s)\u3009, \u03b3 > 0 (3)\nThe soft-min function min\u03b3 is defined as:\nmin\u03b3(a1, ..., aN ) = \u2212\u03b3 log N\u2211 i=1 exp ( \u2212ai \u03b3 ) , (4)\nwith the \u03b3 parameter adjusting the similarity between the soft version and the original DTW algorithm, both being the same when \u03b3 = 0. Eq. 3 has been used successfully as a loss term in related contexts, such as scanpath generation for virtual reality [6] or weakly supervised action alignment and segmentation in videos [58].\nOur Joint KL-DTW Loss While KL-Div accounts for the spatial similarity of two distributions, and DTW focuses on the temporal dimension, none of them suffices on its own in our particular case. We therefore propose a novel\nloss function based on the combination of both KL-Div and DTW, defined as follows:\nLKL\u2212DTW (r\u2032) =\nS\u2211 s=1 DTW \u03b3(r\u2032, s)\n|S| , (5)\nwhere r\u2032 is a predicted sequence of tSPM (see Section 3.2), and s is a ground-truth scanpath from the set of groundtruth ones S for a given image I . DTW \u03b3 is computed as given by Eq. 3. However, we modify the computation of the distance matrix \u2206(r\u2032, s) = [\u03b4(r\u2032i, sj)]i,j such that, instead of \u03b4 being an Euclidean distance, we have:\n\u03b4(r\u2032i, sj) = DKL(r \u2032 i||gsj ), (6)\nwhere r\u2032i is the i th predicted tSPM, gsj is the spatialized representation of point sj as described in Section 3.1, and DKL is the Kullback-Leibler divergence (Eq. 1).\nThis formulation allows our model to be optimized to find an alignment that minimizes both the spatial and the temporal differences between each predicted scanpath and the ground-truth ones, therefore predicting scanpaths that follow a similar distribution as the ground truth. To our knowledge, we are the first to propose such a combination of metrics.\nBias Regularization Term Human gaze data in 2D images is known to be strongly biased towards the center of the images [9]. Although inherent to human nature, such bias hinders the learning process of the network, which can easily overfit to that behavior. Based on this, we include a regularization loss term that penalizes scanpaths whose points tend to stay in the center of the image for a long time, hence eliciting a more exploratory behavior that better reflects ground truth data. Our regularization term is included in the pairwise cost computations \u03b4(r\u2032i, sj) for the distance matrix \u2206, modifying Eq. 6 as follows:\n\u03b4(r\u2032i, sj) = DKL(r \u2032 i||gsj ) + \u03bbCB \u2217 LReg(r\u2032i) (7)\nLReg(r\u2032i) = 1\nDKL(r\u2032i||gc) , (8)\nwhere gc is a Gaussian map representing the aforementioned center bias, computed following the representation\nintroduced in Section 3.1 for a point c in the center of the image.\nIn order to set the relative weight of the regularization term \u03bbCB , we analyzed the datasets used (see Section 3.5), and found that this center bias behavior diminishes over time, with fixations being more widely spread over the image in later time instants. We measured the standard deviation of fixation positions in the ground-truth data, and found them to increase logarithmically over time (R2 = 0.855); we increase \u03bbCB in the same way (see Figure 5)."
        },
        {
            "heading": "3.4 Model Architecture",
            "text": "Our model features a recurrent neural network (RNN), which is able to extract and maintain temporal latent information from the scanpaths it is trained with. Particularly, we choose a convolutional long short-term memory (ConvLSTM) network [17], which is an adaptation of classic LSTMs to work with 2D data, such as images. This type of network has proven to be effective in many different problems, such as weather forecasting [17], video saliency detection [59], or medical image segmentation [60].\nConvLSTMs behave in a similar way to traditional LSTMs, working over four different gates; however, since they handle spatial data, they conduct convolutional operations rather than lineal ones. The ConvLSTM used in this work2 is defined as follows:\nit = \u03c3(Conv(xt;wxi) + Conv(ht\u22121;whi) + bi)\nft = \u03c3(Conv(xt;wxf ) + Conv(ht\u22121;whf ) + bf )\not = \u03c3(Conv(xt;wxo) + Conv(ht\u22121;who) + bo)\ngt = Tanh(Conv(xt;wxg) + Conv(ht\u22121;whg) + bg)\nct = ft ct\u22121 + it gt ht = ot Tanh(ct) (9)\n2. https://github.com/ndrplz/ConvLSTM_pytorch\nwhere xt is the input at a time step t, and ht is the hidden state of the network up to the current time step, which also serves as the output of the network. We refer the reader to the work of Xingjian et al. [17] for an in-depth explanation of the ConvLSTM architecture.\nIn our particular case of scanpath prediction, there is a degree of stochasticity driven by the inter- and intraobserver variability. As a result, given a particular trajectory (sequence of gaze fixation points), instead of predicting the next point in a deterministic manner, we predict a probability distribution (i.e., the previously introduced tSPM). Due to this, we combine the aforementioned ConvLSTM with the recently introduced Bayesian deep learning [61], [62]. Unlike traditional deep learning (DL), where the weights of the network are deterministic, in Bayesian DL the weights of a particular layer are sampled from a probability distribution, and thus the network itself can account for the inherent uncertainty of the data [62]. During the network training, those weight distributions are also optimized. Thus, in our case, we substitute each convolutional operation from Eq.9 with a Bayesian 2D convolution. This way, our ConvLSTM\nwill no longer be deterministic, and will be able to account for the stochastic component of scanpaths.\nInstead of feeding our Bayesian ConvLSTM with the raw images, we preprocess them to facilitate the learning process and enhance our model\u2019s performance. For this, we first extract the main image features with a pretrained VGG19 [63], [64], and a semantic segmentation mask with a pretrained ResNet50 [65]. We then convolve both of them together, to obtain a final, comprehensive, single-channel image feature representation. At each time step, this feature representation is fed to the ConvLSTM alongside with (i) the corresponding Gaussian map representing a fixation, and (ii) a CoordConv layer [51]. CoordConv layers have proven to ease spatial learning and facilitate network convergence. Different from previous approaches [1], [7], [11], we do not resort to saliency for scanpath prediction, since saliency is an aggregated spatial notion that has lost the temporal information, and is not usually available as ground truth.\nWith this input, our network is able to predict a tSPM (see Section 3.2). Then, to choose the actual pixel where the gaze point will fall, we follow a probabilistic weighted sam-\nTABLE 1 Results of our quantitative comparisons. We first include upper (human baseline, Human BL) and lower (random scanpaths, Random BL) baselines for reference. Then, we compare to Sun et al.\u2019s IOR-ROI [11], Itti et al. [1], and LeMeur et al. [7]. We also compute our set of metrics varying our probabilistic threshold th (see Section 3.4). Arrows indicate whether higher or lower is better; boldface highlights the best result for each metric. Overall our model (th = 0.7) yields the best performance across metrics, closest to the human baseline. Moreover, as the last two lines show, decreasing the value of our parameter th (thus leading to more variability in our scanpaths), still leads to good results (see Section 4.3 for details).\nString alignment Curve similarities Time-series analysis Recurrence analysis Model LEV\u2193 SCAM\u2191 HAU\u2193 FRE\u2193 fDTW\u2193 TDE\u2193 REC\u2191 DET\u2191 LAM\u2191 CORM\u2191\nHuman BL 10.77 (1.61) 0.38 (0.06) 95.97 (18.40) 140.02 (26.16) 550.84 (133.71) 42.40 (8.45) 6.69 (3.74) 1.72 (1.51) 6.09 (6.01) 22.11 (7.41) Random BL 12.31 (0.88) 0.20 (0.02) 148.01 (13.76) 199.30 (13.63) 877.15 (71.66) 69.87 (4.34) 0.73 (0.34) 0.02 (0.09) 0.19 (0.25) 3.79 (1.74)\nOurs 11.47 (1.13) 0.34 (0.06) 103.44 (27.13) 144.77 (32.77) 610.02 (155.96) 43.74 (10.25) 3.52 (2.86) 0.64 (0.84) 5.05 (4.96) 13.95 (7.92) IOR-ROI 13.26 (0.71) 0.30 (0.05) 115.50 (20.22) 166.07 (21.69) 777.75 (119.46) 46.98 (7.18) 1.80 (0.98) 0.18 (0.31) 0.81 (1.35) 10.28 (4.43) Itti et al. 14.04 (0.80) 0.23 (0.05) 160.09 (29.31) 207.97 (27.21) 1041.16 (153.97) 63.88 (9.54) 1.02 (1.98) 0.04 (0.22) 0.62 (2.03) 5.84 (6.00) LeMeur et al. 12.58 (0.78) 0.35 (0.04) 104.84 (12.79) 163.59 (20.52) 669.67 (108.49) 39.75 (6.53) 2.39 (1.18) 0.40 (0.48) 2.09 (2.26) 12.54 (4.45) Ours (th = 0.5) 11.60 (0.98) 0.33 (0.06) 103.97 (23.23) 149.37 (30.09) 636.08 (146.44) 45.46 (10.30) 3.01 (2.28) 0.50 (0.58) 3.14 (2.85) 12.96 (6.73) Ours (th = 0.35) 13.26 (0.71) 0.30 (0.05) 102.17 (19.77) 149.99 (28.42) 639.07 (138.27) 45.77 (9.63) 2.82 (2.09) 0.44 (0.54) 2.43 (2.59) 12.88 (6.21)\npling strategy, that again accounts for the stochastic nature of human visual exploration. We first discard all pixels with a probability lower than a threshold th = 0.7 (see Section 4.3 for additional evaluation on this), and then sample the next point based on the predicted map\u2019s probabilities. Once a point si = (xi, yi) has been sampled, a Gaussian map centered in (xi, yi) is again computed (see Section 3.1), and fed to the network for its posterior predictions, until the whole scanpath is predicted.\nOnce the whole scanpath has been predicted, our novel KL-DTW loss (see Section 3.3) is computed between the ground truth and the sequence of tSPM, and the network is then optimized. A complete overview of our model can be seen in Figure 3."
        },
        {
            "heading": "3.5 Datasets and Training Details",
            "text": "Following previous work [11], we train our model over the OSIE dataset [66], which contains 700 different images with their corresponding gaze information for a total of fifteen observers, yielding a total of approximately 10,500 scanpaths. We again follow previous work [11] and discard all the scanpaths with N < 4, and generate scanpaths of length N = 8, which is the mean length of our ground-truth data.\nFor the rest of the scanpaths, in order to train our model, we preprocess each to follow the representaton introduced in Section 3.1. To validate our model, and again inspired by previous approaches [11], we use the MIT low resolution dataset [67]. Please refer to Section 4 for further details.\nWe trained our model using the Hydra [68] and Pytorch Lightning [69] frameworks for PyTorch, logging and checkpointing all the necessary parameters to restore the training process at any point. We use the Adam optimization algorithm [70]. The learning rate has a value of 10\u22124, and we set batch size to 1. We trained our model on a Nvidia RTX 2080 Ti with 11GB of VRAM until convergence, for a total of 22 hours."
        },
        {
            "heading": "4 EVALUATION",
            "text": "We validate the quality of our scanpaths against measured, ground-truth scanpaths, as well as to other existing scanpath prediction methods. Similar to recent work on scanpath generation [6], we rely on the comprehensive set of metrics proposed by Fahimi and Bruce [15], which include string\nFig. 7. We evaluate the spatial convergence of our predicted scanpaths: Our model generates scanpaths that focus on salient regions, and whose aggregation closely resembles ground-truth saliency maps.\nalignment, curve similarity, time-series analysis, and recurrence analysis. We refer the reader to the original publication for further details on the metrics. In addition, we also analyze the performance of our method against ground-truth data for spatial convergence and saliency, inter-observer variability, and fixation prediction."
        },
        {
            "heading": "4.1 Comparison to Other Approaches",
            "text": "Following previous literature in scanpath prediction [11], we generate ten scanpaths per image for our test set with each of the methods we are comparing against: Itti et al.\u2019s\n[1], LeMeur et al.\u2019s work [9], and IOR-ROI [11]. Scanpath length is determined by the mean length of ground-truth data [11]. An illustrative qualitative comparison can be seen in Figure 6. Since some of the models [1], [11] are based on biological mechanisms such as inhibition of return, fixations do not remain in the same region, leading to unnatural scanpaths. Our model and LeMeur et al.\u2019s [9] produce scanpaths that more closely resemble the ground truth. However, our work does not depend on saliency as a proxy, and does not require a module devoted to its prediction. This makes it more general and suitable for data for which ground-truth saliency is not available.\nTable 1 shows the comparisons with quantitative metrics [15]. For reference, we also include human baseline (Human BL) [29] by computing the same metrics for all the ground-truth scanpaths, plus a random baseline (Random BL) generated from random scanpaths. Our models yields the best results in eight of the ten cases, and second in the remaining two.\nAdditional qualitative results can be found in the supplementary material."
        },
        {
            "heading": "4.2 Spatial convergence and saliency",
            "text": "To evaluate the spatial convergence of our predicted scanpaths, we compare saliency maps. We compute such maps by aggregating multiple scanpaths into a heatmap; we then compare them against the ground-truth saliency maps computed from real observers\u2019 data. As can be seen in Figure 7, our generated scanpaths lead to predicted saliency maps that closely resemble the ground truth."
        },
        {
            "heading": "4.3 Scanpath variability",
            "text": "We generate our scanpaths by sampling our generated tSPM (Section 3). The inherent variability that exists between different observers is modeled by the parameter th: higher values lead to more concentrated scanpaths, while lower ones allow our scanpaths to simulate more exploratory visual behaviors. Figure 8 illustrates this. In addition, we\nhave conducted a quantitative analysis (see last two rows of Table 1) showing how, even when eliciting a more exploratory behavior by decreasing th, our scanpaths still outperform previous approaches and remain close to the human baseline."
        },
        {
            "heading": "4.4 Step-wise fixation prediction",
            "text": "As mentioned in Section 2, most existing works take an incomplete scanpath as input, and predict the next fixation point. They thus build each scanpath progressively, usually by optimizing only the prediction of that last point (e.g., by means of MAE [45] or MSE [14] losses). Although this approach neglects the plausibility of the full scanpath as a whole, it may be useful in some cases. Our proposed spatio-temporal loss and probabilistic framework also offer a precise alternative in these situations. Table 2 shows quantitative results for paths of varying lengths: # represents points from the ground-truth scanpath fed to our network, while \u00d7 represents points predicted with our model. Our method produces plausible results from a single groundtruth point, and very quickly approximates the human baseline with only four."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We have presented a novel method for scanpath prediction in 2D images. We introduce a novel spatial scanpath representation that enhances learning the spatial features of images, together with a novel loss function tailored to the spatio-temporal particularities of scanpaths, based on a combination of dynamic time warping and Kullback-Leibler divergence. This allows our model to predict scanpaths that mimic human viewing patterns. We have evaluated our model and compared it to state-of-the-art methods on a large set of metrics that analyze different aspects of scanpaths. Our model outperforms previous approaches, while generating a scanpath in less than a second.\nLimitations and future work Our work is not free from limitations, and offers interesting avenues of future work. When the visual features of the image are too abstract or complex, the performance of our model decreases (see Figure 9). We computed the same set of metrics as in Section 4 and found that, for some particular complex cases, our metrics are closer to the random baseline than to the human baseline. We hypothesize that using a larger dataset and more ground-truth data would ameliorate this. In addition, adding more priors may be helpful, although finding out what priors would apply to the most complex cases is still an open problem.\nAdditionally, exploring the impact of the duration of the fixations could further enhance our model\u2019s performance. Last, our model assumes no prior knowledge or task-oriented scenarios when viewing the images; it would be interesting to devise variations of our model for such particular cases."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (project CHAMELEON, Grant No 682080). This work has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No 956585. This project was also supported\nby a 2020 Leonardo Grant for Researchers and Cultural Creators, BBVA Foundation (the BBVA Foundation accepts no responsibility for the opinions, statements and contents included in the project and/or the results thereof, which are entirely the responsibility of the authors). This work has also received funding from Spain\u2019s Agencia Estatal de Investigaci\u00f3n (project PID2019-105004GB-I00). Additionally, Daniel Martin was supported by a Gobierno de Aragon (2020-2024) predoctoral grant."
        },
        {
            "heading": "5.1 Effect of image complexity",
            "text": "As commented in the main document, the complexity of the image (i.e., the amount of regions of interest (ROI), or their absence) may affect the performance of our model. Thus, we show some particular examples of the behavior of our model in different circumstances, namely:\n\u2022 One ROI: Figures 10, 11, and 12. \u2022 Two ROI: Figures 13, 14, and 15. \u2022 Multiple ROI: Figures 16, and 17. \u2022 No clear ROI: Figures 18, and 19.\nwhile including the rest of the figures for completeness. When few ROI are present, our model is able to correctly\nfocus on them. However, when the image contains too many of them, our model is not always able to recognize them all, and focuses in a subset of them. When the image presents no ROI at all, our scanpaths become more erratic, as no clear interest zone can be focused."
        }
    ],
    "title": "A Probabilistic Time-Evolving Approach to Scanpath Prediction",
    "year": 2022
}