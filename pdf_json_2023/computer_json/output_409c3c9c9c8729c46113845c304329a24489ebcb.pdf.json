{
    "abstractText": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \u201cbeliefs\u201d. This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nora Kassner"
        },
        {
            "affiliations": [],
            "name": "Oyvind Tafjord"
        },
        {
            "affiliations": [],
            "name": "Ashish Sabharwal"
        },
        {
            "affiliations": [],
            "name": "Kyle Richardson"
        },
        {
            "affiliations": [],
            "name": "Hinrich Sch\u00fctze"
        },
        {
            "affiliations": [],
            "name": "Peter Clark"
        }
    ],
    "id": "SP:166922d46d90d6010d543cb6103245295e1f2231",
    "references": [
        {
            "authors": [
                "Bhavana Dalvi",
                "Peter Alexander Jansen",
                "Oyvind Tafjord",
                "Zhengnan Xie",
                "Hannah Smith",
                "Leighanna Pipatanangkura",
                "Peter Clark."
            ],
            "title": "Explaining answers with entailment trees",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Joe Davison",
                "Joshua Feldman",
                "Alexander Rush."
            ],
            "title": "Commonsense knowledge mining from pretrained models",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Kelvin Guu",
                "Percy Liang."
            ],
            "title": "Transforming question answering datasets into natural language inference datasets",
            "venue": "ArXiv, abs/1809.02922.",
            "year": 2018
        },
        {
            "authors": [
                "Yanai Elazar",
                "Nora Kassner",
                "Shauli Ravfogel",
                "Abhilasha Ravichander",
                "E. Hovy",
                "Hinrich Sch\u00fctze",
                "Yoav Goldberg."
            ],
            "title": "Measuring and improving consistency in pretrained language models",
            "venue": "TACL, 9.",
            "year": 2021
        },
        {
            "authors": [
                "Allyson Ettinger."
            ],
            "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "venue": "TACL, 8:34\u201348.",
            "year": 2020
        },
        {
            "authors": [
                "Yuling Gu",
                "Bhavana Dalvi",
                "Peter Clark"
            ],
            "title": "Do language models have coherent mental models of everyday things",
            "year": 2023
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Ignatiev."
            ],
            "title": "RC2: an efficient MaxSAT solver",
            "venue": "J. Satisf. Boolean Model. Comput., 11.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? TACL",
            "year": 2020
        },
        {
            "authors": [
                "Jaehun Jung",
                "Lianhui Qin",
                "Sean Welleck",
                "Faeze Brahman",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin Mann",
                "Sam McCandlish",
                "Christopher Olah",
                "Jared Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "ArXiv, abs/2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Nora Kassner",
                "H. Sch\u00fctze."
            ],
            "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Nora Kassner",
                "Oyvind Tafjord",
                "Hinrich Schutze",
                "Peter Clark."
            ],
            "title": "BeliefBank: Adding memory to a pre-trained language model for a systematic notion of belief",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Tao Li",
                "Vivek Gupta",
                "Maitrey Mehta",
                "Vivek Srikumar."
            ],
            "title": "A logic-driven framework for consistency of neural models",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Hans-Andrea Loeliger."
            ],
            "title": "An introduction to factor graphs",
            "venue": "IEEE Signal Processing Magazine, 21:28\u201341. https://people.binf.ku.dk/\u223cthamelry/MLSB08/hal.pdf.",
            "year": 2004
        },
        {
            "authors": [
                "Qing Lyu",
                "Marianna Apidianaki",
                "Chris CallisonBurch"
            ],
            "title": "Towards faithful model explanation in NLP: A survey. ArXiv, abs/2209.11326",
            "year": 2022
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? A new dataset for open book question answering",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Eric Mitchell",
                "Joseph J. Noh",
                "Siyan Li",
                "William S. Armstrong",
                "Ananth Agarwal",
                "Patrick Liu",
                "Chelsea Finn",
                "Christopher D. Manning."
            ],
            "title": "Enhancing self-consistency and performance of pre-trained language models through natural language inference",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "ChatGPT: Optimizing language models for dialog",
            "venue": "Technical report, openai.com. https://openai.com/blog/chatgpt/.",
            "year": 2022
        },
        {
            "authors": [
                "James D Park."
            ],
            "title": "Using weighted MAX-SAT engines to solve MPE",
            "venue": "AAAI/IAAI.",
            "year": 2002
        },
        {
            "authors": [
                "Matthew E Peters",
                "Mark Neumann",
                "Robert Logan",
                "Roy Schwartz",
                "Vidur Joshi",
                "Sameer Singh",
                "Noah A Smith."
            ],
            "title": "Knowledge enhanced contextual word representations",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Fabio Petroni",
                "Patrick Lewis",
                "Aleksandra Piktus",
                "Tim Rockt\u00e4schel",
                "Yuxiang Wu",
                "Alexander H. Miller",
                "Sebastian Riedel."
            ],
            "title": "How context affects language models\u2019 factual predictions",
            "venue": "Automated Knowledge Base Construction.",
            "year": 2020
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases? In EMNLP",
            "year": 2019
        },
        {
            "authors": [
                "Abhilasha Ravichander",
                "Eduard Hovy",
                "Kaheer Suleman",
                "Adam Trischler",
                "Jackie Chi Kit Cheung."
            ],
            "title": "On the systematicity of probing contextualized word representations: The case of hypernymy in BERT",
            "venue": "Proceedings of the Ninth Joint Conference on Lexical",
            "year": 2020
        },
        {
            "authors": [
                "Kyle Richardson",
                "Ronen Tamari",
                "Oren Sultan",
                "Reut Tsarfaty",
                "Dafna Shahaf",
                "Ashish Sabharwal."
            ],
            "title": "Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "year": 2020
        },
        {
            "authors": [
                "Tian Sang",
                "Paul Beame",
                "Henry A Kautz."
            ],
            "title": "A dynamic approach for MPE and weighted MAX-SAT",
            "venue": "IJCAI.",
            "year": 2007
        },
        {
            "authors": [
                "Eric Schwitzgebel."
            ],
            "title": "Belief",
            "venue": "Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/belief/.",
            "year": 2019
        },
        {
            "authors": [
                "Sanjay Subramanian",
                "Ben Bogin",
                "Nitish Gupta",
                "Tomer Wolfson",
                "Sameer Singh",
                "Jonathan Berant",
                "Matt Gardner."
            ],
            "title": "Obtaining faithful interpretations from compositional neural networks",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi",
                "Peter Clark."
            ],
            "title": "Entailer: Answering questions with faithful and truthful chains of reasoning",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Matt Gardner",
                "Kevin Lin",
                "Peter Clark."
            ],
            "title": "QuaRTz: An open-domain dataset of qualitative relationship questions",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel Weir",
                "Benjamin Van Durme."
            ],
            "title": "Dynamic generation of interpretable inference rules in a neuro-symbolic expert system",
            "venue": "ArXiv, abs/2209.07662.",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovi\u0107."
            ],
            "title": "Teach me to explain: A review of datasets for explainable natural language processing",
            "venue": "NeurIPS Datasets and Benchmarks.",
            "year": 2021
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Scharli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Huai hsin Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "ICLR.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "While large language models (LLMs) are impressive at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \u201cbeliefs\u201d1 about the world, or whether the LLM even has a coherent internal belief system. This general opacity is a growing impediment to widespread use of LLMs, e.g., in critical applications such as medicine, law, and hiring decisions,\n1 We adopt a simple definition of belief, namely that a model believes X if it answers \"yes\" to the question \"Is X true?\". Other definitions could also be used; see Section 2.\nwhere properties of explainability, interpretability, and trust are paramount. Our goal is to help alleviate such opacity by constructing an explicit representation of system beliefs and their inferential relationships (including to answer candidates), so that answers are supported by interpretable chains of reasoning. These constructed belief graphs, e.g., Figures 1 and 2, form a rational layer above the LLM explaining how answers follow from beliefs, and provide a window into some of the latent contents of the model, potentially helping users\nar X\niv :2\n30 5.\n14 25\n0v 2\n[ cs\n.C L\n] 2\n9 O\nct 2\n02 3\nunderstand and trust model answers.\nIn addition, when we do this, we find such graphs expose latent inconsistencies in the model\u2019s beliefs. We show how such inconsistencies can be resolved using constraint satisfaction techniques. When we do this, the rational layer becomes not just a window onto the model, but an active reasoning component in its own right in a larger, overall system, comprising the (frozen) LLM plus rational layer (blue box, Figure 1). We show this results in a more consistent set of beliefs in the overall system, without harming overall answer accuracy (although some individual answers may change). The result is answers supported by faithful, system-believed chains of reasoning drawn from a consistent belief system.\nOur approach, called REFLEX, introduces a rational layer consisting of two parts. First, to produce a belief graph, we recursively ask the LLM to explain why each candidate answer might be true, expressed as a set of sentences that entail the answer. This builds on earlier work on generating entailment-based and chain-of-thought explanations (Tafjord et al., 2022; Weir and Durme, 2022; Wei et al., 2022). We then add a self-verification step to check that the model itself believes those generations (i.e., that the model believes what it says), allowing us to identify sentences reflecting the model\u2019s own internal knowledge. For example, in Figure 1, when asked to explain S1 (\u201cgiraffes give live birth\u201d), the model generates S7 ([because] \u201cmammals give live birth\u201d) and S4 ([and] \u201ca giraffe is a mammal\u201d). Self-querying then checks if the model actually believes its generations (\u201cDo mammals give live birth?\u201d). The answer (\u201cyes\u201d/\u201dno\u201d) assigns a true/false (T/F) value to each generation, indicated in Figure 1 by white/grey nodes. This procedure is then applied recursively to the generated, supporting sentences. The resulting network of model beliefs and their dependencies provides a a window into the model.\nSecond, we apply a formal constraint reasoner to this graph to resolve inconsistencies, by finding the optimal (minimal cost, Section 3.3) way of flipping T/F values. For example, on the left in Figure 1, S2 and S3 (\u201cspiders do/don\u2019t give live birth\u201d) are in an XOR relationship (i.e., exactly one must be false), but both are believed as true (white) by the LLM - a latent contradiction within the LLM. Constraint reasoning then seeks to remove such inconsistencies, here flipping the belief\nvalue on S2 from T to F (Figure 1, right), repairing the contradiction. This builds on earlier techniques (Kassner et al., 2021; Mitchell et al., 2022; Jung et al., 2022), though in a notably richer setting with over 350 nodes and 80 constraints per question, joint inference across answer candidates, and a variety of constraint types. The overall result is a fully autonomous, self-reflective system that is able to deliberate (and if necessary change) its answers, thereby resolving latent inconsistencies that would otherwise go unnoticed, and provide faithful explanations drawn from a consistent belief system.\nWe evaluate our implementation of REFLEX on three datasets: EntailmentBank (Dalvi et al., 2021), OBQA (Mihaylov et al., 2018), and QuaRTz (Tafjord et al., 2019). We find that REFLEX is able to construct belief graphs with significantly improved consistency (by 8%-11% absolute) without harming overall answer accuracy. In addition, answers are now supported by a more consistent, system-believed chain of reasoning, providing a window into the previously latent beliefs of the model. Our contributions are thus:\n1. A new style of system architecture in which an LLM is extended with a rational layer in which an explicit representation of system beliefs and relationships is constructed and which can be reasoned over. This layer provides an interpretable window into system beliefs, adds a systematic reasoning capablity, and allows latent inconsistencies present in the LLM to be repaired. 2. An implementation of this architecture demonstrating that the consistency of the overall system\u2019s network of beliefs can be significantly improved without harming answer accuracy. Answers are now supported by explicit, interpretable chains of reasoning drawn from a more consistent network of beliefs."
        },
        {
            "heading": "2 Related Work",
            "text": "Materializing a Model\u2019s Internal Knowledge: It is now well recognized that LLMs contain extensive world knowledge (Petroni et al., 2019, 2020; Davison et al., 2019; Peters et al., 2019; Jiang et al., 2020; Roberts et al., 2020) that somehow enables them to perform well. Recent work has attempted to expose that knowledge in various ways, both to justify answers and improve performance, and our work falls into this genre. Standard explanation generation methods (Wiegreffe and Marasovic\u0301, 2021) can produce compelling explanations, but\nwith no guarantee that the generated sequence of tokens expresses the model\u2019s internal knowledge, nor entails the actual answer. Similarly, chain-ofthought (CoT) (Wei et al., 2022) and Least-to-Most (Zhou et al., 2023) prompting generate (in different ways) a step-by-step reasoning chain along with an answer, but again with no claim that the chain reflects the model\u2019s internal knowledge nor is valid reasoning (Subramanian et al., 2020).\nTo add semantics to generations, several systems have used self-querying to verify that generations\nreflect model-believed facts (by self-querying \u201cIs p true?\u201d) (e.g., Kassner et al., 2021; Jung et al., 2022), or model-believed rules (by self-querying \u201cDoes p imply q?\u201d) (e.g., Tafjord et al., 2022). We build on these to construct a belief graph, namely a network of model-believed facts and their inferential relationships, which can then be reflected on.\nBeliefs: We refer to the model\u2019s factual opinions as \u201cbeliefs\u201d rather than \u201cknowledge\u201d because those opinions may be wrong. In general, an agent\ncan be said to believe p if it acts as if p was true (Schwitzgebel, 2019). Following Kassner et al. (2021) and Richardson et al. (2022), we take a simple, syntactic operationalization of this, namely the agent answers \u201cyes\u201d to the question \u201cp?\u201d, but also note that more semantic versions could be used, e.g., the agent also answers \u201cyes\u201d to paraphrases and implications of p.\nReducing Inconsistency: LLMs are known to be inconsistent in their answers (Ettinger, 2020; Kassner and Sch\u00fctze, 2020; Davison et al., 2019; Ravichander et al., 2020; Elazar et al., 2021; Subramanian et al., 2020; Gu et al., 2023), and several recent works have used constraint reasoners to identify and reduce inconsistency. BeliefBank used a MaxSAT solver to resolve inconsistencies between model beliefs, but required a hand-provided set of constraint rules (Kassner et al., 2021). ConCoRD (Mitchell et al., 2022) similarly used MaxSAT to ensure model answers were consistent with NLIderived entailment constraints between them, but did not introduce additional model-believed facts and rules. Maieutic Prompting (Jung et al., 2022) also used MaxSAT to resolve inconsistencies between facts in prompt-induced explanation chains. However, those chains were not validated as reflecting model-believed constraint rules2, and did not support conjunction. REFLEX extends these reasoning chains to provide a full semantic account of how answers are supported by the model\u2019s internal knowledge. Additionally, it performs joint reasoning across answer candidates and operates at a much larger scale (e.g., over 350 nodes on average for each question) and with a variety of constraint types."
        },
        {
            "heading": "3 REFLEX: Our Approach",
            "text": ""
        },
        {
            "heading": "3.1 Belief Graphs",
            "text": "Our belief graphs are defined over a set of natural language true/false statements and represent a set of rules that constrain the truth values of these statements. We refer to statements that are factually true in the world as facts. The truth value assigned by a model M to a statement is referred to as M \u2019s belief in that statement (cf. Footnote 1). A model\u2019s internal beliefs may not always align\n2REFLEX checks whether both the statements si, and the rules (si \u2192 h), are believed by the model via self-querying, e.g., by asking \u201cDoes si \u2192 h?\u201d, and also scores the strength of those beliefs. In maieutic prompting, the generated rules are not checked against the model, resulting in rules that the model itself may not believe, if queried about them.\nwith facts. Our goal is to extract a model\u2019s initial beliefs about statements inferentially related to all top-level hypotheses of interest, and perform reasoning to update these beliefs so as to make them more consistent with respect to the rules, and ideally also factually more accurate.\nA belief graph is a type of factor graph commonly used in the probabilistic inference literature (Loeliger, 2004). Formally, it is defined as an undirected graph G = (N,E) with nodes N and edges E. Nodes are of two types: A statement node (referred to as a \"variable node\" in a factor graph) is a triple (s, l, cs) containing a natural language statement s, an associated value l \u2208 {T, F} initially denoting M \u2019s belief that s is true or false, and a confidence cs \u2208 [0, 1] denoting a confidence in that label. A rule node (referred to as a \"factor node\" in a factor graph) is a pair (r, cr) denoting a disjunctive rule or constraint over statements, with confidence cr. It takes the form r = (\u2212s1 \u2228 . . . \u2228 \u2212s\u2113 \u2228 s\u2113+1 \u2228 . . . \u2228 sk). For ease of interpretation, we view this constraint as r = p \u2192 h where p = s1\u2227 . . .\u2227s\u2113 is a conjunctive premise and h = s\u2113+1 \u2228 . . . \u2228 sk is a disjunctive hypothesis. The rule says that if p is true, so must be h; and the contrapositive of this.\nEdges E connect rule nodes to the statements they constrain, denoting their dependence. For legibility, we draw edges directionally to depict the way the rule reads: the statements in p point to r, which in turn points to h. Mathematically, the influence is bidirectional and the depicted directionality is irrelevant during reasoning (Section 3.3), just as in a standard factor graph.\nWe adopt the standard probabilistic semantics of factor graphs, thereby associating a belief graph with a well-defined probability distribution over any set of statement beliefs. For a statement node (s, l, cs), the cost costs for setting it to l is 0, and that for setting it against l is cs; the corresponding weight of this node is ws = exp(\u2212costs). Costs and weights for a rule node (r, cr) are defined similarly, based on whether the beliefs satisfy r or not. Finally, the overall weight of a T/F assignment to all statements is \u220f sws \u00b7 \u220f r wr, which, when normalized by the total weight across all possible assignments, yields a probability distribution over such assignments. We will be interested in finding the most consistent set of beliefs, i.e., a T/F assignment to statements with the minimum overall weight, which is equivalent to minimizing\n\u2211 s costs + \u2211 r costr. This is referred to as the MPE (most probable explanation) problem in the graphical models literature, which we later solve exactly using a MaxSAT constraint solver based on a standard translation of MPE into weighted MaxSAT (Park, 2002; Sang et al., 2007)."
        },
        {
            "heading": "3.2 Constructing Belief Graphs",
            "text": "Given an initial node (statement) s, a belief graph G is produced by a backward-chaining process described below, in which G is recursively expanded to add statements that together may entail s."
        },
        {
            "heading": "3.2.1 Basic Operations",
            "text": "Let h denote a hypothesis (language statement s) of interest and p a premise\u2014a set of statements {s1,. . . ,sn} that together may entail h. Given these, there are three basic operations required to generate belief graphs: 1. h \u21d2 p: Given h, generate a p that may entail h. 2. s \u21d2 (l, cs): Given a statement s, output a\ntrue/false value l and a confidence in the belief that s has truth value l (as assessed via yes/no question-answering).\n3. (p, h) \u21d2 cr: Given p and h, output a confidence that the candidate rule r = p \u2192 h holds.\nThe most important of these is the first operation, in which the model self-generates conjunctive rules concluding h (i.e., reason p for believing h), thus adding new nodes to the graph.\nThere are several ways of implementing these basic functions, and our algorithm is agnostic to the method used. In our work here, we use Entailer, an off-the-shelf T5-11B trained model with these functionalities (Tafjord et al., 2022). Further, since the raw score produced by the model tends to be skewed towards 0 or 1, when computing cs and cr in practice, we re-scale the raw model score using a set of hyperparameters (cf. Appendix B).\nOne may use alternative ways to implement these operators, such as chain-of-thought prompting a model like GPT3 (Wei et al., 2022) or ChatGPT (OpenAI, 2022). For example, to generate a rule concluding a hypothesis h such as \u201cPlants require CO2 to make their food.\u201d, the model could be prompted with h followed by \u201cExplain the last statement with a 2-step reasoning chain.\u201d, the numbered generations forming the premise p. Similarly, generated statements and rules can be validated as reflecting the model\u2019s beliefs by self-querying (\u201cIs s true?\u201d, \u201cDoes p imply h?\u201d), and then using the generated yes/no answer token probabilities as the\nAlgorithm 1 The recursive algorithm for constructing a belief graph of max depth dmax for a hypothesis set H. The subroutine EXTEND-GRAPH takes a partial graph G as an input and extends it in place with one statement and its subgraph.\n1: procedure GENERATE-GRAPH(hypotheses H, max depth dmax): 2: let G = empty graph 3: foreach h \u2208 H 4: call EXTEND-GRAPH(h, 0, dmax, G) 5: add MC rule node (\u2228 h\u2208H h,\u221e ) to G 6: foreach pair (hi, hj) of hypotheses in H 7: add MC rule node (\u00achi \u2228 \u00achj , cmc) to G 8: return G\n9: procedure EXTEND-GRAPH(statement s, current depth d, max depth dmax, partial graph G):\n10: call operator s \u21d2 (l, cs) to score statement s 11: add statement node (s, l, cs) to G 12: gen. the negation sentence negs = neg(s) 13: add rule node (XOR(s,negs), cxor) to G 14: call EXTEND-GRAPH(negs, d+ 1, dmax, G) 15: if d < dmax do: 16: let h = s 17: call operator h \u21d2 p to generate p 18: call operator (p, h) \u21d2 cr to score rule p \u2192 h 19: add rule node (p \u2192 h, cr) to G 20: foreach si \u2208 p 21: call EXTEND-GRAPH(si, d+ 1, dmax, G)\nmodel\u2019s confidence (Kadavath et al., 2022)."
        },
        {
            "heading": "3.2.2 Initial Hypothesis Generation",
            "text": "Given a question, we first generate a set H of hypothesis sentences (e.g., \u201cIs the sky (A) blue (B) yellow\u201d \u2192 { h1 = \u201cThe sky is blue.\u201d, h2 = \u201cThe sky is yellow.\u201d).3 An N -way multiple choice question yields N hypotheses in H. A true/false question yields 2 hypotheses. To handle open-ended questions, candidate answers can be generated, e.g., using nucleus sampling (Holtzman et al., 2019)."
        },
        {
            "heading": "3.2.3 Belief Graph Generation",
            "text": "The belief graph generation process is shown in Algorithm 1. An example of (part of) a generated belief graph is shown in Figure 2.\nGiven a set H of hypotheses, we generate a single belief graph G by using our basic operations (Section 3.2.1) to recursively generate rules that conclude each hi \u2208 H up to a fixed maximum depth dmax. (Each original hi is at depth d = 0.)\nFor each statement s, we also generate nodes negs (and their recursive subgraphs) expressing its negation, e.g., \u201cThe sky is not blue.\u201d from \u201cThe\n3Conversion of a QA pair to a declarative hypothesis D uses a custom T5-11B model trained on the QA2D dataset (Demszky et al., 2018).\nsky is blue.\u201d.4 Each pair s and negs is connected with an XOR rule, indicating a (soft) preference for setting exactly one of them to true; this is represented as two disjunctive constraints (s\u2228negs) and (\u2212s \u2228 \u2212negs) whose weight cxor is a fixed hyperparameter. Lastly, we add a multiple-choice (MC) constraint which has two parts: a hard constraint (with infinite cost) that at least one hypothesis must be chosen, and a soft constraint5 that no more than one should be chosen. The soft constraint is associated with a fixed hyperparameter weight cmc."
        },
        {
            "heading": "3.3 Reasoning Over Belief Graphs",
            "text": "Belief graphs provide a window into the model\u2019s beliefs about some of the relevant statements and their (believed) inferential relationships to candidate answers to a question. As others have shown (Kassner et al., 2021; Mitchell et al., 2022), such beliefs can be inconsistent, and materializing those inconsistencies provides one the opportunity to remove or reduce them.\nIn a similar vein, and as discussed in Section 3.1, REFLEX performs inference over belief graphs in order to compute an updated set of beliefs that is as consistent as possible with the rules. To this end, it converts belief graphs into an equivalent weighted MaxSAT problem and uses an off-theshelf MaxSAT solver (RC2, (Ignatiev, 2019)) to compute the optimal flips of initial true/false beliefs that minimize global inconsistency. It then discards all rules that are in conflict with the updated statement beliefs, obtaining a smaller, updated belief graph. This smaller belief graph produced by REFLEX is self-consistent and provides inferential support for the top-level hypotheses."
        },
        {
            "heading": "3.4 Generating Faithful Explanations",
            "text": "Notably, the smaller updated belief graph produced by REFLEX provides a faithful explanation of the answer it predicts, in the sense that it accurately represents the reasoning process behind the overall system\u2019s prediction (Lyu et al., 2022). This is true as the MaxSAT reasoning process results precisely in a self-consistent set of beliefs from which REFLEX determines whether to believe a candidate answer or not, and produces its final prediction based on this (rather than on the raw LLM output alone; note that we do not make any claims about\n4We use a simple, custom-built utility for this, namely a T5-base model trained on 9k Turk-generated examples.\n5soft, to allow for cases with multiple valid answers, e.g., open-ended questions or those asking for the best answer.\nhow the internal reasoning of the LLM component operates.) Thus, REFLEX provides the user with an interpretable reasoning trace, allowing the user to understand how it derived the answer from more rudimentary facts (Subramanian et al., 2020).\nWe note that the original belief graph (before reasoning) may reveal that the model\u2019s original explanation is, in fact, not faithful to its own beliefs. For example, in Figure 2, the model believes statements 6, 7, and that 6 & 7 entail 2, but does not believe 2 (colored grey). Thus, the global reasoning layer of REFLEX plays a critical role in arriving at faithful explanations."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "The goal of our experiments is to evaluate the extent to which our overall system, namely an LLM plus a self-reflecting, rational layer, helps expose and resolve inconsistencies in the LLM\u2019s beliefs without harming accuracy. Importantly, REFLEX is evaluated in a zero-shot setting, without relying on training instances of the target datasets.\nDatasets. We use the test partitions of three existing multiple-choice datasets: EntailmentBank (Dalvi et al., 2021), OBQA (Mihaylov et al., 2018), and QuaRTz (Tafjord et al., 2019). We chose our datasets as they contain inferentially rich questions (typically) requiring reasoning. The partitions contain 339, 500, and 784 examples, respectively.\nModels. The baseline LLM we use is an LLM that has been trained to perform QA and also supports the basic operations discussed in Sec. 3.2.1, enabling us to assess how much it can be improved by adding a REFLEX layer. To this end, we use a publicly available, frozen, off-the-shelf T5-11B LLM called Entailer (Tafjord et al., 2022). To answer an MC question with this LLM, we score each answer hypothesis (cs, Section 3.2.1) and select the one with the highest truth confidence. If Entailer assigns false values to all answer choices, we select the hypothesis with the lowest false confidence.\nREFLEX then adds a rational layer to this LLM, creating a new system that is also able to self-reflect and modify its beliefs. To ensure the different belief graph scores in REFLEX are appropriately calibrated, we use nine hyperparameters, tuned once on the dev partition of EntailmentBank (Dalvi et al., 2021) and then kept fixed for all experiments. Details are in Appendix B. Note the LLM itself remains frozen, with belief revision occurring in the\nrational (belief graph) layer above it.\nMetrics. For measuring self-consistency, we follow Li et al. (2019) and report the conditional constraint violation (\u03c4 ) metric, defined as follows: the fraction of rules whose premises p are believed true, but whose hypothesis h is not. In other words, over all rules of the form p \u2192 h, \u03c4 is:\n\u03c4 = |{p \u2192 h | p = T, h = F}|\n|{p \u2192 h | p = T}|\nwhere s = T denotes the system believes statement s to be true (similarly for s = F ). The numerator of \u03c4 thus captures the number of constraints the system violates. The denominator captures the number of applicable constraints. We then report the following metric: consistency = 1 - \u03c4 .\nFor QA performance, we report standard multiple-choice accuracy: 1 point for predicting the correct answer, 1/N points for predicting N answers including the correct one, 1/k points for no prediction (k = # answer options), 0 otherwise."
        },
        {
            "heading": "4.1 Results",
            "text": "Consistency. Table 1 shows consistency results on the test partitions of our datasets. We observe significant consistency gains (by 8%-11% absolute), showing REFLEX\u2019s effectiveness at creating a consistent belief network within the overall system.\nAccuracy. Table 2 shows overall performance on our three datasets (test partitions). As can be seen, we observe stable accuracy, as well as the answers now being faithful to the reasoning chains in the belief graph. This is significant, as it allows users to understand how answers follow from system beliefs (and in cases where an LLM belief was flipped, why that belief is untenable in the broader system).\nAblations. To study the impact of the three different types of rules on consistency improvement, we using the EntilmentBank dataset (dev partition).\nTo do this, given the belief graph for a question, we mask out (separately, rather than cumulatively) each type of rule in turn when providing the graph to the MaxSAT solver. We then run the constraint solver and measure the resulting self-consistency of beliefs on the original graph.\nThe results are shown in Table 3 (the MC rule is the constraint that exactly one multiple-choice option should be chosen, Section 3.2.3). The results indicate that all three types of rules contribute to the system\u2019s consistency improvements."
        },
        {
            "heading": "4.2 Success Analysis",
            "text": "We identify three classes of successful reasoning by the constraint reasoner: (a) latent model beliefs correct an initially wrong answer (Figure 3); (b) the system corrects an initially erroneous, latent model belief (Figure 4); and (c) strong model beliefs identify and reject a bad rule (Figure 5). These types of system corrections help to improve accuracy and produce answers supported by valid chains of reasoning, allowing users insight into why an answer follows from the model\u2019s knowledge."
        },
        {
            "heading": "4.3 Failure Analysis",
            "text": "Reasoning can also make mistakes. From a manual analysis of 50 random questions from EntailmentBank that REFLEX answered incorrectly, we identified five main causes of failure and their approximate frequency (Note that multiple categories can apply, hence total is > 100%):\n1. Missing Rules (\u224830%): In some cases, the system generates irrelevant rules but misses an important one needed to support the correct answer, resulting in incorrect conclusions. While somewhat subjective, this is a notable error category that we observe. For example for the question:\nA human cannot survive the loss of (A) The liver [correct] (B) A lung (C) A kidney\nthe system incorrectly concludes (B) is true, ignoring the commonsense rule that with two lungs, a person can survive without one of them.\n2. Incorrect Beliefs (\u224830%): Sometimes the reasoner fails to correct incorrect model beliefs, either because the model\u2019s confidence is high or evidence against them is weak or missing. In the example shown in Figure 7, the model\u2019s strong, incorrect beliefs that \u201criver deltas are reservoirs\u201d and \u201creservoirs always provide freshwater\u201d (untrue of oceans, say) causes it to incorrectly conclude that \u201cdeltas are freshwater reservoirs\u201d.\n3. Incorrect Rules (\u224810%): Rule generation can produce bad rules, e.g., in Figure 5), and in some cases the constraint reasoner fails to reject them if they are strongly believed. In particular, confusion or ambiguity over quantifiers can result in bad rules, e.g., (emphasis added) \u201cSome animals catch their prey with trickery.\u201d & \u201cA spider is a kind of animal.\u201d \u2192 \u201cSpiders catch their prey with trickery.\u201d. Similarly the model generates the fallacy: \u201cSome people don\u2019t mind not moving for an hour\u201d & \u201cbreathing is a kind of movement\u201d \u2192 \u201cSome\npeople don\u2019t mind not breathing for an hour.\u201d\n4. Ambiguous Statements, Unexpected Reasoning (\u224810%): A common cause of error is the surprising ambiguity of belief statements, which can often be read in multiple ways. In several cases, the model adopts a valid but unexpected interpretation, resulting in \u201cerrors\u201d compared to the gold answer label. For example, in Figure 6, the model takes the word \u201calways\u201d in a literal sense (\u201cglaciers will not always be there\u201d), resulting in an answer that differs from the gold label. Developing ways to attach context to these statements to help disambiguate them would help alleviate such errors."
        },
        {
            "heading": "5. Multiple Valid Answers (\u224810%): A final",
            "text": "cause of \u201cerror\u201d - at least with respect to the gold label - is that multiple answers may be valid, and\nthe question is asking for the best answer; eg. for \u201cWhat could fill a beach ball? (A) Oxygen (B) Water ...\u201d, A is labeled correct, while B is also a valid answer. REFLEX (desirably) finds valid reasoning chains for both, but the notion of highest-scoring proof does not fully correlate with the notion of \u201cbest answer\u201d intended by the question author."
        },
        {
            "heading": "5 Future Work",
            "text": "There are several impactful ways this work could be further extended. First, incorporating the question\u2019s context in the belief statements in our rational layer could make the semantics of the beliefs more precise, thus avoiding potential ambiguity in their truth value. Second, one could use the belief graph itself to identify the key reasoning pieces that the LLM is most uncertain about. This could then guide a human-in-the-loop mechanism to correct or validate uncertain pieces via user interaction. Third, maintaining a persistent belief graph over multiple questions could help make the system more consistent across questions. This, in turn, would make a user\u2019s conversational experience with the system more coherent in a longer dialog setting. Lastly, after resolving inconsistencies in the rational layer, we could consider propagating information back to the LLM layer in order to update it (via fine-tuning, model editing, memory-based architectures, etc.),\nhelping avoid similar inconsistencies in the future."
        },
        {
            "heading": "6 Conclusion",
            "text": "While LLMs perform well, the interdependencies between their answers and their other beliefs is opaque, and may even be in conflict. This lack of interpretability is a significant impediment to widespread use of LLMs. To reduce this opacity, and reduce these conflicts, we have proposed REFLEX, a new system architecture in which an explicit, interpretable representation of beliefs - the belief graph - is added as a rational layer above the LLM. This layer providing a window into system beliefs, and allows latent inconsistencies in the LLM alone to reasoned about and repaired. Our implementation shows that belief consistency of the overall system is significantly improved, without harming answer accuracy, resulting in answers supported by interpretable chains of reasoning drawn from a more consistent belief system. This new architecture is an important step towards improving confidence in system behavior, and towards trustable deployment of LLMs in practical applications.\nLimitations\nWe have shown how an LLM can be extended with a self-reflective component, allowing latent model knowledge to be made explicit in the form of a belief graph, providing a window into the model\u2019s system of beliefs. While exciting, there are several limitations with the current work and opportunities for the future.\nFirst, the reasoning component in the rational\nlayer can make mistakes, resulting in the overall system rejecting true statements or accepting false ones. A detailed analysis and classification of these failure modes was presented in Section 4.3.\nSecond, for our experiments, we used the T511B based Entailer system as the baseline LLM. While there is every reason to expect our proposed architecture to be effective in reducing inconsistency with newer and larger LLMs such as ChatGPT and LLaMA, this is still to be evaluated. Doing so would require implementing the basic operations needed to construct belief graphs (Section 3.2.1) using instruction prompting and incontext learning. Other work has demonstrated such implementations (e.g., Wei et al., 2022; Jiang et al., 2020), making the outlook promising, but indeed their combination still needs to be demonstrated at scale in an architecture like REFLEX.\nLastly, we found consistency-minimized belief graphs to be highly valuable in understanding the system\u2019s successes and failures. We expect these graphs to be a valuable starting point for providing explanations and gaining a user\u2019s trust in the system. However, we have not conducted a formal user study to measure this.\nEthics Statement\nLike any other project using LLMs, despite the best intentions there is a risk of the model producing biased or offensive statements as part of its explanations, and thus must be used with care and appropriate guards and warnings."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was made possible, in part, by funding from Open Philanthropy, the European Research Council (#740516) and by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. We also thank Google for providing the TPUs for conducting experiments. Finally, we are grateful for the valuable feedback from the anonymous reviewers."
        },
        {
            "heading": "A Additional Results",
            "text": "We report results on the dev set of the EntailmentBank dataset in Table A1.\nSystem EntailmentBank (dev) Consistency Accuracy LLM 87.5 78.6 LLM + rational layer 96.1 81.8\n(REFLEX)\nTable A1: Results on EntailmentBank (dev), used to tune the system\u2019s hyperparameters."
        },
        {
            "heading": "B Hyperparameters and Runtime",
            "text": "MaxSAT finds the optimal assignment of true/false labels on statement nodes that minimizes the total penalty of constraint violations. If the true/false label on a statement node is flipped, then the penalty is the model confidence cs in the original label. Similarly if a rule (constraint) is violated by the true/false labels on its associated statements, then the penalty is the model confidence cr in that rule.\nWe set a number of hyperparameters to ensure that the various sources of confidence are appropriately balanced, and tune these on a development set (EntailmentBank (dev) which is separate from our test sets). We use the same set of hyperparameters for all test sets.\n1. As raw model confidences cs are highly skewed towards 0 and 1, we re-calibrate these with ek.(cs\u22121), where k is a fixed hyperparameter. Note, that for the MC and XOR rule, the raw input score s is 1.0. 2. We calibrate rule confidences in the same way as we calibrate belief confidences but use separate calibration parameters different types of rules namely:\n\u2022 Entailer rules p \u2192 h \u2022 XOR rules \u2022 MC rules\ni.e., the raw rule score c is re-calibrated to confidence ektype.(c\u22121) where ktype is the respective hyperparameter per rule type. 3. We set three hyperparameters tuning the respective importance of the three different types of rules. Therefore, the final rule score is computed by c = ttype \u2217 ektype.(c\u22121) where ttype is the respective hyperparameter constant per rule type. 4. For xor rules between statements si and negsi,\nHyperparameter Value k 9 kentailer 36 kxor 30 kmc 9 tentailer 1.02 txor 1.1 tmc 0.98 mxor 0.3 dmax 5\nTable B1: Hyperparameters.\nwe remove (ignore) those where there is significant uncertainty, namely where |score(si)\u2212 score(negsi)| \u2264 mxor, where mxor is a tuned hyperparmeter. 5. Additionally, we tune a damping parameter that downscales rules on the boundary of the graph. Belief nodes involved in these rules are not supported by any premises and should therefore have less influence than rules with strong support. 6. Finally, we tune the maximum depth dmax of the belief graph.\nThe performance on this dev set partition is shown in Table A1 and the hyperparameter values are shown in Table B1.\nThe runtime for MaxSAT constraint solving is fast (<1 millisecond per question). However, constructing the belief graph is computationally intensive: Each call to expand or score a node takes \u223c2 seconds, and our graphs typically contain \u223c600 nodes, so if these calls were maximally parallelized, with each step growing the graph one level deeper, the runtime would be the maximum graph depth (5) x 2 seconds = \u223c10 seconds total (or several minutes if a naive sequential implementation were used)."
        }
    ],
    "title": "Language Models with Rationality",
    "year": 2023
}