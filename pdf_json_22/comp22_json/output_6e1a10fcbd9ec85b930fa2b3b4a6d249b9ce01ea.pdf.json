{
    "abstractText": "A canonical problem in social choice is how to aggregate ranked votes: that is, given n voters\u2019 rankings over m candidates, what voting rule f should we use to aggregate these votes and select a single winner? One standard method for comparing voting rules is by their satisfaction of axioms\u2014properties that we want a \u201creasonable\u201d rule to satisfy. This approach, unfortunately, leads to several impossibilities: no voting rule can simultaneously satisfy all the properties we would want, at least in the worst case over all possible inputs. Motivated by this, we consider a relaxation of this worst case requirement. We analyze this through a \u201csmoothed\u201d model of social choice, where votes are (independently) perturbed with small amounts of noise. If no matter which input profile we start with, the probability (postnoise) of an axiom being satisfied is large, we will view it as nearly as good as satisfied\u2014called \u201csmoothed-satisfied\u201d\u2014even if it may be violated in the worst case. Our model is a mild restriction of Lirong Xia\u2019s, and corresponds closely to that in Spielman and Teng\u2019s original work on smoothed analysis. Much work has been done so far in several papers by Xia on which axioms can and cannot be satisfied under such noise. In our paper, we aim to give a more cohesive overview on when smoothed analysis of social choice is useful. Within our model, we give simple sufficient conditions for smoothed-satisfaction or smoothedviolation of several axioms and paradoxes, including most of those studied by Xia as well as some previously unstudied. We then observe that, in a practically important subclass of noise models, although convergence eventually occurs, known rates may require an extremely large number of voters. Motivated by this, we prove bounds specifically within a canonical noise model from this subclass\u2014 the Mallows model. Here, we find a more nuanced picture on exactly when smoothed analysis an help.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bailey Flanigan"
        },
        {
            "affiliations": [],
            "name": "Daniel Halpern"
        },
        {
            "affiliations": [],
            "name": "Alexandros Psomas"
        }
    ],
    "id": "SP:166ca5a9dfa4c6cad36236a956d334f0ff226929",
    "references": [
        {
            "authors": [
                "Kenneth Arrow"
            ],
            "title": "Individual values and social choice",
            "venue": "Nueva York: Wiley,",
            "year": 1951
        },
        {
            "authors": [
                "Pranjal Awasthi",
                "Avrim Blum",
                "Or Sheffet",
                "Aravindan Vijayaraghavan"
            ],
            "title": "Learning mixtures of ranking models",
            "venue": "In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2014
        },
        {
            "authors": [
                "Vidmantas Bentkus"
            ],
            "title": "A lyapunov-type bound in rd",
            "venue": "Theory of Probability & Its Applications,",
            "year": 2005
        },
        {
            "authors": [
                "Andrew C. Berry"
            ],
            "title": "The accuracy of the gaussian approximation to the sum of independent variates",
            "venue": "Transactions of the american mathematical society,",
            "year": 1941
        },
        {
            "authors": [
                "Niclas Boehmer",
                "Piotr Faliszewski",
                "Sonja Kraiczy"
            ],
            "title": "Properties of the mallows model depending on the number of alternatives: A warning for an experimentalist",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Weiwei Cheng",
                "Krzysztof Dembczynski",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Label ranking methods based on the plackett-luce model",
            "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),",
            "year": 2010
        },
        {
            "authors": [
                "Leah Melani Dillman",
                "Don A",
                "Christian"
            ],
            "title": "Survey mode as a source of instability in responses across surveys",
            "venue": "Field methods,",
            "year": 2005
        },
        {
            "authors": [
                "\u00d6mer E\u011fecio\u011flu",
                "Ay\u00e7a E. Giritligil"
            ],
            "title": "The impartial, anonymous, and neutral culture model: a probability model for sampling public preference structures",
            "venue": "The Journal of mathematical sociology,",
            "year": 2013
        },
        {
            "authors": [
                "Carl-Gustav Esseen"
            ],
            "title": "On the liapunov limit error in the theory of probability",
            "venue": "Arkiv fo\u0308r Matematik, Astronomi och Fysik,",
            "year": 1942
        },
        {
            "authors": [
                "Dan S. Felsenthal",
                "Zeev Maoz",
                "Amnon Rapoport"
            ],
            "title": "An empirical evaluation of six voting procedures: do they really make any difference",
            "venue": "British Journal of Political Science,",
            "year": 1993
        },
        {
            "authors": [
                "William V. Gehrlein"
            ],
            "title": "Condorcet\u2019s paradox and the likelihood of its occurrence: different perspectives on balanced preferences",
            "venue": "Theory and decision,",
            "year": 2002
        },
        {
            "authors": [
                "Allan Gibbard"
            ],
            "title": "Manipulation of voting",
            "venue": "schemes. Econometrica,",
            "year": 1973
        },
        {
            "authors": [
                "James Green-Armytage",
                "T Nicolaus Tideman",
                "Rafael Cosman"
            ],
            "title": "Statistical evaluation of voting rules",
            "venue": "Social Choice and Welfare,",
            "year": 2016
        },
        {
            "authors": [
                "Daniel Kahneman",
                "Amos Tversky"
            ],
            "title": "Prospect theory: An analysis of decision under risk",
            "venue": "World Scientific,",
            "year": 2013
        },
        {
            "authors": [
                "Leonard Lee",
                "On Amir",
                "Dan Ariely"
            ],
            "title": "In search of homo economicus: Cognitive noise and the role of emotion in preference consistency",
            "venue": "Journal of consumer research,",
            "year": 2009
        },
        {
            "authors": [
                "Tyler Lu",
                "Craig Boutilier"
            ],
            "title": "Learning mallows models with pairwise preferences",
            "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),",
            "year": 2011
        },
        {
            "authors": [
                "Colin L. Mallows"
            ],
            "title": "Non-null ranking models",
            "venue": "i. Biometrika,",
            "year": 1957
        },
        {
            "authors": [
                "Elchanan Mossel"
            ],
            "title": "Probabilistic view of voting, paradoxes, and manipulation",
            "venue": "Bulletin of the American Mathematical Society,",
            "year": 2022
        },
        {
            "authors": [
                "Elchanan Mossel",
                "Ariel D. Procaccia",
                "Mikl\u00f3s Z R\u00e1cz"
            ],
            "title": "A smooth transition from powerlessness to absolute power",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2013
        },
        {
            "authors": [
                "Herv\u00e9 Moulin"
            ],
            "title": "Condorcet\u2019s principle implies the no show paradox",
            "venue": "Journal of Economic Theory,",
            "year": 1988
        },
        {
            "authors": [
                "Florenz Plassmann",
                "T. Nicolaus Tideman"
            ],
            "title": "How frequently do different voting rules encounter voting paradoxes in three-candidate elections",
            "venue": "Social Choice and Welfare,",
            "year": 2014
        },
        {
            "authors": [
                "Geoffrey Pritchard",
                "Arkadii Slinko"
            ],
            "title": "On the average minimum size of a manipulating coalition",
            "venue": "Social Choice and Welfare,",
            "year": 2006
        },
        {
            "authors": [
                "Daniel A. Spielman",
                "Shang-Hua Teng"
            ],
            "title": "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time",
            "venue": "Journal of the ACM,",
            "year": 2004
        },
        {
            "authors": [
                "Lirong Xia"
            ],
            "title": "The smoothed possibility of social choice",
            "venue": "In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Lirong Xia"
            ],
            "title": "The semi-random satisfaction of voting axioms",
            "venue": "In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Lirong Xia"
            ],
            "title": "How likely are large elections tied",
            "venue": "In Proceedings of the 22nd ACM Conference on Economics and Computation (EC),",
            "year": 2021
        },
        {
            "authors": [
                "Lirong Xia"
            ],
            "title": "Semi-random impossibilities of condorcet criterion",
            "venue": "In Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2023
        },
        {
            "authors": [
                "Lirong Xia"
            ],
            "title": "The impact of a coalition: Assessing the likelihood of voter influence in large elections",
            "venue": "In Proceedings of the 24th ACM Conference on Economics and Computation (EC),",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Motivated by this, we consider a relaxation of this worst case requirement. We analyze this through a \u201csmoothed\u201d model of social choice, where votes are (independently) perturbed with small amounts of noise. If no matter which input profile we start with, the probability (postnoise) of an axiom being satisfied is large, we will view it as nearly as good as satisfied\u2014called \u201csmoothed-satisfied\u201d\u2014even if it may be violated in the worst case.\nOur model is a mild restriction of Lirong Xia\u2019s, and corresponds closely to that in Spielman and Teng\u2019s original work on smoothed analysis. Much work has been done so far in several papers by Xia on which axioms can and cannot be satisfied under such noise. In our paper, we aim to give a more cohesive overview on when smoothed analysis of social choice is useful.\nWithin our model, we give simple sufficient conditions for smoothed-satisfaction or smoothedviolation of several axioms and paradoxes, including most of those studied by Xia as well as some previously unstudied. We then observe that, in a practically important subclass of noise models, although convergence eventually occurs, known rates may require an extremely large number of voters. Motivated by this, we prove bounds specifically within a canonical noise model from this subclass\u2014 the Mallows model. Here, we find a more nuanced picture on exactly when smoothed analysis an help."
        },
        {
            "heading": "1 Introduction",
            "text": "One of the most canonical problems in social choice is how to aggregate votes: that is, given a preference profile consisting of n voters\u2019 rankings over m alternatives, what voting rule f should we use to aggregate this preference profile into a single winner? A predominant way of comparing voting rules is by their satisfaction of axioms\u2014logical statements that describe basic, natural properties that voting rules should satisfy.\nTraditionally, it is said that a voting rule satisfies an axiom if the logical statement specified by the axiom holds on all possible preference profiles; if there exists even one profile on which it does not hold (a counterexample), the rule is said to violate the axiom. Unfortunately, under this worst-case notion of satisfaction, all voting rules violate at least some common-sense axioms. 1 There is hope,\n1E.g., the Gibbard-Satterthwaite theorem says that any onto, single-winner, non-dictatorial voting rule with m > 2 is not strategyproof [12]. For more examples, see the Comparison of electoral systems Wikipedia page.\nar X\niv :2\n20 6.\n14 68\n4v 4\n[ cs\n.G T\n] 5\nA ug\n2 02\n3\nhowever, because this worst-case approach may cover up an important distinction between voting rules: while some rules may fail to satisfy an axiom on large swaths of preference profiles, others may fail only on precisely contrived instances. This distinction is of practical significance because voting rules of the latter type should usually satisfy the axiom in practice, where the lack of perfect correlation between people\u2019s preferences makes extremely contrived counterexamples unlikely to occur.\nIn 2020, a paper by Lirong Xia aimed to capture this intuition by modeling profiles as semirandom\u2014mostly adversarial, but with random perturbations [24]. More precisely, Xia\u2019s model assumes that \u201cnoisy\u201d (but otherwise adversarial) profiles arise in the following way: first, let an adversary choose voters\u2019 \u201ctypes,\u201d with each type corresponding to a distribution over rankings. The ultimate profile is then sampled by selecting each voter\u2019s ranking independently from their type distribution.\nIn this paper, we capture the same intuition using a slightly restricted version of Xia\u2019s semirandom model. Our model produces \u201cnoisy\u201d profiles in the following way: first, let an adversary fix a starting profile. Then, apply a small amount of noise independently to each individual ranking in the profile\u2014 that is, for each ranking, draw a new ranking from some distribution (this distribution can be arbitrary, up to regularity conditions). We will refer to this as the smoothed model, as it is directly analogous to Spielman and Teng\u2019s \u201csmoothed\u201d model in the distinct context of linear programming. We discuss this connection and the precise relationship between our model and Xia\u2019s in Section 1.2. The key takeaway is that ours is a slight restriction, assumed primarily for ease of exposition.\nBecause the smoothed model goes so minimally beyond the worst case, resolving axioms in this way is quite meaningful: the small amount of noise it assumes should be sufficient to escape counterexamples only if they are truly isolated among other profiles, i.e., contrived. The model is also well-motivated practically, as people\u2019s preferences are established to be susceptible to small shocks, in daily life and even in the preference elicitation process [7, 14, 15]. The existing work in this area gives some hope of such positive results: it shows that under all standard voting rules considered, the axioms Group-strategyproofness [28], Resolvability [26], and Participation [25] are satisfied post-noise with high probability as n grows large. In contrast, the axiom Condorcet consistency remains violated with high probability, even after semi-random noise, by a popular class of rules [25]. Here we see heterogeneity across axioms: for some, smoothed noise reliably circumvents impossibilities, whereas, for others, it seems insufficient. This motivates our first question:\nQuestion 1: What properties of axiomatic impossibilities make them resolvable by smoothed noise?\nThis question remains to be fully addressed, in part because existing work has mostly analyzed axioms one by one, using technically intricate arguments to get precise asymptotic convergence rates in n. While this approach has yielded detailed insights about specific axioms and voting rules, it remains to distill higher-level patterns across rules and axioms.\nOur second research question is a refinement of the first and deals with understanding when semi-random noise can circumvent impossibilities, absent a key assumption made in the related work so far. This assumption is called positivity in the related work, and it amounts to assuming that when a ranking is perturbed, there is at least some fixed probability, min-prob, of any other ranking resulting from this perturbation. Although it is not explicitly written, existing upper bounds on rates of convergence for the axioms we consider implicitly depend multiplicatively on 1/min-prob3/2. Although when min-prob is large, this polynomial dependence is not too de-\nmanding, when min-prob is small, enormous numbers of voters may be necessary to guarantee a reasonable probability of axiom satisfaction.\nAlthough assuming that min-prob is large enough to avoid such issues may seem innocuous, it is anything but. This is because noise models in which min-prob is very small represent perhaps the most realistic subclass of noise models: they encompass any noise model in which small perturbations are unlikely to completely reverse someone\u2019s ranking (or make similarly drastic changes). Given that small real-world shocks that may perturb people\u2019s preferences are unlikely to qualitatively change their opinions (to, e.g., the extent that their rankings are reversed), a more realistic noise model might be one in which \u201cless\u201d noise corresponds to lower probability of drawing a perturbed ranking that reflects extreme opinion change. This motivates our second research question:\nQuestion 2: What properties of axiomatic impossibilities make them resolvable by smoothed noise, for noise models in which rankings reflecting extreme opinion change are drawn with low (or zero) probability?"
        },
        {
            "heading": "1.1 Results and Contributions",
            "text": "Question 1. In Section 3, we study the general smoothed model. We distill two patterns: one across axioms where smoothed noise does help, and another across axioms where it does not. In the process, we do the first smoothed (or semi-random) analysis of several core axioms in social choice. We also re-analyze some axioms studied in past work, but we prove these results anew, with derivations that are standardized across axioms and yield bounds that explicitly depend on min-prob, which will be useful in Question 2. We delineate which analyses are new and which are re-proven in Section 1.2.\nAs summarized in Table 1 (located in Section 3.1), we first show negative results for the axioms Condorcet Consistency, Indepen-\ndence of Irrelevant Alternatives, Consistency, Majority. Across all voting rules we study, we find that these axioms are all smoothed-violated whenever they are violated, i.e., smoothed noise is insufficient to circumvent impossibilities for these axioms. Moreover, across axioms and rules, this smoothed violation always holds for the same reason: there exist large contiguous segments of the profile space consisting of counterexamples. As shown in the left-hand pane of Figure 1, if such regions exist, then our starting profile can be chosen to be inside one, in which case small perturbations are insufficient to produce profiles outside it. We formalize this underlying pattern into a generic sufficient condition for smoothed-violation in Lemma 7.\nAs summarized in Table 2 (located in Section 3.2), we next show positive results for the axioms Resolvability and group notions of Strategyproofness, Participation, and Monotonicity. In the spirit of generalizing across axioms, we are able to analyze the latter three at once by proving the smoothed-satisfaction of a more general axiom, called \u03c1(n)-Group-Stability, which requires that the behavior of \u03c1(n) voters should not be able to affect the outcome of the election\n(essentially framing Margin of Victory as a binary property [22, 28]). The common feature of these axioms, which permits their smoothed-satisfaction, is that their counterexamples must occur on or near hyperplanes. As depicted in the right-hand pane of Figure 1, this means that any \u201cwell-behaved\u201d noise, applied to any starting profile, will place very little probability mass on the sliver of counterexamples contained within (potentially some nonzero but shrinking distance of) the measure-zero hyperplane. We formalize this underlying pattern into a generic sufficient condition for smoothed-satisfaction in Lemma 10.\nQuestion 2. In Section 4, we pursue the first convergence rates to smoothed satisfaction that are not parameterized by min-prob, and which do not rely on the minimum probability being nonzero. Although the core ideas we use naturally extend to broader classes of noise models, for concreteness, we pursue these bounds specifically in the Mallows model \u2014perhaps the most canonical model of noisy rankings. This model has the realistic property we want: the probability of drawing a ranking decreases in its Kendall tau distance2 from the original ranking. More precisely, for a noise parameter \u03d5 \u2208 [0, 1], the probability of drawing a ranking at Kendall tau distance d from the original ranking is proportional to \u03d5d. As a result, min-prob for this model is exponentially small: the probability of reversing a ranking due to Mallows noise is \u03d5\u2126(m\n2). The Mallows model falls within our smoothed model, so our impossibilities from Section 3.1 carry over. From among the remaining axioms of interest (those in Section 3.2), we characterize precise convergence rates to smoothed satisfaction of Resolvability and \u03c1(n)-GroupStrategyproofness in the Mallows model.\nWhile min-prob-parameterized upper bounds yielded heterogeneity across axioms, they were almost homogeneous across voting rules per axiom, i.e., for each axiom, either nearly all rules satisfied, or nearly all rules violated them. Interestingly, these new bounds reveal diversity across voting rules as well. As summarized in Table 3, while Plurality and Borda Count are smoothed-satisfied at a rate polynomial in \u03d5 and m, we can lower bound the convergence rates of Maximin and Veto to include a 1/\u03d5\u2126(m) term. This essentially means the number of voters required to guarantee satisfaction is exponential in these parameters, making this property less appealing. Across these voting rules, we can distill a pattern: voting rules whose outcomes are sensitive to changes throughout ranking positions are helped substantially by Mallows noise, while those whose outcomes remain fixed despite potentially many swaps are helped much less. Zooming out, this conceptual finding extends beyond the Mallows model and the studied axioms, suggesting that for reasonable noise models, only voting rules that are truly sensitive to small changes in voters\u2019 preferences will have brittle impossibilities.\nBonus Contribution: smoothed analysis of Arrow\u2019s theorem. As an extension, in Section 5 we do the first smoothed (or semi-random) analysis of Arrow\u2019s Theorem [1]\u2014perhaps one of the most influential impossibilities in social choice. Surprisingly, we show that this impossibility is resolved with high probability under the smoothed model. However, as we show, this resolution is for a trivial reason related to Arrow\u2019s definition of the axiom Non-dictatorship. We therefore identify a slight strengthening of Arrow\u2019s theorem, and pose the open question of whether smoothed noise is sufficient to resolve it.\n2The Kendall tau distance between two rankings \u03c0, \u03c0\u2032 is the number of pairs of candidates on which they disagree."
        },
        {
            "heading": "1.2 Related work",
            "text": "Our model is designed to be a close analog to Spielman and Teng\u2019s celebrated \u201csmoothed\u201d model, introduced in their analysis of the Simplex algorithm [23]. We give a detailed description of the parallels between our model and theirs in Appendix A.1. Within social choice, the model most closely related to ours is Lirong Xia\u2019s semi-random model [24],3 of which our model is a mild restriction. To see how Xia\u2019s model generalizes ours, in our model we always have m! \u201ctypes\u201d, with each type corresponding to a possible starting ranking. Each type\u2019s associated distribution is then the noise added to that ranking. The key restriction we make is that, while Xia\u2019s model allows types with potentially different \u201cshapes\u201d of noise distributions, we assume that all rankings are perturbed by a common noise distribution, just centered at different rankings. This restriction is technically mild, as the assumption critical to analyses in both models is that noise is independent across rankings, and many of our results could be expressed in the more general model but in a less concise way. Because of that, we chose to stick to a slightly more restricted model for ease of exposition.\nBeyond Xia\u2019s model, axiom satisfaction has also been studied in other, less-adversarial randomized models. One popular such model is Impartial Culture, where profiles are drawn uniformly and i.i.d. [11, 13, 18]. Slightly more generally, the axiom Strategyproofness was studied in a nonuniform i.i.d model by Mossel et al. [19]. Further afield, there is empirical work on the frequency of axiom violations on simulated inputs and real elections [21, 10].\nNow we situate our results among past work on axiom satisfaction in the semi-random model. First, our results in Section 4 are completely distinct from this existing line of work, pursuing bounds that do not rely on the positivity assumption core to the semi-random model. In contrast, our results in Section 3 are for generic noise distributions that are subject to the positivity assumption, along with all other regularity conditions imposed in Xia\u2019s semi-random model. In the general smoothed model, we present the first smoothed (or semi-random) analysis of the axioms Consistency, Independence of Irrelevant Alternatives, and Majority, along with Arrow\u2019s impossibility. We also introduce and analyze a new axiom, \u03c1(n)-Group-Stability, which we show generalizes multiple standard axioms. In the interest of identifying patterns, we also reanalyze several axiomatic impossibilities for which there already exist similar bounds,4 though we show them via different proofs and give bounds that are explicit in their dependence on min-prob. For clarity, we distinguish axioms and impossibilities analyzed for the first time in this paper in Table 1, Table 2, and Table 3 with \u2217. The set of voting rules we study overlaps\u2014but not perfectly\u2014with the existing work on these axioms. In Appendix A.2, we offer a detailed comparison between the techniques we use to prove our bounds versus those used in past work."
        },
        {
            "heading": "2 Model",
            "text": "Rankings. There are m candidates M and n voters N . Voters express their preferences over the candidates as complete rankings. Formally, a ranking \u03c0 is a bijection from indices to candidates [m] \u2192 M , where \u03c0(j) represents the candidate ranked j-th in \u03c0. Let a \u227b\u03c0 b to denote\n3To avoid confusion with our model, we clarity: Xia\u2019s model is called \u201csmoothed\u201d in the original paper, but is renamed \u201csemi-random\u201d in most subsequent work.\n4These axiomatic results include Resolvability [26]; Strategyproofness [28]; Condorcet and Participation [25], plus Moulin\u2019s impossibility of satisfying them together [27]; and Condorcet\u2019s voting paradox and the ANR impossibility theorem [24].\nthat candidate a is preferred to b in ranking \u03c0, or formally, \u03c0\u22121(a) < \u03c0\u22121(b). L(M) is the set of m! possible rankings, or just L when M is clear from context. Letting the rankings in L be implicitly ordered, the last (m!-th) ranking in L is \u03c0\u22121, and the set of rankings without this element as L\u22121 = L\\{\u03c0\u22121}. For reasons to be clarified next, we will often work with L\u22121 instead of L.\nProfiles and profile histograms. A profile \u03c0 = (\u03c0i|i \u2208 N) is a vector of n rankings, where \u03c0i is voter i\u2019s ranking. Let \u03a0n = \u220fn i=1 L be the set of all profiles on n voters, and let \u03a0 = \u22c3 n\u2208Z+ \u03a0n be the set of all profiles overall. We define addition over profiles in the natural way: (\u03c0 + \u03c0\u2032) = (\u03c0i|i \u2208 N)\u2225(\u03c0\u2032i|i \u2208 N \u2032).5\nInstead of working directly with profiles, we will work primarily with their histograms. A histogram h is an |L\u22121|-length vector whose \u03c0-th entry h\u03c0 is the proportion of voters with ranking \u03c0 in a profile (h is indexed only up to L\u22121 because the histogram must add to 1, so the m!th index is redundant). The histogram associated with a particular profile \u03c0 is h\u03c0, with \u03c0-th entry h\u03c0\u03c0 := 1/n |{i : \u03c0i = \u03c0}|. We define the simplex of all possible histograms as \u2206h :={ h \u2208 [0, 1]|L\u22121| : \u2211 \u03c0\u2208L\u22121 h\u03c0 \u2264 1 } . Of course, H includes vectors with irrational entries that could never correspond to a well-defined profile; nonetheless, it will be useful to consider the completed space. In order to talk about only the histograms that are realizable from well-defined profiles, we also define HQ = H \u2229Q|L\u22121| to be the subset of H of vectors with rational components.\nFinally, we will use histogram operator H(\u00b7) to transform more general, profile-based objects into their histogram-based analogs. For example, H(\u03c0) = h\u03c0, and H(\u03a0) = HQ. For a single ranking, H(\u03c0) is a |L\u22121|-length basis vector with a 1 at the \u03c0-th index (H(\u03c0\u22121) is the 0s vector). We will also apply this operator to distributions over profiles and rankings in the natural way, first drawing a profile or ranking from the distribution, and then considering its corresponding histogram.\nVoting Rules. A voting rule is a function R : \u03a0 7\u2192 2M mapping a given profile to a set of winning candidates. Then, R(\u03c0) is the set of winners chosen by the voting rule on \u03c0. If a voting rule by its standard definition results in a tie, rather than specifying a tie-breaking rule, we assume it returns all such winners (this assumption is for ease of exposition only). Let R be the set of all voting rules. We will study several specific rules, defined colloquially below and formally in Appendix B.1.\nPositional Scoring Rules (PSRs) are represented by m-length vectors of weakly decreasing scores s1 \u2265 s2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 sm, where s1 = 1 and sm = 0. Candidate a receives si points for each voter that ranks it i-th; the candidate with the most points wins. We consider three specific PSRs, defined by their score vectors: Plurality: (1, 0, . . . , 0, 0), Borda: (1, 1\u2212 1m\u22121 , . . . , 1 m\u22121 , 0), and Veto: (1, 1, . . . , 1, 0). Beyond PSRs, we study the rules Minimax, Kemeny-Young, and Copeland. Minimax selects the candidate with the smallest maximum pairwise domination.6 Kemeny-Young selects the candidate ranked first in the ranking with the minimal sum of Kendall tau distances from voters\u2019 rankings.7 Copeland selects the alternative with the most points, giving a 1 point for each candidate b it pairwise dominates, and 1/2 point for each b with which it pairwise ties. Finally, in our analysis, we will often study a general class of voting rules called hyperplane rules. This class is known to be equivalent to generalized scoring rules [29] and encompasses essentially every standard voting rule, including all those listed above.\n5Here \u2225 represents concatenation, and thus the resulting profile \u03c0+\u03c0\u2032 contains |N |+ |N \u2032| voters. We also extend addition to permit positive integer multiples of profiles: z\u03c0 means adding a profile together z times.\n6Useful definitions: a pairwise-dominates b in \u03c0 when over half of voters rank a ahead of b: \u2223\u2223{\u03c0i|a \u227b\u03c0i b}\u2223\u2223 > n/2.\na and b pairwise tie when \u2223\u2223{\u03c0i|a \u227b\u03c0i b}\u2223\u2223 = n/2. a\u2019s maximum pairwise domination is equal to maxb \u0338=a |{\u03c0i|a \u227b\u03c0 b}|.\n7The Kendall tau distance between rankings \u03c0, \u03c0\u2032 is the total number of swaps required to transform \u03c0 into \u03c0\u2032.\nDefinition 1 (Hyperplane rules [19]). Note that given a set of \u2113 affine-hyperplanes, these hyperplanes partition the space of histograms into at most 3\u2113 regions, as every point is either on a hyperplane or on one of two sides. We say that R is a hyperplane rule if there exists a finite set of affine hyperplanes H1, . . . ,H\u2113 such that R is constant on each such region.\nWe will sometimes subdivide this class of rules further: decisive hyperplane rules are those which output a single winner on profiles that are not on any hyperplane; non-decisive hyperplane rules are all others.\nAxioms. Formally, an axiom is a function A : R \u2192 (\u03a0 \u2192 {True,False}), mapping a voting rule R to another mapping describing whether A is satisfied by R on any given profile. We can think of A(R) as representing the true/false statement \u201cR is consistent with A on \u03c0\u201d. We will study several standard axioms, defined formally in Appendix B.2 and described colloquially below.\nR satisfies Resolvability on \u03c0 if it selects a single winner. R satisfies Condorcet Consistency (abbreviated as Condorcet) on \u03c0 if it selects the Condorcet winner (the candidate that pairwise dominates all other candidates), or by default if \u03c0 has no Condorcet winner. R satisfies Majority on \u03c0 if it selects the majority winner (the candidate ranked first by a majority of voters), or by default if \u03c0 does not have a majority winner. R satisfies Consistency on \u03c0 if there is no partition of \u03c0 into subprofiles such that a unique candidate b, which is not the winner in \u03c0, is chosen as the winner on all subprofiles in that partition. R satisfies Independence of Irrelevant Alternatives (IIA) on \u03c0 if the winner, a, cannot be made to lose to b by adjusting votes in a way that does not change the relative positions of a and b.\nWe also use \u03c1(n)-Group-Stability, which colloquially requires that the outcome of voting rules be stable to a change in the behavior of up to \u03c1(n) voters. This is essentially framing margin of victory as a binary condition.\nDefinition 2 (\u03c1(n)-Group-Stability). For a given rule R, \u03c1(n)-Group-Stability(R) is satisfied if, for every pair of profiles \u03c0,\u03c0\u2032 that differ on at most \u03c1(n) of voters\u2019 rankings, R(\u03c0) = R(\u03c0\u2032).\nThis axiom implies strong, \u03c1(n)-parameterized group-level versions of three axioms of common interest: \u03c1(n)-Group-Strategyproofness\u2014no group of up to \u03c1(n) voters can strategically misreport their preferences in concert and cause R to output an alternative they all weakly prefer, and at least one of them strongly prefers; \u03c1(n)-Group-Participation\u2014no group of up to \u03c1(n) voters can leave the election and cause R to output an alternative they all weakly prefer, and at least one of them strongly prefers); and \u03c1(n)-Group-Monotonicity\u2014no group of up to \u03c1(n) voters can weakly decrease the position of an alternative c, which is not currently a winner by R, in their rankings, and cause c to become a winner by R.8 We formally define these axioms in Appendix B.2."
        },
        {
            "heading": "2.1 Smoothed model of profiles",
            "text": "Noise distributions over rankings. A noise distribution is effectively a distribution over rankings; formally, it is a distribution over permutations \u03c3 : [m] \u2192 [m]. When we \u201capply noise\u201d to a ranking \u03c0 via a noise distribution S\u03d5 (with parameter \u03d5 to be defined later), we are drawing a\n8The fact that \u03c1(n)-Group-Stability implies the first and third of these axioms is by definition. For the second, \u03c1(n)-Group-Stability technically implies \u03c1(n) 1\u2212\u03c1(n) -Group-Participation, since this axiom involves \u03c1(n) voters leaving the electorate; this does not change any of the asymptotic results, and we handle it in our proofs.\nrandom permutation \u03c3 \u223c S\u03d5, and then permuting \u03c0 according to \u03c3. Abusing notation slightly, we represent the distribution over rankings induced by perturbing \u03c0 with noise model S\u03d5 as S\u03d5(\u03c0).9\nIn this paper, we consider the class of noise distributions S, encompassing any distribution over rankings satisfying the minimal regularity conditions in Assumptions 1 and 2 below. All S\u03d5 \u2208 S are parameterized by a dispersion parameter \u03d5 \u2208 [0, 1], which captures the \u201clevel\u201d of noise applied by S\u03d5. We will use \u03d5 to implement multiple different such measures throughout our results. Assumptions 1 and 2 impose the minimal requirements to ensure that \u03d5 reasonably measures the amount of noise, and that the noise distribution is practical to work with. As we implement specific uses of \u03d5, we will impose additional assumptions as needed in later sections.\nAssumption 1 (Extremal values). The distribution S0 is the point mass on the identity (so that \u03d5 = 0 corresponds to no noise added). The distribution S1 is uniform over all permutations (so that \u03d5 = 1 corresponds maximum noise). Note that this implies that for all profiles \u03c0 \u2208 \u03a0, S1(\u03c0) is equivalent to the impartial culture model (see, e.g., [8]).\nAssumption 2 (Continuity). For all \u03c3, the probability S\u03d5 places on \u03c3 is continuous in \u03d5.\nSampling noisy profiles. Applying noise via S\u03d5 to an entire profile means applying it to every ranking within it independently (an assumption also made in the semi-random model [24]). Formally, if \u03c0 is our starting profile, then our noisy profile is drawn from the distribution \u220fn i=1 S\u03d5(\u03c0i), where each S\u03d5(\u03c0i) is independent. The resulting noisy profile is denoted S\u03d5(\u03c0). We will treat S\u03d5, S\u03d5(\u03c0), and S\u03d5(\u03c0) as distributions and random variables interchangeably.\nSince we will work with profiles in histogram form, so we will usually reason about distributions over rankings and profiles projected into histograms space. We express the distribution over ranking histograms induced by noise distribution S\u03d5(\u03c0) asH(S\u03d5(\u03c0)). Where the former is a distribution over rankings, the latter is a distribution over basis vectors. Then, the corresponding noise distribution over profile histograms is, naturally, H(S\u03d5(\u03c0)) = 1/n \u2211n i=1H(S\u03d5(\u03c0i))."
        },
        {
            "heading": "2.2 Smoothed-satisfaction and smoothed-violation of axioms",
            "text": "First, we formally define worst-case notions of axiom satisfaction and violation. Recall that A(R) : \u03a0 \u2192 {True,False} intakes a given profile and outputs whether rule R satisfies axiom A on that profile. Then, we say that \u03c0 is a counterexample to A(R) iff (A(R))(\u03c0) = False. Let \u03a0\u00ac(A(R)) be the set of all profiles that are counterexamples to A(R). Then, in the worst-case sense, R satisfies A if \u03a0\u00ac(A(R)) = \u2205 (i.e., no counterexample exists); otherwise, R violates A.\nNow, we define what it means for R to satisfy or violate A in the smoothed model. Conceptually, R smoothed-satisfies A if the probability that R satisfies A, after applying a noise distribution, converges to 1 as n grows large. In contrast, R smoothed-violates A if probability of R satisfying A being satisfied converges to 0 as n grows large. Note that worst-case satisfaction implies smoothedsatisfaction, and smoothed-violation implies violation.\nDefinition 3 (smoothed-satisfied). Voting rule R S-smoothed-satisfies axiom A at a rate f(n, \u03d5) if, for all n \u2208 Z+ and \u03d5 \u2208 (0, 1],\nsup \u03d5\u2032\u2208[\u03d5,1] sup \u03c0\u2208\u03a0n\nPr [ S\u03d5\u2032(\u03c0) \u2208 \u03a0\u00acA(R) ] \u2264 f(n, \u03d5).\n9More formally, S\u03d5(\u03c0) = \u03c0 + \u03c3 with \u03c3 \u223c S\u03d5. Here, the + operator represents composition: if \u03c3(i) = j, then the i-th ranked candidate in the perturbed ranking will be \u03c0(j).\nDefinition 4 (smoothed-violated). A voting rule R S-smoothed-violates axiom A at a rate of f(n) if there exists \u03d5 \u2208 (0, 1] and a profile \u03c0 of size n such that for all z \u2208 Z+,\ninf \u03d5\u2032\u2208[0,\u03d5]\nPr [ S\u03d5\u2032(z\u03c0) \u2208 \u03a0\u00acA(R) ] \u2265 1\u2212 f(zn).\nPer Definition 3, convergence to smoothed-satisfaction occurs eventually for all \u03d5 \u2208 (0, 1], although the rate depends on \u03d5. When we show smoothed-violation, in contrast, we are saying there exists a constant amount of noise \u03d5 such that this amount, or any less, will not be enough to ensure satisfaction of A by R as n grows large. Note the gap between these two definitions: smoothed-violated is not the negation of smoothed-satisfied, and a claim could therefore be \u201cinbetween\u201d these definitions, satisfying neither. This will not end up being the case for any of the voting rules or criteria we study."
        },
        {
            "heading": "3 Patterns across smoothed-violated, smoothed-satisfied axioms",
            "text": ""
        },
        {
            "heading": "3.1 Condorcet, Majority, Consistency, IIA",
            "text": "This section is dedicated to axioms about which we prove negative results. As summarized in Table 1, we find that for all the voting rules we study, smoothed noise is insufficient to prevent existing violations of any of these axioms. Across these axioms, the reason for this insufficiency is the same: as formalized in our generic sufficient condition in Lemma 7, the histogram space contains contiguous regions of counterexamples.\nTheorem 5. For all A \u2208 {Condorcet, Majority, Consistency, IIA} and all R \u2208 PSRs \u222a {Minimax,Kemeny-Young,Copeland}, if R violates A, then R smoothed-violates A.\nThe proof of this theorem is found in Appendix C.1; we explain the intuition here. The core idea is that for small enough \u03d5, H(S\u03d5(\u03c0)) concentrates at an exponential rate very near the starting histogram h\u03c0 (Lemma 6). This lemma, proven in Appendix C.2, follows from the fact that noise is applied independently across rankings, allowing the application of a simple Hoeffding bound.\nLemma 6. Let S be a noise model. For all \u03b5 > 0, there exists a \u03d5 \u2208 (0, 1] such that for all \u03d5\u2032 \u2208 [0, \u03d5] and profiles \u03c0 \u2208 \u03a0n on n voters,\nPr [\u2225\u2225H(S\u03d5\u2032(\u03c0))\u2212 h\u03c0\u2225\u22251 < \u03b5] > 1\u2212 exp (\u03b52n/2) .\nFrom this concentration follows a general sufficient condition for smoothed-violation (Lemma 7): that there exists a counterexample \u03c0 such that h\u03c0 is contained within a ball of counterexamples. Then, as the noise distribution concentrates around h\u03c0, most of its probability mass is contained in the ball, and the probability of a counterexample converges to 1.\nLemma 7. Fix a noise model S, axiom A, and rule R. Suppose there exists a profile \u03c0 and radius r > 0 such that for all profiles \u03c0\u2032 \u2208 \u03a0 satisfying (1) |\u03c0\u2032| = z|\u03c0| for some z \u2208 Z+ and (2) \u2225h\u03c0\u2032 \u2212 h\u03c0\u22251 < r, it is the case that \u03c0\u2032 \u2208 \u03a0\u00acA(R). Then, R smoothed-violates A at a rate of\nf(n) = exp ( \u2212r2n/2 ) .\nProof. Fix a noise model S, an axiom A, and a voting rule R. Fix a profile \u03c0 and radius r > 0 satisfying the preconditions of the theorem. Choose \u03b5 = r and let \u03d5 be the one from Lemma 6 corresponding to \u03b5 and S. Fix an arbitrary z \u2208 Z+ and \u03d5\u2032 \u2208 [0, \u03d5]. Notice that h\u03c0 = hz\u03c0. Hence, Lemma 6 guarantees that the post-noise histogram lies in an r-radius ball around the original histogram h\u03c0 with high probability\u2014 that is, H(S\u03d5\u2032(z\u03c0)) \u2208 BL1r (h\u03c0) with probability at least 1 \u2212 f(z|\u03c0|) = 1 \u2212 exp ( \u2212r2z|\u03c0|/2 ) . Note also that every profile in the support of S\u03d5\u2032(z\u03c0) is guaranteed to have z|\u03c0| voters. These two facts, taken with both preconditions of the theorem, imply that the post-noise profile histogram is a counterexample with high probability\u2014 that is, S\u03d5\u2032(z\u03c0) \u2208 \u03a0\u00acA(R) with probability at least 1 \u2212 f(z|\u03c0|). It then follows, by the definition of smoothed violation (Definition 4), that R smoothed-violates A at a rate of f(z|\u03c0|), as needed.\nFinally, we conclude Theorem 5 by finding counterexamples for each A,R that exist within balls of other counterexamples, and then applying Lemma 7. Across rules and axioms, simple counterexamples suffice, supporting the intuition that the insufficiency of smoothed noise is not a quirk of these rules and axioms. Rather, smoothed noise may be insufficient across many interpretable rules and axioms, because\u2014as a natural consequence of their interpretability\u2014 they behave similarly on similar profiles."
        },
        {
            "heading": "3.2 Resolvability, Strategyproofness, Participation, and Monotonicity",
            "text": "This section is dedicated to axioms about which we prove (mostly) positive results, summarized in Table 2. Across these axioms, our proofs use a common property that makes smoothed-noise sufficient for circumventing impossibilities: that across voting rules, their counterexamples are restricted to regions on or near a limited number of hyperplanes.\nAs are upper bounds in Xia\u2019s semi-random model [24], our upper bounds here will be parameterized by min-prob(S\u03d5) := min\u03c3 Pr[S\u03d5 = \u03c3], the smallest probability S\u03d5 assigns to any permutation. This parameterization motivates two additional assumptions: Assumption 3 ensures that 1/min-prob(S\u03d5) is well-defined, and Assumption 4 describes how min-prob implements \u03d5 as a measurement of the noisiness of S\u03d5.\nAssumption 3 (Positivity). For all \u03d5 \u2208 (0, 1], min-prob(S\u03d5) > 0. That is, for any nonzero amount of noise, the resulting distribution assigns positive probability to all permutations.\nAssumption 4 (Weak Monotonicity). The value min-prob(S\u03d5) is non-decreasing in \u03d5.10\nFirst, Theorem 8 shows that Resolvability is smoothed-satisfied for all decisive hyperplane rules. Notably, these rules exclude the known rule Copeland, due to its lack of sensitivity to changes in rankings in regions surrounding ties.\nTheorem 8. All decisive hyperplane rules smoothed-satisfy Resolvability at a rate Om(1/ \u221a n\u00b7min-prob(S\u03d5)3/2). All non-decisive hyperplane rules smoothed-violate Resolvability.\nThe formal proof is found in Appendix C.3. This result corely relies on the fact that, essentially regardless of the noise distribution over rankings S\u03d5, the resulting noise distribution over entire profile histograms H(S\u03d5(\u03c0)) must converge uniformly11 to a multi-dimensional Gaussian distribution at a rate of O(1/ \u221a n) (Lemma 9):\nLemma 9. Let S be a noise model, \u03d5 \u2208 [0, 1] a parameter, and \u03c0 \u2208 \u03a0n a profile on n voters. Then, for all convex sets X \u2286 Rm!\u22121,\u2223\u2223Pr [H(S\u03d5(\u03c0)) \u2208 X]\u2212 Pr [N (E[H(S\u03d5(\u03c0))], Cov[H(S\u03d5(\u03c0))] ) \u2208 X]\u2223\u2223 \u2264 O((m!)7/4)\u221a\nn \u00b7min-prob(S\u03d5)3/2 .\nIntuitively, H(S\u03d5(\u03c0)) converges to a Gaussian because it is akin to the sum of independent indicators (literally, independently-drawn rankings represented in histogram space as basis vectors). We prove this convergence in Appendix C.4 using a general form of the Berry Esseen bound [3].\nThis convergence allows us to prove a general sufficient condition for the smoothed-satisfaction: that the set of counterexamples are contained with a measure-zero subset of histograms (Lemma 10).\nLemma 10. Fix a noise model S, voting rule R, and axiom A. If there exists some set X such that (1) X = \u22c3\u2113 j=1Xj where each Xj \u2286 \u2206h is convex, (2) H ( \u03a0\u00acA(R) ) \u2286 X, and (3) X is measure zero (noting that X is necessarily measurable), then R smoothed-satisfies A at a rate of\nf(n, \u03d5) = \u2113 \u00b7O((m!)7/4)\n\u221a n \u00b7min-prob(S\u03d5)3/2 .\nThe formal proof is found in Appendix C.5, and uses that the Gaussian places zero probability mass over measure-zero regions of its support. This concludes our analysis of Resolvability.\n10We note that our results don\u2019t centrally depend on Assumption 4\u2014Assumptions 2 and 3 are sufficient to give the same high-level results. We include Assumption 4 because it is not prohibitive, it simplifies the exposition, and allows us to give more useful parameterized bounds.\n11\u201cUniform\u201d (over profiles) convergence means that for all profiles \u03c0 of any fixed n, H(S\u03d5(\u03c0)) is at most O(1/\u221an) \u201cdistance away\u201d from the the Gaussian distribution with expectation and variance corresponding to that of H(S\u03d5(\u03c0)).\nNow, we show that \u03c1(n)-Group-Stability (Definition 2) is smoothed-satisfied by all hyperplane rules for \u03c1(n) \u2208 o( \u221a n). Because \u03c1(n)-Group-Stability implies \u03c1(n)-Group-Strategyproofness, \u03c1(n)-Group-Participation, and \u03c1(n)-Group-Monotonicity, these axioms must also be smoothedsatisfied by all hyperplane rules. Theorem 11. o( \u221a n)-group-stability is smoothed-satisfied by all hyperplane rules at a rate of Om(1/ \u221a n\u00b7min-prob(S\u03d5)3/2) + o(1).\nWe will now prove Theorem 11 via the following anti-concentration lemma, which states that for any \u03c0, H(S\u03d5(\u03c0)) places limited probability mass within distance \u03b4(n) of any specific hyperplane as n grows large, so long as \u03b4(n) is decreasing sufficiently quickly in n.\nLemma 12. Let G be the set of all hyperplanes in Rm!\u22121. For all noise models S, parameters \u03d5 \u2208 [0, 1], and \u03b4(n) \u2208 o(1/ \u221a n), we have the following, where d is the L1 distance.\nsup G\u2208G sup \u03d5\u2032\u2208[\u03d5,1] sup \u03c0\u2208\u03a0n\nPr [ d(H(S\u03d5\u2032(\u03c0)), G) \u2264 \u03b4(n) ] \u2208 O\n( \u03b4(n)\n\u221a n\u221a\nmin-prob(S\u03d5) +\n1\nmin-prob(S\u03d5)3/2 \u221a n\n) \u2208 o(1).\nThis lemma, proven in Appendix C.6, shows that even if E[H(S\u03d5(\u03c0))] falls within \u03b4(n) distance of the hyperplane (we can think of this as it falling within a \u201cthick\u201d hyperplane), the width of this thick hyperplane is shrinking faster than the distribution over histograms concentrates as n grows large. To make the convergence rate in Lemma 12 o(1), \u03b4(n) must be in o(1/ \u221a n).\nTo apply this lemma to show the smoothed-satisfaction of \u03c1(n)-Group-Stability, first observe that for a group of size \u03c1(n) to be able to impact the outcome of the election in \u03c0 (i.e., for \u03c0 to be a counterexample), that group must be pivotal in \u03c0\u2014that is, h\u03c0 must lie within some \u03c1(n)dependent distance from a profile on which the winner changes. Because the set of such profiles are defined by finitely-many hyperplanes (by definition of hyperplane rules), counterexamples are then restricted to \u201cthick\u201d hyperplanes. To apply Lemma 12 for each such hyperplane, we need their width to be o(1/ \u221a n) in histogram space, corresponding to a coalition of size \u03c1(n) \u2208 o( \u221a n). Then, we conclude Theorem 11 by simply union bounding over the finite number of hyperplanes referred to in Definition 1."
        },
        {
            "heading": "4 Beyond dependence on the minimum probability",
            "text": "All convergence rates to smoothed-satisfaction proven so far\u2014both in Section 3.2 and in the related work [24]\u2014depend on 1/ \u221a n \u00b7min-prob3. As a result, when min-prob is very small, extremely large n is required to get reasonably low probability of an axiom violation: examining the relative n and min-prob dependency above, if we decrease min-prob by factor \u2113, we need to increase the number of voters n by a factor of \u21133 in order to recover the same probability bound.12\nMotivated by the lack of good bounds for the important class of noise models with small min-prob, we now pursue the first min-prob-independent bounds on convergence rates. For concreteness, we specifically pursue these bounds within the well-established Mallows model, as defined\n12One may wonder if different techniques could potentially improve this dependence. Although we do not have a tight cubic lower bound and the exact bound should depend on the exact voting rule/axiom, we can at least get a linear one. Indeed, consider the noise distribution that switches to any ranking other than the starting one with probability \u03b5, but stays on the starting one with probability 1 \u2212 (m! \u2212 1)\u03b5. The minimum probability here is \u03b5, but we need n \u2208 \u03c9(\u03b5/m!) to ensure that the profile changes at all with high probability, a necessary condition to smoothed-satisfy any axiom that fails on even a single profile.\nbelow; however, as we will illustrate in detail throughout this section, our results rely only weakly on the properties of this model, and should apply to more general models as well.\nDefinition 13 (Mallows noise model [17]). Let d : L \u00d7 L \u2192 N be the Kendall tau distance, and let \u03d5 \u2208 [0, 1]. Then, the Mallows model SMallows\u03d5 is defined as follows, where Z = \u2211 \u03c0\u2032\u2208L \u03d5\nd(\u03c0\u2032,\u03c0) is a normalizing term. Pr [ SMallows\u03d5 (\u03c0) = \u03c0\u2032 ] = 1\nZ \u03d5d(\u03c0,\u03c0\n\u2032).\nThe Mallows model is an attractive case study for proving min-prob-independent bounds for two reasons. First, it precisely captures the intuition motivating this analysis: that long-range swaps should be rare under less noise, making min-prob low. Longer range swaps are so rare in this model, in fact, that min-prob(SMallows\u03d5 ) approaches zero exponentially fast as m grows: the maximum Kendall tau distance between rankings is ( m 2 ) , so min-prob(SMallows\u03d5 ) \u2208 \u03d5\u2126(m\n2). This motivates our second reason: that despite its importance in social choice, existing convergence rates grow poorer at an exponential rate for Mallows noise as m gets large and/or \u03d5 gets small.\nWithin the Mallows model, we characterize the precise m,\u03d5, n-dependent rates at which Resolvability and o( \u221a n)-Strategy-proofness are smoothed-satisfied by four diverse voting rules: Plurality, Borda, Veto, and Minimax. Recall that these voting rules are already known to smoothed-satisfy both axioms by our results in Section 3.2. The key difference in this analysis is that, while before n was being treated as the only variable (with m and the noise level treated as constants), we now consider more closely the convergence depends on m and \u03d5. In particular, we are interested in how large n must be (as a function of m and \u03d5) for the rate to be o(1) (i.e., satisfaction occurring with high probability). We summarize our results in Table 3, framed to directly answer this question. The formal statements and proofs are below, in Section 4.1.\nWhat is striking in these results is a clear separation between voting rules, which was not visible in our min-prob-parameterized bounds. The convergence rates of Plurality and Borda do not get dramatically worse as \u03d5 (and thus min-prob) gets small; put another way, as \u03d5 scales down, n must scale up proportionally to maintain roughly the same probability of satisfaction. In contrast, Minimax and Veto require exponentially (in m) large n.13\nWe can, as before, distill a pattern explaining this gap: the voting rules that achieve the best rates (Plurality, Borda are those whose outcomes are more sensitive to local swaps across the support (or, in critical areas of the support, in the case of Plurality). The outcomes of Veto and\n13To put this in perspective, suppose m = 6 and we decrease \u03d5 from 1/5 to 1/10. To maintain a similar probability of violating Resolvability, if we are using Plurality or Borda it is sufficient to double the number of voters; if we are using Veto, one needs at least 24 = 32 times as many voters, and for Minimax, one needs at least 23 = 8 times as many voters. This gap only gets steeper as m gets larger.\nMaximin, in contrast, are fairly insensitive to local swaps, allowing us to show that local swaps are not enough to overcome impossibilities. While this pattern is perhaps unsurprising in retrospect, it may be important in informing the choice of voting rules."
        },
        {
            "heading": "4.1 Formal statements and proofs",
            "text": "We include in the body the proofs for one upper bound (Plurality) and one lower bound (Veto), and defer the proofs for Borda and Minimax to Appendices D.2 and D.3, respectively. Our arguments will rely on only very weak properties of the Mallows model, essentially requiring just that the noise rarely induces swaps between distantly-ranked candidates. To emphasize the kinds of noise models to which our arguments can generalize, we now recap the precise properties of the noise model required for the proof corresponding to each voting rule. For Plurality, the key property of the noise distribution we use is that no alternative is ranked first post-noise with probability near 1. For Borda, the key property is that there is no \u2113 for which two candidates will be exactly \u2113 positions apart post-noise with probability near 1. For Veto, the key property is that there is an exponentially small probability of a given voter moving one of their top two candidates to last place. For Minimax, the key property is that it is unlikely for two candidates a distance of m/2 apart to swap. Before presenting our results, we establish a few useful properties of Mallows noise.\nLemma 14 (First- and last-place probability [2]). From starting ranking \u03c0, the probability that \u03c0(j) is ranked first in a ranking drawn from SMallows\u03d5 (\u03c0) is proportional to \u03d5j , i.e., \u03d5j/ \u2211m j\u2032=1 \u03d5 j\u2032 . Symmetrically, the probability \u03c0(j) is ranked last is proportional to \u03d5m\u2212j .\nFrom [17], if two candidates i, j are k positions apart in \u03c0 (i.e., \u03c0(i) and \u03c0(j) with i\u2212 j = k), the probability that they retain their relative order post-Mallows noise is increasing in k, and is always at least 1/2. This was refined by [5] to an exact value of their swap probability:\nLemma 15 (swap probability [5]). For candidates i, j such that \u03c0(i) and \u03c0(j) with i\u2212 j = k, their probability of swapping post-Mallows noise is\nq(k) := 1\n1\u2212 \u03d5k+1\n( 1\u2212 (1\u2212 \u03d5)k\u03d5 k\n1\u2212 \u03d5k\n) .\nWe will often parameterize our bounds by the probability of the opposite event\u2014that i and j at distance k do swap, which we denote by q\u0304(k) := 1 \u2212 q(k). We will use the following upper bounds on q\u0304(k), derived in Appendix D.1.\nLemma 16. For all k \u2208 [m] and \u03d5 \u2208 [0, 1], q\u0304(k) \u2264 min { k\u03d5k, \u03d5k/2, \u03d51+\u03d5 } .\nProposition 17 (Plurality). Plurality smoothed-satisfies resolvability at a rate of at most O ( m5/2/ \u221a n\u03d5+m exp (\u2212n/6m) ) and, as long as \u03c1(n) \u2264 n6m , smoothed-satisfies \u03c1(n)-group-stability at a rate of\nO ( \u03c1(n)m5/2/ \u221a n\u03d5+m exp (\u2212n/9m) ) .\nProof. We begin with resolvability and show how to extend it to \u03c1(n)-Group-Stability after. Fix a starting profile \u03c0. For a candidate c, let Xci be the indicator that voter i ranks c first, and let pci = Pr[X c i = 1]. Then, let S c = \u2211 iX c i be the random variable representing plurality score\nof c post-noise. It follows that E[Sc] = \u2211\ni\u2208[n] p c i . We will partition the candidates into two sets\nbased on these expectations: L (for \u201clow\u201d) is defined as L = {c|E[Sc] < n2m}, and H (for \u201chigh\u201d) is defined as H = {c|E[Sc] \u2265 n2m}. We will first show Claim 1: with high probability, the winner will be from H. Then, in Claim 2, we will show that the probability any two candidates in H have the same plurality score is small. Union bounding over these two events will upper bound the probability of an unresolvable outcome, i.e., a tie in plurality scores.\nProof of Claim 1. Fix a candidate a \u2208 L. Note that a necessary condition for a to be a plurality winner is for their score to be at least n/m. Standard Chernoff bounds says that Pr[Sa \u2265 (1 + \u03b4)\u00b5] \u2264 exp(\u2212\u03b42\u00b5/(2 + \u03b4)] for all \u03b4 \u2265 0, where \u00b5 = E[Sa]. Choose \u03b4 such that (1 + \u03b4)\u00b5 = n/m. Note that since a \u2208 L, \u00b5 \u2264 n2m , so \u03b4 \u2265 1. Further, this implies that \u03b4\u00b5 \u2265 n 2m and\n\u03b4 2+\u03b4 \u2264 1 3 . Plugging in both of these bounds, we get an upper bound of exp(\u2212 n 6m) on the probability that a wins. Via a union bound, the probability any candidate in L wins is at most |L| exp(\u2212 n6m).\nProof of Claim 2. Fix two candidates a, b \u2208 H. Let Yi = Xai \u2212 Xbi ; then, we can upper-bound Pr[Sa = Sb] by Pr[ \u2211 i Yi = 0]. Because Yi converges to be Gaussian-distributed, and since the\nGaussian places 0 mass on any point, we can upper bound Pr[ \u2211\ni Yi = 0] by the convergence rate of \u2211\ni Yi to the Gaussian, derived via Berry-Esseen:\nPr [\u2211 i Yi = 0 ] \u2264 O ( 1/ \u221a Var[Yi] \u00b7max i (\u03c1i/Var[Yi]) ) (1)\nwhere \u03c1i = E[|Yi \u2212 E[Yi]|3. We now bound each of these terms. First, since each Yi \u2208 [\u22121, 1], |Yi \u2212 EYi| \u2264 2, so maxi(\u03c1i/Var[Yi]) \u2264 2. Next, we expand out Var[Yi] to get the following, where the last step uses that 2E[Xai X b i ] = 0 since only one of these indicators can be 1 for a given i.\nVar[Yi] = E[Y 2i ]\u2212E[Yi]2 = E[(Xai )2] +E[(Xbi )2]\u2212 2E[Xai Xbi ]\u2212E[Xai \u2212Xbi ]2 = pai + pbi \u2212 (pai \u2212 pbi)2,\nNow, Lemma 14, the probability that any candidate \u03c0i(j) ends up in the first position is proportional to \u03d5j . Therefore, (assuming m \u2265 2) each pci is upper-bounded by \u03d5\u2211m j=1 \u03d5\nj \u2264 \u03d5\u03d5+\u03d52 = 1\n1+\u03d5 . We use this to upper-bound (p a i \u2212 pbi)2 as follows:\n(pai \u2212 pbi)2 \u2264 max(pai , pbi)(pai + pbi) \u2264 1/1+\u03d5 \u00b7 (pai + pbi).\nUsing this, we conclude that Var[Yi] \u2265 pai + pbi \u2212 11+\u03d5(p a i + p b i) = \u03d5 1+\u03d5(p a i + p b i). It follows that\u2211\niVar[Yi] \u2265 \u03d5 1+\u03d5(E[S a] + E[Sb]) \u2265 \u03d51+\u03d5n/m \u2265 \u03d5 2 \u00b7 n/m. Plugging our bounds into Equation (1),\nPr [\u2211 i Yi = 0 ] \u2264 O (\u221a m/(n\u03d5) ) .\nUnion bounding over the at most |H|2 \u2264 m2 pairs of candidates in H, the probability such a tie occurs is at most O ( m2 \u221a m/(n\u03d5) ) . Finally, union bounding again over the event that the winner\nwas from L (where |L| \u2264 m), the overall probability of a tie is at most the stated bound.\nFor \u03c1(n)-Group-Stability, we upper bound the probability that any non-winning candidate has plurality score within 2\u03c1(n) of the maximal score. make the threshold to be in L having E[Xc] \u2264 n3m . For Claim 1, we use a threshold of n m \u2212 2\u03c1(n) \u2265 2n 3m . The same argument goes through, we just plug in n3m for \u03b4\u00b5 instead, replacing the 6 in the bound with a 9. For Claim 2, rather than just proving it is unlikely for the scores to be equal, we can do the same analsysis for all differences in the range [\u22122\u03c1(n), 2\u03c1(n)]. Union bounding over all 4\u03c1(n)+1 possible score values, this adds an additional O(\u03c1(n)) factor.\nProposition 18 (Borda). Borda smoothed-satisfies Resolvability at a rate of O ( m3/ \u221a n\u03d5 ) ,\nand smoothed-satisfies \u03c1(n)\u2212Group-Stability at a rate of O ( \u03c1(n)m4/ \u221a n\u03d5 ) .\nProposition 19 (Veto). The rate Veto smoothed-satisfies Resolvability is lower bounded by 1\u2212 n\u03d5m\u22122, and, as long as n \u2265 m, the rate 1-strategy-proofness is smoothed-satisfied is lower bounded by 1\u2212 n\u03d5m\u22122 \u2212m\u03d5\u230a n m \u230b.\nProof. Consider a profile where all voters rank two candidates a and b in their top two positions. We upper bound the probability that either a or b is ranked last by any voter; if this never occurs, then a and b will be tied. To this end, recall that by Lemma 14, a voter\u2019s first-choice alternative will be ranked last post-noise with probability \u03d5m/ \u2211m j=1 \u03d5 j , and their second-choice alternative\nwill be ranked last post-noise with probability \u03d5m\u22121/ \u2211m\nj=1 \u03d5 j . Hence, neither of these candidates\nwill ranked last with probability at least\n1\u2212 \u03d5 m\u22121 + \u03d5m\u2211m\nj=1 \u03d5 j\n= 1\u2212 \u03d5 m\u22122 + \u03d5m\u22121\u22111\nj=0 \u03d5 j\n= 1\u2212 (\u03d5 m\u22122 + \u03d5m\u22121)(1\u2212 \u03d5)\n1\u2212 \u03d5m = 1\u2212 \u03d5\nm\u22122 \u2212 \u03d5m\n1\u2212 \u03d5m \u2265 1\u2212 \u03d5m\u22122.\nUnion bounding over all voters, we get that no voter will place a or b last with probability at least 1\u2212 n\u03d5m\u22122. This lower bounds the rate at which Veto can smoothed-satisfy Resolvability.\nFor 1-Group-Stability, we will have the same construction, except each candidate other than a or b will be ranked last by at least \u230a nm\u230b voters. It is still the case that a and b will never be ranked last with probability at least 1\u2212n\u03d5m\u22122. Note that for any candidate c \u0338= a, b, if a voter ranks them last in the starting profile, they will continue to do so with probability 1\u2211m\nj=0 \u03d5 j =\n1\u2212\u03d5 1\u2212\u03d5m \u2265 1 \u2212 \u03d5.\nThis implies they will continue to be ranked last by at least one of these \u230a nm\u230b voters with probability at least 1\u2212 \u03d5\u230a n m \u230b. Union bounding over these m\u2212 2 \u2264 m candidates, in the sampled profile, with probability 1 \u2212 n\u03d5m\u22122 \u2212 m\u03d5\u230a n m \u230b, candidates a and b will never be ranked last, while all other candidates will be ranekd last at least once. Hence, a and b are tied as veto winners. Further, as there are at least m > m \u2212 2 voters, at least one candidate c must be ranked last at least twice. Consider a voter i that ranked c last, and suppose without loss of generality they prefer a to b. In this case, they can move b to the bottom of their ranking. Now a will be the unique candidate never ranked last, and will hence be the unique winner. This is an improvement for the voter, meaning that the resulting profile does not satisfy 1-Strategy-proofness.\nProposition 20 (Minimax). The rate at which Minimax smoothed-satisfies Resolvability is lower bounded by\n\u2126 ( 1\u221a\nnq\u0304(\u230am/2\u230b) \u2212m exp\n( \u2212n(1\u2212 2q\u0304(\u230am/2\u230b)) 2\n32\n)) ,\nat least for n divisible by 4, and the rate for 1-Group-Strategyproofness is lower bounded by 1\u2212 3nq(\u230am/4\u230b)\u2212O ( m exp ( \u2212(n\u2212 2)(1\u2212 2q\u0304(\u230am/2\u230b))2\n32\n)) .\nNote that using the fact that q\u0304(\u230am/2\u230b) \u2208 O(1/ logm), a necessary condition for a high probability bound is n \u2208 \u03c9(q\u0304(\u230am/2\u230b). We can plug in our various upper bounds from Lemma 16 on q\u0304(\u230am/2\u230b). For the bound in Table 3, the second of these implies that n \u2208 \u03c9(1/(m\u03d5\u230am/2\u230b)) is necessary."
        },
        {
            "heading": "5 Discussion",
            "text": "Because the motivation for the smoothed model is fundamentally practical, we begin by outlining some key practical takeaways from our work. These takeaways help address the question: how much can a bit of smoothed noise give us, when, and why?\nAlthough the related work so far has been largely optimistic about smoothed analysis in social choice, our analysis in Section 3.1 illustrates that for many rules and axioms, smoothed noise may be insufficient to circumvent impossibilities. Our sufficient condition for smoothed violation, moreover, suggests that the insufficiency of smoothed noise may be tied fundamentally to axioms\u2019 and voting rules\u2019 simplicity and interpretability, suggesting that these negative results are difficult to get around without making other compromises.\nOn the other hand, our results in Section 3.2 show that for certain kinds of axioms\u2014 i.e., those whose counterexamples must lie near hyperplanes or other small-measure structures\u2014a small amount of noise is enough to circumvent impossibilities. However, our results in Section 4 paint a more nuanced picture: if we believe that in practice, any perturbations are truly unlikely to cause drastic opinion changes, then our choice of voting rule is far more important than past work suggests. In particular, while past min-prob-parameterized upper bounds yield similar convergence rates across voting rules (for any given axiom), our results in the Mallows model\u2014 for which existing bounds do not usefully apply\u2014show that actually, certain voting rules converge to smoothedsatisfaction far faster than others. The voting rules that converge faster are, perhaps unsurprisingly, those whose outcomes are more sensitive to local swaps.\nWith these takeaways in hand, we now explore some extensions and future work.\nExtension and future work: smoothed analysis of Arrow\u2019s Theorem Our results from Section 3.1 and 3.2, which address the smoothed-satisfaction of individual axioms, also have implications for multi-axiom impossibilities of the form, \u201cthere exists no voting rule that simultaneously satisfies all axioms in A \u2208 A\u201d where A is some collection of axioms. We can now ask the same question replacing \u201csatisfies\u201d with \u201csmoothed-satisfies.\u201d Past work in the semi-random model has studied several multi-axiom impossibilities, and our results from Section 3 immediately recover several of these existing results.14 However, perhaps the most famous multi-axiom impossibility, Arrow\u2019s Theorem [1], has yet to be analyzed under semi-random noise. This impossibility states\n14Our results imply the smoothed-resolution Gibbard-Satterthwaite [12] (Corollary 30), the ANR impossibility theorem (e.g., see [24]) (Corollary 31), and the impossibility of simultaneously satisfying Condorcet and Participation as identified by Moulin [20] (Corollary 32). Our results also imply that the existence of Condorcet cycles (i.e., Condorcet\u2019s Paradox ) is not smoothed-resolved\u2014that is, existing Condorceet cycles remain with high probability in the presence of smoothed noise (Proposition 33).\nthat there is no voting rule that (when m \u2265 3) can simultaneously satisfy IIA, Unanimity, and Non-Dictatorship.15\nIn light of our findings in Section 3.1 that IIA tends to be smoothed-violated, one might guess that Arrow\u2019s Impossibility is smoothed-impossible; i.e., it still holds under smoothed noise. However, due to how Arrow defines the axiom Non-dictatorship [1], it is not hard to show that Arrow\u2019s Theorem is in fact smoothed-resolved. Indeed, a voting rule is said to be a dictatorship if there exists some voter such that the outcome of the voting rule is always that voter\u2019s first choice. Now define an almost-dictatorship voting rule, that chooses voter 1\u2019s favorite alternative on every profile except one arbitrary profile. The existence of this single exceptional profile means that our rule satisfies Non-dictatorship. However, since the rule only differs from a dictatorship on this one profile, it can easily be checked that it smoothed-satisfies IIA and Unanimity.\nThis positive result for Arrow\u2019s theorem is not very conceptually satisfying, as it does not get at the heart of what makes IIA, dictatorship, and unanimity inconsistent. In some sense, this is because satisfying non-dictatorship, as it is defined, is too easy. To say something more meaningful, we propose a strengthening of Non-Dictatorship, called Local Non-Dictatorship, that may be more interesting in the smoothed context: Fix a profile \u03c0 and define voter i\u2019s neighborhood around \u03c0, Ni(\u03c0) \u2286 \u03a0, to be the set of all profiles reachable by switching i\u2019s ranking for another ranking. Then, we say rule R satisfies Local Non-Dictatorship on profile \u03c0, if for all voters i, there is a \u03c0\u2032 \u2208 Ni(\u03c0) such that i\u2019s first choice in \u03c0\u2032 doesn\u2019t win. In a sense, then, satisfying this axiom disallows a voter from being a dictator in any local area of profile space.\nBecause satisfaction of Local Non-Dictatorship implies the satisfaction of Non-Dictatorship, Arrow\u2019s impossibility also implies inconsistency between IIA, Unanimity, and Local NonDictatorship. We refer to this new, stronger impossibility as Strengthed Arrow\u2019s Theorem. As we hoped, Strengthened Arrow\u2019s Theorem is not susceptible to the simple fix discussed above: our almost-dictatorship voting rule fails to smoothed-satisfy Local Non-Dictatorship, since voter 1 is the dictator across almost every neighborhood. Then, the question remains open: can this impossibility be smoothed-resolved?\nFuture work: more general models limiting drastic opinion change Our analyses in Section 4 are restricted to the Mallows model, which is perhaps the poster child for a much broader, practically-motivated assumption: noise-inducing shocks should not, in practice, induce drastic opinion change. Already, our results reveal that this is a practically meaningful restriction, uncovering previously hidden heterogeneity among voting rules\u2019 rates of convergence, due to their varying sensitivity to local swaps.\nGiven the practicality of the above assumption\u2014and the different patterns of convergence that emerge when we apply it\u2014we see investigating the broader space of noise models satisfying this assumptoin as an important frontier. There are many possible such noise models, and many will have min-prob equal to or very near zero, warranting the pursuit of new bounds. Such further analyses may enable more nuanced distinctions between axioms and voting rules, and stronger conclusions over more flexible noise models. Going even further, one exciting potential opportunity in this direction is to potentially trade this restriction for a relaxation of the independence assumption (that noise is applied independently across rankings). This strong assumption\u2014relied upon centrally in both our analyses and those in the related work\u2014is difficult to relax when reasoning about\n15Unanimity requires that if an alternative is ranked first by all voters, it is the winner. Non-Dictatorship means that there exists no voter such that the outcome of the voting rule is always their first choice.\nalmost arbitrary noise distributions. However, there may be hope of relaxing it when considering noise distributions restricted to more closely reflect practice."
        },
        {
            "heading": "A Supplemental Materials from Section 1",
            "text": "A.1 Comparison to Spielman and Teng\u2019s smoothed model\nThe smoothed analysis framework was originally proposed by Spielman and Teng to provide theoretical justification for the Simplex algorithm\u2019s fast runtime in real-world instances, despite its exponential worst-case complexity [23]. In their analysis, they go beyond the worst-case by adding Gaussian noise independently to each entry of the real-valued constraint matrix that is the input to Simplex algorithm. Then, they bound the expected run time of Simplex in the worst-case over inputs, where these guarantees are parameterized by the variance of the Gaussian noise added. Stated as a more general framework, the idea of smoothed analysis is to fix an arbitrary instance, add noise from a parameterized distribution, and then measure the quality of the expected outcome on the worst input (or, whether a property is satisfied with high probability\u2014an alternative formulation proposed by Spielman and Teng that is closer to ours).\nThis is precisely how our model works, but rather than its input being a constraint matrix of real numbers, it is a base profile of complete rankings over m alternatives. This base profile is perturbed by applying generically-structured noise independently across each of its rankings (we refer to this as the independence assumption). When evaluating the probability of a criterion being satisfied post-perturbation, we assume that the base profile is chosen adversarially, i.e., to minimize this probability. The noise distribution we apply to each ranking is parameterized by a value \u03d5 \u2208 [0, 1] to measure the quantity of noise added, analogous to Spielman and Teng\u2019s variance parameter. Our main departure from the original smoothed model is, where Spielman and Teng specify Gaussian noise, we do not assume a specific noise distribution, instead allowing any \u03d5parameterized distribution that is neutral over alternatives.16 Neutrality means that for a given \u03d5, if we permute a ranking and add noise, this is equivalent to adding noise and then permuting the output. In that sense, our noise model over profiles can be specified by a single distribution over one ranking, permuted to be applied to ranking permutations. On this distribution over rankings, we also sometimes assume positivity\u2014that when \u03d5 > 0, this distribution assigns positive probability to all rankings. Under these assumptions, Our class of noise models generalizes the popular Mallows noise model (e.g., [16]) as well as one-dimensional parameterizations of the Plackett-Luce model (e.g., see [6]).17"
        },
        {
            "heading": "A.2 Comparison of techniques with Xia 2020 [24]",
            "text": "At a high level, Xia\u2019s work and our work in the general smoothed model (Section 3) take a similar technical approach: both show that their noise models become well-behaved as the number of voters n grows large, and then use these convergence results to upper and lower bound how much probability mass is placed on \u201cbad\u201d profiles. In both models, the key assumption enabling this type of analysis is the independence of noise across rankings. As a result of this assumption, as n grows large, the resulting distribution over profiles will converge to distributions we understand, analogous to how sums of i.i.d. random variables converge to Gaussians via the Central Limit Theorem, or concentrate around their mean via Hoeffding\u2019s inequality. We prove the convergence\n16We do not commit to a noise distribution because there is no single well-established distribution that is obvious to apply (unlike in the real-valued setting, where the Gaussian is standard).\n17Traditionally, the Plackett-Luce model takes a single m real-valued parameters, one per ranking position. Our model generalizes a variation where each parameter is expressed as a function of \u03d5.\nof our model in Lemmas 6 and 9, corresponding to Xia\u2019s Lemma 1 of [24].18 Our convergence rates across the respective lemmas match asymptotically except when roughly, the set of \u201cbad\u201d profiles for a criterion is extremely small; this case tends to only come into effect for very specific kinds of criterion, none of which we formally analyze in this paper. In the limited cases where such differences do exist, they stem from our proof relying on a multi-dimensional version of the well-known Berry-Esseen theorem, while Xia\u2019s relies on faster convergence of Poisson Multinomial variables.\n18Notice that when comparing formal statements, both models consider the space of histograms (vectors representing the fraction of the profile composed of each ranking) rather than profiles directly, and, while we tend to analyze the set of histograms corresponding to \u201cbad\u201d profiles (e.g., those in which an axiom is not satisfied) directly, Xia considers sets which are solutions to a system of linear equations and inequalities. However, in practice, these approaches end up being quite similar."
        },
        {
            "heading": "B Supplementary Materials from Section 2",
            "text": ""
        },
        {
            "heading": "B.1 Definitions of Voting Rules",
            "text": "We define the voting rules we study as functions of profile histograms h (rather than a profile \u03c0). Further, we define them in the same form: first, we express how they assign candidates\u2019 scores, and then express via an argmaxc that the winner or set of winners constitutes the candidate(s) with the highest (or in one case, lowest) score.\nPositional Scoring Rules (PSRs). For fixed m, a positional scoring rule is characterized by a vector of weights of length m, \u03b1 = (\u03b1c|c \u2208 [m]), where \u03b11 \u2265 \u03b12 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1m. Without loss of generality, we let these weights be translated and scaled such that \u03b11 = 1 and \u03b1m = 0. The winner(s) by a PSR R, characterized by \u03b1,\nR(h) = argmax c \u2211 j\u2208[m] \u03b1j \u2211 \u03c0\u2208L\n\u03c0(j)=c\nh\u03c0\nMinimax. The Minimax winner is the candidate whose greatest pairwise defeat is the smallest:\nMinimax(h) = argmin c max c\u2032 \u0338=c \u2211 \u03c0\u2208L c\u2032\u227b\u03c0c h\u03c0\nKemeny-Young. We define a candidate c\u2019s Kemeny-Young score to be, at a high level, the level of agreement with voters\u2019 rankings of the most-agreeing ranking that ranks c first. Then, the set of winners contains the candidate(s) with the highest Kemeny-Young score.\nKemeny-Young(h) = argmax c max \u03c0:\u03c0(1)=c \u2211 c\u2032,c\u2032\u2032\u2208C c\u2032\u227b\u03c0c\u2032\u2032 \u2211 \u03c0\u2032\u2208L c\u2032\u227b\u03c0\u2032c\u2032\u2032 h\u03c0\u2032"
        },
        {
            "heading": "Copeland.",
            "text": "Copeland(h) = argmax c \u2211 c\u2032 \u0338=c I[c \u227bh c\u2032] + 1/2 \u00b7 I[c \u223ch c\u2032]"
        },
        {
            "heading": "B.2 Definitions of Axioms",
            "text": "We use the following notation. For a ranking \u03c0 \u2208 L, we let \u03c0(j) for an index j \u2208 [m] be the candidate in the j\u2019th position of \u03c0. For a ranking \u03c0 \u2208 L and distinct candidate c, c\u2032 \u2208 M , we use c \u227b\u03c0 c\u2032 to denote that c is ranked higher than c\u2032 in \u03c0. c \u227b\u03c0 c\u2032 when |{i \u2208 [n] : c \u227b\u03c0i c\u2032}| > |{i \u2208 [n] : c\u2032 \u227b\u03c0i c}|.\nResolvability. A voting rule R satisfies Resolvability on \u03c0 iff |R(\u03c0)| = 1 (i.e., there are no ties).\nCondorcet. A Condorcet winner is a candidate that would win in a pairwise election against every other candidate. That is, c is a Condorcet winner in \u03c0 if c \u227b\u03c0 c\u2032 for all c\u2032 \u0338= c. A voting rule\nR satisfies Condorcet Consistency on a given profile \u03c0 if one of two conditions hold: (1) there is no Condorcet winner in \u03c0, or (2) there is a Condorcet winner c, and R(\u03c0) = {c}.\nMajority. A majority winner is a candidate that is ranked first by a majority of agents. That is, c is a majority winner in \u03c0 if\n|{i \u2208 [n] : \u03c0i(c) = 1}| > n/2\nA voting rule R satisfies Majority on a profile \u03c0 if it satisfies one of two conditions: (1) there is no majority winner in \u03c0, or (2) if there is a majority winner a, then R(\u03c0) = {a}.\nConsistency. A voting rule R satisfies Consistency on a profile \u03c0 if the following holds: for all partitions of \u03c0 into sub-profiles, (\u03c01, . . . ,\u03c0t), if R(\u03c0j) for all j \u2208 [t] is the same set of winners W , then R(\u03c0) = W .\nIndependence of Irrelevant Alternatives (IIA). A voting rule R satisfies Independence of irrelevent alternatives (IIA) on profile \u03c0 if the following holds. Suppose R(\u03c0) = a. Then, for all candidates b \u0338= a, if \u03c0\u2032 is such that a \u227b\u03c0i b if and only if a \u227b\u03c0\u2032i b for all voters i, then R(\u03c0 \u2032) \u0338= b.\nThe axioms we study are defined formally as follows:\nDefinition 21 (\u03c1(n)-group-strategyproofness). A voting ruleR satisfies \u03c1(n)-group-strategyproofness on a profile \u03c0 if there exists no group of agents of size at most \u03c1(n) such that if they change their votes, resulting in some profile \u03c0\u2032 with outcome R(\u03c0\u2032), they are all at least as well off and at least one is strictly better off. An agent is at least as well (resp. strictly better) off if their favorite candidate in the set R(\u03c0\u2032) is weakly (resp. strictly) preferred to their favorite candidate in R(\u03c0).\nDefinition 22 (\u03c1(n)-group-monotonicity). A voting ruleR satisfies \u03c1(n)-group-monotonicity on a profile \u03c0 if there exists no candidate a and no group of agents of size at most \u03c1(n) such that if they change their votes without decreasing the position of a in any of their rankings, producing a new profile \u03c0\u2032, then it cannot be that a \u2208 R(\u03c0) and a /\u2208 R(\u03c0\u2032).\nDefinition 23 (\u03c1(n)-group-participation). A voting ruleR satisfies \u03c1(n)-group-participation on a profile \u03c0 if there exists no group of agents of size at most \u03c1(n) such that if they collectively leave the election, producing a new profile \u03c0\u2032, they are all at least as well off and at least one is strictly better off with the outcome R(\u03c0\u2032) than R(\u03c0)."
        },
        {
            "heading": "C Supplementary material from Section 3",
            "text": ""
        },
        {
            "heading": "C.1 Proof of Theorem 5",
            "text": "We prove this result for each axiom separately. For each axiom, we define a sufficient condition for a counterexample \u03c0 to be \u201cstrict\u201d, i.e., all profiles \u03c0\u2032 with histograms nearby to h\u03c0 will also be counterexamples. We then show that the existence of a strict counterexample implies smoothedviolation via Lemma 7. We later give (or point to existing) strict counterexamples for all relevant pairs of rules and axioms. In the following arguments, we say a profile \u03c0 is robust with respect to a voting rule R if there is an \u03b5 > 0 such that all profiles \u03c0\u2032 with \u2225h\u03c0 \u2212 h\u03c0\u2032\u22251 < \u03b5, R(\u03c0\u2032) = R(\u03c0). Notice that for hyperplane rules, all profiles that do not fall on hyperplanes are robust.\nCondorcet: We say a counterexample \u03c0 is a strict counterexample to R satisfying Condorcet if \u03c0 is robust with respect to R, \u03c0 has a strict Condorcet winner a (i.e., a candidate that beats every other candidate on strictly more than half of the voters), and R(\u03c0) \u0338= a. If such a strict counterexample \u03c0 exists, if it is robust with value \u03b51 and a wins with at least a 1/2 + \u03b52 fraction against each candidate, all profiles whose histogram falls within \u03b5 = min(\u03b51, \u03b52) of h\n\u03c0 have the same Condorcet winner and same output under R, and thus are also counterexamples. Hence, Lemma 7 implies smoothed-violation.\nMajority: We say a counterexample \u03c0 is a strict counterexample to R satisfying Majority if \u03c0 is robust and \u03c0 has a strict Majority winner a (i.e., a candidate that is ranked first by strictly more than half of the voters), and R(\u03c0) \u0338= a. If such a strict counterexample \u03c0 exists, if it is robust with value \u03b51 and a is ranked first by at least a 1/2 + \u03b52 fraction of the voters, all profiles whose histogram falls within \u03b5 = min(\u03b51, \u03b52) of h\n\u03c0 have the same Condorcet winner and same output under R, and thus are also counterexamples. Hence, Lemma 7 implies smoothed-violation.\nConsistency: We say a counterexample \u03c0 is a strict counterexample to R satisfying Consistency if \u03c0 = \u03c01 \u222a \u00b7 \u00b7 \u00b7 \u222a\u03c0t such that R(\u03c01) = \u00b7 \u00b7 \u00b7 = R(\u03c0t) \u0338= R(\u03c0) and all of \u03c01, . . . ,\u03c0t and \u03c0 are robust with respect to R. Suppose such a strict counterexample \u03c0 = \u03c01 \u222a \u00b7 \u00b7 \u00b7 \u222a\u03c0t exists. Let \u03b5min be an amount by which all the relevant profiles are robust. Suppose each profile \u03c0j has nj voters and let n = n1 + \u00b7 \u00b7 \u00b7+ nt be the number of voters in \u03c0. Let p = minj nj/n. Notice that any \u03c0\u2032 on zn voters within \u03b5 = p\u03b5min of \u03c0 can be decomposed into \u03c01 \u2032 , . . . ,\u03c0t \u2032 such that each \u03c0j \u2032 has znj voters and h\u03c0 j\u2032 is at most \u03b5min away from h\u03c0 j . Hence, R(\u03c01 \u2032 ) = \u00b7 \u00b7 \u00b7 = R(\u03c0t\u2032) \u0338= R(\u03c0\u2032), so \u03c0\u2032 is a counterexample to Consistency. Hence, Lemma 7 implies smoothed-violation.\nIIA: We say a counterexample \u03c01 is a strict counterexample to R satisfying IIA if there is another profile \u03c02 such that both \u03c01 and \u03c02 are robust with respect to R, and there are candidates a, b such that the relative ranking of a and b are the same under \u03c01i and \u03c0 2 i for all voters i, yet R(\u03c0\n1) = a and R(\u03c02) = b. Suppose such a strict counterexample \u03c01 and \u03c02 that are both robust by at least \u03b5. Consider any profile \u03c01 \u2032 where h\u03c0 1\u2032 is within \u03b5 of h\u03c0 1 . Notice that there must be a profile \u03c02 \u2032 such that h\u03c0 2\u2032 is within \u03b5 of h\u03c0 2 that matches \u03c01 \u2032 in terms of all voter\u2019s relative rankings of a and b, and by robustness R(\u03c01 \u2032 ) = a while R(\u03c02 \u2032 ) = b. Hence, Lemma 7 implies smoothed-violation.\nCounterexamples: The following table points to strict counterexamples that can be used to show the claims above. Most can be found on Wikipedia pages. The missing ones are presented afterwards.\nExample 24. Let R be the positional scoring rule represented by the weights vector (1, \u03b1, . . . , 0) without loss of generality. We assume \u03b1 > 0 is separated from 0 (otherwise, R is just Plurality). Let\n\u03c0 : n ( 1 2 \u2212 \u03b1 4(2\u2212\u03b1) ) voters a1 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am \u227b a2\nn ( 1 2 + \u03b1 4(2\u2212\u03b1) ) voters a2 \u227b a1 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am .\nExample 25. Let R be the positional scoring rule represented by the weights vector (1, \u03b1, . . . , 0) without loss of generality. We assume \u03b1 > 0 is separated from 0 (otherwise, R is just Plurality).\n\u03c01 : n/2 voters a1 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am \u227b a2 n/4 voters a2 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am \u227b a1 n/4 voters a2 \u227b a1 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am\n\u03c02 : n/2 voters a1 \u227b a2 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am n/4 voters a2 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am \u227b a1 n/4 voters a2 \u227b a1 \u227b a3 \u227b \u00b7 \u00b7 \u00b7 \u227b am .\nThis concludes the proof."
        },
        {
            "heading": "C.2 Proof of Lemma 6",
            "text": "Fix a noise model S and \u03b5 > 0. By continuity of S, there exists a \u03d5 > 0 such that for all \u03d5\u2032 \u2208 [0, \u03d5], Pr[S\u03d5\u2032(\u03c0) = \u03c0] > 1\u2212 \u03b5/2. Choose this to be our \u03d5.\nFix such a \u03d5\u2032 and a profile \u03c0 \u2208 \u03a0n. Notice that after applying S, each ranking \u03c0i will stay the same with probability at least 1 \u2212 \u03b5/2. As this is independent accross voters, a straightforward application of Hoeffding\u2019s inequality tells us that at least a 1\u2212\u03b5 fraction of rankings will not change with probability at least 1\u2212 exp(\u03b52n/2), as needed."
        },
        {
            "heading": "C.3 Proof of Theorem 8",
            "text": "For a decisive hyperplane rule, note that all profiles that fail Resolvablity must fall on one of the \u2113 hyperplanes (where by definition \u2113 is finite). Further, note that these hyperplanes are convex and measure-zero. Hence, we can immediately apply Lemma 10 using the hyperplanes as convex sets.\nFor non-decisive hyperplane rules, there must exist a profile \u03c0 not lying on any hyperplane for which Resolvablity fails. Further, h\u03c0 is at least some L1 distance \u03b5 > 0 from all hyperplanes.\nAll such profiles with histograms in this ball have the same outcome as \u03c0 and hence do not satisfy Resolvablity. We can then directly apply Lemma 7 using this profile \u03c0 and r = \u03b5."
        },
        {
            "heading": "C.4 Proof of Lemma 9",
            "text": "To prove this, we first prove the following technical lemma. One takeaway from this lemma it that H(S\u03d5(\u03c0))\u2019s covariance matrix is quite easy to work with: its inverse not only exists but has a simple closed-form, and its eigenvalues are lower-bounded by a constant.\nLemma 26. For all noise models S, parameters \u03d5 \u2208 (0, 1], and rankings \u03c0 \u2208 \u03a0n, the covariance matrix Cov[H(S\u03d5(\u03c0))] is invertible and has all positive real eigenvalues lower bounded by min-prob(S\u03d5)/(m!n).\nProof. We first express the expectation and variance of H(S\u03d5(\u03c0)) in terms of the analogous values for the H(S\u03d5(\u03c0i)). The relationships between these quantities are shown below, derived by applying simple properties of the expectation and variance in conjunction with the fact that H(S\u03d5(\u03c0)) = 1/n \u2211n\ni=1H(S\u03d5(\u03c0i)):\nE[H(S\u03d5(\u03c0))] = 1/n n\u2211\ni=1\nE[H(S\u03d5(\u03c0i))], Cov[H(S\u03d5(\u03c0))] = 1/n2 n\u2211\ni=1\nCov[H(S\u03d5(\u03c0i))]\nWe now use these relationships to find closed forms for each of these objects. Note that E[H(S\u03d5(\u03c0i))] is an |L\u22121|-length vector whose \u03c0-th component is simply Pr[S\u03d5(\u03c0i) = \u03c0]. Cov[H(S\u03d5(\u03c0i))] is a |L\u22121| \u00d7 |L\u22121| matrix whose entries each correspond to a pair of rankings \u03c0, \u03c0\u2032, such that the \u03c0, \u03c0\u2032th entry is equal to the covariance between the random variables H(S\u03d5(\u03c0i))\u03c0 and H(S\u03d5(\u03c0i))\u03c0\u2032 .\nThe covariance matrix Cov[H(S\u03d5(\u03c0i))] is a |L\u22121| \u00d7 |L\u22121| matrix whose entries each correspond to a pair of rankings \u03c0, \u03c0\u2032, such that the \u03c0, \u03c0\u2032-th entry is equal to the covariance between the random variables H(S\u03d5(\u03c0i))\u03c0 and H(S\u03d5(\u03c0i))\u03c0\u2032 . Given that H(S\u03d5(\u03c0i)) can take on the values of only basis vectors, the values of these random variables are either 0 or 1. To characterize these entries, we will use the fact that for general X and Y , Cov(X,Y ) = E[XY ]\u2212 E[X]E[Y ].\nFor distinct rankings \u03c0 \u0338= \u03c0\u2032, at most one of H(S\u03d5(\u03c0i))\u03c0 and H(S\u03d5(\u03c0i))\u03c0\u2032 can be nonzero, so the expectation their product must be 0. Then,\nCov ( H(S\u03d5(\u03c0i))\u03c0, H(S\u03d5(\u03c0i))\u03c0\u2032 ) = 0\u2212 Pr[S\u03d5(\u03c0i) = \u03c0] \u00b7 Pr[S\u03d5(\u03c0i) = \u03c0\u2032].\nFor diagonal entries where \u03c0 = \u03c0\u2032, since the values of our random variables are always 0 or 1, we have\nCov ( H(S\u03d5(\u03c0i))\u03c0, H(S\u03d5(\u03c0i))\u03c0 ) = Pr[S\u03d5(\u03c0i) = \u03c0]\u2212 Pr[S\u03d5(\u03c0i) = \u03c0]2.\nWith these in hand, we now prove the lemma statement. Fix S, \u03d5, and \u03c0. Recall that Cov[H(S\u03d5(\u03c0))] = 1/n2 \u2211n i=1Cov[H(S\u03d5(\u03c0i))]. Hence, we first consider Cov[H(S\u03d5(\u03c0))] for individual rankings \u03c0. Fix an arbitrary ranking \u03c0. First, we will prove that all eigenvalues of Cov[H(S\u03d5(\u03c0))] are positive and the minimum is at least min-prob(S\u03d5)/m!. To simplify notation in the subsequent computations, for the j\u2019th ranking \u03c0\u2032, let qj = Pr[S\u03d5(\u03c0) = \u03c0\u2032]. Recall that in Cov[H(S\u03d5(\u03c0))], the (j, k)-th entry when j = k (a diagonal entry) has value\nqj(1\u2212 qj) and for j \u0338= k, the entry has value \u2212qj \u00b7 qk. We can then write the covariance matrix as\nCov[H(S\u03d5(\u03c0))] =  q1(1\u2212 q1) \u2212q1 \u00b7 q2 \u00b7 \u00b7 \u00b7 \u2212q1 \u00b7 qm!\u22121 \u2212q2 \u00b7 q1 q2(1\u2212 q2) \u00b7 \u00b7 \u00b7 ...\n... . . .\n. . . ...\n\u2212qm!\u22121 \u00b7 q1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 qm!\u22121(1\u2212 qm!\u22121)\n .\nNote that qm! is the probability of the \u201cmissing\u201d ranking, and that \u2211m!\nj=1 qj = 1. We first demonstrate an inverse of Cov[H(S\u03d5(\u03c0))]. Consider the matrix:\nM inv =  1 q1 + 1qm! 1 qm! \u00b7 \u00b7 \u00b7 1qm! 1 qm! 1 q2 + 1qm! \u00b7 \u00b7 \u00b7 ... ... . . . . . . ...\n1 qm!\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1qm!\u22121 + 1 qm!\n .\nMore formally, the jth diagonal entry is 1/qj \u2212 1/qm! and all off-diagonal entries are simply 1/qm!. We now show that Cov[H(S\u03d5(\u03c0))] \u00b7 M inv = Im!\u22121 where Im!\u22121 is the identity matrix, that is, M inv is in fact the inverse of Cov[H(S\u03d5(\u03c0))]. To that end, let us consider the i\u2019th diagonal entry of the product. It is precisely\n(Cov[H(S\u03d5(\u03c0))] \u00b7M inv)jj = m!\u22121\u2211\nk=1,k \u0338=j \u2212qjqk qm! + qj(1\u2212 qj)\n( 1\nqj +\n1\nqm!\n)\n= m!\u22121\u2211\nk=1,k \u0338=j \u2212qjqk qm! + (1\u2212 qj) + qj qm! (1\u2212 qj)\n= m!\u22121\u2211\nk=1,k \u0338=j \u2212qjqk qm! + 1\u2212 qj + qj qm! \u2212 qj \u00b7 qj qm!\n= m!\u22121\u2211 k=1 \u2212qjqk qm! + 1\u2212 qj + qj qm! = \u2212qj(1\u2212 qm!)\nqm! + 1\u2212 qj + qj qm!\n= \u2212qj + qj \u00b7 qm! qm! + 1\u2212 qj \u00b7 qm! qm! + qj qm! = 1.\nFor a non-diagonal entry j, k with j \u0338= k, we have\n(Cov[H(S\u03d5(\u03c0))] \u00b7M inv)jk = m!\u2211\n\u2113=1,\u2113 \u0338=j,k \u2212qjq\u2113 qm! + qj(1\u2212 qj) qm!\n\u2212 qjqk \u00b7 ( 1\nqk +\n1\nqm!\n)\n= m!\u2211 \u2113=1,\u2113 \u0338=j,k \u2212qjq\u2113 qm! + qj qm! \u2212 qjqj qm! \u2212 qj \u2212 qjqk qm!\n= m!\u2211 \u2113=1 \u2212qjq\u2113 qm! + qj qm! \u2212 qj = qj(1\u2212 qm!)\nqm! + qj qm! \u2212 qjqm! qm!\n= 0.\nWe now consider the eigenvalues of Cov[H(S\u03d5(\u03c0))]. Since it is a covariance matrix, it is symmetric, and therefore positive semi-definite. Since we now know it is invertible, it is in fact positive definite. This implies all of its eigenvalues exist and are positive. Further, since the eigenvalues of M inv = Cov[H(S\u03d5(\u03c0))]\u22121 are the reciprocals of the eigenvalues of Cov[H(S\u03d5(\u03c0))], we can lower bound the eigenvalues of Cov[H(S\u03d5(\u03c0))] by upper bounding the the eigenvalues of M inv.\nTo upper bound the maximum eigenvalue of M inv, we can upperbound the maximum absolute row sum. Note that the sum of row j is\n1 qj + m!\u2212 1 qm! \u2264 m! min-prob(S\u03d5) .\nThis lower bounds the minimum eigenvalue of Cov[H(S\u03d5(\u03c0))] by min-prob(S\u03d5)\nm! , as needed. For our fixed profile \u03c0, we now have that each Cov[H(S\u03d5(\u03c0i))] has minimum eigenvalue at\nleast min-prob(S\u03d5)\nm! . Since the minimum eigenvalue of the sum of matrices is at least the sum of the minimum eigenvalues of each matrix, the minimum eigenvalue of \u2211n i=1Cov[H(S\u03d5(\u03c0i))] is at least n \u00b7 min-prob(S\u03d5)m! . Finally, to get Cov[H(S\u03d5(\u03c0))], we scale this sum down by n 2 which scales the minimum eigenvalues equivalently, yielding a minimum eigenvalue of at least min-prob(S\u03d5)\nnm! . Note that the minimum eigenvalue here is positive, meaning Cov[H(S\u03d5(\u03c0))] has no zero eigenvalues, meaning it is invertible.\nProof of Lemma 9. Fix S, \u03d5, and \u03c0 \u2208 \u03a0n. Since H(S\u03d5(\u03c0)) = 1/n \u2211n\ni=1H(S\u03d5(\u03c0i)) where each of these summands is independent, our goal will be to apply a version of the Berry-Esseen bound, as stated below:\nLemma 27 (Restatement of Berry-Esseen as in [3]). Let Y1, . . . , Yn be independent, mean-zero, R\nm!\u22121-valued random variables. Let S = Y1 + \u00b7 \u00b7 \u00b7+ Yn, and let C2 be the covariance matrix of S, assumed invertible. Let N (0, C2) be a m!\u22121-dimensional Gaussian with mean zero and covariance C2. Then for any convex subset X \u2286 Rm!\u22121,\n|Pr[S \u2208 X]\u2212 Pr[N (0, C2) \u2208 X]| \u2264 O((m!\u2212 1)1/4) \u00b7\n( n\u2211\ni=1\nE[|C\u22121Yi|3] ) .\nIn order to apply the Berry-Esseen bound, we use the properties of the covariance matrix Cov[H(S\u03d5(\u03c0))] from Lemma 26.\nBy Lemma 26, Cov[H(S\u03d5(\u03c0))] is invertible, so we can apply the Berry-Esseen bound. For consistency with the form of the stated bound, we first translate both our distribution, H(S\u03d5(\u03c0)), and the Gaussian to which we want to show it converges, to make both mean-zero. That is, we will show the equivalent statement about H(S\u03d5(\u03c0))\u2212E[H(S\u03d5(\u03c0))] approaching N (0,Cov[H(S\u03d5(\u03c0))]). Note that subtracting the expectations of both H(S\u03d5(\u03c0)) and N (E[H(S\u03d5(\u03c0))],Cov[H(S\u03d5(\u03c0))]) translates the the distributions identically, and note that the convexity of the quantified sets X is invariant under\ntranslation. Therefore, proving the claim on the translated version of our distribution implies the claim on our original distribution.\nBy linearity of expectation, we can express our translated distribution as the sum of n independent random variables:\nH(S\u03d5(\u03c0))\u2212 E[H(S\u03d5(\u03c0))] = n\u2211\ni=1\n1/n(H(S\u03d5(\u03c0i))\u2212 E[H(S\u03d5(\u03c0i))]).\nWe let Yi be the random variable distributed as a single term of the above sum, 1/n(H(S\u03d5(\u03c0i)) \u2212 E[H(S\u03d5(\u03c0i))]). Note that Yi has mean zero, and covariance 1/n2 \u00b7 Cov[H(S\u03d5(\u03c0i))].\nNow, all that remains to show is that the following bound on the convergence rate holds:\nO((m!\u2212 1)1/4) \u00b7\n( n\u2211\ni=1\nE [\u2223\u2223\u2223Cov[H(S\u03d5(\u03c0))]\u22121/2Yi\u2223\u2223\u22233] ) \u2264 O((m!\u2212 1) 1/4)\n(\u03bbmin,S,\u03d5)3/2 \u00b7 1\u221a n .\nWe note that the exponentiated Cov[H(S\u03d5(\u03c0))]\u22121/2 in this expression is well defined because the matrix is symmetric.\nWe first show that |Yi| \u2264 2/n for all i, where |Yi| denotes the L2 norm of Yi. Recall that H(S\u03d5(\u03c0i)) is always either a basis vector or the all 0s vector, and E[H(S\u03d5(\u03c0i))] is the vector whose entry corresponding to \u03c0\u2032 is Pr[S\u03d5(\u03c0) = \u03c0\u2032]. Then, after subtracting the second vector from the first, the negative entries in the resulting vector can sum in magnitude to at most the sum of these probabilities, which is at most 1. Similarly, the positive entries can also sum to at most 1. hence, the L1 norm before scaling by 1/n is at most 2. Using the fact that L2 norms are at most L1 norms, we get that this continues to hold for the L2 norm. After dividing by n, we get that |Yi| \u2264 2/n, as needed.\nNow, per Lemma 26, Cov[H(S\u03d5(\u03c0))] has minimum eigenvalue at least min-prob(S)m!n . It therefore holds that Cov[H(S\u03d5(\u03c0))]\u22121/2 has maximum eigenvalue at most(\nmin-prob(S)\nm!n\n)\u22121/2 = \u221a m!n\nmin-prob(S) .\nMultiplying a vector by a matrix can scale the norm by at most the matrix\u2019s maximum eigenvalue. Thus, combined with our observation that |Yi| \u2264 2/n, the following bound will always hold:\n|Cov[H(S\u03d5(\u03c0))]\u22121/2Yi|3 \u2264\n(\u221a m!n\nmin-prob(S) \u00b7 2 n\n)3 =\n8 \u00b7 (m!)3/2\nn3/2 \u00b7min-prob(S\u03d5)3/2\nBecause this bound holds deterministically on the term above, it must hold also for the expectation of the term above. Thus, by summing over all i, we get that\nn\u2211 i=1 E[|Cov[H(S\u03d5(\u03c0))]\u22121/2Yi|3] \u2264 8 (m!)3/2 \u221a n \u00b7min-prob(S\u03d5)3/2 .\nSince O ( (m!\u2212 1)1/4 ) \u00b7 8m!3/2 \u2208 O ( (m!)7/4 ) , multiplying by the Berry-Esseen constant yields the lemma statement."
        },
        {
            "heading": "C.5 Proof of Lemma 10",
            "text": "Fix S, C, and X = \u22c3\u2113\nj=1Xj where each Xj is convex, X is measure 0, and H(\u03a0 \u00acC) \u2286 X. Since X\nis measure 0, this implies each Xj has measure 0. Fix \u03d5 \u2208 (0, 1], a profile \u03c0 \u2208 \u03a0n, and \u03d5\u2032 \u2208 [\u03d5, 1]. Fix an arbitrary Xj . Note that since Xj has measure zero, the probability mass placed on Xj by a Gaussian with invertible covariance matrix is 0. Hence, Lemma 9 immediately imply that Pr[H(S\u03d5\u2032(\u03c0)) \u2208 Xj ]\u22120 \u2264 O((m!) 7/4)\u221a\nn\u00b7min-prob(S\u03d5\u2032 )3/2 . Using the monotonicity of min-prob (Assumption 4),\nwe get that this is at most O((m!) 7/4)\u221a n\u00b7min-prob(S\u03d5)3/2 (with \u03d5\u2032 replaced with \u03d5). Union bounding over all \u2113 sets Xj tells us that\nPr[H(S\u03d5\u2032(\u03c0)) \u2208 X] \u2264 \u2113 \u00b7O((m!)7/4)\n\u221a n \u00b7min-prob(S\u03d5)3/2 ,\nas needed."
        },
        {
            "heading": "C.6 Proof of Theorem 12",
            "text": "Proof. Fix S, \u03d5, and \u03b4(n). Next, fix a number n, a hyperplane G \u2208 G, a \u03d5\u2032 \u2208 [\u03d5, 1], and a \u03c0\u22c6 \u2208 \u03a0n. Here, G is a hyperplane in m! \u2212 1 dimensions, meaning it is defined by a linear equation whose variables are the profile proportions for rankings in L\u22121, each weighted by a coefficient, a\u03c0, along with a constant, b. In other words, it is the set of proportions h satisfying\nb = \u2211\n\u03c0\u2208L\u22121\na\u03c0 \u00b7 h\u03c0. (2)\nFormally, we need to consider only the intersection of G with our convex hull H, but for our results, it is irrelevant whether we formally make this restriction.\nWe will upper bound Pr[d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n)] by an o(1) function of n (allowing this convergence rate to depend on \u03d5, m, and S, but not G, \u03d5\u2032, or \u03c0\u22c6).\nRecall that the hyperplane G has a coefficient for each ranking except the \u201cmissing\u201d ranking, \u03c0\u22121. We choose \u03c0\nmax \u2208 L such that \u03c0max is the ranking corresponding to a largest-magnitude coefficient in the definition of G, i.e., \u03c0max \u2208 argmax\u03c0\u2208L\u22121 |a\u03c0|. Now, we will define two types of events.\nFirst, let Efew = {\u03c0 \u2208 \u03a0n : |{i \u2208 N : \u03c0i \u2208 {\u03c0max, \u03c0\u22121}}| < np} \u2014 that is, Efew is the set of profiles in which fewer than np agents end up voting either \u03c0max or \u03c0\u22121 in H(S\u03d5\u2032(\u03c0\u22c6)).\nLet V\u2265np = {V \u2286 N : |V | \u2265 np} be the collection of all sets of agents of size at least np. For a set of such agents V \u2208 V\u2265np, we denote the complement of this set of agents as V = N \\V . Now, slightly abusing notation, define \u03a0\u0303V = { (\u03c0i)i\u2208V : \u03c0i \u2208 L \\ {\u03c0max, \u03c0\u22121} } to be the set of all partial profiles in which all agents in V have a ranking other than \u03c0max or \u03c0\u22121. For V \u2208 V\u2265np and partial profile \u03c0\u0303 \u2208 \u03a0\u0303V , let EV,\u03c0\u0303 be the event that agents in V either vote \u03c0 or \u03c0\u2032, and all other agents vote as in \u03c0\u0303, that is,\nEV,\u03c0\u0303 = { \u03c0 \u2208 \u03a0 : \u03c0i = \u03c0\u0303i for i \u2208 V and \u03c0i \u2208 {\u03c0max, \u03c0\u22121} for i \u2208 V } .\nIn the remainder of the proof, we will use that the event Efew, along with the set of all events of the form EV,\u03c0\u0303 (that is, for all subsets of agents V \u2208 V\u2265np and partial profiles \u03c0\u0303 \u2208 \u03a0\u0303V ), form a partition of the full space of profiles. To see how we will use this, recall that we are trying to show that d(H(S\u03d5\u2032(\u03c0\u22c6), G) is likely to be somewhat large. We will show this by showing that,\nconditioned on any individual event EV,\u03c0\u0303, the probability of this distance being large is fairly high, and the remaining event, Efew is unlikely to occur.\nWe will show two claims:\n1. There is some f(n) \u2208 o(1) such that\nPr [ Efew ] \u2264 f(n).\n2. There is some g(n) \u2208 o(1) such that for all EV,\u03c0,\nPr [ d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n) | EV,\u03c0\u0303 ] \u2264 g(n).\nWe first show that together these are sufficient to prove the bound. Indeed, by the law of total probability, as these events form a partition Pr [ [d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n) ] = Pr [ d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n) | Efew ] \u00b7 Pr [ Efew\n] + \u2211 EV,\u03c0 Pr [ d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n) | EV,\u03c0 ] \u00b7 Pr [ EV,\u03c0\n] \u2264 Pr [ d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n) | Efew ] \u00b7 f(n) +\n\u2211 EV,\u03c0 g(n) \u00b7 Pr [ EV,\u03c0 ] = Pr [ d(H(S\u03d5\u2032(\u03c0\u22c6)), G) \u2264 \u03b4(n) | Efew ] \u00b7 f(n) + g(n)\n\u2211 EV,\u03c0 Pr [ EV,\u03c0 ] \u2264 1 \u00b7 f(n) + g(n) \u00b7 1 = f(n) + g(n) \u2208 o(1).\nWe now show the claims. The first claim follows from a straightforward Chernoff bound: each of the n agents places probability at least 2p on ending up in either \u03c0max or \u03c0\u22121, hence the expected number of agents with either \u03c0max or \u03c0\u22121 is at least 2np. Then, given that agents\u2019 rankings are sampled independently, the probability of the total number of such agents being less than half of this expectation is exponentially small in n.\nWe now show the second claim. Fix an arbitrary EV,\u03c0\u0303 and let us consider the conditional distribution of H(S\u03d5(\u03c0\u22c6)) conditioned on this event. Note that all proportions in the support of this distribution match on all entries except that of \u03c0max. Let h\u0303\u2212\u03c0max be the partial proportions over all these matching entries, i.e., all rankings except \u03c0max, so that h\u2212\u03c0max = h\u0303\u2212\u03c0max for all h \u2208 H(EV,\u03c0\u0303).\nNow, since a\u03c0max \u0338= 0, by Equation (2), there exists exactly one completion of the partial proportions h\u2212\u03c0max such that the resulting proportion lies on G. Namely,\npV,\u03c0\u0303 := b\u2212\n\u2211 \u03c0\u2208L\\{\u03c0max,\u03c0\u22121} a\u03c0 \u00b7 h\u0303\u03c0\na\u03c0max .\nNote that this completion need not be a valid proportion, as the value assigned to the proportion \u03c0max, pV,\u03c0\u0303, could need to be irrational, negative, or make all proportions add up to strictly more than one.\nNow, we consider all completions of h\u2212\u03c0max which assign values to the proportion of ranking \u03c0max at least \u03b4(n) larger or smaller than pV,\u03c0\u0303. In particular, we will show (1) that all such proportions are at least \u03b4(n) L1 distance from all points on G, and (2) that we are likely to draw such a profile from H(S\u03d5(\u03c0\u22c6)) conditioned on EV,\u03c0\u0303.\nFix such a completion of h\u0303\u2212\u03c0max , call it h, so that |h\u03c0max \u2212 pV,\u03c0\u0303| > \u03b4(n). By this assumption, we have the following, where in the first step we replace b according to Equation (2):\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u03c0\u2208L\u22121 a\u03c0h\u03c0 \u2212 b \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 \u03c0\u2208L\u22121 a\u03c0h\u03c0 \u2212 pV,\u03c0\u0303 \u00b7 a\u03c0max + \u2211 \u03c0\u2208L\\{\u03c0max,\u03c0\u22121} a\u03c0h\u03c0\n\u2223\u2223\u2223\u2223\u2223\u2223 = |a\u03c0max \u00b7 h\u03c0max \u2212 pV,\u03c0\u0303 \u00b7 a\u03c0max | > |\u03b4(n) \u00b7 a\u03c0max |\nSince a\u03c0max is maximal in magnitude over all coefficients in G, the \u03b4(n)-radius L1-ball around h does not intersect G, because changing any entry of h by at most \u03b4(n) cannot change \u2211 \u03c0\u2208L\u22121 a\u03c0p\u03c0 by more than \u03b4(n) \u00b7 a\u03c0max . We have shown that all completions of h\u2212\u03c0max assigning values of \u03c0\nmax outside pV,\u03c0\u0303 \u00b1 \u03b4(n) must be more than \u03b4(n) L1 distance from G. Next, we show that the probability of drawing such a completion is likely. We will do this by upper-bounding the probability placed on h\u03c0max falling in the interval pV,\u03c0\u0303\u00b1\u03b4(n) by showing its distribution is approximately normal using the Berry-Esseen bound, and showing the normal distribution does not place much mass on this small interval.\nTo this end, let Ii be the indicator random variable that agent i \u2208 V chooses \u03c0max (if they do not choose \u03c0max, they choose \u03c0\u22121). Let S = \u2211 i\u2208V Ii be the random variable representing the total number of agents in V that vote for \u03c0max. Note that the resulting proportion H\u03c0max will be in the range pV ,\u03c0\u0303 \u00b1 \u03b4(n) iff S is in the range n(pV ,\u03c0\u0303 \u00b1 \u03b4(n)), an interval of size 2n\u03b4(n). We now show that the probability S is in any interval of size 2n\u03b4(n) is o(1).\nTo do this, we first state the Berry-Esseen bound.\nLemma 28 (Berry-Esseen Bound [4, 9]). Let X1, . . . , Xn be independent random variables with E[Xi] = \u00b5i, E[|X2i \u2212 \u00b5i|] = \u03c32i > 0, and E[|X3i \u2212 \u00b5i|] = \u03c1i < \u221e. Let T = X1 + \u00b7 \u00b7 \u00b7+Xn. Let F be the cdf of T\u2212 \u2211n\ni=1 \u00b5i\u221a\u2211n i=1 \u03c3 2 i . Then, there exists an absolute constant C1 such that\nsup x\u2208R\n|F (x)\u2212 \u03a6(x)| \u2264 C1 \u00b7\n( n\u2211\ni=1\n\u03c32i )\u22121/2 \u00b7 max 1\u2264i\u2264n \u03c1i \u03c32i .\nWe would like to directly apply Lemma 28 to our S, where each Xi = Ii. We first consider the quantity \u03c32i for each i. Note that min-prob(S \u2032\u03d5) \u2264 Pr[Ii = 1] = 1\u2212Pr[Ii = 0] \u2264 1\u2212min-prob(S \u2032\u03d5). Further, by monotonicity (Assumption 4), min-prob(S \u2032\u03d5) \u2265 min-prob(S \u2032\u03d5), so min-prob(S\u03d5) \u2264 Pr[Ii = 1] \u2264 1 \u2212 min-prob(S\u03d5). Since each Ii is a Bernoulli, each \u03c32i \u2265 min-prob(S\u03d5) \u00b7 (1 \u2212 min-prob(S\u03d5)) \u2265 min-prob(S\u03d5)/2. This implies that \u2211n i=1 \u03c3 2 i \u2265 n \u00b7 min-prob(S\u03d5)/2. Further, note that since each Ii is bounded in [0, 1], each \u03c1i \u2264 1. This implies the error bound of Lemma 28 is at most\nC1 \u00b7 ( n \u00b7\nmin-prob(S\u03d5) 2 )\u22121/2 \u00b7 2 min-prob(S\u03d5) = O ( 1 min-prob(S\u03d5)3/2 \u00b7 \u221a n ) .\nFurther, the probability S is in some range of size 2n\u03b4(n) will be equivalent to the probability S\u2212 \u2211n i=1 \u00b5i\u221a\u2211n i=1 \u03c3 2 i is in a specific range of size 2n\u03b4(n)\u221a\u2211n i=1 \u03c3 2 i \u2264 2\u03b4(n) \u221a 2n\u221a min-prob(S\u03d5) . Let F be the CDF of this random variable. The probability this random variable is in a range of this size is at most\nsup x\n( F ( x+ 2\u03b4(n) \u221a 2n\u221a\nmin-prob(S\u03d5)\n) \u2212 F (x) )\n\u2264 sup x\n( \u03a6 ( x+ 2\u03b4(n) \u221a 2n\u221a\nmin-prob(S\u03d5)\n) \u2212 \u03a6(x) ) + 2O ( 1\nmin-prob(S\u03d5)3/2 \u00b7 \u221a n\n)\n=sup x \u222b x+ 2\u03b4(n)\u221a2n\u221amin-prob(S\u03d5) x \u03d5(x) + 2O( 1 min-prob(S\u03d5)3/2 \u00b7 \u221a n )\n\u2264 sup x \u222b x+ 2\u03b4(n)\u221a2n\u221amin-prob(S\u03d5) x e\u221a 2\u03c0 + 2O( 1 min-prob(S\u03d5)3/2 \u00b7 \u221a n )\n=sup x ( e\u221a 2\u03c0 \u00b7 2\u03b4(n) \u221a 2n\u221a\nmin-prob(S\u03d5)\n) + 2O ( 1\nmin-prob(S\u03d5)3/2 \u00b7 \u221a n\n)\n=O ( 1\u221a\nmin-prob(S\u03d5) \u00b7 \u03b4(n)\n\u221a n ) +O ( 1\nmin-prob(S\u03d5)3/2 \u00b7 \u221a n ) \u2208o(1).\nThis is our desired g(n) which completes the proof of the second claim."
        },
        {
            "heading": "D Supplemental Materials for Section 4",
            "text": ""
        },
        {
            "heading": "D.1 Proof of Lemma 16",
            "text": "Proof. We will prove these bounds by lower-bounding q(k). First, we have that\nq(k) = 1\n1\u2212 \u03d5k+1\n( 1\u2212 (1\u2212 \u03d5)k\u03d5 k\n1\u2212 \u03d5k\n) \u2265 1\u2212 (1\u2212 \u03d5)k\u03d5 k\n1\u2212 \u03d5k \u2265 1\u2212 k\u03d5k.\nSecond, by the AM-GM inequality, we have that\nq(k) = 1\n1\u2212 \u03d5k+1\n( 1\u2212 (1\u2212 \u03d5)k\u03d5 k\n1\u2212 \u03d5k\n) \u2265 1\u2212 (1\u2212 \u03d5)k\u03d5 k\n1\u2212 \u03d5k .\nSimplifying, this is equal to 1\u2212 k\u03d5 k\u2211k\u22121\nj=0 \u03d5 j , and the second upper-bound follows:\n1\u2212 k\u03d5 k\u2211k\u22121\nj=0 \u03d5 j = 1\u2212 1 1 k \u2211k j=1 1 \u03d5j \u2265 1\u2212 1(\u220fk j=1 1 \u03d5j )1/k = 1\u2212 \u03d5 k(k+1)2 \u00b7 1k = 1\u2212 \u03d5 k+12 \u2265 1\u2212 \u03d5k/2. Third,\nq(k) \u2265 q(1) = 1 1\u2212 \u03d52\n( 1\u2212 (1\u2212 \u03d5)\u03d5\n1\u2212 \u03d5\n) =\n1\u2212 \u03d5 1\u2212 \u03d52 = 1 1 + \u03d5 = 1\u2212 \u03d5 1 + \u03d5 ."
        },
        {
            "heading": "D.2 Proof of Proposition 18",
            "text": "Proof. We begin with Resolvability and show \u03c1(n)\u2212Group-Stability later. Fix a starting profile \u03c0 and two candidates, a and b. Our approach will be to first upper bound the probability that a and b have the same Borda score post-noise, and then union bound over all pairs of candidates. For a candidate c, let Xci \u2208 {0, . . . ,m\u22121} be the random variable representing the number of Borda points given to candidate c by voter i, post-noise. Let Yi = X a i \u2212 Xbi be the difference in Borda\npoints given to a and b by voter i. Our goal is to upper bound Pr[ \u2211\ni Yi = 0]. As in the proof for Plurality, we will do so via use Berry-Esseen using that the distribution of \u2211 i Yi converges\nto a Gaussian. Since the Gaussian places zero mass on the value zero, we simply upper bound Pr[ \u2211\ni Yi = 0] by the convergence rate of its distribution to a Gaussian, given by Berry-Essen exactly as in Equation (1).\nAgain, bounding this upper bound\u2019s component terms: Note that Yi \u2208 [\u2212(m\u2212 1), (m\u2212 1)], so |Yi \u2212 E[Yi]| \u2264 2m, implying the same bound on maxi \u03c1i\u03c32i . To lower bound each \u03c3 2 i , we consider the distribution of Yi. In particular, we will upper bound the probability of any specific (integer) value by 11+\u03d5 . That is, for all values \u2113,\nPr[Yi = \u2113] \u2264 1\n1 + \u03d5 (3)\nOnce we have shown Inequality (3) holds, note that at most one value \u2113 can be within 1/2 of E[Yi], so this will imply that with probability at least 1\u2212 11+\u03d5 = \u03d5 1+\u03d5 \u2265 \u03d5 2 , |Yi \u2212E[Yi]| \u2265 1/2. This allows us to lower bound the variance by \u03d52 \u00b7 ( 1 2) 2 = \u03d58 . This gives us an overall bound of O( m\u221a n\u03d5 ), and therefore O( m 3\n\u221a n\u03d5 ) after a union bound.\nFinally, we show Inequality (3). Fix a value \u2113. Let \u03c4 \u2208 L(M \\ {a}) be an arbitrary partial ranking of the candidates without \u2113. We will write \u03c3\\{a} = \u03c4 when the partial ranking of \u03c3 without a included matches \u03c4 . We will show that when \u03c3 is sampled from a Mallows model, conditioned on it matching \u03c4 , the probability a is \u2113 positions above b is at most 11+\u03d5 . Since \u03c4 was arbitrary, by law of total probability, this implies the same bound in general. Indeed, note that once \u03c4 is fixed, there are m possible rankings after the insertion of a. At most one can correspond to a difference of \u2113. Call this ranking \u03c3. Further, consider inserting a in the neighboring position. We will call this ranking \u03c3\u2032. Note that the Kendall tau distance from the starting ranking \u03c3i of \u03c3 and \u03c3\n\u2032 can differ by at most 1. This implies that Pr[\u03c3\u2032|\u03c3 \\ {a} = \u03c4 ]/Pr[\u03c3|\u03c3 \\ {a} = \u03c4 ] \u2265 \u03d5. This directly implies that Pr[\u03c3|\u03c3 \\ {a} = \u03c4 ] \u2264 11+\u03d5 , as needed.\nFor \u03c1(n)\u2212Group-Stability, note that a single voter can affect the difference in scores of a candidate by at most 2(m \u2212 1). Hence, as long as the maximal candidate is winning by at least 2(m\u2212 1)\u03c1(n), no group of size \u03c1(n) can change the outcome. Rather than just doing the analysis for \u2211 i Yi = 0, we can do the same for all values in [\u22122(m \u2212 1)\u03c1(n), 2(m \u2212 1)\u03c1(n)], which, via a union bound, adds an additional O(m\u03c1(n)) factor."
        },
        {
            "heading": "D.3 Proof of Proposition 20",
            "text": "Proof. We begin with Resolvability. Fix some n that is divisible by 4 and first, assume m is even (we will describe how to extend it to the odd case later). Consider the following instance. Let a and b be two candidates and let C and D be a partition of the remaining candidates into two sets of equal size, each having m/2\u2212 1 candidates. Let \u03c4C and \u03c4D be arbitrary orders of the candidates in C and D respectively, and let rev(\u03c4C) and rev(\u03c4D) be the reversals of these rankings respectively. Finally, assume the voter rankings are as follows, each occurring n/4 times:\n1. b \u227b rev(\u03c4C) \u227b a \u227b \u03c4D\n2. b \u227b rev(\u03c4D) \u227b a \u227b \u03c4C\n3. a \u227b rev(\u03c4C) \u227b b \u227b \u03c4D\n4. a \u227b rev(\u03c4D) \u227b b \u227b \u03c4C\nWe will show that except for with exponentially small probability, a and b still beat every other candidate in a pairwise competition. However, with a reasonably large probability, a and b are pairwise tied. When these events occur, a and b have the same optimal Minimax score of n/2, while all others have strictly less, and are hence tied.\nFor the first, consider an arbitrary candidate c \u2208 C. The arguments for if c \u2208 D or with b instead of a will be symmetric. Let Xi be a random variable that is 1 if i ranks a above c in the final ranking and \u22121 otherwise. We would like to upper bound the probability that \u2211 iXi \u2264 0. We will do this using an application of Hoeffding\u2019s inequality. To do this, we must obtain bounds on E[ \u2211 iXi].\nSuppose c is ranked k\u2019th in \u03c4C . Note that c is only preferred to a on voters in the first group. For such a voter i, as there is a probability q\u0304(k) of a swap, so E[Xi] = 2q\u0304(k)\u2212 1. For voters in the second group, a is preferred to c by the same magnitude, hence, for such a voter i, E[Xi] = 1\u22122q\u0304(k), the exact negative of the first. For voters in the third group, since a is preferred to c, they retain their order with probability at least 1/2, so for such voters i, E[Xi] \u2265 0. For voters in the fourth\ngroup, a is preferred to c by |D| + 1 + k positions, hence, E[Xi] \u2265 2q(|D| + 1 + k) \u2212 1. Since q\u0304 is decreasing and |D|+1+ k \u2265 m/2, this is at least 1\u2212 2q\u0304(m/2). Putting this together, we have that\nE[ \u2211 i Xi] \u2265 n/4 \u2217 (2q(k)\u2212 1) + n/4(1\u2212 2q(k)) + n/4(1\u2212 2q\u0304(m/2)) = n/4(1\u2212 2q\u0304(m/2)).\nPlugging this into Hoeffding\u2019s inequality, since the variables take on values in [\u22121, 1], this implies that\nPr[ \u2211 i Xi \u2264 0] \u2264 exp ( \u2212n(1\u2212 2q\u0304(m/2)) 2 32 ) .\nUnion bounding over allm candidates choices of c and 2 choices of a and b, we get that the probability a and b pairwise beat all other candidates except for with probability 2m exp ( \u2212n(1\u22122q\u0304(m/2)) 2\n32\n) .\nNext, we consider the relative rankings of a and b. Note that on groups one and two, b is beating a, and on groups three and four, a is beating b. In all cases, the margin of victory is m/2 positions. The probability of a swap on any of these rankings is q\u0304(m/2). Hence, the number of swaps on the first n/2 candidates follows a Bin(n/2, q\u0304(m/2)) distribution, and symmetrically, so does the number of swaps on the second n/2. Candidates a and b will be tied in a pairwise competition if the two (independent) draws from these binomials are equal. We show next that this occurs with probability \u2126( 1\u221a\nnq\u0304(m/2) ) which follows directly from the following lemma.\nLemma 29. The probability two independent draws from a Bin(n, p) distribution are equal is \u2126( 1\u221anp).\nProof. Fix an arbitrary n and p. Note that the mean of a binomial distribution is np. By standard Chernoff bounds, if X follows a Bin(n, p) distribution, Pr[|X \u2212 \u00b5| \u2265 \u03b4\u00b5] \u2264 2 exp(\u2212\u03b42\u00b5/3) for all \u03b4 \u2208 (0, 1) where \u00b5 = np . In particular, plugging in \u03b4 = 2/\u221a\u00b5, we get that with constant probability (\u2265 1 \u2212 2 exp(\u22124/3) \u2265 .47), X is within 2\u221a\u00b5 of its mean. Note that there are at most 4 \u221a np+ 1 integer values within 2 \u221a np of the mean. With constant probability (\u2265 .472), both binomials will take on one of these values. Conditioned on this event, the probability they are equal is at least 14\u221anp+1 . Indeed, suppose conditioned on being one of these 1 4 \u221a np+1 values, each\nwas taken with probability p1, p2, . . . , p4\u221anp+1 with \u22114\u221anp+1 i=1 pi = 1. The probability they are equal\nis \u22114\u221anp+1\ni=1 p 2 i \u2265 \u2211 i pi 4 \u221a np+1 = 1 4 \u221a np+1 . Hence, the probability they are equal is at least \u2126( 1\u221a np).\nCombining the fact that a and b are pairwise tied with probability \u2126( 1\u221a nq\u0304(m/2) ) and pairwise beat all others with probability 1\u2212O ( exp(\u2212n(1\u22122q\u0304(m/2)) 2\n32\n) completes the bound.\nTo handle m which is odd, we can add a dummy candidate to the end of all the rankings. This candidate wins with even smaller probability, so all of the analysis continues to hold. We simply need to replace q\u0304(m/2) with q\u0304(\u230am/2\u230b).\nNext, consider 1-Strategy-Proofness. We will have the following starting profile. Let a, b, x, y be candidates, and let S1, S2, S3, S4 be sets of candidates of size \u230am4 \u230b (as before, we will put remaining candidates at the bottom, but this will not affect the analysis much).\n(n\u2212 2)/4 voters: a \u227b rev(\u03c4S1) \u227b x \u227b rev(\u03c4S2) \u227b b \u227b \u03c4S3 \u227b y \u227b \u03c4S4 (n\u2212 2)/4 voters: a \u227b rev(\u03c4S4) \u227b x \u227b rev(\u03c4S3) \u227b b \u227b \u03c4S2 \u227b y \u227b \u03c4S1 (n\u2212 2)/4 voters: b \u227b rev(\u03c4S1) \u227b y \u227b rev(\u03c4S2) \u227b a \u227b \u03c4S3 \u227b x \u227b \u03c4S4 (n\u2212 2)/4 voters: b \u227b rev(\u03c4S4) \u227b y \u227b rev(\u03c4S3) \u227b a \u227b \u03c4S2 \u227b x \u227b \u03c4S1 1 voter: x \u227b rev(\u03c4S2) \u227b b \u227b rev(\u03c4S4) \u227b y \u227b \u03c4S3 \u227b a \u227b \u03c4S1 1 voter: y \u227b rev(\u03c4S1) \u227b a \u227b rev(\u03c4S3) \u227b b \u227b \u03c4S4 \u227b x \u227b \u03c4S2\nThe analysis will begin similarly to Resolvability, showing that a and b both beat all candidates outside of {a, b, x, y} by a pairwise majority with all but exponentially small probability. Then, we will show that it is reasonably likely that the relative orders of a, b, x, and y remain exactly as they are in the starting profile for all voters. Under this condition, we can check that the winner is in fact b. Indeed, b at minimum pairwise ties with all others, while for all other candidates, we can find a candidate that strictly pairwise beats them, namely a beats x on n\u2212 1 votes, b beats y on n\u22121 votes, and y beats a on n/2+1 votes, and the rest are beaten by both a and b. However, if the last voter bumps up a to the top of their ranking and pushes b to the bottom to instead report a \u227b y \u227b rev(\u03c4S1) \u227b rev(\u03c4S3) \u227b \u03c4S4 \u227b x \u227b \u03c4S2 \u227b b, a will now pairwise beat all other candidates while b will be beaten by x on n/2+ 1 votes. This leads a to be the unique winner instead of b, an improvement according to manipulating voter.\nWe now compute the relevant probability bounds. One can check as we did with resolvability that for any candidate j \u2208 Si, even if they beat one of a or b on a certain number of votes, these can be matched with an equal number where a and b beat them by the same amount. For example, consider a candidate j \u2208 S1 ranked k\u2019th in \u03c4S1 . Although they beat a on the third group of voters, this can be matched to the voters in the fourth group that prefer a to j by the exact same margin. Further, although the last voter prefers j to a, the second to last prefers a to j by the same margin. As before, there is an additional (n \u2212 2)/4 size group that prefers a to j by a margin of at least \u230am/2\u230b. Hence, by the same analysis as for resolvability, we get that a and b strictly pairwise beat all other candidates except for with probability 2m exp ( \u2212(n\u22122)(1\u22122q\u0304(\u230am/2\u230b))2\n32\n) .\nNext, we consider the probability that an individual voter maintains their order of {a, x, b, y}. Suppose, without loss of generality, they rank a \u227b x \u227b b \u227b y. Note that as long as they continue to rank a \u227b x, x \u227b b, and b \u227b y, then by transitivity, the entire partial ranking will remain. Each of these occurs with probability 1\u2212 q(\u230am/4\u230b) as there is always a gap at least this large between the candidates. Hence, union bounding over all voters, we get that all will maintain their orders with probability at least 1 \u2212 3nq(\u230am/4\u230b). Putting this together we have that the overall convergence rate is lower bounded by 1\u2212 3nq(\u230am/4\u230b)\u2212O ( m exp ( \u2212(n\u22122)(1\u22122q\u0304(\u230am/2\u230b))2\n32\n)) ."
        },
        {
            "heading": "E Supplemental Material from Section 5",
            "text": "Corollary 30 (Gibbard-Satterthwaite). Per Table 2, there exist several voting rules that are simultaneously non-dictatorial, permit m \u2265 3, and smoothed-satisfy Strategyproofness.19\nCorollary 31 (ANR). Per Table 1, there exist several voting rules that simultaneously satisfy Anonymity and Neutrality and smoothed-satisfy Resolvability.\n19\u221an-Group-Strategyproofness =\u21d2 1--Group-Strategyproofness, i.e., Strategyproofness.\nCorollary 32 (Condorcet-Participation). Per Theorem 11, there exist several voting rules that simultaneously satisfy Condorcet and smoothed-satisfy Participation.20\nMore general criteria. Here; we consider the more general \u201ccriterion\u201d that \u03c0 contains no Condorcet cycle. Note that this is not strictly an axiom, as whether it holds on a profile \u03c0 depends only on the profile itself\u2014not on any voting rule. To be formal, we think of this as a more generic criterion C : \u03a0 \u2192 {true, false}, representing the true/false statement \u201cC holds on \u03c0\u201d. We can think of A(R) as an example of such a generic criterion.\nProposition 33. Let C be the criterion that \u03c0 contains no Condorcet Cycle. C is smoothedviolated.\nProof. This proposition is proven using the same procedure as for criterion of the form A(R), as before: we find a profile where all nearby histograms have a Condorcet Cycle, implying the preconditions of Lemma 7. We define this profile to be any consistent with the histogram h, in which 1/3 of voters have a \u227b b \u227b c, 1/3 of voters have b \u227b c \u227b a, and 1/3 of voters have c \u227b a \u227b b. This profile has a Condorcet cycle: a pairwise-dominates b, b pairwise dominates c, and c pairwise dominates a.\nNow, we will establish that h satisfies the precondition of Lemma 7. Fix any histogram h\u2032 such that \u2225h\u2212 h\u2032\u22251 < 1/6. Assume for the sake of contradiction that h\u2032 does not contain a Condorcet cycle. It follows that at least one relative pairwise domination flipped from h to h\u2032. All pairwise dominations in h were symmetric, so we wlog consider a\u2019s pairwise domination of b. In order for b to pairwise dominate a, at least 1/2 \u2212 1/3 = 1/6 of voters must have reversed their relative ranking of a and b from h to h\u2032\u2014i.e., \u2225h\u2032 \u2212 h\u22251 \u2265 1/6. This is a contradiction, and we conclude the claim.\n20\u221an-Group-Participation =\u21d2 1-Group-Participation, i.e., Participation."
        }
    ],
    "title": "Smoothed Analysis of Social Choice, Revisited",
    "year": 2023
}