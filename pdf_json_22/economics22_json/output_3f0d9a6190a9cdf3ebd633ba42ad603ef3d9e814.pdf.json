{
    "abstractText": "We present a novel approach to probabilistic electricity price forecasting which utilizes distributional neural networks. The model structure is based on a deep neural network that contains a so-called probability layer. The network\u2019s output is a parametric distribution with 2 (normal) or 4 (Johnson\u2019s SU) parameters. In a forecasting study involving day-ahead electricity prices in the German market, our approach significantly outperforms state-of-the-art benchmarks, including LASSO-estimated regressions and deep neural networks combined with Quantile Regression Averaging. The obtained results not only emphasize the importance of higher moments when modeling volatile electricity prices, but also \u2013 given that probabilistic forecasting is the essence of risk management \u2013 provide important implications for managing portfolios in the power sector.",
    "authors": [
        {
            "affiliations": [],
            "name": "Grzegorz Marcjasza"
        },
        {
            "affiliations": [],
            "name": "Micha\u0142 Narajewskib"
        },
        {
            "affiliations": [],
            "name": "Rafa\u0142 Werona"
        },
        {
            "affiliations": [],
            "name": "Florian Zielb"
        }
    ],
    "id": "SP:6a7a06f31b2547a65478abc23e4d53b17fb9db94",
    "references": [
        {
            "authors": [
                "M. Abadi",
                "A. Agarwal",
                "P Barham"
            ],
            "title": "TensorFlow: LargeScale Machine Learning on Heterogeneous Systems. URL: https: //www.tensorflow.org/. software available from tensorflow.org",
            "year": 2015
        },
        {
            "authors": [
                "E. Abramova",
                "D. Bunn"
            ],
            "title": "Forecasting the intra-day spread densities of electricity prices. Energies",
            "year": 2020
        },
        {
            "authors": [
                "M. Afrasiabi",
                "M. Mohammadi",
                "M. Rastegar",
                "L. Stankovic",
                "S. Afrasiabi",
                "M. Khazaei"
            ],
            "title": "Deep-based conditional probability density function forecasting of residential loads",
            "venue": "IEEE Transactions on Smart Grid",
            "year": 2020
        },
        {
            "authors": [
                "T. Akiba",
                "S. Sano",
                "T. Yanase",
                "T. Ohta",
                "M. Koyama"
            ],
            "title": "Optuna: A next-generation hyperparameter optimization framework",
            "venue": "in: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,",
            "year": 2019
        },
        {
            "authors": [
                "E.A. Barnes",
                "R.J. Barnes"
            ],
            "title": "Controlled abstention neural networks for identifying skillful predictions for classification problems",
            "venue": "Journal of Advances in Modeling Earth Systems",
            "year": 2021
        },
        {
            "authors": [
                "E.A. Barnes",
                "R.J. Barnes",
                "N. Gordillo"
            ],
            "title": "Adding uncertainty to neural network regression tasks in the geosciences arXiv:2109.07250",
            "year": 2021
        },
        {
            "authors": [
                "A.G. Bill\u00e9",
                "A. Gianfreda",
                "F. Del Grosso",
                "F. Ravazzolo"
            ],
            "title": "Forecasting electricity prices with expert, linear, and nonlinear models",
            "venue": "International Journal of Forecasting",
            "year": 2022
        },
        {
            "authors": [
                "J. Browell",
                "C. Gilbert"
            ],
            "title": "Predicting electricity imbalance prices and volumes: Capabilities and opportunities. Energies",
            "year": 2022
        },
        {
            "authors": [
                "A. Brusaferri",
                "M. Matteucci",
                "D. Ramin",
                "S. Spinelli",
                "A. Vitali"
            ],
            "title": "Probabilistic day-ahead energy price forecast by a Mixture Density",
            "venue": "Recurrent Neural Network,",
            "year": 2020
        },
        {
            "authors": [
                "G. Dudek"
            ],
            "title": "Multilayer perceptron for GEFCom2014 probabilistic electricity price forecasting",
            "venue": "International Journal of Forecasting",
            "year": 2016
        },
        {
            "authors": [
                "B. Efron"
            ],
            "title": "Bootstrap Methods: Another Look at the Jackknife",
            "year": 1979
        },
        {
            "authors": [
                "A. Gianfreda",
                "D. Bunn"
            ],
            "title": "A stochastic latent moment model for",
            "year": 2018
        },
        {
            "authors": [
                "T. Gneiting"
            ],
            "title": "Quantiles as optimal point forecasts",
            "venue": "International Journal of Forecasting",
            "year": 2011
        },
        {
            "authors": [
                "Y. He",
                "W. Zhang"
            ],
            "title": "Probability density forecasting of wind power based on multi-core parallel quantile regression neural network",
            "venue": "Knowledge-Based Systems",
            "year": 2020
        },
        {
            "authors": [
                "T. Hong",
                "P. Pinson",
                "S. Fan",
                "H. Zareipour",
                "A. Troccoli",
                "R.J. Hyndman"
            ],
            "title": "Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond",
            "venue": "International Journal of Forecasting",
            "year": 2016
        },
        {
            "authors": [
                "T. Hong",
                "P. Pinson",
                "Y. Wang",
                "R. Weron",
                "D. Yang",
                "H. Zareipour"
            ],
            "title": "Energy forecasting: A review and outlook",
            "venue": "IEEE Open Access Journal of Power and Energy",
            "year": 2020
        },
        {
            "authors": [
                "K. Hubicka",
                "G. Marcjasz",
                "R. Weron"
            ],
            "title": "A note on averaging day-ahead electricity price forecasts across calibration windows",
            "venue": "IEEE Transactions on Sustainable Energy",
            "year": 2018
        },
        {
            "authors": [
                "H. Jahangir",
                "H. Tayarani",
                "S. Baghali",
                "A. Ahmadian",
                "A. Elkamel",
                "M.A. Golkar",
                "M. Castilla"
            ],
            "title": "A novel electricity price forecasting approach based on dimension reduction strategy and rough artificial neural networks",
            "venue": "IEEE Transactions on Industrial Informatics",
            "year": 2019
        },
        {
            "authors": [
                "J. Janczura",
                "E. W\u00f3jcik"
            ],
            "title": "Dynamic short-term risk management strategies for the choice of electricity market based on probabilistic forecasts of profit and risk measures. the German and the Polish market case study",
            "venue": "Energy Economics",
            "year": 2022
        },
        {
            "authors": [
                "T. Janke",
                "F. Steinke"
            ],
            "title": "Forecasting the price distribution of continuous intraday electricity trading. Energies",
            "year": 2019
        },
        {
            "authors": [
                "N.L. Johnson"
            ],
            "title": "Systems of frequency curves generated by methods of translation",
            "venue": "Biometrika",
            "year": 1949
        },
        {
            "authors": [
                "O.A. Karabiber",
                "G. Xydis"
            ],
            "title": "Electricity price forecasting in the Danish day-ahead market using the TBATS",
            "venue": "ANN and ARIMA methods. Energies",
            "year": 2019
        },
        {
            "authors": [
                "D. Keles",
                "J. Scelle",
                "F. Paraschiv",
                "W. Fichtner"
            ],
            "title": "Extended forecast methods for day-ahead electricity spot prices applying artificial neural networks",
            "venue": "Applied Energy",
            "year": 2016
        },
        {
            "authors": [
                "E. Kraft",
                "D. Keles",
                "W. Fichtner"
            ],
            "title": "Modeling of frequency",
            "year": 2020
        },
        {
            "authors": [
                "R. Mori"
            ],
            "title": "Off-block time prediction using operators",
            "year": 2021
        },
        {
            "authors": [
                "M. Narajewski",
                "F. Ziel"
            ],
            "title": "Optimal bidding on hourly and",
            "year": 2022
        },
        {
            "authors": [
                "D. Nix",
                "A. Weigend"
            ],
            "title": "Estimating the mean and variance",
            "year": 1994
        },
        {
            "authors": [
                "J. Nowotarski",
                "R. Weron"
            ],
            "title": "Recent advances in electricity price",
            "year": 2018
        },
        {
            "authors": [
                "I. Oksuz",
                "U. Ugurlu"
            ],
            "title": "Sustainable Energy Reviews",
            "year": 2019
        },
        {
            "authors": [
                "T. Serafin",
                "B. Uniejewski",
                "R. Weron"
            ],
            "title": "Averaging predictive distributions across calibration windows for day-ahead electricity price forecasting",
            "venue": "Energies",
            "year": 2019
        },
        {
            "authors": [
                "R. Sgarlato",
                "F. Ziel"
            ],
            "title": "The role of weather predictions in electricity price forecasting beyond the day-ahead horizon",
            "venue": "IEEE Transactions on Power Systems",
            "year": 2022
        },
        {
            "authors": [
                "R. Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
            "year": 1996
        },
        {
            "authors": [
                "B. Uniejewski",
                "G. Marcjasz",
                "R. Weron"
            ],
            "title": "Understanding intraday electricity markets: Variable selection and very short-term price forecasting using LASSO",
            "venue": "International Journal of Forecasting",
            "year": 2019
        },
        {
            "authors": [
                "J. Viehmann"
            ],
            "title": "2017. State of the German Short-Term Power Market",
            "venue": "Zeitschrift fu\u0308r Energiewirtschaft",
            "year": 2017
        },
        {
            "authors": [
                "C. Wan",
                "Z. Xu",
                "Y. Wang",
                "Z.Y. Dong",
                "K.P. Wong"
            ],
            "title": "A hybrid approach for probabilistic forecasting of electricity price",
            "venue": "IEEE Transactions on Smart Grid",
            "year": 2013
        },
        {
            "authors": [
                "R. Weron"
            ],
            "title": "Electricity price forecasting: A review of the stateof-the-art with a look into the future",
            "venue": "International journal of forecasting",
            "year": 2014
        },
        {
            "authors": [
                "R. Weron",
                "F. Ziel"
            ],
            "title": "Electricity price forecasting, in: Soyta\u015f",
            "venue": "Routledge handbook of energy economics. Routledge,",
            "year": 2019
        },
        {
            "authors": [
                "P.M. Williams"
            ],
            "title": "Using neural networks to model conditional multivariate densities",
            "venue": "Neural Computation",
            "year": 1996
        },
        {
            "authors": [
                "A. Yang",
                "W. Li",
                "X. Yang"
            ],
            "title": "Short-term electricity load forecasting based on feature selection and least squares support vector machines",
            "venue": "Knowledge-Based Systems",
            "year": 2019
        },
        {
            "authors": [
                "M. Zahid",
                "F. Ahmed",
                "N. Javaid",
                "R.A. Abbasi",
                "H.S. Zainab Kazmi",
                "A. Javaid",
                "M. Bilal",
                "M. Akbar",
                "M. Ilahi"
            ],
            "title": "Electricity price and load forecasting using enhanced convolutional neural network and enhanced support vector regression in smart",
            "venue": "grids. Electronics",
            "year": 2019
        },
        {
            "authors": [
                "F. Zhang",
                "H. Fleyeh",
                "C. Bales"
            ],
            "title": "A hybrid model based on bidirectional long short-term memory neural network and Catboost for short-term electricity spot price forecasting",
            "venue": "Journal of the Operational Research",
            "year": 2020
        },
        {
            "authors": [
                "S. Zhou",
                "L. Zhou",
                "M. Mao",
                "H.M. Tai",
                "Y. Wan"
            ],
            "title": "An optimized heterogeneous structure LSTM network for electricity price forecasting",
            "venue": "IEEE Access",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhou",
                "J. Wang",
                "H. Wang",
                "J. Lin"
            ],
            "title": "Panel semiparametric quantile regression neural network for electricity consumption forecasting",
            "venue": "Ecological Informatics",
            "year": 2022
        },
        {
            "authors": [
                "F. Ziel"
            ],
            "title": "Forecasting electricity spot prices using lasso: On capturing the autoregressive intraday structure",
            "venue": "IEEE Transactions on Power Systems",
            "year": 2016
        },
        {
            "authors": [
                "F. Ziel",
                "R. Steinert"
            ],
            "title": "Probabilistic mid-and long-term electricity price forecasting",
            "venue": "Renewable and Sustainable Energy Reviews",
            "year": 2018
        },
        {
            "authors": [
                "F. Ziel",
                "R. Weron"
            ],
            "title": "Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks",
            "venue": "Energy Economics",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "We present a novel approach to probabilistic electricity price forecasting which utilizes distributional neural networks. The model structure is based on a deep neural network that contains a so-called probability layer. The network\u2019s output is a parametric distribution with 2 (normal) or 4 (Johnson\u2019s SU) parameters. In a forecasting study involving day-ahead electricity prices in the German market, our approach significantly outperforms state-of-the-art benchmarks, including LASSO-estimated regressions and deep neural networks combined with Quantile Regression Averaging. The obtained results not only emphasize the importance of higher moments when modeling volatile electricity prices, but also \u2013 given that probabilistic forecasting is the essence of risk management \u2013 provide important implications for managing portfolios in the power sector.\nKeywords: distributional neural network, probabilistic forecasting, quantile regression, lasso, electricity prices, Johnson\u2019s SU distribution"
        },
        {
            "heading": "1. Introduction and motivation",
            "text": "Trading in competitive markets requires precise probabilistic forecasts. Therefore, the attention of researchers and practitioners is shifting recently from point to probabilistic forecasting methods. It is not different in electricity markets since the liberalization starting in 1990s. The point electricity price forecasting (EPF) literature is very broad, and the topic is well-researched (Weron, 2014). However, these models can be used in probabilistic forecasting only to a limited extent, as they mostly predict only the expected price, rarely quantiles or other characteristics. A proper risk optimization that is compulsory in the very volatile and uncertain electricity markets can only be carried out using methods that provide a broader view, such as e.g. distributional forecasting. It did not go unnoticed to researchers (Nowotarski and Weron, 2018; Petropoulos et al., 2022), however the literature on probabilistic EPF is much scarcer than the one on point EPF.\nWe propose a probabilistic EPF approach based on distributional neural networks. More specifically, we consider a \u2018vanilla\u2019 deep neural network (DNN), i.e., a multi-layer perceptron in which the information propagates only forward. We utilize the TensorFlow (Abadi et al., 2015) and Keras (Chollet et al., 2015) frameworks, and let the output layer be a parametric distribution with 2 or 4 parameters (Barnes and Barnes, 2021; Barnes et al., 2021; Salinas et al., 2020). The difference to a standard network providing point forecasts is only in the output layer. Thus, if we have already built a neural network model for point forecasting, it is very easy to convert it to a distributional one. Even though the method is not new (Nix and Weigend, 1994; Williams, 1996), it has not attracted much\nattention. To the best of our knowledge, the only existing distributional networks in the energy forecasting literature use mixtures of normal distributions obtained using complex structures comprising convolutional neural networks (CNN) and gated recurrent units (GRU) (Afrasiabi et al., 2020) or recurrent neural networks (RNN) (Brusaferri et al., 2020; Mashlakov et al., 2021). The distributional deep neural network (DDNN) proposed in this paper is far less complex than the CNNs, GRUs and RNNs, easier to interpret and computationally less demanding.\nNeural network-based models are popular and have been shown to achieve good predictive accuracy in the fields of point EPF (Keles et al., 2016; Lago et al., 2021), load forecasting \u2013 both point (Yang et al., 2019) and quantile (Zhou et al., 2022) as well as quantile wind power forecasting (He and Zhang, 2020). However, despite the relative simplicity of moving from point to probabilistic forecasting using the TensorFlow framework, it has not been utilized in EPF yet.\nThe performance of the model is evaluated using a rolling window forecasting study with the day-ahead electricity prices in Germany. The DDNN is benchmarked against naive bootstrapping and two well performing point EPF approaches (Lago et al., 2021): a lasso estimated autoregression (LEAR) and a DNN, both combined with quantile regression averaging (QRA) for converting point predictions into probabilistic ones.\nThe major contributions of the manuscript are as follows:\n1. It is the first work to consider the DDNN and one of the first to consider probabilistic neural networks in electricity price forecasting.\nPreprint submitted to Elsevier December 13, 2022\nar X\niv :2\n20 7.\n02 83\n2v 2\n[ q-\nfi n.\nST ]\n1 0\nD ec\n2 02\n2\n2. The proposed method is very simple compared to the existing neural network solutions. As we show in this paper, the generalization from a point to a distributional DNN requires almost no effort.\n3. If needed, point forecasts can be easily derived from the predicted distribution.\n4. It is the first in EPF and one of the first studies to use the Johnson\u2019s SU (Johnson, 1949) distribution in probabilistic forecasting (Mori, 2021).\n5. It is a fully automatic forecasting method that may be used with other markets and data. The code is open-source.\n6. The obtained forecasts are interpretable in terms of distribution\u2019s characteristics, and the results provide evidence of the importance of higher moments in EPF.\n7. The aggregation schemes for probabilistic forecasts proposed in the paper yield robust predictions that significantly outperform the state-of-the-art benchmarks in terms of the continuous ranked probability score (CRPS).\nFinally, given that probabilistic forecasting is the essence of risk management, our study provides power market participants with a new, significantly more accurate tool for assessing risks related to trading power portfolios.\nThe remainder of this manuscript is structured as follows. Section 2 introduces the reader to the concept of distributional artificial neural networks. Section 3 provides an overview of the market and data used in the application study. The models, including the DDNN and the hyperparameter tuning, are described in Section 4. The application study together with the results are presented in Section 5. The paper is concluded with a discussion of the main findings in Section 6."
        },
        {
            "heading": "1.1. Probabilistic forecasting literature",
            "text": "The probabilistic EPF literature is not as rich as the one considering point forecasts, what can be deducted from the reviews of Nowotarski and Weron (2018), Ziel and Steinert (2018) and Hong et al. (2020). Researchers consider mainly the day-ahead market, which is the main electricity spot trading place. However in recent years the research focused on other markets as well, e.g. the intraday (Janke and Steinke, 2019; Narajewski and Ziel, 2020a,b; Oksuz and Ugurlu, 2019; Uniejewski et al., 2019) and balancing markets (Browell and Gilbert, 2022; Janczura and W\u00f3jcik, 2022; Kraft et al., 2020). Two widely used and efficient model estimation methods in point EPF are lasso (Narajewski and Ziel, 2020a; Uniejewski et al., 2019; Ziel, 2016; Ziel and Weron, 2018) of Tibshirani (1996) and artificial neural networks (Dudek, 2016; Lago et al., 2021; Luo and Weng, 2019; Oksuz and Ugurlu, 2019; Zahid et al.,\n2019; Zhou et al., 2019). A substantial stream of new EPF research considers hybrid models (Jahangir et al., 2019; Olivares et al., 2022; Oreshkin et al., 2021; Zhang et al., 2020), however, as Lago et al. (2021) conclude, they often avoid proper comparisons to well-established methods. The probabilistic EPF comprises mostly of quantile regression (Maciejowska, 2020; Maciejowska et al., 2016; Marcjasz et al., 2020a), bootstrapping (Efron, 1979) of point forecast residuals (Narajewski and Ziel, 2022; Wan et al., 2013; Ziel and Steinert, 2018), and of RNNs (Brusaferri et al., 2020; Mashlakov et al., 2021). We follow the recommendations of Lago et al. (2021) and compare our model against the very competitive lasso estimated autoregressive (LEAR) and a point deep neural network. These are point forecasting models, thus we apply quantile regression averaging on them. Other methods of obtaining probabilistic forecasts, such as bootstrap, could be considered as well, but we refrain from that for the sake of simplicity. A vast amount of literature concerns also forecast combinations in the electricity markets (Hubicka et al., 2018; Karabiber and Xydis, 2019; Serafin et al., 2019), however due to the complexity of the probabilistic forecast aggregation as pointed out by Berrisch and Ziel (2022), we limit ourselves only to two simple averaging schemes with equal weights that allow to stabilize the neural network predictions."
        },
        {
            "heading": "2. The distributional deep neural network (DDNN) model",
            "text": "We assume that the reader is familiar with and understands the concept of the (feed-forward) deep neural networks (DNN). In this section, we briefly recall the definition and the mathematics behind it to underline the difference between the DNN with point and probabilistic output layers."
        },
        {
            "heading": "2.1. Architecture",
            "text": "Let X \u2208 RD\u00d7N be the input matrix with N denoting the number of features and D the number of observations. Further, let Hi \u2208 RD\u00d7hi be the output matrix of ith hidden layer, Wi \u2208 Rhi\u22121\u00d7hi and bi \u2208 RD\u00d7h1 be the corresponding hidden-layer weights and bias where hi \u2208 N is the number of neurons in ith hidden layer with h0 = N and thus H0 = X. Additionally, denote ai(\u00b7) the ith activation function. Then, for i \u2208 {1, . . . , I} we have\nHi = ai (Hi\u22121Wi + bi) . (1)\nNow, we got to the point where the DNN with point and probabilistic output layers differ. That is to say, in the standard DNN we calculate the output O \u2208 RD\u00d7S , where S is the number of modelled features. Formally,\nO = HIWI+1 + bI+1 (2)\nare the values returned by the network. Such DNN is optimized given the true observation matrix Y \u2208 RD\u00d7S\nwith respect to point losses, e.g. the mean squared error (MSE) or mean absolute error (MAE). In the case of the DDNN, the parameter layer \u0398 \u2208 RD\u00d7S\u00b7P consists of P distribution parameters for each of the S modelled features. It is however computed in the same manner as in equation (2). The final output is made by creating aD\u00d7Sdimensional matrix of the assumed distributions F (\u0398;x). The network is then optimized given the true observation matrix Y \u2208 RD\u00d7S with respect to probabilistic losses, e.g. by maximizing the likelihood for a parametric distribution or by minimizing the continuous ranked probability score (CRPS).\nFigure 1 provides an example with I = 2 hidden layers and this setting we use in the remainder of the manuscript. The number of neurons in the hidden layers is arbitrary, but the same for both DNNs in order to underline the difference between the point and probabilistic networks. We see clearly that the input and hidden layers are identical for both DNNs and only the output part differs.\nAs a final remark of the subsection, we discuss the multivariate output which consists of multiple features and the possible probabilistic distributions. Namely, we allow in the definition for S output features, and in our setting they are all S = 24 hours of the electricity prices of the following day. This can be done as all the day-ahead electricity prices are published at once, and therefore they can share the input regressor set. In other applications this is rather not the case, however such a multivariate setting may still be preserved if one considers S similar time series to be forecasted that may benefit from common regressors.\nThe probabilistic output layer may consist of nearly any implemented probabilistic distribution. Based on application, these can be, e.g., binomial or Poisson if we deal with a discrete problem, gamma or beta if we deal with a continuous problem, but supported only on the positive line, or normal, t or Johnson\u2019s SU if supported on the whole real line. As the electricity prices may be both positive and negative, we use in our study the two-parametric normal and four-parametric Johnson\u2019s SU distributions."
        },
        {
            "heading": "2.2. Regularization",
            "text": "The danger of overfitting the model can be tackled in the DDNN similarly as in the standard one. One could use regularization, a dropout layer or early stopping. We use all of these in our forecasting study, however we approach the regularization of the parameter layer differently.\nThe DNN design allows for Lp regularization of every hidden layer Hi, its weights Wi, and bias bi. Applying it to the DNN we get the following loss with regularization\nLreg(Y ,O) = L(Y ,O) + I\u2211\ni=0\n\u03bb1,i||Hi||p+\n+ I\u2211 i=0 \u03bb2,i||Wi+1||p + I\u2211 i=0 \u03bb3,i||bi+1||p (3)\nwhere || \u00b7 ||p represents the Lp norm. One can flexibly choose between the types of regularization, use both or none, and choose to regularize only some part of the network, e.g., only H1 layer and W2 weights. The \u03bbj,i parameters are subject to hyperparameter tuning. The regularization of the DDNN may be done in the same way as described in Eq. (3), however, we could also regularize each of the distributional parameters separately as follows\nLreg (Y ,F (\u0398;x)) = L(Y ,F (\u0398;x)) + I\u22121\u2211 i=0 \u03bb1,i||Hi||k+\n+ I\u22121\u2211 i=0 \u03bb2,i||Wi+1||k + I\u22121\u2211 i=0 \u03bb3,i||bi+1||k+\n+ P\u2211 p=1 (\u03bb1,I,p||HI ||k + \u03bb2,I,p||WI+1||k + \u03bb3,I,p||bI+1||k) .\n(4) The difference between Eq. (4) and Eq. (3) is the regularization of the last layer. Namely, in Eq. (3) we regularize the whole output layer using the same \u03bbj,I values, whereas in Eq. (4) each parameter p \u2208 {1, . . . , P} is regularized using its own \u03bbj,I,p values. The colour arrows in Figure 1(b) denote separate kernel WI+1 regularization for each of the distributions\u2019 parameters. The reason to use such a differentiation is the possibility to use different amount of inputs\u2019 information for each distribution parameter, what was already observed in the literature (Narajewski and Ziel, 2020b)."
        },
        {
            "heading": "3. The data",
            "text": "The goal in the empirical case study is forecasting dayahead electricity prices in Germany. This section familiarizes the reader with the utilized data, especially the input features and the forecasting objective. The electricity markets in Europe consist of derivative, spot and balancing parts (Viehmann, 2017). The most important is the spot market, particularly the day-ahead auction. It takes place once a day at noon where all S products of the following day are traded in a uniform price auction (Weron and Ziel, 2019). In the majority of countries S = 24, however in some cases like the UK it is S = 48. As all hours of the following day are traded at once, all of them are based on the same set of information. Therefore, in our study we model all the prices using exactly the same input features, what supports the multivariate output of the DDNN presented in Section 2.\nThe considered dataset spans six years of hourly observations from 01.01.2015 to 31.12.2020. The study uses a rolling window what mimics the daily business in practice and is a standard procedure in the EPF literature (Weron, 2014; Weron and Ziel, 2019). The initial in-sample period spans the date range from 01.01.2015 to 26.12.2018 which consists of D = 4 \u00b7364 = 1456 observations. For the purpose of hyperparameter tuning, we split it additionally\nHidden layer\nHidden layer\nOutput layer Input layer\nto training and validation sets. The out-of-sample period starts on 27.12.2018, and ends on 31.12.2020, however the first 182 observations are used to obtain the QRA forecasts and thus are excluded from the analysis. Therefore, the final out-of-sample test set for probabilistic predictions uses 554 days of data. The models are retrained every day using the most recentD observations and the hyperparameters obtained in the tuning that is run on initial in-sample dataset.\nFigure 2 shows plots of the considered features together with the dates and study stages mentioned above. The data contains the day-ahead (DA) electricity prices, DA load forecasts, DA renewable energy sources (RES) forecasts, EU emission allowance prices and fuel: coal, oil and natural gas prices. The RES forecast is a sum of wind offshore, wind onshore and solar generation day-ahead forecasts. The DA prices and load forecasts exhibit strong daily, weekly and annual seasonality. Thus, we model each\nhour of the day separately within a single neural network and also utilize the weekday dummies. We do not construct any regressor explaining the annual behavior as it is well described by the load data. On the other hand, the RES forecasts exhibit only daily and annual seasonality and the EUA and fuel prices are random-walk type processes. These conclusions might not be easy to derive based on Figure 2, however see Ziel and Weron (2018), Sgarlato and Ziel (2022) and Bill\u00e9 et al. (2022) for more insights.\nFigure 3 presents histograms of prices for selected hours. Additionally, we fit there normal and Johnson\u2019s SU dis-\ntributions and plot their densities. Both distributions belong to the location-scale family. The normal distribution N (\u00b5, \u03c32) is a well-known two-parametric distribution with \u00b5 being the location and \u03c3 the scale parameters. The Johnson\u2019s SU distribution J (\u00b5, \u03c3, \u03bd, \u03c4) was first investigated by Johnson (1949) as a transformation of the normal distribu-\ntion. It is a four-parametric distribution with \u00b5 being the location, \u03c3 the scale, \u03bd the skewness and \u03c4 the tail-weight parameters. So far, it has not found application with distributional neural networks. However, it is often used in the context of energy commodities (Abramova and Bunn, 2020; Gianfreda and Bunn, 2018; Patra, 2021). Based on Figure 3 we suspect that the Johnson\u2019s SU is more suitable for modeling the electricity prices than the normal. We observe heavy tails and skewness what cannot be explained by the normal distribution. Thus, in the forecasting study we use both distributions to emphasize the gain that comes from using the more flexible distribution."
        },
        {
            "heading": "4. Models and estimation",
            "text": ""
        },
        {
            "heading": "4.1. Input features",
            "text": "Let us recall that we forecast the S = 24 day-ahead prices on day T , i.e. YT = (YT,1, YT,2, . . . , YT,S) \u2032. The following input features are available for all considered models:\n\u2022 Past day-ahead prices of the previous three days and one week ago, i.e. YT\u22121, YT\u22122, YT\u22123, and YT\u22127.\n\u2022 Day-ahead forecasts of the total load for day T , i.e. XLT = (X L T,1, X L T,2, . . . , X L T,S)\n\u2032, as well as the past values of the previous day and one week ago, i.e. XLT\u22121, and X L T\u22127.\n\u2022 Day-ahead forecasts of renewable energy sources (RES) for day T , i.e. XREST = (X RES T,1 , X RES T,2 , . . . , X RES T,S )\n\u2032, as well as the past values of the previous day, i.e. XREST\u22121 .\n\u2022 EU emission allowance most recent closing price, i.e. XEUAT\u22122 .\n\u2022 Fuels most recent closing prices, i.e. XCoalT\u22122 , X Gas T\u22122,\nand XOilT\u22122.\n\u2022 Weekday dummies, i.e. DoWd(T ) for d = 1, 2, . . . , 7.\nLet us note that the forecasting exercise is performed on day T \u2212 1 before the day-ahead auction. That is to say, we possess only the information available at around 11:30 CET on day T \u2212 1. The considered input does not violate this assumption, and therefore we use e.g. T \u2212 2 lag for the EUA and fuels prices."
        },
        {
            "heading": "4.2. Probabilistic neural network",
            "text": "The probabilistic NN model uses the DDNN described in Section 2. The model consists of 2 hidden layers, S output distributions, and various number of input features. The output distributions are assumed to be either normal or Johnson\u2019s SU and each of them defines a separate model. We regularize the model through input feature selection, dropout layer and L1 regularization of the hidden layers and weights. All these are subject to hyperparameter tuning. Additionally, we tune the activation functions,\nthe number of neurons, and the learning rate. The detailed list of hyperparameters and the process is described in Section 4.2.1.\nThe model is built and estimated using the TensorFlow (Abadi et al., 2015) and Keras (Chollet et al., 2015) frameworks. The hyperparameter optimization is performed with the help of Optuna (Akiba et al., 2019) package, 4 times for result stability reasons, each time consisting of 2048 iterations. We report the results for each of the 4 optimized hyperparameter sets, as well as for 3 different ensembles of the four distributions, as described in Section 5. The model consists naturally also of components that are not tuned in the hyperparameter optimization. That is to say, the model uses additionally an input normalization, negative loglikehood as the loss function, Adam optimizing algorithm, and early stopping callback with patience of 50 epochs. The batch size is fixed to 32, and the maximum number of epochs to 1500. For the rolling prediction, the dataframe was shuffled and 20% was left out for validation.\nProbabilistic neural networks are denoted in the later parts of the paper using DDNN-{distribution}-{run} scheme, where {distribution} is either Normal (or N) or JSU and {run} is either a number from 1 to 4 (corresponding to the individual hyperparameter sets), or an indicator of the ensemble of the four: pEns for the vertical average or qEns for the horizontal averaging. See Section 4.2.1 for the description of different schemes. Note, that when the choice of the distribution is obvious, as in Fig. 5, the {distribution} term may be missing in the model acronym."
        },
        {
            "heading": "4.2.1. Hyperparameter tuning for neural network models",
            "text": "The neural network models (both point and distributional) underwent the hyperparameter optimization considering below hyperparameters and their potential values:\n\u2022 Indicator for inclusion of input features described in Section 4.1 (14 hyperparameters).\n\u2022 Dropout layer \u2013 whether to use the dropout layer after the input layer, and if yes at what rate. The rate parameter is drawn from (0, 1) interval (up to 2 hyperparameters \u2013 the rate is not optimized if dropout layer is not present in the model).\n\u2022 Number of neurons in the hidden layers. The values are chosen from integers from [16, 1024] interval (1 hyperparameter per layer).\n\u2022 Activation functions used in each of the hidden layers. The possible functions are: elu, relu, sigmoid, softmax, softplus, and tanh (1 hyperparameter per layer).\n\u2022 L1 regularization for hidden layers \u2013 whether to use the L1 regularization on the hidden layers and their weights and if yes at what rate. The rate is drawn from (10\u22125, 10) interval on a log-scale (up to 2 hyperparameters per layer \u2013 inclusion of L1 for the layer and the rate).\n\u2022 L1 regularization for distribution layer \u2013 separate for each of the P distribution parameters, where P = 2 for normal and P = 4 for Johnson\u2019s SU distributions \u2013 whether to use the L1 regularization and if yes at what rate (a total of 2P hyperparameters; rates identical to the hidden layer regularization).\n\u2022 Learning rate for the Adam algorithm chosen from the (10\u22125, 10\u22121) interval on a log-scale (1 hyperparameter).\nThe process consists of 2048 iterations of the optimization algorithm which are performed in a hybrid batchrolling approach. Having the first four years (1456 days) at disposal, we split them into training data (the first 1092 days) and validation data (the last 364 days). Note, that the first day of the out-of-sample test window is the day after the end of the hyperparameter validation data, as illustrated on Figure 2 (i.e., there is no data contamination). The hybrid approach is needed to balance two opposing factors. On one hand, a batch estimation (using a single estimation on NN weights) would be less computationally demanding (we would only have 1 neural network trained for every considered hyperparameter set), however the results of such an experiment are very volatile. The best hyperparameter set chosen using the accuracy metric of only a single run would not \u2013 in general \u2013 guarantee a good predictive performance. On the other hand, a rolling setting identical to the one used later (i.e., with a daily recalibration) would be infeasible to compute (as it would take roughly 364 times longer than for batch approach \u2013 we would have 364 neural networks trained for every hyperparameter set). The hybrid approach we have chosen uses 13 recalibrations of neural network models with batches of 28 days estimated using each of the nets. Training data is rolled by 28 days after each step.\nAs mentioned earlier, to counteract the local behavior of the hyperparameter optimizer, we repeat the process four times for each of the neural networks. We observe that the predictive performance across the separate hyperparameter sets is not consistent, however the simple aggregation schemes described below provide results consistently better than any of the inputs.\nThe first of the aggregation schemes is a mixture distribution, which corresponds to averaging the distributions vertically. However, having two distributions with disjoint pdfs (e.g., two copies of the same distributions significantly shifted), the resulting mixture will be very wide, and might have a \u201cgap\u201d in the middle. A more robust alternative is considered, which utilizes horizontal (quantile) averaging \u2013 i.e., a quantile of an ensemble is computed as an arithmetic mean of the same quantiles from all distributions considered. Such an aggregation in an edge case described earlier would result in an unimodal ensemble distribution, which is much sharper than the vertically averaged one."
        },
        {
            "heading": "4.3. Benchmarks",
            "text": ""
        },
        {
            "heading": "4.3.1. The naive model",
            "text": "The first and the simplest benchmark model that we consider is the well-known and widely utilized (Weron, 2014; Ziel and Weron, 2018) naive model. It requires no parameter tuning. Its formula is as follows\nE (YT ) = { YT\u22127, DoWd(T ) = 1 for d = 1, 6, 7, YT\u22121, otherwise. (5)\nIn other words, the naive model uses the prices of yesterday to forecast the prices on Tuesday, Wednesday, Thursday and Friday, and the last week\u2019s prices on Monday, Saturday and Sunday. The price distributions are obtained using the bootstrap method which was first proposed by Efron (1979). We receive the distributions by adding the in-sample bootstrapped errors to the forecasted expected price\nY\u0302 mT = E\u0302 (YT ) + \u03b5\u0302mT for m = 1, . . . ,M (6)\nwhere \u03b5\u0302mT are drawn with replacement in-sample residuals for day T , i.e., we sample from the set of \u03b5\u0302d = Yd\u2212 Y\u0302d for d = 1, . . . , D."
        },
        {
            "heading": "4.3.2. The LEAR model combined with QRA",
            "text": "The first of the models that use the structure presented in Section 4.1 is LEAR point forecasting model that uses Quantile Regression Averaging (QRA) to generate probabilistic forecasts. The LEAR model utilizes the LASSO regularization (Tibshirani, 1996). Such an approach eliminates the need for an additional input selection, as the algorithm itself indirectly chooses the most relevant inputs. The regularization parameter (the sole hyperparameter of the LEAR model) is obtained using 7-fold cross validation and a grid of 100 values automatically chosen by a least angle regression (LARS) based estimator. The LEAR approach encompasses a forecast averaging scheme proposed by Lago et al. (2021) \u2013 four independent forecasts are generated for each hour (based on 56, 84, 1092 and 1456 day rolling calibration windows) and the final output is their simple average. Such an approach allows for a balance of the ability to adapt to rapidly-changing market conditions (thanks to the shorter calibration windows) with robustness coming from the use of long windows. It was shown to provide forecasts that \u2013 on average \u2013 are on par or better than all of the comprising forecasts considered separately (Lago et al., 2021).\nThere are two ways of using a set of four separate forecasts or an ensemble: one that uses the whole information directly (i.e., the separate forecasts), which we will denote QRA (Quantile Regression Averaging) and QRM (Quantile Regression committee Machine) that uses the ensemble of the point predictions (Marcjasz et al., 2020b).\nAside from the input data, the QRA and QRM approaches are identical \u2013 both use quantile regression with\n182 day rolling calibration window to produce the forecast for each of the 99 percentiles, which approximate the predictive distribution relatively well.\nThe LEAR models\u2019 results are denoted by LEAR{CAL} for the point forecast estimated using {CAL} calibration days (e.g., LEAR-1456 for the longest calibration window), LEAR-Ens for an hour-by-hour average of all 4 point forecasts and LEAR-QRA and -QRM for the probabilistic forecasts."
        },
        {
            "heading": "4.3.3. The DNN model combined with QRA",
            "text": "The second set of benchmarks utilizes a point neural network model. It differs from the probabilistic counterpart only in the output construction in the network and hyperparameters corresponding to the missing distribution layer (see Sections 2, 4.2 and Figure 1). The rest of the model setting remains unchanged: DNN model has the same inputs, the same hyperparameter selection and uses the same calibration window lengths and training and validation splits. The loss function for the network is MAE, whereas DDNN uses log-likelihood.\nSimilarly to the DDNN, for the (point) DNN we also derive four independently-trained hyperparameter sets. This allows us to i) measure the robustness of the predictions and ii) apply two quantile-regression based methods (QRA and QRM), similarly as for the LEAR point predictions, also using a 182 day rolling calibration window.\nThe results are marked with DNN-n for the point forecasts (where n = 1, . . . , 4 or Ens) and DNN-QRA and DNN-QRM, respectively for percentile forecasts obtained using quantile regression on the four separate point forecasts and their ensemble."
        },
        {
            "heading": "5. Empirical results",
            "text": "Many earlier works show that forecast averaging is often key to achieving accurate predictions. Here, we also aggregate multiple forecast runs to improve the result accuracy and robustness. However, considering probabilistic forecasts instead of the point ones significantly increases the complexity of the aggregation schemes that can be applied. As the detailed discussion is out of scope of this paper, we opted to include only the simple aggregations, based on the equally-weighted averaging or distribution mixing.\nOn the probabilistic forecasts side, we have four hyperparameter sets chosen in four separate hyperparameter optimization runs for both the normal and JSU DDNNs. We report the errors of each of them separately, as well as the result of two aggregation schemes: an equally-weighted mixture of the four resulting distributions (vertical aggregation) or a mean of values for a given quantile (horizontal aggregation)."
        },
        {
            "heading": "5.1. Evaluation",
            "text": "While the paper focuses on probabilistic forecasting, we are also interested in the accuracy of the point fore-\ncasts. The latter can be easily derived from the probabilistic ones. Following the best practices of Weron and Ziel (2019), we report two point-oriented metrics: the mean absolute error(MAE; we use median statistic from the probabilistic methodologies for this metric) and root mean squared error (RMSE; we use mean statistic).\nWhen it comes to the probabilistic forecasts, we use the CRPS, or rather its approximation \u2013 an average pinball score across 99 percentiles.(Gneiting, 2011; Hong et al., 2016):\nPinball(Q\u0302Yt(q), Yt, q) =\n= (1\u2212 q) ( Q\u0302Yt(q)\u2212 Yt ) forYt < Q\u0302Yt(q), q ( Yt \u2212 Q\u0302Yt(q) ) forYt \u2265 Q\u0302Yt(q), (7)\nwhere Q\u0302Yt(q) is the forecast of the q-th quantile of the price Yt.\nAdditionally for each hour of the day, we perform the Kupiec test (Kupiec, 1995) for unconditional coverage for 50% and 90% prediction intervals (PIs). For the CRPS, we aggregate the score across all forecasted hours, whereas for the Kupiec test, we provide the number of hours which passed the Kupiec test.\nLastly, we use the Diebold-Mariano (DM) test that measures the statistical significance of the difference between the accuracy of the forecasts of two models (here, we use A and B to discern between them) (Muniain and Ziel, 2020; Sgarlato and Ziel, 2022; Uniejewski et al., 2019). Let us denote the vector of errors (here, the CRPS scores) of model Z for day d as LdZ . Then, the multivariate loss differential series\n\u2206dA,B = ||LdA||1 \u2212 ||LdB ||1 (8)\ndefines the difference of the L1 norm of loss vectors. For each pair of models, we compute the p-value of two onesided DM tests \u2013 one with the null hypothesisH0 : E(\u2206dA,B) \u2264 0, which corresponds to the outperformance of model B forecasts by those of model A and the second with the reverse null hypothesis H0 : E(\u2206dA,B) \u2265 0, complementary to the first one. We use the CRPS as the loss function."
        },
        {
            "heading": "5.2. Results",
            "text": "In terms of the CRPS scores, as can be seen in Figure 5 and Table 1, the LEAR-based methods are significantly worse than the neural network-based approaches. However, the performance of the latter is not robust \u2013 runto-run, the CRPS scores differ by as much as 10%. As discussed in Section 5.3, this is not known ex-ante, therefore an aggregation scheme is needed. After ensembling, regardless of the aggregation scheme applied (vertical, horizontal with mean, horizontal with median), we see similar performance. The normally-distributed networks yield a CRPS of ca. 1.35, whereas JSU ones \u2013 1.30, i.e., ca. 3-4% better than the normal. DNN-QR methods can be\nplaced between the probabilistic DDNN ensembles and the individual runs.\nAs shown in Table 1, the neural network-based models are better than LEAR-based ones also for the point forecasts. Interestingly, the ensemble of DNN forecasts has the third lowest error \u2013 both in terms of MAE and RMSE. The best model according to MAE is DDNN-JSU-pEns, followed by its -qEns counterpart \u2013 the two models with the best CRPS score. However, these models are worse w.r.t. the RMSE than all other NN-based models. On the other hand, we see the lowest RMSE for DNN-QR based methods, closesly followed by the DNN-Ens model. The DDNNs are ca 2% (normal) and 7% (JSU) worse. There are only minor differences between the vertical and horizontal aggregation schemes.\nAdditionally, we performed the Kupiec test for unconditional coverage with 5% significance level for 50% and 90% prediction intervals. From what can be seen in Table 1, QRA seems to perform better than QRM \u2013 for both the LEAR and point DNN models. However, the QR-based approahces pass the Kupiec test for at most 10 hours of the day. The probabilistic DDNNs, on the other hand show mixed performance. The p-Ens forecasts are worse than most other methods, while q-Ens are better than QRbased predictions. As the p-Ens models sport the CRPS scores similar to the q-Ens ones, the latter are a much better choice, especially when chosen with a more robust median quantile instead of mean. Note, however, that the worst overall method (Naive benchmark) provides the best coverage for both 50 and 90% PIs.\nLastly, the results of the DM test are visualized in Figure 6. We can observe that DDNN-JSU-qEns is the best model overall, with forecasts significantly better than from any other model (represented by a column with all cells green or yellow). Secondly, the DDNN-N-qEns is also significantly better than both other aggregation schemes. Lastly, QRA models (both LEAR and DNN ones) produce significantly better forecasts than their QRM counterparts."
        },
        {
            "heading": "5.3. The need for multiple hyperparameter sets",
            "text": "Even though the hyperparameter optimization uses a repeated neural network training procedure to mimic the rolling calibration window setting used later for the evaluation, the optimal sets obtained using independent hyperparameter trials differ significantly. Moreover, all the optimal sets have a similar, i.e. within 2% difference, in-sample error metric \u2013 what is not reflected in the out-of-sample error obtained using this set. Here, the differences are much more prominent, up to 10%. The locality of the hyperparameter optimization is clearly visible in the optimal sets chosen in independent trials, despite most of the trials being stalled after around 1000 iterations. Figure 7 shows choice frequency of the considered input features (i.e., the number of hyperparameter sets that uses a particular input variable), described in Section 4.1. All 3 considered\nneural network models are quite consistent when selecting the inputs. The most important ones are the prices of the previous day and two days ago, the current DA forecasts of load and RES, the previous day\u2019s DA forecasts of RES and the recent gas price. The least important are the further lags of prices and load forecast.\nBesides the differences in the inputs chosen, hidden layer sizes are the most prominent, especially for the probabilistic networks. They found optima in both the larger and smaller networks, as shown in Table 2. For example, one of the probabilistic neural networks that used the JSU distribution uses 565 and 962 neurons in the hidden layers (amounting to over 540,000 weights just between the two hidden layers), whereas other had 940 and 58 (over 54,000 weights) or 123 and 668 (over 82,000 weights). Moreover, even the activation functions chosen were not unanimous, but softplus seems to be the best for the first hidden layer. We also observe that the dropout is almost never chosen, similarly to the regularization of the network weights.\nTo conclude, we observe that there is a need for repeating the hyperparameter optimization process. Despite the robust optimization setting, the end results are vastly different \u2013 both in terms of the parameters chosen, and the out-of-sample error metrics. A form of the forecast combination is crucial for the outperformance of QR-based methods."
        },
        {
            "heading": "6. Conclusions",
            "text": "The paper proposes an application of distributional neural networks to probabilistic day-ahead electricity price forecasting and a simple, yet well-performing aggregation scheme for the distributional neural networks that stabilizes the predictions. Since probabilistic forecasting is the\nessence of risk management \u2013 Value-at-Risk (VaR) is nothing else but a quantile forecast \u2013 our study provides important implications for managing portfolios in the power sector.\nComparing the results with the literature approaches, we observe a strong performance of the neural networks \u2013 both the probabilistic forecasts from the proposed methods and from quantile regression applied to their point counterparts are significantly more accurate than the statisticalbased combination of LEAR and quantile regression. The added complexity of the neural network having to model the distribution of the data, rather than just their expected values, proves effective, especially when the limitations incurred by the distribution choice are not too severe.\nInterestingly, the benefit of using distributional neural networks is visible also when mean absolute errors of the median (50th percentile) forecasts are considered. The DDNN-JSU-Ens approach is the only one that outperforms the ensemble of point NNs in this regard."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was partially supported by the Ministry of Science and Higher Education (MNiSW, Poland) through Diamond Grant No. 0219/DIA/2019/48 (to G.M.) and the National Science Center (NCN, Poland) through grant No. 2018/30/A/HS4/00444 (to R.W. and F.Z.)."
        }
    ],
    "title": "Distributional neural networks for electricity price forecasting",
    "year": 2022
}