{
    "abstractText": "A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent\u2019s information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm\u2019s prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Well-intentioned policies aiming to provide more information, such as keeping a \u201chuman-in-the-loop\u201d or requiring maximal prediction accuracy, could strictly worsen decision quality compared to systems with no human or no algorithmic assistance. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.",
    "authors": [],
    "id": "SP:c1db4a9868326b41ea8c9061b2bc7fa313c07104",
    "references": [
        {
            "authors": [
                "USA",
                "April"
            ],
            "title": "Association for Computing Machinery",
            "venue": "ISBN 978-1-4503-9157-3. doi: 10.1145/",
            "year": 2022
        },
        {
            "authors": [
                "Kate Donahue",
                "Alexandra Chouldechova",
                "Krishnaram Kenthapadi"
            ],
            "title": "Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness, June 2022",
            "venue": "URL http://arxiv.org/abs/2202.08821",
            "year": 2022
        },
        {
            "authors": [
                "Ozkan Eren",
                "Naci Mocan"
            ],
            "title": "Emotional Judges and Unlucky Juveniles",
            "venue": "American Economic Journal: Applied Economics,",
            "year": 2018
        },
        {
            "authors": [
                "Bengt Holmstr\u00f6m"
            ],
            "title": "On the theory of delegation",
            "venue": "Technical report, Discussion Paper,",
            "year": 1980
        },
        {
            "authors": [
                "Bengt Robert Holmstr\u00f6m"
            ],
            "title": "On Incentives and Control in Organizations",
            "venue": "Stanford University,",
            "year": 1978
        },
        {
            "authors": [
                "Kosuke Imai",
                "Zhichao Jiang",
                "James Greiner",
                "Ryan Halen",
                "Sooahn Shin"
            ],
            "title": "Experimental Evaluation of Algorithm-Assisted Human Decision-Making: Application to Pretrial Public Safety Assessment, December 2021",
            "venue": "URL http://arxiv.org/abs/2012.02845. arXiv:2012.02845 [cs, stat] version:",
            "year": 2012
        },
        {
            "authors": [
                "Maia Jacobs",
                "Melanie F. Pradier",
                "Thomas H. McCoy",
                "Roy H. Perlis",
                "Finale Doshi-Velez",
                "Krzysztof Z. Gajos"
            ],
            "title": "How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection",
            "venue": "Translational Psychiatry,",
            "year": 2021
        },
        {
            "authors": [
                "Emir Kamenica",
                "Matthew Gentzkow"
            ],
            "title": "Bayesian Persuasion",
            "venue": "American Economic Review,",
            "year": 2011
        },
        {
            "authors": [
                "Jon Kleinberg",
                "Himabindu Lakkaraju",
                "Jure Leskovec",
                "Jens Ludwig",
                "Sendhil Mullainathan"
            ],
            "title": "Human Decisions and Machine Predictions",
            "venue": "The Quarterly Journal of Economics, August 2017",
            "year": 2017
        },
        {
            "authors": [
                "Vivian Lai",
                "Chacha Chen",
                "Q Vera Liao",
                "Alison Smith-Renner",
                "Chenhao Tan"
            ],
            "title": "Towards a science of human-ai decision making: a survey of empirical studies",
            "venue": "arXiv preprint arXiv:2112.11471,",
            "year": 2021
        },
        {
            "authors": [
                "Yichuan Lou"
            ],
            "title": "Optimal Delegation with Information Manipulation, October 2022",
            "venue": "URL https://papers.ssrn.com/abstract=4298900",
            "year": 2022
        },
        {
            "authors": [
                "Bryce McLaughlin",
                "Jann Spiess"
            ],
            "title": "Algorithmic Assistance with Recommendation-Dependent Preferences",
            "venue": "In Proceedings of the 24th ACM Conference on Economics and Computation, EC \u201923,",
            "year": 2023
        },
        {
            "authors": [
                "Sendhil Mullainathan",
                "Ziad Obermeyer"
            ],
            "title": "Diagnosing Physician Error: A Machine Learning Approach to Low-Value Health Care",
            "venue": "The Quarterly Journal of Economics,",
            "year": 2022
        },
        {
            "authors": [
                "Ashesh Rambachan"
            ],
            "title": "Identifying prediction mistakes in observational data",
            "venue": "Harvard University,",
            "year": 2021
        },
        {
            "authors": [
                "Charvi Rastogi",
                "Liu Leqi",
                "Kenneth Holstein",
                "Hoda Heidari"
            ],
            "title": "A Unifying Framework for Combining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making, August 2022",
            "venue": "URL http://arxiv.org/abs/2204.10806",
            "year": 2022
        },
        {
            "authors": [
                "Megan T. Stevenson",
                "Jennifer L. Doleac"
            ],
            "title": "Algorithmic Risk Assessment in the Hands of Humans, September 2022",
            "venue": "URL https://papers.ssrn.com/abstract=3489440",
            "year": 2022
        },
        {
            "authors": [
                "Eleni Straitouri",
                "Lequn Wang",
                "Nastaran Okati",
                "Manuel Gomez Rodriguez"
            ],
            "title": "Improving expert predictions with prediction sets",
            "venue": "arXiv preprint arXiv:2201.12006,",
            "year": 2022
        },
        {
            "authors": [
                "Maren Vairo"
            ],
            "title": "The value of information in delegation",
            "year": 2023
        },
        {
            "authors": [
                "Ruqing Xu",
                "Sarah Dean"
            ],
            "title": "Decision-aid or Controller? Steering Human Decision Makers with Algorithms, March 2023",
            "venue": "URL http://arxiv.org/abs/2303.13712",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n40 2.\n09 38\n4v 2\n[ ec\non .T\nH ]\n2 1"
        },
        {
            "heading": "1 Introduction",
            "text": "Decisions today are increasingly complex and data intensive. Toward the goal of making better decisions, decision-aid algorithms have been developed and employed in many consequential domains, including criminal justice, healthcare, and credit lending systems. These algorithms take in data from decision subjects (e.g., defendants, patients, loan applicants) and generate predictions. These predictions can either be used to make decisions automatically, or be disclosed to human agents who ultimately make a decision. While the agents may have private knowledge that helps to make a better decision, they may also exhibit behavioral biases, cognitive limitations, or private motives that might misalign with the algorithm.\nA real-life example is the Child Protection Service (CPS) in Allegheny County, PA, which utilizes an algorithm to assign risk scores to reports of child maltreatment [Cheng et al., 2022]. If the risk score exceeds certain thresholds, cases are automatically screened-in. Otherwise, a social worker sees the score and other available information, then makes the final decision. Suppose that a social worker under-investigates since investigation is costly to him, but he possesses intuition and contextual information that are inaccessible to the algorithm. How should we design the predictive algorithm to collaborate with such a privately informed but biased agent? When should we pass the prediction and the decision to the agent, for the purpose of better information?\nIndeed, preference misalignment is widespread in other applications. Bank managers may be more risk-averse than a profit-maximizing algorithm when evaluating high-risk opportunities. Physicians may overweight salient but less predictive signals in diagnosing and testing [Mullainathan and Obermeyer, 2022]. Judges can exhibit behavioral inconsistencies or even biases towards certain demographic groups [Eren and Mocan, 2018, Kleinberg et al., 2017]. If the quality of the final decision is of concern, we ought to take into account the agents\u2019 private information and potential misalignment in designing the predictive algorithm, as well as in deciding when the decision authority should be given to an agent.\nWe present a principal-agent model to study the algorithm design and delegation strategy when the agent is privately informed but potentially misaligned. We model the algorithm as a \u201csignal\u201d whose realizations (predictions) convey information about the underlying state. The design of the algorithm corresponds to a persuasion problem where the principal chooses the optimal signal structure. We model the problem of whether to pass the decision to the human as a delegation problem. Our results suggest that delegation is strictly valuable if and only if the principal would make the same decision as the agent had she observed the agent\u2019s information. We find that providing the most informative algorithm may not be optimal even if the principal can choose to act on the algorithm\u2019s prediction. Moreover, the optimal algorithm may provide more information about one state while restricting information about the other. Lastly, we show that naive policies, such as keeping a \u201chuman-in-the-loop\u201d or requiring maximal prediction accuracy, strictly worsen decision quality for some decision subjects, even compared to systems with no human or no algorithmic assistance. We suggest that the underperformance of human-machine collaborations widely observed in empirical settings [Lai et al., 2021] can be understood through this theoretical lens.\nFormally, we consider a model with binary states and binary actions. The principal and the agent each receive a signal that contains information about the underlying state. The principal\u2019s signal is publicly observable while the agent\u2019s signal is private to himself.1 We allow for arbitrary state-dependent preferences for both players but no contingent transfers. The principal faces two decision problems. First, the principal designs the information structure of the public signal, subject to a constraint on the maximal informativeness. After the principal observes the signal realization, she also decides whether to act with the current information, or to delegate the decision to the agent. Although these decisions are made at different times, the principal must take into account the delegation decision in the design of the public signal. Figure 1 illustrates.\nWe begin by characterizing the necessary and sufficient conditions under which delegation is\n1 For the ease of modeling, we assume that the principal always reveals her signal to the agent regardless of whether she delegates to the agent, which makes the principal\u2019s signal essentially public. It is without loss of generality because the agent cannot use the signal when he is not delegated with the decision.\nvaluable. Proposition 1 proves that delegation strictly improves the principal\u2019s payoff if and only if the principal would take the same action as the agent if she were to observe the agent\u2019s signal. In all other cases, the principal is either indifferent or strictly prefers to take control due to the agent\u2019s misalignment. We then analyze the comparative statics on the principal\u2019s payoffs with respect to the informativeness of the agent\u2019s signal and the degree of preference misalignment between the players. Proposition 3 highlights an interesting result: the principal\u2019s delegation payoff could strictly decrease when the agent becomes more informative. In particular, this happens when the principal\u2019s interim posterior (after observing the public signal) prefers the action that is less preferred by the agent. On the other hand, Proposition 6 shows that the principal\u2019s payoff weakly increases when facing a more aligned agent.\nAfter characterizing the optimal delegation strategy, we turn to the principal\u2019s problem of designing the optimal public signal. We show in Proposition 8 that the most informative public signal is optimal if and only if a convexity condition on the posteriors under the maximal signal holds. Otherwise, the optimal signal maximizes information one state while restricting information about the other. Lastly, we highlight in Propositions 9 and 10 that naive policies, such as mandating delegation (keeping a \u201chuman-in-the-loop\u201d) or disregarding persuasion (maximizing prediction accuracy), strictly worsen decision quality for some decision subjects in the absence of perfectly aligned agents and state-revealing signals.\nThis paper combines two strands of literature in economics \u2013 optimal delegation and Bayesian persuasion. The classical delegation problem considers a principal who faces a privately informed but misaligned agent. The principal influences the agent\u2019s behavior by specifying a set of actions (called the delegation set) from which the agent can choose [Holmstr\u00f6m, 1978, 1980]. Following that, Alonso and Matouschek [2008] and Amador and Bagwell [2013] characterize the optimal delegation set under more general preferences. This paper is also related to the Bayesian persuasion problem [Kamenica and Gentzkow, 2011]. The persuasion problem considers the situation where the principal influences the decision of the agent by designing the information structure of a publicly observable signal.\nThis paper differs from and contributes to the above literature by considering the joint design of optimal persuasion and delegation mechanisms. Lou [2022], Vairo [2023] study a similar problem on the joint design of delegation and disclosure rules. However, Lou [2022] focuses on a setting where the principal designs the structure of the agent\u2019s signal, whose realization remains unobservable to the principal. Our paper differs in that the principal designs the public signal instead of the agent\u2019s private signal, and can observe the realization of the public signal and choose to use it directly. On the other hand, Vairo [2023] assumes that the agent\u2019s private information is about his type, which affects the payoff of both players. In contrast, we assume that the agent holds private information about the underlying state, while the preference of the agent is common knowledge. This difference stems from the context of decision-aid algorithms we hope to model, where both the algorithm and the human agent try to learn about the true state and take the optimal action.\nThere have been efforts to model human-machine collaborations from both the fields of economics and computer science. Rastogi et al. [2022] and Donahue et al. [2022] investigate the optimal ways to aggregate independent algorithmic and human predictions, and under what conditions these aggregations outperform individual predictions. Straitouri et al. [2022] proposes an algorithm that selects an optimal set of possible labels and presents it to a human expect for final selection. Xu and Dean [2023] and McLaughlin and Spiess [2023] study theoretically how to adjust the design of decision-aid algorithms to counteract human biases. Agrawal et al. [2018] studies the complemen-\ntarity of machine predictions and humans\u2019 private information about the payoffs of state contingent actions. To our knowledge, this is the first paper to study the design of algorithms and delegation to humans, two elements of human-machine collaboration, in the presence of misaligned agents."
        },
        {
            "heading": "2 Problem Setting",
            "text": "There are two players: a principal (she) and an agent (he). The state of the world \u0398 can take one of the two values, \u03b8 \u2208 {0, 1}. It can represent \u201clow\u201d or \u201chigh,\u201d \u201cbad\u201d or \u201cgood,\u201d \u201cguilty\u201d or \u201cinnocent,\u201d etc. The principal and the agent share a common prior \u00b5 on the probability of the high state, \u00b5 = Pr(\u0398 = 1). Both players receive a publicly observable signal, while the agent also receives a private signal.\nThe principal chooses and commits to the information structure of the public signal. The information structure of the private signal is exogenous and common knowledge. After observing the public signal, the principal decides whether to take an action with the current information or to delegate the decision to the agent. If delegated, the agent observes both the public and private signals and takes an action."
        },
        {
            "heading": "2.1 Signals and Informativeness",
            "text": "We denote the public and private signals as S1 and S2, respectively, with their realizations s1 and s2. The set of possible signals realizations are {0, 1}. We call \u00b5s1 = Pr(\u0398 = 1 | S1 = s1) the interim posterior, which is the updated belief of the players after observing the public signal. We call \u00b5s1s2 = Pr(\u0398 = 1 | S1 = s1, S2 = s2) the final posterior, which is the agent\u2019s posterior after observing both signals. Signals are generated according to the following information structures:\nSignal S1 0 1\nState \u0398 0 p0 1\u2212 p0\n1 1\u2212 p1 p1\nTable 1: Public signal\nSignal S2 0 1\nState \u0398 0 q0 1\u2212 q0\n1 1\u2212 q1 q1\nTable 2: Agent\u2019s signal\nThe table entries represent Pr(s | \u03b8), the conditional probability of observing the signal s given the state \u03b8. We assume that p0 + p1 > 1 and q0 + q1 > 1, meaning that a good (bad) signal is good (bad) news.\nSince we are interested in the principal\u2019s payoffs when the signals become more informative, we need a theory to compare the informativeness of information structures. We adopt a standard measure of informativeness in informational economics, Blackwell informativeness [Blackwell, 1951, 1953]. We say that signal S is (Blackwell) more informative than signal S\u2032 if for any prior, the distribution of posterior beliefs after observing S is a mean-preserving spread of the distribution of posterior beliefs after observing S\u2032. In the context of binary signals, it is equivalent to saying that the interval between the two posteriors after observing the high and low realizations under S strictly contains the interval between the two posteriors under S\u2032.\nBlackwell informativeness is independent of decision-makers\u2019 priors and preferences. This means that any Bayesian agent, facing any decision problem, can obtain a higher expected payoff if she receives a signal that is Blackwell more informative. This is desirable in our setting since it allows the\ncomparison of signal informativeness to be independent of the prior or the preferences. However, we also note that the Blackwell order is incomplete \u2013 not all signals can be ranked in terms of Blackwell informativeness.\nTo make the problem nontrivial, we impose a maximal Blackwell constraint on the information design problem of the principal.2 The principal can only choose from signals that are equally or less informative than the maximally informative signal (i.e., the maximal signal), denoted by \u03bb. The posterior beliefs under the maximal signal are denoted by \u00b5\u03bb\n0 and \u00b5\u03bb 1 ."
        },
        {
            "heading": "2.2 Preferences and Payoffs",
            "text": "The principal and the agent have access to a common set of actions Y = {0, 1}. They have (possibly) misaligned preferences that can be represented by two payoff matrices:\nThe first subscript represents the state whereas the second represents the action. For example, rij represents the payoff of the principal when under state i, action j is taken. On top of the payoff structure, both players are von Neumann-Morgenstern expected utility maximizers. This payoff structure subsumes many commonly assumed payoff functions with binary states and actions, for example, when the principal and the agent have heterogeneous costs of taking certain actions; when the principal and the agent have different benefits of matching the state [Kamenica and Gentzkow, 2011]; and when the agent exhibits loss aversion when taking a risky action in a bad state. We impose the following assumption on the payoff structures:\nAssumption 1. The principal\u2019s payoffs satisfy r00 > r01 and r11 > r10. Similarly, the agent\u2019s payoffs satisfy v00 > v01 and v11 > v10. This assumption has two implications:\n(1) There is no dominant action (i.e., action one would take regardless of the state) for either player.\n(2) Each player\u2019s payoff function of action 1 crosses their payoff function of action 0 from below. This means that both players switch from action 0 to action 1 at some point as their beliefs about state 1 increase.\nWe show in Appendix A that this assumption makes the problem interesting, since the principal never strictly prefers to delegate to the agent when this assumption is violated and the information design problem is trivially solved by choosing the maximally informative signal.\nIn Figure 2, we plot an example of the two players\u2019 payoff functions with respect to the players\u2019 beliefs about the underlying state.\n2 If the principal has access to an unrestricted space of signals, she will simply choose the state-revealing signal and never delegate.\nFor example, UP 0 (\u00b7) denotes the Principal\u2019s payoff when taking action 0, as a function of the belief. The four payoff functions are linear combinations of the discrete payoffs. Assumption 1 guarantees that the payoff functions have intersections. The intersection points \u00b5\u0304P and \u00b5\u0304A represent the cutoff points at which the principal and the agent switch to the higher action, respectively. We call the interval between \u00b5\u0304P and \u00b5\u0304A the disagreement interval (I), since the players prefer different actions in this interval. Note that in this example, \u00b5\u0304P < \u00b5\u0304A, meaning that the principal prefers to take action 1 at a wider range of beliefs compared to the agent. Therefore, we say that action 1 is the principal-preferred action.\nOur analysis greatly simplifies if we look at the payoff envelopes of the principal. If the principal does not delegate, the envelope is simply the maximum of UP\n0 and UP 1 , which is continuous and\nhas a kink at \u00b5\u0304P . If the principal delegates, however, the agent makes the final decision. Thus, the principal\u2019s payoff follows the cutoff point of the agent and becomes discontinuous at \u00b5\u0304A.\n3 We denote them as the non-delegation envelope (VN ) and delegation envelope (VD), respectively, as shown in Figure 3."
        },
        {
            "heading": "2.3 Design Choices",
            "text": "Conditioning delegation on the public signal. In this model, we assume that the principal can condition the delegation decision on the realization of the public signal. Alternatively, one may assume that the principal must commit to delegation or direct action before observing the public signal. We note that these two models correspond to different decision problems: the former captures the decision-making process for each decision subject, whereas the latter chooses between a fully automated system or a human-in-the-loop system for all decisions.\n3 For simplicity of stating theorems, we assume that the agent takes the principal-preferred action when indifferent, so the delegation envelope evaluates to the top point at the discontinuity.\nHeterogeneity of preferences and signals for different decision subjects. One may be concerned with how generalizable our results are when the players\u2019 preferences and signal informativeness may change with respect to different decision subjects. However, we believe that this does not limit the overall applicability of our findings. This is because the decision-making process for each decision subject can be repeated under this framework, but with varied parameters for preferences, signal structures of the agent, and maximal Blackwell constraints.\nComplete information about the agent\u2019s preferences and informativeness. Common to theoretical work in persuasion and delegation, this paper makes the assumption that the preferences of both players and the information structure of the signals are common knowledge. Such assumptions allow us to cleanly characterize the optimal persuasion and delegation mechanisms. We recognize that a promising extension to this paper is to develop methods for separately identifying and estimating human agents\u2019 biases and information. We discuss this in more detail in the future work section."
        },
        {
            "heading": "3 Value of Delegation",
            "text": "In this section, we study the principal\u2019s optimal delegation decision after observing the realization of the public signal. Proposition 1 characterizes the necessary and sufficient condition for delegation to be strictly valuable.\nProposition 1 (Necessary and sufficient condition for strict delegation). Given a public signal realization s1 and the associated interim posterior \u00b5s1, delegation is strictly valuable to the principal if and only if the agent\u2019s final posteriors \u00b5s10 and \u00b5s11 lie on the opposing sides of the disagreement interval. In other words,\n\u00b5s10 < {\u00b5\u0304P , \u00b5\u0304A} < \u00b5s11.\nProof. We first prove the \u201cif\u201d direction. By Bayes\u2019 rule, the expected posteriors equals the prior, i.e., Es2 [\u00b5s1s2 ] = \u00b5s1 . Since the expected payoff of delegation, Es2 [VD(\u00b5s1s2)], is linear in beliefs, it equals the linear combination of VD(\u00b5s10) and VD(\u00b5s11) evaluated at the prior \u00b5s1 . Therefore, if the\nagent\u2019s posteriors lie on the opposing side of the disagreement interval, any linear combination of them is above the principal\u2019s non-delegation envelope VN . Thus, at any prior \u00b5s1 , delegation would give strictly higher payoff than non-delegation, i.e., delegation is strictly valuable.\nWe now prove the \u201conly if\u201d direction. For the sake of contradiction, suppose that \u00b5s10 and \u00b5s11 are not on the opposing sides of the disagreement interval. First observe that delegation cannot be strictly valuable whenever the points (\u00b5s10, VD(\u00b5s10)), (\u00b5s11, VD(\u00b5s11)), and (\u00b5s1 , VD(\u00b5s1)) are co-linear. Since if that is the case,\nEs2 [VD(\u00b5s1s2)] = VD(Es2 [\u00b5s1s2 ]) \u2264 VN (Es2 [\u00b5s1s2 ]).\nThe inequality results from the fact that the delegation envelope lies everywhere weakly below the non-delegation envelope. The only other case where the posteriors are not on the opposing side of the disagreement interval and are not co-linear with the prior is as shown in Figure 4(a). The expected delegation payoff is a linear combination with one posterior in the disagreement interval and another posterior across the discontinuity at \u00b5\u0304A. In this case, delegation makes the principal strictly worse off.\nIntuitively, Proposition 1 shows that the principal strictly prefers to delegate if and only if at the principal\u2019s interim posterior, the agent\u2019s signal can potentially induce two distinct actions that the principal would agree with if she were to know the agent\u2019s signal. To achieve this, the principal\u2019s interim posterior cannot be too confident such that the agent\u2019s signal cannot provide any \u201csurprise.\u201d The agent\u2019s signal also needs to be informative enough compared to his misaligned incentives, which is highlighted in the following Corollary.\nCorollary 2 (Necessary condition for strict delegation). Given a public signal realization s1 and the associated interim posterior \u00b5s1, for delegation to be strictly valuable the following must be true:\n\u00b5s11 \u2212 \u00b5s10 > |\u00b5\u0304P \u2212 \u00b5\u0304A| = |I|,\nthat is, the distance between the agent\u2019s final posteriors must be more than the length of the disagreement interval.\nProof. It follows immediately from Proposition 1.\nThis Corollary captures the intuitive trade-off between the agent\u2019s private information and preference misalignment. A necessary condition for the principal to delegate is that the agent\u2019s signal is informative enough (the distance between posteriors) compared to the degree of preference misalignment (the length of the disagreement interval)."
        },
        {
            "heading": "4 Comparative Statics on Delegation",
            "text": "This section presents the comparative statics results on the principal\u2019s payoffs with respect to a change in agent\u2019s informativeness or misalignment."
        },
        {
            "heading": "4.1 Informativeness of the Agent\u2019s Signal",
            "text": "When the principal decides whether to delegate, one important factor is the informativeness of the agent\u2019s signal. One may think that the principal is more likely to delegate to an agent with a bigger informational advantage. Surprising, this intuition does not always hold.\nAs a signal becomes more informative, Blackwell\u2019s theorem predicts that, for any prior, at least one of the two posteriors under the new signal will move further away from the prior compared to the old signal. We are interested in the comparative statics effect of increasing the agent\u2019s informativeness on two quantities. The first is the principal\u2019s delegation payoff, i.e., the payoff if she delegates to the agent at a given interim posterior. The second is the principal\u2019s payoff at the optimal delegation decision, i.e., the payoff when the she chooses optimally between delegation and taking the utility-maximizing action.\nWe show that the impact of a more informed agent depends on the location of the interim posterior. In some regions, the principal\u2019s delegation payoff can strictly decrease when the agent becomes more informed.\nProposition 3. When the principal\u2019s interim posterior is such that both players agree to take the principal-preferred action, changing the agent\u2019s signal from not very informative to moderately\ninformative can strictly reduce the principal\u2019s payoff of delegation.\nProof. We show that there are a class of situations in which Proposition 3 would apply. Denote as A the range of interim posteriors in which both players agree to take the principal-preferred action. Suppose that \u00b5s1 \u2208 A, and the agent currently receives a signal S2, which induces final posteriors \u00b5s10 and \u00b5s11. Consider the case where \u00b5s10, \u00b5s11 \u2208 A, i.e., when S2 is not too informative so that the agent\u2019s final posteriors are not far from the interim posterior. In this case, the final posteriors are co-linear with the interim posterior, so the principal receives the same payoff for delegation and taking the optimal action.\nSuppose now the agent receives a signal S\u2032 2 that is Blackwell more informative. By Blackwell\u2019s theorem, at least one of the final posteriors \u00b5\u2032s10 and \u00b5 \u2032 s11 would be strictly further away from the interim posterior \u00b5s1 . Consider S \u2032 2 that moves out the posterior closer to the disagreement interval (I) (such a signal must exist once the other posterior arrive at the end point and thus cannot be moved anymore). When the two final posteriors are still co-linear with the interim posterior, the payoff of delegation does not change for the principal. However, once one of the posterior falls into the disagreement interval, the principal\u2019s expected payoff of delegation falls below the non-delegation envelope (as shown in Figure 5(a)-5(b)).\nWe show that there exist cases in which delegating to a more informed agent is strictly worse off for the principal. However, this case only happens when the interim posterior lies in the interval A and when the agent\u2019s signal changes from not very informative to moderately informative. Even when \u00b5s1 \u2208 A, once S2 becomes informative enough so that neither of the posteriors is in the disagreement interval, an further increase in S2\u2019s informativeness would strictly increase the payoff of delegation. In addition, when \u00b5s1 /\u2208 A, increasing the informativeness of S2 always weakly increases the delegation payoff.\nThis proposition presents an interesting result: holding everything else equal, the principal\u2019s delegation payoff may decrease when facing a more informed agent. In particular, this happens when the principal\u2019s interim posterior are such that both players agree to take the principal-preferred action (i.e., the action less preferred by the agent). In this case, the risk of the agent switching to the suboptimal action for the principal upon receiving a somewhat informative signal outweighs the benefit of the signal. It is worth noting that this result echoes Alonso and Matouschek [2008], in\nwhich they show that the principal may give less discretion to an agent with a bigger informational advantage in some situations.\nIn contrast, we note that the principal\u2019s payoff at the optimal delegation decision is unaffected. This is precisely because the principal is indifferent between delegating or not before the agent\u2019s signal increases in informativeness, so she can always fall back to not delegating and ensure the same payoff as before.\nProposition 4. At any principal\u2019s interim posterior, an increase in the informativeness of the agent\u2019s signal weakly improves the principal\u2019s payoff at the optimal delegation decision.\nProof. First, we establish that the case where a marginal increase in the agent\u2019s informativeness can reduce the principal\u2019s delegation payoff only happens when the principal was indifferent between delegation and direct action under the original signal. To see this, note that a decrease in delegation payoff results from one of the final posteriors jumping downward at the discontinuity. Therefore, before the discontinuity, the two final posteriors must be co-linear with the interim posterior, making the principal indifferent between delegation or direct action. In this case, it is weakly optimal for the principal to take direct action at \u00b5s1 before and after the change in the agent\u2019s informativeness. Since the payoff of direct action does not change with the agent\u2019s signal, the principal\u2019s optimal payoff remains constant.\nIn all remaining cases, a marginal increase in the agent\u2019s informativeness weakly improves the principal\u2019s delegation payoff. Since the non-delegation payoff is unaffected, the payoff at the optimal delegation decision also weakly improves.\nTaken together, Propositions 3 and 4 address the question: is more information from the agent always better? The answer depends on whether the tool of delegation is available to the principal. If the principal is free to take her own actions, then having access to a more informed agent is always weakly beneficial. If the principal is required to delegate, however, an improvement in agent\u2019s informativeness does not guarantee a monotonic improvement in the payoff of the principal."
        },
        {
            "heading": "4.2 Degree of Preference Misalignment",
            "text": "The principal\u2019s delegation decision trades off the gain from the agent\u2019s private signal and the loss from preference misalignment. This section studies how the principal\u2019s payoffs change when the agent becomes less aligned with the principal.\nDefinition 5. Holding the principal\u2019s preferences fixed, we say that an agent is more misaligned if the disagreement interval induced by the preferences of the new agent strict contains that of the original agent.\nProposition 6. For any principal\u2019s interim posterior and any agent\u2019s signal, a more misaligned agent weakly decreases the principal\u2019s delegation payoff and payoff at the optimal delegation decision.\nProof. Suppose that the original disagreement interval is I and that it expands to I \u2032 \u2283 I under the new agent. Denote the principal\u2019s original delegation envelope VD and new delegation envelope V \u2032\nD. We consider the principal\u2019s delegation payoff. First, consider the \u00b5s1 and S2 such that both \u00b5s10\nand \u00b5s11 lie outside of the interval I \u2032 \\ I. Observe that VD(x) = V \u2032 D(x) for all x /\u2208 I \u2032 \\ I. Therefore, VD(\u00b5s1s2) = V \u2032 D(\u00b5s1s2). This implies that the delegation payoff Es2 [V \u2032\nD(\u00b5s1s2)], a linear combination of V \u2032D(\u00b5s10) and V \u2032 D(\u00b5s11), also stays the same as that under the original agent.\nThen, we consider the \u00b5s1 and S2 such that at least one of the \u00b5s10 and \u00b5s11 lies in I \u2032 \\ I. Observe that V \u2032D(x) < VD(x) for all x \u2208 I \u2032 \\ I. Therefore, V \u2032D(\u00b5s1s2) < VD(\u00b5s1s2) for at least one of realizations of S2, and stays the same for the other (or neither) realization. The delegation payoff Es2 [V \u2032 D(\u00b5s1s2)], a linear combination of V \u2032 D(\u00b5s10) and V \u2032\nD(\u00b5s11), is (strictly) lower than that under the original agent.\nWe have proved that the principal\u2019s delegation payoff weakly decreases under a more misaligned agent. We note that the principal\u2019s nondelegation payoff is the same regardless of the agent available to her. Therefore, the principal\u2019s payoff at the optimal delegation decision, which is the maximum of the two payoffs, also weakly decreases under a more misaligned agent.\nCorollary 7. For some principal\u2019s interim posterior and some agent\u2019s signal, a more misaligned agent strictly decreases the principal\u2019s delegation payoff and payoff at the optimal delegation decision.\nProposition 6 and Corollary 7 also provide clues on the change of the principal\u2019s delegation behavior when the agent becomes more misaligned. If the principal does not delegate before at a given interim posterior, she would not delegate when the agent becomes more misaligned. If the principal delegates before at a given interim posterior, she may no longer delegate to a more misaligned agent.\nSo far, we have considered the change in preference misalignment induced by a change in the agent\u2019s preferences. It is also possible that a greater preference misalignment is induced by a change in the principal\u2019s preference parameters. In Appendix B, we explore comparative statics results on the principal\u2019s payoffs under such cases."
        },
        {
            "heading": "5 Information Design",
            "text": "The principal not only decides whether to give decision authority to the agent but also designs the informational structure of the public signal. In choosing between public signals, she must take into account the optimal delegation decision in the second stage after the public signal is realized.\nWe are interested in the optimal design of the public signal conditional on making the optimal delegation decision in the second stage. If the signal space is unconstrained, it is trivial that\nthe principal would simply choose the state-revealing signal and never delegate. After we impose constraints on the maximal Blackwell informativeness in the signal space, this question becomes nontrivial: will the principal always choose the most informative signal within those constraints? We show that the answer is no, even if the principal has the option to use the signal by herself.\nWe first introduce a few more notations that would facilitate the analysis. Recall that the red diamond point in Figure 4 represents the principal\u2019s ex-ante delegation payoff, Es2 [VD(\u00b5s1s2)], at the interim posterior \u00b5s1 . Holding fixed the agent\u2019s signal and player\u2019s preferences, this value is a function of the principal\u2019s interim posterior, \u00b5s1 . Therefore, we can plot Es2 [VD(\u00b5s1s2)] as a function of \u00b5s1 for any fixed agent\u2019s signal and player\u2019s preferences.\nIn Figure 7, principal\u2019s delegation envelope (VD) is plotted as dashed lines and her ex-ante delegation payoff (Es2 [VD(\u00b5s1s2)]) is plotted as solid lines. Each point on the solid line is the probability weighted average of two points on the dashed line.4 The red points provide one such example. When the principal\u2019s preference and the agent\u2019s signal are symmetric (r00 = r11, r01 = r10, and q0 = q1 = q), averaging produces a constant payoff in the intermediate section. This special case is proved in Appendix C. In general, the function is a piecewise linear function with three sections: the left and right sections follow the delegation envelope, while the middle section can be flat, upward, or downward sloping.\nThe value of this function represents the principal\u2019s payoff if she were to delegate at a given interim posterior \u00b5s1 . However, this is only one part of the equation in what the principal can obtain at the delegation stage. She also has the choice of taking an action directly according to her interim posterior. Therefore, principal\u2019s optimal delegation-stage payoff is the maximum between the ex-ante delegation payoff and the payoff on the non-delegation envelope. Mathematically, we define:\nH(\u00b5s1) := max{Es2 [VD(\u00b5s1s2)], VN (\u00b5s1)}\nFigure 8(a) and 8(b) plot the principal\u2019s ex-ante delegation payoff, the non-delegation envelope, and the resulting H(\u00b5s1) as the maximum of the two. It is also possible to have the discontinuity past the principal\u2019s cutoff point \u00b5\u0304P , which results in H(\u00b5s1) as plotted in Appendix D, but all analyses below still go through in such cases.\nOne important point in the analysis is the point of discontinuity, which we label as (\u03c1,H(\u03c1)). Since we made the assumption that the agent takes the principal-preferred action when indifferent,\n4 The weights are the probabilities of S2 = 0 and S2 = 1 conditional on s1.\nH is upper semicontinuous at \u03c1. In words, \u03c1 is the interim posterior such that, upon receiving another principal-preferred signal realization, the agent\u2019s final posterior equals \u00b5\u0304A (and thus is indifferent between two actions). Algebraically,\n\u03c1 = (1\u2212 q0)\u00b5\u0304A\n(1\u2212 q0)\u00b5\u0304A + q1(1\u2212 \u00b5\u0304A) if \u00b5\u0304A > \u00b5\u0304P\n\u03c1 = q0\u00b5\u0304A\nq0\u00b5\u0304A + (1\u2212 q1)(1\u2212 \u00b5\u0304A) if \u00b5\u0304A < \u00b5\u0304P\nThe principal\u2019s information design problem is thus choosing the public signal that, given prior \u00b5 and Blackwell constraint \u03bb, generates two interim posteriors \u00b50 and \u00b51 such that H(\u00b50) and H(\u00b51) give the maximal expected payoff. Recall that we denote the posteriors generated by the maximally informative signal subject to Blackwell constraint \u03bb as \u00b5\u03bb\n0 and \u00b5\u03bb 1 . We are now ready to state our\nmain proposition for the optimal information design.\nProposition 8 (Optimal information design). Given a prior \u00b5 and a Blackwell constraint \u03bb, the principal chooses the most informative public signal if and only if there exists a weakly convex and continuous function c such that the points (\u00b5\u03bb\n0 ,H(\u00b5\u03bb 0 )), (\u00b5\u03bb 1 ,H(\u00b5\u03bb 1 )), and (\u03c1,H(\u03c1)) are in the graph\nof c.5 If this is the case, we say that these three points can be \u201cconvexified\u201d. If they cannot be convexified,\n1. If \u00b5 > \u03c1, \u00b5\u2217 1 = \u00b5\u03bb 1 and \u00b5\u2217 0 = \u03c1. The optimal signal generates the high posterior at the Blackwell\nconstraint and the low posterior at \u03c1.\n2. If \u00b5 < \u03c1, \u00b5\u2217 0 = \u00b5\u03bb 0 and \u00b5\u2217 1 = \u03c1. The optimal signal generates the low posterior at the Blackwell\nconstraint and the high posterior at \u03c1.\n3. If \u00b5 = \u03c1, \u00b5\u2217 1 = \u00b5\u2217 0 = \u00b5. The optimal signal is completely uninformative.\nProof. First, if the posteriors of the maximally informative signal lie on the same side of the discontinuity at \u03c1, then it is trivial that they can be convexified with (\u03c1,H(\u03c1)) as one of the end points\n5 For the if direction, we need to assume that the principal chooses the most informative signal when indifferent.\n(Figure 9(e), 9(f)). In this case, the maximal signal is at least weakly optimal since expanding posteriors on a weakly convex function generates a weakly higher expected payoff.\nNext, we consider the case when the two posteriors lie on different sides of the discontinuity at \u03c1. Observe that in this case the principal\u2019s expected payoff always weakly increases when the two posteriors marginally move away from the prior (when the signal becomes more Blackwell informative). Therefore, the maximal signal gives the highest expected payoff among all the signals that generate posteriors on different sides of (\u03c1,H(\u03c1)). What we need to compare now is the payoff of the maximal signal with the payoff of having one of the posteriors stop at \u03c1.\nSuppose that the three points can be convexified, with (\u03c1,H(\u03c1)) in the middle (Figure 9(a), 9(c)). The convexity of the connecting function ensures that the line segment between the two posteriors lies weakly above the line segment connecting either of the posterior with (\u03c1,H(\u03c1)). Therefore, no matter where the prior, the expected payoff generated by the maximal signal exceeds that generated by placing one of the posteriors at \u03c1. Suppose instead that there exists no weakly convex and continuous function connecting the three points. In the case of three points, it has to be the case that they can be connected by a strictly concave function. Then, the line segment between the two posteriors lies strictly below the line segment connecting one of the posteriors with (\u03c1,H(\u03c1)). This proves that restricting one posterior to \u03c1 produces strictly higher expected payoff than the maximal signal. How about moving the posterior even closer to \u00b5? This is not optimal since the section of the curve between \u03c1 and \u00b5 is now weakly convex so contracting the posteriors gives a weakly lower payoff.\nWhy is the point (\u03c1,H(\u03c1)) the optimal posterior when the principal does not give out the most informative signal? As argued before, placing one of the interim posteriors at \u03c1 would make the agent indifferent between the two actions if he receives a principal-preferred realization of the private signal. Therefore, he would take the principal-preferred action out of indifference. If instead, the agent strictly prefers to take the principal-preferred action, then the principal can increase the probability of the principal-preferred realization in the agent-preferred state and still ensures that agent takes the same action.\nFor example, suppose that the principal preferred action is 1. If the agent strictly prefers to take action 1 when observing (S1 = 1, S2 = 1), then the principal could slightly decrease Pr(S1 = 0 | \u0398 = 0) and increase Pr(S1 = 1 | \u0398 = 0) to lower the interim posterior \u00b511 back to \u03c1, while still ensuring that the agent takes action 1 when observing (S1 = 1, S2 = 1). The principal has thus constructed a signal with higher chance of realizing S1 = 1 while leaving the agent\u2019s optimal action given the signal realization unchanged, and thus strictly increased her payoff. This is a generalization of the indifference result in Kamenica and Gentzkow [2011] applied to our setting where the receiver can receive an additional signal."
        },
        {
            "heading": "5.1 Features of the Optimal Information Design",
            "text": "This section highlights features of the optimal information design that may inform the design of real-life algorithms that interact with misaligned agents.\nMaximizing prediction accuracy in the presence of a biased agent. One implication of the optimal information design is that whether one should provide the most informative public signal depends on what the maximally informative signal can achieve. For a given prior, if the maximal signal is sufficiently uninformative, then there is no harm in providing it because: 1) the principal\ncan use it in the delegation stage; and 2) even if the principal decides to delegate, the public signal is not strong enough to sway the agent\u2019s decision. If the maximal signal can generate interim posteriors sufficiently confident (near the end points), then it is strictly optimal to maximize informativeness because the principal and the agent tend to agree near the extreme beliefs. However, if the maximal signal only provides moderately confident interim posteriors, it is usually optimal for the principal to delegate in order to obtain more information. Given that the principal will delegate in the second stage, a \u201cskewed\u201d signal designed to persuade the agent can do better than providing the maximally informative signal.\nOne-sided information. In all but the the edge case (\u00b5 = \u03c1), the optimal information structure maximizes the signal strength on one side. Specifically, when the prior is on the right (left) of the discontinuity point \u03c1, the optimal signal places the posterior at the Blackwell constraint on the right (left) side. This implies different investment decisions between lowering an algorithm\u2019s false negative rate (1\u2212 p1) versus false positive rate (1\u2212 p0) when we care about quality of the the final decisions rather than that of the predictions. For example, if \u00b5 > \u03c1, the optimal signal maximizes the high posterior \u00b51 while potentially restricting the low posterior \u00b50. This implies that the optimal signal features a very low false positive rate and a moderate false negative rate."
        },
        {
            "heading": "6 Policy Restrictions and Decision Quality",
            "text": "In our model, the principal can utilize the tools of persuasion and delegation to mitigate the agent\u2019s preference misalignment. First, as she designs the information structure of the public signal, she can take into account the possibility of passing the information and the decision to the misaligned agent. Second, upon each signal realization, she can decide whether to delegate the decision to the agent or to act directly based on the current information.\nIn real-life applications, however, the principal may have limited access to these tools due to legal, institutional, or moral constraints. For example, Article 14 of the EU AI act requires that high-risk AI systems must be designed such that they can be \u201ceffectively overseen by natural persons\u201d [European Commission, 2021]. In the context of our model, it implies that the principal is restricted to always delegate. In other applications, the algorithm may be required to be \u201ctruthful,\u201d meaning it simply provides the most informative signal regardless of the human it interacts with. Indeed, in most traditional applications of decision-aid algorithms, neither of these strategic tools were available. The algorithm is no more than a prediction tool with no potential to persuade nor any authority to take actions by itself.\nIndeed, these policies, such as designing maximally informative algorithms and obtaining additional information from human agents, improves decision quality when there is no preference misalignment. However, in situations where there is an interaction between the potential biases of the agent and information provision, our analysis show that these naive policies may in fact be welfare worsening for the principal. This is formalized in the following propositions.\nProposition 9. If \u00b5\u0304P 6= \u00b5\u0304A and q0, q1 < 1, there exists interim posterior \u00b5s1 at which delegating to the agent is strictly worse than taking direct action.\nProof. The principal benefits from the option to not delegate when at the interim posterior, the ex-ante payoff of delegation is strictly lower than the non-delegation envelope. In Figure 8(a), it is\nthe part when the solid line (ex-ante payoff of delegation) is strictly lower than the dashed line (nondelegation envelope). This part of sub-optimal delegation is generated by averaging between two final posteriors, one of which falls into the disagreement interval. Unless \u00b5\u0304P = \u00b5\u0304A, the disagreement interval exists. If q0, q1 < 1, the agent\u2019s final posterior can fall into the disagreement interval.\nProposition 10. If \u00b5\u0304P 6= \u00b5\u0304A and q0, q1 < 1, there exists prior \u00b5 and Blackwell constraint \u03bb such that providing no algorithm is strictly better than providing the maximally informative algorithm.\nProof. Providing no (or a completely uninformative) algorithm can be better off than providing the maximal algorithm whenever there is a discontinuity in the optimal delegation-stage payoff and the principal\u2019s prior lies in the flat region of H (as in Figure 9(b)). This discontinuity is a product of the agent\u2019s final posteriors having a discontinuous jump on the delegation envelope VD. Unless \u00b5\u0304P = \u00b5\u0304A, the discontinuity in VD exists. If q0, q1 < 1, the agent\u2019s final posteriors can fall near the discontinuity and create non-convexity in payoffs. In that case, the principal may be better off providing no algorithm and enjoying the delegation payoff at the prior.\nUnless the agent is perfectly aligned or one of his signal realizations is state-revealing, Propositions 9 and 10 imply that there exist cases where it is strictly beneficial to remove the agent from the decision-making process or to provide no algorithmic assistance to the agent. These theoretical results may explain the empirical puzzle of why the introduction of highly predictive decision-aid algorithms often leads to underperforming human-machine collaborations. For example, Jacobs et al. [2021] find that in the medical domain, algorithmic aids did not enhance clinicians\u2019 accuracy in choosing antidepressants, with both aided and unaided clinicians underperforming compared to standalone machine-learning systems. Likewise, Imai et al. [2021] and Stevenson and Doleac [2022] observe in the judicial context that risk assessment tools barely influenced judges\u2019 decisions on pretrial detention and sentencing, failing to notably improve public safety or lower incarceration rates. While these findings may seem puzzling, this paper suggests that the underperformance of human-machine collaborations is not only understandable, but even expected, if no measures are taken to mitigate preference misalignment between algorithm and expert.\nNote that possible misalignment of the agent includes not only any explicit racial or gender biases but also common behavioral biases such as risk aversion, complexity aversion, or preference for inaction. Additionally, the agent must be a Bayesian agent who correctly updates his beliefs upon receiving multiple sources of information. We argue that these conditions are hard to satisfy in real-life human agents. In that case, implementing naive policies that are seemingly conducive to better decision making may in fact worsen decision quality and incur welfare losses for society."
        },
        {
            "heading": "7 Discussion and Future Work",
            "text": "In this paper we consider the joint decision problem of designing prediction algorithms and deciding when to delegate to a privately informed and biased agent. We develop conditions under which delegating to the agent is strictly optimal, and study the change in the principal\u2019s payoffs when the agent becomes better informed or more misaligned. We show that given the optimal delegation decision, the optimal algorithm may not be maximally informative. In particular, the optimal algorithm maximizes information about one state while restricting information about the other. In the absence of perfectly aligned agents and state-revealing signals, we show that there exist cases\nwhere it is strictly beneficial to remove the agent from the decision-making process or to provide no algorithmic assistance.\nOur characterization highlights that naive policies, such as designing maximally informative algorithms and mandating delegation to human agents, may strictly worsen decision quality for some decision subjects. This implies that even well-intentioned policies aiming to provide more information can produce counter-intuitive results if we fail to account for the strategic interaction between information and biases. Echoing recent works in the Economics and Computation area [Xu and Dean, 2023, McLaughlin and Spiess, 2023], our results once again demonstrate that good algorithmic predictions do not directly translate to good final decisions. We suggest that the underperformance of human-machine collaborations widely observed in empirical settings can be understood through this theoretical lens.\nWe recognize that an important direction for future work is to go beyond theoretical characterizations and estimate human agents\u2019 biases and information from data. Separately identifying agents\u2019 biases from their private information can be challenging, as shown in Rambachan et al. [2021]. However, it may be possible with quasi-experimental or experimental choice data observed in human-machine collaborative settings. Related to this, it could be worthwhile to empirically discern agents\u2019 preference biases (due to different objectives) from updating biases (due to incorrect use of algorithmic predictions). Finally, given that the optimal algorithm interacting with biased agents may withhold information about some states, it would be important to test experimentally whether a strategic algorithm can be detected by users who may then stop trusting the algorithm."
        },
        {
            "heading": "A Justification of Assumption 1",
            "text": "Assumption 1 assumes that the principal\u2019s payoffs satisfy r00 > r01 and r11 > r10. Similarly, the agent\u2019s payoffs satisfy v00 > v01 and v11 > v10. We will show that this assumption is without loss of generality, since the principal never strictly prefers delegation when this assumption is not satisfied.\nWe start with the principal. Suppose that one of the inequality is not satisfied for the principal, say, r00 > r01 but r11 \u2264 r10. In this case, the payoff of taking action 0 in any state is weakly higher than that of taking action 1 in the same state. In other words, the principal has a dominant action 0. Therefore, it is never strictly valuable for the principal to delegate the decision to the agent because the principal would maximize her payoff by taking the dominant action.\nSuppose that neither inequalities are satisfied for the principal, i.e., r00 \u2264 r01 and r11 \u2264 r10. In this case, the principal has a preference for \u201cmismatching the state.\u201d In particular, the principal\u2019s preferred action switches from 1 to 0 as her belief of the probability of state 1 increases. We further suppose that the agent has the normal \u201cstate-matching\u201d preferences. Then, the \u201cdisagreement interval (I)\u201d would in fact be two disjoint intervals at the two ends of the unit interval (as shown in Figure 10). The principal\u2019s payoff envelope in this case is also depicted in Figure 10. It is easy to see that the principal cannot strictly improve her payoff via delegation, since no linear combination on the delegation envelope can be above the non-delegation envelope.\nWe now turn to the assumption on the agent. Same as the principal, the agent would have a dominant action if only one of the inequality for the agent is not satisfied. It is then obvious that the principal cannot have strict improvements via delegation if the agent always take one action.\nIf neither inequalities are satisfied for the agent, the problem is symmetric with that of the principal. The \u201cdisagreement\u201d interval would still be disjoint and the principal\u2019s payoff envelope looks like the mirror image of Figure 10. It follows that the principal cannot have strict improvements by delegation.\nFinally, if none of the four inequalities are satisfied, i.e., both players have \u201cstate-mismatching\u201d preferences, then with relabelling the actions, the analysis in the main text will go through."
        },
        {
            "heading": "B Comparative statics on preference misalignment induced by the",
            "text": "principal\nIn this appendix, we consider the cases when the disagreement interval expands not because of a change in the agent\u2019s preferences but that of the principals. There are two ways this can happen, as shown in Figure 11: (1) decrease the principal payoff for the agent\u2019s preferred action, and (2) increase the principal\u2019s payoff for the agent\u2019s less preferred action.\nProposition 11. For any principal\u2019s interim posterior and any agent\u2019s signal, both the principal\u2019s payoff of delegation and payoff at the optimal delegation decision weakly decreases when the disagreement interval expands through (1), and weakly increases when the disagreement interval expands through (2).\nProof. Suppose that the original disagreement interval is I and that it expands to I \u2032 \u2283 I under the new agent. Denote the principal\u2019s original delegation envelope VD and new delegation envelope V \u2032\nD. Denote the principal\u2019s original nondelegation envelope VN and new nondelegation envelope V \u2032\nN . We observe that in case (1), both the principal\u2019s delegation and nondelegation envelope is weakly\nlower than before, i.e., V \u2032D(x) \u2264 VD(x) and V \u2032 N (x) \u2264 VN (x) for all x. Since the principal\u2019s payoff of delegation is a linear combination of two points on the delegation envelope, it is also weakly lower than before. Similarly, the principal\u2019s payoff of nondelegation is a linear combination of two points on the nondelegation envelope, which is weakly lower than before. The principal\u2019s payoff at the optimal delegation decision, being the maximum of the two, is thus weakly lower than before.\nCase (2) is symmetric in that both the principal\u2019s delegation and nondelegation envelope is weakly higher than before, i.e., V \u2032D(x) \u2265 VD(x) and V \u2032\nN (x) \u2265 VN (x) for all x. By a symmetric argument, both the principal\u2019s payoff of delegation and payoff at the optimal delegation decision is weakly higher than before."
        },
        {
            "heading": "C Derivation of the principal\u2019s ex-ante delegation payoff",
            "text": "This appendix derives the general formula for the intermediate section of the principal\u2019s ex-ante delegation payoff function. It also shows that the intermediate section will be flat if the principal\u2019s\npayoffs are symmetric. Since we are considering the posteriors and signals in the delegation stage, we can simplify the notations to omit the mentioning of signal s1. We redefine:\n\u00b5 := \u00b5s1 , \u00b50 := \u00b5s10, \u00b51 := \u00b5s11\nBeing in the intermediate section means that each point is produced by averaging two points on the left and right part of the principal\u2019s delegation envelope. Let g be the principal\u2019s delegation envelope.\ng(\u00b50) = a+ b\u00b50, g(\u00b51) = c+ d\u00b51\nThe slope of the line segment connecting the two points is:\na+ b\u00b50 \u2212 c\u2212 d\u00b51 \u00b50 \u2212 \u00b51\nWe can write out the function of the connecting line segment h:\nh(x) = a+ b\u00b50 \u2212 c\u2212 d\u00b51\n\u00b50 \u2212 \u00b51 (x\u2212 \u00b51) + c+ d\u00b51\nEvaluate h at the prior \u00b5:\nh(\u00b5) = a+ b\u00b50 \u2212 c\u2212 d\u00b51\n\u00b50 \u2212 \u00b51 (\u00b5 \u2212 \u00b51) + c+ d\u00b51\nIt\u2019s useful to write the prior as an weighted average of two posteriors:\n\u00b5 = w1\u00b50 + (1\u2212 w1)\u00b51, w1 = q0(1\u2212 \u00b5) + (1\u2212 q1)\u00b5\nSubstitute in \u00b5:\nh(\u00b5) = a+ b\u00b50 \u2212 c\u2212 d\u00b51\n\u00b50 \u2212 \u00b51 (w1\u00b50 + (1\u2212 w1)\u00b51 \u2212 \u00b51) + c+ d\u00b51\n= a+ b\u00b50 \u2212 c\u2212 d\u00b51\n\u00b50 \u2212 \u00b51 (\u00b50 \u2212 \u00b51)w1 + c+ d\u00b51\n= (a+ b\u00b50 \u2212 c\u2212 d\u00b51)w1 + c+ d\u00b51\n= c+ (a\u2212 c+ b\u00b50)w1 + d\u00b51(1\u2212w1)\n= c+ (a\u2212 c+ b\u00b50)w1 + d(\u00b5 \u2212 w1\u00b50)\n= c+ (a\u2212 c+ (b\u2212 d)\u00b50)w1 + d\u00b5\nSubstitute in \u00b50 and w1 and simplify:\nh(\u00b5) = c+ (a\u2212 c)q0 + (a\u2212 c)(1 \u2212 q0 \u2212 q1)\u00b5+ (b\u2212 bq1 + dq1)\u00b5\nThe slope of h(\u00b5) with respect to \u00b5 is thus:\ns = (a\u2212 c)(1 \u2212 q0 \u2212 q1) + (b\u2212 bq1 + dq1)\nWe can also express them in terms of model primitives by substituting a = r00, b = r10 \u2212 r00, c = r01, and d = r11 \u2212 r01:\ns = r10 \u2212 r01 + (r01 \u2212 r00)q0 + (r11 \u2212 r10)q1\nWhen the principal\u2019s payoff is symmetric (r01 = r10 and r11 = r00) as well as when the public signal is symmetric (q0 = q1), we have that\ns = 0, h(\u00b5) = q.\nD The optimal delegation-stage payoff when \u03c1 > \u00b5\u0304P"
        }
    ],
    "year": 2024
}