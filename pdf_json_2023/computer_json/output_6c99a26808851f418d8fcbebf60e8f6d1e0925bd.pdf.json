{
    "abstractText": "Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts k\u2217 is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when k\u2217 becomes unknown and the true model is over-specified by a Gaussian mixture of k experts where k > k\u2217, our findings suggest that the number of experts selected from the top-K sparse softmax gating function must exceed the total cardinality of a certain number of Voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. Moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Huy Nguyen"
        },
        {
            "affiliations": [],
            "name": "Pedram Akbarian"
        },
        {
            "affiliations": [],
            "name": "Fanqi Yan"
        },
        {
            "affiliations": [],
            "name": "Nhat Ho"
        }
    ],
    "id": "SP:92e4b7f690b4010e755c298137c75aba43d0571a",
    "references": [
        {
            "authors": [
                "S. Balakrishnan",
                "M.J. Wainwright",
                "B. Yu"
            ],
            "title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
            "venue": "Annals of Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "H. Bao",
                "W. Wang",
                "L. Dong",
                "Q. Liu",
                "O.-K. Mohammed",
                "K. Aggarwal",
                "S. Som",
                "S. Piao",
                "F. Wei"
            ],
            "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Bengio"
            ],
            "title": "Deep learning of representations: Looking forward",
            "venue": "In International Conference on Statistical Language and Speech Processing,",
            "year": 2013
        },
        {
            "authors": [
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning",
            "venue": "arxiv preprint arxiv 1406.7362,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Chow",
                "A. Tulepbergenov",
                "O. Nachum",
                "D. Gupta",
                "M. Ryu",
                "M. Ghavamzadeh",
                "C. Boutilier"
            ],
            "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "N. Du",
                "Y. Huang",
                "A.M. Dai",
                "S. Tong",
                "D. Lepikhin",
                "Y. Xu",
                "M. Krikun",
                "Y. Zhou",
                "A. Yu",
                "O. Firat",
                "B. Zoph",
                "L. Fedus",
                "M. Bosma",
                "Z. Zhou",
                "T. Wang",
                "E. Wang",
                "K. Webster",
                "M. Pellat",
                "K. Robinson",
                "K. Meier-Hellstern",
                "T. Duke",
                "L. Dixon",
                "K. Zhang",
                "Q. Le",
                "Y. Wu",
                "Z. Chen",
                "C. Cui"
            ],
            "title": "Glam: Efficient scaling of language models with mixture-of-experts",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "R. Dwivedi",
                "N. Ho",
                "K. Khamaru",
                "M.J. Wainwright",
                "M.I. Jordan",
                "B. Yu"
            ],
            "title": "Sharp analysis of expectation-maximization for weakly identifiable models",
            "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "R. Dwivedi",
                "N. Ho",
                "K. Khamaru",
                "M.J. Wainwright",
                "M.I. Jordan",
                "B. Yu"
            ],
            "title": "Singularity, misspecification, and the convergence rate of EM",
            "venue": "Annals of Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "W. Fedus",
                "B. Zoph",
                "N. Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "A. Guha",
                "N. Ho",
                "X. Nguyen"
            ],
            "title": "On posterior contraction of parameters and interpretability in Bayesian mixture modeling",
            "year": 2021
        },
        {
            "authors": [
                "A. Gulati",
                "J. Qin",
                "C. Chiu",
                "N. Parmar",
                "Y. Zhang",
                "J. Yu",
                "W. Han",
                "S. Wang",
                "Z. Zhang",
                "Y. Wu",
                "R. Pang"
            ],
            "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
            "venue": "In Proc. Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "S. Gupta",
                "S. Mukherjee",
                "K. Subudhi",
                "E. Gonzalez",
                "D. Jose",
                "A. Awadallah",
                "J. Gao"
            ],
            "title": "Sparsely activated mixture-of-experts are robust multi-task learners",
            "venue": "arXiv preprint arxiv 2204.0768,",
            "year": 2022
        },
        {
            "authors": [
                "H. Hazimeh",
                "Z. Zhao",
                "A. Chowdhery",
                "M. Sathiamoorthy",
                "Y. Chen",
                "R. Mazumder",
                "L. Hong",
                "E.H. Chi"
            ],
            "title": "Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "N. Ho",
                "K. Khamaru",
                "R. Dwivedi",
                "M.J. Wainwright",
                "M.I. Jordan",
                "B. Yu"
            ],
            "title": "Instability, computational efficiency and statistical accuracy",
            "venue": "arXiv preprint arXiv:2005.11411,",
            "year": 2020
        },
        {
            "authors": [
                "N. Ho",
                "C.Y. Yang",
                "M.I. Jordan"
            ],
            "title": "Convergence rates for Gaussian mixtures of experts",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "R.A. Jacobs",
                "M.I. Jordan",
                "S.J. Nowlan",
                "G.E. Hinton"
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural Computation,",
            "year": 1991
        },
        {
            "authors": [
                "M.I. Jordan",
                "R.A. Jacobs"
            ],
            "title": "Hierarchical mixtures of experts and the EM algorithm",
            "venue": "Neural Computation,",
            "year": 1994
        },
        {
            "authors": [
                "D. Lepikhin",
                "H. Lee",
                "Y. Xu",
                "D. Chen",
                "O. Firat",
                "Y. Huang",
                "M. Krikun",
                "N. Shazeer",
                "Z. Chen"
            ],
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "B. Li",
                "Y. Shen",
                "J. Yang",
                "Y. Wang",
                "J. Ren",
                "T. Che",
                "J. Zhang",
                "Z. Liu"
            ],
            "title": "Sparse mixtureof-experts are domain generalizable learners",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "H. Liang",
                "Z. Fan",
                "R. Sarkar",
                "Z. Jiang",
                "T. Chen",
                "K. Zou",
                "Y. Cheng",
                "C. Hao",
                "Z. Wang"
            ],
            "title": "M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model- Accelerator Co-design",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "T. Manole",
                "N. Ho"
            ],
            "title": "Refined convergence rates for maximum likelihood estimation under finite mixture models",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "T. Manole",
                "A. Khalili"
            ],
            "title": "Estimating the number of components in finite mixture models via the group-sort-fuse procedure",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "H. Nguyen",
                "T. Nguyen",
                "N. Ho"
            ],
            "title": "Demystifying softmax gating in Gaussian mixture of experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "H. Nguyen",
                "T. Nguyen",
                "K. Nguyen",
                "N. Ho"
            ],
            "title": "Towards convergence rates for parameter estimation in Gaussian-gated mixture of experts. arxiv preprint arxiv 2305.07572",
            "year": 2023
        },
        {
            "authors": [
                "F. Peng",
                "R. Jacobs",
                "M. Tanner"
            ],
            "title": "Bayesian Inference in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models With an Application to Speech Recognition",
            "venue": "Journal of the American Statistical Association,",
            "year": 1996
        },
        {
            "authors": [
                "C. Riquelme",
                "J. Puigcerver",
                "B. Mustafa",
                "M. Neumann",
                "R. Jenatton",
                "A.S. Pint",
                "D. Keysers",
                "N. Houlsby"
            ],
            "title": "Scaling vision with sparse mixture of experts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A. Rives",
                "J. Meier",
                "T. Sercu",
                "S. Goyal",
                "Z. Lin",
                "J. Liu",
                "D. Guo",
                "M. Ott",
                "C.L. Zitnick",
                "J. Ma",
                "R. Fergus"
            ],
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "N. Shazeer",
                "A. Mirhoseini",
                "K. Maziarz",
                "A. Davis",
                "Q. Le",
                "G. Hinton",
                "J. Dean"
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "B. Sturmfels"
            ],
            "title": "Solving Systems of Polynomial Equations",
            "venue": "(Cited on pages",
            "year": 2002
        },
        {
            "authors": [
                "H. Teicher"
            ],
            "title": "On the mixture of distributions",
            "venue": "Annals of Statistics,",
            "year": 1960
        },
        {
            "authors": [
                "H. Teicher"
            ],
            "title": "Identifiability of mixtures",
            "venue": "Annals of Statistics,",
            "year": 1961
        },
        {
            "authors": [
                "H. Teicher"
            ],
            "title": "Identifiability of finite mixtures",
            "venue": "Ann. Math. Statist.,",
            "year": 1963
        },
        {
            "authors": [
                "S. van de Geer"
            ],
            "title": "Empirical Processes in M-estimation",
            "venue": "(Cited on pages",
            "year": 2000
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Topics in Optimal Transportation",
            "venue": "American Mathematical Society,",
            "year": 2003
        },
        {
            "authors": [
                "C. Villani"
            ],
            "title": "Optimal transport: Old and New",
            "venue": "(Cited on page",
            "year": 2008
        },
        {
            "authors": [
                "Z. You",
                "S. Feng",
                "D. Su",
                "D. Yu"
            ],
            "title": "Speechmoe2: Mixture-of-experts model with improved routing",
            "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhou",
                "N. Du",
                "Y. Huang",
                "D. Peng",
                "C. Lan",
                "D. Huang",
                "S. Shakeri",
                "D. So",
                "A. Dai",
                "Y. Lu",
                "Z. Chen",
                "Q. Le",
                "C. Cui",
                "J. Laudon",
                "J. Dean"
            ],
            "title": "Brainformers: Trading simplicity for efficiency",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 9.\n13 85\n0v 1\n[ st\nat .M\nL ]\n2 5\nSe p\n20 23\nTop-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts k\u2217 is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when k\u2217 becomes unknown and the true model is over-specified by a Gaussian mixture of k experts where k > k\u2217, our findings suggest that the number of experts selected from the top-K sparse softmax gating function must exceed the total cardinality of a certain number of Voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. Moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions."
        },
        {
            "heading": "1 Introduction",
            "text": "Introduced by [17] and [18], the mixture of experts (MoE) has been known as a statistical machine learning design that leverages softmax gating functions to blend different expert networks together in order to establish a more intricate model. Recently, there has been a huge interest in a variant of this model called top-K sparse softmax gating MoE, which activates only the best K expert networks for each input based on sparse gating functions [29, 20]. Thus, this surrogate can be seen as a form of conditional computation [3, 4] since it significantly scales up the model capacity while avoiding a proportional increase in the computational cost. These benefits have been empirically demonstrated in several deep learning applications, including natural language processing [19, 7, 10, 38], speech recognition [26, 12, 37], computer vision [6, 27, 21, 2], multi-task learning [14, 13] and other applications [28, 5]. Nevertheless, to the best of our knowledge, the full theoretical analysis of the top-K sparse softmax gating MoE has remained missing in the literature. In this paper, we aim to shed some light on the theoretical understanding of that gating function from a statistical perspective via the convergence analysis of maximum likelihood estimation (MLE) under the top-K sparse softmax gating Gaussian MoE.\nThere have been previous efforts to study the parameter estimation problem in Gaussian MoE models. Firstly, [16] utilized the generalized Wasserstein loss functions [35, 36] to derive the rates for\nestimating parameters in the input-free gating Gaussian MoE. They pointed out that due to an interaction among expert parameters, these rates became increasingly slow when the number of extra experts rose. Subsequently, [24] and [25] took into account the Gaussian MoE with softmax gating and Gaussian gating functions, respectively. Since these gating functions depended on the input, another interaction between gating parameters and expert parameters arose. Therefore, they designed Voronoi loss functions to capture these interactions and observe that the convergence rates for parameter estimation under these settings can be characterized by the solvability of systems of polynomial equations, which are central in algebraic geometry [30].\nTurning to the top-K sparse softmax gating Gaussian MoE, the convergence analysis of the MLE, however, becomes substantially challenging due to the sophisticated structure of the top-K sparse softmax gating function compared to those of softmax gating and Gaussian gating functions. To comprehend these obstacles better, let us introduce the formal formulation of the top-K sparse softmax gating Gaussian MoE and relevant notions.\nProblem setup. Suppose that (X1, Y1), . . . , (Xn, Yn) \u2208 Rd \u00d7 R are i.i.d. samples of size n from the top-K sparse softmax gating Gaussian MoE of order k\u2217 with the conditional density function\ngG\u2217(Y |X) = k\u2217\u2211\ni=1\nSoftmax(TopK((\u03b2\u22171i) \u22a4X,K;\u03b2\u22170i)) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ), (1)\nwhere G\u2217 := \u2211k\u2217\ni=1 exp(\u03b2 \u2217 0i)\u03b4(\u03b2\u22171i,a\u2217i ,b\u2217i ,\u03c3\u2217i ) is a true but unknown mixing measure of order k\u2217 (i.e., a\nlinear combination of k\u2217 Dirac measures) associated with true parameters (\u03b2\u22170i, \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) for i \u2208 {1, 2, . . . , k\u2217} and Dirac measure \u03b4. Additionally, we denote f(\u00b7|\u00b5, \u03c3) as an univariate Gaussian density function with mean \u00b5 and variance \u03c3 while we define for any vectors v = (v1, . . . , vk\u2217) and u = (u1, . . . , uk\u2217) in R k\u2217 that Softmax(vi) := exp(vi)/ \u2211k\u2217 j=1 exp(vj) and\nTopK(vi,K;ui) :=\n{ vi + ui if vi is in the top K elements of v;\n\u2212\u221e otherwise.\nMore specifically, before applying the softmax function in equation (1), we keep only the top K values of (\u03b2\u221711) \u22a4X, . . . , (\u03b2\u22171k\u2217) \u22a4X and their corresponding bias vectors among \u03b2\u221701, . . . , \u03b2 \u2217 0k\u2217\n, while we set the rest to \u2212\u221e to make their gating values vanish. An instance of the top-K sparse softmax gating function is given in equation (3). Furthermore, linear expert functions considered in equation (1) are only for the simplicity of presentation. With similar proof techniques, the results in this work can be extended to general settings of the expert functions, including deep neural networks. In order to obtain an estimate of G\u2217, we use the following maximum likelihood estimation (MLE):\nG\u0302n \u2208 argmax G\n1\nn\nn\u2211\ni=1\nlog(gG(Yi|Xi)). (2)\nUnder the exact-specified settings, i.e., when the true number of expert k\u2217 is known, the maximum in equation (2) is subject to the set of all mixing measures of order k\u2217 denoted by Ek\u2217(\u2126) := {G =\u2211k\u2217\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) : (\u03b20i, \u03b21i, ai, bi, \u03c3i) \u2208 \u2126}. On the other hand, under the over-specified settings, i.e., when k\u2217 is unknown and the true model is over-specified by a Gaussian mixture of k experts where k > k\u2217, the maximum is subject to the set of all mixing measures of order at most k,\ni.e., Ok(\u2126) := {G = \u2211k\u2032 i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) : 1 \u2264 k\u2032 \u2264 k, (\u03b20i, \u03b21i, ai, bi, \u03c3i) \u2208 \u2126}.\nUniversal assumptions. For the sake of theory, we assume that (\u03b2\u22170i, \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) \u2208 \u2126 and X \u2208 X where \u2126 is a compact subset of R \u00d7 Rd \u00d7 Rd \u00d7 R \u00d7 R+, and X is a bounded subset of Rd. Notably, if we translate \u03b2\u22171i and \u03b2 \u2217 0i to \u03b2 \u2217 1i + t1 and \u03b2 \u2217 0i + t2, respectively, then the softmax function in equation (1) will remain unchanged. This indicates that the gating parameters \u03b2\u22171i, \u03b2 \u2217 0i are only identifiable up to some translation. We overcome this issue by simply setting \u03b2\u22171k\u2217 = 0d and \u03b2 \u2217 0k\u2217\n= 0, which will be elaborated in further detail in Appendix C. Moreover, we let (a\u22171, b \u2217 1, \u03c3 \u2217 1), . . . , (a \u2217 k\u2217 , b\u2217k\u2217 , \u03c3 \u2217 k\u2217 ) be pairwise distinct so that the expert networks are different from each other. Finally, to guarantee the dependence on the input of top-K sparse softmax gating function, we also assume that at least one among the true weight parameters \u03b2\u221711, . . . , \u03b2 \u2217 1k\u2217 is non-zero.\nChallenge discussion. In our convergence analysis, there are two main challenges attributed to the structure of the top-K sparse softmax gating function. (1) First, since this gating function divides the input space into multiple regions and each of which has different convergence behavior of the density estimation, it is challenging to derive the convergence rate of g\nG\u0302n to its true counterpart\ngG\u2217 . To this end, we need to guarantee that the values of the TopK function in gG\u2217 align with those in gG\u0302n under some conditions. For instance, under the exact-specified settings, given that components of the MLE G\u0302n, denoted by (\u03b2\u0302 n 1i, a\u0302 n i , b\u0302 n i , \u03c3\u0302 n i ), converge to those of the true mixing measure G\u2217, denoted by (\u03b2\u22171i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ), we have to demonstrate that TopK((\u03b2 \u2217 1i) \u22a4X,K;\u03b2\u22170i) = (\u03b2 \u2217 1i)\n\u22a4X + \u03b2\u22170i occurs if and only if TopK((\u03b2\u0302n1i) \u22a4X,K; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i)\n\u22a4X + \u03b2\u0302n0i when the sample size n is sufficiently large. (2) Second, under the over-specified settings, each component of G\u2217 could be fitted by at least two components of G\u0302n. Therefore, choosing only the best K experts in the formulation of gG\u0302n(Y |X) is insufficient to estimate the true density gG\u2217(Y |X). As a result, it is of great importance to figure out the minimum number of experts selected in the top-K sparse softmax gating function necessary for ensuring the convergence of density estimation.\nContributions. In this work, we provide rigorous statistical guarantees for density estimation and parameter estimation in the top-K sparse softmax gating Gaussian MoE under both the exactspecified and over-specified settings. Our contributions are two-fold and can be summarized as follows:\n1. Exact-specified settings: When the true number of experts k\u2217 is known, we partition the input space X into (k\u2217 K ) regions, which correspond to (k\u2217 K ) different top K experts in the formulation of gG\u2217(Y |X). In each region, we show that for sufficiently large n, TopK((\u03b2\u22171i)\u22a4X,K;\u03b2\u22170i) = (\u03b2\u22171i)\u22a4X+ \u03b2\u22170i occurs if and only if TopK((\u03b2\u0302 n 1i) \u22a4X,K; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i)\n\u22a4X + \u03b2\u0302n0i holds true for almost surely X. Based on this result, we point out that the density estimation gG\u0302n converges to the true density gG\u2217 under the Hellinger distance h at the parametric rate, that is, EX [h(gG\u0302n(\u00b7|X), gG\u2217 (\u00b7|X))] = O\u0303(n \u22121/2). Then, we propose a novel Voronoi metric D1 defined in equation (5) to characterize the parameter estimation rates. By establishing the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] & D1(G,G\u2217) for any G \u2208 Ek\u2217(\u0398), we arrive at another bound D1(G\u0302n, G\u2217) = O\u0303(n\u22121/2), which indicates that the rates for estimating true parameters exp(\u03b2\u22170i), \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i are of the optimal order O\u0303(n\u22121/2).\n2. Over-specified settings: When k\u2217 is unknown and the true model is over-specified by a Gaussian of k experts where k > k\u2217, we demonstrate that in order to ensure the convergence of the\ndensity estimation g G\u0302n to the true density gG\u2217 , the number of experts chosen from gG\u0302n , denoted by K, is necessarily greater than the total cardinality of a certain number of Voronoi cells generated by the support of G\u2217. Furthermore, in a certain input region, we figure out a relation between the top-K sparse softmax gating functions of g\nG\u0302n and gG\u2217 . In particular, for sufficiently large n, we\nhave TopK((\u03b2\u22171j) \u22a4X,K;\u03b2\u22170j) = (\u03b2 \u2217 1j) \u22a4X+\u03b2\u22170j if and only if TopK((\u03b2\u0302 n 1i) \u22a4X,K ; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i) \u22a4X+ \u03b2\u0302n0i for almost surely X and i \u2208 Cj , where Cj is a Voronoi cell defined in equation (4). Given these results, the density estimation rate is shown to remain parametric on the sample size. Additionally, by designing a novel Voronoi loss function D2 in equation (8) to capture an interaction between parameters of the softmax gating and expert functions, we prove that the MLE G\u0302n converges to the true mixing measure G\u2217 at a rate of O\u0303(n\u22121/2). Then, it follows from the formulation of D2 that the estimation rates for parameters \u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j depend on the solvability of a system of polynomial equations arising from the previous interaction, and turn out to be significantly slow.\nOrganization. The rest of our paper is organized as follows. In Section 2, we establish the convergence rates of density estimation and parameter estimation for the top-K sparse softmax gating Gaussian MoE under the exact-specified settings, while the results for such convergence analysis under the over-specified settings are presented in Section 3. Subsequently, we provide a proof sketch for these results in Section 4 before concluding the paper and discussing some future directions in Section 5. Finally, full proofs and additional results are deferred to the supplementary material.\nNotations. First, for any natural numbers m,n \u2208 N such that m \u2265 n, we denote [n] := {1, 2, . . . , n} and (m n ) := m!n!(m\u2212n!) . Next, for any vector u \u2208 Rd, we let |u| := \u2211d i=1 ui and denote \u2016u\u2016 as its 2-norm value. Meanwhile, we define |A| as the cardinality of some set A. Then, for any two probability densities f1 and f2 dominated by the Lebesgue measure \u03bd, we denote V (f1, f2) := 1 2 \u222b |f1 \u2212 f2|d\u03bd as their Total Variation distance, whereas h2(f1, f2) := 1 2 \u222b ( \u221a f1 \u2212 \u221a f2)\n2d\u03bd represents the squared Hellinger distance. Finally, for any two sequences of positive real numbers (an) and (bn), the notations an = O(bn) and an . bn both stand for an \u2264 Cbn for all n \u2208 N for some constant C > 0, while the notation an \u2264 O\u0303(bn) indicates that the previous inequality may depend on some logarithmic term."
        },
        {
            "heading": "2 Exact-specified Settings",
            "text": "In this section, we characterize respectively the convergence rates of density estimation and parameter estimation in the top-K sparse softmax gating Gaussian MoE under the exact-specified settings, namely when the true number of experts k\u2217 is known.\nTo begin with, let us introduce some essential notations and key results to deal with the top-K sparse softmax gating function. It can be seen from equation (1) that whether (a\u2217i )\n\u22a4X + b\u2217i belongs to the top K experts in the true density gG\u2217(Y |X) or not is determined based on the ranking of (\u03b2\u22171i)\n\u22a4X + \u03b2\u22170i, which depends on the input X. Additionally, it is also worth noting that there are a total of q := (k\u2217 K ) different potential choices of top K experts. Thus, we first partition the input space X into q regions in order to specify the top K experts and obtain an according representation of gG\u2217(Y |X) in each region. In particular, for each \u2113 \u2208 [q], we denote {\u21131, \u21132, . . . , \u2113K} as an Kelement subset of [k\u2217] and {\u2113K+1, . . . , \u2113k\u2217} := [k\u2217] \\ {\u21131, \u21132, . . . , \u2113K}. Then, the \u2113-th region of X is\ndefined as\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171i)\u22a4x \u2265 (\u03b2\u22171i\u2032)\u22a4x,\u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nfor any \u2113 \u2208 [q]. From this definition, we observe that if X \u2208 X \u2217\u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = [K], then TopK((\u03b2\u22171i)\u22a4X,K;\u03b2\u22170i) = (\u03b2\u22171i)\u22a4X + \u03b2\u22170i for any i \u2208 [K]. As a result, (a\u22171) \u22a4X+b\u22171, . . . , (a \u2217 K)\n\u22a4X+b\u2217K become the top K experts, and the true conditional density gG\u2217(Y |X) is reduced to:\ngG\u2217(Y |X) = K\u2211\ni=1\nexp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)\u2211K\nj=1 exp((\u03b2 \u2217 1j) \u22a4X + \u03b2\u22170j) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ). (3)\nSubsequently, we assume that the MLE G\u0302n takes the form G\u0302n := \u2211k\u2217\ni=1 exp(\u03b2\u0302 n 0i)\u03b4(\u03b2\u0302n1i,a\u0302ni ,\u0302bni ,\u03c3\u0302ni ) ,\nwhere the MLE\u2019s component \u03c9\u0302ni := (\u03b2\u0302 n 0i, \u03b2\u0302 n 1i, a\u0302 n i , b\u0302 n i , \u03c3\u0302 n i ) converges to the true component \u03c9 \u2217 i := (\u03b2\u22170i, \u03b2 \u2217 1i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) for any i \u2208 [k\u2217]. Since \u03c9\u0302ni becomes increasingly close to \u03c9\u2217i for sufficiently large n, we figure out in the following lemma a relation between the top K experts in gG\u2217(Y |X) and those in gG\u0302n(Y |X):\nLemma 1. For any i \u2208 [k\u2217], let \u03b21i, \u03b2\u22171i \u2208 Rd such that \u2016\u03b21i \u2212 \u03b2\u22171i\u2016 \u2264 \u03b7i for some sufficiently small \u03b7i > 0. Then, for any \u2113 \u2208 [q], unless the set X \u2217\u2113 has measure zero, we obtain that X \u2217\u2113 = X\u2113 where\nX\u2113 := {x \u2208 X : (\u03b21i)\u22a4x \u2265 (\u03b21i\u2032)\u22a4x,\u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}}.\nProof of Lemma 1 is in Appendix A.3. This lemma indicates that for sufficiently large sample size n, TopK((\u03b2\u22171i) \u22a4X,K;\u03b2\u22170i) = (\u03b2 \u2217 1i) \u22a4X+\u03b2\u22170i is equivalent to TopK((\u03b2\u0302 n 1i) \u22a4X,K; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i)\n\u22a4X+ \u03b2\u0302n0i, for almost surely X and i \u2208 [k\u2217]. In other words, (a\u2217i )\u22a4X + b\u2217i is one of the top K experts in gG\u2217(Y |X) if and only if (a\u0302ni )\n\u22a4X + b\u0302ni belongs to the top K experts in gG\u0302n(Y |X). Based on this property, we provide in the following theorem the rate for estimating the true conditional density function gG\u2217 :\nTheorem 1 (Density estimation rate). Given the MLE G\u0302n defined in equation (2), the convergence rate of the conditional density estimation gG\u0302n under the exact-specified settings is illustrated by the following inequality:\nP ( EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] > C \u221a log(n)/n ) . n\u2212c,\nwhere C > 0 and c > 0 are some universal constants that depend on \u2126 and K.\nProof of Theorem 1 can be found in Appendix A.1. It follows from the result of Theorem 1 that the conditional density estimation gG\u0302n converges to its true counterpart gG\u2217 under the Hellinger distance at the parametric rate of order O\u0303(n\u22121/2). This rate plays a critical role in establishing the convergence rates of parameter estimation. In particular, if we are able to derive the following lower bound: EX [h(gG(\u00b7|X), gG\u2217 (\u00b7|X))] & D1(G,G\u2217) for any mixing measure G \u2208 Ek\u2217(\u2126), where the metric D1 will be defined in equation (5), we will achieve our desired parameter estimation rates. Before going into further details, let us introduce the formulation of Voronoi metric D1 that we use for our convergence analysis under the exact-specified settings.\nVoronoi metric. Given an arbitrary mixing measure G with k\u2032 components, we distribute those components to the following Voronoi cells [22] generated by the components of G\u2217:\nCj \u2261 Cj(G) := {i \u2208 [k\u2032] : \u2016\u03b8i \u2212 \u03b8\u2217j\u2016 \u2264 \u2016\u03b8i \u2212 \u03b8\u2217j\u2032\u2016,\u2200j\u2032 6= j}, (4)\nwhere \u03b8i := (\u03b21i, ai, bi, \u03c3i) and \u03b8 \u2217 j := (\u03b2 \u2217 1j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) for any j \u2208 [k\u2217]. Recall that under the exactspecified settings, the MLE G\u0302n is assumed to be within the set of all mixing measures with exactly k\u2217 components. Therefore, we consider k\u2032 = k\u2217 in this case and observe that each Voronoi cell Cj contains only one element. Then, the Voronoi metric D1 is defined as follows:\nD1(G,G\u2217) := max {\u21131,...,\u2113K}\nK\u2211\nj=1\n[ \u2211\ni\u2208C\u2113j\nexp(\u03b20i) ( \u2016\u2206\u03b21i\u2113j\u2016+ \u2016\u2206ai\u2113j\u2016+ \u2016\u2206bi\u2113j\u2016+ \u2016\u2206\u03c3i\u2113j\u2016 )\n+ \u2223\u2223\u2223 \u2211\ni\u2208C\u2113j\nexp(\u03b20i)\u2212 exp(\u03b2\u22170\u2113j ) \u2223\u2223\u2223 ] , (5)\nwhere the above maximum is subject to all K-element subsets {\u21131, . . . , \u2113K} of [k\u2217]. Additionally, we denote \u2206\u03b21i\u2113i := \u03b21i \u2212 \u03b2\u22171\u2113j , \u2206ai\u2113j := ai \u2212 a \u2217 \u2113j , \u2206bi\u2113j := bi \u2212 b\u2217\u2113j and \u2206\u03c3i\u2113j := \u03c3i \u2212 \u03c3 \u2217 \u2113j\n. Now, we are ready to characterize the convergence rates of parameter estimation in the top-K sparse softmax gating Gaussian MoE.\nTheorem 2 (Parameter estimation rate). Under the exact-specified settings, the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] & D1(G,G\u2217) holds for any mixing measure G \u2208 Ek\u2217(\u2126). Consequently, we can find a universal constant C1 > 0 depending only on G\u2217, \u2126 and K such that\nP ( D1(G\u0302n, G\u2217) > C1 \u221a log(n)/n ) . n\u2212c1 ,\nwhere c1 > 0 is a universal constant that depends only on \u2126.\nProof of Theorem 2 is in Appendix A.2. This theorem reveals that the Voronoi metric D1 between the MLE G\u0302n and the true mixing measure G\u2217, i.e. D1(G\u0302n, G\u2217), vanishes at the parametric rate of order O\u0303(n\u22121/2). As a result, the rates for estimating ground-truth parameters exp(\u03b2\u22170i), \u03b2\u22171i, a\u2217i , b\u2217i , \u03c3\u2217i are optimal at O\u0303(n\u22121/2) for any i \u2208 [k\u2217]."
        },
        {
            "heading": "3 Over-specified Settings",
            "text": "In this section, we continue to carry out the same convergence analysis for the top-K sparse softmax gating Gaussian MoE as in Section 2 but under the over-specificed settings, that is, when the true number of experts k\u2217 becomes unknown.\nRecall that under the over-specified settings, we look for the MLE G\u0302n within Ok(\u2126), i.e. the set of all mixing measures with at most k components, where k > k\u2217. Thus, there could be some components (\u03b2\u22171i, a \u2217 i , b \u2217 i , \u03c3 \u2217 i ) of the true mixing measure G\u2217 approximated by at least two fitted components (\u03b2\u0302n1i, a\u0302 n i , b\u0302 n i , \u03c3\u0302 n i ) of the MLE G\u0302n. Moreover, since the true density gG\u2217(Y |X) is associated with top K experts and each of which corresponds to one component of G\u2217, we need to select more than K experts in the formulation of density estimation. In particular, for any mixing measure\nG = \u2211k\u2032\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ok(\u2126), let us consider a new formulation of conditional density used for estimating the true density under the over-specified settings as follows:\ngG(Y |X) := k\u2032\u2211\ni=1\nSoftmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) \u00b7 f(Y |(ai)\u22a4X + bi, \u03c3i).\nHere, gG(Y |X) is equipped with top-K sparse softmax gating, where 1 \u2264 K \u2264 k\u2032 is fixed and might be different from K. Additionally, the definition of MLE in equation (2) also changes accordingly to this new density function. Then, we demonstrate in Proposition 1 an interesting phenomenon that the density estimation gG\u0302n cannot converge to gG\u2217 under the Hellinger distance when K <\nmax{\u21131,...,\u2113K} \u2211K j=1 |C\u2113j |. Proposition 1. If 1 \u2264 K < max{\u21131,...,\u2113K} \u2211K j=1 |C\u2113j |, then the following inequality holds true:\ninf G\u2208Ok(\u2126)\nEX [h(gG(\u00b7|X), gG\u2217(\u00b7|X))] > 0.\nProof of Proposition 1 is deferred to Appendix B.3. Following from the result of Proposition 1, we will consider only the regime when max{\u21131,...,\u2113K} \u2211K j=1 |C\u2113j | \u2264 K \u2264 k in the rest of this section. As the number of experts chosen in the density estimation changes from K to K, it is necessary to partition the input space X into q := ( k K ) regions. More specifically, for any \u2113 \u2208 q, we denote {\u21131, \u21132, . . . , \u2113K} as an K-element subset of [k] and {\u2113K+1, . . . , \u2113k} := [k] \\ {\u21131, \u21132, . . . , \u2113K}. Then, we define the \u2113-th region of X as follows:\nX \u2113 := {x \u2208 X : (\u03b21i)\u22a4x \u2265 (\u03b21i\u2032)\u22a4x,\u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}}.\nInspired by the result of Lemma 1, we continue to present in Lemma 2 a relation between the top K experts in gG\u2217(Y |X) and the top K experts in gG\u0302n(Y |X). Lemma 2. For any j \u2208 [k\u2217] and i \u2208 Cj, let \u03b21i, \u03b2\u22171j \u2208 Rd that satisfy \u2016\u03b21i \u2212 \u03b2\u22171j\u2016 \u2264 \u03b7j for some sufficiently small \u03b7j > 0. Additionally, for max{\u21131,...,\u2113K} \u2211K j=1 |C\u2113j | \u2264 K \u2264 k, we assume that there exist \u2113 \u2208 [q] and \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = C\u21131 \u222a . . . \u222a C\u2113K . Then, if the set X \u2217\u2113 does not have measure zero, we achieve that X \u2217\u2113 = X \u2113. Proof of Lemma 2 is in Appendix B.4. Different from Lemma 1, we need to impose on Lemma 2 an assumption that there exist indices \u2113 \u2208 [q] and \u2113 \u2208 [q] that satisfy {\u21131, . . . , \u2113K} = C\u21131 \u222a . . . \u222a C\u2113K . This assumption means that each component (\u03b2\u0302n1i, a\u0302ni , b\u0302ni , \u03c3\u0302ni ) corresponding to the top K experts in g\nG\u0302n must converge to some component which corresponds to the top K experts in gG\u2217 .\nConsequently, for X \u2208 X \u2217\u2113 , if TopK((\u03b2\u22171j)\u22a4X,K;\u03b2\u22170j) = (\u03b2\u22171j)\u22a4X + \u03b2\u22170j holds true, then we achieve that TopK((\u03b2\u0302n1i) \u22a4X,K; \u03b2\u0302n0i) = (\u03b2\u0302 n 1i)\n\u22a4X + \u03b2\u0302n0i and vice versa, for any j \u2208 [k\u2217] and i \u2208 Cj. Given the result of Lemma 2, we are now able to derive the convergence rate of the density estimation gG\u0302n to its true counterpart gG\u2217 under the over-specified settings in Theorem 3.\nTheorem 3 (Density estimation rate). With the MLE G\u0302n, the conditional density estimation g G\u0302n (Y |X) admits the following convergence rate under the over-specified settings:\nP ( EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] > C \u2032\u221alog(n)/n ) . n\u2212c \u2032 ,\nwhere C \u2032 > 0 and c\u2032 > 0 are some universal constants that depend on \u2126 and K.\nProof of Theorem 3 is in Appendix B.1. Although there is a modification in the number of chosen experts in g\nG\u0302n , Theorem 3 verifies that the convergence rate of this density estimation to gG\u2217 under\nthe over-specified settings remains the same as that under the exact-specified settings, which is of order O\u0303(n\u22121/2). Subsequently, we will leverage this result for our convergence analysis of parameter estimation under the over-specified settings, which requires us to design a new Voronoi metric.\nVoronoi metric. Regarding the top-K sparse softmax gating, challenges comes not only from the TopK function but also from the Softmax function. In particular, there is a complex interaction between the numerators of softmax weights and the expert functions in the Gaussian density. Moreover, Gaussian density parameters also interacts with each other. These two interactions are respectively illustrated by the following partial differential equations (PDE):\n\u22022s(X,Y )\n\u2202\u03b21\u2202b =\n\u2202s(X,Y )\n\u2202a ;\n\u2202s(X,Y )\n\u2202\u03c3 =\n1 2 \u00b7 \u2202\n2s(X,Y )\n\u2202b2 , (6)\nwhere we denote s(X,Y ) := exp(\u03b2\u22a41 X) \u00b7 f(Y |a\u22a4X + b, \u03c3). To cope with those interactions, we need to take into account the solvability of a system of polynomial equations. More specifically, for any m \u2265 2, let r(m) be the smallest natural number r such that the following system of polynomial equations does not have any non-trivial solution for the unknown variables {z1j , z2j , z3j , z4j , z5j}mi=1:\nm\u2211\ni=1\n\u2211\n(\u03b11,\u03b12,\u03b13,\u03b14)\u2208J\u03b71,\u03b72\nz25i z \u03b11 1i z \u03b12 2i z \u03b13 3i z \u03b14 4i\n\u03b11! \u03b12! \u03b13! \u03b14! = 0, (7)\nfor all (\u03b71, \u03b72) \u2208 Nd \u00d7N such that 0 \u2264 |\u03b71| \u2264 r, 0 \u2264 \u03b72 \u2264 r\u2212 |\u03b71| and |\u03b71|+ \u03b72 \u2265 1. Here, a solution to the above system is called non-trivial if all of variables z5i are different from zero, whereas at least one among variables z3i is non-zero. In addition, the set J\u03b71,\u03b72 is defined as:\nJ\u03b71,\u03b72 := {(\u03b11, \u03b12, \u03b13, \u03b14) \u2208 Nd \u00d7 Nd \u00d7 N\u00d7 N : \u03b11 + \u03b12 = \u03b71, |\u03b12|+ \u03b13 + \u03b14 = \u03b72}.\nIn a general case when d \u2265 1 and m \u2265 2, it is non-trivial to figure out the exact value of r(m) [30]. However, when m is small, [24] show that r(2) = 4 and r(3) = 6. Additionally, it is worth noting that r(m) is a monotonically increasing function of m. Based on these observations, they also conjectured that r(m) = 2m for any m \u2265 2 and d \u2265 1.\nGiven the above results, we design a Voronoi metric D2 to capture the convergence rates of parameter estimation in the top-K sparse softmax gating Gaussian MoE under the over-specified settings as\nD2(G,G\u2217) := max {\u21131,...,\u2113K}\n{ \u2211\nj\u2208[K], |C\u2113j |=1\n\u2211\ni\u2208C\u2113j\nexp(\u03b20i) [ \u2016\u2206\u03b21i\u2113j\u2016+ \u2016\u2206ai\u2113j\u2016+ |\u2206bi\u2113j |+ |\u2206\u03c3i\u2113j | ]\n+ \u2211\nj\u2208[K], |C\u2113j |>1\n\u2211\ni\u2208C\u2113j\nexp(\u03b20i) [ \u2016\u2206\u03b21i\u2113j\u2016 r\u0304(|C\u2113j |) + \u2016\u2206ai\u2113j\u2016 r\u0304(|C\u2113j |) 2 + |\u2206bi\u2113j | r\u0304(|C\u2113j |) + |\u2206\u03c3i\u2113j | r\u0304(|C\u2113j |) 2 ]\n+\nK\u2211\nj=1\n\u2223\u2223\u2223 \u2211\ni\u2208C\u2113j\nexp(\u03b20i)\u2212 exp(\u03b2\u22170\u2113j ) \u2223\u2223\u2223 } , (8)\nfor any mixing measure G \u2208 Ok(\u2126). Here, the above maximum is subject to all K-element subsets {\u21131, . . . , \u2113K} of [k\u2217]. Then, we point out in the following theorem that those parameter estimation rates vary with the values of function r(\u00b7)\nTheorem 4 (Parameter estimation rate). Under the over-specified settings, the Hellinger lower bound EX [h(gG(\u00b7|X), gG\u2217 (\u00b7|X))] & D2(G,G\u2217) holds for any mixing measure G \u2208 Ok(\u2126). Consequently, we can find a universal constant C2 > 0 depending only on G\u2217, \u2126 and K such that\nP ( D2(G\u0302n, G\u2217) > C2 \u221a log(n)/n ) . n\u2212c2 ,\nwhere c2 > 0 is a universal constant that depends only on \u2126.\nProof of Theorem 4 is in Appendix B.2. A few remarks on this theorem are in order.\n(i) Under the over-specified settings, the MLE G\u0302n converges to the true mixing measure G\u2217 at the rate of order O\u0303(n\u22121/2) under the Voronoi metric D2. Let us denote Cnj = Cj(G\u0302n), then this result indicates that the estimation rates for ground-truth parameters exp(\u03b2\u22170j), \u03b2 \u2217 1j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j fitted by only one component, i.e. |Cnj | = 1, remain the same at O\u0303(n\u22121/2) as those in the exact-specified settings.\n(ii) However, for ground-truth parameters which are approximated by at least two components, i.e. |Cnj | > 1, the rates for estimating them depend on their corresponding Voronoi cells, and become significantly low when the cardinality of those Voronoi cells increase. In particular, both \u03b2\u22171j and b \u2217 j admit the estimation rate of order O\u0303(n\u22121/2r(|Cnj |)). Meanwhile, the convergence rates of estimating a\u2217j and \u03c3 \u2217 j are of the same order O(n\u22121/r(|C n j |)). For instance, assume that (\u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) has two fitted components, which means that |Cnj | = 2 and therefore, r(|Cnj |) = 6. Then, the estimation rates for \u03b2\u22171j , b \u2217 j and a \u2217 j , \u03c3 \u2217 j are O\u0303(n\u22121/8) and O\u0303(n\u22121/4), respectively.\n(iii) Suppose that the MLE G\u0302n consists of k\u0302n components. Since every Voronoi cell among Cn1 , . . . , Cnk\u2217 contains at least one element and the total number of elements of those cells equal to k\u0302n, then the maximum cardinality of a Voronoi cell turns out to be k\u0302n \u2212 k\u2217 + 1. This maximum value is attained when there is exactly one ground-truth component (\u03b2\u22171j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ) fitted by more than one component. An instance for this scenario is when |Cn1 | = k\u0302n \u2212 k\u2217 + 1 and |Cn2 | = . . . = |Cnk\u2217 | = 1. Under this setting, the first ground-truth parameters \u03b2\u221711, b \u2217 1 and a \u2217 1, \u03c3 \u2217 1 achieve their worst possible estimation rates of order O\u0303(n\u22121/2r(k\u0302n\u2212k\u2217+1)) and O\u0303(n\u22121/r(k\u0302n\u2212k\u2217+1)), respectively. For example, if k\u0302n = 5 and k\u2217 = 3, then the rates for estimating \u03b2\u221711, b \u2217 1 are O\u0303(n\u22121/12), while those for a\u22171, \u03c3\u22171 are O\u0303(n\u22121/6)."
        },
        {
            "heading": "4 Proof Sketch",
            "text": "In this section, we present a proof sketch for both Theorem 2 and Theorem 4 with a convention that the notation D used in this sketch indicates either the metric D1 or D2. Note that the Hellinger distance h is lower bounded by the Total Variation distance V , it is sufficient to demonstrate that EX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] & D(G,G\u2217). For that purpose, we divide this bound into two parts.\nPart 1: We aim to show that lim\u03b5\u21920 infG:D(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))]/D(G,G\u2217) > 0. Assume by contrary that there exists a sequence Gn = \u2211kn i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ok(\u2126) that satisfies D(Gn, G\u2217) \u2192 0 and EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))]/D(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. As a result, for any j \u2208 [k\u2217], we have \u2211 i\u2208Cj exp(\u03b2 n 0i) \u2192 exp(\u03b2\u22170j) and (\u03b2n1i, ani , bni , \u03c3ni ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) for all i \u2208 Cj . WLOG, we may assume that the maximum value of D is attained at {\u21131, . . . , \u2113K} = {1, . . . ,K}. Now, we separate the rest of this part into three steps as follows:\nStep 1: Taylor expansion. Let us consider X \u2208 X \u2217\u2113 , which means that TopK((\u03b2\u22171j)\u22a4X + \u03b2\u22170j) = (\u03b2 \u2217 1j)\n\u22a4X + \u03b2\u22170j for any j \u2208 [K]. Then, it follows from either Lemma 1 or Lemma 2 that TopK((\u03b2n1i) \u22a4X + \u03b2n0i) = (\u03b2 n 1i)\n\u22a4X + \u03b2n0i for any i \u2208 Cj. Therefore, we obtain explicit representations of gGn(Y |X) and gG\u2217(Y |X) as in equation (3). Given these representations, we would like to decompose the term Hn := [\u2211K j=1 exp((\u03b2 \u2217 1j) \u22a4X + \u03b2\u22170j) ] \u00b7 [gGn(Y |X) \u2212 gG\u2217(Y |X)] into a linear combination of elements which belongs to a linearly independent set. First, we write\nHn = K\u2211\nj=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ s(X,Y |\u03b2n1i, ani , bni , \u03c3ni )\u2212 s(X,Y |\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j )\u2212 t(X,Y |\u03b2n1i)\n+ t(X,Y |\u03b2\u22171j) ] + K\u2211\nj=1\n(\u2211\ni\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j)\n)[ s(X,Y |\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j )\u2212 t(X,Y |\u03b2\u22171j) ] ,\nwhere two functions s and t are defined as s(X,Y |\u03b21, a, b, \u03c3) := exp(\u03b2\u22a41 X)f(Y |a\u22a4X + b, \u03c3) and t(X,Y |\u03b21) := exp(\u03b2\u22a41 X)gGn(Y |X). Next, we apply the Taylor expansions of orders r1j and r2j to functions s(X,Y |\u03b2n1i, ani , bni , \u03c3ni ) and t(X,Y |\u03b2n1i), respectively. More specifically, for j \u2208 [K] such that |Cj | = 1, if we choose r1j = r2j = 1, then the derivative terms in the previous expansions are linearly independent. However, if |Cj | > 1, we need higher Taylor orders to capture the convergence behavior of fitted components, which will be specified in Step 3. Then, by denoting the expert function as h1(X, a, b) = a\n\u22a4X+ b, we can rewrite Hn as a linear combination of linearly independent derivative terms:\nHn = K\u2211\nj=1\n\u2211\n0\u2264|\u03b71|+\u03b72\u22642r1j\nSnj,\u03b71,\u03b72 \u00b7X \u03b71 exp((\u03b2\u22171i)\n\u22a4X) \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j )\n+\nK\u2211\nj=1\n\u2211\n0\u2264|\u03b3|\u2264r2j T nj,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) +R(X,Y ),\nwhere \u03b71, \u03b3 \u2208 Rd and \u03b72 \u2208 R while R(X,Y ) stands for a Taylor remainder such that the ratio R(X,Y )/D(Gn, G\u2217) approaches zero as n goes to infinity.\nStep 2: Non-vanishing coefficients. Now, we show that at least one among ratios Snj,\u03b71,\u03b72/D(Gn, G\u2217) and T nj,\u03b3/D(Gn, G\u2217) will not converge to zero. Assume by contrary that all of them vanish as n goes to infinity. It is worth emphasizing that due to the PDEs in equation (6), there are many linearly dependent derivative terms arising from the Taylor expansions of high orders. Thus, we have to take the summation of coefficients of these linearly dependent terms and end up with the formulations of Snj,\u03b71,\u03b72 and T n j,\u03b3. When these coefficients vanish, we arrive at the system of polynomial equations in\nequation (7). Based on the derivation of this system, it must admit at least one non-trivial solution. Therefore, if we set r1j = r(|Cj |) and r2j = 2 for j \u2208 [K] : |Cj | > 1, then it follows from the definition of function r that the above system does not have any non-trivial solutions, which is a contradiction.\nStep 3: Fatou\u2019s lemma. From the hypothesis, the term EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))]/D(Gn, G\u2217) vanish as n \u2192 \u221e. By applying the Fatou\u2019s lemma, we get that [gGn(Y |X)\u2212gG\u2217(Y |X)]/D(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). This indicates that Hn/D(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Since Hn is a linear combination of linearly independent terms, all the ratios of associated coefficients and D(Gn, G\u2217) must converge to zero, which contradicts the result in Step 2. Hence, we finish the proof of part 1.\nPart 2: In this part, we prove that infG:D(G,G\u2217)>\u03b5 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))]/D(G,G\u2217) > 0. Assume that this claim does not hold, then there exists a mixing measure G\u2032 that satisfies gG\u2032(Y |X) = gG\u2217(Y |X) for almost surely (X,Y ). Since the top-K sparse softmax gating Gaussian MoE is identifiable (see Proposition 2 in Appendix C), we obtain that G\u2032 \u2261 G, which implies that D(G\u2032, G\u2217) = 0. This contradicts the hypothesis D(G\u2032, G\u2217) > \u03b5 > 0. Hence, the proof is completed."
        },
        {
            "heading": "5 Conclusion and Future Directions",
            "text": "In this paper, we provide a convergence analysis for density estimation and parameter estimation in the top-K sparse softmax gating Gaussian MoE. To overcome the complex structure of top-K sparse softmax gating function, we first establish a connection between the outputs of TopK function associated with the density estimation and the true density in each input region partitioned by the gating function, and then construct novel Voronoi loss functions among parameters to capture different behaviors of these input regions. Under the exact-specified settings, we show that the rates for estimating the true density and true parameters are both parametric on the sample size. Meanwhile, to ensure the convergence of density estimation under the over-specified settings, we show that the minimum number of experts chosen in the top-K sparse softmax gating function must be at least the total cardinality of a certain number of Voronoi cells generated by the true parameters. Although the density estimation rate remains parametric, the parameter estimation rates witness a sharp drop because of an interaction between the softmax gating and expert functions. Based on the results of this paper, there are a few potential future directions.\nFirstly, a question of how to estimate the true number of experts k\u2217 and the number of experts selected in the top-K sparse softmax gating function K naturally arises from this work. Since the parameter estimation rates under the over-specified settings decrease proportionately to the number of fitted experts k, one possible approach to estimate k\u2217 is to reduce the value of k until these rates reach the optimal order O\u0303(n\u22121/2). That reduction can be done by merging close estimated parameters within their convergence rates to the true parameters [11] or by penalizing the log-likelihood function of the top-K sparse softmax gating Gaussian mixture of experts using the differences among the parameters [23]. As a result, the number of experts chosen in the density estimation K also decreases accordingly and approximates the value of K.\nSecondly, the theoretical results established in the paper are under the well-specified assumption, namely, the data are assumed to be generated from a top-K sparse softmax gating Gaussian mixture\nof experts. However, this assumption can be violated in real-world data settings when the data are not necessarily generated from that model. Under those misspecified settings, the MLE converges to the mixing measure G \u2208 minG\u2208Ok(\u2126) KL(P (Y |X)||gG(Y |X)) where P (Y |X) is the true conditional distribution of Y given X and KL stands for the Kullback-Leibler divergence. As the space Ok(\u2126) is non-convex, the existence of G is not unique. Furthermore, the current analysis of the MLE under the misspecified settings of statistical models is mostly under the convex settings of the function space [34], which is not applicable to the current non-convex misspecified settings of the top-K sparse softmax gating Gaussian mixture of experts. Therefore, it is necessary to develop a new analysis as well as a new metric to establish the convergence rate of the MLE to the set of G.\nFinally, the paper only studies the convergence rate of the MLE to the true parameters. However, since the log-likelihood function of the top-K sparse softmax gating Gaussian mixture of experts is highly non-concave, the EM algorithm is used to approximate the MLE. While the statistical guarantee of the EM has been established both under the exact-specified and over-specified settings of vanilla Gaussian mixture models [1, 9, 8], to the best of our knowledge such guarantee has not been studied in the setting of top-K sparse softmax gating Gaussian mixture of experts. A potential approach to establishing the statistical behaviors of the EM algorithm in that model is to utilize the population-to-sample analysis that has been widely used in previous works to study the EM algorithm [1, 15]. We leave that direction for the future work."
        },
        {
            "heading": "Supplement to \u201cStatistical Perspective of",
            "text": ""
        },
        {
            "heading": "Top-K Sparse Softmax Gating Mixture of Experts\"",
            "text": "In this supplementary material, we provide rigorous proofs for all results under the exact-specified settings in Appendix A, while those for the over-specified settings are then presented in Appendix B. Finally, we study the identifiability of the top-K sparse softmax gating Gaussian mixture of experts (MoE) in Appendix C."
        },
        {
            "heading": "A Proof for Results under the Exact-specified Settings",
            "text": "In this appendix, we present the proofs for Theorem 1 in Appendix A.1, while that for Theorem 2 is then given in Appendix A.2. Lastly, the proof of Lemma 1 is provided in Appendix A.3."
        },
        {
            "heading": "A.1 Proof of Theorem 1",
            "text": "In this appendix, we conduct a convergence analysis for density estimation in the top-K sparse softmax gating Gaussian MoE using proof techniques in [34]. For that purpose, it is necessary to introduce some essential notations and key results first.\nLet Pk\u2217(\u0398) := {gG(Y |X) : G \u2208 Ek\u2217(\u2126)} be the set of all conditional density functions of mixing measures in Ek\u2217(\u2126). Next, we denote by N(\u03b5,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) the covering number of metric space (Pk\u2217(\u2126), \u2016 \u00b7 \u20161). Meanwhile, HB(\u03b5,Pk\u2217(\u2126), h) represents for the bracketing entropy of Pk\u2217(\u2126) under the Hellinger distance. Then, we provide in the following lemma the upper bounds of those terms.\nLemma 3. If \u2126 is a bounded set, then the following inequalities hold for any 0 < \u03b7 < 1/2:\n(i) logN(\u03b7,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) . log(1/\u03b7);\n(ii) HB(\u03b7,Pk\u2217(\u2126), h) . log(1/\u03b7). Proof of Lemma 3 is in Appendix A.1.2. Subsequently, we denote\nP\u0303k\u2217(\u2126) := {g(G+G\u2217)/2(Y |X) : G \u2208 Ek\u2217(\u2126)}; P\u03031/2k\u2217 (\u2126) := {g 1/2 (G+G\u2217)/2\n(Y |X) : G \u2208 Ek\u2217(\u2126)}. In addition, for each \u03b4 > 0, we define a Hellinger ball centered around the conditional density function gG\u2217(Y |X) and met with the set P\u0303 1/2 k\u2217 (\u2126) as\nP\u03031/2k\u2217 (\u2126, \u03b4) := {g 1/2 \u2208 P\u03031/2k\u2217 (\u2126) : h(g, gG\u2217) \u2264 \u03b4}.\nTo capture the size of the above Hellinger ball, [34] suggest using the following quantity:\nJB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)) := \u222b \u03b4\n\u03b42/213 H\n1/2 B (t, P\u0303 1/2 k\u2217 (\u2126, t), \u2016 \u00b7 \u2016)dt \u2228 \u03b4, (9)\nwhere t\u2228\u03b4 := max{t, \u03b4}. Given those notations, let us recall a standard result for density estimation in [34].\nLemma 4 (Theorem 7.4, [34]). Take \u03a8(\u03b4) \u2265 JB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)) such that \u03a8(\u03b4)/\u03b4 2 is a non-increasing\nfunction of \u03b4. Then, for some sequence (\u03b4n) and universal constant c which satisfy \u221a n\u03b42n \u2265 c\u03a8(\u03b4), we obtain that\nP ( EX [ h(g\nG\u0302n (\u00b7|X), gG\u2217(\u00b7|X))\n] > \u03b4 ) \u2264 c exp(\u2212n\u03b42/c2),\nfor any \u03b4 \u2265 \u03b4n Proof of Lemma 4 can be found in [34]. Now, we are ready to provide the proof for convergence rate of density estimation in Theorem 1 in Appendix A.1.1."
        },
        {
            "heading": "A.1.1 Main Proof",
            "text": "It is worth noting that for any t > 0, we have\nHB(t, P\u03031/2k\u2217 (\u2126, t), \u2016 \u00b7 \u2016) \u2264 HB(t,Pk\u2217(\u2126, t), h). Then, the integral in equation (9) is upper bounded as follows:\nJB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)) \u2264 \u222b \u03b4\n\u03b42/213 H\n1/2 B (t,Pk\u2217(\u2126, t), h)dt \u2228 \u03b4 .\n\u222b \u03b4\n\u03b42/213 log(1/t)dt \u2228 \u03b4, (10)\nwhere the second inequality follows from part (ii) of Lemma 3.\nAs a result, by choosing \u03a8(\u03b4) = \u03b4 \u00b7 \u221a log(1/\u03b4), we can verify that \u03a8(\u03b4)/\u03b42 is a non-increasing function of \u03b4. Furthermore, the inequality in equation (10) indicates that \u03a8(\u03b4) \u2265 JB(\u03b4, P\u03031/2k\u2217 (\u2126, \u03b4)). Next, let us consider a sequence (\u03b4n) defined as \u03b4n := \u221a log(n)/n. This sequence can be validated\nto satisfy the condition \u221a n\u03b42n \u2265 c\u03a8(\u03b4) for some universal constant c. Therefore, by Lemma 4, we reach the conclusion of Proposition 1:\nP ( EX [h(gG\u0302n(\u00b7|X), gG\u2217(\u00b7|X))] > C \u221a log(n)/n ) . n\u2212c,\nfor some universal constant C depending only on \u2126."
        },
        {
            "heading": "A.1.2 Proof of Lemma 3",
            "text": "Part (i). In this part, we will derive the following upper bound for the covering number of metric space (Pk\u2217(\u2126), \u2016 \u00b7 \u20161) for any 0 < \u03b7 < 1/2 given the bounded set \u2126:\nlogN(\u03b7,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) . log(1/\u03b7).\nTo begin with, we define \u0398 := {(a, b, \u03c3) \u2208 Rd \u00d7 R \u00d7 R+ : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}. Note that \u2126 is a bounded set, then \u0398 also admits this property. Thus, there exists an \u03b7-cover of \u0398, denoted by \u0398\u03b7. Additionally, we also define \u2206 := {(\u03b20, \u03b21) \u2208 Rd\u00d7R : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}, and \u2206\u03b7 be an \u03b7-cover of \u2206. Then, it can be validated that |\u0398\u03b7| \u2264 O(\u03b7\u2212(d+1)k\u2217) and |\u2206\u03b7| \u2264 O(\u03b7\u2212(d+3)k\u2217).\nSubsequently, for each G = \u2211k\u2217\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ek\u2217(\u2126), we take into account two other mixing measures. The first measure is G\u2032 =\n\u2211k\u2217 i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i), where (ai, bi, \u03c3i) \u2208 \u0398\u03b7 is the clos-\nest points to (ai, bi, \u03c3i) in this set for all i \u2208 [k\u2217]. The second one is G := \u2211k\u2217\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) in which (\u03b20i, \u03b21i) \u2208 \u2206\u03b7 for any i \u2208 [k\u2217]. Next, let us define\nT := {gG \u2208 Pk\u2217(\u2126) : (\u03b20i, \u03b21i) \u2208 \u2206\u03b7, (ai, bi, \u03c3i) \u2208 \u0398\u03b7,\u2200i \u2208 [k\u2217]},\nthen it is obvious that gG \u2208 T . Now, we will show that T is an \u03b7-cover of metric space (Pk\u2217(\u2126), \u2016\u00b7\u20161) with a note that it is not necessarily the smallest cover. Indeed, according to the triangle inequality,\n\u2016gG \u2212 gG\u20161 \u2264 \u2016gG \u2212 gG\u2032\u20161 + \u2016gG\u2032 \u2212 gG\u20161. (11)\nThe first term in the right hand side can be upper bounded as follows:\n\u2016gG \u2212 gG\u2032\u20161 \u2264 k\u2217\u2211\ni=1\n\u222b\nX\u00d7Y\n\u2223\u2223\u2223f(Y |a\u22a4i X + bi, \u03c3i)\u2212 f(Y |a\u22a4i X + bi, \u03c3i) \u2223\u2223\u2223d(X,Y )\n.\nk\u2217\u2211\ni=1\n\u222b\nX\u00d7Y\n( \u2016ai \u2212 ai\u2016+ \u2016bi \u2212 bi\u2016+ \u2016\u03c3i \u2212 \u03c3i\u2016 ) d(X,Y )\n=\nk\u2217\u2211\ni=1\n( \u2016ai \u2212 ai\u2016+ \u2016bi \u2212 bi\u2016+ \u2016\u03c3i \u2212 \u03c3i\u2016 )\n. \u03b7. (12)\nNext, we will also demonstrate that \u2016gG\u2032 \u2212 gG\u20161 . \u03b7. For that purpose, let us consider q := ( k K ) K-element subsets of {1, . . . , k}, which are assumed to take the form {\u21131, \u21132, . . . , \u2113K} for any \u2113 \u2208 [q]. Additionally, we also denote {\u2113K+1, . . . , \u2113k} := {1, . . . , k} \\ {\u21131, . . . , \u2113K} for any \u2113 \u2208 [q]. Then, we define\nX\u2113 := {x \u2208 X : \u03b2\u22a41ix \u2265 \u03b2\u22a41i\u2032x : i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}}, X\u0303\u2113 := {x \u2208 X : \u03b2 \u22a4 1ix \u2265 \u03b2 \u22a4 1i\u2032x : i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}}.\nBy using the same arguments as in the proof of Lemma 1 in Appendix A.3, we achieve that either X\u2113 = X\u0303\u2113 or X\u2113 has measure zero for any \u2113 \u2208 [q]. As the Softmax function is differentiable, it is a\nLipschitz function with some Lipschitz constant L \u2265 0. Since X is a bounded set, we may assume that \u2016X\u2016 \u2264 B for any X \u2208 X . Next, we denote\n\u03c0\u2113(X) := ( \u03b2\u22a41\u2113ix+ \u03b2 \u22a4 0\u2113i )K i=1 ; \u03c0\u2113(X) := ( \u03b2 \u22a4 1\u2113ix+ \u03b2 \u22a4 0\u2113i )K i=1 ,\nfor any K-element subset {\u21131, . . . \u2113K} of {1, . . . , k\u2217}. Then, we get\n\u2016Softmax(\u03c0\u2113(X))\u2212 Softmax(\u03c0\u2113(X))\u2016 \u2264 L \u00b7 \u2016\u03c0\u2113(X)\u2212 \u03c0\u2113(X)\u2016\n\u2264 L \u00b7 K\u2211\ni=1\n( \u2016\u03b21\u2113i \u2212 \u03b21\u2113i\u2016 \u00b7 \u2016X\u2016 + |\u03b20\u2113i \u2212 \u03b20\u2113i | )\n\u2264 L \u00b7 K\u2211\ni=1\n( \u03b7B + \u03b7 )\n. \u03b7.\nBack to the proof for \u2016gG\u2032 \u2212 gG\u20161 . \u03b7, it follows from the above results that\n\u2016gG\u2032 \u2212 gG\u20161 = \u222b\nX\u00d7Y |gG\u2032(Y |X) \u2212 gG(Y |X)| d(X,Y )\n\u2264 q\u2211\n\u2113=1\n\u222b\nX\u2113\u00d7Y |gG\u2032(Y |X)\u2212 gG(Y |X)| d(X,Y )\n\u2264 q\u2211\n\u2113=1\n\u222b\nX\u2113\u00d7Y\nK\u2211\ni=1\n\u2223\u2223\u2223Softmax(\u03c0\u2113(X)i)\u2212 Softmax(\u03c0\u2113(X)i) \u2223\u2223\u2223 \u00b7 \u2223\u2223\u2223f(Y |a\u22a4\u2113iX + b\u2113i , \u03c3\u2113i) \u2223\u2223\u2223 d(X,Y )\n. \u03b7, (13)\nFrom the results in equations (11), (12) and (13), we deduce that \u2016gG \u2212 gG\u20161 . \u03b7. This implies that T is an \u03b7-cover of the metric space (Pk\u2217(\u2126), \u2016 \u00b7 \u20161). Consequently, we achieve that\nN(\u03b7,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) . |\u2206\u03b7| \u00d7 |\u0398\u03b7| \u2264 O(1/\u03b7(d+3)k),\nwhich induces the conclusion of this part\nlogN(\u03b7,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) . log(1/\u03b7).\nPart (ii). Moving to this part, we will provide an upper bound for the bracketing entropy of Pk\u2217(\u2126) under the Hellinger distance:\nHB(\u03b7,Pk\u2217(\u2126), h) . log(1/\u03b7).\nRecall that \u0398 and X are bounded sets, we can find positive constants \u2212\u03b3 \u2264 a\u22a4X + b \u2264 \u03b3 and u1 \u2264 \u03c3 \u2264 u2. Let us define\nQ(Y |X) :=    1\u221a 2\u03c0u1 exp ( \u2212 Y 28u2 ) , for |Y | \u2265 2\u03b3\n1\u221a 2\u03c0u1 , for |Y | < 2\u03b3\nThen, it can be validated that f(Y |a\u22a4X + b, \u03c3) \u2264 Q(X,Y ) for any (X,Y ) \u2208 X \u00d7 Y.\nNext, let \u03c4 \u2264 \u03b7 which will be chosen later and {g1, . . . , gN} be an \u03c4 -cover of metric space (Pk\u2217(\u2126), \u2016\u00b7 \u20161) with the covering number N := N(\u03c4,Pk\u2217(\u2126), \u2016 \u00b7 \u20161). Additionally, we also consider brackets of the form [\u03a8Li (Y |X),\u03a8Ui (Y |X)] where\n\u03a8Li (Y |X) := max{gi(Y |X) \u2212 \u03c4, 0} \u03a8Ui (Y |X) := max{gi(Y |X) + \u03c4,Q(Y |X)}.\nThen, we can check that Pk\u2217(\u2126) \u2286 \u22c3N i=1[\u03a8 L i (Y |X),\u03a8Ui (Y |X)] and\n\u03a8Ui (Y |X)\u2212\u03a8Li (Y |X) \u2264 min{2\u03b7,Q(Y |X)}\n. Let S := max{2\u03b3,\u221a8u2} log(1/\u03c4), we have for any i \u2208 [N ] that\n\u2016\u03a8Ui \u2212\u03a8Li \u20161 = \u222b\n|Y |<2\u03b3 [\u03a8Ui (Y |X)\u2212\u03a8Li (Y |X)] dXdY +\n\u222b\n|Y |\u22652\u03b3 [\u03a8Ui (Y |X) \u2212\u03a8Li (Y |X)] dXdY\n\u2264 S\u03c4 + exp ( \u2212 S 2\n2u2\n) \u2264 S\u2032\u03c4,\nwhere S\u2032 is some positive constant. This inequality indicates that\nHB(S \u2032\u03c4,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) \u2264 logN(\u03c4,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) \u2264 log(1/\u03c4).\nBy setting \u03c4 = \u03b7/S\u2032, we obtain that HB(\u03b7,Pk\u2217(\u2126), \u2016 \u00b7 \u20161) . log(1/\u03b7). Finally, since the norm \u2016 \u00b7 \u20161 is upper bounded by the Hellinger distance, we reach the conclusion of this part:\nHB(\u03b7,Pk\u2217(\u2126), h) . log(1/\u03b7).\nHence, the proof is completed."
        },
        {
            "heading": "A.2 Proof of Theorem 2",
            "text": "Since the Hellinger distance is lower bounded by the Total Variation distance, that is h \u2265 V , we will prove the following Total Variation lower bound:\nEX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] & D1(G,G\u2217),\nwhich is then respectively broken into local part and global part as follows:\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] D1(G,G\u2217) > 0, (14)\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)>\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] D1(G,G\u2217) > 0, (15)\nfor some constant \u03b5\u2032 > 0.\nProof of claim (14): It is sufficient to show that\nlim \u03b5\u21920 inf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] D1(G,G\u2217) > 0.\nAssume that this inequality does not hold, then since the number of experts k\u2217 is known in this case, there exists a sequence of mixing measure Gn := \u2211k\u2217 i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ek\u2217(\u2126) such that both D1(Gn, G\u2217) and EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))]/D1(Gn, G\u2217) approach zero as n tends to infinity. Now, we define\nCnj = Cj(Gn) := {i \u2208 [k\u2217] : \u2016\u03c9ni \u2212 \u03c9\u2217j\u2016 \u2264 \u2016\u03c9ni \u2212 \u03c9\u2217s\u2016, \u2200s 6= j},\nfor any j \u2208 [k\u2217] as k\u2217 Voronoi cells with respect to the mixing measure Gn, where we denote \u03c9ni := (\u03b2 n 1i, a n i , b n i , \u03c3 n i ) and \u03c9 \u2217 j := (\u03b2 \u2217 1j , a \u2217 j , b \u2217 j , \u03c3 \u2217 j ). As we use asymptotic arguments in this proof, we can assume without loss of generality (WLOG) that these Voronoi cells does not depend on n, that is, Cj = Cnj . Next, it follows from the hypothesis D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e that each Voronoi cell contains only one element. Thus, we continue to assume WLOG that Cj = {j} for any j \u2208 [k\u2217], which implies that (\u03b2n1j , a n j , b n j , \u03c3 n j ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) and exp(\u03b2n0j) \u2192 exp(\u03b2\u22170j) as n \u2192 \u221e.\nSubsequently, to specify the top K selection in the formulations of gGn(Y |X) and gG\u2217(Y |X), we divide the covariate space X into some subsets in two ways. In particular, we first consider q := (k\u2217 K ) different K-element subsets of [k\u2217], which are assumed to take the form {\u21131, . . . , \u2113K}, for \u2113 \u2208 [q]. Additionally, we denote {\u2113K+1, . . . , \u2113k\u2217} := [k\u2217] \\ {\u21131, . . . , \u2113K}. Then, we define for each \u2113 \u2208 [q] two following subsets of X :\nX n\u2113 := { x \u2208 X : (\u03b2n1j)\u22a4x \u2265 (\u03b2n1j\u2032)\u22a4x : \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } , X \u2217\u2113 := { x \u2208 X : (\u03b2\u22171j)\u22a4x \u2265 (\u03b2\u22171j\u2032)\u22a4x : \u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } .\nSince (\u03b2n0j , \u03b2 n 1j) \u2192 (\u03b2\u22170j , \u03b2\u22171j) as n \u2192 \u221e for any j \u2208 [k\u2217], we have for any arbitrarily small \u03b7j > 0 that \u2016\u03b2n1j \u2212 \u03b2\u22171j\u2016 \u2264 \u03b7j and |\u03b2n0j \u2212 \u03b2\u22170j | \u2264 \u03b7j for sufficiently large n. By applying Lemma 1, we obtain that X n\u2113 = X \u2217\u2113 for any \u2113 \u2208 [q] for sufficiently large n. WLOG, we assume that\nD1(Gn, G\u2217) = K\u2211\ni=1\n[ exp(\u03b2n0i) ( \u2016\u2206\u03b2n1i\u2016+ \u2016\u2206ani \u2016+ \u2016\u2206bni \u2016+ \u2016\u2206\u03c3ni \u2016 ) + \u2223\u2223\u2223 exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i) \u2223\u2223\u2223 ] ,\nwhere we denote \u2206\u03b2n1i := \u03b2 n 1i \u2212 \u03b2\u22171i, \u2206ani := ani \u2212 a\u2217i , \u2206bni := bni \u2212 b\u2217i and \u2206\u03c3ni := \u03c3ni \u2212 \u03c3\u2217i .\nLet \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Then, for almost surely (X,Y ) \u2208 X \u2217\u2113 \u00d7 Y, we can rewrite the conditional densities gGn(Y |X) and gG\u2217(Y |X) as\ngGn(Y |X) = K\u2211\ni=1\nexp((\u03b2n1i) \u22a4X + \u03b2n0i)\u2211K\nj=1 exp((\u03b2 n 1j) \u22a4X + \u03b2n0j) \u00b7 f(Y |(ani )\u22a4X + bni , \u03c3ni ),\ngG\u2217(Y |X) = K\u2211\ni=1\nexp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)\u2211K\nj=1 exp((\u03b2 \u2217 1j) \u22a4X + \u03b2\u22170j) \u00b7 f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ).\nNow, we break the rest of our arguments into three steps:\nStep 1 - Taylor expansion: In this step, we take into account Hn := [\u2211K i=1 exp((\u03b2 \u2217 1i) \u22a4X+\u03b2\u22170i) ] \u00b7 [gGn(Y |X)\u2212gG\u2217(Y |X)]. Then, Hn can be represented as follows:\nHn = K\u2211\ni=1\nexp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ]\n\u2212 K\u2211\ni=1\nexp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)gGn(Y |X) \u2212 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) ]\n+ K\u2211\ni=1\n[ exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i) ][ exp((\u03b2\u22171i) \u22a4X + \u03b2\u22170i)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\u2212 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) ] .\nBy applying the first-order Taylor expansion to the first term in the above representation, which is denoted by An, we get that\nAn =\nK\u2211\ni=1\n\u2211\n|\u03b1|=1\nexp(\u03b2n0i)\n\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b11+\u03b12 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202|\u03b12|+\u03b13+\u03b14f\n\u2202h |\u03b12|+\u03b13 1 \u2202h \u03b14 2\n(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y ),\nwhere R1(X,Y ) is a Taylor remainder that satisfies R1(X,Y )/D\u20321(X,Y ) \u2192 0 as n \u2192 \u221e. Recall that f is the univariate Gaussian density, then we have\n\u2202\u03b14f \u2202h\u03b142 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) = 1 2\u03b14 \u00b7 \u2202 2\u03b14f \u2202h2\u03b141 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ),\nwhich leads to\nAn = K\u2211\ni=1\n\u2211\n|\u03b1|=1\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b11+\u03b12 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202|\u03b12|+\u03b13+2\u03b14f\n\u2202h |\u03b12|+\u03b13+2\u03b14 1\n(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y )\n=\nK\u2211\ni=1\n2\u2211\n|\u03b71|+\u03b72=1\n\u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R1(X,Y ), (16)\nwhere we denote \u03b71 = \u03b11 + \u03b12 \u2208 Nd, \u03b72 = |\u03b12|+ \u03b13 + 2\u03b14 \u2208 N and an index set\nJ\u03b71,\u03b72 := {(\u03b1i)4i=1 \u2208 Nd \u00d7 Nd \u00d7 N\u00d7 N : \u03b11 + \u03b12 = \u03b71, \u03b13 + 2\u03b14 = \u03b72 \u2212 |\u03b12|}. (17)\nBy arguing in a similar fashion for the second term in the representation of Hn, we also get that\nBn := \u2212 K\u2211\ni=1\n\u2211\n|\u03b3|=1\nexp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R2(X,Y ),\nwhere R2(X,Y ) is a Taylor remainder such that R2(X,Y )/D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Putting the above results together, we rewrite the quantity Hn as follows:\nHn =\nK\u2211\ni=1\n\u2211\n0\u2264|\u03b71|+\u03b72\u22642 Uni,\u03b71,\u03b72 \u00b7X \u03b71 exp((\u03b2\u22171i) \u22a4X)\n\u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+\nK\u2211\ni=1\n\u2211\n0\u2264|\u03b3|\u22641 W ni,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gGn(Y |X) +R1(X,Y ) +R2(X,Y ), (18)\nin which we respectively define for each i \u2208 [K] that\nUni,\u03b71,\u03b72 := \u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14 ,\nW ni,\u03b3 := exp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u03b3 ,\nfor any (\u03b71, \u03b72) 6= (0d, 0) and |\u03b3| 6= 0d. Otherwise, Uni,0d,0 = \u2212W n i,0d := exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i).\nStep 2 - Non-vanishing coefficients:\nMoving to the second step, we will show that not all the ratios Uni,\u03b71,\u03b72/D1(Gn, G\u2217) tend to zero as n goes to infinity. Assume by contrary that all of them approach zero when n \u2192 \u221e, then for (\u03b71, \u03b72) = (0d, 0), it follows that\n1\nD1(Gn, G\u2217) \u00b7\nK\u2211\ni=1\n\u2223\u2223\u2223 exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i) \u2223\u2223\u2223 = K\u2211\ni=1\n|Unj,\u03b71,\u03b72 | D1(Gn, G\u2217) \u2192 0. (19)\nAdditionally, for tuples (\u03b71, \u03b72) where \u03b71 \u2208 {e1, e2, . . . , ed} with ej := (0, . . . , 0, 1\ufe38\ufe37\ufe37\ufe38 j\u2212th , 0, . . . , 0) and \u03b72 = 0, we get\n1\nD1(Gn, G\u2217) \u00b7\nK\u2211\ni=1\nexp(\u03b2n0i)\u2016\u2206\u03b2n1i\u20161 = K\u2211\ni=1\n|Unj,\u03b71,\u03b72 | D1(Gn, G\u2217) \u2192 0.\nBy using similar arguments, we end up having\n1\nD1(Gn, G\u2217) \u00b7\nK\u2211\ni=1\nexp(\u03b2n0i) [ \u2016\u2206\u03b2n1i\u20161 + \u2016\u2206ani \u20161 + |\u2206bni |+ |\u2206\u03c3ni | ] \u2192 0.\nDue to the topological equivalence between norm-1 and norm-2, the above limit implies that\n1\nD1(Gn, G\u2217) \u00b7\nK\u2211\ni=1\nexp(\u03b2n0i) [ \u2016\u2206\u03b2n1i\u2016+ \u2016\u2206ani \u2016+ |\u2206bni |+ |\u2206\u03c3ni | ] \u2192 0. (20)\nCombine equation (19) with equation (20), we deduce that D1(Gn, G\u2217)/D1(Gn, G\u2217) \u2192 0, which is a contradiction. Consequently, at least one among the ratios Uni,\u03b71,\u03b72/D1(Gn, G\u2217) does not vanish as n tends to infinity.\nStep 3 - Fatou\u2019s contradiction:\nLet us denote by mn the maximum of the absolute values of U n i,\u03b71,\u03b72 /D1(Gn, G\u2217) and W ni,\u03b3/D1(Gn, G\u2217). It follows from the result achieved in Step 2 that 1/mn 6\u2192 \u221e.\nRecall from the hypothesis that EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))]/D1(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Thus, by the Fatou\u2019s lemma, we have\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))] D1(Gn, G\u2217) = 1 2 \u00b7 \u222b lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| D1(Gn, G\u2217) dXdY.\nThis result indicates that |gGn(Y |X) \u2212 gG\u2217(Y |X)|/D1(Gn, G\u2217) tends to zero as n goes to infinity for almost surely (X,Y ). As a result, it follows that\nlim n\u2192\u221e Hn mnD(Gn, G\u2217) = lim n\u2192\u221e |gGn(Y |X) \u2212 gG\u2217(Y |X)| mnD1(Gn, G\u2217) = 0.\nNext, let us denote Uni,\u03b71,\u03b72/[mnD1(Gn, G\u2217)] \u2192 \u03c4i,\u03b71,\u03b72 and W ni,\u03b3/[mnD1(Gn, G\u2217)] \u2192 \u03bai,\u03b3 with a note that at least one among them is non-zero. From the formulation of Hn in equation (18), we deduce that\nK\u2211\ni=1\n\u2211\n0\u2264|\u03b71|+\u03b72\u22642 \u03c4i,\u03b71,\u03b72 \u00b7X\u03b71 exp((\u03b2\u22171i)\u22a4X)\n\u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+\nK\u2211\ni=1\n\u2211\n0\u2264|\u03b3|\u22641 \u03bai,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gG\u2217(Y |X) = 0, (21)\nfor almost surely Y . This equation is equivalent to\nK\u2211\ni=1\n\u2211\n0\u2264|\u03b71|\u22641\n  \u2211\n0\u2264\u03b72\u22642\u2212|\u03b3| \u03c4i,\u03b71,\u03b72\n\u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) + \u03bai,\u03b71gG\u2217(Y |X)\n \n\u00d7 X\u03b71 exp((\u03b2\u22171i)\u22a4X) = 0,\nfor almost surely Y . Note that \u03b2\u221711, . . . , \u03b2 \u2217 1K admits pair-wise different values, then {exp((\u03b2\u22171i)\u22a4X) : i \u2208 [K]} is a linearly independent set, which leads to\n\u2211\n0\u2264|\u03b71|\u22641\n  \u2211\n0\u2264\u03b72\u22642\u2212|\u03b3| \u03c4i,\u03b71,\u03b72\n\u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) + \u03bai,\u03b71gG\u2217(Y |X)\n X\u03b71 = 0,\nfor any i \u2208 [K] for almost surely Y . It is clear that the left hand side of the above equation is a polynomial of X belonging to the compact set X . As a result, we get that\n\u2211\n0\u2264\u03b72\u22642\u2212|\u03b3| \u03c4i,\u03b71,\u03b72\n\u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) + \u03bai,\u03b71gG\u2217(Y |X) = 0,\nfor any i \u2208 [K], 0 \u2264 |\u03b71| \u2264 1 and almost surely Y . Since (a\u22171, b\u22171, \u03c3\u22171), . . . , (a\u2217K , b\u2217K , \u03c3\u2217K) have pair-wise distinct values, those of ((a\u22171) \u22a4X+b\u22171, \u03c3 \u2217 1), . . . , ((a \u2217 K) \u22a4X+b\u2217K , \u03c3 \u2217 K) are also pair-wise different. Thus,\nthe set {\n\u2202\u03b72f \u2202h\n\u03b72 1\n(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ), gG\u2217(Y |X) : i \u2208 [K] } is linearly independent. Consequently, we\nobtain that \u03c4i,\u03b71,\u03b72 = \u03bai,\u03b3 = 0 for any i \u2208 [K], 0 \u2264 |\u03b71|+ \u03b72 \u2264 2 and 0 \u2264 |\u03b3| \u2264 1, which contradicts the fact that at least one among these terms is different from zero.\nHence, we can find some constant \u03b5\u2032 > 0 such that\ninf G\u2208Ek\u2217 (\u2126):D1(G,G\u2217)\u2264\u03b5\u2032 EX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] D1(G,G\u2217) > 0.\nProof of claim (15): Assume by contrary that this claim is not true, then we can seek a sequence G\u2032n \u2208 Ek\u2217(\u2126) such that D1(G\u2032n, G\u2217) > \u03b5\u2032 and\nlim n\u2192\u221e EX [V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] D1(G\u2032n, G\u2217) = 0,\nwhich directly implies that EX [V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] \u2192 0 as n \u2192 \u221e. Recall that \u2126 is a compact set, therefore, we can replace the sequence G\u2032n by one of its subsequences that converges to a mixing measure G\u2032 \u2208 Ek\u2217(\u2126). Since D1(G\u2032n, G\u2217) > \u03b5\u2032, this result induces that D1(G\u2032, G\u2217) > \u03b5\u2032.\nSubsequently, by means of the Fatou\u2019s lemma, we achieve that\n0 = lim n\u2192\u221e\nEX [2V (gG\u2032n(\u00b7|X), gG\u2217 (\u00b7|X))] \u2265 \u222b\nlim inf n\u2192\u221e\n\u2223\u2223\u2223gG\u2032n(Y |X)\u2212 gG\u2217(Y |X) \u2223\u2223\u2223 d\u00b5(Y )\u03bd(X).\nIt follows that gG\u2032(Y |X) = gG\u2217(Y |X) for almost surely (X,Y ). From Proposition 2, we know that the top-K sparse softmax gating Gaussian mixture of experts is identifiable, thus, we obtain that G\u2032 \u2261 G\u2217. As a consequence, D1(G\u2032, G\u2217) = 0, contradicting the fact that D1(G\u2032, G\u2217) > \u03b5\u2032 > 0.\nHence, the proof is completed."
        },
        {
            "heading": "A.3 Proof of Lemma 1",
            "text": "Let \u03b7i = Mi\u03b5, where \u03b5 is some fixed positive constant and Mi will be chosen later. For an arbitrary \u2113 \u2208 [q], since X and \u2126 are bounded sets, there exists some constant c\u2217\u2113 \u2265 0 such that\nmin x,i,i\u2032\n[ (\u03b2\u22171i) \u22a4x\u2212 (\u03b2\u22171i\u2032)\u22a4x ] = c\u2217\u2113\u03b5, (22)\nwhere the minimum is subject to x \u2208 X \u2217\u2113 , i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}. We will point out that c\u2217\u2113 > 0. Assume by contrary that c \u2217 \u2113 = 0. For x \u2208 X \u2217\u2113 , we may assume for any 1 \u2264 i < j \u2264 k\u2217 that\n(\u03b2\u22171\u2113i) \u22a4x \u2265 (\u03b2\u22171\u2113j ) \u22a4x.\nSince c\u2217\u2113 = 0, it follows from equation (22) that (\u03b2 \u2217 1\u2113K )\u22a4x\u2212 (\u03b2\u22171\u2113K+1) \u22a4x = 0, or equivalently\n(\u03b2\u22171\u2113K \u2212 \u03b2 \u2217 1\u2113K+1 )\u22a4x = 0.\nIn other words, X \u2217\u2113 is a subset of\nZ := {x \u2208 X : (\u03b2\u22171\u2113K \u2212 \u03b2 \u2217 1\u2113K+1 )\u22a4x = 0}.\nSince \u03b21\u2113K \u2212 \u03b21\u2113K+1 6= 0d and the distribution of X is continuous, it follows that the set Z has measure zero. Since X \u2217\u2113 \u2286 Z, we can conclude that X \u2217\u2113 also has measure zero, which contradicts the hypothesis of Lemma 1. Therefore, we must have c\u2217\u2113 > 0.\nAs X is a bounded set, we assume that \u2016x\u2016 \u2264 B for any x \u2208 X . Let x \u2208 X \u2217\u2113 , then we have for any i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} that\n\u03b2\u22a41ix = (\u03b21i \u2212 \u03b2\u22171i)\u22a4x+ (\u03b2\u22171i)\u22a4x \u2265 \u2212Mi\u03b5B + (\u03b2\u22171i\u2032)\u22a4x+ c\u2217\u2113\u03b5 = \u2212Mi\u03b5B + c\u2217\u2113\u03b5+ (\u03b2\u22171i\u2032 \u2212 \u03b21i\u2032)\u22a4x+ \u03b2\u22a41i\u2032x \u2265 \u22122Mi\u03b5B ++c\u2217\u2113\u03b5+ \u03b2\u22a41i\u2032x.\nBy setting Mi \u2264 c\u2217\u2113 2B\n, we get that x \u2208 X\u2113, which means that X \u2217\u2113 \u2286 X\u2113. Similarly, assume that there exists some constant c\u2113 \u2265 0 that satisfies\nmin x,i,i\u2032\n[ (\u03b2\u22171i) \u22a4x\u2212 (\u03b2\u22171i\u2032)\u22a4x ] = c\u2217\u2113\u03b5.\nHere, the above minimum is subject to x \u2208 X\u2113, i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}. If Mi \u2264\nc\u2113 2B , then we also receive that X\u2113 \u2286 X \u2217\u2113 .\nHence, if we set Mi = 1\n2B min{c\u2217\u2113 , c\u2113}, we reach the conclusion that X \u2217\u2113 = X\u2113."
        },
        {
            "heading": "B Proof for Results under Over-specified Settings",
            "text": "In this appendix, we first provide the proofs of Theorem B.1 and Theorem 4 in Appendix B.1 and Appendix B.2, respectively. Subsequently, we present the proof for Proposition 1 in Appendix B.3, while that for Lemma 2 is put in Appendix B.4."
        },
        {
            "heading": "B.1 Proof of Theorem 3",
            "text": "In this appendix, we follow proof techniques presented in Appendix A.1 to demonstrate the result of Theorem 3. Recall that under the over-specified settings, the MLE G\u0302n belongs to the set of all mixing measures with at most k > k\u2217 components, i.e. Ok(\u2126). Interestingly, if we can adapt the result of part (i) of Lemma 3 to the over-specified settings, then other results presented in Appendix A.1 will also hold true. Therefore, our main goal is to derive following bound for any 0 < \u03b7 < 1/2 under the over-specified settings:\nlogN(\u03b7,Pk(\u2126), \u2016 \u00b7 \u20161) . log(1/\u03b7),\nwhere Pk(\u2126) := {gG(Y |X) : G \u2208 Ok(\u2126)}. For ease of presentation, we will reuse the notations defined in Appendix A.1 with Ek\u2217(\u2126) being replaced by Ok(\u2126). Now, let us recall necessary notations\nfor this proof.\nFirstly, we define \u0398 = {(a, b, \u03c3) \u2208 Rd \u00d7 R \u00d7 R+ : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}, and \u0398\u03b7 is an \u03b7-cover of \u0398. Additionally, we also denote \u2206 := {(\u03b20, \u03b21) \u2208 Rd \u00d7 R : (\u03b20, \u03b21, a, b, \u03c3) \u2208 \u2126}, and \u2206\u03b7 be an \u03b7-cover of \u2206. Next, for each mixing measure G = \u2211k i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ok(\u2126), we denote G\u2032 = \u2211k\ni=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) in which (ai, bi, \u03c3i) \u2208 \u0398\u03b7 is the closest point to (ai, bi, \u03c3i) in this set for any i \u2208 [k]. We also consider another mixing measure G := \u2211ki=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) \u2208 Ok(\u2126) where (\u03b20i, \u03b21i) \u2208 \u2206\u03b7 is the closest point to (\u03b20i, \u03b21i) in this set for any i \u2208 [k].\nSubsequently, we define\nL := {gG \u2208 Pk(\u2126) : (\u03b20i, \u03b21i) \u2208 \u2206\u03b7, (ai, bi, \u03c3i) \u2208 \u0398\u03b7}.\nWe demonstrate that L is an \u03b7-cover of the metric space (Pk(\u2126), \u2016 \u00b7 \u20161), that is, for any gG \u2208 Pk(\u2126), there exists a density gG \u2208 L such that \u2016gG \u2212 gG\u20161 \u2264 \u03b7. By the triangle inequality, we have\n\u2016gG \u2212 gG\u20161 \u2264 \u2016gG \u2212 gG\u2032\u20161 + \u2016gG\u2032 \u2212 gG\u20161. (23)\nFrom the formulation of G\u2032, we get that\n\u2016gG \u2212 gG\u2032\u20161 \u2264 k\u2211\ni=1\n\u222b\nX\u00d7Y\n\u2223\u2223\u2223f(Y |a\u22a4i X + bi, \u03c3i)\u2212 f(Y |a\u22a4i X + bi, \u03c3i) \u2223\u2223\u2223d(X,Y )\n.\nk\u2211\ni=1\n\u222b\nX\u00d7Y\n( \u2016ai \u2212 ai\u2016+ |bi \u2212 bi|+ |\u03c3i \u2212 \u03c3i| ) d(X,Y )\n. \u03b7 (24)\nBased on inequalities in equations (23) and (24), it is sufficient to show that \u2016gG\u2032 \u2212 gG\u20161 . \u03b7. For any \u2113 \u2208 [q], let us define\nX \u2113 := {x \u2208 X : (\u03b21i)\u22a4x \u2265 (\u03b21i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}}, X \u2032 \u2113 := {x \u2208 X : (\u03b21i)\u22a4x \u2265 (\u03b21i\u2032)\u22a4x, \u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}}.\nSince the Softmax function is differentiable, it is a Lipschitz function with some Lipschitz constant L \u2265 0. Assume that \u2016X\u2016 \u2264 B for any X \u2208 X and denote\n\u03c0\u2113(X) := ( \u03b2\u22a4 1\u2113i x+ \u03b2\u22a4 0\u2113i )K i=1 ; \u03c0\u2113(X) := ( \u03b2 \u22a4 1\u2113ix+ \u03b2 \u22a4 0\u2113i )K i=1 ,\nfor any K-element subset {\u21131, . . . \u2113K} of {1, . . . , k}. Then, we have\n\u2016Softmax(\u03c0\u2113(X))\u2212 Softmax(\u03c0\u2113(X))\u2016 \u2264 L \u00b7 \u2016\u03c0\u2113(X)\u2212 \u03c0\u2113(X)\u2016\n\u2264 L \u00b7 K\u2211\ni=1\n( \u2016\u03b21\u2113i \u2212 \u03b21\u2113i\u2016 \u00b7 \u2016X\u2016 + |\u03b20\u2113i \u2212 \u03b20\u2113i | )\n\u2264 L \u00b7 K\u2211\ni=1\n( \u03b7B + \u03b7 )\n. \u03b7.\nBy arguing similarly to the proof of Lemma 1 in Appendix A.3, we receive that either X \u2113 = X \u2032\u2113 or X \u2113 has measure zero for any \u2113 \u2208 [q]. As a result, we deduce that\n\u2016gG\u2032 \u2212 gG\u2217\u20161 \u2264 q\u2211\n\u2113=1\n\u222b\nX \u2113 \u00d7Y\n|gG\u2032(Y |X)\u2212 gG|d(X,Y )\n\u2264 q\u2211\n\u2113=1\n\u222b\nX \u2113 \u00d7Y\nK\u2211\ni=1\n\u2223\u2223\u2223Softmax(\u03c0\u2113(X)i)\u2212 Softmax(\u03c0\u2113(X)i) \u2223\u2223\u2223 \u00b7 \u2223\u2223\u2223f(Y |a\u22a4\u2113iX + b\u2113i , \u03c3\u2113i) \u2223\u2223\u2223d(X,Y )\n. \u03b7.\nThus, L is an \u03b7-cover of the metric space (Pk(\u2126), \u2016 \u00b7 \u20161), which implies that\nN(\u03b7,Pk(\u2126), \u2016 \u00b7 \u20161) . |\u2206\u03b7| \u00d7 |\u0398\u03b7| \u2264 O(\u03b7\u2212(d+1)k)\u00d7O(\u03b7\u2212(d+3)k) = O(\u03b7\u2212(2d+4)k). (25)\nHence, logN(\u03b7,Pk(\u2126), \u2016 \u00b7 \u20161) . log(1/\u03b7)."
        },
        {
            "heading": "B.2 Proof of Theorem 4",
            "text": "Similar to the proof of Theorem 2 in Appendix A, our objective here is also to derive the Total Variation lower bound adapted to the over-fitted settings:\nEX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] & D2(G,G\u2217).\nSince the global part of the above inequality can be argued in the same fashion as in Appendix A, we will focus only on demonstrating the following local part via the proof by contradiction method:\nlim \u03b5\u21920 inf G\u2208Ok(\u0398):D2(G,G\u2217)\u2264\u03b5 EX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] D2(G,G\u2217) > 0. (26)\nAssume that the above claim does not hold true, then we can find a sequence of mixing measures Gn := \u2211kn i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ok(\u2126) such that D2(Gn, G\u2217) \u2192 0 and\nEX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]/D2(Gn, G\u2217) \u2192 0,\nwhen n goes to infinity. Additionally, by abuse of notation, we reuse the set of Voronoi cells Cj, for j \u2208 [k\u2217], defined in Appendix A. Due to the limit D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e, it follows that for any j \u2208 [k\u2217], we have \u2211 i\u2208Cj exp(\u03b2 n 0i) \u2192 exp(\u03b2\u22170j) and (\u03b2n1i, ani , bni , \u03c3ni ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) for all i \u2208 Cj . WLOG, we may assume that\nD2(Gn, G\u2217) = \u2211\nj\u2208[K], |Cj |>1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ \u2016\u2206\u03b2n1ij\u2016r\u0304(|Cj |) + \u2016\u2206anij\u2016 r\u0304(|Cj |) 2 + |\u2206bnij|r\u0304(|Cj |) + |\u2206\u03c3nij| r\u0304(|Cj |) 2 ]\n+ \u2211\nj\u2208[K], |Cj |=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ \u2016\u2206\u03b2n1ij\u2016+ \u2016\u2206anij\u2016+ |\u2206bnij |+ |\u2206\u03c3nij| ] + K\u2211 j=1 \u2223\u2223\u2223 \u2211 i\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j) \u2223\u2223\u2223.\nRegarding the top-K selection in the conditional density gG\u2217 , we partition the covariate space X in a similar fashion to Appendix A. More specifically, we consider q = (k\u2217 K ) subsets {\u21131, . . . , \u2113K} of {1, . . . , k\u2217} for any \u2113 \u2208 [q], and denote {\u2113K+1, . . . , \u2113k\u2217} := [k\u2217] \\ {\u21131, . . . , \u2113K}. Then, we define\nX \u2217\u2113 := { x \u2208 X : (\u03b2\u22171j)\u22a4x \u2265 (\u03b2\u22171j\u2032)\u22a4x,\u2200j \u2208 {\u21131, . . . , \u2113K}, j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} } ,\nfor any \u2113 \u2208 [q]. On the other hand, we need to introduce a new partition method of the covariate space for the weight selection in the conditional density gGn . In particular, let K \u2208 N such that k \u2212 k\u2217 +K \u2264 K \u2264 k and q := ( k K ) . Then, for any \u2113 \u2208 [q], we denote (\u21131, . . . , \u2113k) as a permutation of (1, . . . , k) and\nX n \u2113 := { x \u2208 X : (\u03b2n1i)\u22a4x \u2265 (\u03b2n1i\u2032)\u22a4x,\u2200i \u2208 {\u21131, . . . , \u2113K}, i\u2032 \u2208 {\u2113K+1, . . . , \u2113k} } .\nLet X \u2208 X \u2217\u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. If {\u21131, . . . \u2113K} 6= C1 \u222a . . . \u222a CK for any \u2113 \u2208 [q], then V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))/D2(Gn, G\u2217) 6\u2192 0 as n tends to infinity. This contradicts the fact that this term must approach zero. Therefore, we only need to consider the scenario when there exists \u2113 \u2208 [q] such that {\u21131, . . . \u2113K} = C1 \u222a . . . \u222a CK . Recall that we have (\u03b2n0i, \u03b2n1i) \u2192 (\u03b2\u22170j , \u03b2\u22171j) as n \u2192 \u221e for any j \u2208 [k\u2217] and i \u2208 Cj. Thus, for any arbitrarily small \u03b7j > 0, we have that \u2016\u03b2n1i \u2212 \u03b2\u22171j\u2016 \u2264 \u03b7j and |\u03b2n0i \u2212 \u03b2\u22170j | \u2264 \u03b7j for sufficiently large n. Then, it follows from Lemma 2 that X \u2217\u2113 = X n\u2113 for sufficiently large n. This result indicates that X \u2208 X n \u2113 .\nThen, we can represent the conditional densities gG\u2217(Y |X) and gGn(Y |X) for any sufficiently large n as follows:\ngG\u2217(Y |X) = K\u2211\nj=1\nexp((\u03b2\u22171j) \u22a4X + \u03b2\u22170j)\u2211K\nj\u2032=1 exp((\u03b2 \u2217 1j\u2032) \u22a4X + \u03b2\u22170j\u2032) \u00b7 f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ),\ngGn(Y |X) = K\u2211\nj=1\n\u2211\ni\u2208Cj\nexp((\u03b2n1i) \u22a4X + \u03b2n0i)\u2211K\nj\u2032=1 \u2211 i\u2032\u2208Cj\u2032 exp((\u03b2 n 1i\u2032) \u22a4X + \u03b2n0i\u2032) \u00b7 f(Y |(ani )\u22a4X + bni , \u03c3ni ).\nNow, we reuse the three-step framework in Appendix A.\nStep 1 - Taylor expansion:\nFirstly, by abuse of notations, let us consider the quantity\nHn := [ K\u2211\nj=1\nexp((\u03b2\u22171j) \u22a4X + \u03b2\u22170j) ] \u00b7 [gGn(Y |X) \u2212 gG\u2217(Y |X)].\nSimilar to Step 1 in Appendix A, we can express this term as\nHn = K\u2211\nj=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171j)\u22a4X)f(Y |(a\u2217j )\u22a4X + b\u2217j , \u03c3\u2217j ) ]\n\u2212 K\u2211\nj=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)gGn(Y |X)\u2212 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) ]\n+\nK\u2211\nj=1\n[\u2211\ni\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j)\n][ exp((\u03b2\u22171j) \u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\u2212 exp((\u03b2\u22171j)\u22a4X)gGn(Y |X) ]\n:= An +Bn + En.\nNext, we proceed to decompose An based on the cardinality of the Voronoi cells as follows: An = \u2211\nj:|Cj |=1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ]\n+ \u2211\nj:|Cj |>1\n\u2211 i\u2208Cj exp(\u03b2n0i) [ exp((\u03b2n1i) \u22a4X)f(Y |(ani )\u22a4X + bni , \u03c3ni )\u2212 exp((\u03b2\u22171i)\u22a4X)f(Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) ] .\nBy applying the Taylor expansions of order 1 and r\u0304(|Cj |) to the first and second terms of An, respectively, and following the derivation in equation (16), we arrive at\nAn = \u2211\nj:|Cj |=1\n\u2211\ni\u2208Cj\n\u2211\n1\u2264|\u03b71|+\u03b72\u22642\n\u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R3(X,Y )\n+ \u2211\nj:|Cj |>1\n\u2211\ni\u2208Cj\n\u2211\n1\u2264|\u03b71|+\u03b72\u22642r\u0304(|Cj |)\n\u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14\n\u00d7X\u03b71 exp((\u03b2\u22171i)\u22a4X) \u00b7 \u2202\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i ) +R4(X,Y ),\nwhere the set J\u03b71,\u03b72 is defined in equation (17) while Ri(X,Y ) is a Taylor remainder such that Ri(X,Y )/D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for i \u2208 {3, 4}. Similarly, we also decompose Bn according to the Voronoi cells as An but then invoke the Taylor expansions of order 1 and 2 to the first term and the second term, respectively. In particular, we get\nBn = \u2212 \u2211\nj:|Cj |=1\n\u2211\ni\u2208Cj\n\u2211\n|\u03b3|=1\nexp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R5(X,Y )\n\u2212 \u2211\nj:|Cj |>1\n\u2211\ni\u2208Cj\n\u2211\n1\u2264|\u03b3|\u22642\nexp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R6(X,Y ),\nwhere R5(X,Y ) and R6(X,Y ) are Taylor remainders such that their ratios over D2(Gn, G\u2217) ap-\nproach zero as n \u2192 \u221e. Subsequently, let us define\nSnj,\u03b71,\u03b72 := \u2211\ni\u2208Cj\n\u2211\n\u03b1\u2208J\u03b71,\u03b72\nexp(\u03b2n0i)\n2\u03b14\u03b1! \u00b7 (\u2206\u03b2n1i)\u03b11(\u2206ani )\u03b12(\u2206bni )\u03b13(\u2206\u03c3ni )\u03b14 ,\nT nj,\u03b3 := \u2211\ni\u2208Cj\nexp(\u03b2n0i)\n\u03b3! (\u2206\u03b2n1i) \u03b3 ,\nfor any (\u03b71, \u03b72) 6= (0d, 0) and |\u03b3| 6= 0d, while for (\u03b71, \u03b72) = (0d, 0) we set\nSni,0d,0 = \u2212T n i,0d\n:= \u2211\ni\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170i).\nAs a consequence, it follows that\nHn =\nK\u2211\nj=1\n2r\u0304(|Cj |)\u2211\n|\u03b71|+\u03b72=0 Snj,\u03b71,\u03b72 \u00b7X\n\u03b71 exp((\u03b2\u22171i) \u22a4X) \u00b7 \u2202\n\u03b72f\n\u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+\nK\u2211\nj=1\n1+1{|Cj |>1}\u2211\n|\u03b3|=0 T nj,\u03b3 \u00b7X\u03b3 exp((\u03b2n1i)\u22a4X)gGn(Y |X) +R5(X,Y ) +R6(X,Y ). (27)\nStep 2 - Non-vanishing coefficients:\nIn this step, we will prove by contradiction that at least one among the ratios Snj,\u03b71,\u03b72/D2(Gn, G\u2217) does not converge to zero as n \u2192 \u221e. Assume that all these terms go to zero, then by employing arguments for deriving equations (19) and (20), we get that\n1 D2(Gn, G\u2217) \u00b7 [ K\u2211\nj=1\n\u2223\u2223\u2223 \u2211\ni\u2208Cj exp(\u03b2n0i)\u2212 exp(\u03b2\u22170j)\n\u2223\u2223\u2223\n+ \u2211\nj:|Cj |=1\n\u2211 i\u2208Cj exp(\u03b2n0i) ( \u2016\u2206\u03b2n1ij\u2016+ \u2016\u2206anij\u2016+ |\u2206bnij|+ |\u2206\u03c3nij| )] \u2192 0.\nCombine this limit with the representation of D2(Gn, G\u2217), we have that 1\nD2(Gn, G\u2217) \u00b7\n\u2211\nj:|Cj|>1\n\u2211 i\u2208Cj exp(\u03b2n0i) ( \u2016\u2206\u03b2n1ij\u2016r\u0304(|Cj |) + \u2016\u2206anij\u2016 r\u0304(|Cj |) 2 + |\u2206bnij |r\u0304(|Cj |) + |\u2206\u03c3nij| r\u0304(|Cj |) 2 ) 6\u2192 0.\nThis result implies that we can find some index j\u2032 \u2208 [K] : |Cj\u2032 | > 1 that satisfies 1\nD2(Gn, G\u2217) \u00b7 \u2211\ni\u2208Cj\u2032 exp(\u03b2n0i)\n( \u2016\u2206\u03b2n1ij\u2032\u2016r\u0304(|Cj\u2032 |) + \u2016\u2206anij\u2032\u2016 r\u0304(|C j\u2032 |) 2 + |\u2206bnij\u2032 |r\u0304(|Cj\u2032 |) + |\u2206\u03c3nij\u2032 | r\u0304(|C j\u2032 |) 2 ) 6\u2192 0.\nFor simplicity, we may assume that j\u2032 = 1. Since Sn1,\u03b71,\u03b72/D2(Gn, G\u2217) vanishes as n \u2192 \u221e for any (\u03b71, \u03b72) \u2208 Nd \u00d7 N such that 1 \u2264 |\u03b71|+ \u03b72 \u2264 r\u0304(|Cj |), we divide this term by the left hand side of the above equation and achieve that\n\u2211 i\u2208C1 \u2211 \u03b1\u2208J\u03b71,\u03b72 exp(\u03b2n0i) 2\u03b14\u03b1! (\u2206\u03b2n1i1) \u03b11(\u2206ani1) \u03b12(\u2206bni1) \u03b13(\u2206\u03c3ni1) \u03b14\n\u2211 i\u2208C1 exp(\u03b2 n 0i) ( \u2016\u2206bni1\u2016r\u0304(|C1|) + \u2016\u2206ani1\u2016 r\u0304(|C1|) 2 + |\u2206bni1|r\u0304(|C1|) + |\u2206\u03c3ni1| r\u0304(|C1|) 2 ) \u2192 0, (28)\nfor any (\u03b71, \u03b72) \u2208 Nd \u00d7N such that 1 \u2264 |\u03b71|+ \u03b72 \u2264 r\u0304(|C1|).\nSubsequently, we define Mn := max{\u2016\u2206bni1\u2016, \u2016\u2206ani1\u20161/2, |\u2206bni1|, |\u2206\u03c3ni1|1/2 : i \u2208 C1} and pn := max{exp(\u03b2n0i) : i \u2208 C1}. As a result, the sequence exp(\u03b2n0i)/pn is bounded, which indicates that we can substitute it with its subsequence that admits a positive limit z25i := limn\u2192\u221e exp(\u03b2 n 0i)/pn. Therefore, at least one among the limits z25i equals to one. Furthermore, we also denote\n(\u2206\u03b2n1i1)/Mn \u2192 z1i, (\u2206ani1)/Mn \u2192 z2i, (\u2206bni1)/Mn \u2192 z3i, (\u2206\u03c3ni1)/(2Mn) \u2192 z4i.\nFrom the above definition, it follows that at least one among the limits z1i, z2i, z3i and z4i equals to either 1 or \u22121. By dividing both the numerator and the denominator of the term in equation (28), we arrive at the following system of polynomial equations:\n\u2211\ni\u2208C1\n\u2211\n\u03b1\u2208J\u03b71,\u03b72\nz25i z \u03b11 1i z \u03b12 2i z \u03b13 3i z \u03b14 4i\n\u03b11! \u03b12! \u03b13! \u03b14! = 0,\nfor all (\u03b71, \u03b72) \u2208 Nd \u00d7 N : 1 \u2264 |\u03b71| + \u03b72 \u2264 r\u0304(|C1|). Nevertheless, from the definition of r\u0304(|C1|), we know that the above system does not admit any non-trivial solutions, which is a contradiction. Consequently, not all the ratios Snj,\u03b71,\u03b72/D2(Gn, G\u2217) tend to zero as n goes to infinity.\nStep 3 - Fatou\u2019s contradiction:"
        },
        {
            "heading": "It follows from the hypothesis that EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))]/D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e. Then,",
            "text": "by applying the Fatou\u2019s lemma, we get\n0 = lim n\u2192\u221e EX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))] D2(Gn, G\u2217) = 1 2 \u00b7 \u222b lim inf n\u2192\u221e |gGn(Y |X)\u2212 gG\u2217(Y |X)| D2(Gn, G\u2217) dXdY,\nwhich implies that |gGn(Y |X)\u2212 gG\u2217(Y |X)|/D2(Gn, G\u2217) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ).\nNext, we define mn as the maximum of the absolute values of S n j,\u03b71,\u03b72/D2(Gn,G\u2217). It follows from Step 2 that 1/mn 6\u2192 \u221e. Moreover, by arguing in the same way as in Step 3 in Appendix A, we receive that\nHn/[mnD2(Gn, G\u2217)] \u2192 0 (29) as n \u2192 \u221e. By abuse of notations, let us denote\nSnj,\u03b71,\u03b72/[mnD2(Gn, G\u2217)] \u2192 \u03c4j,\u03b71,\u03b72 , T nj,\u03b3/[mnD2(Gn, G\u2217)] \u2192 \u03baj,\u03b3 .\nHere, at least one among \u03c4j,\u03b71,\u03b72 , \u03baj,\u03b3 is non-zero. Then, by putting the results in equations (27) and (29) together, we get\nK\u2211\ni=1\n2r\u0304(|Cj |)\u2211\n|\u03b71|+\u03b72=0 \u03c4i,\u03b71,\u03b72 \u00b7X\u03b71 exp((\u03b2\u22171i)\u22a4X)\n\u2202\u03b72f \u2202h\u03b721 (Y |(a\u2217i )\u22a4X + b\u2217i , \u03c3\u2217i )\n+ K\u2211\ni=1\n1+1{|Cj |>1}\u2211\n|\u03b3|=0 \u03bai,\u03b3 \u00b7X\u03b3 exp((\u03b2\u22171i)\u22a4X)gG\u2217(Y |X) = 0.\nArguing in a similar fashion as in Step 3 of Appendix A, we obtain that \u03c4j,\u03b71,\u03b72 = \u03baj,\u03b3 = 0 for any j \u2208 [K], 0 \u2264 |\u03b71|+ \u03b72 \u2264 2r\u0304(|Cj |) and 0 \u2264 |\u03b3| \u2264 1 + 1{|Cj |>1}. This contradicts the fact that at least one among them is non-zero. Hence, the proof is completed."
        },
        {
            "heading": "B.3 Proof of Proposition 1",
            "text": "Since the Hellinger distance is lower bounded by the Total Variation distance, i.e. h \u2265 V , it is sufficient to show that\ninf G\u2208Ok(\u2126)\nEX [V (gG(\u00b7|X), gG\u2217(\u00b7|X))] > 0.\nFor that purpose, we first demonstrate that\nlim \u03b5\u21920 inf G\u2208Ok(\u2126):D2(G,G\u2217)\u2264\u03b5\nEX [V (g(\u00b7|X), gG\u2217(\u00b7|X))] > 0. (30)\nAssume by contrary that the above claim is not true, then we can find a sequence of mixing measures Gn = \u2211kn i=1 exp(\u03b2 n 0i)\u03b4(\u03b2n1i,ani ,bni ,\u03c3ni ) \u2208 Ok(\u2126) that satisfies D2(Gn, G\u2217) \u2192 0 and\nEX [V (gGn(\u00b7|X), gG\u2217 (\u00b7|X))] \u2192 0\nwhen n tends to infinity. By applying the Fatou\u2019s lemma, we have\n0 = lim n\u2192\u221e\nEX [V (gGn(\u00b7|X), gG\u2217(\u00b7|X))]\n\u2265 1 2\n\u222b\nX\u00d7Y lim inf n\u2192\u221e\n|gGn(Y |X) \u2212 gG\u2217(Y |X)|d(X,Y ). (31)\nThe above results indicates that gGn(Y |X) \u2212 gG\u2217(Y |X) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). WLOG, we may assume that\nmax {\u21131,...,\u2113K}\nK\u2211\nj=1\n|C\u2113j | = |C1|+ |C2|+ . . . + |CK |.\nLet us consider X \u2208 X \u2217\u2113 , where \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Since D2(Gn, G\u2217) converges to zero, it follows that (\u03b2n1i, a n i , b n i , \u03c3 n i ) \u2192 (\u03b2\u22171j , a\u2217j , b\u2217j , \u03c3\u2217j ) and \u2211 i\u2208Cj exp(\u03b2 n 0i) \u2192 exp(\u03b2\u22170j) for any i \u2208 Cj and j \u2208 [k\u2217]. Thus, we must have that X \u2208 X \u2113 for some \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = C1 \u222a . . . \u222a CK . Otherwise, gGn(Y |X) \u2212 gG\u2217(Y |X) 6\u2192 0, which is a contradiction. However, as K < \u2211K j=1 |Cj |, the fact that {\u21131, . . . , \u2113K} = C1 \u222a . . . \u222a CK cannot occur. Therefore, we reach the claim in equation (30). Consequently, there exists some positive constant \u03b5\u2032 such that\ninf G\u2208Ok(\u2126):D2(G,G\u2217)\u2264\u03b5\u2032\nEX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] > 0.\nGiven the above result, it suffices to point out that\ninf G\u2208Ok(\u2126):D2(G,G\u2217)>\u03b5\u2032\nEX [V (gG(\u00b7|X), gG\u2217 (\u00b7|X))] > 0. (32)\nWe continue to use the proof by contradiction method here. In particular, assume that the inequality (32) does not hold, then there exists a sequence of mixing measures G\u2032n \u2208 Ok(\u2126) such that D2(G\u2032n, G\u2217) > \u03b5\u2032 and EX [V (gG\u2032n(\u00b7|X), gG\u2217(\u00b7|X))] \u2192 0. By invoking the Fatou\u2019s lemma as in equation (31), we get that gG\u2032n(Y |X) \u2212 gG\u2217(Y |X) \u2192 0 as n \u2192 \u221e for almost surely (X,Y ). Since \u2126 is a compact set, we can substitute (Gn) with its subsequence which converges to some mixing measure G\u2032 \u2208 Ok(\u2126). Then, the previous limit implies that gG\u2032(Y |X) = gG\u2217(Y |X) for almost surely (X,Y ). From the result of Proposition 2 in Appendix C, we know that the top-K sparse softmax gating Gaussian MoE is identifiable. Therefore, we obtain that G\u2032 \u2261 G\u2217, or equivalently, D2(G\u2032, G\u2217) = 0.\nOn the other hand, due to the hypothesis D2(G\u2032n, G\u2217) > \u03b5\u2032 for any n \u2208 N, we also get that D2(G\u2032, G\u2217) > \u03b5\u2032 > 0, which contradicts the previous result. Hence we reach the claim in equation (32) and totally completes the proof."
        },
        {
            "heading": "B.4 Proof of Lemma 2",
            "text": "Let \u03b7j = Mj\u03b5, where \u03b5 is some fixed positive constant and Mi will be chosen later. As X and \u2126 are bounded sets, we can find some constant c\u2217\u2113 \u2265 0 such that\nmin x,j,j\u2032\n[ (\u03b2\u22171j) \u22a4x\u2212 (\u03b2\u22171j\u2032)\u22a4x ] = c\u2217\u2113\u03b5,\nwhere the above minimum is subject to x \u2208 X \u2217\u2113 , j \u2208 {\u21131, . . . , \u2113K} and j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217}. By arguing similarly to the proof of Lemma 1 in Appendix A.3, we deduce that c\u2217\u2113 > 0.\nSince X is a bounded set, we may assume that \u2016x\u2016 \u2264 B for any x \u2208 X . Let x \u2208 X \u2217\u2113 and \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = C\u21131 \u222a . . . \u222a C\u2113K . Then, for any i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}, we have that\n\u03b2\u22a41ix = (\u03b21i \u2212 \u03b2\u22171j)\u22a4x+ (\u03b2\u22171j)\u22a4x \u2265 \u2212Mi\u03b5B + (\u03b2\u22171j\u2032)\u22a4x+ c\u2217\u2113\u03b5 = \u2212Mi\u03b5B + c\u2217\u2113\u03b5+ (\u03b2\u22171j\u2032 \u2212 \u03b21i\u2032)\u22a4x+ \u03b2\u22a41i\u2032x \u2265 \u22122Mi\u03b5B + c\u2217\u2113\u03b5+ \u03b2\u22a41i\u2032x,\nwhere j \u2208 {\u21131, . . . , \u2113K} and j\u2032 \u2208 {\u2113K+1, . . . , \u2113k\u2217} such that i \u2208 Cj and i\u2032 \u2208 Cj\u2032 . If Mj \u2264 c\u2217\u2113 2B , then we get that x \u2208 X\u2113, which leads to X \u2217\u2113 \u2286 X \u2113.\nAnalogously, assume that there exists some constant c\u2113 \u2265 0 such that\nmin x,j,j\u2032\n[ (\u03b2\u22171j) \u22a4x\u2212 (\u03b2\u22171j\u2032)\u22a4x ] = c\u2217\u2113\u03b5,\nwhere the minimum is subject to x \u2208 X \u2113, i \u2208 {\u21131, . . . , \u2113K} and i\u2032 \u2208 {\u2113K+1, . . . , \u2113k}. Then, if Mj \u2264\nc\u2113 2B , then we receive that X \u2113 \u2286 X \u2217\u2113 .\nAs a consequence, by setting Mj = 1 2B min{c\u2217\u2113 , c\u2113}, we achieve the conclusion that X \u2113 = X \u2217\u2113 .\nC Identifiability of the Top-K Sparse Softmax Gating Gaussian\nMixture of Experts\nIn this appendix, we study the identifiability of the top-K sparse softmax gating Gaussian MoE, which plays an essential role in ensuring the convergence of the MLE G\u0302n to the true mixing measure G\u2217 under Voronoi loss functions.\nProposition 2 (Identifiability). Let G and G\u2032 be two arbitrary mixing measures in Ok(\u0398). Suppose that the equation gG(Y |X) = gG\u2032(Y |X) holds for almost surely (X,Y ) \u2208 X \u00d7Y, then it follows that G \u2261 G\u2032.\nProof of Proposition 2. First, we assume that two mixing measures G and G\u2032 take the following forms: G = \u2211k i=1 exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) and G \u2032 = \u2211k\u2032 i=1 exp(\u03b2 \u2032 0i)\u03b4(\u03b2\u20321i,a\u2032i,b\u2032i,\u03c3\u2032i). Recall that gG(Y |X) = gG\u2032(Y |X) for almost surely (X,Y ), then we have k\u2211\ni=1\nSoftmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) \u00b7 f(Y |a\u22a4i X + bi, \u03c3i)\n=\nk\u2032\u2211\ni=1\nSoftmax(TopK((\u03b2\u20321i) \u22a4X,K;\u03b2\u20320i)) \u00b7 f(Y |(a\u2032i)\u22a4 + b\u2032i, \u03c3\u2032i). (33)\nDue to the identifiability of the location-scale Gaussian mixtures [31, 32, 33], we get that k = k\u2032 and { Softmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) : i \u2208 [k] } \u2261 { Softmax(TopK((\u03b2\u20321i) \u22a4X,K;\u03b2\u20320i)) : i \u2208 [k] } ,\nfor almost surely X. WLOG, we may assume that\nSoftmax(TopK((\u03b21i) \u22a4X,K;\u03b20i)) = Softmax(TopK((\u03b2 \u2032 1i) \u22a4X,K;\u03b2\u20320i)), (34)\nfor almost surely X for any i \u2208 [k]. Since the Softmax function is invariant to translations, it follows from equation (34) that \u03b21i = \u03b2 \u2032 1i + v1 and \u03b20i = \u03b2 \u2032 0i + v0 for some v1 \u2208 Rd and v0 \u2208 R. Notably, from the assumption of the model, we have \u03b21k = \u03b2 \u2032 1k = 0d and \u03b20k = \u03b2 \u2032 0k = 0, which implies that v1 = 0d and v2 = 0. As a result, we obtain that \u03b21i = \u03b2 \u2032 1i and \u03b20i = \u03b2 \u2032 0i for any i \u2208 [k].\nLet us consider X \u2208 X\u2113 where \u2113 \u2208 [q] such that {\u21131, . . . , \u2113K} = {1, . . . ,K}. Then, equation (33) can be rewritten as\nK\u2211\ni=1\nexp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |a\u22a4i X + bi, \u03c3i) =\nK\u2211\ni=1\nexp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |(a\u2032i)\u22a4X + b\u2032i, \u03c3\u2032i), (35)\nfor almost surely (X,Y ). Next, we denote J1, J2, . . . , Jm as a partition of the index set [k], where m \u2264 k, such that exp(\u03b20i) = exp(\u03b20i\u2032) for any i, i\u2032 \u2208 Jj and j \u2208 [m]. On the other hand, when i and i\u2032 do not belong to the same set Jj , we let exp(\u03b20i) 6= exp(\u03b20i\u2032). Thus, we can reformulate equation (35) as\nm\u2211\nj=1\n\u2211 i\u2208Jj exp(\u03b20i) exp(\u03b2 \u22a4 1iX)f(Y |a\u22a4i X + bi, \u03c3i) = m\u2211 j=1 \u2211 i\u2208Jj exp(\u03b2\u22a41iX)f(Y |(a\u2032i)\u22a4X + b\u2032i, \u03c3\u2032i),\nfor almost surely (X,Y ). This results leads to {((ai)\u22a4X + bi, \u03c3i) : i \u2208 Jj} \u2261 {((a\u2032i)\u22a4X + b\u2032i, \u03c3\u2032i) : i \u2208 Jj}, for almost surely X for any j \u2208 [m]. Therefore, we have\n{(ai, bi, \u03c3i) : i \u2208 Jj} \u2261 {(a\u2032i, b\u2032i, \u03c3\u2032i) : i \u2208 Jj},\nfor any j \u2208 [m]. As a consequence,\nG = m\u2211\nj=1\n\u2211 i\u2208Jj exp(\u03b20i)\u03b4(\u03b21i,ai,bi,\u03c3i) = m\u2211 j=1 \u2211 i\u2208Jj exp(\u03b2\u20320i)\u03b4(\u03b2\u20321i,a\u2032i,b\u2032i,\u03c3\u2032i) = G \u2032.\nHence, we reach the conclusion of this proposition."
        }
    ],
    "title": "Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts",
    "year": 2023
}