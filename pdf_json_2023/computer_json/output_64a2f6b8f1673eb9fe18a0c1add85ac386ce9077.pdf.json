{
    "abstractText": "We generalize the class vectors found in neural networks to linear subspaces (i.e. points in the Grassmann manifold) and show that the Grassmann Class Representation (GCR) enables the simultaneous improvement in accuracy and feature transferability. In GCR, each class is a subspace and the logit is defined as the norm of the projection of a feature onto the class subspace. We integrate Riemannian SGD into deep learning frameworks such that class subspaces in a Grassmannian are jointly optimized with the rest model parameters. Compared to the vector form, the representative capability of subspaces is more powerful. We show that on ImageNet-1K, the top-1 error of ResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced by 5.6%, 4.5%, 3.0% and 3.5%, respectively. Subspaces also provide freedom for features to vary and we observed that the intra-class feature variability grows when the subspace dimension increases. Consequently, we found the quality of GCR features is better for downstream tasks. For ResNet50-D, the average linear transfer accuracy across 6 datasets improves from 77.98% to 79.70% compared to the strong baseline of vanilla softmax. For Swin-T, it improves from 81.5% to 83.4% and for Deit3, it improves from 73.8% to 81.4%. With these encouraging results, we believe that more applications could benefit from the Grassmann class representation. Code is released at https://github.com/innerlee/GCR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoqi Wang"
        },
        {
            "affiliations": [],
            "name": "Zhizhong Li"
        },
        {
            "affiliations": [],
            "name": "Wayne Zhang"
        }
    ],
    "id": "SP:e476e6cc68f30bdc2407bd138a0b6fc55c0f151b",
    "references": [
        {
            "authors": [
                "P-A Absil",
                "Robert Mahony",
                "Rodolphe Sepulchre"
            ],
            "title": "Optimization algorithms on matrix manifolds. In Optimization Algorithms on Matrix Manifolds",
            "year": 2009
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Amar Shah",
                "Yoshua Bengio"
            ],
            "title": "Unitary evolution recurrent neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Nitin Bansal",
                "Xiaohan Chen",
                "Zhangyang Wang"
            ],
            "title": "Can we gain more from orthogonality regularizations in training deep networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Gary Becigneul",
                "Octavian-Eugen Ganea"
            ],
            "title": "Riemannian adaptive optimization methods",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Lucas Beyer",
                "Olivier J H\u00e9naff",
                "Alexander Kolesnikov",
                "Xiaohua Zhai",
                "A\u00e4ron van den Oord"
            ],
            "title": "Are we done with imagenet",
            "venue": "arXiv preprint arXiv:2006.07159,",
            "year": 2020
        },
        {
            "authors": [
                "Silvere Bonnabel"
            ],
            "title": "Stochastic gradient descent on riemannian manifolds",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2013
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101\u2013mining discriminative components with random forests",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "John S Bridle"
            ],
            "title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition",
            "venue": "In Neurocomputing: Algorithms, architectures and applications,",
            "year": 1990
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Tongliang Liu",
                "Mingming Gong",
                "Stefanos Zafeiriou"
            ],
            "title": "Sub-center arcface: Boosting face recognition by large-scale noisy web faces",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Niannan Xue",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Arnout Devos",
                "Matthias Grossglauser"
            ],
            "title": "Regression networks for meta-learning few-shot classification",
            "venue": "ICML Workshop on Automated Machine Learning (AutoML 2020),",
            "year": 2020
        },
        {
            "authors": [
                "Alan Edelman",
                "Tom\u00e1s A Arias",
                "Steven T Smith"
            ],
            "title": "The geometry of algorithms with orthogonality constraints",
            "venue": "SIAM journal on Matrix Analysis and Applications,",
            "year": 1998
        },
        {
            "authors": [
                "Jihun Hamm",
                "Daniel D Lee"
            ],
            "title": "Grassmann discriminant analysis: a unifying view on subspace-based learning",
            "venue": "In Proceedings of the 25th international conference on Machine learning,",
            "year": 2008
        },
        {
            "authors": [
                "Mehrtash Harandi",
                "Basura Fernando"
            ],
            "title": "Generalized backpropagation, \u00e9tude de cas: Orthogonality",
            "venue": "arXiv preprint arXiv:1611.05927,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Tong He",
                "Zhi Zhang",
                "Hang Zhang",
                "Zhongyue Zhang",
                "Junyuan Xie",
                "Mu Li"
            ],
            "title": "Bag of tricks for image classification with convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Hiroyuki Kasai",
                "Pratik Jawanpuria",
                "Bamdev Mishra"
            ],
            "title": "Riemannian adaptive stochastic gradient algorithms on matrix manifolds",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Simon Kornblith",
                "Ting Chen",
                "Honglak Lee",
                "Mohammad Norouzi"
            ],
            "title": "Why do better loss functions lead to less transferable features",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Krause",
                "Jia Deng",
                "Michael Stark",
                "Li Fei-Fei"
            ],
            "title": "Collecting a large-scale dataset of fine-grained cars",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Mario Lezcano Casado"
            ],
            "title": "Trivializations for gradient-based optimization on manifolds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mario Lezcano-Casado",
                "David Mart\u0131nez-Rubio"
            ],
            "title": "Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zhizhong Li",
                "Deli Zhao",
                "Zhouchen Lin",
                "Edward Y. Chang"
            ],
            "title": "Determining step sizes in geometric optimization algorithms",
            "venue": "In 2015 IEEE International Symposium on Information Theory (ISIT),",
            "year": 2015
        },
        {
            "authors": [
                "Zhizhong Li",
                "Deli Zhao",
                "Zhouchen Lin",
                "Edward Y. Chang"
            ],
            "title": "A new retraction for accelerating the riemannian three-factor low-rank matrix completion algorithm",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Bamdev Mishra",
                "Rodolphe Sepulchre"
            ],
            "title": "R3mc: A riemannian three-factor algorithm for low-rank matrix completion",
            "venue": "In 53rd IEEE Conference on Decision and Control,",
            "year": 2014
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey E Hinton"
            ],
            "title": "When does label smoothing help",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Maximillian Nickel",
                "Douwe Kiela"
            ],
            "title": "Learning continuous hierarchies in the lorentz model of hyperbolic geometry",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "Sixth Indian Conference on Computer Vision, Graphics & 9 Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Madhav Nimishakavi",
                "Pratik Kumar Jawanpuria",
                "Bamdev Mishra"
            ],
            "title": "A dual framework for low-rank tensor completion",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Mete Ozay",
                "Takayuki Okatani"
            ],
            "title": "Training cnns with normalized kernels",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Vardan Papyan",
                "XY Han",
                "David L Donoho"
            ],
            "title": "Prevalence of neural collapse during the terminal phase of deep learning training",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Haozhi Qi",
                "Chong You",
                "Xiaolong Wang",
                "Yi Ma",
                "Jitendra Malik"
            ],
            "title": "Deep isometric learning for visual recognition",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Qi Qian",
                "Lei Shang",
                "Baigui Sun",
                "Juhua Hu",
                "Hao Li",
                "Rong Jin"
            ],
            "title": "Softtriple loss: Deep metric learning without triplet sampling",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Soumava Kumar Roy",
                "Mehrtash Harandi",
                "Richard Nock",
                "Richard Hartley"
            ],
            "title": "Siamese networks: The tale of two manifolds",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Soumava Kumar Roy",
                "Zakaria Mhammedi",
                "Mehrtash Harandi"
            ],
            "title": "Geometry aware constrained optimization techniques for deep learning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Christian Simon",
                "Piotr Koniusz",
                "Richard Nock",
                "Mehrtash Harandi"
            ],
            "title": "Adaptive subspaces for few-shot learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Deit iii: Revenge of the vit",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Andrea Vedaldi",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Fixing the train-test resolution discrepancy",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Haoqi Wang",
                "Zhizhong Li",
                "Litong Feng",
                "Wayne Zhang"
            ],
            "title": "Vim: Out-of-distribution with virtual-logit matching",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiayun Wang",
                "Yubei Chen",
                "Rudrasis Chakraborty",
                "Stella X Yu"
            ],
            "title": "Orthogonal convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Satosi Watanabe",
                "Nikhil Pakvasa"
            ],
            "title": "Subspace method of pattern recognition",
            "venue": "In Proc. 1st. IJCPR,",
            "year": 1973
        },
        {
            "authors": [
                "Ross Wightman",
                "Hugo Touvron",
                "Herve Jegou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Di Xie",
                "Jiang Xiong",
                "Shiliang Pu"
            ],
            "title": "All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yaodong Yu",
                "Kwan Ho Ryan Chan",
                "Chong You",
                "Chaobing Song",
                "Yi Ma"
            ],
            "title": "Learning diverse and discriminative representations via the principle of maximal coding rate reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tong Zhang",
                "Pan Ji",
                "Mehrtash Harandi",
                "Richard Hartley",
                "Ian Reid"
            ],
            "title": "Scalable deep k-subspace clustering",
            "venue": "In Asian Conference on Computer Vision,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The scheme deep feature\u2192fully-connected \u2192softmax\u2192cross-entropy loss has been the standard practice in deep classification networks. Columns of the weight parameter in the fully-connected layer are the class representative vectors and serve as the prototype for classes. The vector class representation has achieved huge\n* Equal contribution. Work is done when Haoqi was at SenseTime. \u2020 Corresponding author: Wayne Zhang.\nsuccess, yet it is not without imperfections. In the study of transferable features, researchers noticed a dilemma that representations with higher classification accuracy lead to less transferable features for downstream tasks [19]. This is connected to the fact that they tend to collapse intra-class variability of features, resulting in loss of information in the logits about the resemblances between instances of different classes [29]. The neural collapse phenomenon [34] indicates that as training progresses, the intra-class variation becomes negligible, and features collapse to their class means. As such, this dilemma inherently originates from the practice of representing classes by a single vector. This motivates us to study representing classes by high-dimensional subspaces.\nRepresenting classes as subspaces in machine learning can be dated back, at least, to 1973 [49]. This core idea is reemerging recently in various contexts such as clustering [54], few-shot classification [12, 41] and out-of-distribution detection [47], albeit in each case a different concrete instantiation was proposed. However, very few works study the subspace representation in large-scale classification, a fundamental computer vision task that benefits numerous downstream tasks. We propose the Grassmann Class Representation (GCR) to fill this gap and study its impact on classification and feature transferability via extensive experiments. To be specific, each class i is associated with a linear subspace Si, and for any feature vector x, the i-th logit li is defined as the norm of its projection onto the subspace Si,\nli := \u2225\u2225projSix\u2225\u2225 . (1)\nIn the following, we answer the two critical questions,\n1. How to effectively optimize the subspaces in training? 2. Is Grassmann class representation useful?\nSeveral drawbacks and important differences in previous works make their methodologies hard to generalize to the large-scale classification problem. Firstly, their subspaces might be not learnable. In ViM [47], DSN [41] and the SVD formulation of [54], subspaces are obtained post hoc\nar X\niv :2\n30 8.\n01 54\n7v 1\n[ cs\n.C V\n] 3\nA ug\n2 02\nby PCA-like operation on feature matrices without explicit parametrization and learning. Secondly, for works with learnable subspaces, their learning procedure for subspaces might not apply. For example, in RegressionNet [12], the loss involves pairwise subspace orthogonalization, which does not scale when the number of classes is large because the computational cost will soon be infeasible. And thirdly, the objective of [54] is unsupervised subspace clustering, which needs substantial changes to adapt to classification.\nIt is well known that the set of k-dimensional linear subspaces form a Grassmann manifold, so finding the optimal subspace representation for classes is to optimize on the Grassmannian. Therefore, a natural solution to Question 1 is to use geometric optimization [13], which optimizes the objective function under the constraint of a given manifold. Points being optimized are moving along geodesics instead of following the direction of Euclidean gradients. We implemented an efficient Riemannian SGD for optimization in the Grassmann manifold in Algorithm 1, which integrates the geometric optimization into deep learning frameworks so that the subspaces in Grassmannian and the model weights in Euclidean are jointly optimized.\nThe Grassmann class representation sheds light on the incompatibility issue between accuracy and transferability. Features can vary in a high-dimensional subspace without harming the accuracy. We empirically verify this speculation in Section 5, which involves both CNNs (ResNet [16], ResNet-D [17], ResNeXt [52], VGG13-BN [42]) and vision transformers (Swin [26] and Deit3 [45]). We found that with larger subspace dimensions k, the intra-class variation increase, and the feature transferability improve. The classification performance of GCR is also superior to the vector form. For example, on ImageNet-1K, the top-1 error rates of ResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced relatively by 5.6%, 4.5%, 3.0%, and 3.5%, respectively.\nTo summarize, our contributions are three folds. (1) We propose the Grassmann class representation and learn the subspaces jointly with other network parameters with the help of Riemannian SGD. (2) We showed its superior accuracy on large-scale classification both for CNNs and vision transformers. (3) We showed that features learned by the Grassmann class representation have better transferability."
        },
        {
            "heading": "2. Related Work",
            "text": "Geometric Optimization [13] developed the geometric Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds in their seminal paper. Riemannian SGD was introduced in [6] with an analysis on convergence and there are variants such as Riemannian SGD with momentum [40] or adaptive [18]. Other popular Euclidean optimization methods such as Adam are also studied in the Riemannian manifold context [4]. [23] study the special case of SO(n) and U(n) and uses the exponential map to enable\nEuclidean optimization methods for Lie groups. The idea was generalized into trivialization in [22]. Our Riemannian SGD Algorithm 1 is tailored for Grassmannian, so we use the closed-form equation for geodesics. Applications of geometric optimization include matrix completion [27, 25, 24, 32], hyperbolic taxonomy embedding [30], to name a few. [14] proposed the Grassmann discriminant analysis, in which features are modeled as linear subspaces.\nOrthogonal Constraints Geometric optimization in deep learning is mainly used for providing orthogonal constraints in the design of network structure [15, 33], aiming to mitigate the gradient vanishing or exploding problems. Orthogonality are also enforced via regularizations [2, 51, 3, 37, 48]. Contrastingly, we do not change the network structures, and focus ourselves on the subspace form of classes. SiNN [39] uses the Stiefel manifold to construct Mahalanobis distance matrices in Siamese networks to improve embeddings in metric learning. It does not have the concept of classes.\nImproving Feature Diversity Our GCR favors the intraclass feature variation by providing a subspace to vary. There are other efforts to encourage feature diversity. SoftTriplet loss [38] and SubCenterArcFace [10] model each class as local clusters with several centers or sub-centers. [55] uses a global orthogonal regularization to drive local descriptors spread out in the features space. [53] proposes to learn low-dimensional structures from the maximal coding rate reduction principle. The subspaces are estimated using PCA on feature vectors after the training.\nClasses as Subspaces ViM [47] uses a subspace to denote the out-of-distribution class, which is obtained via PCA-like postprocessing after training. kSCN [54] uses subspaces to model clusters in unsupervised learning. Parameters of models and subspaces are optimized alternatively in a wakeand-sleep fashion. CosineSoftmax [19] defines logits via the inner product between the feature and normalized class vector. Since the class vector is normalized to be unit length, it is regarded as representing the class as a 1-dimensional subspace. ArcFace [11] improves over cosine softmax by adding angular margins to the loss. RegressionNet [12] uses the subspace spanned by the K feature vectors of each class in the N -way K-shot classification. The computational cost of its pairwise subspace orthogonalization loss is quadratic w.r.t. the number of classes and becomes infeasible when the number of classes is large. DSN [41] for few-shot learning computed subspaces from the data matrix rather than parametrized and learned, and its loss also involves pairwise class comparison which does not scale. Different from these formulations, we explicitly parametrize classes as highdimensional subspaces and use geometric optimization to learn them in supervised learning."
        },
        {
            "heading": "3. Preliminaries",
            "text": "In this section, we briefly review the essential concepts in geometric optimization. Detailed exposition can be found in [13, 1]. Given an n-dimensional Euclidean space Rn, the set of k-dimensional linear subspaces forms the Grassmann manifold G(k, n). A computational-friendly representation for subspace S \u2208 G(k, n) is an orthonormal matrix S \u2208 Rn\u00d7k, where STS = Ik and Ik is the k \u00d7 k identity matrix. Columns of the matrix S can be interpreted as an orthonormal basis for the subspace S. The matrix form is not unique, as right multiplying an orthonormal matrix will produce a new matrix representing the same subspace. Formally, Grassmannian is a quotient space of the Stiefel manifold and the orthogonal group G(k, n) = St(k, n)/O(k), where St(k, n) = {X \u2208 Rn\u00d7k|XTX = Ik} and O(k) = {X \u2208 Rk\u00d7k|XTX = Ik}. When the context is clear, we use the space S and one of its matrix forms S interchangeably.\nGiven a function f : G(k, n) \u2192 R defined on the Grassmann manifold, the Riemannian gradient of f at point S \u2208 G(k, n) is given by [13, Equ. (2.70)],\n\u2207f(S) = fS \u2212 SST fS , (2)\nwhere fS is the Euclidean gradient with elements (fS)ij = \u2202f \u2202Sij . When performing gradient descend on the Grassmann manifold, and suppose the current point is S and the current Riemannian gradient is G, then the next point is the endpoint of S moving along the geodesic toward the tangent G with step size t. The geodesic is computed by [13, Equ. (2.65)],\nS(t) = (SV cos(t\u03a3) +U sin(t\u03a3))V T , (3)\nwhere U\u03a3V T = G is the thin SVD of G."
        },
        {
            "heading": "4. Learning Grassmann Class Representation",
            "text": "Denote the weight of the last fully-connected (fc) layer in a classification network by W \u2208 Rn\u00d7C and the bias by b \u2208 RC , where n is the dimension of features and C is the number of classes. The i-th column vector wi of W is called the i-th class representative vector. The i-th logit is computed as the inner product between a feature x and the class vector (and optionally offset by a bias bi), namely wTi x+ bi. We extend this well-established formula to a multi-dimensional subspace form li :=\n\u2225\u2225projSix\u2225\u2225 where Si \u2208 G(k, n) is a k-dimensional subspace in the n-dimensional feature space. We call Si the i-th class representative space, or class space in short. Comparing the new logit to the standard one, the inner product of feature x with class vector is replaced by the norm of the subspace projection projSix and the bias term is omitted. We found that normalizing features to a constant length \u03b3 improves training. Incorporating this, Equ. (1) becomes\nli := \u2225\u2225\u2225\u2225projSi \u03b3x\u2225x\u2225 \u2225\u2225\u2225\u2225 . (4)\nWe assume x has been properly normalized throughout this paper so that we can simply use Equ. (1) in the discussion. We call this formulation of classes and logits the Grassmann Class Representation (GCR).\nThe subspace class formulation requires two changes to an existing network. Firstly, the last fc layer is replaced by the Grassmann fully-connected layer, which transforms features to logits using Equ. (4). Details can be found in Section 4.1. Secondly, the optimizer is extended to process the new geometric layer, which is explained in Section 4.2. Ultimately, parameters of the geometric layer are optimized using Riemannian SGD, while other parameters are simultaneously optimized using SGD, AdamW, or Lamb, etc."
        },
        {
            "heading": "4.1. Grassmann Class Representation",
            "text": "Suppose for class i \u2208 {1, 2, . . . , C}, its subspace representation is Si \u2208 G(ki, n), where the dimension ki is a hyperparameter and is fixed during training. The tuple of subspaces (S1, S2, . . . , SC) will be optimized in the product space G(k1, n)\u00d7G(k2, n)\u00d7\u00b7 \u00b7 \u00b7\u00d7G(kC , n). Denote a matrix instantiation of Si as Si \u2208 Rn\u00d7k, where the column vectors form an orthonormal basis of Si, then we concatenate these matrices into a big matrix\nS = [S1 S2 \u00b7 \u00b7 \u00b7 SC ] \u2208 Rn\u00d7(k1+k2+\u00b7\u00b7\u00b7+kC). (5)\nThe matrix S consists of the parameters that are optimized numerically. For a feature x, the product STi x gives the coordinate of projSix under the orthonormal basis formed by the columns of Si. By definition in Equ. (1), the logit for class i and the (normalized) feature x is\nli = \u2225\u2225projSix\u2225\u2225 = \u2225\u2225STi x\u2225\u2225 . (6)\nGrassmann Fully-Connected Layer We implement the geometric fully-connected layer using the plain old fc layer. The shape of the weight S is n\u00d7 (k1 + k2 + \u00b7 \u00b7 \u00b7+ kC), as shown in Equ. (5). In the forward pass, the input feature is multiplied with the weight matrix to get a temporary vector t = STx, then the first element of the output is the norm of the sub-vector (t1, . . . , tk1), and the second element of the output is the norm of (tk1+1, tk1+2, . . . , tk1+k2), and so on. If all ki\u2019s be the same value k, as in our experiments, then the computation can be conveniently paralleled in one batch using tensor computation libraries.\nParameter Initialization Each matrix instantiation of the subspace should be initialized as an orthonormal matrix. To be specific, each block Si of the weight S in Equ. (5) is orthonormal, while the matrix S needs not be orthonormal. For each block Si, we first fill them with standard Gaussian noises and then use qf(Si), namely the Q factor of its QR decomposition, to transform it to an orthonormal matrix. The geometric optimization Algorithm 1 will ensure their orthonormality during training."
        },
        {
            "heading": "4.2. Optimize the Subspaces",
            "text": "Geometric optimization is to optimize functions defined on manifolds. The key is to find the Riemannian gradient w.r.t. the loss function and then descend along the geodesic. Here the manifold in concern is the Grassmannian G(k, n). As an intuitive example, G(1, 2), composed of all lines passing through the origin in a two-dimensional plane, can be pictured as a unit circle where each point on it denotes the line passing through that point. Antipodal points represent the same line. To illustrate how geometric optimization works, we define a toy problem on G(1, 2) that maximizes the norm of the projection of a fixed vector x0 onto a line through the origin, namely maxS\u2208G(1,2) \u2225projSx0\u2225.\nAs shown in Fig. 1, we represent S with a unit vector w \u2208 S. Suppose at step t, the current point is w(t), then it is easy to compute that the Euclidean gradient at w(t) is d = x0, and the Riemannian gradient g is the Euclidean gradient d projected to the tangent space of G(1, 2) at point w(t). The next iterative point w(t+1) is to move w(t) along the geodesic toward the direction g. Without geometric optimization, the next iterative point would have lied at w(t) + \u03b3d, jumping outside of the manifold.\nThe following proposition computes the Riemannian gradient for the subspace in Equ. (1).\nProposition 1. Let S \u2208 Rn\u00d7k be a matrix instantiation of subspace S \u2208 G(k, n), and x \u2208 Rn is a vector in Euclidean space, then the Riemannian gradient G of l(S,x) = \u2225projSx\u2225 w.r.t. S is\nG = 1\nl (In \u2212 SST )xxTS. (7)\nProof. Rewrite \u2225projSx\u2225 = \u221a xTSSTx, and compute the Euclidean derivatives as\n\u2202l\n\u2202S =\n1 l xxTS, \u2202l \u2202x = 1 l SSTx. (8)\nThen Equ. (7) follows from Equ. (2).\nAlgorithm 1 An Iteration of the Riemannian SGD with Momentum for Grassmannian at Iteration t Input: Learning rate \u03c4 > 0, momentum \u00b5 \u2208 [0, 1), Grass-\nmannian weight matrix S(t) \u2208 Rn\u00d7k, momentum buffer M (t\u22121) \u2208 Rn\u00d7k, Euclidean gradient D \u2208 Rn\u00d7k.\n1: Riemannian gradient by Equ. (2), G\u2190 (In \u2212 SST )D. 2: Approximately parallel transport M to the tangent space\nof current point S(t) by projection\nM \u2190 (In \u2212 SST )M (t\u22121). (10)\n3: Update momentum M (t) \u2190 \u00b5M +G. 4: Move along geodesic using Equ. (3). If U\u03a3V T =\nM (t) is the thin SVD, then S(t+1) \u2190 ( S(t)V cos(\u03c4\u03a3) +U sin(\u03c4\u03a3) ) V T .\n5: (Optional) Orthogonalization S(t+1) \u2190 qf(S(t+1)).\nWe give a geometric interpretation of Proposition 1. Let w1 be the unit vector along direction projSx, then expand it to an orthonormal basis of S, say {w1,w2, . . . ,wk}. Since the Riemannian gradient is invariant to matrix instantiation, we can set S = [w1 w2 \u00b7 \u00b7 \u00b7 wk]. Then Equ. (7) becomes\nG = [ (In \u2212 SST )x 0 \u00b7 \u00b7 \u00b7 0 ] , (9)\nsince wi \u22a5 x, i = 2, 3, . . . , k and wT1 x = l. Equ. (9) shows that in the single-sample case, only one basis vector w1, the unit vector in S that is closest to x, needs to be rotated towards vector x.\nRiemannian SGD Parameters of non-geometric layers are optimized as usual using traditional optimizers such as SGD, AdamW, or Lamb during training. For the geometric Grassmann fc layer, its parameters are optimized using the Riemannian SGD (RSGD) algorithm. The pseudo-code of our implementation of RSGD with momentum is described in Algorithm 1. We only show the code for the single-sample, single Grassmannian case. It is trivial to extend them to the batch version and the product of Grassmannians. In step 2, we use projection to approximate the parallel translation of momentum, and the momentum update formula in step 3 is adapted from the official PyTorch implementation of SGD. Weight decay does not apply here since spaces are scaleless. Note that step 5 is optional since S(t+1) in theory should be orthonormal. In practice, to suppress the accumulation of numerical inaccuracies, we do an extra orthogonalization step using qf(\u00b7) every 5 iterations. Algorithm 1 works seamlessly with traditional Euclidean optimizers and converts the gradient from Euclidean to Riemannian on-the-fly for geometric parameters."
        },
        {
            "heading": "5. Experiment",
            "text": "In this section, we empirically study the influence of the Grassmann class representation under different settings. In Section 5.1, GCR demonstrates superior performance on the large-scale ImageNet-1K classification, a fundamental vision task. We experimented with both CNNs and vision transformers and observed consistent improvements. Then, in Section 5.2, we show that GCR improves the feature transferability by allowing larger intra-class variation. The choice of hyper-parameters and design decisions are studied in Section 5.3. Extra supportive experiments are presented in the supplementary material.\nExperiment Settings For baseline methods, unless stated otherwise, we use the same training protocols (including the choice of batch size, learning rate policy, augmentation, optimizer, loss, and epochs) as in their respective papers. The input size is 224\u00d7 224 for all experiments, and checkpoints with the best validation scores are used. All codes, including the implementation of our algorithm and re-implementations of the compared baselines, are implemented based on the mmclassification [28] package. PyTorch [36] is used as the training backend and each experiment is run on 8 NVIDIA Tesla V100 GPUs using distributed training.\nNetworks for the Grassmann class representation are set up by the drop-in replacement of the last linear fc layer in baseline networks with a Grassmann fc layer. The training protocol is kept the same as the baseline whenever possible. One necessary exception is to enhance the optimizer (e.g., SGD, AdamW or Lamb) with RSGD (i.e., RSGD+SGD, RSGD+AdamW, RSGD+Lamb) to cope with Grassmannian layers. To reduce the number of hyper-parameters, we simply set the subspace dimension k to be the same for all classes and we use k = 8 throughout this section unless otherwise specified. Suppose the dimension of feature space is n, then the Grassmann fully-connected layer has the geometry of \u03a01000i=1 G(8, n). For hyper-parameters, we set \u03b3 = 25. Experiments with varying k\u2019s can be found in Section 5.2 and experiments on tuning \u03b3 are discussed in Section 5.3."
        },
        {
            "heading": "5.1. Improvements on Classification Accuracy",
            "text": "We apply Grassmann class representation to the largescale classification task. The widely used ImageNet-1K [9] dataset, containing 1.28M high-resolution training images and 50K validation images, is used to evaluate classification performances. Experiments are organized into three groups which support the following observations. (1) It has superior performance compared with different ways of representing classes. (2) Grassmannian improves accuracy on different network architectures, including CNNs and the latest vision transformers. (3) It also improves accuracy on different training strategies for the same architecture.\nOn Representing Classes In this group, we compare seven alternative ways to represent classes. (1) Softmax [8] is the plain old vector class representation using the fc layer to get logits. (2) CosineSoftmax [19] represents a class as a 1-dimensional subspace since the class vector is normalized to be unit length. We set the scale parameter to 25 and do not add a margin. (3) ArcFace [11] improves over cosine softmax by adding angular margins to the loss. The default setting (s = 64,m = 0.5) is used. (4) MultiFC is an ensemble of independent fc layers. Specifically, we add 8 fc heads to the network. These fc layers are trained side by side, and their losses are then averaged. When testing, the logits are first averaged, and then followed by softmax to output the ensembled prediction. (5) SoftTriple [38] models each class by 8 centers. The weighted average of logits computed from multiple class centers is used as the final logit. We use the recommended parameters (\u03bb = 20, \u03b3 = 0.1, \u03c4 = 0.2 and \u03b4 = 0.01) from the paper. (6) SubCenterArcFace [10] improves over ArcFace by using K sub-centers for each class and in training only the center closest to a sample is activated. We set K = 8 and do not drop sub-centers or samples since ImageNet is relatively clean. (7) The last setting is our GCR with subspace dimension k = 8. For all seven settings ResNet50-D is used as the backbone network and all models are trained on ImageNet-1K using the same training strategy described in the second row of Tab. 2.\nResults are listed in Tab. 1, from which we find that the Grassmann class representation is most effective. Compared with the vector class representation of vanilla softmax, the top-1 accuracy improves from 78.04% to 79.26%, which amounts to 5.6% relative error reduction. Compared with previous ways of 1-dimensional subspace representation, i.e. CosineSoftmax and ArcFace, our GCR improves the top-1 accuracy by 0.96% and 2.60%, respectively. Compared with the ensemble of multiple fc, the top-1 is improved by 1.92%. Interestingly, simply extending the class representation to multiple centers such as SoftTriple (75.55%) and SubCenterArcFace (77.10%) does not result in good performances when training from scratch on the ImageNet-1K dataset. SoftTriple was designed for fine-grained classification and\nTable 2: Comparing Grassmann class representation (k = 8) with vector class representation on different architectures. Validation accuracy on ImageNet. n is the feature dimension, BS means batch size, WarmCos means using warm up together with the cosine learning rate decay. CE is cross-entropy, LS is label smoothing, and BCE is binary cross-entropy.\nSetting Vector Class Representation Grassmann Class Representation (k = 8) Architecture n BS Epoch Lr Policy Loss Optimizer Top1 Top5 Loss Optimizer Top1 Top5\nResNet50 [16] 2048 256 100 Step CE SGD 76.58 93.05 CE RSGD+SGD 77.77(\u21911.19) 93.67(\u21910.62) ResNet50-D [17] 2048 256 100 Cosine CE SGD 78.04 93.89 CE RSGD+SGD 79.26(\u21911.22) 94.44(\u21910.55) ResNet101-D [17] 2048 256 100 Cosine CE SGD 79.32 94.62 CE RSGD+SGD 80.24(\u21910.92) 94.95(\u21910.33) ResNet152-D [17] 2048 256 100 Cosine CE SGD 80.00 95.02 CE RSGD+SGD 80.44(\u21910.44) 95.21(\u21910.19) ResNeXt50 [52] 2048 256 100 Cosine CE SGD 78.02 93.98 CE RSGD+SGD 79.00(\u21910.98) 94.28(\u21910.30) VGG13-BN [42] 4096 256 100 Step CE SGD 72.02 90.79 CE RSGD+SGD 73.40(\u21911.38) 91.30(\u21910.51) Swin-T [26] 768 1024 300 WarmCos LS AdamW 81.06 95.51 LS RSGD+AdamW 81.63(\u21910.57) 95.77(\u21910.26) Deit3-S [45] 384 2048 800 WarmCos BCE Lamb 81.53 95.21 CE RSGD+Lamb 82.18(\u21910.65) 95.73(\u21910.52)\nSubCenterArcFace was designed for face verification. Their strong performances in their intended domains do not naively generalize here. This substantiates that making the subspace formulation competitive is a non-trivial contribution.\nOn Different Architectures We apply Grassmann class representation to eight network architectures, including six CNNs (ResNet50 [16], ResNet50/101/152-D [17], ResNetXt50 [52], VGG13-BN [42]) and two transformers (Swin [26], Deit3 [45]). For each model, we replace the last fc layer with Grassmannian fc and compare performances before and after the change. Their training settings together with validation top-1 and top-5 accuracies are listed in Tab. 2. The results show that GCR is effective across different model architectures. For all architectures, the improvement on top-1 is in the range 0.44\u22121.38%. The improvement is consistent not only for different architectures, but also across different optimizers (e.g., SGD, AdamW, Lamb) and different feature space dimensions (e.g., 2048 for ResNet, 768 for Swin, and 384 for Deit3).\nOn Different Training Strategies In this group, we train ResNet50-D with the three training strategies (RSB-A3, RSB-A2, and RSB-A1) proposed in [50], which aim to push the performance of ResNets to the extreme. Firstly, we train ResNet50-D with the original vector class representation and get top-1 accuracies of 79.36%, 80.29%, and 80.53%, respectively. Then, we replace the last classification fc with the Grassmann class representation (k = 8), and their top-1 accuracies improve to 79.88%, 80.74%, and 81.00%, respectively. Finally, we add the FixRes [46] trick to the three strategies, namely training on 176 \u00d7 176 image resolution and when testing, first resize to 232\u00d7 232 and then center crop to 224 \u00d7 224. We get further boost in top-1 which are 80.20%, 81.04% and 81.29%, respectively. Results are summarized in Fig. 2.\nFigure 2: Validation accuracies of ResNet50-D on ImageNet-1K under different training strategies (RSB-A3, RSB-A2, and RSBA1). Green bars are vector class representations; yellow bars are Grassmannian with k = 8; blue bars added the FixRes trick when training Grassmannian. The best top-1 of ResNet50-D is 81.29%."
        },
        {
            "heading": "5.2. Improvements on Feature Transferability",
            "text": "In this section, we study the feature transferability of the Grassmann class representation. Following [19] on the study of better losses vs. feature transferability, we compare GCR with five different losses and regularizations. They are Softmax [8], Cosine Softmax [19], Label Smoothing [44] (with smooth value 0.1), Dropout [43] (with drop ratio 0.3), and the Sigmoid [5] binary cross-entropy loss. Note that baselines in Tab. 2 that do not demonstrate competitive classification performances are not listed here. The feature transfer benchmark dataset includes CIFAR-10 [21], CIFAR-100 [21], Food-101 [7], Oxford-IIIT Pets [35], Stanford Cars [20], and Oxford 102 Flowers [31]. All models are pre-trained on the ImageNet-1K dataset with the same training procedure as shown in the second row of Tab. 2. When testing on the transferred dataset, features (before the classification fc and Grassmann fc) of pre-trained networks are extracted. We fit linear SVMs with the one-vs-rest multi-class policy on each of the training sets and report their top-1 accuracies or mean class accuracies (for Pets and Flowers) on their test set. The regularization parameter for SVM is grid searched with candidates [0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20] and determined by five-fold cross-validation on the training set.\nResults The validation accuracies of different models on ImageNet-1K are listed in the second group of columns in Tab. 3. All GCR models (k = 1, 4, 8, 16, 32) achieve higher top-1 and top-5 accuracies than all the baseline methods with different losses or regularizations. Within a suitable range, a larger subspace dimension k improves the accuracy greater. However, when the subspace dimension is beyond 16, the top-1 accuracy begins to decrease. When k = 32, the top-1 is 78.63%, which is still 0.33% higher than the best classification baseline CosineSoftmax.\nThe linear transfer results are listed in the fourth group of columns in Tab. 3. Among the baseline methods, we find that Softmax and Sigmoid have the highest average linear transfer accuracies, which are 77.98% and 78.11%, respectively. Other losses demonstrate worse transfer performance than Softmax. For the Grassmann class representation, we observe a monotonic increase in average transfer accuracy when k increases from 1 to 32. When k = 1, the cosine softmax and the GCR have both comparable classification accuracies and comparable transfer performance. This can attribute to their resemblances in the formula. The transfer accuracy of GCR (73.64%) is lower than Softmax (77.98%) at this stage. Nevertheless, when the subspace dimension k\nincreases, the linear transfer accuracy gradually improves, and when k = 8, the transfer performance (77.64%) is on par with the Softmax. When k \u2265 16, the transfer performance surpasses all the baselines.\nIn Tab. 4, we show that features of the GCR version of Swin-T and Deit3 increase the average transfer accuracy by 1.9% and 7.6%, respectively.\nIntra-Class Variability Increases with Dimension The intra-class variability is measured by first computing the mean pairwise angles (in degrees) between features within the same class and then averaging over classes. Following the convention in the study of neural collapse [34], the global-centered training features are used. [19] showed that alternative objectives which may improve accuracy over Softmax by collapsing the intra-class variability (see the Variability column in Tab. 3), degrade the quality of features on downstream tasks. Except for the Sigmoid, which has a similar intra-class variability (60.20) to Softmax (60.12), all other losses, including CosineSoftmax, LabelSmoothing, and Dropout, have smaller feature variability within classes (in the range from 54.79 to 56.87). However, the above conclusion does not apply when the classes are modeled by subspaces. For Grassmann class representation, we observed that if k is not extremely large, then as k increases, both the top-1 accuracy and the intra-class variability grow. This indicates that representing classes as subspaces enables the simultaneous improvement of inter-class discriminability and intra-class variability.\nThis observation is also in line with the class separation index R2. R2 is defined as one minus the ratio of the average intra-class cosine distance to the overall average cosine distance [19, Eq. (11)]. [19] founds that greater class separation R2 is associated with less transferable features. Tab. 3 shows that when k increases, the class separation monotonically decreases, and the transfer performance grows accordingly."
        },
        {
            "heading": "5.3. Design Choices and Analyses",
            "text": "In this section, we use experiments to support our design choices and provide visualizations for the principal angles between class representative spaces.\nChoice of Gamma In Tab. 5, we give more results with different values of \u03b3 when subspace dimension k = 8. We find \u03b3 = 25 has good performance and use it throughout the paper without further tuning.\nImportance of Normalizing Features Normalizing the feature in Equ. (4) is critical to the effective learning of the Grassmann class representations. In Tab. 6 we compare results with/without feature normalization and observed a significant performance drop without normalization.\nPrincipal Angles Between Class Representative Spaces When classes are subspaces, relationships between classes can be measured by k principal angles, which contain richer information than a single angle between two class vectors. The principal angles between two k-dimensional subspaces S and R are recursively defined as,\ncos(\u03b8i) = max s\u2208S max r\u2208R\nsTr = sTi ri,\ns.t.\u2225s\u2225 = \u2225r\u2225 = 1, sTsj = rTrj = 0, j \u2264 i\u2212 1, (11)\nfor i = 1, . . . , k and \u03b8i \u2208 [0, \u03c0/2]. In Fig. 3, we illustrate the smallest and largest principal angles between any pair of classes for a model with k = 8. From the figure, we can see that the smallest principal angle reflects class similarity, and the largest principal angle is around \u03c0/2. A smaller angle means the two classes are correlated in some direction, and a \u03c0/2 angle means that some directions in one class subspace are completely irrelevant (orthogonal) to the other class.\nNecessity of Geometric Optimization To investigate the necessity of constraining the subspace parameters to lie in the Grassmannian, we replace the Riemannian SGD with the vanilla SGD and compare it with Riemannian SGD. Note that with SGD, the logit formula \u2225STi x\u2225 no longer means the projection norm because Si is not guaranteed to be orthonormal anymore. With vanilla SGD, we get top-1 78.55% and top-5 94.18% when k = 8. The top-1 is 0.71% lower than models trained by Riemannian SGD."
        },
        {
            "heading": "6. Limitation and Future Direction",
            "text": "Firstly, a problem that remains open is how to choose the optimal dimension. Currently, we treat it as a hyperparameter and decide it empirically. Secondly, we showed that the Grassmann class representation allows for greater intra-class variability. Given this, it is attractive to explore extensions to explicitly promote intra-class variability. For example, a promising approach is to combine it with selfsupervised learning. We hope our work would stimulate progresses in this direction."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this work, we proposed the Grassmann class representation as a drop-in replacement of the conventional vector class representation. Classes are represented as high-dimensional subspaces and the geometric structure of the corresponding Grassmann fully-connected layer is the product of Grassmannians. We optimize the subspaces using the optimization and provide an efficient Riemannian SGD implementation tailored for Grassmannians. Extensive experiments demonstrate that the new Grassmann class representation is able to improve classification accuracies on large-scale datasets and boost feature transfer performances at the same time."
        },
        {
            "heading": "A. An Alternative Form of Riemannian SGD",
            "text": "As discussed in Section 4.2, an important ingredient of the geometric optimization algorithm is to move a point in the direction of a tangent vector while staying on the manifold (e.g., see the example in Fig. 1). This is accomplished by the retraction operation (please refer to [1, Section 4.1] for its mathematical definition). In the fourth step of Alg. 1, the retraction is implemented by computing the geodesic curve on the manifold that is tangent to the vector M (t). An alternative implementation of retraction other than moving parameters along the geodesic is to replace step 4 with the Euclidean gradient update S(t+1) \u2190 S(t) + \u03c4M (t) and then follow by the orthogonalization described in step 5. In this case, step 5 is not optional anymore since S(t+1) will move away from the Grassmannian after the Euclidean gradient update. The orthogonalization pulls S(t+1) back to the Grassmann manifold. For ease of reference, we call this version of Riemannian SGD as Alg. 1 variant. We compare the two implementations in Tab. 7. The results show that the Grassmann class representation is effective on both versions of Riemannian SGD. We choose Alg. 1 because it is faster than the Alg. 1 variant. The thin SVD used in Equ. (3) can be efficiently computed via the gesvda approximate algorithm provided by the cuSOLVER library, which is faster than a QR decomposition on GPUs (see Tab. 9)."
        },
        {
            "heading": "B. Details on Step 5 of Algorithm 1",
            "text": "The numerical inaccuracy is caused by the accumulation of tiny computational errors of Equ. (3). After running many iterations, the matrix S might not be perfectly orthogonal. For example, after 100, 1000, and 5000 iterations of the Grassmannian ResNet50-D with subspace dimension k = 8, we observed that the error max i\u2225STi Si \u2212 I\u2225\u221e is 1.9e-5, 9.6e-5 and 3.7e-4, respectively. After 50 epochs, the error accumulates to 0.0075. So, we run step 5 every 5 iterations to keep both the inaccuracies and the extra computational cost at a low level at the same time."
        },
        {
            "heading": "C. The Importance of Joint Training",
            "text": "The joint training of the class subspaces and the features is essential. To support this claim, we add an experiment\n(first row of Tab. 8) that only fine-tunes the class subspaces from weights pre-trained using the regular softmax. We find that if the feature is fixed, changing the regular fc to the geometric version does not increase performance noticeably (top-1 from 78.04% of the regular softmax version to 78.14% of the Grassmann version). For comparison, we also add another experiment that fine-tunes all parameters (second row of Tab. 8). But when all parameters are free to learn, the pre-trained weights provide a good initialization that boosts the top-1 to 79.44%.\nD. Influence on Training Speed\nDuring training, the most costly operation in Alg. 1 is SVD. The time of SVD and QR on typical matrix sizes encountered in an iteration of Alg. 1 is benchmarked in Tab. 9. We can see that (1) SVD (with gesvda solver) is faster QR decomposition, and (2), when the subspace dimension is no greater than 2, CPU is faster than GPU. Based on these observations, in our implementation, we compute SVD on CPU when k \u2264 2 and on GPU in other cases. Overall, when computing SVD, it adds roughly 5ms to 30ms overhead to the iteration time.\nTo measure the actual impact on training speed, we show the average iteration time (including a full forward pass and a\nfull backward pass) of the vector class representation version vs. the Grassmann class representation version on different network architectures in Fig. 4. Overall, the Grassmann class representation adds about 0.3% (Deit3-S) to 35.0% (ResNet50-D) overhead. The larger the model, and the large the batch size, the smaller the relative computational cost."
        },
        {
            "heading": "E. More Visualizations on Principal Angles",
            "text": "Due to limited space, we only showed the visualization of the maximum and the minimum principal angles in Fig. 3. Here, we illustrate all eight principal angles in the GCR (k = 8) setting in Fig. 5."
        },
        {
            "heading": "F. Details on the Intra-Class Variability",
            "text": "In Section 5.2, we introduced the intra-class variability which is defined as the mean pairwise angles (in degrees) between features within the same class and then averaged over all classes. For models trained on the ImageNet-1K, we randomly sampled 200K training samples and use their global-centered feature to compute the intra-class variability. Suppose the set of global-centered features of class i is Fi, then\nvariability := 1 C |Fi|2 C\u2211 i=1 \u2211 xj ,xk\u2208Fi \u2220(xj ,xk) (12)\nwhere C is the number of classes, \u2220(\u00b7, \u00b7) is the angle (in degree) between two vectors, and |Fi| is the cardinality of the set Fi."
        },
        {
            "heading": "G. Details on Transfer Datasets",
            "text": "In this section, we give the details of the datasets that are used in the feature transferability experiments. They are\nCIFAR-10 [21], CIFAR-100 [21], Food-101 [7], Oxford-IIIT Pets [35], Stanford Cars [20], and Oxford 102 Flowers [31]. The number of classes and the sizes of the training set and testing set are shown in Tab. 10."
        },
        {
            "heading": "H. Details on Linear SVM Hyperparameter",
            "text": "In Tab. 3, we used five-fold cross-validation on the training set to determine the regularization parameter of the linear SVM. The parameter is searched in the set [0.1, 0.2, 0.5, 1, 2, 5, 10, 15, 20]. Tab. 11 lists the selected regularization parameter of each setting. Both the crossvalidation procedure and the SVM are implemented using the sklearn package. As a pre-processing step, the features are divided by the average norm of the respective training set, so that SVMs are easier to converge. The max iteration of SVM is set to 10,000."
        },
        {
            "heading": "I. Feature Transfer Using KNN",
            "text": "In Tab. 3, we have tested the feature transferability using linear SVM. Here we provide transfer results by KNN in Tab. 12. The hyperparameter K in KNN is determined by five-fold cross-validation on the training set. The candidate values are 1, 3, 5, . . . , 49. Our GCR demonstrates the best performance both on both CNNs and Vision Transformers. For the ResNet50-D backbone, Grassmann with k = 32 has a better performance in both classification accuracy and transferability than all the baseline methods. On Swin-T, our method surpasses the original Swin-T by 2.76% on average. On Deit3-S, our method is 13.81% points better than the original Deit3-S. The experiments on KNN reinforced our conclusion that GCR improves large-scale classification accuracy and feature transferability simultaneously."
        }
    ],
    "title": "Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation",
    "year": 2023
}