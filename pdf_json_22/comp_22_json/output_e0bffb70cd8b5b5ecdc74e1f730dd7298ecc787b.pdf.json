{
    "abstractText": "Deep models must learn robust and transferable representations in order to perform well on new domains. While domain transfer methods (e.g., domain adaptation, domain generalization) have been proposed to learn transferable representations across domains, they are typically applied to ResNet backbones pre-trained on ImageNet. Thus, existing works pay little attention to the effects of pre-training on domain transfer tasks. In this paper, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization, namely: network architectures, size, pre-training loss, and datasets. We observe that simply using a state-of-the-art backbone outperforms existing stateof-the-art domain adaptation baselines and set new baselines on OfficeHome and DomainNet improving by 10.7% and 5.5%. We hope that this work can provide more insights for future domain transfer research.",
    "authors": [
        {
            "affiliations": [],
            "name": "Donghyun Kim"
        },
        {
            "affiliations": [],
            "name": "Kaihong Wang"
        },
        {
            "affiliations": [],
            "name": "Stan Sclaroff"
        },
        {
            "affiliations": [],
            "name": "Kate Saenko"
        }
    ],
    "id": "SP:34c19e643a4a0a2f206c5e2dcc3adad108ccaa5e",
    "references": [
        {
            "authors": [
                "Y. Bai",
                "J. Mei",
                "A.L. Yuille",
                "C. Xie"
            ],
            "title": "Are transformers more robust than CNNs? In: Advances in Neural Information Processing Systems (NeurIPS)",
            "venue": "vol. 34",
            "year": 2021
        },
        {
            "authors": [
                "H. Bao",
                "L. Dong",
                "S. Piao",
                "F. Wei"
            ],
            "title": "BEit: BERT pre-training of image transformers",
            "venue": "In: International Conference on Learning Representations (2022),",
            "year": 2022
        },
        {
            "authors": [
                "D. Bashkirova",
                "D. Hendrycks",
                "D. Kim",
                "S. Mishra",
                "K. Saenko",
                "K. Saito",
                "P. Teterwak",
                "B. Usman"
            ],
            "title": "Visda-2021 competition universal domain adaptation to improve performance on out-of-distribution data",
            "venue": "arXiv preprint arXiv:2107.11011",
            "year": 2021
        },
        {
            "authors": [
                "S. Beery",
                "E. Cole",
                "A. Gjoka"
            ],
            "title": "The iwildcam 2020 competition dataset",
            "venue": "arXiv preprint arXiv:2004.10340",
            "year": 2020
        },
        {
            "authors": [
                "S. Ben-David",
                "J. Blitzer",
                "K. Crammer",
                "A. Kulesza",
                "F. Pereira",
                "J.W. Vaughan"
            ],
            "title": "A theory of learning from different domains, vol",
            "venue": "79. Springer",
            "year": 2010
        },
        {
            "authors": [
                "B. Bhushan Damodaran",
                "B. Kellenberger",
                "R. Flamary",
                "D. Tuia",
                "N. Courty"
            ],
            "title": "Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation",
            "venue": "European Conference on Computer Vision (ECCV). pp. 447\u2013463",
            "year": 2018
        },
        {
            "authors": [
                "M. Caron",
                "I. Misra",
                "J. Mairal",
                "P. Goyal",
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS). vol. 33, pp. 9912\u20139924",
            "year": 2020
        },
        {
            "authors": [
                "M. Caron",
                "H. Touvron",
                "I. Misra",
                "H. J\u00e9gou",
                "J. Mairal",
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 9650\u20139660",
            "year": 2021
        },
        {
            "authors": [
                "S. Changpinyo",
                "P. Sharma",
                "N. Ding",
                "R. Soricut"
            ],
            "title": "Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3558\u20133568",
            "year": 2021
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International Conference on Machine Learning (ICML). pp. 1597\u20131607. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In: International Conference on Learning Representations (ICLR)",
            "year": 2021
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "P. Fischer",
                "J.T. Springenberg",
                "M. Riedmiller",
                "T. Brox"
            ],
            "title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 38(9), 1734\u20131747",
            "year": 2015
        },
        {
            "authors": [
                "Y. Ganin",
                "E. Ustinova",
                "H. Ajakan",
                "P. Germain",
                "H. Larochelle",
                "F. Laviolette",
                "M. Marchand",
                "V. Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "Journal of Machine Learning Research (JMLR) 17(1), 2096\u20132030",
            "year": 2016
        },
        {
            "authors": [
                "S. Gidaris",
                "P. Singh",
                "N. Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "arXiv preprint arXiv:1803.07728",
            "year": 2018
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572",
            "year": 2014
        },
        {
            "authors": [
                "I. Gulrajani",
                "D. Lopez-Paz"
            ],
            "title": "In search of lost domain generalization",
            "venue": "In: International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "K. He",
                "H. Fan",
                "Y. Wu",
                "S. Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9729\u20139738",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770\u2013778",
            "year": 2016
        },
        {
            "authors": [
                "D. Hendrycks",
                "T. Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "In: International Conference on Learning Representations (2019),",
            "year": 2019
        },
        {
            "authors": [
                "G. Hinton",
                "O. Vinyals",
                "J. Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531",
            "year": 2015
        },
        {
            "authors": [
                "J. Hoffman",
                "E. Tzeng",
                "J. Donahue",
                "Y. Jia",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "One-shot adaptation of supervised deep convolutional models",
            "venue": "International Conference on Learning Representations (ICLR)",
            "year": 2014
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7132\u20137141",
            "year": 2018
        },
        {
            "authors": [
                "J. Huang",
                "Q. Dong",
                "S. Gong",
                "X. Zhu"
            ],
            "title": "Unsupervised deep learning by neighbourhood discovery",
            "venue": "arXiv preprint arXiv:1904.11567",
            "year": 2019
        },
        {
            "authors": [
                "C. Jia",
                "Y. Yang",
                "Y. Xia",
                "Y.T. Chen",
                "Z. Parekh",
                "H. Pham",
                "Q. Le",
                "Y.H. Sung",
                "Z. Li",
                "T. Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference on Machine Learning (ICML). pp. 4904\u20134916. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "J. Jiang",
                "B. Chen",
                "B. Fu",
                "M. Long"
            ],
            "title": "Transfer-learning-library",
            "venue": "https://github. com/thuml/Transfer-Learning-Library",
            "year": 2020
        },
        {
            "authors": [
                "Y. Jin",
                "X. Wang",
                "M. Long",
                "J. Wang"
            ],
            "title": "Minimum class confusion for versatile domain adaptation",
            "venue": "European Conference on Computer Vision (ECCV). pp. 464\u2013480. Springer",
            "year": 2020
        },
        {
            "authors": [
                "D. Kim",
                "K. Saito",
                "T.H. Oh",
                "B.A. Plummer",
                "S. Sclaroff",
                "K. Saenko"
            ],
            "title": "CDS: Crossdomain self-supervised pre-training",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 9123\u20139132",
            "year": 2021
        },
        {
            "authors": [
                "P.W. Koh",
                "S. Sagawa",
                "H. Marklund",
                "S.M. Xie",
                "M. Zhang",
                "A. Balsubramani",
                "W. Hu",
                "M. Yasunaga",
                "R.L. Phillips",
                "I Gao"
            ],
            "title": "Wilds: A benchmark of in-the-wild distribution shifts",
            "venue": "International Conference on Machine Learning (ICML). pp. 5637\u20135664. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "S. Kornblith",
                "J. Shlens",
                "Q.V. Le"
            ],
            "title": "Do better ImageNet models transfer better? In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "venue": "pp. 2661\u20132671",
            "year": 2019
        },
        {
            "authors": [
                "R. Krishna",
                "Y. Zhu",
                "O. Groth",
                "J. Johnson",
                "K. Hata",
                "J. Kravitz",
                "S. Chen",
                "Y. Kalantidis",
                "L.J. Li",
                "Shamma",
                "D.A"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International Journal of Computer Vision (IJCV) 123(1), 32\u201373",
            "year": 2017
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "ImageNet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems (NIPS). vol. 25",
            "year": 2012
        },
        {
            "authors": [
                "J. Li",
                "R. Selvaraju",
                "A. Gotmare",
                "S. Joty",
                "C. Xiong",
                "S.C.H. Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS). vol. 34",
            "year": 2021
        },
        {
            "authors": [
                "T.Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "European Conference on Computer Vision (ECCV). pp. 740\u2013755. Springer",
            "year": 2014
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 10012\u201310022",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "H. Mao",
                "C.Y. Wu",
                "C. Feichtenhofer",
                "T. Darrell",
                "S. Xie"
            ],
            "title": "A ConvNet for the 2020s",
            "venue": "arXiv preprint arXiv:2201.03545",
            "year": 2022
        },
        {
            "authors": [
                "M. Long",
                "Z. Cao",
                "J. Wang",
                "M.I. Jordan"
            ],
            "title": "Conditional adversarial domain adaptation",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS). pp. 1640\u20131650",
            "year": 2018
        },
        {
            "authors": [
                "M. Long",
                "H. Zhu",
                "J. Wang",
                "M.I. Jordan"
            ],
            "title": "Unsupervised domain adaptation with residual transfer networks",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS). pp. 136\u2013144",
            "year": 2016
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of Machine Learning Research (JMLR) 9(11)",
            "year": 2008
        },
        {
            "authors": [
                "M. Noroozi",
                "P. Favaro"
            ],
            "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
            "venue": "European Conference on Computer Vision (ECCV). pp. 69\u201384. Springer",
            "year": 2016
        },
        {
            "authors": [
                "V. Ordonez",
                "G. Kulkarni",
                "T. Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS). vol. 24",
            "year": 2011
        },
        {
            "authors": [
                "X. Peng",
                "Q. Bai",
                "X. Xia",
                "Z. Huang",
                "K. Saenko",
                "B. Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 1406\u20131415",
            "year": 2019
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "International Conference on Machine Learning (ICML). pp. 8748\u20138763. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International Journal of Computer Vision (IJCV) 115(3), 211\u2013252",
            "year": 2015
        },
        {
            "authors": [
                "K. Saenko",
                "B. Kulis",
                "M. Fritz",
                "T. Darrell"
            ],
            "title": "Adapting visual category models to new domains",
            "venue": "European Conference on Computer Vision (ECCV). pp. 213\u2013226. Springer",
            "year": 2010
        },
        {
            "authors": [
                "S. Sagawa",
                "P.W. Koh",
                "T. Lee",
                "I. Gao",
                "S.M. Xie",
                "K. Shen",
                "A. Kumar",
                "W. Hu",
                "M. Yasunaga",
                "H. Marklund",
                "S. Beery",
                "E. David",
                "I. Stavness",
                "W. Guo",
                "J. Leskovec",
                "K. Saenko",
                "T. Hashimoto",
                "S. Levine",
                "C. Finn",
                "P. Liang"
            ],
            "title": "Extending the WILDS benchmark for unsupervised adaptation",
            "venue": "In: International Conference on Learning Representations (2022),",
            "year": 2022
        },
        {
            "authors": [
                "K. Saito",
                "D. Kim",
                "S. Sclaroff",
                "T. Darrell",
                "K. Saenko"
            ],
            "title": "Semi-supervised domain adaptation via minimax entropy",
            "venue": "IEEE International Conference on Computer Vision (ICCV)",
            "year": 2019
        },
        {
            "authors": [
                "K. Saito",
                "D. Kim",
                "S. Sclaroff",
                "K. Saenko"
            ],
            "title": "Universal domain adaptation through self supervision",
            "venue": "arXiv preprint arXiv:2002.07953",
            "year": 2020
        },
        {
            "authors": [
                "K. Saito",
                "D. Kim",
                "P. Teterwak",
                "S. Sclaroff",
                "T. Darrell",
                "K. Saenko"
            ],
            "title": "Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 9184\u2013 9193",
            "year": 2021
        },
        {
            "authors": [
                "K. Saito",
                "Y. Ushiku",
                "T. Harada",
                "K. Saenko"
            ],
            "title": "Strong-weak distribution alignment for adaptive object detection",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6956\u20136965",
            "year": 2019
        },
        {
            "authors": [
                "K. Saito",
                "K. Watanabe",
                "Y. Ushiku",
                "T. Harada"
            ],
            "title": "Maximum classifier discrepancy for unsupervised domain adaptation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3723\u20133732",
            "year": 2018
        },
        {
            "authors": [
                "P. Sharma",
                "N. Ding",
                "S. Goodman",
                "R. Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2556\u20132565",
            "year": 2018
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556",
            "year": 2014
        },
        {
            "authors": [
                "C. Sun",
                "A. Shrivastava",
                "S. Singh",
                "A. Gupta"
            ],
            "title": "Revisiting unreasonable effectiveness of data in deep learning era",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 843\u2013852",
            "year": 2017
        },
        {
            "authors": [
                "M. Tan",
                "Q. Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "International Conference on Machine Learning (ICML). pp. 6105\u2013 6114. PMLR",
            "year": 2019
        },
        {
            "authors": [
                "H. Tang",
                "K. Chen",
                "K. Jia"
            ],
            "title": "Unsupervised domain adaptation via structurally regularized deep clustering",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8725\u20138735",
            "year": 2020
        },
        {
            "authors": [
                "H. Touvron",
                "M. Cord",
                "M. Douze",
                "F. Massa",
                "A. Sablayrolles",
                "H. J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "International Conference on Machine Learning (ICML). pp. 10347\u201310357. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "Y.H. Tsai",
                "K. Sohn",
                "S. Schulter",
                "M. Chandraker"
            ],
            "title": "Domain adaptation for structured output via discriminative patch representations",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 1456\u20131465",
            "year": 2019
        },
        {
            "authors": [
                "E. Tzeng",
                "J. Hoffman",
                "N. Zhang",
                "K. Saenko",
                "T. Darrell"
            ],
            "title": "Deep domain confusion: Maximizing for domain invariance",
            "venue": "arXiv preprint arXiv:1412.3474",
            "year": 2014
        },
        {
            "authors": [
                "G. Van Horn",
                "O. Mac Aodha",
                "Y. Song",
                "Y. Cui",
                "C. Sun",
                "A. Shepard",
                "H. Adam",
                "P. Perona",
                "S. Belongie"
            ],
            "title": "The inaturalist species classification and detection dataset",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8769\u20138778",
            "year": 2018
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "year": 2017
        },
        {
            "authors": [
                "N. Venkat",
                "J.N. Kundu",
                "D. Singh",
                "A Revanur"
            ],
            "title": "Your classifier can secretly suffice multi-source domain adaptation",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS). vol. 33, pp. 4647\u20134659",
            "year": 2020
        },
        {
            "authors": [
                "H. Venkateswara",
                "J. Eusebio",
                "S. Chakraborty",
                "S. Panchanathan"
            ],
            "title": "Deep hashing network for unsupervised domain adaptation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5018\u20135027",
            "year": 2017
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona",
                "S. Belongie"
            ],
            "title": "The Caltech-UCSD birds-200-2011 dataset",
            "year": 2011
        },
        {
            "authors": [
                "S. Wang",
                "X. Chen",
                "Y. Wang",
                "M. Long",
                "J. Wang"
            ],
            "title": "Progressive adversarial networks for fine-grained domain adaptation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9213\u20139222",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wu",
                "Y. Xiong",
                "S.X. Yu",
                "D. Lin"
            ],
            "title": "Unsupervised feature learning via nonparametric instance discrimination",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3733\u20133742",
            "year": 2018
        },
        {
            "authors": [
                "Q. Xie",
                "M.T. Luong",
                "E. Hovy",
                "Q.V. Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10687\u201310698",
            "year": 2020
        },
        {
            "authors": [
                "S. Xie",
                "R. Girshick",
                "P. Doll\u00e1r",
                "Z. Tu",
                "K. He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1492\u20131500",
            "year": 2017
        },
        {
            "authors": [
                "R. Xu",
                "Z. Chen",
                "W. Zuo",
                "J. Yan",
                "L. Lin"
            ],
            "title": "Deep cocktail network: Multi-source unsupervised domain adaptation with category shift",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3964\u20133973",
            "year": 2018
        },
        {
            "authors": [
                "R. Xu",
                "G. Li",
                "J. Yang",
                "L. Lin"
            ],
            "title": "Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation",
            "venue": "IEEE International Conference on Computer Vision (ICCV). pp. 1426\u20131435",
            "year": 2019
        },
        {
            "authors": [
                "K. You",
                "Y. Liu",
                "J. Wang",
                "M. Long"
            ],
            "title": "Logme: Practical assessment of pre-trained models for transfer learning",
            "venue": "International Conference on Machine Learning (ICML). pp. 12133\u201312143. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "T. Liu",
                "M. Long",
                "M. Jordan"
            ],
            "title": "Bridging theory and algorithm for domain adaptation",
            "venue": "International Conference on Machine Learning (ICML). pp. 7404\u20137413. PMLR",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhu",
                "F. Zhuang",
                "D. Wang"
            ],
            "title": "Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence. vol. 33, pp. 5989\u20135996",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Transfer Learning; Pre-training; Domain Generalization; Domain Adaptation"
        },
        {
            "heading": "1 Introduction",
            "text": "It is well-known that deep models often perform poorly on out-of-distribution test data [21]. Domain transfer has been an active research topic for years, aiming to learn more robust feature representations that generalize from training data (source domains) to novel data distributions (target domains). There has been significant progress in domain transfer for visual recognition tasks, such as image classification [13], semantic segmentation [57] and object detection [49].\nDomain transfer consists of two steps: 1) pre-training, where a model is first pre-trained on an upstream task with a massive supervised dataset, e.g., ImageNet, and 2) transfer (adaptation), where the model is fine-tuned on downstream multi-domain data, see Fig. 1-(a). In the latter step, Domain Adaptation (DA) tunes on both a labeled source and an unlabeled target domain, while Domain Generalization (DG) tunes only on labeled source data. While many DA and DG methods (e.g., adversarial learning [13,36,58], entropy optimization [36,46] or clustering [23]) have been proposed and studied extensively in prior work, little attention has been paid to pre-training for domain transfer. In this paper, we provide comprehensive experiments and an in-depth analysis of pre-training. ar X\niv :2\n20 3.\n11 81\n9v 3\n[ cs\n.C V\n] 2\n0 Ju\nl 2 02\nPre-training is a very successful transfer learning technique for many visual tasks, including domain transfer tasks, as it provides a strong initial representation [10,29]. Pre-training is especially useful when annotations are limited. We decompose pre-training into three parts: (a) network architecture (backbone), (b) dataset, and (c) loss function. It is a common practice of most domain transfer methods to use a ResNet backbone pre-trained on ImageNet-1K with a supervised loss function (i.e., cross-entropy loss). We argue that this evaluation standard is outdated and ignores the effect of modern large-scale pre-training on domain transfer. To illustrate the potential impact of pre-training, Fig. 1- (b) shows an experiment that compares the performance of different backbones to SOTA results on the DomainNet [41] DA benchmark. Simply using a recent backbone [35] pre-trained with ImageNet-22K with no adaptation outperforms existing domain transfer methods. This raises the question, will SOTA DA methods still provide similar gains if applied to the stronger backbones?\nTo fully explore these issues, in this paper we pose the following questions:\n1. What is the effect of network architecture? ResNet-based backbones [18] are commonly used in domain generalization [16], single source [47,13,36] and multi-source DA [41,61]. Since larger and more powerful backbones such as Swin-Transformer [34] or ConvNext [35] have been recently proposed, we ask whether they may be more robust to domain shift. Transformers were recently shown to be more robust than CNNs to image corruptions and ad-\nversarial examples [1]. We thus conduct an extensive analysis of the impact of network size and architectures, including state-of-the-art Transformers and CNNs on domain transfer tasks.\n2. What is the effect of pre-training dataset? Several datasets that are larger than the standard ImageNet-1K could potentially improve transfer: ImageNet-22K [43], JFT-300M [20] and Conceptual Captions [9,51]. These datasets have been very effective for diverse downstream visual tasks (e.g., [11,32]), but not well explored for domain transfer tasks. We therefore study the effect of a wider range of pre-training datasets, including ImageNet-21K, JFT-300M, and language-vision datasets, on domain transfer. 3. Supervised vs. Self-supervised Pre-training. In terms of loss functions, self-supervised learning (e.g., [8,10,65]) has obtained powerful performance on diverse visual tasks and often outperforms its supervised counterparts on downstream problems [3,8,10]. We therefore compare self-supervised and supervised pre-training for domain transfer. 4. Domain Adaptation with SOTA Pre-training. Finally, we investigate a fundamental research question in domain transfer: With the help of the state-of-the-art pre-trained models, do we still need sophisticated domain adaptation methods? We explore the applicability of several existing DA methods to our more advanced pre-training setting.\nWe conduct the study on four standard multi-domain benchmarks. While we find that with better pre-training, DA methods still improve performance compared to a source-only trained model, an outdated DA method outperforms state-of-the-art DA methods. This raises serious fundamental research questions about the current evaluation protocol.\nIn summary, our work\u2019s main contribution is to provide the field with a broad comparison of modern pre-training approaches for domain transfer tasks. To our knowledge, this is the first work to do such an in-depth analysis. One of our key findings is that SOTA pre-training outperforms SOTA domain transfer methods by a large margin even without access to a target domain, as shown in Fig. 1-(b). We also observe network architectures, sizes, and pre-training datasets play a big role but in a domain-dependent way. Finally, we show that SOTA DA methods work less than older DA methods under modern pre-training. We hope our work will modernize current domain transfer benchmarks and provide helpful and practical insights for future domain adaptation research."
        },
        {
            "heading": "2 Related Work",
            "text": "Domain Transfer. Domain transfer tasks aim to improve generalization and mitigate domain shift between source and target domains. We study generalization to natural data shifts caused by the changes in visual styles, background, lighting, etc. [19,28,44] as opposed to artificial corruptions [15]. In this problem setup, we are given a single source domain or multi-source labeled domains. The key is how to learn transferable features that will be useful for the unlabeled target domain. Depending on the specific setup, this task can be categorized into\ntwo: (1) domain adaptation (DA) where we can access the target domain and (2) domain generalization (DG) where we do not have access to a target domain. Depending on the number of labeled source domains, each category can be further divided into single-source or multi-source DA (or DG). Typically, there are two stages: (1) pre-training and (2) adaptation. Most of these methods focus on the second adaptation stage for domain alignment with adversarial domain classifier [13,36], entropy optimization [36,46,27], minimizing maximum discrepancy across domain distributions [50,71], maximum mean discrepancy [37], or optimal transport [6]. While domain alignment methods have been proposed actively in recent years for the adaptation stage, the importance of the pre-training stage has not been well explored. Pre-training can provide strong weight initialization by learning a general transferable representation that can be useful for diverse downstream tasks [29]. While typical DA or DG methods use ResNet backbones pre-trained on ImageNet-1K, we focus on the pre-training stage and provide an in-depth analysis of its effects on domain transfer tasks.\nNetwork Architectures and Datasets for Pre-training. Since the transferability of the model is closely correlated with the performance on downstream tasks as shown in [29], having a strong pre-trained model is important. In terms of architectures, convolutional neural networks (CNN) has been standard and state-of-the-art models in many visual tasks for years. After the introduction of AlexNet [31] with ImageNet-1K [43], new CNN-based architectures have been proposed with deeper, wider, and more effective convolutional layers, e.g., VGGNet [52], ResNe(X)t [18,67], SENet [22], EfficientNet [54], and ConvNeXt [35]. A newer line of work uses self-attention layers or Transformers for vision. Inspired by the Transformers for NLP [60], transformers for vision have been introduced in Vision Transformers (ViT) [11] and shows encouraging results by training with larger training sets than ImageNet-1K such as JFT-300M [53] or ImageNet-22K. DeiT [56] propose an efficient training strategy to train ViT. Swin Transformers employ a hierarchical transformer with a sliding window strategy where selfattention is performed within a local window. Swin Transformers achieve stateof-the-art performance in a range of computer vision tasks including object detection and segmentation. Bai et al. [1] show that Transformers can improve the generalization capability on out-of-distribution samples compared to CNNs. However, Liu et al. [35] propose ConvNeXt, which modernizes the ResNet architecture and show that a CNN can still outperform Transformers in vision and more robust to distribution shift. In addition to network architectures, it is shown that larger pre-training datasets such as ImageNet-22K, JFT-300M, or image-text pairs can further improve the transferability [11,24,32,35,42,66]. Inspired by these observations, we further study the effect of backbones and pre-training datasets on domain transfer evaluation benchmarks.\nSelf-supervised learning. Self-supervised learning [12,14,39,65] devises pretext tasks with self-supervisory signals without requiring human annotations. These pretext tasks allow a model to learn discriminative and transferable representations with only unlabeled data for later use in downstream tasks. Representative methods include: solving a jigsaw puzzle [39], rotation prediction [14],\nInstance Discrimination (ID) [10,17,65], contrasting cluster assignments [7], self knowledge distillation [8], and masked image modeling [2]. Instance Discrimination [65] learns an embedding that maps visually similar images closer to each other and far from dissimilar images by classifying an image as its unique class. Some of these self-supervised methods outperform the supervised pre-training on several downstream tasks. For example, SwAV outperforms its supervised pre-training on object detection and image classification tasks on VOC [59] and INaturalist [59]. It is notable that in the VisDA-2021 competition for universal domain adaptation [3], the self-supervised masked image modeling approach with a transformer backbone [2] is the first place solution. We further investigate the effect of self-supervised pre-training approaches for domain transfer tasks."
        },
        {
            "heading": "3 Analysis Setup",
            "text": "Our goal is to analyze the effect of pre-training on domain transfer tasks. We assume a single source domain Ds = {(xsi , ysi )} Ns i=1 with Ns images x and labels y and an unlabeled target domain Dt = {xti} Nt i=1. Given a pre-trained model f , we evaluate two types of domain transfer tasks: 1) domain generalization, i.e. fine-tune f on Ds and test on Dt, and 2) domain adaptation, i.e. fine-tune f on Ds,Dt and test on Dt. Pre-training Datasets. Typically, ImageNet-1K is widely used for pre-training. ImageNet-1K contains 1.2M images of mutually exclusive 1000 classes. ImageNet22K (the superset of ImageNet-1K) is also used for pre-training (e.g., [34,35]), which contains 14.1M images of 22K classes. In addition, Xie et al. [66] use a larger dataset JFT-300M to further improve the accuracy. Recently, languagevision models [24,42,32] can be used in image classification using image and text description pairs. We choose ALBEF [32], which achieves the-state-of-theart performance and uses publicly available language-vision datasets. In total, ALBEF is pre-trained on ImageNet-1K, web crawled datasets (Conceptual Captions [9,51], SBU Captions [40]) and two human annotated datasets (COCO[33] and Visual Genome [30]). We explore models pre-trained on these datasets. Downstream Datasets. We choose Office-Home (OH) [62], DomainNet (DN) [41], CUB [64,63], and iWildCAM2020 (WILD) [4,28,45]. Office-Home contains 15K images from 4 domains (Real (Rw), Painting (Pa), Clipart (Cl), Art (Ar)) on 65 classes. DomainNet contains 586K images from 6 domains (Clipart (Cl), Infograph (In), Painting (Pa), Quickdraw (Qu), Real (Rw), Sketch(Sk)) on 345 classes. Office-Home and DomainNet contain many common classes with ImageNet such as a chair, clock, and table. CUB contains 15K images from two domains (Real and Painting) on 200 fine-grained bird classes. For WILD, the source domain and target domain contains 182 different animal species from camera traps in different locations spread across multiple countries in the world. The source domain contains 129K images from 243 camera traps and the target domain contains 14K from 32 camera traps. Then we use the test set with 42K images from 48 different camera traps. The sets of camera traps in source and target domains are disjoint. The images of WILD are all realistic images from\ncamera traps. ImageNet contains many animal classes, but the annotations are more fine-grained in CUB and WILD. While the main cause of domain-shift is visual styles in Office-Home, DomainNet, CUB, the domain-shift of WILD is mainly caused by location differences between camera traps in the world. Backbone. For CNNs, we investigate the variants of ResNet [18], EfficientNet [54], and ConvNeXt [35]. For Transformers, we explore the variants of ViT [11], DeiT [56] and Swin [34]. Variants include different depths and sizes (e.g., Swin-{S,B,L}). Self-supervised Learning for Pre-training. In addition to the supervised pre-training, we also explore recent self-supervised learning approaches for pretraining. We study SwAV [7], MoCo [17], DINO [8], and BEiT [2]. Challenges of Evaluating Models. We use pre-trained models, which are publicly available. One of the big challenges is that it is not possible to fairly compare all possible combinations of pre-training datasets, backbones, and selfsupervised learning. For example, SwAV (self-supervised method) only provides pre-trained models on one architecture (e.g., ResNet-50) and ImageNet-1K. Therefore, it is not possible to fairly compare these with self-supervised pretraining and supervised pre-training, which use the state-of-the-art architecture (e.g., ConvNeXt) and larger datasets (e.g., ImageNet-22K). Fine-tuning Details. From each pre-trained model, we fine-tune the model with a downstream dataset. We use source domain data to train a model and keep 20% of the source data as a validation set. We choose the learning rate and optimizer by tuning on the validation set. We test different learning rates (lr=1e-1, 1e-2, 1e-3) of SGD and learning rates (lr=1e-3, 1e-4, 1e-5) of the Adam optimizer. We add a new FC layer for downstream tasks and train it from scratch with a learning rate 10 times that of the pre-trained layers. We use the image size of 224\u00d7 224 with random resized cropping. We also use random color jittering, gray-scaling, and horizontal flipping for augmentation for all models. Additional training details can be found in appendix."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, our goal is to explore the effects of pre-training for domain transfer. We reiterate that most of the prior domain adaptation (DA) or generalization (DG) work use a ResNet backbone pre-trained on ImageNet-1K. A model is denoted by X-Y where X represents the name of architecture and Y represents the size of the backbone. For example, Swin-T, Swin-S, and Swin-B represent the tiny, small, and base model of Swin Transformer. Unless specified otherwise, pre-trained models are trained with a supervised loss (i.e., cross-entropy loss). We now evaluate different pre-trained models in domain transfer tasks. In Sec. 4.1, we investigate single source DG and analyze the architecture, pretraining datasets, and loss functions. We also compare these models with the existing DA works. In Sec. 4.2, we explore the existing DA with new architectures. Lastly, we provide feature analysis in Sec. 4.3. Our code is available at: https://github.com/VisionLearningGroup/Benchmark_Domain_Transfer."
        },
        {
            "heading": "4.1 Single Source Domain Generalization",
            "text": "For this experiment, we fine-tune different pre-trained models with only a single source domain. We take the Real domain as the source domain on Office-Home, DomainNet, and CUB and treat the remaining domains as target domains. For WILD, we follow the split in [28]. We do not use the target domain data. Analysis of Network Architectures. We first compare generalization performance of architectures in Table 1. All models are pre-trained on ImageNet-1K. ConvNeXt and Transformers (DeiT, Swin) outperform their ResNet counterparts. In this experiment, Transformer models achieve the highest accuracy on average. Swin-T outperforms ResNet-50 by 3.7%. This improvement becomes larger in the deeper model. Swin-S outperforms ResNet-101 by 4.3%. We further analyze the effect of depth in a later section. The big difference between ConvNeXt and Transformers is in the CUB experiment. While DeiT-S significantly improves the accuracy by 15.7%, ConvNeXt-T could not improve much compared to ResNet. However, ConvNeXt attains slightly higher accuracy on DomainNet compared to Swin. This suggests that CNN and Transformers may be robust to different types of domain shift. We put additional results of larger networks (e.g., DeiT-B) in the appendix. Analysis of Pre-training Datasets. We now analyze the effect of additional datasets during pre-training as shown in Fig. 2. Due to the availability of pretrained models (see Challenges of Evaluating Models in the above section), we use different backbones to compare the effect of pre-training datasets. In Fig. 2-(a), we compare the accuracy between ImageNet-1K and ImageNet-22K on SwinB and ConvNeXt-B for Office-Home and DomainNet. We report the accuracy averaged over all settings in each benchmark. In both architectures, pre-training with ImageNet-22K boosts the accuracy for all benchmarks. Especially, there are significant boosts in accuracy on Office-Home and CUB. To be specific, the accuracy of ConvNeXt-B on CUB increases by 19.8%. In Fig. 2-(b), we study the effect of JFT-300M. Since a supervised pre-trained model on JFT-300M is not released publicly, we use the publicly available self-trained EfficientNet-B7 [66] on ImageNet-1K and JFT-300M. While it shows similar accuracy improvements on Office-Home, the accuracy improvements on CUB are smaller than ImageNet-\n22K. This could be because self-training uses pseudo-labeling and the number of classes is still limited to 1K. In Fig. 2-(c), we study the effect of vision-language datasets containing image-text pairs. We study ALBEF, which uses ViT-B as an image encoder and BERTbase as a text encoder. We made one modification for ALBEF. We first extract the sentence representation from the text encoder with the prompt template \u201cA photo of a {label}\u201d following [42]. Then we initialize the weights of the last FC layer with the sentence representations. While it shows similar behavior on Office-Home, it seriously hurts the performance of CUB. In contrast, ALBEF obtains the highest accuracy on DomainNet compared to all the other models. This indicates that improvement depends on both the pretraining dataset and the downstream task at the same time.\nAnalysis of Network Depth. We investigate the accuracy gained from the deeper layers. In Fig. 3, we report the accuracy of ConvNeXt-{B,L,XL}, Swin{B,L}, and ViT-{S,B,L} pre-trained on ImageNet-22K. In general, ViT shows bigger changes in accuracy according to its depth. The accuracy of Swin and ConvNeXt increases slightly on the benchmarks except for CUB. In WILD, all the architectures show minimal changes according to their depth. When comparing the adaptation methods, we often choose a shallow and light model to reduce computational costs. For example, most of the domain generalization methods [16] employ ResNet-18 rather than ResNet-101. However, the results suggest that we should be careful when choosing a backbone. If we compare adaptation methods and want to use a shallow model for efficiency, it is more desirable to use Swin or ConvNeXt than ViT.\nSupervised vs. Self-supervised Learning. Recently, several self-supervised learning (e.g., [8,10,65]) methods outperformed their supervised counterparts on various downstream tasks [2,3,7,8,10]. In Table 2, we investigate the effect of\nself-supervised learning approaches in pre-training for domain transfer. In Table 2-(a), we observe that supervised learning (denoted as Sup.) performs better than self-supervised learning in most cases. Especially on Office-Home and CUB, self-supervised learning significantly hurts performance compared to supervised learning. In Table 2-(b), we explore the BEiT, the winner of the VisDA 2021 DA competition [3]. BEiT uses both self-supervised and supervised learning. When self-supervised learning is combined with supervised learning and improves the performance on CUB, DomainNet (denoted as DN), and WILD. While most of the self-supervised approaches focus on only unlabeled data, combining it with labels should be considered to improve further in future research.\nComparison with Domain Adaptation Baselines. We provide a performance comparison with the existing DA baselines in Table 3. In this comparison we use Office-Home and DomainNet, which are most extensively explored in prior\nwork. In each table, (I) reports the performance of single source DA, where a labeled source domain and unlabeled target domain data is used together for adaptation. In (II), multiple labeled source domains are used in addition to the unlabeled target domain. (I,II) use ResNet backbones pre-trained on ImageNet1K. (III) only uses one single source (Real) domain with the recent backbones and larger pre-training datasets. It is surprising that (III), the state-of-art backbones pre-trained on a larger dataset, significantly outperform DA baselines (I, II) despite being trained only on a single source and not using any adaptation on the target domain. While ConvNext-XL obtains the best results and outperforms adaptation baselines by up to 10.7% on Office-Home, ViT-B with ALBEF gains more improvements and outperforms adaptation baselines by up to 5.5% on DomainNet. This observation raises a question, is it still fair and reasonable to use ResNet backbones pre-trained on ImageNet-1K as a standard backbone for the comparison of adaptation methods? From the results, it is clear that the the the standard backbone in the existing domain adaptation benchmarks are outdated. The pre-training stage for DA needs to be updated to reflect recent advances in pre-training."
        },
        {
            "heading": "4.2 Domain Adaptation with Modern Pre-training",
            "text": "The observation in Sec. 4.1 leads us to the next question, will SOTA DA methods still provide gains when these are applied to the recent stronger architectures and pre-training? We study the transferability of prior adaptation methods with new architectures pre-trained on larger datasets. Transferability of Domain Adaptation. We employ DANN (JMLR\u201916), CDAN (NeurIPS\u201918), AFN (ICCV\u201919), MDD (ICML\u201919), and MCC(ECCV\u201920). Table 4 provides the performance of domain adaptation between Swin-L and ConvNeXt-XL pre-trained on ImageNet-22K. First, DA methods still improve the accuracy on average compared to source only (SO) models. However, we\nobserve negative transfer in some settings where DA hurts the performances compared SO. To be specific, all the adaptation methods badly affect the performance on Real \u2192 Clipart(Cl) in Office-Home. Second, the relative ranking among adaptation methods on new architectures is different from the ranking on ResNet-based backboes. While MCC obtains SOTA accuracy on ResNet, but CDAN outperforms AFN, MDD, and MCC with these new architectures, which was proposed earlier than the others. This behavior raises another practical question, which adaptation method should we consider the state-of-the-art? We certainly want to have a model with strong performance, but the existing adaptation benchmark with outdated pre-training can not choose the strongest model. We argue that adaptation methods should have transferability on various backbones in order to avoid the potential risk of overfitting to a specific backbone, so that it is able to successfully transfer to new architectures in future.\nAnalysis on Hyper-parameter. In this experiment, we analyze the transferability of hyper-parameters to new architectures in each adaptation method. The question is whether the optimal hyper-parameters in a shallow network (e.g., ResNet-50) are still optimal in a deep network (e.g., ConvNeXt-XL). We believe this is a practically important question as training a big network is computationally expensive. Hyper-parameter search in a big network can be prohibitively expensive in terms of computational cost. Therefore, the desired property is that hyper-parameters in adaptation methods are transferable between different architectures and their depth. Fig. 4 shows the analysis of performance depending on the hyper-parameters between a shallow and deep network. We investigate two adaptation methods CDAN and MCC. Following [48], we vary the tradeoff (\u03bb = 0.05, 0.1, 0.5, 1.0, 2.5, 5.0) hyper-parameter in CDAN, which controls the trade-off between domain confusion loss and classification loss on the source domain. The default \u03bb in ResNet is 1.0. For MCC, we vary the temperature hyper-parameter (\u03b7=1.5, 2.0, 2.5, 3.0, 3.5), which affects of classifier\u2019s confusion loss. The default \u03b7 in ResNet is 2.5. We employ ResNet-50 as a shallow network and ConvNeXt-XL as a deep network for CNN, and Swin-S as a shallow network and Swin-L as a deep network. The shallow networks are pre-trained on ImageNet-1K and deep networks are pre-trained on ImageNet-22K. The accuracy\nacross the depth (shallow vs. deep) and architectures (CNN vs. Transformers) show similar tendency and obtain the highest accuracy with the same hyperparameter values. Therefore, we observe that the default hyper-parameters of \u03bb and \u03b7 in shallow networks are transferable to deep networks. Additionally, the sensitivity of deep networks is small compared to that of shallow networks. We measure the standard deviation of accuracy. Swin-L obtains a standard deviation of 4.2%, but Swin-S obtains a much higher standard deviation of 8.3%."
        },
        {
            "heading": "4.3 Feature Analysis",
            "text": "We provide feature analysis in this section. We use features directly obtained from each pre-training without fine-tuning on domain adaptation benchmarks.\nFeature Visualization. First, we show the t-SNE [38] feature visualization of each pre-trained models in Fig. 5. We compare (a) ResNet-50 pre-trained on ImageNet-1K with (b) ConvNeXt-XL, (c) Swin-L pre-trained on ImageNet22K, and (d) ViT-B from ALBEF [32]. We directly extract features from the pre-trained models on the Real (colored by red) and Clipart (colored by blue) domains in Office-Home. While the red and blue dots are highly separated in ResNet-50, these are aligned with each other in ConvNeXt-XL and Swin-L. It is also clear that ConvNeXt-XL and Swin-L obtain better clustered and discriminative representations. (d) ViT-B pre-trained (ALBEF) on image-text pairs shows different patterns, where the blue dots are red dots are somewhat aligned but it does not provide clustered representations. This could be probably due to that the pretext task of ALBEF is to align image and text, but not classification.\nAnalysis on Feature Transferability. We employ LogME [70] to evaluate the transferability of features for downstream tasks. LogME is used to assess pre-trained models, which estimates the maximum value of label evidence from the extracted features of downstream data. A higher value LogME implies better transferability to downstream tasks. We measure LogME of each pre-training for\nall domains in the benchmarks as shown in Fig. 6. As expected, ResNet backbones pre-trained on ImageNet-1K obtain very low values of LogME compared to the state-of-the-art backbones. We reiterate that the pre-training stage should be modernized according to the recent advances in computer vision."
        },
        {
            "heading": "5 Conclusions",
            "text": "Most domain transfer works pay little attention to the importance of the pretraining stage. In this work, we provide an in-depth analysis of the effect of modern pre-training on domain transfer. We summarize some of our key findings:\n1. What makes strong pre-training for domain transfer? In Sec. 4.1, we observe that many factors, including network architecture, pre-training dataset, and network size contribute to the improvements in domain transfer tasks. However, there is no single winner across all benchmark datasets. The transferability of pre-training depends on the target benchmark, adaptation method, and network depth. Most importantly, we observe that simply using the SOTA pre-training outperforms all the domain adaptation baselines. 2. Do we still need domain adaptation with modern pre-training? In Sec. 4.2, while we find that adaptation methods still improve the accuracy with modern pre-training, the relative ranking of domain adaptation methods is not preserved. With modern pre-training, an outdated DA method performs better than more recent DA methods in our experiments.\nLimitations. Due to the availability of pre-trained models, we could not analyze full ablations (e.g., SOTA backbones pre-trained on JFT-300M). In this work, we use a very simple fine-tuning strategy by adding a single FC layer, but there could be other simple better ways to fine-tune pre-trained models for downstream tasks. In addition to image classification tasks, other computer vision tasks including domain adaptive object detection, segmentation, or video domain adaptation should be explored with modern pre-training in future research. We hope future work should use these results as a new baseline. Acknowledgments. This work was supported by DARPA LwLL and NSF Award No. 1535797"
        },
        {
            "heading": "6 Appendix",
            "text": "Summary. We provide additional details and experimental results in this supplementary material. Future domain adaptation work can use our code1 for a new baseline for domain transfer tasks."
        },
        {
            "heading": "6.1 Additional Training Details",
            "text": "Our implementation is based on the timm2 library and the transfer learning library in [25]. We directly use the implementation of the the transfer learning library in [25], which supports domain adaptation baselines (DANN, CDAN, MCC, AFN, and MDD). For some pre-trained weights not available in the timm library, we directly use the publicly released pre-trained weights from the authors. In Fig. 7, we show example images from different domains in each dataset, which show the type of domain shift in each benchmark."
        },
        {
            "heading": "6.2 Source Accuracy on Single Source Generalization",
            "text": "We provide a comparison of source accuracy on the source validation set on ConvNext and Swin Transformers in Fig. 8. We compare the source accuracy between shallow models, ConvNeXt-S and Swin-S pre-trained on ImageNet-1K, and deep models, ConvNeXt-XL and Swin-L pre-trained on ImageNet-22K. We observe that deep models, ConvNeXt-XL and Swin-L, obtain higher source accuracy on all the benchmarks compared to the shallow models, ConvNeXt-S, Swin-S. According to the theory in in [5], the expected target error T (h) can be bounded by the expected source error, the discrepancy between the source and target domains, and the shared error of the ideal joint hypothesis \u03bb.\nT (h) \u2264 S(h) + d1 (DS ,DT ) + \u03bb (1)\nPrior domain adaptation methods assume that expected source error S(h) is low since we assume many source labels. Therefore, prior domain adaptation methods focus on minimizing the discrepancy between the source and target domains. However, as shown in Fig. 8, there is a gap in S(h) between pretrained models. Using modern pre-training can further reduce the upper bound of expected target error with a lower value of S(h).\nAdditionally, we observe that there are noticeable inconsistencies between the source validation accuracy and target accuracy across different optimizers and learning rates in shallow models trained on ImageNet-1K (e.g., ResNet50, Tiny, Small models of ConvNeXtand Swin) when fine-tuned on CUB and WILD. That means using the higher source validation accuracy for model selection can obtain a very lower target accuracy. We choose the best optimizer and learning rate on target accuracy but use early stopping with source validation accuracy only on these cases for proper comparisons. However, these\n1 https://github.com/VisionLearningGroup/Benchmark_Domain_Transfer 2 https://github.com/rwightman/pytorch-image-models\nArt Clipart Product Real Office-Home CUB Painting Real\nWILDS Location A Location B\nDomainNet Clipart Infograph Painting Quickdraw Real Sketch\ngaps become smaller in deeper models trained ImageNet-22K and we observe similar trends between source validation and target accuracy. This also suggests that deep models trained on a larger dataset can obtain better domain invariant features than shallow models trained on a smaller dataset."
        },
        {
            "heading": "6.3 Additional Results on Single Source Generalization",
            "text": "In our main paper, we only use the image encoder in ALBEF. We also try to finetune the text encoder in ALBEF. Table 5 shows that the results of fine-tuning text and image encoder in ALBEF. We observe that fine-tuning the text encoder is not helpful on Office-Home. In Table 6, we provide an accuracy comparison on different pre-training datasets and network architectures. In Tables 7 and 8, we provide additional results trained on other source domains on Office-Home and DomainNet."
        }
    ],
    "title": "A Broad Study of Pre-training for Domain Generalization and Adaptation",
    "year": 2022
}