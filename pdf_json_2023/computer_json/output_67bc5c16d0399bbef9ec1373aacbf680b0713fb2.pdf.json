{
    "abstractText": "In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shi-Yu Huang"
        },
        {
            "affiliations": [],
            "name": "Yun-Chen Yang"
        },
        {
            "affiliations": [],
            "name": "Yu-Ru Su"
        },
        {
            "affiliations": [],
            "name": "Bo-Cheng Lai"
        },
        {
            "affiliations": [],
            "name": "Javier Duarte"
        },
        {
            "affiliations": [],
            "name": "Scott Hauck"
        },
        {
            "affiliations": [],
            "name": "Shih-Chieh Hsu"
        },
        {
            "affiliations": [],
            "name": "Jin-Xuan Hu"
        },
        {
            "affiliations": [],
            "name": "Mark S. Neubauer"
        }
    ],
    "id": "SP:c5814e230dcce553033a23d5a007c44e2099f584",
    "references": [
        {
            "authors": [
                "A. Ryd",
                "L. Skinnari"
            ],
            "title": "Tracking triggers for the HL-LHC",
            "venue": "arXiv:2010.13557, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "P. Billoir"
            ],
            "title": "Progressive track recognition with a kalman-like fitting procedure",
            "venue": "Comput. Phys. Commun., vol. 57, no. 1-3, pp. 390\u2013394, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "P. Billoir",
                "S. Qian"
            ],
            "title": "Simultaneous pattern recognition and track fitting by the kalman filtering method",
            "venue": "Nucl. Instrum. Methods Phys. Res. A, vol. 294, no. 1-2, pp. 219\u2013228, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "R. Mankel"
            ],
            "title": "A concurrent track evolution algorithm for pattern recognition in the HERA-B main tracking system",
            "venue": "Nucl. Instrum. Methods Phys. Res. A, vol. 395, no. 2, pp. 169\u2013184, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "R. Fr\u00fchwirth"
            ],
            "title": "Application of kalman filtering to track and vertex fitting",
            "venue": "Nucl. Instrum. Methods Phys. Res. A, vol. 262, no. 2-3, pp. 444\u2013450, 1987.",
            "year": 1987
        },
        {
            "authors": [
                "CERN"
            ],
            "title": "CERN yellow reports: Monographs, vol 4 (2017): Highluminosity large hadron collider (HL-LHC) technical design report v. 0.1",
            "venue": "Jan. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Tao"
            ],
            "title": "Level-1 track finding with an all-FPGA system at CMS for the HL-LHC",
            "venue": "arXiv:1901.03745, 2019.",
            "year": 1901
        },
        {
            "authors": [
                "J. Shlomi",
                "P. Battaglia",
                "J.-R. Vlimant"
            ],
            "title": "Graph neural networks in particle physics",
            "venue": "Machine Learning: Science and Technology, vol. 2, no. 2, p. 021001, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Ju",
                "S. Farrell",
                "P. Calafiura",
                "D. Murnane",
                "Prabhat",
                "L. Gray",
                "T. Klijnsma",
                "K. Pedro",
                "G. Cerati",
                "J. Kowalkowski",
                "G. Perdue",
                "P. Spentzouris",
                "N. Tran",
                "J.-R. Vlimant",
                "A. Zlokapa",
                "J. Pata",
                "M. Spiropulu",
                "S. An",
                "A. Aurisano",
                "J. Hewes",
                "A. Tsaris",
                "K. Terao",
                "T. Usher"
            ],
            "title": "Graph neural networks for particle reconstruction in high energy physics detectors",
            "venue": "arXiv:2003.11603, 2020.",
            "year": 2003
        },
        {
            "authors": [
                "J. Duarte",
                "J.-R. Vlimant"
            ],
            "title": "Graph neural networks for particle tracking and reconstruction",
            "venue": "Artificial Intelligence for High Energy Physics. WORLD SCIENTIFIC, 2022, pp. 387\u2013436.",
            "year": 2022
        },
        {
            "authors": [
                "J. Pata",
                "J. Duarte",
                "J.-R. Vlimant",
                "M. Pierini",
                "M. Spiropulu"
            ],
            "title": "MLPF: efficient machine-learned particle-flow reconstruction using graph neural networks",
            "venue": "Eur. Phys. J. C Part. Fields, vol. 81, no. 5, pp. 1\u201314, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.M. Sirunyan",
                "C. collaboration"
            ],
            "title": "Particle-flow reconstruction and global event description with the cms detector",
            "venue": "JINST, vol. 12, no. 10, p. 10003, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "G. DeZoort",
                "S. Thais",
                "J. Duarte",
                "V. Razavimaleki",
                "M. Atkinson",
                "I. Ojalvo",
                "M. Neubauer",
                "P. Elmer"
            ],
            "title": "Charged particle tracking via edgeclassifying interaction networks",
            "venue": "Comput. Softw. Big Sci., vol. 5, no. 1, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Yan",
                "L. Deng",
                "X. Hu",
                "L. Liang",
                "Y. Feng",
                "X. Ye",
                "Z. Zhang",
                "D. Fan",
                "Y. Xie"
            ],
            "title": "HyGCN: A GCN accelerator with hybrid architecture",
            "venue": "2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2020, pp. 15\u201329.",
            "year": 2020
        },
        {
            "authors": [
                "S. Liang",
                "Y. Wang",
                "C. Liu",
                "L. He",
                "H. Li",
                "D. Xu",
                "X. Li"
            ],
            "title": "EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks",
            "venue": "IEEE Trans. Comput., vol. 70, no. 9, pp. 1511\u20131525, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Kiningham",
                "P. Levis",
                "C. Re"
            ],
            "title": "GRIP: A graph neural network accelerator architecture",
            "venue": "IEEE Trans. Comput., pp. 1\u201312, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Geng",
                "A. Li",
                "R. Shi",
                "C. Wu",
                "T. Wang",
                "Y. Li",
                "P. Haghi",
                "A. Tumeo",
                "S. Che",
                "S. Reinhardt",
                "M.C. Herbordt"
            ],
            "title": "AWB-GCN: A graph convolutional network accelerator with runtime workload rebalancing",
            "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 2020, pp. 922\u2013936.",
            "year": 2020
        },
        {
            "authors": [
                "J. Li",
                "A. Louri",
                "A. Karanth",
                "R. Bunescu"
            ],
            "title": "GCNAX: A flexible and energy-efficient accelerator for graph convolutional neural networks",
            "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2021, pp. 775\u2013788.",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "Y. Wang",
                "X. Xie",
                "X. Hu",
                "A. Basak",
                "L. Liang",
                "M. Yan",
                "L. Deng",
                "Y. Ding",
                "Z. Du",
                "Y. Xie"
            ],
            "title": "Rubik: A hierarchical architecture for efficient graph neural network training",
            "venue": "IEEE Trans. Comput.-aided Des. Integr. Circuits Syst., vol. 41, no. 4, pp. 936\u2013949, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "W. Hamilton",
                "Z. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "FastML Team"
            ],
            "title": "fastmachinelearning/hls4ml",
            "venue": "2021. [Online]. Available: https://github.com/fastmachinelearning/hls4ml",
            "year": 2021
        },
        {
            "authors": [
                "J. Duarte"
            ],
            "title": "Fast inference of deep neural networks in FPGAs for particle physics",
            "venue": "JINST, vol. 13, no. 07, p. P07027, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Zabi",
                "J.W. Berryhill",
                "E. Perez",
                "A.D. Tapper"
            ],
            "title": "The Phase-2 Upgrade of the CMS Level-1 Trigger",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Elabd",
                "V. Razavimaleki",
                "S.-Y. Huang",
                "J. Duarte",
                "M. Atkinson",
                "G. DeZoort",
                "P. Elmer",
                "S. Hauck",
                "J.-X. Hu",
                "S.-C. Hsu"
            ],
            "title": "Graph neural networks for charged particle tracking on fpgas",
            "venue": "Frontiers in big Data, vol. 5, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.W. Battaglia",
                "R. Pascanu",
                "M. Lai",
                "D. Rezende",
                "K. Kavukcuoglu"
            ],
            "title": "Interaction networks for learning about objects, relations and physics",
            "venue": "vol. 29, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Heintz",
                "V. Razavimaleki",
                "J. Duarte",
                "G. DeZoort",
                "I. Ojalvo",
                "S. Thais",
                "M. Atkinson",
                "M. Neubauer",
                "L. Gray",
                "S. Jindariani",
                "N. Tran",
                "P. Harris",
                "D. Rankin",
                "T. Aarrestad",
                "V. Loncar",
                "M. Pierini",
                "S. Summers",
                "J. Ngadiuba",
                "M. Liu",
                "E. Kreinar",
                "Z. Wu"
            ],
            "title": "Accelerated charged particle tracking with graph neural networks on FPGAs",
            "venue": "arXiv:2012.01563, 2020.",
            "year": 2012
        },
        {
            "authors": [
                "S. Amrouche",
                "L. Basara",
                "P. Calafiura",
                "V. Estrade",
                "S. Farrell",
                "D.R. Ferreira",
                "L. Finnie",
                "N. Finnie",
                "C. Germain",
                "V.V. Gligorov",
                "T. Golling",
                "S. Gorbunov",
                "H. Gray",
                "I. Guyon",
                "M. Hushchyn",
                "V. Innocente",
                "M. Kiehn",
                "E. Moyse",
                "J.-F. Puget",
                "Y. Reina",
                "D. Rousseau",
                "A. Salzburger",
                "A. Ustyuzhanin",
                "J.-R. Vlimant",
                "J.S. Wind",
                "T. Xylouris",
                "Y. Yilmaz"
            ],
            "title": "The tracking machine learning challenge: Accuracy phase",
            "venue": "The NeurIPS \u201918 Competition. Cham: Springer International Publishing, 2020, pp. 231\u2013264.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs\nShi-Yu Huang\u2217, Yun-Chen Yang\u2217, Yu-Ru Su\u2217, Bo-Cheng Lai\u2217, Javier Duarte\u2020, Scott Hauck\u2021, Shih-Chieh Hsu\u00a7, Jin-Xuan Hu\u2217, Mark S. Neubauer\u00b6,\n\u2217Institute of Electronics, National Yang Ming Chiao Tung University, Taiwan \u2020Department of Physics, University of California San Diego, USA\n\u2021Department of Electrical and Computer Engineering, University of Washington, USA \u00a7Department of Physics, University of Washington, USA\n\u00b6Department of Physics, University of Illinois at Urbana-Champaign, USA\nAbstract\u2014In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.\nI. INTRODUCTION\nParticle trajectory reconstruction in Large Hadron Collider (LHC) is a vital task for collision analysis, which requires accurate and in-time reconstruction to decide which collision events to read out [1]. The existing reconstruction algorithms are based on Kalman filter [2], [3], [4], [5], which are difficult to meet strict latency requirements due to the quadratically increasing complexity. The High-Luminosity LHC project [6], [7] aims to boost the instantaneous luminosity by 5x to 7x in 2027, even exacerbating the design challenges for trajectory reconstruction. Recent research shows that edge-classifying GNNs (Graph Neural Networks) achieve high accuracy in trajectory reconstruction and are scalable with the increased luminosity [8], [9], [10], [11], [12], [13], making them a preferred solution for future collision analysis in LHC.\nOne of the main concerns for GNNs to be implemented in the LHC system is the long processing latency with irregular data accesses. Recently proposed GNN accelerators [14], [15], [16], [17], [18], [19] are designed to focus on node data, such as Graph Convolutional Networks (GCNs) [20] and GraphSAGE [21]. These cannot be applied directly to the edge classifying GNNs in trajectory reconstruction, which uses edge embedding and predicts the results on edges. Moreover, preprocessing on graphs is not fit for trajectory reconstruction which has relatively smaller but dynamic graph properties. State-of-the-art accelerators optimize their performance by reducing irregularity, such as rearranging processing patterns and order to better fit with their architectures [15], [19], or\nmonitor the utilization of processing elements (PEs) to balance the workload [17]. These techniques are beneficial for static and large graphs, where stable graph characteristics can be reused. However, it is not efficient to spend long preprocessing time for one-time use on small graphs with dynamic features.\nIn this paper, we propose an efficient architecture on FPGAs to support edge-classifying GNNs, meeting the timing requirement of LHC tasks. There are three novel contributions in the proposed architecture. First, a modular parallel architecture facilitates the design and scaling of the architecture with the size of the graphs. Second, efficient data allocation and buffer arrays considerably reduce memory conflicts during parallel data accesses. Third, exploitation of the geometry of collision events significantly lowers the graph irregularity by constraining the node connections, and thus increases processing parallelism.\nThis work is implemented with the high-level-synthesis framework, hls4ml [22], [23], which enables an automatic translation of machine learning models to FPGA designs. The experiments were performed on real collision graphs. The results on a Xilinx Virtex UltraScale+ VU9P FPGA show that the proposed GNN architecture achieves 1,625x and 1,574x speedup respectively compared with a Intel Xeon W-2125 CPU and an NVIDIA RTX2080 GPU. Section II of this paper discusses the background of particle tracking. Section III introduces the proposed GNN architecture. Section IV evaluates the performance and Section V concludes this work."
        },
        {
            "heading": "II. BACKGROUND",
            "text": ""
        },
        {
            "heading": "A. Large Hadron Collider System",
            "text": "The Large Hadron Collider (LHC) is the largest and most powerful particle accelerator in the world [6]. For high-energy particle physics collider experiments in the LHC, protonproton collisions occur at a frequency of 40MHz and produce data at a rate of roughly 40 TB/s [1]. After the collision, the trackers record the locations of particle detections (\u201chits\u201d) and transfer this information to the trigger systems. The trigger system will perform trajectory reconstruction to recognize which hits belong to the same particle, as shown in Fig. 1. The collision events are processed by 18 FPGAs in a multiplexed\nar X\niv :2\n30 6.\n11 33\n0v 2\n[ cs\n.A R\n] 2\n7 Ju\nn 20\n23\nmanner, where each FPGA needs to handle 2.22 million graphs per second (MGPS) [24].\nTrackers are composed of cylindrical detecting layers [6], [13]. These layers are immersed in an axis-aligned magnetic field, and their geometry is naturally described by cylindrical coordinates. We focus on the innermost layers, a highly granular set of 4 barrel and 14 endcap layers [25]."
        },
        {
            "heading": "B. GNN-based Algorithms for Track Reconstruction",
            "text": "The edge classifying GNN algorithm is based on interaction networks (IN) [26]. IN is a physics-motivated GNN capable of analyzing objects and their relations. Hit information is embedded in the node feature, and trajectory segment information is embedded in the edge feature. The index set stores the sender and receiver node indexes of each edge. There are three types of functions in IN: Edgeblock, Aggregate, and Nodeblock. Functions in Edgeblock and Nodeblock are multi-layer perceptrons (MLPs) that re-embed edge and node features according to their input. Aggregate accumulates edge features to their receiver nodes."
        },
        {
            "heading": "C. Designs of GNN Accelerators on FPGAs",
            "text": "Several studies have implemented GNNs on FPGAs for particle physics [8], [25], [27]. [8] focuses on jet tagging, which targets fully connected graphs and aims to predict features of the entire graph instead of each individual edge. While [8] addresses the issue of irregular access with fully connected graph properties, it is not suitable for the LHC application. The graphs generated from the LHC consist of hundreds of nodes per graph. Utilizing the methods from [8] may generate excessive unnecessary connections which may be tens to hundreds of times greater than the original graph, leading to a significant impact on processing time."
        },
        {
            "heading": "III. A LOW LATENCY GNN ARCHITECTURE FOR TRAJECTORY RECONSTRUCTION",
            "text": ""
        },
        {
            "heading": "A. Overview Architecture",
            "text": "Based on the GNN computation flow of IN in Section II-B, the computation can be split into pipeline stages at the function level. We use the Vivado HLS dataflow architecture to implement the pipeline. Fig. 2a shows the proposed pipeline. We design a modularized parallel architecture for each function, including Edgeblock, Aggregate and Nodeblock. Each function is composed of several processing elements (PEs) as basic compute units. Between these functions, we insert FIFO\nbuffers with different depths to ensure that data would not be stuck in the dataflow paths. With this architecture, users can configure the pipeline and scale the system throughput with the available resources on FPGAs."
        },
        {
            "heading": "B. Modular Parallel Architecture",
            "text": "The modular parallel architecture enables parallel processing of each function. The following introduces the design and optimizations of these functions.\n1) Edgeblock: The Edgeblock computation involves accessing edge features, edge indexes, and connected node features. The access of node features changes dynamically according to the edge indexes. To address this irregularity, we added node arrays, which contain the features of all the nodes in the graph, into each PE to support concurrent accesses to node features. As shown in Fig. 2b, during the computation, the edge features (ei,j) and edge indexes (i, j) of each edge will be sent to PEs. In each PE, node features (Xi and Xj) are accessed based on the edge indexes. After the above steps, multiplier engines in PEs will process the MLP computation and output the updated edge features.\n2) Aggregate: The purpose of Aggregate is to send the updated edge features to their receiver nodes. During this process, the aggregate function first reads updated edge features and edge indexes. Based on the edge indexes, the aggregated edge features are accessed and added to the updated edge features, and then stored back to internal registers. The architecture of Aggregate PE is shown in Fig. 2c. After aggregating all updated edge features, the parallel adder tree accumulates the values of the same node indexes. With this architecture, multiple Aggregate PEs can process multiple edges simultaneously.\n3) Nodeblock: The Nodeblock is used for re-embedding node features by the original and aggregated node features. The data access of the Nodeblock computation is more regular when compared with Edgeblock and Aggregate. For each node, Nodeblock collects node features and aggregated node features, and then uses MLPs to obtain updated node features."
        },
        {
            "heading": "C. Exploiting Geometric Property of LHC Trackers",
            "text": "While the architecture in the previous section successfully enables significant parallelism between processing elements (PEs), the individual memory in PEs costs considerable amount of BlockRAMs (BRAM) in an FPGA. Therefore, we propose a method that can reduce memory utilization and enable a more parallel architecture by taking advantage of the data properties of LHC detectors.\nIn Section II-A, we introduced the architecture of the LHC particle trackers and how hit data is applied to the input graphs of GNNs. In the original graph constructed from LHC trackers, an edge from a node could connect to any other node in a graph. This assumption could cause excessive number of edges in the graph. Since the LHC trackers are composed of cylindrical layers surrounding the colliding beams, the particles have to pass through the inner tracker layer first and then move out, as shown in Fig. 3a. These trajectories exhibit similar connection behaviors. For example, hits on the B1 layer\nonly connect to the B2 layer or E1 layer. The relationship between hits and legal edges can be applied to a geometryconstrained graph. We reorganize the input graph structure by grouping hits based on their layer locations. We partition the graphs into 13 parallel sub-graphs, each containing only two node groups, as shown in Fig. 3b and 3c.\nWith the geometry-constrained approach, an edge can only exist between specific node groups. This reduces the number of candidate nodes in node arrays used in Edgeblock PEs and Aggregate PEs, resulting in significant reduction of memory usage. Furthermore, since these 13 sub-graphs are independent of each other, they can be assigned to different PEs and computed in parallel. By relieving memory usage pressure and improving parallelism, the performance is greatly enhanced compared to the original design."
        },
        {
            "heading": "IV. EVALUATION",
            "text": ""
        },
        {
            "heading": "A. Experiment Setup",
            "text": "We implemented our design with Vivado HLS 2019.2 and loaded it onto a Xilinx Virtex UltraScale+ VU9P FPGA. The clock frequency of the design runs at 200 MHz. The resource utilization is from the v-synthesis report. The performance is based on the simulation result of the generated HDL code. We use the metric of Million Graphs Per Second (MGPS) to measure the system throughput.\nWe evaluate our architecture with the TrackML dataset [28] generated by CERN. All the data points are based on a fixed point format of 7 integer bits and 7 fractional bits. This is the same format used in [25] to ensure acceptable accuracy.\nA system PE in the experiment contains an Edgeblock PE, an Aggregate PE, and a Nodeblock PE. The graph size of the dataset will be elaborated in Section IV-B. The performance of our proposed architecture will be evaluated in Sections IV-C to IV-E. In Section IV-F, we compare the performance of our architecture with the CPU, GPU, and prior FPGA designs."
        },
        {
            "heading": "B. Supporting In-time Graph Processing of Collision Events",
            "text": "The ultimate goal of this work is to perform in-time trajectory reconstruction based on the graphs generated from LHC collision events. Input graphs are prepared based on the same flow as the prior work [25]. Each graph is divided into two sectors based on the position z of hits. We use the graph size that can cover 95 percentile of collision events as the nominal size, which contains 739 nodes with 1252 edges. According to [24], these graphs should be computed at the throughput higher than 2.22 MGPS.\nTable I compares the three proposed architectures. The architecture MPA represents the Modular Parallel Architecture introduced in Section III-B. MPAgeo and MPAgeo rsrc are the extended designs of MPA with the proposed techniques of geometry-constrained optimization and data-aware resource allocation respectively. The designs of MPAgeo and MPAgeo rsrc will be elaborated in Section IV-D and IV-E. Latency measures the time from input graph to the output result. The design can take a new input in every Interval time and attain throughput in MGPS. As shown in Table I, the proposed MPAgeo rsrc meets the LHC requirement by supporting graphs of 739 nodes with 1252 edges at throughput of 3.225 MGPS."
        },
        {
            "heading": "C. Scalability of MPA (Modular Parallel Architecture)",
            "text": "In Section III-B, we introduced the MPA architecture. The processing throughput of the architecture can scale by deploying more PEs. To evaluate the scalability of MPA, Fig. 4 illustrates the performance and resource utilization of MPA\nfrom one PE to eight PEs. The results show the latency and interval can be reduced by deploying more PEs. However, when scaling up the number of PEs, BRAMs will become the limiting factor of FPGA resources."
        },
        {
            "heading": "D. MPA with Geometry-constrained Optimization",
            "text": "By taking advantage of the geometry-constrained property described in Section III-C, MPAgeo not only relieves the constraints of the range of node in each PE, but also makes node groups independent to others. MPAgeo alleviates the resource demand of BRAM and allows the deployment of more PEs for greater processing parallelism. There are 11 node groups and 13 edge groups. We allocate one PE for each of these groups and result in a total of 11 Nodeblock PEs, 13 Edgeblock PEs, and 13 Aggregate PEs. As shown in Table I, MPAgeo achieves 13% improvement in throughput compared to the original MPA architecture."
        },
        {
            "heading": "E. Data-aware Resource Allocation",
            "text": "We further analyze the distribution of graph sizes in the dataset and propose the design MPAgeo rsrc which applies data-aware resource allocation. After applying the geometryconstrained property, the number of nodes are not evenly distributed across different layers. The barrel layers (B1 to B4) contain more nodes and connections than endcap layers (E1 to E7). To address this issue, we propose the design MPAgeo rsrc which classifies the node groups into two types. As shown in Fig. 3b, the layers B1 to B4 contain relatively more nodes and belong to type A, while layers E1 to E7 with fewer nodes are assigned to type B. We will assign two PEs to process each node group of type A, and one PE to handle each node group in type B. For the edge groups, we apply the same allocation principle as for node groups."
        },
        {
            "heading": "F. Comparison with Previous Designs",
            "text": "1) Comparison with Previous GNN Trajectory Reconstruction on FPGA: There are two architectures in the previous\nwork [25] of GNN for trajectory reconstruction on FPGA. The throughput-optimized design (ThrpOpt) focuses on attaining high throughput, but would reduce the graph size it can handle. The resource-optimized design (RsrcOpt) aims to accommodate large graphs, but would suffer from low throughput. Table III compares the performance between these two architectures and our proposed architecture. The platform of all the three architectures is XCVU9P, and the frequency is 200 MHz. ThrpOpt design can achieve a higher throughput of 200 MGPS, but can only handle small graphs of 28 nodes with 56 edges. RsrcOpt architecture can accommodate large graphs of 448 nodes with 896 edges, but with lower throughput than the ThrpOpt design. Our proposed MPAgeo resrc can handle the largest graph (739 nodes with 1252 edges) among all the designs, and attains higher throughput than the RsrcOpt design.\n2) Comparison with CPU and GPU: We execute the same particle-tracking GNN algorithm on an Intel(R)Xeon(R) W2125 CPU and an NVIDIA GeForce RTX 2080 Ti (CUDA 10.2) based on PyTorch (1.11.0) and the PyTorch Geometric 2.0.4 framework. We ran 1000 graphs on each platform. Each graph contains 739 nodes and 1252 edges. Table IV shows the details of experiment and normalized throughput. Our proposed design on FPGA achieved significantly higher throughput of 1,625x and 1,574x when compared with CPU and GPU respectively."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "We propose a novel architecture for particle-tracking GNNs on FPGAs. By utilizing LHC detector geometry, our design reduces graph complexity and FPGA resource requirements. The modular architecture of processing units and buffers also efficiently handle the irregular data access patterns and facilitate design scalability to support large graphs while attaining high parallelism and computation throughput. Experiment results show that our design achieves 1,625x speedup compared to the CPU, and 1,574x speedup compared to the GPU."
        },
        {
            "heading": "VI. ACKNOWLEDGMENTS",
            "text": "Huang, Yang, Su, Lai and Hu are supported by National Science and Technology Council grant 111-2221-E-A49-092MY3. Duarte, Hauck, Hsu and Neubauer are supported by National Science Foundation (NSF) grants No. 2117997."
        }
    ],
    "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs",
    "year": 2023
}