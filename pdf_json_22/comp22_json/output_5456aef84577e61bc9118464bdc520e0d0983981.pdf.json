{
    "abstractText": "Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple scoring function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to a larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance against high-dimensional models with a much smaller model size. We make our code publicly available.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yun-Cheng Wang"
        }
    ],
    "id": "SP:eef122c023583d4df98fed972644294a94e70335",
    "references": [
        {
            "authors": [
                "Ivana Balazevic",
                "Carl Allen",
                "Timothy Hospedales"
            ],
            "title": "TuckER: Tensor factorization for knowledge graph completion",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Ivana Bala\u017eevi\u0107",
                "Carl Allen",
                "Timothy Hospedales",
                "First Last"
            ],
            "title": "Multi-relational poincar\u00e9 graph embeddings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Kurt Bollacker",
                "Colin Evans",
                "Praveen Paritosh",
                "Tim Sturge",
                "Jamie Taylor"
            ],
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,",
            "year": 2008
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garcia-Duran",
                "Jason Weston",
                "Oksana Yakhnenko"
            ],
            "title": "Translating embeddings for modeling multi-relational data",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Antoine Bordes",
                "Xavier Glorot",
                "Jason Weston",
                "Yoshua Bengio"
            ],
            "title": "A semantic matching energy function for learning with multi-relational data",
            "venue": "Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Leo Breiman",
                "Jerome H Friedman",
                "Richard A Olshen",
                "Charles J Stone"
            ],
            "title": "Classification and regression",
            "year": 2017
        },
        {
            "authors": [
                "Ines Chami",
                "Adva Wolf",
                "Da-Cheng Juan",
                "Frederic Sala",
                "Sujith Ravi",
                "Christopher R\u00e9"
            ],
            "title": "Low-dimensional hyperbolic knowledge graph embeddings",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6901\u20136914,",
            "year": 2020
        },
        {
            "authors": [
                "Linlin Chao",
                "Jianshan He",
                "Taifeng Wang",
                "Wei Chu"
            ],
            "title": "PairRE: Knowledge graph embeddings via paired relation vectors. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4360\u20134369",
            "venue": "Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.336. URL https://aclanthology.org/2021.acl-long.336",
            "year": 2021
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "XGBoost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Joachim Daiber",
                "Max Jakob",
                "Chris Hokamp",
                "Pablo N Mendes"
            ],
            "title": "Improving efficiency and accuracy in multilingual entity extraction",
            "venue": "In Proceedings of the 9th international conference on semantic systems,",
            "year": 2013
        },
        {
            "authors": [
                "Tim Dettmers",
                "Pasquale Minervini",
                "Pontus Stenetorp",
                "Sebastian Riedel"
            ],
            "title": "Convolutional 2D knowledge graph embeddings",
            "venue": "In Thirty-second AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "In NIPS Deep Learning and Representation Learning Workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Raphael Hoffmann",
                "Congle Zhang",
                "Xiao Ling",
                "Luke Zettlemoyer",
                "Daniel S Weld"
            ],
            "title": "Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies",
            "year": 2011
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Huang",
                "Jingyuan Zhang",
                "Dingcheng Li",
                "Ping Li"
            ],
            "title": "Knowledge graph embedding based question answering",
            "venue": "In Proceedings of the twelfth ACM international conference on web search and data mining,",
            "year": 2019
        },
        {
            "authors": [
                "Denis Krompa\u00df",
                "Stephan Baier",
                "Volker Tresp"
            ],
            "title": "Type-constrained representation learning in knowledge graphs",
            "venue": "In International semantic web conference,",
            "year": 2015
        },
        {
            "authors": [
                "C.-C. Jay Kuo",
                "Azad M Madni"
            ],
            "title": "Green learning: Introduction, examples and outlook",
            "venue": "Journal of Visual Communication and Image Representation,",
            "year": 2022
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yang Liu",
                "Xuan Zhu"
            ],
            "title": "Learning entity and relation embeddings for knowledge graph completion",
            "venue": "In Twenty-ninth AAAI conference on artificial intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Farzaneh Mahdisoltani",
                "Joanna Biega",
                "Fabian Suchanek"
            ],
            "title": "YAGO3: A knowledge base from multilingual wikipedias. In 7th biennial conference on innovative data systems research",
            "venue": "CIDR Conference,",
            "year": 2014
        },
        {
            "authors": [
                "George A Miller"
            ],
            "title": "Wordnet: a lexical database for english",
            "venue": "Communications of the ACM,",
            "year": 1995
        },
        {
            "authors": [
                "Dai Quoc Nguyen",
                "Tu Dinh Nguyen",
                "Dat Quoc Nguyen",
                "Dinh Phung"
            ],
            "title": "A novel embedding model for knowledge base completion based on convolutional neural network. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Apoorv Saxena",
                "Aditay Tripathi",
                "Partha Talukdar"
            ],
            "title": "Improving multi-hop question answering over knowledge graphs using knowledge base embeddings",
            "venue": "In Proceedings of the 58th annual meeting of the association for computational linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Danqi Chen",
                "Christopher D Manning",
                "Andrew Ng"
            ],
            "title": "Reasoning with neural tensor networks for knowledge base completion",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Zhi-Hong Deng",
                "Jian-Yun Nie",
                "Jian Tang"
            ],
            "title": "RotatE: Knowledge graph embedding by relational rotation in complex space",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Shikhar Vashishth",
                "Soumya Sanyal",
                "Partha Talukdar",
                "Yiming Yang"
            ],
            "title": "A re-evaluation of knowledge graph completion methods",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5516\u20135522,",
            "year": 2020
        },
        {
            "authors": [
                "Kristina Toutanova",
                "Danqi Chen"
            ],
            "title": "Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd workshop on continuous vector space models and their compositionality",
            "year": 2015
        },
        {
            "authors": [
                "Th\u00e9o Trouillon",
                "Johannes Welbl",
                "Sebastian Riedel",
                "\u00c9ric Gaussier",
                "Guillaume Bouchard"
            ],
            "title": "Complex embeddings for simple link prediction",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107",
                "Markus Kr\u00f6tzsch"
            ],
            "title": "Wikidata: a free collaborative knowledgebase",
            "venue": "Communications of the ACM,",
            "year": 2014
        },
        {
            "authors": [
                "Kai Wang",
                "Yu Liu",
                "Qian Ma",
                "Quan Z Sheng"
            ],
            "title": "Mulde: Multi-teacher knowledge distillation for low-dimensional knowledge graph embeddings",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Wang",
                "Dingxian Wang",
                "Canran Xu",
                "Xiangnan He",
                "Yixin Cao",
                "Tat-Seng Chua"
            ],
            "title": "Explainable reasoning over knowledge graphs for recommendation",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Yun-Cheng Wang",
                "Xiou Ge",
                "Bin Wang",
                "C.-C. Jay Kuo"
            ],
            "title": "KGBoost: A classification-based knowledge base completion method with negative sampling",
            "venue": "Pattern Recognition Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Wang",
                "Jianwen Zhang",
                "Jianlin Feng",
                "Zheng Chen"
            ],
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Yikun Xian",
                "Zuohui Fu",
                "Shan Muthukrishnan",
                "Gerard De Melo",
                "Yongfeng Zhang"
            ],
            "title": "Reinforcement knowledge graph reasoning for explainable recommendation",
            "venue": "In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval,",
            "year": 2019
        },
        {
            "authors": [
                "Han Xiao",
                "Minlie Huang",
                "Yu Hao",
                "Xiaoyan Zhu"
            ],
            "title": "TransA: An adaptive approach for knowledge graph embedding",
            "venue": "CoRR, abs/1509.05490,",
            "year": 2015
        },
        {
            "authors": [
                "Yijing Yang",
                "Wei Wang",
                "Hongyu Fu",
                "C-C Jay Kuo"
            ],
            "title": "On supervised feature selection from high dimensional feature spaces",
            "venue": "APSIPA Transactions on Signal and Information Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Qianjin Zhang",
                "Ronggui Wang",
                "Juan Yang",
                "Lixia Xue"
            ],
            "title": "Knowledge graph embedding by reflection transformation",
            "venue": "Knowledge-Based Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yongqi Zhang",
                "Quanming Yao",
                "Wenyuan Dai",
                "Lei Chen"
            ],
            "title": "AutoSF: Searching scoring functions for knowledge graph embedding",
            "venue": "IEEE 36th International Conference on Data Engineering (ICDE),",
            "year": 2020
        },
        {
            "authors": [
                "Yushan Zhu",
                "Wen Zhang",
                "Mingyang Chen",
                "Hui Chen",
                "Xu Cheng",
                "Wei Zhang",
                "Huajun Chen"
            ],
            "title": "Dualde: Dually distilling knowledge graph embedding for faster and cheaper reasoning",
            "venue": "In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Knowledge graphs (KGs) store human knowledge in a graph-structured format, where nodes and edges denote entities and relations, respectively. A (head entity, relation, tail entity) factual triple, denoted by (h, r, t), is a basic component in KGs. In many knowledge-centric artificial intelligence (AI) applications, such as question answering [Huang et al., 2019, Saxena et al., 2020], information extraction [Hoffmann et al., 2011, Daiber et al., 2013], and recommendation [Wang et al., 2019, Xian et al., 2019], KG plays an important role as it provides explainable reasoning paths to predictions. However, most KGs suffer from the incompleteness problem; namely, a large number of factual triples are missing, leading to performance degradation in downstream applications. Thus, there is growing interest in developing KG completion (KGC) methods to solve the incompleteness problem by inferring undiscovered factual triples based on existing ones. Knowledge graph embedding (KGE) methods have been widely used to solve the incompleteness problem. Embeddings for entities and relations are stored as model parameters and updated by maximizing triple scores among observed triples while minimizing those among negative triples. The number of free parameters in a KGE model is linear to the embedding dimension and the number of entities and relations in KGs, i.e. O((|E|+ |R|)d), where |E| is the number of entities, |R| is the number of relations, and d is the embedding dimension. Since KGE models usually require a higher-dimensional embedding space for a better reasoning capability, they require large model sizes (i.e. parameter numbers) to achieve satisfactory performance as demonstrated in Fig. 1. To this end, it is challenging for them to handle large-scale KGs with lots of entities and relations in resource-constrained platforms such as mobile/edge computing. A KGC method that has good reasoning capability in low dimensions is desired Kuo and Madni [2022].\n1https://github.com/yunchengwang/GreenKGC\nar X\niv :2\n20 8.\n09 13\n7v 2\n[ cs\n.A I]\nThe requirement of high-dimensional embeddings for popular KGE methods comes from the over-simplified scoring functions [Xiao et al., 2015]. Thus, classification-based KGC methods, such as ConvE [Dettmers et al., 2018], aim to increase the reasoning capabilities in low dimensions by adopting neural networks (NNs) as powerful decoders. As a result, they are more efficient in parameter scaling than KGE models [Dettmers et al., 2018]. However, NNs demand longer inference time and more computation power due to their deep architectures. The long inference time of the classification-based methods also limits their applicability to some tasks that require real-time inference. Recently, DualDE [Zhu et al., 2022] applied Knowledge Distillation (KD) [Hinton et al., 2015] to train powerful low-dimensional embeddings. Yet, it demands three stages of embedding training: 1) training high-dimensional KGE, 2) training lowdimensional KGE with the guidance of high-dimensional KGE, and 3) multiple rounds of student-teacher interactions. Its training process is time-consuming and may fail to converge when the embeddings are not well-initialized.\nHere, we propose a new KGC method that works well under low dimensions and name it GreenKGC. GreenKGC consists of three modules: 1) representation learning, 2) feature pruning, and 3) decision learning. Each of them is trained independently. In Module 1, we leverage a KGE method, called the baseline method, to learn high-dimensional entity and relation representations. In Module 2, a feature pruning process is applied to the high-dimensional entity and relation representations to yield discriminant low-dimensional features for triples. In addition, we observe that some feature dimensions are more powerful than others in different relations. Thus, we group relations with similar discriminant feature dimensions for parameter savings and better performance. In Module 3, we train a binary classifier for each relation group so that it can predict triple\u2019s score in inference. The score is a soft prediction between 0 and 1, which indicates the probability of whether a certain triple exists or not. Finally, we propose two novel negative sampling schemes, embedding-based and ontology-based, for classifier training in this work. They are used for hard negative mining, where these hard negatives cannot be correctly predicted by the baseline KGE methods.\nWe conduct extensive experiments and compare the performance and model sizes of GreenKGC with several representative KGC methods on link prediction datasets. Experimental results show that GreenKGC can achieve good performance in low dimensions, i.e. 8, 16, 32 dimensions, compared with SOTA low-dimensional methods. In addition, GreenKGC shows competitive or better performance compared to the high-dimensional KGE methods with a much smaller model size. We also conduct experiments on a large-scale link prediction datasets with over 2.5M entities and show that GreenKGC can perform well with much fewer model parameters. Ablation studies are also conducted to show the effectiveness of each module in GreenKGC."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 KGE Methods",
            "text": "Distance-based KGE methods model relations as affine transformations from head entities to tail entities. For example, TransE [Bordes et al., 2013] models relations as translations, while RotatE [Sun et al., 2019] models relations as rotations in the complex embedding space for better expressiveness on symmetric relations. Recent work has tried to model relations as scaling [Chao et al., 2021] and reflection [Zhang et al., 2022] operations in order to handle particular relation patterns. Semantic-matching KGE methods, such as RESCAL [Lin et al., 2015] and DistMult [Bordes et al., 2014], formulate the scoring functions as similarities among head, relation, and tail embeddings. ComplEx [Trouillon et al., 2016] extends such methods to a complex space for better expressiveness on asymmetric relations. Recently,\nTuckER [Balazevic et al., 2019] and AutoSF [Zhang et al., 2020] allow more flexibility in modeling similarities. Though KGE methods are simple, they often require a high-dimensional embedding space to be expressive."
        },
        {
            "heading": "2.2 Classification-based KGC Methods",
            "text": "NTN [Socher et al., 2013] adopts a neural tensor network combined with textual representations of entities. ConvKB [Nguyen et al., 2018] uses 1 \u00d7 3 convolutional filters followed by several fully connected (FC) layers to predict triple scores. ConvE [Dettmers et al., 2018] reshapes entity and relation embeddings into 2D images and uses 3\u00d7 3 convolutional filters followed by several FC layers to predict the scores of triples. Though NN-based methods can achieve good performance in a lower dimension, they have several drawbacks, such as long inference time and large model. KGBoost [Wang et al., 2022] is a classification-based method that doesn\u2019t use NNs. Yet, it assigns one classifier for each relation so it\u2019s not scalable to large-scale datasets."
        },
        {
            "heading": "2.3 Low-dimensional KGE Methods",
            "text": "Recently, research on the design of low-dimensional KGE methods has received attention. MuRP [Bala\u017eevic\u0301 et al., 2019] embeds entities and relations in a hyperbolic space due to its effectiveness in modeling hierarchies in KGs. AttH [Chami et al., 2020] improves hyperbolic KGE by leveraging hyperbolic isometries to model logical patterns. MulDE [Wang et al., 2021] adopts Knowledge Distillation [Hinton et al., 2015] on a set of hyperbolic KGE as teachers to learn powerful embeddings in low dimensions. However, embeddings in hyperbolic space are hard to be used in other downstream tasks. In Euclidean space, DualDE [Zhu et al., 2022] adopts Knowledge Distillation to learn low-dimensional embeddings from high-dimensional ones for smaller model sizes and faster inference time. Yet, it requires a long training time to reduce feature dimension. GreenKGC has two clear advantages over existing low-dimensional methods. First, it fully operates in the Euclidean space. Second, it does not need to train new low-dimensional embeddings from scratch, thus requiring a shorter dimension reduction time."
        },
        {
            "heading": "3 Methodology",
            "text": "GreenKGC is presented in this section. It consists of three modules: representation learning, feature pruning, and decision learning, to obtain discriminant low-dimensional triple features and predict triple scores accurately. An overview of GreenKGC is given in Fig. 2. Details of each module will be elaborated below."
        },
        {
            "heading": "3.1 Representation Learning",
            "text": "We leverage existing KGE models, such as TransE [Bordes et al., 2013] and RotatE [Sun et al., 2019], to obtain good initial embeddings for entities and relations, where their embedding dimensions can be high to be expressive. Yet, the\ninitial embedding dimension will be largely reduced in the feature pruning module. In general, GreenKGC can build upon any existing KGE models. We refer to the KGE models used in GreenKGC as our baseline models. We include the training details for baseline models in Appendix 6.1 as they are not the main focus of this paper."
        },
        {
            "heading": "3.2 Feature Pruning",
            "text": "In this module, a small subset of feature dimensions in high-dimensional KG representations from Module 1 are preserved, while the others are pruned, to form low-dimensional discriminant KG features.\nDiscriminant Feature Test (DFT). DFT is a supervised feature selection method recently proposed in Yang et al. [2022]. All training samples have a high-dimensional feature set as well as the corresponding labels. DFT scans through each dimension in the feature set and computes its discriminability based on sample labels. DFT can be used to reduce the dimensions of entity and relation embeddings while preserving their power in downstream tasks such as KGC.\nHere, we extend DFT to the multivariate setting since there are multiple variables in each triple. For example, TransE [Bordes et al., 2013] has 3 variables (i.e. h, r, and t) in each feature dimension. First, for each dimension i, we learn a linear transformation wi to map multiple variables [hi, ri, ti] to a single variable xi in each triple, where hi, ri, ti represents the i-th dimension in the head, relation, and tail representations, respectively. Such a linear transformation can be learned through principal component analysis (PCA) using singular value decomposition (SVD). As a result, wi is the first principal component in PCA. However, linear transformations learned from PCA are unsupervised and cannot separate observed triples from negatives well. Alternatively, we learn the linear transformation through logistic regression by minimizing the binary cross-entropy loss\nL = \u2212 y log(\u03c3(wi[hi, ri, ti]T )) \u2212 (1\u2212 y) log(1\u2212 \u03c3(wi[hi, ri, ti]T )),\n(1)\nwhere y = 1 for observed triples (h, r, t) and y = 0 for corrupted triples (h\u2032, r, t\u2032). Afterward, we can apply the standard DFT to each dimension.\nDFT adopts cross-entropy (CE) to evaluate the discriminant power of each dimension as CE is a typical loss for binary classification. Dimensions with lower CE imply higher discriminant power. We preserve the feature dimensions with the lowest CE and prune the remaining to obtain low-dimensional features. Details for training DFT are given in Appendix 6.2.\nKG partitioning. Given that relations in KGs could be different (e.g. symmetric v.s. asymmetric and films v.s. sports), a small subset of feature dimensions might not be discriminant for all relations. Thus, we first partition them into disjoint relations groups, where relations in each group have similar properties. Then, we perform feature pruning within each relation group and select the powerful feature dimensions correspondingly.\nWe hypothesize that relations that have similar properties are close in the embedding space. Therefore, we use k-Means to cluster relation embeddings into relation groups. To verify our hypothesize, we show the grouping results on WN18RR in Table 1. Without categorizing relations into different logical patterns explicitly, relations of similar patterns\ncan be clustered together in the embedding space. For example, most relations in cluster #0 are symmetric ones. All relations in the cluster #1 are N-to-1. The remaining two relations in cluster #2 are 1-to-N with the highest tail-per-head ratio. While we observe cardinality-based grouping for relations in WN18RR, which mostly contains abstract concepts, for FB15k-237 and YAGO3-10, relations with similar semantic meanings are often grouped after KG partitioning.\nFurthermore, we evaluate how different numbers of relation groups, k, can affect the feature pruning process. In Fig. 3, as the lower CE reflects more discriminant features, we can obtain more powerful features when k becomes larger, i.e. partitioning KG into more relation groups. Thus, for each dataset, we select the optimal k when the average CE starts to converge. We elaborate on the high-level intuition on why combining feature pruning and KG partitioning works with KGE models. First, KGE models are isotropic, meaning each dimension can be handled by DFT independently. Second, some feature dimensions are more powerful than others in different relations. Thus, we group relations that with the same discriminant feature dimensions for parameter savings."
        },
        {
            "heading": "3.3 Decision Learning",
            "text": "We formulate KGC as a binary classification problem in each relation group. We adopt binary classifiers as decoders since they are more powerful than simple scoring functions. The binary classifiers take pruned triple features as inputs and predict soft probabilities (between 0 and 1) of triples as outputs. We also conduct classifier training with hard negative mining so as to train a powerful classifier.\nBinary classification. The binary classifiers, g(\u2217), take a low-dimensional triple feature x and predict a soft label y\u0302 = g(x) \u2208 [0, 1]. The label y = 1 for the observed triples and y = 0 for the sampled negatives. We train a binary classifier by minimizing the following negative log-likelihood loss:\nl(y, y\u0302) = \u2212 y log(y\u0302) \u2212 (1\u2212 y) log(1\u2212 y\u0302), (2)\nIn general, we select a nonlinear classifier to accommodate nonlinearity in sample distributions.\nNegative sampling. Combining KGE with classifiers is non-trivial because it\u2019s challenging to obtain high-quality negative samples for classifier training, given that negative samples are not explicitly labeled in the KGs. Therefore, it is desired to mine hard negative cases for baseline KGE models so as to train a powerful classifier. We propose two negative sampling schemes for classifier training. First, most KGE models can only capture the coarse entity type information. For example, they may predict a location given the query (Mary, born_in, ?) yet without an exact answer. Thus, we draw negative samples within the entity types constrained by relations [Krompa\u00df et al., 2015] to enhance the capability to predict the exact answer. Such a negative sampling scheme is called ontology-based negative sampling. We also investigate the sampling of hard negatives that cannot be trivially obtained from original KGE methods. Negatives with higher embedding scores fr(hi, ti) tend to be predicted wrongly in the baseline methods. To handle it, we rank all\nrandomly sampled negative triples and select the ones with higher embedding scores as hard negatives for classifier training. Such a negative sampling strategy is called embedding-based negative sampling."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets. We consider four link prediction datasets for performance benchmarking: FB15k-237 [Bordes et al., 2013, Toutanova and Chen, 2015], WN18RR [Bordes et al., 2013, Dettmers et al., 2018], YAGO3-10 [Dettmers et al., 2018], and ogbl-wikikg2 [Hu et al., 2020]. Their statistics are summarized in Table 2. FB15k-237 is a subset of Freebase [Bollacker et al., 2008] that contains real-world relationships. WN18RR is a subset of WordNet [Miller, 1995] containing lexical relationships between word senses. YAGO3-10 is a subset of YAGO3 [Mahdisoltani et al., 2014] that describes the attributes of persons. ogbl-wikikg2 is extracted from wikidata [Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014] capturing the different types of relations between entities in the world. Among the four, ogbl-wikikg2 is a large-scale dataset with more than 2.5M entities.\nImplementation details. We adopt TransE [Bordes et al., 2013] and RotatE [Sun et al., 2019] as the baseline models and learn 500 dimensions initial representations for entities and relations. The feature dimensions are then reduced in the feature pruning process. We compare among GreenKGC using RotatE as the baseline in all ablation studies. To partition the KG, we determine the number of groups k for each dataset when the average cross-entropy of all feature dimensions converges. As a result, k = 3 for WN18RR, k = 5 for FB15k-237 and YAGO3-10, and k = 20 for ogbl-wikikg2.\nFor decision learning, we consider several tree-based binary classifiers, including Decision Trees [Breiman et al., 2017], Random Forest [Breiman, 2001], and Gradient Boosting Machines [Chen and Guestrin, 2016], as they match the intuition of the feature pruning process and can accommodate non-linearity in the sample distribution. The hyperparameters are searched among: tree depth l \u2208 {3, 5, 7}, number of estimators n \u2208 {400, 800, 1,200, 1,600, 2,000}, and learning rate lr \u2208 {0.05, 0.1, 0.2}. The best settings are chosen based on MRR in the validation set. As a result, we adopt Gradient Boosting Machine for all datasets. l = 5, n = 1200, lr = 0.2 for FB15k-237 and YAGO3-10, l = 3, n = 1600, lr = 0.1 for WN18RR, and l = 7, n = 2000, lr = 0.05 for ogbl-wikikg2. We adopt ontology-based negative sampling to train classifiers for FB15k-237, YAGO3-10, and ogbl-wikikg2, and embedding-based negative\nsampling for WN18RR. Baseline KGEs are trained on NVIDIA Tesla P100 GPUs and binary classifiers are trained on AMD EPYC 7542 CPUs.\nEvaluation metrics. For the link prediction task, the goal is to predict the missing entity given a query triple, i.e. (h, r, ?) or (?, r, t). The correct entity should be ranked higher than other candidates. Here, several common ranking metrics are used, such as MRR (Mean Reciprocal Rank) and Hits@k (k=1, 3, 10). Following the convention in Bordes et al. [2013], we adopt the filtered setting, where all entities serve as candidates except for the ones that have been seen in training, validation, or testing sets."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Results in low dimensions. In Table 3, we compare GreenKGC with KGE, classification-based, and low-dimensional KGE methods in low dimensions, i.e. d = 32. Results for other methods in Table 3 are either directly taken from [Chami et al., 2020, Zhu et al., 2022] or, if not presented, trained by ourselves using publicly available implementations with hyperparameters suggested by the original papers. KGE methods cannot achieve good performance in low dimensions due to over-simplified scoring functions. Classification-based methods achieve performance better than KGE methods as they adopt NNs as complex decoders. Low-dimensional KGE methods provide state-of-the-art KGC solutions in low\ndimensions. Yet, GreenKGC outperforms them in FB15k-237 and YAGO3-10 in all metrics. For WN18RR, the baseline KGE methods perform poorly in low dimensions. GreenKGC is built upon KGEs, so this affects the performance of GreenKGC in WN18RR. Thus, GreenKGC is more suitable for instance-based KGs, such as Freebase and YAGO, while hyperbolic KGEs, such as MuRP and AttH model the concept-based KGs, such as WordNet, well.\nWe show the performance curves of various methods as a function of embedding dimensions in Fig. 4. We see that the performance of KGE methods (i.e. TransE and RotatE) drops significantly as the embedding dimension is lower. For ConvKB, although its performance is less influenced by dimensions due to a complex decoder, it performs poorly compared to other methods in general. For ConvE, although it claims it\u2019s more efficient in parameter scaling [Dettmers et al., 2018], its performance actually degrades significantly in dimensions lower than 64. In addition, it also doesn\u2019t perform well when the dimension is larger. Thus, the performance of ConvE is sensitive to the embedding dimension. MuRP, AttH, and GreenKGC are the only methods that can offer reasonable performance as the dimension goes to as low as 8 dimensions.\nComparison with baseline KGE. One unique characteristic of GreenKGC is to prune a high-dimensional KGE into low-dimensional triple features and make predictions with a binary classifier as a powerful decoder. We evaluate the capability of GreenKGC in saving the number of parameters and maintaining the performance by pruning original 500-dimensional KGE to 100-dimensional triple features in Table 4. As shown in the table, GreenKGC can achieve competitive or even better performance with around 5 times smaller model size. Especially, Hits@1 is retained the most and even improved compared to the high-dimensional baselines. In addition, GreenKGC using TransE as the baseline can outperform high-dimensional TransE in all datasets. Since the TransE scoring function is simple and fails to model some relation patterns, such as symmetric relations, incorporating TransE with a powerful decoder, i.e. a binary classifier, in GreenKGC successfully overcomes deficiencies of adopting an over-simplified scoring function. For all datasets, 100-dimensional GreenKGC could generate better results than 100-dimensional baseline models.\nWe further compare GreenKGC and its baseline KGEs on a large-scale dataset, ogbl-wikikg2. Table 5 shows the results. We reduce the feature dimensions from 500 to 100 for RotatE and 250 to 50 for TransE and achieve a 5x smaller model size while retaining around 80% of the performance. Compared with the baseline KGEs in the same feature dimension, GreenKGC can improve 51.6% in MRR for RotatE and 37.2% in MRR for TransE. Therefore, the results demonstrate the advantages in performance to apply GreenKGC to large-scale KGs in a constrained resource."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "Feature pruning. We evaluate the effectiveness of the feature pruning scheme in GreenKGC in Table 6. We use \u201cw/o pruning\" to denote the baseline 32 dimensions KGE directly followed by the decision learning module. Also, we compare the following feature pruning schemes: 1) random pruning, 2) pruning based on variance, 3) pruning based on feature importance from a Random Forest classifier, 4) pruning dimensions with low CE (i.e. the most discriminant\nones), in DFT, and 5) pruning dimensions with high CE (i.e. the least discriminant ones) in DFT. As shown in the table, our method to prune the least discriminant features in DFT achieves the best performance on both datasets. In contrast, pruning the most discriminant features in DFT performs the worst. Thus, DFT module can effectively differentiate the discriminability among different features. Using variance to prune achieves similar results as \u201cw/o pruning\" and random pruning. Pruning based on feature importance shows better results than \u201cw/o pruning\", random and pruning, and pruning based on variance, but performs worse than DFT. In addition, feature importance needs to consider all feature dimensions at once, while in DFT, each feature dimension is processed individually. Thus, DFT is also more memory efficient than calculating feature importance.\nFig. 5 plots the sorted discriminability of features in different pruning schemes. From the figure, the high variance region is flat, so it\u2019s difficult to identify the most discriminant features using their variances. For feature importance, some of the feature dimensions have zero scores. Therefore, pruning based on feature importance might ignore some discriminant features. In the DFT curve, there is a \u201cshoulder point\" indicating only around 100 feature dimensions are more discriminant than the others. In general, we can get good performance in low dimensions as long as we preserve dimensions lower than the shoulder point and prune all other dimensions.\nKG partitioning. Figure 6 shows GreenKGC performance with different numbers of relation groups k, where k = 1 means no KG partitioning. A larger k will give a better performance on both FB15k-237 and WN18RR. Without using KG partitioning performs much worse than using KG partitioning. Note that with a larger k, GreenKGC has more model parameters since we need more classifiers. The model complexity is O(|E|d + k\u0398), where \u0398 is the model complexity for the classifier. Thus, we can adjust k based on the tradeoff of performance convergence and memory efficiency."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "A lightweight KGC method, called GreenKGC, was proposed in this work to make accurate link predictions in low dimensions. It consists of three modules that can be trained individually: 1) representation learning, 2) feature pruning, and 3) decision learning. Experimental results in low dimensions demonstrate GreenKGC can achieve satisfactory performance in as low as 8 dimensions. In addition, experiments on ogbl-wikikg2 show GreenKGC can get competitive results with much fewer model parameters. Furthermore, the ablation study shows the effectiveness of KG partitioning and feature pruning.\nModularized GreenKGC allows several future extensions. First, GreenKGC can be combined with new embedding models as initial features. In general, using a more expressive KGE model can lead to better final performance. Second, individual modules can be fine-tuned for different applications. For example, since the feature pruning module and the decision-learning module are supervised, they can be applied to various applications. Finally, different negative sampling strategies can be investigated in different applications.\nReference Ivana Balazevic, Carl Allen, and Timothy Hospedales. TuckER: Tensor factorization for knowledge graph completion. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5185\u20135194, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1522. URL https://aclant hology.org/D19-1522.\nIvana Bala\u017eevic\u0301, Carl Allen, Timothy Hospedales, and First Last. Multi-relational poincar\u00e9 graph embeddings. Advances in Neural Information Processing Systems, 32, 2019.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250, 2008.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.\nAntoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. A semantic matching energy function for learning with multi-relational data. Machine Learning, 94(2):233\u2013259, 2014.\nLeo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.\nLeo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and regression trees. Routledge, 2017.\nInes Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher R\u00e9. Low-dimensional hyperbolic knowledge graph embeddings. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6901\u20136914, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020 .acl-main.617. URL https://aclanthology.org/2020.acl-main.617.\nLinlin Chao, Jianshan He, Taifeng Wang, and Wei Chu. PairRE: Knowledge graph embeddings via paired relation vectors. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4360\u20134369, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.336. URL https://aclanthology.org/2021.acl-long.336.\nTianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.\nJoachim Daiber, Max Jakob, Chris Hokamp, and Pablo N Mendes. Improving efficiency and accuracy in multilingual entity extraction. In Proceedings of the 9th international conference on semantic systems, pages 121\u2013124, 2013.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D knowledge graph embeddings. In Thirty-second AAAI conference on artificial intelligence, 2018.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/abs/1503.02531.\nRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 541\u2013550, 2011.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118\u201322133, 2020.\nXiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 105\u2013113, 2019.\nDenis Krompa\u00df, Stephan Baier, and Volker Tresp. Type-constrained representation learning in knowledge graphs. In International semantic web conference, pages 640\u2013655. Springer, 2015.\nC.-C. Jay Kuo and Azad M Madni. Green learning: Introduction, examples and outlook. Journal of Visual Communication and Image Representation, page 103685, 2022.\nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Twenty-ninth AAAI conference on artificial intelligence, 2015.\nFarzaneh Mahdisoltani, Joanna Biega, and Fabian Suchanek. YAGO3: A knowledge base from multilingual wikipedias. In 7th biennial conference on innovative data systems research. CIDR Conference, 2014.\nGeorge A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995.\nDai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. A novel embedding model for knowledge base completion based on convolutional neural network. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 327\u2013333, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2053. URL https://aclanthology.org/N18-2053.\nApoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4498\u20134507, 2020.\nRichard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. Advances in neural information processing systems, 26, 2013.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. RotatE: Knowledge graph embedding by relational rotation in complex space. In International Conference on Learning Representations, 2019. URL https://openreview.n et/forum?id=HkgEQnRqYQ.\nZhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha Talukdar, and Yiming Yang. A re-evaluation of knowledge graph completion methods. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5516\u20135522, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020 .acl-main.489. URL https://aclanthology.org/2020.acl-main.489.\nKristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd workshop on continuous vector space models and their compositionality, pages 57\u201366, 2015.\nTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International conference on machine learning, pages 2071\u20132080. PMLR, 2016.\nDenny Vrandec\u030cic\u0301 and Markus Kr\u00f6tzsch. Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10):78\u201385, 2014.\nKai Wang, Yu Liu, Qian Ma, and Quan Z Sheng. Mulde: Multi-teacher knowledge distillation for low-dimensional knowledge graph embeddings. In Proceedings of the Web Conference 2021, pages 1716\u20131726, 2021.\nXiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. Explainable reasoning over knowledge graphs for recommendation. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 5329\u20135336, 2019.\nYun-Cheng Wang, Xiou Ge, Bin Wang, and C.-C. Jay Kuo. KGBoost: A classification-based knowledge base completion method with negative sampling. Pattern Recognition Letters, 157:104\u2013111, 2022.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.\nYikun Xian, Zuohui Fu, Shan Muthukrishnan, Gerard De Melo, and Yongfeng Zhang. Reinforcement knowledge graph reasoning for explainable recommendation. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pages 285\u2013294, 2019.\nHan Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. TransA: An adaptive approach for knowledge graph embedding. CoRR, abs/1509.05490, 2015. URL http://arxiv.org/abs/1509.05490.\nYijing Yang, Wei Wang, Hongyu Fu, C-C Jay Kuo, et al. On supervised feature selection from high dimensional feature spaces. APSIPA Transactions on Signal and Information Processing, 11(1), 2022.\nQianjin Zhang, Ronggui Wang, Juan Yang, and Lixia Xue. Knowledge graph embedding by reflection transformation. Knowledge-Based Systems, 238:107861, 2022.\nYongqi Zhang, Quanming Yao, Wenyuan Dai, and Lei Chen. AutoSF: Searching scoring functions for knowledge graph embedding. In 2020 IEEE 36th International Conference on Data Engineering (ICDE), pages 433\u2013444. IEEE, 2020.\nYushan Zhu, Wen Zhang, Mingyang Chen, Hui Chen, Xu Cheng, Wei Zhang, and Huajun Chen. Dualde: Dually distilling knowledge graph embedding for faster and cheaper reasoning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 1516\u20131524, 2022."
        },
        {
            "heading": "6 Appendix",
            "text": ""
        },
        {
            "heading": "6.1 Training Procedure for Baseline KGE Models",
            "text": "To train the baseline KGE model as the initial entity and relation representations, we adopt the self-adversarial learning process in Sun et al. [2019] and use this codebase2. That is, given an observed triple (h, r, t) and the KGE model fr(h, t), we minimize the following loss function\nL = \u2212 log(\u03c3(fr(h, t)))\n\u2212 n\u2211\ni=1\np(h\u2032i, r, t \u2032 i) log(\u03c3(\u2212fr(h\u2032i, t\u2032i))),\n(3)\nwhere (h\u2032i, r, t \u2032 i) is a negative sample and\np(h\u2032j , r, t \u2032 j) =\nexp(\u03b1fr(h \u2032 j , t \u2032 j))\u2211n\ni=1 exp(\u03b1fr(h \u2032 i, t \u2032 i))\n, (4)\nwhere \u03b1 is the temperature to control the self-adversarial negative sampling. We summarize the scoring functions for some common KGE models and their corresponding number of variables per dimension in Table 7. In general, GreenKGC can build upon any existing KGE models."
        },
        {
            "heading": "6.2 DFT Implementation Details",
            "text": "To calculate the discriminant power of each dimension, we iterate through each dimension in the high-dimension feature set and calculate the discriminant power based on sample labels. More specifically, we model KGC as a binary classification task. We assign label yi = 1 to the ith sample if it is an observed triple and yi = 0 if it is a negative sample. For the dth dimension, we split the 1D feature space into left and right subspaces and calculate the cross-entropy in the form of\nH(d) = NLH\n(d) L +NRH (d) R\nNL +NR , (5)\nwhere NL and NR are the numbers of samples in the left and right intervals, respectively,\nH (d) L = \u2212PL,1 log(PL,1)\u2212 PL,0 log(PL,0), (6)\nH (d) R = \u2212PR,1 log(PR,1)\u2212 PR,0 log(PR,0), (7)\nand where PL,1 = 1NL \u2211NL\ni=1 yi, and PL,0 = 1\u2212 PL,1 and similarly for PR,1 and PR,0. A lower cross-entropy value implies higher discriminant power.\nFig. 7 shows histograms of linearly transformed 1D triple variables in two different feature dimensions. As seen in the figure, samples in Fig. 7 (a), i.e. the feature dimension with the lower cross-entropy, are more separable than that in Fig. 7 (b), i.e. the feature dimension with the higher cross-entropy. Therefore, a lower cross-entropy implies a more discriminant feature dimension.\n2https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding"
        },
        {
            "heading": "6.3 Relation Categories",
            "text": "We further evaluate GreenKGC in different relation categories. Following the convention in Wang et al. [2014], we divide the relations into four categories: 1-to-1, 1-to-N, N-to-1, and N-to-N. They are characterized by two statistical numbers, head-per-tail (hpt), and tail-per-head (tph), of the datasets. If tph < 1.5 and hpt < 1.5, the relation is treated as 1-to-1; if tph < 1.5 and hpt \u2265 1.5, the relation is treated as 1-to-N; if tph \u2265 1.5 and hpt < 1.5, the relation is treated as N-to-1; if tph \u2265 1.5 and hpt \u2265 1.5, the relation is treated as N-to-N. Table 8 summarizes the results for different relation categories in FB15k-237 under 32 dimensions. In the lowdimensional setting, GreenKGC is able to outperform other methods in all relation categories. Specifically, GreenKGC performs especially well for many-to-1 predictions (i.e. predicting heads for 1-to-N relations, and predicting tails for N-to-1 relations). Such results demonstrate the advantage of using classifiers to make accurate predictions when there is only one valid target."
        },
        {
            "heading": "6.4 Time Analysis on Feature Pruning",
            "text": "Table 9 shows the required training time for DualDE [Zhu et al., 2022], a knowledge distillation method, and GreenKGC, to reduce 512 dimensions TransE embeddings to 100 dimensions. As shown in the table, GreenKGC achieves around 20x faster training time compared to DualDE, especially in YAGO3-10, which is a larger-scale dataset. Besides, in knowledge distillation methods, low-dimensional embeddings are randomly initialized and trained with the guidance of high-dimensional embeddings. Thus, the quality of the low-dimensional embeddings highly depends on good initialization. On the contrary, the feature pruning process in GreenKGC selects a subset of powerful feature dimensions without learning new features from scratch. In addition, it is also memory-efficient since it only processes one feature dimension at once."
        },
        {
            "heading": "6.5 Comparison with NN-based Methods",
            "text": "Inference time analysis. We compare GreenKGC with two other NN-based methods in Table 10 in terms of performance, number of free parameters, and inference time. They are ConvKB [Nguyen et al., 2018] and ConvE [Dettmers et al., 2018]. We adopt TransE as the baseline in GreenKGC to match the number of parameters in\nthe embedding layer for a fair comparison. As compared with ConvKB, GreenKGC achieves significantly better performance with slightly more parameters. As compared with ConvE, GreenKGC uses fewer parameters and demands a shorter inference time since ConvE adopts a multi-layer architecture. GreenKGC also offers better performance compared to ConvE.\nPrediction distribution. It was reported in Sun et al. [2020] that the predicted scores for all candidates on FB15k-237 are converged to 1 with ConvKB [Nguyen et al., 2018]. This is unlikely to be true, given the fact that KGs are often highly sparse. The issue is resolved after ConvKB is implemented with PyTorch3, but the performance on FB15k-237 is still not as good as ConvKB originally reported in the paper. The issue shows the problem of end-to-end optimization. That is, it is difficult to control and monitor every component in the model. This urges us to examine whether GreenKGC has the same issue. Fig. 8 shows the sorted predicted scores of a query (38th Grammy Awards, award_winner, ?) in FB15k-237. We see from the figure that only very few candidates have positive scores close to 1, while other candidates receive negative scores of 0. The formers are valid triples. The score distribution is consistent with the sparse nature of KGs."
        },
        {
            "heading": "6.6 Ablation on Negative Sampling",
            "text": "We evaluate the effectiveness of the two proposed negative sampling (i.e., ontology- and embedding-based) methods in Table 11. In FB15k-237, both are more effective than randomly drawn negative samples. The ontology-based one gives better results than the embedding-based one. In WN18RR, the embedding-based one achieves the best results. Since there is no clear entity typing in WordNet, the ontology-based one performs worse than the randomly drawn one. We can conclude that to correct failure cases in the baseline KGE, ontology-based negative sampling is effective for KGs consisting of real-world instances, such as FB15k-237, while embedding-based negative sampling is powerful for concept KGs such as WN18RR."
        },
        {
            "heading": "6.7 Performance as Training Progresses",
            "text": "We plot the AUC-PR and MRR curve for training/validation, and testing in Fig. 9a and Fig. 9b, respectively. We use AUC-PR to monitor the training of the classifiers. AUC-PR starts to converge for both training and validation sets after 200 iterations. We record the link prediction results on the testing set every 100 iterations. Though the AUC-PR improves slightly after 200 iterations, the MRR starts to converge after 600 iterations.\n3https://github.com/daiquocnguyen/ConvKB/issues/5"
        }
    ],
    "title": "GREENKGC: A LIGHTWEIGHT KNOWLEDGE GRAPH COMPLETION METHOD",
    "year": 2023
}