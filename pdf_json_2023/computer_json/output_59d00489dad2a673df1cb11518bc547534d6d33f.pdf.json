{
    "abstractText": "Due to the prevalence of scale variance in nature images, we propose to use image scale as a self-supervised signal for Masked Image Modeling (MIM). Our method involves selecting random patches from the input image and downsampling them to a low-resolution format. Our framework utilizes the latest advances in superresolution (SR) to design the prediction head, which reconstructs the input from low-resolution clues and other patches. After 400 epochs of pre-training, our Super Resolution Masked Autoencoders (SRMAE) get an accuracy of 82.1% on the ImageNet-1K task. Image scale signal also allows our SRMAE to capture scale invariance representation. For the very low resolution (VLR) recognition task, our model achieves the best performance, surpassing DeriveNet by 1.3%. Our method also achieves an accuracy of 74.84% on the task of recognizing low-resolution facial expressions, surpassing the current state-of-the-art FMD by 9.48%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiming Wang"
        }
    ],
    "id": "SP:e7112ada0449a0b3b5d1b11c01b6624e751155a0",
    "references": [
        {
            "authors": [
                "Omid Abdollahi Aghdam"
            ],
            "title": "Exploring Factors for Improving Low Resolution Face Recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
            "year": 2019
        },
        {
            "authors": [
                "Sungsoo Ahn"
            ],
            "title": "Variational information distillation for knowledge transfer",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "Aharon Azulay",
                "Yair Weiss"
            ],
            "title": "Why do deep convolutional networks generalize so poorly to small image transformations?",
            "year": 2018
        },
        {
            "authors": [
                "Alexei Baevski"
            ],
            "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
            "venue": "Proceedings of the 39th International Conference on Machine Learning. Ed. by Kamalika Chaudhuri et al",
            "year": 2022
        },
        {
            "authors": [
                "Hangbo Bao"
            ],
            "title": "BEiT: BERT Pre-Training of Image Transformers",
            "year": 2021
        },
        {
            "authors": [
                "Mathilde Caron"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision",
            "year": 2021
        },
        {
            "authors": [
                "Hanting Chen"
            ],
            "title": "Pre-Trained Image Processing Transformer",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2021
        },
        {
            "authors": [
                "Jun Chen"
            ],
            "title": "Efficient self-supervised vision pretraining with local masked reconstruction",
            "year": 2022
        },
        {
            "authors": [
                "Mark Chen"
            ],
            "title": "Generative pretraining from pixels",
            "venue": "In: International conference on machine learning. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "Yizhen Chen",
                "Haifeng Hu"
            ],
            "title": "Facial expression recognition by inter-class relational learning",
            "venue": "IEEE Access",
            "year": 2019
        },
        {
            "authors": [
                "Tao Dai"
            ],
            "title": "Second-Order Attention Network for Single Image Super-Resolution",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2019
        },
        {
            "authors": [
                "Jia Deng"
            ],
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018",
            "venue": "DOI: 10.48550/ARXIV.1810.04805. URL: https://arxiv.org/",
            "year": 2018
        },
        {
            "authors": [
                "Chao Dong"
            ],
            "title": "Image Super-Resolution Using Deep Convolutional Networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2016
        },
        {
            "authors": [
                "Xiaoyi Dong"
            ],
            "title": "Peco: Perceptual codebook for bert pre-training of vision transformers",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "DOI: 10.48550/ARXIV.2010.11929. URL: https://arxiv.org/abs/",
            "year": 2020
        },
        {
            "authors": [
                "Shiming Ge"
            ],
            "title": "Low-Resolution Face Recognition in the Wild via Selective Knowledge Distillation",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2019
        },
        {
            "authors": [
                "Raghav Goyal"
            ],
            "title": "The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)",
            "year": 2017
        },
        {
            "authors": [
                "Klemen Grm",
                "Walter J. Scheirer",
                "Vitomir \u0160truc"
            ],
            "title": "Face Hallucination Using Cascaded Super-Resolution and Identity Priors",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2020
        },
        {
            "authors": [
                "Shuhang Gu",
                "Nong Sang",
                "Fan Ma"
            ],
            "title": "Fast image super resolution via local regression",
            "venue": "Proceedings of the 21st International Conference on Pattern Recognition",
            "year": 2012
        },
        {
            "authors": [
                "Kaiming He",
                "Jian Sun",
                "Xiaoou Tang"
            ],
            "title": "Single image haze removal using dark channel prior",
            "venue": "IEEE transactions on pattern analysis and machine intelligence",
            "year": 2010
        },
        {
            "authors": [
                "Kaiming He"
            ],
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "Zibin He"
            ],
            "title": "Fakd: Feature-affinity based knowledge distillation for efficient image superresolution",
            "venue": "IEEE International Conference on Image Processing (ICIP). IEEE",
            "year": 2020
        },
        {
            "authors": [
                "Byeongho Heo"
            ],
            "title": "A comprehensive overhaul of feature distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Byeongho Heo"
            ],
            "title": "Knowledge transfer via distillation of activation boundaries formed by hidden neurons",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "year": 2019
        },
        {
            "authors": [
                "Lang Huang"
            ],
            "title": "Green hierarchical vision transformer for masked image modeling",
            "year": 2022
        },
        {
            "authors": [
                "Zhenhua Huang"
            ],
            "title": "Feature map distillation of thin nets for low-resolution object recognition",
            "venue": "IEEE Transactions on Image Processing",
            "year": 2022
        },
        {
            "authors": [
                "Zheng Hui",
                "Xiumei Wang",
                "Xinbo Gao"
            ],
            "title": "Fast and Accurate Single Image Super-Resolution via Information Distillation Network",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2018
        },
        {
            "authors": [
                "Zheng Hui"
            ],
            "title": "Lightweight Image Super-Resolution with Information Multi-Distillation Network",
            "venue": "Proceedings of the 27th ACM International Conference on Multimedia. MM \u201919",
            "year": 2019
        },
        {
            "authors": [
                "Ylva Jansson",
                "Tony Lindeberg"
            ],
            "title": "Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales",
            "venue": "Journal of Mathematical Imaging and Vision",
            "year": 2022
        },
        {
            "authors": [
                "Hadi Kazemi",
                "Fariborz Taherkhani",
                "Nasser M. Nasrabadi"
            ],
            "title": "Identity-Aware Deep Face Hallucination via Adversarial Face Verification",
            "venue": "IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",
            "year": 2019
        },
        {
            "authors": [
                "Jangho Kim",
                "SeongUk Park",
                "Nojun Kwak"
            ],
            "title": "Paraphrasing complex network: Network compression via factor transfer",
            "venue": "Advances in neural information processing systems",
            "year": 2018
        },
        {
            "authors": [
                "Nikos Komodakis",
                "Sergey Zagoruyko"
            ],
            "title": "Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer",
            "venue": "ICLR",
            "year": 2017
        },
        {
            "authors": [
                "Micha\u0142 Koziarski",
                "Bogus\u0142aw Cyganek"
            ],
            "title": "Impact of low resolution on image recognition with deep neural networks: An experimental study",
            "venue": "In: International Journal of Applied Mathematics and Computer Science",
            "year": 2018
        },
        {
            "authors": [
                "Shan Li",
                "Weihong Deng",
                "JunPing Du"
            ],
            "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2017
        },
        {
            "authors": [
                "Xiang Li"
            ],
            "title": "Uniform masking: Enabling mae pre-training for pyramid-based vision transformers with locality",
            "year": 2022
        },
        {
            "authors": [
                "Jingyun Liang"
            ],
            "title": "SwinIR: Image Restoration Using Swin Transformer",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. Oct",
            "year": 2021
        },
        {
            "authors": [
                "Bee Lim"
            ],
            "title": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin"
            ],
            "title": "Microsoft COCO: Common Objects in Context",
            "venue": "Ed. by David Fleet et al. Cham: Springer International Publishing,",
            "year": 2014
        },
        {
            "authors": [
                "Tony Lindeberg"
            ],
            "title": "Scale invariant feature transform",
            "year": 2012
        },
        {
            "authors": [
                "Zhisheng Lu"
            ],
            "title": "Transformer for Single Image Super-Resolution",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Lucey"
            ],
            "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
            "year": 2010
        },
        {
            "authors": [
                "Xiaotong Luo"
            ],
            "title": "LatticeNet: Towards Lightweight Image Super-Resolution with Lattice Block",
            "venue": "Computer Vision \u2013 ECCV 2020. Ed. by Andrea Vedaldi et al. Cham: Springer International Publishing,",
            "year": 2020
        },
        {
            "authors": [
                "Fabio Valerio Massoli",
                "Giuseppe Amato",
                "Fabrizio Falchi"
            ],
            "title": "Cross-resolution learning for face recognition",
            "venue": "Image and Vision Computing",
            "year": 2020
        },
        {
            "authors": [
                "Tomer Michaeli",
                "Michal Irani"
            ],
            "title": "Nonparametric blind super-resolution",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision",
            "year": 2013
        },
        {
            "authors": [
                "Yuval Netzer"
            ],
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
            "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning",
            "year": 2011
        },
        {
            "authors": [
                "Shuxin Ouyang"
            ],
            "title": "A survey on heterogeneous face recognition: Sketch, infra-red, 3D and low-resolution",
            "venue": "Image and Vision Computing",
            "year": 2016
        },
        {
            "authors": [
                "Wonpyo Park"
            ],
            "title": "Relational knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "Nikolaos Passalis",
                "Anastasios Tefas"
            ],
            "title": "Learning deep representations with probabilistic knowledge transfer",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Deepak Pathak"
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2016
        },
        {
            "authors": [
                "Baoyun Peng"
            ],
            "title": "Correlation congruence for knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Angelo Porrello",
                "Luca Bergamini",
                "Simone Calderara"
            ],
            "title": "Robust re-identification by multiple views knowledge distillation",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Lu Qi"
            ],
            "title": "Multi-scale aligned distillation for low-resolution detection",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In: International Conference on Machine Learning. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "Adriana Romero"
            ],
            "title": "Fitnets: Hints for thin deep nets",
            "venue": "arXiv preprint arXiv:1412.6550",
            "year": 2014
        },
        {
            "authors": [
                "Maneet Singh"
            ],
            "title": "DeriveNet for (Very) Low Resolution Image Classification",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2022
        },
        {
            "authors": [
                "Maneet Singh"
            ],
            "title": "Dual directed capsule network for very low resolution image recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Maneet Singh"
            ],
            "title": "Dual directed capsule network for very low resolution image recognition",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Wonchul Son"
            ],
            "title": "Densely guided knowledge distillation using multiple teacher assistants",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2021
        },
        {
            "authors": [
                "Long Sun"
            ],
            "title": "Lightweight Image Super-Resolution via Weighted Multi-Scale Residual Network",
            "venue": "IEEE/CAA Journal of Automatica Sinica",
            "year": 2021
        },
        {
            "authors": [
                "Radu Timofte",
                "Vincent De Smet",
                "Luc Van Gool"
            ],
            "title": "A+: Adjusted anchored neighborhood regression for fast super-resolution",
            "venue": "Computer Vision\u2013ACCV 2014: 12th Asian Conference on Computer Vision, Singapore,",
            "year": 2014
        },
        {
            "authors": [
                "Radu Timofte",
                "Vincent De Smet",
                "Luc Van Gool"
            ],
            "title": "Anchored neighborhood regression for fast example-based super-resolution",
            "venue": "Proceedings of the IEEE international conference on computer vision",
            "year": 2013
        },
        {
            "authors": [
                "Hugo Touvron"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In: International conference on machine learning. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "Frederick Tung",
                "Greg Mori"
            ],
            "title": "Similarity-preserving knowledge distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Kafeng Wang"
            ],
            "title": "Pay attention to features, transfer learn faster CNNs",
            "venue": "In: International conference on learning representations",
            "year": 2020
        },
        {
            "authors": [
                "Zhangyang Wang"
            ],
            "title": "Studying Very Low Resolution Recognition Using Deep Networks",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "Zhangyang Wang"
            ],
            "title": "Studying very low resolution recognition using deep networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2016
        },
        {
            "authors": [
                "Chen Wei"
            ],
            "title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "Tete Xiao"
            ],
            "title": "Unified Perceptual Parsing for Scene Understanding",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Zhenda Xie"
            ],
            "title": "SimMIM: A Simple Framework for Masked Image Modeling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2022
        },
        {
            "authors": [
                "Kaipeng Zhang"
            ],
            "title": "Super-identity convolutional neural network for face hallucination",
            "venue": "Proceedings of the European conference on computer vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Kangkai Zhang"
            ],
            "title": "Student network learning via evolutionary knowledge distillation",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Zhang"
            ],
            "title": "CAE v2: Context Autoencoder with CLIP Target. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yulun Zhang"
            ],
            "title": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
            "year": 2018
        },
        {
            "authors": [
                "Zhanpeng Zhang"
            ],
            "title": "From facial expression recognition to interpersonal relation prediction",
            "venue": "In: International Journal of Computer Vision",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou"
            ],
            "title": "Scene Parsing Through ADE20K Dataset",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2017
        },
        {
            "authors": [
                "Jinghao Zhou"
            ],
            "title": "iBOT: Image BERT Pre-Training with Online Tokenizer",
            "venue": "DOI: 10.48550/ARXIV.2111.07832. URL: https://arxiv.org/abs/2111.07832",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Masked Image Modeling (MIM)[5, 23, 71] self-supervisely learns deep representations by masking a portion of input signals and predicting these masked signals. MIM has shown has shown great success in downstream tasks such as image classification [13], object detection [40], semantic segmentation [77], and video classification [19].\nThe MIM method is a form of self-supervised learning that utilizes signals during the learning process. Researchers are currently exploring the use of multiple self-supervised signals for learning through a detailed analysis of the MIM method. MAE consider the original pixel intensity as a self-supervised signal by reconstructing it based on partial observation. MaskFeat demonstrated that masked features like graident histogram is a powerful self-supervised signal. CAEv2 utilizes CLIP as a self-supervised signal.\nIn our study, we propose using scale variance as a self-supervised signal. Scale variance is prevalent in nature images due to the continuous variation of the projection size of an object relative to the human eye or camera sensor as its distance changes [31]. This scale variance poses numerous challenges for modern neural networks [3]. A minute variation in size that is imperceptible to the human eye can cause a significant change in the output. Thus, scale invariance is an integral aspect of the advancement in computer vision[41]. Recent studies have demonstrated that resolution/scale is an efficient self-supervised signal that enables the network to generalize its object detection capability to low-resolution (LR) images.\nWe introduce scale as a self-supervised signal in our proposed novel MIM algorithm called Super Resolution Masked Autoencoders (SRMAE). Fig. 1 illustrates that the input image is divided into a set of patches, some of which are downsampled into low-resolution (LR) patches. On the other hand, the remaining patches are called high-resolution (HR) ones, similar to the MAE [23], where our\nPreprint. Under review.\nar X\niv :2\n30 8.\n08 88\n4v 1\n[ cs\n.C V\n] 1\n7 A\nug 2\nSRMAE also encodes the HR patches. In contrast to MAE, which solely reconstructs the input image from these HR patches, we use both LR and encoded HR patches to reconstruct the original signal. The empty mask token of MAE is substituted with the LR signal in the prediction head. Additionally, we employ a High Preserving Block (HPB) module [42] to extract features from LR patches before reconstructing the original signal with a lightweight ViT (Vision Transformers) structure to benefit from the super-resolution (SR) technique.\nThe original image is termed as high-resolution (HR) image. Similar to the ViT [17], the highresolution image and the low-resolution image are divided into two sets of regular non-overlapping patches. We sample a subset of patches from the high-resolution patches and take the low-resolution patches that are relative to the unsampled high-resolution patches as the subsequent prediction head input. The encoder of SRMAE, similar to the encoder of MAE [23], processes only these sampled high-resolution patches. Nevertheless, unlike the prediction head of MAE, we preserve the lowresolution signals of patches that have not been encoded. The complete set of tokens comprises encoded high-resolution patches and selected low-resolution patches that act as the input to the SRMAE prediction head. In contrast, the full set of tokens for MAE consists of encoded visible patches and mask tokens. We add positional embeddings to all full set tokens, and the prediction head restores the resolution for the selected low-resolution patches. Rather than reconstructing pixels like MAE, we need to recover the resolution. To achieve this goal, we use an HPB module [42] that is appropriate for extracting features before the lightweight ViT structure. In the fine-tuning stage of ImageNet-1K, SRMAE with ViT-B achieved a top-1 accuracy of 82.1%, which is 1% lower than MAE with an equal number of training epochs.\nThe use of self-supervision with the scale signal makes our SRMAE capable of capturing scaleinvariant representations. (Very) low resolution (VLR/LR) recognition is particularly challenging as neuron networks have to recognise under various scales. We thus evaluated the performance of our SRMAE on several VLR/LR tasks. In particular, we conducted a VLR digit classification experiment on the SVHN dataset using images of resolution 8\u00d78. We conducted LR facial expression classification experiments using images of resolution 100\u00d7100 on the CK+, RAF-DB, and ExpW datasets. Our SRMAE shows strong scale-invariant ability when compared with state-of-the-art (SOTA) methods such as DeriveNet[57], which obtained a top-1 classification accuracy of 87.85% on the SVHN dataset, and FMD[28], which achieved 65.37% top-1 accuracy on the ExpW dataset. Our SRMAE has achieved SOTA performance on some VLR/LR recognition datasets. On the SVHN dataset, our SRMAE achieved a top-1 accuracy of 89.14%, which is 1.3% higher than the previous state-of-the-art (87.85%[57]). On the ExpW dataset, our SRMAE achieved a top-1 accuracy of 74.84%, which is a significant improvement of 9.5% over the previous state-of-the-art top-1 accuracy of 65.37%.\nIn this paper, we mainly make the following contributions:\n(i) For the first time, we propose to utilise scale as a self-supervised signal for Masked Image Modeling (MIM). Our SRMAE is a simple and easy-to-implement framework preserving scale invariance.\n(ii) Our framework could leverage the latest super-resolution (SR) architecture advance to design the prediction head that predicts masked original signals from low-resolution clues.\n(iii) SRMAE achieves close to SOTA results in different resolution visual tasks, such as finetuning on ImageNet-1K, VLR digit classification on SVHN dataset and LR facial expression classification on ExpW dataset."
        },
        {
            "heading": "2 Related Work",
            "text": "Masked Image Modeling Masked Image Modeling (MIM) has become a popular pretext task for visual representation learning, inspired by BERT[14] for Masked Language Modeling[5, 23, 71]. The context encoder approach[51] is a groundbreaking work in this field, which masks a rectangular section of the original images and predicts the missing pixels. iGPT[9], ViT[17], and BEiT[5] were the first notable works that recognized the MIM learning strategy on modern vision Transformers. Several target signals have been developed for the mask-prediction pretext task in MIM, such as normalized pixels[23, 71], discrete tokens[5, 16], HOG features[69], deep features[4, 78], and CLIP[74]. A major bottleneck for the industrial applications of MIM is that these models often require large computational resources and substantial pre-training time. As a result, some works have accelerated the process by employing the asymmetric encoder-decoder strategy[23, 27] or reducing the input patches[8, 37]. In this work, we leverage scale as a self-supervised signal to learn scale-invariant representation.\nSuper Resolution Compared to traditional image super resolution methods [21, 63, 62, 46, 22] which are generally model-based, learning based methods have become more popular due to their impressive performance. The convolutional neural network (CNN)-based super-resolution model holds an important position in super-resolution work, with SRCNN[15] being the first model to utilize CNN in SISR. Since then, many works[39, 75, 12, 30, 29, 44] have continuously improved the CNN-based super-resolution model. With the increasing use of Vision Transformer in the field of vision [17], IPT [7] has utilized a transformer-based network as a pre-trained model. SwinIR [38] introduced the Swin Transformer to SISR, while ESRT [42] uses a lightweight CNN backbone to extract deep features and obtain long-range dependency relationships through a lightweight Transformer, combining the CNN and Transformer frameworks. In this study, the HPB module in ESRT [42] was adopted to extract features for resolution recovery since the HPB module aligns well with our prediction head in predicting the original resolution signal from chosen LR patches and remaining HR clues.\nVLR/LR recognition VLR/LR images (or regions of interest) often contain less information content, rendering ineffective feature extraction and classification, thus can intuitively demonstrate the model\u2019s learning ability for the features of low-resolution images. In practical applications, we often face lowquality images, which may be low-resolution, noisy, blurry, or have low dynamic ranges. According to the conclusion in [35], even when the degradation in image quality is imperceptible to the naked eye, the performance of deep neural networks will significantly decline. At the same time, the paper also indicates that super-resolution can effectively alleviate the decrease in classification accuracy caused by low resolution. In existing VLR/LR recognition technology, HR information is usually used to improve the classification model, which can be divided into image-level, feature-level, and classifier-level. In terms of image-level technology, Grm et al. [20] proposed a cascaded superresolution network and a set of face recognition models as priors, while Kazemi et al. completed the same task using a multi-scale generative adversarial network [32]. Researchers have also proposed VLR/LR algorithms that merge HR information at the feature or classifier level [48, 1, 67, 18]. In the work of [57], effective class boundaries were learned through joint training of two losses, and a new data augmentation based on a multi-resolution pyramid was used for training. In this work, the combination of super-resolution and the MIM model has made our model more robust for low-resolution image tasks. Therefore, we demonstrate the scale invariance ability of our model through the VLR/LR recognition task."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 A Revisit of MIM",
            "text": "The MIM framework involves masking parts of the input image and then predicting their information based on the unmasked observation. It consists of four main components: Masking strategy, encoder architecture, prediction head, and prediction target. The masking strategy determines how to select and mask an area. The encoder architecture extracts a latent feature representation for the masked image, which is then used to predict the original signal within the masked area. The prediction head is applied to the latent feature representation to generate one form of the original signals within the masked area. The prediction target defines the form of the original signals to be predicted.\nBoth our SRMAE and MAE belong to the MIM framework. Our SRMAE is a modification of MAE\u2019s structure, so based on the MIM framework, we will first highlight the similarities and differences between our approach and the well-known MAE. MAE treats the original pixel intensity as the prediction target by reconstructing it, while we use scale as our target by recovering the resolution. The masking strategy in MAE samples a subset of patches and remove the remaining ones, sending the remaining visible patches to the encoder for learning latent representation. Our SRMAE gains low-resolution signals through downsampling and utilizes these removed patches in MAE, which will be explained in more detail later. The encoder used in MAE is a ViT and applied only on sampled patches, which is also the structure we adopted. The prediction head in MAE is a lightweight decoder used for pre-training to perform the image reconstruction task. Our prediction head design differs from that of MAE, and we will elaborate on this in the following section."
        },
        {
            "heading": "3.2 SRMAE",
            "text": "Directly applying the original pixel intesity as self-surpervised signal by reconstructing would not be suitable for images with varying scales. We introduce scale as self-supervised signal to utilize the image information at different resolutions. The overall pipeline of SRMAE is shown in Figure 1.\nApplying the original pixel intensity as a self-supervised signal by reconstruction would be unsuitable for images with varying scales. To account for such images, we introduce scale as a self-supervised signal to utilize the information at different resolutions. The overall pipeline of our SRMAE is illustrated in Figure 1.\nInstantiations Our work leverages this process to train a novel MIM model, using scale as the self-supervision signal. An example will illustrate the main structure and training process of our model. Given an input image x, we first perform the encoder stage. We divide x into non-overlapping image patches, represented by xp. At the same time, we downsample and patchify x to obtain a low-resolution xl. We then perform a masking operation, selecting a random mask M and replacing the corresponding parts of the patches with [MASK], obtaining xph = x\np\u2299M+[MASK]\u2299(1\u2212M). xph is a learnable embedding indicating masked patches. We then add positional information to x p h and learn the image\u2019s latent representation xe through the Vision Transformer. Then, in the prediction head, we concatenate xl and xe based on their corresponding positional information. This replaces the patches marked with [MASK] with the corresponding patches from xl. We preliminarily extract information that favors super-resolution from the concatenated patches sequence using a replaceable super-resolution module. Subsequently, we apply a lightweight Vision Transformer[17] that recovers the resolution, resulting in yp. The recovery loss is obtained by calculating the mean squared error (MSE) between yp and the original image patches xp, Lrec = ||(yp \u2212 xp)\u2299 (1\u2212M)||22. Only the loss generated by the parts replaced by xl is calculated[23].\nDuring the pre-training stage, we use scale as the self-supervised learning signal, combined with information from input images at different scales to learn the deep cross-resolution representation of images. After pre-training, we abandon the prediction head and only use the encoder for downstream visual tasks under different resolution conditions.\nDownsampling To utilize scale as a self-supervised signal for training, one must obtain images with resolutions that differ from the original image. Additionally, in order to maintain the positional information of images, it is required to stitch together images with different resolutions to their initial positions once resolution changes have been made. As such, the downsampling process involves two steps: image downsampling and dimensions adaptation.\nAs show in bottom part of 1. The first step is to perform a downsampling process. In order to fully utilize the latest advanced super-resolution structure subsequently when restoring the image\u2019s resolution, we referred to some papers on super-resolution[15, 61, 42, 7] where the frequently used scaling factors are 2, 3, and 4. Therefore, in this study, we applied these ubiquitous scaling factors to take full advantage of this advanced technology, which is widely recognized in the literature for its effectiveness. For example, when using a scaling factor of 4, we downscaled the original 224\u00d7 224 image to 1/4 of its resolution, resulting in a 56 \u00d7 56 image. In the second step, we concatenated the high-resolution patches with the low-resolution patches. Since the dimensions of HR and LR images do not match, we used the nearest neighbor interpolation method to resize the LR image to 224\u00d7 224 dimensions. After that, we divided this interpolated image into non-overlapping patches of size 16\u00d7 16. Encoder Only visible patches xp = {xip|M i = 1} are fed to the encoder following[23] and mapped to the high resolution patches xe across a stack of transformer blocks. The operation of encoder is based on self-attention. In this paper, we utilize ViT-Base to form the encoder.\nPrediction head Our study aimed to create a prediction head that could predict resolution recovery in conjunction with super-resolution technology. In contrast to the prediction head design of MAE, our SRMAE model\u2019s input for the prediction head consists of a patch sequence composed of highresolution patches xe = {xie|M i = 1} output by the encoder phase and the low-resolution patches xl = {xil|M i = 0}. We altered the input of the prediction head to enable resolution restoration instead of image reconstruction by using low-resolution image patches in place of [MASK] training patches utilized in MAE.\nThe prediction head, illustrated in 1, consists of a High Preserving Block (HPB) module and a lightweight Vision Transformer (ViT). To enhance the resolution recovery ability of the prediction head for super-resolution tasks, we extracted applicable modules from advanced super-resolution works. The HPB module proposed in ESRT captures image texture details in its own super-resolution model structure, making it suitable for feature extraction from both high-resolution and low-resolution patches. As such, we used it as the first processing module in our prediction head. The implementation of a transformer in IPT for super-resolution tasks demonstrates that the lightweight ViT used in our prediction head also possesses super-resolution ability. As a result, by combining effective modules from super-resolution works with the ViT in the prediction head, we achieved resolution recovery of low-resolution patches."
        },
        {
            "heading": "4 Experiments",
            "text": "The SRMAE framework is designed for learning scale-invariant representations. In this section, we conducted extensive experiments to evaluate the effectiveness of the SRMAE framework in learning\nscale-invariant representations. We evaluated the ability of SRMAE to maintain scale invariance for high-resolution images by conducting experiments on the ImageNet-1K dataset [13]. We fine-tuned the entire set of parameters of the classification task using a fine-tuning protocol and reported the results of transfer learning to assess the quality of scale invariance for high-resolution images. In addition, we conducted numeric classification tasks on the SVHN dataset[47] at very low resolutions and low-resolution object recognition tasks on the CK+[43], RAF-DB[36], and ExpW[76] datasets to assess the ability of SRMAE in preserving scale invariance for low-resolution images."
        },
        {
            "heading": "4.1 High-quality Images Tasks",
            "text": "Settings The imageNet-1K(IN-1K)[13] dataset contains 1.3 million images categorized into 1000 classes for image classification and is divided into training and validation sets. The evaluation protocol is pre-training followed by end-to-end fine-tuning. We utilized vanilla ViT[17] base models without modification and pre-trained our models without labels on the IN-1K training set at a resolution of 2242. To minimize data augmentation, we used random resized cropping and horizontal flipping. We randomly mask out 75% of total image patches following MAE[23]. For downsampling the images during pre-training, we employed the commonly-used four-fold downsampling strategy used in super-resolution experimentation[15, 42]. We follow the default finetuneing parameters of the MAE[23]. For finetuning, we report the classification accuracy on the IN-1K validation set of the finetuned SRMAE encoders.\nClassification on ImageNet-1K We present the accuracy of SRMAE on Table 1 and conduct comparisions with previous mask autoencoding methods. For fair comparison, we estimate the pre-training duration of each model on the same machine with one Tesla V100-32G GPU. We report the running time on single GPU, denoted as \u2019Pretraining Time\u2019. We pre-trained SRMAE for 400 epochs, obtaining a top-1 accuracy of 82.1%, which is 0.3% higher than from-scratch. Compared with the MAE pretrained for 400 epochs, our finetuning accuracy is 1% lower with the same number of pretraining epochs. Our finetuning accuracy is 1.2% lower than that of the MAE pretrained for 800 epochs, despite the close pretraining time. CCKD[52] is a comparative study in subsequent LR recognition experiments, and it also reports the top-1 accuracy on ImageNet-1K. Compared with CCKD, our top-1 accuracy improved by 14.4%, far exceeding their results. We believe that the use of scale signal leads to s subpar from-scratch and fine-tune performance, and leave further investigations of this phenomenon to future work. We note, however, that our method still improves over these handling low-resolution models by as large a margin as other methods. Our method still achieves significant improvement compared to previously used models (CCKD[52], AT[34]) for low-resolution tasks.\nSemantic segmentation on ADE20K We utilized UperNet [70] and followed the semantic segmentation code of [5, 23]. Our SRMAE\u2019s mIoU, as presented in Figure 3, was 14.55 at the beginning of training and improved to 35.5 at the end, a rise of 20.95 points. To compare our results, we replicated\nMAE\u2019s semantic segmentation experiment, which yielded an initial mIoU score of 28.05, which improved to 46.1 at the end, a rise of 18.05 points. Although our transfer learning ability was inferior to MAE, our training improvement was 2.9 points higher. We evaluated the loss results and observed that the loss values were consistently within similar levels, as shown on the right side of Figure 3. However, on the left-hand side, a noticeable gap in mIoU persisted. Because we based our code on MAE\u2019s implementation, we speculate that our model modifications made the initialization of other segments irregular, leading to unsatisfactory mIoU scores."
        },
        {
            "heading": "4.2 Low-quality Images Tasks",
            "text": "Datasets The SRMAE has been evaluated on two different tasks: VLR digit classification and LR facial expression classification. The first task utilized one dataset, whereas the second task used three datasets to evaluate the scale invariance ability of SRMAE. Details for each benchmark datasets ars as below:\n(i) SVHN[47]: We employed the Street View House Numbers (SVHN) dataset to assess the efficacy of SRMAE in classifying very low-resolution digits. This dataset comprises 0-9 digit images that were captured from natural scenes in the real world, at a 32\u00d732 resolution. It consists of 73,257 images for training and 26,031 images for testing. Consistent with established protocols set forth by Wang, et al. [67], the test dataset was created via subsampling the images by a factor of 2, resulting in an 8\u00d78 resolution that represents the lowest resolution among the four datasets. (ii) CK+[43]: The CK+ dataset contains 593 image sequences, each describing the transition process from a neutral expression to an emotional peak. We followed the protocol of [11] and used six prototype expressions (anger, disgust, fear, happiness, sadness, and surprise) in the training and testing process. In 327 image sequences, we selected the first frame as the neutral expression. Therefore, we obtained a total of seven expressions, with 1254 facial expression images, including the background, at a resolution of 640 \u00d7 490 pixels. In the experiment, we converted the images to 100 \u00d7 100 pixels for low-resolution testing. (iii) RAF-DB[36]: The RAF-DB database contains 12,271 training images and 3,068 testing images, with each picture being 100 \u00d7 100 pixels and using seven emotion labels including anger, disgust, fear, happiness, sadness, surprise, and neutral.\n(iv) ExpW[76]: The ExpW database is one of the largest facial expression databases, containing 91,793 network images. The facial area of these images ranges from 23\u00d723 pixels to 180\u00d7180 pixels. We use the dlib library to extract the facial area from these images. To meet the experimental requirements, images with a facial area smaller than 80\u00d780 pixels were filtered out, while those\nAlgorithm CK+[43] RAF-DB [36] ExpW[76]Extra Data Top-1 Acc. Extra Data Top-1 Acc. Extra Data Top-1 Acc.\nequal to or greater than 80\u00d780 pixels were resized to 100\u00d7100 pixels. In the end, a total of 31,127 processed images were obtained.\nVLR digit classification on SVHN dataset As the SVHN dataset contains sufficient data, we pretrain our SRMAE on SVHN dataset without label followed by end-to-end fine-tuning on SVHN dataset. Table 2 presents the top-1 and top-5 classification accuracies on the SVHN dataset. The SRMAE model achieved a top-1 classification accuracy of 89.14% and a top-5 classification accuracy of 98.40%, demonstrating an improvement of over 1% from the state-of-the-art result.We used MAE to conduct the same experiment, and the final results showed that the top-1 classification accuracy was 89.10%, while the top-5 classification accuracy was 98.38%. Our results show an improvement compared with those reported.\nThe improved performance demonstrates that using scale as a self-supervised signal can enhance the ability of the model to recognize extremely low-resolution images(e.g., digits) and achieve advanced levels.\nLR facial expression classification on CK+ dataset: In the experimental setup, we discovered that the CK+ dataset had limited data. Thus, training the model using CK+ dataset alone may not produce sophisticated results. We employed the model that was pre-trained for 400 epochs on the ImageNet-1K[13] dataset and fine-tuned it on the CK+ dataset. According to the results presented in 3, our model attained a top-1 accuracy of 94.21% on this dataset, which is within 1.4% of the state-of-the-art performance. We conducted an unfair comparison with MAE, due to the difference in the pre-training process. MAE employed a ViT-B model that was pre-trained for 1600 epochs and fine-tuned it for the downstream task, resulting in a top-1 accuracy of 95.04%.\nLR Facial Expression Classification on RAF-DB dataset: The size of the RAF-DB dataset is still relatively small. We also use the model that was pre-trained for 400 epochs on the ImageNet-1K dataset. We fine-tuned it on the RAF-DB dataset. According to the results presented in 3, the model\u2019s performance on the RAF-DB dataset was found to be 5% lower than that of the state-of-the-art models. We also conducted an unfair comparison with MAE dut to MAE employed a model that was pre-trained for 1600 epochs. The results in 3 demonstrated that MAE yielded good results on the RAF-DB dataset. Therefore, we attribute our relatively lower performance to the inadequate pre-training of our model. If more comprehensive pre-training is undertaken, we anticipate that our results may significantly improve.\nLR Facial Expression Classification on ExpW dataset: The ExpW dataset is one of the largest databases consisting of facial expressions. Hence, we forewent the use of supplementary datasets [13] and exclusively employed the ExpW dataset for pre-training, followed by fine-tuning during downstream tasks. Our model yielded a top-1 classification accuracy of 74.84%, as per the results presented in 3. This accuracy is 9.48% higher than that of the previous state-of-the-art model, indicative of a significant improvement in our methodology. Additionally, we conducted a fair comparison with MAE on this dataset, without using any extra pre-training datasets. Our experimental results revealed that our model achieved a top-1 accuracy of 74.53% on this dataset, which is marginally below that of SRMAE by 0.3%.\nSRMAE exhibits strong robustness in low-resolution tasks, particularly in larger datasets where the advantages of self-supervised learning can be fully utilized, leading to excellent performance. Moreover, SRMAE improves the MIM model\u2019s learning of low-resolution image features under similar experimental conditions by using scale as the self-supervisory signal, which renders it more suitable for low-resolution tasks compared to MAE."
        },
        {
            "heading": "5 Conclusion and Discussion",
            "text": "We present a robust and effective MIM framework, named SRMAE, that is capable of learning scale-invariant deep representation and apply it to visual tasks of different scales. TOur method leverages scale as a self-supervised signal to facilitate resolution restoration, which replaces image reconstruction and adapts easily to scale-varied tasks. We have achieved extensive results in tasks such as high-resolution image classification and low-resolution object recognition. To the best of our knowledge, this is the first model that achieves close to SOTA results for high-resolution image classification and low-resolution object recognition.\nA natural extension in the future would involve using more modules that can enhance the superresolution ability to further improve performance."
        }
    ],
    "title": "SRMAE: Masked Image Modeling for Scale-Invariant Deep Representations",
    "year": 2023
}