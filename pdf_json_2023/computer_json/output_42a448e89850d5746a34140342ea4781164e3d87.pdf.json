{
    "abstractText": "Search is an important technique in program synthesis that allows for adaptive strategies such as focusing on particular search directions based on execution results. Several prior works have demonstrated that neural models are effective at guiding program synthesis searches. However, a common drawback of those approaches is the inability to handle iterative loops, higher-order functions, or lambda functions, thus limiting prior neural searches from synthesizing longer and more general programs. We address this gap by designing a search algorithm called LAMBDABEAM that can construct arbitrary lambda functions that compose operations within a given DSL. We create semantic vector representations of the execution behavior of the lambda functions and train a neural policy network to choose which lambdas to construct during search, and pass them as arguments to higher-order functions to perform looping computations. Our experiments show that LAMBDABEAM outperforms neural, symbolic, and LLM-based techniques in an integer list manipulation domain.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kensen Shi"
        },
        {
            "affiliations": [],
            "name": "Hanjun Dai"
        }
    ],
    "id": "SP:7c3d49c5a8e7d02f07a79b1dcecddbc9acfb6492",
    "references": [
        {
            "authors": [
                "Miltiadis Allamanis",
                "Earl T Barr",
                "Premkumar Devanbu",
                "Charles Sutton"
            ],
            "title": "A survey of machine learning for big code and naturalness",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "Rajeev Alur",
                "Arjun Radhakrishna",
                "Abhishek Udupa"
            ],
            "title": "Scaling enumerative program synthesis via divide and conquer",
            "venue": "In International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS),",
            "year": 2017
        },
        {
            "authors": [
                "Matej Balog",
                "Alexander L Gaunt",
                "Marc Brockschmidt",
                "Sebastian Nowozin",
                "Daniel Tarlow"
            ],
            "title": "DeepCoder: Learning to write programs",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Shraddha Barke",
                "Hila Peleg",
                "Nadia Polikarpova"
            ],
            "title": "Just-in-time learning for bottom-up enumerative synthesis",
            "venue": "In Object-Oriented Programming,",
            "year": 2020
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "Hal Daum\u00e9 III",
                "John Langford"
            ],
            "title": "Learning to search better than your teacher",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2015
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde",
                "Jared Kaplan",
                "Harri Edwards",
                "Yura Burda",
                "Nicholas Joseph",
                "Greg Brockman",
                "Alex Ray",
                "Raul Puri",
                "Gretchen Krueger",
                "Michael Petrov",
                "Heidy Khlaaf",
                "Girish Sastry",
                "Pamela Mishkin",
                "Brooke Chan",
                "Scott Gray",
                "Nick Ryder",
                "Mikhail Pavlov",
                "Alethea Power",
                "Lukasz Kaiser",
                "Mohammad Bavarian",
                "Clemens Winter",
                "Philippe Tillet",
                "Felipe Such",
                "Dave Cummings",
                "Matthias Plappert",
                "Fotios Chantzis",
                "Elizabeth Barnes",
                "Ariel Herbert-Voss",
                "Will Guss",
                "Alex Nichol",
                "Igor Babuschkin",
                "Suchir Balaji",
                "Shantanu Jain",
                "Andrew Carr",
                "Jan Leike",
                "Josh Achiam",
                "Vedant Misra",
                "Evan Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Dawn Song"
            ],
            "title": "Execution-guided neural program synthesis",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Xinyun Chen",
                "Dawn Song",
                "Yuandong Tian"
            ],
            "title": "Latent execution for neural program synthesis",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann",
                "Parker Schuh",
                "Kensen Shi",
                "Sasha Tsvyashchenko",
                "Joshua Maynez",
                "Abhishek Rao",
                "Parker Barnes",
                "Yi Tay",
                "Noam Shazeer",
                "Vinodkumar Prabhakaran",
                "Emily Reif",
                "Nan Du",
                "Ben Hutchinson",
                "Reiner Pope",
                "James Bradbury",
                "Jacob Austin",
                "Michael Isard",
                "Guy Gur-Ari",
                "Pengcheng Yin",
                "Toju Duke",
                "Anselm Levskaya",
                "Sanjay Ghemawat",
                "Sunipa Dev",
                "Henryk Michalewski",
                "Xavier Garcia",
                "Vedant Misra",
                "Kevin Robinson",
                "Liam Fedus",
                "Denny Zhou",
                "Daphne Ippolito",
                "David Luan",
                "Hyeontaek Lim",
                "Barret Zoph",
                "Alexander Spiridonov",
                "Ryan Sepassi",
                "David Dohan",
                "Shivani Agrawal",
                "Mark Omernick",
                "Andrew M. Dai",
                "Marie Pellat",
                "Aitor Lewkowycz",
                "Erica Moreira",
                "Rewon Child",
                "Oleksandr Polozov",
                "Katherine Lee",
                "Zongwei Zhou",
                "Xuezhi Wang",
                "Brennan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "PaLM: Scaling language modeling with pathways",
            "venue": "Journal of Machine Learning Research (JMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Alonzo Church"
            ],
            "title": "The calculi of lambda-conversion",
            "year": 1985
        },
        {
            "authors": [
                "Hal Daum\u00e9 III",
                "John Langford",
                "Daniel Marcu"
            ],
            "title": "Search-based structured prediction",
            "venue": "Machine Learning,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Jonathan Uesato",
                "Surya Bhupatiraju",
                "Rishabh Singh",
                "Abdel-Rahman Mohamed",
                "Pushmeet Kohli"
            ],
            "title": "RobustFill: Neural program learning under noisy I/O",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Ellis",
                "Maxwell Nye",
                "Yewen Pu",
                "Felix Sosa",
                "Josh Tenenbaum",
                "Armando Solar-Lezama"
            ],
            "title": "Write, execute, assess: Program synthesis with a REPL",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Ellis",
                "Catherine Wong",
                "Maxwell Nye",
                "Mathias Sabl\u00e9-Meyer",
                "Lucas Morales",
                "Luke Hewitt",
                "Luc Cary",
                "Armando Solar-Lezama",
                "Joshua B Tenenbaum"
            ],
            "title": "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
            "venue": "In Programming Language Design and Implementation (PLDI),",
            "year": 2021
        },
        {
            "authors": [
                "John K Feser",
                "Swarat Chaudhuri",
                "Isil Dillig"
            ],
            "title": "Synthesizing data structure transformations from input-output examples",
            "venue": "In Programming Language Design and Implementation (PLDI),",
            "year": 2015
        },
        {
            "authors": [
                "George Fink",
                "Matt Bishop"
            ],
            "title": "Property-based testing: a new approach to testing for assurance",
            "venue": "ACM SIGSOFT Software Engineering Notes,",
            "year": 1997
        },
        {
            "authors": [
                "Justin Gottschlich",
                "Armando Solar-Lezama",
                "Nesime Tatbul",
                "Michael Carbin",
                "Martin Rinard",
                "Regina Barzilay",
                "Saman Amarasinghe",
                "Joshua B Tenenbaum",
                "Tim Mattson"
            ],
            "title": "The three pillars of machine programming",
            "venue": "In International Workshop on Machine Learning and Programming Languages (MAPL at PLDI),",
            "year": 2018
        },
        {
            "authors": [
                "Sumit Gulwani",
                "Oleksandr Polozov",
                "Rishabh Singh"
            ],
            "title": "Program synthesis",
            "venue": "Foundations and Trends\u00ae in Programming Languages,",
            "year": 2017
        },
        {
            "authors": [
                "Joey Hong",
                "David Dohan",
                "Rishabh Singh",
                "Charles Sutton",
                "Manzil Zaheer"
            ],
            "title": "Latent programmer: Discrete latent codes for program synthesis",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Woosuk Lee",
                "Kihong Heo",
                "Rajeev Alur",
                "Mayur Naik"
            ],
            "title": "Accelerating search-based program synthesis using learned probabilistic models",
            "venue": "In Programming Language Design and Implementation",
            "year": 2018
        },
        {
            "authors": [
                "Zohar Manna",
                "Richard J Waldinger"
            ],
            "title": "Toward automatic program synthesis",
            "venue": "Communications of the ACM,",
            "year": 1971
        },
        {
            "authors": [
                "Vijayaraghavan Murali",
                "Letao Qi",
                "Swarat Chaudhuri",
                "Chris Jermaine"
            ],
            "title": "Neural sketch learning for conditional program generation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Renato Negrinho",
                "Matthew Gormley",
                "Geoffrey Gordon"
            ],
            "title": "Learning beam search policies via imitation learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Renato Negrinho",
                "Matthew Gormley",
                "Geoffrey Gordon"
            ],
            "title": "An empirical investigation of beamaware training in supertagging",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP,",
            "year": 2020
        },
        {
            "authors": [
                "Maxwell Nye",
                "Luke Hewitt",
                "Joshua Tenenbaum",
                "Armando Solar-Lezama"
            ],
            "title": "Learning to infer program sketches",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Augustus Odena",
                "Charles Sutton"
            ],
            "title": "Learning to represent programs with property signatures",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Augustus Odena",
                "Kensen Shi",
                "David Bieber",
                "Rishabh Singh",
                "Charles Sutton",
                "Hanjun Dai"
            ],
            "title": "BUSTLE: Bottom-up program synthesis through learning-guided exploration",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin C Pierce"
            ],
            "title": "Types and programming languages",
            "venue": "MIT press,",
            "year": 2002
        },
        {
            "authors": [
                "Stephane Ross",
                "Geoffrey Gordon",
                "Drew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2011
        },
        {
            "authors": [
                "Kensen Shi",
                "Jacob Steinhardt",
                "Percy Liang"
            ],
            "title": "FrAngel: Component-based synthesis with control structures",
            "venue": "Proceedings of the ACM on Programming Languages,",
            "year": 2019
        },
        {
            "authors": [
                "Kensen Shi",
                "David Bieber",
                "Charles Sutton"
            ],
            "title": "Incremental sampling without replacement for sequence models",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Kensen Shi",
                "David Bieber",
                "Rishabh Singh"
            ],
            "title": "TF-Coder: Program synthesis for tensor manipulations",
            "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),",
            "year": 2022
        },
        {
            "authors": [
                "Kensen Shi",
                "Hanjun Dai",
                "Kevin Ellis",
                "Charles Sutton"
            ],
            "title": "CrossBeam: Learning to search in bottom-up program synthesis",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Disha Shrivastava",
                "Hugo Larochelle",
                "Daniel Tarlow"
            ],
            "title": "Learning to combine per-example solutions for neural program synthesis",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig"
            ],
            "title": "A syntactic neural model for general-purpose code generation",
            "venue": "In Assocation for Computational Linguistics (ACL),",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Program synthesis involves finding a program meeting a given specification of what the program should do [21, 18]. When the specification is in the form of input/output examples, known as programming by example (PBE), combinatorial search has been an especially popular technique [2, 30, 3, 32, 4, 27, 33]. Learning can also play a key role in PBE, because well-designed search algorithms can learn to adapt to information collected during the ongoing search, such as execution results or other analyses of candidate programs considered so far. This information can be used to prune redundant parts of the search space or focus on parts deemed more promising. For example, DeepCoder [3] and TF-Coder [32] use neural models to define a search space that is explored by traditional non-neural search, while BUSTLE [27], CROSSBEAM [33], and Execution-Guided Synthesis [7] use neural models to guide the search process itself. However, those prior works are unable to generate programs with arbitrary looping computations, whether implemented via loop control structures or through the use of higher-order functions with arbitrary lambda functions.1 Even though large language models (and other sequence models) can output programs with loops and are very effective at synthesizing programs from natural language [6], PBE demands a more systematic search strategy that adapts to valuable information like execution results during the search.\n1DeepCoder [3] supports higher-order functions but only a small set of hardcoded lambda functions. Execution-Guided Synthesis [7] supports variable-free while loops, but not loops with an iteration variable.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 6.\n02 04\n9v 2\n[ cs\n.L G\n] 2\n8 O\nThe fundamental question explored in this paper is whether a neural program synthesis search policy can learn to reason about lambdas and higher-order functions, which would enable the synthesis of arbitrary looping computations that were not previously possible with neural synthesis search techniques that rely on intermediate expression evaluation. Previous work [27, 33] has shown that neural models can effectively guide search when every candidate program can be evaluated to produce a concrete value for the model to inspect, allowing it to make decisions based on comparisons between explored values and the desired output. Lambda functions however are extremely different: they represent plans of functionality to be performed later, without specifying the context in which this functionality will be used. As a result, reasoning about lambdas requires a more abstract form of planning. Without knowing how the lambda might be used later, the search policy must understand the different behaviors of lambdas, predict whether a lambda will be useful for a given task to prioritize search directions, and recognize when and how to use lambdas within higher-order functions to actually perform useful computations.\nIn order to design a neural search algorithm that handles lambdas and higher-order functions, we address some key difficulties. One challenge is in the algebraic representation and subsequent manipulation of lambdas. We want to represent \u201cpractically equivalent\u201d lambdas like \u03bbx. x + 1, \u03bby. y + 1, and \u03bbx, y. x + 1 in a canonical way to prune the search space. If \u03bbx. x + 1 is their canonical representation, then how can we reuse that lambda to create a new lambda such as \u03bbx, y. (x+ 1)\u00d7 (y + 1) where the \u201c\u25e6+ 1\u201d functionality is used in different ways? We address this challenge by defining a new MERGE operation that combines lambda expressions into larger ones while allowing for variable renaming and adhering to other representational constraints, therefore enabling a bottom-up search algorithm to systematically build larger lambdas from existing ones.\nA second difficulty is in the encoding of lambdas when used as inputs to neural models. A naive representation would be to encode the code tokens, but slight changes in the code could lead to drastic differences in the lambda\u2019s behavior. Instead, we introduce a method of encoding lambdas that more directly reflects their execution semantics. We do this using property signatures [26] in a way agnostic to how the lambda is used later (i.e., what inputs are given to the lambda by a higher-order function), but still analyzing the lambda\u2019s behavior in the context of the current PBE task. One conclusion of our work is that this encoding does enable neural models to reason about lambdas effectively.\nWe present a new neural synthesis search method called LAMBDABEAM, which combines our solutions to these challenges within the search framework of the recent work CROSSBEAM [33]. CROSSBEAM performs a bottom-up search applying DSL operations to previously-explored values, using a neural search policy to choose the operation\u2019s arguments with a pointer network. Thus, in LAMBDABEAM, the neural policy is able to reason about lambdas by choosing which ones to construct and when to use them, such that the search direction is tailored to the synthesis task.\nWe demonstrate the effectiveness of LAMBDABEAM in the DeepCoder [3] domain of integer list manipulation. We extend the DeepCoder DSL by adding many first-order operations, keeping its higher-order functions, and replacing its limited set of hardcoded lambda functions with arbitrary lambdas using compositions of other DSL operations. Using a benchmark suite containing 100 natural hand-crafted evaluation tasks and 100 synthetically-generated tasks, we experimentally show that LAMBDABEAM outperforms prior approaches including state-of-the-art symbolic search, neural sequence models trained from scratch, and a 62 billion parameter large language model (LLM). We release our LAMBDABEAM code and trained model checkpoints at https://github.com/ ellisk42/LambdaBeam."
        },
        {
            "heading": "2 Background",
            "text": "Programming By Example Programming by Example (PBE) is the task of synthesizing programs that satisfy a given set of input/output (I/O) examples. In this task, we have a domain-specific language (DSL) L describing a space of programs, and a set of example inputs I = {I1, . . . , IN} and corresponding outputs O = {O1, . . . , ON}. The goal is to find a program P \u2208 L such that P (Ii) = Oi for all i \u2208 {1, . . . , N}. The DSL L describes atomic values (constants and input variables) and operations that can be applied to arguments to produce new values. Programs in L are arbitrarily-nested compositions of operations applied to atomic values or other such compositions.\n\u03bb-Calculus The lambda calculus [10, 28] is a formalism for universal computation. A lambda calculus term is either a variable, function application, or a lambda abstraction. Lambda abstractions\ndefine new functions by introducing new lexically-scoped variables, as well as a function body (which is itself a term). We consider lambda abstractions that are allowed to introduce multiple variables at once (we do not \u201cCurry\u201d our lambdas). Terms in the lambda calculus are constructed recursively from smaller terms, resulting in tree-like structures like in Figure 3(a). We extend \u03bb-calculus with domain-specific primitives (add, sort, map, filter, etc.) as well as a basic type system known as simply-typed lambda calculus; see [28] for details.\nProperty Signatures During a neural program synthesis search, many different expressions are encountered, possibly including lambda functions with arbitrary input and output types. How might a neural model learn to reason about these expressions? One option is to represent expressions as source code and apply standard text encoders like recurrent neural networks or Transformers. However, two expressions with similar source code might have very different execution behavior, or two syntactically distinct expressions might have identical semantics. An alternative approach that is more semantically aware is based on property signatures [26, 27], inspired by property-based testing in the programming languages literature [16]. Unary properties describe a single value by mapping it to a boolean, such as whether a list is sorted, or whether a string is empty. Binary properties can describe the relationship between the input and output of a PBE task, such as whether the input and output lists have the same length. In either case, given a list of k property functions, we can evaluate all property functions to form a vector of length k called a property signature. Each element of the signature is either True, False, or N/A.2 This vector may be used as input to a deep neural network. Furthermore, given multiple executions of an expression (e.g., over different examples), we can identify how often each property holds, leading to a more granular representation.\nCROSSBEAM Our work LAMBDABEAM builds upon the prior work CROSSBEAM [33]. Both systems have a similar overall structure, illustrated in Figure 1 with differences shown in red. The core\n2A property might be not applicable (N/A) if it does not apply to the types of objects currently under consideration, or if the property is inconclusive because the code execution resulted in an error.\nwhere x is an input variable. The model predicts blue term pointers and variables, the search tries every red operation, and unshaded code tokens in (b) are known from the other shaded tokens.\nidea in CROSSBEAM is to use a neural policy network to guide a bottom-up search over programs, where execution results of explored expressions provide a powerful signal for the policy to guide the search by exploring new expressions whose values are closer to the intended output.\nIn CROSSBEAM, a table stores the expressions explored so far in search, along with their execution values when run on the I/O examples. As diagrammed in Figure 2, the Value Module encodes each explored value into a vector that is stored in a matrix ES . Meanwhile, the I/O Module encodes the I/O examples into an analogous vector eIO. Then, the Search Context Summary Module combines the value encodings ES and the I/O encoding eIO to produce a latent \u201csummary\u201d ec of the search context so far. From this, the Argument Selector Module, which is a recurrent pointer network [35], predicts an argument list for a DSL operation via a sequence of pointers into the table of explored values, thus generating a new expression to explore. By structuring the search in this way, the neural policy is able to take a \u201chands on\u201d role during search, using information in the search context to choose which programs should be explored next.\nThe network is trained on-policy using imitation learning. Each item in the training set consists of a set of I/O examples and a target program. During training, we run the search algorithm on the I/O examples where at each step the policy network proposes values to explore by predicting argument lists for DSL operations. We then apply a softmax loss that encourages the policy network to make predictions that would lead to progress according to the target program, instead of other argument lists that the policy network actually proposed.\nDuring evaluation, getting argument lists via beam search as in training can lead to the search stalling if all new values are semantically equivalent to values already seen, since beam search is deterministic if the value set is unchanged. To address this, during evaluation CROSSBEAM randomly samples different argument lists using UniqueRandomizer [31] to avoid duplicate samples."
        },
        {
            "heading": "3 The LAMBDABEAM Approach",
            "text": ""
        },
        {
            "heading": "3.1 Building \u03bb-Terms",
            "text": "LAMBDABEAM constructs terms in the lambda calculus during its search. A natural design choice is to construct terms in a way that avoids considering semantically equivalent expressions. For example, the terms (\u03bbx, y. x\u2212 y), (\u03bba, b. a\u2212 b), and (\u03bby, x. y \u2212 x) are all capable of expressing exactly the same computations, so there should be a single canonical way of building this family of terms.\nAn important aspect of our canonicalization of semantically equivalent expressions is to enforce that every term constructed during search has no free variables (but terms may include variables referring to the inputs given in the I/O examples). Enforcing this property means that we can treat every term we build during search as an ordinary program, and run it on different inputs to probe its input-output behavior. However, the most straightforward way of building lambda terms bottom-up violates this important property. Consider the term t5 = (\u03bbv1. map(\u03bbu. v1 + u2, sort(x))) whose tree structure is illustrated in Figure 3(a), and where x refers to an input variable. This has subterms such as \u03bbu. v1 + u2, where the variable v1 occurs free. As v1 is free, it is unclear what the semantics of this expression should be. Constructing the expression with v1 free would also introduce a spurious redundancy with \u03bbv1, u. v1 + u2. During search, we would like to keep only a canonical version of\nthose two terms to better prune the search space, which in this case would be t3 = \u03bbv1, v2. v1 + v22 , which as shown in Figure 3(b) can be used to construct the larger term t5.\nTo build terms bottom-up while maintaining desired constraints such as that every intermediate program has no free variables, we define an operator for algebraically combining smaller terms to build larger programs. This operator, which we call MERGE, takes as input a primitive DSL function f and a list of terms a1 . . . aK constructed earlier during search, and it returns a new term that applies f to a1 . . . aK . MERGE does extra bookkeeping to ensure that every function (both f and any ak that are lambda terms) is called with the correct number of arguments, and that the final output of MERGE has no free variables. The function f can be a higher-order function, such as map, or a normal first-order function, such as addition and subtraction.\nAdditional arguments to MERGE specify how to unify lambda arguments (if any) that appear in a1 . . . aK . To evaluate MERGE on f and a1 . . . aK , we first apply each lambda argument ak to a tuple of variable names, denoted ik. This gives a name to each argument of each ak. By reusing the same name across different ak\u2019s, the same variable can be shared across arguments. For instance, if the function f is multiplication (\u00d7), and we are merging with the arguments a1 = (\u03bbv. v + 1) and a2 = (\u03bbv. v\u2212 1), then we can share the variable by setting i1 = i2 = [v1], giving (\u03bbv. v+ 1)(v1)\u00d7 (\u03bbv. v\u2212 1)(v1) = (v1 +1)\u00d7 (v1\u2212 1). Alternatively, if we set i1 = [v1] and i2 = [v2], then merging gives a different program, (v1 + 1)\u00d7 (v2 \u2212 1). These tuples of variable names, {ik}Kk=1, are also input to MERGE, because they determine how variable names are unified across arguments. Finally MERGE runs f on the arguments {ak(ik)}Kk=1, pads the expression with lambdas at the beginning to bind any new free variables, and lastly canonicalizes variable naming and variable ordering.\nSpecial care is needed for the arguments of higher-order functions like map, which themselves need to be functions. So far, each argument ak(ik) evaluates to a concrete value, not a function. To handle higher-order functions, MERGE automatically adds extra lambdas to each function-valued argument. These extra lambdas have variables denoted u1, u2, . . .. All other variables used to unify variables across arguments are denoted v1, v2, . . .. Although this may seem ad-hoc at first, this convention actually corresponds to a well-known canonicalization of typed lambda forms known as \u03b7-long normal form [28]. Putting everything together, MERGE is defined as\nMERGE (f, a1, i1, a2, i2, . . . ) = \u03bbv1v2... . f (\u03bbu1u2...u\u21131 . a1(i1), \u03bbu1u2...u\u21132 . a2(i2), . . .) where {v1, v2, . . .} = \u22c3 k ik \u2212 {uj}max{\u21131,\u21132,... }j=1 (1)\nwhere \u2113k is the arity of the kth argument to f . For example, the higher-order function map first takes a function expecting one argument, so \u21131 = 1, followed by a list (not a function) expecting no arguments, so \u21132 = 0. We also enforce that |ik| = arity(ak). The MERGE operation is complete, in the sense that we can use it to generate any PBE solution within the DSL (see Appendix A).\nFundamentally, our neural model predicts the inputs to MERGE. It scans through its different operators (functions f ), and for each operator, generates arguments ak by pointing into the set of explored values, and variables ik by emitting special tokens corresponding to v1, u1, v2, u2, etc. Figure 3(b) and (c) illustrate how a nontrivial lambda expression is built step by step, with the tokens emitted by the neural model highlighted in blue. In this figure, each call to MERGE in the third column returns the term that appears in the middle column, e.g., MERGE(square , v1 , [ ]) returns \u03bbv1. square(v1 ) and so on. Critically, MERGE makes sure that (1) every intermediate term that the model builds along the way can be evaluated so that the neural model can inspect its progress, and also (2) intermediate terms are generated with every function application, giving fine-grained feedback to the model.\nWe define the weight of an expression to be the number of nodes in its tree representation using the MERGE operator. More specifically, atomic terms like variable names and literal constants have weight 1, and a term constructed with MERGE has weight 1 (for the operation) plus the sum of the weights of all terms and variables in the MERGE call. For example, in Figure 3(b), terms t1 through t5 have weights 1, 2, 5, 2, and 10 respectively."
        },
        {
            "heading": "3.2 Learning over \u03bb-Expressions",
            "text": "One core technical challenge is how to encode lambda expressions to allow neural models to reason about them. In LAMBDABEAM we solve this by constructing a new generalization of property\nsignatures which is designed to represent lambda expressions and non-lambda expressions using similar sets of properties.\nNon-lambda expressions can be evaluated to produce a single result per I/O example. However, we cannot evaluate lambda expressions in the same way, because we do not know how the lambda expression will be used in the eventual solution, so we do not know what its arguments will be. For instance, if x is an input list, the expressions zipwith(\u03bbu1, u2. u1 + u2, x, sort(x)) and scanl1(\u03bbu1, u2. u1 + u2, x) use the same lambda expression \u03bbu1, u2. u1 + u2 but for different sets of (u1, u2) arguments. Thus, in order to describe the lambda expression\u2019s execution behavior, we run the lambda on a fixed set of canonical argument tuples based on the number of arguments and their types. These argument tuples are held constant across training and evaluation so that the model can learn from consistent signals.\nIn our experiments, we hardcoded the canonical argument tuples without changing them afterward, trying to cover common values and a variety of scenarios. For instance, our integer arguments include those between \u22123 and 5 inclusive, and other integers with varying combinations of magnitude, sign, even/odd parity, primality, and so on. There is also a tradeoff in the number of canonical argument tuples, where having more leads to finer-grained execution information but more time spent running lambdas during search. In our experiments, we use 16 canonical argument tuples for each combination of tuple length and argument types in our DSL. Note that the lambda expression can refer to inputs from the I/O examples. Instead of running the lambda on each argument tuple for each example, for efficiency, we run on each argument tuple once, under the context of one I/O example which is changed per argument tuple in a round-robin fashion.\nTo represent a lambda f , we evaluate it on each canonical argument tuple ti with I/O example (I,O). First we evaluate f on ti, yielding a result ri = f(ti, I). Then we use a signature of unary properties to describe ri. Second, we use a signature of binary properties to compare ri and O; intuitively, this helps the model learn about what work remains. Similarly, we use the binary properties to compare ri and tij , for each argument tij \u2208 ti. By concatenating these, we obtain a single property vector for each tuple ti. Finally, we then reduce the property vectors across the runs of the lambda, i.e., computing for each property the fraction of runs where it is applicable and the fraction of applicable runs where it is True. Encoding non-lambda expressions is similar, except that we use I/O examples (Ii, Oi) in place of the canonical tuples. Note that the reduced property signatures for lambda and non-lambda expressions have different formats and different lengths, and hence they are embedded by separate parts of the neural model (Section 3.3).\nThe properties used in our property signatures come from combinatorially combining hand-designed features as \u201cbuilding blocks\u201d to create a rich space of properties that describe individual objects as well as comparisons between two objects. Appendix B contains more details."
        },
        {
            "heading": "3.3 LAMBDABEAM Model and Search",
            "text": "To guide the bottom-up search over lambda and non-lambda expressions, we generally follow the design of the neural policy network in CROSSBEAM [33], with the following major changes:\nValue Module We maintain a set of explored values S which contains variable tokens for constructing lambdas, lambda expressions, and non-lambda expressions including constants and input variables. The Value Module embeds each element of S forming a matrix of embeddings ES \u2208 R|S|\u00d7d. Elements of S are embedded as follows. A variable token is embedded as a vector of trainable parameters. Note that the set of such variable tokens is fixed and determined by the DSL.3 A lambda expression is embedded by s + z, where s is the property signature of this lambda function encoded by an MLP, and z is an embedding of the weight of this value. Non-lambda expressions are embedded like lambda expressions except using a different MLP to encode their property signatures.\nArgument Selector Module Given an operator op, we use an operator-specific LSTMop to select the arguments using a pointer mechanism [35] from the encoded value matrix ES , in an autoregressive way. In addition to selecting arity(op) arguments, for an argument ak that is a lambda expression, we also need to predict the variables ik as required for MERGE, where ik is a tuple of arity(ak) variable tokens. All of the ak and ik predictions are done as a single autoregressive sequence. Furthermore,\n3The higher-order functions in our DSL expect lambdas with at most 2 variables. Thus, it is unnecessary to create lambdas with 3+ variables, so the only variables needed for MERGE are v1, v2, u1, and u2.\nfor convenience we predict all of the ak arguments first, followed by the ik variable tuples which are constrained (via masking and padding) to include exactly arity(ak) variable tokens.\nSearch with Restarts We also change the inference time search procedure. Recall that CROSSBEAM uses random sampling during evaluation, making the search nondeterministic. In LAMBDABEAM, instead of performing one synthesis search until timeout, we restart the search from scratch whenever the search has run for a certain amount of time without finding a solution. Even though this discards work done in previous searches, in practice this helps LAMBDABEAM solve more tasks because it may be otherwise difficult to recover from exploring the wrong search direction."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we experimentally evaluate the effectiveness of LAMBDABEAM, comparing to prior neural and symbolic approaches in an integer list manipulation domain."
        },
        {
            "heading": "4.1 Integer List Manipulation DSL",
            "text": "To measure a synthesizer\u2019s ability to create and use lambda expressions, we create a domain-specific language (DSL) that emphasizes lambda expressions and higher-order functions. Specifically, the DSL from DeepCoder [3] includes higher-order functions and has been used in subsequent work [37, 34]. However, DeepCoder\u2019s DSL only contains a hardcoded set of lambda functions and is not expressive enough to fully exercise LAMBDABEAM\u2019s ability to create arbitrary lambda expressions. Therefore, we extend DeepCoder\u2019s DSL by allowing lambda expressions to include arbitrary compositions of DSL operations, and replacing the hardcoded lambda functions with DSL operations and literal constants that enable a superset of the original functionality. For example, DeepCoder\u2019s original DSL includes hardcoded lambdas such as (\u03bbx. x\u2212 1), (\u03bbx, y. x\u2212 y), and (\u03bbx, y. max{x, y}). By introducing first-order functions including subtract and max, and constant literals including 0 and 1, we can create the hardcoded lambdas as well as lambdas like (\u03bbx.max{x, 0\u2212 x}) that were not possible in the original DeepCoder DSL. Additionally, we add a new if-then-else operation, which further enriches our space of possible programs. The full DSL contains 23 first-order operations, 5 higher-order operations, and 6 integer literals, described fully in Appendix C. In our DSL, all integers are in the range [\u2212256, 255] as in DeepCoder, and lists have lengths in the range [0, 10]."
        },
        {
            "heading": "4.2 Experimental Setup",
            "text": "Training Data Similar to previous works including CROSSBEAM, we create synthetic training data by generating random tasks within our DSL. This is done by performing exhaustive bottom-up searches starting from random inputs and enumerating programs in order of increasing weight, and then sampling a subset of the resulting programs to serve as training tasks. Each task has between 2 and 5 I/O examples and between 1 and 3 input variables, and we sample tasks such that approximately 80% of them use lambdas in the ground-truth program. We used a time limit of 1 hour per data generation search (reaching programs of weight at most 12), sampling up to 1600 tasks per search, and performing enough searches parallelized across cloud CPU workers such that training uses less than 1 epoch over the dataset. We furthermore removed from the training dataset all programs that would solve any of our 200 evaluation tasks, described below.\nEvaluation Tasks For evaluation, we use 100 handwritten evaluation tasks plus 100 synthetically generated tasks, with a time limit of 10 minutes per task. The handwritten tasks include all 9 \u201cexample programs\u201d from Appendix A of the DeepCoder paper [3], plus other tasks that we created from scratch by brainstorming many natural but varied tasks that we could solve using our DSL (near the end of this process, it became quite difficult to come up with new tasks that were not merely slight variations of existing ones). Handwritten tasks include between 1 and 3 input variables, 3 I/O examples if the output is a list or 5 I/O examples if the output is an integer, and a handwritten ground-truth solution that has minimal weight to our knowledge. When creating I/O examples, we aimed to make the examples informative but reasonably succinct with lists of length at most 10. Every DSL operation is used in the solution for at least 4 handwritten tasks, and every higher-order operation is used in at least 10. For the 100 synthetic tasks, we sampled distinct random programs using the same procedure as for generating training data, except also enforcing that we have exactly\n10 tasks of each weight between 3 and 12 inclusive. Appendix D contains example handwritten and synthetic tasks, along with LAMBDABEAM\u2019s solutions for them.\nApproaches Our experiments compare several approaches:\n1. LAMBDABEAM with random restarts: We trained the LAMBDABEAM model using on-policy training as in CROSSBEAM. The model has about 13 million trainable parameters and was trained on about 6.5 million tasks, which took about a week of training using 8 V100 GPUs. See Appendix E for more details on the model architecture and training. During evaluation, we use only 1 V100 GPU and perform random restarts every 6 seconds on the handwritten tasks, or every 30 seconds on the synthetic tasks, both chosen from a coarse search over restart frequencies. We run this approach for 5 trials using different randomness for the UniqueRandomizer sampling method carried over from CROSSBEAM.\n2. LAMBDABEAM without random restarts: We use the LAMBDABEAM approach without random restarts as an ablation, also for 5 trials with different randomness for UniqueRandomizer sampling.\n3. Enumeration: We run the same exhaustive bottom-up enumerative synthesis algorithm that was used to create the training data. We use 5 trials with different random orderings of DSL operations, which changes the enumeration order of programs with the same weight.\n4. RobustFill [12]: This approach treats the synthesis problem as a sequence to sequence prediction task from the I/O examples to the program tokens. Specifically, we train a plain 3-layer LSTMbased encoder-decoder model on our synthetic training dataset, using approximately the same number of trainable parameters and training tasks as for the LAMBDABEAM model. We get model predictions via a single beam search of size 65536 which nearly exhausts the GPU memory and evaluate all resulting programs on the I/O examples. Since the beam search is deterministic, we perform 5 trials by re-training the model with different initializations.\n5. \u03bb2 [15]: This is a state-of-the-art symbolic program synthesizer that handles lambda functions and higher-order functions. We implemented our DSL within the \u03bb2 framework (using the more extensible version provided by the \u03bb2 authors). \u03bb2 is deterministic so we only use 1 trial.\n6. Python-Finetuned LLM: We try asking a pretrained large language model (LLM) to solve our evaluation tasks using Python code. Specifically, we use PaLM 62B that was trained for longer as described in Appendix F of Chowdhery et al. [9], with further fine-tuning on general Python code. The prompt contains natural language instructions and 2 examples of an I/O specification followed by Python code that solves the task (for 2 new tasks), and then the I/O specification of the evaluation task we wish to solve.4 We repeatedly draw batches of 16 independent samples with temperature sampling and run those programs on the I/O examples, until a solution is found or timeout is reached. We ran the LLM using 16 accelerators so this approach uses significantly more compute than the others over the same time limit. We repeat for 3 trials with different randomness for temperature sampling."
        },
        {
            "heading": "4.3 Results",
            "text": "Figure 4 plots the synthesis performance of the various methods over time. Notably, LAMBDABEAM with restarts is the best approach for both handwritten and synthetic tasks. The gap is wider on the handwritten tasks where LAMBDABEAM with restarts solves 67.2 out of 100 tasks on average, which is 24% more tasks than the next best method \u03bb2. Figure 5 plots the various approaches\u2019 success rates for different task weights, which can be used as a measure of task difficulty. As expected, we observe that all methods perform worse on harder tasks with larger weight, but that LAMBDABEAM with restarts generally achieves higher success rates on the difficult tasks compared to other methods. This means that our approach scales better to larger programs compared to the other methods, except the LLM which has a more constant but lower success rate overall.5\n4In preliminary experiments, we found that providing 3 few-shot examples in the prompt led to slower sampling without much change in program quality. On the other hand, using only 1 example led to noticeably worse program quality. We also tried asking for programs within our DSL via few-shot examples, but this was not as successful because the LLM was not trained on our DSL.\n5The LLM predicts Python code instead of using our DSL, so the weight according to our DSL is an inaccurate measure of the complexity of the corresponding Python code. Furthermore, difficulty for the LLM is more closely correlated with how \u201cnatural\u201d the task is, i.e., its similarity to programs in the LLM\u2019s training data.\nRunning LAMBDABEAM with random restarts helps overall but more so for the handwritten tasks. We believe this is because the synthetic evaluation tasks have the same distribution as the training tasks while the handwritten tasks are different. So, for the handwritten tasks, exploring the wrong part of the search space early on might cause further mistakes that lead the search astray, while the model may be better trained to stay on track for the synthetic tasks. This would also explain why more frequent restarts are effective for the handwritten tasks. We note that random restarts would not be possible for \u03bb2, enumeration, or RobustFill\u2019s beam search, and would not help the LLM where each sample is already independent.\nWe also identify false positives by running solutions on 2 held-out test cases per task, generated mostly synthetically with some manual editing. The results are in Figure 6, showing that LAMBDABEAM with restarts has the highest number of true positive solutions on handwritten tasks by a margin of nearly 8 tasks, while barely losing to Enumeration on synthetic tasks.6 While symbolic approaches (Enumeration and \u03bb2) have fewer false positives due to focusing on small solutions, we observe that LAMBDABEAM has the fewest false positives among the neural approaches. The LLM produces many false positives on the synthetic tasks where the ground-truth solutions are less similar to programs seen during its training, and in fact many of its false positive solutions are if-elif-else chains that hardcode the examples in some way (which is feasible to implement in Python but not in our DSL). Finally, we note that some false positive solutions could be transformed into true positives with a postprocessing step, e.g., one that attempts to simplify or minimize subtrees of the solution. In this sense, false positive solutions may still be useful for synthesis, and LAMBDABEAM with restarts achieves the highest number of total positive solutions by a wide margin.\nAppendix F contains analysis showing some of the differences in distributions between the handwritten and synthetic evaluation tasks, which helps to contextualize the experimental results. For example, lambda expressions are used in 85% of the handwritten tasks but only 53% of the synthetic tasks. The median task weight is 9 for handwritten tasks and only 7.5 for synthetic tasks. These comparisons suggest that the handwritten tasks are harder than the synthetic tasks on average, which\n6The synthetic tasks have randomly-generated I/O examples that are overall less informative than those in the handwritten tasks, and the \u201ccorrect\u201d solution is not chosen to be natural but rather is one with minimal weight by construction. Enumeration has an unfair advantage here, being the only method in our comparison that is guaranteed to find a minimal weight solution.\nis also reflected in the overall performance in Figure 4. We observe that LAMBDABEAM achieves a greater performance gap over the other approaches on the handwritten tasks versus on the synthetic tasks, which is a promising trend because the handwritten tasks are both harder and more natural.\nAlthough LAMBDABEAM resolves CROSSBEAM\u2019s limitations of not handling lambdas or looping computations, some other limitations are carried over. On-policy training is slow due to performing search during training, but this could be addressed with an initial training phase of off-policy teacher forcing. At evaluation time, even with UniqueRandomizer sampling to avoid duplicate argument lists within one sampling phase, our approach still encounters many duplicate values across sampling phases and across restarts. Finally, our DSL is small compared to general programming languages."
        },
        {
            "heading": "5 Related Work",
            "text": "Machine learning for program synthesis has been an active area [17, 18, 1]. Within programming by example, deep learning architectures for sequences, like LSTMs and Transformers, have been particularly effective [12]. Our work builds upon CROSSBEAM [33], which itself combines three lines of research in program synthesis. The first are learned search strategies for program synthesis, that is, using a learned policy or value function to guide search [36, 20, 13], or multi-level strategies that combine the results of search over different spaces [25, 22, 19, 34]. The second are execution-guided neural synthesis methods, which guide the search over partial programs by evaluating them [37, 13, 7, 27, 8]. Finally, CROSSBEAM\u2019s use of imitation learning to train the policy is inspired by work in learning to search [11, 29, 5] and beam-aware training [23, 24].\nIn contrast, we are unaware of previous work that synthesizes helper functions, such as lambda functions, during neural program synthesis. The original DeepCoder DSL contains only a small set of predefined lambda functions. Even within symbolic program synthesis, \u03bb2 is one of the few examples of work that synthesizes lambda expressions [15]. To control the size of the search space, \u03bb2 employs type-directed synthesis, but we handle more general domains where the type system is not informative enough to reduce the search space sufficiently. DreamCoder [14] can also infer ad-hoc helper functions like \u03bb2, but its neural network provides no fine-grained guidance on how to compose those lambdas. Because DreamCoder is an algorithm for enriching an impoverished DSL to improve a neurally-guided program search, one could combine DreamCoder\u2019s DSL enrichment process with LAMBDABEAM\u2019s search strategy. Other work reuses fragments of code from partially-correct solutions [30, 34], but these are executable portions of straightline code, not lambda functions.\nOur integer manipulation domain is inspired by DeepCoder [3] and subsequent work [37, 34]."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduced the first neural search method for programming by example that is able to synthesize intermediate helper functions (lambdas) by resolving two key difficulties. First, we algebraically represent lambda expressions in a canonical way and construct new lambdas with the MERGE operator that enforces desirable representational constraints. Second, we encode arbitrary lambda functions as inputs to a neural network by using property signatures to analyze the lambda\u2019s execution semantics. With these innovations, LAMBDABEAM learns a neural policy to drive a bottom-up search over programs. We experimentally show that LAMBDABEAM outperforms symbolic search, a sequence model, and a pretrained code LLM with 62 billion parameters."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to thank Henryk Michalewski for his thoughtful ideas, and Christian Walder, Rif Saurous, and the anonymous reviewers for their helpful comments."
        },
        {
            "heading": "A Completeness of MERGE",
            "text": "The MERGE operation is complete in the sense that it can generate all possible solutions in the domain-specific language (DSL) for a programming-by-example (PBE) problem.\nWe formalize our DSL in a subset of the lambda calculus. Let X = {x1, . . . , xm} be the set of input variables for the PBE task, V be a countable set of variables that is disjoint from X , F be the set of primitive functions in the DSL, and C be a set of constants in the DSL. Then, our lambda calculus is:\nT ::= x | v | c | f(t1, . . . , tk) | \u03bbv1...vn. t for x \u2208 X , c \u2208 C, f \u2208 F ,\nv, v1, . . . , vn \u2208 V, t, t1, . . . , tk \u2208 T.\nLet M be the set of terms obtainable by repeatedly applying MERGE (including the initial terms usable by MERGE):\nM ::= x | \u03bbv. v | c | MERGE(f,a1, i1, . . . ,ak, ik) for x \u2208 X , v \u2208 V, c \u2208 C, f \u2208 F ,\na1, . . . ,ak \u2208M, i1, . . . , ik \u2208 V\u2217.\nWe restate the definition\nMERGE (f,a1, i1,a2, i2, . . . ) = \u03bbv1v2... . f (\u03bbu1u2...u\u21131 . a1(i1), \u03bbu1u2...u\u21132 . a2(i2), . . .) where {v1, v2, . . .} = \u22c3 k ik \u2212 {uj}max{\u21131,\u21132,... }j=1\nFrom this definition, it is clear that M \u2286 T , i.e., MERGE is closed within the lambda calculus. However, M \u0338= T because MERGE imposes certain constraints, e.g., \u03bbv. x is in T but cannot be constructed by MERGE. To precisely describe the constraints resulting from MERGE, we introduce the following definitions:\n\u2022 Terms x \u2208 X , v \u2208 V , and c \u2208 C are atomic. \u2022 A term s = \u03bbv1...vn. t (possibly with n = 0 such that s is not a lambda expression) has exact\nlambda variables if FreeVars(t)\u2212X = {v1, . . . , vn}. Note that s having exact lambda variables implies that FreeVars(s) \u2286 X .\n\u2022 A term typechecks if every function application has the correct arity for every argument, e.g., Map(t1, t2) expects t1 to have arity 1, while t2 should have arity 0.\nWe now let S = {s \u2208 T | s has exact lambda variables and typechecks}. The completeness of MERGE, in the sense that it can generate all solutions s to PBE problems (once the input variables x1, . . . , xm are bound), follows from the two claims below.\nClaim 1. If p = \u03bbx1...xm. s is a solution to a PBE problem, such that p(x1, . . . , xm) = y for all I/O examples (x1, . . . , xm)\u2192 y in the PBE specification, then s \u2208 S. That is, S is broad enough to cover all solutions to PBE problems.\nProof. To ensure that p = \u03bbx1...xm. s is a valid solution program, we must have FreeVars(s)\u2212X = \u2205 (so there are no unbound variables), s must have arity 0 (since all inputs x1, . . . , xm are already bound), and s must typecheck to avoid runtime errors. Together, these imply that s \u2208 S.\nClaim 2. S \u2286M . That is, MERGE can create any term in S, including all solutions to PBE problems.\nProof. Let s be any term in S. We will proceed by induction on the depth of s.\nAs the base case, if s is atomic, then s = x or s = c, so s is immediately in M . Note that s cannot be v because v does not have exact lambda variables.\nThen, we assume the inductive hypothesis that any term in S, with depth less than that of s, is in M . There are two inductive cases to consider: either s = \u03bbv1...vn. f(t1, . . . , tk) where n might be 0, or s = \u03bbv1...vn. t where n > 0 and t is atomic. Because s has exact lambda variables, the latter scenario is only possible if s = \u03bbv. v, which is immediately in M .\nThe remaining case is s = \u03bbv1...vn. f(t1, . . . , tk). Consider any tj for 1 \u2264 j \u2264 k. We will construct aj and ij such that MERGE(f,a1, i1, . . . ,ak, ik) = s.\n\u2022 If tj is atomic, then either tj = x, tj = v, or tj = c. If tj = v, then set aj = \u03bbv. v and ij = [v]; otherwise, set aj = tj and ij = [ ], the empty tuple. In each case, aj \u2208M . Because s typechecks, we know that the j-th argument to f expects arity 0, so in the MERGE definition, \u2113j = 0 and thus the j-th argument to f expands to aj(ij) = tj in each case.7\n\u2022 If tj is not atomic, then let tj = \u03bbu1...u\u2113j . r, where \u2113j is the expected arity of the j-th argument to f (because s typechecks). Let {v\u20321, . . . , v\u2032d} = FreeVars(r)\u2212X . Set aj = \u03bbv\u20321...v\u2032d. r and ij = [v\u20321, . . . , v \u2032 d], so that when applying MERGE, the j-th argument to f expands to \u03bbu1...u\u2113j . aj(ij) =\n\u03bbu1...u\u2113j . r = tj . Furthermore, aj has exact lambda variables by construction, and it typechecks because r typechecks, so aj \u2208 S and aj \u2208M by the inductive hypothesis.\nWith these choices of aj and ij , when expanding MERGE(f,a1, i1, . . . ,ak, ik) according to the definition, the j-th argument to f becomes tj , and the lambda variables \u22c3 k ik \u2212 {uj} max{\u21131,\u21132,... } j=1 are exactly FreeVars(f(t1, . . . , tk)) = {v1, . . . , vn} since s has exact lambda variables. Therefore, s = \u03bbv1...vn. f(t1, . . . , tk) = MERGE(f,a1, i1, . . . ,ak, ik), so s \u2208M ."
        },
        {
            "heading": "B More Details on Property Signatures",
            "text": "Here we describe in more detail the properties we use to encode lambda and non-lambda values. We define the following helper functions to organize the properties.\nFirst, TypeProperties(x) represents the type of x as a boolean one-hot list. In our DSL, this returns a tuple of 5 booleans, representing whether x is a lambda, boolean, int, list, or None (which is used to indicate an error, e.g., signaling that an index is out of bounds).\nNext, we define BasicProperties(x) to evaluate hand-designed \u201cbasic\u201d properties that describe objects of each different type in the DSL. This returns a fixed-length vector of property results (each being True, False, or N/A). Note that, if x has type \u03c4 , then all properties for type \u03c4 evaluate to True or False, while all properties for all other types \u03c4 \u2032 \u0338= \u03c4 evaluate to N/A. For our DSL, we use the following basic properties:\n\u2022 For boolean x: x itself. \u2022 For integer x: whether x equals \u22121, 0, 1, and 2; whether x is positive and negative; whether x is\neven; whether x is 0 and 1 modulo 3; and whether |x| is less than 5, 10, 20, 35, 50, 75, and 100. \u2022 For list x: whether x is sorted, whether x is sorted in reverse, and whether x contains all unique\nelements.\nThen, Relevant(x) returns related objects that are relevant to understanding x. For our DSL, this is only x itself for integer and boolean x, but for list x, the \u201crelevant\u201d objects are: x itself; the length of x; the number of distinct elements in x; the max, min, range, and sum of x; and the first and last elements of x (defaulting to 0 if x is empty).\nThis culminates in ObjectSignature(x) which takes a single DSL object x and returns a fixed-length vector of property results, containing TypeProperties(x) followed by BasicProperties(r) for each r \u2208 Relevant(x). For example, these properties include \u201cassuming x is an int, is x is even?\u201d (a basic\n7In practice for simplicity, when tj = v, we simply set aj = tj and ij = [ ] as in the other cases, even though aj = v does not have exact lambda variables.\nproperty applied to x) as well as \u201cassuming x is a list, are there an even number of elements in x?\u201d (a basic property applied to an object relevant to x). By applying basic properties to relevant objects in this compositional way, we reduce the effort needed to specify a large number of properties.\nWe furthermore encode comparisons between two objects. We define ComparisonProperties(x, y) which evaluates hand-designed properties for comparing two objects x and y of the same type, for each different type in the DSL. This returns a fixed-length vector of property results, where a property for comparing type \u03c4 evaluates to N/A if x and y are not of type \u03c4 . For our DSL, we use the following comparison properties:\n\u2022 For boolean x and y: whether x = y. \u2022 For integer x and y: whether x = y, x < y, and x > y; whether x is a factor of y and vice versa;\nand whether |x\u2212 y| is less than 2, 5, 10, and 20. \u2022 For list x and y: whether x = y; whether x is longer, shorter, or equal length compared to y;\nwhether the lengths differ by at most 1; whether all xi < yi for xi, yi \u2208 zip(x, y) and similarly for other comparisons \u2264, >, \u2265, =, and \u0338=; whether x and y contain the same set of elements; and whether x contains a subset of elements compared to y and vice versa.\nThese properties are used in ComparisonSignature(x, y) which computes a fixed-length list of property results for any two DSL objects x and y of any type, containing ComparisonProperties(rx, y) for all rx \u2208 Relevant(x) where type(rx) = type(y), and ComparisonProperties(x, ry) for all ry \u2208 Relevant(y) where type(ry) = type(x). Thus, \u201cassuming x is an int and y is a list, is x a factor of the length of y?\u201d is one resulting property. As usual, if x and y do not match the types assumed by the property, then the property evaluates to N/A.\nIn the I/O Module of the neural policy (as in CROSSBEAM [33]), we use property signatures to encode a set of I/O examples. For each example ({I1, . . . , In}, O) we concatenate ObjectSignature(O) with ObjectSignature(Ii) and ComparisonSignature(Ii, O) for all 1 \u2264 i \u2264 n. Then, we reduce these across I/O examples, computing for each property the fraction of examples where it is applicable (not N/A), and the fraction of examples where it is True among those where it is applicable (defaulting to 0.5 if it is N/A for all examples).\nIn the Value Module of the neural policy, we use property signatures to encode a value (lambda or non-lambda expression) that was found during search. To encode a lambda expression, we run it on canonical input tuples as described in Section 3.2. For each run of the lambda on canonical input tuple ti = (ti,1, . . . , ti,m) using an I/O example (I,O) where the lambda evaluates to a result ri, we concatenate ObjectSignature(ri), ComparisonSignature(ri, O), and ComparisonSignature(ti,j , ri) for all 1 \u2264 j \u2264 m, and then reduce these across the runs of the lambda. To encode a non-lambda expression during search, for each I/O example (I,O) where the expression evaluates to a result r, we concatenate ObjectSignature(r) with ComparisonSignature(r,O), and then reduce these across I/O examples. Note that the signatures for values found during search do not contain comparisons to the I/O example inputs, because what ultimately matters is whether the value is useful for creating the output later, not how the value was created from the inputs.\nIn our implementation, encoding the set of I/O examples results in a property signature of length 1230, encoding a lambda expression results in a property signature of length 558, and encoding a non-lambda expression results in a property signature of length 359."
        },
        {
            "heading": "C Extension of the DeepCoder DSL",
            "text": "As mentioned in Section 4.1, we extended the DSL from DeepCoder [3]. Atomic terms in the DSL include variable names and the constant literals \u22121, 0, 1, 2, 3, and 4. The DSL contains 23 first-order and 5 higher-order operations, listed below with type annotations and Python implementations:\n# 23 first-order operations\ndef Add(x: int, y: int) -> int: return x + y\ndef Subtract(x: int, y: int) -> int: return x - y\ndef Multiply(x: int, y: int) -> int: return x * y\ndef IntDivide(x: int, y: int) -> int: return x // y\ndef Square(x: int) -> int: return x ** 2\ndef Min(x: int, y: int) -> int: return min(x, y)\ndef Max(x: int, y: int) -> int: return max(x, y)\ndef Greater(x: int, y: int) -> bool: return x > y\ndef Less(x: int, y: int) -> bool: return x < y\ndef Equal(x: int, y: int) -> bool: return x == y\ndef IsEven(x: int) -> bool: return x % 2 == 0\ndef IsOdd(x: int) -> bool: return x % 2 != 0\ndef If(c: bool, x: int, y: int) -> int: return x if c else y\ndef Head(xs: list) -> int: return xs[0]\ndef Last(xs: list) -> int: return xs[-1]\ndef Take(n: int, xs: list) -> list: return xs[:n]\ndef Drop(n: int, xs: list) -> list: return xs[n:]\ndef Access(n: int, xs: list) -> int: return xs[n]\ndef Minimum(xs: list) -> int: return min(xs)\ndef Maximum(xs: list) -> int: return max(xs)\ndef Reverse(xs: list) -> list: return list(reversed(xs))\ndef Sort(xs: list) -> list: return sorted(xs)\ndef Sum(xs: list) -> int: return sum(xs)\n# 5 higher-order operations\ndef Map(f: Callable[[int], int], xs: list) -> list: return [f(x) for x in xs]\ndef Filter(f: Callable[[int], bool], xs: list) -> list: return [x for x in xs if f(x)]\ndef Count(f: Callable[[int], bool], xs: list) -> int: return len([x for x in xs if f(x)])\ndef ZipWith(f: Callable[[int, int], int], xs: list, ys: list) -> list: return [f(x, y) for x, y in zip(xs, ys)]\ndef Scanl1(f: Callable[[int, int], int], xs: list) -> list: ys = [xs[0]] for n in range(1, len(xs)):\nys.append(f(ys[n-1], xs[n])) return ys"
        },
        {
            "heading": "D Example Tasks",
            "text": "This section contains selected example problems from our 100 handwritten and 100 synthetic evaluation tasks. Each task is given a name for convenience purposes only, which is not used by any method in our experiments.\nD.1 Handwritten Task \u201cmap:replace\u201d\nThis task has 3 inputs (x, f, and r), 3 examples demonstrating the task (\u201cin x, find instances of f and replace them with r\u201d), and a handwritten ground-truth solution using a relatively complicated lambda function:\ninputs_dict = { 'x': [[7, 2, 4, 6, 4, 2, 5],\n[-6, -3, 4, 3, -5, -3, 2, 1, 5], [18, 48, 27, 26, 27, 27, 28, 17, 27, 33]],\n'f': [4, -3, 27], 'r': [-1, 7, 99],\n} outputs = [[7, 2, -1, 6, -1, 2, 5],\n[-6, 7, 4, 3, -5, 7, 2, 1, 5], [18, 48, 99, 26, 99, 99, 28, 17, 99, 33]]\nsolution = 'Map(lambda u1: If(Equal(u1 , f), r, u1), x)'\nIn inputs dict, each of the entries for x, f, and r is a list of length 3, which contains the input for each of the 3 examples. Similarly, outputs is a list containing the output for each example. solution is our handwritten solution.\nLAMBDABEAM + Restarts finds the same solution of weight 10 in each of the 5 trials, taking a median time of 202 seconds:\nMap(lambda u1: (lambda v1: If(( lambda v1: Equal(f, v1))(v1), r, v1))(u1), x)\nThe solution looks complicated due to the MERGE operation causing lots of variable renames (i.e., ak(ik) in the MERGE definition). We have implemented an algorithm to simplify the solution by statically resolving these renames. In this case, the solution simplifies to\nMap(lambda u1: If(Equal(f, u1), r, u1), x)\nwhich is essentially identical to the ground-truth solution.\nD.2 Handwritten Task \u201cmulti:multiply odds\u201d\nThis task has 1 input and uses multiple higher-order functions to compute a running product of only the odd elements:\ninputs_dict = { 'x': [[3, 5, 8, 2, 1],\n[5, 2, 1, 3, 3, 1, 4], [3, -4, -1, 8, 2, 0, -3, 0, 9, -1]],\n} outputs = [[3, 15, 15],\n[5, 5, 15, 45, 45], [3, -3, 9, 81, -81]]\nsolution = 'Scanl1(lambda u1 , u2: Multiply(u1, u2), Filter(lambda u1: IsOdd(u1), x))'\nIn each of the 5 trials, LAMBDABEAM + Restarts finds the same solution of weight 11 that simplifies to the ground-truth solution, taking a median time of 75 seconds.\nD.3 Synthetic Task \u201csynthetic:weight 9 function 7\u201d\nThis task clips every element to the range [0, 4]: inputs_dict = {\n'x1': [[-9, -2, -10, -6, 0, -10, -6, 3, 1], [-1, -5, 8, 5]]\n} outputs= [[0, 0, 0, 0, 0, 0, 0, 3, 1], [0, 0, 4, 4]] solution = 'Map(lambda u1: Min(4, Max(0, u1)), x1)'\nLAMBDABEAM + Restarts finds a correct solution in all 5 trials with a median time of 38 seconds, but the solutions are slightly different (the simplified solutions are listed):\nZipWith(lambda u1, u2: Min(4, Max(0, u2)), x1 , x1) ZipWith(lambda u1, u2: Min(4, Max(0, u1)), x1 , x1) Reverse(ZipWith(lambda u1, u2: Min(4, Max(0, u2)), x1 , Reverse(x1))) Reverse(Map(lambda u1: Min(4, Max(0, u1)), Reverse(x1))) # found in two trials\nNote that these are not the shortest solutions, but nevertheless all of these solutions are equivalent to the ground-truth solution. LAMBDABEAM\u2019s solutions could benefit from a postprocessing simplification step, as discussed in Section 4.3."
        },
        {
            "heading": "E More Details on LAMBDABEAM Architecture and Training",
            "text": "In our experiments, we used the following hyperparameters for the LAMBDABEAM model architecture and training procedure. Refer to Figure 2 for a diagram showing how the different modules interact.\n\u2022 I/O Module: this encodes a property signature of the I/O examples using a 2-layer ReLU-MLP with hidden size and output size of 512.\n\u2022 Value Module: this encodes each value\u2019s property signature using a 2-layer ReLU-MLP with hidden size of 512 and output (embedding) size of 256, with a layer-norm applied after each linear projection. We use different MLPs for lambda and non-lambda expressions.\n\u2022 Search Context Summary Module: this module needs to represent the entire search state at the current stage, including the current operator to be expanded, the I/O specification, and the values explored so far. We compute the average of the set of value embeddings, concatenate it with the I/O embedding, and then apply a projection layer (denoted as MLPop in Figure 2, which projects back to the embedding dimension) to get a vector representation. The model parameters used in the projection layers are indexed by the operator (i.e., we use different sets of trainable parameters for different operators).\n\u2022 Argument Selector Module: we use an operator-specific 3-layer LSTM with hidden size of 256. The prediction head is a 2-layer MLP with hidden size of 512.\n\u2022 During training, we generate on-policy data with beam size 10, use an effective batch size of 32, and use the Adam optimizer with a constant learning rate of 5\u00d7 10\u22124.\n\u2022 During evaluation, we use UniqueRandomizer with beam size 10."
        },
        {
            "heading": "F Analysis of Handwritten and Synthetic Tasks",
            "text": "Table 1 shows some differences in the distributions between our handwritten and synthetic evaluation tasks. This analysis may help contextualize the experimental results in Section 4.\nFor example, Figure 5 shows that the LLM solved abnormally many synthetic tasks in the 11-12 weight bucket. In fact, for synthetic tasks of weight 8 or more, every one of the LLM\u2019s \u201csolutions\u201d are actually false positives using some form of \u201cif the input is \u27e8hardcoded\u27e9 then return \u27e8hardcoded\u27e9\u201d logic, which is easier to implement when the output is an integer as opposed to a list. Table 1 shows that there are abnormally many synthetic tasks of weight 11-12 that have integer outputs, which helps to explain the results."
        }
    ],
    "title": "LAMBDABEAM: Neural Program Search with Higher-Order Functions and Lambdas",
    "year": 2023
}