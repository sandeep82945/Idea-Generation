{
    "abstractText": "Graph-based methods have hitherto been used to pursue coherent patterns of data due to their ease of implementation and efficiency. These methods have been increasingly applied in multi-view learning and achieved promising performance in various clustering tasks. However, despite their noticeable empirical success, existing graph-based multi-view clustering methods may still suffer the suboptimal solution considering that multi-view data can be very complicated in raw feature space. Moreover, existing methods usually adopt the similarity metric by an ad hoc approach, which largely simplifies the relationship among real-world data and results in an inaccurate output. To address these issues, we propose to seamlessly integrate metric learning and graph learning for multiview clustering. Specifically, we employ a useful metric to depict the inherent structure with linearity-aware of affinity graph representation learned based on the self-expressiveness property. Furthermore, instead of directly utilizing the raw features, we prefer to recover a smooth representation such that the geometric structure of the original data can be retained. We model the above concerns into a unified learning framework, and hence complement each learning subtask in a mutual reinforcement manner. The empirical studies corroborate our theoretical findings and demonstrate that the proposed method is able to boost the multi-view clustering performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuze Tan"
        },
        {
            "affiliations": [],
            "name": "Yixi Liu"
        },
        {
            "affiliations": [],
            "name": "Hongjie Wu"
        },
        {
            "affiliations": [],
            "name": "Jiancheng Lv"
        },
        {
            "affiliations": [],
            "name": "Shudong Huang"
        }
    ],
    "id": "SP:2b4924c91e37e7aa7f5179f0b595ed40af5cdfd5",
    "references": [
        {
            "authors": [
                "J. Benesty",
                "J. Chen",
                "Y. Huang",
                "I. Cohen"
            ],
            "title": "Pearson correlation coefficient",
            "venue": "Noise Reduction in Speech Processing, 1\u20134. Springer.",
            "year": 2009
        },
        {
            "authors": [
                "S. Bickel",
                "T. Scheffer"
            ],
            "title": "Multi-view clustering",
            "venue": "Proceedings of the IEEE International Conference on Data Mining, volume 4, 19\u201326.",
            "year": 2004
        },
        {
            "authors": [
                "J. Duchi",
                "S. Shalev-Shwartz",
                "Y. Singer",
                "T. Chandra"
            ],
            "title": "Efficient projections onto the l 1-ball for learning in high dimensions",
            "venue": "Proceedings of the International Conference on Machine Learning, 272\u2013279.",
            "year": 2008
        },
        {
            "authors": [
                "E. Elhamifar",
                "R. Vidal"
            ],
            "title": "Sparse subspace clustering: Algorithm, theory, and applications",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11): 2765\u20132781.",
            "year": 2013
        },
        {
            "authors": [
                "H. Gao",
                "F. Nie",
                "X. Li",
                "H. Huang"
            ],
            "title": "Multi-view subspace clustering",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 4238\u20134246.",
            "year": 2015
        },
        {
            "authors": [
                "J. Huang",
                "F. Nie",
                "H. Huang"
            ],
            "title": "A new simplex sparse learning model to measure data similarity for clustering",
            "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, 3569\u20133575.",
            "year": 2015
        },
        {
            "authors": [
                "S. Huang",
                "Z. Kang",
                "I.W. Tsang",
                "Z. Xu"
            ],
            "title": "Autoweighted multi-view clustering via kernelized graph learning",
            "venue": "Pattern Recognition, 88: 174\u2013184.",
            "year": 2019
        },
        {
            "authors": [
                "S. Huang",
                "Z. Kang",
                "Z. Xu"
            ],
            "title": "Auto-weighted multiview clustering via deep matrix decomposition",
            "venue": "Pattern Recognition, 97: 1\u201311.",
            "year": 2020
        },
        {
            "authors": [
                "S. Huang",
                "I.W. Tsang",
                "Z. Xu",
                "J. Lv",
                "Q. Liu"
            ],
            "title": "CDD: Multi-view Subspace Clustering via Cross-view Diversity Detection",
            "venue": "Proceedings of the ACM International Conference on Multimedia, 2308\u20132316.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Kang",
                "W. Zhou",
                "Z. Zhao",
                "J. Shao",
                "M. Han",
                "Z. Xu"
            ],
            "title": "Large-scale multi-view subspace clustering in linear time",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, 4412\u20134419.",
            "year": 2020
        },
        {
            "authors": [
                "A. Kumar",
                "H. Daum\u00e9"
            ],
            "title": "A co-training approach for multi-view spectral clustering",
            "venue": "Proceedings of the International Conference on Machine Learning, 393\u2013400.",
            "year": 2011
        },
        {
            "authors": [
                "A. Kumar",
                "P. Rai",
                "H. Daume"
            ],
            "title": "Co-regularized multi-view spectral clustering",
            "venue": "Advances in Neural Information Processing Systems, 24: 1\u20139.",
            "year": 2011
        },
        {
            "authors": [
                "Z. Li",
                "C. Tang",
                "X. Liu",
                "X. Zheng",
                "W. Zhang",
                "E. Zhu"
            ],
            "title": "Consensus graph learning for multi-view clustering",
            "venue": "IEEE Transactions on Multimedia, 24: 2461\u20132472.",
            "year": 2021
        },
        {
            "authors": [
                "G. Liu",
                "Z. Lin",
                "S. Yan",
                "J. Sun",
                "Y. Yu",
                "Y. Ma"
            ],
            "title": "Robust recovery of subspace structures by low-rank representation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1): 171\u2013184.",
            "year": 2012
        },
        {
            "authors": [
                "J. Liu",
                "X. Liu",
                "Y. Yang",
                "X. Guo",
                "M. Kloft",
                "L. He"
            ],
            "title": "Multiview subspace clustering via co-training robust data representation",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 5177\u20135189.",
            "year": 2021
        },
        {
            "authors": [
                "J. Liu",
                "X. Liu",
                "Y. Yang",
                "L. Liu",
                "S. Wang",
                "W. Liang",
                "J. Shi"
            ],
            "title": "One-pass multi-view clustering for large-scale data",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 12344\u201312353.",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu",
                "S. Wang",
                "P. Zhang",
                "K. Xu",
                "X. Liu",
                "C. Zhang",
                "F. Gao"
            ],
            "title": "Efficient one-pass multi-view subspace clustering with consensus anchors",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 7576\u2013 7584.",
            "year": 2022
        },
        {
            "authors": [
                "C. Lu",
                "J. Feng",
                "Z. Lin",
                "T. Mei",
                "S. Yan"
            ],
            "title": "Subspace clustering by block diagonal representation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2): 487\u2013501.",
            "year": 2018
        },
        {
            "authors": [
                "C.-Y. Lu",
                "H. Min",
                "Z.-Q. Zhao",
                "L. Zhu",
                "D.-S. Huang",
                "S. Yan"
            ],
            "title": "Robust and efficient subspace segmentation via least squares regression",
            "venue": "Proceedings of the European Conference on Computer Vision, 347\u2013360.",
            "year": 2012
        },
        {
            "authors": [
                "S. Luo",
                "C. Zhang",
                "W. Zhang",
                "X. Cao"
            ],
            "title": "Consistent and specific multi-view subspace clustering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 3730\u20133737.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Ma",
                "Z. Kang",
                "G. Luo",
                "L. Tian",
                "W. Chen"
            ],
            "title": "Towards clustering-friendly representations: subspace clustering via graph filtering",
            "venue": "Proceedings of the ACM International Conference on Multimedia, 3081\u20133089.",
            "year": 2020
        },
        {
            "authors": [
                "F. Nie",
                "G. Cai",
                "X. Li"
            ],
            "title": "Multi-view clustering and semi-supervised classification with adaptive neighbours",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2408\u20132414.",
            "year": 2017
        },
        {
            "authors": [
                "F. Nie",
                "J. Li",
                "X Li"
            ],
            "title": "Self-weighted Multiview Clustering with Multiple Graphs",
            "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "P. Schober",
                "C. Boer",
                "L.A. Schwarte"
            ],
            "title": "Correlation coefficients: appropriate use and interpretation",
            "venue": "Anesthesia & Analgesia, 126(5): 1763\u20131768.",
            "year": 2018
        },
        {
            "authors": [
                "D.I. Shuman",
                "S.K. Narang",
                "P. Frossard",
                "A. Ortega",
                "P. Vandergheynst"
            ],
            "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
            "venue": "IEEE signal Processing Magazine, 30(3): 83\u201398.",
            "year": 2013
        },
        {
            "authors": [
                "M. Sun",
                "P. Zhang",
                "S. Wang",
                "S. Zhou",
                "W. Tu",
                "X. Liu",
                "E. Zhu",
                "C. Wang"
            ],
            "title": "Scalable multi-view subspace clustering with unified anchors",
            "venue": "Proceedings of the ACM International Conference on Multimedia, 3528\u20133536.",
            "year": 2021
        },
        {
            "authors": [
                "C. Tang",
                "X. Liu",
                "X. Zhu",
                "E. Zhu",
                "Z. Luo",
                "L. Wang",
                "W. Gao"
            ],
            "title": "CGD: Multi-view clustering via cross-view graph diffusion",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, 5924\u20135931.",
            "year": 2020
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of Machine Learning Research, 9(11): 2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "H. Wang",
                "Y. Yang",
                "B. Liu"
            ],
            "title": "GMC: Graph-based multi-view clustering",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 32(6): 1116\u20131129.",
            "year": 2019
        },
        {
            "authors": [
                "S. Wang",
                "X. Liu",
                "X. Zhu",
                "P. Zhang",
                "Y. Zhang",
                "F. Gao",
                "E. Zhu"
            ],
            "title": "Fast parameter-free multi-view subspace clustering with consensus anchor guidance",
            "venue": "IEEE Transactions on Image Processing, 31: 556\u2013568.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xu",
                "S. Chen",
                "J. Li",
                "J. Qian"
            ],
            "title": "Linearity-aware subspace clustering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 8770\u20138778.",
            "year": 2022
        },
        {
            "authors": [
                "J. Yang",
                "J. Liang",
                "K. Wang",
                "P.L. Rosin",
                "M.H. Yang"
            ],
            "title": "Subspace clustering via good neighbors",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(6): 1537\u20131544.",
            "year": 2020
        },
        {
            "authors": [
                "C. You",
                "D. Robinson",
                "R. Vidal"
            ],
            "title": "Scalable sparse subspace clustering by orthogonal matching pursuit",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3918\u20133927.",
            "year": 2016
        },
        {
            "authors": [
                "K. Zhan",
                "F. Nie",
                "J. Wang",
                "Y. Yang"
            ],
            "title": "Multiview consensus graph clustering",
            "venue": "IEEE Transactions on Image Processing, 28(3): 1261\u20131270.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhang",
                "H. Fu",
                "Q. Hu",
                "X. Cao",
                "Y. Xie",
                "D. Tao",
                "D. Xu"
            ],
            "title": "Generalized latent multi-view subspace clustering",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(1): 86\u201399.",
            "year": 2018
        },
        {
            "authors": [
                "C. Zhang",
                "Q. Hu",
                "H. Fu",
                "P. Zhu",
                "X. Cao"
            ],
            "title": "Latent multi-view subspace clustering",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4279\u20134287.",
            "year": 2017
        },
        {
            "authors": [
                "P. Zhang",
                "X. Liu",
                "J. Xiong",
                "S. Zhou",
                "W. Zhao",
                "E. Zhu",
                "Z. Cai"
            ],
            "title": "Consensus one-step multi-view subspace clustering",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 4676\u20134689.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "H. Liu",
                "Q. Li",
                "X.-M. Wu"
            ],
            "title": "Attributed graph clustering via adaptive graph convolution",
            "venue": "Proceedings of the International Conference on International Joint Conferences on Artificial Intelligence, 4327\u20134333.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Learning the underlying similarity relationships contributes to capturing the inherent information of data and thus further facilitates the performance of downstream tasks. Considering the data points typically lying in latent subspaces, the subspace clustering technique plays an essential role to group data with respect to their underlying subspaces. These methods first exploit a similarity graph to represent the relationship of data by means of the self-expressiveness property and then perform the spectral clustering on the obtained graph to achieve the final clustering result (Ma et al. 2020). These methods are essentially graph-based methods, among which Sparse Subspace Clustering (SSC) (Elhamifar and Vidal 2013), Least Squares Regression (LSR) (Lu et al. 2012) and Low-Rank Representation (LRR) (Liu et al. 2012) are\n*Corresponding author. Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nfrequently employed in various practical fields of machine learning.\nIt is widely known that real-world data generally contain multiple views that are collected from multiple channels (Bickel and Scheffer 2004; Gao et al. 2015; Huang, Kang, and Xu 2020). For instance, a face image might consist of various features such as ambient light and a person\u2019s emotion, where one type of feature corresponds to a distinct view of data. Unlike traditional single-view algorithms, multi-view subspace clustering methods can precisely reshape the correlation between data samples. Assuming that a comprehensive latent representation can be generated from multi-view data, (Zhang et al. 2017) designed a multi-view subspace clustering method by searching the underlying latent representation from multiple views. To better fit the reallife data, the work (Luo et al. 2018) exploited one common consistent structure and a group of view-specific representations simultaneously to construct subspace representations. In (Huang et al. 2021), the diversity of multi-view data is redefined to represent a much broader concept than the noises and a holistic design of the clustering method is adopted by simultaneously considering the diverse parts and consistent pure components. (Zhang et al. 2020) introduced a one-step multi-view subspace clustering algorithm by integrating the steps of similarity learning, multi-view information fusion, and final clustering, which is distinguished from existing methods with isolated learning. To reduce the computational complexity of multi-view clustering, (Liu et al. 2022) employed the optimal anchor graph to directly output the clustering labels and incorporated the processes of graph construction and anchor learning into an integrated model to facilitate subsequent clustering tasks.\nAlthough the aforementioned multi-view clustering algorithms showcase prominent performance, there are still two major drawbacks. For one thing, they assume that each sample can be well represented from the original data space, which may yield insufficient representation as real-world data generally consist of redundancy and are easily corrupted. Therefore, it is elusive to recover a separable representation to facilitate the subsequent clustering module. For another, blindly utilizing the Euclidean distance to measure the similarity between samples ignores the inherent correlation between samples to a large extent and thus fails to explore the underlying data distribution accurately. In this\npaper, we propose to seamlessly integrate metric learning and graph learning for multi-view clustering. We first recover a smooth representation by exploring the geometric structure of data. Then we employ a useful metric to depict the inherent structure with linearity-aware of the affinity graph obtained based on the smooth representation. We model the above concerns into a unified learning framework and hence complement each learning subtask in a mutual reinforcement manner. Our empirical experiments demonstrate the effectiveness and the superiority of the proposed method. The main contributions of our work are outlined as follows:\n\u2022 We propose to recover a separable representation and better fit the inherent structure with the help of smooth representation learning and linearity-aware metric module respectively. We model the above concerns into a unified learning framework and hence lead to a novel model termed metric multi-view graph clustering.\n\u2022 An efficient algorithm is introduced to solve the optimization problem. By leveraging the subtasks of smooth representation learning, multiple similarity graphs recovering, and the consensus graph fusing in our joint model, each subtask is alternately boosted towards an overall optimal solution.\n\u2022 Extensive experiments on benchmark datasets are conducted to demonstrate the superiority of our model, compared with other state-of-the-arts.\nNotations. In this paper, we utilize the normal italic symbols (e.g, z), boldface lowercase symbols (e.g, z), and bold capital symbols (e.g, Z) to represent scalars, vectors, and matrixes, respectively. 1 represents a column vector with all its elements equal to 1. I is an identity matrix. \u2225\u00b7\u2225F denotes the Frobenius norm of a matrix."
        },
        {
            "heading": "Related Work",
            "text": "It is well-known that subspace clustering methods aim to exploit underlying subspaces in the form of the affinity graph (Lu et al. 2018). In general, subspace learning is based on the self-expression of data samples, i.e., each sample can be reconstructed by a linear combination of the others in a union of subspaces (Elhamifar and Vidal 2013; Huang et al. 2019). Given a data matrix X = [x1, . . . ,xn] \u2208 Rn\u00d7d, where n is the number of instances and d denotes the feature dimension. According to the self-expressiveness property, we have\nxi = \u2211 j xjzij\ns.t. \u2200i, zij \u2265 0, (1)\nwhere the combination coefficient zij denotes the similarity between the original data sample xi and xj . Accordingly, one can recover the coefficient matrix Z = [zij ] \u2208 Rn\u00d7n by solving\nminZ \u2211n\ni=1 \u2225\u2225\u2225xi \u2212\u2211j xjzij\u2225\u2225\u22252 F\ns.t. \u2200i, zij \u2265 0, (2)\nwhere Z can also be treated as a similarity graph.\nAccording to Eq. (2), the subspace clustering model can be formulated as\nminZ \u2225X\u2212XZ\u22252F + \u03d5 \u2225Z\u2225 2 F\ns.t. Z \u2265 0, (3)\nwhere \u03d5 is a parameter to balance the corresponding regularization term. Based on Eq. (3), plenty of subspace clustering algorithms have been studied up to now (You, Robinson, and Vidal 2016; Yang et al. 2020).\nIt is clear that Eq. (3) can be extended to the multiview domain. Given a multi-view data with m views{ X(1), . . . ,X(m) | X(v) \u2208 Rn\u00d7dv } where dv is the feature number in the v-th view, the multi-view version of Eq. (3) is\nminZ(v) \u2211m v=1 \u2225\u2225X(v) \u2212X(v)Z(v)\u2225\u22252 F + \u03d5 \u2225\u2225Z(v)\u2225\u22252 F\ns.t. Z(v) \u2265 0, (4)\nwhere Z(v) represents the affinity matrix of the v-th view."
        },
        {
            "heading": "The Proposed Method",
            "text": "Considering that the raw data might not be separable into subspaces due to the natural existence of redundancy and corruption, we propose to recover a smooth representation for each view, inspired by the graph filtering technique (Shuman et al. 2013). It is based on the fact that a signal is smooth by nature and associated adjacent nodes often share similar feature values. Since the smoother signals are often equipped with lower frequencies, retaining the lowfrequency components is much preferred rather than highfrequency ones as they represent the smoothly changing structure (e.g. background) and the rapidly changing details (e.g. corruptions) respectively. Hence we employ a low-pass filter to obtain a smooth representation for each view. By taking each column of X(v) as a graph signal, the formulation of k-order graph filtering on X(v) can be designed as\nX\u0304(v) = ( I\u2212 L (v)\n2\n)k X(v), (5)\nwhere X\u0304 represents a smoothed representation, L(v) is the normalized graph Laplacian and k is a positive integer to consider the k-hop neighborhood relations (Zhang et al. 2019).\nIt is common knowledge that the core of clustering is to measure the similarity and dissimilarity among the given samples. Therefore, what kind of metric should be utilized to measure the distance between two data points is critical for the clustering task. As shown in Figure 1, Shiba inu 1 (S1) is closer to the golden retriever 1 (G1) than Shiba inu 3 (S3) under the metric of Euclidean distance, i.e., d1 > d2. However, it is obvious that both S1 and S3 belong to the same kind of dog, while S1 and G1 are not. In this paper, we employ a useful metric to depict the inherent similarity relationship with linearity-aware of the affinity graph. Here we first introduce the Pearson Correlation Coefficient (Benesty et al. 2009; Schober, Boer, and Schwarte 2018; Xu et al. 2022) as follows:\nDefinition 1 (Pearson Correlation Coefficient) Assumed a data matrix X \u2208 Rm\u00d7n, where m stands for the feature dimension and n represents the number of instances. Choosing two arbitrary samples xi,xj from X, making sure xi\u2212 x\u0304i,xj \u2212 x\u0304j \u0338= 0, where x\u0304i = 1m \u2211m k=1 x k i . The Pearson Correlation Coefficient between them is formally defined as:\nPCC (xi,xj) = (xi \u2212 x\u0304i)\u22a4 (xj \u2212 x\u0304j) \u2225xi \u2212 x\u0304i\u22252 \u2225xj \u2212 x\u0304j\u22252 . (6)\nThe boundedness of Eq. (6) is given below.\nTheorem 1 (Boundedness of the Coefficient) As for two arbitrary instances xi,xj \u2208 X, one of the most fundamental properties is that:\n\u22121 \u2264 PCC (xi,xj) \u2264 1. (7)\nProof 1 Focusing on the formulation of PCC and without loss of generality, we reformulate Eq. (6) as:\nPCC = (xi \u2212 x\u0303i) \u22a4 (\u03bbxi \u2212 \u03bbx\u0303i)\n\u2225xi \u2212 x\u0303i\u22252 \u2225\u03bbxi \u2212 \u03bbx\u0303i\u22252 (8)\nwhere \u03bbxi equals xj to depict the linear relationship between these two samples. And by simple algebra, we have:\nPCC = \u03bb (xi \u2212 x\u0303i) \u22a4 (xi \u2212 x\u0303i) |\u03bb| ( \u2225xi \u2212 x\u0303i\u222522 ) = \u03bb |\u03bb| (9)\nTherefore, the range of PCC varying from -1 to 1 is proven.\nBased on the Pearson Correlation Coefficient(PCC) in Eq. (6), we take a step forward that considers the linear relationship as a metric to evaluate the similarity among samples. The linearity-aware metric is explained in Definition 2. Definition 2 Linearity-Aware Metric According to theorem 1, PCC succeed in depicting the linear relationship between instances. In light of this, to pursue a novel metric that is suitable for the following optimization framework, we have: L (xi,xj) = 1\u2212 PCC. (10) In this way, we establish a much more discriminating model and we are now able to separate those samples which are closer under the Euclidean distance metric but do not have much linear relation. Therefore, we will obtain a more accurate and linear distinguishable outcome. The illustration of the improvement obtained by our method is shown in Figure 1. The conditions for the linearity-aware metric are: \u2022 L (xi,xj) = 2. It suggests that xi and xj are completely\nnegative correlated. \u2022 1 < L (xi,xj) < 2. It suggests that there is a \u201dcertain\ndegree\u201d of negative linear relationship between xi and xj . \u2022 L (xi,xj) = 1. xi and xj are not correlated. \u2022 0 < L (xi,xj) < 1. It suggests that there is a certain\ndegree of positive linear relationship between xi and xj . \u2022 L (xi,xj) = 0. xi and xj are completely positive corre-\nlated. Taking Figure 1 as an example, \u03b82 between S1 and W1 is 90 degrees, which means they are not correlated. \u03b84 between S1 and C1 is larger than 90 degrees but smaller than 180 degrees, which means they have a certain degrees of negative correlation. And \u03b83 between S1 and G1 is smaller than 90 degrees. But \u03b81 between S1 and S3 is smaller than \u03b83. Therefore, S1 and S3 have a much stronger correlation. In this regard, this showcases the feasibility of the linearityaware metric. Intuitively, we learn the inherent similarity graph S(v) for each view with linearity-aware as follows\nminS \u2211m\nv=1 \u2211n i=1 \u2211n j=1 L2 ( z (v) i , z (v) j ) s (v) ij + \u03b3\u2225S(v)\u22252F\ns.t. sv\u22a4i 1 = 1, 0 \u2264 s (v) ij \u2264 1,\n(11) where \u03b3 represents the trade-off parameter.\nCombining Eq. (4), Eq. (5) and Eq. (11), we arrive at minZ(v),S(v) L1 ( X\u0304(v);Z(v),S(v) ) =\u2211m\nv=1 \u2225\u2225X\u0304(v) \u2212 X\u0304(v)Z(v)\u2225\u22252 F + \u03d5 \u2225\u2225Z(v)\u2225\u22252 F\n+ \u2211m\nv=1 \u2211n i=1 \u2211n j=1 L2 ( z (v) i , z (v) j ) s (v) ij + \u03b3\u2225S(v)\u22252F\ns.t. sv\u22a4i 1 = 1, 0 \u2264 s (v) ij \u2264 1,\n(12) One of the most critical procedures in multi-view clustering is to effectively integrate information from different views. In light of (Nie et al. 2017), we utilize an adaptive weight strategy to meet the consistency among all views:\nminC L2 ( S(v);C ) = \u2211m v=1 w (v) \u2225\u2225C\u2212 S(v)\u2225\u22252 F (13)\nwhere C is the consensus graph, and w(v) is a self-tuned parameter used to weigh the importance of S(v) which can be defined as\nw(v) = 1 2 \u2225\u2225C\u2212 S(v)\u2225\u2225\nF\n. (14)\nNote that Eq. (14) is essentially an inverse distance weighting.\nWith the coefficient matrix and similarity graphs learned in Eq. (11), and the consensus graph achieved in Eq. (13), our model can be finally modeled as\nminZ(v),S(v),C L1 ( X\u0304(v);Z(v),S(v) ) \ufe38 \ufe37\ufe37 \ufe38\ngraph learning\n+L2 ( S(v);C ) \ufe38 \ufe37\ufe37 \ufe38\ngraph fusion\ns.t. Z(v) \u2265 0, s(v)\u22a4i 1 = 1, 0 \u2264 s (v) ij \u2264 1,\n1\u22a4ci = 1, 0 \u2264 cij \u2264 1. (15)"
        },
        {
            "heading": "Optimization",
            "text": "Since Eq. (15) is not jointly convex in all variables, we propose to solve this non-convex problem by adapting an alternative optimization algorithm.\nupdate Z(v): To obtain the coefficient matrix Z(v) of each view, we need to solve\nminZv \u2211m\nv=1 \u2225\u2225X\u0304(v) \u2212 X\u0304(v)Z(v)\u2225\u22252 F + \u03d5 \u2225\u2225Z(v)\u2225\u22252 F\ns.t. Z(v) \u2265 0, (16)\nsince Eq. (16) is independent for each v, we can solve it separately\nminZv \u2225\u2225X\u0304(v) \u2212 X\u0304(v)Z(v)\u2225\u22252\nF + \u03d5 \u2225\u2225Z(v)\u2225\u22252 F\ns.t. Z(v) \u2265 0, (17)\nTaking the derivative of Eq. (17) and setting it to zero, we then have\nZ(v) = max (( X\u0304(v)\u22a4X\u0304(v) + \u03d5I )\u22121 X\u0304(v)\u22a4X\u0304(v), 0 ) .\n(18) update S(v): With other variables fixed, the correspond-\ning optimization problem of each view becomes\nminS(v) \u2211n i=1 \u2211n j=1 L2 ( z (v) i , z (v) j ) s (v) ij +\n\u03b3\u2225S(v)\u22252F + w(v) \u2225\u2225\u2225C\u2212 S(v)\u2225\u2225\u22252\nF .\n(19)\nFor the simplicity of calculation, we denote L2 ( z (v) i , z (v) j ) as d(v)ij , thus Eq. (19) can be written in a convenient form\ns (v)\ni(t+1) =argmin\ns (v)\ni(t)\n\u2225\u2225\u2225\u2225\u2225\u2225s(v)i(t) + ( d (v) i \u2212 2w(v)ci ) 2 ( \u03b3 + w(v) ) \u2225\u2225\u2225\u2225\u2225\u2225 2\n2\ns.t. s(v)\u22a4i 1 = 1, 0 \u2264 sij \u2264 1,\n(20)\nAlgorithm 1: Algorithm for Metric Multi-view Subspace Clustering\nInput: Feature matrix of m views {X(1),X(2), . . . ,X(m)}; the number of clusters p; balance parameters \u03d5 and \u03b3. Initialize: w(v) = 1m , S = I. Construct the initial subspace Z(v) of X(v) by solving Eq. (4). Output: The consensus graph C and the final clustering result.\n1: repeat 2: Update X\u0304(v) according to Eq. (5), where L(v) is the normalized graph Laplacian of S(v). 3: Update the coefficient matrix Z(v) according to Eq. (18). 4: Update the similarity graph S(v) by solving Eq. (21). 5: Update consensus similarity graph C by solving Eq.(25). 6: Update the self-tuned weight parameter w(v) according to Eq. (14). 7: until converge 8: Perform the standard spectral clustering on C to obtain\nthe final clustering results.\nwhere s(v) i(t+1) means s(v)i in the (t + 1)-th iteration. Since \u03d5, w(v), dvi and ci are constant values when updating S\n(v), we further simplify Eq. (20) by letting \u03bb = ( \u03b3 + w(v) ) and\nh (v) i = ( d (v) i \u2212 2w(v)ci ) . In this way, Eq. (20) can be written as\ns (v)\ni(t+1) =argmin\ns (v)\ni(t)\n\u2225\u2225\u2225\u2225s(v)i(t) + 12\u03bbhi \u2225\u2225\u2225\u22252 2\ns.t. s(v)\u22a4i 1 = 1, 0 \u2264 sij \u2264 1.\n(21)\nEq. (21) can be solved by off-the-shelf algorithm proposed in (Duchi et al. 2008; Huang, Nie, and Huang 2015).\nupdate C: While other variables are fixed, Eq. (15) is equivalent to\nminC \u2211m v=1 w (v) \u2225\u2225C\u2212 S(v)\u2225\u22252 F\ns.t. 1\u22a4ci = 1,C \u2265 0. (22)\nEq. (22) can be reformulated in an element-wise form minC \u2211m v=1 w (v) \u2211n i,j=1 ( cij \u2212 s(v)ij )2 s.t. 1\u22a4ci = 1,C \u2265 0. (23)\nIt\u2019s self-evident that each view of Eq. (23) is independent from one another. Therefore we optimize it by each row i:\nminci \u2211m v=1 \u2211n j=1 w (v) ( cij \u2212 s(v)ij )2 s.t. 1\u22a4ci = 1,C \u2265 0,\n(24)\nwhere cij stands for the j-th element with respect to the row vector ci. Thus, Eq. (24) can be rewritten as\nminci \u2211m v=1 w (v) \u2225\u2225\u2225ci \u2212 s(v)i \u2225\u2225\u22252 2\ns.t. 1\u22a4ci = 1,C \u2265 0. (25)\nEq. (25) can be solved by utilizing the algorithm proposed in (Wang, Yang, and Liu 2019).\nThe detailed algorithm to solve Eq. (15) is summarized in Algorithm 1."
        },
        {
            "heading": "Time Complexity Analysis",
            "text": "Obviously, there are five steps that mainly resolve the time complexity of Algorithm 1. Recall that n, m, and p represent the number of data samples, views, and clusters, respectively. Let Nv be the number of nonzero entries of S(v) (both S(v) and Z(v) are sparse). The time complexity of each step is given in Table 1.\nIn practical, we have m \u226a n and p \u226a n, thus the overall time complexity is bounded by O ( n2 ) ."
        },
        {
            "heading": "Experiment",
            "text": "In this section, we demonstrate the efficiency and the superiority of our proposed method on several benchmark data sets, compared with other state-of-the-art multi-view clustering methods."
        },
        {
            "heading": "Experimental Setup",
            "text": "Intending to achieve a comprehensive evaluation, we compare our proposed method with several competitors: Multiview Spectral Clustering with Co-training strategy(Cotrain) (Kumar and Daume\u0301 2011), Multi-view Spectral Clustering with Co-regularized strategy(Co-reg) (Kumar, Rai, and Daume 2011), Self-Weighted Multi-view Clustering(SwMC) (Nie et al. 2017), Generalized Latent MultiView Subspace Clustering(LRMSC) (Zhang et al. 2018), Consistent and Specific Multi View Subspace Clustering(CSMSC) (Luo et al. 2018), Multi-view Consensus Graph Clustering(MCGC) (Zhan et al. 2019), Graph based Multi-view Clustering(GMC) (Wang, Yang, and Liu 2019), Consensus One-step Multi-view Subspace Clustering(COMVSC) (Zhang et al. 2020), Large-scale Multiview Subspace Clustering in Linear Time(LMVSC) (Kang et al. 2020), multi-view clustering via Cross-view Graph Diffusion(CGD) (Tang et al. 2020), Multi-view Subspace Clustering via Co-training Robust Data Representation(CoMSC) (Liu et al. 2021a), One-pass Multi-view Clustering for Large-scale Data(OPMC) (Liu et al. 2021b), Scalable Multi-view Subspace Clustering with Unified Anchors(SMVSC) (Sun et al. 2021), Fast Parameter-free Multiview Subspace Clustering with Consensus Anchor Guid-\nance(FPMVS) (Wang et al. 2021),m Consensus Graph Learning for Multi-view Clustering(CGL) (Li et al. 2021).\nWe compare the above-mentioned methods with our proposed algorithm on several benchmark datasets: HAR is a Human Activity Recognition dataset which consists of 2941 samples and 6 classes. Cora is a popular dataset which is composed of 2708 instances and 7 categories, MSRC includes 240 images and 8 classes. Each class contains 30 images. Inspired by (Nie, Cai, and Li 2017), we extract 5 features from MSRC to comprehensively capture the intrinsic character within each image. Newsgroups (NGs) stands for one of the subsets of 20 Newsgroups datasets. It contains 500 newsgroups which are described from 3 different views. ORL is a well-known human face dataset that includes 400 images from 20 individuals. Yale contains 165 samples with 15 classes and is depicted from 3 different views.\nTo achieve the fairness of our experiment, the parameters of all compared algorithms are tuned to optimal values.\nMoreover, we run each method 10 times with the average results being recorded. The parameter setting of our method will be discussed in later subsection."
        },
        {
            "heading": "Results and Analysis",
            "text": "In order to adequately evaluate our method and compared algorithms, we adapt four widely used criteria including normalized mutual information (NMI), accuracy (ACC), Purity, and F-score. The clustering results are reported in Tables 2- 7. We can arrive at a conclusion that our method is very effective and competitive, in view of the fact that the proposed method outperforms other competitors in the majority of cases. In detail, our method consistently obtains the best\nresults in terms of NMI, Purity, and F-score on all datasets. While for ACC, the proposed method surpasses the compared methods except in one case on dataset HAR, where the second best performance is achieved.\nWe further visualize the clustering results of NGs with t-SNE (Van der Maaten and Hinton 2008). As shown in Figure 2, we can see CoMSC, CSMSC and LRMSC are able to divide the data samples into different clusters, yet there are no clear boundaries between them. What\u2019s worse, MCGC and SwMC even cannot find a clear clustering structure, which obviously fails to achieve a good performance. On the contrary, the clustering results obtained by our method showcase a more compact structure with clear boundaries,\nwhich demonstrates the superiority of our method."
        },
        {
            "heading": "Sensitivity Analysis",
            "text": "With the aim of studying what impact different parameter settings will have on the clustering results, we vary three parameters: \u03b3 and \u03d5 in the ranges[1, 1e1, 1e2, 1e3, 1e4, 1e5], and the filter order k in [1, 2, 3, 4, 5, 6] respectively. Taking the ORL dataset as an example, we can see the clustering performance is relatively stable with respect to different k and \u03d5, whereas it is a little sensitive to different \u03b3, as shown in Figure 3. Considering that \u03b3 is the balance parameter for\nmetric learning, we can come to a conclusion that the best metric differs from data to data. Generally, we can achieve superior performance when \u03b3 varies in the range [1e1,1e3]."
        },
        {
            "heading": "Convergence Analysis",
            "text": "Since our model is essentially a nonconvex problem and is optimized with an iterative algorithm, it is critical to validate its convergence property. This section empirically showcases the convergence property and studies how fast the proposed algorithm can converge. As can be seen in Figure 4, our algorithm converges fast and the objective value varies little within a few iterations, which demonstrates the efficiency of the proposed algorithm. Note that despite the nonconvexity of our objective function described in Eq. (15), we can still search for the optimal solution for each variable and finally, the algorithm will converge to a local minima."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, we introduce a novel graph-based multi-view clustering method that measures the distance between instances from an innovative perspective. Unlike most existing methods considering Euclidean distance as the measurement of similarity between samples, our proposed method is capable of both the global structure of the input data and exploring the linear relationship between two local neighbors. Meanwhile, the graph filter that we used significantly boosts the robustness of the method. Finally, we integrate filter learning, subspace learning, and linearity-aware metric learning into one unified framework to achieve collaborative learning. Therefore, our method generates outstanding results on several authoritative benchmark datasets and is proven to outperform current state-of-the-art methods."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the Key Program of National Science Foundation of China (Grant No. 61836006), par-\ntially supported by the National Science Foundation of China under Grant 62106164, and the Sichuan Science and Technology Program under Grants 2021ZDZX0011 and 2022YFG0188."
        }
    ],
    "title": "Metric Multi-View Graph Clustering",
    "year": 2023
}