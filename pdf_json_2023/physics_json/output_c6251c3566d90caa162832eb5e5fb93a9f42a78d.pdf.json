{
    "abstractText": "Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery. Our code and data are available at https://github.com/SCIRHI/ArtificiallyR2R.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuhan Chen"
        },
        {
            "affiliations": [],
            "name": "Nuwa Xi"
        },
        {
            "affiliations": [],
            "name": "Yanrui Du"
        },
        {
            "affiliations": [],
            "name": "Haochun Wang"
        },
        {
            "affiliations": [],
            "name": "Chen Jianyu"
        },
        {
            "affiliations": [],
            "name": "Sendong Zhao"
        },
        {
            "affiliations": [],
            "name": "Bing Qin"
        }
    ],
    "id": "SP:b98f7dfdd5d26052b74bf615518855720af71dc0",
    "references": [
        {
            "authors": [
                "A.C. Anderson"
            ],
            "title": "The process of structure-based drug design",
            "venue": "Chemistry & biology, 10(9): 787\u2013797.",
            "year": 2003
        },
        {
            "authors": [
                "V. Bagal",
                "R. Aggarwal",
                "P. Vinod",
                "U.D. Priyakumar"
            ],
            "title": "MolGPT: molecular generation using a transformerdecoder model",
            "venue": "Journal of Chemical Information and Modeling, 62(9): 2064\u20132076.",
            "year": 2021
        },
        {
            "authors": [
                "S. Banerjee",
                "A. Lavie"
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 65\u201372.",
            "year": 2005
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "S. Bubeck",
                "V. Chandrasekaran",
                "R. Eldan",
                "J. Gehrke",
                "E. Horvitz",
                "E. Kamar",
                "P. Lee",
                "Y.T. Lee",
                "Y. Li",
                "S Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712",
            "year": 2023
        },
        {
            "authors": [
                "D. Christofidellis",
                "G. Giannone",
                "J. Born",
                "O. Winther",
                "T. Laino",
                "M. Manica"
            ],
            "title": "Unifying molecular and textual representations via multi-task language modelling",
            "venue": "arXiv preprint arXiv:2301.12586.",
            "year": 2023
        },
        {
            "authors": [
                "S. Curtarolo",
                "G.L. Hart",
                "M.B. Nardelli",
                "N. Mingo",
                "S. Sanvito",
                "O. Levy"
            ],
            "title": "The high-throughput highway to computational materials design",
            "venue": "Nature materials, 12(3): 191\u2013201.",
            "year": 2013
        },
        {
            "authors": [
                "V. Cuzzucoli Crucitti",
                "A. Ilchev",
                "J.C. Moore",
                "H.R. Fowler",
                "J.-F. Dubern",
                "O. Sanni",
                "X. Xue",
                "B.K. Husband",
                "A.A. Dundas",
                "S Smith"
            ],
            "title": "Predictive Molecular Design and Structure\u2013Property Validation of Novel TerpeneBased, Sustainably Sourced Bacterial Biofilm-Resistant",
            "year": 2023
        },
        {
            "authors": [
                "Z. Dai",
                "V.Y. Zhao",
                "J. Ma",
                "Y. Luan",
                "J. Ni",
                "J. Lu",
                "A. Bakalov",
                "K. Guu",
                "K. Hall",
                "M.-W. Chang"
            ],
            "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Du",
                "S. Zhao",
                "Y. Chen",
                "R. Bai",
                "J. Liu",
                "H. Wu",
                "H. Wang",
                "B. Qin"
            ],
            "title": "The CALLA Dataset: Probing LLMs\u2019 Interactive Knowledge Acquisition from Chinese Medical Literature",
            "venue": "arXiv preprint arXiv:2309.04198.",
            "year": 2023
        },
        {
            "authors": [
                "J.L. Durant",
                "B.A. Leland",
                "D.R. Henry",
                "J.G. Nourse"
            ],
            "title": "Reoptimization of MDL keys for use in drug discovery",
            "venue": "Journal of chemical information and computer sciences, 42(6): 1273\u20131280.",
            "year": 2002
        },
        {
            "authors": [
                "C. Edwards",
                "T. Lai",
                "K. Ros",
                "G. Honke",
                "K. Cho",
                "H. Ji"
            ],
            "title": "Translation between Molecules and Natural Language",
            "venue": "2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Edwards",
                "C. Zhai",
                "H. Ji"
            ],
            "title": "Text2mol: Crossmodal molecule retrieval with natural language queries",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 595\u2013607.",
            "year": 2021
        },
        {
            "authors": [
                "N. Ferruz",
                "S. Schmidt",
                "B. H\u00f6cker"
            ],
            "title": "ProtGPT2 is a deep unsupervised language model for protein design",
            "venue": "Nature communications, 13(1): 4348.",
            "year": 2022
        },
        {
            "authors": [
                "N. Frey",
                "R. Soklaski",
                "S. Axelrod",
                "S. Samsi",
                "R. GomezBombarelli",
                "C. Coley",
                "V. Gadepally"
            ],
            "title": "Neural scaling of deep chemical models",
            "year": 2022
        },
        {
            "authors": [
                "T. Gaudelet",
                "B. Day",
                "A.R. Jamasb",
                "J. Soman",
                "C. Regep",
                "G. Liu",
                "J.B. Hayter",
                "R. Vickers",
                "C. Roberts",
                "J Tang"
            ],
            "title": "Utilizing graph machine learning within drug discovery and development",
            "venue": "Briefings in bioinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "J. Li",
                "Y. Liu",
                "W. Fan",
                "X.-Y. Wei",
                "H. Liu",
                "J. Tang",
                "Q. Li"
            ],
            "title": "Empowering Molecule Discovery for MoleculeCaption Translation with Large Language Models: A ChatGPT Perspective. arXiv preprint arXiv:2306.06615",
            "year": 2023
        },
        {
            "authors": [
                "Lin",
                "C.-Y."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "A. Liu",
                "S. Swayamdipta",
                "N.A. Smith",
                "Y. Choi"
            ],
            "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, 6826\u20136847.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "W. Zhang",
                "Y. Xia",
                "L. Wu",
                "S. Xie",
                "T. Qin",
                "M. Zhang",
                "T.-Y. Liu"
            ],
            "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 1606\u20131616. Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "A. Malte",
                "P. Ratadiya"
            ],
            "title": "Evolution of transfer learning in natural language processing",
            "venue": "arXiv preprint arXiv:1910.07370.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Miao",
                "Y. Li",
                "X. Wang",
                "W.-C. Tan"
            ],
            "title": "Snippext: Semi-supervised opinion mining with augmented data",
            "venue": "Proceedings of The Web Conference 2020, 617\u2013628.",
            "year": 2020
        },
        {
            "authors": [
                "F.P. Miller",
                "A.F. Vandome",
                "J. McBrewster"
            ],
            "title": "Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker, hamming distance",
            "year": 2009
        },
        {
            "authors": [
                "S. Min",
                "M. Lewis",
                "L. Zettlemoyer",
                "H. Hajishirzi"
            ],
            "title": "MetaICL: Learning to Learn In Context",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2791\u20132809.",
            "year": 2022
        },
        {
            "authors": [
                "L. Ouyang",
                "J. Wu",
                "X. Jiang",
                "D. Almeida",
                "C. Wainwright",
                "P. Mishkin",
                "C. Zhang",
                "S. Agarwal",
                "K. Slama",
                "A Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "K. Papineni",
                "S. Roukos",
                "T. Ward",
                "W.-J. Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "G.A. Patani",
                "E.J. LaVoie"
            ],
            "title": "Bioisosterism: a rational approach in drug design",
            "venue": "Chemical reviews, 96(8): 3147\u20133176.",
            "year": 1996
        },
        {
            "authors": [
                "D. Polykovskiy",
                "A. Zhebrak",
                "B. Sanchez-Lengeling",
                "S. Golovanov",
                "O. Tatanov",
                "S. Belyaev",
                "R. Kurbanov",
                "A. Artamonov",
                "V. Aladinskiy",
                "M Veselov"
            ],
            "title": "Molecular sets (MOSES): a benchmarking platform for molecular generation models",
            "venue": "Frontiers in pharmacology,",
            "year": 2020
        },
        {
            "authors": [
                "K. Preuer",
                "P. Renz",
                "T. Unterthiner",
                "S. Hochreiter",
                "G. Klambauer"
            ],
            "title": "Fr\u00e9chet ChemNet distance: a metric for generative models for molecules in drug discovery",
            "venue": "Journal of chemical information and modeling, 58(9): 1736\u2013 1741.",
            "year": 2018
        },
        {
            "authors": [
                "C. Raffel",
                "N. Shazeer",
                "A. Roberts",
                "K. Lee",
                "S. Narang",
                "M. Matena",
                "Y. Zhou",
                "W. Li",
                "P.J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research, 21(1): 5485\u20135551.",
            "year": 2020
        },
        {
            "authors": [
                "A.S. Rifaioglu",
                "H. Atas",
                "M.J. Martin",
                "R. Cetin-Atalay",
                "V. Atalay",
                "T. Do\u011fan"
            ],
            "title": "Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",
            "venue": "Briefings in bioinformatics, 20(5): 1878\u20131912.",
            "year": 2019
        },
        {
            "authors": [
                "D. Rogers",
                "M. Hahn"
            ],
            "title": "Extended-connectivity fingerprints",
            "venue": "Journal of chemical information and modeling, 50(5): 742\u2013754.",
            "year": 2010
        },
        {
            "authors": [
                "O. Rubin",
                "J. Herzig",
                "J. Berant"
            ],
            "title": "Learning To Retrieve Prompts for In-Context Learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2655\u20132671.",
            "year": 2022
        },
        {
            "authors": [
                "T. Schick",
                "H. Sch\u00fctze"
            ],
            "title": "Generating Datasets with Pretrained Language Models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6943\u20136951. Online and Punta Cana, Dominican Republic: Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "N. Schneider",
                "R.A. Sayle",
                "G.A. Landrum"
            ],
            "title": "Get Your Atoms in Order An Open-Source Implementation of a Novel and Robust Molecular Canonicalization Algorithm",
            "venue": "Journal of chemical information and modeling, 55(10): 2111\u20132120.",
            "year": 2015
        },
        {
            "authors": [
                "R. Sennrich",
                "B. Haddow",
                "A. Birch"
            ],
            "title": "Improving Neural Machine Translation Models with Monolingual Data",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 86\u201396.",
            "year": 2016
        },
        {
            "authors": [
                "K. Singhal",
                "S. Azizi",
                "T. Tu",
                "S.S. Mahdavi",
                "J. Wei",
                "H.W. Chung",
                "N. Scales",
                "A. Tanwani",
                "H. Cole-Lewis",
                "S Pfohl"
            ],
            "title": "Large language models encode clinical knowledge",
            "year": 2023
        },
        {
            "authors": [
                "T.T. Tanimoto"
            ],
            "title": "Elementary mathematical theory of classification and prediction",
            "year": 1958
        },
        {
            "authors": [
                "H. Wang",
                "S. Zhao",
                "Z. Qiang",
                "Z. Li",
                "N. Xi",
                "Y. Du",
                "M. Cai",
                "H. Guo",
                "Y. Chen",
                "H Xu"
            ],
            "title": "2023a. Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. arXiv preprint arXiv:2309.04175",
            "year": 2023
        },
        {
            "authors": [
                "W.Y. Wang",
                "D. Yang"
            ],
            "title": "That\u2019s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets",
            "venue": "Proceedings of the 2015 conference on empirical methods in natural language pro-",
            "year": 2015
        },
        {
            "authors": [
                "Y. Wang",
                "Y. Kordi",
                "S. Mishra",
                "A. Liu",
                "N.A. Smith",
                "D. Khashabi",
                "H. Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wang",
                "L. Liang",
                "Z. Yin",
                "J. Lin"
            ],
            "title": "Improving chemical similarity ensemble approach in target prediction",
            "venue": "Journal of cheminformatics, 8: 1\u201310.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Wang",
                "T. Liu",
                "H. Peng",
                "Y. Fang"
            ],
            "title": "Advances in molecular design and photophysical engineering of perylene bisimide-containing polyads and multichromophores for film-based fluorescent sensors",
            "venue": "The Journal of Physical Chemistry B, 127(4): 828\u2013837.",
            "year": 2023
        },
        {
            "authors": [
                "J. Wei",
                "K. Zou"
            ],
            "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "D. Weininger"
            ],
            "title": "SMILES, a chemical language and information system",
            "venue": "1. Introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1): 31\u201336.",
            "year": 1988
        },
        {
            "authors": [
                "D.S. Wishart",
                "Y.D. Feunang",
                "A.C. Guo",
                "E.J. Lo",
                "A. Marcu",
                "J.R. Grant",
                "T. Sajed",
                "D. Johnson",
                "C. Li",
                "Z Sayeeda"
            ],
            "title": "DrugBank 5.0: a major update to the DrugBank database for 2018",
            "venue": "Nucleic acids research,",
            "year": 2018
        },
        {
            "authors": [
                "N. Xi",
                "S. Zhao",
                "H. Wang",
                "C. Liu",
                "B. Qin",
                "T. Liu"
            ],
            "title": "UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language",
            "venue": "arXiv preprint arXiv:2307.05355.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Zeng",
                "Y. Yao",
                "Z. Liu",
                "M. Sun"
            ],
            "title": "A deeplearning system bridging molecule structure and biomedical text with comprehension comparable to human professionals",
            "venue": "Nature communications, 13(1): 862.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Molecule discovery plays a critical role in numerous scientific domains including chemistry (Wang et al. 2023b; Cuzzucoli Crucitti et al. 2023), pharmacology (Patani and LaVoie 1996; Anderson 2003), and materials science (Curtarolo et al. 2013). However, traditional molecule design methods are frequently faced with challenges such as high costs, lengthy development processes, and limited success rates. Introducing a new drug to the market, for instance, might demand over a billion dollars and more than a decade of development (Gaudelet et al. 2021).\nWith the advent of artificial intelligence (AI), innovative cross-modal methods are ushering in new ways to synthesize and analyze complex molecular structures, enhancing efficiency and reshaping the fields of computational chemistry and material science. Edwards et al. (2022) proposed a novel approach to directly translate molecules to corresponding captions and generate molecular structures from\n*Corresponding author Copyright \u00a9 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThe molecule is an\naliphatic alcohol that\nis octane substituted by a hydroxy group at\nposition 3...\nCCCCCC(CC)O\nnatural language text, shown in Figure 1. This cross-modal method heralds a future in which the design and study of specialized molecules can be achieved through simple natural language sentences.\nVarious attempts have been made to resolve these tasks. MolT5 (Edwards et al. 2022) uses SMILES (Simplified Molecular Input Line Entry System) (Weininger 1988) and molecule description respectively for masked language modeling (MLM) (Raffel et al. 2020) as pre-training. Liu et al. (2023) pre-train models with causal language modeling (CLM) on the sequences that blend biomedical literature with molecular structural representations, derived from replacing molecular entities with their SMILES representations. However, these studies are limited by the scarcity of parallel molecule-description pairs, rendering direct sequence-to-sequence training unfeasible. The effectiveness of sequence-to-sequence (seq2seq) training is evident in Christofidellis et al. (2023), where the annotated data from the downstream dataset is incorporated for pretraining, albeit in a significantly lower ratio compared to the unannotated data. The primary bottleneck is the annotation process itself: the annotation of these pairs demands specialized knowledge in molecular chemistry, rendering largescale human annotation both expensive and difficult.\nInspired by the great success of LLMs in natural language processing (NLP) and related fields (Bagal et al. 2021; Frey et al. 2022; Ferruz, Schmidt, and Ho\u0308cker 2022), we propose to mitigate the low-resource difficulty by using artificially-\nar X\niv :2\n30 9.\n05 20\n3v 2\n[ cs\n.C L\n] 2\n1 D\nec 2\n02 3\nreal data generated by LLMs. Unlike \u201creal data\u201d, which originates from genuine experimental or observational sources, this \u201cpseudo data\u201d or \u201cartificially-real data\u201d is crafted artificially. While it mirrors the format of real data, its content does not depict actual real-world observations, making it potentially unsuitable for direct real-world applications.\nOur approach begins by creating a comprehensive pseudo dataset intended for seq2seq pre-training. We collect 1M unlabeled molecules from PubChem and use the in-context learning ability of LLMs to generate descriptive captions for these molecules. To ensure the integrity and diversity of this pseudo data, we adopt a retrieval-based one-shot prompting strategy during generation. Through this way, we construct the first artificially-real dataset, PseudoMD-1M, consisting of 1,020,139 pseudo molecule-description pairs.\nBased on this dataset, we explore the optimal method to leverage pseudo data. We propose two primary methods: 1) using pseudo data exclusively during pre-training for domain adaptation, and 2) integrating pseudo data with real data during fine-tuning as a data augmentation technique. To offer a comprehensive evaluation, we further compile DrugBank-23, a novel dataset derived from a different data source than existing datasets.\nIn summary, our contributions are as follows:\n\u2022 We are the first to incorporate LLMs for low-resource molecule discovery. Using artificially-real data generated by LLMs, we are able to mitigate the data scarcity for the tasks. We release PseudoMD-1M, the first artificiallyreal dataset for cross-modal molecule discovery, which is 33\u00d7 larger than existing real datasets.\n\u2022 We explore the effective construction and utilization of pseudo data. We specifically investigate two principal techniques, including using pseudo data as domain adaptation and data augmentation. We conduct comprehensive experiments on existing datasets, and provide our new dataset called DrugBank-23, which adds a novel data source compared to current datasets.\n\u2022 Experimental results show that despite smaller model size and amount of pre-training data, models using artificially-real data as domain adaptation outperform all prior methods. Furthermore, our method shows continuous improvement with increasing volumes of pseudo data, underscoring its promising future applications."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Cross-modal Molecule Discovery",
            "text": "With the advancement of in-silico molecule discovery methods, the field of molecule exploration is undergoing a transformative shift away from its resource-intensive and costly origins (Rifaioglu et al. 2019; Gaudelet et al. 2021). Edwards, Zhai, and Ji (2021) introduce a new task Text2Mol, which uses descriptions as search queries to retrieve the target molecules. Following this, Edwards et al. (2022) propose two innovative tasks: molecule captioning and text-guided de novo molecule generation. These tasks aim at translating between molecular structures and natural language texts. MolXPT (Liu et al. 2023) leverages literature annotations of\nmolecules to construct a pre-training dataset. Christofidellis et al. (2023) further improves the field with multi-task learning, which combines single-domain and cross-domain datasets for joint training. Most recently, Li et al. (2023) propose a strategy that enables LLMs to accomplish both molecule captioning and text-guided molecule generation tasks. Here we take one step further to construct a large number of high-quality parallel data pairs, in response to the data scarcity that limits the performance of the above approaches."
        },
        {
            "heading": "2.2 Large Language Models",
            "text": "LLMs have achieved significant success in natural language processing by scaling up to billions of parameters (Brown et al. 2020; Ouyang et al. 2022). Trained on vast corpora (Singhal et al. 2023), LLMs show more general intelligence (Bubeck et al. 2023) and remarkable capabilities such as incontext learning (Rubin, Herzig, and Berant 2022; Min et al. 2022). They have also obtained promising performance in chemical (Bagal et al. 2021; Frey et al. 2022), biological (Ferruz, Schmidt, and Ho\u0308cker 2022; Xi et al. 2023) and medical (Wang et al. 2023a; Du et al. 2023) domains. Due to their great generation capability, numerous works have relied on LLMs to generate data for various purposes, including creating semantic textual similarity datasets (Schick and Schu\u0308tze 2021), augmenting natural language inference (Liu et al. 2022), automatically formulating instructions (Wang et al. 2022) and improving few-shot retrieval (Dai et al. 2022). Inspired by these achievements, we aim to employ LLMs to generate parallel data, addressing data scarcity in cross-modal molecule discovery."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Task Overview",
            "text": "Here we introduce two primary tasks for cross-modal molecule discovery. First proposed by Edwards et al. (2022), the two tasks act as a bridge between molecule discovery and NLP and can be considered as cross-modal translation tasks.\nMolecular captioning As illustrated in Figure 1a, Given the SMILES representation SM of molecule M, the task is to generate the corresponding descriptions DM. Text-Based de novo Molecule Generation As shown in Figure 1b, given the descriptions DM of molecules M, the task is to generate its corresponding SMILES SM."
        },
        {
            "heading": "3.2 Artificially-real Data Generation",
            "text": "High-quality pseudo data is the foundation for further exploration. Here we propose PseudoMD-1M, the first pseudo dataset composed of 1M parallel molecule-description data pairs. To acquire sufficient data, we leverage a vast number of unlabeled molecules and use LLMs to generate corresponding descriptions. We begin by collecting 1.1 million unannotated SMILES strings of molecules from PubChem (Kim et al. 2023). We then employ a rigorous filtering procedure to filter out the SMILES in downstream datasets to ensure that there is no overlap between the collected molecules and those contained in the real datasets (Edwards, Zhai, and Ji 2021; Zeng et al. 2022). By doing so, we ensure that no\nCN(C1=CC=CC=C1)C2=CC=C(C=C2)C=C3C(=O)N(C(=S)S3)C4=CC(=CC=C4)Cl Stage #1: Molecule Retrieval\nOutput Description\nMorgan FingerPrints\nTop-k results\nThe molecule is a member of the class of sulfonamide-containing compounds. The compound is an inhibitor of carbonic anhydrase, which is a zinccontaining enzyme that catalyzes the reversible hydration of carbon dioxide. It is also an aromatic amine, a thiazole, and a chlorobenzene. The molecules structural features make it a promising candidate for the treatment of diseases such as glaucoma and epilepsy.\nStage #2: Few-Shot Prompting As an AI model, you have been trained with vast amounts of biomedical, scientific, and pharmaceutical data. # Role Definition You need to convert Simplified Molecular Input Line Entry System (SMILES) formulas of drugs into comprehensive descriptions, which typically include the drug's properties, indications, and pharmacological mechanisms. # Task Description # Few-shot Example # Example 1: SMILES: {SMILES Place Holder} Description:{Description Place Holder} # Example 2: \u2026 # Output Control Using these examples as a guide, and provide a direct response without any introductory or additional text. Now generate a similar detailed description for the following SMILES formula of a drug: {SMILES Place Holder} The molecule is an inhibitor of cystic fibrosis transmembrane conductance regulator, a membrane protein and chloride channel in vertebrates\u2026. # Example 1 # Example 2 The molecule is a member of the class of thiazolidinones. It is a thiazolidinone, a tertiary amino compound, an aromatic ether and an olefinic compound\u2026. ...\nCCC1=CN=C(C=C1)C\nsupplementary information about the molecules present in the real datasets is accidentally incorporated, thereby maintaining the integrity and independence of the training process. With ChatGPT API, we generate textual descriptions that encompass key aspects such as properties and structural features for each unannotated molecule. To improve the quality of generated descriptions, we implement a retrievalbased prompt paradigm that comprises two main stages as follows: Molecule Retrieval and Few-Shot Prompting.\nMolecule Retrieval In-context learning (Brown et al. 2020) is one of the emergent abilities of LLMs, and the instances used in the prompts given to the LLMs play an important role in the generation quality. As molecules with similar structures often display corresponding characteristics (Wang et al. 2016), we retrieve the descriptions of annotated molecules that resemble the unlabeled molecule, us-\ning them as the few-shot instance during prompting. Specifically, we collect 37,898 annotated molecules with captions from PubChem(Kim et al. 2023), then retrieve the molecules with top-k Tanimoto similarity (Tanimoto 1958), a standard measure in cheminformatics. To prevent information leakage during testing, we exclude the molecules that are contained in the real data test set (Edwards, Zhai, and Ji 2021; Zeng et al. 2022). This process enables the models to learn from the information embedded within the descriptions of molecules that possess similar properties, ensuring a more tailored and accurate representation. Figure 3 shows the estimate of the data quality, indicating that the few-shot prompting approach (in blue) yields higher-quality data, more closely resembling real data than without.\nFew-Shot Prompting Upon retrieving the top-k results for each unlabeled molecule from our local database, we select one example using a weighted distribution, where molecules with higher similarity have a greater chance of being chosen. This selected example is then incorporated into the final prompt. We opt for one-shot prompting to minimize generation costs, as expenses increase linearly with the number of instances included in few-shot prompts. This weighted selection method prevents repetitive selection of the same molecule as the few-shot example, thereby improving the diversity during generation while maintaining the similarity between the molecule to be annotated and the fewshot example. As shown in Figure 2, the complete prompt comprises role definition, task description, few-shot example, and output control. The role definition and task description give LLMs the general context and enable its learned knowledge, while the few-shot example acts like a supplementary material for the LLMs to refer to. Then, with the output control for format clarification, the LLMs should be able to generate the desired description."
        },
        {
            "heading": "3.3 Approaches to Utilize Artificially Real Data",
            "text": "The ways to utilize the pseudo data decide how the model will perform on real data. We propose and explore two primary strategies to optimize the use of pseudo data.\nModel\nTraditional Training\nReal data\nPseudo data\nShuffled\nModel\nTrainin\u2460\u2461\u2462\nPseudo Data as Data Augmentation Data augmentation strategy can be roughly categorized into two kinds, modification of existing dat and generation of pseudo data. The former takes an existing data instance and makes certain alterations to it without changing its inherent me ning or label, such as rotation, flipping, and cropping for images (Krizhevsky, Sutskever, and Hinton 2012), or synonym replacement for text (Wang and Yang 2015; Wei and Zou 2019; Miao et al. 2020). This method is more about adding variability and noise to existing data instances than generating completely new ones. The latter, on the other hand, involves creating new data instances that did not exist in the original dataset based on the characteristics and distribution of the original data, which is an efficient alternative when real data is scarce or when creating new real data is costly or unfeasible. Existing applications include back translation for text (Sennrich, Haddow, and Birch 2016), and GANs for images (Goodfellow et al. 2014).\nInspired by the latter techniques, we explore the use of pseudo data as data augmentation. As shown in Figure 4, we keep the original data in the training set and augment them with pseudo data during fine-tuning. Using the same method as described in Figure 3, we assess the distribution of the real training set and the sample the augmented pseudo data based on the same distribution, ensuring consistency in the overall dataset distribution before and after data augmentation. We hope that this data augmentation approach using pseudo data will expose the model to a broader range of data patterns and scenarios, thus enhancing its ability to recognize complex patterns and generalize its learning to unseen data.\nPseudo Data as Domain Adaptation Models pre-trained on general domain might perform less ideally when it is applied to specific domains for which they were not explicitly trained (Malte and Ratadiya 2019). In our case, the SMILES appears as an unfamiliar symbol to such models, making the direct fine-tuning approach less efficient. To bridge this gap, we use pseudo data as a second pretraining stage for domain adaptation. As shown in Figure 4, we train the model using pseudo data for two concurrent cross-modal translation tasks: molecular captioning and text-based de novo molecule generation. Using a direct and bidirectional seq2seq approach, this stage is intended to em-\npower the model to not only recognize the SMILES representation but also to grasp the relationship between natural language and SMILES. Given that our primary focus at this stage is not on data authenticity, pseudo data emerges as a preferable choice, particularly because it provides a large number of parallel data pairs for supervised seq2seq training compared to real datasets. We then further fine-tune it on real data to refine and enhance the model\u2019s understanding of SMILES for further authenticity \u2013 a critical aspect for applications like drug discovery."
        },
        {
            "heading": "4 Experiments",
            "text": "To validate the effectiveness of using pseudo data, we conduct comprehensive experiments comparing our proposed approaches with existing methods. We further conduct experiments to demonstrate how the balance between real data and pseudo data could affect model performance. All the experiments are conducted on both molecular captioning and molecule generation. The implementation details are listed in Appendix C."
        },
        {
            "heading": "4.1 Settings",
            "text": "Datasets Currently, only a few datasets with parallel molecule-description pairs exist, including ChEBI-20 (Edwards, Zhai, and Ji 2021) and PCdes (Zeng et al. 2022), both constructed using data from PubChem (Kim et al. 2023). To enhance evaluation comprehensiveness, we assemble a new dataset called DrugBank-23, based on DrugBank (Wishart et al. 2018). We experiment on all three datasets (ChEBI-20, PCdes, and DrugBank-23). The detailed information about these datasets is listed in Table 1.\nModels We evaluate the following methods: \u2022 T5 (Raffel et al. 2020). T5 directly fine-tuned on down-\nstream datasets. \u2022 MolT5 (Edwards et al. 2022). T5 pre-trained with MLM\nusing SMILES and molecule descriptions respectively, then fine-tuned on downstream datasets.\n\u2022 ChatGPT (Li et al. 2023). GPT-3.5-Turbo using few-shot prompting strategy. We cite the results from the original paper on ChEBI-20, then apply the same strategy to test on the other datasets.\n\u2022 MolXPT (Liu et al. 2023). GPT-2 pre-trained with CLM using abstracts of biomedical literature where molecules are replaced with the corresponding SMILES, then finetuned on downstream datasets. As the model is currently unavailable, we cite their results on ChEBI-20.\n\u2022 Text&Chem T5 (Christofidellis et al. 2023). T5 pretrained using multi-task learning, then fine-tuned on downstream datasets.\n\u2022 Aug-T5 (ours). T5 fine-tuned on datasets augmented with pseudo data from PseudoMD-1M, sampled from 1k to 512k, doubling at each step. We report the optimal performances for each dataset. See Appendix D for details.\n\u2022 Ada-T5 (ours). T5 pre-trained using moleculedescription pairs from PseudoMD-1M as domain adaptation, then fine-tuned on downstream datasets.\nAs shown in Table 2, both our proposed methods utilize the smallest model scale, pre-training data, and steps, while Aug-T5 requires no additional pre-training. We first test our methods on T5small (Aug-T5/Ada-T5) and then apply them to T5base (Aug-T5base/Ada-T5base).\nMetrics Following existing studies (Edwards et al. 2022; Liu et al. 2023; Christofidellis et al. 2023), we evaluate the results for molecular captioning with BLEU-2, BLEU4 (Papineni et al. 2002), ROUGE-1, ROUGE-2, ROUGE-L (Lin 2004) and METEOR (Banerjee and Lavie 2005), and BLEU-4 (Papineni et al. 2002), Accuracy (Edwards et al. 2022), Validity (Polykovskiy et al. 2020), Levenshtein distance (Miller, Vandome, and McBrewster 2009), MACCSFTS (Durant et al. 2002), RDK-FTS (Schneider, Sayle, and Landrum 2015), Morgan-FTS (Rogers and Hahn 2010) and FCD (Preuer et al. 2018) for text-based de novo molecule\ngeneration. Selected metrics are presented in Tables 3, 4 and Figures 5 and 6, with comprehensive results in Appendix D."
        },
        {
            "heading": "4.2 Comparison with Existing Methods",
            "text": "Results on Molecular Captioning Table 3 shows the results of different models for molecule captioning. Ada-T5 outperforms all previous methods and achieves the stateof-the-art on all three datasets across all the metrics. Compared to the previous state-of-the-art, Ada-T5 uses less than 3% of the pre-training data and only a third of the model parameters, yet requires fewer training steps, demonstrating the effectiveness and computational efficiency of highquality pseudo data. On the other hand, Aug-T5 outperforms T5, MolT5, ChatGPT and has comparable performance with MolXPT and Text&Chem T5, using 9%-30% of the parameters and requires no pre-training. This highlights the benefit from the enhanced diversity of descriptions by incorporating pseudo data into the training set. Meanwhile, AdaT5base makes an extra but relatively little progress compared to Ada-T5, indicating that although using pseudo data for domain adaptation could also benefit from the expansion of model size like most methods, the exploitation of pseudo data only demands a relatively small number of parameters. In contrast, Aug-T5base mirrors the results of its smaller version, indicating that for data augmentation, simply increasing the model scale may not offer substantial benefits. One thing to notice is that despite the data used to train the model is generated by ChatGPT API, both our trained models can still beat ChatGPT across different metrics. This indicates that although ChatGPT can accomplish the task to a certain extent, the data it generated can still help the models achieve a more seamless transition through pre-training from general domain to this domain.\nResults on Text-Based Molecule Generation Table 4 presents the results of different models for molecule generation. Ada-T5 achieves the best performance in all three datasets across almost all metrics, demonstrating its capability to generate high-quality SMILES. The only exception is that the MolXPT slightly surpasses Ada-T5 by 0.009 in ChEBI-20 dataset on the validity metric, which is calculated using RDkit to simply check whether the string can be successfully converted to a molecule object without errors and whether the molecule represents a realistic and feasible chemical structure, without any comparison to the targeted SMILES and the input descriptions. Despite this one slight superiority, MolXPT performs significantly worse than AdaT5 on other metrics, meaning that although it can generate slightly more valid SMILES, it does not take into account the designated instructions, ergo making it one step away from real-world application.\nOn the other hand, Aug-T5 surpasses some existing methods in certain datasets on specific metrics. However, its consistency falls short compared to Ada-T5. This variability may be traced back to the construction of moleculedescription data pairs in pseudo data: the LLMS use the real SMILES are used as the input, leaving only the description part of the pseudo data genuinely \u201cpseudo\u201d. This means that when training Aug-T5 on molecule captioning, it gets the\nauthentic SMILES; but when training on molecule generation, it gets the pseudo description. Consequently, the gap between the input training data leads to the gap between the model performance on different tasks. Furthermore, compared with the results for molecular captioning, the base counterparts of both methods for molecule generation exhibit pronounced enhancements, which could also attributed to the gap between the input data, as using the \u201dpseudo\u201d part as the input for molecule generation might offer more space for improvements, especially for larger-scale models that can better tolerate the \u201cpseudo\u201d data nuances.\nThe difference between Aug-T5 and Ada-T5 also indicates the importance of data authenticity and the difference between real data and pseudo data: as Ada-T5 is later finetuned with 100% real data (in comparison with Aug-T5, which is fine-tuned with the mix of real data and pseudo data), its misunderstandings about SMILES during domain adaptation through pseudo data are corrected and therefore has a better overall performance. This further stresses that using pseudo data for direct application may not be the optimal way to exploit its potential."
        },
        {
            "heading": "4.3 Effect of the amount of pseudo data",
            "text": "In order to further demonstrate how the amount of pseudo data could affect model performance, we experiment on ChEBI-20, the largest and most widely used dataset, with varying numbers of pseudo data samples N from 1k to 512k.\nResults on Molecular Captioning Figure 5 shows the results of Ada-T5 and Aug-T5 for molecular captioning with different amounts of pseudo data. Both Ada-T5 and AugT5 exhibit significant improvements when a modest amount of pseudo data is incorporated into their training. With just 1k pseudo data, both methods can surpass T5large and ChatGPT and achieve a comparative performance to MolT5large and MolXPT. This phenomena is often seen in other data augmentation strategies (Wei and Zou 2019; Sennrich, Haddow, and Birch 2016), and can be attributed to the moderate noise introduced by the pseudo data, which in turn bolsters model generalization. As the amount of pseudo data increases, Ada-T5 and Aug-T5 exhibit different tendencies. The performance of Aug-T5 begins to decline when the number of pseudo data samples reaches 4k, and sees a sharp drop when it exceeds 32k. This is possibly due to the im-\nbalance between real data and pseudo data: As the model becomes increasingly exposed to unreal patterns from the pseudo data, it might shift its attention away from genuine patterns. Consequently, the real patterns are overlooked by the model that focuses on the artificial ones. In contrast, Ada-T5 thrives with the increasing amount of pseudo data, evidenced by the growth of overall metrics. One possible explanation is that Ada-T5 only uses pseudo data for pretraining, with follow-up fine-tuning using real data. Thus, the increase of pseudo data does not twist its grasp of genuine patterns, but instead, further amplifies the proficiency of the model during subsequent training.\nResults on Text-Based Molecule Generation Figure 6 shows the results of Ada-T5 and Aug-T5 for molecule generation with different amounts of pseudo data. Ada-T5 shows the same superiority and trend as it does in molecular captioning with more pseudo data incorporated, while Aug-T5 displays a non-linear trend, with the optimal choice of the amount of pseudo data significantly larger than when applying Aug-T5 for molecular captioning. The reason might lie in the dual nature of pseudo data: it introduces both linguistic patterns and noise. Initially, a little bit of pseudo data bolsters model generalization by acting as a regularizer. But as more is added, an overbundance of noise degrades the results. However, once a critical mass of pseudo data is reached, the model starts to recognize more subtle and broader linguistic patterns amidst the noise, which helps in generating more accurate SMILES strings, leading to the observed spike in performance. After this peak, the overwhelming volume of pseudo data might reintroduce the dominance of noise, causing a decrease in performance.\nThe distinct behavior of Aug-T5 in molecular caption-\ning versus molecule generation highlights their inherent differences. Molecular captioning, being more flexible, can buffer linguistic variations, downplaying minor gains from pseudo data and instead more affected by noise. In contrast, molecule generation requires recognizing specific linguistic cues from descriptions that lead to exact structural changes in the SMILES output, making it more receptive to the subtle intricacies but can also discern and benefit from the subtle patterns present in pseudo data. Overall, these results indicate that the impact of pseudo data varies, depending on its inherent nature and the specific task at hand."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we introduce a novel approach that enhances low-resource cross-modal molecule discovery by leveraging artificially-real data generated by LLMs. By incorporating a retrieval-based few-shot prompting strategy, we are able to produce high-quality pseudo molecule-description pairs. To mitigate the scarcity of data, we released two datasets: PseudoMD-1M, the first artificially-real dataset for molecule description, and DrugBank-23, a real moleculedescription dataset constructed from a novel source. We propose to use pseudo data for domain adaptation and for data augmentation to explore its optimal utilization. Experiments across different datasets show that the former can best exploit the potential of pseudo data, achieving better performance with less parameters and training data. Furthermore, as the performance of the model continues to benefit from the increasing amount of pseudo data, our approach shows the great potential of pseudo data, thereby providing a novel and promising approach for addressing low-resource challenge in cross-modal molecule discovery."
        },
        {
            "heading": "6 Acknowledgements",
            "text": "We express our gratitude to the anonymous reviewers for their valuable feedback. This research was supported by the National Key R&D Program of China (2021ZD0113302), the National Natural Science Foundation of China Youth Fund (62206079), and the Heilongjiang Provincial Natural Science Foundation of China (YQ2022F006). We also appreciate Du Xiaoman Technology\u2019s support for our research."
        },
        {
            "heading": "A More information about datasets",
            "text": "A.1 PseudoMD-1M The PseudoMD-1M dataset comprises 1,020,139 moleculedescription pairs. Every molecule is represented using its Canonical SMILES notation, sourced from PubChem via the PUG View API. On average, each description within PseudoMD-1M contains 5.11 sentences, 106.47 words, and 165.07 tokens, as shown in Figure 7a and Figure 7b. We provide five examples in Table 5. While we have employed simple rule-based post-processing, we cannot guarantee its complete accuracy and ensure that there remains free from errors, biases, or other inaccuracies. Consequently, this dataset should not be employed in real-world applications.\nA.2 DrugBank-23 The DrugBank-23 dataset comprises 17,109 moleculedescription pairs, sourced from the DrugBank dataset. In line with prior studies, we represent each molecule using its Canonical SMILES notation and descriptions with fewer than 20 words are dropped. On average, each description contains 4.12 sentences, 64.86 words, and 119.45 tokens. Detailed distributions of word and token counts can be seen in Figure 8a and 8b, respectively. Representative examples are provided in Table 6."
        },
        {
            "heading": "B Prompt",
            "text": "As shown in Figure 9, we give a specific example of prompt we use during pseudo data generation.\nC Implementation We conducted our experiments on Ubuntu 22.04 using A100-PCIE-40GB and A100-SXM4-80GB with CUDA 11.7.1. Our primary dependencies are Python 3.11.4, PyTorch 2.0.1, Transformers 4.31.0, and Numpy 1.24.3. We initialize our Ada-T5 and Aug-T5 using public T5.1.1 checkpoints.\nDuring pre-training, we used a learning rate of 1e-3, batch size of 128, random seed of 42, 1000 warm-up steps, 100,000 total training steps, and weight decay of 0.1.\nFor fine-tuning, we adopted the configuration in the paper \u201cTranslation between Molecules and Natural Language\u201d, with a learning rate of 1e-3, 50,000 fine-tuning steps, weight decay of 0.1, batch size of 32, random seed of 42, and 1000 warm-up steps.\nFor evaluation, we use a greedy search with a maximum generation length of 512 during generation. The evaluation metrics script is derived from the paper \u201cTranslation between Molecules and Natural Language\u201d.\nFor other hyperparameters, we relied on the default settings of the T5ForConditionalGeneration class in Huggingface."
        },
        {
            "heading": "D Complete results of all experiments",
            "text": "D.1 Comparison to existing methods We report the complete results compared to existing methods for both molecular captioning task and molecule generation task. The results of molecular captioning task are shown in Table 7. The results of molecule generation task are shown in Table 8. Based on existing studies, we select comprehensive metrics for two tasks respectively.\nFor the molecular captioning task, we select the following metrics : BLEU-2, BLEU-4, ROUGE-1, ROUGE2, ROUGE-L and METEOR. BLEU, ROUGE, and METEOR are key metrics for evaluating machine-generated text. BLEU and ROUGE scores primarily focus on n-gram precision, gauging how closely the generated structures align with the reference structures. Specifically, they identify the overlap in terms of sequential words or subwords of varying lengths. On the other hand, METEOR captures both exact matches and paraphrases, offering a more comprehensive view of how closely the generated content mirrors the reference. This holistic approach allows METEOR to detect nuances often overlooked by strictly precision-focused metrics. All the metrics are better when higher (denoted by \u2191).\nFor the text-based de novo molecule generation, we select the following metrics: BLEU-4, Accuracy , Validity, Levenshtein distance, MACCS-FTS, RDK-FTS, Morgan-FTS and FCD. The SMILES notation represents molecules as textual sequences, allowing the use of BLEU for evaluation. Accuracy is based on exact SMILES string matches, while the\nLevenshtein distance reveals structural differences by measuring the edits required to transform one string into another. Validity is determined by successfully converting the SMILES string to an RDkit object without errors. Regarding fingerprint scores, we use three methods: MACCS, RDK, and Morgan. These are evaluated using fingerprint Tanimoto similarity (FTS). To compare the fingerprints of two molecules, Tanimoto similarity is employed, with the average similarity reported across the evaluation dataset. Among these metrics, lower values are preferable for Levenshtein distance and FCD (denoted by \u2193), while higher values are better for the rest (denoted by \u2191).\nD.2 Effect of the amount of pseudo data\nHere we report the complete results of Ada-T5 and Aug-T5 across data sizes ranging from 1k to 512k, using ChEBI-20, the largest and most widely used dataset. The data are organized into four tables: Tables 10 and 9 detail Ada-T5\u2019s performance on molecular captioning and molecule generation, while Tables 11 and 12 highlight Aug-T5\u2019s performance for these tasks.\nIn the main content, we report the optimal performances of Aug-T5 and Aug-T5base for each dataset, both of which use the same configuration. For detailed data incorporation, in the molecular captioning task, we used 2k extra pseudo data for ChEBI-20 and PCdes, and 4k for DrugBank-23. For molecule generation, we used 32k for ChEBI-20, 2k for PCdes, and 1k for DrugBank-23."
        },
        {
            "heading": "E Performance Analysis: Real vs. Pseudo Data Fine-tuning",
            "text": "To further demonstrate the necessity of our exploration of how to effectively utilize pseudo data, we conduct a comparative experiment on ChEBI-20 for molecular captioning, to show that the direct application of pseudo data as a substitution of real data hardly works for real-world scenarios and cannot exploit the full potential of pseudo data compared to our proposed methods.\nFigure 10 displays the trends of BLEU-4 and Rouge-2 scores. As training data volume grows, performance improves significantly with real data, whereas performance with pseudo data remains stable at a relatively low level. Table 13 and 14 provide detailed results for models trained on real and pseudo data, respectively, and all metrics are con-\nsistent with the trends observed with BLEU-4 and Rouge2. These divergent performance trends highlight the value of data authenticity and the gap between real and pseudo data. While our pseudo data cannot yet match and replace real data, incorporating it using our methods substantially enhances model performance, underscoring the efficacy of our techniques for exploiting the potential of pseudo data.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n1k 2k 4k 8k 16 k 24 k 26 k 32 k 64 k 12 8k 25 6k 51 2k\nReal Pseudo\n(a) BLEU-4.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n1k 2k 4k 8k 16 k 24 k 26 k 32 k 64 k 12 8k 25 6k 51 2k\nReal Pseudo\n(b) Rouge-2.\nFigure 10: Trends in metrics as pseudo and real data volumes increase."
        }
    ],
    "title": "From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery",
    "year": 2023
}