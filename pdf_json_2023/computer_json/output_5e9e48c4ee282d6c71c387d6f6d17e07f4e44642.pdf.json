{
    "abstractText": "Detecting deception by human behaviors is vital in many fields such as custom security and multimedia anti-fraud. Recently, audio-visual deception detection attracts more attention due to its better performance than using only a single modality. However, in real-world multi-modal settings, the integrity of data can be an issue (e.g., sometimes only partial modalities are available). The missing modality might lead to a decrease in performance, but the model still learns the features of the missed modality. In this paper, to further improve the performance and overcome the missing modality problem, we propose a novel Transformer-based framework with an Audio-Visual Adapter (AVA) to fuse temporal features across two modalities efficiently. Extensive experiments conducted on two benchmark datasets demonstrate that the proposed method can achieve superior performance compared with other multi-modal fusion methods under flexible-modal (multiple and missing modalities) settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhaoxu Li"
        },
        {
            "affiliations": [],
            "name": "Zitong Yu"
        },
        {
            "affiliations": [],
            "name": "Nithish Muthuchamy Selvaraj"
        },
        {
            "affiliations": [],
            "name": "Xiaobao Guo"
        },
        {
            "affiliations": [],
            "name": "Bingquan Shen"
        },
        {
            "affiliations": [],
            "name": "Adams Wai-Kin Kong"
        },
        {
            "affiliations": [],
            "name": "Alex Kot"
        }
    ],
    "id": "SP:db5765f9cdb60d9c66d4ab07a9995253d90156b6",
    "references": [
        {
            "authors": [
                "Bella M DePaulo",
                "James J Lindsay",
                "Brian E Malone",
                "Laura Muhlenbruck",
                "Kelly Charlton",
                "Harris Cooper"
            ],
            "title": "Cues to deception",
            "venue": "Psychological bulletin, vol. 129, no. 1, pp. 74, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "Julia Bell Hirschberg",
                "Stefan Benus",
                "Jason M Brenier",
                "Frank Enos",
                "Sarah Friedman",
                "Sarah Gilman",
                "Cynthia Girand",
                "Martin Graciarena",
                "Andreas Kathol",
                "Laura Michaelis"
            ],
            "title": "Distinguishing deceptive from nondeceptive speech",
            "venue": "2005.",
            "year": 2005
        },
        {
            "authors": [
                "Timothy R Levine",
                "Steven A McCornack"
            ],
            "title": "Theorizing about deception",
            "venue": "Journal of Language and Social Psychology, vol. 33, no. 4, pp. 431\u2013440, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Aldert Vrij",
                "P\u00e4r Anders Granhag"
            ],
            "title": "Eliciting cues to deception and truth: What matters are the questions asked",
            "venue": "Journal of Applied Research in Memory and Cognition, vol. 1, no. 2, pp. 110\u2013117, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Gemma Warren",
                "Elizabeth Schertler",
                "Peter Bull"
            ],
            "title": "Detecting deception from emotional and unemotional cues",
            "venue": "Journal of Nonverbal Behavior, vol. 33, no. 1, pp. 59\u201369, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Gogate",
                "Ahsan Adeel",
                "Amir Hussain"
            ],
            "title": "Deep learning driven multimodal fusion for automated deception detection",
            "venue": "2017 IEEE symposium series on computational intelligence (SSCI). IEEE, 2017, pp. 1\u20136.",
            "year": 2017
        },
        {
            "authors": [
                "Hamid Karimi",
                "Jiliang Tang",
                "Yanen Li"
            ],
            "title": "Toward end-to-end deception detection in videos",
            "venue": "2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1278\u20131283.",
            "year": 2018
        },
        {
            "authors": [
                "Zhe Wu",
                "Bharat Singh",
                "Larry Davis",
                "V Subrahmanian"
            ],
            "title": "Deception detection in videos",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, 2018, vol. 32.",
            "year": 2018
        },
        {
            "authors": [
                "Leena Mathur",
                "Maja J Matari\u0107"
            ],
            "title": "Introducing representations of facial affect in automated multimodal deception detection",
            "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction, 2020, pp. 305\u2013314.",
            "year": 2020
        },
        {
            "authors": [
                "Danilo Avola",
                "Luigi Cinque",
                "Gian Luca Foresti",
                "Daniele Pannone"
            ],
            "title": "Automatic deception detection in rgb videos using facial action units",
            "venue": "Proceedings of the 13th International Conference on Distributed Smart Cameras, 2019, pp. 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "Mingyu Ding",
                "An Zhao",
                "Zhiwu Lu",
                "Tao Xiang",
                "Ji- Rong Wen"
            ],
            "title": "Face-focused cross-stream network for deception detection in videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7802\u20137811.",
            "year": 2019
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameterefficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning. PMLR, 2019, pp. 2790\u2013 2799.",
            "year": 2019
        },
        {
            "authors": [
                "Shibo Jie",
                "Zhi-Hong Deng"
            ],
            "title": "Convolutional bypasses are better vision transformer adapters",
            "venue": "arXiv preprint arXiv:2207.07039, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Ver\u00f3nica P\u00e9rez-Rosas",
                "Mohamed Abouelenien",
                "Rada Mihalcea",
                "Mihai Burzo"
            ],
            "title": "Deception detection using real-life trial data",
            "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, 2015, pp. 59\u201366.",
            "year": 2015
        },
        {
            "authors": [
                "Viresh Gupta",
                "Mohit Agarwal",
                "Manik Arora",
                "Tanmoy Chakraborty",
                "Richa Singh",
                "Mayank Vatsa"
            ],
            "title": "Bag-oflies: A multimodal dataset for deception detection",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0\u2013 0.",
            "year": 2019
        },
        {
            "authors": [
                "Kaipeng Zhang",
                "Zhanpeng Zhang",
                "Zhifeng Li",
                "Yu Qiao"
            ],
            "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
            "venue": "IEEE signal processing letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-andexcitation networks",
            "venue": "CVPR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Anjith George",
                "S\u00e9bastien Marcel"
            ],
            "title": "Cross modal focal loss for rgbd face anti-spoofing",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7882\u20137891.",
            "year": 2021
        },
        {
            "authors": [
                "Menglin Jia",
                "Luming Tang",
                "Bor-Chun Chen",
                "Claire Cardie",
                "Serge Belongie",
                "Bharath Hariharan",
                "Ser- Nam Lim"
            ],
            "title": "Visual prompt tuning",
            "venue": "arXiv preprint arXiv:2203.12119, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Deception detection, multi-modal\n1. INTRODUCTION\nWith the rapid development of the internet, security has become an important issue that can reduce millions of money being lost. An effective deception detection method can play a vital role and is widely used to protect people in the area of border security, anti-fraud, business negotiations and etc.\nEarly works in deception detection research were based on the psychological premises of deception [1, 2, 3, 4, 5], such as physiological, visual, vocal, verbal, and behavioral cues. Deep learning pre-trained models have achieved comparable or even better performance than humans in many complex tasks so that deep convolutional neural networks (CNN) [6] and vision transformer (ViT) [7] become mainstream in many computer vision tasks. In the deception detection area, existing models can be divided into single-modal and multi-modal models. For multi-modal deception detection, Gogate et al. [8] presented a fusion model combining audio cues along with visual and textual cues for the first time. Karimi et al. [9] proposed a robust method to capture rich information into a coherent end-to-end model. Wu et al. [10] studied the importance of different modalities like vision (low-level video\n* Corresponding author\nfeatures), audio (Mel-frequency Cepstral Coefficients), and text for this task. Mathur et al. [11] presented a novel method to analyze the discriminative power of facial affect for this task, and interpretable features from visual, vocal, and verbal modalities.\nMost existing works [12, 13, 8] of multi-modal deception detection focus on the ideal situations in all modalities available at the deployment stage. However, in real-world practical scenarios, sometimes only partial modalities are available. In this case, the well-trained multi-modal models might perform even worse than the single-modal models. In this paper, we establish a flexible-modal deception detection benchmark and propose a novel fusion method for audio-visual deception detection. Our contributions include:\n\u2022 We establish the first flexible-modal deception detection benchmark with both intra- and cross-dataset testings under both full multi-modal scenario (i.e., Vision+Audio), and two missing-modal scenarios (Missing Vision, Missing Audio). \u2022 We propose a novel Transformer-based framework with an Audio-Visual Adapter (AVA) module to fuse features across modalities efficiently. AVA significantly improves the performance of Transformer baselines in both flexible-modal intra- and cross-testings.\n2. METHODOLOGY\nTo take advantage of the powerful ImageNet pre-trained ViT and ResNet models, we propose a novel multi-modal deception detection framework to efficiently fine-tune the pre-trained models with audio-visual temporal adapters while fixing all pre-trained ViT weights. In this section, we will first introduce the whole model structure in Sec. 2.1. It consists of two unshared ResNet18 for visual and audio feature extraction, and then both visual and audio features are projected into temporal tokens and passed forward transformer encoders with global audio-visual attention. Besides, we will introduce the Audio-Visual Adapters in Sec. 2.2, which can plug in transformer encoders and efficiently aggregate the audio-visual cues among their local temporal neighborhoods."
        },
        {
            "heading": "2.1. Multimoal Deception Detection Framework",
            "text": "As illustrated in the left part of Fig. 1, the XA and XiV (i = 1,..., T ) respectively denote audio and visual inputs, where T\nar X\niv :2\n30 2.\n05 72\n7v 1\n[ cs\n.C V\n] 1\n1 Fe\nb 20\n23\nis the frame numbers of visual input. After the feature extractor, the outputs can be formulated as\nF iV = RNV(X i V), FA = RNA(XA), (1)\nwhere RNV and RNA are ImageNet pre-trained Visual ResNet model and Audio ResNet model of Fig.1. The T - frame visual features F iV are concatenated in the time domain to form FV . Then two Conv Projections CAproj and C V proj via convolutional projection with different sizes of the temporal kernel to align the visual and audio modalities along the time dimension. Specifically, the concatenated visual features FV and audio features FA are passed over CVproj and C A proj to generate the visual tokens TV and audio tokens TA, respectively. All visual and audio tokens and a learnable class token TC are concatenated to extract the cross-modal feature and added with position embeddings. Then N transformer blocks Ejtrans(j = 1, ..., N ) are cascaded for global audio-visual feature interaction. Each transformer block is formed by two Layer Norm (LN) layers, a Multi-Head Self-Attention (MHSA) module, Audio-Visual Adapters (AVA), and a MultiLayer Perception (MLP) module. The output [TFC , T F V , T F A ] of the transformer can be formalized as follows TV = C V proj(Concat(F i V)), (i = 1, ..., T ),\nTA = C A proj(FA),\n[TFC , T F V , T F A ] = Etrans(Concat(TC,TV,TA)).\n(2)\nFinally, TFC is sent to a classification head Ehead for binary truth/lie prediction."
        },
        {
            "heading": "2.2. Audio-Visual Adapter",
            "text": "To alleviate the overfitting issues of the transformer finetuning, we propose to train the Audio-Visual Adapter (AVA) to fuse the visual and audio tokens efficiently while fixing all the pre-trained parameters of transformer encoders Etrans. These adapters are placed in parallel with MHSA and MLP\nmodules which are shown in the right part of Fig.1. Unlike channel-wise NLP adapter [14] and spatial-wise Convadapter [15], our method is designed to capture the temporal multi-modal feature. The concatenated audio-visual tokens are first reshaped to align the visual and audio tokens which are at the same time. Then a 1D convolution layer with temporal kernel size k and padding k//2 is used to aggregate the local temporal multi-modal features. The output A(T ) of AVA can be formulated as TReALL = Reshape(GELU(LD(Concat(TC,TV,TA)))),\nTF = Conv(T Re ALL),\nA(TF ) = LU(GELU(Reshape(TF))),\n(3) where LD, LU , and Conv are linear down, linear up, and 1D temporal Conv layers in Fig.1, and TReALL and TF represent the reshaped temporally-aligned tokens and fused tokens, respectively. Specifically, the input tokens T j and the output EjT of each encoders with AVA can be formulated as:\nT j\u2032 = T j +MHSA(LN(Ti)) + A(Tj),\nEjT = T j \u2032 + LN(MLP(T j \u2032)) + A(T j \u2032).\n(4)\n3. EXPERIMENT"
        },
        {
            "heading": "3.1. Flexible-Modal Benchmark",
            "text": "Datasets. Two multi-modal deception detection datasets are used in the flexible-modal benchmark. RLtrial: Real-life trial [16] consists of 121 deceptive and truthful video clips with 27.7 seconds for deceptive and 28.3 seconds for truthful clips on average respectively, from real court trials. The data includes approximately 21 female and 35 unique male speakers ranging between 16 and 60 years. BOL: Bag of lies [17] consists of 325 annotated videos (163 truth and 162 lie) recorded by 35 subjects. It includes 4 types of data such as video, audio, EEG, and Eye gaze. In our experiments, we focus on audio-visual deception detection.\nProtocols and evaluation metrics. We evaluate models under flexible-modal protocols on both intra-dataset (5-fold for RLtrial and 3-fold for BOL) and cross-dataset (trained on RLtrial/BOL tested on BOL/RLtrial) testings. In terms of flexible-modal settings, we train the multi-modal models with both audio and video modalities, and evaluate them under Vision-based (V), Audio-based (A) single-modal, and Vision+Audio-based (V&A) scenarios. Compared with training unified multi-modal models (see \u2018Unified\u2019 in Tables 1, 2 and 3) with audio-visual modalities, we also consider singlemodal results (see \u2018Single\u2019 in Tables 1, 2 and 3) that train separate models with only audio or video data. We evaluate our method and other fusion methods with Accuracy (ACC), F1 score (F1), and Area Under Curve (AUC)."
        },
        {
            "heading": "3.2. Implementation Details",
            "text": "All the experiments are implemented with Pytorch on one NVIDIA RTX A5000 GPU. The batch size is 8, and we use the SGD optimizer with the learning rate (lr) of 1e-4, the weight delay of 5e-5, and the momentum of 0.9. We use StepLR as the learning rate scheduler with a step size of 20 and the gamma of 0.1. ResNet18 and the encoder of ViT are fine-tuned based on the ImageNet/ImageNet-21K pre-trained models with 25 epochs of training. The missing modalities experiments are simply blocked inputs as zeros in the testing phase of Protocols 1 and 2. Data pre-processing. For each video clip, we uniformly sampled T = 20 images then use MTCNN face detector [18] to crop the face areas. After the cropping process, all images are normalized and resized to 224 \u00d7 224. For audio data, we transfer the raw audio to the Mel spectrogram image with the size of 640 \u00d7 480 using torchaudio. Model details. We use pre-trained ResNet18 [6] as the backbone for the single-modal feature extraction. Specifically, when cascaded with transformer blocks, we tokenize the T - frame face features from temporal dimensions 512 to 768, and also tokenize the audio spectrogram features (512 \u00d7 T \u00d7 15 for dimension \u00d7 time \u00d7 frequency) into size 512 \u00d7 T using global convolution on frequency dimension. We then utilize\nthe first 8 transformer encoder layers (N=8) from ImageNet21K pre-trained ViT model for global temporal feature refinement. For the AVA, temporal kernel size k=5 is utilized. Fusion baselines. Besides fusion with AVA, we also compare with other fusion baselines: 1) Concat: Concatenate two features in the channel domain and then aggregate the multi-modal heterogeneous features with a lightweight fusion operator. 2) SE-Concat: Squeeze-and-excitation (SE) module [19] is utilized in each independent modality branch first. With the channel-wise self-calibration via the SE module, the refined features are then concatenated. 3) CMFS: Cross-modal focal loss [20] is used to modulate the loss contribution of each channel as a function of the confidence of individual channels. 4) Prompt: Visual Prompt Tuning method [21] introduces a small amount of task-specific learnable tokens while freezing the entire pre-trained transformer blocks during deception detection training."
        },
        {
            "heading": "3.3. Intra Testing",
            "text": "The experimental results of flexible-modal intra-dataset testing on RLtrial and BOL datasets are shown in Table 1 and Table 2, respectively. It is clear that 1) models with audio modality usually perform better than Visual modality; and 2) the separate trained \u2018ResNet+Transformer (A or V)\u2019 achieve worse performance than multi-modal models with two modalities (A&V), indicating the necessity of multi-modal cues for deception detection. Impact of fusion modules. As we can see from the \u2018Unified\u2019 block in Table 1 and Table 2, compared with the singlemodal models, most of the fusion methods can increase the performance. In contrast, we can find from the results \u2018ResNet+Transformer+AVA\u2019 that the proposed AVA improves the accuracy remarkably (with 61.20% and 74.62% for testing on BOL and testing on RL, respectively). Results of missing modalities. The missing-modal testings indicate that the multi-modal model can benefit from more modalities, but it might overfit on a better modal when missing modality. The results of two datasets tests of our method are not better than the fusion result when one modality is missed but still better than the single-modal model, indicat-\nTable 3: Results of cross-dataset testings between RLtrial and BOL.\nMethod Train Test Train on RL Test on BOL Train on BOL Test on RLACC(%) AUC(&) F1(%) ACC(%) AUC(&) F1(%)\nSingle ResNet A A 50.46 50.50 43.11 47.66 47.22 28.92V V 51.40 50.96 26.51 50.15 50.02 65.09\nResNet+Transformer A A 52.62 52.51 64.35 51.40 51.15 52.99V V 52.92 52.85 67.36 54.21 53.79 62.82\nUnified\nResNet+Transformer+Concat V&A V&A 50.76 50.63 66.80 53.27 53.05 54.87 V&A A 50.15 50.30 62.86 56.07 56.15 59.13 V&A V 54.77 54.78 66.94 54.21 53.79 25.71 ResNet+Transformer+SE-Concat V&A V&A 50.15 50.26 41.77 50.46 50.30 42.42 V&A A 50.15 50.31 12.5 51.40 51.22 39.53 V&A V 51.69 51.66 66.26 58.88 58.49 66.25\nResNet+Transformer+CMFL V&A V&A 50.46 50.58 66.80 52.34 51.90 64.56 V&A A 51.69 51.59 66.94 51.40 50.94 65.41 V&A V 51.38 51.53 66.80 53.27 52.87 64.56 ResNet+Transformer+Prompt V&A V&A 50.15 50.00 66.80 52.34 52.11 37.04 V&A A 50.15 50.00 66.80 53.27 53.07 40.48 V&A V 52.31 52.24 66.80 54.21 54.37 67.52 Ours (ResNet+Transformer+AVA) V&A V&A 58.77 58.73 63.19 59.81 59.59 53.03 V&A A 55.08 55.07 63.91 58.87 58.68 58.91 V&A V 55.38 55.29 67.51 60.75 60.67 66.25\n(b) (c)(a)\nFig. 2: Ablation of the (a) number of encoders without AVA; (b) temporal kernel size of AVA; (c) position of AVA.\ning the model can actually learn the complementary features across modalities and also alleviates the overfitting problem."
        },
        {
            "heading": "3.4. Cross Testing",
            "text": "Table 3 shows the results of flexible-modal cross-dataset testings between RLtrial and BOL. Due to the domain shifts (e.g., environments and stakes) between two datasets, the performance of both separate and unified models are unsatisfactory. Impact of fusion modules. The results shows that other fusion methods cannot bring obvious benefits than the single modal model on cross-dataset testings. Compared with other fusion methods, the proposed AVA is suitable with multimodal architectures both on two cross-dataset tests and improves the testing accuracy results significantly. Impact of missing modalities. Similar to intra-dateset testings, the fusion methods cannot achieve a satisfactory accuracy under missing modality scenarios due to the lack of partial modality cues and overfitting on partial/full modalities. In contrast, the proposed method can still have a reasonable accuracy under two missing-modal scenarios."
        },
        {
            "heading": "3.5. Ablation Study",
            "text": "We also conduct elaborate ablation studies of AVA on the intra-dataset testings of RLtrail. Number of encoders without AVA. As shown in Fig.3(a), our model performs the best when all eight encoders assemble with AVA. The performance drops when more encoder layers are without AVA. Fewer encoder layers without AVA might lead to modality-aware overfitting (e.g., poor performance when missing audio modality), although the fusion\nperformance is getting better. Kernel size of AVA. The effect of 1D convolutional kernel size k in AVA is presented in Fig.3(b). AVA with k=5 achieves the best performance on both full and missing modalities settings. The performance dropped when using larger or smaller kernel sizes due to the overfitting or limited local temporal contexts, respectively. Note that AVA outperforms the NLP adapter [14] (i.e., k=0) by a large margin. Position of AVA. The results of AVA positions inside the transformer encoders are illustrated in Fig.3(c). It is clear that the model works best when assembling AVA parallelly on both MHSA and FFN layers. The fusion performance of MHSA was better than those placed in FFN layers. For missing modalities, the performance of MHSA+FFN is close to MHSA but obviously better than those placed in FFN.\n4. CONCLUSION\nIn this paper, we propose a novel Transformer-based framework with Audio-Visual Adapter modules for multi-modal deception detection. We also provide sufficient baselines using other deep learning methods and feature fusion strategies for flexible-modal deception detection. In the future, we will investigate the scenarios when only partial modality data are available at the training stage. Acknowledgment This work was carried out at the RapidRich Object Search (ROSE) Lab, Nanyang Technological University, Singapore. The research is supported by the DSO National Laboratories, under the project agreement No. DSOCL21238.\n5. REFERENCES\n[1] Bella M DePaulo, James J Lindsay, Brian E Malone, Laura Muhlenbruck, Kelly Charlton, and Harris Cooper, \u201cCues to deception.,\u201d Psychological bulletin, vol. 129, no. 1, pp. 74, 2003.\n[2] Julia Bell Hirschberg, Stefan Benus, Jason M Brenier, Frank Enos, Sarah Friedman, Sarah Gilman, Cynthia Girand, Martin Graciarena, Andreas Kathol, Laura Michaelis, et al., \u201cDistinguishing deceptive from nondeceptive speech,\u201d 2005.\n[3] Timothy R Levine and Steven A McCornack, \u201cTheorizing about deception,\u201d Journal of Language and Social Psychology, vol. 33, no. 4, pp. 431\u2013440, 2014.\n[4] Aldert Vrij and Pa\u0308r Anders Granhag, \u201cEliciting cues to deception and truth: What matters are the questions asked,\u201d Journal of Applied Research in Memory and Cognition, vol. 1, no. 2, pp. 110\u2013117, 2012.\n[5] Gemma Warren, Elizabeth Schertler, and Peter Bull, \u201cDetecting deception from emotional and unemotional cues,\u201d Journal of Nonverbal Behavior, vol. 33, no. 1, pp. 59\u201369, 2009.\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.\n[8] Mandar Gogate, Ahsan Adeel, and Amir Hussain, \u201cDeep learning driven multimodal fusion for automated deception detection,\u201d in 2017 IEEE symposium series on computational intelligence (SSCI). IEEE, 2017, pp. 1\u20136.\n[9] Hamid Karimi, Jiliang Tang, and Yanen Li, \u201cToward end-to-end deception detection in videos,\u201d in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1278\u20131283.\n[10] Zhe Wu, Bharat Singh, Larry Davis, and V Subrahmanian, \u201cDeception detection in videos,\u201d in Proceedings of the AAAI conference on artificial intelligence, 2018, vol. 32.\n[11] Leena Mathur and Maja J Mataric\u0301, \u201cIntroducing representations of facial affect in automated multimodal deception detection,\u201d in Proceedings of the 2020 International Conference on Multimodal Interaction, 2020, pp. 305\u2013314.\n[12] Danilo Avola, Luigi Cinque, Gian Luca Foresti, and Daniele Pannone, \u201cAutomatic deception detection in rgb videos using facial action units,\u201d in Proceedings of the 13th International Conference on Distributed Smart Cameras, 2019, pp. 1\u20136.\n[13] Mingyu Ding, An Zhao, Zhiwu Lu, Tao Xiang, and JiRong Wen, \u201cFace-focused cross-stream network for deception detection in videos,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7802\u20137811.\n[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly, \u201cParameterefficient transfer learning for nlp,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 2790\u2013 2799.\n[15] Shibo Jie and Zhi-Hong Deng, \u201cConvolutional bypasses are better vision transformer adapters,\u201d arXiv preprint arXiv:2207.07039, 2022.\n[16] Vero\u0301nica Pe\u0301rez-Rosas, Mohamed Abouelenien, Rada Mihalcea, and Mihai Burzo, \u201cDeception detection using real-life trial data,\u201d in Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, 2015, pp. 59\u201366.\n[17] Viresh Gupta, Mohit Agarwal, Manik Arora, Tanmoy Chakraborty, Richa Singh, and Mayank Vatsa, \u201cBag-oflies: A multimodal dataset for deception detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0\u2013 0.\n[18] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao, \u201cJoint face detection and alignment using multitask cascaded convolutional networks,\u201d IEEE signal processing letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.\n[19] Jie Hu, Li Shen, and Gang Sun, \u201cSqueeze-andexcitation networks,\u201d in CVPR, 2018.\n[20] Anjith George and Se\u0301bastien Marcel, \u201cCross modal focal loss for rgbd face anti-spoofing,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7882\u20137891.\n[21] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and SerNam Lim, \u201cVisual prompt tuning,\u201d arXiv preprint arXiv:2203.12119, 2022."
        }
    ],
    "title": "FLEXIBLE-MODAL DECEPTION DETECTION WITH AUDIO-VISUAL ADAPTER",
    "year": 2023
}