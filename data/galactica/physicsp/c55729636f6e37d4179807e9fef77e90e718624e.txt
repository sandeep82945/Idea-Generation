Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pretrains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the total energy to search for weight distribution of the three pretext tasks since total energy corresponding to the quality of 3D conformer. Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines. 1 conclusion, as shown in Figure 7. The influence of pre-training dataset sizes. From Figure 5, it can be seen that for most downstream tasks, the performance increases with the amount of pre-trained data, although there is a marginal effect. However, on Tox21 and Sider, when the pre-training dataset
is further increased, the benefits of pre-training will decrease. One possible reason is that the backbone of GPS itself exhibits overfitting on the two data sets, and further injection of prior knowledge would not alleviate it [46]. Number of conformers for pre-training. For the multiple conformers of a single molecule, we verify whether useing these conformers can benefit the downstream task performance, as shown in Figure 6. The experiment shows that adding conformers for pre-training within a certain range can bring further pre-training benefits. However, consistent with the findings of previous work [1, 27, 43], we observe that there is a bottleneck in the improvement brought by adding more conformers. One accepted conclusion is that the top-5 conformers sampled according to the energy are sufficient to cover 80% of the equilibrium state, and further addition
of conformers cannot supplement more abundant geometric prior for downstream tasks. As discussed in Section 4.3, we observe that the GPS backbone is prone to overfitting on these two few-shot datasets, and the results show that further introducing too many geometric priors may not lead to better gains in this case. 4.4 pre-training on large-scale dataset Datasets. As the graph-level track of OGB-LSC [15], PCQM4Mv2 is a quantum chemistry dataset under the PubChemQC project [32]. The dataset contains a total of 3.74 million molecules, of which the 3D geometric information of 3.37 million training samples is obtained by DFT optimization. We adhered to the default trainvalidation split provided by OGB, and the test set was reserved for the competition and leaderboard and not publicly disclosed. In order
to approach the large-scale virtual screening scenario, the challenge [15] does not provide 3D conformers of the valid dataset and test dataset, and requires the property inference of 150k molecules to be completed within 4 hours using a single GPU, which means that it is not feasible to calculate the geometry of all test samples in the inference stage. Therefore, existing 3D GNN, e.g., SMP, can not work on this challenge due to the expensive computation cost of conformer generation. Baselines. Because the label of test datasets is officially hidden, we compared the results of validation with the top-tier method on the OGB leaderboard 2, which includes GRPE [34], TokenGT [20], EGT [18] , GPS [40], GEM-2 [26], Vis-Net [52] and TransformerM [29]. In addition, we also compared GPS++ [30], the winner solution at OGB LSC@ NeruIPS 2022 Challenge.Note that we only report the performance of single models from these solutions for fair comparisons, though it is clear that various engineering tricks,
2https://ogb.stanford.edu/docs/lsc/leaderboards/#pcqm4mv2
like ensemble, can be applied to further improve the performance of all methods including our 3D PGT. Results. The results of validation MAE are shown in Table 4. First, compared with the existing GNNs and Graph Transformers which do not consider 3D geometry information, 3D PGT has been significantly improved by introducing generative pre-training tasks. For GPS, 3D PGT has achieved 10.6% relative MAE reduction by the designed pre-training framework. Our 3D PGT outperforms the champion solution GPS++ in terms of single-model performance, proving the advantage of our pre-training framework while using the same backbone. 5 conclusion and futurework In this work, we proposed 3D PGT, a 3D pre-training framework which focus on incorporating 3D information for benefiting the molecular property prediction when the 3D structures is unavailable. In 3D PGT, we designed multiple generative pre-training tasks which can bring geometric prior to finetune stage. To better integrate these pre-training tasks and make their benefits generalize, we designed a surrogate metric of pre-training to search the adaptive weight of each pre-task. The experiment we designed proves that the potential geometric prior is not only beneficial to quantum chemical property prediction, but also to the prediction in pharmacology, physical chemistry and biophysics, etc. Moreover, 3D PGT outperforms all baselines from top solutions for large-scale molecular prediction in OGB leaderboard. acknowledgment Q. Yao is supported by NSF of China (No. 92270106). a dataset details a.1 pre-training datasets with 3d geometry Following themainstream experimental setup, we use three datasets containing 3D information for pre-training, and the detailed parameters are shown in Table 6. The three datasets are:
â€¢ QM9 [39] contains 134k molecules carrying 3D coordinates, each containing only one low-energy conformer. The largest atoms in QM9 are just nine heavy atoms, and each molecule is annotated with 12 quantum mechanical features. â€¢ GEOM-Drugs [2] contains 304kmolecules related to biology and pharmacology, and each molecule contains multiple 3D conformers, and Gibbs free energy and integrated energy are annotated as regression targets. The entire dataset contains 16 elements in total. â€¢ PCQM4Mv2 [15] contains 3.74m molecules with HOMOLUMO energy gap annotated as regression targets, it is a quantum chemistry dataset originally curated under the PubChemQCproject [32]. A total of 3.37m samples of low-energy conformers in the data set are annotated, while the remaining molecules that only contain 2D structures will be used as the validation set and the test set for the competition. b further method details Due to space limitations, the baseline details, technical details, and additional relationship study results compared in the experiment can be seen here: https://github.com/LARS-research/3D-PGT/blob/ main/Supplementary_Appendix.pdf
B.1 Computational Acceleration of Bond Angles and Dihedral Angles
Due to the high computational complexity of bond angles and dihedral angles, we reduce the computational complexity of bond angle and dihedral angle to linear time complexity, and the search of bond angle and dihedral angleâ€™s node index is replaced by the traversal of the node set ğ‘‰ and the edge set ğ¸. Inspired by the Runtime Geometry Calculation (RGC) [51] which directly calculates the geometric information from the direction unit which only sums the vectors from the target node to its neighbors once, we design two loss functions based on the sum of cosine values of bond angle and dihedral angle. RGC calculates the sum of bond angleâ€™s cosine values as follows:
Â®ğ‘¢ğ‘– ğ‘— = Â®ğ‘Ÿğ‘– ğ‘— âˆ¥Â®ğ‘Ÿğ‘– ğ‘— âˆ¥ , Â®ğ‘¢ğ‘–ğ‘˜ = Â®ğ‘Ÿğ‘–ğ‘˜ âˆ¥Â®ğ‘Ÿğ‘–ğ‘˜ âˆ¥ , (10)
Î˜ğ‘– = ğ‘ğ‘–âˆ‘ï¸ ğ‘—=1 ğ‘ğ‘–âˆ‘ï¸ ğ‘˜=1 cosğ›¼ ğ‘—ğ‘–ğ‘˜ = ğ‘ğ‘–âˆ‘ï¸ ğ‘—=1 ğ‘ğ‘–âˆ‘ï¸ ğ‘˜=1 âŒ© Â®ğ‘Ÿğ‘– ğ‘— , Â®ğ‘Ÿğ‘–ğ‘˜ âŒª , (11)
where Â®ğ‘Ÿğ‘– ğ‘— is the vector from node ğ‘– to its neighboring node ğ‘— . Record Â®ğ‘¢ğ‘– ğ‘— as the unit vector of Â®ğ‘Ÿğ‘– ğ‘— and Â®ğ‘£ğ‘– as the sum of all unit vectors from node ğ‘– , Â®ğœ” ğ‘—ğ‘– denotes the vector rejection RejÂ®ğ‘¢ğ‘– ğ‘— (Â®ğ‘£ğ‘– ), which represents the vector component of Â®ğ‘£ğ‘– perpendicular to Â®ğ‘¢ğ‘– ğ‘— . Î˜ğ‘– denotes the sum of cosine values of ğ›¼ ğ‘—ğ‘–ğ‘˜ , which denotes the angle formed by node i and two of its neighboring nodes ğ‘–, ğ‘— . For the sum of dihedral
angleâ€™s cosine values, RGC is expressed as: Â®ğ‘£ğ‘– = ğ‘ğ‘–âˆ‘ï¸ ğ‘—=1 Â®ğ‘¢ğ‘– ğ‘— , Â®ğœ”ğ‘– ğ‘— = RejÂ®ğ‘¢ğ‘– ğ‘— (Â®ğ‘£ğ‘– ) = Â®ğ‘£ğ‘– âˆ’ âŒ© Â®ğ‘£ğ‘– , Â®ğ‘¢ğ‘– ğ‘— âŒª Â· Â®ğ‘¢ğ‘– ğ‘— , (12)
Î¦(ğ‘–, ğ‘— ) = ğ‘ğ‘–âˆ‘ï¸ ğ‘—=1 ğ‘ğ‘–âˆ‘ï¸ ğ‘˜=1 cosğœ‘ğ‘šğ‘– ğ‘—ğ‘› = âŒ© Â®ğœ”ğ‘– ğ‘— , Â®ğœ” ğ‘—ğ‘– âŒª , (13)
where, the direction unit Â®ğ‘£ğ‘– is the sum of all unit vectors from node ğ‘– to its all neighboring nodes ğ‘— , Â®ğœ” ğ‘—ğ‘– denotes the vector rejection RejÂ®ğ‘¢ğ‘– ğ‘— (Â®ğ‘£ğ‘– ), which represents the vector component of Â®ğ‘£ğ‘– perpendicular to Â®ğ‘¢ğ‘– ğ‘— , termed as the vector rejection. Take Î˜ and Î¦ as target ,we design two loss functions based on the sum of cosine values of bond angle and dihedral angle as shown in Section 3.2. B.2 Supplementary ablation study We further design an ablation study on PCQM4Mv2 to verify the impact of each module in 3D PGT. From the results in Table 5, we can summarize that: a) The backbone which is combined with local message passing and global attention module has more obvious performance advantages. b) The single generative pre-training task only focuses on the reconstruction of a local descriptor. Combining them can bring more significant pre-training benefits. c). The effectiveness of searched adaptive weights has been clearly verified. c experiments details We report the detailed hyperparameters setup for pre-training in Table 7. At the same time, the hyperparameter search space during finetune in all downstream tasks is shown in Table 8. In addition, in order to facilitate the implementation of various GNN variants, we use the popular GNN library: PYG (Pytorch Geometric) (version 2.0.1). For pre-training on different datasets, we set the max epoch number as Table 7 and stop pre-training when the validation loss does not improve for 10 consecutive epochs.