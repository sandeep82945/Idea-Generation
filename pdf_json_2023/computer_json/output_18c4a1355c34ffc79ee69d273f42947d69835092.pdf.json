{
    "abstractText": "We explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning that is reducing the number of DNN parameters (weights). Our numerical results show that this pruning leads to a drastic reduction of parameters while not reducing the accuracy of DNNs and CNNs 1. Moreover, pruning the fully connected DNNs actually increases the accuracy and decreases the variance for random initializations. Our numerics indicate that this enhancement in accuracy is due to the simplification of the loss landscape. We next provide rigorous mathematical underpinning of these numerical results by proving the RMT-based Pruning Theorem. Our results offer valuable insights into the practical application of RMT for the creation of more efficient and accurate deep-learning models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Leonid Berlyand"
        },
        {
            "affiliations": [],
            "name": "Etienne Sandier"
        },
        {
            "affiliations": [],
            "name": "Lei Zhang"
        }
    ],
    "id": "SP:0926833cb5ab24bb7cbad4833578edaf9ac882c4",
    "references": [
        {
            "authors": [
                "Joshua Agterberg",
                "Zachary Lubberts",
                "Carey E Priebe"
            ],
            "title": "Entrywise estimation of singular vectors of low-rank matrices with heteroskedasticity and dependence",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Xing Anhao",
                "Zhang Pengyuan",
                "Pan Jielin",
                "Yan Yonghong"
            ],
            "title": "Svd-based dnn pruning and retraining",
            "venue": "Journal of Tsinghua University (Science and Technology),",
            "year": 2016
        },
        {
            "authors": [
                "Herv\u00e9 Abdi",
                "Lynne J Williams"
            ],
            "title": "Principal component analysis",
            "venue": "Wiley Interdisciplinary Reviews: Computational Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Zhigang Bao",
                "Xiucai Ding",
                "Wang",
                "Ke"
            ],
            "title": "Singular vector and singular subspace distribution for the matrix denoising model",
            "year": 2021
        },
        {
            "authors": [
                "Florent Benaych-Georges",
                "Raj Rao Nadakuditi"
            ],
            "title": "The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices",
            "venue": "Advances in Mathematics,",
            "year": 2011
        },
        {
            "authors": [
                "Florent Benaych-Georges",
                "Raj Rao Nadakuditi"
            ],
            "title": "The singular values and vectors of low rank perturbations of large rectangular random matrices",
            "venue": "Journal of Multivariate Analysis,",
            "year": 2012
        },
        {
            "authors": [
                "Leonid Berlyand",
                "Pierre-Emmanuel Jabin",
                "C Alex Safsten"
            ],
            "title": "Stability for the training of deep neural networks and other classifiers",
            "venue": "Mathematical Models and Methods in Applied Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Zhigang Bao",
                "Dong Wang"
            ],
            "title": "Eigenvector distribution in the critical regime of bbp transition",
            "venue": "Probability Theory and Related Fields,",
            "year": 2022
        },
        {
            "authors": [
                "Yuxin Chen",
                "Chen Cheng",
                "Jianqing Fan"
            ],
            "title": "Asymmetry helps: Eigenvalue and eigenvector analyses of asymmetrically perturbed low-rank matrices",
            "venue": "Annals of statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Romain Couillet",
                "Merouane Debbah"
            ],
            "title": "Random matrix methods for wireless communications",
            "year": 2011
        },
        {
            "authors": [
                "Chenghao Cai",
                "Dengfeng Ke",
                "Yanyan Xu",
                "Kaile Su"
            ],
            "title": "Fast learning of deep neural networks via singular value decomposition",
            "venue": "In Pacific Rim International Conference on Artificial Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Romain Couillet",
                "Zhenyu Liao"
            ],
            "title": "Random Matrix Methods for Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Prathapasinghe Dharmawansa",
                "Pasan Dissanayake",
                "Yang Chen"
            ],
            "title": "The eigenvectors of single-spiked complex wishart matrices: Finite and asymptotic analyses",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2022
        },
        {
            "authors": [
                "James W Demmel"
            ],
            "title": "Matrix computations (gene",
            "venue": "h. golub and charles f. van loan). SIAM Review,",
            "year": 1986
        },
        {
            "authors": [
                "Yann N Dauphin",
                "Razvan Pascanu",
                "Caglar Gulcehre",
                "Kyunghyun Cho",
                "Surya Ganguli",
                "Yoshua Bengio"
            ],
            "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Mahyar Fazlyab",
                "Alexander Robey",
                "Hamed Hassani",
                "Manfred Morari",
                "George Pappas"
            ],
            "title": "Efficient and accurate estimation of lipschitz constants for deep neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Rong Ge",
                "Furong Huang",
                "Chi Jin",
                "Yang Yuan"
            ],
            "title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Jungang Ge",
                "Ying-Chang Liang",
                "Zhidong Bai",
                "Guangming Pan"
            ],
            "title": "Large-dimensional random matrix theory and its applications in deep learning and wireless communications",
            "venue": "Random Matrices: Theory and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Li Deng",
                "Dong Yu",
                "George E Dahl",
                "Abdel-rahman Mohamed",
                "Navdeep Jaitly",
                "Andrew Senior",
                "Vincent Vanhoucke",
                "Patrick Nguyen",
                "Tara N Sainath"
            ],
            "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2012
        },
        {
            "authors": [
                "Geoffrey E Hinton",
                "Nitish Srivastava",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan R Salakhutdinov"
            ],
            "title": "Improving neural networks by preventing co-adaptation of feature detectors",
            "venue": "arXiv preprint arXiv:1207.0580,",
            "year": 2012
        },
        {
            "authors": [
                "Zheng Tracy Ke",
                "Yucong Ma",
                "Xihong Lin"
            ],
            "title": "Estimation of the number of spiked eigenvalues in a covariance matrix by bulk eigenvalue matching analysis",
            "venue": "Journal of the American Statistical Association,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Holger K\u00f6sters",
                "Alexander Tikhomirov"
            ],
            "title": "Limiting spectral distributions of sums of products of non-hermitian random matrices",
            "venue": "arXiv preprint arXiv:1506.04436,",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "Bernhard Boser",
                "John Denker",
                "Donnie Henderson",
                "Richard Howard",
                "Wayne Hubbard",
                "Lawrence Jackel"
            ],
            "title": "Handwritten digit recognition with a back-propagation network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1989
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Yasaman Bahri",
                "Ethan Dyer",
                "Jascha Sohl-Dickstein",
                "Guy Gur-Ari"
            ],
            "title": "The large learning rate phase of deep learning: the catapult mechanism",
            "venue": "arXiv preprint arXiv:2003.02218,",
            "year": 2020
        },
        {
            "authors": [
                "William E Leeb"
            ],
            "title": "Matrix denoising for weighted loss functions and heterogeneous signals",
            "venue": "SIAM Journal on Mathematics of Data Science,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Mahoney",
                "Charles Martin"
            ],
            "title": "Traditional and heavy tailed self regularization in neural network models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Charles H Martin",
                "Michael W Mahoney"
            ],
            "title": "Heavy-tailed universality predicts trends in test accuracies for very large pre-trained deep neural networks",
            "venue": "In Proceedings of the 2020 SIAM International Conference on Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Charles H Martin",
                "Michael W Mahoney"
            ],
            "title": "Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Alexandrovich Marchenko",
                "Leonid Andreevich Pastur"
            ],
            "title": "Distribution of eigenvalues for some sets of random matrices",
            "venue": "Matematicheskii Sbornik,",
            "year": 1967
        },
        {
            "authors": [
                "Charles H Martin",
                "Tongsu Peng",
                "Michael W Mahoney"
            ],
            "title": "Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data",
            "venue": "Nature Communications,",
            "year": 2021
        },
        {
            "authors": [
                "Xuran Meng",
                "Jianfeng Yao"
            ],
            "title": "Impact of classification difficulty on the weight matrices spectra in deep learning and application to early-stopping",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Naumov",
                "Vladimir Spokoiny",
                "Vladimir Ulyanov"
            ],
            "title": "Bootstrap confidence sets for spectral projectors of sample covariance",
            "venue": "Probability Theory and Related Fields,",
            "year": 2019
        },
        {
            "authors": [
                "Sean O\u2019Rourke",
                "Van Vu",
                "Ke Wang"
            ],
            "title": "Random perturbation of low rank matrices: Improving classical bounds",
            "venue": "Linear Algebra and its Applications,",
            "year": 2018
        },
        {
            "authors": [
                "SEAN O\u2019ROURKE",
                "VU VAN",
                "Ke Wang"
            ],
            "title": "Matrices with gaussian noise: optimal estimates for singular subspace perturbation",
            "venue": "arXiv e-prints, pages",
            "year": 2018
        },
        {
            "authors": [
                "Leonid Pastur"
            ],
            "title": "On random matrices arising in deep neural networks. gaussian case",
            "venue": "arXiv preprint arXiv:2001.06188,",
            "year": 2020
        },
        {
            "authors": [
                "Jiyoung Park",
                "Ian Pelakh",
                "Stephan Wojtowytsch"
            ],
            "title": "Minimum norm interpolation by perceptra: Explicit regularization and implicit bias",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Lutz Prechelt"
            ],
            "title": "Early stopping\u2014but when? Neural Networks: Tricks of the Trade",
            "venue": "Second Edition,",
            "year": 2012
        },
        {
            "authors": [
                "Leonid Pastur",
                "Victor Slavin"
            ],
            "title": "On random matrices arising in deep neural networks: General iid case",
            "venue": "Random Matrices: Theory and Applications,",
            "year": 2023
        },
        {
            "authors": [
                "Ilan Price",
                "Jared Tanner"
            ],
            "title": "Dense for the price of sparse: Improved performance of sparsely initialized networks via a subspace offset",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Markus Ringn\u00e9r"
            ],
            "title": "What is principal component analysis",
            "venue": "Nature Biotechnology,",
            "year": 2008
        },
        {
            "authors": [
                "Vadim Ivanovich Serdobolskii"
            ],
            "title": "Multivariate statistical analysis: A high-dimensional approach, volume 41",
            "venue": "Springer Science & Business Media,",
            "year": 2000
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "The Journal of Machine Learning Research,",
            "year": 1929
        },
        {
            "authors": [
                "Yitzchak Shmalo",
                "Jonathan Jenkins",
                "Oleksii Krupchytskyi"
            ],
            "title": "Deep learning weight pruning with rmt-svd: Increasing accuracy and reducing overfitting",
            "venue": "arXiv preprint arXiv:2303.08986,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Sutskever",
                "James Martens",
                "George Dahl",
                "Geoffrey Hinton"
            ],
            "title": "On the importance of initialization and momentum in deep learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Thiziri Nait Saada",
                "Jared Tanner"
            ],
            "title": "On the initialisation of wide low-rank feedforward neural networks",
            "venue": "arXiv preprint arXiv:2301.13710,",
            "year": 2023
        },
        {
            "authors": [
                "Max Staats",
                "Matthias Thamm",
                "Bernd Rosenow"
            ],
            "title": "Boundary between noise and information applied to filtering neural network weight matrices",
            "venue": "arXiv preprint arXiv:2206.03927,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le"
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Matthias Thamm",
                "Max Staats",
                "Bernd Rosenow"
            ],
            "title": "Random matrix analysis of deep neural network weight matrices",
            "venue": "Physical Review E,",
            "year": 2022
        },
        {
            "authors": [
                "Sunil Vadera",
                "Salem Ameen"
            ],
            "title": "Methods for pruning deep neural networks",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability by roman vershynin, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Jian Xue",
                "Jinyu Li",
                "Yifan Gong"
            ],
            "title": "Restructuring of deep neural network acoustic models with singular value decomposition",
            "venue": "In Interspeech,",
            "year": 2013
        },
        {
            "authors": [
                "Xuanzhe Xiao",
                "Zeng Li",
                "Chuanlong Xie",
                "Fengwei Zhou"
            ],
            "title": "Heavy-tailed regularization of weight matrices in deep neural networks",
            "venue": "arXiv preprint arXiv:2304.02911,",
            "year": 2023
        },
        {
            "authors": [
                "Yuhui Xu",
                "Yuxi Li",
                "Shuai Zhang",
                "Wei Wen",
                "Botao Wang",
                "Wenrui Dai",
                "Yingyong Qi",
                "Yiran Chen",
                "Weiyao Lin",
                "Hongkai Xiong"
            ],
            "title": "Trained rank pruning for efficient deep neural networks",
            "venue": "Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition",
            "year": 2019
        },
        {
            "authors": [
                "Huanrui Yang",
                "Minxue Tang",
                "Wei Wen",
                "Feng Yan",
                "Daniel Hu",
                "Ang Li",
                "Hai Li",
                "Yiran Chen"
            ],
            "title": "Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Zhixiang Zhang",
                "Guangming Pan"
            ],
            "title": "Tracy-widom law for the extreme eigenvalues of large signal-plus-noise matrices",
            "venue": "arXiv preprint arXiv:2009.12031,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Contents"
        },
        {
            "heading": "1 Introduction 3",
            "text": ""
        },
        {
            "heading": "2 Acknowledgements 4",
            "text": ""
        },
        {
            "heading": "3 Background on Deep Learning 4",
            "text": ""
        },
        {
            "heading": "4 Numerical Algorithm and Experiments 6",
            "text": "4.1 Numerical Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.1.1 An overview of the Marchenko-Pastur (MP) distribution and its applications in machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4.1.2 Using MP for pruning DNN weights . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.1.3 MP and Tracy Widom distribution for DNN training . . . . . . . . . . . . . . . . 7\n4.2 Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2.1 Training of fully connected DNNs on MNIST: simplifying the loss landscape . . . 10 4.2.2 Simplifying the loss landscape for fully connected DNNs on Fashion MNIST . . . 12\n1Code for this work can be found at https://gist.github.com/yspennstate/48b2a89c08714a6a4b7d00a8e19ea7e7\nar X\niv :2\n31 0.\n03 16\n5v 2\n[ cs\n.L G\n4.2.3 MP-based pruning with sparsification for fully connected DNNs on Fashion MNIST 14 4.2.4 MP-based pruning of CNNs on MNIST and Fashion MNIST . . . . . . . . . . . . 15 4.2.5 MP-based pruning with sparsification for CNN trained on Fashion MNIST . . . . 17 4.2.6 Numerics for training DNNs on CIFAR-10: reducing parameters via MP-based pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.2.7 MP-based pruning with sparsification for CNN trained on CIFAR10 . . . . . . . 20"
        },
        {
            "heading": "5 Mathematical underpinning of numerical results 21",
            "text": "5.1 The classification confidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.2 How pruning affects classification confidence (for deterministic weight layer matrices) . . 21 5.3 Assumptions on the random matrix R and the deterministic matrix S . . . . . . . . . . 22 5.4 Key technical Lemma: removing random weights for DNN with arbitrary many layers\ndoes not affect classification confidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.5 Pruning Theorem for DNN with arbitrary many layers: how pruning random weights using PM distribution affects the classification confidence . . . . . . . . . . . . . . . . . 26 5.6 Simple example of DNN with one hidden layer . . . . . . . . . . . . . . . . . . . . . . . . 27 5.7 Pruning Theorem for accuracy: how pruning affects accuracy . . . . . . . . . . . . . . . 29"
        },
        {
            "heading": "6 Appendix A: Some known results on perturbation of matrices 35",
            "text": "6.1 Asymptotics of singular values and singular vectors of deformation matrix . . . . . . . . 35 6.2 Gershgorin\u2019s Circle Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37"
        },
        {
            "heading": "7 Appendix B: An Approximation Lemma \u2013 pruned matrix W 1 approximates the deterministic matrix S 37",
            "text": "7.1 Numerics for Example 5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7.2 Details for Example 5.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39"
        },
        {
            "heading": "8 Appendix C: Proof for Pruning Theorem 40",
            "text": "8.1 Proof for key technical Lemma 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 8.2 Proof of Pruning Theorem for a single object . . . . . . . . . . . . . . . . . . . . . . . . 41 8.3 Proof of Pruning Theorem for accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
        },
        {
            "heading": "9 Appendix D: Other algorithms required for implementing RMT-SVD based pruning of DNN 43",
            "text": "9.1 BEMA algorithm for finding \u03bb` . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 9.2 The role of singular value decomposition in deep learning . . . . . . . . . . . . . . . . . 46 9.3 Eliminating singular values while preserving accuracy . . . . . . . . . . . . . . . . . . . . 46 9.4 MP fit Criteria: Checking if the ESD of X fits a MP distribution . . . . . . . . . . . . . 48"
        },
        {
            "heading": "10 Appendix E: Some of the proofs and numerics 49",
            "text": "10.1 Proof of Lemma 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 10.2 Numerical example used to calculate \u03b4X. . . . . . . . . . . . . . . . . . . . . . . . . . . 50 10.3 Hyperparameters for Subsection 4.2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 10.4 Hyperparameters for Subsection 4.2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 10.5 Hyperparamters for Subsection 4.2.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 10.6 The hyperparamters for Subsection 4.2.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . 52"
        },
        {
            "heading": "1 Introduction",
            "text": "Deep neural networks have become a dominant tool for tackling classification tasks, where objects within a set S \u0102 Rn are catergorized. DNNs are trained on labeled datasets T \u0102 Rn, by optimizing a loss function such as cross-entropy loss function in (2) to maximize classification accuracy. Through this training process, DNNs have achieved state-of-the-art results on many real-world classification challenges, including handwriting recognition [LBD`89], image classification [KSH17], speech recognition [HDY`12], and natural language processing [SVL14].\nOverfitting is a common challenge for the training of DNNs, which occurs when the model\u2019s complexity results in memorization of the training data rather than generalization to new data. Consequently, the model\u2019s performance on the test set deteriorates despite high training set accuracy. To counteract overfitting, different regularization techniques such as dropout [SHK`14], early stopping [Pre12] and weight decay regularization [PPW23] have been developed.\nRecently, RMT has been used in deep learning for addressing overfitting [MM21, MM19]. Similar works have used RMT to obtain RMT-based stopping criteria, see [MY23], and regularization, see [XLXZ23]. It has also been shown that RMT can be used to predict DNN performance without access to the test set, see [MPM21, MM20], and in general to study the spectrum of weight layers [TSR22] and the input-output Jacobina matrix [Pas20, PS23]. RMT-based initializations were also studied in [ST23]. However, these RMT works in deep learning focused on issues other than utilizing RMT-based pruning in DNNs, which is the focus of our work. Specifically, we study the applications of RMT for pruning DNNs during training. We present numerical simulations on simple DNN models trained on the MNIST, Fashion MNIST and CIFAR-10 datasets. Generally, these RMT techniques can be extended to other DNN types and any fully connected or convolutional layer of a pretrained DNNs to reduce layer parameters while preserving or even enhancing accuracy.\nOur RMT pruning approach simplifies DNNs, enabling them to find deeper minima on the loss landscape of the training set. As a result, DNNs can achieve higher accuracy directly on the training set. DNNs, during their training, essentially navigate a complex, multi-dimensional loss landscape in search of the global minimum - the optimal solution. However, the nature of these landscapes can often be rugged, filled with numerous sub-optimal local minima that trap the learning process. By implementing RMT pruning, the landscape becomes smoother, less prone to local minima, and more navigable for the learning algorithm. This makes the optimization process more efficient and enables the DNN to find deeper minima for the loss of the training set.\nThe works [YTW`20, XLG13, CKXS14, APJY16, XLZ`19] utilized SVD to eliminate small singular values from DNN weight matrices. This pruning of singular values was used to prune the parameters in the weight layer matrices of the DNNs, similar to our work. This pruning was based on techniques such as energy ratio thresholds and monitoring the error of a validation set. However, this energy threshold comes from empirical observations, whereas the the MP threshold used for pruning in our work is justified theoretically by RMT and applied to fully connected networks and simple CNNs to establish agreement between theory and numerics.\nOther pruning methods can be found in [VA22], in which the authors categorize over 150 studies into three pruning categories: methods that use magnitude based pruning, methods that utilise clustering to identify redundancy, and methods that use sensitivity analysis to assess the effect of pruning. Our work is mostly related to the first method of pruning, we use MP-based pruning to prune small singular values, together with sparsification to prune all weights of the DNN bigger than some threshold (and set them to 0), see Subsections 4.2.3, 4.2.5 and 4.2.7. To the best of our knowledge, we are the first to use the MP distribution as a threshold for pruning. Furthermore, the Pruning Theorem 5.4 provides mathematical justification for the MP-based pruning approach, while Lemma 5.3 provides mathematical justification for the sparsification approach. Other types of pruning methods, such as pruning at initialization, have also been used, see [PT21].\nIn [STR22], the MP distribution was used to decrease the size of large singular values, which allows for the extraction of the denoised matrix from the original (noisy) one. Then, a validation set was used to determine the SVD pruning threshold for filtering noisy data in DNN classification. However, there are important distinctions with our work. First, we use a MP threshold for pruning weights directly, without access to a validation set. Second, the pruning in [STR22] is done after training, whereas pruning in our work is done during training which corresponds to a different mechanism of improvement of accuracy via simplification of loss landscape during training. Thirdly, the authors of [STR22] focused on how pruning small singular values can improve the accuracy of DNNs trained on noisy data. On the other hand, we study how the pruning of small singular values can guide the pruning of weights of the DNN that are random due to initialization. Moreover, our theoretical approach is applicable to both sources of randomness; initialization of weights and noise in the data.\nIn contrast with the above numerical works, our work also provides rigorous theoretical underpinning on the relation between RMT-based pruning of DNN weight matrices and accuracy. To this end, we establish rigorous mathematical results (the Pruning Theorem) which explain the effectiveness of our RMT-based algorithm. The theoretical results will help elucidate the underlying mechanisms of the numerical algorithm, demonstrating why it successfully reduces the number of parameters in a DNN without reducing accuracy. These theoretical results will allow for the development of RMT-based pruning for state of the art DNNs such as ResNets and VITs.\nThe remainder of this paper is organized as follows: In Section 3, we present an overview of DNN training. In Section 4, we present the numerical results\nof this paper. In Section 5, we present the Pruning Theorem."
        },
        {
            "heading": "2 Acknowledgements",
            "text": "The work of LB was partially supported by NSF grant DMS-2005262. LB and ES are grateful to the Labex B\u00e9zout Foundation for supporting the stay of LB while visiting Universit\u00e9 Paris-Est which helped to facilitate the collaboration between ES, LB, and YS on this work."
        },
        {
            "heading": "3 Background on Deep Learning",
            "text": "DNNs have become a widely-used method for addressing classification problems, in which a collection of objects S \u0102 Rn is assigned to one of K classes. The objective is to approximate an exact classifier \u03c8, which maps an element s P S \u0102 Rn to a probability vector pp1psq, . . . , pKpsqq. In this vector, pipsq \u201c 1 and pj \u201c 0 for j \u2030 ipsq, where ipsq denotes the correct class for s. The exact classifier \u03c8 is known only for a training set T , and DNNs are trained to approximate \u03c8 by constructing a parameterized classifier \u03d5p\u03b1, sq with the aim of extending \u03c8 from T to all of S via \u03d5p\u03b1, sq.\nThis is accomplished by finding parameters \u03b1 that allow \u03d5p\u03b1, sq to map s P T to the same class as \u03c8 while maintaining the classifier\u2019s ability to generalize to elements s P S. The parameters \u03b1 are optimized by minimizing a loss function, aiming to enhance the accuracy as the loss declines.\nIn this study, a DNN is represented as a composition of two functions: the softmax function \u03c1 and an intermediate function Xp\u00a8, \u03b1q. The function Xp\u00a8, \u03b1q is defined as a composition of affine transformations and nonlinear activations, as follows:\n\u2022 Mlp\u00a8, \u03b1lq is an affine function that maps RNl\u00b41 to RNl , and depends on a parameter matrix Wl of size Nl \u02c6Nl\u00b41 and a bias vector \u03b2l.\n\u2022 \u03bb : Rm \u00de\u00d1 Rm is a nonlinear activation function.\n\u2022 Xp\u00a8, \u03b1q \u201c \u03bb \u02ddMk \u00a8 \u00a8 \u00a8\u03bb \u02ddM1, where k is the number of layers in the DNN. Note, each \u03bb here might be different from the others, given that the domains of each differ.\nLastly, \u03c1 is the softmax function, which normalizes the output of Xp\u00a8, \u03b1q into probabilities. The components of \u03c1 are calculated as:\n\u03c1ips, \u03b1q \u201c exppXips, \u03b1qq\n\u0159K i\u201c1 exppXips, \u03b1qq\n. (1)\nThe DNN\u2019s output, \u03d5, is a vector representing the probabilities of an object s P S belonging to a particular class i. \u03d5 \u201c \u03d5ps, \u03b1q, where \u03b1 P R\u03bd is the DNN\u2019s parameter space and \u03bd \" 1 is the dimension of the parameter space. The goal is to train the DNN \u03d5 to approximate the exact classifier by minimizing a loss function, such as the cross-entropy loss function\nL\u0304p\u03b1q \u201c \u00b4 1|T | \u00ff\nsPT log\n` pipsqps, \u03b1q \u02d8 . (2)\nTraining a DNN essentially involves traversing a high-dimensional, non-convex loss landscape to locate the global minimum. But the complexity of these landscapes frequently leads to local minima, saddle points, or flat regions, all of which trap the learning process, impeding it from reaching an optimal solution [DPG`14]. These issues amplify as the dimensionality (and thus the complexity) of the DNN increases [CHM`15].\nIn local minima, the gradient of the loss function equals zero, but it is not the global minimum, thus the algorithm incorrectly assumes it has found the best possible solution. Saddle points, on the other hand, are points where the gradient is zero but they are neither a global nor a local minimum. They are particularly problematic in high-dimensional spaces, a common feature in deep learning.\nTo overcome these obstacles, various sophisticated optimization techniques are employed. For example, optimization algorithms such as Momentum, RMSProp, or Adam are designed to prevent getting stuck by adding additional components to the update rule, which can help in navigating the complex optimization landscape. These methods imbue the optimization process with a form of \u2018memory\u2019 of previous gradients, enabling it to continue its search even in flat regions, hence helping to escape local minima and saddle points.\nDNNs with their intricate and numerous parameters, offer formidable modeling capabilities [GBC16]. However, the same attribute that enables their power can also serve as a curse during the training process. Theoretically, DNNs, due to their extensive parameterization, should reach high levels of accuracy on their training sets [ZBH`21]. But in practice, the accuracy on the training set can often plateau, suggesting the DNN is getting stuck in a local minimum of the loss function [GHJY15]. This forms a critical impediment, limiting the achievable accuracy on both the training and test sets [HS97].\nThus, despite the vast number of parameters, DNNs can often find themselves stranded in areas of poor performance. This seemingly paradoxical occurrence is attributable to the interplay between the DNN\u2019s architecture, the data it is training on, and the optimization process being employed [GBC16]. Factors like poor initialization, inappropriate learning rates, or the vanishing/exploding gradients problem can cause the DNN to settle in sub-optimal regions of the loss landscape [GB10].\nTechniques like gradient clipping [PMB13] can aid in overcoming these issues. Regularization techniques, which either penalize complex models or enforce sparsity in the weight matrix, can also assist in avoiding local minima [GBC16]. Nonetheless, these methods do not alter the fundamental structure of the loss landscape, indicating that the problem of local minima remains [GHJY15].\nThe potential of the RMT approach stands out in this context. We show that utilizing RMT in pruning the DNN\u2019s weight layers simplifies the loss landscape. This simplification reduces the incidence of local minima and saddle points, aiding the optimization process in its quest for a global minimum. In doing so, the DNN might attain higher levels of accuracy on the training set directly without reaching a plateau. This results in an overall enhancement in model performance, as higher training set accuracy generally translates to improved performance on the test set, assuming overfitting does not occur."
        },
        {
            "heading": "4 Numerical Algorithm and Experiments",
            "text": "In this section, we focus on the training of two DNNs: the normal DNN which keeps all of its singular values and a pruned DNN based on algorithm 1, see Subsection 4.1.2. Each DNN is trained for a predetermined number of epochs, with the number of epochs varying per example. The DNNs are also trained for multiple seeds to ensure the reproducibility of the simulations.\nThe performance of the DNNs is evaluated by plotting the average accuracy and variance of accuracy for the different seeds. This allows us to visually compare the performance of both the normal and pruned DNNs. For more numerical results using a slightly different RMT training approach, see [SJK23]."
        },
        {
            "heading": "4.1 Numerical Algorithm",
            "text": ""
        },
        {
            "heading": "4.1.1 An overview of the Marchenko-Pastur (MP) distribution and its applications in machine learning",
            "text": "We start with the MP distribution from RMT. This distribution is of fundamental importance in RMT and has numerous applications, such as signal processing, wireless communications, and machine learning, as described in [Ver18, GLBP21, Ser00, CD11]. The MP distribution characterizes the limiting spectral density of large random matrices and conveys information about the asymptotic distribution of eigenvalues in a random matrix, predicting the behavior of random matrices under various conditions. Additionally, the MP distribution is utilized in principal component analysis (PCA) and other dimension reduction techniques, see [AW10, BS14, Rin08].\nTo begin, we introduce the empirical spectral distribution (ESD) of an N \u02c6M matrix G as follows:\nDefinition 4.1. The ESD of an N \u02c6M matrix G is given by:\n\u00b5GM \u201c 1\nM\nM \u00ff i\u201c1 \u03b4\u03c3i , (3)\nwhere \u03c3i denotes the ith non-zero singular values of G, and \u03b4 represents the Dirac measure.\nTheorem 4.2 (Marchenko and Pastur (1967) [MP67]). Let W be an N \u02c6 M random matrix with M \u010f N . The entries Wi,j are independent and identically distributed random variables with mean 0 and variance \u03c32 \u0103 8. Define X \u201c 1NW\nTW . Assuming that N \u00d1 8 and MN \u00d1 c P p0,`8q, the ESD of X, denoted by \u00b5XM , converges weakly in distribution to the Marchenko-Pastur probability distribution:\n1\n2\u03c0\u03c32\na\np\u03bb` \u00b4 xqpx\u00b4 \u03bb\u00b4q cx 1r\u03bb\u00b4,\u03bb`sdx (4)\nwith\n\u03bb\u02d8 \u201c \u03c32p1 \u02d8 ? cq2. (5)\nThis theorem asserts that the eigenvalue distribution of a random matrix converges to the MarchenkoPastur distribution as its dimensions increase. The MP distribution is a deterministic distribution, dependent on two parameters: the variance of the random variables in the initial matrix, \u03c32, and the ratio of the number of columns to the number of rows, c."
        },
        {
            "heading": "4.1.2 Using MP for pruning DNN weights",
            "text": "As stated in Section 3, a DNN is a composition of affine functionsMl and non-linear activation functions. The affine functions Ml can be thought of as a N \u02c6M matrix Wl of parameters and a bias vector \u03b2l. In this work, we only focus on the matrixWl of parameters. It has been shown thatWl can be studied using the spiked model approach in random matrices, with the ESD ofXl \u201c 1NW T l Wl having some eigenvalues which are bigger than \u03bb` and some eigenvalues which are smaller than \u03bb`, see [MM21, STR22]. In this paper, we focus on weight layer matrices Wl which were initialized in such a way that ? NWlp0q satisfy the assumptions of W given in Theorem 4.2. Then, we look at the ESD of Bl \u201c W Tl Wl, without normalizing by 1N . This setting is more applicable for the situation in which the components of R are i.i.ds taken from Np0, 1N q.\nThus, we take Blptq \u201c WlptqTWlptq, with Wlptq a N \u02c6M weight of the lth layer matrix at time t of DNN training. We use RMT to study the deformed matrix Wlptq \u201c Rlptq ` Slptq, with Rlptq random and Slptq a deterministic matrix. One can assume that during training we go from Wlp0q \u201c Rl (i.e. Wl is random) to Wlptfinalq \u201c Rlptfinalq ` Slptfinalq, with }Slptfinalq} \u2030 0 and tfinal the final training time. Meaning that as t \u00d1 tfinal, }Slptq} grows and so Wlptq becomes less random.\nAn important question is: why does training reduce randomness in weight matrices?\n\u2022 Suppose a DNN has only one weight layer matrix W . Before training starts, the matrix W p0q is initialized with DNN weights. Entries of W arranged into a vector \u03b1p0q are chosen randomly, meaning W p0q is fully random. A gradient descent step can be written as:\n\u03b1pn` 1q \u201c \u03b1pnq \u00b4 \u03c4\u2207Lp\u03b1pnqq. (6)\nThe loss gradient \u2207Lp\u03b1pnqq is determined by the training data T , which is mostly deterministic. That is, when we take a step from n \u201c 0 to n \u201c 1, \u03b1p0q, the random DNN parameters, are gradually replaced with deterministic parameters and this process continues throughout training.\n\u2022 However, T is only mostly deterministic. Each object s P T is sampled from a probability distribution and is a random variable, so T contains some randomness.\n\u2022 In practice, the randomness of \u03b1pnq decreases as n \u00d1 8 (||Spnq|| increases), but some randomness due to the data remains.\nWe make the following observations based on the singular values of the weight layer matrix Wl:\n\u2022 Observation 1: Singular values \u03c3i of Wl that are smaller than a threshold a\n\u03bb` are likely to be singular values of Rl, where \u03bb` is the upper bound of the MP distribution of RTR. For more on this observation, see [TSR22, STR22].\n\u2022 Observation 2: Rl does not enhance the accuracy of a DNN. In other words, the random components of the weight layers do not contain any valuable information and therefore do not improve accuracy, see Lemma 5.3.\nBased on these observations, the main idea is to remove some randomness from the DNN by eliminating some singular values of Wl that are smaller than the threshold a\n\u03bb`. Algorithm 1 describes this procedure."
        },
        {
            "heading": "4.1.3 MP and Tracy Widom distribution for DNN training",
            "text": "We use the bulk eigenvalue matching analysis (BEMA) algorithm (see Subsection 9.1) to find the MP distribution that best fits the ESD of Xl \u201c W Tl Wl, with Wl a weight layer matrix. We then use the\nAlgorithm 1 Optimized DNN Training and Pruning for Parameter Efficiency Require: \u2113, a predetermined number of epochs; \u03c4 , a threshold for the MP fit criteria in Subsection\n9.4; fpepochq, a monotonically decreasing function from 1 to 0 (i.e. (7)) and for each 1 \u010f l \u010f L and weight layer matrix Wl state splitl \u201c false. 1: Initialize: Train the DNN for \u2113 epochs. Take epoch :\u201c \u2113. 2: while a predefined training condition is met (i.e. epoch \u010f 100q do 3: for each l, if splitl \u201c false then for weight matrix Wl in the DNN \u03d5 do 4: Perform SVD on Wl to obtain Wl \u201c Ul\u03a3lV Tl . 5: Calculate eigenvalues of W Tl Wl. 6: Apply BEMA algorithm (see Subsection 9.1) to find the best fit MP distribution for ESD of X \u201c W Tl Wl and corresponding \u03bb`. 7: Check if ESD of X fits the MP distribution using MP fit criteria from Subsection 9.4 and threshold \u03c4 . 8: if ESD fits the MP distribution then 9: Eliminate the portion (1 \u00b4 fpepochq) of singular values smaller than a \u03bb` to obtain \u03a31\nand form W 1l \u201c Ul\u03a31lV Tl . 10: Use \u03a31 to create W 11,l \u201c Ul a \u03a31l and W 1 2,l \u201c a \u03a31lV T l . 11: if W 11,l and W 1 2,l together have fewer parameters than W 1 l then 12: Replace Wl in the DNN \u03d5 with W 11,lW 1 2,l, change splitl \u201c true. 13: else 14: Replace Wl in the DNN \u03d5 with W 1l . 15: end if 16: else 17: Don\u2019t replace Wl. 18: end if 19: end for 20: Train the DNN for \u2113 epochs. Take epoch :\u201c epoch ` \u2113. 21: for each l, if splitl \u201c true do 22: if for Wl :\u201c W 11,lW 12,l the ESD of Xl fits the MP distribution with thresholds \u03c4 and \u03bb` and\nif, we (hypothetically) applied steps 4-12 to Wl, the number of parameters in the DNN \u03d5 would decrease then\n23: replace W 11,lW 1 2,l with Wl and splitl \u201c false. 24: else 25: Don\u2019t change anything. 26: end if 27: end for 28: end while\nTracy Widom distribution (see [KML21]) to find a confidence interval for the \u03bb` of the ESD of Xl and then prune the small singular values of Wl based on the MP-based threshold a\n\u03bb`, see Subsection 9.1 for more details on the Tracy Widom distribution and the BEMA algorithm. The steps of this procedure are shown in Algorithm 1.\nIn step 9 of Algorithm 1, we ensure not to eliminate all of the small singular values (i.e., singular values whose corresponding eigenvalues fall within the MP distribution). Striking a balance between removing the smaller singular values and retaining some is found to be essential. Removing all of the smaller singular values might result in underfitting of the DNN, thereby inhibiting its learning capability. Conversely, retaining some of the smaller singular values adds a degree of randomness in the weight layer matrix Wl, which is found to positively impact the DNN\u2019s performance."
        },
        {
            "heading": "4.2 Numerical Experiments",
            "text": "Numerical simulations presented in this paper show that MP-based pruning enhances the accuracy of DNNs while reducing the number of DNN weights (parameters). The first set of numerical simulations employs fully connected DNNs trained on the MNIST and Fashion MNIST datasets, revealing that MP-based pruning during training improves accuracy by 20-30% while reducing the parameter count by 30-50%. These findings are consistent across various architectures and weight initializations, underscoring the consistency of the MP-based pruning approach. Further, the combination of this approach and sparsification (eliminating parameters below a certain threshold, see [VA22]) leads to even more significant reductions in parameters (up to 99.8%) while increasing accuracy (by 20-30%). This reduction in parameters is greater than what is achievable through sparsification alone (99.5%), see Subsection 4.2.3.\nFor simplicity of presentation, we choose to demonstrate the MP-based pruning for fully connected DNNs. In short, the idea is as follows. First, we observe that weight layer matrices Wl have singular values of two types: those which contain information and the ones which don\u2019t and therefore can be removed (pruned). This separation is done via MP threshold a\n\u03bb`. Furthermore, we demonstrate that pruning based on this MP threshold preserves DNN accuracy. These numerical findings are supported by rigorous mathematical results (Theorem 5.7). In fact, for the case of full connected layers we show numerically that MP-based pruning simplifies the loss landscape, leading to a significant increase in DNN accuracy (by 20-30%). Finally, we show that a combination of MP-based pruning with sparsification preserves or even increases accuracy while reducing parameters by 99.8% vs. MPbased pruning alone, with a reduction of 30-50%, or sparsification alone, with a reduction of 99.5%. Our theoretical results also explain why sparsification does not reduce accuracy, see Lemma 5.3 and Remark 5.3.\nOur numerics explores the application of MP-based pruning on DNNs that already achieve relatively high accuracy on MNIST (Section 4.2.1), Fashion MNIST (Section 4.2.2), and CIFAR10 datasets (Section 4.2.6), including those using Convolutional Neural Networks (CNNs) and sparsification techniques (Section 4.2.3,4.2.4,4.2.5 4.2.7). Our results show a substantial reduction in parameters (over 95%), while preserving accuracy through a combination of MP-based pruning during training and post-training sparsification, surpassing the efficiency of using sparsification alone (80-90% reduction in parameters). These extensive simulations, for various architectures and initializations, demonstrate the consistency and wide applicability of MP-based pruning in optimizing DNN performance.\nTraining and testing procedure\nThe training and testing procedure for each network consists of the following steps:\n1. We follow the standard partition for MNIST and Fashion MNIST, with 60,000 images for training and 10,000 images for testing.\n2. We train the network for a certain number of epochs.\n3. We test the network after each epoch and store the accuracy for later comparison.\n4. For the pruned DNN, we apply algorithm 1 after a set number of epochs (defined by the split frequency)."
        },
        {
            "heading": "4.2.1 Training of fully connected DNNs on MNIST: simplifying the loss landscape",
            "text": "The results of the simulations are presented in the examples below. These examples show figures and tables which compare the average accuracy and variance of accuracy for both the normally trained and pruned DNNs, as well as the average loss and number of parameters in both DNNs. Detailed discussion and analysis of these results are presented in the examples.\nTraining hyperparameters:\n\u2022 Split frequency (\u2113) (every how many epochs we split the pruned DNN and remove small singular values): 7\n\u2022 goodness of fit (GoF or \u03c4) = .7\nSee Subsection 10.3 for the other hyperparameters in these simulations. Remark 4.1. The algorithm for finding the GoF parameter is given in Subsection 9.4. It is used to determine if the assumption given in Theorem 5.4, that Wl \u201c Rl `Sl, is reasonable and that the weight layers can reasonably be modeled as a spiked model (that is Wl is a deformed matrix).\nExample 4.3. We conducted several numerical simulations to compare the performance of the normal DNNs, trained using conventional methods, and pruned DNNs, trained using our RMT approach.\nIn all simulations, the networks start with different initial topologies and are trained over a course of 40 epochs. The portion of singular values smaller than a\n\u03bb` that we retain (see step 3 in algorithm 1) is given by the linear function:\nfpepochq \u201c max \u02c6 0,\u00b4 1 30 \u00a8 epoch ` 1 \u02d9 . (7)\nThe topologies and the results of the simulations are summarized in Table 1 and Fig. 1. The results indicate a consistent trend across different topologies: the pruned DNNs outperform the normal DNNs in terms of accuracy on the test set, while also displaying smaller variance across multiple runs. Furthermore, the pruned DNNs consistently achieve a significant reduction in parameters by the end of the training, see Remark 4.3.\nRemark 4.2. We see that in these examples, the goodness of fit parameter can be very large (even 1) and does not seem to change the accuracy of the DNN. This is not always the case, especially for state-of-the-art pretrained DNNs, as we will show in future works. See also Subsection 4.2.4 for an example when GoF must be smaller.\nRemark 4.3. In Table 2, we observe the effect of our RMT training approach on the number of parameters in our DNN with different topologies. Each topology started with a fixed number of parameters, and, by the end of training, we see a significant reduction in the number of parameters across all topologies for the pruned DNN. For each topology and across all seeds, the reduction in the number of parameters was consistent, indicating the robustness of our training process in pruning the network while maintaining performance.\nSimplification of loss landscape for more efficient training As mentioned, a common challenge with DNNs is the complex and high-dimensional loss landscape due to the large number of parameters. This complexity often leads to local minima or saddle points that hinder optimal training. However, by using this clever RMT pruning approach, we effectively eliminate redundant parameters, thereby simplifying the loss landscape. This simplification allows us to avoid suboptimal local minima and converge more readily to a global minimum.\nThis improved optimization efficiency is evident when comparing the loss and accuracy of the\noriginal and pruned DNNs on both training and test sets, see Table 3 and Fig. 2. The pruned DNNs achieve lower loss and higher accuracy on the training set directly, indicating that they are finding deeper minima in the loss landscape and avoid subobtimal local minima.\nThus, the RMT pruning approach not only significantly reduces the complexity of DNNs but also enhances their performance by improving their optimization efficiency. Despite the reduction in parameters, the pruned DNNs still exhibit excellent performance on both training and test sets (even higher accuracy then the normally trained DNNs), demonstrating the effectiveness of this approach."
        },
        {
            "heading": "4.2.2 Simplifying the loss landscape for fully connected DNNs on Fashion MNIST",
            "text": "In this section, we trained the normal and pruned DNNs on the data set Fashion MNIST and we look at the performance of both DNNs on the training and test set. Again the pruned DNN obtains higher accuracy and lower loss on both the training and test sets, evidence that pruning the DNN using RMT simplifies the loss landscape and allows the DNN to find a deeper global minimum.\nTraining hyperparameters:\n\u2022 Split frequency (every how many epochs we split the modified DNN and remove small singular values): 7\n\u2022 goodness of fit (GoF) = .7\nThe other hyperparameters for the simulations in this subsection can be found in Subsection 10.4.\nIn all simulations, the networks start with different initial topologies, are trained over a course of 70 epochs, and the portion of singular values smaller than a\n\u03bb` that we retain is given by the linear function:\nfpepochq \u201c max \u02c6 0,\u00b4 1 60 \u00a8 epoch ` 1 \u02d9\n(8)\nExample 4.4. The topologies and the results of the simulations are summarized in Table 4 and Fig. 3.\nAs with MNIST, in the case of training on Fashion MNIST the results indicate a consistent trend across different topologies: the pruned DNNs outperform the normal DNNs in terms of accuracy on the test set, while also displaying smaller variance across multiple runs. Furthermore, the pruned DNNs consistently achieve a significant reduction in parameters by the end of the training, see Table 5.\nIn Table 5, we observe the effect of our RMT training approach on the number of parameters in our DNN with different topologies. Each topology started with a fixed number of parameters, and by the end of training, we see a significant reduction in the number of parameters across all topologies for the pruned DNN. For each topology and across all seeds, the reduction in the number of parameters was consistent, indicating the robustness of our training process in pruning the network while maintaining performance.\nRemark 4.4. Again we see that the RMT approach helps simplify the loss landscape so that during gradient decent the pruned DNN finds a deeper global minimum than the normal DNN. We can see this by looking at the accuracy of the DNNs on the training set, see Fig. 4.\nOne can also see that the pruned DNN is obtaining a deeper global minimum by looking at Table 6."
        },
        {
            "heading": "4.2.3 MP-based pruning with sparsification for fully connected DNNs on Fashion MNIST",
            "text": "We train a fully connected DNN on Fashion MNIST to achieve \u201e 89% accuracy (on the test set) with the same MP-based pruning approach as in Subsection 4.2.2 for a DNN with the topology [784, 3000,3000,3000,3000,500, 10]. At the end of training, we employ the sparsification method by setting to zero weights in the DNN smaller than the sparsification threshold \u03be. Fig. 5 shows the accuracy of the DNN vs. number of parameters kept (determined by varying \u03be). This additional sparsification leads to a large reduction in parameters, by over 99.5%, without a significant drop in accuracy (\u201e .5% drop). Assuming that the weights of the DNN which are smaller than the threshold \u03be are i.i.d.s. from a distribution with zero mean and bounded variance, Lemma 5.3 provides an explanation for why removing the small weights (sparsification) does not affect accuracy.\nAlternatively, one can prune the weight layers during training by combining the MP-based pruning approach together with sparsification (i.e. removing weights smaller than the threshold \u03be every couple of epochs). We preformed this training on the above DNN, with \u03be depending on the epoch. In our case, we initially took \u03be \u201c 0.001 and set it to grow linearly so by the end of training \u03be \u201c 0.02. We achieved a 88% accuracy, while the final DNN had 71, 331 parameters (keeping \u201e .2% parameters).\nFinally, we tried the sparsification pruning method during training without MP-based pruning. Again, we initially took \u03be \u201c 0.001 and set it to grow linearly so that by the end of training \u03be \u201c 0.02. Similar with our observations from subsection 4.2.2, the DNN plateaued at \u201e 70% accuracy, while having 128, 533 parameters at the end of training (keeping \u201e .4% parameters). Thus, we see that a combination of MP-based pruning with sparsification is useful for pruning while also increasing DNN accuracy for fully connected DNNs trained on Fashion MNIST."
        },
        {
            "heading": "4.2.4 MP-based pruning of CNNs on MNIST and Fashion MNIST",
            "text": "In our further exploration, we perform numerical simulations on Convolutional Neural Networks (CNNs) using MNIST and Fashion MNIST. In this simulation, our primary objective is to investigate the effect of pruning the small singular values of the convolutional layers. The overall goal in this example is to reduce the number of parameters in the CNN while at the same time preserving its accuracy.\nGiven the multidimensional nature of convolutional layers, direct application of singular value decomposition is not straightforward. To overcome this challenge, we first transform each convolutional layer into a 2-dimensional matrix. Specifically, for a convolutional layer with dimensions m\u02c6n\u02c6 p\u02c6 q (where m is the number of output channels, n is the number of input channels, and p\u02c6 q is the kernel size), we reshape it into a matrix of size m\u02c6 npq.\nAfter this flattening process, we proceed with the pruning as before, employing SVD to remove the smaller singular values. This step essentially compresses the convolutional layer, reducing its complexity while hopefully maintaining its representational capability.\nThe hyperparameters for the simulations in this example can be found in Subsection 10.5. The learning rate (lr) is also modified every epoch to be:\nlrn \u201c lrn\u00b41 \u02da .96 (9)\nwhere lrk is the learning rate at epoch k. Thus it decays over the learning time, see [GBC16] for more information.\nExample 4.5. In this first example, we trained a CNN on MNIST for 30 epochs with a split frequency of 13. The convolutional layers are given by r1, 64, 128, 256, 512s, and the model starts with one input\nchannel, and then each subsequent number represents the number of filters in each subsequent convolutional layer. Therefore, the model has 4 convolutional layers with filter sizes of 64, 128, 256, and 512, respectively. We apply a kernel for each layer of size 3 \u02c6 3.\nThe fully connected layers are given by r41472, 20000, 10000, 5000, 3000, 1400, 10s, we see that the model has 6 fully connected layers. The GoF parameter for the fully connected layers is .6 while the GoF parameter for the convolutional layers is .05.\nIn these numerical simulations, the portion of singular values smaller than a\n\u03bb` that we retain is given by the linear function:\nfpepochq \u201c max \u02c6 0,\u00b4 1 20 \u00a8 epoch ` 1 \u02d9\n(10)\nThe accuracy of this DNN on the training and test set is given in Fig. 6.\nIn Example 4.5, it was observed that the pruned CNN exhibited a slightly lower accuracy in comparison to the normally trained CNN. Remarkably, despite this marginal drop in performance, the pruned CNN managed to maintain this level of accuracy with approximately half of the parameters used by the normally trained CNN. While the normally trained CNN has 1, 100, 323, 974 parameters (on account of how large the fully connected layers are), the pruned CNN has 583, 670, 449 parameters.\nIn terms of performance on the training set, the normally trained CNN demonstrated an accuracy of 100%, an indicator of its potential overfitting to the training data. This is in contrast with the pruned CNN, which displayed a lower accuracy on the training set. The narrower gap between the training set and test set accuracies for the pruned CNN could be interpreted as a sign of reduced variance between the training and test set, suggesting less overfitting in the pruned model. At the same time, the fact that the normal CNN archives high accuracy on the training set suggest that the loss function in this example is simple\u2013i.e. finding the global max is simple. This might be why the pruned CNN does not outperform the normal CNN.\nIn this example, the drop in test set accuracy for the pruned CNN is dependent on the Goodness-ofFit (GoF) parameter. As the GoF parameter becomes more restrictive, the drop in accuracy becomes less pronounced. However, it is important to note that a more restrictive GoF parameter also leads to a smaller reduction in parameters. These observations suggest a delicate balance between the GoF parameter, model complexity (as indicated by the number of parameters), and model performance.\nExample 4.6. In this example, we trained a CNN on the fashion MNIST dataset for 70 epochs with a split frequency of 17. The convolutional layers are given by r1, 64, 128, 256, 512s, We again apply a kernel for each layer of size 3\u02c63. The fully connected layers are given by r41472, 10000, 5000, 5000, 10s. The GoF parameter for the fully connected layers is .7 while the GoF parameter for the convolutional layers is .15.\nIn this numerical simulation, the portion of singular values smaller than a\n\u03bb` that we retain is given by the linear function:\nfpepochq \u201c max \u02c6 0,\u00b4 1 60 \u00a8 epoch ` 1 \u02d9\n(11)\nThe accuracy of this DNN on the training and test set is given in Fig. 7.\nIn Example 4.6, the pruned CNN exhibited a slightly lower accuracy in comparison to the conventionally trained CNN. Despite this marginal drop in performance, the pruned CNN maintained this level of accuracy with approximately half of the parameters used by the conventionally trained CNN. While the normally trained CNN had 491, 381, 774 parameters, the pruned CNN utilized only 261, 891, 332 parameters.\nIn terms of performance on the training set, the conventionally trained DNN demonstrated an accuracy of approximately 99%, suggesting potential overfitting to the training data. On the other hand, the pruned DNN displayed a lower accuracy on the training set. The smaller variance between the training and test set accuracies for the pruned DNN could again be interpreted as a sign of less overfitting."
        },
        {
            "heading": "4.2.5 MP-based pruning with sparsification for CNN trained on Fashion MNIST",
            "text": "We train the CNN found in Example 4.6 to achieve \u201e 92% accuracy (on the Fashion MNIST test set) with the same MP-based pruning approach as in Subsection 4.2.4. At the end of the training, we employ sparsification by setting to zero all weights in the DNN smaller than some threshold \u03be.\nAs shown in Fig. 8a, this additional sparsification leads to a large reduction in parameters, by over 99.5%, without a significant drop in accuracy (\u201e .1% drop). As mentioned, Lemma 5.3 provides an explanation for why removing the small weights does not affect accuracy, as these weights appear to correspond to the noise in the weight layers and removing them should not change accuracy.\nFinally, we applied the sparsification pruning method after training without MP-based pruning (during training). Fig. 8b shows that the pruning threshold seems to affect the accuracy of the DNN in a much more significant manner. That is, even when pruning 95% of the parameters, the accuracy drops by multiple percentage points. We see that a combination of MP-based pruning with sparsification is useful for pruning while also ensuring the DNN accuracy does not decrease much, for CNNs trained on Fashion MNIST."
        },
        {
            "heading": "4.2.6 Numerics for training DNNs on CIFAR-10: reducing parameters via MP-based pruning",
            "text": "In this numerical simulation, we applied the RMT algorithm to prune a DNN trained on the CIFAR-10 dataset. The CIFAR-10 dataset consists of 60,000 color images spanning 10 different classes. The dataset is split into a training set and a test set. The training set contains 50,000 images, while the test set comprises 10,000 images, which is standard.\nThroughout the training process, we tracked the performance metrics of both the pruned and\nnormally trained DNNs on both the test and training sets. Our analysis showed that the pruned network, despite having a reduced number of parameters, managed to achieve performance metrics comparable to those of the normally trained network. Additionally, the pruning process significantly reduced the number of parameters in the pruned DNN, resulting in a more efficient network with a lower computational footprint. The hyperparameters for the simulations in this subsection can be found in Subsection 10.6.\nThe lr is also modified every epoch to be:\nlrn \u201c lrn\u00b41 \u02da .96 (12)\nwhere lrk is the learning rate at epoch k.\nExample 4.7. In this example, we trained a CNN on CIFAR10 for 350 epochs with a split frequency of 40. The convolutional layers are given by r3, 32, 64, 128, 256, 512s. We again apply a kernel for each layer of size 3 \u02c6 3.\nThe fully connected layers are given by r8192, 500, 10s. The GoF parameter for the fully connected layers is .08 while the GoF parameter for the convolutional layers is .06.\nIn this simulation, the portion of singular values smaller than \u03bb` that we retain is given by the linear function:\nfpepochq \u201c max \u02c6 0,\u00b4 1 200 \u00a8 epoch ` 1 \u02d9\n(13)\nThe accuracy of this DNN on the training and test set is given in Fig. 9.\nIn Example 4.2.6, it was observed that the pruned DNN exhibited a slightly lower accuracy in comparison to the normally trained DNN. Again, despite this marginal drop in performance, the pruned DNN managed to maintain this level of accuracy with much fewer parameters than was used by the normally trained DNN. While the normally trained DNN has 5, 673, 090 parameters, the pruned DNN has 3, 949, 078 parameters."
        },
        {
            "heading": "4.2.7 MP-based pruning with sparsification for CNN trained on CIFAR10",
            "text": "We train the CNN found in Example 4.7 on CIFAR10 to achieve \u201e 82% accuracy (on the test set) with the same MP-based pruning approach as in Subsection 4.2.6. At the end of training, we sparsify the DNN by setting to zero weights in the DNN smaller than some threshold \u03be.\nAs shown in Fig. 10a, this additional sparsification leads to a large reduction in parameters, by over 97%, without a significant drop in accuracy (\u201e 0 drop). Finally, we tried the sparsification pruning method after training without MP-based pruning (during training). Fig. 10b shows that the threshold pruning seems to affect the accuracy of the DNN in a much more significant manner. That is, even when pruning 80% of the parameters, the accuracy drops by \u201e 2%. We see that a combination of MP-based pruning with sparsification is useful for pruning while also ensuring the DNN accuracy does not decrease much, for CNNs trained on CIFAR-10."
        },
        {
            "heading": "5 Mathematical underpinning of numerical results",
            "text": "In this section, we introduce the Pruning Theorem, which provides the relationship between the accuracy of a DNN before and after being pruned. First, we introduce an important tool for this analysis, the classification confidence of a DNN."
        },
        {
            "heading": "5.1 The classification confidence",
            "text": "We now introduce the classification confidence, see [BJS21]. Take Xps, \u03b1q to be the output of the final layer in our DNN before softmax. The classification confidence is defined as follows:\n\u03b4Xps, \u03b1q :\u201c Xipsqps, \u03b1q \u00b4 max j\u2030ipsq Xjps, \u03b1q. (14)\nIn other words,\n\u2022 \u03b4Xps, \u03b1ptqq \u0105 0 \u00f1 s is well-classified by \u03d5.\n\u2022 \u03b4Xps, \u03b1ptqq \u0103 0 \u00f1 s is misclassified by \u03d5\nFor T 1 the test set we can now define the accuracy of the DNN on T 1 using the classification confidence,\nacc\u03b1ptq \u201c # pts P T 1 : \u03b4Xps, \u03b1ptqq \u0105 0uq\n#T 1 (15)"
        },
        {
            "heading": "5.2 How pruning affects classification confidence (for deterministic weight layer matrices)",
            "text": "Now, we state a theoretical result that shows how, at least for simple DNN models, pruning the singular values of the weight layers of a DNN impacts the DNNs accuracy. This result is not based on RMT but will help in understanding the results which follow. In the following lemma, we assume that we are given a threshold a\n\u03bb` which we use to prune the singular values of the layers of the DNN. In general, this threshold is given by the MP distribution and numerically can be found using the BEMA algorithm, see Subsection 9.1.\nLemma 5.1. Let W1,W2, \u00a8 \u00a8 \u00a8WL and \u03b21, \u03b22, \u00a8 \u00a8 \u00a8\u03b2L be the weight matrices and bias vectors of a DNN with the absolute value activation function. Assume we prune a layer matrix Wb to obtain W 1b by removing singular values of Wb smaller than a\n\u03bb`. For any input s (either from the training set T or the test set T 1), denote the change in classification confidence due to pruning as:\n\u2206p\u03b4Xq \u201c |\u03b4Xps, \u03b1Wbq \u00b4 \u03b4Xps, \u03b1W 1bq|. (16)\nHere Xps, \u03b1Wbq and Xps, \u03b1W 1bq are the outputs of the final layer before softmax of the DNN with weight layer matrices Wb and W 1b respectively.\nThen\n\u2206p\u03b4Xq \u010f a 2\u03bb`}\u03bb \u02dd pWb\u00b41 ` \u03b2b\u00b41q \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02dd pW1 ` \u03b21qs}2\u03c3maxpWb`1q . . . \u03c3maxpWLq, (17)\nSee Subsection 10.1 for a proof of this Lemma.\nRemark 5.1. For the simplified case when the bias vectors are zero, this lemma says that the change in classification confidence \u03b4X after pruning is bounded by:\n\u2206p\u03b4Xq \u010f a 2\u03bb`}\u03bb \u02ddWb\u00b41 \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02ddW1s}2\u03c3maxpWb`1q . . . \u03c3maxpWLq. (18)\nThis means that if elements were well classified before the pruning and a\n2\u03bb`}\u03bb \u02dd Wb\u00b41 \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02dd W1s}2\u03c3maxpWb`1q . . . \u03c3maxpWLq is small relative to \u03b4Xps, \u03b1W q, then after pruning s will stay accurately classified.\nA crucial observation from the lemma is the product of the maximum singular values, denoted as \u03c3maxpWb`1q . . . \u03c3maxpWLq. These singular values can be considerably large, implying that their product can amplify the magnitude of the bound (18), thereby making it substantial.\nAt first glance, this might seem concerning as it suggests that pruning might lead to a large drop in the network\u2019s accuracy. However, this is not necessarily a grave issue. Subsequent sections will introduce two more theoretical results, based on RMT, which will elucidate why, in practice, this potential drop in classification confidence due to pruning does not occur.\nFurthermore, it is crucial to note that the lemma provides a worst-case scenario. In real-world scenarios, the actual impacts of pruning are expected to be much milder than what the lemma indicates. This is a common theme in theoretical computer science and machine learning: the worst-case doesn\u2019t always reflect the average or common case.\nFurthermore, the product \u03c3maxpWb`1q . . . \u03c3maxpWLq is used as a naive bound on the Lipschitz constant of the function WL \u02dd \u03bb \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02dd Wb`1. In practice, this value can be substantially smaller. There exist other methodologies for estimating the Lipschitz constant of this function which might yield a more conservative estimate. See [FRH`19] for more on the numerical estimation of the Lipschitz constant in deep learning.\nAnother practical takeaway from this theorem is the preference to prune the final layers of the DNN rather than the earlier layers. The reasoning is simple: pruning the latter stages has a lesser effect on the overall accuracy, making it a safer bet in terms of maintaining the network\u2019s performance. However, this also depends on }\u03bb \u02dd pWb\u00b41 ` \u03b2b\u00b41q \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02dd pW1 ` \u03b21qs}2 which depends on the earlier layers in the network.\nIn essence, while the theorem paints a potentially alarming picture of pruning\u2019s effects, practical simulations and further theoretical results can assuage these concerns. The nuanced understanding provided by the theorem can guide efficient pruning strategies, ensuring minimal loss in accuracy.\nExample 5.2. The following example shows the histogram of \u03b4X of a trained DNN for the problem given in Subsection 10.2. We train a DNN with one hidden layer. The weight layer matrices W1, W2 were initialized with components taken from i.i.ds, normally distribution with zero mean and variance 1 Nl\n. We obtained a 98% accuracy on the training set, which had 1000 objects. \u03b4X of the test set, after the 600th epoch of training, is given in Fig. 11.\nFor the most part, we have }s}2 \u010f ? 2 and if we were to prune the first layer of the DNN we would\nobtain a\n2\u03bb`\u03c3maxpW2q}s}2 \u010f 6.5. However we see that for many objects s, \u03b4X can be much larger than 6.5.\nNext we would like to obtain a better result than Lemma 5.1 using the properties of random matrices."
        },
        {
            "heading": "5.3 Assumptions on the random matrix R and the deterministic matrix S",
            "text": "We considered a class of admissible matrices W , where W \u201c R`S and W , R and S satisfy the following three assumptions. The first assumption is a condition on R:\nAssumption 1: Assume R is a random N \u02c6 M matrix with entries taken from i.i.ds with zero mean and variance 1N .\nWe then assume the following for the deterministic matrix S: Assumption 2: Assume S is a deterministic matrix with S \u201c\n\u0159r i\u201c1 \u03c3iuiv T i \u201c U\u03a3V T , with \u03c3i the singular values\nand ui, vTi column and row vectors of U and V . Thus, S has r non-zero singular values corresponding to the diagonal entries of \u03a3 and all other singular values of S are zero. We also assume that these r singular values of S have multiplicity 1.\nFinally, we assume for W :\u201c R ` S: Assumption 3: Take \u03c3i to be the singular values of S, with corresponding left and right singular vectors ui and vTi and \u03c3 1 i to be the singular values of W \u201c R ` S, with corresponding left and right singular vectors u1i and v 1T i . First we assume that N M \u00d1 c P p0,`8q as N \u00d1 8. Second, assume also that we know explicit functions g\u03c3i,R, gvi,R and gui,R such that as N \u00d1 8:\n\u03c31ipW q a.s.\u00dd\u00dd\u00d1\n#\ng\u03c3i,R \u03c3i \u0105 \u03b8\u0304p\u03bb`q a\n\u03bb` \u03c3i \u0103 \u03b8\u0304p\u03bb`q, (19)\n| \u0103 u1i, ui \u0105 |2 a.s.\u00dd\u00dd\u00d1\n#\ngui,R \u03c3i \u0105 \u03b8\u0304p\u03bb`q 0 \u03c3i \u0103 \u03b8\u0304p\u03bb`q,\n(20)\nand\n|xv1i, viy|2 a.s.\u00dd\u00dd\u00d1\n#\ngvi,R \u03c3i \u0105 \u03b8\u0304p\u03bb`q 0 \u03c3i \u0103 \u03b8\u0304p\u03bb`q,\n(21)\nThird, also assume that for i \u2030 j:\n| \u0103 v1i, vj \u0105 |2 a.s.\u00dd\u00dd\u00d1 0 (22)\nand\n| \u0103 u1i, uj \u0105 |2 a.s.\u00dd\u00dd\u00d1 0. (23)\nHere we take \u03b8\u0304p\u03bb`q to be a known explicit function depending on \u03bb`, for example see (45). In the Pruning Theorem (5.4), we assume that a weight layer Wb of the DNN satisfies Assumptions 1-3,\nthat is Wb \u201c Rb ` Sb, with Rb a random matrix, Sb a low-rank deterministic matrix, and that the non-zero singular values of Sb are bigger than some threshold \u03b8\u0304p\u03bb`q. Empirically, it has been observed that these assumptions are reasonable for weight matrices of a DNN; see [TSR22, STR22]. There are various spiked models in which assumption 3 holds, for more on the subject see [BGN12, DDC22, CL22, BDWK21, OVW18a, ALP22, CCF21, BW22, Lee21, ZP20, DDC22, OVW18b]. Also, a number of works in RMT addressed the connection between a random matrix R and the singular values and singular vectors of the deformed matrix W \u201c R`S, see [BGN11, BGN12]. For example, one can show that the following two simpler assumptions on the matrices R and S are sufficient to ensure that R and S satisfy the above assumptions 1\u00b4 3. Recall that a bi-unitary invariant random matrix R is a matrix with components taken from i.i.ds such that for any two unitary matrices U and V T , the components of the matrix URV T have the same distribution as the components of R. We then assume:\nAssumption 1\u2019 (statistical isotropy): Assume R to be a bi-unitary invariant random N\u02c6M matrix with components taken from i.i.ds with zero mean and variance 1N .\nWe then assume the following for the deterministic matrix S: Assumption 2\u2019 (low rank of deterministic matrix): Assume S is a deterministic matrix with\nS \u201c \u0159r i\u201c1 \u03c3iuiv T i \u201c U\u03a3V T , with \u03c3i the singular values and ui, vTi column and row vectors of U and V . Thus, S has r non-zero singular values contained on the diagonal entries of \u03a3 and all other singular values are zero. We also assume that these r singular values of S have multiplicity 1. Finally, we assume that NM \u00d1 c P p0,`8q as N \u00d1 8.\nAn explicit relationship between assumptions 1\u2019-2\u2019 and assumptions 1-3 can be found in [BGN12]. The assumption 11 is indeed strong, as it implies that the random matrix R is random in every direction. In other words, for any unitary matrices U and V T , the matrix URV T has the same distribution as R. Random matrices with complex Gaussian entries, also known as Ginibre matrices, are a class of random matrices that are bi-unitary invariant [KT15].\nAssumptions 2 and 21 are related to the low-rank property of the deterministic matrix, see [TSR22, STR22] for how this assumption is related to DNNs. We consider the case where we initialize the weight layer of a DNN using a Gaussian random matrix (see Example 6.3) and, after training, we obtain that Wl \u201c Rl ` Sl with Rl still a Gaussian random matrix and Sl having low rank. The Pruning Theorem 5.4 can be then employed to determine that removing the small singular values of Wl will not affect much the accuracy of the DNN. This is because the deformed model Wl \u201c Rl `Sl satisfies Assumptions 1-3, see Example 6.3. This insight can be used to reduce the number of parameters in the DNN without sacrificing its performance, as will be further discussed in Subsection 9.3.\nWe now formulate theoretical results that provide a rigorous relationship between pruning and accuracy. Note that these results are applicable to DNNs with the following architecture: Consider a DNN, denoted by \u03d5, with weight layer matrices W1, \u00a8 \u00a8 \u00a8 ,Wn and the absolute values activation function. We assume the layer maps of the DNN are compositions of linear maps and activation functions; however, the results can also be adapted to the case when the DNN is a composition of affine maps composed with activation functions, that is when we add basis. The central idea in these results can be described as follows. Suppose a weight layer Wl of the DNN satisfies the above Assumptions 1, 2 and 3. Then, the removal of small singular values of Wl, smaller than the MP-based threshold a\n\u03bb`, does not change the classification confidence of an object s P Rn by a \"large amount\" (see (26) and (30)). That is, the classification confidence before pruning and after pruning are essentially the same for sufficiently large matrix Wl.\nIn essence, these results suggest that it is possible to maintain the performance of the DNN while reducing the number of parameters by eliminating the small singular values, which are considered less influential in terms of the network\u2019s overall accuracy. This insight can be used to create more efficient DNN architectures, leading to reduced computational complexity and memory requirements without sacrificing model performance, see Appendix C."
        },
        {
            "heading": "5.4 Key technical Lemma: removing random weights for DNN with arbitrary many layers does not affect classification confidence",
            "text": "First, we introduce a result based on the assumption that we can directly know what parts of the weight layer matrices are deterministic and what parts are random. For a DNN \u03d5 with weight layer matrices W1, \u00a8 \u00a8 \u00a8 ,WL we start by defining,\ng\u03d5ps, bq :\u201c }\u03bb \u02ddWb\u00b41 \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02ddW1s}2\u03c3maxpWb`1q . . . \u03c3maxpWLq, (24)\nwhere s is an element of the test or training set. The following lemma describes how the classification confidence changes when the weight layer matrix Wb \u201c Rb `Sb is changed with the weight layer matrix Sb\u2013 the ultimate pruning.\nLemma 5.3. Take \u03d5 to be a DNN with weight layer matrices W1, \u00a8 \u00a8 \u00a8 ,WL and absolute value activation function and fix object s from the test set T 1. Assume for some b that Wb \u201c Rb `Sb, with Rb a N \u02c6M random matrix satisfying Assumption 1 and matrix Sb a deterministic matrix satisfying Assumption 2 given in Subsection 5.3.\nSuppose we replace the weight layer matrix Wb with the deterministic matrix Sb. Then we have that there DDpNq, apNq, such that for the classification confidence threshold of the non-pruned DNN:\nE :\u201c apNqg\u03d5ps, bq (25)\nwe have the conditional probability\nP \u02c6 \u03b4Xps, \u03b1Sbq \u011b 0 | \u03b4Xps, \u03b1Wbq \u011b E \u02d9 \u011b 1 \u00b4DpNq, (26)\nwith DpNq, apNq \u00d1 0 as N \u00d1 8 and g\u03d5ps, bq coming from (24). Here, \u03b1Sb are the parameters of the DNN which has the weight matrix Sb and \u03b1Wb are the parameters of the DNN with the weight layer matrix Wb.\nHere, apNq \u201c 2 N 3 4 . The proof for this lemma can be found in Subsection 8.1.\nRemark 5.2. Here, we take s from the test set T 1; however, the result also holds if we take s from the training set T . Furthermore, using the proof given in Subsection 8.1, one can show a more general result. That is, taking\n\u2206p\u03b4Xq :\u201c |\u03b4Xps, \u03b1Sbq \u00b4 \u03b4Xps, \u03b1Wbq|, (27)\nwe have that P \u02c6 \u2206p\u03b4Xq \u010f E \u02d9 \u011b 1 \u00b4DpNq. (28)\nRemark 5.3. Lemma 5.3 addresses the removal of parameters while preserving accuracy in a more general context than MP-based pruning. In particular, it also explains the numerics of parameter removal via sparsification (see Section 4). Indeed, in Lemma 5.3, the random matrix Rb has entries taken from i.i.ds with zero mean and variance 1N . Therefore, as N \u00d1 8 the entries of Rb are small w.r.t. sparsification threshold \u03bepNq. If, in addition, we assume that the entries of Sb are large (c.f. assumption in Theorem 5.4), then large entries of Wb \u201c Rb ` Sb are entries of Sb with high probability. Therefore, sparsifying Wb by removing the entries smaller than the \u03bepNq amounts to replacing the weight layer matrix Wb with the deterministic matrix Sb. Therefore, Lemma 5.3 implies that sparsification preserves accuracy in the sense of (26).\nRemark 5.4. Imagine you are trying to predict the weather. Initially, your prediction is based on both the randomized patterns you have observed over time (noise in input layer) and your initial random\nweights of the DNN (matrix R) and the deterministic factors you are sure of (matrix S). Now, if you decide to base your prediction just on the deterministic factors (that is, totally remove the random part of the weight layer matrix). Then how much would your confidence in the prediction change? This Lemma provides a bound on that change.\nThe Lemma states that there exists a function DpNq \u00d1 0 as N \u00d1 8 (the size of the matrix) increases. The magnitude of this change in classification confidence (how much our \"confidence\" drops when we remove the random part) is given by E, which is related to the combined affects of all layers up to b and the maximum scaling factors (or singular values) of layers after b.\nMost importantly, the conditional probability states that if our initial confidence (with the random matrix) was bounded away by E \u0105 0, then after removing the randomness, our confidence would most likely be at least 0. And as the size N of the matrix increases, the probability that our classification confidence would be bigger than 0 becomes closer to 1.\nIn essence, even if we remove the randomness from our prediction model (in this case, the DNN), we can still be quite confident about our predictions, especially as our layer widths grow.\nRemark 5.5. We would like to understand what happens to the classification confidence threshold E in (26) as N \u00d1 8. Assuming that\n}\u03bb \u02ddWb\u00b41 \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02ddW1s}2\u03c3maxpWb`1q . . . \u03c3maxpWLq \u010f C (29)\nfor all N , then E \u00d1 0 as N \u00d1 8. This is because apNq \u201c 2 N 3 4 , goes to zero as N increases. Consequently, the contribution from apNq becomes negligible, implying that the effect of dropping the random matrix and only keeping the deterministic matrix becomes inconsequential. Recall that \u03c3maxpWb`1q . . . \u03c3maxpWLq is a theoretical bound on the Lipschitz constant of Wb`1 \u00a8 \u00a8 \u00a8WL, but numerically one might be able to obtain a better bound, see e.g, [FRH`19]. Here, we call the Lipschitz constant of a matrix A the Lipschitz constant of the linear map corresponding to that matrix, and this definition extends to the product of matrices."
        },
        {
            "heading": "5.5 Pruning Theorem for DNN with arbitrary many layers: how pruning random weights using PM distribution affects the classification confidence",
            "text": "Theorem 5.4 (The Pruning Theorem for a single object). Take \u03d5 to be a DNN with weight layer matrices W1, \u00a8 \u00a8 \u00a8 ,Wn and absolute value activation functions and take some s P T 1, with T 1 the test set. Assume for some b that Wb \u201c Rb ` Sb, with Rb a N \u02c6 M random matrix satisfying assumption 1, matrix Sb a deterministic matrix satisfying assumption 2 and Rb ` Sb satisfying assumption 3, for the assumptions given in Subsection 5.3. Further, assume that all the non-zero singular values of S are bigger than \u03b8\u0304p\u03bb`q with \u03bb` given by the MP distribution of the ESD of RTR as N \u00d1 8 and \u03b8\u0304p\u03bb`q given in (19).\nConstruct the truncated matrix W 1b by pruning the singular values of Wb smaller than a\n\u03bb`. Then we have that there exists an explicit function fWb \u0105 0 such that @\u03f5, DC\u03f5pNq so that for the classification confidence threshold of the non-pruned DNN:\nE1 :\u201c p1 ` \u03f5qp ? 2p1 ` \u03f5qmintfWb , a \u03bb`u ` apNqqg\u03d5ps, bq (30)\nwe have the conditional probability\nP \u02c6 \u03b4Xps, \u03b1W 1bq \u011b 0 | \u03b4Xps, \u03b1Wbq \u011b E 1 \u02d9 \u011b 1 \u00b4 C\u03f5pNq, (31)\nwith C\u03f5pNq, apNq \u00d1 0 as N \u00d1 8 and g\u03d5ps, bq coming from (24). fWb is given in Lemma 7.3. Also, \u03b1Wb are the parameters of the DNN that has the weight matrix Wb and \u03b1W 1b are the parameters of the DNN with the weight layer matrix W 1b.\nSee Subsection 8.2 for a proof of this theorem.\nRemark 5.6. Here we take s from the test set T 1; however, the result also holds if we take s from the training set T . Furthermore, using the proof given in Subsection 8.2, one can show a more general result. That is, taking\n\u2206p\u03b4Xq :\u201c |\u03b4Xps, \u03b1W 1bq \u00b4 \u03b4Xps, \u03b1Wbq|, (32)\nwe have that P \u02c6 \u2206p\u03b4Xq \u010f E1 \u02d9 \u011b 1 \u00b4 C\u03f5pNq. (33)\nPruning Theorem 5.4 shows that if we replace the matrix Wb with a truncated matrix W 1b, then for any given object s P T 1, we have that if the classification confidence \u03b4Xp\u03b1, sq is positive enough for matrix Wb, it stays positive for the truncated matrix W 1b with high probability. In other words, almost all well-classified objects remain well-classified after replacing Wb with W 1b. We also show numerically that it is easier to prevent overfitting using matrix W 1b instead of the larger matrix Wb. We verified that removing small singular values based on the MP-based threshold a\n\u03bb` for the case when the weight matrices Wb were initialized with Np0, 1N q does not reduce the accuracy of the DNN; see Example 9.3. Here, fW \u201c }W 1 \u00b4 S} and for a large class of RMT matrix models (see assumptions 1-3 in Subsection 5.3), for the case N \u00d1 8, we obtain fW based on the singular values of W only. Remark 5.7. The assumptions made in Theorem 5.4 are quite natural and hold for a wide range of DNN architectures. Assumption 1 focuses on the random matrix R. This assumption ensures that the random matrix R captures the essential randomness in the weight layer while also satisfying the requirements given in Theorem 4.2.\nAssumption 2 pertains to the deterministic matrix S, which is assumed to have a specific structure, with r non-zero singular values and all other singular values being zero. Moreover, these r singular values have multiplicity 1, which is a reasonable expectation for a deterministic matrix that contributes to the information content in the layer Wb. Assumption 3 holds for many spiked models and has been studied in much detail.\nThe assumption that the singular values of the deterministic matrix S are larger than some \u03b8\u0304p\u03bb`q is also quite natural, see [TSR22, STR22]. This is because the deterministic matrix S represents the information contained in the weight layer, and its singular values are expected to be large, reflecting the importance of these components in the overall performance of the DNN. On the other hand, the random matrix R captures the inherent randomness in the weight layer, and with high probability depending on N , its singular values should be smaller than the MP-based threshold. This means that there is a clear boundary between the information and noise in the layer Wb which is also natural, see [STR22].\nThis distinction between the singular values of S and R highlights the separation between the information and noise in the weight layer, allowing us to effectively remove the small singular values without impacting the accuracy of the DNN. The assumption thus provides a solid basis for studying the behavior of DNNs with weight layers modeled as spiked models. It contributes to our understanding of the effects of removing small singular values based on the random matrix theory MP-based threshold a\n\u03bb`.\nRemark 5.8. In our work, we leverage the Marchenko-Pastur distribution to select significant singular values for the low-rank approximation of our weight layers Wl. Other low-rank approximation techniques, such as the bootstrapping technique proposed in [NSU19], could potentially be integrated with the Marchenko-Pastur distribution to further refine the low-rank approximation of Wl."
        },
        {
            "heading": "5.6 Simple example of DNN with one hidden layer",
            "text": "The following is a simple example of the Pruning Theorem:\nExample 5.5. Take \u03d5 to be a DNN with three weight layer matrices W1,W2,W3 and the absolute value activation function and take s P T 1. This is:\n\u03d5p\u03b1, sq \u201c \u03bb \u02ddW3 \u02dd \u03bb \u02ddW2 \u02dd \u03bb \u02ddW1s, s P Rn. (34)\nAssume W2 \u201c S2 ` R2 satisfies Assumptions 1-3 and W1, W3 are arbitrary. More specifically, assume R2 to be a random matrix with i.d.ds taken from the distribution Np0, 1N q and S to be a N \u02c6N deterministic matrix with non-zero singular values bigger than 1.\nTake W 1 to be the same as W but with all the singular values of W smaller than 2 set to zero. Then for fW the positive function given in (36) we have that @\u03f5 \u0105 0:\nP \u02c6 \u03b4Xps, \u03b1W 1q \u011b 0 | \u03b4Xps, \u03b1W q \u011b p1 ` \u03f5q ? 2papNq ` fW2q}W1s}2\u03c3maxpW3q \u02d9 \u011b 1 \u00b4 C\u03f5pNq, (35)\nwith C\u03f5pNq, apNq \u00d1 0 as N \u00d1 8. Here we have that for \u03c3r the smallest non-zero singular value of S:\nfW \u201c max 1\u010fi\u010fr\n$\n&\n%\ng f f e d\n1 \u00b4 1 \u03c32i\n\u02dc\n\u03c32i ` \u02c6 1 ` \u03c32i \u03c3i\n\u02d92 \u00b8\n\u00b4 p1 ` \u03c32i q \u02c6 2 \u00b4 1 \u03c32i \u02d9\nd\n1 \u00b4 1 \u03c32i\n,\n.\n-\n. (36)\nNote that as \u03c3r \u00d1 8 we have fW \u00d1 1. In fact, we show numerically that already for \u03c3r \u011b 5 we have |fW \u00b4 1| \u010f .03, see Fig 12.\nNote that this estimate is given in terms of the singular values of S (which are \u03c3i), however, the singular values of S might not be known. Nevertheless, by Theorem 6.3 we have that as N \u00d1 8 the singular values of S can be obtained directly from the singular values of W via \u03c31i \u201c 1`\u03c32i \u03c31\n. Thus, as N \u00d1 8 this estimate can be obtained in terms of singular values of W only, which is why we use the notation fW and not fS. For simplicity, we keep using the current notation.\nWe numerically checked that for R a 3000\u02c63000 random matrix initialized with the above Gaussian distribution, and for S a diagonal matrix with 5 non-zero singular values given by 30, 40, 50, 60, 70, we have }S \u00b4 W 1}2 \u00ab fW \u00ab 1, see Fig. 13. Thus, in this example fW \u0103 a \u03bb`, given that a\n\u03bb` \u201c 2, and so in (86) we would have mintfWb , a\n\u03bb`u \u00ab 1. Thus, Theorem 5.4 provides a better result than Lemma 5.1. For more on this example, see Subsection 7.1.\nIt is an important question: under what conditions of R and S would we have that fW \u201c c such that c \u0103 a \u03bb`.\nExample 5.6. Consider R to be an n \u02c6 n symmetric (or Hermitian) matrix with independent, zero mean, normally distributed entries. The variance of the entries is \u03c3 2\nn on the diagonal and \u03c32\n2n on the off-diagonal.\nIn the setting where S \u201c \u0159r i\u201c1 \u03c3iuiu T i , let u 1 i be the unit eigenvectors of W \u201c R` S associated with\nits r largest eigenvalues. Assuming that for all 1 \u010f i \u010f r we have \u03c3i \u0105 \u03c3, then:\nfW \u201c max 1\u010fi\u010fr\ng f f e d\n1 \u00b4 \u03c3 2\n\u03c32i\n\u02dc\n\u03c32i ` \u02c6 \u03c3i ` \u03c32\n\u03c3i\n\u02d92 \u00b8\n\u00b4 \u03c3i\n\u02dc\n\u02c6\n\u03c3i ` \u03c32\n\u03c3i\n\u02d9 \u02c6\n2 \u00b4 \u03c3 2\n\u03c32i\n\u02d9\n\u02dc d\n1 \u00b4 \u03c3 2\n\u03c32i\n\u00b8\u00b8\n. (37)\nIn this example, a\n\u03bb` \u201c 2\u03c3. Thus, in Fig. 14 we compare fW vs 2\u03c3 for the case when the singular values of S are 70, 80, 90, 100, 110 (i.e. r \u201c 5). As mentioned, it would be interesting to try and find a probability distribution which if R is initialized with would result in a very small fW for reasonable assumptions on the singular values of S.\nSee Subsection 7.2 for more information."
        },
        {
            "heading": "5.7 Pruning Theorem for accuracy: how pruning affects accuracy",
            "text": "In this subsection, we present a version of the Pruning Theorem for accuracy, which describes how the accuracy of a DNN is affected by pruning. We present this Theorem for DNNs with one hidden layer. However, it can be generalized for DNNs with more layers.\nWe first recall the notion of the good set of a DNN, introduced in [BJS21]. The good set is a subset of the test set T 1 defined as follows: for \u03b7 \u011b 0, the good set of margin \u03b7 at time t is\nG\u03b7ptq,\u03b1 :\u201c ts P T 1 : \u03b4Xps, \u03b1ptqq \u0105 \u03b7u. (38)\nBasically, the good set consists of positively classified objects whose classification confidence is bounded below by \u03b7. We next formulate the Pruning Theorem for accuracy. Loosely speaking, it says\nthat for some threshold Eacc (see (43)), we have that the accuracy of the DNN after pruning is bounded from below by the number\n|GEacc,\u03b1| |T 1| , (39)\nwhere for a finite set A we have that |A| is the number of elements in that set.\nTheorem 5.7 (Pruning Theorem for accuracy). Let \u03d5 be a DNN with weight layer matrices W1,W2,W3, and \u03bb the absolute value activation function:\n\u03d5p\u03b1, sq \u201c \u03bb \u02ddW3 \u02dd \u03bb \u02ddW2 \u02dd \u03bb \u02ddW1s, s P Rn. (40) Assume W2 \u201c S2 ` R2 satisfies Assumptions 1-3 in Subsection 5.3, and W1, W3 are arbitrary\nmatrices. Construct the truncated matrix W 12 by pruning singular values of W2 smaller than a\n\u03bb`. For every \u03f5, introduce the classification confidence threshold for the non-pruned DNN as the smallest number Eacc \u011b 0 for which we satisfy:\nEacc \u201c p1 ` \u03f5qp ? 2pfW2 ` 2N\u00b4 3 4 qq max\nsPGEacc,\u03b1 }W1s}2\u03c3maxpW3q ppositiveq, (41)\nwith fW2 \u0105 0 an explicit rational function of W2. Then we have @\u03f5:\nP \u02c6 acc\u03b11ptq \u011b |GEacc,\u03b1|\n|T 1|\n\u02d9\n\u011b p1 \u00b4 C\u03f5pNqq|GEacc\u03b1| (42)\nwith C\u03f5pNq \u00d1 0 as N \u00d1 8. Here \u03b1, \u03b11 are the parameters of the non-pruned and pruned DNNs, respectively and acc\u03b11ptq is given in (15).\nSee Subsection 8.3 for a proof of this Theorem.\nRemark 5.9. One can always find a Eacc \u011b 0 which satisfies\nEacc \u201c p1 ` \u03f5qp ? 2pfW2 ` 2N\u00b4 3 4 qq max\nsPGEacc,\u03b1 }W1s}2\u03c3maxpW3q ppositiveq. (43)\nThis is because 0 \u010f p1 ` \u03f5qp ? 2pfW2 ` 2N\u00b4 3 4 qqmaxsPG0,\u03b1 }W1s}2\u03c3maxpW3q. Finally, for large enough \u03b7, we have p1 ` \u03f5qp ? 2pfW2 ` 2N\u00b4 3 4 qqmaxsPG\u03b7,\u03b1 }W1s}2\u03c3maxpW3q \u201c 0."
        },
        {
            "heading": "6 Appendix A: Some known results on perturbation of matrices",
            "text": "Matrix perturbation theory is concerned with understanding how small changes in a matrix can affect its properties, such as eigenvalues, eigenvectors, and singular values. In this section, we state a couple of known results from matrix perturbation theory."
        },
        {
            "heading": "6.1 Asymptotics of singular values and singular vectors of deformation matrix",
            "text": "The results in this subsection are taken from [BGN12]. Given the assumptions 1\u2019-2\u2019 on R and S described in Section 5.3, the authors were able to show that the largest eigenvalues and corresponding eigenvectors of W \u201c S `R are well approximated by the largest eigenvalues and eigenvectors of S.\nWe start by defining the following function:\nD\u00b5Rpzq \u201c \u201e \u017c\nz z2 \u00b4 t2d\u00b5Rptq \u0237 \u02c6 \u201e c \u017c\nz z2 \u00b4 t2d\u00b5Rptq ` 1 \u00b4 c z\n\u0237\n(44)\nfor z \u0105 a \u03bb`, with \u03bb` given by the MP distribution of RTR. Take D\u00b41\u00b5Rp\u00a8q to be its functional inverse.\nSet \u03b8\u0304 \u201c D\u00b5Rp a \u03bb`q\u00b4 1 2 (45)\nTheorem 6.1. Theorem for large singular values [Benaych-Georges and Nadakuditi (2012)] Take W \u201c R ` S, with W,R and S all N \u02c6 M matrices satisfying assumptions 11 \u00b4 21. The r largest singular values of W , denoted as \u03c31ipW q for 1 \u010f i \u010f r, exhibit the following behaviour as N \u00d1 8:\n\u03c31ipW q a.s.\u00dd\u00dd\u00d1\n#\nD\u00b41\u00b5Rp 1 p\u03c3iq2 q \u03c3i \u0105 \u03b8\u0304 a\n\u03bb` \u03c3i \u0103 \u03b8\u0304 (46)\nTheorem 6.2. Norm of projection of largest singular vectors [Benaych-Georges and Nadakuditi (2012)] Take indices i0 P t1, ..., ru such that \u03c3i0 \u0105 \u03b8\u0304. Take \u03c31i0 \u201c \u03c3 1 i0\npW q and let u1, v1 be left and right unit singular vectors of W associated with the singular value \u03c31i0 and u, v be the corresponding singular vectors of S. Then we have, as N \u00d1 8:\n| \u0103 u1,Spantui s.t. \u03c3i \u201c \u03c3i0u \u0105 |2 a.s.\u00dd\u00dd\u00d1 \u00b42\u03d5\u00b5Rp\u03c1q\n\u03c32i0D 1 \u00b5R\np\u03c1q (47)\nand\n| \u0103 v1, Spantvi s.t. \u03c3i \u201c \u03c3i0u \u0105 |2 a.s.\u00dd\u00dd\u00d1 \u00b42\u03d5\u00b5Rp\u03c1q\n\u03c32i0D 1 \u00b5\u0303R\np\u03c1q (48)\nHere \u03c1 \u201c D\u00b41\u00b5Rp 1 p\u03c3i0 q2 q and \u00b5\u0303R \u201c c\u00b5R ` p1 ` cq\u03b40. Further,\n| \u0103 u1,Spantui s.t. \u03c3i \u2030 \u03c3i0u \u0105 |2 a.s.\u00dd\u00dd\u00d1 0 (49)\n| \u0103 v1, Spantvi s.t. \u03c3i \u2030 \u03c3i0u \u0105 |2 a.s.\u00dd\u00dd\u00d1 0 (50)\nExample 6.3. Take S \u201c \u0159r i\u201c1 \u03c3iuiv T i to be a N \u02c6N deterministic matrix, with \u03c3i the singular values and vi and ui the singular vectors of S. Take R to be a N\u02c6N random matrix with real i.i.d components taken from normal distribution Np0, 1N q. For W \u201c R ` S we have:\nTheorem 6.4. (Theorem for large singular values for Example 6.3) The r largest singular values of W , denoted \u03c31ipW q for 1 \u010f i \u010f r, exhibit the following behaviour as N \u00d1 8:\n\u03c31ipW q a.s.\u00dd\u00dd\u00d1\n#\n1`\u03c32i \u03c3i\n\u03c3i \u0105 1 2 \u03c3i \u0103 1\nTheorem 6.5. (Theorem for large singular vectors for Example 6.3) Assuming that the r largest singular values of W have multiplicity 1, then the right and left singular vectors u1i, v 1 i of W corresponding with the r largest singular values \u03c31ipW q exhibits the following behaviour as N \u00d1 8:\n| \u0103 vi, v1i \u0105 |2, | \u0103 ui, u1i \u0105 |2 a.s.\u00dd\u00dd\u00d1\n#\np1 \u00b4 1 \u03c32i q \u03c3i \u0105 1 0 \u03c3i \u0103 1\nRemark 6.1. We say that Xn \u00d1 X in probability if @\u03f5 Pp|Xn \u00b4 X| \u0105 \u03f5q \u00d1 0 as n \u00d1 8. One can show that Xn \u00d1 X a.s. implies that Xn \u00d1 X in law. Thus for Theorems 6.1 and 6.2 we have that if the r large singular values of S are bigger than 1 and have multiplicity 1 then there exists some constant BN p\u03f5q with BN p\u03f5q \u00d1 0 as N \u00d1 8 such that for the r largest singular values of r and their corresponding singular vectors and @\u03f5 we have:\nPp|\u03c31ipW q \u00b4 1 ` \u03c32i \u03c3i | \u0105 \u03f5q \u0103 BN p\u03f5q, (51)\nPp|| \u0103 vi, v1i \u0105 |2 \u00b4 p1 \u00b4 1\n\u03c32i q| \u0105 \u03f5q \u0103 BN p\u03f5q (52)\nand\nPp|| \u0103 ui, u1i \u0105 |2 \u00b4 p1 \u00b4 1\n\u03c32i q| \u0105 \u03f5q \u0103 BN p\u03f5q. (53)\nFinally, we also have that\nPp| \u0103 ui, u1j \u0105 | \u0105 \u03f5q \u0103 BN p\u03f5q. (54)\nand\nPp| \u0103 vi, v1j \u0105 | \u0105 \u03f5q \u0103 BN p\u03f5q. (55)\nfor i \u2030 j."
        },
        {
            "heading": "6.2 Gershgorin\u2019s Circle Theorem",
            "text": "Finally we state Gershgorin\u2019s Circle Theorem:\nTheorem 6.6 (Gershgorin\u2019s Circle Theorem). Let B \u201c rbijs be an n \u02c6 n complex matrix. Define the Gershgorin discs Di for 1 \u010f i \u010f n as\nDi \u201c # z P C : |z \u00b4 bii| \u010f \u00ff\nj\u2030i |bij |\n+\n. (56)\nThen, every eigenvalue \u03bb of the matrix B lies within at least one of the Gershgorin discs Di.\nRemark 6.2. We apply Theorem 6.6 for almost diagonal matrices when (56) estimates how close the eigenvalues are to the diagonal elements. This closeness is estimated in terms of the magnitude of the non-diagonal elements."
        },
        {
            "heading": "7 Appendix B: An Approximation Lemma \u2013 pruned matrix W 1 approximates the deterministic matrix S",
            "text": "Assume we are given a deterministic matrix S and we add to it a random matrix R, for the R and S given in Example 6.3. Suppose we take W \u201c S ` R, it is well known that one can find a rank k approximation of W . This is done by taking the SVD of W \u201c U\u03a3V T and setting all but the top k singular values in \u03a3 to zero. The following is a known theorem of this result:\nTheorem 7.1. Given the singular value decomposition (SVD) of W \u201c U\u03a3V T , where U and V are unitary matrices and \u03a3 is a diagonal matrix containing the singular values of W , the rank k approximation of W is given by:\nW\u0303k \u201c Uk\u03a3kV Tk (57)\nwhere Uk and Vk are the matrices obtained by retaining only the first k columns of U and V , respectively, and \u03a3k is the matrix obtained by retaining only the first k diagonal entries of \u03a3. Here we use W\u0303k to distinguish it from the weight layer matrix Wk. This approximation represents the best rank k approximation to W in the following sense:\nW\u0303k \u201c minrankpXq\u201ck }W \u00b4X}F (58)\nwhere X is an arbitrary matrix of rank k and } \u00a8 }F is the Frobenius norm.\nSee [Dem86] for more on this result. In this section, we wish to obtain a slightly different result in a similar direction. We wish to show that for the R and S given in Example 6.3, W\u0303r is a good approximation of S (recall S has r non-zero singular values). That means that Wr is a good approximation of the deterministic part of W . We, therefore, state a lemma that shows that:\n}pW\u0303r \u00b4 Sqz} \u0103 fW }z}. (59)\nRather than state this lemma in terms of W\u0303r, we state them in terms of W 1, which is defined as follows:\nDefinition 7.2. Take W \u201c R ` S, with R and S given in assumptions 1 \u00b4 3 from Section 5.3. Take W \u201c U\u03a3V T to be the SVD of W and take\n\u03a31i,j \u201c # \u03a3i,j \u03a3i,j \u0105 a \u03bb`\n0 \u03a3i,j \u010f a \u03bb` (60)\nThen we obtain the truncated matrix W 1 by taking W 1 \u201c U\u03a31V T .\nNext we present a approximation lemma which gives a bound for }pW 1 \u00b4 Sqz}2, where z is any vector. This lemma describes how well S is being approximated by W 1.\nApproximation Lemma 7.3. Assume that W \u201c R ` S, with R a N \u02c6 M random matrix and S a deterministic matrix satisfying assumptions 1\u00b4 3, for the assumptions in Section 5.3. Assume that the singular values of S are bigger than \u03b8\u0304, given in assumption 3, and all singular values have multiplicity one. Take z to be any vector in Rn. Then for W 1 given in Def. 7.2 we have that DfW \u0105 0, DB\u02daN p\u03f5q such that @\u03f5 \u0105 0:\nP \u02c6 }pW 1 \u00b4 Sqz}2 \u011b p1 ` \u03f5qfW }z}2 \u02d9 \u0103 B\u02daN p\u03f5q, (61)\nwith B\u02daN p\u03f5q \u00d1 0 as N \u00d1 8. Moreover, assuming that \u03c3r is the smallest singular values of S, we have:\nfW \u201c max 1\u010fi\u010fr\nb? gvipg2\u03c3i ` \u03c3 2 i q \u00b4 g\u03c3i\u03c3ip1 ` gviq ? gui , (62)\nwith g\u03c3i , gui and gvi given in assumption 3.\nRemark 7.1. Here fW depends on the distribution of the eigenvalues of R and on the eigenvalues of S.\nExample 7.4. Assume that W is the matrix given in Example 6.3. Then\nfW \u201c max 1\u010fi\u010fr\n$\n&\n%\ng f f e d\n1 \u00b4 1 \u03c32i\n\u02dc\n\u03c32i ` \u02c6 1 ` \u03c32i \u03c3i\n\u02d92 \u00b8\n\u00b4 p1 ` \u03c32i q \u02c6 2 \u00b4 1 \u03c32i \u02d9\nd\n1 \u00b4 1 \u03c32i\n,\n.\n-\n. (63)\nExample 7.5. Assume that W \u201c R ` S, with R a N \u02c6 M random matrix satisfying assumptions 11 and S a deterministic matrix satisfying assumption 21, for the assumptions in Section 5.3. Assume that the singular values of S are bigger than \u03b8\u0304, given in (45), and all singular values have multiplicity one. Then for W 1 given in Def. 7.2 we have that:\nfW \u201c max 1\u010fi\u010fr\ng f f e d\n\u00b42\u03d5\u00b5Rp\u03c1q \u03c32rD 1 \u00b5\u0303R p\u03c1qpD \u00b41 \u00b5Rpp 1 p\u03c3iq2 q2 ` \u03c32i qq \u00b4 \u03c3iD \u00b41 \u00b5Rp 1 p\u03c3iq2 qp1 ` \u00b42\u03d5\u00b5Rp\u03c1q \u03c32rD 1 \u00b5\u0303R p\u03c1qq\nd\n\u00b42\u03d5\u00b5Rp\u03c1q \u03c32rD 1 \u00b5R p\u03c1q .\nProof. We prove this for the simple case when S \u201c \u0159r i\u201c1 \u03c3iuiv T i and for the example given in 6.3. The proof for the more general case is the same. Take W 1 \u201c UW\u03a31WV TW and S \u201c US\u03a31SV TS to be the SVD of W 1 and S, with \u03a31S a r\u02c6 r matrix, and assume the smallest singular value of S, which is \u03c3r, is bigger than 1. Then we have\n}pW 1 \u00b4 Sqz}2 \u201c }pUW\u03a31WV TW \u00b4 US\u03a31SV TS qz}\n\u010f b \u03bbmaxppUW\u03a31WV TW \u00b4 US\u03a31SV TS qT pUW\u03a31WV TW \u00b4 US\u03a31SV TS qq}z}2, (64)\nwith \u03bbmaxpAq the largest eigenvalue of A. Thus we obtain\n}pW 1 \u00b4 Sqz}2 \u010f b\n\u03bbmaxppVW\u03a312WV TW ` VS\u03a312SV TS \u00b4 VW\u03a31WUTWUS\u03a31SV TS \u00b4 VS\u03a31SUTS UW\u03a31WV TW qq}z}2. (65)\nWe can multiply the right and left of the matrix pVW\u03a312WV TW ` VS\u03a312SV TS \u00b4 VW\u03a31WUTWUS\u03a31SV TS \u00b4 VS\u03a3 1 SU T S UW\u03a3 1 WV T W q by VW and V TS respectively, without changing the eigenvalue, to obtain:\n}pW 1 \u00b4 Sqz}2 \u010f b\n\u03bbmaxppV TS VW\u03a312W ` \u03a312SV TS VW \u00b4 V TS VW\u03a31WUTWUS\u03a31SV TS VW \u00b4 \u03a31SUTS UW\u03a31W qq}z}2. (66)\nBy (55), (54) and Theorem 6.6, since the sum of the off diagonal elements of\nG :\u201c pV TS VW\u03a312W ` \u03a312SV TS VW \u00b4 V TS VW\u03a31WUTWUS\u03a31SV TS VW \u00b4 \u03a31SUTS UW\u03a31W qq (67)\ncan be made arbitrarily small with high probability as N \u00d1 8, DB\u02daN p\u03f5q so that for large enough N we have:\nP \u02c6 |\u03bbmaxpGq \u00b4 p1 ` \u03f5q\u03bbmaxpD2p\u03a312W ` \u03a312S q \u00b4 \u03a31W\u03a31Spp1 `D22qD1qq| \u0105 0 \u02d9 \u0103 B\u02daN p\u03f5q, (68)\nwhere D1 is the diagonal matrix containing \u0103 ui, u\u0303i \u0105 on its diagonal and all other elements zero and D2 the diagonal matrix containing \u0103 vi, v\u0303i \u0105 on its diagonal and all other elements zero. In fact, because S only has r singular values, we obtain less than r \u02c6 r non-zero off-diagonal elements for the matrices UTWUS , U T S UW , V T WVS and V T S VS . Thus, because r is fixed we have by Theorems 6.4, 6.5 and Remark 6.1, that DB\u02daN p\u03f5q such that for large enough N :\nP \u02c6 \u203a \u203a pW 1 \u00b4 Sqz \u203a \u203a\n2 \u011b p1 ` \u03f5q max\n1\u010fi\u010fr\n\"\ng f f f f f f f f e d 1 \u00b4 1 \u03c32i \u02dc \u03c32i ` \u02c6 1 ` \u03c32i \u03c3i \u02d92 \u00b8\n\u00b4 p1 ` \u03c32i q \u02c6 2 \u00b4 1 \u03c32i \u02d9\nd\n1 \u00b4 1 \u03c32i\n}z}2 *\u02d9 \u010f B\u02daN p\u03f5q. (69)\nIn fact, using the above argument we can show that as N \u00d1 8:\n}pW 1 \u00b4 Sq} a.s.\u00dd\u00dd\u00d1 max 1\u010fi\u010fr\n$\n&\n%\ng f f e d\n1 \u00b4 1 \u03c32i\n\u02dc\n\u03c32i ` \u02c6 1 ` \u03c32i \u03c3i\n\u02d92 \u00b8\n\u00b4 p1 ` \u03c32i q \u02c6 2 \u00b4 1 \u03c32i \u02d9\nd\n1 \u00b4 1 \u03c32i\n,\n.\n-\n. (70)\nThis completes the proof."
        },
        {
            "heading": "7.1 Numerics for Example 5.5",
            "text": "In the following subsection we provide a figure of the dot products of the 5 left and right singular values for the matrices W \u201c R ` S and S described in Example 5.5, see Fig 15. As mentioned earlier, the 5 singular values of S were 30, 40, 50, 60, 70. We see that the dot product of the left and right singular vectors of W and S can be approximated almost perfectly by the equation b\n1 \u00b4 1 \u03c32i , see (51). That is,\nfor \u03c35 \u201c 30, we have b\n1 \u00b4 1 \u03c32i \u00ab 0.99944 and indeed x\u0103 u5, u15 \u0105\u00ab .0.99943 and similarly for the other dot products."
        },
        {
            "heading": "7.2 Details for Example 5.6",
            "text": "The details provided in the subsection were taken from [BGN11]. Under the setting given in Example 5.6, let u1i be a unit-norm eigenvectors of R ` S associated with its r largest eigenvalues. We have for 1 \u010f i \u010f r,\n\u03bbipR ` Sq a.s.\u00dd\u00d1\n#\n\u03c3i ` \u03c3 2\n\u03c3i if \u03c3i \u0105 \u03c3,\n2\u03c3 otherwise,\nas n \u00d1 8. We also have\n|xu, u1y|2 a.s.\u00dd\u00d1 # 1 \u00b4 \u03c32 \u03c32i if \u03c3i \u0105 \u03c3, 0 otherwise."
        },
        {
            "heading": "8 Appendix C: Proof for Pruning Theorem",
            "text": "The proof of the Pruning Theorem 5.4 consists of two steps. First, we show how classification confidence changes when the weight matrix Wb is replaced with its deterministic part Sb. Second, we approximate the deterministic matrix Sb with the pruned matrix W 1b, keeping track of the corresponding change in the classification confidence. The key idea in the second step is to use asymptotics of the spectrum of deformed matrices W \u201c R`S with finite number of singular values of S, see [BGN12], in combination with GCT."
        },
        {
            "heading": "8.1 Proof for key technical Lemma 5.3",
            "text": "Proof. We first provide a proof for the case when our DNN has only one layer matrix W . We proceed by showing that if }s}2 is independent of N then as N \u00d1 8, \u03b4Xp\u03b1W , sq\u00b4\u03b4Xp\u03b1S , sq \u201c 0 in probability, with \u03b1W the parameters of the DNN with weight layer W and \u03b1S the parameters of the same DNN but with weight layer S. This means that the random matrix R does not improve the accuracy of the DNN as N \u00d1 8.\nMore generally, we also show that:\nP \u02c6 \u03b4Xps, \u03b1Sq \u011b 0 | \u03b4Xps, \u03b1W q \u011b apNqq}s}2 \u02d9 \u011b 1 \u00b4 1 N 1 2 , (71)\nwith apNq \u201c 2 N 3 4 .\nWe start by approximating the components of Rs and showing that they are small and go to 0 as N \u00d1 8. When R is a random matrix with components taken from i.i.d with mean 0 variance 1 N , we have that pRsqk is a random variable taken from a distribution with mean 0 and variance \u03c32pRsqk \u201c \u0159M i\u201c1 s 2 i 1 N \u201c }s} 2 2 1 N . Thus, pRsqk is also a random variable with 0 mean and variance }s} 2 2 1 N .\nThen, using Chebyshev\u2019s inequality (see Theorem 8.1) and taking k \u201c N 1 4 we obtain\nPr\n\u02c6\n|pRsqk| \u011b 1\nN 3 4\n}s}2 \u02d9 \u010f 1 N 1 2 . (72)\nThus given that:\n\u03b4Xp\u03b1W , sq \u00b4 \u03b4Xp\u03b1S , sq \u201c pRsqipsq \u00b4 max j\u2030ipsq ppS `Rqsqj ` max j\u2030ipsq pSsqj , (73)\nby (72), we have\nP \u02c6 p\u03b4Xp\u03b1W , sq \u00b4 \u03b4Xp\u03b1S , sqq \u011b 1\nN 3 4\n}s}2 \u00b4 max j\u2030ipsq ppS `Rqsqj ` max j\u2030ipsq\npSsqj \u02d9 \u010f 1 N 1 2 . (74)\nSuppose maxj\u2030ipsqpSsqj is satisfied for the component k\u02dapNq, meaning that:\nmax j\u2030ipsq\npSsqj \u201c pSsqk\u02dapNq. (75)\nThen, again by (72) we have:\nP \u02c6 |pS `Rqsqk\u02da \u00b4 pSsqk\u02da | \u011b 1\nN 3 4\n}s}2 \u02d9 \u010f 1 N 1 2\n(76)\ngiven that S and R are independent from each other and S is deterministic. Given that maxj\u2030ipsqpS ` Rqsj \u011b pS `Rqsk\u02da , from (74) we obtain\nP \u02c6 p\u03b4Xp\u03b1W , sq \u00b4 \u03b4Xp\u03b1S , sqq \u011b 1\nN 3 4\n}s}2 \u00b4 pS `Rqsqk\u02da ` pSsqk\u02da \u02d9 \u010f 1 N 1 2 . (77)\nFinal Result from (76):\nP \u02c6 \u03b4Xps, \u03b1W q \u00b4 \u03b4Xps, \u03b1Sq \u010f 2\nN 3 4\n}s}2 \u02d9 \u011b 1 \u00b4 1 N 1 2\n(78)\nwith apNq \u201c 2 N 3 4 .\nWhen the DNN has more than one layer, we continue this proof with the same steps found in Subsection 10.1."
        },
        {
            "heading": "8.2 Proof of Pruning Theorem for a single object",
            "text": "Proof for the Pruning Theorem 5.4:\nProof. We first provide a proof for the case when our DNN has only one layer matrix W . By Lemma 5.3 we have that there DDpNq such that for\nE :\u201c apNqg\u03d5ps, bq (79)\nwe have the conditional probability\nPp\u03b4Xps, \u03b1Sq \u011b 0 | \u03b4Xps, \u03b1W q \u011b Eq \u011b 1 \u00b4DpNq, (80)\nwith DpNq, apNq \u00d1 0 as N \u00d1 8 and g\u03d5ps, bq coming from (24). Again, \u03b1S are the parameters of the DNN which has the weight matrix S and \u03b1W are the parameters of the DNN with the weight layer matrix W .\nWe then use Approximation Lemma 7.3 to obtain that DfW , DB\u02daN p\u03f5q such that @\u03f5:\nPp}pW 1 \u00b4 Sqz}2 \u0105 p1 ` \u03f5qfW }z}2q \u0103 B\u02daN p\u03f5q, (81)\nwith B\u02daN p\u03f5q \u00d1 0 as N \u00d1 8. We then follow an argument similar to that given for Lemma 10.1. That is, the change in classification confidence due to pruning is:\n\u2206p\u03b4Xq \u201c |\u03b4Xps, \u03b1Sq \u00b4 \u03b4Xps, \u03b1W 1q|\nFor a particular component i, the change \u2206Xi due to pruning is given by:\n\u2206Xi \u201c |Xips, \u03b1Sq \u00b4Xips, \u03b1W 1q|\nWe have that Xps, \u03b1Sq \u201c \u03bb \u02dd pW qs and Xps, \u03b1W 1q \u201c \u03bb \u02dd pW 1qs. Given that \u03bb is the absolute value activation function, for any scalar values x and y, we have:\n|\u03bbpxq \u00b4 \u03bbpyq| \u010f |x\u00b4 y|. (82)\nThus, from (81) we have }Xps, \u03b1Sq \u00b4 Xps, \u03b1W 1q} \u010f }pS \u00b4 W 1qs} \u010f p1 ` \u03f5qfW }s}2, with probability 1 \u00b4B\u02daN p\u03f5q. Furthermore,\n\u2206Xmax ` \u2206Xmax-1 \u010f ? 2p1 ` \u03f5qfW }s}2, (83)\nwith probability 1 \u00b4B\u02daN p\u03f5q. Here \u2206Xmax-1 is the change in the component of X which has the second to biggest change, and \u2206Xmax is the change in the change in the component of X which had the biggest change.\nThen, using the same steps given in Lemma 10.1, we obtain:\n\u2206p\u03b4Xq \u010f \u2206Xmax ` \u2206Xmax-1 \u010f ? 2p1 ` \u03f5qfW }s}2, (84)\nwith probability 1 \u00b4B\u02daN p\u03f5q. Thus, given that: Pp\u03b4Xps, \u03b1Sq \u011b 0 | \u03b4Xps, \u03b1W q \u011b Eq \u011b 1 \u00b4DpNq, (85) we have that there exists an explicit function fW \u0105 0 such that @\u03f5 \u0105 0 there DC\u03f5pNq such that for\nE1 :\u201c p ? 2p1 ` \u03f5qmintfW , a \u03bb`u ` apNqqg\u03d5ps, bq (86)\nwe have the conditional probability\nPp\u03b4Xps, \u03b1W 1q \u011b 0 | \u03b4Xps, \u03b1W q \u011b E1q \u011b 1 \u00b4 C\u03f5pNq, (87)\nwith C\u03f5pNq, apNq \u00d1 0 as N \u00d1 8 and g\u03d5ps, bq coming from (24). fW is given in Lemma 7.3. When the DNN has more than one layer, we continue this proof with the same steps found in Subsection 10.1.\nTheorem 8.1 (Chebyshev\u2019s inequality). Let X be a random variable with mean \u00b5 and finite non-zero variance \u03c32. Then for any real number k \u0105 0,\nPrp|X \u00b4 \u00b5| \u011b k\u03c3q \u010f 1 k2 . (88)"
        },
        {
            "heading": "8.3 Proof of Pruning Theorem for accuracy",
            "text": "This Theorem follows from the Pruning Theorem 5.4. Under the assumptions of this theorem and from the Pruning Theorem, we have that @\u03f5 \u0105 0,@s P T 1:\nP \u02c6 \u03b4Xps, \u03b1W 1q \u011b 0 | \u03b4Xps, \u03b1W q \u011b p1 ` \u03f5q ? 2papNq ` fW2q}W1s}2\u03c3maxpW3q \u02d9 \u011b 1 \u00b4 C\u03f5pNq, (89)\nwith C\u03f5pNq, apNq \u00d1 0 as N \u00d1 8 and fW2 given by the Approximation Lemma 7.3. In this case, E1 from the Pruning Theorem given in (30) is equal to p1` \u03f5q ? 2papNq `fW2q}W1s}2\u03c3maxpW3q. By taking\nEacc \u201c p1 ` \u03f5q ? 2pfW2 ` 2N\u00b4 3 4 q max\nsPGEacc,\u03b1 }W1s}2\u03c3maxpW3q, (90)\nwe make the classification confidence threshold of the non-pruned DNN independent on s. We then obtain, from (89), that for all s:\nP \u02c6 \u03b4Xps, \u03b1W 1q \u011b 0 | \u03b4Xps, \u03b1W q \u011b Eacc \u02d9 \u011b 1 \u00b4 C\u03f5pNq. (91)\nGiven that this is independent on s, we obtain the final result:\nP \u02c6 GEacc,\u03b1 \u0102 G0,\u03b11 \u02d9 \u011b p1 \u00b4 C\u03f5pNqq|GEacc,\u03b1|. (92)\nThe theorem then follows from the fact that acc\u03b11ptq \u201c |G0,\u03b11 | |T 1| ."
        },
        {
            "heading": "9 Appendix D: Other algorithms required for implementing RMTSVD based pruning of DNN",
            "text": ""
        },
        {
            "heading": "9.1 BEMA algorithm for finding \u03bb`",
            "text": "The following is the BEMA algorithm for finding best fit \u03bb` of 1NR TR based on the ESD of X \u201c R`S. It is used in the analysis of matrices with the information plus noise structure (i.e. deformed matrices), where one wants to determine the rightmost edge of the compact support of the MP distribution. In this context, the Tracy-Widom distribution provides the limiting distribution of the largest eigenvalue \u03bb` of large random matrices, allowing us to compute a confidence interval for \u03bb` in the presence of the Mar\u010denko-Pastur distribution, see [KML21]. The BEMA algorithm is computationally efficient and has been shown to provide accurate results for matrices with the information plus noise structure. More details on the algorithm and its relationship with the MP and Tracy-Widom distributions can be found in [KML21]. Here we present a simplified version of the algorithm for R a N \u02c6N matrix: Remark 9.1. The algorithm depends on parameters \u03b1 P p0, 1{2q, \u03b2 P p0, 1q. We show this by varying \u03b1 and \u03b2 for the case found in Example 9.1. See Fig. 16a and 16b. The red line is \u03bb` \u201c 4, which is the correct \u03bb` of 1NR\nTR. In this example, while dependence on \u03b1 is insignificant for sufficiently large values, dependence on \u03b2 allows us to control the confidence that the eigenvalues of the random matrix R will be smaller than the estimator for \u03bb` of the MP distribution. In all the numerical simulations given in Section 4, we took \u03b1 \u201c .1 and \u03b2 \u201c .1. It would be interesting to try and see what happens when we take a larger \u03b2, as it would prevent the algorithm from pruning too many parameters but might lead to even higher accuracy.\nIn the numeric portion of the paper we always divide RTR by 1N when obtaining the ESD of X regardless of the original distribution of the initial random matrix Rp0q. If Rpoq is distributed using Np0, 1N q, dividing R\nTR by N does not seem change the fact that the ESD of X is given by the MP distribution.\nAlgorithm 2 Computation of \u03bb` using MP and Tracy-Widom Distributions 1: Choose parameters \u03b1 P p0, 1{2q, \u03b2 P p0, 1q. 2: for each \u03b1N \u010f k \u010f p1 \u00b4 \u03b1qN do 3: Obtain qk, the pk{Nq upper-quantile of the MP distribution with \u03c32 \u201c 1 and c \u201c 1. \u0179 Each qk\nis a solution to qk \u015f\n0\n1 2\u03c0 ? p4\u00b4\u03bbq\u03bb \u03bb \u201c k{N\n4: end for 5: Compute \u03c3\u03022 \u201c \u0159 \u03b1N\u010fk\u010fp1\u00b4\u03b1qN qk\u03bbk \u0159\n\u03b1N\u010fk\u010fp1\u00b4\u03b1qN q 2 k . \u0179 where \u03bbk is the kth smallest eigenvalue of X 6: Obtain t1\u00b4\u03b2 , the p1 \u00b4 \u03b2q quantile of Tracy-Widom distribution. 7: Return \u03bb` \u201c \u03c3\u03022r4 ` 24{3t1\u00b4\u03b2 \u00a8N\u00b42{3s.\n(a) Dependence of algorithm the choice of \u03b1, \u03b2 \u201c 0.5. In this example the rank of the deterministic matrix S is fairly low.\n(b) Dependence of algorithm on the choice of \u03b2, \u03b1 \u201c 0.25.\nExample 9.1. In this example, we create a random N \u02c6 N matrix R with components taken from i.i.ds using the normal distribution of zero mean and unit variance (\u03c32 \u201c 1). We take S to be a N \u02c6N deterministic matrix with components given by\nSri, js \u201c tanp\u03c0 2 ` 1 j ` 1q ` cospiq \u00a8 logpi` j ` 1q ` sinpjq \u00a8 cosp i j q, (93)\nW \u201c R ` S and X \u201c 1NW TW . The BEMA algorithm is used to find the \u03bb` of the ESD of X, as described in Subsection 9.1. R is a random matrix satisfying the conditions of Theorem 4.2, and so the ESD of 1NR\nTR converges to the Marchenko-Pastur distribution as N \u00d1 8 and has a \u03bb` that determines the rightmost edge of its compact support. We can imagine a situation in which R is not directly known, and the goal is to find an estimator of \u03bb` from the ESD of X. See Fig. 17 for the result of the ESD of X with the Marchenko-Pastur distribution that best fits the ESD shown in red.\nThe bulk of the eigenvalues are well-fit by the MP distribution, but some eigenvalues bleed out to the right of \u03bb`. These eigenvalues correspond to the singular values of S. The direct calculation of the \u03bb` of the MP distribution corresponding to 1NR\nTR gives \u03bb` \u201c \u03c32 \u00a8 p1 ` 1q2 \u201c 4, and the \u03bb` obtained to fit the bulk of the ESD of X and the \u03bb` of 1NR TR are approximately the same.\nThe BEMA algorithm will be employed to estimate \u03bb` from the ESD of Xlptq. As the DNN training progresses, it is expected that the majority of the eigenvalues of Xlptq will conform to the MP distribution. Nonetheless, some eigenvalues may extend beyond the bulk of the MP distribution and be associated with the singular values of Slptq. The purpose of the BEMA algorithm is to identify the furthest edge of the MP distribution, which helps determine the value of \u03bb`. Understanding \u03bb` is crucial as it offers insights into the DNN\u2019s behavior during training and its capacity to generalize to new data.\nIn combination with the SVD, the BEMA algorithm can be applied to decide which singular values of the DNN\u2019s weight matrices Wl should be eliminated during training. The SVD breaks down the weight matrix into its singular values and singular vectors, allowing for an RMT-based analysis of\ntheir distribution. Utilizing the BEMA algorithm, one can pinpoint the eigenvalues associated with the singular values of Sl and differentiate them from the eigenvalues related to the singular values of Rl. By removing the eigenvalues corresponding to Rl, the DNN\u2019s training process can be made more effective and efficient."
        },
        {
            "heading": "9.2 The role of singular value decomposition in deep learning",
            "text": "Consider a N \u02c6 M matrix A. A singular value decomposition (SVD) of A consists of a factorization A \u201c U\u03a3V T , where:\n\u2022 U is an N \u02c6N orthogonal matrix.\n\u2022 V is an M \u02c6M orthogonal matrix.\n\u2022 \u03a3 is an N \u02c6M matrix with the ith diagonal entry equal to the ith singular value \u03c3i and all other entries of \u03a3 being zero. For \u03bbi, the eigenvalues of a matrix X \u201c W TW , the singular values of W are given by \u03c3i \u201c ? \u03bbi. Consequently, singular values are connected to eigenvalues of the symmetrization of a matrix W . For a DNN\u2019s Wl, it has been demonstrated that discarding small singular values of Wl through its SVD during the DNN\u2019s training can decrease the number of parameters while improving accuracy, as shown in [YTW`20, XLG13, CKXS14, APJY16]. In the remainder of this work, we illustrate how RMT can aid in identifying the singular values to be removed from a DNN layer without compromising the DNN\u2019s accuracy.\nIn particular, the BEMA algorithm can be combined with the SVD of Wl to ascertain which singular values should be removed during the DNN\u2019s training. To achieve this, one first computes the SVD of Wl and then calculates the eigenvalues of the symmetrized matrix Xl \u201c 1NW T l Wl. The eigenvalues derived from the symmetrization can be linked to the singular values of Wl through N\u03bbi \u201c \u03c32i . Employing the BEMA algorithm to estimate the value of \u03bb` allows for the determination of a threshold for the singular values of Wl. Singular values below the threshold can be removed without impacting the DNN\u2019s accuracy, as they are likely less crucial for the DNN\u2019s performance. This process can be carried out iteratively during the DNN\u2019s training since the threshold can be updated as training advances."
        },
        {
            "heading": "9.3 Eliminating singular values while preserving accuracy",
            "text": "In this subsection, we demonstrate how SVD can be employed to remove the random components of Wl without compromising accuracy. This could potentially lead to a significant reduction in the number of parameters in the DNN, resulting in faster training.\nAlgorithm 3 Pruning a Weight Matrix from a Trained DNN 1: Acquire a weight matrix Wl from a trained DNN. 2: Perform SVD on Wl: Wl \u201c U\u03a3V T . 3: Calculate the eigenvalues \u03bbi of the square matrix 1NW T l Wl.\n4: Apply the BEMA algorithm from Subsection 9.1 to find the best fit MP distribution for the ESD of X \u201c 1NW T l Wl and its corresponding \u03bb`. \u0179 See Fig. 18 5: Determine whether the ESD of X fits the MP distribution using the algorithm in Subsection 9.4. \u0179 Ensures Wl \u201c R ` S assumption is valid 6: Replace a portion, e.g., 0.1, of the singular values less than a\n\u03bb`N with zeros to form a new diagonal matrix \u03a31 and the truncated matrix W 1l .\n7: Use \u03a31 to obtain W 11,l \u201c U ? \u03a31 and W 12,l \u201c ? \u03a31V T .\nExample 9.2. Consider an original DNN with two hidden layers, each consisting of 10 nodes. The total number of parameters in this case would be 100. By employing SVD and removing 8 small singular values in the weight layer matrix of this DNN, we can split the hidden layer into two, resulting in a new DNN with three hidden layers. The first layer will have 10 nodes, the second layer will have 2 nodes, and the third layer will have 10 nodes. By keeping only two singular values in the SVD, we now have only 20 parameters, see Figure 19. In practice, we don\u2019t actually split the layer\nExample 9.3. We used the above approach for a DNN trained on MNIST. In this example the DNN has two layers, the first with a 784\u02c6 1000 matrix W1 and the second with a 1000\u02c6 10 matrix W2. The activation function was ReLU. We trained the DNN for 10 epocs and achieved a 98% accuracy on the test set.\nWe perform a SVD on W1, in this case \u03a3 is a 784 \u02c6 1000 matrix. Even if we only keep the biggest 60 \u03c3i of W1 and transform the first layer into two layers W1,1 and W2,1 the accuracy is still 98%. W1 had 784, 000 parameters, while W1,1 and W2,1 have 784p60q ` 1, 000p60q \u201c 107, 040 parameters (not including the bias vector parameters). This is a reduction by over 85%. In Fig. 20 we show how the accuracy of the DNN depends on the number of singular values which we keep. The red line corresponds\nto the threshold given by the MP distribution (via \u03bb`) for how many of the large singular values should be kept. As the figure shows, this threshold is highly accurate. This example also numerically confirms Theorem 5.4 and shows that the threshold given in the theorems (for which singular values to keep) is highly useful and accurate."
        },
        {
            "heading": "9.4 MP fit Criteria: Checking if the ESD of X fits a MP distribution",
            "text": "This subsection details a procedure to evaluate whether the ESD of a square matrix X is possibly drawn from a specific MP distribution (with a possibility of spiked eigenvalues). The initial step of this procedure relies on the BEMA method to identify the most fitting MP distribution. This optimal fitting distribution provides a theoretical cumulative distribution function (CDF), while the empirical cumulative spectral distribution related to X can be computed. A comparison of these two distributions allows us to dismiss the hypothesis that X follows the suggested MP distribution if the difference between the distributions is substantial. Let\u2019s formalize these concepts, starting with the concept of an empirical cumulative spectral distribution.\nDefinition 9.4. Assume G is an N \u02c6M matrix and its ESD \u00b5GM is defined as in Definition 4.1. The empirical cumulative spectral distribution of G, symbolized as FG : R \u00d1 R, is defined as:\nFGpaq \u201c \u00b5Gmpp\u00b48, asq (94)\nInterestingly, the cumulative distribution functions for the MP distribution are known and can be expressed in a closed form. With these equations, we can comprehensively describe our procedure. We set a tuning parameter \u03b3 P p0, 1q corresponding to the sensitivity of our test.\nThis procedure computes the maximum difference between the expected and empirical cumulative distribution functions by sampling at each point in the empirical distribution. Since this is intended for the unique case of testing for spiked MP distributions, we can utilize this information to enhance our test over simply calculating the L8 difference between the expected and empirical distributions.\nThis enhancement is reflected in the step which calculates ilow and ihigh. As BEMA only uses data in the quantile between p\u03b1, 1 \u00b4 \u03b1q to find the best fit, it is logical to only examine for fit within the same range. In this context, we would expect a spiked MP distribution to be poorly approximated by its generative MP distribution around the highest eigenvalues (i.e., the spiked values), and hence it makes sense to only test the bulk values for goodness of fit.\nAlgorithm 4 Assessing Conformance to the MP Distribution\n1: Accept X \u201c 1NW TW as input, where W is an N \u02c6M matrix. 2: Calculate the spectrum of X \u201c t\u03c31, . . . , \u03c3Mu. 3: Compute the empirical cumulative spectral distribution of X, denoted FX . 4: Execute the BEMA method with parameters \u03b1 and \u03b2 to determine \u03c3\u03022, the anticipated variance of\neach coordinate of W . 5: Calculate 0 \u010f ilow \u0103 ihigh \u010f M such that ilow is the smallest integer with ilowM \u011b \u03b1 and ihigh is the\nlargest integer with ihighM \u010f 1 \u00b4 \u03b1. 6: Define F 1X as the theoretical cumulative distribution function for the MP distribution with param-\neters \u03c3\u03022 and \u03bb \u201c N{M . 7: Evaluate s \u201c maxiPrilow,ihighs |FXpiq \u00b4 F 1 Xpiq|. 8: if s \u0105 \u03b3 then 9: Dismiss the hypothesis that X follows the proposed distribution.\n10: else 11: Do not reject this hypothesis. 12: end if"
        },
        {
            "heading": "10 Appendix E: Some of the proofs and numerics",
            "text": ""
        },
        {
            "heading": "10.1 Proof of Lemma 5.1",
            "text": "Proof. The classification confidence before pruning is:\n\u03b4Xps, \u03b1Wbq \u201c Xipsqps, \u03b1Wbq \u00b4 max j\u2030ipsq Xjps, \u03b1Wbq\nAfter pruning it is: \u03b4Xps, \u03b1W 1bq \u201c Xipsqps, \u03b1W 1bq \u00b4 maxj\u2030ipsq Xjps, \u03b1W 1bq\nFor simplicity, we will start by proving the theorem for the case of a DNN with only one layer matrix W and a bias vector \u03b2. Thus, we take Xps, \u03b1Wbq \u201c Xps, \u03b1W q and Xps, \u03b1W 1bq \u201c Xps, \u03b1W 1q.\nThen, the change in classification confidence due to pruning is:\n\u2206p\u03b4Xq \u201c |\u03b4Xps, \u03b1W q \u00b4 \u03b4Xps, \u03b1W 1q|\nFor a particular component i, the change \u2206Xi due to pruning is given by:\n\u2206Xi \u201c |Xips, \u03b1W q \u00b4Xips, \u03b1W 1q|\nWe have that Xps, \u03b1W q \u201c \u03bb \u02dd pW ` \u03b2qs and Xps, \u03b1W 1q \u201c \u03bb \u02dd pW 1 ` \u03b2qs. Given that \u03bb is the absolute value activation function, for any scalar values x and y, we have:\n|\u03bbpxq \u00b4 \u03bbpyq| \u010f |x\u00b4 y|. (95)\nThus }Xps, \u03b1W q\u00b4Xps, \u03b1W 1q} \u010f }pW\u00b4W 1qs} \u010f a \u03bb`}s}2. If the change in the norm of the entire output vector due to pruning is at most a\n\u03bb`}s}2, then the maximum change in any individual component must also be bounded by that amount. That is:\n\u2206Xmax \u010f a \u03bb`}s}2.\nFurthermore,\n\u2206Xmax ` \u2206Xmax-1 \u010f a 2\u03bb`}s}2, (96)\nwith \u2206Xmax-1 the change in the component of X which has the second to biggest change and \u2206Xmax the change component of X which had the biggest change.\nNow assume maxj\u2030ipsq Xjps, \u03b1W 1q \u010f maxj\u2030ipsq Xjps, \u03b1W q, then given that |Xipsqps, \u03b1W q\u00b4Xipsqps, \u03b1W 1q| \u010f a\n\u03bb`}s}2, we must have \u03b4Xps, \u03b1W 1q \u201c Xipsqps, \u03b1W 1q \u00b4 maxj\u2030ipsq Xjps, \u03b1W 1q \u010f a \u03bb`}s}2. Next, assume maxj\u2030ipsq Xjps, \u03b1W 1q \u011b maxj\u2030ipsq Xjps, \u03b1W q. Let\u2019s expand the change in \u03b4X due to\npruning:\n\u2206p\u03b4Xq \u201c |pXipsqps, \u03b1W q \u00b4 max j\u2030ipsq Xjps, \u03b1W qq \u00b4 pXipsqps, \u03b1W 1q \u00b4 max j\u2030ipsq Xjps, \u03b1W 1qq|\n\u010f |Xipsqps, \u03b1W q \u00b4Xipsqps, \u03b1W 1q| ` | max j\u2030ipsq Xjps, \u03b1W q \u00b4 max j\u2030ipsq Xjps, \u03b1W 1q|.\nTake k\u02da to be an integer such that maxj\u2030ipsq Xjps, \u03b1W 1q \u201c Xk\u02daps, \u03b1W 1q, we have, given that maxj\u2030ipsq Xjps, \u03b1W 1q \u011b maxj\u2030ipsq Xjps, \u03b1W q, that:\n|Xipsqps, \u03b1W q \u00b4Xipsqps, \u03b1W 1q| ` | max j\u2030ipsq Xjps, \u03b1W q \u00b4 max j\u2030ipsq Xjps, \u03b1W 1q|\n\u010f |Xipsqps, \u03b1W q \u00b4Xipsqps, \u03b1W 1q| ` |Xk\u02daps, \u03b1W 1q \u00b4Xk\u02daps, \u03b1W q|. (97)\nConsider the worst-case scenario where the change is maximally concentrated in the components ipsq and k\u02da. Thus, given (96) we have:\n\u2206p\u03b4Xq \u010f \u2206Xmax ` \u2206Xmax-1 \u010f a 2\u03bb`}s}2. (98)\nNext, we consider the case where the DNN has multiple layers given by the matrices W1, \u00a8 \u00a8 \u00a8 ,WL and bias vectors \u03b21, \u00a8 \u00a8 \u00a8\u03b2L and we prune the last matrix layer of the DNN, i.e. we prune layer matrix WL to obtain W 1L. In this case, we can reduce this problem to that of a DNN with a single layer matrix WL and a single bias vector \u03b2L and an input vector z \u201c \u03bb \u02ddWL\u00b41 \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02ddW1s. Then, by the above argument, we have:\n\u2206p\u03b4Xq \u010f a 2\u03bb`}\u03bb \u02dd pWL\u00b41 ` \u03b2L\u00b41q \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02dd pW1 ` \u03b21qs}2. (99)\nFinally, we assume that we prune a layer Wb to obtain W 1b. By a similar argument to what we have above, we see that the max change of a component of the output vector Xps, \u03b1Wbq after pruning is:\n\u2206Xmax \u010f a \u03bb`}\u03bb \u02dd pWb\u00b41 ` \u03b2b\u00b41q \u02dd \u00a8 \u00a8 \u00a8 \u02dd \u03bb \u02dd pW1 ` \u03b21qs}2\u03c3maxpWb`1q . . . \u03c3maxpWLq,\ngiven that }\u03bb \u02dd pAz ` \u03b2q \u00b4 \u03bb \u02dd pAz1 ` \u03b2q}2 \u010f \u03c3maxpAq}z1 \u00b4 z}2. Again we obtain,\n\u2206Xmax `\u2206Xmax-1 \u010f a 2\u03bb`}\u03bb \u02dd pWb\u00b41 `\u03b2b\u00b41q \u02dd \u00a8 \u00a8 \u00a8 \u02dd\u03bb \u02dd pW1 `\u03b21qs}2\u03c3maxpWb`1q . . . \u03c3maxpWLq. (100)\nThus, this completes the proof of the lemma."
        },
        {
            "heading": "10.2 Numerical example used to calculate \u03b4X.",
            "text": "In this demonstration, we have a set of two-dimensional data points, and our objective is to identify the boundary that divides them into two categories. The dataset originates from a randomly constructed polynomial function of a designated degree. We then sample data points uniformly across a spectrum of x-values. For every x-value, the polynomial function gives a y-value. These points are then slightly\noffset either upwards or downwards, forming two distinguishable point clusters labeled as red and blue. Points positioned above the polynomial curve get a blue label, and those below are tagged red. We also add Gaussian noise to slightly modify the y-values, causing a few red data points to appear below the boundary and some blue ones above. This scenario is depicted in Fig. 21.\nThe goal is to harness a DNN to capture the decision boundary demarcating the two clusters. This DNN is designed to process a two-dimensional data point, producing a binary output indicating whether the point is red or blue. Given that the dataset is artificially curated, the actual decision boundary is known, allowing us to gauge the efficacy of our DNN.\nFor this task, our neural network model had one hidden layer with 500 neurons. We also used the ReLU activation function. Training is executed using the cross-entropy loss complemented with the Adam optimizer."
        },
        {
            "heading": "10.3 Hyperparameters for Subsection 4.2.1",
            "text": "\u2022 Number of epochs: 40\n\u2022 Number of seeds: 10\n\u2022 Learning rate (lr): 0.02\n\u2022 Momentum: 0.9\n\u2022 Batch size: 128\n\u2022 L2 regularization on loss: 0.0005. (The regularization term is applied for both DNNs, the normal and pruned versions).\nSee [LBD`20], [SMDH13], [SMDH13] and [HSK`12] for information on learning rate, momentum, batch size and regularization, respectively."
        },
        {
            "heading": "10.4 Hyperparameters for Subsection 4.2.2",
            "text": "\u2022 Number of epochs: 70\n\u2022 Number of seeds: 10\n\u2022 Learning rate (lr): 0.02.\n\u2022 Momentum: 0.9\n\u2022 Batch size: 128\n\u2022 L2 regularization on loss: 0.0005."
        },
        {
            "heading": "10.5 Hyperparamters for Subsection 4.2.4",
            "text": "Training hyperparameters:\n\u2022 Number of epochs: Depends on the example.\n\u2022 Number of seeds: 5\n\u2022 Learning rate (lr): 0.02.\n\u2022 Momentum: 0.9\n\u2022 Batch size: 128\n\u2022 Split frequency: Depends on the example.\n\u2022 L2 regularization on loss: .001.\n\u2022 goodness of fit (GoF): changes for every simulation"
        },
        {
            "heading": "10.6 The hyperparamters for Subsection 4.2.6",
            "text": "Training hyperparameters:\n\u2022 Number of epochs: 300.\n\u2022 Number of seeds: 5\n\u2022 Learning rate (lr): 0.001.\n\u2022 Momentum: 0.9\n\u2022 Batch size: 128\n\u2022 Split frequency (every how many epochs we split the pruned DNN and remove small singular values): 40.\n\u2022 L2 regularization on loss: .001.\n\u2022 goodness of fit (GoF): 0.08 for fully connected layers, 0.06 for convolutional layers."
        }
    ],
    "title": "Enhancing Accuracy in Deep Learning Using Random Matrix Theory",
    "year": 2023
}