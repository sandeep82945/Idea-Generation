{
    "abstractText": "Multitrack music transcription aims to transcribe a music audio input into the musical notes of multiple instruments simultaneously. It is a very challenging task that typically requires a more complex model to achieve satisfactory result. In addition, prior works mostly focus on transcriptions of regular instruments, however, neglecting vocals, which are usually the most important signal source if present in a piece of music. In this paper, we propose a novel deep neural network architecture, Perceiver TF, to model the time-frequency representation of audio input for multitrack transcription. Perceiver TF augments the Perceiver architecture by introducing a hierarchical expansion with an additional Transformer layer to model temporal coherence. Accordingly, our model inherits the benefits of Perceiver that posses better scalability, allowing it to well handle transcriptions of many instruments in a single model. In experiments, we train a Perceiver TF to model 12 instrument classes as well as vocal in a multi-task learning manner. Our result demonstrates that the proposed system outperforms the state-of-the-art counterparts (e.g., MT3 and SpecTNT) on various public datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei-Tsung Lu"
        },
        {
            "affiliations": [],
            "name": "Ju-Chiang Wang"
        },
        {
            "affiliations": [],
            "name": "Yun-Ning Hung"
        }
    ],
    "id": "SP:2b3fa4c8e7147e739be72dc06289e96ceb0c711d",
    "references": [
        {
            "authors": [
                "Kin Wai Cheuk",
                "Dorien Herremans",
                "Li Su"
            ],
            "title": "Reconvat: A semi-supervised automatic music transcription framework for low-resource real-world data",
            "venue": "Proc. ACM Multimedia, 2021, pp. 3918\u20133926.",
            "year": 2021
        },
        {
            "authors": [
                "Josh Gardner",
                "Ian Simon",
                "Ethan Manilow",
                "Curtis Hawthorne",
                "Jesse Engel"
            ],
            "title": "MT3: Multi-task multitrack music transcription",
            "venue": "Proc. ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Felix Gimeno",
                "Andy Brock",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Joao Carreira"
            ],
            "title": "Perceiver: General perception with iterative attention",
            "venue": "Proc. ICML, 2021, pp. 4651\u20134664.",
            "year": 2021
        },
        {
            "authors": [
                "Wei-Tsung Lu",
                "Ju-Chiang Wang",
                "Minz Won",
                "Keunwoo Choi",
                "Xuchen Song"
            ],
            "title": "SpecTNT: A time-frequency transformer for music audio",
            "venue": "Proc. ISMIR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Stefan Uhlich",
                "Marcello Porcu",
                "Franck Giron",
                "Michael Enenkl",
                "Thomas Kemp",
                "Naoya Takahashi",
                "Yuki Mitsufuji"
            ],
            "title": "Improving music source separation based on deep neural networks through data augmentation and network blending",
            "venue": "Proc. ICASSP, 2017, pp. 261\u2013265.",
            "year": 2017
        },
        {
            "authors": [
                "Xuchen Song",
                "Qiuqiang Kong",
                "Xingjian Du",
                "Yuxuan Wang"
            ],
            "title": "Catnet: Music source separation system with mixaudio augmentation",
            "venue": "arXiv preprint arXiv:2102.09966, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Zafar Rafii",
                "Antoine Liutkus",
                "Fabian-Robert St\u00f6ter",
                "Stylianos Ioannis Mimilakis",
                "Derry FitzGerald",
                "Bryan Pardo"
            ],
            "title": "An overview of lead and accompaniment separation in music",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 8, pp. 1307\u20131335, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Yun-Ning Hung",
                "Yi-An Chen",
                "Yi-Hsuan Yang"
            ],
            "title": "Multitask learning for frame-level instrument recognition",
            "venue": "Proc. ICASSP, 2019, pp. 381\u2013385.",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Jansson",
                "Rachel M Bittner",
                "Sebastian Ewert",
                "Tillman Weyde"
            ],
            "title": "Joint singing voice separation and f0 estimation with deep u-net architectures",
            "venue": "Proc. EUSIPCO, 2019, pp. 1\u20135.",
            "year": 2019
        },
        {
            "authors": [
                "Yu-Te Wu",
                "Berlin Chen",
                "Li Su"
            ],
            "title": "Multi-instrument automatic music transcription with self-attention-based instance segmentation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2796\u20132809, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Keitaro Tanaka",
                "Takayuki Nakatsuka",
                "Ryo Nishikimi",
                "Kazuyoshi Yoshii",
                "Shigeo Morishima"
            ],
            "title": "Multi-instrument music transcription based on deep spherical clustering of spectrograms and pitchgrams",
            "venue": "Proc. ISMIR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Kin Wai Cheuk",
                "Keunwoo Choi",
                "Qiuqiang Kong",
                "Bochen Li",
                "Minz Won",
                "Amy Hung",
                "Ju-Chiang Wang",
                "Dorien Herremans"
            ],
            "title": "Jointist: Joint learning for multi-instrument transcription and its applications",
            "venue": "arXiv preprint arXiv:2206.10805, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Jun-You Wang",
                "Jyh-Shing Roger Jang"
            ],
            "title": "On the preparation and validation of a large-scale dataset of singing transcription",
            "venue": "Proc. ICASSP, 2021, pp. 276\u2013280.",
            "year": 2021
        },
        {
            "authors": [
                "Sangeun Kum",
                "Jongpil Lee",
                "Keunhyoung Luke Kim",
                "Taehyoung Kim",
                "Juhan Nam"
            ],
            "title": "Pseudo-label transfer from framelevel to note-level in a teacher-student framework for singing transcription from polyphonic music",
            "venue": "Proc. ICASSP, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Jui-Yang Hsu",
                "Li Su"
            ],
            "title": "Vocano: A note transcription framework for singing voice in polyphonic music",
            "venue": "Proc. ISMIR, 2021, pp. 293\u2013300.",
            "year": 2021
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu"
            ],
            "title": "Conformer: Convolutionaugmented transformer for speech recognition",
            "venue": "Proc. IN- TERSPEECH, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Won",
                "K. Choi",
                "X. Serra"
            ],
            "title": "Semi-supervised music tagging transformer",
            "venue": "Proc. ISMIR, 2021, pp. 769\u2013776.",
            "year": 2021
        },
        {
            "authors": [
                "Yun-Ning Hung",
                "Ju-Chiang Wang",
                "Xuchen Song",
                "Wei-Tsung Lu",
                "Minz Won"
            ],
            "title": "Modeling beats and downbeats with a time-frequency transformer",
            "venue": "Proc. ICASSP, 2022, pp. 401\u2013 405.",
            "year": 2022
        },
        {
            "authors": [
                "Ju-Chiang Wang",
                "Yun-Ning Hung",
                "Jordan B.L. Smith"
            ],
            "title": "To catch a chorus, verse, intro, or anything else: Analyzing a song with structural functions",
            "venue": "Proc. ICASSP, 2022, pp. 416\u2013 420.",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks",
            "venue": "European conference on computer vision. Springer, 2016, pp. 630\u2013645.",
            "year": 2016
        },
        {
            "authors": [
                "Curtis Hawthorne",
                "Erich Elsen",
                "Jialin Song",
                "Adam Roberts",
                "Ian Simon",
                "Colin Raffel",
                "Jesse Engel",
                "Sageev Oore",
                "Douglas Eck"
            ],
            "title": "Onsets and frames: Dual-objective piano transcription",
            "venue": "Proc. ISMIR, 2018, pp. 50\u201357.",
            "year": 2018
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "KyungHyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "Proc. NeurIPS, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Ethan Manilow",
                "Gordon Wichern",
                "Prem Seetharaman",
                "Jonathan Le Roux"
            ],
            "title": "Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity",
            "venue": "Proc. WASPAA. IEEE, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel"
            ],
            "title": "Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching",
            "venue": "2016, Columbia University.",
            "year": 2016
        },
        {
            "authors": [
                "Curtis Hawthorne",
                "Andriy Stasyuk",
                "Adam Roberts",
                "Ian Simon",
                "Cheng-Zhi Anna Huang",
                "Sander Dieleman",
                "Erich Elsen",
                "Jesse Engel",
                "Douglas Eck"
            ],
            "title": "Enabling factorized piano music modeling and generation with the maestro dataset",
            "venue": "Proc. ICLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Qingyang Xi",
                "Rachel M Bittner",
                "Johan Pauwels",
                "Xuzhou Ye",
                "Juan Pablo Bello"
            ],
            "title": "Guitarset: A dataset for guitar transcription",
            "venue": "Proc. ISMIR, 2018, pp. 453\u2013460.",
            "year": 2018
        },
        {
            "authors": [
                "Sangeun Kum",
                "Juhan Nam"
            ],
            "title": "Joint detection and classification of singing voice melody using convolutional recurrent neural networks",
            "venue": "Applied Sciences, vol. 9, no. 7, pp. 1324, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Qiuqiang Kong",
                "Yin Cao",
                "Haohe Liu",
                "Keunwoo Choi",
                "Yuxuan Wang"
            ],
            "title": "Decoupling magnitude and phase estimation with deep resunet for music source separation",
            "venue": "Proc. ISMIR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Paszke"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Neural Information Processing Systems, 2019, vol. 32.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Proc. ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Andrew McLeod",
                "Mark Steedman"
            ],
            "title": "Evaluating automatic polyphonic music transcription",
            "venue": "Proc. ISMIR, 2018, pp. 42\u201349.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Time-frequency, Perceiver, automatic music transcription, multi-task learning, random-mixing augmentation.\n1. INTRODUCTION\nAutomatic music transcription (AMT) is a Music Information Retrieval (MIR) task that aims to transcribe a music audio input into a sequence of musical notes where each note contains attributes of onset, pitch, duration, and velocity. The output is typically delivered in the format of MIDI. In a multitrack setting, an AMT system should identify every instrument that is present in the input, and estimate the associated notes accordingly into a channel of the MIDI output. Ideally speaking, using the identified instrument for each corresponding channel, the synthesized audio mixture from the output MIDI should resemble the original input audio in a musically plausible way.\nAlthough recent years have seen significant progress using deep learning techniques [1, 2], our analysis and review indicate that two major challenges have not yet been addressed effectively: model scalability and instrument discrimination. Multitrack AMT is generally regarded as a very challenging task. The number of commonly used instruments can be up to 100. Among them, musical notes of regular instruments like guitar, violin, and synthesizers are difficult to characterize due to their tremendous variations in timbre, expressivity, and playing techniques. Other than that, vocals, which usually are the most predominant instrument if present, vary their timbre and pitch to convey lyrics and expressions. To handle all instruments simultaneously, it requires better model scalability. Our observations on existing multitack AMT systems reveal that they oftentimes result in many false positive notes for popular pitched instruments like\npiano and guitar. For instance, notes of string ensemble are massively captured by piano. This might be because the system does not provide clear timbre-dependent features or it is not robust to timbral variations across different instruments. We believe this problem can be mitigated if the system can discriminate each instrument source from the mixture while making inference.\nTo address model scalability, we propose Perceiver TF, which is an augmented variant of the Perceiver [3]. Perceiver has been well-known for its better scalability in the Transformer family to tackle high-dimensional data input. In this work, we adopt spectrogram for audio input, with T and F representing the lengths of the time- and frequency-axes, respectively. For multitrack AMT, capability to model the timbre-dependent pitches of multiple instruments is crucial, so more comprehensive operations are needed to capture useful features along the high-resolution frequency axis. Recently, the SpecTNT architecture [4] was proposed for this purpose and achieved state-of-the-art performance in vocal melody extraction (a sub-task of AMT). SpecTNT consists of two Transformers in a hierarchical structure, where the lower-level Transformer performs self-attention directly on the spectrum of a frame. However, such design leads to a cubic complexity of attention computation, i.e., O(TF 2 + T 2), limiting its expandability for more complex tasks. To this end, we conceive a non-trivial combination of Perceiver and SpecTNT: expanding Perceiver to be hierarchical. The resulting Perceiver TF takes advantage of the cross-attention to extract spectral features into a latent bottleneck for each frame, and adds an additional Transformer for self-attention along the time axis, overall resulting in a quadratic complexity of O(TF +T 2). Since F is typically large, this complexity reduction is significant, allowing the model to handle more instruments simultaneously.\nTo address instrument discrimination, we adopt the randommixing augmentation technique learned from music source separation (MSS) [5, 6], which aims to separate each instrument stem from the input audio mixture [7]. Moreover, we train our AMT model in a multi-task learning fashion, with each sub-task modeling the transcription of an instrument. This multi-task design along with the random-mixing technique allows more flexibility to train with enormous amount of augmented training samples. Our strategy differs from previous works that jointly train the AMT task with instrument recognition [8] or MSS [9] to help inform the model of instrumentdependent features. To our knowledge, little work has been done using random-mixing technique to improve multitrack AMT.\n2. RELATED WORK\nMulti-instrument AMT has been explored in several previous works. Wu et al. [10] and Hung et al. [8] trained a transcription model with related tasks in a multi-task learning fashion. Tanaka et al. used clustering approaches to separate transcribed instruments [11], while Cheuk et al. used unsupervised learning techniques to im-\nar X\niv :2\n30 6.\n10 78\n5v 1\n[ cs\n.S D\n] 1\n9 Ju\nn 20\n23\nprove transcription on low-resource datasets [1, 12]. These prior examples demonstrated that models based on the pianoroll representation are able to capture instrument-dependent onset, pitch, and duration of notes. Different from the pianoroll approach, Gardner et al. [2] created a new paradigm that proposes a sequence-to-sequence model, called MT3, to tackle multitrack AMT. They trained a standard encoder-decoder Transformer to model multitrack MIDI tokens from audio, and demonstrated state-of-the-art performance on several public datasets.\nBy contrast, vocal transcription is usually treated as an independent task in the literature, even though it shares the same goal of AMT. Due to the lack of training data, few works focused on transcribing note-level outputs from polyphonic music audio. Recently, Wang et al. released a human annotated dataset including 500 Chinese songs [13]. They provide a CNN based model (EFN) for a baseline of the task. In [14], a teacher-student training scheme is proposed to utilize pseudo labels derived from F0 estimations of vocal. Lately, [15] proposed a vocal transcription system that requires an MSS as front-end. In this work, we propose a unified framework that combines vocal and multi-instrument transcriptions, and it does not rely on pre-trained modules such as an MSS front-end.\n3. METHODOLOGY\nIn this work, we adopt the pianoroll approach instead of the sequence-to-sequence (seq-to-seq) approach for two major reasons. First, it is easier to manipulate the loss computation to learn from partially labeled data. For example, it is non-trivial to train a seq-to-seq model that joints a vocal transcription dataset where the MIDI ground truth of accompaniments is not available. Second, the inference time complexity of seq-to-seq depends on the number of notes (tokens) due to the auto-regressive nature. If the audio input contains many instruments with complex, dense polyphonic notes, the inference will be very slow. Although our proposed model is\nalso a Transformer-oriented architecture, we focus on the encoder part to predict the pianoroll directly.\nThe following sections explain the proposed model architecture (Sections 3.1 \u2013 3.3) and the random-mixing augmentation technique (Section 3.4). Our model consists of three sub-modules: convolutional module, Perceiver TF module, and output module. The input spectrogram is first passed through the convolution module for local feature aggregation. Then, the perceiver TF module, which includes multiple Perceiver TF blocks, extracts the features and outputs the temporal embeddings at each time step. Lastly, the output module projects the temporal embeddings into desired dimensions for pianoroll outputs."
        },
        {
            "heading": "3.1. Convolutional Module",
            "text": "Using convolutional neural network (CNN) as the front-end of Transformer-based models has became a common design choice in speech recognition pipeline [16]. Previous works have also found that the CNN front-end plays an crucial role in SpecTNT and MIRTransformer for many MIR tasks [4, 17, 18, 19]. Following this practice, we stack multiple residual units [20] with average pooling to reduce the dimensionality of the frequency axis. We denote the resulting time-frequency representation as S = [S0, S1, . . . , ST\u22121] \u2208 RT\u00d7F\u00d7C , where T , F , and C represent the dimensions of time, frequency, and channel, respectively."
        },
        {
            "heading": "3.2. Perceiver TF Module",
            "text": "A conventional Perceiver architecture contains two major components [3]: (i) a cross-attention module that maps the input data and a latent array into a latent array; (ii) a Transformer tower that maps a latent array into a latent array. Upon this structure, our design principle to expand Perceiver is twofold. (1) We consider the spectral representation of a time step, St, is pivotal to carry the pitch and timbral information, so it serves as the input data for the cross-attention module to project the spectral information into a latent array for the time step t. Each latent array is responsible for extracting the local spectral features. (2) Having a sequence of latent arrays of different time steps, we need a Transformer to exchange the local spectral information along the time axis to learn their temporal coherence.\nThe Perceiver TF architecture is illustrated in Fig. 1. A Perceiver TF block contains three Transformer-style modules: spectral crossattention, latent Transformer, and temporal Transformer, which are responsible for modeling the spectral, channel-wise, and temporal information, respectively. Each of them includes the attention mechanism and a position-wise feed-forward network.\nThe spectral cross-attention (SCA) module operates directly on an input spectral representation St and projects it into the Key (K) and Value (V ) matrices. Unlike the traditional Transformer, the cross-attention module in Perceiver maps a latent array into the Query (Q) matrix and then performs the QKV self-attention accordingly. We follow the Perceiver design to initialize a set of K learnable latent arrays \u03980 \u2208 RK\u00d7D , where K is the index dimension, and D is the channel dimension. Then, we repeat \u03980 for T times and associate each to a time step t, which is then denoted as \u03980t , such that \u039800 = \u039801 = . . .\u03980T\u22121, meaning that all latent arrays are from the same initialization across the time axis. This \u0398ht plays an important role of carrying the spectral information from the first Perceiver TF block throughout the entire stack of blocks. The query-key-value (QKV ) attention of our SCA of the h-th iteration can be written as: fSCA : {\u0398ht , St} \u2192 \u0398 (h+1) t , and this process will be repeated as the Perceiver TF block repeats in order to maintain\nthe connection between \u0398ht and the input St. The design of the cross-attention module is the key that significantly improves the computational scalability of Perceiver. For instance, our SCA results in O(FK), which is much cheaper than O(F 2) of the spectral Transformer in SpecTNT [4], given that K (dimension of the latent array) is typically small (i.e., K \u226a F ).\nThe latent Transformer module takes place after the SCA module. It contains a stack of N Transformers to perform standard self-attention on the latent arrays of \u0398ht . The resulting complexity O(NK2) is efficient as well. In the context of AMT, this process means the interactions among the onsets, pitches, and instruments are explicitly modeled. To perform multitrack AMT, we initialize K latent arrays and train each latent array to handle one specific task. Following [21], for an instrument, we arrange two latent arrays to model the onset and frame-wise (pitch) activations, respectively. This leads to K = 2J , where J is the number of target instruments.\nThe temporal Transformer module is placed to enable the communication between any pairs of \u0398ht of different time steps. To make the temporal Transformer understand the time positions of each latent array, we add a trainable positional embedding to each \u03980t during the initialization. Let \u03b8ht (k), k = 0, . . . , K\u20131, denote each latent array in \u0398ht , we arrange K parallel standard Transformers in which each serves the corresponding input sequence of latent arrays: [\u03b8h0 (k), \u03b8h1 (k), . . . , \u03b8hT\u22121(k)]. The module is repeated M times, yielding a complexity of O(MT 2).\nFinally, we repeat L times the Perceiver TF block to form the overall module. Note that, different from the original Perceiver, the weights of spectral cross-attention and latent Transformer are not shared across the repeated blocks."
        },
        {
            "heading": "3.3. Output Module",
            "text": "We utilize two GRU modules [22] with sigmoid activation function for the onset and frame-wise latent array outputs, respectively. We follow prior work [21] that uses the onset outputs to condition the frame-wise activation."
        },
        {
            "heading": "3.4. Multi-task Training Loss",
            "text": "We formulate the loss function for training the proposed model:\nL = J\u22121\u2211 j=0 (ljonset + l j frame) (1)\nwhere l is the binary cross-entropy loss between the ground-truth and prediction, ljonset and l j frame are respectively the onset and frame activation losses for instrument j. Note that the losses for all J instruments should be computed, regardless of whether the corresponding instruments are active or not in a training sample. Therefore, a zero output is expected for instruments that are not present in the sample.\n4. EXPERIMENTS"
        },
        {
            "heading": "4.1. Datasets",
            "text": "We use four public datasets for evaluation. Slakh2100 [23] contains 2100 pieces of multitrack MIDI and the corresponding synthesized audio. The MIDI files are a subset of Lakh dataset [24], and the audio samples were synthesized by professional-grade software. Instruments were grouped into 12 MIDI classes defined in the Slakh dataset.1 We used the official train/validation/test splits in our exper-\n1There is no \u201dSound Effects\u201d, \u201dPercussive\u201d and \u201dEthnic\u201d instruments. We grouped \u201dStrings\u201d and \u201dEnsemble\u201d into one instrument class.\niments. MAESTROv3 [25] contains about 200 hours of piano solo recordings with the aligned note annotations acquired by the MIDI capturing device on piano. We follow the official train/validation/test splits. GuitarSet [26] contains 360 high-quality guitar recordings and their synchronized note annotations. Since there is no official splits for this dataset, we follow the setting in [2]. The first two progressions of each style are used for training, and the last one is for testing. MIR-ST500 [13] contains 500 Chinese-pop songs with note annotations for the lead vocal melody. We used the official train-test split. Although around 10% of the training set is missing due to failure links, we ensure the testing set is complete for fair comparison."
        },
        {
            "heading": "4.2. Data Augmentations",
            "text": "Annotating data for multitrack AMT is labor intensive. To better exploit the data at hand, we apply two data augmentation techniques during training. Following previous works [4, 27], pitch-shifting is randomly performed to all the non-percussive instruments during training. We introduce the cross-dataset random-mixing (RM) technique. Let us first define three types of datasets:\n\u2022 Multi-track: each sample contains multi-tracks of instrumentwise audio stems with polyphonic notes (e.g., Slakh), and no vocal signals are present.\n\u2022 Single-track: each sample contains only a single non-vocal stem with polyphonic notes (e.g., MAESTRO and GuitarSet).\n\u2022 Vocal-mixture: each sample is a full mixture of music with monophonic notes only for lead vocal (e.g., MIR-ST500). We employ a MSS tool [28] to separate each sample into vocal and accompaniment stems.\nEach training sample is excerpted from a random moment of its original song with a duration depending on the model input length (e.g., 6 seconds). Suppose we want to transcribe J classes of instruments, and the corresponding instrument set is denoted as \u2126 = {\u03c9j}J\u22121j=0 . Then, we apply three treatments to the three mentioned types of datasets respectively as follows.\nFirst, for a training sample si from a multi-track dataset, we denote its instrumentation template as \u00b5i \u2286 \u2126, indicating the instruments present in si. Then, for each instrument \u03c9j in \u00b5i, it has a p% chance to be replaced by a \u03c9j in \u00b5u, where i \u0338= u (i.e., a different sample). Second, for a sample si from a single-track dataset, we randomly pick an existing instrumentation template \u00b5u (i \u0338= u) as its background. If the instrument of si is present in \u00b5u, that stem will be removed from \u00b5u. For instance, if si is a piano solo, then we will remove the piano stem from \u00b5u. From our preliminary experiment, presenting the solo example to model training without mixing it with a background can degrade the performance. Lastly, for a sample si from a vocal-mixture dataset, it has a q% chance to replace its background by two methods: (i) like the single-track treatment, we randomly pick an existing \u00b5u (i \u0338= u) as its background; or (ii) we randomly pick an accompaniment stem separated from sv , where i \u0338= v. For the second method, since the selected accompaniment stem does not have the ground-truth notes, we mask the instrument outputs and only count the loss for the vocal output (see Eq. 1)."
        },
        {
            "heading": "4.3. Implementation Details",
            "text": "We implemented our system using PyTorch [29]. The audio waveform is re-sampled to 16kHz sampling rate. We set the model input length to be 6 seconds. The log-magnitude spectrogram is then computed using 2048 samples of Hann window and a hop size of 320 samples (i.e., 20 ms). The convolutional module contains 3 residual\nblocks, each of them has 128 channels and is followed by an average pooling layer with a time-frequency filter of (1, 2).\nFor the Perceiver TF module, we use the following parameters (referring to Fig. 1): (i) depending on different experiment configurations, initialize 2J latent arrays, each uses a dimension of 128; (ii) stack L = 3 Perceiver TF blocks; (iii) for each Perceiver TF block, use 1 spectral cross-attention layer, N = 2 latent Transformer layers, and M = 2 temporal Transformer layers. All the Transformer layers has an hidden size of 128 with 8 heads for the multi-head attention. Finally, the output module is a 2-layer Bi-directional GRU with 128 hidden units. All of the Transformer module in the Perceiver TF include dropout with a rate of 0.15. The output dimension for onset and frame activations are 128 and 129, respectively, where 128 corresponds to the MIDI pitches, and the additional 1 dimension in the frame activation is for the silence. We use AdamW [30] as the learning optimizer. The initial learning rate and weight decay rate are set to 10\u22123 and 5\u00d7 10\u22123, respectively.\nFor final output, we take a threshold of 0.25 for both the onset and frame probability outputs to get the binary representations, so the frame-wise activations can be merged to generate each note in a piano-roll representation. No further post-processing is applied.\nFor data augmentation, all of the non-percussive instruments of a training example have a 100% probability to be pith-shift up or down by at most 3 semi-tones. For random-mixing, we use p = 25% and q = 50% for data from multi-track and vocal-mixture datasets, respectively. To generate an input sample, all the instrument stems in each training example are linearly summed up."
        },
        {
            "heading": "4.4. Baselines",
            "text": "Two state-of-the-art models, MT3 [2] and SpecTNT [4], are selected as the baselines. For MT3, we replicated the model following [2],2 which includes the official model checkpoint and inference pipeline\n2https://github.com/magenta/mt3/blob/main/mt3/colab/ music transcription with transformers.ipynb\non the test set. For SpecTNT, we adopted the configuration used for vocal melody extraction reported in [4]. In the preliminary experiments, we found it non-trivial to successfully train the original SpecTNT on Slakh2100 under the multi-instrument setting, so we skip this experiment. For vocal transcription, the best results of EFN [13] and JDCnote(L+U) [14] are reported."
        },
        {
            "heading": "4.5. Evaluation Metrics",
            "text": "We use \u201cOnset F1\u201d score, which indicates the correctness of both pitches and onset timestamps, as the evaluation metric for comparison with previous work [2]. To further evaluate the performance of multi-instrument transcription, we report the \u201dMulti-instrument Onset F1\u201d score for the Slakh dataset. The outputs from our replicated MT3 model are grouped into 12 instrument classes based on their program numbers. The Multi-instrument Onset F1 score we used only counts Onset F1, which is similar to the MV2H metric [31]. It could be slightly different from the one used in [2], since the \u201cDrums\u201d outputs do not contain clear offset information."
        },
        {
            "heading": "4.6. Result and Discussion",
            "text": "Table 1 shows the comparison in terms of Onset F1 between the proposed model and baselines. The proposed model and SpecTNT which directly model the spectral inputs with the attention mechanism shows higher performance for cases even trained on low resources of a single dataset, such as GuitarSet. On MIR-ST500, the proposed model significantly outperforms the baselines. Although SpecTNT (Single) performs slightly better than our model on MAESTRO, we still consider Perceiver TF to be more advantageous to practical use for its better inference efficiency.\nTable 2 presents the Multi-instrument Onset F1 (instrumentweighted average) and the Onset F1 scores of individual instrument classes on Slakh2100 to reveal instrument-wise performance. Compared to MT3\u2020, our model without the random-mixing augmentation (No-RM) performs significantly better on less-common instruments such as \u201cPipe\u201d (the Onset F1 score is upper by over 100%). Applying random-mixing in training can further boost the performance in all cases, indicating the technique indeed improves the model robustness to discriminate between different instruments. Finally, we observe that combining multi-instrument and vocal transcriptions can improve the vocal transcription alone, as the combined model is trained with more randomly mixed vocal-accompaniment samples.\n5. CONCLUSION\nWe have presented Perceiver TF, a novel architecture that adequately addresses the model scalability problem for multitrack AMT. To address the instrument discrimination issue, we have proposed the random-mixing augmentation technique, which significantly facilitates the data usability across datasets. Our system has demonstrated state-of-the-art performance on different public datasets. We believe Perceiver TF is generic and can be applied to other analogous tasks.\n6. REFERENCES\n[1] Kin Wai Cheuk, Dorien Herremans, and Li Su, \u201cReconvat: A semi-supervised automatic music transcription framework for low-resource real-world data,\u201d in Proc. ACM Multimedia, 2021, pp. 3918\u20133926.\n[2] Josh Gardner, Ian Simon, Ethan Manilow, Curtis Hawthorne, and Jesse Engel, \u201cMT3: Multi-task multitrack music transcription,\u201d in Proc. ICLR, 2021.\n[3] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira, \u201cPerceiver: General perception with iterative attention,\u201d in Proc. ICML, 2021, pp. 4651\u20134664.\n[4] Wei-Tsung Lu, Ju-Chiang Wang, Minz Won, Keunwoo Choi, and Xuchen Song, \u201cSpecTNT: A time-frequency transformer for music audio,\u201d in Proc. ISMIR, 2021.\n[5] Stefan Uhlich, Marcello Porcu, Franck Giron, Michael Enenkl, Thomas Kemp, Naoya Takahashi, and Yuki Mitsufuji, \u201cImproving music source separation based on deep neural networks through data augmentation and network blending,\u201d in Proc. ICASSP, 2017, pp. 261\u2013265.\n[6] Xuchen Song, Qiuqiang Kong, Xingjian Du, and Yuxuan Wang, \u201cCatnet: Music source separation system with mixaudio augmentation,\u201d arXiv preprint arXiv:2102.09966, 2021.\n[7] Zafar Rafii, Antoine Liutkus, Fabian-Robert Sto\u0308ter, Stylianos Ioannis Mimilakis, Derry FitzGerald, and Bryan Pardo, \u201cAn overview of lead and accompaniment separation in music,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 8, pp. 1307\u20131335, 2018.\n[8] Yun-Ning Hung, Yi-An Chen, and Yi-Hsuan Yang, \u201cMultitask learning for frame-level instrument recognition,\u201d in Proc. ICASSP, 2019, pp. 381\u2013385.\n[9] Andreas Jansson, Rachel M Bittner, Sebastian Ewert, and Tillman Weyde, \u201cJoint singing voice separation and f0 estimation with deep u-net architectures,\u201d in Proc. EUSIPCO, 2019, pp. 1\u20135.\n[10] Yu-Te Wu, Berlin Chen, and Li Su, \u201cMulti-instrument automatic music transcription with self-attention-based instance segmentation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2796\u20132809, 2020.\n[11] Keitaro Tanaka, Takayuki Nakatsuka, Ryo Nishikimi, Kazuyoshi Yoshii, and Shigeo Morishima, \u201cMulti-instrument music transcription based on deep spherical clustering of spectrograms and pitchgrams,\u201d in Proc. ISMIR, 2020.\n[12] Kin Wai Cheuk, Keunwoo Choi, Qiuqiang Kong, Bochen Li, Minz Won, Amy Hung, Ju-Chiang Wang, and Dorien Herremans, \u201cJointist: Joint learning for multi-instrument transcription and its applications,\u201d arXiv preprint arXiv:2206.10805, 2022.\n[13] Jun-You Wang and Jyh-Shing Roger Jang, \u201cOn the preparation and validation of a large-scale dataset of singing transcription,\u201d in Proc. ICASSP, 2021, pp. 276\u2013280.\n[14] Sangeun Kum, Jongpil Lee, Keunhyoung Luke Kim, Taehyoung Kim, and Juhan Nam, \u201cPseudo-label transfer from framelevel to note-level in a teacher-student framework for singing transcription from polyphonic music,\u201d in Proc. ICASSP, 2022.\n[15] Jui-Yang Hsu and Li Su, \u201cVocano: A note transcription framework for singing voice in polyphonic music.,\u201d in Proc. ISMIR, 2021, pp. 293\u2013300.\n[16] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., \u201cConformer: Convolutionaugmented transformer for speech recognition,\u201d in Proc. INTERSPEECH, 2020.\n[17] M. Won, K. Choi, and X. Serra, \u201cSemi-supervised music tagging transformer,\u201d in Proc. ISMIR, 2021, pp. 769\u2013776.\n[18] Yun-Ning Hung, Ju-Chiang Wang, Xuchen Song, Wei-Tsung Lu, and Minz Won, \u201cModeling beats and downbeats with a time-frequency transformer,\u201d in Proc. ICASSP, 2022, pp. 401\u2013 405.\n[19] Ju-Chiang Wang, Yun-Ning Hung, and Jordan B. L. Smith, \u201cTo catch a chorus, verse, intro, or anything else: Analyzing a song with structural functions,\u201d in Proc. ICASSP, 2022, pp. 416\u2013 420.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cIdentity mappings in deep residual networks,\u201d in European conference on computer vision. Springer, 2016, pp. 630\u2013645.\n[21] Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, and Douglas Eck, \u201cOnsets and frames: Dual-objective piano transcription,\u201d in Proc. ISMIR, 2018, pp. 50\u201357.\n[22] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d in Proc. NeurIPS, 2014.\n[23] Ethan Manilow, Gordon Wichern, Prem Seetharaman, and Jonathan Le Roux, \u201cCutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity,\u201d in Proc. WASPAA. IEEE, 2019.\n[24] Colin Raffel, \u201cLearning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching,\u201d 2016, Columbia University.\n[25] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas Eck, \u201cEnabling factorized piano music modeling and generation with the maestro dataset,\u201d in Proc. ICLR, 2019.\n[26] Qingyang Xi, Rachel M Bittner, Johan Pauwels, Xuzhou Ye, and Juan Pablo Bello, \u201cGuitarset: A dataset for guitar transcription.,\u201d in Proc. ISMIR, 2018, pp. 453\u2013460.\n[27] Sangeun Kum and Juhan Nam, \u201cJoint detection and classification of singing voice melody using convolutional recurrent neural networks,\u201d Applied Sciences, vol. 9, no. 7, pp. 1324, 2019.\n[28] Qiuqiang Kong, Yin Cao, Haohe Liu, Keunwoo Choi, and Yuxuan Wang, \u201cDecoupling magnitude and phase estimation with deep resunet for music source separation.,\u201d in Proc. ISMIR, 2021.\n[29] Paszke et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d in Neural Information Processing Systems, 2019, vol. 32.\n[30] Ilya Loshchilov and Frank Hutter, \u201cDecoupled weight decay regularization,\u201d in Proc. ICLR, 2017.\n[31] Andrew McLeod and Mark Steedman, \u201cEvaluating automatic polyphonic music transcription.,\u201d in Proc. ISMIR, 2018, pp. 42\u201349."
        }
    ],
    "title": "MULTITRACK MUSIC TRANSCRIPTION WITH A TIME-FREQUENCY PERCEIVER",
    "year": 2023
}