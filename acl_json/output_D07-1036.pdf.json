{
    "abstractText": "Parallel corpus is an indispensable resource for translation model training in statistical machine translation (SMT). Instead of collecting more and more parallel training corpora, this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora. Two kinds of methods are proposed: offline data optimization and online model optimization. The offline method adapts the training data by redistributing the weight of each training sentence pairs. The online method adapts the translation model by redistributing the weight of each predefined submodels. Information retrieval model is used for the weighting scheme in both methods. Experimental results show that without using any additional resource, both methods can improve SMT performance significantly.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yajuan L\u00fc"
        },
        {
            "affiliations": [],
            "name": "Jin Huang"
        },
        {
            "affiliations": [],
            "name": "Qun Liu"
        }
    ],
    "id": "SP:ff5bddea58f349daf6a714a3eac8b62ce5102063",
    "references": [
        {
            "authors": [
                "Jisong Chen",
                "Rowena Chau",
                "Chung-Hsing Yeh"
            ],
            "title": "Discovering Parallel Text from the World Wide Web",
            "venue": "ACSW Frontiers 2004: 157-161",
            "year": 2004
        },
        {
            "authors": [
                "Stanley F. Chen",
                "Joshua Goodman."
            ],
            "title": "An Empirical Study of Smoothing Techniques for Language Modeling",
            "venue": "Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.",
            "year": 1998
        },
        {
            "authors": [
                "Matthias Eck",
                "Stephan Vogel",
                "Alex Waibel"
            ],
            "title": "Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval",
            "venue": "Proceedings of Fourth International Conference on Language Resources and Evaluation:327-330",
            "year": 2004
        },
        {
            "authors": [
                "Matthias Eck",
                "Stephan Vogel",
                "Alex Waibel"
            ],
            "title": "Low cost portability for statistical machine translation based on n-gram coverage",
            "venue": "MT Summit X: 227-234.",
            "year": 2005
        },
        {
            "authors": [
                "Zhongjun He",
                "Yang Liu",
                "Deyi Xiong",
                "Hongxu Hou",
                "Qun Liu"
            ],
            "title": "ICT System Description for the 2006 TC-STAR Run#2 SLT Evaluation",
            "venue": "Proceedings of TCSTAR Workshop on Speech-to-Speech Translation: 63-68",
            "year": 2006
        },
        {
            "authors": [
                "Philipp Koehn",
                "Franz J. Och",
                "Daniel Marcu."
            ],
            "title": "Statistical phrase-based translation",
            "venue": "Proceedings of HLT-NAACL 2003: 127\u2013133.",
            "year": 2003
        },
        {
            "authors": [
                "Arne Mauser",
                "Richard Zens",
                "Evgeny Matusov",
                "Sasa Hasan",
                "Hermann Ney"
            ],
            "title": "The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation",
            "venue": "Proceedings of International Workshop on Spoken Language Translation.:103-110",
            "year": 2006
        },
        {
            "authors": [
                "Dragos Stefan Munteanu",
                "Daniel Marcu"
            ],
            "title": "Improving Machine Translation Performance by Exploiting Comparable Corpora",
            "venue": "Computational Linguistics, 31 (4): 477-504",
            "year": 2005
        },
        {
            "authors": [
                "Dragos Stefan Munteanu",
                "Daniel Marcu"
            ],
            "title": "Extracting Parallel Sub-Sentential Fragments from Comparable Corpora",
            "venue": "ACL-2006: 81-88",
            "year": 2006
        },
        {
            "authors": [
                "Jian-Yun Nie",
                "Michel Simard",
                "Pierre Isabelle",
                "Richard Durand"
            ],
            "title": "Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web",
            "venue": "SIGIR-1999: 74-81",
            "year": 1999
        },
        {
            "authors": [
                "Franz Josef Och"
            ],
            "title": "Minimum Error Rate Training in Statistical Machine Translation",
            "venue": "ACL-2003:160-167.",
            "year": 2003
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
            "venue": "ACL-2002: 311\u2013 318",
            "year": 2002
        },
        {
            "authors": [
                "Philip Resnik",
                "Noah A. Smith"
            ],
            "title": "The Web as a Parallel Corpus",
            "venue": "Computational Linguistics 29(3): 349-380",
            "year": 2003
        },
        {
            "authors": [
                "Almut Silja Hildebrand",
                "Matthias Eck",
                "Stephan Vogel",
                "Alex Waibel"
            ],
            "title": "Adaptation of the Translation Model for Statistical Machine Translation based on Information Retrieval",
            "venue": "Proceedings of EAMT 2005: 133-142.",
            "year": 2005
        },
        {
            "authors": [
                "Richard Zens",
                "Franz Josef Och",
                "Hermann Ney"
            ],
            "title": "Phrase-Based Statistical Machine Translation",
            "venue": "Annual German Conference on AI, KI 2002, Vol. LNAI 2479: 18-32",
            "year": 2002
        },
        {
            "authors": [
                "Ying Zhang",
                "Almut Silja Hildebrand",
                "Stephan Vogel"
            ],
            "title": "Distributed Language Modeling for N-best List Re-ranking",
            "venue": "EMNLP-2006:216-223",
            "year": 2006
        },
        {
            "authors": [
                "Bing Zhao",
                "Matthias Eck",
                "Stephan Vogel"
            ],
            "title": "Language Model Adaptation for Statistical Machine Translation with structured query models",
            "venue": "COLING2004",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 343\u2013350, Prague, June 2007. c\u00a92007 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Statistical machine translation relies heavily on the available training data. Typically, the more data is used to estimate the parameters of the translation model, the better it can approximate the true translation probabilities, which will obviously lead to a higher translation performance. However, large corpora are not easily available. The collected corpora are usually from very different areas. For example, the parallel corpora provided by LDC come from quite different domains, such as Hongkong laws, Hangkong Hansards and Hongkong news. This results in the problem that a translation system trained on data from a particular\ndomain(e.g. Hongkong Hansards) will perform poorly when translating text from a different domain(e.g. news articles). Our experiments also show that simply putting all these domain specific corpora together will not always improve translation quality. From another aspect, larger amount of training data also requires larger computational resources. With the increasing of training data, the improvement of translation quality will become smaller and smaller. Therefore, while keeping collecting more and more parallel corpora, it is also important to seek effective ways of making better use of available parallel training data.\nThere are two cases when we train a SMT system. In one case, we know the target test set or target test domain, for example, when building a specific domain SMT system or when participating the NIST MT evaluation1. In the other case, we are unaware of any information of the testing data. This paper presents two methods to exploit full potential of the available parallel corpora in the two cases. For the first case, we try to optimize the training data offline to make it match the test data better in domain, topic and style, thus improving the translation performance. For the second case, we first divide the training data into several domains and train submodels for each domain. Then, in the translation process, we try to optimize the predefined models according to the online input source sentence. Information retrieval model is used for similar sentences retrieval in both methods. Our preliminary experiments show that both methods can improve SMT performance without using any additional data.\n1 http://www.nist.gov/speech/tests/mt/\n343\nThe remainder of this paper is organized as follows: Section 2 describes the offline data selection and optimization method. Section 3 describes the online model optimization method. The evaluation and discussion are given in section 4. Related work is introduced before concluding."
        },
        {
            "heading": "2 Offline training data optimization",
            "text": "In offline training data optimization, we assume that the target test data or target test domain is known before building the translation model. We first select sentences similar to the test text using information retrieval method to construct a small and adapted training data. Then the extracted similar subset is used to optimize the distribution of the whole training data. The adapted and the optimized training data will be used to train new translation models."
        },
        {
            "heading": "2.1 Similar data selection using TF-IDF",
            "text": "We use information retrieval method for similar data retrieval. The standard TF-IDF (Term Frequency and Inverse Document Frequency) term weighting scheme is used to measure the similarity between the test sentence and the training sentence.\nTF-IDF is a similarity measure widely used in information retrieval. Each document i is represented as a vector , is the size of the vocabulary. is calculate as follows: D ),...,,( 21 inii www n\nijw )log( jijij idftfw \u00d7=\nwhere, ij is the term frequency(TF) of the j-th word in the vocabulary in the document , i.e. the number of occurrences; tf iD j is the inverse document frequency(IDF) of the j-th word calculated as below: idf\nth term-j # # containingdocuments documentsidf j = .\nThe similarity between two documents is then defined as the cosine of the angle between the two vectors.\nWe perform information retrieval using the Lemur toolkit2. The source language part of the parallel training data is used as the document collection. Each sentence represents one document. Each sentence from the test data or test domain is used as one separate query. In the sentence retrieval 2 http://www.cs.cmu.edu/~lemur/\nprocess, both the query and the document are converted into vectors by assigning a term weight to each word. Then the cosine similarity is calculated proportional to the inner product of the two vectors. All retrieved sentences are ranked according to their similarity with the query. We pair each of the retrieved sentences with the corresponding target part and the top N most similar sentences pairs are put together to form an adapted parallel data. N ranges from one to several thousand in our experiments. Since Lemur toolkit gives the similarity score for each retrieved sentences, it is also possible to select the most similar sentences according to the similarity score.\nNote that the selected similar data can contain duplicate sentences as the top N retrieval results for different test sentences can contain the same training sentences. The duplicate sentences will force the translation probability towards the more often seen words. Intuitively, this could help. In experiment section, we will compare experimental results by keeping or removing duplicates to see how the duplicate sentences affect the translations.\nThe selected subset contains the similar sentences with the test data or test domain. It matches the test data better in domain, topic and style. Hopefully, training translation model using this adapted parallel data may helpful for improving translation performance. In addition, the translation model trained using the selected subset is usually much smaller than that trained using the whole translation data. Limiting the size of translation model is very important for some real applications. Since SMT systems usually require large computation resource. The complexity of standard training and decoding algorithm depends mainly on the size of the parallel training data and the size of the translation model. Limiting the size of the training data with the similar translation performance would also reduce the memories and speed up the translations.\nIn the information retrieval process, we only use the source language part for document indexing and query generating. It is easy to get source part of the test data. This is different from the common language model adaptation methods, which have to do at lease one pass machine translation to get the candidate English translation as query(Zhao 2004, Zhang 2006). So our method has the advantage that it is independent from the quality of baseline translation system."
        },
        {
            "heading": "2.2 Training data optimization",
            "text": "There are two factors on training data that influence the translation performance of SMT system: the scale and the quality. In some sense, we improve the quality of the training data by selecting the similar sentence to form an adapted training set. However, we also reduce the scale of the training data at the same time. Although this is helpful for some small device applications, it is also possible to induce the data sparseness problem. Here, we introduce a method to optimize between the scale and the quality of the training data.\nThe basic idea is that we still use all the available training data; by redistributing the weight of each sentence pairs we adapt the whole training data to the test domain. In our experiments, we simply combine the selected small similar subset and the whole training data. The weights of each sentence pairs are changed accordingly. Figure 1 shows the procedure of the optimization.\nAs can be seen, through the optimization, the weight of the similar sentence pairs are increased, while the general sentence pairs still have an ordinary weight. This make the translation model inclined to give higher probabilities to the adapted words, and at the same time avoid the data sparseness problem. Since we only change the weight of the sentence pairs, and no new training data is introduced, the translation model size trained on the optimized data will keep as the same as the original one. We use GIZA++ toolkit3 for word align-\n3 http://www.fjoch.com/GIZA++.html\nment training in the training process. The input training file formats for GIZA++ is as follows: Each training sentence pair is stored in three lines. The first line is the number of times this sentence pair occurred. The second line is the source sentence where each token is replaced by its unique integer id and the third is the target sentence in the same format. To deal with our optimized training data, we only need to change the number of sentence pairs in the first line accordingly. This will not call for extra training time and memory for the whole training process.\nIt might be beneficial to investigate other sophisticated weighting schemes under the similar idea, such as to give more precise fractional weights to the sentences according the retrieval similarity scores."
        },
        {
            "heading": "3 Online model optimization",
            "text": "In most circumstances, we don\u2019t know exactly the test data or the test domain when we train a machine translation system. This results in the fact that the performance of the translation system highly depends on the training data and the test data it is used in. To alleviate this blindfold status and maximize the potential of the available training corpora, we propose a novel online model optimization method. The basic idea is that: several candidate translation models are prepared in training stage. In particularly, a general model is also prepared. Then, in the translation process, the similarity between the input sentence and the predefined models is calculated online to get the weights of each model. The optimized model is used to translate the input sentence.\nThere are two problems in the method: how to prepare submodels in training process and how to optimize the model weight online in translation process."
        },
        {
            "heading": "3.1 Prepare the submodels",
            "text": "There are several ways to prepare submodels in training process. If the training data comes from very different sources, we can divide the data according to its origins. Otherwise, we can use clustering method to separate the training corpus into several classes. In addition, our offline data adaptation method can also be used for submodel preparation. For each candidate domain, we can use the\nsource side of a small corpus as queries to extract a domain specific training set. In this case, a sentence pair in the training data may occur in several sub training data, but this doesn\u2019t matter. The general model is used when the online input is not similar to any prepared submodels. We can use all available training data to train the general model since generally larger data can get better model even there are some noises."
        },
        {
            "heading": "3.2 Online model weighting",
            "text": "We also use TF-IDF information retrieval method for online model weighting. The procedure is as follows:\nFor each input sentence: 1. Do IR on training data collection, using the input sentence as query. 2. Determine the weights of submodels according to the retrieved sentences. 3. Use the optimized model to translate the sentence. The information retrieval process is the same as the offline data selection except that each retrieved sentence is attached with the sub-corpus information, i.e. it belongs to which sub-models in the training process.\nWith the sub-corpus information, we can calculate the weights of submodels. We get the top N most similar sentences, and then calculate proportions of each submodel\u2019s sentences. The proportion can be calculated use the count of the sentences or the similarity score of the sentences. The weight of each submodel can be determined according to the proportions.\nOur optimized model is the log linear interpolation of the sub-models as follows:\n\u220f =\n\u00d7= M\ni i icepcepcep 1 0 )|()|()|(\u02c6 0 \u03b4\u03b4\n\u2211 =\n+= M\ni ii e cepcepe 1 00 )))|(log())|(log((maxarg\u02c6 \u03b4\u03b4 where, 0 is the probability of general model, ip is the probability of submodel i. 0 p \u03b4 is the weight of general model. i\u03b4 is the weight of submodel i. Each model i is also implemented using log linear model in our SMT system. So after the log operation, the submodels are interpolated linearly.\nIn our experiments, the interpolation factor i\u03b4 is determined using the following four simple weighting schemes:\nWeighting scheme 1:\n;0 ;1 ;00 === \u2260max_modelimax_model \u03b4\u03b4\u03b4 Weighting scheme 2:\nif Proportion(max_model) > 0.5 Use weighting scheme1; else\n;0 ;1 0 == i\u03b4\u03b4 Weighting scheme 3:\n);(Proportion ;00 ii model= = \u03b4 \u03b4\nWeighting scheme 4:\nif Proportion(max_model) > 0.5 Use weighting scheme3; else\n);( Proportion5.0 ;5.0 0 ii model\u00d7= = \u03b4 \u03b4\nwhere, modeli is the i-th submodel, . Proportion (model )...1( Mi = i) is the proportion of modeli in the retrieved results. We use count for proportion calculation. max_model is the submodel with the max proportion score.\nThe training and translation procedure of online model optimization is illustrated in Figure 2.\nThe online model optimization method makes it possible to select suitable models for each individual test sentence. Since the IR process is done on a fixed training data, the size of the index data is quite small compared with the web IR. The IR process will not take much time in the translation."
        },
        {
            "heading": "4 Experiments and evaluation",
            "text": ""
        },
        {
            "heading": "4.1 Experimental setting",
            "text": "We conduct our experiments on Chinese-toEnglish translation tasks. The baseline system is a variant of the phrase-base SMT system, implemented using log-linear translation model (He et al. 2006). The baseline SMT system is used in all experiments. The only difference between them is that they are trained on different parallel training data.\nIn training process, we use GIZA++4 toolkit for word alignment in both translation directions, and apply \u201cgrow-diag-final\u201d method to refine it (Koehn et al., 2003). We change the preprocess part of GIZA++ toolkit to make it accept the weighted training data. Then we use the same criterion as suggested in (Zens et al., 2002) to do phrase extraction. For the log-linear model training, we take minimum-error-rate training method as described in (Och, 2003). The language model is trained using Xinhua portion of Gigaword with about 190M words. SRI Language Modeling toolkit5 is used to train a 4-gram model with modified Kneser-Ney smoothing(Chen and Goodman, 1998). All experiments use the same language model. This ensures that any differences in performance are caused only by differences in the parallel training data.\nOur training data are from three LDC corpora as shown in Table 1. We random select 200,000 sentence pairs from each corpus and combine them together as the baseline corpus, which includes 16M Chinese words and 19M English words in total. This is the usual case when we train a SMT system, i.e. we simply combine all corpora from different origins to get a larger training corpus.\nWe use the 2002 NIST MT evaluation test data as our development set, and the 2005 NIST MT test data as the test set in offline data optimization experiments. In both data, each sentence has four\n4 http://www.fjoch.com/GIZA++.html 5 http://www.speech.sri.com/projects/srilm/\nhuman translations as references. The translation quality is evaluated by BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl6 with case-sensitive matching of n-grams."
        },
        {
            "heading": "4.2 Baseline experiments",
            "text": "We first train translation models on each sub training corpus and the baseline corpus. The development set is used to tune the feature weights. The results on test set are shown in Table 2.\nFrom the results we can see that although the size of each sub training corpus is similar, the translation results from the corresponding system are quite different on the same test set. It seems that the FBIS corpus is much similar to the test set than the other two corpora. In fact, it is the case. The FBIS contains text mainly from China mainland news stories, while the 2005 NIST test set also include lots of China news text. The results illustrate the importance of selecting suitable training data.\nWhen combining all the sub corpora together, the baseline system gets a little better result than the sub systems. This indicates that larger data is useful even it includes some noise data. However, compared with the FBIS corpus, the baseline corpus contains three times larger data, while the improvement of translation result is not significant. This indicates that simply putting different corpora together is not a good way to make use of the available corpora.\n6http://www.nist.gov/speech/tests/mt/resources/scoring.htm"
        },
        {
            "heading": "4.3 Offline data optimization experiments",
            "text": "We use baseline corpus as initial training corpus, and take Lemur toolkit to build document index on Chinese part of the corpus. The Chinese sentences in development set and test set are used as queries. For each query, N = 100, 200, 500, 1000, 2000 similar sentences are retrieved from the indexed collection. The extracted similar sentence pairs are used to train the new adapted translation models. Table 3 illustrates the results. We give the distinct pair numbers for each adapted set and compare the size of the translation models. To illustrate the effect of duplicate sentences, we also give the results with duplicates and without duplicates (distinct).\nThe results show that: 1. By using similar data selection, it is possible to use much smaller training data to get comparable or even better results than the baseline system. When N=200, using only 1/4 of the training data and 1/3 of the model size, the adapted translation model achieves comparable result with the baseline model. When N=500, the adapted model outperforms the baseline model with much less training data. The results indicate that relevant data is better data. The method is particular useful for SMT applications on small device.\n2. In general, using duplicate data achieves better results than using distinct data. This justifies our idea that give a higher weight to more similar data will benefit.\n3. With the increase of training data size, the translation performance tends to improve also. However, when the size of corpus achieves a certain scale, the performance may drop. This maybe because that with the increase of the data, noisy data may also be included. More and more included noises may destroy the data. It is necessary to use a development set to determine an optimal size of N.\nWe combine each adapted data with the baseline corpus to get the optimized models. The results are shown in Table 4. We also compare the adapted models (TopN) and the optimized models (TopN+) in the table.\nWithout using any additional data, the optimized models achieve significant better results than the baseline model by redistributing the weight of training sentences. The optimized models also outperform adapted models when the size of the adapted data is small since they make use of all the available data which decrease the influence of data sparseness. However, with the increase of the adapted data, the performance of optimized models is similar to that of the adapted models."
        },
        {
            "heading": "4.4 Online model optimization experiments",
            "text": "Since 2005 NIST MT test data tends bias to FBIS corpus too much, we build a new test set to evaluate the online model optimization method. We randomly select 500 sentences from extra part of FBIS, HK_Hansards and HK_News corpus respectively (i.e the selected 1500 test sentences are not included in any of the training set). The corresponding English part is used as translation reference. Note that there is only one reference for each test sentence. We also include top 500 sentence and their first reference translation of 2005 NIST MT test data in the new test set. So in total, the new test contains 2000 test sentences with one translation reference for each sentence. The test set is used to simulate SMT system\u2019s online inputs which may come from various domains.\nThe baseline translation results are shown in Table 5. We also give results on each sub test set (denotes as Xcorpus_part). Please note that the absolute BLEU scores are not comparable to the previous experiments since there is only one reference in this test set.\nAs expected, using the same domain data for training and testing achieves the best results as indicate by bold fonts. The results demonstrate again that relevant data is better data.\nTo test our online model optimization method, we divide the baseline corpus according to the origins of sub corpus. That is, the FBIS, HK_ Hansards and HK_News models are used as three submodels and the baseline model is used as general model. The four weighting schemes described in section 3.2 are used as online weighting schemes individually. The experimental results are shown in Table 6. S_i indicates the system using weighting scheme i.\nDifferent weighting schemes don\u2019t show significant improvements from each other. However, all the four weighting schemes achieve better results than the baseline system. The improvements are shown not only on the whole test set but also on each part of the sub test set. The results justify the effectiveness of our online model optimization method."
        },
        {
            "heading": "5 Related work",
            "text": "Most previous research on SMT training data is focused on parallel data collection. Some work tries to acquire parallel sentences from web (Nie et al. 1999; Resnik and Smith 2003; Chen et al. 2004). Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). These work aims to collect more\nparallel training corpora, while our work aims to make better use of existing parallel corpora.\nSome research has been conducted on parallel data selection and adaptation. Eck et al. (2005) propose a method to select more informative sentences based on n-gram coverage. They use ngrams to estimate the importance of a sentence. The more previously unseen n-grams in the sentence the more important the sentence is. TF-IDF weighting scheme is also tried in their method, but didn\u2019t show improvements over n-grams. This method is independent of test data. Their goal is to decrease the amount of training data to make SMT system adaptable to small devices. Similar to our work, Hildebrand et al. (2005) also use information retrieval method for translation model adaptation. They select sentences similar to the test set from available in-of-domain and out-of-domain training data to form an adapted translation model. Different from their work, our method further use the small adapted data to optimize the distribution of the whole training data. It takes the full advantage of larger data and adapted data. In addition, we also propose an online translation model optimization method, which make it possible to select adapted translation model for each individual sentence.\nSince large scale monolingual corpora are easier to obtain than parallel corpora. There has some research on language model adaptation recent years. Zhao et al. (2004) and Eck et al.(2004) introduce information retrieval method for language model adaptation. Zhang et al.(2006) and Mauser et al.(2006) use adapted language model for SMT re-ranking. Since language model is built for target language in SMT, one pass translation is usually needed to generate n-best translation candidates in language model adaptation. Translation model adaptation doesn\u2019t need a pre-translation procedure. Comparatively, it is more direct. Language model adaptation and translation model adaptation are good complement to each other. It is possible that combine these two adaptation approaches could further improve machine translation performance."
        },
        {
            "heading": "6 Conclusion and future work",
            "text": "This paper presents two new methods to improve statistical machine translation performance by making better use of the available parallel training corpora. The offline data selection method\nadapts the training corpora to the test domain by retrieving similar sentence pairs and redistributing their weight in the training data. Experimental results show that the selected small subset achieves comparable or even better performance than the baseline system with much less training data. The optimized training data can further improve translation performance without using any additional resource. The online model optimization method adapts the translation model to the online test sentence by redistributing the weight of each predefined submodels. Preliminary results show the effectiveness of the method. Our work also demonstrates that in addition to larger training data, more relevant training data is also important for SMT model training.\nIn future work, we will improve our methods in several aspects. Currently, the similar sentence retrieval model and the weighting schemes are very simple. It might work better by trying other sophisticated similarity measure models or using some optimization algorithms to determine submodel\u2019s weights. Introducing language model optimization into our system might further improve translation performance."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by National Natural Science Foundation of China, Contract No. 60603095 and 60573188."
        }
    ],
    "title": "Improving Statistical Machine Translation Performance by Training Data Selection and Optimization",
    "year": 2007
}