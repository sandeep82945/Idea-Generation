Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is α-IP stable if the average distance of every data point to its own cluster is at most α times the average distance to any other cluster. Unfortunately, determining if a dataset admits a 1-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an o(n)-IP stable clustering always exists, as the prior state of the art only guaranteed an O(n)-IP stable clustering. We close this gap in understanding and show that an O(1)-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient, near-optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters. Supported by DFF-International Postdoc Grant 0164-00022B from the Independent Research Fund Denmark. Supported in part by an NSF Graduate Research Fellowship and a Hertz Fellowship Significant part of works was done while P.S. was a Ph.D. candidate at Northwestern University 1 ar X iv :2 30 9. 16 84 0v 1 [ cs .D S] 2 8 Se p 20 23 1 conclusion We presented a deterministic polynomial time algorithm which provides an O(1)-approximate IP stable clustering of n points in a general metric space, improving on prior works which only guaranteed an O(n)-approximate IP stable clustering. We also generalized IP stability to f -stability and provided an algorithm which finds an exact Min-IP stable clustering and a 3-approximation for Max-IP stability, both of which hold for all k and in general metric spaces. Is there an efficient algorithm which returns an O(α∗)-IP stable clustering? • For what specific metrics (other than the line or tree metrics with k = 2) can we get 1-IP stable clusterings efficiently? a discussion on the run-time of algorithm 2 We remark that the runtime of our O(1)-approximate IP-stable clustering algorithm can potentially be improved if we assume special structure about the metric space, such as a tree or Euclidean metric. In special cases, we can improve the running time by appealing to particular properties of the metric which allow us to either calculate distances or implement our subroutines faster. For example for tree metrics, all distances can be calculated in O(n2) time, even though T = O(n). Likewise for the Euclidean case, we can utilize specialized algorithms for computing the all pairs distance matrix, which obtain speedups over the naive methods [IS22], or use geometric point location data structures to quickly compute quantities such as |B(x, r)| [Sno04]. Our presentation is optimized for simplicity and generality so detailed discussions of specific metric spaces are beyond the scope of the work. b random clustering in unweighted graphs In this appendix, we show that for unweigthed, undirected, graphs (where the distance d(u, v) between two vertices u and v is the length of the shortest path between them), randomly k-coloring the nodes gives an O(1)-approximate IP-stable clustering whenever k = O(n1/2/ log n). We start with the following lemma. Lemma B.1. Let γ = O(1) be a constant. There exists a constant c > 0 (depending on γ) such that the following holds: Let T = (V,E) be an unweighted tree on n nodes rooted at vertex r. Suppose that we randomly k-color the nodes of T . Let Vi ⊆ V be the nodes of color i, let Xi = ∑ v∈Vi d(r, v), and
let X = ∑ v∈V d(r, v). If k ≤ c √ n logn , then with probability 1−O(n −γ), it holds that X/2 ≤ kXi ≤ 2X for all i ∈ [k]. Proof. We will fix i, and prove that the bound X/2 ≤ Xi ≤ 2X holds with probability 1−O(n−γ−1). Union bounding over all i then gives the desired result. Let ∆ = maxv∈V d(r, v) be the maximum distance from the root to any vertex of the tree. We may assume that ∆ ≥ 5 as otherwise the result follows directly from a simple Chernoff bound. Since the tree is unweighted and there exists a node v of distance ∆ to r, there must also exist nodes of distances 1, 2, . . . ,∆− 1 to r, namely the nodes on the path from r to v. For the remaining nodes, we know that the distance is at least 1. Therefore,∑
v∈V d(r, v) ≥ (n−∆− 1) + ∆∑ j=1 j = n+ ( ∆ 2 ) − 1 ≥ n+ ∆ 2 3 ,
and so µi = E[Xi] ≥ n+∆ 2/3
k . Since the variables (d(r, v)[v ∈ Vi])v∈V sum to Xi and are independent and bounded by ∆, it follows by a Chernoff bound that for any 0 ≤ δ ≤ 1,
Pr [|Xi − µi| ≥ δµi] ≤ 2 exp ( −δ
2µi 3∆
) . By the AM-GM inequality, µi ∆ ≥ 1 k
( n
∆ +
∆
3 ) ≥ 2 √ n√ 3k . Putting δ = 1/2, the bound above thus becomes Pr [ |Xi − µi| ≥
µi 3
] ≤ 2 exp ( − √ n
6 √ 3k
) ≤ 2 exp ( − √ n
11k
) ≤ 2n− 1 11c ,
where the last bound uses the assumption on the magnitude of k in the lemma. Choosing c = 111(γ+1) , the desired result follows. Next, we state our result on the O(1)-approximate IP-stability for randomly colored graphs. Theorem B.2. Let γ = O(1) and k ≤ c √ n
logn for a sufficiently small constant c. Let G = (V,E) be an unweighted, undirected graph on n nodes, and suppose that we k-color the vertices of G randomly. Let Vi denote the nodes of color i. With probability at least 1− n−γ, (V1, . . . , Vk) forms an O(1)-approximate IP-clustering. Proof. Consider a node u and let Xi = ∑
v∈Vi\{u} d(u, v). Node that the distances d(u, v) are exactly the distances in a breath first search tree rooted at v. Thus, by Lemma B.1, the Xi’s are all within a constant factor of each other with probability 1−O(n−γ−1). Moreover, a simple Chernoff bound
shows that with the same high probability, |Vi| = nk + O (√ n logn k ) = Θ ( n k ) for all i ∈ [k]. In
particular, the values Yi = Xi|Vi\{u}| for i ∈ [k] also all lie within a constant factor of each other which implies that u is O(1)-stable in the clustering (V1, . . . , Vk). Union bounding over all nodes u, we find that with probability 1−O(n−γ), (V1, . . . , Vk) is an O(1)-approximate IP-clustering. Remark B.3. The assumed upper bound on k in Theorem B.2 is necessary (even in terms of log n). Indeed, consider a tree T which is a star S on n−k log k vertices along with a path P of length k log k having one endpoint at the center v of the star. With probability Ω(1), some color does not appear on P . We refer to this color as color 1. Now consider the color of the star center. With probability at least 9/10, say, this color is different from 1 and appears Ω(log k) times on P with average distance Ω(k log k) to the star center v. Let the star center have color 2. With high probability, each color appears Θ(n/k) times in S. Combining these bounds, we find that with constant probability, the average distance from v to vertices of color 1 is O(1), whereas the average distance from v to vertices of color 2 is Ω ( 1 + k 2(log k)2
n
) . In particular for the algorithm to give an O(1)-approximate IP-stable
clustering, we need to assume that k = O ( √
n logn
) . c additional empirical evaluations We implement our O(1)-approximation algorithm for IP-clustering. These experiments extend those of [AAK+22] and confirm their experimental findings: k-means++ is a strong baseline for IP-stable clustering. Nevertheless, our algorithm is competitive with it while guaranteeing robustness against worst-case datasets, a property which k-means++ does not posses. Our datasets are the following. There are three datasets from [DG19] used in [AAK+22], namely, Adult, Drug [FMM+17], and IndianLiver. We also add two additional datasets from UCI Machine Learning Repository [DG19], namely, BreastCancer and Car. For IP-clustering, we also consider a synthetic dataset which is the hard instance for k-means++ given in [AAK+22]. Our goal is to show that our IP-clustering algorithm is practical and in real world datasets, is competitive with respect to k-means++, which was the best algorithm in the experiments in [AAK+22]. Furthermore, our algorithm is robust and outperform k-means++ for worst case datasets. As before, all experiments were performed in Python 3. We use the k-means++ implementation of Scikit-learn package [PVG+11]. We note that in the default implementation in Scikit-learn, k-means++ is initiated many times with different centroid seeds. The output is the best of 10 runs
by default. As we want to have control of this behavior, we set the parameter n_init=1 and then compute the average of many different runs. Additionally to the metrics used in the main experimental section, we also compute the number of unstable points, defined as the size of the set U = {x ∈ M : x is not stable}. In terms of clustering qualities, we additionally measure three quantities. First, we measure “cost”, which is the average within-cluster distances. Formally, Cost = ∑k i=1 1
(|Ci|2 )
∑ x,y∈Ci,x ̸=y d(x, y). We then
measure k-center costs, defined as the maximum distances from any point to its center. Here, centers are given naturally from k-means++ and our algorithm. Finally, k-means costs, defined as k-means-cost = ∑k i=1 1 |Ci| ∑ x,y∈Ci,x ̸=y d(x, y) 2. C.1 Hard Instance for k-means++ for IP-Stability
We briefly describe the hard instance for k-means++ for the standard IP-stability formulation given in [AAK+22]; see their paper for full details. The hard instance consists of a gadget of size 4. In the seed-finding phase of k-means++, if it incorrectly picks two centers in the gadget, then the final clustering is not β-approximate IP-stable, where β is a configurable parameter. The instance for k-clustering is produced by concatenating these gadgets together. In such an instance, with a constant probability, the clustering returned by k-means++ is not β-approximate IP-stable and in particular. We remark that the proof of Theorem 2 in [AAK+22] easily implies that k-means++ cannot have an approximation factor better than nc for some absolute constant c > 0, i.e., we can insure β = Ω(nc). Here, we test both our algorithm and k-means++ in an instance with 8,000 points (for k = 2, 000 clusters). IP-Stability results We first discuss five real dataset. We tested the algorithms for the range of k up to 25. The result in Figures 2 and 3 is consistent with the experiments in the previous paper as we see that k-means++ is a very competitive algorithm for these datasets. For small number of clusters, our algorithm sometimes outperforms k-means++. We hypothesize that on these datasets, especially for large k, clusters which have low k-means cost separate the points well and therefore are good clusters for IP-stability. Next we discuss the k-means++ hard instance. The instance used in Figure 3 was constructed with β = 50. We vary k but omit the results for higher k values since the outputs from both algorithms are stable. We remark that the empirical results with different β gave qualitatively similar results. For maximum and mean violation, our algorithm outperforms k-means++ (Figure 3).