{
    "abstractText": "Evolutionary differential equation discovery proved to be a tool to obtain equations with less a priori assumptions than conventional approaches, such as sparse symbolic regression over the complete possible terms library. The equation discovery field contains two independent directions. The first one is purely mathematical and concerns differentiation, the object of optimization and its relation to the functional spaces and others. The second one is dedicated purely to the optimizatioal problem statement. Both topics are worth investigating to improve the algorithm\u2019s ability to handle experimental data a more artificial intelligence way, without significant pre-processing and a priori knowledge of their nature. In the paper, we consider the prevalence of either single-objective optimization, which considers only the discrepancy between selected terms in the equation, ormulti-objective optimization,which additionally takes into account the complexity of the obtained equation. The proposed comparison approach is shown on classical model examples \u2013 Burgers equation, wave equation, and Korteweg de Vries equation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mikhail Maslyaev"
        }
    ],
    "id": "SP:3e3e5c51170037b5b571503e69a35f6cc9cf4971",
    "references": [
        {
            "authors": [
                "H. Cao",
                "L. Kang",
                "Y. Chen"
            ],
            "title": "Evolutionary Modeling of Systems of Ordinary Differential Equations with Genetic Programming",
            "venue": "Genetic Programming and Evolvable Machines",
            "year": 2000
        },
        {
            "authors": [
                "Urban Fasel",
                "J Nathan Kutz",
                "Bingni W Brunton",
                "Steven L Brunton"
            ],
            "title": "Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise 1https://github.com/ITMO-NSS-team/EPDE_GECCO_experiments limit, with active learning and control",
            "venue": "Proceedings of the Royal Society A",
            "year": 2022
        },
        {
            "authors": [
                "Han Gao",
                "Matthew J Zahr",
                "Jian-Xun Wang"
            ],
            "title": "Physics-informed graph neural galerkin networks: A unified framework for solving pde-governed forward and inverse problems",
            "venue": "Computer Methods in Applied Mechanics and Engineering",
            "year": 2022
        },
        {
            "authors": [
                "L Gao",
                "Urban Fasel",
                "Steven L Brunton",
                "J Nathan Kutz"
            ],
            "title": "Convergence of uncertainty estimates in Ensemble and Bayesian sparse model discovery",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Hisao Ishibuchi",
                "Yusuke Nojima",
                "Tsutomu Doi"
            ],
            "title": "Comparison between single-objective and multi-objective genetic algorithms: Performance comparison and performance measures",
            "venue": "IEEE International Conference on Evolutionary Computation",
            "year": 2006
        },
        {
            "authors": [
                "Q. Zhang K. Li",
                "K. Deb",
                "S. Kwong"
            ],
            "title": "An Evolutionary Many- Objective Optimization Algorithm Based on Dominance and Decomposition",
            "venue": "IEEE Transactions on Evolutionary Computation) 19,",
            "year": 2015
        },
        {
            "authors": [
                "Lu Lu",
                "Xuhui Meng",
                "Zhiping Mao",
                "George Em Karniadakis"
            ],
            "title": "DeepXDE: A deep learning library for solving differential equations",
            "venue": "SIAM Rev. 63,",
            "year": 2021
        },
        {
            "authors": [
                "Mikhail Maslyaev",
                "Alexander Hvatov"
            ],
            "title": "Multi-Objective Discovery of PDE Systems Using Evolutionary Approach",
            "venue": "IEEE Congress on Evolutionary Computation (CEC)",
            "year": 2021
        },
        {
            "authors": [
                "Mikhail Maslyaev",
                "Alexander Hvatov"
            ],
            "title": "Solver-Based Fitness Function for the Data-Driven Evolutionary Discovery of Partial Differential Equations",
            "venue": "IEEE Congress on Evolutionary Computation (CEC)",
            "year": 2022
        },
        {
            "authors": [
                "Mikhail Maslyaev",
                "Alexander Hvatov",
                "Anna V Kalyuzhnaya"
            ],
            "title": "Partial differential equations discovery with EPDE framework: application for real and synthetic data",
            "venue": "Journal of Computational Science",
            "year": 2021
        },
        {
            "authors": [
                "Daniel A Messenger",
                "David M Bortz"
            ],
            "title": "Weak SINDy for partial differential equations",
            "venue": "J. Comput. Phys",
            "year": 2021
        },
        {
            "authors": [
                "Lizhen Nie",
                "Veronika Ro\u010dkov\u00e1"
            ],
            "title": "Bayesian Bootstrap Spike-and-Slab LASSO",
            "venue": "J. Amer. Statist. Assoc. 0,",
            "year": 2022
        },
        {
            "authors": [
                "M Raissi",
                "P Perdikaris",
                "GE Karniadakis"
            ],
            "title": "Physics informed deep learning (Part II): Data-driven discovery of nonlinear partial differential equations",
            "year": 2017
        },
        {
            "authors": [
                "Mikhail Sarafanov",
                "Valerii Pokrovskii",
                "NikolayONikitin"
            ],
            "title": "Evolutionary Automated Machine Learning for Multi-Scale Decomposition and Forecasting of Sensor Time Series",
            "venue": "IEEE Congress on Evolutionary Computation (CEC)",
            "year": 2022
        },
        {
            "authors": [
                "Hayden Schaeffer"
            ],
            "title": "Learning partial differential equations via data discovery and sparse optimization",
            "venue": "Proc. R. Soc. A 473,",
            "year": 2017
        },
        {
            "authors": [
                "H. Schaeffer",
                "R. Caflisch",
                "C.D. Hauck",
                "S. Osher"
            ],
            "title": "Learning partial differential equations via data discovery and sparse optimization",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science (2017)",
            "year": 2017
        },
        {
            "authors": [
                "Pongpisit Thanasutives",
                "Takashi Morita",
                "Masayuki Numao",
                "Ken ichi Fukui"
            ],
            "title": "Noise-aware physics-informed machine learning for robust PDE discovery. Machine Learning: Science and Technology 4, 1 (feb 2023)",
            "venue": "Companion, July 15\u201319,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 6.\n17 03\n8v 1\n[ cs\n.L G\n] 2\n9 Ju\nn 20\n23\nEvolutionary differential equation discovery proved to be a tool to obtain equations with less a priori assumptions than conventional approaches, such as sparse symbolic regression over the complete possible terms library. The equation discovery field contains two independent directions. The first one is purely mathematical and concerns differentiation, the object of optimization and its relation to the functional spaces and others. The second one is dedicated purely to the optimizatioal problem statement. Both topics are worth investigating to improve the algorithm\u2019s ability to handle experimental data a more artificial intelligence way, without significant pre-processing and a priori knowledge of their nature. In the paper, we consider the prevalence of either single-objective optimization, which considers only the discrepancy between selected terms in the equation, ormulti-objective optimization,which additionally takes into account the complexity of the obtained equation. The proposed comparison approach is shown on classical model examples \u2013 Burgers equation, wave equation, and Korteweg - de Vries equation.\nCCS CONCEPTS\n\u2022 Applied computing \u2192 Mathematics and statistics; \u2022 Computing methodologies\u2192 Heuristic function construction.\nKEYWORDS\nsymbolic regression, dynamic systemmodeling, interpretable learning, differential equations, sparse regression\nACM Reference Format: Mikhail Maslyaev and Alexander Hvatov. 2023. Comparison of Single- and Multi- Objective Optimization Quality for Evolutionary Equation Discovery. InGenetic and Evolutionary Computation Conference Companion (GECCO \u201923 Companion), July 15\u201319, 2023, Lisbon, Portugal. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3583133.3590601"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "The recent development of artificial intelligence has given high importance to problems of interpretable machine learning. In many cases, users value models not only for their quality of predicting the state of the studied system but also for the ability to provide\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this workmust be honored. For all other uses, contact the owner/author(s). GECCO \u201923 Companion, July 15\u201319, 2023, Lisbon, Portugal \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0120-7/23/07. https://doi.org/10.1145/3583133.3590601\nsome information about its operation. In the case ofmodeling physical processes, commonly, the most suitable models have forms of partial differential equations. Thus many recent studies aimed to develop the concept of data-driven differential equations discovery. In the paper, data-driven discovery implies obtaining a differential equation from a set of empirical measurements, describing the dynamics of a dependent variable in some domain. Furthermore, equation-based models can be incorporated into pipelines of automated machine learning, that can include arbitrary submodels, with approach, discussed in paper [14].\nInitial advances in differential equations discovery were made with symbolic regression algorithm, as in [1]. The algorithm employs genetic programming to detect the graph, that represents differential equation. One of the groups of the most simple yet practical techniques of equation construction is based on the sparse linear regression (least absolute shrinkage and selection operator), introduced inworks [11], [15], [16], and other similar projects. This approach has limited flexibility, having applicability restrictions in cases of the equation with low magnitude coefficients, being discovered on noisy data. This issue is addressed by employing Bayesian interference as in [12] to estimate the coefficients of the equation, as in work [4]. To account for the uncertainty in the resulting model, the approximating term library can be biased statistically [2]. Physics-informed neural networks (PINN) form the next class of data-driven equation discovery tools, representing the process dynamics with artificial neural networks. The primary research on this topic is done in work [13], while recent advances have been made in incorporating more complex types of neural networks in the PINNs [3, 17].\nIn recent studies [7, 10], evolutionary algorithms have proved to be a rather flexible tool for differential equation discovery, demanding only a few assumptions about the process properties. The problem is stated as the process representation error minimization. Implementing multi-objective evolutionary optimization, first introduced for DE systems, as in [8], seems to be a feasible way to improve the quality of the equation search, operating on fewer initial assumptions and providing higher diversity among the processed candidates. Additional criteria can represent other valuable properties of the constructed models, namely conciseness.\nThis study compares the performance of single- and multi- ob-\njective optimization.Namely, the hypothesis that themulti-objective optimization creates and preserves diversity in the population and thus may achieve a better fitness function values, than that of a single-objective approach.The theoretical comparison shows that multi-objective algorithms allow escaping local minima as soon as the number of objectives is reasonably small [5]. For equation discovery applications, the function landscapes have a more complex\nstructure, so increased diversity of the population can benefit the resulting quality."
        },
        {
            "heading": "2 ALGORITHM DESCRIPTION",
            "text": "The data-driven differential equation identification operates on problems of selecting amodel for dynamics of the variableD = D (C, x) in a spatio-temporal domain (0,) ) > \u03a9, that is implicitly described by differential equationEq. 1 with corresponding initial and boundary conditions. It can be assumed, that the order of the unknown equation can be arbitrary, but rather low (usually of second or third order).\n(C, x, D, mD mC , mD mG1 , ... mD mG= ) = 0 (1)\nBoth multi-objective and single-objective approaches have the same core of \"graph-like\" representation of a differential equation (encoding) and similar evolutionary operators thatwill be described further."
        },
        {
            "heading": "2.1 Differential equation representation",
            "text": "To represent the candidate differential equation the computational graph structure is employed. A fixed three-layer graph structure is employed to avoid the infeasible structures, linked to unconstrained graph construction and overtraining issues, present in symbolic regression. The lowest level nodes contain tokens, middle nodes and the root are multiplication and summation operations. The data-driven equations take the form of a linear combination of product terms, represented by the multiplication of derivatives, other functions and a real-valued coefficient Eq. 2.\n{\n\u2032 (C, x, D, mDmC , mD mG1 , ... mDmG= ) = \u2211 8 U8 \u220f 9 58 9 = 0 \u2032 (D) |\u0393 = 0 (2)\nHere, the factors 58 9 are selected from the user-defined set of elementary functions, named tokens. The problem of an equation search transforms into the task of detecting an optimal set of tokens to represent the dynamics of the variableD (C,x), and forming the equation by evaluating the coefficients U = (U1, ... U<).\nDuring the equation search, we operate with tensors of token values, evaluated on grids DW = D (CW , xW ) in the processed domain (0,) ) > \u03a9.\nSparsity promotion in the equation operates by filtering out nominal terms with low predicting power and is implementedwith LASSO regression. For each individual, a term (without loss of generality, we can assume that it is the<-th term) is marked to be a \"right-hand side of the equation\" for the purposes of term filtering and coefficient calculation. The terms )8 = \u220f 9 58 9 are paired with real-value coefficients obtained from the optimization subproblem of Eq. 3. Finally, the equation coefficients are detected by linear regression.\nU\u2032 = argmin U\n( | | \u2211\n8, 8\u2260<\nU\u20328\n\u220f\n9\n58 9 \u2212 \u220f\n9\n5<9 | |2 + _ | |U \u2032 | |1) (3)\nIn the initialization of the algorithm equation graphs are randomly constructed for each individual from the sets of user-defined tokens with a number of assumptions about the structures of the \u201cplausible equations\u201d."
        },
        {
            "heading": "2.2 Mechanics of implemented evolutionary operators",
            "text": "To direct the search for the optimal equations, standard evolutionary operators of mutation and cross-over have been implemented. While the mechanics of single- and multi-objective optimization in the algorithm differ, they work similarly on the stage of applying equation structure-changing operators.With the graph-like encoding of candidate equations, the operators can be represented as changes, introduced into its subgraphs.\nThe algorithm properties to explore structures are provided by mutation operators, which operate by random token and term exchanges. The number of terms to change has no strict limits. For tokens with parameters (?:+1, ... ?=) \u2208 R =\u2212: , such as a parametric representation of an unknown external dependent variable, parameters are also optimized: themutation is donewith a random Gaussian increment.\nIn order to combine structural elements of better equations, the cross-over operator is implemented. The interactions between parent equations are held on a term-level basis. The sets of terms pairs from the parent equation are divided into three groups: terms identical in both equations, terms that are present in both equations but have different parameters or only a few tokens inside of them are different, and the unique ones. The cross-over occurs for the two latter groups. For the second group it manifests as the parameter exchange between parents: the new parameters are selected from the interval between the parents\u2019 values.\nCross-over between unique terms works as the complete exchange between them. The constructionof exchange pairs between these tokens works entirely randomly."
        },
        {
            "heading": "2.3 Optimization of equation quality metric",
            "text": "The selection of the optimized functional distinguishes multiple approaches to the differential equation search. First of all, a more trivial optimization problem can be stated as in Eq. 4, where we assume the identity of the equation operator \u2032 (D) = 0 to zero as in Eq. 2.\n&>? ( \u2032 (D)) = | | \u2032 (D) | |= = | |\n\u2211\n8\nU8\n\u220f\n9\n58 9 | |= \u2212\u2192 min U8 C8 9 (4)\nAn example of a more complex optimized functional is the norm of a discrepancy between the input values of the modelled variable and the solution proposed by the algorithm differential equation, estimated on the same grid. Classical solution techniques can not be applied here due to the inability of a user to introduce the partitioning of the processed domain, form finite-difference schema without a priori knowledge of an equation, proposed by evolutionary algorithm. An automatic solving method for candidate equation (viewed as in Eq. 6) quality evaluation is introduced in [9] to work around this issue.\n&B>; ( \u2032 (D)) = | |D \u2212 D | |= \u2212\u2192 min\nU8 C8 9 (5)\n\u2032 (D) = 0 : \u2032 (D) = \u2211\n8\nU8\n\u220f\n9\n58 9 = 0 (6)\nWhile both quality metrics Eq. 4 and Eq. 5 in ideal conditions provide decent convergence of the algorithm, in the case of the noisy data, the errors in derivative estimations can make differential operator discrepancy from the identity (as in problem in Eq. 4) an unreliable metric. Applying the automatic solving algorithm has high computational cost due to training a neural network to satisfy the discretized equation and boundary operators.\nAs the single-objective optimization method for the study, we have employed a simple evolutionary algorithm with a strategy that minimizes one of the aforementioned quality objective functions. Due to the purposes of experiments on synthetic noiseless data, the discrepancy-based approach has been adopted."
        },
        {
            "heading": "2.4 Multi-objective optimization application",
            "text": "As we stated earlier, in addition to process representation, the conciseness is also a valuable for regulating the interpretability of the model. Thus the metric of this property can be naturally introduced as Eq. 7, with an adjustment of counting not the total number of active terms but the total number of tokens (:8 for 8 \u2212 C\u210e term).\n( \u2032 (D)) = #( \u2032) = \u2211\n8\n:8 \u2217 1U8\u22600 (7)\nIn addition to evaluating the quality of the proposed solution from the point of the equation simplicity, multi-objective enables the detection of systems of differential equations, optimizing qualities of modeling of each variable.\nWhile there are many evolutionary multi-objective optimization algorithms,MOEADD (Multi-objective evolutionary algorithm based on dominance and decomposition) [6] algorithm has proven to be an effective tool in applications of data-driven differential equations construction.We employ baseline version of theMOEADD from the aforementioned paper with the following parameters: PBI penalty factor \\ = 1.0, probability of parent selection inside the sector neighbourhood X = 0.9 (4 nearest sector are considered as \u201cneighbouring\u201d) with 40% of individuals selected as parents. Evolutionary operator parameters are: crossover rate (probability of affecting individual terms): 0.3 and mutation rate of 0.6.The result of the algorithm is the set of equations, ranging from themost simplistic constructions (typically in forms of m =D\nmG= :\n= 0) to the highly com-\nplex equations, where extra terms probably represents the noise components of the dynamics."
        },
        {
            "heading": "3 EXPERIMENTAL STUDY",
            "text": "This section of the paper is dedicated to studying equation discovery framework properties. As the main object of interest, we designate the difference of derived equations between single- and multi-objective optimization launches. The validation was held on the synthetic datasets, where modelled dependent variable is obtained from solving an already known and studied equation.\nThe tests were held on three cases: wave, Burgers and Kortewegde Vries equations due to unique properties of each equation. The algorithms were tested in the following pattern: 64 evolutionary iterations for the single-objective optimization algorithm and 8 iterations of multi-objective optimization for the populations of 8 candidate equations, which resulted in roughly similar resource\nconsumption.10 independent runs are conducted with each setup. The main equation quality indicator in our study is the statistical analysis of the objective function mean (` = ` (& ( \u2032))) and variance f2 = (f (& ( \u2032)))2 among the different launches.\nThe first equation was the wave equation as on Eq. 8 with the necessary boundary and initial conditions. The equation is solved with the Wolfram Mathematica software in the domain of (G, C) \u2208 [0, 1] > [0, 1] on a grid of 101 > 101. Here, we have employed numerical differentiation procedures.\nm2D mC2 = 0.04 m2D mG2 (8)\nThe algorithm\u2019s convergence due to the relatively simple structure was ensured in the case of both algorithms: the algorithm proposes the correct structure during the initialization or in the initial epochs of the optimization. However, such a trivial case can be a decent indicator of the \u201cideal\u201d algorithm behaviour. The values of examined metrics for this experiment and for the next ones are presented on Tab. 1.\nequation is provided in Fig. 1.\nAnother examination was performed on the solution of Burgers\u2019 equation, which has amore complex, non-linear structure. The problemwas set as in Eq. 9, for a case of a processwithout viscosity, thus omitting term a m 2D mC2 . As in the previous example, the equation was solved with the Wolfram Mathematica toolkit.\nmD mC + D mD mG = 0 (9)\nDerivatives used during the equation search were computed analytically due to the function not being constant only on small domain.\nThe presence of other structures that have relatively low opti-\nmized function values, such as D\u2032GD \u2032 C = D \u2032\u2032 CC , makes this case of data rather informative. Thus, the algorithm has a local optimum that is far from the correct structure from the point of error metric.\nThe final set-up for an experiment was definedwith a non-homogeneous\nKorteweg-de Vries equation, presented in Eq. 10. The presence of external tokens in separate terms in the equationmakes the search more difficult.\nmD mC + 6D mD mG + m3D mG3 = cos C sin C (10)\nThe experiment results indicate that the algorithm may detect the same equation in multiple forms. Each term of the equation may be chosen as the \u201cright-hand side\u201d one, and the numerical error with different coefficient sets can also vary."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "This paper examines the prospects of using multi-objective optimization for the data-driven discovery of partial differential equations. While initially introduced for handling problems of deriving systems of partial differential equations, the multi-objective view of the problem improves the overall quality of the algorithm. The improved convergence, provided by higher candidate individual diversity, makes the process more reliable in cases of equations with complex structures, as was shown in the examples of Burgers\u2019 and Korteweg-de Vries equations.\nThe previous studies have indicated the algorithm\u2019s reliability, converging to the correct equation, while this research has proposed a method of improving the rate at which the correct structures are identified. This property is valuable for real-world applications because incorporating large and complete datasets improves the noise resistance of the approach.\nThe further development of the proposed method involves introducing techniques for incorporating expert knowledge into the search process. This concept can help generate preferable candidates or exclude infeasible ones even before costly coefficient calculation and fitness evaluation procedures."
        },
        {
            "heading": "5 CODE AND DATA AVAILABILITY",
            "text": "The numerical solution data and the Python scripts, that reproduce the experiments, are available at the GitHub repository 1."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This research is financially supported by the Ministry of Science and Higher Education, agreement FSER-2021-0012."
        }
    ],
    "title": "Comparison of Single- and Multi- Objective Optimization ality for Evolutionary Equation Discovery",
    "year": 2023
}