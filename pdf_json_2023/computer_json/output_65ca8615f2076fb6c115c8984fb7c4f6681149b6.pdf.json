{
    "abstractText": "Speech super-resolution (SSR) aims to recover a high resolution (HR) speech from its corresponding low resolution (LR) counterpart. Recent SSR methods focus more on the reconstruction of the magnitude spectrogram, ignoring the importance of phase reconstruction, thereby limiting the recovery quality. To address this issue, we propose mdctGAN, a novel SSR framework based on modified discrete cosine transform (MDCT). By adversarial learning in the MDCT domain, our method reconstructs HR speeches in a phase-aware manner without vocoders or additional post-processing. Furthermore, by learning frequency consistent features with self-attentive mechanism, mdctGAN guarantees a high quality speech reconstruction. For VCTK corpus dataset, the experiment results show that our model produces natural auditory quality with high MOS and PESQ scores. It also achieves the state-of-theart log-spectral-distance (LSD) performance on 48 kHz target resolution from various input rates. Code is available from https://github.com/neoncloud/mdctGAN",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenhao Shuai"
        },
        {
            "affiliations": [],
            "name": "Chaohua Shi"
        },
        {
            "affiliations": [],
            "name": "Lu Gan"
        },
        {
            "affiliations": [],
            "name": "Hongqing Liu"
        }
    ],
    "id": "SP:559a43f0375397b75bea4fb99b03be933198db51",
    "references": [
        {
            "authors": [
                "K. Kumar",
                "R. Kumar",
                "T. de Boissiere",
                "L. Gestin",
                "W.Z. Teoh",
                "J. Sotelo",
                "A. de Br\u00e9bisson",
                "Y. Bengio",
                "A.C. Courville"
            ],
            "title": "MelGAN: generative adversarial networks for conditional waveform synthesis",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 14 881\u2013 14 892. [Online]. Available: https://proceedings.neurips.cc/paper/ 2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html",
            "year": 2019
        },
        {
            "authors": [
                "A. v. d. Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A.W. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "WaveNet: A generative model for raw audio",
            "venue": "The 9th ISCA Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15 September 2016. ISCA, 2016, p. 125. [Online]. Available: http://www.isca-speech.org/archive/SSW 2016/abstracts/ ssw9 DS-4 van den Oord.html",
            "year": 2016
        },
        {
            "authors": [
                "J. Kong",
                "J. Kim",
                "J. Bae"
            ],
            "title": "HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/ hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html",
            "year": 2020
        },
        {
            "authors": [
                "H. Liu",
                "W. Choi",
                "X. Liu",
                "Q. Kong",
                "Q. Tian",
                "D. Wang"
            ],
            "title": "Neural vocoder is all you need for speech super-resolution",
            "venue": "Interspeech 2022, 23nd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18 - 22 September 2022. ISCA, 2022, p. Forthcoming. [Online]. Available: https://doi.org/10.48550/arXiv.2203.14941",
            "year": 2022
        },
        {
            "authors": [
                "Y. Hu",
                "Y. Liu",
                "S. Lv",
                "M. Xing",
                "S. Zhang",
                "Y. Fu",
                "J. Wu",
                "B. Zhang",
                "L. Xie"
            ],
            "title": "DCCRN: deep complex convolution recurrent network for phase-aware speech enhancement",
            "venue": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020. ISCA, 2020, pp. 2472\u20132476. [Online]. Available: https://doi.org/10.21437/Interspeech.2020-2537",
            "year": 2020
        },
        {
            "authors": [
                "S. Lv",
                "Y. Hu",
                "S. Zhang",
                "L. Xie"
            ],
            "title": "DCCRN+: channel-wise subband DCCRN with SNR estimation for speech enhancement",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 2816\u20132820. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-1482",
            "year": 2021
        },
        {
            "authors": [
                "K. Zhang",
                "Y. Ren",
                "C. Xu",
                "Z. Zhao"
            ],
            "title": "WSRGlow: A glowbased waveform generative model for audio super-resolution",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 1649\u20131653. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-892",
            "year": 2021
        },
        {
            "authors": [
                "R. Yoneyama",
                "R. Yamamoto",
                "K. Tachibana"
            ],
            "title": "Nonparallel High-Quality Audio Super Resolution with Domain Adaptation and Resampling CycleGANs",
            "venue": "2022. [Online]. Available: https: //arxiv.org/abs/2210.15887",
            "year": 2022
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998\u2013 6008. [Online]. Available: https://proceedings.neurips.cc/paper/ 2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
            "year": 2017
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A.C. Courville",
                "Y. Bengio"
            ],
            "title": "Generative Adversarial Nets",
            "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2014, pp. 2672\u20132680. [Online]. Available: https://proceedings.neurips.cc/paper/2014/hash/ 5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html",
            "year": 2014
        },
        {
            "authors": [
                "T. Wang",
                "M. Liu",
                "J. Zhu",
                "A. Tao",
                "J. Kautz",
                "B. Catanzaro"
            ],
            "title": "High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
            "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. IEEE Computer Society, 2018, pp. 8798\u20138807. [Online]. Available: http://openaccess.thecvf.com/content cvpr 2018/html/ Wang High-Resolution Image Synthesis CVPR 2018 paper. html",
            "year": 2018
        },
        {
            "authors": [
                "E. Bakhturina",
                "V. Lavrukhin",
                "B. Ginsburg",
                "Y. Zhang"
            ],
            "title": "Hi-Fi multi-speaker English TTS dataset",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 2776\u20132780. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-1599",
            "year": 2021
        },
        {
            "authors": [
                "M. Bosi",
                "R.E. Goldberg",
                "J.L. Mitchell"
            ],
            "title": "Introduction to digital audio coding and standards",
            "venue": "J. Electronic Imaging, vol. 13, no. 2, pp. 399\u2013400, 2004. [Online]. Available: https://doi.org/10.1117/1.1695413",
            "year": 2004
        },
        {
            "authors": [
                "H. Wang",
                "D. Wang"
            ],
            "title": "Towards robust speech superresolution",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., vol. 29, pp. 2058\u20132066, 2021. [Online]. Available: https: //doi.org/10.1109/TASLP.2021.3054302",
            "year": 2021
        },
        {
            "authors": [
                "N.C. Rakotonirina"
            ],
            "title": "Self-attention for audio super-resolution",
            "venue": "2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), Gold Coast, Australia, October 25-28, 2021. IEEE, 2021, pp. 1\u20136. [Online]. Available: https://doi.org/10.1109/MLSP52302.2021.9596082",
            "year": 2021
        },
        {
            "authors": [
                "J. Lee",
                "S. Han"
            ],
            "title": "Nu-wave: A diffusion probabilistic model for neural audio upsampling",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 1634\u20131638. [Online]. Available: https: //doi.org/10.21437/Interspeech.2021-36",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [Online]. Available: http://arxiv.org/abs/1412.6980",
            "year": 2015
        },
        {
            "authors": [
                "P. Micikevicius",
                "S. Narang",
                "J. Alben",
                "G.F. Diamos",
                "E. Elsen",
                "D. Garc\u0131\u0301a",
                "B. Ginsburg",
                "M. Houston",
                "O. Kuchaiev",
                "G. Venkatesh",
                "H. Wu"
            ],
            "title": "Mixed precision training",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Online]. Available: https://openreview.net/forum?id=r1gs9JgRZ",
            "year": 2018
        },
        {
            "authors": [
                "V. Kuleshov",
                "S.Z. Enam",
                "S. Ermon"
            ],
            "title": "Audio super-resolution using neural networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. [Online]. Available: https://openreview.net/forum?id= S1gNakBFx",
            "year": 2017
        },
        {
            "authors": [
                "S.E. Eskimez",
                "K. Koishida"
            ],
            "title": "Speech super resolution generative adversarial network",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019. IEEE, 2019, pp. 3717\u20133721. [Online]. Available: https://doi.org/10.1109/ICASSP.2019.8682215",
            "year": 2019
        },
        {
            "authors": [
                "S. Han",
                "J. Lee"
            ],
            "title": "Nu-wave 2: A general neural audio upsampling model for various sampling rates",
            "venue": "Interspeech 2022, 23nd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18 - 22 September 2022. ISCA, 2022, p. Forthcoming. [Online]. Available: https://doi.org/10.21437%2Finterspeech.2022-45",
            "year": 2022
        },
        {
            "authors": [
                "C.-Y. Yu",
                "S.-L. Yeh",
                "G. Fazekas",
                "H. Tang"
            ],
            "title": "Conditioning and sampling in variational diffusion models for speech superresolution",
            "venue": "arXiv preprint arXiv:2210.15793, 2022. [Online]. Available: https://arxiv.org/abs/2210.15793",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Speech super-resolution (SSR) is the task of recovering highresolution (HR) speech from low-resolution (LR) speech. SSR presents many practical applications in fields such as teleconferencing and speech recognition. It is crucial to correctly reconstruct the phase information when recovering the high frequency components. Recent frequency domain-based SSR research [1\u20138] has primarily focused on recovering amplitude information, and it often requires additional steps to reconstruct the missing phase.\nVocoders [1\u20133], which recover the phase by discriminating in the time domain, are frequently used to reconstruct the raw waveform. Utilizing vocoders, a two-stage speech superresolution method [4] was proposed to predict high-resolution speech mel-spectrograms at first, then applying a vocoder for raw waveform reconstruction. Though inspiring, such a twostage generation process can lead to instability during training. Alternatively, recent approaches attempt to model phase and magnitude (or real and imaginary part of complex-valued spectrograms) by complex-valued neural networks or fusion modules [5\u20137]. The waveform can then be reconstructed by simply applying an inversion. However, the convergence of complex neural networks is not guaranteed. Separating the treatment of the complex-valued features leads to implicit modeling of phase and magnitude. \u2217 Major work done during undergraduate at CQUPT & Brunel \u2020 Intelligent Speech and Audio Lab, CQUPT\nTo address these issues, we propose to perform SSR in the modified discrete cosine transform (MDCT) domain, a real-valued, lossless transform, which enables a joint magnitude and phase estimation. To that aim, we propose mdctGAN, a frequency-attentive, phase-aware SSR network with a transformer-based [9] conditional Generative Adversarial Networks (cGAN) [10, 11] architecture. In the network design, we introduce a transformer bottleneck stack in the generator network for global attention on frequency-consistent features. Transformer-based models are inherently data-hungry. However, the current VCTK dataset [2] used for the SSR task is not sufficient to fully exploit the potential of our model. Hence, we added the HiFi-TTS dataset [12], a large-scale, high-quality speech synthesis corpus, for pre-training. And we fine-tuned our model on the VCTK corpus [2] to achieve an output sampling rate up to 48 kHz. Specifically, the contributions of this work are as follows. \u2022 We propose a vocoder-free method that performs speech\nsuper-resolution with transformer-based cGAN. It outperforms previous works in LSD scores. Meanwhile, it also achieves a high score in subjective tests.\n\u2022 We develop the pseudo-log operation for dynamic compression of MDCT coefficients, which is essential to the production of phase-aware, high-quality speech.\n\u2022 We show by experiments that our models can learn and thereby generate the phase information encoded in the MDCT coefficients well, demonstrating the great potential in producing high-quality speech audio."
        },
        {
            "heading": "2. Proposed Method",
            "text": ""
        },
        {
            "heading": "2.1. The MDCT-based processing",
            "text": "2.1.1. Basics of the MDCT\nThe reconstruction of the speech phase is crucial to the quality of the generated speech. The commonly used mel-spectrograms do not contain the phase information of the audio signal and therefore require additional algorithms for phase recovery. For this reason, we propose SSR in the MDCT domain, which guarantees the phase-aware speech reconstruction with a real-valued spectrogram. MDCT is widely used in audio compression, e.g. mp3, ac3 [13]. Just as the short-time Fourier transform (STFT), the audio is first split into blocks, with a 50% overlap between each block. Then, a forward MDCT is applied to each block, given below:\nXi[k] = N\u22121\u2211 n=0 w[n]xi[n] cos ( 2\u03c0 N (n+ n0)(k + 1 2 ) ) k = 0, ..., N/2\u2212 1, (1)\nar X\niv :2\n30 5.\n11 10\n4v 2\n[ ee\nss .A\nS] 1\n9 M\nay 2\n02 3\nwhere n0 = N/4 + 1/2, xi[n] is the n-th sample in the i-th block of the audio, and w[n] is the window function applied to each block to reduce spectrum leakage. The corresponding inverse MDCT (iMDCT) of the i-th block is:\nx\u2032i[n] = 4\nN w[n] N/2\u22121\u2211 k=0 Xi[k] cos ( 2\u03c0 N (n+ n0)(k + 1 2 ) ) n = 0, ..., N \u2212 1.\n(2) To produce the full audio, each recovered block should be overlap-added together to eliminate the aliasing. In this paper,w[n] was chosen as the Kaiser-Bessel-derived window [13]. And we implemented fast MDCT/iMDCT modules using FFT.\nThis time-frequency domain representation is similar to that of the STFT, as shown in Figure 1, where the resonant peaks of the speech signal can be observed, indicating that it is also an effective representation of the frequency component distribution of the signal. The MDCT has the following advantages: \u2022 It is a real-valued, invertible linear transform so that the out-\nput is well compatible with existing deep learning neural networks.\n\u2022 The phase information of raw waveform is encoded into the sign of MDCT coefficient, allowing the neural networks to model and reconstruct phase and amplitude of signal components with MDCT spectra only.\n2.1.2. Pseudo-log dynamic range compression\nIn a typical speech processing pipeline, small frequency components are revealed using decibel-scaled STFT magnitudes rather than the raw spectrogram. However, MDCT encodes the phase\ninformation into the sign of the coefficients, and unfortunately, we cannot perform logarithmic on negative values. In order to maintain the sign of the coefficients while compressing the dynamic range of the signal, we introduce a pseudo-logarithmic operation based on arcsinh(x):\nplog(x) = arcsinh(x)\nln(10) = log10(x+\n\u221a x2 + 1). (3)\nWe illustrate the impact of dynamic compression in Figure 2. The ablation experiments demonstrated that without the plog operation, the model cannot be trained. The plog function in (3) has the following desirable properties. \u2022 It is differentiable with respect to R and is oddly symmetric.\nHence, it preserves the polarity of both positive and negative MDCT coefficients.\n\u2022 By using the asymptotic expansion of arcsinh(x), it can be shown that as x \u2192 +\u221e, plog(x) \u2192 log10(2x). It compresses the dynamic range similar to decibel-scale of MDCT (or STFT) magnitude, as shown in Figure 2.\nTo make better use of the non-linear interval of the plog function, we multiply the raw MDCT coefficients with a gain of \u03b1 = 103. We have found that the histogram distributions of the transformed spectra mostly lie in the interval of [\u22125, 5], so we scale them to [\u22121, 1] by dividing 5. Alternating positive and negative patterns are frequently observed in an MDCT spectrogram, shown in Figure 2. To prompt the model with magnitude information, we also appended the corresponding absolute values to the input, which also normalised to the [\u22121, 1] interval, guiding the network to produce clearer high-frequency details."
        },
        {
            "heading": "2.2. Network architecture",
            "text": "In this work, we mainly focus on training a network G to map the LR spectrogram to S\u0302R \u2032 = G(LR) by minimizing the error between S\u0302R \u2032\nand HR\u2212 LR. The final output is defined as S\u0302R = S\u0302R \u2032 +LR. Figure 3 shows the overview of the proposed mdctGAN architecture. We want our model to generate the fine structure of the spectrogram (e.g. the resonant peaks of speech) while remaining globally consistent with the base frequency component. Inspired by work on image translation [11], we have designed a transformer-based generator architecture that works from coarse to fine, as well as a discriminator that judges the spectrogram from multiple scales.\n2.2.1. Generator\nAs shown in Figure 3, our generator consists of two subnetworks: Gglobal (the global generator network, working on the \u2193\u2193 2 input) and Glocal (the local enhancer network, working on the full size spectrogram) achieving coarse-to-fine spectrogram generation [11]. Both sub-networks use Unet-like architectures, yet differ in size and depth, consisting of three submodules: a spectrogram encoder that extracts features at multiple scales through cascaded convolution layers, a bottleneck block stack and a decoder that progressively up-samples features with bilinear interpolation (denoted by Interp \u2191\u2191 2 in the Up-Sample Layer). The bottleneck of Gglobal also contains Transformer blocks (yellow boxes in the middle of Glocal) for learning frequency consistent features. Compared with the original stride = 2 transpose-convolution used in [11], this upsampling operator can reduce the checkerboard artefacts more effectively, as demonstrated in the ablation study of Section 3.5. Finally, the model produces a full-band SR spectrogram by summing the residual path of the input LR spectrogram.\n2.2.2. Discriminator\nTo achieve our vocoder-free SSR goal, we only use a multi-scale discriminator supervised in the MDCT domain. It contains a total of 3 discriminators, and all have the same network structure but operating at 3 different scales by downsampling the input. All discriminators follow the Patch-GAN\u2019s architecture, with basic blocks of cascaded Convolution-InstanceNormLeakyReLU (slope = 0.2) layers. This design allows the generator to efficiently produce both globally consistent spectrograms (coarse-level supervision) and finer detail information (fine-level supervision)."
        },
        {
            "heading": "2.3. Loss function",
            "text": "In this work, the total loss function Lt consists of an adversarial loss and a feature matching loss, which is similar to that in [11].\nLt = min G\n[( max\nD1,D2,D3 3\u2211 i=1 VGAN (G,Di))\n)\n+ \u03bbfeat 3\u2211 i=1 VFeat(G,Di)\n] ,\nwhere \u03bbfeat is the gain of feature matching loss. Here, VGAN (G,Di) represents the adversarial loss of each Di\nVGAN (G,Di) =E(LR,HR)\u223cpdata(LR,HR)[log(Di(LR,HR))] +ELR\u223cpdata(LR)[log(1\u2212Di(G(LR)))].\nAs shown in the bottom right corner of Figure 3, {Di}3i=1 process the original signal, down-sampled versions with decimation factors of 2 and 4, respectively. In this way, the discriminator network processes inputs from coarse to fine. Each feature loss function VFeat(G,Di) takes the following form\nVFeat(G,Di)\n=E(LR,HR)\u223cpdata(LR,HR) 1\nNk 3\u2211 i=1 \u2211 k \u2225\u2225\u2225F ikpos \u2212 F ikneg\u2225\u2225\u2225 1 ,\nwhere F ikpos = Dki (LR,HR) corresponds to intermediate feature maps at the k-th layer of the i-th discriminator Di for the pair (LR,HR). Likewise, F ikneg = Dki (LR;G(LR)) represents that of (LR, G(LR)), denoted as \u201cOpposite Features\u201d in\nFigure 3. As MDCT encodes the phase information, we do not need to design an additional time domain penalty term."
        },
        {
            "heading": "3. Experiments",
            "text": ""
        },
        {
            "heading": "3.1. Dataset & Pre-processing",
            "text": "In this study, we train our model on a dataset composed of the CSTR\u2019s VCTK speech dataset [2] and the Hi-Fi TTS dataset [12]. The sampling rate of this joint dataset is at least 44.1 kHz, and the total duration is up to 292 hours, ensuring a high-resolution and high-quality speech corpus.\nWe randomly select a 32512-point clip from each input HR audio. We construct the (LR,HR) training pair by filtering out signal components above the Nyquist frequency of LR to simulate the loss of high-frequency components during down-sampling. All filtering configurations use the default values of torchaudio.functional.resample(). For the MDCT layer, we set the frame length N = 512, which yields 256 points per frame after the FFT; the hop length H = 256, which produces 128 frames for a 32512-point segment. Thus, a single spectrogram has a size of 128\u00d7 256."
        },
        {
            "heading": "3.2. Evaluation metrics",
            "text": "Following previous works [4,14\u201316], we use the signal-to-noise ratio (SNR) and Log-spectral distance (LSD) as evaluation metrics to assess the proposed model. Specifically, given a reference signal x and a corresponding approximation x\u0302, SNR is given by\nSNR(x\u0302,x) = 10 log10 ||x||22 ||x\u2212 x\u0302||22\n(4)\nThe LSD is defined as\nLSD(x, x\u0302) = 1\nT T\u2211 t=1 \u221a\u221a\u221a\u221a 1 F F\u2211 f=1 ( log10 X2[t, f ] X\u03022[t, f ] )2 (5)\nwhere T represents the period, X and X\u0302 represent the magnitude spectra of x and x\u0302, respectively, t and f are the index of frame and frequency, respectively. A lower LSD score and higher SNR value indicate a better SR performance. We use the averaged LSD and SNR scores of the VCTK-test as the final result of our model.\nTable 1: A comparison of SNR and LSD scores with 48 kHz target.\nModel Name # Params\nInput sampling rate 24 kHz (2\u00d7 SR) 16 kHz (3\u00d7 SR) 12 kHz (4\u00d7 SR) 8 kHz\n(6\u00d7 SR) SNR \u2191 LSD \u2193 SNR \u2191 LSD \u2193 SNR \u2191 LSD \u2193 SNR \u2191 LSD \u2193\nAudioUNet 70.9M 22.68 1.01 - - 17.15 2.24 - - MUGAN 70.9M 24.81 0.90 - - 16.87 2.12 - - WSRGlow1 229M 26.60 0.70 22.60 0.84 21.20 0.94 18.60 1.05 NU-Wave 2 1.70M 28.40 0.77 24.00 0.93 21.60 1.01 18.80 1.14\nUDM+ - - 0.64 - 0.79 - 0.84 - - Proposed 103M 26.26 0.61 23.46 0.69 21.74 0.77 18.93 0.81\nGround Truth\n50 100 150 200\n8\n16\n24\n32\n40\n48\nF s\nProposed\n50 100 150 200\n8\n16\n24\n32\n40\n48\nF s\nWSRGlow\n50 100 150 200\n8\n16\n24\n32\n40\n48\nF s\nNU-Wave 2\n50 100 150 200\n8\n16\n24\n32\n40\n48\nF s\nFigure 4: Visualised comparison of 4\u00d7 SR. Note that our model produces richer harmonics (better zoom in).\nTable 2: MOS (\u2191)\nTarget Input LR HR SR\n48 kHz 8 kHz 2.5 4.5 3.8 12 kHz 2.8 4.8 4.2 16 kHz 3.0 4.8 4.5 24 kHz 4.0 4.8 4.7\nTable 3: PESQ-wb (\u2191)\nTarget Models PESQ-wb\n16 kHz (2\u00d7 SR)\nUDM+ 2.93 NU-Wave 2 3.38\nNVSR 3.47 Proposed 3.50\nTable 4: Ablation studies for 8 kHz to 48 kHz SR.\nModel # Parameters SNR \u2191 LSD \u2193 Proposed 103M 18.92 0.81 w/o plog 103M Failed to train\nw/o pre-train 103M 11.25 (-7.67) 0.84 (+0.03) w/o Transf blocks 143M 11.47 (-7.45) 1.60 (+0.79) w/o Interp up-sampling 72.3M 17.55 (-1.37) 0.88 (+0.07)\nIn addition to objective evaluation metrics, we also used subjective Mean Opinion Score (MOS) and Wide-band (8k\u219216k) Perceptual Evaluation of Speech Quality (PESQ-wb) to assess the quality of generated SR audio and compared it to that of the original HR audio."
        },
        {
            "heading": "3.3. Training methods and techniques",
            "text": "For our proposed model, we first pre-train it on a joint dataset of HiFi-TTS+VCTK with 120 epochs for learning SSR up to 44.1 kHz. After 60 epochs, the learning rate is linearly reduced to 0. We then fine-tune the model with 80 epochs to learn SSR up to 48 kHz by using only the VCTK part of pre-training dataset with 48 kHz audio only. During fine-tuning, the encoders and bottlenecks in Gglobal and Glocal are frozen. After 40 epochs, the learning rate is linearly reduced to 0. All models were trained on an Nvidia RTX3090 GPU using an Adam optimiser [17] with \u03b21 = 0.5, \u03b22 = 0.999 and = 10\u22126. And Automatic Mixed Precision (AMP) [18] is enabled."
        },
        {
            "heading": "3.4. Results",
            "text": "We chose several state-of-the-art methods as baseline models to compare with our mdctGAN, including AudioUnet [19], MUGAN [20], WSRGlow [7], NU-Wave 2 [21], NVSR [4] and UDM+ [22]. All models are using a 48 kHz output target. Here, the performance of these baseline models on the VCTK dataset [2] is measured using the published models and results of respective authors. Table 1 compares the mdctGAN with other baseline models at a target of 48 kHz using SNR and LSD scores with different input sampling rates. Our model achieves the best LSD scores in all cases, especially for lower input sampling rates. Specifically, our model is more advantageous at 12 kHz and 8 kHz inputs that provides gains of 0.07dB and 0.33dB over the second-best models, respectively. In terms of SNR, our model also yields the best results for 12 kHz and 8 kHz inputs. Both WSRGlow and NU-Wave 2 exceed our proposed model at 24 kHz input and NU-Wave 2 is slightly better than ours at 16 kHz. Note that the mdctGAN is more memory efficient than WSRGlow in terms of the number of model parameters. Figure 4 compares our output with others and the ground truth. And our model produces richer harmonics with greater high-frequency details than competitors. Table 2 shows that our model achieved high MOS that are consistent with the ground\ntruth at various input sampling rates. For 16 to 48 kHz SR, our method are also competitive compared to recent works [8], with Hifi-GAN, WSRGlow, and Dual-Cycle-GAN achieving mean MOS of 4.23, 4.23, and 4.51, respectively. Moreover, our method outperformed competing methods in terms of PESQ scores, shown in Table 3. This indicates that our model is able to generating SR audio that is close to the natural listening quality."
        },
        {
            "heading": "3.5. Ablation Study",
            "text": "To verify the effectiveness of the key components in our model, different configurations are evaluated as follows: i) removing the plog dynamic compressing ; ii) using only VCTK dataset for network training; iii) substituting all Transformer Blocks with ResNet Blocks; iv) using transpose-conv for up-sampling. Table 4 indicates that without plog compression, the network cannot be trained. Pre-training on a larger dataset substantially increases the network\u2019s overall performance. In addition, the addition of Transformer Blocks results in considerable improvements in both SNR and LSD values. Furthermore, with interpolation up-sampling, better performance are obtained at the expense of the increased number of parameters."
        },
        {
            "heading": "4. Conclusion and Future work",
            "text": "We present mdctGAN, a novel SSR method adapting a transformer-based GAN to reconstruct high-quality speech. It works on MDCT domain without additional phase estimation to recover raw waveforms. By incorporating MDCT with multiple critical enhancements, including pseudo-log compression and Transformer blocks, we have successfully proposed an SSR framework and evaluated it on the VCTK test dataset. mdctGAN outperformed previous models for 48 kHz target with various input resolution settings and achieved state-of-the-art LSD scores. The quality of our model\u2019s results was further validated by subjective metrics, MOS and PESQ.\nDespite the many advantages of our proposed approach, there is still room for improvement. Our model needs to be trimmed for real-time processing. Moreover, the SNR is not optimal at low input sampling rates. In the future, we plan to improve mdctGAN to make it more compact and enhance its SR quality. We also encourage further research to follow our proposed MDCT-based approach to achieve better speech enhancement."
        },
        {
            "heading": "5. References",
            "text": "[1] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z.\nTeoh, J. Sotelo, A. de Bre\u0301bisson, Y. Bengio, and A. C. Courville, \u201cMelGAN: generative adversarial networks for conditional waveform synthesis,\u201d in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 14 881\u2013 14 892. [Online]. Available: https://proceedings.neurips.cc/paper/ 2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html\n[2] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d in The 9th ISCA Speech Synthesis Workshop, Sunnyvale, CA, USA, 13-15 September 2016. ISCA, 2016, p. 125. [Online]. Available: http://www.isca-speech.org/archive/SSW 2016/abstracts/ ssw9 DS-4 van den Oord.html\n[3] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/ hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html\n[4] H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian, and D. Wang, \u201cNeural vocoder is all you need for speech super-resolution,\u201d in Interspeech 2022, 23nd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18 - 22 September 2022. ISCA, 2022, p. Forthcoming. [Online]. Available: https://doi.org/10.48550/arXiv.2203.14941\n[5] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, \u201cDCCRN: deep complex convolution recurrent network for phase-aware speech enhancement,\u201d in Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020. ISCA, 2020, pp. 2472\u20132476. [Online]. Available: https://doi.org/10.21437/Interspeech.2020-2537\n[6] S. Lv, Y. Hu, S. Zhang, and L. Xie, \u201cDCCRN+: channel-wise subband DCCRN with SNR estimation for speech enhancement,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 2816\u20132820. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-1482\n[7] K. Zhang, Y. Ren, C. Xu, and Z. Zhao, \u201cWSRGlow: A glowbased waveform generative model for audio super-resolution,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 1649\u20131653. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-892\n[8] R. Yoneyama, R. Yamamoto, and K. Tachibana, \u201cNonparallel High-Quality Audio Super Resolution with Domain Adaptation and Resampling CycleGANs,\u201d 2022. [Online]. Available: https: //arxiv.org/abs/2210.15887\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998\u2013 6008. [Online]. Available: https://proceedings.neurips.cc/paper/ 2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[10] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio, \u201cGenerative Adversarial Nets,\u201d in Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, 2014, pp. 2672\u20132680. [Online]. Available: https://proceedings.neurips.cc/paper/2014/hash/ 5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html\n[11] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro, \u201cHigh-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,\u201d in 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. IEEE Computer Society, 2018, pp. 8798\u20138807. [Online]. Available: http://openaccess.thecvf.com/content cvpr 2018/html/ Wang High-Resolution Image Synthesis CVPR 2018 paper. html\n[12] E. Bakhturina, V. Lavrukhin, B. Ginsburg, and Y. Zhang, \u201cHi-Fi multi-speaker English TTS dataset,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 2776\u20132780. [Online]. Available: https://doi.org/10.21437/Interspeech.2021-1599\n[13] M. Bosi, R. E. Goldberg, and J. L. Mitchell, \u201cIntroduction to digital audio coding and standards,\u201d J. Electronic Imaging, vol. 13, no. 2, pp. 399\u2013400, 2004. [Online]. Available: https://doi.org/10.1117/1.1695413\n[14] H. Wang and D. Wang, \u201cTowards robust speech superresolution,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 29, pp. 2058\u20132066, 2021. [Online]. Available: https: //doi.org/10.1109/TASLP.2021.3054302\n[15] N. C. Rakotonirina, \u201cSelf-attention for audio super-resolution,\u201d in 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), Gold Coast, Australia, October 25-28, 2021. IEEE, 2021, pp. 1\u20136. [Online]. Available: https://doi.org/10.1109/MLSP52302.2021.9596082\n[16] J. Lee and S. Han, \u201cNu-wave: A diffusion probabilistic model for neural audio upsampling,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 1634\u20131638. [Online]. Available: https: //doi.org/10.21437/Interspeech.2021-36\n[17] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [Online]. Available: http://arxiv.org/abs/1412.6980\n[18] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garc\u0131\u0301a, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu, \u201cMixed precision training,\u201d in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Online]. Available: https://openreview.net/forum?id=r1gs9JgRZ\n[19] V. Kuleshov, S. Z. Enam, and S. Ermon, \u201cAudio super-resolution using neural networks,\u201d in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. [Online]. Available: https://openreview.net/forum?id= S1gNakBFx\n[20] S. E. Eskimez and K. Koishida, \u201cSpeech super resolution generative adversarial network,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019. IEEE, 2019, pp. 3717\u20133721. [Online]. Available: https://doi.org/10.1109/ICASSP.2019.8682215\n[21] S. Han and J. Lee, \u201cNu-wave 2: A general neural audio upsampling model for various sampling rates,\u201d in Interspeech 2022, 23nd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18 - 22 September 2022. ISCA, 2022, p. Forthcoming. [Online]. Available: https://doi.org/10.21437%2Finterspeech.2022-45\n[22] C.-Y. Yu, S.-L. Yeh, G. Fazekas, and H. Tang, \u201cConditioning and sampling in variational diffusion models for speech superresolution,\u201d arXiv preprint arXiv:2210.15793, 2022. [Online]. Available: https://arxiv.org/abs/2210.15793"
        }
    ],
    "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
    "year": 2023
}