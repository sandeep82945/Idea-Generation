{
    "abstractText": "The availability of data in expressive styles across languages is limited, and recording sessions are costly and time consuming. To overcome these issues, we demonstrate how to build low-resource, neural text-to-speech (TTS) voices with only 1 hour of conversational speech, when no other conversational data are available in the same language. Assuming the availability of non-expressive speech data in that language, we propose a 3-step technology: 1) we train an F0-conditioned voice conversion (VC) model as data augmentation technique; 2) we train an F0 predictor to control the conversational flavour of the voice-converted synthetic data; 3) we train a TTS system that consumes the augmented data. We prove that our technology enables F0 controllability, is scalable across speakers and languages and is competitive in terms of naturalness over a state-of-the-art baseline model, another augmented method which does not make use of F0 information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Giulia Comini"
        },
        {
            "affiliations": [],
            "name": "Goeric Huybrechts"
        },
        {
            "affiliations": [],
            "name": "Manuel Sam Ribeiro"
        },
        {
            "affiliations": [],
            "name": "Adam Gabry\u015b"
        },
        {
            "affiliations": [],
            "name": "Jaime Lorenzo-Trueba"
        }
    ],
    "id": "SP:3a60c02ea3eba732c297d8a7cd5f10d3e7b82424",
    "references": [
        {
            "authors": [
                "A. van den Oord",
                "S. Dieleman",
                "H. Zen",
                "K. Simonyan",
                "O. Vinyals",
                "A. Graves",
                "N. Kalchbrenner",
                "A.W. Senior",
                "K. Kavukcuoglu"
            ],
            "title": "Wavenet: A generative model for raw audio",
            "venue": "CoRR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. van den Oord",
                "Y. Li",
                "I. Babuschkin",
                "K. Simonyan",
                "O. Vinyals",
                "K. Kavukcuoglu",
                "G. van den Driessche",
                "E. Lockhart",
                "L.C. Cobo",
                "F. Stimberg",
                "N. Casagrande",
                "D. Grewe",
                "S. Noury",
                "S. Dieleman",
                "E. Elsen",
                "N. Kalchbrenner",
                "H. Zen",
                "A. Graves",
                "H. King",
                "T. Walters",
                "D. Belov",
                "D. Hassabis"
            ],
            "title": "Parallel wavenet: Fast High- Fidelity Speech Synthesis",
            "venue": "ICML, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Sotelo",
                "S. Mehri",
                "K. Kumar",
                "J.F. Santos",
                "K. Kastner",
                "A.C. Courville",
                "Y. Bengio"
            ],
            "title": "Char2Wav: End-to-End Speech Synthesis",
            "venue": "ICLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.\u00d6. Arik",
                "M. Chrzanowski",
                "A. Coates",
                "G.F. Diamos",
                "A. Gibiansky",
                "Y. Kang",
                "X. Li",
                "J. Miller",
                "A.Y. Ng",
                "J. Raiman",
                "S. Sengupta",
                "M. Shoeybi"
            ],
            "title": "Deep Voice: Real-time Neural Text-to-Speech",
            "venue": "ICML, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Shen",
                "R. Pang",
                "R.J. Weiss",
                "M. Schuster",
                "N. Jaitly",
                "Z. Yang",
                "Z. Chen",
                "Y. Zhang",
                "Y. Wang",
                "RJ-S. Ryan",
                "R.A. Saurous",
                "Y. Agiomyrgiannakis",
                "Y. Wu"
            ],
            "title": "Natural TTS synthesis by conditioning wavenet on MEL spectrogram predictions",
            "venue": "ICASSP, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Ping",
                "K. Peng",
                "J. Chen"
            ],
            "title": "ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech",
            "venue": "ICLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zheng",
                "X. Li",
                "F. Xie",
                "L. Lu"
            ],
            "title": "Improving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer",
            "venue": "ICASSP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "X. Tan",
                "T. Qin",
                "S. Zhao",
                "Z. Zhao",
                "T. Liu"
            ],
            "title": "Fast- Speech 2: Fast and High-Quality End-to-End Text to Speech",
            "venue": "ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "D. Stanton",
                "Y. Zhang",
                "R.J. Skerry-Ryan",
                "E. Battenberg",
                "J. Shor",
                "Y. Xiao",
                "F.R.Y. Jia",
                "R.A. Saurous"
            ],
            "title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to- End Speech Synthesis",
            "venue": "ICML, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "R.J. Skerry-Ryan",
                "E. Battenberg",
                "Y. Xiao",
                "Y. Wang",
                "D. Stanton",
                "J. Shor",
                "R.J. Weiss",
                "R. Clark",
                "R.A. Saurous"
            ],
            "title": "Towards Endto-End Prosody Transfer for Expressive Speech Synthesis with Tacotron",
            "venue": "ICML, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Lee",
                "K. Park",
                "D. Kim"
            ],
            "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable neural text-to-Speech",
            "venue": "Interspeech, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Huybrechts",
                "T. Merritt",
                "G. Comini",
                "B. Perz",
                "R. Shah",
                "J. Lorenzo-Trueba"
            ],
            "title": "Low-Resource Expressive Text-To-Speech Using Data Augmentation",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Shah",
                "K. Pokora",
                "A. Ezzerg",
                "V. Klimkov",
                "G. Huybrechts",
                "B. Putrycz",
                "D. Korzekwa",
                "T. Merritt"
            ],
            "title": "Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech",
            "venue": "CoRR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.\u00d6. Arik",
                "G.F. Diamos",
                "A. Gibiansky",
                "J. Miller",
                "K. Peng",
                "W. Ping",
                "J. Raiman",
                "Y. Zhou"
            ],
            "title": "Deep Voice 2: Multi-Speaker Neural Text-to-Speech",
            "venue": "CoRR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Ping",
                "K. Peng",
                "A. Gibiansky",
                "S.\u00d6. Arik",
                "A. Kannan",
                "S. Narang",
                "J. Raiman",
                "J. Miller"
            ],
            "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
            "venue": "ICLR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Taigman",
                "L. Wolf",
                "A. Polyak",
                "E. Nachmani"
            ],
            "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
            "venue": "ICLR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S.\u00d6. Arik",
                "J. Chen",
                "K. Peng",
                "W. Ping",
                "Y. Zhou"
            ],
            "title": "Neural voice cloning with a few samples",
            "venue": "NeurIPS, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Lee",
                "T. Kim",
                "S. Lee"
            ],
            "title": "Voice Imitating Text-to-Speech Neural Networks",
            "venue": "CoRR, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Jia",
                "Y. Zhang",
                "R.J. Weiss",
                "Q. Wang",
                "F.R.J. Shen",
                "Z. Chen",
                "P. Nguyen",
                "R. Pang",
                "I. Lopez-Moreno",
                "Y. Wu"
            ],
            "title": "Transfer Learning from Speaker Verification to Multispeaker Text-To- Speech Synthesis",
            "venue": "NeurIPS, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Chen",
                "T. Tu",
                "C. Yeh",
                "H. Lee"
            ],
            "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
            "venue": "Interspeech, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Tits",
                "K. El Haddad",
                "T. Dutoit"
            ],
            "title": "Exploring Transfer Learning for Low Resource Emotional TTS",
            "venue": "IntelliSys, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhang",
                "Y. Lin"
            ],
            "title": "Unsupervised Learning for Sequence-to- Sequence Text-to-Speech for Low-Resource Languages",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Cooper",
                "C. Lai",
                "Y. Yasuda",
                "F. Fang",
                "X. Wang",
                "N. Chen",
                "J. Yamagishi"
            ],
            "title": "Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Embeddings",
            "venue": "ICASSP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Casanova",
                "C. Shulby",
                "E. G\u00f6lge",
                "N.M. M\u00fcller",
                "F.S. de Oliveira",
                "A. C\u00e2ndido J\u00fanior",
                "A. da Silva Soares",
                "S.M. Alu\u0131\u0301sio",
                "M.A. Ponti"
            ],
            "title": "SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text- To-Speech Model",
            "venue": "CoRR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Jia",
                "Y. Zhang",
                "R.J. Weiss",
                "Q. Wang",
                "F.R.J. Shen",
                "Z. Chen",
                "P. Nguyen",
                "R. Pang",
                "I. Lopez-Moreno",
                "Y. Wu"
            ],
            "title": "Transfer Learning from Speaker Verification to Multispeaker Text-To- Speech Synthesis",
            "venue": "NeurIPS, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Hwang",
                "R. Yamamoto",
                "E. Song",
                "J. Kim"
            ],
            "title": "TTS-by- TTS: TTS-Driven Data Augmentation for Fast and High-Quality Speech Synthesis",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Karlapati",
                "A. Moinet",
                "A. Joly",
                "V. Klimkov",
                "D. S\u00e1ez-Trigueros",
                "T. Drugman"
            ],
            "title": "CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Qian",
                "Z. Jin",
                "M. Hasegawa-Johnson",
                "G.J. Mysore"
            ],
            "title": "F0- Consistent Many-To-Many Non-Parallel Voice Conversion Via Conditional Autoencoder",
            "venue": "ICASSP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Luo",
                "J. Chen",
                "T. Takiguchi",
                "Y. Ariki"
            ],
            "title": "Emotional Voice Conversion Using Dual Supervised Adversarial Networks With Continuous Wavelet Transform F0 Features",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., vol. 27, no. 10, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhou",
                "B. Sisman",
                "H. Li"
            ],
            "title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data",
            "venue": "Odyssey, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "K. Zhou",
                "B. Sisman",
                "R. Liu",
                "H. Li"
            ],
            "title": "Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Le Moine",
                "N. Obin",
                "A. Roebel"
            ],
            "title": "Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels",
            "venue": "EUSIPCO, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Lancucki"
            ],
            "title": "Fastpitch: Parallel Text-to-Speech with Pitch Prediction",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Valle",
                "J. Li",
                "R. Prenger",
                "B. Catanzaro"
            ],
            "title": "Mellotron: Multispeaker Expressive Voice Synthesis by Conditioning on Rhythm, Pitch and Global Style Tokens",
            "venue": "ICASSP, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "venue": "ICLR, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Morise",
                "F. Yokomori",
                "K. Ozawa"
            ],
            "title": "WORLD: A vocoderbased high-quality speech synthesis system for real-time applications",
            "venue": "IEICE Trans. Inf. Syst., vol. 99-D, no. 7, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Zhang",
                "S. Pan",
                "L. He",
                "Z. Ling"
            ],
            "title": "Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis",
            "venue": "ICASSP, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Tyagi",
                "M. Nicolis",
                "J. Rohnke",
                "T. Drugman",
                "J. Lorenzo- Trueba"
            ],
            "title": "Dynamic prosody generation for speech synthesis using linguistics-driven acoustic embedding selection",
            "venue": "Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "venue": "ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Jiao",
                "A. Gabrys",
                "G. Tinchev",
                "B. Putrycz",
                "D. Korzekwa",
                "V. Klimkov"
            ],
            "title": "Universal neural vocoding with parallel wavenet",
            "venue": "ICASSP, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B Series"
            ],
            "title": "Method for the subjective assessment of intermediate quality level of audio systems",
            "venue": "International Telecommunication Union Radiocommunication Assembly, 2014.",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Neural text-to-speech (TTS) has been shown to produce highquality synthetised speech from written text. State-of-the-art TTS models usually require a large amount of high-quality speech data [1\u20138]. Traditionally, these data were recorded in a flat, neutral style, however, with the advancement in TTS research, a new interest in various speech styles has arisen [9\u201313]. Thus, in order to generate a varied portfolio of TTS voices which covers voices in multiple styles and languages, a large quantity of data is needed. Consequently, in an effort to mitigate data collection, which is costly and time consuming, interest in low-resource TTS research has grown significantly. The two main research trends of low-resource TTS focus on: 1) transfer learning to a low-resource voice, leveraging a large multi-speaker dataset [14\u201324]; 2) data augmentation, leveraging multi-speaker data [25], voice conversion (VC) capabilities [12, 13], and TTS [26]. The main limitation of these approaches is the assumption that a large amount of data are available for any language and style. In order to make the technology scalable, we propose a language-agnostic method which relies on a minimal amount of such data, regardless of its style. Our work is inspired by [12], which shows that it is possible to build high-quality voices with only a small amount of speech data (target data). [12] leverages an unlimited amount of data from other voices (supporting data), which are converted to the target voice via VC. A multi-speaker TTS model is then trained with the original and augmented data. We remove the main constraint of [12], i.e. the assumption that a large amount of expressive data are available for any language; we assume instead to have flat, inexpressive, neutral-style supporting data. We also reduce the amount of supporting data from an \u201cunlim-\nited amount\u201d to 8-10 hours. Further, our low-resource target data are recorded in an expressive, friendly and spontaneous style, which we call conversational style. We show that our technology is language-agnostic, building 9 voices in 5 locales, while using the same configuration of hyper-parameters. As in [12] we perform data augmentation via VC, using a model based on Copycat [27]. Since we have supporting neutral data, but we would like the augmented data to retain the conversational style of the target voice, we enable the possibility of controlling the output speaker style during VC, leveraging log-F0 (F0) information. Controlling prosody with F0 information has been extensively explored in both VC [28] (especially for emotion conversion [29\u201332]) and TTS literature [4,8,14,33,34]. We learn how to model F0 through a separate model, which is used to produce F0 trajectories in the expressive, personalised style of the target speaker. The output of such model is then used to feed the VC model at inference time. Our 3-steps pipeline consists of: 1) an F0-conditioned VC model to do data augmentation; 2) an F0 predictor that controls the conversational style of the voice-converted synthetic data; 3) a TTS system that consumes the augmented data. The main contributions of this paper are: 1) an F0 predictor model that produces reliable F0 trajectories; 2) a method to control the style of the augmented samples consumed by the TTS model; 3) a scalable, language-agnostic technology for building high-quality, low-resource TTS voices."
        },
        {
            "heading": "2. Methodology",
            "text": "Our goal is to build low-resource, neural TTS voices with only 1 hour of conversational speech, when no other conversational data are available in the same language. In order to do so, we additionally require 8-10 hours of neutral supporting data from another voice in the same gender and locale as the target voice. Given these conditions, we propose a 3-step technology. Firstly, a modified version of Copycat [27] (VC) which makes use of F0 trajectories is trained in order to augment the target speaker data. Secondly, a neural F0 predictor learns to predict F0 trajectories, which are in turn injected into the VC system at inference time. In this way we leverage the supporting data, which are converted to the target speaker identity, but also the target data, from which we learn the conversational style that we want to maintain during VC. Finally, a single speaker TTS model is trained with the original and synthetic data."
        },
        {
            "heading": "2.1. F0-conditioned Voice Conversion",
            "text": "Our proposed VC approach (Figure 1) is a modification of CopyCat [27], with two additions: the concatenation of the speaker embeddings to the upsampled phonemes (before passing them to the phoneme encoder), as in [12]; frame-level F0 as input to the decoder, similarly to [28], except for the fact that we do not apply normalisation. The idea behind this architec-\nar X\niv :2\n20 7.\n14 60\n7v 1\n[ ee\nss .A\nS] 2\n9 Ju\nl 2 02\n2\nture is to learn to disentangle between linguistic information, prosody content and speaker identity, so that VC is performed effectively. [27] uses a variational auto-encoder (VAE) [35] to preserve the supporting data prosody, downsampling and then upsampling the latent spectrogram representation along the time dimension. If the supporting speaker is recorded in a neutral, inexpressive style, her prosody will be passed onto the target speaker during inference. However, in our use-case we would like to maintain the conversational, spontaneous style of the target speaker. To be able to control the style of the converted samples at inference time, we condition the decoder with F0, interpolating it over the unvoiced phonemes. Any other prosodic information which is not F0 and duration is still going to be carried over by the reference encoder. The model is trained on the target and supporting speakers\u2019 data for 100k steps with batch size of 32, and then fine-tuned on the target speaker data for an additional 4k steps. Fine-tuning has been shown to remove the potential remaining speaker leakage [12], without degrading the quality of the synthetic samples. F0 is extracted by the WORLD vocoder [36]. We use a Kullback\u2013Leibler divergence (KLD) loss for training the VAE reference encoder and an L1 loss is computed between the oracle and the predicted mel-spectrogram."
        },
        {
            "heading": "2.2. F0 predictor",
            "text": "As mentioned in the previous section, our aim is to generate synthetic data which mimic the conversational style of the target speaker voice. The architecture of the VC system (Section 2.1) allows us to control prosody at inference time, conditioning the decoder with an F0 trajectory. Since we do not want to maintain the neutral-style F0 trajectories of the supporting speaker, we learn to reproduce the target speaker F0 by training a separate F0 predictor. The F0 predictor output is then used in the VC system at inference time. The model architecture (Figure 2) consists of: a Tacotron 2 text encoder [5] which, similarly to the VC system, receives as input state-level phonetic information upsampled at frame level; a parallel decoder that, given the encoded phonemes and a one-hot speaker ID flag, is trained to predict the F0 trajectories. The F0 predictor is trained for 10k steps on the neutral supporting and conversational target speaker data, and fine-tuned for another 10k steps on the target\nspeaker data only. Interpolation of F0 is used for assigning an F0 value to the unvoiced phonemes. An L1 loss is computed between the oracle and the predicted F0 trajectories."
        },
        {
            "heading": "2.3. Text-to-Speech",
            "text": "Our TTS model is based on Tacotron 2 [5]. As highlighted in Figure 1, it consists of a phonetic encoder, an additional VAE [35] that captures prosodic information [37, 38], and an auto-regressive decoder. A single-head location-attention mechanism [39] is used between the encoder and the decoder. Our TTS systems are trained on 1 hour of target speaker real data and 8-10 hours of synthetic data, generated by the F0conditioned VC system (where the F0 trajectory is the one predicted by the F0 predictor of Section 2.2). As in [12], we additionally fine-tune the TTS models for 4k steps on the target speaker data. Fine-tuning guides the model towards the space of the real data, which results in a refinement in signal and segmental quality, as well as expressiveness. At inference time we condition the decoder with a VAE z-vector computed averaging the target speaker real data. Finally, the predicted melspectrograms are converted to audio waveforms by a Parallel Wavenet universal neural vocoder [40]."
        },
        {
            "heading": "3. Experiments",
            "text": "In this section we assess the strengths of our approach, performing objective analyses of the components of the data augmentation pipeline, as well as perceptual evaluations on the final TTS voices. More specifically, we aim to demonstrate that: 1) the F0-predictor model produces conversational F0 trajectories; 2) we are able to achieve a good level of F0 controllability in VC; 3) the voice-converted samples resemble the target speaker F0 distribution; 4) our proposed technology is able to build low-resource TTS voices and either improves or is on par with the previous best augmented system [12] across different languages.\nExperimental setting. To demonstrate that our technology is scalable in terms of languages and voices, we build 9 conversational TTS voices1, of which 5 are females and 4 are males, in 5 different locales (Canadian French, French, Italian,\n1Samples available on www.amazon.science/low-data-no-problem\nGerman and Spanish), using only 1 hour of data for each target voice. In each experiment we additionally use 8-10 hours of supporting speaker data, i.e. neutral recordings from a voice of the same gender as the target speaker. The data used in these experiments have been professionally recorded and are part of our internal dataset. Every model has been trained in parallel on 4 GPUs (Tesla V100-SXM2-32GB) with batch size of 32, and the same hyper-parameters have been used across all the experiments. The test set consists of 200 samples per voice. For each target voice we also build a baseline system based on [12], which is trained in the same fashion as the proposed one, with the only difference that the synthetic data have been generated with a VC conversion system that does not make use of F0 (therefore the F0-predictor is not needed). Everything else besides the F0-conditioning component is as described in Sections 2.1 and 2.3."
        },
        {
            "heading": "3.1. Objective analysis",
            "text": "F0 predictor quality assessment. Following the procedure described in Section 2.2, we train an F0 predictor for each one of the 9 target voices. Every model is trained on target and supporting speaker pairs. We use the Root Mean Square Error (RMSE) and Pearson\u2019s Correlation Coefficient (Correlation) as metrics to compare the predicted F0 trajectories and the oracle ones in the log domain. The objective metrics are computed on an interpolated F0. Across all the voices, we get an average RMSE of 49.6 and 0.65 for Correlation. We will explain the implication of the results later in this section\nF0 controllability in VC. The main advantage of an F0conditioned model is the ability to gain control over F0 at inference time, which is crucial for handling speaking styles. In this analysis we show how the proposed VC system (Section 2.1) is able to follow the injected F0 trajectory. The left column of Figure 3 presents three examples of injected trajectories (in blue) together with their counterpart extracted from synthetic audio files (in orange). The injected trajectories come from the F0 predictor model from Section 2.2. In the second column of Figure 3, we show how, for the same source samples of the left column, the extracted F0 changes when we inject artificially constructed trajectories. Our model does well in following realistic F0s, while it struggles with the artificial trajectories. It can be argued that the decoder is trained to reconstruct somewhat\n\u201cintelligible\u201d speech, while the artificial trajectories might not follow the short-term F0 variation that depends on the phonetic sequence. The result is essentially a compromise between an attempt to both reconstruct speech and follow the F0 trajectory. Over 200 converted samples, the average RMSE between the injected and extracted F0 curves is 12.01 and the average Correlation is 0.94. As a reference, we compute the same values when injecting the flat line, the sinusoidal curve and the linear ascendent line, obtaining, as average over the 200 samples, 13.56, 106.42 and 76.87 for RMSE, and 0, 0.86 and 0 for average Correlation, respectively. Note that from a mathematical perspective, a correlation of 0 between the flat linear trajectories and the extracted ones is expected.\nF0 distribution in the synthetic data. In this experiment we show that our VC technology generates synthetic samples that reflect the speaking style of the target speaker. We analyse the distributions of F0 and delta-F0 values of both F0conditioned and non F0-conditioned VC models. We obtain the delta-F0 values by computing the first derivative on the F0 trajectories. Delta-F0 encodes the speed of change which we believe is an important feature for characterising styles. To perform this analysis, we generate synthetic data with three data augmentation techniques: 1) No-F0 VC is Copycat as in the baseline [12], without F0 conditioning; 2) Orig-F0 VC is the proposed VC model (Section 2.1), where at inference time the decoder is conditioned with the original F0 of the supporting speaker, rescaled by the mean of the target speaker; 3) Pred-F0 VC is Orig-F0 VC, where at inference time the decoder is conditioned with the F0 trajectory produced by the F0 predictor of Section 2.2.\nWe compare the F0 distributions of the synthetic samples: firstly, we convert the F0 values into probability distributions; then we compute the KLD of each system with respect to the target speaker F0 (and delta-F0) distributions. We perform this analysis over 4 speakers from 4 locales (Canadian French, French, Italian, Spanish). We chose these speakers since they have a wide range of absolute distance in terms of F0 mean and variance with respect to their respective supporting speakers. Per-speaker F0 mean and variance have been computed across the overall speaker data in the linear frequency scaling. We observed that if a target speaker has a much higher F0 variance with respect to the supporting speaker, i.e. the target speaker is more expressive than the supporting one, it is usually harder to produce synthetic data as expressive as the target voice. Having the control over F0 can help to mitigate this effect.\nIn Table 1 we observe that, for Speaker 1, Pred-F0 VC has the smallest KLD value with respect to the real data, while for Speaker 4, the speaker which is the closest to her supporting counter-part, the best system is Orig-F0 VC. Generally, PredF0 VC gets small KLD values for the delta-F0 distributions. Our intuition is that when the supporting and the target speakers are close in terms of F0 behaviour, using the supporting speaker original F0 (rescaled by the target speaker F0 mean) is enough to resemble the F0 distribution of the target speaker (see Speaker 4). When the speakers are further apart, as in the case of Speaker 1, having an F0-predictor can have a high impact. On the other hand, Orig-F0 VC is also the model that leads to higher fluctuations in terms of KLD scores, which indicates that its performances are less controllable, since we do not put any restrictions on the supporting speaker except for being of the same locale and gender of the target voice. No-F0 VC can also lead to low KLD values, however, as in the case of OrigF0 VC, it can be unpredictable. Ultimately, Pred-F0 VC gives us the advantage of controllability and shows consistently low\nKLD values across speakers. We believe that controlling the F0 generation with an high-quality F0 predictor is a key factor in order to consistently produce synthetic data that resemble the target voice\u2019s typical prosody and speaking patterns."
        },
        {
            "heading": "3.2. Perceptual evaluations",
            "text": "We design the experiments in order to highlight how our technology: 1) scales across languages; 2) scales across voices; 3) outperforms our baseline, the state-of-the-art data augmentation approach which does not make use of F0 [12]. A few additional observations: for space reasons, we did not provide results for the TTS systems augmented with data from Orig-F0 VC (the VC system which uses the supporting speaker rescaled F0 to condition the decoder), since they were generally worse than the proposed approach; we also did not use a multi-speaker model as a baseline since [12] already showed that data augmentation leads to significant improvements. Our experiments confirmed the finding.\nEvaluation protocol. We evaluate our models running MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor)-like [41] tests on Naturalness. In the evaluations we omit the reference system, while we use the original recordings, as upper anchor, and the baseline system based on [12]. Each one of the 200 test-cases is evaluated by 15 different users. We apply a mechanism to get the fairest possible results: we select the test utterances to be mainly long utterances, i.e. more than 95% of the utterances contain 10 or more words. This gives the listeners a higher chance of detecting bad TTS samples. The issue with short utterances is that there is less chance of detecting incorrect intonation, rhythm, etc.\nResults. In Table 2 we show the results for the MUSHRA Naturalness evaluations for 9 synthetic voices. We can observe that our proposed data augmentation approach, TTS from F0 VC, statistically improves the baseline, TTS from no-F0 VC, for\n5 out of 9 voices. In the remaining 4 cases, our approach is on par with the baseline. On average, when there is an improvement, our models close the gap between the baselines and the recordings by 16.6%. For two of the voices where we are on par with the baseline (the Canadian French and French female voices), the proposed approach is very close to recordings, being respectively at 94.5% and 90.5% of Naturalness relative to recordings, making it hard to beat the already high performing baseline system. These results indicate that, generally, having the control on F0 during the data augmentation phase leads to more natural TTS voices. Additionally, we demonstrate that we are able to get high-quality voices across female and male speakers belonging to 5 different locales. On average, the proposed method scores 85.92% in terms of Naturalness with respect to natural recordings."
        },
        {
            "heading": "4. Conclusion and future work",
            "text": "In this work we present a language-agnostic methodology to build low-resource TTS conversational voices. The proposed technology augments the low-resource target data via a VC system that enables F0 controllability. An external F0 predictor ensures that the augmented samples resemble the conversational style of the target speaker. Finally, a TTS model consumes the original and augmented data. The ability of learning and controlling a speaker expressiveness means that we do not need to rely on any other conversational supporting data to build a TTS voice. Therefore our technology is largely scalable to voices and languages. Our experiments demonstrate that: 1) the F0predictor model produces reliable F0 trajectories; 2) we are able to achieve a high level of F0 controllability in VC; 3) the voiceconverted samples resemble the target speaker F0 distribution; 4) we produce high-quality TTS voices from low-resource data, being on par or improving the previous best augmented system; 5) our technology is language-agnostic, hence largely scalable."
        },
        {
            "heading": "5. References",
            "text": "[1] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d CoRR, 2016.\n[2] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo, F. Stimberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman, E. Elsen, N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Walters, D. Belov, and D. Hassabis, \u201cParallel wavenet: Fast HighFidelity Speech Synthesis,\u201d in ICML, 2018.\n[3] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner, A. C. Courville, and Y. Bengio, \u201cChar2Wav: End-to-End Speech Synthesis,\u201d in ICLR, 2017.\n[4] S. O\u0308. Arik, M. Chrzanowski, A. Coates, G. F. Diamos, A. Gibiansky, Y. Kang, X. Li, J. Miller, A. Y. Ng, J. Raiman, S. Sengupta, and M. Shoeybi, \u201cDeep Voice: Real-time Neural Text-to-Speech,\u201d in ICML, 2017.\n[5] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, RJ-S. Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu, \u201cNatural TTS synthesis by conditioning wavenet on MEL spectrogram predictions,\u201d in ICASSP, 2018.\n[6] W. Ping, K. Peng, and J. Chen, \u201cClariNet: Parallel Wave Generation in End-to-End Text-to-Speech,\u201d in ICLR, 2019.\n[7] Y. Zheng, X. Li, F. Xie, and L. Lu, \u201cImproving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer,\u201d in ICASSP, 2020.\n[8] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, \u201cFastSpeech 2: Fast and High-Quality End-to-End Text to Speech,\u201d in ICLR, 2021.\n[9] Y. Wang, D. Stanton, Y. Zhang, R. J. Skerry-Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. R., and R. A. Saurous, \u201cStyle Tokens: Unsupervised Style Modeling, Control and Transfer in End-toEnd Speech Synthesis,\u201d in ICML, 2018.\n[10] R. J. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton, J. Shor, R. J. Weiss, R. Clark, and R. A. Saurous, \u201cTowards Endto-End Prosody Transfer for Expressive Speech Synthesis with Tacotron,\u201d in ICML, 2018.\n[11] K. Lee, K. Park, and D. Kim, \u201cSTYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable neural text-to-Speech,\u201d in Interspeech, 2021.\n[12] G. Huybrechts, T. Merritt, G. Comini, B. Perz, R. Shah, and J. Lorenzo-Trueba, \u201cLow-Resource Expressive Text-To-Speech Using Data Augmentation,\u201d in ICASSP, 2021.\n[13] R. Shah, K. Pokora, A. Ezzerg, V. Klimkov, G. Huybrechts, B. Putrycz, D. Korzekwa, and T. Merritt, \u201cNon-Autoregressive TTS with Explicit Duration Modelling for Low-Resource Highly Expressive Speech,\u201d CoRR, 2021.\n[14] S. O\u0308. Arik, G. F. Diamos, A. Gibiansky, J. Miller, K. Peng, W. Ping, J. Raiman, and Y. Zhou, \u201cDeep Voice 2: Multi-Speaker Neural Text-to-Speech,\u201d CoRR, 2017.\n[15] W. Ping, K. Peng, A. Gibiansky, S. O\u0308. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller, \u201cDeep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,\u201d in ICLR, 2018.\n[16] Y. Taigman, L. Wolf, A. Polyak, and E. Nachmani, \u201cVoiceLoop: Voice Fitting and Synthesis via a Phonological Loop,\u201d in ICLR, 2018.\n[17] S. O\u0308. Arik, J. Chen, K. Peng, W. Ping, and Y. Zhou, \u201cNeural voice cloning with a few samples,\u201d in NeurIPS, 2018.\n[18] Y. Lee, T. Kim, and S. Lee, \u201cVoice Imitating Text-to-Speech Neural Networks,\u201d CoRR, 2018.\n[19] Y. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. R., Z. Chen, P. Nguyen, R. Pang, I. Lopez-Moreno, and Y. Wu, \u201cTransfer Learning from Speaker Verification to Multispeaker Text-ToSpeech Synthesis,\u201d in NeurIPS, 2018.\n[20] Y. Chen, T. Tu, C. Yeh, and H. Lee, \u201cEnd-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning,\u201d in Interspeech, 2019.\n[21] N. Tits, K. El Haddad, and T. Dutoit, \u201cExploring Transfer Learning for Low Resource Emotional TTS,\u201d in IntelliSys, 2019.\n[22] H. Zhang and Y. Lin, \u201cUnsupervised Learning for Sequence-toSequence Text-to-Speech for Low-Resource Languages,\u201d in Interspeech, 2020.\n[23] E. Cooper, C. Lai, Y. Yasuda, F. Fang, X. Wang, N. Chen, and J. Yamagishi, \u201cZero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Embeddings,\u201d in ICASSP, 2020.\n[24] E. Casanova, C. Shulby, E. Go\u0308lge, N. M. Mu\u0308ller, F. S. de Oliveira, A. Ca\u0302ndido Ju\u0301nior, A. da Silva Soares, S. M. Alu\u0131\u0301sio, and M. A. Ponti, \u201cSC-GlowTTS: an Efficient Zero-Shot Multi-Speaker TextTo-Speech Model,\u201d CoRR, 2021.\n[25] Y. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. R., Z. Chen, P. Nguyen, R. Pang, I. Lopez-Moreno, and Y. Wu, \u201cTransfer Learning from Speaker Verification to Multispeaker Text-ToSpeech Synthesis,\u201d in NeurIPS, 2018.\n[26] M. Hwang, R. Yamamoto, E. Song, and J. Kim, \u201cTTS-byTTS: TTS-Driven Data Augmentation for Fast and High-Quality Speech Synthesis,\u201d in ICASSP, 2021.\n[27] S. Karlapati, A. Moinet, A. Joly, V. Klimkov, D. Sa\u0301ez-Trigueros, and T. Drugman, \u201cCopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech,\u201d in Interspeech, 2020.\n[28] K. Qian, Z. Jin, M. Hasegawa-Johnson, and G. J. Mysore, \u201cF0Consistent Many-To-Many Non-Parallel Voice Conversion Via Conditional Autoencoder,\u201d in ICASSP, 2020.\n[29] Z. Luo, J. Chen, T. Takiguchi, and Y. Ariki, \u201cEmotional Voice Conversion Using Dual Supervised Adversarial Networks With Continuous Wavelet Transform F0 Features,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 27, no. 10, 2019.\n[30] K. Zhou, B. Sisman, and H. Li, \u201cTransforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data,\u201d in Odyssey, 2020.\n[31] K. Zhou, B. Sisman, R. Liu, and H. Li, \u201cSeen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset,\u201d in ICASSP, 2021.\n[32] C. Le Moine, N. Obin, and A. Roebel, \u201cTowards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels,\u201d in EUSIPCO, 2021.\n[33] A. Lancucki, \u201cFastpitch: Parallel Text-to-Speech with Pitch Prediction,\u201d in ICASSP, 2021.\n[34] R. Valle, J. Li, R. Prenger, and B. Catanzaro, \u201cMellotron: Multispeaker Expressive Voice Synthesis by Conditioning on Rhythm, Pitch and Global Style Tokens,\u201d in ICASSP, 2020.\n[35] D. P. Kingma and J. Welling, \u201cAuto-Encoding Variational Bayes,\u201d in ICLR, 2014.\n[36] M. Morise, F. Yokomori, and K. Ozawa, \u201cWORLD: A vocoderbased high-quality speech synthesis system for real-time applications,\u201d IEICE Trans. Inf. Syst., vol. 99-D, no. 7, 2016.\n[37] Y. Zhang, S. Pan, L. He, and Z. Ling, \u201cLearning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,\u201d in ICASSP, 2019.\n[38] S. Tyagi, M. Nicolis, J. Rohnke, T. Drugman, and J. LorenzoTrueba, \u201cDynamic prosody generation for speech synthesis using linguistics-driven acoustic embedding selection,\u201d in Interspeech, 2020.\n[39] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural Machine Translation by Jointly Learning to Align and Translate,\u201d in ICLR, 2015.\n[40] Y. Jiao, A. Gabrys, G. Tinchev, B. Putrycz, D. Korzekwa, and V. Klimkov, \u201cUniversal neural vocoding with parallel wavenet,\u201d in ICASSP, 2021.\n[41] B Series, \u201cMethod for the subjective assessment of intermediate quality level of audio systems,\u201d International Telecommunication Union Radiocommunication Assembly, 2014."
        }
    ],
    "title": "Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation",
    "year": 2022
}