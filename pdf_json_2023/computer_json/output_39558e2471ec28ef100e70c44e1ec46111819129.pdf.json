{
    "abstractText": "Deep Speech Enhancement Challenge is the 5th edition of deep noise suppression (DNS) challenges organized at ICASSP 2023 Signal Processing Grand Challenges. DNS challenges were organized during 2019-2023 to stimulate research in deep speech enhancement (DSE) [1]. Previous DNS challenges were organized at INTERSPEECH 2020, ICASSP 2021, INTERSPEECH 2021, and ICASSP 2022. From prior editions, we learnt that improving signal quality (SIG) is challenging particularly in presence of simultaneously active interfering talkers and noise. This challenge aims to develop models for joint denosing, dereverberation and suppression of interfering talkers. When primary talker wears a headphone, certain acoustic properties of their speech such as direct-to-reverberation (DRR), signal to noise ratio (SNR) etc. make it possible to suppress neighboring talkers even without enrollment data for primary talker. This motivated us to create two tracks for this challenge: (i) Track-1 Headset; (ii) Track-2 Speakerphone. Both tracks has fullband (48kHz) training data and testset, and each testclips has a corresponding enrollment data (10-30s duration) for primary talker. Each track invited submissions of personalized and non-personalized models all of which are evaluated through same subjective evaluation. Most models submitted to challenge were personalized models, same team is winner in both tracks where the best models has improvement of 0.145 and 0.141 in challenge\u2019s Score as compared to noisy blind testset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harishchandra Dubey"
        },
        {
            "affiliations": [],
            "name": "Ashkan Aazami"
        },
        {
            "affiliations": [],
            "name": "Vishak Gopal"
        },
        {
            "affiliations": [],
            "name": "Babak Naderi"
        },
        {
            "affiliations": [],
            "name": "Sebastian Braun"
        },
        {
            "affiliations": [],
            "name": "Ross Cutler"
        },
        {
            "affiliations": [],
            "name": "Alex Ju"
        },
        {
            "affiliations": [],
            "name": "Mehdi Zohourian"
        },
        {
            "affiliations": [],
            "name": "Min Tang"
        },
        {
            "affiliations": [],
            "name": "Hannes Gamper"
        },
        {
            "affiliations": [],
            "name": "Mehrsa Golestaneh"
        },
        {
            "affiliations": [],
            "name": "Robert Aichner"
        }
    ],
    "id": "SP:e4ee5ff5a53313a567cc0de75f919786907b5b5f",
    "references": [
        {
            "authors": [
                "Harishchandra Dubey",
                "Vishak Gopal",
                "Ross Cutler",
                "Ashkan Aazami",
                "Sergiy Matusevych",
                "Sebastian Braun",
                "Sefik Emre Eskimez",
                "Manthan Thakker",
                "Takuya Yoshioka",
                "Hannes Gamper"
            ],
            "title": "ICASSP 2022 deep noise suppression challenge",
            "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9271\u20139275.",
            "year": 2022
        },
        {
            "authors": [
                "Hyeong-Seok Choi",
                "Hoon Heo",
                "Jie Hwan Lee",
                "Kyogu Lee"
            ],
            "title": "Phase-aware single-stage speech denoising and dereverberation with U-net",
            "venue": "arXiv preprint arXiv:2006.00687, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "Chandan KA Reddy",
                "Vishak Gopal",
                "Ross Cutler",
                "Ebrahim Beyrami",
                "Roger Cheng",
                "Harishchandra Dubey",
                "Sergiy Matusevych",
                "Robert Aichner",
                "Ashkan Aazami",
                "Sebastian Braun"
            ],
            "title": "The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results",
            "venue": "arXiv preprint arXiv:2005.13981, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "Chandan KA Reddy",
                "Harishchandra Dubey",
                "Kazuhito Koishida",
                "Arun Nair",
                "Vishak Gopal",
                "Ross Cutler",
                "Sebastian Braun",
                "Hannes Gamper",
                "Robert Aichner",
                "Sriram Srinivasan"
            ],
            "title": "INTERSPEECH 2021 Deep Noise Suppression Challenge",
            "venue": "ISCA INTERSPEECH, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Chandan K.A. Reddy",
                "Harishchandra Dubey",
                "Vishak Gopal",
                "Ross Cutler",
                "Sebastian Braun",
                "Hannes Gamper",
                "Robert Aichner",
                "Sriram Srinivasan"
            ],
            "title": "ICASSP 2021 Deep Noise Suppression Challenge",
            "venue": "IEEE ICASSP, 2021, pp. 6623\u20136627.",
            "year": 2021
        },
        {
            "authors": [
                "Yangyang Xia",
                "Sebastian Braun",
                "Chandan KA Reddy",
                "Harishchandra Dubey",
                "Ross Cutler",
                "Ivan Tashev"
            ],
            "title": "Weighted speech distortion losses for neural-network-based real-time speech enhancement",
            "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 871\u2013875.",
            "year": 2020
        },
        {
            "authors": [
                "Babak Naderi",
                "Ross Cutler"
            ],
            "title": "Subjective evaluation of noise suppression algorithms in crowdsourcing",
            "venue": "ISCA INTER- SPEECH, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Chandan KA Reddy",
                "Vishak Gopal",
                "Ross Cutler"
            ],
            "title": "Dnsmos p. 835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors",
            "venue": "ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 886\u2013890.",
            "year": 2022
        },
        {
            "authors": [
                "Nauman Dawalatabad",
                "Mirco Ravanelli",
                "Fran\u00e7ois Grondin",
                "Jenthe Thienpondt",
                "Brecht Desplanques",
                "Hwidong Na"
            ],
            "title": "Ecapa-tdnn embeddings for speaker diarization",
            "venue": "arXiv preprint arXiv:2104.01466, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Brecht Desplanques",
                "Jenthe Thienpondt",
                "Kris Demuynck"
            ],
            "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
            "venue": "arXiv preprint arXiv:2005.07143, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "Jort F Gemmeke",
                "Daniel PW Ellis",
                "Dylan Freedman",
                "Aren Jansen",
                "Wade Lawrence",
                "R Channing Moore",
                "Manoj Plakal",
                "Marvin Ritter"
            ],
            "title": "Audio set: An ontology and human-labeled dataset for audio events",
            "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, pp. 776\u2013780.",
            "year": 2017
        },
        {
            "authors": [
                "Sefik Emre Eskimez",
                "Takuya Yoshioka",
                "Alex Ju",
                "Min Tang",
                "Tanel Parnamaa",
                "Huaming Wang"
            ],
            "title": "Real-time joint personalized speech enhancement and acoustic echo cancellation with e3net",
            "venue": "arXiv preprint arXiv:2211.02773, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Manthan Thakker",
                "Sefik Emre Eskimez",
                "Takuya Yoshioka",
                "Huaming Wang"
            ],
            "title": "Fast real-time personalized speech enhancement: End-to-end enhancement network (e3net) and knowledge distillation",
            "venue": "arXiv preprint arXiv:2204.00771, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Harishchandra Dubey",
                "Vishak Gopal",
                "Ross Cutler",
                "Ashkan Aazami",
                "Sergiy Matusevych",
                "Sebastian Braun",
                "Sefik Emre Eskimez",
                "Manthan Thakker",
                "Takuya Yoshioka",
                "Hannes Gamper",
                "Robert Aichner"
            ],
            "title": "ICASSP 2022 Deep Noise Suppression Challenge",
            "venue": "ICASSP, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Jont B Allen",
                "David A Berkley"
            ],
            "title": "Image method for efficiently simulating small-room acoustics",
            "venue": "The Journal of the Acoustical Society of America, vol. 65, no. 4, pp. 943\u2013950, 1979.",
            "year": 1979
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Deep Speech Enhancement, Personalized P.835, Perceptual Speech Quality, Personalized Noise Suppression"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Deep Speech Enhancement (DSE) models perform significantly better than their classical counterparts [1\u20136]. We open-sourced training datasets and test sets, Personalized ITU-T P.835 subjective evaluation framework [7] as part of the previous DNS challenges. Our GitHub repository1 open-sourced Personalized and Non-personalized DNSMOS P.835 [8] and word accuracy (WAcc) APIs to empower iterative model improvements for teams articipating in the challenge. This reduces the barrier to entry in field and provides standard tools for evaluation of DSE models. Like previous challenges, each track has two testsets: (i) development (dev) test set which was released at the beginning of the challenge; (ii) blind test set released a few days before the final challenge deadline. While dev testset enables intermediate model evaluations, blind set is used for final ranking of models based on challenge metric (Score). Registration and submission of enhanced clips was done through CMT site https://cmt3.research.microsoft.com/DNSChallenge2023, cloud\n1 https://github.com/microsoft/DNS-Challenge\nstorage and Azure Blobs as per participant\u2019s preference. Questions related to challenge can be sent to dns challenge@microsoft.com.\nIn this challenge, participants could use any datasets including external corpora, challenge training datasets to do model training. We do not prohibit the use of headset corpora for training speakerphone models and vice versa. Participants were required to describe the datasets used for training their models in sufficient detail in their extended journal papers, and provide a brief coverage in 2-page ICASSP grand challenge paper. Challenge website https: //aka.ms/5th-dns-challenge has details of scope and requirements; definitions of algorithmic latency, processing latency, causal model, Real-time factor (RTF) and associated challenge rules; and name of winning teams etc. We verified whether the top models are realtime or not and provide this information on our website. Verification was done with a NRT testset which contains clips with overlapping segments. Previous challenge websites are linked at https: //aka.ms/dns-challenge.\nWe introduced the following changes in this challenge: (i) there are two tracks: Headset, and Speakerphone both has desktop and mobile recordings in testsets; (ii) All testclips in both tracks has 10- 30s enrollment speech (primary talker) with or without noise; (iii) Personalized P.835 framework is improved and now includes voice recognition, robust spam filtering and more accurate evaluation of enhanced testclips with noise and neighboring talkers; (iv) Personalized P.835 framework uses cleaned enrollment speech which was enhanced using a non-causal model. Cleaned enrollment speech helps human rater perform subjective evaluation more consistently.; (v) Personalized and non-personalized models for a track were sent to same subjective evaluation and ranked together, i.e. personalized and non-personalized models are treated alike and compared against each other. (vi) Track-1 and Track-2 has their separate subjective evaluations to ensure we have headset testclips evaluated together, and speakerphone testclips evaluated together.\nMost of the models submitted to challenge were personalized models. Winning teams in both tracks have similar ranking in each track. We invited challenge participants to submit their Preference-2 models which were not sent for subjective evaluation. These enhanced clips are expected to be used in semi-supervised training of personalized DNSMOS P.835."
        },
        {
            "heading": "2. CHALLENGE TRACKS",
            "text": "Enhancing speech quality (i.e., eliminating speech distortion) and suppressing noise, reverberation and neighboring talkers are two trade-offs DSE models must handle. DSE models submitted to challenge were supposed to do joint denoising and dereverberation in presence of neighboring (interfering) talkers. The goal is to enhance audio signal and preserve the primary talker while suppressing the\nar X\niv :2\n30 3.\n11 51\n0v 2\n[ cs\n.S D\n] 9\nM ay\n2 02\n3\nneighboring talkers, noise, and reverberation. All datasets used in this challenge were full band (48 kHz). It aimed to study headset and speakerphone DSE in separate tracks thereby creating possibility of new insights. Acoustic properties of headset scenarios can be leveraged in developing models for suppressing neighboring talkers without enrollment speech. Non-personalized model not requiring enrollment speech are suitable for consumers/industry with strict privacy requirements.\nThis challenge has two tracks: Track-1 Headset; Track-2 Speakerphone. Dev and blind testset for both tracks were different. The testsets were collected using similar procedure except that the Track1 testsets were collected using Headset devices while Track-2 testsets were collected using Speakerphone devices. Each test clip in both tracks has enrollment speech with 30s duration. The enrollment speech can be noise-free or noisy and with or without reverberation. This facilitates multi-condition enrollment of primary talkers which serves as a measure of robustness for personalized models which use enrollment speech as additional input for enhancing the testclips. Participants could choose to work on models with speaker enrollment or without it for one or both tracks. Each team was asked to submit 1-4 models depending on what models they trained. Each participating team could submit a maximum of one personalized and one non-personalized model for each track, e.g., a team can submit one personalized and one non-personalized model for Track 1 but not two personalized or two non-personalized model for Track 1. Similarly, another team could submit 4 models, personalized and non-personalized model for Track 1 and personalized and non-personalized model for Track 2. This rule facilitates almost equal representation of personalized and non-personalized models, and also similar participation in both tracks.\nWhile submitting the blind set enhanced clips, participants were asked to submit their preference for models submitted in each Track, e.g. they had to assign Preference-1 to their best model and Preference-2 to their second best model in each track. From the subjective evaluation of dev testsets, we noticed that subjective results were taking several days of time to get back the results. Also, very large batches of enhanced testclips may also results in some inconsistency in evaluation results. We used Preference-1 clips from all teams in both tracks for final evaluation based on enhanced blind set. Preference-2 enhanced clips could be leveraged in semi-supervised training of Personalized DNSMOS (PDNSMOS) P.835 model or in another subjective evaluation for generating labels for training of PDNSMOS P.835. All models for a track were evaluated and ranked together i.e., both personalized and non-personalized models for Track 1 went through one subjective evaluation. Similarly, for Track 2, all models in one subjective evaluation. Participants were encouraged to conduct experiments with both personalized and non-personalized models to elucidate the benefits of personalization. Though, this was not a requirement for this challenge."
        },
        {
            "heading": "3. CHALLENGE DATASETS",
            "text": ""
        },
        {
            "heading": "3.1. Training Data",
            "text": "We used a machine learning model for extracting the near-field headset-like clean speech which is released as clean speech for headset track. We used entire clean speech from 4th DNS Challenge as clean speech for speakerphone track. Noise dataset and impulse responses are same as in 4th DNS Challenge [1]. Participants can reuse near-field clean speech in Track 1 by convolving it with impulse responses for generating the training data for Track 2. The provided datasets include different languages with talker and device variety.\nWe also provide speaker ID information for all clean speech clips to facilitate development of personalized models for both tracks. We also provide code for extracting speaker embeddings based on stateof-the-art ECAPA-TDNN embeddings trained on Voxceleb [9\u201311]. Along with clean speech, we added clean speech with emotions such as crying, yelling, laughter, or singing to training data. Entire training set consists of clean speech clips in English and 10 non-English languages.\nClean speech in the Track-2 training set is a total of 760.53 hours: read speech (562.72 hours), singing voice (8.80 hours), emotional speech (3.6 hours), Chinese Mandarin (185.41 hours). Track1 clean speech is obtained by doing near-end speech extraction on Track-2 dataset. Clean speech has four subsets: (i) Read speech recorded in clean conditions; (ii) Singing clean speech; (iii) Emotional clean speech; and (iv) non-English clean speech.\nNoise data included in the training set is chosen from AudioSet [12] and is identical to noise set in 4th DNS Challenge [1]. AudioSet is a collection of about 2 million human-labeled 10s sound clips extracted from YouTube videos. There are over a million clips in AudioSet with audio classes music and speech and less than 200 clips for classes such as toothbrush, creak, etc. Approximately 42% of the clips have a single class, but the rest may have 2 to 15 labels. Hence, we developed a sampling approach to balance the dataset in such a way that each class has at least 500 clips. We also used a speech activity detector to remove the clips with any kind of speech activity, to strictly separate speech and noise data. Audioset clips were made available at 44.1kHz, we upsampled those to fullband (48 kHz). The resulting noise dataset has 152 audio classes and 60,000 clips [1]. In total, there are 181 hours of noise data in the training set."
        },
        {
            "heading": "3.2. Development Testset",
            "text": "Both test sets consist of fullband audio clips recorded in realworld scenarios collected through crowd-sourcing where workers read provided text prompts and record their voice using desktop/laptop/mobile devices in the presence of noise and/or neighboring talkers. We include some new noise types in the test set covering relevant real-world scenarios, device variety and added paralinguistic test set as new category. Our dev test set consists of real-world test clips recorded by crowd-sourced workers.\nThe development test set for the non-personalized track consists of 600 real recordings. All clips contain noisy speech in the English language. Among these, 193 test clips have emotional speech in the presence of noise. There are six emotion types, namely happy, sad, angry, yelling, crying, and laughter. Crowd-sourced workers were asked to read provided text prompts and create emotional events in each test clip. The remaining clips contain the voice of a talker reading text in the presence of the following noise types: fan, air conditioner, typing, door shutting, clatter noise, car noise (i.e., standing near a car on a busy street or standing outside the car), kitchen noise (noise from kitchen utensils, dish scrubbing etc.), dish washer, running water, opening chips bags, munching or eating, creaking chair, heavy breathing, copy machine, baby crying, dog barking, insidecar noise (e.g., sitting on a passenger seat in a car which is being driven by someone else), mouse clicks, mouse scroll wheel, touch pad clicks, etc. Each test clip was recorded at 48 kHz with a duration of 10 to 20 seconds. Workers were asked to record in the near-field (close-talk) and far-field with distances of 1, 2, and 3 meters. All test clips in the non-personalized development test set were recorded using a laptop or desktop computer. Both development and blind test set have 2.5 minutes of enrollment speech for primary talkers\nto be used in personalized denoising. PDNS leverages speaker embedding (features) for preserving only the primary talker in a noisy environment while suppressing the neighboring talkers and noise. The development test set for the personalized track consists of 1443 real recordings. All clips contain noisy speech in the English language. Among these, 193 test clips have emotional speech in the presence of noise and are identical to the emotional test clips in the non-personalized track. There are 737 test clips where the primary talker reads the provided text in the presence of the same noise types as those in the non-personalized track. Each test clip was recorded at 48 kHz with a duration of 10\u201320 seconds. Workers were asked to record in the near-field (close-talk) and far-field with distances of 1, 2, and 3 meters. There are 166 test clips with the primary talker speaking in the presence of a neighboring talker and noise where both the noise and neighboring talker are simultaneously active in the primary talker\u2019s background. There are 347 test clips where the primary talker is speaking in the presence of a neighboring speaker with no background noise. Thus, we have simulated three scenarios for PDNS: (i) primary talker in the presence of noise; (ii) primary talker in the presence of neighboring talker; and (iii) primary talker in the presence of simultaneously active neighboring talker and noise. All test clips in the personalized development test set were recorded using a laptop or desktop computer."
        },
        {
            "heading": "3.3. Blind Testset",
            "text": "We include some new noise types in the test set covering relevant real-world scenarios, device variety and added paralinguistic test set as new category. Blind testset consists of real testclips which were not previously used in any challenge and not otherwise available publically. Our test set consists of real-world test clips recorded by crowd-sourced workers. Some of the devices used for collecting blind set includes [list of devices]. Blind set clips were chosen to confirm blind set specifications which were also shared with crowdsourced workers. We performed rigorous quality assurance (QA) to ensure blind set is representative of real-world scenarios in terms of speaker variety, device variety, variety in acoustic scenarios, different direct-to-reverb-ratio (DRR), different T60 which is achieved by changing the relative and absolute position of primary and interfering talkers, noise source and presence of reflecting surfaces etc.\nUnlike previous challenges, Blind testset in this challenge contains paralinguistic test clips. These contains standard forms of paralanguage [13] including but not limited to The throat-clear, \u201dhmm\u201d or \u201dmhm\u201d, \u201dHuh?\u201d or \u201dwhat?, Gasps, Sighs, Moans and groans, Deceptive speech, Sincere speech, Speech with high-base, Speech with high-pitch, Speech with low-pitch, Confident speech, Tired speech (when talker is tired), Persuasive speech, Voice change mid-clip (i.e. mimicry in last 50% of the clip).\nBlind testset will also include emotional speech including but not limited to happy, sad, angry, yelling, crying, and laughter. Blind testset include real test clips with high reverberation, high reverberation with noise, and noise in presence of interfering talkers. Testset noises include but not limited to Office scenarios (Typing, AC, Door shutting, Eating/munching, Copy machine, Squeaking chair, Notification sounds etc.), Home scenarios (Baby crying, Dogs, TV, Radiators, hair dryer, kitchen noise, running water etc.), Appliances (Washer Dryer, Dishwasher, Coffee maker, kitchen noise, Vacuum cleaner etc.), Fire alarm, Car, Inside parked car on busy road, Incar neighboring talkers, Traffic Road, Car noise (from machinery, control systems, turn signal etc.), Cafe\u0301, Coffee machine, Blender, Background babble, Airport Announcements etc."
        },
        {
            "heading": "4. EVALUATION SETUP",
            "text": ""
        },
        {
            "heading": "4.1. Baseline Models",
            "text": "Along with training datasets and testsets, we also provide a baseline model (or enhanced clips) for both tracks.\nBaseline models were personalized and non-personalized variants of models presented in [14]."
        },
        {
            "heading": "4.2. Subjective Evaluation",
            "text": ""
        },
        {
            "heading": "4.2.1. Cleaning Enrollment Clips",
            "text": "We cleaned (enhanced) enrollment clips with a non-causal model based on E3Net architecture [15]. The subjective evaluation uses only 5s of enrollment speech. We manually choose a 5s segment either from the enhanced enrollment clip ensure all long pause (\u00bf0.2s) are removed and resulting 5s audio is normalized to ensure easier recognition of primary talker by human raters. The non-causal DSE model is a fullband time domain domain based on end-to-end enhancement network (E3Net) which was originally proposed for the task of personalized speech enhancement using wideband signals [15].\nNon-causal model is non-personalized and is trained to remove background noise and reverberation for fullband speech. This model uses a learnable encoder and decoder instead of STFT and iSTFT which can mitigate the problem of imperfect phase reconstruction that exists in most time-frequency based speech enhancement methods. This model is based on 20ms frames with a stride of 10ms. The core block in non-causal model is a bidirectional LSTM which offers improved performance for offline processing which is suitable for enhancing enrollment clips for subjective evaluations.\nThis model was trained on training clean speech and noise from he 4th DNS challenge [16]. Impulse responses used for training this model consists of 150,000 simulated impulse responses at sampling rate 48kHz using the Image Source method [17]. The reverberant speech and noise signals were mixed with a signal-to-noise ratio (SNR) drawn from a Gaussian distribution with N(-5,20) dB. All training samples were 10s segments. Training data consists of synthesized 1000 hours of fullband data. For increasing the robustness of the model with respect to audios with different bandwidths, namely narrowband and wideband audios, we also downsampled half of the 1000 hours training set to 16kHz and 8kHz respectively and upsampled them back to 48 kHz and combined these with the training set. Thus, the final training dataset has 3000 hours of data."
        },
        {
            "heading": "4.2.2. Personalized P.835 Framework",
            "text": "This challenge relies on ITU-T P.835 [7] subjective evaluation framework. A modified version (Personalized) ITU-T P.835 was used for measuring the performance of personalized DSE models. Personalized P.835 framework uses 5s of clean enrollment speech for primary talkers to help human raters recognize the primary talker\u2019s voice while assigning subjective scores. Human raters were instructed to focus on the quality of the voice of the primary talker when two or more talkers were present in a test clip. In addition, personalized P.835 subjective framework is improved to include voice recognition, better spam filtering and more accurate evaluation of enhanced testclips with noise and neighboring talkers.\nIn all test clips, we have only one primary talker in enrollment clip while noisy test clip may have noise, reverberation, one or more neighboring talkers in addition to a primary talker. The goal of DSE\nmodels is to preserve primary talker\u2019s speech while suppressing everything else.\nCrowd-sourced workers doing subjective evaluation are instructed to rate interfering talkers as undesirable signal so the model which suppresses interfering talkers is rated higher. Personalized P.835 subjective framework provides three scores, namely speech quality (SIG), background noise quality (BAK), and overall audio quality (OVRL). We evaluated all challenge models through our Personalized P.835 framework to get the subjective ratings used for computing the challenge metric."
        },
        {
            "heading": "4.3. Word Accuracy",
            "text": "We computed word accuracy (WAcc) from a state-of-the-art speech recognition system. We did WAcc computation ourselves during last week of the challenge to ensure all models are evaluated in exactly same way. WAcc is an objective metric for measuring the impact of speech enhancement on speech recognition transcription service. WAcc is defined as WAcc = 1\u2212 WER,\nwhere WER is the word error rate of speech recognition system. We transcribed the entire blind set for both tracks to obtained the ground-truth. Dev testset was not transcribed. We computed WAcc results internally. Unlike subjective P.835 framework which uses only 7s of manually chosen segment from noisy or enhanced clips, WAcc engine uses entire testclips. Blind set test clips are of different duration from 10s to more than 6 minutes.\nBlind testset was collected from crowd-sourced data collection using various recording apps where the worker was provided with a text prompts to read from. Since there are often reading errors, omissions of words etc., prompts do not reflect the correction transcription. We used a five step approach to obtain ground-truth transcription for blind testset. In first step, we just obtain the prompts for each testclips in blind set. In second step, we obtained the transcripts from the state-of-the-art speech recognition engine for each testclips in blind set. In third step, expert human listeners listened each testclips and generated the corresponding human transcripts. Expert listeners were advised to listen audio clips several times until they were confident about their transcription. In fourth step, we compute word error rate (WER) for each testclip in blind set. Then, the testclips with WER \u00bf 0.5 were chosen for fifth round of listening.\nIn fifth round of listening, human listeners listen the clips with WER \u00bf 0.5 and validated/corrected the human transcriptions. We found that a very few clips were needed to be corrected at fifth stage this elucidates the robustness of our transcription approach.\nWe found that several clips in blind set results had WER \u00bf 0.5, it is often due to insertion error where DSE models leaked the secondary talker\u2019s voice and those speech were transcribed by speech recognition engine. During five step approach for generating ground-truth transcription, we found out that there were several clips where neighboring talker was sounding very similar to primary talker (e.g., same accent, same gender, similar loudness and sitting close to each other), some of those clips took more than 10 times listening to correctly transcribe, some of those were not possible to transcribe and hence skipped. Thus, our WAcc was computed over entire duration of selected clips with ground-truth transcripts. It is important to note that speech recognition engine is not personalized and does not use any enrollment speech to focus on primary talker. In future, it would be interesting to develop a personalized speech recognition engine. WAcc ground-truth transcripts only contained words spoken by the primary talker, thus treating interfering talker as undesirable signal."
        },
        {
            "heading": "4.4. Challenge Metric",
            "text": "Like past challenges, models in both tracks were ranked in terms of a final score obtained by weighted average of subjective P.835 scores and word accuracy (WAcc). Four metrics, three personalized P.835 subjective scores namely SIG, BAK, OVRL and WAcc from a speech recognition system were used to evaluate challenge models. Metrics on the blind test set were combined into a final score for ranking the models. Higher WAcc shows superior speech enhancement performance with respect to speech recognition. Higher P.835 scores shows better subjective speech quality. The final score is computed as Final score = 0.5[WAcc + 0.25(OVRL \u2212 1)].\nWe evaluated the submitted models based on Personalized ITUT P.835 subjective evaluation scores, namely speech quality (SIG), background noise quality (BAK), and overall audio quality (OVRL), and WAcc from a state-of-the-art speech recognition system. WAcc is an objective metric for measuring the impact of speech enhancement on speech recognition transcription service."
        },
        {
            "heading": "5. RESULTS & DISCUSSIONS",
            "text": "There were 11 submissions in Track-1 and 11 submissions in Track2 out of which 10 teams participated in both tracks. Almost all models in both tracks were personalized models. We had two baseline models for Track-1 out of which one was non-personalized model. We had personalized model as baseline for Track-2. By dint of acoustic properties of primary speech in a headset scenarios, a nonpersonalized model may be able to suppress neighboring talkers and noise, hence we included a non-personalized model. Each team was asked to provide enhanced clips from their models. Teams were allowed to submit maximum of two models (at most one personalized and at most one non-personalized) per track. Since subjective evaluations may take very long-time if number of submitted models is large, we ask participants to provide their Preference of models. We chose the first Preference model from each team in their respective tracks for subjective evaluations. We also conducted a dev testset subjective evaluation based on enhanced dev set and provided the results to participants.\nFig. 1 show the subjective personalized P.835 scores, WAcc and challenge metric (Score) for all teams sorted in decreasing order of performance. dMOS for SIG, BAK, OVRL refers to the difference in SIG, BAK, OVRL between the enhanced clip and corresponding noisy clip. Similarly, dWAcc is the difference in WAcc between the enhanced clip and noisy clip. We conducted ANOVA test on top models in each track to determine statistical significance (see https: //aka.ms/5th-dns-challenge) for ANOVA results."
        },
        {
            "heading": "6. CONCLUSIONS",
            "text": "This challenge has a more diverse blind test set collected through crowdsourcing using multiple data vendors. We included paralinguistic testclips and leakage testclips in blind set. We enhanced and chose the enrollment speech segment for primary talkers, and noisy segment from testclips to ensure robust subjective evaluations. We verified if winning models are causal with helps of our NRT testset. Results show degradation in signal quality (SIG) for most of the models. It may be due to two reasons: (i) testset is significantly challenging; (ii) Personalized model end up suppressing the primary talker. Noticeable SIG degradation\u2019s happen due to suppression of primary talker\u2019s speech and/or leakage of interfering talker and noise. Detailed analysis of results from current and previous DNS challenges will be covered in our extended OJSP journal paper. We encourage all the challenge participants and readers to send us their feedback and contribute to DNS Challenge repository."
        },
        {
            "heading": "7. REFERENCES",
            "text": "[1] Harishchandra Dubey, Vishak Gopal, Ross Cutler, Ashkan Aazami, Sergiy Matusevych, Sebastian Braun, Sefik Emre Eskimez, Manthan Thakker, Takuya Yoshioka, Hannes Gamper, et al., \u201cICASSP 2022 deep noise suppression challenge,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9271\u20139275.\n[2] Hyeong-Seok Choi, Hoon Heo, Jie Hwan Lee, and Kyogu Lee, \u201cPhase-aware single-stage speech denoising and dereverberation with U-net,\u201d arXiv preprint arXiv:2006.00687, 2020.\n[3] Chandan KA Reddy, Vishak Gopal, Ross Cutler, Ebrahim Beyrami, Roger Cheng, Harishchandra Dubey, Sergiy Matusevych, Robert Aichner, Ashkan Aazami, Sebastian Braun,\net al., \u201cThe interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,\u201d arXiv preprint arXiv:2005.13981, 2020.\n[4] Chandan KA Reddy, Harishchandra Dubey, Kazuhito Koishida, Arun Nair, Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes Gamper, Robert Aichner, and Sriram Srinivasan, \u201cINTERSPEECH 2021 Deep Noise Suppression Challenge,\u201d ISCA INTERSPEECH, 2021.\n[5] Chandan K. A. Reddy, Harishchandra Dubey, Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes Gamper, Robert Aichner, and Sriram Srinivasan, \u201cICASSP 2021 Deep Noise Suppression Challenge,\u201d in IEEE ICASSP, 2021, pp. 6623\u20136627.\n[6] Yangyang Xia, Sebastian Braun, Chandan KA Reddy, Harishchandra Dubey, Ross Cutler, and Ivan Tashev, \u201cWeighted speech distortion losses for neural-network-based real-time speech enhancement,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 871\u2013875.\n[7] Babak Naderi and Ross Cutler, \u201cSubjective evaluation of noise suppression algorithms in crowdsourcing,\u201d in ISCA INTERSPEECH, 2021.\n[8] Chandan KA Reddy, Vishak Gopal, and Ross Cutler, \u201cDnsmos p. 835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors,\u201d in ICASSP 2022- 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 886\u2013890.\n[9] Nauman Dawalatabad, Mirco Ravanelli, Franc\u0327ois Grondin, Jenthe Thienpondt, Brecht Desplanques, and Hwidong Na, \u201cEcapa-tdnn embeddings for speaker diarization,\u201d arXiv preprint arXiv:2104.01466, 2021.\n[10] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck, \u201cEcapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\u201d arXiv preprint arXiv:2005.07143, 2020.\n[11] \u201cDNSMOS Git Repo,\u201d https://huggingface.co/speechbrain/ spkrec-ecapa-voxceleb, [Online; accessed 2022-09-01].\n[12] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, pp. 776\u2013780.\n[13] \u201cParalanguage,\u201d https://en.wikipedia.org/wiki/Paralanguage, [Online; accessed 2022-09-01].\n[14] Sefik Emre Eskimez, Takuya Yoshioka, Alex Ju, Min Tang, Tanel Parnamaa, and Huaming Wang, \u201cReal-time joint personalized speech enhancement and acoustic echo cancellation with e3net,\u201d arXiv preprint arXiv:2211.02773, 2022.\n[15] Manthan Thakker, Sefik Emre Eskimez, Takuya Yoshioka, and Huaming Wang, \u201cFast real-time personalized speech enhancement: End-to-end enhancement network (e3net) and knowledge distillation,\u201d arXiv preprint arXiv:2204.00771, 2022.\n[16] Harishchandra Dubey, Vishak Gopal, Ross Cutler, Ashkan Aazami, Sergiy Matusevych, Sebastian Braun, Sefik Emre Eskimez, Manthan Thakker, Takuya Yoshioka, Hannes Gamper, and Robert Aichner, \u201cICASSP 2022 Deep Noise Suppression Challenge,\u201d in ICASSP, 2022.\n[17] Jont B Allen and David A Berkley, \u201cImage method for efficiently simulating small-room acoustics,\u201d The Journal of the Acoustical Society of America, vol. 65, no. 4, pp. 943\u2013950, 1979."
        }
    ],
    "title": "ICASSP 2023 DEEP NOISE SUPPRESSION CHALLENGE",
    "year": 2023
}