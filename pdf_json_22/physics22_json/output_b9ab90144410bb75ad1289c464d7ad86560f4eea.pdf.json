{
    "abstractText": "For many quantum systems intended for information processing, one detects the logical state of a qubit by integrating a continuously observed quantity over time. For example, ion and atom qubits are typically measured by driving a cycling transition and counting the number of photons observed from the resulting fluorescence. Instead of recording only the total observed count in a fixed time interval, one can observe the photon arrival times and get a state detection advantage by using the temporal structure in a model such as a Hidden Markov Model. We study what further advantage may be achieved by applying pulses to adaptively transform the state during the observation. We give a three-state example where adaptively chosen transformations yield a clear advantage, and we compare performances on an ion example, where we see improvements in some regimes. We provide a software package that can be used for exploration of temporally resolved strategies with and without adaptively chosen transformations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shawn Geller"
        },
        {
            "affiliations": [],
            "name": "Daniel C. Cole"
        },
        {
            "affiliations": [],
            "name": "Scott Glancy"
        },
        {
            "affiliations": [],
            "name": "Emanuel Knill"
        }
    ],
    "id": "SP:5795eb5f3b5471fa29fcd0122d86e1214cdbb52a",
    "references": [
        {
            "authors": [
                "J.A. Tropp"
            ],
            "title": "User-Friendly Tail Bounds for Sums of Random Matrices",
            "venue": "Found. Comput. Math. 12,",
            "year": 2012
        },
        {
            "authors": [
                "C.E. Langer"
            ],
            "title": "High Fidelity Quantum Information Processing with Trapped Ions",
            "venue": "Ph.D. thesis, University of Colorado at Boulder",
            "year": 2006
        },
        {
            "authors": [
                "Z.-H. Ding",
                "J.-M. Cui",
                "Y.-F. Huang",
                "C.-F. Li",
                "T. Tu",
                "G.-C. Guo"
            ],
            "title": "Fast High-Fidelity Readout of a Single Trapped-Ion Qubit via Machine-Learning Methods",
            "venue": "Phys. Rev. Appl",
            "year": 2019
        },
        {
            "authors": [
                "A. Seif",
                "K.A. Landsman",
                "N.M. Linke",
                "C. Figgatt",
                "C. Monroe",
                "M. Hafezi"
            ],
            "title": "Machine Learning Assisted Readout of Trapped-Ion Qubits",
            "venue": "Journal of Physics B: Atomic, Molecular and Optical Physics",
            "year": 2018
        },
        {
            "authors": [
                "E. Magesan",
                "J.M. Gambetta",
                "A.D. C\u00f3rcoles",
                "J.M. Chow"
            ],
            "title": "Machine Learning for Discriminating Quantum Measurement Trajectories and Improving Readout",
            "venue": "Physical Review Letters 114,",
            "year": 2015
        },
        {
            "authors": [
                "L. Rabiner"
            ],
            "title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition",
            "venue": "Proc. IEEE 77,",
            "year": 1989
        },
        {
            "authors": [
                "S.S. Elder",
                "C.S. Wang",
                "P. Reinhold",
                "C.T. Hann",
                "K.S. Chou",
                "B.J. Lester",
                "S. Rosenblum",
                "L. Frunzio",
                "L. Jiang",
                "R.J. Schoelkopf"
            ],
            "title": "High-Fidelity Measurement of Qubits",
            "venue": "Encoded in Multilevel Superconducting Circuits, Phys. Rev. X 10,",
            "year": 2020
        },
        {
            "authors": [
                "L.A. Martinez",
                "Y.J. Rosen",
                "J.L. DuBois"
            ],
            "title": "Improving Qubit Readout with Hidden Markov Models",
            "venue": "Phys. Rev. A: At. Mol. Opt. Phys. 102,",
            "year": 2020
        },
        {
            "authors": [
                "C.T. Hann",
                "S.S. Elder",
                "C.S. Wang",
                "K. Chou",
                "R.J. Schoelkopf",
                "L. Jiang"
            ],
            "title": "Robust Readout of Bosonic Qubits in the Dispersive Coupling Regime",
            "venue": "Phys. Rev. A: At. Mol. Opt. Phys",
            "year": 2018
        },
        {
            "authors": [
                "B. Hemmerling",
                "F. Gebert",
                "Y. Wan",
                "P.O. Schmidt"
            ],
            "title": "A Novel, Robust Quantum Detection Scheme",
            "venue": "New J. Phys",
            "year": 2012
        },
        {
            "authors": [
                "S. Geller",
                "S. Glancy",
                "E. Knill"
            ],
            "title": "Permuted Hidden Markov Models for State Inference, https://github.com/usnistgov/perm_hmm (2021)",
            "year": 2021
        },
        {
            "authors": [
                "K.J. \u00c5str\u00f6m"
            ],
            "title": "Optimal Control of Markov Processes with Incomplete State Information I",
            "venue": "J. Math. Anal. Appl. 10,",
            "year": 1965
        },
        {
            "authors": [
                "H. Kurniawati",
                "D. Hsu",
                "W.S. Lee"
            ],
            "title": "SARSOP: Efficient Point-Based Pomdp Planning by Approximating Optimally Reachable Belief Spaces",
            "venue": "in Robotics: Science and Systems,",
            "year": 2008
        },
        {
            "authors": [
                "A.-G. Paschke"
            ],
            "title": "9Be Ion Qubit Control Using an Optical Frequency Comb",
            "venue": "Ph.D. thesis, Hannover: Gottfried Wilhelm Leibniz Universita\u0308t Hannover",
            "year": 2017
        },
        {
            "authors": [
                "J. Medford",
                "J. Beil",
                "J. Taylor",
                "E. Rashba",
                "H. Lu",
                "A. Gossard",
                "C.M. Marcus"
            ],
            "title": "Quantum- Dot-Based Resonant Exchange Qubit",
            "venue": "Phys. Rev. Lett. 111,",
            "year": 2013
        },
        {
            "authors": [
                "H. Chernoff"
            ],
            "title": "Sequential Design of Experiments",
            "venue": "Ann. Math. Statist. 30,",
            "year": 1959
        },
        {
            "authors": [
                "Y. Chen",
                "S.H. Hassani",
                "A. Karbasi",
                "A. Krause"
            ],
            "title": "Sequential Information Maximization: When is Greedy Near-Optimal",
            "venue": "in Conference on Learning Theory (PMLR,",
            "year": 2015
        },
        {
            "authors": [
                "D. Kartik",
                "A. Nayyar",
                "U. Mitra"
            ],
            "title": "Active Hypothesis Testing: Beyond Chernoff-Stein",
            "venue": "IEEE International Symposium on Information Theory (ISIT) (IEEE,",
            "year": 2019
        },
        {
            "authors": [
                "M.M. Wilde"
            ],
            "title": "From Classical to Quantum Shannon Theory (Cambridge",
            "year": 2011
        },
        {
            "authors": [
                "J.L.A. Combes"
            ],
            "title": "Rapid Measurement and Purification Using Quantum Feedback Control, Ph.D",
            "year": 2010
        },
        {
            "authors": [
                "J. Combes",
                "H.M. Wiseman",
                "K. Jacobs"
            ],
            "title": "Rapid Measurement of Quantum Systems",
            "venue": "Using Feedback Control, Phys. Rev. Lett. 100,",
            "year": 2008
        },
        {
            "authors": [
                "D.E. Kirk"
            ],
            "title": "Optimal Control Theory: An Introduction",
            "venue": "(Courier Corporation,",
            "year": 2004
        },
        {
            "authors": [
                "B. Alt",
                "M. Schultheis"
            ],
            "title": "Koeppl, POMDPs in Continuous Time and Discrete Spaces",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "S.L. Todaro",
                "V. Verma",
                "K.C. McCormick",
                "D. Allcock",
                "R. Mirin",
                "D.J. Wineland",
                "S.W. Nam",
                "A.C. Wilson",
                "D. Leibfried"
            ],
            "title": "Slichter, State Readout of a Trapped Ion Qubit using a Trap-Integrated Superconducting Photon Detector",
            "venue": "Phys. Rev. Lett. 126,",
            "year": 2021
        },
        {
            "authors": [
                "A.C. Keith",
                "C.H. Baldwin",
                "S. Glancy",
                "E. Knill"
            ],
            "title": "Joint Quantum-State and Measurement Tomography with Incomplete Measurements",
            "venue": "Physical Review A",
            "year": 2018
        },
        {
            "authors": [
                "K.P. Murphy"
            ],
            "title": "Machine Learning: a Probabilistic Perspective",
            "venue": "(MIT press,",
            "year": 2012
        },
        {
            "authors": [
                "J. Shao"
            ],
            "title": "Mathematical Statistics (Springer",
            "venue": "New York,",
            "year": 2003
        },
        {
            "authors": [
                "R. Bellman"
            ],
            "title": "Dynamic Programming, Rand Corporation Research Study (Princeton",
            "year": 1957
        },
        {
            "authors": [
                "A.R. Cassandra",
                "L.P. Kaelbling",
                "J.A. Kurien"
            ],
            "title": "Acting Under Uncertainty: Discrete Bayesian Models for Mobile-Robot Navigation",
            "venue": "Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS\u201996,",
            "year": 1996
        }
    ],
    "sections": [
        {
            "text": "Improving quantum state detection with adaptive sequential observations\nShawn Geller,1, 2 Daniel C. Cole,1, \u2217 Scott Glancy,1 and Emanuel Knill1, 3\n1National Institute of Standards and Technology, Boulder, Colorado 80305, USA 2Department of Physics, University of Colorado, Boulder, Colorado 80309, USA\n3Center for Theory of Quantum Matter,\nUniversity of Colorado, Boulder, Colorado 80309, USA\nFor many quantum systems intended for information processing, one detects the\nlogical state of a qubit by integrating a continuously observed quantity over time.\nFor example, ion and atom qubits are typically measured by driving a cycling tran-\nsition and counting the number of photons observed from the resulting fluorescence.\nInstead of recording only the total observed count in a fixed time interval, one can\nobserve the photon arrival times and get a state detection advantage by using the\ntemporal structure in a model such as a Hidden Markov Model. We study what\nfurther advantage may be achieved by applying pulses to adaptively transform the\nstate during the observation. We give a three-state example where adaptively cho-\nsen transformations yield a clear advantage, and we compare performances on an\nion example, where we see improvements in some regimes. We provide a software\npackage that can be used for exploration of temporally resolved strategies with and\nwithout adaptively chosen transformations."
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Quantum information processing requires high fidelity single-shot readout of states. In a typical example, readout of the state of an ion or atom qubit is performed by observing the fluorescence from driving a cycling transition. For a qubit whose states are superpositions of two atomic levels, a goal is to distinguish between the two levels with the highest possible fidelity. A common way to define the fidelity is as the average probability of correctly determining the level when the levels are prepared uniformly randomly. High fidelity readout is helpful in quantum error-correction for syndrome measurements to minimize the probability of misidentifying errors. High fidelity readout can also significantly reduce the number of measurements needed to characterize states or processes in quantum tomography. The improvement can be substantial when the states measured have high fidelity with respect to a target state such as a Bell state [1].\nA standard readout method for \u2018bright\u2019 and \u2018dark\u2019 atomic levels (labeled |b\u3009 and |d\u3009 respectively) is to drive a cycling transition such that |b\u3009 fluoresces while |d\u3009 does not. The emitted photons are detected with some efficiency, and the output of the measurement is the number of photons detected. The presence of background photons, unwanted transitions between states and the desire to have short observation times prevent arbitrarily high fidelity measurement. The atom is inferred to be in the state |b\u3009 if the number of photons detected exceeds a threshold, otherwise it is inferred to be in the state |d\u3009. One strategy to improve the\n\u2217 Current address: ColdQuanta, Inc., Boulder, Colorado 80301, USA\nar X\niv :2\n20 4.\n00 71\n0v 2\n[ qu\nan t-\nph ]\n7 A\npr 2\n02 2\n2 readout fidelity is to use repetitive readout with ancillary atoms as demonstrated in Ref. [2]. A less demanding strategy is to record and use the arrival time of photons as discussed in Ref. [3]. The arrival-time record can also be used with machine learning strategies to improve readout, as demonstrated in Ref. [4\u20138]. All these methods take advantage of the fact that measurements are processes and can yield a time-resolved record that contains valuable information about the initial logical state. While we focus on atoms and fluorescence measurement, this situation is also common for other systems being investigated for quantum information processing, such as superconducting qubits [9].\nThe measurement processes of interest are well described by hidden Markov models (HMMs)[10]. An HMM is a stochastic discrete-time process on a finite number of \u201chidden\u201d states with stochastic output. It is characterized by a finite state space S, an output space Y , a stochastic transition matrix A that describes the probabilities of transitioning from the current state to the next in a time step, an output process matrix B that determines the probability of an output given the current state, and an initial state distribution \u03bd that determines the probability of the HMM starting in a particular state. For applications to quantum measurement, the goal is to infer the starting state of the HMM from the sequence of outputs observed during the process. It is usually possible to infer the parameters of the HMM by fitting to experimental data using standard algorithms [11], and one can assist such an algorithm by initializing it with parameters that are motivated by theory. Once the parameters are known, statistical decision methods for HMMs can be applied to estimate the initial state, and this has been demonstrated in several recent works [12\u201315].\nBut it is possible to do even better if the systems of interest permit sufficient control to modify the steps during the measurement. For atoms, the simplest such modification is to apply pulses between otherwise identical steps to permute the levels. This can improve the measurement fidelity or shorten average measurement times by taking advantage of the fact that not all levels are equally distinguishable by their output fluorescence. For example, two non-fluorescing levels are difficult to distinguish, but if the output observed so far suggests that the atom was prepared in a non-fluorescing level, we can swap one of these levels with a fluorescing one to learn which one was present initially, provided the measurement process has not yet induced transitions between levels. While in principle swaps can be applied in any system that permits such control, we expect them to be most useful for improving detection fidelity in systems where the time it takes to apply a swap is small compared to the lifetime of the system. An example of such a system, which we discuss in this paper, is a 9Be+ hyperfine qubit with microwave controls.\nAn improvement in measurement fidelity by applying a fixed swap was demonstrated in [16, 17], where the authors use a single \u03c0 pulse to implement a level swap in the middle of the readout cycle and postselect on the event that the two halves of the readout cycle give different outputs. Strategies that use postselection can provide conditional advantages in measurement fidelity over those that do not, but we do not consider postselected measurement fidelities here. In this work, rather than using only a fixed swap, we formalize and explore measurement processes with state permutations adaptively chosen during a single readout cycle based on the outputs observed so far in that cycle.\nTo support exploration of measurement processes with adaptively chosen state permutations, we developed a software package to simulate and optimize policies for choosing permutations [18]. We consider two examples to illustrate and compare measurement strategies. The first is a 9Be+ ion, and the second is an idealized three-state example. For these two examples, we computed the optimal policy for small numbers of steps, and for the 9Be+\n3 example, we compared the performance of a heuristic policy that is easier to compute than the optimal one. Improvements for the 9Be+ example are moderate, but they illustrate what can be achieved with adaptive control policies in an experimentally relevant system. Improvements for the three-state example are more significant, showing that adaptive policies can be very useful in some cases.\nThe heuristic policy is suboptimal for a range of parameters, demonstrating that heuristic approaches can yield undesirable results. It is possible to formulate the problem in terms of partially observed Markov decision processes (POMDPs) [19], which generalize HMMs by including output-dependent actions. In general, such problems are hard [20], but approximate solutions exist [21]. Because the problems considered here are small, we did not need to apply approximate solutions, but applications of the proposed techniques to other systems with larger numbers of steps or larger output spaces will require approximation.\nIn Sect. 2, we describe the problem of initial state inference under repeated measurements, subject to adaptive control of the underlying states. We discuss requirements for an experimental implementation of the proposed protocol in Sect. 3. The two examples are introduced and analyzed in Sect. 4 and Sect. 5. We discuss the results and suggest extensions of this work in Sect. 6, and we offer concluding comments in Sect. 7. Mathematical formalisms, and technical descriptions of policies and their computations are in the appendices."
        },
        {
            "heading": "2. STATEMENT OF PROBLEM",
            "text": "Consider a generic atom being measured by observing fluorescence from driving a cycling transition. We can divide the measurement into equal-time intervals referred to as \u2018steps\u2019. For each step, we record the number of photons detected. Many measurement configurations have the property that the system observed can be treated as decohered in the measurement basis before each measurement step. For atoms, the measurement basis is determined by the atomic levels, and the effective decoherence is a consequence of the dynamics of the levels and the nature of the driving light. Here we assume that the levels are non-degenerate, and each level can be treated as a single quantum state.\nWhen decoherence between the levels does not arise naturally, in many cases it is possible to enforce this decoherence and eliminate memory of coherences between levels by active means such as random phase changes or appropriately randomized pulses between steps. Given the lack of phase memory, the action of a step has a classical description. In particular, the initial quantum state is a probability distribution over the levels S that constitute the measurement basis elements. The output and next level have a joint probability distribution that depends on the current level.\nUnder these conditions the problem is reduced to a classical one, and we describe the measurement dynamics with a Hidden Markov Model (HMM), which is a discrete-time stochastic process. From here on \u201cstate\u201d refers to a classical deterministic state, and we use probability distributions over states and outputs to describe processes. We describe the mathematics of HMMs in App. B, and an exposition of how to describe measurement of a 9Be+ hyperfine qubit in terms of an HMM is given in App. C.\nThe description of a sequence of n measurement steps requires the sequence of states sn = (s1, . . . , sn) and the sequence of outputs or observations y\nn = (y1, . . . , yn). The corresponding random variables (RVs) are denoted by Sn and Y n. We use the usual convention that RVs are denoted by capital letters and their values by the corresponding lower-case letters. For\n4 any sequence xn, we use xj to denote the initial subsequence that has j elements, x1, . . . , xj. After observing the outputs yn, we make an inference of the state occupied at the beginning of the first step, also called the initial state. The random variable for the inferred initial state is denoted S\u03021. Using the HMM, we apply the maximum likelihood method to infer the initial state. To characterize the performance of our inference method, we use the infidelity P(S1 6= S\u03021). The symbol P denotes \u201cprobability of\u201d and its argument is an event. To compute the infidelity, we use the fact that the infidelity is the expectation value of the indicator function of the event that the inference is incorrect, averaged over the prior P(s1) and over the model P(yn|s1) of the event that the inference is incorrect. That is,\nP(S1 6= S\u03021) = \u2211 yn,s1 I[s1 6= s\u03021(yn)]P(yn|s1)P(s1), (2.1)\nwhere the indicator function I[E] \u2208 {0, 1} is 1 if and only if E is true. As discussed in App. D, maximum likelihood inference minimizes the infidelity for the uniform prior. Eq. 2.1 can be computed exactly using the HMM if the number of steps is not too large.\nIt is possible to modify the measurement sequence by using the data observed thus far to act on the system using controls of the underlying states. We specialize to a set A of permutations of underlying states, usually implemented with \u03c0-pulses between the states. As shown in Fig. 1, at each step, we apply a permutation of the underlying states (this can be the identity permutation) that depends on the data seen thus far. A rule that determines which permutation to apply at each step is called a policy. We would like to minimize the infidelity Eq. 2.1 over the space of all possible policies. In general, computing the optimal policy requires an exponential time calculation. This motivates the search for policies that are easier to compute, but that are possibly suboptimal. One possible policy is to choose the permutation that maximizes the mutual information between the initial state and the\n5 next g outputs. We call this the minimum posterior entropy heuristic. The mathematical descriptions of the HMM with adaptive permutations, the optimal permutation policy, and the minimum posterior entropy heuristic policy are given in App. E.\nIn Sect. 4 we compare the performance of the optimal policy and the minimum posterior entropy heuristic policy for a certain set of permutations to a policy that applies no permutations in the context of a 9Be+ ion qubit under repeated fluorescence measurement. In Sect. 5, we compare the performance of the optimal policy to that of the trivial policy that does not apply permutations in an idealized three state example."
        },
        {
            "heading": "3. PROSPECTS FOR EXPERIMENTAL IMPLEMENTATION",
            "text": "In this work, we focus on the 9Be+ hyperfine qubit as a motivating example. The fluorescence detection process in 9Be+ can be paused; by turning off the detection laser, the qubit retains its state for a very long time. By turning off the laser, applying the permutation pulse, then turning the laser back on, we can justify the assumption that the permutations occur instantaneously. In other systems such as superconducting qubits, the instantaneous action assumption does not hold. To account for the finite duration of actions, one can use the formalism of partially observable Markov decision processes (POMDPs) [19] to account for the possibility that the transition probabilities and the output probabilities directly depend on the action. The connection between the problem considered in this work with POMDPs is discussed in App. E.\nThe policies presented in this work are computed in advance. The advantage of computing a policy in advance is that it can be turned into a look-up table, so that the logic involved can be implemented on a field-programmable gate array (FPGA). One downside is that if the number of steps is too large, the look-up table can grow to the point that it cannot be stored in memory. Another strategy is to compute a policy as the data comes in. Such strategies are not memory-limited, but have the downside that typically floating-point calculations are involved, making them difficult to implement and run them in real time on an FPGA.\n4. EXAMPLE: 9Be+ HYPERFINE QUBIT\nThe 9Be+ ground-state hyperfine qubit is measured by distinguishing a level that fluoresces from ones that do not. The level that fluoresces does so by participating in a cycling transition, driven by a detection laser. A cycling transition involves driving the internal state to one that is outside the ground-state manifold, which in turn immediately decays back to the state from which it came, emitting a fluorescence photon in the process. Ideally, the other states remain undisturbed in this process. In reality, imperfect polarization of the detection laser can drive transitions from the bright state to dark states, while off-resonant driving leads to transitions between other pairs of levels. Since we only consider the case of perfect polarization in this work, if the ion ever enters the bright state, it remains there for the rest of the measurement sequence.\nA level diagram of the 9Be+ system is in Fig. 2. The fluorescing, \u2018bright\u2019, level is (F,mF ) = (2, 2), and the level with the least probability of transitioning to (2, 2) is (1,\u22121), so we use these two levels as possible initial states, with the uniform prior P(s1 = (2, 2)) = P(s1 = (1,\u22121)) = 1/2. The rate equations for this system are derived in [22]. The HMM parameters were obtained by integrating the rate equations for a 9Be+ qubit tuned to the first-order\n6\nbright state. This is the permutation \u03c4 considered in the main text. Reversing the order gives its inverse \u03c4\u22121. When constructing policies, we allow the actions { \u03c4, \u03c4\u22121, } where is the identity\npermutation.\nmagnetic-field-insensitive point for the |F = 1,mF = 1\u3009 \u2194 |F = 2,mF = 0\u3009 transition [3, 23]. The rate equations are parameterized by the background detected photon rate \u03b3bg, the bright-state detected photon rate \u03b3c, the fractions of \u03c3\n\u2212 and \u03c0 polarizations of the detection laser, and the on-resonance saturation parameter for the cycling transition r that parameterizes the number of scattered photons per unit time. To simplify the equations we assume that the polarization of the detection laser is perfectly \u03c3+. We set the other parameters for the rate equation as follows: \u03b3c = 30 photons per 330 \u00b5s, \u03b3bg = 0.06\u03b3c, and r = 1/2.\nBy integrating the rate equations for a time per step \u2206t, we derive an HMM that describes the repeated measurement setup, as detailed in App. C. We display the resulting transition matrices in Fig. 3, for different values of \u2206t considered. We note that in Fig. 3 (b), we observe large probabilities of transitioning between states. This is to be expected, as the detection laser is assumed to have \u03c3+ polarization, which leads to off-resonant driving of population to states with larger mF .\nBecause the complexity of the calculations grows very rapidly with the number of possible outcomes, it is expedient to modify the output distributions to have fewer possible outputs. We reduce the possible outputs by identifying sets of numbers of photons collected in a step, and keeping only the information of which set the photon count lies in. A set of photon numbers is called a \u2018bin\u2019. To select a set of bins, we minimized the infidelity over possible choices of bins, at n = 6 steps, when applying no permutations, for nb = 4 bins. To reduce computation time, we restrict the search to bins containing consecutive numbers of photons. The optimization for selecting bins is presented formally in App. C. We show the resulting binned distributions in Fig. 4 for the largest \u2206t considered. We performed this optimization separately for each \u2206t we considered. Once we find the optimal binning, we use the same binning for all the different policies considered.\nThe set of actions A used consists of the permutations \u03c4 , which sends the dark level to\n7\nthe bright level, its inverse \u03c4\u22121, and the identity permutation. The permutation \u03c4 is a composition of three pairwise swaps as shown in Fig. 2. The pairwise swaps that are used in creating \u03c4 are a subset of the transitions allowed by selection rules that are experimentally convenient to implement. While in principle we could add more actions to the set, we found that adding more actions does not appreciably improve measurement fidelity. We assume that the error introduced by applying actions is negligible. In the 9Be+ system, the state of the system is very well preserved when the detection laser is turned off. By assuming that the detection laser is turned off when the permutations are applied, we can treat the actions as effectively happening instantaneously.\nWe wish to analyze the effectiveness of using permutation actions in improving measurement fidelity. For n = 6 steps, we are able to compute the optimal policy (see App. E) using nb = 4 bins. However, the computational work required in computing the optimal policy grows as O((|A||Y|)n) so this computation becomes intractable as the number of bins |Y| or the number of steps n grows. We are thus also interested in the performance of heuristic policies that are more easily computed, especially in the regime where they can be compared to the optimal policy. An heuristic policy we choose for this comparison is the minimum posterior entropy policy that minimizes the average entropy of the initial state conditional on the next g observations, as detailed in App. E. Here we pick g = 2. To understand the effectiveness of using active permutation policies, we would like to compare these policies to the policy of applying no actions at all. To demonstrate the gain in fidelity achieved by using timing information, we also compare to a discrimination method that uses only aggregate probability of collecting numbers of photons over a total detection period, a method we refer to as the histogram method. We do not bin the distributions of total photons when computing infidelities for the histogram method.\n8\nTo summarize, the four methods we compare are as follows.\nName Permutation policy Model\nHistogram None applied Total counts\nNo Perms None applied HMM\nMin Entropy Minimize expected entropy of next two observations HMM\nExhaustive Minimize infidelity HMM\n(4.1)\nThe \u201cModel\u201d column indicates which model is used for the different methods. \u201cTotal counts\u201d means that the model ignores timing information, instead using just the total number of photons observed. \u201cHMM\u201d means that the model used is an HMM with a binned output distribution. We infer the initial state using maximum likelihood in every method, which incorporates the corresponding model and applied permutations.\nWe have plotted in Fig. 5 the measurement infidelity P(S\u03021 6= S1) of various methods of estimating the initial state, as a function of n\u2206t. For very small total times, transitions are rare overall, and therefore there is nothing to be gained by performing the time-resolved readout as we do in this work; it suffices to instead use the histogram method that performs\n9\nstate classification based solely on the aggregate number of photons. As shorthand, we refer to the various curves in Fig. 5 by their legends.\nNote that the regime of small times leads to large measurement error for any strategy chosen. At around 20\u00b5s, the Min Entropy and Exhaustive curves diverge from the other two. Here there is a gain due to applying the permutations, but the histogram method still coincides with the HMM method that does not apply permutations. Near 70\u00b5s, the Min Entropy curve diverges from the Exhaustive curve, indicating that our heuristic minimum posterior entropy policy underperforms the optimal one. At about 100\u00b5s, the minimum posterior entropy heuristic underperforms even the histogram method. At around 145\u00b5s, the Min Entropy curve rejoins the No Perms curve. Near 155\u00b5s, the No Perms and Histogram curves slightly diverge, indicating that our method of binning the outcomes into 4 bins causes us to incur a loss of distinguishability. Close to 200\u00b5s, the Min Entropy and No Perms curves\n10\ndiverge from the Histogram curve, indicating that it is worth using a time-resolved method for inference in this regime. Finally, beyond this point, the non-histogram methods begin to coincide. This indicates that there is nothing further to be gained from active policies for large \u2206t.\nWe see that the Min-entropy heuristic policy performs unpredictably in the different regimes. This is due to the fact that it does not incorporate information from all possible future paths. If the policy diverges from the optimal policy at any step, the resulting states that are realized are different, and the likelihoods of different possible yn change dramatically. The best method to check for the performance of a heuristic policy is to directly calculate the infidelity, as we do in this paper."
        },
        {
            "heading": "5. EXAMPLE: THREE-STATE MODEL",
            "text": "As a toy example, we consider a simple three-state model that illuminates the advantages that can be gained with permutation policies. The model is shown in Fig. 6. The model has three states S = {0, 1, 2} and three possible outputs Y = {0, 1, 2}.\nTo keep the number of HMM parameters low, it is symmetric under interchange of the labels 0 and 2 of both states and outputs. The transition rates and probability of outcomes shown in Fig. 6 depend on two parameters, a and b, which are intended to be small compared to 1. In this case, the state s = 1 has a short lifetime in terms of the number of steps and quickly transitions to s = 0 or s = 2 with equal probability, at a much larger rate than 0 and 2 transitioning to 1. States 0 and 2 do not transition directly between each other and have small probabilities of transitioning back to s = 1 compared to the rates out of s = 1. The output y = 0 is only possible for states s = 0 and s = 1, while the output y = 2 is only possible for s = 1 and s = 2. The output y = 1 is equally likely for all states, and has low probability. Thus the output y = 0 excludes s = 2 and y = 2 excludes s = 0, while y = 1 has no information about which state produced it. We assume that the initial state distribution is uniform on the three states and aim to infer the initial state from the outputs produced after up to six steps.\nWe computed the measurement fidelities both for the policy without permutations and the policy with the permutations selected according to the optimal policy at n = 6 steps for a range of parameters a and b. Here the permutations we allow are the transpositions between any two states. We choose to stop after n = 6 steps because we found that the measurement infidelity changes very little after reaching 6 steps, for the range of parameters shown in Fig. 7.\nThe gain in distinguishability is shown in Fig. 7 (b), which shows the ratio of the measurement infidelities without and with the permutation policy. We see that for small a and small b this ratio is large, demonstrating the relative improvement achieved. We can reason through this gain in distinguishability as follows. Consider the case where the first observation is y1 = 0. This excludes the possibility that s1 = 2, but the posterior distribution has support on both s0 (P(S1 = 0|y1 = 0) = 2/3) and s1 (P(S1 = 1|y1 = 0) = 1/3). If s1 = 1 and no action is taken, the state is likely to transition to s2 = 0 or s2 = 2, with equal probability. The transition to s2 = 0 results in loss of memory that s1 = 1. This is prevented by swapping the states 1 and 2. In this case the likely next output is y2 = 2 which excludes the possibility that s1 = 0. On the other hand, if s1 = 0, then the next outcome is likely again y2 = 0, and we can exclude the possibility that s1 = 1. A similar improvement in distinguishing the initial states is obtained when y1 = 2, in which case it helps to swap the\n11\nstates 0 and 1. Thus, an adaptive choice of permutation based on the first output improves the measurement fidelity for this model. Furthermore, because the swap required to achieve the improvement depends on the first outcome, any outcome-independent choice of action performs less well for two steps.\nThe pattern leading to the adaptive improvement of the three state model generalizes. Suppose that the outputs so far significantly narrow the likely initial states to a subset of states, with memory of the initial state still present but the states within the subset not easily distinguished by future outputs. Whenever this is the case, one can gain an advantage by moving some of the states of this subset to another set of more distinguishable states. The three-state model provides a situation where an adaptive permutation policy is strictly better than every non-adaptive such policy."
        },
        {
            "heading": "6. DISCUSSION",
            "text": "We have investigated policies for applying permutations between steps of a repetitive measurement. In our treatment of the measurement process, we have assumed that the HMM parameters are known. In atomic systems, it is often possible to determine these parameters from the physical constants associated with the atomic levels, along with measured parameters such as Rabi rates. In quantum systems with superconducting qubits [24] or electrically-defined quantum dots [25], the physics is less constrained, and it may be difficult to infer the HMM from the measured or calibrated parameters. Instead, it is possible to learn the HMM by observing the measurement process for many steps. Many tools are available for inferring the transition and output matrices from such observations [10]. We recommend investigating use of these tools, while taking advantage of known physical constraints and measured or calibrated parameters, with the aim of improving the modeling of quantum measurement processes. As mentioned in Sec. 3, when implementing permutations in systems with shorter lifetimes, the transition matrices and output matrices become\n12\ndirectly dependent on which permutation is applied. Tools used for inference of HMMs can be adapted to the problem of inference of these action dependent models.\nIn specifying and applying permutation policies for improving measurement fidelity, we have assumed that the permutations are applied perfectly. As noted in [16, 17], this assumption may not be realistic. The model can be readily adapted to take account of errors in applying the permutations. Changing the transition matrix associated with the chosen permutation to a general Markov process poses no difficulty and makes it possible to account for known errors or noise in applying the chosen permutation.\nIn the absence of transitions, and where any permutation is an allowed action, optimizing the measurement infidelity is an instance of an active sequential hypothesis testing problem [26]. Computing the optimal policy for this type of problem is substantially simpler. The complexity is dominated by the number of possible posterior distributions on the input states given the outputs. This number grows as a polynomial with the number of steps, although its degree may be high depending on the number of allowed actions. For these problems, there are guarantees that the greedy policy used in this work has close to optimal mutual information between the initial state and the data [27]. One may also be interested in minimizing the number of measurement rounds, instead of fixing it as we have done in this work. One way to study this problem is to introduce a \u201cdiscount factor\u201d that exponentially suppresses rewards that are reached after many measurement rounds. One can then obtain\n13\nupper and lower bounds on the minimal discounted measurement infidelity [28]. A potential line of future work is to extend these results to the case of non-trivial transition matrices.\nIn general, a quantum measurement process can be described by a quantum instrument. A quantum instrument has classical outputs and output-conditional side-effects on the quantum system being observed. One way to specify the instrument is as the composition of a minimally disturbing general quantum measurement, given as a positive operator-valued measure (POVM), followed by output-conditional quantum operations. See Ref. [29] for technical details. Because the processes of interest in this work were totally dephasing, it was sufficient to use a classical description, but it is also of interest [30] to study optimal policies for measurement processes whose instruments are not totally dephasing.\nIn many situations, including those involving 9Be+ qubits, the physically implemented measurements involve continuous monitoring, and events such as detection times of photons are recorded. The formalism used in our work requires discretizing time to allow us to model measurement of 9Be+ as a discrete time Markov process. To take full advantage of the measurement process requires modeling by a continuous-time process[31],[32]. This problem can be formalized with the Hamilton-Jacobi-Bellman equation [33],[34].\nThe 9Be+ example has the property that the graph of allowable transitions is directed and acyclic. An HMM with this property is called a left-right HMM, and simpler algorithms exist for computing likelihoods in this degenerate case [10]. While we did not take advantage of this structure in this work, it is likely that using this structure would lead to simpler algorithms for computing the optimal policy.\nIn this study, we focused on optimizing measurement fidelity given the number of steps of the HMM measurement model that are observed. The HMMs relevant for atomic measurement eventually lose memory of the initial state, so observing for more steps yields rapidly diminishing fidelity gains. In many applications, it is desirable to minimize the average time required to complete a measurement, which implies a trade-off between measurement time and measurement fidelity. Examples where measurement time matters are for applications involving feed-forward such as quantum error correction, and in characterization experiments dominated by measurement time. The techniques discussed here can be used to explore the measurement-fidelity measurement-time trade-off. However, there is a way in which one can reduce average measurement time, possibly without losing measurement fidelity. In particular, it is possible to terminate the measurement early if the outputs so far indicate a particular initial state sufficiently strongly. Such a scheme was introduced in Ref. [35] and related approaches are in current use [36]. HMMs can be used to improve these schemes\u2019 time and fidelity performance beyond what can be achieved using likelihood ratio tests computed from models with independent and identical outputs. One can incorporate the cost of an additional measurement explicitly in the cost function to minimize, and again consider the advantage that could be gained by implementing an adaptive strategy. In the case of a transition matrix equal to the identity, this problem has been studied in [37]. There, the authors take as their cost function the expected number of measurements plus the expected measurement infidelity, with a variable weighting between the two terms. Upper and lower bounds on the optimal cost are then obtained. It would be interesting to extend these results to the scenarios with general transition matrices considered here.\n14"
        },
        {
            "heading": "7. CONCLUSION",
            "text": "We have investigated the use of adaptively chosen actions to improve measurement fidelity in quantum measurements that are realized as sequential observations with complete decoherence. We considered two examples, one motivated by 9Be+ ion qubits, the other a three-state toy example. We focused on actions consisting of permuting the states of the system.\nOur study of 9Be+ measurements indicates parameter regimes where an improvement is achieved and suggests future work to take advantage of adaptive permutation policies. We discussed a number of paths forward and open problems, such as that of optimizing the trade-off between measurement times and measurement fidelity, finding better policies, and extensions to continuous-time measurement processes."
        },
        {
            "heading": "Appendix A: Outline of Appendix",
            "text": "In the appendices below, we explain the mathematical formalism involved in the calculations used in the main text. In App. B we introduce repetitive quantum measurements. We discuss why, in many experiments, it is possible to describe such measurements with an effective classical model, namely an HMM. In App. C we discuss a few subtleties involved in discretizing continuous-time dynamics so that it can be modeled as an HMM, illustrating our solution in the context of the 9Be+ example. In Sect. D we describe the problem of inferring the initial state from a sequence of measurements and its solution by the maximum likelihood estimate. The method of using adaptively chosen actions during repetitive measurements is defined in App. E. Therein, we discuss how to compute the optimal policy using the Bellman Equation, and present our heuristic policy that chooses actions based on minimizing the entropy of the initial state. We also sketch how to reduce the problem of computing an optimal policy to a POMDP in App. E. Finally, our implementation and simulation is outlined in App. F."
        },
        {
            "heading": "Appendix B: Repetitive measurement models",
            "text": "Here we discuss the formalism of Hidden Markov Models (HMMs), and show how they describe repeated measurements of quantum systems. As discussed in the main text, for a quantum system under interrogation by a measurement process, such as an atom under fluorescence detection, the state of the system can be effectively dephased, and thus has a classical description. The initial quantum state is a probability distribution over the levels S (assumed to be nondegenerate) that constitute the measurement basis elements, and that the output has a probability distribution conditional on the current level and previous level, and the next level has a probability distribution conditional on the current level. This is immediately in the form of an HMM step, except that the current outcome may depend on the previous level as well as the current level. The dependence on the previous level can be accounted for by expanding the HMM state space to include memory of the previous outcome, as explained below. For atoms with cycling transitions, the transitions between levels result from non-ideal cycling. Having reviewed the reduction from a quantum model to a classical stochastic one, we now use the term \u201cstate\u201d to refer to a classical level.\nWe use the convention that an upper case variable X refers to a random variable (RV),\n15\nwhile its corresponding lower case variable refers to a particular instantiation of the RV. We use P to denote \u201cprobability of\u201d, whose argument is an event, so that the expression P(X = x) refers to the probability that the RV X has a particular value x. Because we are interested in stochastic processes, we also use the notation for sequences of RVs as in the main text. A variable Si is the RV for the stochastic process S at the step i, while the variable Si indicates the subsequence of the first i steps of the process S, Si = S1, . . . , Si. We adopt similar conventions for particular sequences of values, so that si = s1, . . . , si.\nWe now introduce notation for HMMs. An HMM has state space S, output space Y , transition matrix A, and output matrix B. We use the notation A(s\u2032|s) for the transition probability to next state s\u2032 given current state s, and B(y|s) for the probability of output y given current state s. We denote the state and observation at step i as si and yi, respectively. Let \u03bd be the initial state distribution defined by \u03bd(s1) = P(s1), where the expression s1 abbreviates the event that S1 = s1. The probabilities of the state and output sequences are determined by unraveling the transitions according to\nP(sn) = A(sn|sn\u22121)P(sn\u22121) = \u03bd(s1) n\u220f i=2 A(si|si\u22121),\nP(yn|sn) = n\u220f i=1 B(yi|si). (B.1)\nAppendix C: Example: Fluorescence detection of 9Be+ hyperfine qubit\nTo describe the measurement dynamics of 9Be+ measurement as an HMM, we address the following issues. While dynamics of physical systems usually take place continuously in time, an HMM is a discrete-time model, and therefore the physical dynamics need to be discretized. Second, the distributions of outcomes can contain more information than is useful, which can complicate computations. We thus simplify the outcome distributions by a binning procedure. Third, as mentioned in the introduction, the probability of transitioning to a different state sometimes depends on the measurement outcome. This is accounted for by expanding the state space to include the current measurement outcome, allowing the transition matrix A to depend on both the physical state and the outcome.\nReduction from continuous to discrete time An HMM description of a fluorescence measurement can be derived from the continuous time Markov process modeling the stochastic dynamics of the levels and the detection of fluorescence from the cycling level while driving the cycling transition.\nTransitions between levels are described by the transition-rate matrix Q. The off-diagonal entries Qss\u2032 of Q are the non-negative transition rates, and the diagonal entries Qs\u2032s\u2032 = \u2212 \u2211\ns 6=s\u2032 Qss\u2032 are the total rates of departure from level s \u2032 to other levels. The transition-\nrate matrix can be integrated to obtain the probabilities R(s|s\u2032) of starting in level s\u2032 and ending in level s for a measurement step of period \u2206t. Then R(s|s\u2032) = (eQ\u2206t)s,s\u2032 . The photon emission rate for level s is Es \u2265 0. For simplicity, we make the approximation that at most one transition occurs in any given step. If the ion is in level si\u22121 at the beginning of step i, and in level si at the end of the step, and transitions at a particular time t, the distribution of collected photons is Poissonian with mean determined by Esi , Esi\u22121 , and t. The distribution J(o|s, s\u2032) of the number o of collected photons given the system starts in level s\u2032 and ends in level s is then the mixture of these Poisson distributions [3]. We can\n16\ncompute the probability of the system starting the step in the level s\u2032, observing o photons during the step and ending in level s as U(s, o|s\u2032) = J(o|s, s\u2032)R(s|s\u2032).\nThe output distributions are supported on all nonnegative integers, but collecting a large number of photons in a single step is very unlikely. We thus restrict the output space to be the set Y = {0, . . . , nmax \u2212 1, nmax}, where we obtain outcome nmax if at least nmax photons were collected during the step. We chose nmax = 15 so that the probability of collecting nmax or more photons from the bright state in a time step of duration \u2206t = 53.9\u00b5s is less than 10\u22123.\nOutcome dependence In the model described in the previous paragraph, the output depends on the previous and current state. In order to describe such models, we introduce notation for HMM state spaces that expand the physically relevant state space. Let S be the set of physical states. The state space T of the expanded HMM is related to the physical states by a map \u03b1 : T \u2192 S. In the 9Be+ example, we let S be the set of levels, S = {(F,mF )|F \u2208 {1, 2},mF \u2208 {\u2212F, . . . , F}}. We let T consist of pairs (s, o) of physical states s \u2208 S and outputs o \u2208 Y , and we define \u03b1((s, o)) = s. Here, s represents the current physical state and the o is the output observed during the step resulting in state s. For this example, it is also useful to define the map \u03c1 : T \u2192 Y , \u03c1((s, o)) = o. We then define the transition and output matrices as follows. The transition probability from previous state s\u2032, with previously recorded observation o\u2032, to the current state s with observation o recorded in transitioning to s is\nA(s, o|s\u2032, o\u2032) = U(s, o|s\u2032). (C.1)\nBecause the transition matrix now captures both the state transition and the probability of a state emitting an outcome, the outcome process matrix on the expanded state is deterministic, so that B(o|l) = \u03b4o,\u03c1(l) is the new outcome process matrix. Thus, the outcome matrix merely describes the fact that the physical state is unobservable. For the initial state distribution, we must pick a convention for the mapping from the physical initial state distribution to that for the HMM. Let S1 be the random variable describing the initial physical state. For the fluorescence example we take the distribution over initial HMM states to be \u03bd(s, o) = P(S1 = s)\u03b4o,0.\nBinning For our purposes, the outcome distributions contain more information than necessary, and the computations grow rapidly in complexity with the number of outcomes. Given an HMM M with an output space Y , we thus seek a related model that has fewer possible outputs. We accomplish this by partitioning the outcome space into bins.\nA partition \u00b5 = {bi} of Y with nb bins is a set of nb nonempty subsets of the set Y , such that the sets bi are pairwise disjoint, i 6= j =\u21d2 bi \u2229 bj = \u2205, and cover the set Y , \u222aibi = Y . An element bi \u2208 \u00b5 is called a bin. The set of bins \u00b5 becomes the output space of the binned model M\u00b5. By keeping only the information of which bin the outcome lies in, the resulting distribution is again an HMM.\nFor the 9Be+ example, the binned model has initial state distribution \u03bd\u00b5, transition matrix\n17\nA\u00b5, and output matrix B\u00b5, defined by \u03bd\u00b5(s, bi) = \u2211 y\u2208bi \u03bd(s, y) (C.2)\nA\u00b5(s, bi|s\u2032, bj) = \u2211 y\u2208bi U(s, y|s\u2032) (C.3)\nB\u00b5(bi|s) = \u2211 y\u2208bi B(y|s). (C.4)\nNote that the bins cannot depend on the state. To determine what bins to use, it is useful to have a quantitative cost function to apply to the bins. Specifically, if f is a function that takes an HMM M\u00b5 and gives a cost associated with that model, we wish to choose the binning \u00b5\u2217nb with nb bins that solves the minimization problem\nargmin \u00b5\nf(M\u00b5)\nsubject to \u00b5 ` Y |\u00b5| = nb.\nwhere the notation \u00b5 ` Y denotes that \u00b5 is a partition of Y . We use the infidelity (Eq. 2.1) of the model induced by the binning as our cost function f to determine the optimal binning when we compute policies in Sect. 4, but in general different heuristic costs can be used [38]."
        },
        {
            "heading": "Appendix D: Initial state inference",
            "text": "Given the measurement process described in App. B, our goal is to infer, after some number of observations, the initial physical state of the system. So consider now a generic HMM with state space T , physical state space S identified by \u03b1 : T \u2192 S, initial physical state space L, transition matrix A and output matrix B. We assume that the initial state is uniformly distributed over L. Let S\u03021 = \u03c6(Y n) be the random variable that is the output of the initial-state estimation procedure \u03c6 determined by the observed outputs Y n. Recall the definition of the measurement infidelity in Eq. 2.1, P(S1 6= S\u03021). We will sometimes also refer to the fidelity F = P(S1 = S\u03021) = 1\u2212 P(S1 6= S\u03021).\nThe maximal measurement fidelity is achieved by choosing this function to be the Bayesian, maximum a-posteriori (MAP) estimate [39]. The MAP estimate s\u03021 is the physical state with the highest posterior probability given the n observed outcomes:\ns\u03021 = argmax s1\u2208S\nP(s1|yn). (D.1)\nAccording to Bayes\u2019s rule, P(s1|yn) = P(yn|s1)P(s1)/P(yn). Because the denominator is independent of s1 and the prior distribution is uniform, The MAP estimate is the same as the maximum likelihood (ML) estimate [40]:\ns\u03021 = argmax s1\u2208S\nP(yn|s1). (D.2)\nThis estimate can be computed step-by-step by keeping track of the list of values ( P(yk, lk|s1) ) lk\u2208T ,s1\u2208L\n18\nfor k = 1 to k = n, where the list is updated by applying the recursive expression\nP(yk, lk|s1) = \u2211\nlk\u22121\u2208T\nP(yk\u22121, lk\u22121|s1)P(yk, lk|yk\u22121, lk\u22121, s1)\n= \u2211\nlk\u22121\u2208T\nP(yk\u22121, lk\u22121|s1)P(yk, lk|lk\u22121)\n= \u2211\nlk\u22121\u2208T\nP(yk\u22121, lk\u22121|s1)B(yk|lk)A(lk|lk\u22121), (D.3)\nwhich takes advantage of the HMM conditional independence properties. The values are initialized with\nP(y1, l1|s1) = B(y1|l1)I[s1 = \u03b1(l1)]\u03bd(l1) / \u2211 l\u20321:s1=\u03b1(l \u2032 1) \u03bd(l\u20321) . (D.4)\nFrom the final values P(yn, ln|s1), the MAP estimate is obtained according to\ns\u03021 = argmax s1\u2208L \u2211 ln P(yn, ln|s1). (D.5)\nBy our construction of the expanded HMM, this is equal to the expression in Eq. (D.2). The measurement fidelity F can be computed exactly if the number of possible outcome sequences yn is not too large, or by empirically sampling the HMM output sequences and computing the Bayesian posterior probabilities for each sample."
        },
        {
            "heading": "Appendix E: Adaptive measurement policies",
            "text": "Modification of HMM To accommodate actions that can be taken between steps of an HMM we introduce a set of possible actions A, where each action a in A is a statetransforming process. In general, a can be a stochastic process, where the probability that a results in state s\u2032 given that the current state is s is denoted by a(s\u2032|s). The process resulting from modifying the HMM by applying action ak after observing yk in the k\nth step can be thought of as an HMM with step-dependent transition probabilities, where the transition matrix A at step k is replaced by the composition of A with the action ak. The modified transition probabilities are then\nAk(sk+1|sk) = \u2211 s A(sk+1|s)ak(s|sk). (E.1)\nThe probability of a state sequence is accordingly given by\nP(sn) = \u03bd(s1) n\u220f i=2 Ai\u22121(si|si\u22121). (E.2)\n19\nFor computing the final estimate, it suffices to modify the expression for P(yk, sk+1|l1) in Eq. D.3 by replacing A(sk|sk\u22121) with Ak\u22121(sk|sk\u22121), which depends on the policy\u2019s choice of action at step k \u2212 1.\nThe action at step k is chosen based on the observations so far given by yk. A policy is a specific strategy for choosing the action. Policies are chosen to maximize a reward. If the reward can be expressed as a sum of rewards at each step, this fits the framework of partially observed Markov decision processes (POMDPs) [19]. For our application, policies are chosen to maximize the measurement fidelity, which is expressed in terms of a decision made after the last step. It is possible to change the model to an equivalent one fitting the POMDP formalism, as discussed below.\nWe consider actions that permute states, so A is a set of state permutations including the identity permutation. We denote such permutations by \u03c3 and write \u03c3(s) for the state resulting from applying \u03c3 to state s. The process is schematically shown in Fig. 1. Note that there are n\u2212 1 actions for a sequence of n steps.\nIn the case that the HMM state space T does not coincide with the physical state space S, it is necessary to relate physical actions to actions on the HMM state space. For the 9Be+ system, recall that the HMM states are given by pairs (s, o) of physical states s \u2208 S and outputs o \u2208 Y , where S is the set of levels, S = {(F,mF )|F \u2208 {1, 2},mF \u2208 {\u2212F, . . . , F}}, and Y is the set of photon counts, Y = {n|n = 0, 1, . . . , nmax}. Given a permutation \u03c3 that acts on the physical states S, we induce a permutation \u03c3\u0303 that acts on T , by taking \u03c3\u0303((l, o)) = (\u03c3(l), o). The permutations for the 9Be+ example are then {\u03c3\u0303|\u03c3 \u2208 {\u03c4, \u03c4\u22121, }}, where the permutation \u03c4 is defined in Fig. 2.\nBellman Equation Our task now is to find a policy, consisting of permutations to apply, that maximizes the measurement fidelity. For sake of clarity, we assume in this appendix that the HMM state space coincides with the physical state space, but the ideas discussed here can be extended when this is not the case, by maximizing the probability of inferring the initial physical state instead of the initial HMM state. In the following, we formally express the maximization problem for computing the optimal policy, then introduce belief states, and show their use in computing the optimal policy.\nTo maximize the measurement fidelity over the choices of \u03c3n\u22121, we use the law of total expectation to compute that\nmax \u03c3n\u22121\nP ( S\u03021 = S1|\u03c3n\u22121 ) =\nEY1 (\nmax \u03c31\nEY2|\u03c31 ( \u00b7 \u00b7 \u00b7max\n\u03c3n\u22122 EYn\u22121|\u03c3n\u22122 ( max \u03c3n\u22121 EYn|\u03c3n\u22121 ( P ( S\u03021 = S1|Y n\u03c3n\u22121 ))) \u00b7 \u00b7 \u00b7 ))\n(E.3)\nThis is called the Bellman Equation [41], [42]. One way to compute Eq. E.3 is to compute all possible posterior distributions of the initial state S1, then to compute the expectation of the reward P(S\u03021 = S1|Y n\u03c3n\u22121) for all possible sets of actions \u03c3n\u22121. The posterior distributions are computed iteratively. To describe the computation, we introduce the distributions\n\u03b2k(s1, sk|yk, \u03c3k\u22121) = P(s1, sk|yk, \u03c3k\u22121) (E.4)\nand\n\u03b7k(s1, sk+1|yk, \u03c3k) = P ( s1, sk+1 \u2223\u2223yk, \u03c3k). (E.5)\n20\nThe \u03b2k and \u03b7k are called belief states. The belief state \u03b2k describes the state of knowledge of the current and initial HMM states immediately after observing outcome yk, but before we apply the permutation \u03c3k. After applying \u03c3k and allowing the system to transition, the state of knowledge is described by \u03b7k. Because our goal is to infer the initial state of the system, note that it is necessary to keep track of the state of knowledge of the initial state. To predict how the dynamics will effect our estimate, it is also necessary to have an estimate of the current state. We therefore keep track of sk as well in the belief state.\nThe belief state is iteratively updated according to the observations yn and permutations \u03c3n\u22121. At step k, we use the observation yk to update the belief state \u03b7k\u22121 to \u03b2k according to Bayes\u2019 rule\n\u03b2k(s1, sk|yk, \u03c3k\u22121) = hBayes(\u03b7k\u22121, yk) = \u03b7k\u22121(s1, sk|yk\u22121, \u03c3k\u22121)B(yk|sk)\u2211 sk \u03b7k\u22121(s1, sk|yk\u22121, \u03c3k\u22121)B(yk|sk) . (E.6)\nNext, we use the permutation \u03c3k to update the belief state \u03b2k is updated to \u03b7k according the transition matrix of the HMM,\n\u03b7k(s1, sk+1|yk, \u03c3k) = hTrans(\u03b2k, \u03c3k) = \u2211 sk \u03b2k(s1, sk|yk, \u03c3k\u22121)A(sk+1|\u03c3k(sk)). (E.7)\nIn Eq. E.6, we used the fact that the current outcome depends only on the current state, and in Eq. E.7, we used the fact that the next state depends only on the current state, which are the two defining features of the HMM.\nThe initial belief state \u03b70 is determined by the prior distribution, \u03b70(s1, s \u2032 1|y0, \u03c30) =\n\u03bd(s1)\u03b4s1,s\u20321 . Here, the index s \u2032 1 is for the \u201ccurrent\u201d state after having seen no data (y 0) and having applied no permutations (\u03c30). This index is updated after each application of hTrans, updating the step index each time. In contrast, the other index s1 is the one that describes the initial state, and continues to describe the initial state even after the various updates using hTrans, hBayes.\nThe belief states are labelled by sequences of observations yn and permutations \u03c3n\u22121, and the set of all accessible belief states has the structure of a tree, see Fig. 8. The children of a node \u03b7k\u22121 of the tree are those \u03b2k that are obtained from a Bayes update as in Eq. E.6,\n21\nwhere the different children are labelled by the possible values of yk. Similarly, the children of a node \u03b2k are those \u03b7k that can be obtained from Eq. E.7, where the different children are labelled by the possible values of \u03c3k.\nOnce all the belief states have been computed, the exhaustive search algorithm suggested by Eq. E.3 can be directly implemented. To perform this computation using the tree, we start at the leaves by computing the probability of correct inference\nP(S1 = S\u03021|yn, \u03c3n\u22121) = max s1 \u2211 sn \u03b2n(s1, sn|yn\u03c3n\u22121). (E.8)\nNext, we compute the expectation EYn|\u03c3n\u22121 ( P(S1 = S\u03021|yn, \u03c3n\u22121) ) = \u2211 yn P(S1 = S\u03021|yn, \u03c3n\u22121) \u2211 sn B(yn|sn) \u2211 s1 \u03b7n\u22121(s1, sn|yn\u22121\u03c3n\u22121).\n(E.9)\nThis is the innermost expectation in Eq. E.3. We next compute the maximum over the permutation \u03c3n\u22121 directly. We have thus computed the (n\u2212 1)th step of the optimal policy. By then iterating expectation and maximization steps, we compute the optimal policy of Eq. E.3.\nMinimum posterior entropy heuristic In the examples in Sect. 5 and Sect. 4, we compute the optimal policy using exhaustive search for a small set of actions and a small set of outcomes, but as the numbers of actions and outcomes increase, the exhaustive search algorithm becomes extremely expensive to compute. This is because the work required scales as O((|A||Y|)n), determined by the size of the tree. For more than a few steps, computing the optimal policy is currently infeasible. This motivates the use of heuristic policies. We here discuss an heuristic policy that performs well in some regimes, as a possible alternative to the optimal policy when it is not available.\nOur heuristic policy is also a belief-state-based algorithm, but while exhaustive search constructs a tree of height 2n+ 1, we construct O(|Y|n) smaller trees of height 2g + 1 for a fixed g. To compute our heuristic, at each step we compute a measure of concentration of the posterior probability distribution of the initial physical state given the output of the next g steps, and choose the permutations that optimize the expectation of that measure. This strategy is in a sense greedy and its performance depends on the measure of concentration used. For this study, we used the posterior entropy as a measure of concentration, with less entropy indicating higher concentration. We call this the minimum posterior entropy heuristic. This is similar to the policy given in Ref. [43], except that there, the authors minimize posterior entropy of the HMM state at the next step, not that of the initial state. Note that measurement fidelity is determined by the maximum probability of the posterior. Maximum posterior probability is also a measure of concentration but is insensitive to probabilities other than the maximum one, which may result in blind spots for a greedy algorithm using maximum posterior probability. We distinguish here between the policy that involves maximizing the maximum probability of the posterior at n steps and that at g steps. The former is the optimal policy, since at n steps the maximum probability of the posterior is the measurement fidelity, while at g steps it is merely an (uncontrolled) approximation of the fidelity.\nAs in App. E, we assume in the following discussion that the HMM state space coincides with the physical state space, but again the calculations can be adapted to a case where\n22\nthey differ, by changing the cost function to be the posterior entropy of the physical initial state. For choosing the permutation to be applied at step k after having observed yk and having applied actions \u03c3k\u22121, we compute all possible belief states after g more steps, given by\nP ( s1, sk+g \u2223\u2223\u2223yk, \u03c3k\u22121, yk+gk+1 , \u03c3k+g\u22121k ), (E.10) for all possible choices of yk+gk+1 , \u03c3 k+g\u22121 k , where we use the abbreviation y k+g k+1 to mean the subsequence yk+1, . . . yk+g, and similarly for \u03c3 k+g\u22121 k . We can then compute the associated initial state distributions,\nP ( s1 \u2223\u2223\u2223yk, \u03c3k\u22121, yk+gk+1 , \u03c3k+g\u22121k ) = \u2211 sk+g P ( s1, sk+g \u2223\u2223\u2223yk, \u03c3k\u22121, yk+gk+1 , \u03c3k+g\u22121k ), (E.11) and the corresponding entropy\nH(S1|yk+g, \u03c3k+g\u22121) = \u2212 \u2211 s1 P ( s1 \u2223\u2223yk+g, \u03c3k+g\u22121) log(P(s1\u2223\u2223yk+g, \u03c3k+g\u22121)). (E.12)\nViewing this entropy as a cost function, we can then choose the permutations \u03c3k+g\u22121k that minimize this cost, and apply the permutation \u03c3k.\nTo compute the distributions in Eq. E.10, we take the tree-based approach described in Fig. 8. The root of the tree is the belief state \u03b2k(s1, sk|yk, \u03c3k\u22121). By iteratively applying Eqs. E.6,E.7 for the various choices of yk+gk+1 , \u03c3 k+g\u22121 k , we can compute the desired distributions in Eq. E.10. The cost function can be written\nmin \u03c3k+g\u22121k\nH ( S1 \u2223\u2223\u2223yk, \u03c3k\u22121, Y k+gk+1 \u03c3k+g\u22121k ) = min \u03c3k EYk+1|\u03c3k ( \u00b7 \u00b7 \u00b7 min \u03c3k+g\u22121 EYk+g |\u03c3k+g\u22121 ( H ( S1\n\u2223\u2223\u2223yk, \u03c3k\u22121, Y k+gk+1 \u03c3k+g\u22121k )) \u00b7 \u00b7 \u00b7), (E.13) so a method similar to that used to solve the Bellman Equation can be applied to obtain the solution to Eq. E.13.\nAfter applying \u03c3k and observing yk+1, we need to update the tree of belief states. Instead of recomputing the whole tree, we take a dynamic programming approach and leverage the fact that we have already computed some of the possible future belief states. Rather than recomputing the new root of the tree, we can simply use the already computed \u03b2k+1(s1, sk+1|yk+1, \u03c3k) as the new root of the tree. All but the last step of the possible future paths have also already been computed, so we can just apply Eqs. E.7,E.6 to the\ndistributions P ( s1, sk+g \u2223\u2223\u2223yk, \u03c3k\u22121, yk+gk+1 , \u03c3k+g\u22121k ) to obtain the new belief states P ( s1, sk+g+1\n\u2223\u2223\u2223yk+1, \u03c3k, yk+g+1k+2 , \u03c3k+gk+1). Reduction to POMDP Although we were able to compute the optimal policy for the examples considered in this paper, in general it requires a very large computation. For the general case, it is useful to apply the existing framework of partially observable Markov\n23\ndecision processes (POMDPs) [19], to leverage existing approximate algorithms (e.g. [21]). A POMDP is a generalization of an HMM that includes output-dependent actions, and includes a reward that is given at each step. The goal of policy planning in POMDPs is to maximize this reward.\nTo use the framework of POMDPs, it is necessary to express the reward, in our case the measurement fidelity, as a sum of rewards at each step. We now sketch how to adjust the model to accomplish this. We expand the state space to keep track of the initial state and the number of steps. The new state space is S \u00d7L\u00d7 {1, . . . , n}. The transition and output matrices are redefined accordingly. The action at each step is either a permutation or, for the last step, a decision action that is the estimate of the initial state. Possible decision actions are in one-to-one correspondence with L, so the set of possible actions is the union of the set of allowed permutations and L. The reward at each step is 0 if the step number encoded in the extended state is not n or the decision action does not correspond to the initial state. If the step number is n and the decision action agrees with the initial state, the reward is 1. With this definition, the expectation of the sum of the rewards at each step is the measurement fidelity."
        },
        {
            "heading": "Appendix F: Implementation and simulation",
            "text": "The code used in our simulations of HMMs with permutations is available on Github [18]. The code uses the pyro package [44] and the underlying PyTorch [45] package for computation of relevant probabilities.\nFor the examples given in the main text, we computed the measurement infidelity numerically without resorting to Monte Carlo techniques. In particular, for a given number n of steps, we considered all of the |Y|n possible output sequences. For each such sequence yn, we computed the probability that it occurs and the probability of incorrectly identifying the initial state conditional on yn. The measurement infidelity is obtained by summing the product of these two probabilities over Yn. The number of possible data sequences is exponential in n, which limits the number of steps for which it is possible to avoid Monte Carlo sampling. Computation of the probability of an outcome sequence and of the posterior initial state distribution used the forward-backward algorithm [11], which is built into the packages we used."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work includes contributions of the National Institute of Standards and Technology, which are not subject to U.S. copyright. The use of trade, product and software names is for informational purposes only and does not imply endorsement or recommendation by the U.S. government. S. Geller acknowledges support from the Professional Research Experience Program (PREP) operated jointly by NIST and the University of Colorado. D. C. C. acknowledges support from a National Research Council postdoctoral fellowship. E. K. thanks Dietrich Leibfried for introducing them in the early days of ion trap quantum computing to the idea of adaptively chosen pulses for improving measurement fidelity. We thank Zachary Sunberg for discussions on the POMDP formalism. We thank Giorgio Zarantonello for computations involving the transition rates in 9Be+. We thank Ting Rei Tan, Mohammad Alhejji, Alexander Kwiatkowski, Arik Avagyan, Akira Kyle, and Stephen Erickson for\n24\nhelpful suggestions and comments.\n[1] J. A. Tropp, User-Friendly Tail Bounds for Sums of Random Matrices, Found. Comput. Math.\n12, 389 (2012).\n[2] D. Hume, T. Rosenband, and D. J. Wineland, High-Fidelity Adaptive Qubit Detection through\nRepetitive Quantum Nondemolition Measurements, Phys. Rev. Lett. 99, 120502 (2007).\n[3] C. E. Langer, High Fidelity Quantum Information Processing with Trapped Ions, Ph.D. thesis,\nUniversity of Colorado at Boulder (2006).\n[4] G. Liu, M. Chen, Y.-X. Liu, D. Layden, and P. Cappellaro, Repetitive Readout Enhanced by\nMachine Learning, Mach. Learn.: Sci. Technol. 1, 015003 (2020).\n[5] Z.-H. Ding, J.-M. Cui, Y.-F. Huang, C.-F. Li, T. Tu, and G.-C. Guo, Fast High-Fidelity\nReadout of a Single Trapped-Ion Qubit via Machine-Learning Methods, Phys. Rev. Appl. 12,\n014038 (2019).\n[6] S. Crain, C. Cahall, G. Vrijsen, E. E. Wollman, M. D. Shaw, V. B. Verma, S. W. Nam,\nand J. Kim, High-Speed Low-Crosstalk Detection of a 171Yb+ Qubit using Superconducting\nNanowire Single Photon Detectors, Communications Physics 2, 1 (2019).\n[7] A. Seif, K. A. Landsman, N. M. Linke, C. Figgatt, C. Monroe, and M. Hafezi, Machine\nLearning Assisted Readout of Trapped-Ion Qubits, Journal of Physics B: Atomic, Molecular\nand Optical Physics 51, 174006 (2018).\n[8] E. Magesan, J. M. Gambetta, A. D. Co\u0301rcoles, and J. M. Chow, Machine Learning for Discrim-\ninating Quantum Measurement Trajectories and Improving Readout, Physical Review Letters\n114, 200501 (2015).\n[9] J. Gambetta, W. A. Braff, A. Wallraff, S. M. Girvin, and R. J. Schoelkopf, Protocols for\nOptimal Readout of Qubits Using a Continuous Quantum Nondemolition Measurement, Phys.\nRev. A: At. Mol. Opt. Phys. 76 (2007).\n[10] Y. Ephraim and N. Merhav, Hidden Markov Processes, IEEE Trans. Inf. Theory 48, 1518\n(2002).\n[11] L. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recog-\nnition, Proc. IEEE 77, 257 (1989).\n[12] S. S. Elder, C. S. Wang, P. Reinhold, C. T. Hann, K. S. Chou, B. J. Lester, S. Rosenblum,\nL. Frunzio, L. Jiang, and R. J. Schoelkopf, High-Fidelity Measurement of Qubits Encoded in\nMultilevel Superconducting Circuits, Phys. Rev. X 10, 011001 (2020).\n[13] L. A. Martinez, Y. J. Rosen, and J. L. DuBois, Improving Qubit Readout with Hidden Markov\nModels, Phys. Rev. A: At. Mol. Opt. Phys. 102, 062426 (2020).\n[14] C. T. Hann, S. S. Elder, C. S. Wang, K. Chou, R. J. Schoelkopf, and L. Jiang, Robust Readout\nof Bosonic Qubits in the Dispersive Coupling Regime, Phys. Rev. A: At. Mol. Opt. Phys. 98\n(2018).\n[15] J. C. Curtis, C. T. Hann, S. S. Elder, C. S. Wang, L. Frunzio, L. Jiang, and R. J. Schoelkopf,\nSingle-Shot Number-Resolved Detection of Microwave Photons with Error Mitigation, Phys.\nRev. A: At. Mol. Opt. Phys. 103, 023705 (2021).\n[16] S. Wo\u0308lk, C. Piltz, T. Sriarunothai, and C. Wunderlich, State Selective Detection of Hyperfine\nQubits, J. Phys. B: At. Mol. Opt. Phys. 48, 075101 (2015).\n[17] B. Hemmerling, F. Gebert, Y. Wan, and P. O. Schmidt, A Novel, Robust Quantum Detection\nScheme, New J. Phys. 14, 023043 (2012).\n25\n[18] S. Geller, S. Glancy, and E. Knill, Permuted Hidden Markov Models for State Inference,\nhttps://github.com/usnistgov/perm_hmm (2021).\n[19] K. J. A\u030astro\u0308m, Optimal Control of Markov Processes with Incomplete State Information I, J.\nMath. Anal. Appl. 10, 174 (1965).\n[20] C. H. Papadimitriou and J. N. Tsitsiklis, The Complexity of Markov Decision Processes, Math.\nOper. Res. 12, 441 (1987).\n[21] H. Kurniawati, D. Hsu, and W. S. Lee, SARSOP: Efficient Point-Based Pomdp Planning by\nApproximating Optimally Reachable Belief Spaces., in Robotics: Science and Systems, Vol.\n2008 (Zurich, Switzerland., 2008).\n[22] A.-G. Paschke, 9Be+ Ion Qubit Control Using an Optical Frequency Comb, Ph.D. thesis,\nHannover: Gottfried Wilhelm Leibniz Universita\u0308t Hannover (2017).\n[23] M. Acton, K.-A. Brickman, P. Haljan, P. Lee, L. Deslauriers, and C. Monroe, Near-Perfect\nSimultaneous Measurement of a Qubit Register, arXiv preprint quant-ph/0511257 (2005).\n[24] M. Kjaergaard, M. E. Schwartz, J. Braumu\u0308ller, P. Krantz, J. I.-J. Wang, S. Gustavsson, and\nW. D. Oliver, Superconducting Qubits: Current State of Play, Annu. Rev. Condens. Matter\nPhys. 11, 369 (2020).\n[25] J. Medford, J. Beil, J. Taylor, E. Rashba, H. Lu, A. Gossard, and C. M. Marcus, Quantum-\nDot-Based Resonant Exchange Qubit, Phys. Rev. Lett. 111, 050501 (2013).\n[26] H. Chernoff, Sequential Design of Experiments, Ann. Math. Statist. 30, 755 (1959).\n[27] Y. Chen, S. H. Hassani, A. Karbasi, and A. Krause, Sequential Information Maximization:\nWhen is Greedy Near-Optimal?, in Conference on Learning Theory (PMLR, 2015) pp. 338\u2013\n363.\n[28] D. Kartik, A. Nayyar, and U. Mitra, Active Hypothesis Testing: Beyond Chernoff-Stein, in\n2019 IEEE International Symposium on Information Theory (ISIT) (IEEE, 2019) pp. 897\u2013\n901.\n[29] M. M. Wilde, From Classical to Quantum Shannon Theory (Cambridge University Press,\n2011).\n[30] J. Barry, D. T. Barry, and S. Aaronson, Quantum Partially Observable Markov Decision\nProcesses, Phys. Rev. A: At. Mol. Opt. Phys. 90, 032311 (2014).\n[31] J. L. A. Combes, Rapid Measurement and Purification Using Quantum Feedback Control,\nPh.D. thesis, Griffith University (2010).\n[32] J. Combes, H. M. Wiseman, and K. Jacobs, Rapid Measurement of Quantum Systems Using\nFeedback Control, Phys. Rev. Lett. 100, 160503 (2008).\n[33] D. E. Kirk, Optimal Control Theory: An Introduction (Courier Corporation, 2004).\n[34] B. Alt, M. Schultheis, and H. Koeppl, POMDPs in Continuous Time and Discrete Spaces,\nAdvances in Neural Information Processing Systems 33, 13151 (2020).\n[35] A. H. Myerson, D. J. Szwer, S. C. Webster, D. T. C. Allcock, M. J. Curtis, G. Imreh, J. A.\nSherman, D. N. Stacey, A. M. Steane, and D. M. Lucas, High-Fidelity Readout of Trapped-Ion\nQubits, Phys. Rev. Lett. 100 (2008).\n[36] S. L. Todaro, V. Verma, K. C. McCormick, D. Allcock, R. Mirin, D. J. Wineland, S. W. Nam,\nA. C. Wilson, D. Leibfried, and D. Slichter, State Readout of a Trapped Ion Qubit using a\nTrap-Integrated Superconducting Photon Detector, Phys. Rev. Lett. 126, 010501 (2021).\n[37] M. Naghshvar and T. Javidi, Active Sequential Hypothesis Testing, Ann. Statist. 41, 2703\n(2013).\n[38] A. C. Keith, C. H. Baldwin, S. Glancy, and E. Knill, Joint Quantum-State and Measurement\nTomography with Incomplete Measurements, Physical Review A 98, 042318 (2018).\n26\n[39] K. P. Murphy, Machine Learning: a Probabilistic Perspective (MIT press, 2012).\n[40] J. Shao, Mathematical Statistics (Springer, New York, 2003).\n[41] R. Bellman, Dynamic Programming, Rand Corporation Research Study (Princeton University\nPress, 1957).\n[42] M. J. Kochenderfer, T. A. Wheeler, and K. H. Wray, Algorithms for Decision Making (MIT\nPress, 2022).\n[43] A. R. Cassandra, L. P. Kaelbling, and J. A. Kurien, Acting Under Uncertainty: Discrete\nBayesian Models for Mobile-Robot Navigation, in Proceedings of IEEE/RSJ International\nConference on Intelligent Robots and Systems. IROS\u201996, Vol. 2 (IEEE, 1996) pp. 963\u2013972.\n[44] E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh,\nP. A. Szerlip, P. Horsfall, and N. D. Goodman, Pyro: Deep Universal Probabilistic Program-\nming, J. Mach. Learn. Res. 20, 28:1 (2019).\n[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, Pytorch: an Imperative Style,\nHigh-Performance Deep Learning Library, in Advances in Neural Information Processing Systems 32 , edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche\u0301-Buc, E. Fox, and R. Garnett (Curran Associates, Inc., 2019) pp. 8024\u20138035."
        }
    ],
    "title": "Improving quantum state detection with adaptive sequential observations",
    "year": 2022
}