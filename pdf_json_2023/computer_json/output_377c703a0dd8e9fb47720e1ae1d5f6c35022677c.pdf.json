{
    "abstractText": "An important problem in machine learning theory is to understand the approximation and generalization properties of two-layer neural networks in high dimensions. To this end, researchers have introduced the Barron space Bs(\u03a9) and the spectral Barron space Fs(\u03a9), where the index s \u2208 [0, \u221e) indicates the smoothness of functions within these spaces and \u03a9 \u2282 Rd denotes the input domain. However, the precise relationship between the two types of Barron spaces remains unclear. In this paper, we establish a continuous embedding between them as implied by the following inequality: For any \u03b4 \u2208 (0, 1), s \u2208 N+ and f : \u03a9 7\u2192 R, it holds that \u03b4\u2016 f \u2016Fs\u2212\u03b4(\u03a9) .s \u2016 f \u2016Bs(\u03a9) .s \u2016 f \u2016Fs+1(\u03a9). Importantly, the constants do not depend on the input dimension d, suggesting that the embedding is effective in high dimensions. Moreover, we also show that the lower and upper bound are both tight.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Wu"
        }
    ],
    "id": "SP:4a3be00160ec6f9a83d37d416a4da8c068cd408a",
    "references": [
        {
            "authors": [
                "A.R. Barron"
            ],
            "title": "Universal approximation bounds for superpositions of a sigmoidal function",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 1993
        },
        {
            "authors": [
                "L. Breiman"
            ],
            "title": "Hinging hyperplanes for regression, classification, and function approximation",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 1993
        },
        {
            "authors": [
                "A. Caragea",
                "P. Petersen"
            ],
            "title": "Voigtlaender, Neural network approximation and estimation of classifiers with classification boundary in a Barron",
            "year": 2011
        },
        {
            "authors": [
                "Z. Chen",
                "J. Lu",
                "Y. Lu"
            ],
            "title": "On the representation of solutions to elliptic PDEs in Barron spaces",
            "venue": "in: Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function, Math",
            "venue": "Control Signals Systems,",
            "year": 1989
        },
        {
            "authors": [
                "C. Domingo-Enrich",
                "Y. Mroueh"
            ],
            "title": "Tighter sparse approximation bounds for ReLU neural networks",
            "venue": "in: International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "W. E",
                "C. Ma",
                "L. Wu"
            ],
            "title": "A priori estimates of the population risk for two-layer neural networks",
            "venue": "Commun. Math. Sci.,",
            "year": 2019
        },
        {
            "authors": [
                "W. E",
                "C. Ma",
                "L. Wu"
            ],
            "title": "The Barron space and the flow-induced function spaces for neural network models",
            "venue": "Constr. Approx.,",
            "year": 2022
        },
        {
            "authors": [
                "E S. W"
            ],
            "title": "Wojtowytsch, Some observations on high-dimensional partial differential equations with Barron data",
            "venue": "in: Proceedings of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "E S. W"
            ],
            "title": "Wojtowytsch, Representation formulas and pointwise properties for Barron functions, Calc",
            "venue": "Var. Partial Differential Equations,",
            "year": 2022
        },
        {
            "authors": [
                "W. E",
                "B. Yu"
            ],
            "title": "The deep Ritz method: A deep learning-based numerical algorithm for solving variational problems",
            "venue": "Commun. Math. Stat.,",
            "year": 2018
        },
        {
            "authors": [
                "J.M. Klusowski",
                "A.R. Barron"
            ],
            "title": "Risk bounds for high-dimensional ridge function combinations including neural networks",
            "year": 2016
        },
        {
            "authors": [
                "B. Li",
                "S. Tang",
                "H. Yu"
            ],
            "title": "Better approximations of high dimensional smooth functions by deep neural networks with rectified power units",
            "venue": "Commun. Comput. Phys.,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Li",
                "C. Ma",
                "L. Wu"
            ],
            "title": "Complexity measures for neural networks with general activation functions using path-based norms",
            "year": 2009
        },
        {
            "authors": [
                "J. Lu",
                "Y. Lu"
            ],
            "title": "A priori generalization error analysis of two-layer neural networks for solving high dimensional Schr\u00f6dinger eigenvalue problems",
            "venue": "Comm. Amer. Math. Soc.,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Lu",
                "J. Lu",
                "M. Wang"
            ],
            "title": "A priori generalization analysis of the deep Ritz method for solving high dimensional elliptic partial differential equations",
            "venue": "in: Proceedings of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Meng",
                "P. Ming"
            ],
            "title": "A new function space from Barron class and application to neural network approximation",
            "venue": "Commun. Comput. Phys.,",
            "year": 2022
        },
        {
            "authors": [
                "B. Neyshabur",
                "R. Tomioka",
                "N. Srebro"
            ],
            "title": "Norm-based capacity control in neural networks",
            "venue": "in: JMLR: Workshop and Conference Proceedings,",
            "year": 2015
        },
        {
            "authors": [
                "G. Ongie",
                "R. Willett",
                "D. Soudry",
                "N. Srebro"
            ],
            "title": "A function space view of bounded norm infinite width ReLU nets: The multivariate case",
            "venue": "in: International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "R. Parhi",
                "R.D. Nowak"
            ],
            "title": "Banach space representer theorems for neural networks and ridge splines",
            "venue": "J. Mach. Learn. Res.,",
            "year": 1960
        },
        {
            "authors": [
                "A. Pinkus"
            ],
            "title": "Approximation theory of the MLP model in neural networks",
            "venue": "Acta Numer.,",
            "year": 1999
        },
        {
            "authors": [
                "J.W. Siegel",
                "J. Xu"
            ],
            "title": "Approximation rates for neural networks with general activation functions",
            "venue": "Neural Netw.,",
            "year": 2020
        },
        {
            "authors": [
                "J.W. Siegel",
                "J. Xu"
            ],
            "title": "Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural networks, Found",
            "venue": "Comput. Math.,",
            "year": 2022
        },
        {
            "authors": [
                "J.W. Siegel",
                "J. Xu"
            ],
            "title": "Characterization of the variation spaces corresponding to shallow neural networks",
            "venue": "Constr. Approx.,",
            "year": 2023
        },
        {
            "authors": [
                "D. So",
                "W. Ma\u0144ke",
                "H. Liu",
                "Z. Dai",
                "N. Shazeer",
                "Q.V. Le"
            ],
            "title": "Searching for efficient transformers for language modeling",
            "venue": "in: Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "J. Xu"
            ],
            "title": "Finite neuron method and convergence analysis",
            "venue": "Commun. Comput. Phys.,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "\u03b4\u2016 f \u2016Fs\u2212\u03b4(\u2126) .s \u2016 f \u2016Bs(\u2126) .s \u2016 f \u2016Fs+1(\u2126).\nImportantly, the constants do not depend on the input dimension d, suggesting that the embedding is effective in high dimensions. Moreover, we also show that the lower and upper bound are both tight.\nKeywords: Barron space, Two-layer neural network, High-dimensional approximation, Embedding theorem, Fourier transform.\nArticle Info.: Volume: X Number: X Pages: 1 - 12 Date: /2023 doi.org/10.4208/jml.230530\nArticle History: Received: 30/05/2023 Accepted: 17/12/2023\nCommunicated by: Weinan E"
        },
        {
            "heading": "1 Introduction",
            "text": "A (scaled) two-layer neural network is given by\nfm(x; \u03b8) = 1\nm\nm\n\u2211 j=1\naj\u03c3 ( wTj x + bj ) , (1.1)\nwhere \u03c3 : R 7\u2192 R is a nonlinear activation function; aj, bj \u2208R, wj \u2208R d, \u03b8 = {(aj, wj, bj)} m j=1, m and d denote the network width and the input dimension, respectively. The extra scale factor in (1.1) is introduced to facilitate our subsequent analysis and it does not change the network\u2019s approximation power. Additionally, throughout this paper, we assume the input domain \u2126 \u2282 Rd to be compact and focus on the case of activation function ReLUs with s \u2265 0 \u03c3(z) = max(0, z)s.\nThe cases of s = 0 and s = 1 correspond to the Heaviside step function and vanilla ReLU function, respectively. The case of s \u2265 2 has also found applications in solving PDEs [11, 13, 26] and natural language processing [25].\n*leiwu@math.pku.edu.cn\nhttps://www.global-sci.com/jml Global Science Press\nCybenko [5] showed that functions in C(\u2126) can be approximated arbitrarily well by two-layer neural networks with respect to the uniform metric. However, the approximation can be arbitrarily slow. Pinkus [21] expanded on this by showing that for functions belonging in Ck(\u2126), the approximation by two-layer neural networks can achieve a rate of O(m\u2212k/d). This rate, unfortunately, is subject to the curse of dimensionality since it diminishes as d increases. These suggest that mere continuity and smoothness are not sufficient to ensure an efficient approximation in high dimensions. Then it is natural to ask: What kind of regularity can ensure the efficient approximation by two-layer neural networks? Before proceeding to review previous studies attempting to answer this question. We need a dual norm for handling the compactness of input domain.\nDefinition 1.1 ([1]). Given a compact set \u2126, we define \u2016v\u2016\u2126 = supx\u2208\u2126 |v T x|.\nWe begin by considering the spectral Barron spaces [3, 22, 24, 26], which are defined as follows.\nDefinition 1.2. Let \u2126 \u2282 Rd be a compact domain. For f : \u2126 7\u2192 R and s \u2265 0, define\n\u2016 f\u2016Fs(\u2126) = inf fe|\u2126= f\n\u222b\nRd (1 + \u2016\u03be\u2016\u2126)\ns \u2223\u2223 f\u0302e(\u03be) \u2223\u2223d\u03be,\nwhere the infimum is taken over all extensions of f . Let\nFs(\u2126) := { f : \u2126 7\u2192 R : \u2016 f\u2016Fs(\u2126) < \u221e}.\nThen, the spectral Barron space is defined as Fs(\u2126) equipped with the \u2016 \u00b7 \u2016Fs(\u2126) norm.\nIn the above definition, we consider measure-valued Fourier transform as done in [1]. It is worth noting that Definition 1.2 bears resemblance to the Fourier-based characterization of Sobolev spaces, denoted as\n\u2016 f\u20162Hs = \u222b\nRd (1 + \u2016\u03be\u2016)s| f\u0302 (\u03be)|2 d\u03be.\nThe major distinction lies in the fact that the moment in Definition 1.2 is calculated with respect to | f\u0302 (\u03be)| instead of | f\u0302 (\u03be)|2. It was proved in [26] that if \u2016 f\u2016Fs(\u2126) < \u221e, then functions in Fs(\u2126) can be approximated by two-layer ReLUs\u22121 networks without suffering the curse of dimensionality. Specifically, the approximation error obeys the Monte-Carlo error rate O(m\u22121/2), where m denotes the network width. The special case of s = 1 was first considered in the pioneer work of Barron [1]. Subsequently, the case of s = 2 was studied in [2, 12]. More recently, the extension to general positive integer s was provided in [3, 22, 26].\nThe Fourier-based characterization, while explicit, is not necessarily tight as it may exclude functions that can be effectively approximated by two-layer neural networks. [19,20] considered similar characterizations based on Radon transform instead of Fourier transform, which can yield a tight characterization for the case of d = 1. Moreover, [7,8] offered a probabilistic generalization of Barron\u2019s analysis [1]. In these studies, functions satisfying the following expectation representation are taken into consideration:\nf\u03c1(x) = E(a,w,b)\u223c\u03c1 [ a\u03c3(wT x + b) ] , \u2200 x \u2208 \u2126, (1.2)\nwhere \u03c1 \u2208 P(R\u00d7Rd \u00d7R). This can be obtained from (1.1) by taking m \u2192 \u221e and applying the law of large numbers. One can view f\u03c1 as an infinitely-wide two-layer neural network. It is important to note that the expectation representation in (1.2) only needs to hold in \u2126 instead of the entire space Rd. Accordingly, the (probabilistic) Barron spaces are defined as follows.\nDefinition 1.3. Given s \u2265 0 and f : \u2126 7\u2192 R, let\nA f := { \u03c1 \u2208 P(R \u00d7 Rd \u00d7 R) : f\u03c1|\u2126 = f } .\nThen, the Barron norm of f and the associated Barron space is defined by\n\u2016 f\u2016Bs(\u2126) := inf\u03c1\u2208A f E(a,w,b)\u223c\u03c1\n[ |a|(\u2016w\u2016\u2126 + |b|) s ] .\nLet Bs(\u2126) = { f : \u2126 7\u2192 R : \u2016 f\u2016Bs(\u2126) < \u221e}.\nThen the Barron space is defined as Bs(\u2126) equipped with the \u2016 \u00b7 \u2016Bs(\u2126) norm.\nThe above definition is a slight generalization of the one originally proposed in [8], where only the case of s = 1 is considered. Following the proofs in [7, 8], one can easily show that approximating and estimation error for learning functions in Bs with twolayer ReLUs networks follow the Monte-Carlo rates O(m\u22121/2) and O(n\u22121/2), respectively. Here n denotes the number of training samples. Recently, [23] established a sharper approximation rate of O(m\u22121/2\u2212(s+1/2)/d). However, it is important to note that this rate improvement is less significant in high dimensions and additionally, the hidden constants in [23] may have an exponential dependence on d. Compared with the Fourier-based characterization in Definition 1.2, the above expectation-based characterization is more natural and complete. Specifically, [8] provided an inverse approximation theorem, showing that if f can be approximated by two-layer ReLU networks with bounded path norm [18], it must lie in B1(\u2126)."
        },
        {
            "heading": "1.1 Our contribution",
            "text": "Recently, Barron-type spaces defined above have been adopted to explore various highdimensional problems. For instance, [4, 9, 15, 16] established some regularity theories of high-dimensional PDEs with Barron-type spaces. Hence, it is natural to ask: What is the relationship between them? [1,2,12,26] already showed that Fs+1(\u2126) \u2286 Bs(\u2126). Moreover, [10] provided a specific example showing that F2(\u2126) ( B1(\u2126), implying that B1(\u2126) is strictly larger than F2(\u2126). Along this line of work, our major contribution is the following precise embedding result.\nTheorem 1.1. Let \u2126 \u2282 Rd be a compact set. For any s \u2208 N+, f \u2208 Bs(\u2126), \u03b4 \u2208 (0, 1), we have\n\u03b4\u2016 f\u2016Fs\u2212\u03b4(\u2126) .s \u2016 f\u2016Bs(\u2126) .s \u2016 f\u2016Fs+1(\u2126),\nwhere s in the upper bound can take the value of 0.\nNote that the hidden embedding constants depend solely on the value of s. This suggests that the embedding revealed in Theorem 1.1 is effective in high dimensions. Additionally, as per our current proof, the smoothness index s is required to be a positive integer, though Bs(\u2126) is defined for any s in the range of [0, \u221e). However, we conjecture that analogous results would apply for any s \u2208 (0,+\u221e) as discussed in Remark 2.1, which we leave for future work.\nAdditionally, we would like to clarify that the upper bound in Theorem 1.1 has already been implicitly established in previous works. Specifically, the case of s = 0 was proven in the pioneering work of Barron [1] albeit presented in a different form. Subsequently, the analysis was extended to the case of s = 1 in [2, 12], and further generalized to arbitrary non-negative integer values of s in [22, 26]. Our major contribution is the lower bound, which is critical for establishing the embedding between the two types of Barron spaces and the proof is presented in Section 2.1. In Theorem 1.1, the upper bound is stated for the sake of completeness.\nWe mention that [17] establishes the embedding among spectral Barron spaces and some classical spaces such as the Sobolev space, Besov space, and Bessel potential space. In contrast, we focus on the embedding between the Barron spaces and spectral Barron spaces.\nTightness. For the upper bound, [3, Proposition 7.4] shows that when \u2126 has nonempty interior, if Fs(\u2126) \u2282 B1(\u2126), then we must have s \u2265 2. This implies that the upper bound is tight. The following proposition shows that the lower bound in Theorem 1.1 is also tight in the sense that the value of \u03b4 cannot be taken to zero.\nProposition 1.1. Let \u2126 = [\u22121, 1] and f (x) = max(1 \u2212 |x|, 0) for x \u2208 \u2126. Then,\n\u2016 f\u2016B1(\u2126) \u2264 3, \u2016 f\u2016F1(\u2126) = +\u221e.\nLet t(x) := max(1 \u2212 |x|, 0) for any x \u2208 R be the triangular hat function (see Fig. 1.1).\nWe have\nt\u0302(\u03be) = 1 \u2212 cos(\u03be)\n\u03c0\u03be2 .\nNote that t(\u00b7) is a zero extension of f and \u222b\nR\n(1 + |\u03be|) \u2223\u2223t\u0302(\u03be) \u2223\u2223 d\u03be = +\u221e.\nHowever, this does not directly imply \u2016 f\u2016F1(\u2126) = \u221e, since the spectral Barron norm is defined by taking the infimum over all possible extensions. We refer to Section 2.2 for a rigorous proof."
        },
        {
            "heading": "2 Proofs",
            "text": "Notation. We use X .\u03b1 Y to denote X \u2264 C\u03b1Y where C\u03b1 is a positive constant that depends only on \u03b1. For a vector v, let \u2016v\u2016p = (\u2211j v p j ) 1/p. Let\nS d\u22121 = { x \u2208 Rd : \u2016x\u20162 = 1 } ,\nS d\u22121 \u2126\n= { x \u2208 Rd : \u2016x\u2016\u2126 = 1 } .\nDenote by 1S the indicator function of the set S, satisfying 1S(x) = 1 for x \u2208 S, and 0 otherwise. For a metric space X, denote by P(X) the set of probability measures over X.\nThroughout this paper, we define Fourier transform as follows:\nf\u0302 (\u03be) = 1\n(2\u03c0)d\n\u222b\nRd e\u2212i\u03be Tx f (x)dx,\nand the inverse Fourier transform is given by\nf (x) = \u222b\nRd ei\u03be\nTx f\u0302 (\u03be)d\u03be.\nNote that in these definitions, the terms f (x)dx and f\u0302 (\u03be)d\u03be should be interpreted as a finite measure in a broad sense. Moreover, we will use the identity: for d = 1,\n\u03b4(\u03be) = 1\n2\u03c0\n\u222b\nR\ne\u2212i\u03bex dx.\nBefore proceeding to the proof, we first clarify several important issues that might be ignored. Both types of Barron functions are defined on a compact domain \u2126 instead of the whole space Rd and thus, Barron norms depend on the underlying domain \u2126. When estimating Barron norms, one need to be careful with the choice of extensions. A naive extension may yield a significantly loose bound of the Fs(\u2126) norm [6] and Bs(\u2126) norm."
        },
        {
            "heading": "2.1 Proof of Theorem 1.1",
            "text": "We start by considering the case of single neurons. For any w \u2208 Sd\u22121 \u2126 , b \u2208 R, the single neuron \u03c3w,b : \u2126 7\u2192 R is given by \u03c3w,b(x) = \u03c3(w T x + b). Note that the domain of \u03c3w,b is \u2126\ninstead of Rd. In particular, when d = 1 and w = 1, we write \u03c3b = \u03c3w,b for simplicity. The following lemma characterizes the Fourier transform of a single neuron.\nLemma 2.1. Let \u03c3w,b : \u2126 7\u2192 R with \u2016w\u2016\u2126 = 1 be a single neuron and g : R 7\u2192 R be any extension of 1[\u22121,1]. Then, Gw,b(x) := \u03c3w,b(x)g(w Tx) is an extension of \u03c3w,b, satisfying\n\u222b\nRd (1 + \u2016\u03be\u2016\u2126)\ns \u2223\u2223G\u0302w,b(\u03be) \u2223\u2223d\u03be = \u222b\nR\n(1 + |v|)s \u2223\u2223h\u0302\u03c3,b(v) \u2223\u2223 dv, (2.1)\nwhere h\u03c3,b(z) = \u03c3(z + b)g(z) is an extension of \u03c3b : [\u22121, 1] 7\u2192 R.\nProof. Let Q = (w, w2, . . . , wd) T \u2208 Rd\u00d7d with w2, . . . , wd being orthonormal and w T i w = 0 for i = 2, . . . , d. Then, by letting \u03be\u0304 = (Q\u22121)T\u03be, we have\nG\u0302w,b(\u03be) = 1\n(2\u03c0)d\n\u222b\nRd \u03c3 ( wTx + b ) g ( wTx ) e\u2212i\u03be Tx dx\n= 1\n(2\u03c0)d\n\u222b\nRd \u03c3(y1 + b)g(y1)e\n\u2212i\u03beTQ\u22121y 1\n|det Q| dy (y = Qx)\n= 1\n|det Q|\n( 1\n2\u03c0\n\u222b\nRd \u03c3(y1 + b)g(y1)e\n\u2212i\u03be\u03041y1 dy1 ) d \u220f j=2 \u03b4(\u03be\u0304 j)\n= 1\n|det Q| h\u0302\u03c3,b(\u03be\u03041)\nd\n\u220f j=2 \u03b4(\u03be\u0304 j). (2.2)\nNow, we have \u222b\nRd (1 + \u2016\u03be\u2016\u2126)\ns \u2223\u2223G\u0302w,b(\u03be) \u2223\u2223 d\u03be\n= \u222b\nRd\n( 1 + \u2016QT \u03be\u0304\u2016\u2126 )s 1 |det Q| \u2223\u2223h\u0302\u03c3,b(\u03be\u03041) \u2223\u2223 d\n\u220f j=2\n\u03b4 ( \u03be\u0304 j ) |detQ|d\u03be\u0304\n= \u222b\nRd\n( 1 + \u2225\u2225\u2225\u2225\u03be\u03041w + d\n\u2211 j=2 \u03be\u0304 jwj \u2225\u2225\u2225\u2225 \u2126 )s \u2223\u2223h\u0302\u03c3,b(\u03be\u03041) \u2223\u2223 d \u220f j=2 \u03b4 ( \u03be\u0304 j ) d\u03be\u0304\n= \u222b\nR\n( 1 + \u2016w\u2016\u2126 \u2223\u2223\u03be\u03041 \u2223\u2223)s\u2223\u2223h\u0302\u03c3,b ( \u03be\u03041 )\u2223\u2223d\u03be\u03041\n= \u222b\nR\n( 1 + \u2223\u2223\u03be\u03041 \u2223\u2223)s\u2223\u2223h\u0302\u03c3,b ( \u03be\u03041 )\u2223\u2223d\u03be\u03041, (2.3)\nwhere the first step uses \u03be = QT \u03be\u0304 and the last step is due to \u2016w\u2016\u2126 = 1.\nThe above lemma provides a way to estimating spectral Barron norms of single neurons. What remains is to determine an extension g such that the right-hand side of the Eq. (2.1) to be as small as possible. To this end, we first consider the one-dimensional case.\nWhen d = 1, for any b \u2208 R, let \u03c3b = \u03c3(\u00b7 + b) : [\u22121, 1] 7\u2192 R. When it is clear from the context, we also use \u03c3b denote the single neuron define on the entire space. Let \u03c7 : R 7\u2192 R\nbe a smooth cutoff function, satisfying \u03c7 \u2208 C\u221ec (R) and \u03c7(z) = 1 for any z \u2208 [\u22121, 1] and supp \u03c7 = [\u22122, 2]. Given any b \u2208 R, we shall consider the following extension of a single neuron:\nh\u03c3,b(z) = \u03c7(z)\u03c3b(z) : R 7\u2192 R.\nLemma 2.2. Let \u03c3(z) = max(0, z)s with s \u2208 N. Then,\n\u2223\u2223h\u0302\u03c3,b(\u03be) \u2223\u2223 .s (1 + |b|)s\n(1 + |\u03be|)s+1 .\nRemark 2.1. The proof uses explicitly the condition of s \u2208 N. However, according to the relationship between the smoothness of a function and the decay of the Fourier transform, we anticipate that the same result holds for any s \u2208 [0, \u221e).\nProof. Using the product rule, we have for any k \u2208 N+,\nh (k) \u03c3,b(z) =\nk\n\u2211 i=0\n( k\ni\n) \u03c3 (i) b (z)\u03c7 (k\u2212i)(z),\nand prove the theorem for the following two cases separately. Without lose of generality, we consider here only the case of b \u2265 0. When b is negative, the proof is similar.\nCase 1: b \u2265 2. In this case, h\u03c3,b(\u00b7) = \u03c3b(\u00b7)\u03c7(\u00b7) \u2208 C \u221e(R). Without loss of generality,\nwe consider the case of b \u2265 1, for which\nh\u03c3,b(z) =    0, if z < \u22122, (z + b)s\u03c7(z), if z \u2208 [\u22122, 2],\n0, if z > 2.\n\u2022 When z \u2208 [\u22122, 2], we have \u03c3 (k) b (z) = 0 for k > s and |\u03c3 (k) b (z)| .s (1 + |b|) s for k \u2264 s. Hence, for any k \u2208 N, we have\n\u2223\u2223h(k)b (z) \u2223\u2223 = min{k,s}\n\u2211 i=0\n( k\ni\n) \u03c3 (i) b (z)\u03c7 (k\u2212i)(z)\n.s\nmin{k,s}\n\u2211 i=0\n\u2223\u2223\u03c3(i)b (z) \u2223\u2223\u2223\u2223\u03c7(k\u2212i)(z) \u2223\u2223\n.s\nmin{k,s}\n\u2211 k=0\n\u2223\u2223\u03c3(k)b (z) \u2223\u2223 .s (1 + |b|)s .\n\u2022 When |z| > 2, |h (k) \u03c3,b(z)| = 0 for any k \u2208 N.\nCombining two cases leads to for any k \u2208 N,\n\u2225\u2225h(k)\u03c3,b \u2225\u2225 L1(R) = \u222b 2\n\u22122\n\u2223\u2223h(k)\u03c3,b(z) \u2223\u2223dz .s (1 + |b|)s .\nThis implies\n\u2223\u2223h\u0302b(\u03be) \u2223\u2223 = \u2223\u2223\u2223\u2223 1\n2\u03c0(\u2212i\u03be)s+1\n\u222b\nR\nh (s+1) \u03c3,b (x)e \u2212i\u03bex dx \u2223\u2223\u2223\u2223 .s (1 + |b|)s\n|\u03be|s+1 . (2.4)\nCase 2: 0 \u2264 b < 2. In this case, h\u03c3,b is piecewise smooth, given by\nh\u03c3,b(z) =\n{ 0, if z \u2264 \u2212b,\n(z + b)s\u03c7(z), if z > \u2212b.\nConsequently, h (s) \u03c3,b(\u00b7) is bounded and has only one discontinuity point at z = \u2212b. By adopting the product rule in a way similar as the above, it is not hard to show that for all k \u2208 N,\nh (k) \u03c3,b(z) = 0, \u2200 z \u2208 (\u2212\u221e,\u2212b) \u222a [2,+\u221e), \u2223\u2223h(k)\u03c3,b(z) \u2223\u2223 .s 1, \u2200 z \u2208 (\u2212b, 2], (2.5)\nand limz\u2192(\u2212b)+ h (s) \u03c3,b(z) exists with | limz\u2192(\u2212b)+ h (s) \u03c3,b(z)| .s 1.\nNoting that\nh\u0302\u03c3,b(\u03be) = 1\n2\u03c0\n\u222b \u221e\n\u2212\u221e h\u03c3,b(z)e\n\u2212i\u03bez dz\n= 1\n2\u03c0(\u2212i\u03be)s\n\u222b \u221e\n\u2212\u221e h (s) \u03c3,b(z)e \u2212i\u03bez dz\n= 1\n2\u03c0(\u2212i\u03be)s\n\u222b 2\n\u2212b h (s) \u03c3,b(z)e \u2212i\u03bez dz\n= 1\n2\u03c0(\u2212i\u03be)s\n( e\u2212i\u03bez\n\u2212i\u03be h (s) \u03c3,b(z)\n\u2223\u2223\u2223 2 \u2212b + \u222b 2 \u2212b h (s+1) \u03c3,b (z) e\u2212i\u03bez i\u03be dz ) ,\nand applying (2.5), we have\n|h\u0302\u03c3,b(\u03be)| .s 1\n|\u03be|s+1\n( 1 + \u222b 2\n\u2212b dz\n) .\n1\n|\u03be|s+1 .s\n(1 + |b|)s\n|\u03be|s+1 , (2.6)\nwhere the last step uses the assumption of |b| \u2264 2.\nOn the other hand, when |\u03be| \u2264 1, we have for any b \u2208 R that\n\u2223\u2223h\u0302\u03c3,b(\u03be) \u2223\u2223 \u2264 1\n2\u03c0\n\u222b\nR\n\u2223\u2223h\u03c3,b(z)e\u2212i\u03bez dz \u2223\u2223 \u2264 1\n2\u03c0\n\u222b 2\n\u22122 |h\u03c3,b(z)|dz .s (1 + |b|)\ns . (2.7)\nThen, combining (2.7) with (2.4) and (2.6) yields\n\u2223\u2223h\u0302\u03c3,b(\u03be) \u2223\u2223 .s (1 + |b|)s\n(1 + |\u03be|)s+1 .\nThe proof is complete.\nLemma 2.3. Given any w \u2208 Sd\u22121 \u2126 , b \u2208 R, consider the extension\nHw,b(x) := \u03c3 ( wTx + b ) \u03c7 ( wTx ) .\nThen, for any \u03b4 \u2208 (0, 1), we have \u222b\nR\n(1 + \u2016\u03be\u2016\u2126) s\u2212\u03b4 \u2223\u2223H\u0302w,b(\u03be) \u2223\u2223d\u03be .s \u03b4\u22121(1 + |b|)s . (2.8)\nProof. By Lemmas 2.1 and 2.2, we have \u222b\nR\n(1 + \u2016\u03be\u2016\u2126) s\u2212\u03b4 \u2223\u2223H\u0302w,b(\u03be) \u2223\u2223\n= \u222b\nR\n(1 + |v|)s\u2212\u03b4 \u2223\u2223h\u0302\u03c3,b(v) \u2223\u2223dv\n.s\n\u222b\nR\n(1 + |v|)s\u2212\u03b4 (1 + |b|)s\n(1 + |v|)s+1 dv\n.s (1 + |b|) s \u222b\nR\n1\n(1 + |v|)1+\u03b4 dv .s\n(1 + |b|)s\n\u03b4 .\nThe proof is complete.\nThe proof of Theorem 1.1. We are now ready to prove the main theorem. For any f\u2208Bs(\u2126)\nand \u03b5 > 0, there exists \u03c1\u01eb \u2208 P(R \u00d7 S d\u22121 \u2126 \u00d7 R) such that\nf (x) = \u222b\na\u03c3(wT x + b)d\u03c1\u01eb(a, w, b), \u2200 x \u2208 \u2126, \u222b\n|a|(1 + |b|)s d\u03c1\u01eb(a, w, b) \u2264 \u2016 f\u2016Bs(\u2126) + \u01eb,\nwhere we have used the positive homogeneity of ReLUs and set w \u2208 Sd\u22121 \u2126 . Let\nfe(x) = \u222b a\u03c3(wT x + b)\u03c7(wT x)d\u03c1\u01eb(a, w, b) = \u222b aHw,b(x)d\u03c1\u01eb(a, w, b), \u2200 x \u2208 R d,\nwhere Hw,b : R d 7\u2192 R is the extension of \u03c3w,b defined in Lemma 2.3. Then, fe is an extension of f and satisfies\nf\u0302e(\u03be) = \u222b aH\u0302w,b(\u03be)d\u03c1\u01eb(a, w, b).\nAccording to Lemma 2.3, we have \u222b\nRd (1 + \u2016\u03be\u2016\u2126)\ns\u2212\u03b4 \u2223\u2223 f\u0302e(\u03be) \u2223\u2223 d\u03be\n\u2264 \u222b |a|\n(\u222b\nRd (1 + \u2016\u03be\u2016\u2126)\ns\u2212\u03b4 \u2223\u2223H\u0302w,b(\u03be) \u2223\u2223d\u03be ) d\u03c1\u01eb(a, w, b)\n.s\n\u222b |a| (1 + |b|)s\n\u03b4 d\u03c1\u01eb(a, w, b) \u2264\n1 \u03b4 (\u2016 f\u2016Bs(\u2126) + \u01eb).\nBy the definition of spectral Barron norm, it follows that\n\u2016 f\u2016Fs\u2212\u03b4(\u2126) .s \u03b4 \u22121(\u2016 f\u2016Bs(\u2126) + \u01eb).\nTaking \u01eb \u2192 0 completes the proof. The converse direction follows from [12, 24, 26]."
        },
        {
            "heading": "2.2 Proof of Proposition 1.1",
            "text": "Proof. Notice that f (\u00b7) can be exactly represented as a two-layer neural network for x \u2208 [\u22121, 1]:\nf (x) = \u03c3(1)\u2212 \u03c3(x)\u2212 \u03c3(\u2212x).\nHence, f is a Barron function and obviously, \u2016 f\u2016B1(\u2126) \u2264 3. What remains is to show that\n\u222b (1 + |\u03be|) \u2223\u2223 f\u0302e(\u03be) \u2223\u2223d\u03be = \u221e\nholds for any extension fe. Suppose, to the contrary, that there exists an extension fe such that \u222b\n(1 + |\u03be|) \u2223\u2223 f\u0302e(\u03be) \u2223\u2223 d\u03be < \u221e.\nThen f\u0302e d\u03be represents a finite measure over R d and fe is continuous in \u2126. By the Fourier inverse theorem, we have\nf (x) = \u222b ei\u03bex f\u0302e(\u03be)dx, \u2200 x \u2208 \u2126.\nFor any x \u2208 (\u22121/2, 0) \u222a (0, 1/2) and sufficiently small \u03b4,\nf (x + \u03b4)\u2212 f (x) \u03b4 = \u222b ei\u03bex ei\u03be\u03b4 \u2212 1 \u03b4 f\u0302e(\u03be)d\u03be. (2.9)\nThe integrand on the right side of (2.9) is bounded by |\u03be|| f\u0302e(\u03be)|, which is integrable by the assumption. Consequently, by the dominated convergence theorem, for x \u2208 (\u22121/2, 0) \u222a (0, 1/2), we have\nf \u2032(x) = lim \u03b4\u21920\nf (x + \u03b4)\u2212 f (x) \u03b4 = \u222b ei\u03bex lim \u03b4\u21920 ei\u03be\u03b4 \u2212 1 \u03b4 f\u0302e(\u03be)d\u03be = \u222b i\u03beei\u03bex f\u0302e(\u03be)d\u03be.\nAgain, by dominated convergence theorem and taking x \u2192 0,\nlim x\u21920 f \u2032(x) = lim x\u21920\n\u222b i\u03beei\u03bex f\u0302e(\u03be)d\u03be = \u222b lim x\u21920 i\u03beei\u03bex f\u0302e(\u03be)d\u03be = i \u222b \u03be f\u0302e(\u03be)d\u03be.\nThis contradicts the fact that limx\u21920 f \u2032(x) does not exist."
        },
        {
            "heading": "3 Concluding remark",
            "text": "In this paper, we establish a continuous embedding for Barron-type spaces over compact domains. Crucially, the embedding constants do not depend on the input dimension,\nimplying that the embedding is effective in high dimensions. We thus establish a more unifying perspective for understanding the high-dimensional approximation of two-layer neural networks. This embedding result has potential implications for the analysis of approximating solutions of high-dimensional PDEs with two-layer neural networks [4, 9, 16].\nFor future work, it is promising to extend our embedding result to the case of s \u2208 (0, \u221e) as discussed in Remark 2.1. Additionally, our proof heavily relies on the positive homogeneity property of the ReLUs activation function. It would be interesting to extend our analysis to Barron spaces associated with general activation functions [14]."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Professor Weinan E and Dr. Jihong Long for helpful discussions and to anonymous reviewers for detailed and constructive feedback."
        }
    ],
    "title": "Embedding Inequalities for Barron-Type Spaces",
    "year": 2023
}