{
    "abstractText": "Thanks to the impressive progress of large-scale visionlanguage pretraining, recent recognition models can classify arbitrary objects in a zero-shot and open-set manner, with a surprisingly high accuracy. However, translating this success to semantic segmentation is not trivial, because this dense prediction task requires not only accurate semantic understanding but also fine shape delineation and existing vision-language models are trained with image-level language descriptions. To bridge this gap, we pursue shapeaware zero-shot semantic segmentation in this study. Inspired by classical spectral methods in the image segmentation literature, we propose to leverage the eigen vectors of Laplacian matrices constructed with self-supervised pixelwise features to promote shape-awareness. Despite that this simple and effective technique does not make use of the masks of seen classes at all, we demonstrate that it outperforms a state-of-the-art shape-aware formulation that aligns ground truth and predicted edges during training. We also delve into the performance gains achieved on different datasets using different backbones and draw several interesting and conclusive observations: the benefits of promoting shape-awareness highly relates to mask compactness and language embedding locality. Finally, our method sets new state-of-the-art performance for zero-shot semantic segmentation on both Pascal and COCO, with significant margins. Code and models will be accessed at SAZS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinyu Liu"
        },
        {
            "affiliations": [],
            "name": "Beiwen Tian"
        },
        {
            "affiliations": [],
            "name": "Zhen Wang"
        },
        {
            "affiliations": [],
            "name": "Rui Wang"
        },
        {
            "affiliations": [],
            "name": "Kehua Sheng"
        },
        {
            "affiliations": [],
            "name": "Bo Zhang"
        },
        {
            "affiliations": [],
            "name": "Hao Zhao"
        },
        {
            "affiliations": [],
            "name": "Guyue Zhou"
        }
    ],
    "id": "SP:83d7638ee028c2c7635b563851e00348ba687b29",
    "references": [
        {
            "authors": [
                "Ya\u011fiz Aksoy",
                "Tae-Hyun Oh",
                "Sylvain Paris",
                "Marc Pollefeys",
                "Wojciech Matusik"
            ],
            "title": "Semantic soft segmentation",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2018
        },
        {
            "authors": [
                "Donghyeon Baek",
                "Youngmin Oh",
                "Bumsub Ham"
            ],
            "title": "Exploiting a joint embedding space for generalized zero-shot semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Shubhankar Borse",
                "Ying Wang",
                "Yizhe Zhang",
                "Fatih Porikli"
            ],
            "title": "Inverseform: A loss function for structured boundary-aware segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Maxime Bucher",
                "Tuan-Hung Vu",
                "Matthieu Cord",
                "Patrick P\u00e9rez"
            ],
            "title": "Zero-shot semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Qifeng Chen",
                "Dingzeyu Li",
                "Chi-Keung Tang"
            ],
            "title": "Knn matting",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Xiaoxue Chen",
                "Tianyu Liu",
                "Hao Zhao",
                "Guyue Zhou",
                "Ya-Qin Zhang"
            ],
            "title": "Cerberus transformer: Joint semantic, affordance and attribute parsing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Mark Everingham",
                "SM Eslami",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes challenge: A retrospective",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Weibo Gao",
                "Qi Liu",
                "Zhenya Huang",
                "Yu Yin",
                "Haoyang Bi",
                "Mu-Chun Wang",
                "Jianhui Ma",
                "Shijin Wang",
                "Yu Su"
            ],
            "title": "Rcd: Relation map driven cognitive diagnosis for intelligent education systems",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Golnaz Ghiasi",
                "Xiuye Gu",
                "Yin Cui",
                "Tsung-Yi Lin"
            ],
            "title": "Open-vocabulary image segmentation",
            "venue": "arXiv preprint arXiv:2112.12143,",
            "year": 2021
        },
        {
            "authors": [
                "Zhangxuan Gu",
                "Siyuan Zhou",
                "Li Niu",
                "Zihan Zhao",
                "Liqing Zhang"
            ],
            "title": "Context-aware feature generation for zeroshot semantic segmentation",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Bharath Hariharan",
                "Pablo Arbel\u00e1ez",
                "Lubomir Bourdev",
                "Subhransu Maji",
                "Jitendra Malik"
            ],
            "title": "Semantic contours from inverse detectors",
            "venue": "In 2011 international conference on computer vision,",
            "year": 2011
        },
        {
            "authors": [
                "Ping Hu",
                "Stan Sclaroff",
                "Kate Saenko"
            ],
            "title": "Uncertainty-aware learning for zero-shot semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Joel Janai",
                "Fatma G\u00fcney",
                "Aseem Behl",
                "Andreas Geiger"
            ],
            "title": "Computer vision for autonomous vehicles: Problems, datasets and state of the art",
            "venue": "Foundations and Trends\u00ae in Computer Graphics and Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bu Jin",
                "Xinyu Liu",
                "Yupeng Zheng",
                "Pengfei Li",
                "Hao Zhao",
                "Tong Zhang",
                "Yuhang Zheng",
                "Guyue Zhou",
                "Jingjing Liu"
            ],
            "title": "Adapt: Action-aware driving caption transformer",
            "venue": "arXiv preprint arXiv:2302.00673,",
            "year": 2023
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Mannat Singh",
                "Yann LeCun",
                "Gabriel Synnaeve",
                "Ishan Misra",
                "Nicolas Carion"
            ],
            "title": "Mdetrmodulated detection for end-to-end multi-modal understanding",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Naoki Kato",
                "Toshihiko Yamasaki",
                "Kiyoharu Aizawa"
            ],
            "title": "Zero-shot semantic segmentation via variational mapping",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Weicheng Kuo",
                "Anelia Angelova",
                "Jitendra Malik",
                "Tsung-Yi Lin"
            ],
            "title": "Shapemask: Learning to segment novel objects by refining shape priors",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Anat Levin",
                "Dani Lischinski",
                "Yair Weiss"
            ],
            "title": "A closed-form solution to natural image matting",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2007
        },
        {
            "authors": [
                "Boyi Li",
                "Kilian Q Weinberger",
                "Serge Belongie",
                "Vladlen Koltun",
                "Ren\u00e9 Ranftl"
            ],
            "title": "Language-driven semantic segmentation",
            "venue": "arXiv preprint arXiv:2201.03546,",
            "year": 2022
        },
        {
            "authors": [
                "Linjie Li",
                "Yen-Chun Chen",
                "Yu Cheng",
                "Zhe Gan",
                "Licheng Yu",
                "Jingjing Liu"
            ],
            "title": "Hero: Hierarchical encoder for video+ language omni-representation pre-training",
            "venue": "arXiv preprint arXiv:2005.00200,",
            "year": 2020
        },
        {
            "authors": [
                "Pengfei Li",
                "Beiwen Tian",
                "Yongliang Shi",
                "Xiaoxue Chen",
                "Hao Zhao",
                "Guyue Zhou",
                "Ya-Qin Zhang"
            ],
            "title": "Toist: Task oriented instance segmentation transformer with noun-pronoun distillation",
            "venue": "arXiv preprint arXiv:2210.10775,",
            "year": 2022
        },
        {
            "authors": [
                "Peike Li",
                "Yunchao Wei",
                "Yi Yang"
            ],
            "title": "Consistent structural relation learning for zero-shot segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Li",
                "Xiaoxue Chen",
                "Hao Zhao",
                "Jiangtao Gong",
                "Guyue Zhou",
                "Federico Rossano",
                "Yixin Zhu"
            ],
            "title": "Understanding embodied reference with touch-line transformer",
            "venue": "arXiv preprint arXiv:2210.05668,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Sifei Liu",
                "Shalini De Mello",
                "Jinwei Gu",
                "Guangyu Zhong",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Learning affinity via spatial propagation networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xiangbin Liu",
                "Liping Song",
                "Shuai Liu",
                "Yudong Zhang"
            ],
            "title": "A review of deep-learning-based medical image segmentation methods. Sustainability",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Fengmao Lv",
                "Haiyang Liu",
                "Yichen Wang",
                "Jiayi Zhao",
                "Guowu Yang"
            ],
            "title": "Learning unbiased zero-shot semantic segmentation networks via transductive transfer",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Christian Rupprecht",
                "Iro Laina",
                "Andrea Vedaldi"
            ],
            "title": "Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "George A Miller"
            ],
            "title": "Wordnet: a lexical database for english",
            "venue": "Communications of the ACM,",
            "year": 1995
        },
        {
            "authors": [
                "Juhong Min",
                "Dahyun Kang",
                "Minsu Cho"
            ],
            "title": "Hypercorrelation squeeze for few-shot segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Khoi Nguyen",
                "Sinisa Todorovic"
            ],
            "title": "Feature weighting and boosting for few-shot segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Giuseppe Pastore",
                "Fabio Cermelli",
                "Yongqin Xian",
                "Massimiliano Mancini",
                "Zeynep Akata",
                "Barbara Caputo"
            ],
            "title": "A closer look at self-training for zero-label semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),",
            "year": 2014
        },
        {
            "authors": [
                "Trung Pham",
                "Thanh-Toan Do",
                "Gustavo Carneiro",
                "Ian Reid"
            ],
            "title": "Bayesian semantic instance segmentation in open set world",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Alexey Bochkovskiy",
                "Vladlen Koltun"
            ],
            "title": "Vision transformers for dense prediction",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Guangyi Chen",
                "Yansong Tang",
                "Zheng Zhu",
                "Guan Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Denseclip: Language-guided dense prediction with contextaware prompting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "Unet: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical image computing and computer-assisted intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Alexander Schick",
                "Mika Fischer",
                "Rainer Stiefelhagen"
            ],
            "title": "Measuring and evaluating the compactness of superpixels",
            "venue": "In Proceedings of the 21st international conference on pattern recognition",
            "year": 2012
        },
        {
            "authors": [
                "Jianjun Shen",
                "Siyi Lu",
                "Ruize Qu",
                "Hao Zhao",
                "Yu Zhang",
                "An Chang",
                "Li Zhang",
                "Wei Fu",
                "Zhipeng Zhang"
            ],
            "title": "Measuring distance from lowest boundary of rectal tumor to anal verge on ct images using pyramid attention pooling transformer",
            "venue": "Computers in Biology and Medicine,",
            "year": 2023
        },
        {
            "authors": [
                "Jianbo Shi",
                "Jitendra Malik"
            ],
            "title": "Normalized cuts and image segmentation",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "Towaki Takikawa",
                "David Acuna",
                "Varun Jampani",
                "Sanja Fidler"
            ],
            "title": "Gated-scnn: Gated shape cnns for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Beiwen Tian",
                "Liyi Luo",
                "Hao Zhao",
                "Guyue Zhou"
            ],
            "title": "Vibus: Data-efficient 3d scene parsing with viewpoint bottleneck and uncertainty-spectrum modeling",
            "venue": "ISPRS Journal of Photogrammetry and Remote Sensing,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuotao Tian",
                "Hengshuang Zhao",
                "Michelle Shu",
                "Zhicheng Yang",
                "Ruiyu Li",
                "Jiaya Jia"
            ],
            "title": "Prior guided feature enrichment network for few-shot segmentation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Wang",
                "Xudong Zhang",
                "Yutao Hu",
                "Yandan Yang",
                "Xianbin Cao",
                "Xiantong Zhen"
            ],
            "title": "Few-shot semantic segmentation with democratic attention networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Jingdong Wang",
                "Ke Sun",
                "Tianheng Cheng",
                "Borui Jiang",
                "Chaorui Deng",
                "Yang Zhao",
                "Dong Liu",
                "Yadong Mu",
                "Mingkui Tan",
                "Xinggang Wang"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yongqin Xian",
                "Subhabrata Choudhury",
                "Yang He",
                "Bernt Schiele",
                "Zeynep Akata"
            ],
            "title": "Semantic projection network for zero-and few-label semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mengde Xu",
                "Zheng Zhang",
                "Fangyun Wei",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Xiang Bai"
            ],
            "title": "A simple baseline for zeroshot semantic segmentation with pre-trained vision-language model",
            "venue": "arXiv preprint arXiv:2112.14757,",
            "year": 2021
        },
        {
            "authors": [
                "Fisher Yu",
                "Vladlen Koltun",
                "Thomas Funkhouser"
            ],
            "title": "Dilated residual networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yuhui Yuan",
                "Xilin Chen",
                "Jingdong Wang"
            ],
            "title": "Objectcontextual representations for semantic segmentation",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Pengju Zhang",
                "Yihong Wu",
                "Jiagang Zhu"
            ],
            "title": "Semi-global shape-aware network",
            "venue": "arXiv preprint arXiv:2012.09372,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhao",
                "Ming Lu",
                "Anbang Yao",
                "Yiwen Guo",
                "Yurong Chen",
                "Li Zhang"
            ],
            "title": "Pointly-supervised scene parsing with uncertainty mixture",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2020
        },
        {
            "authors": [
                "Hang Zhao",
                "Xavier Puig",
                "Bolei Zhou",
                "Sanja Fidler",
                "Antonio Torralba"
            ],
            "title": "Open vocabulary scene parsing",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yupeng Zheng",
                "Chengliang Zhong",
                "Pengfei Li",
                "Huan-ang Gao",
                "Yuhang Zheng",
                "Bu Jin",
                "Ling Wang",
                "Hao Zhao",
                "Guyue Zhou",
                "Qichao Zhang"
            ],
            "title": "Steps: Joint self-supervised nighttime image enhancement and depth estimation",
            "venue": "arXiv preprint arXiv:2302.01334,",
            "year": 2023
        },
        {
            "authors": [
                "Leisheng Zhong",
                "Yu Zhang",
                "Hao Zhao",
                "An Chang",
                "Wenhao Xiang",
                "Shunli Zhang",
                "Li Zhang"
            ],
            "title": "Seeing through the occluders: Robust monocular 6-dof object pose tracking via model-guided video object segmentation",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Semantic segmentation has been an established research area for some time now, which aims to predict the categories of an input image in a pixel-wise manner. In real-world applications including autonomous driving [18], medical diagnosis [32,47] and robot vision and navigation [9,64], an accurate semantic segmentation module provides a pixel-wise\nunderstanding of the input image and is crucial for subsequent tasks (like decision making or treatment selection).\nDespite that significant progress has been made in the field of semantic segmentation [6,7,33,50,53,55,58,60,62], most existing methods focus on the closed-set setting in which dense prediction is performed on the same set of categories in training and testing time. Thus, methods that are trained and perform well in the closed-set setting may fail when applied to the open world, as pixels of unseen objects in the open world are likely to be assigned categories that are seen during training, causing catastrophic consequences in safety-critical applications such as autonomous driving [63]. Straightforward solutions include fine-tuning or retraining the existing neural networks, but it is impractical to enumerate unlimited unseen categories during retraining, let along large quantities of time and efforts needed.\nMore recent works [4, 15, 25, 28, 41] address this issue by shifting to the zero-shot setting, in which the methods are evaluated with semantic categories that are unseen during training. While large-scale pre-trained visual-language models such as CLIP [42] or ALIGN [19] shed light on the\nar X\niv :2\n30 4.\n08 49\n1v 1\n[ cs\n.C V\n] 1\n7 A\npr 2\npotential of solving zero-shot tasks with priors contained in large-scale pre-trained model, how to perform dense prediction task in this setting is still under-explored. One recent approach by Li et.al. [25] closes the gap by leveraging the shared embedding space for languages and images, but fails to effectively segment regions with fine shape delineation. If the segmented shape of the target object is not accurate, it will be a big safety hazard in practical applications, such as in autonomous driving.\nInspired by the classical spectral methods and their intrinsic capability of enhancing shapeawareness, we propose a novel Shape-Aware Zero-Shot semantic segmentation framework (SAZS) to address the task of zero-shot semantic segmentation. Firstly, the framework enforces vision-language alignment on the training set using known categories, which exploits rich language priors in the largescale pre-trained vision-language model CLIP [42]. Meanwhile, the framework also jointly enforces the boundary of predicted semantic regions to be aligned with that of the ground truth regions.\nLastly, we leverage the eigenvectors of Laplacian of affinity matrices that is constructed by features learned in a self-supervised manner, to decompose inputs into eigensegments. They are then fused with learning-based predictions from the trained model. The fusion outputs are taken as the final predictions of the framework.\nAs illustrated in Fig. 1, compared with [25], the predictions of our approach are better aligned with the shapes of objects.\nWe also demonstrate the effectiveness of our approach with elaborate experiments on PASCAL-5i and COCO-20i, the results of which show that our method outperforms former state-of-the-arts [4, 25, 37, 38, 52, 54] by large margins. By examining a) the correlation between shape compactness of target object and IoU and b) the correlation between the language embedding locality and IoU, we discover the large impacts on the performance brought by the distribution of language anchors and object shapes. Via extensive analyses, we demonstrate the effectiveness and generalization of SAZS framework\u2019s shape perception for segmenting semantic categories in the open world."
        },
        {
            "heading": "2. Related works",
            "text": ""
        },
        {
            "heading": "2.1. Zero-Shot Semantic Segmentation",
            "text": "The main goal of the zero-shot semantic segmentation task(ZSS) is to perform pixel-wise predictions for objects that are unseen during training. Recent works on ZSS have seen two main branches: the generative methods and the discriminative methods.\nThe generative methods [4, 15, 28] produce synthesized features for unseen categories.\nZS3Net [4] utilizes a generative model to create a visual\nrepresentation of objects that were not present in the training data. This is achieved by leveraging pre-trained word embeddings. CaGNet [15] highlights the impact of contextual information on pixel-level features through a network learning to generate specific contextual pixel-level features. In CSRI [28], constraints are introduced to the generation of unseen visual features by exploiting the structural relationships between seen and unseen categories.\nAs for the discriminative methods, SPNet [54] leverages similarities between known categories to transfer learned representations to other unknown categories. Baek et al. [2] employ visual and semantic encoders to learn a joint embedding space with the semantic encoder converting the semantic features into semantic prototypes. Naoki et al. [22] introduce variational mapping by projecting the class embeddings from the semantic to the visual space. Lv et al. [34] present a transductive approach using target images to mitigate the prediction bias towards seen categories. LSeg [25] proposes a language-driven ZSS model, mapping pixels and names of labels into a shared embedding space for pixelwise dense prediction.\nThough much pioneering efforts have been spent, the dense prediction task requires fine shape delineation while most existing vision-language models are trained with image-level language descriptions. How to effectively address these problems is the focus of our work."
        },
        {
            "heading": "2.2. Shape-aware Segmentation",
            "text": "Shape awareness is beneficial to dense prediction tasks. Most of the semantic segmentation methods [6, 33, 45, 53] cannot preserve object shapes since they only focus on feature discriminativeness but ignore proximity between central and other positions.\nMeanwhile, SGSNet [59] takes a hierarchical approach to aggregating the global context when modeling longrange dependencies, considering feature similarity and proximity to preserve object shapes. ShapeMask [23] refines the coarse shapes into instance-level masks. The shape priors provide powerful clues for prediction. Gated-SCNN [49] proposes a two-stream architecture for semantic segmentation that explicitly captures shape information as a separate processing branch. The key point is to enable the interactive flow of information between the two networks, allowing the shape stream to focus on learning and processing of edge information. Liu et al. [31] construct the spatial propagation networks for learning the affinity matrix. The affinity matrix allows a tractable modeling of the dense, global pairwise relationships of pixels."
        },
        {
            "heading": "2.3. Spectral Methods for Segmentation",
            "text": "Among different segmentation schemes, spectral methods for clustering employ the eigenvectors of a matrix derived from the distance between points, which have been\nsuccessfully used in many applications.\nShi et al. [48] regard image segmentation as a graph partitioning problem, which proposes a novel global criterion, the normalized cut, to segment the image. Soft segmentations [1] are generated automatically by fusing high-level and low-level image features in a graph structure. The purpose of constructing this graph is to enable the corresponding Laplacian matrix and its eigenvectors to reveal semantic objects and soft transitions between objects. In our work, we utilize the eigensegments obtained by selfsupervised spectral decomposition with the network outputs as the framework\u2019s predictions to avoid the bias of the learning-based model on the training set and further improve shape-awareness."
        },
        {
            "heading": "2.4. Vision-Language Modeling",
            "text": "An extensive group of works have investigated the zeroshot semantic segmentation task. The key idea behind many of these works is to exploit priors encoded in pretrained word embeddings to generalize to unseen classes and achieve dense predictions [4, 13, 15, 17, 20, 22, 25, 27\u201329, 39, 54, 61], such as word2vec [36], GloVe [40] or BERT [10]. CLIP [42] has recently demonstrated impressive zero-shot generalization in various image-level classification tasks. As a result, several works have since exploited the vision-language embedding space learned by CLIP [42] to enhance dense prediction capabilities [25, 44, 56]. CLIP develops contrastive learning with a large-capacity language and visual feature encoder to train extremely robust models for zero-shot image classification. But the performance of large-scale pre-trained vision encoders transferred\nto pixel-level classification work is unsatisfactory. Unfortunately, the direct utilization of the extracted image-level vision-language features ignores the discrepancy between image-level and the pixle-level dense predicition task, the latter of which is the focus of our work. According to our study, the shape-aware prior and supervision can bridge this discrepancy and get more accurate segmentation results."
        },
        {
            "heading": "3. Methods",
            "text": "The goal of zero-shot semantic segmentation is to extend semantic segmentation task to the unseen categories other than those in the training datasets. One potential approach to introduce extra priors is to leverage pre-trained vision-language models, yet most of these models focus on the image-level prediction and fail to transfer to dense prediction tasks.\nTo this end, we propose a novel method named ShapeAware Zero-Shot Semantic Segmentation (SAZS).\nThis approach leverages the rich language priors contained in the pre-trained CLIP [42] model, while also exploiting the proximity between local regions to perform the boundary detection task with constraints. Meanwhile, we utilize spectral decomposition of self-supervised visual features to improve our approach\u2019s sensitivity to shape, and integrate this with pixel-wise prediction.\nThe overall pipeline of our methods is depicted in Fig. 2. The input image is first transformed by an image encoder into pixel-wise embeddings, which are then aligned with precomputed text embeddings obtained by the text encoder of pre-trained CLIP model (Part A in Fig. 2). Meanwhile, an extra head in the image encoder is used to predict the boundaries in patches, which are optimized towards the ground-truth edges obtained from segmentation ground truths (Part B in Fig. 2). In addition, we further exploit proximity of local regions during inference by decomposing the image by spectral analysis and fusing the output eigensegments with class-agnostic segmentation results (Part C in Fig. 2).\nIn the following section, we first formally define the addressed task and introduce the notations in Sec. 3.1. Then we describe the loss designs for the vision-language alignment and boundary prediction in Sec. 3.2 and Sec. 3.3, respectively. The inference pipeline involving spectral decomposition of the proposed affinity matrix is introduced in Sec. 3.4."
        },
        {
            "heading": "3.1. Task Definition",
            "text": "Following HSNet [37], we denote the training set by Dtrain = {(I,M,S)} and testing set by Dtest = {(I,M,U)}, where I \u2208 RH\u00d7W\u00d73 and M \u2208 RH\u00d7W\u00d7C denote an input image and the corresponding ground-truth semantic mask with digit encoding. S denotes the set of K\npotential labels in I , while U denotes the set of unseen categories during testing. The two sets are strictly exclusive in our setting (i.e., S \u2229 U = \u2205).\nBefore inferencing on Dtest targeting U , the model is trained on Dtrain with ground-truth labels from S. This means the categories in the test set are never seen during training, making the task formulated in a zero-shot setting. Once the model is well-trained, it is expected to generalize to unseen categories and achieve high performance for dense prediction of target objects in the open world."
        },
        {
            "heading": "3.2. Pixel-wise Vision-Language Alignment",
            "text": "Comparing distances between pixel features and different text anchor features in the shared feature space is a straightforward approach for zero-shot semantic segmentation. However, while the pioneer work CLIP [42] introduces a shared feature space for visual and text inputs, the image-level CLIP visual encoder is infeasible for dense prediction tasks since fine details in images, as well as the correlation between pixels, are lost. In this section, we describe our approach to address this issue by optimizing a dense visual encoder separate of CLIP and enforcing the pixel-wise output features towards the text anchors in the CLIP feature space during training.\nVisual Encoder We employ Dilated residual networks (DRN) [57] and Dense Prediction Transformers (DPT) [43] to encoder images into pixel-level embeddings. More specifically, an input image of size H \u00d7W \u00d7 3 is first processed with standard augmentation to H\u0303 \u00d7 W\u0303 \u00d7 3 and then passed as input to the visual encoder, resulting in a feature map FV \u2208 RH\u0303\u00d7W\u0303\u00d7D, where D is the feature size in DPT.\nText Encoder While most concurrent methods use digit labels (e.g., 0, 1, 2) to represent categories, we take embeddings of the category names (e.g. \u201dairplane\u201d, \u201dcat\u201d ) as the anchors of feature space. These embeddings are obtained with the CLIP text encoder. Specifically, we adopt the pre-trained CLIP text encoder to map the names of K categories from S into CLIP feature space as the anchor features FT \u2208 RK\u00d7D, which is later used as targets for optimization.\nNote that, the visual features FV and the text features FT have the same dimension D.\nVision-Language Alignment To enforce visionlanguage alignment, the distances between pixels and corresponding semantic category should be minimized while the distances between pixels and other categories should be maximized. Under the assumption that pixelwise vision and language features are embedded in the same feature space, we leverage the cosine similarity \u27e8\u00b7, \u00b7\u27e9\nas the quantitative distance metric between features and propose the alignment loss as the sum of cross entropy losses over seen classes of all pixels:\nLalign = H\u0303,W\u0303\u2211 i,j\n( \u2212 log e\n\u27e8FV [i,j],FT [kij ]\u27e9\u2211|S| k\u2032=1 e \u27e8FV [i,j],FT [k\u2032]\u27e9\n) (1)\nIn Eq. 1, FV [i, j] denotes pixel visual feature at position (i, j), FT [k] denotes k-th text anchor features and kij denotes index of ground-truth category of pixel at (i, j)."
        },
        {
            "heading": "3.3. Shape Constraint",
            "text": "Since CLIP is trained on an image-level task, simply leveraging the priors in the CLIP feature space may be insufficient for dense prediction tasks. To address this issue, we introduce boundary detection as a constraint task, so that the visual encoder is able to aggregate finer information contained in images. Inspired by InverseForm [3], we address this constraint task by optimizing the affine transformation between ground-truth edges and edges in feature maps towards identity transformation matrix.\nMore specifically, as shown in Fig. 2, we extract middlelayer features of the visual encoders and split them into patches. On the one hand, the ground truth edges within the patches are obtained by applying Sobel operator on ground truth semantic masks. On the other hand, the feature patches are processed by a boundary head. Then, we calculate the affine transform matrix \u03b8\u0302i for the i-th patch between ground-truth edges and processed feature patches with a pre-trained MLP. Note that, this MLP is trained in advance with edge masks and not optimized during our method\u2019s training. We optimize this affine transform matrix towards identity matrix by:\nLshape = 1\nT T\u2211 i=1 \u2223\u2223\u2223\u03b8\u0302i \u2212 I\u2223\u2223\u2223 F\n(2)\nwhere T denotes the number of patches and |\u00b7| denotes Frobenius norm.\nFurthermore, we directly calculate the binary cross entropy loss Lbce between the predicted edge masks of the whole image and corresponding ground truths to further optimize the performance of boundary detection.\nAfter jointly training on the task of boundary detection, the visual encoder is enabled to collect and leverage shape priors in the input images. Ablation studies detailed later show that shape awareness introduced by Lshape and Lbce brings about notable improvements.\nFinally, the overall loss to optimize during training is:\nL = Lalign + \u03bb1Lshape + \u03bb2Lbce (3)\nwhere \u03bb1 and \u03bb2 are loss weights."
        },
        {
            "heading": "3.4. Self-supervised Spectral Decomposition",
            "text": "We seek to decompose the input images into eigensegments with clear boundaries in an unsupervised manner, and then fuse these eigensegments with the predictions of the neural networks in the fusion module in Fig. 2 .\nThe derivation of affinity matrix is the key to spectral decomposition. Following Melas-Kyriazi et al. [35], we first leverage the features f from the attention block of the last layer of a pre-trained self-supervised transformer (i.e., DINO [5]). The affinity between pixel i and j is defined as:\nZsem(i, j) = fi \u00b7 fTj (4) Note that, the self-supervised transformer is only used during inference and its weights are not optimized.\nWhile the affinities derived from transformer features are rich in semantic information, the low-level proximity including color similarity and spatial distance is missing. Inspired by image matting [8, 24], we first transfrom the input image into the HSV color space: X(i) = (cos(h), sin(h), s, v, x, y)i, where h, s, v are the respective HSV coordinates and (x, y) are the spatial coordinates of pixel i. Then, the affinity between pixels is defined as\nZshape(i, j) = 1\u2212 \u2225X(i)\u2212X(j)\u22252, j \u2208 KNN(i) (5)\nwhere \u2225 \u00b7 \u22252 denotes 2-norm. The overall affinity matrix is defined as the weighted sum of the two:\nZ(i, j) = Zsem + \u03bb \u00b7 Zshape (6)\nWith the affinity matrix, we now can compute the eigenvectors of the Laplacian L of the affinity matrix, which are used to decompose the image into multiple eigensegments."
        },
        {
            "heading": "3.5. Inference",
            "text": "Given an image for inference, we first encode the phrases of the categories using the pre-trained text encoder CLIP and obtain textual features FT \u2208 RC\u00d7D for C categories, each of which is represented by a D-dimension embedding. Then we leverage the trained visual encoder to obtain the visual feature map FV \u2208 RH\u0303\u00d7W\u0303\u00d7D. The final logits F\u0302ij = FV(i, j) \u00b7 FTT are calculated as the cosine similarities between the visual feature map and textual features. In the mean time, we employ the pre-trained DINO to extract semantic features in an unsupervised manner and calculate the top K spectral eigensegments Ek (K = 5 in our implementation). The final prediction results are generated by the fusion module, which selects from the sets of predictions according to the maximal IoU (denoted as \u03a6FUSE) of the Ek and argmax F\u0302ij .\nPredij = \u03a6FUSE ( Ek, argmax F\u0302ij ) k \u2208 {0, 1, \u00b7 \u00b7 \u00b7K}\n(7)"
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Datasets",
            "text": "We extensively evaluate our method on two datasets dedicated for the task of zero-shot semantic segmentation: PASCAL-5i [12] and COCO-20i [30]. Built upon PASCAL VOC 2012 [12] and augmented by SBD [16], PASCAL5i contains 20 categories which are further divided into 4 folds denoted by 50, 51, 52 and 53. Each image is annotated with 5 categories within each fold. Similarly, based on MS COCO [30], COCO-20i is a more challenging dataset with 80 categories divided into four folds denoted by 200, 201, 202 and 203, and each of the four folds contains 20 categories. Of the four folds in the two datasets, one is used for evaluation (i.e., the target fold) while the other three are used for training. In the following section, we denote each experiment setup by the target fold.\nFollowing prior literature on zero-shot semantic segmentation, we adopt mean intersection over union (mIoU) and foreground-background IoU (FBIoU) as the evaluation metrics. Specifically, mIoU is the average of IoUs of the categories in target fold and FBIoU is the average of foreground IoU and background IoU."
        },
        {
            "heading": "4.2. Implementation Details",
            "text": "In our experiments, we employ the pre-trained CLIPViT-B/32 as the text encoder. Background or unknown category is regarded as \u201dothers\u201d when mapped from text to CLIP features. The visual encoder is implemented by DRN [57] or DPT [43] with ViT [11] as the backbone. When training on the task of boundary detection, each feature map for the shape boundary and the corresponding ground truth are splitted into 3 \u00d7 6 patches. Each patch pair is then fed into the MLP in Part B of Fig. 2 to calculate the affine transformation matrix.\nDuring training, the network is optimized by an SGD optimizer with a momentum of 0.95 and a learning rate of 5 \u00d7 10\u22125 decayed by a polynomial scheduler. With ViT as the backbone of visual encoder, the training process finishes within 5 epochs on 4 NVIDIA Tesla V100 GPUs with a batch size of 6."
        },
        {
            "heading": "4.3. Results",
            "text": "The proposed method SAZS has been evaluated on the PASCAL-5i and COCO-20i datasets under zero-shot settings, alongside several baselines for comparison. The performances are reported in Tab. 1. With DRN as the visual encoder backbone, our method achieves large margins over the strong baseline LSeg [25], with mIoU improved by 6.1% and 4.8% on PASCAL-5i and COCO-20i respectively. Our model also outperforms LSeg [25] by large margins with the ViT backbone underlying DPT, with mIoU improved by 7.2% and 11.2%. The performance enhance-\nments of SAZS remain consistent across different visual encoder choices, highlighting its effectiveness.\nIn addition, we conduct cross-dataset validation by training on the COCO-20i dataset and testing on PASCAL-5i. As shown in Table 2, our method outperforms OpenSeg [14] and LSeg+ [25] in zero-shot dense prediction tasks with clear margins. It is worth noting that all three methods are trained on a larger semantic segmentation dataset (COCO20i). These performance gaps demonstrate the generalization ability of our shape-aware training framework across datasets. We also provide qualitative results for the proposed method SAZS. in Fig. 3 and Fig. 4. In these figures, we illustrate the predictions of SAZS with and without shape awareness on COCO-20i and PASCAL-5i respectively, showing its ability to make precise predictions on both seen and unseen categories."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "To further demonstrate the effectiveness of design choices in our approach, we perform detailed ablation studies by evaluating our method with or without shape constraint during training as well as the fusion of network predictions with eigensegments. Results on PASCAL-5i are reported in Tab. 3 and results on COCO-20i are reported in Tab. 4 and Tab. 5.\nEffects of Shape-awareness The motivation for auxiliary constraint Lshape is to learn the shape priors of images contained in the target boundaries. We observe that without training on the constraint task of boundary detection, the performances of the proposed method tend to decline. Specifically as reported in Tab. 4 and Tab. 5, the mIoU of SAZS drops by 1.4% and by 1.5% with ViT and DRN backbone on COCO-20i when training without Lshape. The performance gaps clearly indicate the significant role played by shape-awareness in the proposed SAZS framework.\nEffect of Fusion with Spectral Eigensegments We also demonstrate the importance of fusing with spectral eigensegments during inference. Without the fusion module, the mIoU dramatically decreases by 7.0% on PASCAL5i and by 6.2% (ViT backbone) and 8.6% (DRN backbone) on COCO-20i, as reported in Tab. 3, Tab. 4 and Tab. 5. These large margins indicate that eigensegments obtained by spectral decomposition of the affinity matrices largely suppress the bias on the training dataset and seen categories.\n4.5. Ablation of Zsem and Zshape We conduct an ablation experiment on the PASCAL-5i dataset to investigate the effects of Zsem and Zshape in our fusion module. As shown in Table 6, both Zsem and Zshape contribute to improved segmentation performance, but using Zsem alone yields better results than using Zshape alone.\nModel Backbone external dataset target dataset PASCAL-5i\nLSeg ViT-L % !(seen classes) 52.3\nSPNet ResNet % !(seen classes) 18.3 ZS3Net ResNet % !(seen classes) 38.3\nLSeg ResNet % !(seen classes) 47.4\nLSeg+ ResNet COCO % 59.0 OpenSeg [14] ResNet COCO % 60.0\nOurs DRN COCO % 62.7\nTable 2. The cross dataset mIoU results of our model and previous SOTA methods on PASCAL-5i.\nModel Fusion Lshape 50 51 52 53 mIoU\nSAZS ! ! 62.7 64.3 60.6 50.2 59.4 SAZS ! 63.1 62.4 59.0 49.2 58.4 SAZS ! 59.7 63.4 44.3 42.2 52.4 SAZS 59.2 61.9 43.8 41.9 51.7\nLSeg [25] 61.3 63.6 43.1 41.0 52.3\nTable 4. Ablation study on COCO-20i (ViT backbone)\nModel Fusion Lshape 200 201 202 203 mIoU\nSAZS ! ! 34.2 36.5 34.6 35.6 35.2 SAZS ! 33.7 38.2 33.4 35.5 35.2 SAZS ! 28.4 27.6 25.4 25.1 26.6 SAZS 24.2 28.5 24.4 23.3 25.1\nLSeg [25] 22.1 25.1 24.9 21.6 23.4\nTable 5. Ablation study on COCO-20i (DRN backbone)\nWhile the segmentation performance obtained by combining the two is slightly higher than that achieved with Zsem alone, using both requires fine-tuning the hyper-parameter \u03bb, which can be unstable and requires additional effort."
        },
        {
            "heading": "4.6. Effects of Target Shape Compactness",
            "text": "In this section, we investigate the impact of shapeawareness on the performance of SAZS in zero-shot seman-\nCup*\nCat\nKeyboard\nPizza*\nPerson\nDining Table\nInput Image Ours Ground TruthOurs w/o shape awareness\nFigure 3. Qualitative results of COCO-20i. The first and last columns are the input images and the corresponding ground-truth semantic masks for different categories. The second and the third columns are the predictions by SAZS without and with shape awareness, respectively. * denotes unseen categories during training phase) and yellow boxes mark poorly segmented regions.\nModel external dataset Zshape Zsem PASCAL-5i\nSAZS COCO 58.4 SAZS COCO ! 58.6 SAZS COCO ! 62.7\nTable 6. Impact of Zshape and Zsem Of fusion module (in the cross-dataset setting of Tabel. 2).\ntic segmentation by analyzing the correlation between the mean intersection-over-union (mIoU) and the shape compactness (CO) of each category. Shape compactness, as proposed by Schick and others in 2012 [46], is a commonly used metric for measuring the similarity of superpixels to circles, which we use to characterize the shapes of objects in the input images.\nFor each input image in the PASCAL-5i dataset, we\ncollected the compactness (CO) metric of the ground-truth mask for the target object to describe its shape. We then calculated the variance of CO for each object category and plotted the results in Fig. 5a. The sample points in the figure represent the IoU and CO variance of each category, with the color indicating the experiment settings. This analysis aims to investigate how shape-awareness affects the SAZS\u2019s performance on zero-shot semantic segmentation.\nThe results demonstrate a negative correlation between the IoU and the CO variance of a specific category (with a Pearson correlation coefficient of r > 0.7 and P <= 0.001), and the degree of correlation is higher for SAZS than for the baselines. These findings strongly suggest that shape-awareness can improve segmentation performance when objects have more stable shapes, and that SAZS is more able to leverage shape information compared to the other baselines. The experiments were conducted on the PASCAL-5i dataset."
        },
        {
            "heading": "4.7. Effects of the Language Embedding Locality",
            "text": "Intuitively, distribution of language anchors in the latent feature space may largely affect vision-language alignment and thus the performance of the proposed method. Inspired by recent research [21,26,42], we model the distribution by the embedding locality of anchors which is defined by the mean value and standard deviation of euclidean distances in the feature space between one anchor and all other anchors.\nFor each category in each setting of experiments, we calculate its embedding locality and report the results collected on PASCAL-5i in Fig. 5b. The coordinates of sample points\nrepresent the IoU and the embedding locality of the corresponding category while the colors of the sample points denote the experiment settings.\nAccording to the plotted results, we observe a negative linear correlation (with Pearson correlation coefficient r > 0.5 and P \u2264 0.05) between the embedding locality mean and IoU of a certain category, indicating that the closer a category is in the feature space to the others, the easier it is for the visual and text embeddings which leads to higher performances. Also, the degree of relevance of SAZS is the highest among all methods which implies that SAZS is able to better align pixel-wise visual embeddings towards the text anchors in the CLIP feature space."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we present a novel framework for ShapeAware Zero-Shot semantic segmentation (abbreviated as SAZS). The proposed framework leverages the rich priors contained in the feature space of a large-scale pretrained visual-language model, while also incorporating shape-awareness through joint training on a boundary detection constraint task. This is necessary to compensate for the absence of fine-grained features in the feature space. In addition, self-supervised spectral decomposition is used to obtain feature vectors for images, which are fused with the network predictions as prior knowledge to enhance the model\u2019s ability to perceive shapes.\nExtensive experiments demonstrate the state-of-the-art performance of SAZS with significant margins over previous methods. Correlation analysis further highlights the impact of shape compactness and distribution of language anchors on the framework\u2019s performance. Our approach effectively exploits the shape of targets and feature priors, showing the highest correlation among all compared methods and proving the novelty of the shape-aware design."
        },
        {
            "heading": "6. Appendix",
            "text": ""
        },
        {
            "heading": "6.1. Per-Category Evaluation",
            "text": "Table 8 and Table 9 demonstrate our per-category zero-shot semantic mIoU results on COCO-20i [30] and PASCAL-5i [12], respectively. The mIoU of our proposed SAZS network structure demonstrates superior performance compared to the baseline. We also observed that certain categories often appear as small regions, such as ties, or have complicated internal structures, such as people. For these categories, textual feature guidance alone cannot provide sufficient information for semantic parsing, and the baseline without shape-awareness cannot effectively segment objects under self-supervision. However, when using a SAZS model, the mIoUs of these categories better align with the shapes of the objects than the baseline, which confirms that shape awareness indeed improves zero-shot learning."
        },
        {
            "heading": "6.2. Speed and Complexity",
            "text": "We conducted experiments to analyze the per-episode inference time and floating point operations per second (FLOPs) in order to demonstrate the complexity of our proposed approach. The results are summarized in Table 7 for the COCO-20i dataset. Compared to the baseline model without the fusion module, SAZS had slower inference time, but significantly better performance. Although losses, including Lshape, in our model did not add any time cost during inference, there is still potential for optimization in terms of inference speed and model complexity, which is exactly the direction for our future research."
        },
        {
            "heading": "6.3. More Qualitative Results",
            "text": "In this section, we provide additional qualitative results of our model with a ViT-L backbone on PASCAL-5i and COCO-20i datasets to demonstrate the model\u2019s ability to perform semantic segmentation on previously unseen categories. Fig. 7 showcases the results on PASCAL-5i, where all categories are unseen in their respective fold. The images presented in the figure vary in their content and complexity, and we display different visualizations of SAZS to demonstrate its versatility.\nThe results presented in Fig.7 demonstrate the efficacy of SAZS in distinguishing the target semantic objects, such as bicycle, dining table, and TV monitor, from distractors like person, dog, and keyboard. Furthermore, in Fig. 7, SAZS accurately segments multiple instances of the target object, as is the case with the train, potted plant, and TV monitor.\nOverall, these results demonstrate the robustness of our model in semantically segmenting novel categories with high precision and accuracy, even in complex scenes. In this section, we present the visualization of COCO-20i in Fig.8, which includes both seen and unseen categories. We se-\nlected 20 scene and attribute labels with different semantics and multiple objects to demonstrate the versatility of SAZS. Despite the presence of noise and complexity in the scenes, SAZS accurately recognizes novel categories that are small and intricate, as illustrated by the examples of broccoli, pottedplant, and skis in Fig.8.\nIn particular, in the second image of lines 2 and 3 of Figure 1, where multiple species appear in the scene with complex shapes, SAZS performs sharp object edge segmentation to accurately distinguish broccoli, carrots, and hot dogs.\nGiven the diversity of the presented scenes, we believe that SAZS is precise enough to be applied to various scenarios, including open scenario understanding and intelligent service robots."
        },
        {
            "heading": "6.4. More Scatter Analysis",
            "text": "Fig.6 presents additional scatterplots and corresponding Pearson analysis results for the pascal dataset. The sample points in Fig.6 represent the IoU and CO variance of each model, and they demonstrate a negative correlation. The results indicate that our approaches, particularly those that incorporate shape-awareness, can increase the correlation between per-category IoU results and CO. For example, in the third column of Fig. 6, the Pearson correlation coefficient r of SAZS is 0.13 higher than that of the baseline."
        }
    ],
    "title": "Delving into Shape-aware Zero-shot Semantic Segmentation",
    "year": 2023
}