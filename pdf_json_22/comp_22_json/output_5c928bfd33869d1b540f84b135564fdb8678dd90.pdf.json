{
    "abstractText": "Visual inertial odometry and SLAM algorithms are widely used in various fields, such as service robots, drones, and autonomous vehicles. Most of the SLAM algorithms are based on assumption that landmarks are static. However, in the real-world, various dynamic objects exist, and they degrade the pose estimation accuracy. In addition, temporarily static objects, which are static during observation but move when they are out of sight, trigger false positive loop closings. To overcome these problems, we propose a novel visual-inertial SLAM framework, called DynaVINS, which is robust against both dynamic objects and temporarily static objects. In our framework, we first present a robust bundle adjustment that could reject the features from dynamic objects by leveraging pose priors estimated by the IMU preintegration. Then, a keyframe grouping and a multi-hypothesis-based constraints grouping methods are proposed to reduce the effect of temporarily static objects in the loop closing. Subsequently, we evaluated our method in a public dataset that contains numerous dynamic objects. Finally, the experimental results corroborate that our DynaVINS has promising performance compared with other state-of-the-art methods by successfully rejecting the effect of dynamic and temporarily static objects. Our code is available at https://github.com/url-kaist/dynaVINS.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seungwon Song"
        },
        {
            "affiliations": [],
            "name": "Hyungtae Lim"
        },
        {
            "affiliations": [],
            "name": "Alex Junho Lee"
        },
        {
            "affiliations": [],
            "name": "Hyun Myung"
        }
    ],
    "id": "SP:c6707cc312a1d45a00022708453a56ad4db71aab",
    "references": [
        {
            "authors": [
                "S. Jung",
                "D. Choi",
                "S. Song",
                "H. Myung"
            ],
            "title": "Bridge inspection using unmanned aerial vehicle based on HG-SLAM: Hierarchical graph-based SLAM",
            "venue": "Remote Sens., vol. 12, no. 18, pp. 3022\u20133041, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Mur-Artal",
                "J.M.M. Montiel",
                "J.D. Tard\u00f3s"
            ],
            "title": "ORB- SLAM: A versatile and accurate monocular SLAM system",
            "venue": "IEEE Trans. Robot., vol. 31, no. 5, pp. 1147\u20131163, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Qin",
                "P. Li",
                "S. Shen"
            ],
            "title": "VINS-Mono: A robust and versatile monocular visual-inertial state estimator",
            "venue": "IEEE Trans. Robot., vol. 34, no. 4, pp. 1004\u20131020, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A.I. Mourikis",
                "S.I. Roumeliotis"
            ],
            "title": "A multi-state constraint Kalman filter for vision-aided inertial navigation",
            "venue": "Proc. IEEE Int. Conf. Robot. Autom., 2007, pp. 3565\u20133572.",
            "year": 2007
        },
        {
            "authors": [
                "C. Campos",
                "R. Elvira",
                "J.J.G. Rodr\u00edguez",
                "J.M.M. Montiel",
                "J.D. Tard\u00f3s"
            ],
            "title": "ORB-SLAM3: An accurate open-source library for visual, visual\u2013inertial, and multimap SLAM",
            "venue": "IEEE Trans. Robot., vol. 37, no. 6, pp. 1874\u20131890, 2021.",
            "year": 1874
        },
        {
            "authors": [
                "S. Leutenegger",
                "S. Lynen",
                "M. Bosse",
                "R. Siegwart",
                "P. Furgale"
            ],
            "title": "Keyframe-based visual-inertial odometry using nonlinear optimization",
            "venue": "Int. J. Robot. Res., vol. 34, no. 3, pp. 314\u2013 334, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Bloesch",
                "S. Omari",
                "M. Hutter",
                "R. Siegwart"
            ],
            "title": "Robust visual inertial odometry using a direct EKF-based approach",
            "venue": "Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2015, pp. 298\u2013304.",
            "year": 2015
        },
        {
            "authors": [
                "K. Minoda",
                "F. Schilling",
                "V. W\u00fcest",
                "D. Floreano",
                "T. Yairi"
            ],
            "title": "VIODE: A simulated dataset to address the challenges of visual-inertial odometry in dynamic environments",
            "venue": "IEEE Robot. Autom. Lett., vol. 6, no. 2, pp. 1343\u20131350, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Bescos",
                "J.M. F\u00e1cil",
                "J. Civera",
                "J. Neira"
            ],
            "title": "DynaSLAM: Tracking, mapping, and inpainting in dynamic scenes",
            "venue": "IEEE Robot. Autom. Lett., vol. 3, no. 4, pp. 4076\u20134083, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Fan",
                "H. Han",
                "Y. Tang",
                "T. Zhi"
            ],
            "title": "Dynamic objects elimination in SLAM based on image fusion",
            "venue": "Pattern Recognit. Lett., vol. 127, pp. 191\u2013201, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Canovas",
                "M. Rombaut",
                "A. N\u00e8gre",
                "D. Pellerin",
                "S. Olympieff"
            ],
            "title": "Speed and memory efficient dense RGB-D SLAM in dynamic scenes",
            "venue": "Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2020, pp. 4996\u20135001.",
            "year": 2020
        },
        {
            "authors": [
                "R. Long",
                "C. Rauch",
                "T. Zhang",
                "V. Ivan",
                "S. Vijayakumar"
            ],
            "title": "RigidFusion: Robot localisation and mapping in environments with large dynamic rigid objects",
            "venue": "IEEE Robot. Autom. Lett., vol. 6, no. 2, pp. 3703\u20133710, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Bescos",
                "C. Campos",
                "J.D. Tard\u00f3s",
                "J. Neira"
            ],
            "title": "DynaSLAM II: Tightly-coupled multi-object tracking and SLAM",
            "venue": "IEEE Robot. Autom. Lett., vol. 6, no. 3, pp. 5191\u2013 5198, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Qiu",
                "T. Qin",
                "W. Gao",
                "S. Shen"
            ],
            "title": "Tracking 3-D motion of dynamic objects using monocular visual-inertial sensing",
            "venue": "IEEE Trans. Robot., vol. 35, no. 4, pp. 799\u2013816, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "E. Olson",
                "P. Agarwal"
            ],
            "title": "Inference on networks of mixtures for robust robot mapping",
            "venue": "Int. J. Robot. Res., vol. 32, no. 7, pp. 826\u2013840, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "N. S\u00fcnderhauf",
                "P. Protzel"
            ],
            "title": "Switchable constraints for robust pose graph SLAM",
            "venue": "Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2012, pp. 1879\u20131884.",
            "year": 2012
        },
        {
            "authors": [
                "H. Yang",
                "P. Antonante",
                "V. Tzoumas",
                "L. Carlone"
            ],
            "title": "Graduated non-convexity for robust spatial perception: From nonminimal solvers to global outlier rejection",
            "venue": "IEEE Robot. Autom. Lett., vol. 5, no. 2, pp. 1127\u20131134, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Q.-Y. Zhou",
                "J. Park",
                "V. Koltun"
            ],
            "title": "Fast global registration",
            "venue": "Eur. Conf. Comput. Vis. Springer, 2016, pp. 766\u2013782.",
            "year": 2016
        },
        {
            "authors": [
                "S. Song",
                "H. Lim",
                "S. Jung",
                "H. Myung"
            ],
            "title": "G2P-SLAM: Generalized RGB-D SLAM framework for mobile robots in low-dynamic environments",
            "venue": "IEEE Access, vol. 10, pp. 21 370\u201321 383, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Xiao",
                "J. Wang",
                "X. Qiu",
                "Z. Rong",
                "X. Zou"
            ],
            "title": "Dynamic- SLAM: Semantic monocular visual localization and mapping based on deep learning in dynamic environment",
            "venue": "Robot. Auton. Syst., vol. 117, pp. 1\u201316, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.J. Black",
                "A. Rangarajan"
            ],
            "title": "On the unification of line processes, outlier rejection, and robust statistics with applications in early vision",
            "venue": "Int. J. Comput. Vis., vol. 19, no. 1, pp. 57\u201391, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "P.J. Huber"
            ],
            "title": "Robust estimation of a location parameter",
            "venue": "Breakthroughs in Statistics. Springer, 1992, pp. 492\u2013518.",
            "year": 1992
        },
        {
            "authors": [
                "P. Babin",
                "P. Gigu\u00e8re",
                "F. Pomerleau"
            ],
            "title": "Analysis of robust functions for registration algorithms",
            "venue": "Proc. IEEE Int. Conf. Robot. Autom., 2019, pp. 1451\u20131457.",
            "year": 2019
        },
        {
            "authors": [
                "S. Geman",
                "D.E. McClure",
                "D. Geman"
            ],
            "title": "A nonlinear filter for film restoration and other problems in image processing",
            "venue": "CVGIP: Graph. Models Image Process., vol. 54, no. 4, pp. 281\u2013289, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "D. Galvez-L\u00f3pez",
                "J.D. Tardos"
            ],
            "title": "Bags of binary words for fast place recognition in image sequences",
            "venue": "IEEE Trans. Robot., vol. 28, no. 5, pp. 1188\u20131197, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Z. Zhang",
                "D. Scaramuzza"
            ],
            "title": "A tutorial on quantitative trajectory evaluation for visual(-inertial) odometry",
            "venue": "Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2018, pp. 7244\u20137251.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nSimultaneous localization and mapping (SLAM) algorithms have been widely exploited in various robotic applications that require precise positioning or navigation in environments where GPS signals are blocked. Various types of sensors have been used in SLAM algorithms [1]. In particular, visual sensors such as monocular cameras [2\u20134] and stereo cameras [5\u20137] are widely used because of their relatively low cost and weight with rich information.\nVarious visual SLAM methods have been studied for more than a decade. However, most researchers have assumed that landmarks are implicitly static; thus, many visual SLAM methods still have potential risks when interacting with realworld environments that contain various dynamic objects. Only recently, several studies focused on dealing with dynamic objects solely using visual sensors.\nMost of the studies [9\u201311] address the problems by detecting the regions of dynamic objects via depth clustering, feature reprojection, or deep learning. Moreover, some researchers incorporate the dynamics of the objects into the\n1Seungwon Song,1Hyungtae Lim and 1Hyun Myung are with School of Electrical Engineering at KAIST, Daejeon, 34141, Republic of Korea.{sswan55, shapelim,hmyung}@kaist.ac.kr\n2Alex Junho Lee is with the Department of Civil and Environmental Engineering at KAIST, Daejeon, 34141, Republic of Korea. alex_jhlee@kaist.ac.kr\n* Corresponding author: Prof. Hyun Myung This work was supported by the \u201cIndoor Robot Spatial AI Technology Developmen\u201d project funded by KT (KT award B210000715). The students are supported by the BK21 FOUR from the Ministry of Education (Republic of Korea).\noptimization framework [12\u201314]. However, geometry-based methods require accurate camera poses; hence they can only deal with limited fractions of dynamic objects. In addition, deep-learning-aided methods have the limitation of solely working for predefined objects.\nIn the meanwhile, visual-inertial SLAM (VI-SLAM) frameworks [3\u20137] have been proposed by integrating an inertial measurement unit (IMU) into the visual SLAM. Unlike the visual SLAMs, a motion prior from the IMU helps the VI-SLAM algorithms to tolerate scenes with dynamic objects to some degree. However, if the dominant dynamic objects occlude most of the view as shown in Fig. 1(b), the problem cannot be solved solely using the motion prior.\nIn addition, in real-world applications, temporarily static objects are static while being observed but in motion when they are not under observation. These objects may lead to a critical failure in the loop closure process due to false positives as shown in Fig. 1(c). To deal with temporarily static objects, robust back-end methods [15\u201318] are proposed to reduce the effect of the false positive loop closures in the optimization. However, since they focused on the instantaneous false positive loop closures, they cannot deal with the persistent false positive loop closures caused by the temporarily static objects.\nIn this study, to address the aforementioned problems, we propose a robust VI-SLAM framework, called DynaVINS, which is robust against dynamic and temporarily static objects. Our contributions are summarized as follows:\n\u2022 The robust VI-SLAM approach is proposed to handle\ndominant, undefined dynamic objects that cannot be solved solely by learning-based or vision-only methods. \u2022 A novel bundle adjustment (BA) pipeline is proposed\nfor simultaneously estimating camera poses and discarding the features from the dynamic objects that deviate significantly from the motion prior. \u2022 A robust global optimization with constraints grouped\ninto multiple hypotheses is proposed to reject persistent loop closures from the temporarily static objects.\nIn the remainder of this letter, we introduce the robust BA method for optimizing moving windows in Section III, methods for the robust global optimization in Section IV, and compare our proposed method with other state-of-theart (SOTA) methods in various environments in Section V."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "A. Visual-inertial SLAM\nAs mentioned earlier, to address the limitations of the visual SLAM framework, VI-SLAM algorithms have been recently proposed to correct the scale and camera poses by adopting the IMU. MSCKF [4] was proposed as an extended Kalman filter(EKF)-based VI-SLAM algorithm. ROVIO [7] also used an EKF, but proposed a fully robocentric and direct VI-SLAM framework running in real time.\nThere are other approaches using optimization. OKVIS [6] proposed a keyframe-based framework and fuses the IMU preintegration residual and the reprojection residual in an optimization. ORB-SLAM3 [5] used an ORB descriptor for the feature matching, and poses and feature positions are corrected through an optimization. VINS-Fusion [3], an extended version of VINS-Mono, supports a stereo camera and adopts a feature tracking, rather than a descriptor matching, which makes the algorithm faster and more robust.\nHowever, these VI-SLAM methods described above still have potential limitations in handling the dominant dynamic objects and the temporarily static objects."
        },
        {
            "heading": "B. Dynamic Objects Rejection in Visual and VI SLAM",
            "text": "Numerous researchers have proposed various methods to handle dynamic objects in visual and VI SLAM algorithms. Fan et al. [10] proposed a multi-view geometry-based method using an RGB-D camera. After obtaining camera poses by minimizing the reprojection error, the type of each feature point is determined as dynamic or static by the geometric relationship between the camera movement and the feature. Canovas et al. [11] proposed a similar method, but adopted a surfel, similar to a polygon, to enable a realtime performance by reducing the number of items to be computed. However, multi-view geometry-based algorithms assumed that the camera pose estimation is accurate enough, leading to the failure when the camera pose estimation is inaccurate owing to the dominant dynamic objects.\nOne of the solutions to this problem is to employ a wheel encoder. G2P-SLAM [19] rejected loop closure matching results with a high Mahalanobis distance from the estimated pose by the wheel odometry, which is invariant to the effect of dynamic and temporarily static objects. Despite the advantages of wheel encoder, these methods are highly dependent on the wheel encoder, limiting their own applicability.\nAnother feasible approach is to adopt deep learning networks to identify predefined dynamic objects. In the DynaSLAM [9], masked areas of the predefined dynamic objects using a deep learning network were eliminated and the remainder was determined via multi-view geometry. In the Dynamic SLAM [20], a compensation method was adopted to make up for missed detections in a few keyframes using sequential data. Although the deep learning methods can successfully discard the dynamic objects even if they are temporarily static, these methods are somewhat problematic for the following two reasons: a) the types of dynamic objects have to be predefined, and b) sometimes, only a part of the dynamic object is visible as shown in Fig. 1(b). For these reasons, the objects may not be detected occasionally.\nOn the other hand, methods for tracking a dynamic object\u2019s motion have been proposed. RigidFusion [12] assumed that only a single dynamic object is in the environment and estimated the motion of the dynamic object. Qiu et al. [14] combined a deep learning method and VINS-Mono [3] to track poses of the camera and object simultaneously. DynaSLAM II [13] identified dynamic objects, similar to DynaSLAM [9], then, within the BA factor graph, the poses of static features and the camera were estimated while estimating the motion of the dynamic objects simultaneously."
        },
        {
            "heading": "C. Robust Back-End",
            "text": "In the graph SLAM field, several researchers have attempted to discard incorrectly created constraints. For instance, max-mixture [15] employed a single integrated Bayesian framework to eliminate the incorrect loop closures, while switchable constraint [16] is proposed to adjust the weight of each constraint to eliminate false positive loop closures in the optimization. However, false-positive loop closures can be expected to be consistent and occur persistently by the temporarily static objects. These robust kernels are not appropriate to handling such persistent loop closures.\nOn the other hand, the Black-Rangarajan (B-R) duality [21] is proposed to unify robust estimation and outlier rejection process. Some methods [17, 18] utilize B-R duality in point cloud registration and pose graph optimization (PGO) to reduce the effect of false-positive matches even if they are dominant. These methods are useful for rejecting outliers in a PGO. However, repeatedly detected false-positive loop closures from similar objects are not considered. Moreover, B-R duality is not yet utilized in the BA of the VI-SLAM.\nTo address the aforementioned limitations, we improve the VI-SLAM to minimize the effect of the dynamic and temporarily static objects by adopting the B-R duality not only in the graph structure but also in the BA framework by reflecting the IMU prior and the feature tracking information."
        },
        {
            "heading": "III. ROBUST BUNDLE ADJUSTMENT",
            "text": ""
        },
        {
            "heading": "A. Notation",
            "text": "In this letter, the following notations are defined. The ith camera frame and the j-th tracked feature are denoted as Ci and fj , respectively. For two frames CA and CB , T A B \u2208 SE(3) denotes the pose of CA relative to CB . And the pose of CA in the world frame W can be denoted as T A W .\nB is a set of indices of the IMU preintegrations, and P is a set of visual pairs (i, j) where i corresponds to the frame Ci and j to the feature fj . Because the feature fj is tracked across multiple camera frames, different camera frames can contain the same feature fj . Thus, a set of indices of all tracked features in the current moving window is denoted as FP , and a set of indices of the camera frames that contain the feature fj is denoted as P(fj). In the visual-inertial optimization framework of the current sliding window, X represents the full state vector that contains sets of poses and velocities of the keyframes, biases of the IMU, i.e., acceleration and gyroscope biases, and estimated depth of the features as in [3]."
        },
        {
            "heading": "B. Conventional Bundle Adjustment",
            "text": "In the conventional visual-inertial state estimator [3], the\nvisual-inertial BA formulation is defined as follows:\nmin X\n{\n\u2225 rp \u2212 HpX \u2225 2 +\n\u2211\nk\u2208B\n\u2225 rI(z\u0302 bk bk+1 , X ) \u22252 P\nbk bk+1\n+ \u2211\n(i,j)\u2208P\n\u03c1H(\u2225 rP(z\u0302 Ci j , X ) \u2225\n2 P Ci j )\n   ,\n(1)\nwhere \u03c1H( \u00b7 ) denotes the Huber loss [22]; rp, rI , and rP represent residuals for marginalization, IMU, and visual reprojection measurements, respectively; z\u0302bkbk+1 and z\u0302 Ci j stand for observations of IMU and feature points; Hp denotes a measurement estimation matrix of the marginalization, and P denotes the covariance of each term. For convenience, rI(z\u0302 bk bk+1 , X ) and rP(z\u0302 Ci j , X ) are simplified as r k I and r j,i P , respectively.\nThe Huber loss does not work successfully once the ratio of outliers increases. This is because the Huber loss does not entirely reject the residuals from outliers [23]. On the other hand, the redescending M-estimators, such as GemanMcClure (GMC) [24], ignore the outliers perfectly once the residuals are over a specific range owing to their zerogradients. Unfortunately, this truncation triggers a problem that features considered as outliers would never become\ninliers even though the features are originated from static objects.\nTo address these problems, our BA method consists of two parts: a) a regularization factor that leverages the IMU preintegration and b) a momentum factor for considering the previous state of each weight to cover the case where the preintegration becomes temporarily inaccurate."
        },
        {
            "heading": "C. Regularization Factor",
            "text": "First, to reject the outlier features while robustly estimating the poses, we propose a novel loss term inspired by the B-R duality [21] as follows:\n\u03c1(wj , r j P ) = w2j r j P + \u03bbw\u03a6 2(wj), (2)\nwhere r j P\ndenotes \u2211\ni\u2208P(fj) \u2225 rj,i P \u2225\n2 for simplicity, wj \u2208\n[0, 1] denotes the weight corresponding to each feature fj , and fj with wj close to 1 is determined as a static feature; \u03bbw \u2208 R\n+ is a constant parameter; \u03a6(wj) denotes the regularization factor of the weight wj and is defined as follows:\n\u03a6(wj) = 1 \u2212 wj . (3)\nThen, \u03c1(wj , r j P\n) in (2) is adopted instead of the Huber norm in the visual reprojection term in (1). Hence, the BA formulation can be expressed as:\nmin X ,W\n{\n\u2225 rp \u2212 HpX \u2225 2 +\n\u2211\nk\u2208B\n\u2225 rkI \u2225 2 +\n\u2211\nj\u2208FP\n\u03c1(wj , r j\nP )\n}\n, (4)\nwhere W = \u00b6wj \u2663j \u2208 FP\u2662 represents the set of all weights. By adopting weight and regularization factor inspired by B-R duality, the influence of features with a high reprojection error compared to the estimated state can be reduced while maintaining the state estimation performance. The details will be covered in the remainder of this subsection.\n(4) is solved using an alternating optimization [21]. Because the current state X can be estimated from the IMU preintegration and the previously optimized state, unlike other methods [17, 18], W is updated first with the fixed X . Then, X is optimized with the fixed W .\nWhile optimizing W , all terms except weights are constants. Hence, the formulation for optimizing weights can be expressed as follows:\nmin W\n{ \u2211\nj\u2208FP\n\u03c1(wj , r j\nP )\n}\n. (5)\nBecause the weight wj is independent to each other, (5) can be optimized independently for each wj as follows:\nmin wj \u2208[0,1]\n   w 2 j   \u2211\ni\u2208P(fj )\n\u2225rj,i P\n\u2225 2\n\n + \u03bbw\u03a6 2(wj)\n   . (6)\nBecause the terms in (6) are in a quadratic form w.r.t. wj , the optimal wj can be derived as follows:\nwj = \u03bbw\nr j P + \u03bbw . (7)\nAs mentioned previously, the weights are first optimized based on the estimated state. Thus the weights of features with high reprojection errors start with small values. However, as shown in Fig. 3(a), the loss of the feature \u03c1(wj , r j P\n) is a convex function unless the weight is zero, and there is a non-zero gradient not only in the loss of an inlier feature but also in the loss of an outlier feature, which means that the new feature affects the BA regardless of the type at first.\nWhile the optimization step is repeated until the states and the weights are converged, the weights of the outlier features are lowered and their losses are more flattened. As a result, the losses of the outlier features approach zero-gradient and cannot affect the BA.\nAfter convergence, the weight can be expressed using the\nreprojection error as in (7). Thus the converged loss \u03c1\u0304(rj P\n) can be derived by applying (7) to (2) as follows:\n\u03c1\u0304(rj P\n) = \u03bbwr\nj P\n\u03bbw + r j\nP\n. (8)\nAs shown in Fig. 3(b), increasing \u03bbw affects \u03c1\u0304(r j P\n) in two directions: increasing the gradient value and convexity. By increasing the gradient value, the visual reprojection residuals affect the BA more than the marginalization and IMU preintegration residuals. And by increasing the convexity, some of the outlier features can affect the BA.\nTo sum up, the proposed factor benefits from both Huber loss and GMC by adjusting the weights in an adaptive way; our method efficiently filters out outliers, but does not entirely ignore outliers in the optimization at first as well."
        },
        {
            "heading": "D. Weight Momentum Factor",
            "text": "When the motion becomes aggressive, the IMU preintegration becomes imprecise, and thus the estimated state becomes inaccurate. In this case, the reprojection residuals of the features from the static objects become larger; hence, by the regularization factor, those features will be ignored in the BA process even though the previous weights were close to one.\nIf increasing \u03bbw to solve this problem, even the features with high reprojection residuals by dynamic objects are used. Therefore, the result of the BA will be inaccurate. Thus, increasing \u03bbw is not enough to cope this problem. To solve this issue, an additional factor, a weight momentum factor, is proposed to make the previously estimated feature weights unaffected by an aggressive motion.\nBecause the features are continuously tracked, each feature fj is optimized nj times with its previous weight w\u0304j . In order to make the current weight tend to remain at w\u0304j , and to increase the degree of the tendency as nj increases, the weight momentum factor \u03a8(wj) is designed as follows:\n\u03a8(wj) = nj(w\u0304j \u2212 wj). (9)\nThen, adding (9) to (2), the modified loss term can be\nderived as follows:\n\u03c1m(wj , r j P ) = w2j \u2211\ni\u2208P(fj)\n\u2225 rj,i P\n\u2225 2\n+ \u03bbw\u03a6 2(wj) + \u03bbm\u03a8 2(wj),\n(10)\nwhere \u03bbm \u2208 R + represents a constant parameter to adjust the effect of the momentum factor on the BA.\nIn summary, proposed robust BA can be illustrated as Fig. 4. The previous weights of the tracked features are used in the weight momentum factor, and the weights of all features in the current window are used in the regularization factor. As a result, the robust BA is expressed as follows:\nmin X ,W\n{\n\u2225 rp\u2212HpX \u2225 2 +\n\u2211\nk\u2208B\n\u2225 rkI \u2225 2+\n\u2211\nj\u2208FP\n\u03c1m(wj , r j\nP )\n}\n. (11)\n(11) can be solved by using the alternating optimization in the same way as (4). The alternating optimization is iterated until X and W are converged. Then, the converged loss \u03c1\u0304m(r j P ) can be derived. \u03c1\u0304m(r j P\n) w.r.t. w\u0304j and nj is shown in Fig. 3(c) and (d), respectively.\nAs shown in Fig. 3(c), if w\u0304 is low, the gradient of the loss is small even when r\nj P is close to 0. Thus, the\nfeatures presumably originated from dynamic objects don\u2019t\nhave much impact on the BA even if their reprojection errors are low in the current step.\nFurthermore, as shown in Fig. 3(d), if w\u0304j is zero, the gradient gets smaller as nj increases; hence the tracked outlier feature has less effect on the BA, and the longer it is tracked, the less it affects the BA.\nFor the stereo camera configuration, in addition to the reprojection on one camera, reprojections on the other camera in the same keyframe, rstereo P , or another keyframe, ranother P , exist. In that case, weights are also applied to the reprojection r another P because it is also affected by the movement of features, while rstereo P is invariant to the movement of features and is only adopted as the criterion for the depth estimation."
        },
        {
            "heading": "IV. SELECTIVE GLOBAL OPTIMIZATION",
            "text": "In the VIO framework, the drift is inevitably cumulative along the trajectory because the optimization is performed only within the moving window. Hence, a loop closure detection, e.g. using DBoW2 [25], is necessary to optimize all trajectories.\nIn a typical visual SLAM, all loop closures are exploited even if some of them are from temporarily static objects. Those false positive loop closures may lead to the failure of the SLAM framework. Moreover, features from the temporarily static objects and from the static objects may exist at the same keyframe. Therefore, in this section, we propose a method to eliminate the false positive loop closures while maintaining the true positive loop closures."
        },
        {
            "heading": "A. Keyframe Grouping",
            "text": "Unlike conventional methods that treat loop closures individually, in this study, loop closures from the same features are grouped, even if they are from different keyframes. As a result, only one weight per a group is used, allowing for faster optimization.\nAs shown in Fig. 5(a), before grouping the loop closures, adjacent keyframes that share at least a minimum number of tracked features have to be grouped. The group starting from the i-th camera frame Ci is defined as follows:\nGroup(Ci) = \u00b6Ck\u2663 \u2663F k i \u2663 \u2265 \u03b1, k \u2265 i\u2662, (12)\nwhere \u03b1 represents a minimum number of tracked features, and Fki represents the set of features tracked from Ci to Ck. For simplicity, Group(Ci) will be denoted as Gi hereinafter."
        },
        {
            "heading": "B. Multiple Hypotheses Clustering",
            "text": "After keyframes are grouped as in the previous subsection, DBoW2 is employed to identify the similar keyframe Cm with each keyframe Ck in the current group Gi starting from Ci (Ck \u2208 Gi and m < i). Note that Ck is skipped if there is no similar keyframe. After identifying up to three different m for k, a feature matching is conducted between Ck and these keyframes, and the relative pose T km can be obtained. Using T km, the estimated pose of Ck in the world frame, mT k W , can be obtained as follows:\nmT k W = T k m \u00b7 T m W , (13)\nwhere T mW represents the pose of Cm in the world frame. However, it is difficult to directly compute the similarity between the loop closures from different keyframes in the\nk,mT i W = T i k \u00b7 mT k W . (14)\nIf the features used for matchings are from the same object, the estimated T iW of the matchings will be located close to each other, even if Ck and Cm of the matchings are different. Hence, after calculating Euclidean distances between the loop closure\u2019s estimated T iW , the similar loop closures with the small Euclidean distance can be clustered as shown in Fig. 5(c).\nDepending on which loop closure cluster is selected, the trajectory result from the graph optimization varies. Therefore, each cluster can be called a hypothesis. To reduce the computational cost, top-two hypotheses were adopted by comparing the cardinality of the loop closures within the hypothesis. These two hypotheses of the current group Gi are denoted as H0i and H 1 i .\nHowever, it is not yet possible to distinguish between true or false positive hypotheses. Hence, the method for determining the true positive hypothesis among the candidate hypotheses will be described in the next section."
        },
        {
            "heading": "C. Selective Optimization for Constraint Groups",
            "text": "Most of the recent visual SLAM algorithms use a graph optimization. Let C, T , L, and W denote the sets of keyframes, poses, loop closures, and all weights, respectively. Then the graph optimization can be denoted as:\nmin T\n{ \u2211\ni\u2208C\n\u2225 r(T ii+1, T ) \u2225 2\nP T i+1\nT i\n\ufe38 \ufe37\ufe37 \ufe38\nlocal edge\n+ \u2211\n(j,k)\u2208L\n\u03c1H \u2225 r(T j k , T ) \u2225 2 PL\n\ufe38 \ufe37\ufe37 \ufe38\nloop closure edge\n}\n,\n(15)\nwhere T ii+1 represents the local pose between two adjacent keyframes Ci and Ci+1; T j k is the relative pose between Cj\nand Ck from the loop closure; P T i+1\nT i and PL denote the\ncovariance of the local pose and loop closure, respectively.\nFor the two hypotheses of group Gi, weights are denoted as w0i and w 1 i , a sum of the weights as wi, and the set of hypotheses as H. Using a similar procedure as in Section III.C, Black-Rangarajan duality is applied to (15) as follows:\nmin T ,W\n{ \u2211\ni\u2208C\n\u2225 r(T ii+1, T ) \u2225 2\nP T i+1\nT i\n+ \u2211\nHi\u2208H\n(( \u2211\n(j,k)\u2208H0 i\n\u2225 w0i\n\u2663H0i \u2663 r(T jk , T ) \u2225 2 PL )\n\ufe38 \ufe37\ufe37 \ufe38\nresidual for hypothesis 0\n+ ( \u2211\n(j,k)\u2208H1 i\n\u2225 w1i\n\u2663H1i \u2663 r(T jk , T ) \u2225 2 PL )\n\ufe38 \ufe37\ufe37 \ufe38\nresidual for hypothesis 1 (optional)\n+ \u03bbl\u03a6 2 l (wi)\n\ufe38 \ufe37\ufe37 \ufe38\nhypothesis regularization function\n) } ,\n(16) where \u03bbl \u2208 R + is a constant parameter. The regularization factor for the loop closure, \u03a6l, is defined as follows:\n\u03a6l(wi) = 1 \u2212 wi = 1 \u2212 (w 0 i + w 1 i ), (17)\nwhere w0i , w 1 i \u2208 [0, 1]. To ensure that the weights are not affected by the number of loop closures in the hypothesis, the weights are divided by the cardinality of each hypothesis.\nThen, (16) is optimized in the same manner as (11). Accordingly, only the hypothesis with a high weight is adopted in the optimization. In addition, all weights can be close to 0 when all hypotheses are false positives due to the multiple temporarily static objects. Hence, the failure caused by false positive hypotheses can be prevented.\nBecause keyframe poses are changed after the optimization, the hypothesis clustering in Section IV.B is conducted again for all groups for the next optimization."
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS",
            "text": "To evaluate the proposed algorithm, we compare ours with SOTA algorithms, namely, VINS-Fusion [3], ORBSLAM3 [5], and DynaSLAM [9]. Each algorithm is tested in a mono-inertial (-M-I) and a stereo-inertial (-S-I) mode. Note that an IMU is not used in DynaSLAM, so it is only tested in a stereo (-S) mode and compared with the -S-I mode of other algorithms. It could be somewhat unfair, but the comparison is conducted to stress the necessity for an IMU when dealing with dynamic environments."
        },
        {
            "heading": "A. Dataset",
            "text": "VIODE Dataset VIODE dataset [8] is a simulated dataset that contains lots of moving objects, such as cars or trucks, compared with conventional datasets. In addition, the dataset includes overall occlusion situations, where most parts of the image are occluded by dominant dynamic objects as shown in Fig. 1. Note that the sub-sequence name none to high means how many dynamic objects exist in the scene. Our Dataset Unfortunately, VIODE dataset does not contain harsh loop closing situations caused by temporarily static objects. Accordingly, we obtained our dataset with four sequences to evaluate our global optimization. First, Static sequence validates the dataset. In Dynamic follow sequence, a dominant dynamic object moves in front of the\ncamera. Next, in Temporal static sequence, the same object is observed from multiple locations. In other words, the object is static while being observed, and then it moves to a different position. Finally, in E-shape sequence, the camera moves along the shape of the letter E. The checkerboard is moved while not being observed, thus it will be observed at the three end-vertices of the E-shaped trajectory in the camera perspective, which triggers the false-positive loop closures. Note that the feature-rich checkerboard is used in the experiment to address the effect of false loop closures."
        },
        {
            "heading": "B. Error Metrics",
            "text": "The accuracy of the estimated trajectory from each algorithm is measured by Absolute Trajectory Error (ATE) [26], which directly measures the difference between points of the ground truth and the aligned estimated trajectory. In addition, for the VIODE dataset, the degradation rate [8], rd = ATEhigh/ATEnone, is calculated to determine the robustness of the algorithm."
        },
        {
            "heading": "C. Evaluation on the VIODE Dataset",
            "text": "First, the effects of the proposed factors on the BA time cost and accuracy are analyzed as shown in Table I. Ours with only the regularization factor has a better result than VINS-Fusion, but with the momentum factor together, it shows not only the outperforming result than VINS-Fusion, but also the less time due to a previous information. Moreover, although the BA time of ours was increased due to additional optimization steps, it is sufficient for high-level control of robots.\nAs shown in Table II and Fig. 6, the SOTA methods show precise pose estimation results in static environments. However, they struggle with the effect of dominant dynamic objects. In particular, even though DynaSLAM employs a semantic segmentation module, DynaSLAM tends to diverge or shows large ATE compared with other methods as the number of dynamic objects increases (from none to high). This performance degradation is due to the overall occlusion situations, leading to the failure of the semantic segmentation module and the absence of features from static objects.\nSimilarly, although ORB-SLAM3 tries to reject the frames with inaccurate features, it diverges when dominant dynamic objects exist in parking_lot mid, high and city_day high sequences. However, especially in parking_lot low sequence, there is only one vehicle that is far from the camera, and it occludes an unnecessary background environment. As a consequence, ORBSLAM3-S-I outperforms other algorithms.\nVINS-Fusion is less hindered by the dynamic objects because it tries to remove the features with an incorrectly\nTABLE II. Comparison with state-of-the-art methods (RMSE of ATE in [m]). *: Failure case (diverged), -M-I: Mono-inertial mode, -S: Stereo mode, -S-I: Stereo-inertial mode, SC: Switchable Constraints [16] Parameters for DynaVINS in VIODE: \u03bbw = 1.0, \u03bbm = 0.2 and in our dataset: \u03bbw = 1.0, \u03bbm = 1.0, \u03bbl = 1.0.\nMethod\nVIODE [8] Our dataset\ncity_day city_night parking_lot Static Dynamic\nfollow\nTemporal\nstatic E-shape none low mid high none low mid high none low mid high\nORB-SLAM3-M-I 1.940 0.857 4.486 * * * * * 0.147 0.175 0.145 0.194 0.379 1.374 0.775 * VINS-Fusion-M-I 0.210 0.182 0.560 0.510 0.328 0.371 0.457 0.464 0.102 0.138 0.707 1.135 0.080 0.463 0.414 0.727 VINS-Fusion-M-I with SC 0.091 0.736\nDynaVINS-M-I 0.224 0.167 0.154 0.364 0.189 0.181 0.184 0.256 0.097 0.120 0.118 0.149 0.048 0.141 0.051 0.107\nDynaSLAM-S 1.621 1.426 1.638 * 3.333 3.314 3.074 3.865 0.108 0.170 * * 0.081 1.017 0.467 0.937 ORB-SLAM3-S-I 0.302 0.419 0.217 * 0.709 0.895 1.693 3.006 0.148 0.067 * * 0.069 * 0.067 0.476 VINS-Fusion-S-I 0.150 0.203 0.234 0.373 0.317 0.507 0.494 0.828 0.121 0.121 0.212 0.278 0.029 0.383 0.229 0.711 VINS-Fusion-S-I with SC 0.034 0.686\nDynaVINS-S-I 0.171 0.178 0.091 0.148 0.213 0.182 0.201 0.198 0.049 0.042 0.064 0.042 0.032 0.038 0.025 0.029\nnone low mid high\nVIODE city day\n10!1\n100\n101\nA T E\n[m ]\nVINS-Fusion-M-I ORB-SLAM3-M-I DynaVINS-M-I (Ours)\n(a) Mono-inertial mode\nnone low mid high\nVIODE city day\n10!2\n10!1\n100\nA T E\n[m ]\nDynaSLAM-S VINS-Fusion-S-I ORB-SLAM3-S-I DynaVINS-S-I (Ours)\n(b) Stereo and stereo-inertial mode\nFig. 6. ATE results of state-of-the-art algorithms and ours on the city_day sequences of the VIODE dataset [8]. Note that the y-axis is expressed in logarithmic scale. Our algorithm shows promising performance with less performance degeneration compared with the other state-of-the-art methods.\nestimated depth (negative or far) after BA. However, those features have affected the BA before they are removed. As\na result, as the number of the features from dynamic objects increases, the trajectory error of VINS-Fusion gets higher.\nIn contrast, our proposed method shows promising performance in both mono-inertial and stereo-inertial modes. For example, in parking_lot high sequence as shown in Fig. 7(a)\u2013(b), ours performs stable pose estimation even when other algorithms are influenced by dynamic objects. Moreover, even though the number of dynamic objects increases, a performance degradation remains small compared to other methods in all scenes. This confirms that our method overcomes the problems caused by dynamic objects owing to our robust BA method, which is also supported by Table III. In other words, our proposed method successfully rejects all dynamic features by adjusting the weights in an adaptive way. Also, our method could be even robust against the overall occlusion situations, as shown in Fig. 1(b).\nInterestingly, our proposed robust BA method enables robustness against changes in illuminance by rejecting the inconsistent features (e.g., low weight features in dark area of Fig. 7(c)). Accordingly, our method shows remarkable performance compared with the SOTA methods in city_night scenes where not only dynamic objects exist, but also there is a lack of illuminance. Note that -M-I of ours has better result than -S-I. This is because the stereo reprojection, rstereo P , can be inaccurate in low-light conditions."
        },
        {
            "heading": "D. Evaluation on Our Dataset",
            "text": "In the static case, all algorithms have low ATE values. This sequence validates that our dataset is correctly obtained.\nHowever, in Dynamic follow, other algorithms tried to track the occluding object. Hence, not only failures of BA but also false-positive loop closures are triggered. Consequently, other algorithms except ours have higher ATEs.\nFurthermore, in Temporal static, ORB-SLAM3 and VINS-Fusion can eliminate the false-positive loop closure in the stereo-inertial case. However, in the mono-inertial case, due to an inaccurate depth estimation, they cannot reject the false-positive loop closures. Additionaly, VINS-Fusion with Switchable Constraints [16] can also reject the false-positive loop closures, but ours has a better performance as shown in Table II.\nFinally, in E-shape case, other algorithms fail to optimize the trajectory, as illustrated in Fig. 8(a), owing to the false-positive loop closures. Also VINS-Fusion with Switchable Constraints cannot reject the false-positive loop closures that are continuously generated. However, ours optimizes the weight of each hypothesis, not individual loop closures. Hence, false-positive loop closures are rejected in the optimization irrespective of the number of them, as illustrated in Fig. 8(b). Ours does not use any object-wise information from the image; hence the features from the same object can be divided into different hypotheses, as depicted in Fig. 1(c)."
        },
        {
            "heading": "VI. CONCLUSIONS",
            "text": "In this study, DynaVINS has been proposed, which is a robust visual-inertial SLAM framework based on the robust BA and the selective global optimization in dynamic environments. The experimental evidence corroborated that our algorithm works better than other algorithms in simulations and in actual environments with various dynamic objects. In future works, we plan to improve the speed and the performance. Moreover, we will adopt the concept of DynaVINS to the LiDAR-Visual-Inertial (LVI) SLAM framework."
        }
    ],
    "title": "DynaVINS: A Visual-Inertial SLAM for Dynamic Environments",
    "year": 2022
}