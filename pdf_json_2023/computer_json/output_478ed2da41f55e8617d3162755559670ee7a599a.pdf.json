{
    "abstractText": "We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing methods for depth from stereo treat different stereo frames independently, leading to temporally inconsistent depth predictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly diminishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate disparity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new benchmark dataset containing synthetic videos of people and animals in scanned environments, which provides complementary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves the quality of predictions of our proposed DynamicStereo as well as prior methods. Finally, it acts as a benchmark for consistent stereo methods. Project page: https://dynamic-stereo.github.io/",
    "authors": [
        {
            "affiliations": [],
            "name": "Nikita Karaev"
        },
        {
            "affiliations": [],
            "name": "Ignacio Rocco"
        },
        {
            "affiliations": [],
            "name": "Benjamin Graham"
        },
        {
            "affiliations": [],
            "name": "Natalia Neverova"
        },
        {
            "affiliations": [],
            "name": "Andrea Vedaldi"
        },
        {
            "affiliations": [],
            "name": "Christian Rupprecht"
        }
    ],
    "id": "SP:7cca6c0fc2496b7f086921320b2b8387bd1f8bd6",
    "references": [
        {
            "authors": [
                "Abhishek Badki",
                "Alejandro Troccoli",
                "Kihwan Kim",
                "Jan Kautz",
                "Pradeep Sen",
                "Orazio Gallo"
            ],
            "title": "Bi3d: Stereo depth estimation via binary classifications",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Ballas",
                "Li Yao",
                "Chris Pal",
                "Aaron C. Courville"
            ],
            "title": "Delving deeper into convolutional networks for learning video representations",
            "venue": "In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In Proceedings of the International Conference on Machine Learning (ICML), July 2021",
            "year": 2021
        },
        {
            "authors": [
                "Stan Birchfield",
                "Carlo Tomasi"
            ],
            "title": "Depth discontinuities by pixel-to-pixel stereo",
            "venue": "International Journal of Computer Vision,",
            "year": 1999
        },
        {
            "authors": [
                "D.J. Butler",
                "J. Wulff",
                "G.B. Stanley",
                "M.J. Black"
            ],
            "title": "A naturalistic open source movie for optical flow evaluation",
            "venue": "European Conf. on Computer Vision (ECCV), Part IV,",
            "year": 2012
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-toend object detection with transformers",
            "venue": "In European conference on computer vision. Springer,",
            "year": 2020
        },
        {
            "authors": [
                "Jia-Ren Chang",
                "Yong-Sheng Chen"
            ],
            "title": "Pyramid stereo matching network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Philipp Fischer",
                "Eddy Ilg",
                "Philip Hausser",
                "Caner Hazirbas",
                "Vladimir Golkov",
                "Patrick van der Smagt",
                "Daniel Cremers",
                "Thomas Brox"
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Philipp Fischer",
                "Eddy Ilg",
                "Philip Hausser",
                "Caner Hazirbas",
                "Vladimir Golkov",
                "Patrick van der Smagt",
                "Daniel Cremers",
                "Thomas Brox"
            ],
            "title": "FlowNet: Learning optical flow with convolutional networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoyang Guo",
                "Kai Yang",
                "Wukui Yang",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "title": "Group-wise correlation stereo network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Heiko Hirschm\u00fcller",
                "Peter R Innocent",
                "Jon Garibaldi"
            ],
            "title": "Real-time correlation-based stereo vision with reduced border errors",
            "venue": "International Journal of Computer Vision,",
            "year": 2002
        },
        {
            "authors": [
                "Alex Kendall",
                "Hayk Martirosyan",
                "Saumitro Dasgupta",
                "Peter Henry",
                "Ryan Kennedy",
                "Abraham Bachrach",
                "Adam Bry"
            ],
            "title": "End-to-end learning of geometry and context for deep stereo regression",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Vladimir Kolmogorov",
                "Ramin Zabih"
            ],
            "title": "Computing visual correspondence with occlusions using graph cuts",
            "venue": "In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001,",
            "year": 2001
        },
        {
            "authors": [
                "Johannes Kopf",
                "Xuejian Rong",
                "Jia-Bin Huang"
            ],
            "title": "Robust consistent video depth estimation",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Philipp Kr\u00e4henb\u00fchl"
            ],
            "title": "Free supervision from video games",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jiankun Li",
                "Peisen Wang",
                "Pengfei Xiong",
                "Tao Cai",
                "Ziwei Yan",
                "Lei Yang",
                "Jiangyu Liu",
                "Haoqiang Fan",
                "Shuaicheng Liu"
            ],
            "title": "Practical stereo matching via cascaded recurrent network with adaptive correlation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoshuo Li",
                "Xingtong Liu",
                "Nathan Drenkow",
                "Andy Ding",
                "Francis X. Creighton",
                "Russell H. Taylor",
                "Mathias Unberath"
            ],
            "title": "Revisiting stereo depth estimation from a sequenceto-sequence perspective with transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Zhaoshuo Li",
                "Wei Ye",
                "Dilin Wang",
                "Francis X Creighton",
                "Russell H Taylor",
                "Ganesh Venkatesh",
                "Mathias Unberath"
            ],
            "title": "Temporally consistent online depth estimation in dynamic scenes",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengqi Li",
                "Tali Dekel",
                "Forrester Cole",
                "Richard Tucker",
                "Noah Snavely",
                "Ce Liu",
                "William T Freeman"
            ],
            "title": "Learning the depths of moving people by watching frozen people",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Zhengfa Liang",
                "Yiliu Feng",
                "Yulan Guo",
                "Hengzhu Liu",
                "Wei Chen",
                "Linbo Qiao",
                "Li Zhou",
                "Jianfeng Zhang"
            ],
            "title": "Learning for disparity estimation through feature constancy",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lahav Lipson",
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "Raft-stereo: Multilevel recurrent field transforms for stereo matching",
            "venue": "In International Conference on 3D Vision (3DV),",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Xuan Luo",
                "Jia-Bin Huang",
                "Richard Szeliski",
                "Kevin Matzen",
                "Johannes Kopf"
            ],
            "title": "Consistent video depth estimation",
            "year": 2020
        },
        {
            "authors": [
                "N. Mayer",
                "E. Ilg",
                "P. H\u00e4usser",
                "P. Fischer",
                "D. Cremers",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",
            "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Nikolaus Mayer",
                "Eddy Ilg",
                "Philip Hausser",
                "Philipp Fischer",
                "Daniel Cremers",
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,",
            "year": 2016
        },
        {
            "authors": [
                "Moritz Menze",
                "Andreas Geiger"
            ],
            "title": "Object scene flow for autonomous vehicles",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Alexey Bochkovskiy",
                "Vladlen Koltun"
            ],
            "title": "Vision transformers for dense prediction",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Katrin Lasinger",
                "David Hafner",
                "Konrad Schindler",
                "Vladlen Koltun"
            ],
            "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Stephan R Richter",
                "Zeeshan Hayder",
                "Vladlen Koltun"
            ],
            "title": "Playing for benchmarks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Stephan R Richter",
                "Vibhav Vineet",
                "Stefan Roth",
                "Vladlen Koltun"
            ],
            "title": "Playing for data: Ground truth from computer games",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "Unet: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical image computing and computer-assisted intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Sebastien Roy",
                "Ingemar J Cox"
            ],
            "title": "A maximum-flow formulation of the n-camera stereo correspondence problem",
            "venue": "In International Conference on Computer Vision,",
            "year": 1998
        },
        {
            "authors": [
                "Paul-Edouard Sarlin",
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "title": "Superglue: Learning feature matching with graph neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Scharstein",
                "Heiko Hirschm\u00fcller",
                "York Kitajima",
                "Greg Krathwohl",
                "Nera Ne\u0161i\u0107",
                "Xi Wang",
                "Porter Westling"
            ],
            "title": "High-resolution stereo datasets with subpixel-accurate ground truth",
            "venue": "In German conference on pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Thomas Schops",
                "Johannes L Schonberger",
                "Silvano Galliani",
                "Torsten Sattler",
                "Konrad Schindler",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "A multi-view stereo benchmark with highresolution images and multi-camera videos",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Leslie N Smith",
                "Nicholay Topin"
            ],
            "title": "Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006",
            "year": 2019
        },
        {
            "authors": [
                "Julian Straub",
                "Thomas Whelan",
                "Lingni Ma",
                "Yufan Chen",
                "Erik Wijmans",
                "Simon Green",
                "Jakob J. Engel",
                "Raul Mur-Artal",
                "Carl Ren",
                "Shobhit Verma",
                "Anton Clarkson",
                "Mingfei Yan",
                "Brian Budge",
                "Yajie Yan",
                "Xiaqing Pan",
                "June Yon",
                "Yuyang Zou",
                "Kimberly Leon",
                "Nigel Carter",
                "Jesus Briales",
                "Tyler Gillingham",
                "Elias Mueggler",
                "Luis Pesqueira",
                "Manolis Savva",
                "Dhruv Batra",
                "Hauke M. Strasdat",
                "Renzo De Nardi",
                "Michael Goesele",
                "Steven Lovegrove",
                "Richard Newcombe"
            ],
            "title": "The Replica dataset: A digital replica of indoor spaces",
            "venue": "arXiv preprint arXiv:1906.05797,",
            "year": 1906
        },
        {
            "authors": [
                "Deqing Sun",
                "Daniel Vlasic",
                "Charles Herrmann",
                "Varun Jampani",
                "Michael Krainin",
                "Huiwen Chang",
                "Ramin Zabih",
                "William T. Freeman",
                "Ce Liu"
            ],
            "title": "Autoflow: Learning a better training set for optical flow",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Jiaming Sun",
                "Zehong Shen",
                "Yuang Wang",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "LoFTR: Detector-free local feature matching with transformers",
            "venue": "CVPR, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Tankovich",
                "Christian Hane",
                "Yinda Zhang",
                "Adarsh Kowdle",
                "Sean Fanello",
                "Sofien Bouaziz"
            ],
            "title": "Hitnet: Hierarchical iterative tile refinement network for real-time stereo matching",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "RAFT: recurrent all-pairs field transforms for optical flow (extended abstract)",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Tremblay",
                "Thang To",
                "Stan Birchfield"
            ],
            "title": "Falling things: A synthetic dataset for 3d object detection and pose estimation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Geert Van Meerbergen",
                "Maarten Vergauwen",
                "Marc Pollefeys",
                "Luc Van Gool"
            ],
            "title": "A hierarchical symmetric stereo algorithm using dynamic programming",
            "venue": "International Journal of Computer Vision,",
            "year": 2002
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Chaoyang Wang",
                "Simon Lucey",
                "Federico Perazzi",
                "Oliver Wang"
            ],
            "title": "Web stereo video supervision for depth prediction from dynamic scenes",
            "venue": "In 2019 International Conference on 3D Vision (3DV),",
            "year": 2019
        },
        {
            "authors": [
                "Wenshan Wang",
                "Delong Zhu",
                "Xiangwei Wang",
                "Yaoyu Hu",
                "Yuheng Qiu",
                "Chen Wang",
                "Yafei Hu",
                "Ashish Kapoor",
                "Sebastian Scherer"
            ],
            "title": "Tartanair: A dataset to push the limits of visual slam",
            "venue": "In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "Wenshan Wang",
                "Delong Zhu",
                "Xiangwei Wang",
                "Yaoyu Hu",
                "Yuheng Qiu",
                "Chen Wang",
                "Yafei Hu",
                "Ashish Kapoor",
                "Sebastian Scherer"
            ],
            "title": "Tartanair: A dataset to push the limits of visual slam",
            "venue": "In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "Yao Yao",
                "Zixin Luo",
                "Shiwei Li",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Mvsnet: Depth inference for unstructured multi-view stereo",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Jure Zbontar",
                "Yann LeCun"
            ],
            "title": "Computing the stereo matching cost with a convolutional neural network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Feihu Zhang",
                "Victor Prisacariu",
                "Ruigang Yang",
                "Philip HS Torr"
            ],
            "title": "Ga-net: Guided aggregation net for end-toend stereo matching",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Feihu Zhang",
                "Xiaojuan Qi",
                "Ruigang Yang",
                "Victor Prisacariu",
                "Benjamin Wah",
                "Philip Torr"
            ],
            "title": "Domain-invariant stereo matching networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Zhoutong Zhang",
                "Forrester Cole",
                "Richard Tucker",
                "William T Freeman",
                "Tali Dekel"
            ],
            "title": "Consistent depth of moving objects in video",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Estimating depth from stereo is a fundamental computer vision problem, with applications in 3D reconstruction, robot navigation, and human motion capture, among others. With the advent of consumer devices featuring multiple cameras, such as AR glasses and smartphones, stereo can simplify the 3D reconstruction of everyday scenes, extracting them as content to be experienced in virtual or mixed reality, or for mixed reality pass-through.\nDepth from stereo takes as input two images capturing the same scene from different viewpoints. It then finds pairs of matching points, a problem known as disparity estimation. Since the two cameras are calibrated, the matched points can be projected into 3D using triangulation. While this process is robust, it is suboptimal when applied to video data, as it can only reconstruct stereo frames individually, ignoring the fact that the observations infer properties of the same underlying objects over time. Even if the camera moves or the scene deforms non-rigidly, the instantaneous 3D reconstructions are highly correlated and disregarding this fact can result in inconsistencies.\nIn this paper, we thus consider the problem of dynamic\nar X\niv :2\n30 5.\n02 29\ndepth from stereo to improve the temporal consistency of stereo reconstruction from video data.\nTraditional approaches to stereo compute the matching costs between local image patches, aggregating those in an objective function, and optimizing the latter together with a regularization term to infer disparities. Examples of such approaches include max-flow [35] and graph-cut [15]. More recently, stereo methods have used deep networks learned from a large number of image pairs annotated with groundtruth disparities [12, 14, 18, 23]. They usually follow an approach similar to the traditional methods, but using deep CNN features for computing the matching costs, and replacing the per-image optimization by a pre-trained regression deep network, which processes the cost volume and outputs the estimated disparities.\nIn the video setting, matching quality can potentially be improved by looking for matches across space and time. For instance, points occluded in one camera at a given point in time may be visible from both cameras at other times.\nTransformer architectures have shown that attention can be a powerful and flexible method for pooling information over a range of contexts [6, 8, 9, 42]. Our DynamicStereo model incorporates self- and cross-attention to extract relevant information across space, time and stereo pairs. Our architecture relies on divided attention [3] to allow efficient processing of this high-dimensional space.\nAs a learning-based approach, we wish to learn priors from data representative of real-life 3D dynamic reconstruction applications, where videos depict people or animals moving and interacting with objects. There are several synthetic video datasets [10,27,41] commonly used for training stereo and optical flow methods, but they contain abstract scenes with several layers of moving objects that share little resemblance to real-life. More realistic stereo datasets also exist [45, 49], but they either do not contain video sequences or are focused on static scenes. Given these limitations, as an additional contribution we propose a new synthetic stereo dataset showing moving human and animal characters inside realistic physical spaces from the Replica dataset [40]. We call this new dataset Dynamic Replica (DR), and we use it for learning dynamic stereo matches. DR contains 524 videos of virtual humans and animals embedded in realistic digital scans of physical environments (see Tab. 1 and Fig. 2). We show that DR can significantly boost the quality of dynamic stereo methods compared to training them only on existing depth-from-stereo datasets.\nTo summarise, we make three contributions. (1) We introduce DynamicStereo, a transformer-based architecture that improves dynamic depth from stereo by jointly processing stereo videos. (2) We release Dynamic Replica, a new benchmark dataset for learning and evaluating models for dynamic depth from stereo. (3) We demonstrate state-ofthe-art dynamic stereo results in a variety of benchmarks."
        },
        {
            "heading": "2. Related work",
            "text": "Depth from stereo. Stereo matching is a classic problem in computer vision. The traditional way of solving it is to compute local matching costs between image patches and then either perform local aggregation [4, 13, 46, 56], or a global optimization based on an energy function featuring data and regularization terms [15, 35].\nMore recently, deep learning has become the dominant approach to stereo matching. Zbontar and LeCun [52] proposed to use a CNN to compute the matching cost between image patches. Then Mayer et al. [27] introduced the first fully learning-based approach to stereo estimation. Inspired by traditional stereo algorithms, the next line of work [7, 12, 14, 51, 53, 54] applied 3D convolutions in their fully learning-based approaches. They construct a dense 3D cost volume between the left and the right 2D feature-maps before filtering it with a 3D-CNN. These approaches often fail to generalize to data that they were not trained on.\nMore recent works focused on improving the computational efficiency of stereo estimation [1,22,23,43]. Inspired by RAFT [44] which constructs a 4D cost volume between all pairs of pixels for optical flow, RAFT-Stereo [23] restricts this volume to 3D by collapsing it to the epipolar line. Similarly to RAFT, it iteratively updates the prediction at high resolution allowing to maintain global context and recover details. These updates are performed by a GRU [2] that operates at multiple resolutions.\nCRE-Stereo [18] proposes to gradually increase resolution during iterative updates which simplifies the architecture. However, the lack of information propagation from lower to higher resolution may result in a loss of context. Our architecture also refines disparity in a coarse-to-fine manner, but unlike CRE-Stereo, we fuse low-resolution and high-resolution features together and propagate them to the final prediction. This allows high-resolution layers to use these features and keep track of previous states.\nSome recent works incorporate attention [47]. Stereo Transformer [19] replaces cost volume with dense pixel\nmatching using attention. LoFTR [42] and SuperGlue [36] use combinations of self and cross-attention layers for sparse feature matching. Inspired by these methods, CREStereo [18] uses self and cross-attention to improve convolutional features. These works focus on disparity estimation for individual frames. As we predict disparity for videos, we apply attention across time, space, and stereo frames.\nDynamic video depth. Monocular depth estimators like MiDaS [30, 31] attempt to estimate depth from a single image. Due to the ambiguity of this task, they tend to be far less accurate than methods that use stereo. Recent works have proposed to extend these methods to use monocular videos instead, relying on motion parallax in static areas [21] or fusing information extracted from all the frames of a video, in an off-line fashion.\nConsistent Video Depth (CVD) [25] assumes that objects move almost rigidly across neighboring frames. Robust CVD (RCVD) [16] excludes dynamic objects from the optimization objective. Dynamic VD (DVD) [55] explicitly models motion of non-rigid objects with a scene-flow network. All these methods require fine-tuning a monocular depth predictor like MiDAS on a specific video, a process that must be carried out from scratch for each new video.\nDynamic depth from stereo. The work of Li et al. (CODD) [20] is the closest to ours. It is based on three different networks: stereo, motion, and fusion, that are trained separately. During inference, consistency is achieved by extracting information from the memory state that is updated after each iteration. It is an online approach that assumes no access to future frames and thus does not allow global optimization over the whole sequence. Our network does not need a memory state and learns consistency from data. It can be applied in both online and offline settings.\nDatasets for learning depth from stereo. Training data is an important factor for learning-based stereo algorithms. It is challenging to collect real data for this task because disparity is extremely difficult to annotate. [28, 48] Active sensors such as time-of-flight cameras can provide ground\ntruth depth. KITTI [28] shows that converting such ground truth depth to pixel-accurate annotations is still challenging for dynamic objects due to the low frame rate and imprecise estimations. Synthetic datasets [17, 32, 33] can simplify the data collection process. SceneFlow [27] is the first large-scale synthetic dataset for disparity and optical flow estimation. It allowed training fully learning-based methods for these tasks. SceneFlow is less realistic compared to MPI Sintel [5], a much smaller dataset with optical flow and disparity annotations. Falling Things [45] is realistic and relatively large but does not contain non-rigid objects. TartanAir [49] is a realistic SLAM dataset with ground-truth stereo information but only few non-rigid objects.\nTo the best of our knowledge, we are the first to introduce a large-scale semi-realistic synthetic dataset with a focus on non-rigid objects for disparity estimation."
        },
        {
            "heading": "3. Method",
            "text": "We first formalize the dynamic depth from stereo problem. Given a sequence S = [(ILt , I R t )]1\u2264t\u2264T \u2208 R2\u00d73\u00d7H\u00d7W of T rectified stereo frames, the task is to predict a sequence of disparity maps D\u0302 = [Dt]1\u2264t\u2264T \u2208 RH\u00d7W aligned with the left frames ILt . While most disparity estimation methods operate on single frames D\u0302t = \u03a6(St), here we learn a model that operates on sequences of length T , constructing a function D\u0302 = \u03a6(S). This has the advantage that the model can fuse information along time and thus improve its temporal consistency.\nThe challenge is to design an architecture that can pass information across such a large volume of data efficiently. We achieve this via an encoder-decoder design (Fig. 3), building on prior work [11, 18, 34]. The encoder extracts features independently from all the frames and obtains a multi-scale representation of their content. The decoder then matches progressively more detailed features to recover the disparities from coarse to fine. Low-resolution matches easily spot large displacements of large image regions, capturing the rough structure of the disparity map, whereas high-resolution matches recover the details.\nFor efficiency, matches are always carried out along epipolar lines. The task of exchanging information across space, view and time is delegated to two mechanisms. First, the encoder terminates in a transformer network that updates the lowest-resolution feature by running attention across these three dimensions in turn. This is efficient because it is done only for the lowest-resolution features. Second, the decoder further propagates information as it matches features to recover the disparities. The decoder itself consists of update blocks (Fig. 4a) that use both space and time information to gradually refine the disparity."
        },
        {
            "heading": "3.1. Encoder",
            "text": "The encoder starts by extracting features from every frame Ivt , v \u2208 {L,R}, t \u2208 {1, . . . , T} of the stereo video independently by applying the same CNN F to them. As it is typical for CNN backbones, the resolution of the output feature map is lower than the resolution of the input image. Features are extracted with minimum stride k = 4 and are further down-sampled with average pooling to obtain features at 1/8 and 1/16 of the original resolution. Overall, this results in the feature maps \u03d5(Ivt )k \u2208 Rd\u00d7 H k \u00d7 W k , k \u2208 {4, 8, 16}. We will use the symbol \u03d5k \u2208 RT\u00d72\u00d7d\u00d7 H k \u00d7 W k to refer to the combined five-dimensional (temporal, stereo and spatial) feature volume at resolution k.\nInformation Propagation. The backbone F processes frames independently, so we require a different mechanism to exchange of information between left and right views v and different timestamps t. We do so by passing the features to a transformer network that uses self and crossattention. Ideally, attention should compare feature maps across views, time, and space. However, it is computationally demanding to apply attention jointly even with linear [42] space and stereo attention. We thus rely on divided attention [3] to attend these three dimensions individually. We call this a Space-Stereo-Time attention block\n(SST, Fig. 4b) and we repeat it four times. Since attention remains computationally expensive, we only apply SST to the lowest-resolution feature map \u03d516 only."
        },
        {
            "heading": "3.2. Decoder",
            "text": "The output of the encoder is a multi-resolution feature volume, where the lowest resolution features incorporate information across time, view and space dimensions due to the SST block described above. The task of the decoder is to convert this feature volume into the final disparity values.\nThe decoder is based on three ideas: (1) Disparities are updated from coarse to fine [18], using features of increasing resolution to update earlier estimates. (2) At each resolution, a feature correlation volume is computed and correspondences are refined iteratively [44] with reference to this volume. (3) Similar to the SST attention blocks in the encoder, information is exchanged between the three dimensions (space, view and time) throughout decoding. We describe next each component in detail. Iterative correspondence updates. Our model produces a sequence of progressively more accurate disparity estimates D\u0302 = D\u0302(m) \u2208 RHk \u00d7Wk , 0 \u2264 m \u2264 M starting from D\u0302(0) = 0 and then applying M times the update rule:\nD\u0302(m+1) = D\u0302(m) + g(D\u0302(m), \u03d5). (1)\nWe apply the rule M4 times to obtain disparities D\u0302 (m) at\nthe coarser resolution (H16 , W 16 ). We then upsample D\u0302 (M4 ) to resolution (H8 , W 8 ), apply the update rule M 4 more times to obtain D\u0302( M 2 ), upsample the disparity again, and apply the update M2 more times to obtain D\u0302 (M) at resolution (H4 , W 4 ). Finally, we upsample the predicted disparity D\u0302(M) to (H,W ). Different versions of the function g are trained for each of the three resolutions and upsampling uses the same mechanism as in RAFT.\nThe detailed structure of the update network g is illustrated in Fig. 4a and described below.\n<latexit sha1_base64=\"bYhKfuePrnMN28wYdmjx5PvTPig=\">AAAD2HicbVJNb9NAEHUTPkr4auHIZUSN1B6IkiIBx0qtBAcOBTVtRZ1W6/U4XnW9u/KOC5EViQMIceWnceNP8BuYdQNtAytZGs3Mm3nvjVOnlafB4OdSp3vt+o2by7d6t+/cvXd/ZfXBvrd1JXEkrbbVYSo8amVwRIo0HroKRZlqPEhPt0P94Awrr6zZo6nDcSkmRuVKCuLUyerSryTFiTKNRENYzXrrYgMSwo+U5s3IZYIQUm3lKcSTeNaHA4TaI1CBIOuqYhBkyjtRKZoCelJlQMRJIajZmR036+XGLIYkAbLgRen0HGsZ6501mTITcFbxHGEyUOQBnXJWiwoMqkmRstLC2izMyCtbXsB1KwHOrK5LXrkdgwhk8hxbWjyfK6HF92GPQZyoNYV9l+ELnLf/clYecpaawQdFRbv1QuiivkC9bWOSAriJwOYAOQqqK4RSOIa4Qh2/iVsLeZoB4ZyeAo+U+JQ5INMnJh5IpZhbxvE0J7wPlMP+czq2JldT8DPknu3Aq3cjDll73Z7LX+Xah16CJvtz35OVtUF/0D74NxjOg7Vo/nZPVn4kmZXssCGpmczRcOBo3LBEJTXOegmTYgmnYoJHHBpRoh837Y85gyctY9bCH5+kzV5GNKL0flqm3Mk3KPxiLST/VzuqKX85bpRhJ9DI80V5rVtX+C9n+RVKYnszJSTboCTIQlRCsge+xyYMFyX/G+xv9ofP+5tvN9e24rkdy9Gj6HG0Hg2jF9FW9DrajUaR7Iw6Tedz50v3ffdT92v323lrZ2mOeRhded3vvwFEZj8q</latexit>\nCorrelation volume. Similar to classical disparity estimation methods, g starts by computing the correlation volumes between left and right features Ct,s,k = corr(\u03d5Lt,s,k, \u03d5 R t,s,k) \u2208 R H sk\u00d7 W sk\u00d7 W sk for each feature resolution k and at different scales s \u2208 {1, 2, 4, 8}. The correlation is computed along epipolar lines, which correspond to image rows as the input images are rectified. Each element (h,w,w\u2032) of Ct,s,k is the inner product between feature vectors of \u03d5Lt,s,k and \u03d5 R t,s,k:\nCt,s,k(h,w,w \u2032) = 1\u221a d \u27e8\u03d5Lt,s,k(h,w), \u03d5Rt,s,k(h,w\u2032)\u27e9. (2)\nThus, Ct,s,k(h,w,w\u2032) is proportional to how well the left image point (h,w) matches the right image point (h,w\u2032). As the correlation volume does not depend on the update iteration m, it is computed only once. Correlation lookup. The function g updates the disparity at location (h,w) by observing the correlation volume in a local neighbourhood centered on the current disparity value D\u0302(m)t (h,w). The necessary samples are collected and stacked as feature channels of the tensor\nC\u0302 (m) t,k (h,w)=cat\ns,\u03b4\n[ Ct,s,k ( h\ns , w s , w+D\u0302\n(m) t (h,w)\ns +\u03b4\n)] .\n(3) The current correlation estimate C\u0302(m)t,k \u2208 R H k \u00d7 W k \u00d74(2\u2206+1) captures the correlation volume across all four scales and in a neighborhood \u03b4 \u2208 {\u2212\u2206, . . . ,\u2206} for additional context, with cat representing the concatenation operation. Modality fusion. The estimated correlation, disparity, and feature maps need to be combined to provide the update block with enough information to update the disparity. Using a late fusion scheme, we encode correlation C\u0302(m)t,k and disparity D\u0302(m)t separately, before concatenating them with\nthe feature map of the left frame \u03d5Lt,k, as it is the reference frame. To incorporate temporal and spatial information, we apply self-attention across time (T ) and space (Hk , W k ) to the output of the modality fusion step. For efficiency, we do it only for k = 16. Architecture details can be found in Fig. 4 and the supplementary material.\n3D CNN-based GRU. The update function g is implemented using a 3D convolutional GRU.\nAt each step m, the GRU takes as input the fused features and a hidden state that is initialized with the reference feature map \u03d5L. All internal operations of the GRU are implemented as separable 3D convolutions across space and time to propagate temporal and spatial information. The output of each iteration is an update to the current disparity estimate as in eq. (1). Each subsequent iteration is preceded by correlation lookup and modality fusion."
        },
        {
            "heading": "3.3. Training",
            "text": "Due to its iterative nature, the proposed model generates M predictions for every timestamp. During training, we supervise the network over the full sequence of predictions D\u0302\n(m) t , with exponentially increasing weights towards the fi-\nnal estimate at step M as follows:\nL(D\u0302,D) = T\u2211\nt=1 M\u2211 m=1 \u03b3M\u2212m\u2225D\u0302(m)t \u2212Dt\u2225, (4)\nwhere \u03b3 = 0.9 and D is the ground truth disparity sequence. Lower resolution disparity estimates are up-sampled to ground truth resolution."
        },
        {
            "heading": "3.4. Inference",
            "text": "The model is trained on stereo sequences with a fixed length of T . To increase the temporal coherence at test time,\nwe apply it to videos of arbitrary length using a sliding window approach with overlap, where we discard the predictions in overlapping time steps. For details see supp. mat. Videos shorter than T time steps can be used by repeating the first or the last frame up to the minimal length T . This corresponds to a static scene without moving camera and is well within the training distribution of the model."
        },
        {
            "heading": "4. Dynamic Replica: dynamic stereo dataset",
            "text": "Training temporally consistent models requires a dataset with stereo videos and dense ground-truth annotations. While the SceneFlow dataset [26] fulfils both criteria, it is comprised of very short sequences of randomly textured objects moving. In this paper, we introduce a more realistic synthetic dataset and benchmark of animated humans and animals in every-day scenes: Dynamic Replica.\nThe dataset consists of 524 videos of synthetic humans and animals performing actions in virtual environments (see Tab. 2). Training and validation videos are 10 seconds long and each contain 300 frames. There are 484 training and 20 validation videos. Our test split consists of 20 videos of length 30 seconds to benchmark models on longer videos.\nVideos are rendered at a resolution of 1280\u00d7720. This is close to the resolution of modern screens and higher than the resolution of such popular stereo datasets as Sintel (1024\u00d7536) and SceneFlow (960\u00d7540).\nThe dataset is based on Facebook Replica [40] reconstructions of indoor spaces. We take 375 3D scans of humans from the RenderPeople1 dataset and animate them using motion capture sequences from real humans scans. We use artist-created animated 3D models of animals from 13\n1http://renderpeople.com/\ncategories (chimp, dog, horse, sheep, etc.). We use different environments, scans, textures and motions for training, validation and test splits to evaluate generalization to unseen environments and dynamic objects.\nWe randomize camera baselines in our training subset to ensure generalization across different stereo setups. Baselines are sampled uniformly between 4cm and 30cm.\nFor each scene we generate a camera trajectory imitating a person filming the scene with their mobile phone or AR glasses. These virtual cameras have smooth trajectories and are located approximately at 1.25m above the ground.\nAll samples in the dataset contain ground-truth depth maps, optical flow, foreground / background segmentation masks and camera parameters for both stereo views."
        },
        {
            "heading": "5. Experiments",
            "text": "We structure the experiments as follows. First, we evaluate the Dynamic Replica dataset by comparing generalization performance of prior models trained on other datasets and on Dynamic Replica. Then we evaluate our model and compare it to the state of the art in temporal consistency. Finally, we ablate design choices in the model architecture and verify the importance of its individual components. Implementation Details. We implement DynamicStereo in PyTorch [29] and train on 8 NVIDIA TESLA Volta V100 32GB GPUs. The SF version is trained for 70k iterations with a batch size 8. We train the DR+SF version for 120k iterations which takes about 4 days.\nBoth models are trained on random 384\u00d7512 crops of sequences of length T = 5 and evaluated in the original resolution with T = 20 and overlapping windows of size 10. During training, we use the AdamW optimizer [24] and set\nthe number of iterative updates M = 20. We train with onecycle learning rate schedule [39] with a maximum learning rate 3 \u00b7 10\u22124. We set the lookup neighborhood \u2206 = 4 (see Sec. 3.2). For attention layers we use positional encoding in both space and time. We apply linear attention [42] for space and use standard quadratic attention for time. For other implementation details, please see supp. mat."
        },
        {
            "heading": "5.1. Dynamic Replica",
            "text": "In Tab. 3 we show results obtained by training two recent state-of-the-art models, RAFT-Stereo [23] and CREStereo [18] on SceneFlow, our dataset and their combination and testing the models\u2019 generalization to other disparity estimation datasets. While the main objective of our method is to produce temporally consistent disparity estimates, here we train state-of-the-art disparity estimation models to evaluate the usefulness of the dataset in this setting.\nSceneFlow is an abstract dataset of moving shapes on colorful backgrounds. It aims at generalization through domain randomization, while our dataset consists of more realistic home and office scenes with people. Thus, while training on Dynamic Replica alone improves performance on the DR test set, it does not generalize as well as models trained on SceneFlow. However, combining both datasets boosts performance across both, datasets and models.\nThis highlights the benefits of including Dynamic Replica in the standard set of disparity estimation training datasets, even if its main goal is to enable training of temporally consistent models on longer sequences."
        },
        {
            "heading": "5.2. Temporal Consistency",
            "text": "The main objective of our method and dataset is to enable training of temporally consistent disparity estimators.\nTo evaluate temporal consistency, we compute the temporal end-point-error (TEPE) defined as follows:\nTEPE(D\u0302,D)= \u221a\u221a\u221a\u221aT\u22121\u2211 t=1 ( (D\u0302t\u2212D\u0302t+1)\u2212(Dt\u2212Dt+1) )2 . (5)\nThis effectively measures the variation of the end-pointerror across time. Lower values mean greater temporal consistency. In Tab. 5 we show that our model is more temporally consistent than prior methods across training datasets and benchmarks. It is even better than CRE-Stereo [18] that is trained on a combination of seven datasets. Additionally, models trained with Dynamic Replica are more consistent than models trained on SceneFlow alone.\nWith the above experiments, we have shown that Dynamic Replica is a useful dataset for training temporally consistent disparity estimators as well as standard stereoframe models, and that our proposed model improves over the state of the art in both tasks when trained on our dataset.\nIn Tab. 4, we show a qualitative comparison of the temporal consistency between our model and RAFT-Stereo on a real-world sequence. We show the mean reconstruction over time of the sequence and color each pixel more red, the higher its variance across time. Our model significantly reduces the flickering that single-timestep models such as RAFT-Stereo produce. For more qualitative results and videos, please see the supplementary material."
        },
        {
            "heading": "5.3. Ablation Studies",
            "text": "In this section, we validate our method by ablating the design choices of our model. We train the model on SceneFlow [27] for 50k iterations with the hyper-parameters described in Implementation Details (see Sec. 5). We evaluate these models on the clean pass of Sintel [5] and on the test split of Dynamic Replica. We measure both accuracy and\ntemporal consistency. For accuracy, we use an end-pointerror threshold of 3px for Sintel and 1px for DR. This shows the proportion of pixels with an end-point-error higher than 3px. For consistency, we use TEPE (see Sec. 5.2).\nUpdate Block. In Tab. 6 we compare sharing weights of the three blocks across the three resolutions of the decoder to learning separate update blocks. As different scales exploit features of different resolutions, learning separate update blocks improves the results over weight-sharing.\nUpdate Block Convolution. While prior works such as CRE Stereo use 2D convolutions in the iterative update block as they operate on single time steps, we find it beneficial to extend the processing of the update block across time (Tab. 7). This results in a general improvement but shows especially large improvements in temporal consistency.\nPlease see the supplement for additional analysis."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we make two main contributions. We introduce a new stereo video dataset\u2014Dynamic Replica\u2014that allows training temporally consistent disparity estimators. Additionally, we introduce a new method that is able to improve over the state of the art in temporally consistent stereo estimation using the new dataset.\nWe show that other methods benefit from training on this new dataset as it contains realistic scenes and can thus reduce the domain gap between the real world and typical synthetic, abstract training datasets such as SceneFlow. Our combines spatial, temporal, and stereo information, enabling precise and consistent predictions across time. In extensive ablation studies, we show that each component of the model contributes to the final performance."
        },
        {
            "heading": "Acknowledgements",
            "text": "C. R. is supported by VisualAI EP/T028572/1. We would like to thank Filippos Kokkinos for helping to capture test videos, Roman Shapovalov and Luke Melas-Kyriazi for insightful discussions."
        },
        {
            "heading": "A. Additional Ablations",
            "text": "We ablate the proposed SST block and the choice of attention in the update block. We train the model on SceneFlow [27] for 50k iterations with the same hyper-parameters as in the main paper. SST Block Attention We evaluate the choice of attention types of the SST-Block in Tab. 10. We find that including attention layers generally improves disparity estimation both in terms of accuracy and temporal consistency. Attention across space, stereo pairs, and time achieves the best results. Interestingly, time attention also improves accuracy, potentially through the use of multiple viewpoints over time improving the precise location of correspondences. Update Block Attention In Tab. 9 we compare different choices of attention inside the Update Block. The model with a combination of space and time attention performs well on both datasets. Similarly, improvements are gained in both stereo and temporal metrics.\nB. Implementation details Here we provide additional implementation details. Training For all the DR & SF dataset generalization experiments, we sample the same number of frames from both DR and SF. For temporal consistency experiments, we sample the same number of sequences from DR and SF.\nWe found that learnable positional encoding for time can generalize better during inference on longer sequences. We thus use learnable encoding for time and sin / cos Fourier features for space. Augmentations During training, we set image saturation to a value sampled uniformly between 0 and 1.4. We stretch right frames to simulate imperfect rectification: it is stretched by a factor sampled uniformly from [2\u22120.2, 20.4]. Following [44], we simulate occlusions by randomly erasing rectangular regions from each frame with probability 0.5. Inference For better temporal consistency during inference, we split the input video into 20-frame chunks with an overlap of 10 frames. We then apply the model to each chunk and discard the first and the last 5 frames of each prediction to compose the final sequence of disparity estimations. Space-Stereo-Time attention We add time and position encoding to left and right input feature tensors and reshape them to (B \u2217 T, H16 \u2217 W16 , d). Then we apply linear selfattention [42] across space to both tensors and cross attention across space between left and right tensors. Finally, we reshape left and right tensors to (B\u2217 H16 \u2217W16 , T, d) and apply standard attention across time. 3D CNN-based GRU For efficiency, each 3D GRU module is composed of three separable height-width-time GRUs\nwith kernel sizes (1\u00d7 1\u00d7 5), (5\u00d7 1\u00d7 1), and (1\u00d7 5\u00d7 5). Upsampling To pass the output of each update block g to a higher resolution update block, we use a combination of convex upsampling from RAFT [44] and standard bi-linear upsampling."
        },
        {
            "heading": "C. Limitations",
            "text": "While our method is more temporally consistent than previous work, it still is not fully stable over time. This partially comes form the fact that the method is evaluated in a sliding window fashion resulting in low-frequency oscillations at the scale of the window size (1-2 sec). Extending the window size is currently not possible due to memory limitations.\nAs with any stereo-matching method, exceedingly large untextured scene parts such as walls and other surfaces are difficult to predict accurately. Learning from DynamicReplica helps to learn priors to mitigate this issue but does not solve it completely.\nAs dense groundtruth information is near impossible to collect, evaluation and training rely on synthetic datasets such as DynamicReplica. Generalization to the real world can only be assessed qualitatively and might not fully reflect the performance on artificial scenes."
        }
    ],
    "title": "DynamicStereo: Consistent Dynamic Depth from Stereo Videos",
    "year": 2023
}