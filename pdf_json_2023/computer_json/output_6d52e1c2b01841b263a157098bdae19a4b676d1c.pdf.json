{
    "abstractText": "Recent semi-supervised learning (SSL) methods typically include a filtering strategy to improve the quality of pseudo labels. However, these filtering strategies are usually handcrafted, and do not change as the model is being updated, resulting in a lot of correct pseudo labels being discarded and incorrect pseudo labels being selected during the training process. In this work, we observe that the distribution gap between the confidence values of correct and incorrect pseudo labels emerges at the very beginning of the training, which can be utilized to filter pseudo labels. Based on this observation, we propose a Self-Adaptive Pseudo-Label Filter (SPF), which automatically filters noise in pseudo labels in accordance with model evolvement by modeling the confidence distribution throughout the training process. Specifically, with an online mixture model, we weight each pseudo-labeled sample by the posterior of it being correct, which takes into consideration of the confidence distribution at that time. Unlike previous handcrafted filters, our SPF evolves together with the deep neural network without manual tuning. Extensive experiments demonstrate that incorporating SPF into the existing SSL methods can help improve the performance of SSL, especially when the labeled data is extremely scarce.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Zhu"
        },
        {
            "affiliations": [],
            "name": "Zhanghan Ke"
        },
        {
            "affiliations": [],
            "name": "Rynson Lau"
        }
    ],
    "id": "SP:0f042dae0402a4a030214f61a8209ac76c081a73",
    "references": [
        {
            "authors": [
                "Eric Arazo",
                "Diego Ortego",
                "Paul Albert",
                "Noel O\u2019Connor",
                "Kevin McGuinness"
            ],
            "title": "Unsupervised label noise modeling and loss correction",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ekin D Cubuk",
                "Alex Kurakin",
                "Kihyuk Sohn",
                "Han Zhang",
                "Colin Raffel"
            ],
            "title": "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin Raffel"
            ],
            "title": "Mixmatch: A holistic approach to semi-supervised learning",
            "venue": "In NeurIPS, 2019",
            "year": 2019
        },
        {
            "authors": [
                "George Casella",
                "Roger Berger"
            ],
            "title": "Statistical Inference",
            "venue": "Cengage Learning,",
            "year": 2002
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "In CVPR,",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "David W Hosmer Jr.",
                "Stanley Lemeshow",
                "Rodney X Sturdivant"
            ],
            "title": "Applied logistic regression, volume 398",
            "year": 2013
        },
        {
            "authors": [
                "Zijian Hu",
                "Zhengyu Yang",
                "Xuefeng Hu",
                "Ram Nevatia"
            ],
            "title": "Simple: Similar pseudo label exploitation for semisupervised classification",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Ahmet Iscen",
                "Giorgos Tolias",
                "Yannis Avrithis",
                "Ondrej Chum"
            ],
            "title": "Label propagation for deep semi-supervised learning",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Zhanghan Ke",
                "Daoye Wang",
                "Qiong Yan",
                "Jimmy Ren",
                "Rynson W.H. Lau"
            ],
            "title": "Dual student: Breaking the limits of the teacher in semi-supervised learning",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report, University of Toronto,",
            "year": 2009
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "NeurIPS, 25:1097\u20131105,",
            "year": 2012
        },
        {
            "authors": [
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "Temporal ensembling for semisupervised learning",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Dong-Hyun Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
            "venue": "In ICML Workshop,",
            "year": 2013
        },
        {
            "authors": [
                "Junnan Li",
                "Richard Socher",
                "Steven C.H. Hoi"
            ],
            "title": "Dividemix: Learning with noisy labels as semi-supervised learning",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Suichan Li",
                "Bin Liu",
                "Dongdong Chen",
                "Qi Chu",
                "Lu Yuan",
                "Nenghai Yu"
            ],
            "title": "Density-aware graph for deep semi-supervised visual recognition",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Long",
                "Evan Shelhamer",
                "Trevor Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "In CVPR, pages 3431\u20133440,",
            "year": 2015
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: stochastic gradient descent with warm restarts",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Islam Nassar",
                "Samitha Herath",
                "Ehsan Abbasnejad",
                "Wray Buntine",
                "Gholamreza Haffari"
            ],
            "title": "All labels are not created equal: Enhancing semi-supervision via label grouping and co-training",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "M-E. Nilsback",
                "A. Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "In ICVGIP,",
            "year": 2008
        },
        {
            "authors": [
                "Avital Oliver",
                "Augustus Odena",
                "Colin Raffel",
                "Ekin D Cubuk",
                "Ian J Goodfellow"
            ],
            "title": "Realistic evaluation of deep semisupervised learning algorithms",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Haim Permuter",
                "Joseph Francos",
                "Ian Jermyn"
            ],
            "title": "A study of gaussian mixture models of color and texture features for image classification and segmentation",
            "venue": "Pattern Recognition,",
            "year": 2006
        },
        {
            "authors": [
                "Hieu Pham",
                "Zihang Dai",
                "Qizhe Xie",
                "Quoc V Le"
            ],
            "title": "Meta pseudo labels",
            "venue": "In CVPR, pages 11557\u201311568,",
            "year": 2021
        },
        {
            "authors": [
                "Siyuan Qiao",
                "Wei Shen",
                "Zhishuai Zhang",
                "Bo Wang",
                "Alan Yuille"
            ],
            "title": "Deep co-training for semi-supervised image recognition",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Joseph Redmon",
                "Santosh Divvala",
                "Ross Girshick",
                "Ali Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Zhongzheng Ren",
                "Raymond Yeh",
                "Alexander Schwing"
            ],
            "title": "Not all unlabeled data are equal: Learning to weight data in semi-supervised learning",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "In NeurIPS, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "Koray Kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le"
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Xu",
                "Lei Shang",
                "Jinxing Ye",
                "Qi Qian",
                "Yu-Feng Li",
                "Baigui Sun",
                "Hao Li",
                "Rong Jin"
            ],
            "title": "Dash: Semi-supervised learning with dynamic thresholding",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "In BMVC, 2016",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Deep neural networks (DNNs) have revolutionized computer vision research and become the de facto framework for many applications, such as image classification [7, 13], object detection [6,26], and semantic segmentation [18,28]. Containing millions of trainable parameters, DNNs are extremely data hungry. However, collecting manually labeled data requires a tremendous amount of human efforts, which are both tedious and time consuming. To alleviate such a burden, deep semi-supervised learning (SSL), which leverages a limited amount of labeled data and massive amount of unlabeled data to train DNNs, has attracted more and more attention.\nA series of recent state-of-the-art deep SSL methods can be summarized with the teacher-student framework, in\nwhich pseudo labels1 are generated with a teacher model when training the student model with unlabeled samples. However, as the teacher model for producing the pseudo labels is typically constructed from the student model itself (i.e., self-training) [9, 17, 29, 30] or co-trained with the student model (i.e., co-training) [11, 20, 24, 25], the resulting pseudo labels are not guaranteed to be correct. Those incorrect pseudo labels may reinforce the incorrectness of the student model and prevent it from learning new knowledge, which is known as confirmation bias [30].\nTo mitigate the problem of confirmation bias, several handcrafted pseudo-label filtering strategies have been adopted by existing works. The most related to ours is pseudo-label filtering with confidence thresholding [11, 29, 32,33]. Several works [29,32] have shown that simply keeping the pseudo labels with higher confidence values than a constant threshold (e.g., 0.95) is effective for learning with unlabeled data. However, we show in this paper a risk induced by using a constant threshold: the DNN tends to overfit the labeled data during the initial stage of the training process, due to the fact that almost all pseudo labels are rejected by the high threshold value. Although more and more unlabeled samples are being accepted gradually as the model evolves, the decision boundary may be more or less already dominated by the labeled data. In other words, the confirmation bias has occurred. As an example, Figure 1 demonstrates this phenomenon. A simple improvement is to use a progressive threshold during the training process, as in [33]. Nonetheless, a handcrafted threshold requires carefully tuning and can be suboptimal. To handle this problem, we propose a Self-Adaptive Pseudo-Label Filter (SPF), which automatically filter noise in pseudo labels in accordance with model evolvement by dynamically modelling the confidence distribution with an unsupervised mixture model throughout the training process. Unlike [33], our method automatically adapts to the constantly evolving model without manual tuning, and achieves better performances especially in the scenario where the amount of labeled data is extremely scarce.\n1In some literature, the term pseudo label refers to a constructed \u201chard\" class assignment for a piece of unlabeled data in pseudo-labeling based methods, excluding the generated \u201csoft\" target in consistency regularization based methods. Here, we use it to refer to both.\nar X\niv :2\n30 9.\n09 77\n4v 1\n[ cs\n.L G\n] 1\n8 Se\np 20\n23\nIn summary, the main contributions of our paper include:\n1. We reveal a risk of filtering with a constant confidence threshold: it may cause overfitting of the labeled training data in the early stage, which in turn causes confirmation bias.\n2. To achieve flexible pseudo-label filtering with model evolvement considered, we propose a self-adaptive approach based on an unsupervised mixture model, which can be plug-and-play into existing SSL methods.\n3. Experimental results on popular SSL benchmarks show that our approach helps improve the performances of existing methods significantly."
        },
        {
            "heading": "2. Background: Pseudo-Label Filtering in SSL",
            "text": "We focus on semi-supervised image classification. A series of recent state-of-the-art methods take advantage of abundant unlabelled data by constructing pseudo labels for them with a teacher model, as shown in Figure 2. However, since the teacher model is either constructed from the student model itself or co-trained with it, the resulting pseudo labels are noisy. To suppress the noise, some simple label filtering strategies have been incorporated as a component.\nFormally, given a labeled dataset X = {(x, y)}, which contains (image, label) tuples, and an unlabeled dataset U = {u}, which contains only images, a C-class image classifier\nh\u03b8(\u00b7) is learnt by minimizing the cost function:\nL(\u03b8;X ,U) = \u2211\n(x,y)\u2208X\n\u2113S(h\u03b8(x), y)+\n\u03bb \u2211 u\u2208U w(y\u0303u)\u2113U (h\u03b8(A(u)), y\u0303u), (1)\nwhere \u2113S(\u00b7) and \u2113U (\u00b7) denote per-sample supervised and unsupervised losses, respectively. \u03bb \u2208 R+ denotes the weight of the unsupervised loss. A(\u00b7) is the data augmentation (prediction augmentation in Figure 2) applied to the unlabeled data. w(y\u0303u) \u2208 [0, 1] is the filtering weight to suppress the noise in the pseudo labels. Basically, we expect w(\u00b7) to output large values (close to 1) for those correct pseudo labels, but very small values (near 0) for those incorrect ones.\nAs pseudo labels produced during the initial stage of the training process are usually of low quality, earlier works [3, 14, 30] typically ramp up the weight of the unlabeled loss (i.e., \u03bb in Eq. 1) from 0 to a pre-defined value. Such a strategy can be regarded as keeping \u03bb constant while using the temporal filtering function:\nw(y\u0303u) = g(t), \u2200y\u0303u, (2)\nwhere g(t) is a monotonically increasing function (e.g., linear [3] or sigmoid [30]) with respect to the training epoch t. However, this strategy is not efficient since all pseudo labels share the same w(y\u0303u) over the same period of time, without considering the correctness of individual samples.\nAlternatively, some recent state-of-the-art methods [29, 32] adopt a fixed high confidence threshold \u03c4 to filter unreliable pseudo labels. They keep only those pseudo labels with a confidence score (i.e., the predicted probability for the most possible class) higher than \u03c4 for network update. Formally, the filtering function that they use is:\nw(y\u0303u) = 1(max(y\u0303u) \u2265 \u03c4). (3)\nHere, 1(\u00b7) denotes the indicator function that outputs 1 if the condition is true and 0 otherwise. In other words, Eq. 3 estimates a binary weight for each sample, i.e., it considers the per-sample correctness. Besides, it provides a similar effect as ramping up \u03bb in Eq. 1 because there should be more and more pseudo labels with confidence scores exceeding \u03c4 as training progresses [29].\nNonetheless, using a constant threshold \u03c4 is still problematic as it is not adaptive to model evolvement. In fact, at the beginning of training, \u03c4 can be too radical that only few unlabeled data contribute to the training. This often makes the model quickly overfit the labeled data, especially when the size of the labeled data is very small. Besides the example (on the toy dataset) shown in Figure 1, we have\nalso observed this phenomenon in practice, as shown in Figure 3. In this example, we train WideResNet-28-8 [34] 300 epochs on a 2500-label split of CIFAR-100. We monitor the loss (i.e., cross entropy) on the test data and labelled training data. Between \u223c 20th epoch and \u223c 40th epoch (the two vertical dashed lines in Figure 3), pseudo-label filtering with a constant threshold causes overfitting on the labelled data, as the test loss is increasing while the training loss is decreasing.\nThere are at least two approaches that may help mitigate this issue. The first one is to use a progressive threshold during the training process, based on a predefined curriculum [33]. However, exploring a suitable tuning curriculum for the threshold is challenging as its pace should be interactively changing as the model evolves. The second one is to regard the filtering weights w(y\u0303u) as hyperparameters and optimize them periodically with bi-level optimization [27]. However, in this way, a held-out labeled validation set is required, which is expensive in the scenario of semi-supervised learning. Our goal in this paper is to achieve self-adaptive pseudo-label filtering, which takes model evolvement into consideration but without manual tuning and extra data annotations."
        },
        {
            "heading": "3. Self-Adaptive Pseudo-Label Filtering (SPF)",
            "text": "Instead of using a constant threshold, our key idea of this work is to filter pseudo labels at each epoch by considering the latest confidence distribution. Specifically, at the end of each epoch, we update a Beta Mixture Model (BMM) with the confidence scores collected during the epoch. In the following epoch, with the updated BMM from the last epoch, we then weight each pseudo-labelled sample with the posterior of it being correct when computing the loss. Hence, the deep classifier and the BMM are alternatively updated and evolve together. Figure 4 summarizes our method."
        },
        {
            "heading": "3.1. Modeling Confidence Distribution with BMM",
            "text": "The mixture model is a widely used unsupervised modeling technique [1,16,23] to reveal the statistics of the latent subpopulations (e.g., the correct and incorrect pseudo labels referred to in this paper) within an overall population. A general mixture model parameterizes the overall distribution as a weighted summation of several homogeneous or inhomogeneous components, each is individually a distribution (e.g., Gaussian and Poisson). Hence, the probability density function (PDF) of a mixture model is written as:\np(z|\u03d5) = m\u2211 c=1 \u03b3cp(z|c), (4)\nwhere m is the number of components of the mixture. p(z|c) and \u03b3c are the PDF and weight for each component c, respectively. We use \u03d5 to denote the collection of parameters of the mixture model, which includes the weights {\u03b3c}mc=1 and parameters of components (e.g., mean and standard deviation if a component is a Gaussian).\nIn our case, the number of components (i.e., m) is determined as 2 since we want to separate pseudo labels into exactly two groups: the correct and incorrect ones. For each component, we assume that it is a Beta distribution (thus forming a Beta Mixture Model):\np(z|c) = B(z|\u03b1c, \u03b2c) = \u0393(\u03b1c + \u03b2c)\n\u0393(\u03b1c)\u0393(\u03b2c) z\u03b1c\u22121(1\u2212 z)\u03b2c\u22121,\n(5) where c = {1, 2} to index the component. \u03b1c and \u03b2c are the two shape parameters controlling the shape of the Beta distribution. \u0393(\u00b7) is the Gamma function. Note that, as the Beta distribution is defined over the interval (0, 1), it is a natural choice for modelling distribution of probabilities or proportions [4], also the confidence scores in our case. In\naddition, it has a flexible shape that may model both symmetric and skewed distributions [1], both of which occur in our case, as shown in Figure 5. In contrast, the popular Gaussian distribution is defined over an unbounded range, and can only be used to model symmetric distributions.\nThe mixture model is suitable as the meta model for identifying correct and incorrect pseudo labels for three main reasons. First, it requires no supervision signals (i.e., whether a pseudo label is correct or not), which are not available in practice. Second, it is simple enough and introduces no extra hyper-parameters and hence requires no manual tuning. Third, inference and fitting are both efficient, thus causing neglectable overheads."
        },
        {
            "heading": "3.2. Alternative Update of DNN and BMM",
            "text": "With the confidence distribution paramterized by BMM, we are able to compute the posterior of a pseudo label being correct. We use the posterior as filtering weight to suppress the noise in pseudo labels when updating the deep neural network. As the DNN is continuously evolving, the BMM should be periodically updated to capture the latest confidence distribution. Therefore, the DNN and BMM are alternatively updated as below.\nUpdate DNN. Within the tth epoch, for each sample u with pseudo label y\u0303u, we compute the filtering weight w(y\u0303u) for it as the posterior of its pseudo label being correct based on the BMM:\nw(y\u0303u) = p(j|y\u0303u) = \u03b3 (t) j p(z = max(y\u0303u)|j)\u22112\nc=1 \u03b3 (t) c p(z = max(y\u0303u)|c)\ns.t. j = arg max c=1,2 E[p(z|c)] = arg max c=1,2\n\u03b1 (t) c\n\u03b1 (t) c + \u03b2 (t) c\n.\n(6) Note that, the second row which takes expectation on the\nAlgorithm 1: SSL with self-adaptive pseudo-label filtering. Input: Neural network h\u03b8(\u00b7), pseudo-label construction strategy T , labeled training set X , unlabeled training set U ,\nbatch size of labeled data B, relative batch size of unlabeled data \u00b5, learning rate \u03b7, training epoch N Output: The trained neural network h\u03b8(\u00b7)\n1 for t = 1 to N do 2 Empty the set of collected confidence scores S \u2190 \u2205; 3 for i = 1 to |U\u222aX|(1+\u00b5)B do 4 Sample batches X \u2032 = {(xb, yb)}Bb=1 \u2286 X , U \u2032 = {ub} \u00b5B b=1 \u2286 U ; 5 Construct pseudo labels Q \u2190 {y\u0303u | u \u2208 U \u2032} with srategy T ; 6 Compute per-sample filtering weight according to Eq. 6; 7 Update neural network by gradient descent \u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00b7 \u2207\u03b8L(\u03b8,X \u2032,U \u2032); 8 Collect confidence score S \u2190 S \u222a {max(y\u0303u) | y\u0303u \u2208 Q}; 9 end\n10 Update the BMM with collected statistics S by Expectation-Maximization;"
        },
        {
            "heading": "11 end",
            "text": "distribution on p(z|c) in Eq. 6 is to choose the component for the correct pseudo labels, which should have a higher average confidence score. By incorporating Eq. 6 into Eq. 1, we can then update the DNN parameters \u03b8 with gradient descent as usual.\nUpdate BMM. During the tth training epoch, we collect the confidence scores of the pseudo labels, which are denoted as S = {max(y\u0303u) | u \u2208 U}. At the end of epoch t, we compute new BMM parameters \u03d5(t+1) = {\u03b1(t+1)1 , \u03b2 (t+1) 2 , \u03b1 (t+1) 1 , \u03b2 (t+1) 2 , \u03b3 (t+1) 1 , \u03b3 (t+1) 2 } by fitting S via Expectation-Maximization (see the Supplemental for details). Algorithm 1 summarizes the steps."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we first conduct experiments on four image classification datasets (CIFAR-10, CIFAR-100 [12], Mini-ImageNet [31], and Oxford Flower [21]) to evaluate the effectiveness of our method. We then ablate different components of it to justify our design choices and to demonstrate its effectiveness. Unless stated otherwise, we implement our method SPF-RA by incorporating SPF to FixMatch-RA [29]. We report the mean and standard deviation of error rates over 3 runs. Refer to the Supplemental for the implementation details."
        },
        {
            "heading": "4.1. Comparison on CIFAR-10 and CIFAR-100",
            "text": "CIFAR-10 [12] is a dataset with 60K 32 \u00d7 32 images evenly distributed across 10 classes. The training set and test set contain 50K and 10K images, respectively. CIFAR100 is similar to CIFAR-10, but with 100 classes. On these two datasets, we compare our method SPF-RA with several state-of-the-art methods closely related to ours, includ-\ning FixMatch-RA [29], Dash-RA [33], RYS2 [27]. Besides, some other popular methods such as \u03a0-Model [14], Pseudo-Labeling [15], MixMatch [3], UDA [32], ReMixMatch [2] and SimPLE [9] are also included in the comparison. Following these works, we use Wide-ResNet-28-2 [34] for CIFAR-10, and Wide-ResNet-28-8 for CIFAR-100.\nWe perform our experiments by varying the amount of labeled data, following standard SSL evaluation protocols [2, 3, 22, 29]. Table 1 shows the performance comparison on top-1 error rate (lower is better). We can see that SPF-RA is the best-performing one in most splits, which demonstrates the effectiveness of our self-adaptive label filtering approach. In addition, we observe that the improvement is especially remarkable when the amount of labeled data is extremely scarce. Specifically, when there are only 4 labels per-class, SPF-RA decreases the error rate by more than 6% on both CIFAR-10 and CIFAR-100, as it can mitigate overfitting on labeled data, which is typically more severe under such an extreme scenario.\nAs SPF is a plug-and-play component, we also incorporate it into another method, i.e., MeanTeacher, referred to as SPF-MT. The performance comparison of MeanTeacher and SPF-MT on CIFAR-10/100 is listed in Table 2. SPF shows clear advantages here again."
        },
        {
            "heading": "4.2. Comparison on Mini-ImageNet and Flower",
            "text": "To demonstrate the advantages of SPF on more complex datasets, we conduct experiments on both MiniImageNet [31] and Oxford Flower Dataset [21]. In these experiments, we use ResNet-18 [7] architecture.\nCompared to CIFAR-10/100, Mini-ImageNet is a more complex data set as its categories and images are directly sampled from the large-scale ImageNet [5]. It consists of\n2The authors did not name their algorithm in [27]. Following [33], we name it RYS.\n100 classes with 600 images per class. Image resolution is downscaled to 84 \u00d7 84. We compare our SPF-RA method with several latest methods, including MeanTeacher [30], LP [10], DAG [17], and FixMatch-RA [29], under 4k and 10k labels. We show all results in Table. 3. SPF-RA achieves notable improvement over the FixMatch-RA baseline and the previous state-of-the-art (DAG). Specifically, SPF-RA reduces the error rate by 9.86% compared with FixMatch-RA and by 5.86% compared with DAG under 4k labels.\nOxford Flower [21] is a fine-grained image classification dataset that consists of 102 different categories of flowers. Each category consists of between 40 and 258 images. Since fine-grained classification is extremely challenging when the number of labels is scarce, previous works usually\ndid not experiment with such a dataset. We compare SPFRA with the supervised only baseline and FixMatch-RA. As shown in Table. 4, SPF-RA still has a clear improvement on this dataset. Under the SSL setting with 204 labels, i.e., only two labeled samples per class, SPF-RA outperforms FixMatch-RA by 30.08% on accuracy."
        },
        {
            "heading": "4.3. Ablation Study",
            "text": "To further understand how our meta model based label filter helps improve the SSL performances, we perform an extensive ablation study on a moderately challenging setting: CIFAR-100 dataset with 400 labels. Due to the number of experiments in our ablation study, we run 800 epochs to save the GPU running time. Nevertheless, the model can still converge, as we apply the cosine annealing learning rate decay [19]. Table 5 shows the results. We can see that with our meta model based label filter, SPF-RA achieves a top-1 error rate of 47.16\u00b1 1.56%, remarkably outperforming 60.56 \u00b1 1.79% achieved by FixMatch-RA (which uses constant confidence thresholding (CCT)). In addition, although we reduced the number of training epochs to 800, SPF-RA still outperforms FixMatch-RA, which has more than 10k training epochs and achieves a top-1 error of 48.85 \u00b1 1.75% as shown in Table 1. This further demonstrates the superiority of our approach.\nConfidence weighting (CW) and confidence threshold\nramp-up (CTR). In our filtering approach, we apply a meta model (i.e. BMM) to fit the confidence distribution of pseudo labels, and compute the posterior of a pseudo label being correct as its weight based on the meta model. A naive baseline is directly using the confidence score as the weight without any modelling. We denote this as confidence weighting (CW). In addition, as shown in Figure 5(d), our method produces the effect of adaptively increasing the threshold. Hence, we also compare our method with confidence threshold ramp-up (CTR), which gradually increases the confidence threshold according to a predefined schedule. We include two baselines, \u201cCTR-Sigmoid\u201d and \u201cCTRLinear\u201d, which use the sigmoid and linear ramp-up functions, respectively. Both of them gradually increase the threshold from 0 to 0.95 during the initial 40% epochs.\nWe show the results in Table 5. First, CW achieves a lower error rate than constant confidence thresholding (CCT). This may be due to the fact that CW on the one hand provides a natural filter that is able to assign higher weights to the correct pseudo labels than to the incorrect ones, and on the other hand avoids overfitting on labeled data by taking every unlabeled data into consideration even in the early stage. Our meta model based filter further improves the performance by giving even higher weights to those high-confidence pseudo labels and lower weights to those low-confidence ones, by taking consideration of the whole confidence distribution. Second, both CTR-Sigmoid and CTR-Linear outperform the plain baseline CT, indicating that it is beneficial to consider model evolvement. Although tuning the ramp-up schedule may help improve the performance, it requires an exhaustive trial-and-error loop. Instead, ours method, which is based on the meta model, is self-adaptive and performs significantly better.\nHard masking and soft weighting. Just like in confidence thresholding [29, 33], it is also possible to binarize the posterior using some threshold, yielding a \u201chard\u201d masking version of our approach. We study the effect of using different\nthresholds. Figure 6 (Left) shows the results. Using a high threshold value of 0.95 or extremely low threshold value of 0 significantly increases the test error by more than 3.2%, indicating that the quality and quantity of the pseudo labels are both important. In addition, although the best quantityquality trade-off can be achieved by using a threshold value of 0.2, it still cannot outperform \u201csoft\u201d weighting. In summary, hard masking is not able to outperform soft weighting. Instead, it introduces an extra hyper-parameter, i.e., a threshold value.\nRobustness of the meta model to update frequency. In Algorithm 1, we update the meta model once per-epoch. However, it is possible to change the update frequency. Note that while a more frequent update allows the meta model to respond more promptly to the learning state of the network, it sacrifices the quantity of the confidence scores for the pseudo labels collected to fit the meta model, causing inaccurate estimation of the meta model parameters. Here, we try increasing/decreasing the update frequency by a factor of two and four. Figure 6 (Right) shows the test error w.r.t. the update frequency. We can see that while the lowest test error is achieved at the default frequency (once per-epoch), the performance degrades only slightly if we either increase or decrease the update frequency of the meta model. This means that our approach is robust to the update frequency of the meta model."
        },
        {
            "heading": "5. Related Works",
            "text": "Semi-supervised Learning (SSL). SSL is a transversal task for different domains with a huge diversity of approaches. We focus on methods closely related to ours, i.e., those using deep neural networks for image classification. When learning from labeled data, these methods use a crossentropy (or similar) loss as in standard supervised learning. When learning from unlabeled data, pseudo labels are constructed with a teacher model. The teacher model can be\na fixed pretrained model [15]. However, a fixed teacher is not able to correct itself, leading to inferior performances to the student model. Hence, recent approaches typically construct the teacher model from the student model itself on the fly [3, 9, 17, 29], or by co-training together with the student model [11, 25]. Nonetheless, it is still not guaranteed that the generated pseudo labels are correct as the teacher model itself is not perfect. Those incorrect pseudo labels lead the student model to reinforce its incorrectness, hence preventing it from learning new knowledge, which is known as confirmation bias [30].\nAs a pseudo-label filtering method, our approach aims to suppress noise in pseudo labels dynamically as the model evolves, and can easily be incorporated into the existing SSL methods to help improve their performances.\nPseudo-label filtering. To mitigate the confirmation bias, existing SSL methods are typically equipped with some pseudo-label filtering strategy. A simple and straightforward approach is to gradually ramp up the weight of unlabeled loss [2, 3, 30], which can be viewed as filtering out pseudo labels produced during the initial stage of the training process. These pseudo labels are expected to be of low quality as the teacher model is in its infancy. However, this approach is not efficient as it treats all pseudo labels produced at a specific period of time equally, without considering per-sample correctness. Alternatively, some SSL methods [29, 32] apply a constant confidence threshold (e.g., 0.95) for pseudo-label filtering. Only those pseudo labels with a confidence score (i.e., the predicted probability for the most possible class) higher than the threshold is kept for network update. Such a method is further improved by using a progressive threshold over the training process [33]. Although progressive thresholding is effective, it raises a question: what is the problem with a constant high threshold while a high threshold seems to reject more incorrect pseudo labels? Besides, the approach proposed in [33] still relies on a predefined schedule to adjust the threshold w.r.t. the training progress. Hence, it requires careful tuning and may result in suboptimal performance of the final model.\nIn this paper, we reveal a risk caused by a high constant threshold: it may cause the deep network to overfit the labeled data in the initial stage, as it masks out majority of the pseudo-labelled data. To address this problem, we propose a fully self-adaptive filtering strategy, which is based on an unsupervised mixture model and hence requires no manual tuning."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we have discussed the drawbacks of existing pseudo-label filtering approaches for SSL. In particular, we have shown that pseudo-label filtering with a constant threshold, which is commonly used in recent state-of-theart SSL methods, does not take model evolvement into consideration. This causes overfitting on the labeled training data in initial training stage. While progressive thresholding may help improve pseudo-label filtering, a handcrafted schedule requires carefully tuning and may be suboptimal. To mitigate this problem, we have proposed a self-adaptive label filtering approach by learning a meta model online. It is interactively changing as the model evolves. Our method can be plug-and-play into existing SSL methods. We experimentally show that our proposed method helps improve the performances of existing SSL methods, especially when the labelled data is extremely scarce.\nOur meta model does have limitations. As it is not updated in real time, our approach may not be stable if the DNN evolves rapidly (e.g., if a large learning rate is applied). Note that our meta model is updated with the statistics collected from the last epoch. The confidence distribution captured from it has a small lag to the actual instant confidence distribution. If the model evolves smoothly, which is typically the case, the lag has very small effects. However, if the model evolves rapidly, the filtering weight will be wrongly computed with a stale meta model. Figure 7 demonstrates such a failure on the toy dataset. In practice, this kind of problem can occur if an extremely large learning rate is used.\nAppendix In this Appendix, we first provide the ExpectationMaximization procedure for Beta Mixture Model (Section A) and implementation details for our experiments (Section B). We then discuss more details about feature and model selection for our meta model (Section C) and overheads caused by the meta model (Section D)."
        },
        {
            "heading": "A. Expectation-Maximization for BMM",
            "text": "We use an Expectation-Maximization procedure to estimate the parameters of our meta model, i.e. the twocomponent BMM. Specifically, we introduce a latent variable W \u2208 Rn\u00d72, in which Wij is the responsibility of component j for the sample point zi such that \u22112 j=1 Wij = 1,\u2200i. In E-step, we fix the visible parameter collection \u03d5 = {\u03b11, \u03b21, \u03b12, \u03b22, \u03b31, \u03b32} of BMM, and compute the latent variable W using Bayes rule:\nWij = p(j|si) = \u03b3jB(zi|\u03b1j , \u03b2j)\u22112\nk=1 \u03b3kB(zi|\u03b1k, \u03b2k) , (7)\nwhere i = 1, 2, ..., n and j = 1, 2 index the rows and columns of the latent variable W respectively. In M-step, given W computed in the E-step, we first compute the mean \u00b5j and variance \u03c32j for each component:\n\u00b5j = \u2211n i=1 Wijsi\u2211n i=1 Wij , \u03c32j = \u2211n i=1 Wij(si \u2212 \u00b5j)2\u2211n i=1 Wij . (8)\nThen we update the shape parameters of each component, i.e. \u03b1j and \u03b2j , using the method of moments:\n\u03b1j = \u00b5j( \u00b5j(1\u2212 \u00b5j)\n\u03c32j \u22121), \u03b2j = (1\u2212\u00b5j)( \u00b5j(1\u2212 \u00b5j) \u03c32j \u22121).\n(9) Finally, the weights of components \u03b3j are calculated as:\n\u03b3j = 1\nn n\u2211 i=1 Wij . (10)\nIn our experiments, following [1], we run the above E- and M-steps for 10 iterations in each fitting.\nB. Implementation Details In this section, we provide implementation details for the experiments. All experiments are performed on 4 singleGPU machines, each of which is equipped with an NVIDIA 2080Ti 11GB GPU.\nB.1. CIFAR-10/100 Experiments\nFollowing FixMatch [29], we use SGD optimizer with Nesterov momentum to optimize the deep neural network.\nMomentum of the optimizer is set to 0.9. We apply a cosine learning rate decay schedule as \u03b7 = \u03b70 cos( 7\u03c0k16K ), where \u03b70 is the initial learning rate, k is the current training step, and K is the total training step. The initial learning rate \u03b70 is set as 0.03.\nFor CIFAR-10 experiments, we use WideResNet-28-2 architecture, and set the weight decay 5e\u2212 4. In each training step, we feed the network a mini-batch containing 64 labeled samples and 7\u00d7 64 = 448 unlabeled samples. The network is trained for 10k epochs. Such a setup is identical to it used in FixMatch [29].\nFor CIFAR-100 experiments, we use WideResNet-28-8 architecture and correspondingly change weight decay to 5e \u2212 3. To fit mini-batches into 11GB GPU memory, we decrease the batch size from 512 to 256. To save GPU running time, we also decrease the training epoch. We find that training for 2k epochs is sufficient for our algorithm (SPF-RA) to outperform FixMatch-RA [29]. For fair comparison, we also run FixMatch-RA under the same setting. The comparison of top-1 error is shown in Table 6. We can see that FixMatch-RA has higher error rates when the training epoch is decreased to 2k. Hence our SPF-RA (trained for 2k epochs) still performs best.\nB.2. Mini-ImageNet and Oxford Flower\nFor the models trained on Mini-ImageNet and Oxford Flower, we apply the ResNet-18 [7] architecture. We use SGD with a learning rate of 0.01. The batch size is 128, and the weight decay is 1e\u22123. The training epoch is 1000 for Mini-ImageNet and 300 for Oxford Flower. The learning rate decay schedule is the same as that used in the CIFAR experiments."
        },
        {
            "heading": "C. Feature and Model Selection for Meta Model",
            "text": "We fit a probabilistic meta model online to help identify correct and incorrect pseudo labels in our approach. In this section, we provide more details about feature and model selection for the meta model.\nFor feature selection, an alternative to the confidence score is loss value, which has been shown helpful for recognizing noisy manual annotations in dataset [1, 16]. For model selection, as we use the Beta Mixture Model (BMM)\nin our approach, a straightforward alternative is the more popular Gaussian Mixture Model (GMM). We do not consider training a supervised model with meta statistics collected from labeled training data since there is a distributional gap between it of labeled and unlabeled training data as shown in Figure 8(a) and Figure 8(b). Such a gap makes the supervised model cannot be well transferred to unlabeled training data.\nAs the meta model is applied for online binary classification (i.e. identifying correct and incorrect pseudo labels), we compare different feature and model combinations in terms of the Area Under the Receiver Operating Characteristic (AUROC) evolution during training. Note that a higher AUROC score means better classification performance. Empirically, acceptable discrimination requires an AUROC score higher than 0.7 [8]. Figure 8(c) shows the results. First, we can see that the correct and incorrect pseudo labels are more discriminative with confidence score than loss value. Intuitively, this is because the unsupervised loss is more dominated by the perturbation strength of inputs or the gap between teacher model and student model, instead of the correctness of the pseudo label. Second, when using confidence as the input feature for our meta model, though the BMM and GMM produce similar classification performance at the beginning of training, BMM gradually outperforms GMM by a considerable margin. This is due to the confidence distributions become more and more skewed towards 1, and Beta distribution is flexible to handle such skewness. On the contrary, Gaussian distribution is always in symmetric shape."
        },
        {
            "heading": "D. Overheads of the Meta Model",
            "text": "The fitting and inference of our meta model can both cause overheads. Note that the fitting happens at the end of each epoch, and the inference happens every training step\nwhen we feed DNN a mini-batch. Hence we compare both average epoch time and batch time in Table 7. We can see that the overheads caused by our meta model are less than 2%, which is neglectable."
        }
    ],
    "title": "Towards Self-Adaptive Pseudo-Label Filtering for Semi-Supervised Learning",
    "year": 2023
}