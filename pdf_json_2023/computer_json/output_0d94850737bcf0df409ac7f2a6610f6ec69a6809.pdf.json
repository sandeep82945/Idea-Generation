{
    "abstractText": "Keyphrase Extraction (KE) is a critical component in Natural Language Processing (NLP) systems for selecting a set of phrases from the document that could summarize the important information discussed in the document. Typically, a keyphrase extraction system can significantly accelerate the speed of information retrieval and help people get first-hand information from a long document quickly and accurately. Specifically, keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. In this paper, we introduce keyphrase extraction, present a review of the recent studies based on pre-trained language models, offer interesting insights on the different approaches, highlight open issues, and give a comparative experimental study of popular supervised as well as unsupervised techniques on several datasets. To encourage more instantiations, we release the related files mentioned in this paper1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mingyang Song"
        },
        {
            "affiliations": [],
            "name": "Yi Feng"
        },
        {
            "affiliations": [],
            "name": "Liping Jing"
        }
    ],
    "id": "SP:b51114fe005757e34ec348465012d229c79dd6f8",
    "references": [
        {
            "authors": [
                "Joshua Ainslie",
                "Santiago Onta\u00f1\u00f3n",
                "Chris Alberti",
                "Vaclav Cvicek",
                "Zachary Fisher",
                "Philip Pham",
                "Anirudh Ravula",
                "Sumit Sanghai",
                "Qifan Wang",
                "Li Yang."
            ],
            "title": "ETC: encoding long and structured inputs in transformers",
            "venue": "Proceedings of the 2020 Conference",
            "year": 2020
        },
        {
            "authors": [
                "Rabah Alzaidy",
                "Cornelia Caragea",
                "C. Lee Giles."
            ],
            "title": "Bi-lstm-crf sequence labeling for keyphrase extraction from scholarly documents",
            "venue": "WWW, pages 2551\u20132557. ACM.",
            "year": 2019
        },
        {
            "authors": [
                "Isabelle Augenstein",
                "Mrinal Das",
                "Sebastian Riedel",
                "Lakshmi Vikraman",
                "Andrew McCallum."
            ],
            "title": "Semeval 2017 task 10: Scienceie - extracting keyphrases and relations from scientific publications",
            "venue": "SemEval@ACL, pages 546\u2013555. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Marco Basaldella",
                "Elisa Antolli",
                "Giuseppe Serra",
                "Carlo Tasso."
            ],
            "title": "Bidirectional LSTM recurrent neural network for keyphrase extraction",
            "venue": "Digital Libraries and Multimedia Archives - 14th Italian Research Conference on Digital Libraries, IRCDL",
            "year": 2018
        },
        {
            "authors": [
                "Kamil Bennani-Smires",
                "Claudiu Musat",
                "Andreea Hossmann",
                "Michael Baeriswyl",
                "Martin Jaggi."
            ],
            "title": "Simple unsupervised keyphrase extraction using sentence embeddings",
            "venue": "CoNLL, pages 221\u2013229. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Adrien Bougouin",
                "Florian Boudin",
                "B\u00e9atrice Daille."
            ],
            "title": "Topicrank: Graph-based topic ranking for keyphrase extraction",
            "venue": "IJCNLP, pages 543\u2013551. Asian Federation of Natural Language Processing / ACL.",
            "year": 2013
        },
        {
            "authors": [
                "Adrien Bougouin",
                "Florian Boudin",
                "B\u00e9atrice Daille."
            ],
            "title": "Keyphrase annotation with graph co-ranking",
            "venue": "COLING, pages 2945\u20132955. ACL.",
            "year": 2016
        },
        {
            "authors": [
                "Ricardo Campos",
                "V\u00edtor Mangaravite",
                "Arian Pasquali",
                "Al\u00edpio M\u00e1rio Jorge",
                "C\u00e9lia Nunes",
                "Adam Jatowt."
            ],
            "title": "A text feature based automatic keyword extraction method for single documents",
            "venue": "ECIR, volume 10772 of Lecture Notes in Computer Science,",
            "year": 2018
        },
        {
            "authors": [
                "Ricardo Campos",
                "V\u00edtor Mangaravite",
                "Arian Pasquali",
                "Al\u00edpio M\u00e1rio Jorge",
                "C\u00e9lia Nunes",
                "Adam Jatowt."
            ],
            "title": "Yake! collection-independent automatic keyword extractor",
            "venue": "ECIR, volume 10772 of Lecture Notes in Computer Science, pages 806\u2013810. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Cornelia Caragea",
                "Florin Adrian Bulgarov",
                "Andreea Godea",
                "Sujatha Das Gollapalli."
            ],
            "title": "Citationenhanced keyphrase extraction from research papers: A supervised approach",
            "venue": "EMNLP, pages 1435\u2013 1446. ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Boli Chen",
                "Yao Fu",
                "Guangwei Xu",
                "Pengjun Xie",
                "Chuanqi Tan",
                "Mosha Chen",
                "Liping Jing."
            ],
            "title": "Probing bert in hyperbolic spaces",
            "venue": "International Conference on Learning Representations. 2161",
            "year": 2021
        },
        {
            "authors": [
                "Soheil Danesh",
                "Tamara Sumner",
                "James H. Martin."
            ],
            "title": "Sgrank: Combining statistical and graphical methods to improve the state of the art in unsupervised keyphrase extraction",
            "venue": "*SEM@NAACL-HLT, pages 117\u2013126. The *SEM 2015 Organizing Com-",
            "year": 2015
        },
        {
            "authors": [
                "Wietse de Vries",
                "Andreas van Cranenburgh",
                "Malvina Nissim."
            ],
            "title": "What\u2019s so special about bert\u2019s layers? a closer look at the nlp pipeline in monolingual and multilingual models",
            "venue": "EMNLP (Findings), pages 4339\u20134350. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Haoran Ding",
                "Xiao Luo."
            ],
            "title": "Attentionrank: Unsupervised keyphrase extraction using self and cross attentions",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1919\u20131928.",
            "year": 2021
        },
        {
            "authors": [
                "Samhaa R. El-Beltagy",
                "Ahmed A. Rafea."
            ],
            "title": "Kpminer: A keyphrase extraction system for english and arabic documents",
            "venue": "Inf. Syst., 34(1):132\u2013144.",
            "year": 2009
        },
        {
            "authors": [
                "Corina Florescu",
                "Cornelia Caragea."
            ],
            "title": "A new scheme for scoring phrases in unsupervised keyphrase extraction",
            "venue": "ECIR, volume 10193 of Lecture Notes in Computer Science, pages 477\u2013483.",
            "year": 2017
        },
        {
            "authors": [
                "Corina Florescu",
                "Cornelia Caragea."
            ],
            "title": "Positionrank: An unsupervised approach to keyphrase extraction from scholarly documents",
            "venue": "ACL (1), pages 1105\u20131115. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Sujatha Das Gollapalli",
                "Xiaoli Li",
                "Peng Yang."
            ],
            "title": "Incorporating expert knowledge into keyphrase extraction",
            "venue": "AAAI, pages 3180\u20133187. AAAI Press.",
            "year": 2017
        },
        {
            "authors": [
                "Maria P. Grineva",
                "Maxim N. Grinev",
                "Dmitry Lizorkin."
            ],
            "title": "Extracting key terms from noisy and multitheme documents",
            "venue": "WWW, pages 661\u2013670. ACM.",
            "year": 2009
        },
        {
            "authors": [
                "Kazi Saidul Hasan",
                "Vincent Ng."
            ],
            "title": "Automatic keyphrase extraction: A survey of the state of the art",
            "venue": "ACL (1), pages 1262\u20131273. The Association for Computer Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "Chong Huang",
                "Yonghong Tian",
                "Zhi Zhou",
                "Charles X. Ling",
                "Tiejun Huang."
            ],
            "title": "Keyphrase extraction using semantic networks structure analysis",
            "venue": "ICDM, pages 275\u2013284. IEEE Computer Society.",
            "year": 2006
        },
        {
            "authors": [
                "Anette Hulth."
            ],
            "title": "Improved automatic keyword extraction given more linguistic knowledge",
            "venue": "EMNLP.",
            "year": 2003
        },
        {
            "authors": [
                "Anette Hulth."
            ],
            "title": "Enhancing linguistically oriented automatic keyword extraction",
            "venue": "HLT-NAACL (Short Papers). The Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "James Jardine",
                "Simone Teufel."
            ],
            "title": "Topical PageRank: A model of scientific expertise for bibliographic search",
            "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501\u2013510, Gothenburg,",
            "year": 2014
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What does bert learn about the structure of language? In ACL (1), pages 3651\u20133657",
            "venue": "Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Xin Jiang",
                "Yunhua Hu",
                "Hang Li."
            ],
            "title": "A ranking approach to keyphrase extraction",
            "venue": "SIGIR, pages 756\u2013757. ACM.",
            "year": 2009
        },
        {
            "authors": [
                "Karen Sp\u00e4rck Jones."
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "J. Documentation, 60(5):493\u2013502.",
            "year": 2004
        },
        {
            "authors": [
                "Su Nam Kim",
                "Olena Medelyan",
                "Min-Yen Kan",
                "Timothy Baldwin."
            ],
            "title": "Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles",
            "venue": "SemEval@ACL, pages 21\u201326. The Association for Computer Linguistics.",
            "year": 2010
        },
        {
            "authors": [
                "M. Krapivin",
                "M. Marchese"
            ],
            "title": "Large dataset for keyphrase extraction",
            "year": 2009
        },
        {
            "authors": [
                "Mayank Kulkarni",
                "Debanjan Mahata",
                "Ravneet Arora",
                "Rajarshi Bhowmik."
            ],
            "title": "Learning rich representation of keyphrases from text",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Niraj Kumar",
                "Kannan Srinathan."
            ],
            "title": "Automatic keyphrase extraction from scientific documents using n-gram filtration technique",
            "venue": "ACM Symposium on Document Engineering, pages 199\u2013208. ACM.",
            "year": 2008
        },
        {
            "authors": [
                "Xinnian Liang",
                "Shuangzhi Wu",
                "Mu Li",
                "Zhoujun Li."
            ],
            "title": "Unsupervised keyphrase extraction by jointly modeling local and global context",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 155\u2013164, Online",
            "year": 2021
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "EMNLP/IJCNLP (1), pages 3728\u20133738. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyuan Liu",
                "Peng Li",
                "Yabin Zheng",
                "Maosong Sun."
            ],
            "title": "Clustering to find exemplar terms for keyphrase extraction",
            "venue": "EMNLP, pages 257\u2013266. ACL.",
            "year": 2009
        },
        {
            "authors": [
                "Debanjan Mahata",
                "John Kuriakose",
                "Rajiv Ratn Shah",
                "Roger Zimmermann."
            ],
            "title": "Key2vec: Automatic ranked keyphrase extraction from scientific articles using phrase embeddings",
            "venue": "Proceedings of the 2018 Conference of the North American Chap-",
            "year": 2018
        },
        {
            "authors": [
                "Debanjan Mahata",
                "Rajiv Ratn Shah",
                "John Kuriakose",
                "Roger Zimmermann",
                "John R. Talburt."
            ],
            "title": "Theme-weighted ranking of keywords from text documents using phrase embeddings",
            "venue": "IEEE 1st Conference on Multimedia Information Processing and",
            "year": 2018
        },
        {
            "authors": [
                "Rui Meng",
                "Sanqiang Zhao",
                "Shuguang Han",
                "Daqing He",
                "Peter Brusilovsky",
                "Yu Chi."
            ],
            "title": "Deep keyphrase generation",
            "venue": "ACL, pages 582\u2013592. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "Textrank: Bringing order into text",
            "venue": "EMNLP, pages 404\u2013411. ACL.",
            "year": 2004
        },
        {
            "authors": [
                "Funan Mu",
                "Zhenting Yu",
                "Lifeng Wang",
                "Yequan Wang",
                "Qingyu Yin",
                "Yibo Sun",
                "Liqun Liu",
                "Teng Ma",
                "Jing Tang",
                "Xing Zhou."
            ],
            "title": "Keyphrase extraction with span-based feature representations",
            "venue": "CoRR, abs/2002.05407.",
            "year": 2020
        },
        {
            "authors": [
                "David Newman",
                "Nagendra Koilada",
                "Jey Han Lau",
                "Timothy Baldwin."
            ],
            "title": "Bayesian text segmentation for index term identification and keyphrase extraction",
            "venue": "COLING, pages 2077\u20132092. Indian Institute of Technology Bombay.",
            "year": 2012
        },
        {
            "authors": [
                "Chau Q. Nguyen",
                "Tuoi T. Phan."
            ],
            "title": "An ontologybased approach for key phrase extraction",
            "venue": "ACL/IJCNLP (Short Papers), pages 181\u2013184. The Association for Computer Linguistics.",
            "year": 2009
        },
        {
            "authors": [
                "Thuy Dung Nguyen",
                "Min-Yen Kan."
            ],
            "title": "Keyphrase extraction in scientific publications",
            "venue": "ICADL, volume 4822 of Lecture Notes in Computer Science, pages 317\u2013326. Springer.",
            "year": 2007
        },
        {
            "authors": [
                "Eirini Papagiannopoulou",
                "Grigorios Tsoumakas."
            ],
            "title": "Local word vectors guiding keyphrase extraction",
            "venue": "Inf. Process. Manag., 54(6):888\u2013902.",
            "year": 2018
        },
        {
            "authors": [
                "Eirini Papagiannopoulou",
                "Grigorios Tsoumakas."
            ],
            "title": "A review of keyphrase extraction",
            "venue": "CoRR, abs/1905.05044.",
            "year": 2019
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "NAACL-HLT, pages 2227\u20132237. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Dhruva Sahrawat",
                "Debanjan Mahata",
                "Haimin Zhang",
                "Mayank Kulkarni",
                "Agniv Sharma",
                "Rakesh Gosangi",
                "Amanda Stent",
                "Yaman Kumar",
                "Rajiv Ratn Shah",
                "Roger Zimmermann"
            ],
            "title": "Keyphrase extraction as sequence labeling using contextualized embeddings",
            "year": 2020
        },
        {
            "authors": [
                "Arnav Saxena",
                "Mudit Mangal",
                "Goonjan Jain."
            ],
            "title": "Keygames: A game theoretic approach to automatic keyphrase extraction",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 2037\u20132048.",
            "year": 2020
        },
        {
            "authors": [
                "Mingyang Song",
                "Yi Feng",
                "Liping Jing."
            ],
            "title": "Hyperbolic relevance matching for neural keyphrase extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Mingyang Song",
                "Yi Feng",
                "Liping Jing."
            ],
            "title": "Utilizing BERT intermediate layers for unsupervised keyphrase extraction",
            "venue": "Proceedings of the 5th International Conference on Natural Language and Speech Processing (ICNLSP 2022), pages 277\u2013281,",
            "year": 2022
        },
        {
            "authors": [
                "Mingyang Song",
                "Liping Jing",
                "Lin Xiao."
            ],
            "title": "Importance Estimation from Multiple Perspectives for Keyphrase Extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Si Sun",
                "Chenyan Xiong",
                "Zhenghao Liu",
                "Zhiyuan Liu",
                "Jie Bao."
            ],
            "title": "Joint keyphrase chunking and salience ranking with bert",
            "venue": "CoRR, abs/2004.13639.",
            "year": 2020
        },
        {
            "authors": [
                "Yi Sun",
                "Hangping Qiu",
                "Yu Zheng",
                "Zhongwei Wang",
                "Chaoran Zhang."
            ],
            "title": "Sifrank: A new baseline for unsupervised keyphrase extraction based on pre-trained language model",
            "venue": "IEEE Access, 8:10896\u2013 10906.",
            "year": 2020
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Jian Tang",
                "Pan Du",
                "Zhi-Hong Deng",
                "Jian-Yun Nie."
            ],
            "title": "Divgraphpointer: A graph pointer network for extracting diverse keyphrases",
            "venue": "SIGIR, pages 755\u2013764. ACM.",
            "year": 2019
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "Bert rediscovers the classical nlp pipeline",
            "venue": "ACL (1), pages 4593\u20134601. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Takashi Tomokiyo",
                "Matthew Hurst."
            ],
            "title": "A language model approach to keyphrase extraction",
            "venue": "pages 33\u201340. Association for Computational Linguistics.",
            "year": 2003
        },
        {
            "authors": [
                "Peter D. Turney."
            ],
            "title": "Learning to extract keyphrases from text",
            "venue": "National Research Council Canada, Institute for Information Technology, Technical Report ERB-1057.",
            "year": 1999
        },
        {
            "authors": [
                "Peter D. Turney."
            ],
            "title": "Learning algorithms for keyphrase extraction",
            "venue": "Inf. Retr., 2(4):303\u2013336.",
            "year": 2000
        },
        {
            "authors": [
                "Xiaojun Wan",
                "Jianguo Xiao."
            ],
            "title": "Collabrank: Towards a collaborative approach to single-document keyphrase extraction",
            "venue": "COLING, pages 969\u2013976.",
            "year": 2008
        },
        {
            "authors": [
                "Xiaojun Wan",
                "Jianguo Xiao."
            ],
            "title": "Single document keyphrase extraction using neighborhood knowledge",
            "venue": "AAAI, pages 855\u2013860. AAAI Press.",
            "year": 2008
        },
        {
            "authors": [
                "Rui Wang",
                "Wei Liu",
                "Chris McDonald."
            ],
            "title": "Using word embeddings to enhance keyword identification for scientific publications",
            "venue": "Databases Theory and Applications - 26th Australasian Database Conference, ADC 2015, Melbourne, VIC, Australia, June 4-",
            "year": 2015
        },
        {
            "authors": [
                "Yanan Wang",
                "Qi Liu",
                "Chuan Qin",
                "Tong Xu",
                "Yijun Wang",
                "Enhong Chen",
                "Hui Xiong."
            ],
            "title": "Exploiting topic-based adversarial neural network for cross-domain keyphrase extraction",
            "venue": "IEEE International Conference on Data Mining, ICDM 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Yansen Wang",
                "Zhen Fan",
                "Carolyn Penstein Ros\u00e9."
            ],
            "title": "Incorporating multimodal information in opendomain web keyphrase extraction",
            "venue": "EMNLP (1), pages 1790\u20131800. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Ian H. Witten",
                "Gordon W. Paynter",
                "Eibe Frank",
                "Carl Gutwin",
                "Craig G. Nevill-Manning."
            ],
            "title": "Kea: Practical automatic keyphrase extraction",
            "venue": "ACM DL, pages 254\u2013255. ACM.",
            "year": 1999
        },
        {
            "authors": [
                "Lee Xiong",
                "Chuan Hu",
                "Chenyan Xiong",
                "Daniel Campos",
                "Arnold Overwijk."
            ],
            "title": "Open domain web keyphrase extraction beyond language modeling",
            "venue": "EMNLP/IJCNLP (1), pages 5174\u20135183. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Wei You",
                "Dominique Fontaine",
                "Jean-Paul A. Barth\u00e8s."
            ],
            "title": "Automatic keyphrase extraction with a refined candidate set",
            "venue": "Web Intelligence, pages 576\u2013579. IEEE Computer Society.",
            "year": 2009
        },
        {
            "authors": [
                "Linhan Zhang",
                "Qian Chen",
                "Wen Wang",
                "Chong Deng",
                "Shiliang Zhang",
                "Bing Li",
                "Wei Wang",
                "Xin Cao."
            ],
            "title": "Mderank: A masked document embedding rank approach for unsupervised keyphrase extraction",
            "venue": "CoRR, abs/2110.06651.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization",
            "venue": "CoRR, abs/1905.06566.",
            "year": 2019
        },
        {
            "authors": [
                "Yongzheng Zhang",
                "A. Nur Zincir-Heywood",
                "Evangelos E. Milios."
            ],
            "title": "World wide web site summarization",
            "venue": "Web Intell. Agent Syst., 2(1):39\u201353.",
            "year": 2004
        },
        {
            "authors": [
                "Ming Zhong",
                "Pengfei Liu",
                "Yiran Chen",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Extractive summarization as text matching",
            "venue": "ACL, pages 6197\u2013 6208. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Xuan Zhou",
                "Xiao Zhang",
                "Chenyang Tao",
                "Junya Chen",
                "Bing Xu",
                "Wei Wang",
                "Jing Xiao."
            ],
            "title": "Multigrained knowledge distillation for named entity recognition",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2153\u20132164 May 2-6, 2023 \u00a92023 Association for Computational Linguistics\nKeyphrase Extraction (KE) is a critical component in Natural Language Processing (NLP) systems for selecting a set of phrases from the document that could summarize the important information discussed in the document. Typically, a keyphrase extraction system can significantly accelerate the speed of information retrieval and help people get first-hand information from a long document quickly and accurately. Specifically, keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. In this paper, we introduce keyphrase extraction, present a review of the recent studies based on pre-trained language models, offer interesting insights on the different approaches, highlight open issues, and give a comparative experimental study of popular supervised as well as unsupervised techniques on several datasets. To encourage more instantiations, we release the related files mentioned in this paper1."
        },
        {
            "heading": "1 Introduction",
            "text": "Keyphrase extraction is a fundamental task in NLP for identifying and extracting a set of keyphrases from the document that could summarize the important information discussed in the source document (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019). Keyphrases have enabled accurate and fast searching for the document from a large text corpus and have exhibited their potential in improving many NLP tasks, such as text summarization (Zhang et al., 2004). Various information filtering and extracting techniques are becoming critical with the ever-increasing amount of text data. Owing to its potential importance, keyphrase extraction has received more and more attention from\n\u2217Corresponding author. 1https://github.com/MySong7NLPer/\nKeyphraseExtractionSurvey\nNLP researchers. However, the keyphrase extraction task is far from being solved: state-of-the-art performance on keyphrase extraction is still lower than other core NLP tasks. Our goal in this paper is to investigate the state-of-the-art models in keyphrase extraction, examine the primary sources of errors made by existing systems, and discuss the challenges ahead.\nThe first keyphrase extraction task was organized by Turney (1999), which defines the keyphrase extraction task as \u201cthe automatic selection of important and topical phrases from the body of a document\u201d. Since then, there have been numerous keyphrase extraction models (Witten et al., 1999; Turney, 2000; Tomokiyo and Hurst, 2003; Hulth, 2004; Wan and Xiao, 2008a; Jiang et al., 2009; Liu et al., 2009; Grineva et al., 2009; Nguyen and Phan, 2009; Bougouin et al., 2013; Caragea et al., 2014; Danesh et al., 2015; Bougouin et al., 2016; Florescu and Caragea, 2017a; Campos et al., 2018a; Alzaidy et al., 2019). In the past two decades, keyphrase extraction methods have experienced the development from traditional approaches to deep learning methods (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019). With the recent development of Pre-trained Language Models (PLMs) (Devlin et al., 2019; Liu et al., 2019), many NLP tasks have significantly changed, that is, how to adopt and leverage pre-trained language models in the specific task. Therefore, many keyphrase extraction models (Sun et al., 2020a; Song et al., 2021) adopt PLMs as the embedding layer.\nWe present a comprehensive survey of recent advances in neural keyphrase extraction. We describe the neural keyphrase extraction systems based on pre-trained language models, which depend on different paradigms (e.g., one-stage (Wang et al., 2020) and two-stage (Sun et al., 2020a)), various tasks (e.g., classification and ranking (Mu et al., 2020; Sun et al., 2020a)), different learning strategies (e.g., supervised (Song et al., 2021) and un-\n2153\nsupervised (Ding and Luo, 2021)), and variants of pre-trained language models (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)).\nFurthermore, we re-implement and collect the results of the mentioned models on several benchmark keyphrase extraction datasets. We illustrate the results in Table 3 and Table 2 and discuss in Section 6 how neural keyphrase extraction systems have improved performance over past works, including supervised and unsupervised models. Furthermore, we provide resources, including links to share the current neural keyphrase extraction systems and links to share the code for each category of the neural keyphrase extraction approaches. To the best of our knowledge, this is the first survey focusing on the keyphrase extraction task based on recent pre-trained language models.\nOverall, this paper first discusses previous surveys on keyphrase extraction in Section 2.1 and give a briefly introduction about pre-trained language models in Section 2.2. Then we highlight standard, past, and recent benchmark keyphrase extraction datasets (from shared tasks and other research) in Section 3 and evaluation metrics in Section 4. We then describe neural keyphrase extraction systems in Section 5. Next, we give the analysis and discussion in Section 6. Finally, we summarize the conclusions and future directions of neural keyphrase extraction in Section 7. The limitations of our work is presented in Section 8."
        },
        {
            "heading": "2 Preliminary",
            "text": "In this section, we claim the differences between the current survey and the existing surveys. Next, we present the background of pre-trained language models and their importance in NLP."
        },
        {
            "heading": "2.1 Previous Surveys",
            "text": "The first comprehensive keyphrase extraction survey was Hasan and Ng (2014), which covered a variety of unsupervised and supervised keyphrase extraction models, highlighted common features used by existing models during that time, and explained evaluation metrics that are still in use today. Papagiannopoulou and Tsoumakas (2019) present a more recent keyphrase extraction survey that mainly included many unsupervised and supervised models based on deep learning. Furthermore, Papagiannopoulou and Tsoumakas (2019) also provides a list of popular keyphrase extraction datasets and a thorough empirical study.\nThe existing keyphrase extraction surveys primarily cover early feature-engineered and neuralbased keyphrase extraction models (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019). There is not yet, to our knowledge, a comprehensive survey of keyphrase extraction based on pretrained language models."
        },
        {
            "heading": "2.2 Pre-trained Language Models",
            "text": "Recently, pre-trained language models have advanced the state-of-the-art in many NLP tasks ranging from textual similarity to text summarization (Zhang et al., 2019; Liu and Lapata, 2019; Zhong et al., 2020) and named entity recognition (Zhou et al., 2021). State-of-the-art pre-trained models include LSTM-based language models (e.g., ELMo (Peters et al., 2018)) and Transformer-based language models (e.g., BERT2 (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)). Specifically, the transformer-based models learn bidirectional representations for words based on a masked language model and sentence adjacency training objective (Devlin et al., 2019). Simply using contextualized embeddings obtained from the transformerbased pre-trained language models in place of traditional embeddings has resulted in state-of-the-art performance on a range of NLP tasks. Therefore, pre-trained language models have been employed as encoders for obtaining word-, sentence-, and document-level representations to assist the downstream tasks."
        },
        {
            "heading": "3 Keyphrase Extraction Dataset",
            "text": "Since the first shared task on KE (Turney, 1999), many shared tasks and benchmark datasets for KE have been created. Specifically, OpenKP (Xiong et al., 2019), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010), SemEval2017 (Augenstein et al., 2017), and KP20k (Meng et al., 2017) were created from scientific articles in English.\nCompared with other datasets, KP20k contains a large amount of annotation data, so it is often used as the dataset to train the neural-based KE models recently. Meanwhile, in recent papers (Sun et al., 2020a; Song et al., 2021), Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin and Marchese, 2009), SemEval2010 (Kim et al., 2010), and SemEval2017 (Augenstein et al., 2017)\n2https://huggingface.co/bert-base-cased\ndatasets are often used as the zero-shot test sets to verify the robustness of the KE models trained by the KP20k dataset. Furthermore, KE tasks have also been organized on newswire articles in English, e.g., DUC2001 (Wan and Xiao, 2008b). Table 1 summarizes the statistics of several commonly used benchmark datasets."
        },
        {
            "heading": "4 Keyphrase Extraction Evaluation",
            "text": "This section describes evaluation metrics for measuring recent state-of-the-art keyphrase extraction baselines on commonly-used datasets. Designing a suitable evaluation metric for the keyphrase extraction task is by no means an easy study (Hasan and Ng, 2014). To score the output of a keyphrase extraction model, the traditional approach, which is also adopted by the SemEval-2010 (Kim et al., 2010) shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the ground-truth keyphrases and those in the model output adopting exact and partial matching (Papagiannopoulou and Tsoumakas, 2019), and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F1-score (F1).\nAs mentioned earlier, such evaluation usually operates based on exact matches between the predicted and ground-truth keyphrases. However, such a strategy cannot account for partial matches or semantic similarity. For example, if the prediction is \"keyphrase extraction model\" and the ground truth is \"keyphrase extraction system\", despite both semantic similarity and partial matching, the score will be 0. These minor deviations are ubiquitous in\nkeyphrase extraction, yet they are harshly penalized by the \"exact match\" evaluation metrics."
        },
        {
            "heading": "5 Neural Keyphrase Extraction Models with Pre-trained Language Models",
            "text": "There are two popular pipelines in the keyphrase extraction task, including one-stage and two-stage frameworks, as illustrated in Figure 1. The former mainly refers to using the task reformulation to address the keyphrase extraction task, which often treats the keyphrase extraction task as a sequence labeling task. The latter represents a more general framework, which usually operates in two procedures: (1) extracting a set of words/phrases that serve as candidate phrases using some heuristics and (2) determining which candidate phrases are keyphrases using supervised or unsupervised methods (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).\nTypically, supervised methods perform better on specific domain tasks. However, this kind of method takes a lot of labor to annotate the corpus, and the model after training may overfit and not work well on other KE datasets. On the contrary, unsupervised methods do not need to annotate the corpus and usually have better data generalization in different domains. Still, the performance is often insufficient due to the lack of annotated data. Overall, we defined the above two procedures as the candidate keyphrase extraction and keyphrase importance estimation. In this paper, we distinguish the existing methods into three categories depending on the recent state-of-the-art baselines (with\npre-trained language models as the backbone), including two-stage unsupervised, two-stage supervised, and one-stage supervised models."
        },
        {
            "heading": "5.1 Two-Stage Unsupervised Keyphrase Extraction Models",
            "text": "As noted before, unsupervised keyphrase extraction systems generally extract a set of phrases from the source document as candidates by using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum (Hasan and Ng, 2014). The main steps of the commonly used candidate keyphrases extraction methods for the recent unsupervised keyphrase extraction models are as follows, (1) tokenizing the document and tagging the document with partof-speech (POS) tags via the StanfordCoreNLP Tools3; (2) extracting candidate phrases based on part-of-speech tags by the regular expression via the python package NLTK4. Furthermore, different pruning heuristics have been designed for pruning candidates that are unlikely to be keyphrases to obtain a better candidate set (Huang et al., 2006; Kumar and Srinathan, 2008; El-Beltagy and Rafea, 2009; Newman et al., 2012; You et al., 2009). After obtaining candidates, keyphrases are determined by estimating the importance of each candidate through various strategies. Here, to facilitate the introduction, we divide the methods of importance estimation into two categories, namely, traditional methods and embedding-based methods.\nTraditional unsupervised keyphrase extraction systems can be mainly divided into statistics-based (Jones, 2004; Campos et al., 2018b), topic-based (Liu et al., 2009; Jardine and Teufel, 2014), and graph-based (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b; Bougouin et al., 2013; Florescu and Caragea, 2017b) methods. Generally, these\n3https://stanfordnlp.github.io/CoreNLP 4https://github.com/nltk\nmodels primarily use different features of documents (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.) to estimate the importance of each candidate phrase and discriminate whether a candidate phrase is a keyphrase (Hasan and Ng, 2014; Papagiannopoulou and Tsoumakas, 2019).\nHowever, these traditional unsupervised models estimate the importance scores of candidate phrases based on the surface-level features, ignoring the high-level features (e.g., syntactic and semantic information) of natural languages, which leads to extract wrong keyphrases. Therefore, recent studies focus on embedding-based models (Wang et al., 2015; Mahata et al., 2018a; Papagiannopoulou and Tsoumakas, 2018; Sahrawat et al., 2020; Kulkarni et al., 2022; Song et al., 2022b), which leverage pretrained embeddings (containing high-level features) to obtain phrase and document embeddings and calculate the importance scores of candidate phrases for extracting keyphrases. Wang et al. (2015) is the first work to explore utilizing word embedding and frequency to generate weighted edges between words, then using the weighted PageRank algorithm to compute and rank candidate scores. Key2vec (Mahata et al., 2018a) proposes an effective way of processing text documents for training multi-word phrase embeddings that are used for topic representations of scientific articles and ranking of keyphrases extracted from them using the topic-weighted PageRank algorithm. Mahata et al. (2018b) uses a combination of theme-weighted personalized PageRank algorithm and neural phrase embeddings for extracting and ranking keyphrases. EmbedRank (Bennani-Smires et al., 2018) ranks candidate phrases by measuring the semantic similarity between each candidate phrase and document embeddings.\nWith the development of pre-trained language\nmodels (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERta (Liu et al., 2019)), SIFRank5 (Sun et al., 2020b) improves candidate phrase and document embeddings from EmbedRank with the pre-trained language model ELMo (Peters et al., 2018) and achieves better performance. JointGL6 (Liang et al., 2021) integrates boundary-aware phrase centrality (the semantic similarities are calculated between all candidate phrases for identifying which candidate is better) and phrase-document relevance (the semantic similarities are calculated between candidate phrases and their corresponding document) from both local and global views, then used both jointly to determine the importance of each candidate. AttentionRank7 (Ding and Luo, 2021) adopts a pre-trained language model to calculate the self-attention of a candidate within the context of a sentence, and the cross-attention between a candidate and sentences within the source document to evaluate the local and global importance of each candidate. MDERank8 (Zhang et al., 2021) proposes to rank candidates using the similarity between the BERT embeddings of the source document and the masked document. Totally, these models achieve state-ofthe-art performance in the unsupervised keyphrase extraction task, benefiting from the development of representation learning."
        },
        {
            "heading": "5.2 Two-Stage Supervised Keyphrase Extraction Models",
            "text": "Different from two-stage unsupervised approaches, supervised approaches generally combine candidate keyphrase extraction and keyphrase importance estimation via an end-to-end learning framework, guide the whole model to rank and extract keyphrases through annotated data and optimize the two stages simultaneously. Therefore, to obtain sufficient candidates, the recent supervised models (Xiong et al., 2019; Sun et al., 2020a; Song et al., 2021, 2022a) directly extract n-grams from the document as candidates. Then propose, various approaches to estimate the importance scores of candidates. To estimate the importance of candidate phrases, similar to unsupervised models, supervised models (Xiong et al., 2019; Sun et al., 2020a; Song et al., 2021) also obtain phrase and document representations by adopting pre-trained\n5https://github.com/sunyilgdx/SIFRank 6https://github.com/xnliang98/uke_ccrank 7https://github.com/hd10-iupui/AttentionRank 8https://github.com/linhanz/mderank\nlanguage models as the backbone, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), etc.\nFirstly, BLING-KPE (Xiong et al., 2019) formulates keyphrase extraction as an n-gram level keyphrase chunking task to determine whether a candidate is a keyphrase, which incorporates pre-trained embeddings (i.e., ELMo (Peters et al., 2018)) into a convolutional transformer network to model n-gram representations. BLING-KPE achieves significant improvement over previous models. To leverage external knowledge to assist keyphrase extraction, SMART-KPE9 (Wang et al., 2020) also shows that incorporating multimodal information in web pages, such as font, size, and DOM features, can bring further improvement for open-domain web keyphrase extraction. Later, Ainslie et al. (2020) replaces the full self-attention of Transformers with local-global attention, which significantly boosts the keyphrase extraction performance for long documents. SKE-BASE-RANK (Mu et al., 2020) proposes a span-based keyphrase extraction model to model the relationships between candidates and the document in context.\nJointKPE10 (Sun et al., 2020a) proposes an opendomain keyphrase extraction approach built on pretrained language models (Devlin et al., 2019; Liu et al., 2019), which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the whole document and is jointly trained on the keyphrase chunking task to guarantee the phraseness of keyphrase candidates. KIEMP11 (Song et al., 2021) proposes estimating the importance score of each candidate from multiple perspectives and introducing a matching module to match the high-level concept between the document and candidates to enhance the relevance of extracted keyphrases. To extract more relevant keyphrases, HyperMatch12 (Song et al., 2022a) proposes a new matching framework and explores keyphrase extraction in the hyperbolic space. Concretely, HyperMatch first maps phrase and document representations into the same hyperbolic space and explicitly models the relevance between candidate phrases and the document as the phrase-document relevance via the Poincar\u00e9 distance to extract keyphrases.\n9https://github.com/victorywys/SMART-KPE 10https://github.com/thunlp/BERT-KPE 11https://github.com/MySong7NLPer/KIEMP 12https://github.com/MySong7NLPer/HyperMatch"
        },
        {
            "heading": "5.3 One-Stage Supervised Keyphrase Extraction Models",
            "text": "A major limitation of the above two-stage supervised approaches is classifying the labels of each candidate phrase independently while ignoring the dependencies that could potentially exist between candidates. Therefore, recent studies (Gollapalli et al., 2017; Basaldella et al., 2018; Wang et al., 2018; Alzaidy et al., 2019; Sun et al., 2019; Mu et al., 2020; Sahrawat et al., 2020) formulated keyphrase extraction as sequence labeling and showed that using linear-chain Conditional Random Fields improved the performance over baseline models for this task. Then, Mu et al. (2020) proposes SKE-BASE-CLS and -RANK, which directly extracts span-based phrase representations from all the document tokens via pre-trained language models and further learn to capture the interaction between them and their corresponding document to get better ranking results. Furthermore, this kind of model can extract overlapped keyphrases (Mu et al., 2020)."
        },
        {
            "heading": "6 Discussion",
            "text": "In this section, we report the results of the recent unsupervised and supervised keyphrase extraction baselines, which all adopt pre-trained language\nmodels as the backbone, as shown in Table 2 and Table 3. Specifically, Table 2 presents the results of the traditional unsupervised methods and the unsupervised embedding-based keyphrase extraction baselines discussed in Section 5.1 on the DUC2001 (Wan and Xiao, 2008b), Inspec (Hulth, 2003), SemEval2010 (Kim et al., 2010), and SemEval2017 (Augenstein et al., 2017) datasets. Embeddingbased two-stage models without PLMs indicate that the models do not use pre-trained language models as the backbone to obtain representations. Table 3 shows the results of all the different categories of the supervised keyphrase extraction systems discussed in Section 5.2 and Section 5.3 on the KP20k (Meng et al., 2017) and OpenKP (Xiong et al., 2019) datasets.\nOur first finding from the survey is those twostage embedding-based systems with static embeddings outperform two-stage traditional methods, despite the latter\u2019s access to different valuable features (e.g., word frequency, position, linguistic properties, topic, length, the relationship between words, external knowledge-based information, etc.). This further demonstrates the necessity of studying embedding-based methods.\nOur second finding is those embedding-based systems with PLMs outperform embedding-based approaches with static embeddings in most cases.\nHowever, not all embedding-based systems with PLMs are superior to embedding-based systems with static embeddings. The former generally outperforms the latter when adopting the same importance estimation strategy, but the estimation strategy can significantly affect the results of keyphrase extraction. To sum up, effectively using pre-trained embeddings to estimate the importance score of each candidate is a critical part of improving the performance of keyphrase extraction. Furthermore, there is still interesting progress to be made by leveraging a self-supervised learning strategy to optimize embedding-based systems. MDERank uses a simple yet effective contrastive learning strategy to optimize embedding-based systems, achieving better performance.\nOur third finding is that the embedding-based methods have slight improvement on long document datasets (e.g., SemEval2010), and all unsupervised methods have poor effects on long document datasets. This demonstrates that keyphrase extraction from long documents is still a challeng-\ning problem. Our final finding is that two-stage supervised keyphrase extraction methods are superior to onestage supervised keyphrase extraction methods, as illustrated in Table 3. In addition, the two-stage method has higher scalability and adaptability than the one-stage method, such as handling long and extremely long documents."
        },
        {
            "heading": "7 Conclusion and Future Directions",
            "text": "We summarize the recent neural keyphrase extraction models based on pre-trained language models. Our survey of models for keyphrase extraction, covering both unsupervised and supervised models, has yielded several important insights. The analysis revealed that there are at least six major challenges ahead."
        },
        {
            "heading": "7.1 Improving the Quality of Generated Candidate Keyphrases",
            "text": "Many heuristic rules have proven effective with a high recall to cover most of the gold keyphrases\nof source documents, which determines the upper bound of the performance of keyphrase extraction (Hasan and Ng, 2014). Intuitively, better candidate keyphrase extraction strategies are required to generate a set of candidate keyphrases with a higher recall from the source document to improve the upper-bound performance of keyphrase extraction. Recent work (Jawahar et al., 2019) demonstrates that the intermediate layers of BERT encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle, and semantic features at the top, as mentioned in Section 2.2. They also observe that BERT mostly captures phrase-level information in the lower layers and gradually dilutes this information in higher layers. In addition, the number of candidate keyphrases will increase as the document length increases. Therefore, how constructing candidate keyphrases using the potential knowledge of pre-trained language models is a valuable research direction."
        },
        {
            "heading": "7.2 Improving Evaluation Metric",
            "text": "As mentioned in Section 4, the existing evaluation metrics occur when a keyphrase extraction system extracts a keyphrase from candidates that is semantically equivalent to a ground-truth keyphrase but is considered erroneous by a scoring function because it fails to recognize that the predicted keyphrase and the corresponding gold keyphrase are semantically equivalent.\nIn other words, an evaluation error is not made by a keyphrase extraction system, but a mistake due to an unformed scoring function (Hasan and Ng, 2014). Therefore, a more suitable evaluation metric is required to evaluate the predicted keyphrases by adopting the semantic-based matching metric instead of the exact matching evaluation metric. In the future, using pre-trained language models (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)) to construct a new semantic-aware evaluation metric similar to BERTScore (Zhang et al., 2020) may be an interesting and valuable research direction."
        },
        {
            "heading": "7.3 Reducing Over-Generation Error",
            "text": "Over-generation errors occur when a keyphrase extraction system correctly predicts a candidate as a keyphrase because it contains a word that frequently appears in the associated document but at the same time erroneously outputs other candidates\nas keyphrases because they have the same word in the document.\nAs mentioned before, for example, if the prediction is \"keyphrase extraction challenge\" and the ground truth is \"keyphrase extraction system\", despite both semantic similarity and partial matching, the score will be 0. These minor deviations are ubiquitous in keyphrase extraction, yet they are harshly penalized by the \"exact match\" evaluation metrics. There are often some non-keyphrases in the candidates. Half of the content of such phrases is very relevant to the core information of the document, but the other half is meaningless. These candidate keyphrases are usually hard to extract and treated as hard samples, which is one of the main reasons for reducing keyphrase extraction performance. The above issues can be solved by modifying the traditional evaluation metrics with semantic weighting."
        },
        {
            "heading": "7.4 Handling Long Document",
            "text": "Generally, two main challenges exist in keyphrase extraction systems equipped with pre-trained language models (e.g., BERT (Devlin et al., 2019)) as the backbone when extracting keyphrases from a long document, especially for an extremely long document.\nThe first challenge is that pre-trained language models can not directly model the complete context information when facing long documents due to the length limitation of pre-trained language models.\nThe second challenge is that as the length of the document increases, the difficulty of estimating the importance scores of candidate phrases also increases (specifically for the number of candidates), resulting in the reduction of keyphrase extraction accuracy."
        },
        {
            "heading": "7.5 Improving Domain Generalization",
            "text": "For news or scientific documents, the authors usually annotate a set of keyphrases for their articles (Meng et al., 2017; Augenstein et al., 2017). However, there is typically a lack of keyphrases as the label information for their corresponding documents in other specific domains.\nMost existing keyphrase extraction datasets and studies are based on news or scientific documents and lack datasets and research related to other domains. Therefore, the task worthy of investigation is to transfer the keyphrase extraction model from the scientific domain to other domains to build a\ndomain-specific keyphrase extraction model with various domain generalization strategies."
        },
        {
            "heading": "7.6 Probing Pre-trained Language Model for Keyphrase Extraction",
            "text": "In addition to using transformer-based pre-trained language models (e.g., BERT) in NLP tasks and end applications, research has also been done on BERT, especially to reveal what linguistic information is available in different parts of the model (Jawahar et al., 2019; de Vries et al., 2020; Chen et al., 2021). It has been noted that BERT progressively acquires linguistic information roughly in the same order as the classic language processing pipeline (Tenney et al., 2019a,b): surface features are expressed in lower layers, syntactic features more in middle layers, and semantic ones in higher layers (Jawahar et al., 2019). Making full use of the above hierarchy information may effectively improve the performance of keyphrase extraction."
        },
        {
            "heading": "8 Limitations",
            "text": "The main goal of this paper is to provide a survey of the existing models. Since we do not propose new models, there are no potential social risks to the best of our knowledge. Our work may benefit the research community by providing more introspection into the current state-of-the-art neural keyphrase extraction approaches with pre-trained language models."
        },
        {
            "heading": "9 Acknowledgments",
            "text": "We thank the three anonymous reviewers for their helpful comments. This work was partly supported by the Fundamental Research Funds for the Central Universities (2019JBZ110); the National Natural Science Foundation of China under Grant 62176020; the National Key Research and Development Program (2020AAA0106800); the Beijing Natural Science Foundation under Grant L211016; CAAI-Huawei MindSpore Open Fund; and Chinese Academy of Sciences (OEIP-O-202004)."
        }
    ],
    "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models",
    "year": 2023
}