{
    "abstractText": "Modeling 3D avatars benefits various application scenarios such as AR/VR, gaming, and filming. Character faces contribute significant diversity and vividity as a vital component of avatars. However, building 3D character face models usually requires a heavy workload with commercial tools, even for experienced artists. Various existing sketch-based tools fail to support amateurs in modeling diverse facial shapes and rich geometric details. In this paper, we present SketchMetaFace a sketching system targeting amateur users to model high-fidelity 3D faces in minutes. We carefully design both the user interface and the underlying algorithm. First, curvature-aware strokes are adopted to better support the controllability of carving facial details. Second, considering the key problem of mapping a 2D sketch map to a 3D model, we develop a novel learning-based method termed \u201cImplicit and Depth Guided Mesh Modeling\u201d (IDGMM). It fuses the advantages of mesh, implicit, and depth representations to achieve high-quality results with high efficiency. In addition, to further support usability, we present a coarse-to-fine 2D sketching interface design and a data-driven stroke suggestion tool. User studies demonstrate the superiority of our system over existing modeling tools in terms of the ease to use and visual quality of results. Experimental analyses also show that IDGMM reaches a better trade-off between accuracy and efficiency. SketchMetaFace is available at https://zhongjinluo.github.io/SketchMetaFace/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhongjin Luo"
        },
        {
            "affiliations": [],
            "name": "Dong Du"
        },
        {
            "affiliations": [],
            "name": "Heming Zhu"
        },
        {
            "affiliations": [],
            "name": "Yizhou Yu"
        },
        {
            "affiliations": [],
            "name": "Hongbo Fu"
        },
        {
            "affiliations": [],
            "name": "Xiaoguang HanB"
        }
    ],
    "id": "SP:a0dbf1b94e54a6864dbb88582505c3e9d3a8a8df",
    "references": [
        {
            "authors": [
                "T. Igarashi",
                "S. MATSUOKA",
                "H. TANAKA"
            ],
            "title": "Teddy: A sketching interface for 3d freeform design",
            "venue": "Computer graphics proceedings, annual conference series. Association for Computing Machinery SIGGRAPH, 1999, pp. 409\u2013416. 1, 3",
            "year": 1999
        },
        {
            "authors": [
                "A. Nealen",
                "T. Igarashi",
                "O. Sorkine",
                "M. Alexa"
            ],
            "title": "Fibermesh: designing freeform surfaces with 3d curves",
            "venue": "ACM SIGGRAPH 2007 papers, 2007, pp. 41\u2013es. 1, 3",
            "year": 2007
        },
        {
            "authors": [
                "P. Borosan",
                "M. Jin",
                "D. DeCarlo",
                "Y. Gingold",
                "A. Nealen"
            ],
            "title": "RigMesh: Automatic rigging for part-based shape modeling and deformation",
            "venue": "ACM Transactions on Graphics (TOG), vol. 31, no. 6, pp. 198:1\u2013198:9, Nov. 2012. [Online]. Available: http://doi.acm.org/10.1145/2366145.2366217 1, 4",
            "year": 2012
        },
        {
            "authors": [
                "D. S\u1ef3kora",
                "L. Kavan",
                "M. \u010cad\u0131\u0301k",
                "O. Jamri\u0161ka",
                "A. Jacobson",
                "B. Whited",
                "M. Simmons",
                "O. Sorkine-Hornung"
            ],
            "title": "Ink-and-ray: Bas-relief meshes for adding global illumination effects to hand-drawn characters",
            "venue": "ACM Transactions on Graphics (TOG), vol. 33, no. 2, pp. 1\u201315, 2014. 1",
            "year": 2014
        },
        {
            "authors": [
                "H. Pan",
                "Y. Liu",
                "A. Sheffer",
                "N. Vining",
                "C.-J. Li",
                "W. Wang"
            ],
            "title": "Flow aligned surfacing of curve networks",
            "venue": "ACM Transactions on Graphics (TOG), vol. 34, no. 4, pp. 1\u201310, 2015. 1",
            "year": 2015
        },
        {
            "authors": [
                "C. Li",
                "H. Pan",
                "Y. Liu",
                "X. Tong",
                "A. Sheffer",
                "W. Wang"
            ],
            "title": "Bendsketch: modeling freeform surfaces through 2d sketching",
            "venue": "ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp. 1\u201314, 2017. 1, 3, 4",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhong",
                "Y. Gryaditskaya",
                "H. Zhang",
                "Y.-Z. Song"
            ],
            "title": "Deep sketchbased modeling: Tips and tricks",
            "venue": "2020 International Conference on 3D Vision (3DV). IEEE, 2020, pp. 543\u2013552. 1",
            "year": 2020
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "A study of deep single sketch-based modeling: View/style invariance, sparsity and latent space disentanglement",
            "venue": "Computers & Graphics, vol. 106, pp. 237\u2013247, 2022. 1",
            "year": 2022
        },
        {
            "authors": [
                "P. Xu",
                "T.M. Hospedales",
                "Q. Yin",
                "Y.-Z. Song",
                "T. Xiang",
                "L. Wang"
            ],
            "title": "Deep learning for free-hand sketch: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 285\u2013312, 2022. 1",
            "year": 2022
        },
        {
            "authors": [
                "X. Han",
                "C. Gao",
                "Y. Yu"
            ],
            "title": "Deepsketch2face: a deep learning based sketching system for 3d face and caricature modeling",
            "venue": "ACM Transactions on graphics (TOG), vol. 36, no. 4, pp. 1\u201312, 2017. 1, 2, 3, 4, 8",
            "year": 2017
        },
        {
            "authors": [
                "Z. Luo",
                "J. Zhou",
                "H. Zhu",
                "D. Du",
                "X. Han",
                "H. Fu"
            ],
            "title": "Simpmodeling: Sketching implicit field to guide mesh modeling for 3d animalmorphic head design",
            "venue": "The 34th Annual ACM Symposium on User Interface Software and Technology, 2021, pp. 854\u2013863. 1, 2, 3, 4, 7, 8",
            "year": 2021
        },
        {
            "authors": [
                "S. Saito",
                "Z. Huang",
                "R. Natsume",
                "S. Morishima",
                "A. Kanazawa",
                "H. Li"
            ],
            "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization",
            "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 2304\u20132314. 1, 6",
            "year": 2019
        },
        {
            "authors": [
                "S. Saito",
                "T. Simon",
                "J. Saragih",
                "H. Joo"
            ],
            "title": "Pifuhd: Multi-level pixelaligned implicit function for high-resolution 3d human digitization",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 84\u201393. 1, 5, 6, 11",
            "year": 2020
        },
        {
            "authors": [
                "C. Li",
                "H. Pan",
                "Y. Liu",
                "X. Tong",
                "A. Sheffer",
                "W. Wang"
            ],
            "title": "Robust flowguided neural prediction for sketch-based freeform surface modeling",
            "venue": "ACM Transactions on Graphics (TOG), vol. 37, no. 6, pp. 1\u201312, 2018. 1",
            "year": 2018
        },
        {
            "authors": [
                "C. Li",
                "H. Pan",
                "A. Bousseau",
                "N.J. Mitra"
            ],
            "title": "Sketch2cad: Sequential cad modeling by sketching in context",
            "venue": "ACM Transactions on Graphics (TOG), vol. 39, no. 6, pp. 1\u201314, 2020. 1, 3",
            "year": 2020
        },
        {
            "authors": [
                "D. Du",
                "X. Han",
                "H. Fu",
                "F. Wu",
                "Y. Yu",
                "S. Cui",
                "L. Liu"
            ],
            "title": "Sanihead: Sketching animal-like 3d character heads using a view-surface collaborative mesh generative network",
            "venue": "IEEE Transactions on Visualization and Computer Graphics, 2020. 1, 2, 3",
            "year": 2020
        },
        {
            "authors": [
                "E. Iarussi",
                "D. Bommes",
                "A. Bousseau"
            ],
            "title": "Bendfields: Regularized curvature fields from rough concept sketches",
            "venue": "ACM Transactions on Graphics (TOG), vol. 34, no. 3, pp. 1\u201316, 2015. 1, 3",
            "year": 2015
        },
        {
            "authors": [
                "Z. Lun",
                "M. Gadelha",
                "E. Kalogerakis",
                "S. Maji",
                "R. Wang"
            ],
            "title": "3d shape reconstruction from sketches via multi-view convolutional networks",
            "venue": "2017 International Conference on 3D Vision (3DV). IEEE, 2017, pp. 67\u201377. 1, 2, 3",
            "year": 2017
        },
        {
            "authors": [
                "J. Delanoy",
                "M. Aubry",
                "P. Isola",
                "A.A. Efros",
                "A. Bousseau"
            ],
            "title": "3d sketching using multi-view deep volumetric prediction",
            "venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques, vol. 1, no. 1, pp. 1\u201322, 2018. 1, 2, 3",
            "year": 2018
        },
        {
            "authors": [
                "J. Wang",
                "J. Lin",
                "Q. Yu",
                "R. Liu",
                "Y. Chen",
                "S.X. Yu"
            ],
            "title": "3d shape reconstruction from free-hand sketches",
            "venue": "Computer Vision\u2013ECCV 2022 Workshops: Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII. Springer, 2023, pp. 184\u2013202. 1, 2, 3",
            "year": 2022
        },
        {
            "authors": [
                "N. Wang",
                "Y. Zhang",
                "Z. Li",
                "Y. Fu",
                "W. Liu",
                "Y.-G. Jiang"
            ],
            "title": "Pixel2mesh: Generating 3d mesh models from single rgb images",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 52\u201367. 1, 11",
            "year": 2018
        },
        {
            "authors": [
                "B. Guillard",
                "E. Remelli",
                "P. Yvernay",
                "P. Fua"
            ],
            "title": "Sketch2mesh: Reconstructing and editing 3d shapes from sketches",
            "venue": "Proceedings of JOURNAL OF LTEX CLASS FILES, VOL. XX, NO. X, XXXX 14 the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 13 023\u201313 032. 1",
            "year": 2021
        },
        {
            "authors": [
                "P. Isola",
                "J.-Y. Zhu",
                "T. Zhou",
                "A.A. Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1125\u2013 1134. 1, 5",
            "year": 2017
        },
        {
            "authors": [
                "S.-Y. Chen",
                "F.-L. Liu",
                "Y.-K. Lai",
                "P.L. Rosin",
                "C. Li",
                "H. Fu",
                "L. Gao"
            ],
            "title": "Deepfaceediting: Deep face generation and editing with disentangled geometry and appearance control",
            "venue": "ACM Trans. Graph., vol. 40, no. 4, jul 2021. [Online]. Available: https://doi.org/10.1145/3450626.3459760 2",
            "year": 2021
        },
        {
            "authors": [
                "S.-Y. Chen",
                "W. Su",
                "L. Gao",
                "S. Xia",
                "H. Fu"
            ],
            "title": "Deepfacedrawing: Deep generation of face images from sketches",
            "venue": "ACM Transactions on Graphics (TOG), vol. 39, no. 4, pp. 72\u20131, 2020. 2",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xiao",
                "H. Zhu",
                "H. Yang",
                "Z. Diao",
                "X. Lu",
                "X. Cao"
            ],
            "title": "Detailed facial geometry recovery from multi-view images by learning an implicit function",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2022. 2",
            "year": 2022
        },
        {
            "authors": [
                "Z. Bai",
                "Z. Cui",
                "J.A. Rahim",
                "X. Liu",
                "P. Tan"
            ],
            "title": "Deep facial non-rigid multi-view stereo",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 2",
            "year": 2020
        },
        {
            "authors": [
                "M. B R",
                "A. Tewari",
                "H.-P. Seidel",
                "M. Elgharib",
                "C. Theobalt"
            ],
            "title": "Learning complete 3d morphable face models from images and videos",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2",
            "year": 2021
        },
        {
            "authors": [
                "P. Garrido",
                "M. Zollh\u00f6fer",
                "D. Casas",
                "L. Valgaerts",
                "K. Varanasi",
                "P. Perez",
                "C. Theobalt"
            ],
            "title": "Reconstruction of personalized 3d face rigs from monocular video",
            "venue": "ACM Trans. Graph. (Presented at SIGGRAPH 2016), vol. 35, no. 3, pp. 28:1\u201328:15, 2016. 2",
            "year": 2016
        },
        {
            "authors": [
                "C. Cao",
                "Y. Weng",
                "S. Zhou",
                "Y. Tong",
                "K. Zhou"
            ],
            "title": "Facewarehouse: A 3d facial expression database for visual computing",
            "venue": "IEEE Transactions on Visualization and Computer Graphics, vol. 20, no. 3, pp. 413\u2013425, 2013. 2",
            "year": 2013
        },
        {
            "authors": [
                "Y. Deng",
                "J. Yang",
                "S. Xu",
                "D. Chen",
                "Y. Jia",
                "X. Tong"
            ],
            "title": "Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0\u20130. 2",
            "year": 2019
        },
        {
            "authors": [
                "L. Tran",
                "X. Liu"
            ],
            "title": "Nonlinear 3d face morphable model",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7346\u20137355. 2",
            "year": 2018
        },
        {
            "authors": [
                "E. Richardson",
                "M. Sela",
                "R. Or-El",
                "R. Kimmel"
            ],
            "title": "Learning detailed face reconstruction from a single image",
            "venue": "proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1259\u2013 1268. 2",
            "year": 2017
        },
        {
            "authors": [
                "A. Tuan Tran",
                "T. Hassner",
                "I. Masi",
                "E. Paz",
                "Y. Nirkin",
                "G. Medioni"
            ],
            "title": "Extreme 3d face reconstruction: Seeing through occlusions",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3935\u20133944. 2",
            "year": 2018
        },
        {
            "authors": [
                "J. Liu",
                "Y. Chen",
                "C. Miao",
                "J. Xie",
                "C.X. Ling",
                "X. Gao",
                "W. Gao"
            ],
            "title": "Semisupervised learning in reconstructed manifold space for 3d caricature generation",
            "venue": "Computer Graphics Forum, vol. 28, no. 8. Wiley Online Library, 2009, pp. 2104\u20132116. 2",
            "year": 2009
        },
        {
            "authors": [
                "J. Zhang",
                "H. Cai",
                "Y. Guo",
                "Z. Peng"
            ],
            "title": "Landmark detection and 3d face reconstruction for caricature using a nonlinear parametric model",
            "venue": "arXiv preprint arXiv:2004.09190, 2020. 2",
            "year": 2004
        },
        {
            "authors": [
                "Q. Wu",
                "J. Zhang",
                "Y.-K. Lai",
                "J. Zheng",
                "J. Cai"
            ],
            "title": "Alive caricature from 2d to 3d",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 7336\u20137345. 2",
            "year": 2018
        },
        {
            "authors": [
                "S.-H. Zhang",
                "Y.-C. Guo",
                "Q.-W. Gu"
            ],
            "title": "Sketch2model: View-aware 3d modeling from single free-hand sketches",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6012\u20136021. 2, 3",
            "year": 2021
        },
        {
            "authors": [
                "P.N. Chowdhury",
                "T. Wang",
                "D. Ceylan",
                "Y.-Z. Song",
                "Y. Gryaditskaya"
            ],
            "title": "Garment ideation: Iterative view-aware sketch-based garment modeling",
            "venue": "10th International Conference on 3D Vision (3DV 2022). 2, 3",
            "year": 2022
        },
        {
            "authors": [
                "C. Ding",
                "L. Liu"
            ],
            "title": "A survey of sketch based modeling systems",
            "venue": "Frontiers of Computer Science, vol. 10, no. 6, pp. 985\u2013999, 2016. 3",
            "year": 2016
        },
        {
            "authors": [
                "O.A. Karpenko",
                "J.F. Hughes"
            ],
            "title": "Smoothsketch: 3d free-form shapes from complex sketches",
            "venue": "ACM SIGGRAPH 2006 Papers, 2006, pp. 589\u2013598. 3",
            "year": 2006
        },
        {
            "authors": [
                "R. Schmidt",
                "B. Wyvill",
                "M.C. Sousa",
                "J.A. Jorge"
            ],
            "title": "Shapeshop: Sketchbased solid modeling with blobtrees",
            "venue": "ACM SIGGRAPH 2007 courses, 2007, pp. 43\u2013es. 3",
            "year": 2007
        },
        {
            "authors": [
                "A. Bernhardt",
                "A. Pihuit",
                "M.-P. Cani",
                "L. Barthe"
            ],
            "title": "Matisse: Painting 2d regions for modeling free-form shapes",
            "venue": "SBM\u201908-Eurographics Workshop on Sketch-Based Interfaces and Modeling. Eurographics Association, 2008, pp. 57\u201364. 3",
            "year": 2008
        },
        {
            "authors": [
                "P. Joshi",
                "N.A. Carr"
            ],
            "title": "Repouss\u00e9: Automatic inflation of 2d artwork.",
            "venue": "SBM,",
            "year": 2008
        },
        {
            "authors": [
                "Y. Gingold",
                "T. Igarashi",
                "D. Zorin"
            ],
            "title": "Structured annotations for 2dto-3d modeling",
            "venue": "ACM SIGGRAPH Asia 2009 papers, 2009, pp. 1\u20139. 3",
            "year": 2009
        },
        {
            "authors": [
                "L. Olsen",
                "F. Samavati",
                "J. Jorge"
            ],
            "title": "Naturasketch: Modeling from images and natural sketches",
            "venue": "IEEE Computer Graphics and Applications, vol. 31, no. 6, pp. 24\u201334, 2011. 3",
            "year": 2011
        },
        {
            "authors": [
                "S.-H. Bae",
                "R. Balakrishnan",
                "K. Singh"
            ],
            "title": "Ilovesketch: as-natural-aspossible sketching system for creating 3d curve models",
            "venue": "Proceedings of the 21st annual ACM symposium on User interface software and technology, 2008, pp. 151\u2013160. 3",
            "year": 2008
        },
        {
            "authors": [
                "R. Schmidt",
                "A. Khan",
                "K. Singh",
                "G. Kurtenbach"
            ],
            "title": "Analytic drawing of 3d scaffolds",
            "venue": "ACM SIGGRAPH Asia 2009 papers, 2009, pp. 1\u201310. 3",
            "year": 2009
        },
        {
            "authors": [
                "C. Shao",
                "A. Bousseau",
                "A. Sheffer",
                "K. Singh"
            ],
            "title": "Crossshade: shading concept sketches using cross-section curves",
            "venue": "ACM Transactions on Graphics (TOG), vol. 31, no. 4, pp. 1\u201311, 2012. 3",
            "year": 2012
        },
        {
            "authors": [
                "B. Xu",
                "W. Chang",
                "A. Sheffer",
                "A. Bousseau",
                "J. McCrae",
                "K. Singh"
            ],
            "title": "True2form: 3d curve networks from 2d sketches via selective regularization",
            "venue": "ACM Transactions on Graphics (TOG), vol. 33, no. 4, pp. 1\u201313, 2014. 3",
            "year": 2014
        },
        {
            "authors": [
                "M. Eitz",
                "R. Richter",
                "T. Boubekeur",
                "K. Hildebrand",
                "M. Alexa"
            ],
            "title": "Sketchbased shape retrieval.",
            "venue": "ACM Trans. Graph.,",
            "year": 2012
        },
        {
            "authors": [
                "B. Li",
                "Y. Lu",
                "F. Duan",
                "S. Dong",
                "Y. Fan",
                "L. Qian",
                "H. Laga",
                "H. Li",
                "Y. Li",
                "P. Liu",
                "M. Ovsjanikov",
                "H. Tabia",
                "Y. Ye",
                "H. Yin",
                "Z. Xue"
            ],
            "title": "3D Sketch-Based 3D Shape Retrieval",
            "venue": "Eurographics Workshop on 3D Object Retrieval, A. Ferreira, A. Giachetti, and D. Giorgi, Eds. The Eurographics Association, 2016. 3",
            "year": 2016
        },
        {
            "authors": [
                "A. Qi",
                "Y. Gryaditskaya",
                "J. Song",
                "Y. Yang",
                "Y. Qi",
                "T.M. Hospedales",
                "T. Xiang",
                "Y.-Z. Song"
            ],
            "title": "Toward fine-grained sketch-based 3d shape retrieval",
            "venue": "IEEE transactions on image processing, vol. 30, pp. 8595\u2013 8606, 2021. 3",
            "year": 2021
        },
        {
            "authors": [
                "L. Luo",
                "Y. Gryaditskaya",
                "T. Xiang",
                "Y.-Z. Song"
            ],
            "title": "Structure-aware 3d vr sketch to 3d shape retrieval",
            "venue": "arXiv preprint arXiv:2209.09043, 2022. 3",
            "year": 2022
        },
        {
            "authors": [
                "L. Fan",
                "R. Wang",
                "L. Xu",
                "J. Deng",
                "L. Liu"
            ],
            "title": "Modeling by drawing with shadow guidance",
            "venue": "Computer Graphics Forum, vol. 32, no. 7. Wiley Online Library, 2013, pp. 157\u2013166. 3",
            "year": 2013
        },
        {
            "authors": [
                "X. Xie",
                "K. Xu",
                "N.J. Mitra",
                "D. Cohen-Or",
                "W. Gong",
                "Q. Su",
                "B. Chen"
            ],
            "title": "Sketch-to-design: Context-based part assembly",
            "venue": "Computer Graphics Forum, vol. 32, no. 8. Wiley Online Library, 2013, pp. 233\u2013245. 3",
            "year": 2013
        },
        {
            "authors": [
                "F. Wang",
                "L. Kang",
                "Y. Li"
            ],
            "title": "Sketch-based 3d shape retrieval using convolutional neural networks",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1875\u20131883. 3, 11",
            "year": 2015
        },
        {
            "authors": [
                "Y. Zhong",
                "Y. Qi",
                "Y. Gryaditskaya",
                "H. Zhang",
                "Y.-Z. Song"
            ],
            "title": "Towards practical sketch-based 3d shape generation: The role of professional sketches",
            "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 9, pp. 3518\u20133528, 2020. 3",
            "year": 2020
        },
        {
            "authors": [
                "Z. Cheng",
                "M. Chai",
                "J. Ren",
                "H.-Y. Lee",
                "K. Olszewski",
                "Z. Huang",
                "S. Maji",
                "S. Tulyakov"
            ],
            "title": "Cross-modal 3d shape generation and manipulation",
            "venue": "Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part III. Springer, 2022, pp. 303\u2013321. 3",
            "year": 2022
        },
        {
            "authors": [
                "D. Kong",
                "Q. Wang",
                "Y. Qi"
            ],
            "title": "A diffusion-refinement model for sketchto-point modeling",
            "venue": "Proceedings of the Asian Conference on Computer Vision, 2022, pp. 1522\u20131538. 3",
            "year": 2022
        },
        {
            "authors": [
                "W. Su",
                "D. Du",
                "X. Yang",
                "S. Zhou",
                "H. Fu"
            ],
            "title": "Interactive sketch-based normal map generation with deep neural networks",
            "venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques, vol. 1, no. 1, pp. 1\u201317, 2018. 3",
            "year": 2018
        },
        {
            "authors": [
                "H. Huang",
                "E. Kalogerakis",
                "E. Yumer",
                "R. Mech"
            ],
            "title": "Shape synthesis from sketches via procedural models and convolutional networks",
            "venue": "IEEE transactions on visualization and computer graphics, vol. 23, no. 8, pp. 2003\u20132013, 2016. 3",
            "year": 2003
        },
        {
            "authors": [
                "D. Du",
                "H. Zhu",
                "Y. Nie",
                "X. Han",
                "S. Cui",
                "Y. Yu",
                "L. Liu"
            ],
            "title": "Learning part generation and assembly for sketching man-made objects",
            "venue": "Computer Graphics Forum. Wiley Online Library, 2020. 3",
            "year": 2020
        },
        {
            "authors": [
                "G. Nishida",
                "I. Garcia-Dorado",
                "D.G. Aliaga",
                "B. Benes",
                "A. Bousseau"
            ],
            "title": "Interactive sketching of urban procedural models",
            "venue": "ACM Transactions on Graphics (TOG), vol. 35, no. 4, pp. 1\u201311, 2016. 3 JOURNAL OF LTEX CLASS FILES, VOL. XX, NO. X, XXXX 15",
            "year": 2016
        },
        {
            "authors": [
                "D. DeCarlo",
                "A. Finkelstein",
                "S. Rusinkiewicz",
                "A. Santella"
            ],
            "title": "Suggestive contours for conveying shape",
            "venue": "ACM SIGGRAPH 2003 Papers, 2003, pp. 848\u2013855. 5, 7",
            "year": 2003
        },
        {
            "authors": [
                "L. Mescheder",
                "M. Oechsle",
                "M. Niemeyer",
                "S. Nowozin",
                "A. Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4460\u20134470. 5, 6",
            "year": 2019
        },
        {
            "authors": [
                "J.J. Park",
                "P. Florence",
                "J. Straub",
                "R. Newcombe",
                "S. Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 165\u2013174. 5, 6, 11",
            "year": 2019
        },
        {
            "authors": [
                "Z. Chen",
                "H. Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5939\u20135948. 5, 6",
            "year": 2019
        },
        {
            "authors": [
                "A. Newell",
                "K. Yang",
                "J. Deng"
            ],
            "title": "Stacked hourglass networks for human pose estimation",
            "venue": "European conference on computer vision. Springer, 2016, pp. 483\u2013499. 5",
            "year": 2016
        },
        {
            "authors": [
                "W.E. Lorensen",
                "H.E. Cline"
            ],
            "title": "Marching cubes: A high resolution 3d surface construction algorithm",
            "venue": "ACM siggraph computer graphics, vol. 21, no. 4, pp. 163\u2013169, 1987. 6",
            "year": 1987
        },
        {
            "authors": [
                "M. Botsch",
                "L. Kobbelt"
            ],
            "title": "A remeshing approach to multiresolution modeling",
            "venue": "Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, 2004, pp. 185\u2013192. 6, 7",
            "year": 2004
        },
        {
            "authors": [
                "O. Sorkine",
                "D. Cohen-Or",
                "Y. Lipman",
                "M. Alexa",
                "C. R\u00f6ssl",
                "H.-P. Seidel"
            ],
            "title": "Laplacian surface editing",
            "venue": "Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing, 2004, pp. 175\u2013184. 6, 7",
            "year": 2004
        },
        {
            "authors": [
                "A. Sharf",
                "T. Lewiner",
                "A. Shamir",
                "L. Kobbelt",
                "D. Cohen-Or"
            ],
            "title": "Competing fronts for coarse\u2013to\u2013fine surface reconstruction",
            "venue": "Computer Graphics Forum, vol. 25, no. 3. Wiley Online Library, 2006, pp. 389\u2013 398. 7",
            "year": 2006
        },
        {
            "authors": [
                "P.M. Bartier",
                "C.P. Keller"
            ],
            "title": "Multivariate interpolation to incorporate thematic surface data using inverse distance weighting (idw)",
            "venue": "Computers & Geosciences, vol. 22, no. 7, pp. 795\u2013799, 1996. 7",
            "year": 1996
        },
        {
            "authors": [
                "E. Ilg",
                "N. Mayer",
                "T. Saikia",
                "M. Keuper",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2462\u20132470. 7",
            "year": 2017
        },
        {
            "authors": [
                "Y. Qiu",
                "X. Xu",
                "L. Qiu",
                "Y. Pan",
                "Y. Wu",
                "W. Chen",
                "X. Han"
            ],
            "title": "3dcaricshop: A dataset and a baseline method for single-view 3d caricature face reconstruction",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10 236\u201310 245. 8",
            "year": 2021
        },
        {
            "authors": [
                "A. Bangor",
                "P. Kortum",
                "J. Miller"
            ],
            "title": "Determining what individual sus scores mean: Adding an adjective rating scale",
            "venue": "Journal of usability studies, vol. 4, no. 3, pp. 114\u2013123, 2009. 9",
            "year": 2009
        },
        {
            "authors": [
                "T. Groueix",
                "M. Fisher",
                "V.G. Kim",
                "B.C. Russell",
                "M. Aubry"
            ],
            "title": "A papierm\u00e2ch\u00e9 approach to learning 3d surface generation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 216\u2013224. 11",
            "year": 2018
        },
        {
            "authors": [
                "C.B. Choy",
                "D. Xu",
                "J. Gwak",
                "K. Chen",
                "S. Savarese"
            ],
            "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
            "venue": "European conference on computer vision. Springer, 2016, pp. 628\u2013644. 11",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Sketch-based 3D Modeling, 3D Face Modeling\n\u2726"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "C REATING 3D virtual avatars is a prolonged research topic incomputer graphics and benefits various usage scenarios such as filming, gaming, and art designing. Typically, this process is a highly skilled task, as experienced artists need to spend several days or even months formally sculpting high-fidelity 3D faces with vivid surface details using commercialized 3D modeling tools (e.g., ZBrush, MAYA, and 3D MAX). To assist amateur users in freely instantiating their ideas as professional modelers, researchers in computer graphics and human-computer interaction have designed systems that allow users to model 3D shapes with freehand sketches based on geometric principles [1], [2], [3], [4], [5], [6]. Although traditional sketch-based 3D modeling systems, such as Teddy [1] and FiberMesh [2], enable amateur users to create 3D models, they usually require tremendous manual work to specify complex geometry.\nThanks to the recent progress in deep learning, the understanding of freehand sketches and the quality of single-view generation have reached an unprecedented level. Several intelligent sketchbased modeling systems have been developed to enable novice users to create visually plausible 3D models within a few minutes [7], [8], [9]. Closest to our work, DeepSketch2Face [10] presents the first deep learning-based sketching system for modeling 3D faces by mapping 2D sketches into a parametric space for face generation. However, considering the limited representa-\n\u2022 Z. Luo, D. Du, H Zhu, and X. Han are with the School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China. Y. Yu is with the Department of Computer Science at the University of Hong Kong. H. Fu is with the School of Creative Media, City University of Hong Kong. \u2022 X. Han is the corresponding author. E-mail: hanxiaoguang@cuhk.edu.cn Manuscript received xxxx.\ntion power of the parametric model, DeepSketch2Face can only produce 3D human faces with fixed styles and cannot be used for sculpting expressive skin wrinkles. SimpModeling [11] proposed a two-phase scheme that allows for diverse animalmorphic head modeling using 3D sketches. Nevertheless, it is challenging for users to work with as it relies on complicated and user-unfriendly 3D interactions. Additionally, the system struggles to generate fine-grained details due to the ambiguity of mono-typed strokes and the limited capability of PIFu [12], [13].\nIn this paper, we design and present SketchMetaFace, a powerful sketch-based 3D face modeling system that addresses the following challenges:\nAccuracy. Recent learning-based sketching systems [10], [11], [14], [15], [16] allow novice users to create visually-plausible 3D models with a few strokes. However, they are not capable of designing shapes with fine-grained details. To assist users in conveying their ideas more accurately, we adopt curvature lines [6], [14], [17] in learning-based 3D face modeling. We will demonstrate how the curvature-aware strokes significantly boost the quality of detailed surfaces generated from sketches.\nAlthough existing models [18], [19], [20], [21], [22] can map 2D images, including sketch images, to 3D shapes, they may fail to generate watertight 3D meshes with delicate details. A straightforward way to produce shapes with surface details is to blend high-quality multi-view depth maps generated by image translation [23]. Nonetheless, it is nontrivial to fuse the generated depth maps seamlessly into a watertight mesh. An alternative approach is to adopt the pixel-aligned implicit function (PIFu) [12], [13] to reconstruct watertight 3D shapes from single images. However, PIFu exhibits bounded performance in generating highfidelity geometric details. Inspired by the fact that the depth map\nar X\niv :2\n30 7.\n00 80\n4v 2\n[ cs\n.C V\n] 4\nJ ul\n2 02\n3\nproduced by image translation contains more intriguing details than PIFu-generated shapes, we propose IDGMM, i.e., Implicit and Depth Guided Mesh Modeling. It enjoys the merits of mesh, depth-map and implicit representations to produce high-quality 3D shapes from curvature-aware sketch images.\nUsability. While curvature-aware strokes empowers users to create 3D faces with fine-grained details, it may increase their cognitive load. To address this issue, we interview potential users and thoroughly analyse their requirements. We design our system based on the analyzed requirements and formulate a coarse-tofine interactive scheme: users first get used to the system with mono-typed sketches and then switch to fine-detail crafting with curvature-aware strokes soon as users get familiar with the system. We also carefully design a stroke suggestion component that bridges the gap between coarse and detailed sketching. Moreover, to follow the \u201cas-2D-as-possible\u201d principle, we keep the placement of ears as the only 3D interaction in our system.\nTo demonstrate the effectiveness of our system, we carefully conduct user studies, from which we conclude that our proposed system exhibits better usability than existing sketch-based 3D face modeling systems [10], [11]. Our system allows amateur users to create diverse shapes with fine-grained geometric details. By conducting comparisons against existing inference algorithms for mapping a single sketch to a 3D model, we demonstrate that results generated by our proposed IDGMM better reflect the appearances of the input sketches. Ablation studies are conducted to justify each design in our interface and algorithm. The contributions of our paper can be summarized as follows:\n\u2022 We present a novel, easy-to-use sketching system that allows amateur users to model high-fidelity 3D character faces in minutes (as seen in Fig. 1). \u2022 We carefully design a user interface: 1) the face modeling work follows a coarse-to-fine scheme and relies mainly on intuitive 2D freehand sketches; 2) we adopt curvatureaware strokes for modeling geometric details; 3) we introduce a data-driven suggestion tool to ease the cognitive load throughout the sketching process. \u2022 We propose a novel method, i.e., Implicit and Depth Guided Mesh Modeling (IDGMM), which fuses the advantages of mesh, implicit, and depth representations for detailed geometry inference from 2D sketches."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "In this section, we will present relevant studies on 3D avatar modeling, geometrical sketch-based modeling, and data-driven sketch-based modeling. We are aware of the breathtaking progress in sketch-based 2D image generation of faces [24], [25]. However, we will not discuss these in detail due to the page limit."
        },
        {
            "heading": "2.1 3D Face from 2D Image",
            "text": "Creating visually plausible 3D avatars is a long-standing computer graphics and vision problem. Compared with 3D face reconstruction methods, which take multi-view [26], [27] or monocular video [28], [29] as input, single image reconstruction (SVR) and sketch-based modeling provide more casual means for novices to customize 3D faces. Single-image 3D face reconstruction can be roughly divided into two streams, namely, photo-realistic human face reconstruction and caricature face reconstruction.\nThe works on single-image photo-realistic face reconstruction can be further separated into two genres, i.e., parametric and shape-from-shading methods. However, neither can be directly adopted for modeling detailed 3D faces. Parametric-based models [30], [31], [32] fall short in representing shapes with novel and customized surface details. Shape-from-shading-based methods [33], [34] suffer from deriving geometric clues from nonphoto-realistic image inputs, e.g., caricature images and sketches.\nCompared with single-image realistic 3D faces generation, which has been extensively studied and achieved exceptionally high quality, the researches on 3D caricature avatars are relatively sparse. A possible reason is that caricature 3D faces are shapes with more diversified geometry, making them extremely hard to be regularized into a single parametric model losslessly. Some work [35], [36], [37] introduced deformations to increase the capability of parametric models. However, their works are still far from generating high-fidelity 3D caricature shapes of various styles. More importantly, given that most single-image caricature face modeling methods require high-quality images as input, novice users cannot further customize the shape as they wish.\nRecently, researchers have also explored various schemes for interactive modeling from 2D sketch images [10], [16], [18], [19], [20], [38], [39]. In line with our work, DeepSketch2Face [10] proposed a sketch modeling system that allows users to create\ncaricature heads from scratch. Their method relies on a CNNbased model to parse a user-drawn sketch as the parameters for a morphable face model. However, since the 3D caricature shape is confined to the parametric caricature face model, DeepSketch2Face cannot faithfully reflect large deformations and wrinkle details presented in the sketch. To address this issue, SAniHead [16] proposed a view-surface collaborative mesh generative network, which turns dual-view freehand sketches into animalmorphic heads. Nevertheless, it fails to synthesize novel shapes deviating from training datasets due to the restricted generalization ability of their network. Our system utilizes the advantages of mesh, depth-map, and implicit representations to generate highquality 3D shapes from curvature-aware sketch images."
        },
        {
            "heading": "2.2 Geometrical Sketch-based Modeling",
            "text": "Designing free-form 3D shapes via freehand sketching has drawn considerable attention in recent decades [40]. Igarashi et al. [1] pioneer by proposing the first sketch-based modeling system that allows users to create 3D shapes from scratch by sketching 2D contour lines. A large stream of subsequent researches [41], [42], [43], [44], [45], [46] has mainly focused on designing novel interpolation functions to interpolate sketched contours lines smoothly. Unlike the sketch-based modeling systems mentioned above, which take 2D sketches as input, Fibermesh [2] allows users to model free-form surfaces by sketching and manipulating 3D curves. While Fibermesh [2] and its follow-up systems [47], [48] reduce the ambiguity remarkably with explicitly defined 3D curves, they are not capable of or are not friendly for novice users to carve organic surface details (e.g., skin wrinkles).\nTo emboss interpolated surfaces with sharper details, various methods introduce sketches with different semantics [49], [50] or curvature cues [6], [17] to formulate more determined constraints. However, additional inputs may significantly increase novice users\u2019 cognitive load. Inspired by BendSketch [6], our system allows users to draw with curvature-aware strokes, which serve as a less ambiguous means for users to specify the bulge and sink on faces accurately. To reduce the cognitive load of using curvature-aware strokes, we introduce a carefully designed sketch suggestion module to support amateurs in getting familiar with our system intuitively."
        },
        {
            "heading": "2.3 Data-driven Sketch-based Modeling",
            "text": "The recent decade has witnessed the emergence of data-driven methods for sketch-based 3D shape generation thanks to largescale 3D datasets. The data-driven sketch-based modeling systems can be roughly divided into two streams regarding the shape generation approaches, i.e., retrieval-based and learning-based.\nRetrieval-based methods [51], [52], [53], [54] consume a freehand sketch for the query and search for the most similar shape from the data warehouse as the reconstruction output. Fan et al. [55] propose a suggestive interface with shadow guidance to guide object sketching. However, shadow guidance may introduce severe visual cluttering for sketches with different semantics. Xie et al. [56] proposed to retrieve candidate object parts from a database with part sketches for further assembly. Recently, deep neural networks have been applied for retrieval-based sketch modeling systems [57], which have shown their superiority compared to their traditional learning-based counterparts in handling noisy sketch input created by novice users. However, limited by the\ncapacity of the data warehouse, retrieval-based sketch modeling may produce shapes that drift away from input sketches.\nIn recent years, learning-based solutions have been popular for sketch-based 3D shape generation and editing [10], [11], [15], [18], [19], [20], [38], [39], [58], [59], [60], [61], [62], [63], [64]. For example, Nishida et al. [64] proposed inferring urban building parameters from freehand sketches with convolutional neural networks, while Huang et al. [62] presented an interactive modeling system that infers parameters for procedural modeling from sketches. DeepSketch2Face [10] proposed a deep regression model that converts a sketch into the parameters of a morphable 3D caricature face model. However, the above parametric regression-based methods work only for 3D shapes within a specific category that can be easily parameterized. Du et al. [63] adopted implicit learning to produce artificial object parts from sketches and proposed a deep regression model to predict the position of the parts, while Sketch2CAD [15] enables users to achieve controllable part-based CAD object modeling by sketching in context. SimpModeling [11] utilized a coarse-to-fine modeling scheme, allowing users to create desired animalmorphic heads with 3D curves and on-surface sketching. We argue that 2D sketching would be more intuitive than 3D sketching since most novice users are more familiar with 2D interfaces and interactions. Furthermore, SimpModeling falls short in generating fine-grained geometric details due to the ambiguity of mono-typed strokes and the bounded capability of its shape-generation network. In this paper, our system allows users to create 3D high-fidelity facial models with 2D curvature-aware sketches intuitively."
        },
        {
            "heading": "3 USER INTERFACE",
            "text": "This section first summarizes the requirements of designing sketch-based modeling for novice users to customize high-fidelity faces of highly diversified styles. On top of the design goals, we will introduce the crucial designs of our system and justify how they reflect the design goals. Please refer to the accompanying video for sketch-based modeling in action."
        },
        {
            "heading": "3.1 Design Requirements and Analysis",
            "text": "In the design process of our sketch-based 3D face modeling system, we interviewed 11 participants with different levels of modeling experience to analyze the demands for a user-friendly sketch-based modeling interface. Three of these participants were modelers with more than five years of experience in 3D modeling, while the rest were novice users with no or little knowledge of 3D modeling. Based on the responses, we summarize the following design goals and the corresponding design choices for our system: Coarse to Fine (R1). After briefly introducing the background knowledge about sketch-based 3D shape modeling, we first discuss whether users prefer working in a top-down or bottom-up manner. All experts and most novice users preferred to model the shape in a top-down manner. Therefore, our proposed sketchbased modeling system allows users to model 3D faces in a coarseto-fine manner [11]. In the coarse stage, users can design the contour and the attachment of the faces (e.g., ears). After users finish designing a coarse head shape, they will move on to the finegrained shape modeling stage, where they can carve geometrical details such as wrinkles, mouths, eyes, etc. Note that we treat ears as attachments and adjust their position through 3D interactive operations in the coarse stage since it is difficult to determine the 3D location of attachments just via frontal-view sketching.\nAs 2D as Possible (R2). When discussing whether 3D interactions should be dominant in the system, most novice users mentioned that they prefer to express their ideas through 2D drawings. Interestingly, even professional modelers agree that 2D interactions should be the dominant interaction for the system, as they believe novices may get bored with manipulating the cameras and the 3D shapes. To this end, our system follows the \u201cas-2D-as-possible\u201d principle. Users can finish most of the design only with a 2D sketch pad, and 3D interactions (e.g., tuning the layout of ears) are introduced only when necessary. Agile and Precise (R3). While some amateurs mentioned that they want to carve a 3D face carefully according to a reference character face, others only intend to customize a visually-plausible 3D face with a few strokes. Hence, our system allows users to customize 3D faces with different degrees of interaction complexity, as shown in the demo video. Novice users can quickly orchestrate a visually plausible 3D face with the dedicated sketch stroke suggestion module. The sketch stroke suggestions also serve as a decent initialization for detailed face modeling. For users who are interested in carving customized surface details, we provide curvature-aware strokes that allow the specification of surface details to be more precise."
        },
        {
            "heading": "3.2 Coarse Shape Modeling",
            "text": "To support the design requirements mentioned in Section 3.1, in our system, the modeling of high-fidelity 3D faces is decomposed into coarse shape modeling and fine detail sketching (R1). Users may start designing a coarse 3D face by drawing face contour lines on the 2D sketching pad view, as illustrated in Fig. 2. Novice users could switch to the symmetric sketching mode. Under this mode, mirror-symmetrical strokes will be generated as the user draws on the sketch pad. In this stage, our system can produce a 3D model in a real-time manner by responding to each drawing operation. Profile Depth Editing. The essence of our system lies in eliminating 3D user interactions (R2). However, the generated 3D faces with single-view contour strokes lack depth variances along the z-axis due to the missing constraints on the depth channel. To this end, we deliberately design a profile depth editing interaction\nscheme that allows users to specify the face contours in the lateral view. Once users switch to the depth editing mode, a new canvas will appear with an initial side-view rendered 3D face contour. As seen in Fig. 2, novice users may design shapes with sharp-variant depth by revising the profile sketch without directly manipulating the 3D shapes. Ear Modeling. The attachments of 3D faces, i.e., the ears, play an essential role in shaping a virtual character\u2019s characteristics and styles. Unlike nose, eyes, and mouth, ears (and other face attachments) are of greater diversity in 3D layout, making it challenging to use only frontal-view sketching to express. To this end, our system uses separate meshes to represent the face and the ears for better expressiveness. Users may customize the ears by drawing their contour lines on the 2D sketch pad view, like specifying the coarse head shape. Specifically, the ears (also for other attachments like horns) are sketched on individual canvas layers, which facilitate users to manipulate their 2D attachment layouts and help the backend models learn diversified attachment shapes. As illustrated in Fig. 2, users can modify the 3D layout of the ears in the 3D view for more precise control of the generated shape. Users can also copy attachments as in RigMesh [3]. It is worth mentioning that layout manipulation and attachment copying are the only 3D operations in the whole modeling procedure (R2)."
        },
        {
            "heading": "3.3 Fine Detail Sketching",
            "text": "After the user customizes the coarse face shape, they may further characterize the detailed facial geometry, e.g., eyes, noses, mouth, and wrinkles. Although previous works, e.g., DeepSketch2Face [10] and SimpModeling [11], allow users to edit surface details through 2D and 3D sketching, they fall short in generating diversified and controllable surface details due to the ambiguous mono-typed sketch strokes. Curvature-aware Strokes. We adopt curvature-aware strokes [6] to alleviate the sketch\u2019s ambiguity, enabling users to carve surface details precisely (R3). Specifically, two types of strokes (i.e., ridge and valley) are defined. Before each stroke drawing, the user needs to pick a type first. Different stroke types are visualized with different colors (i.e., red for ridge and green for valley). Our\nsystem also supports tunable depth for each stroke, which defines the curvature amplitude, i.e., greater depth (darker color) means a higher ridge or deeper valley.\nStroke Suggestions. While curvature-aware strokes significantly improve the controllability of our system, they inevitably bring additional cognitive load for novice users. To address this, we carefully design a data-driven stroke suggestion tool. Consider a scenario when a user wishes to draw a pig nose on the face, as illustrated in Fig. 3. Our system allows the user to pick the \u201cnose\u201d type and select a \u201cpig\u201d style first, and then draw a contour to specify the rough shape and the location where they wish to place the nose. After that, a set of strokes with the specified category, as well as the corresponding shapes, is retrieved from the database and presented as \u201cSuggestion\u201d. The user can picks one which can be placed automatically or after manually adjusting the location and size. Users were provided 20 suggestions each time, and the retrieved sketches are editable. With such a suggestion tool, amateurs can quickly compile a neat 3D face model with the highquality sketch strokes in the database and kick off instantiating their ideas on a decent basis. The suggestion tool is implemented by a retrieval neural network based on the auto-encoder structure, please refer to the supplemental materials for details. Instant Shape Preview. An instant preview of the 3D shape could serve as guidance for further sketching. However, due to the geometry complexity, the model inference in the stage of finedetail sketching takes around 0.5s, making it unable to support real-time response. Our video shows that we adopt image space rendering and generate the frontal-view normal map as a real-time shape preview. Please refer to the supplemental materials for the implementation details of the instant preview module."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "In this section, we present the details of the backend models that support the interactive sketching interface. Overview. Following our coarse-to-fine interface design, we discuss the algorithms used for the two stages accordingly. In the coarse stage, as illustrated in Fig. 4, we propose a part-separated implicit learning method that maps the coarse input sketch Sr to separated part meshes (i.e., face and attachments). After the user tunes the part layout, these separated meshes are merged into a single mesh Mc. We then render the outer contour [65] of Mc into the sketch image Sc, on which users can add fine strokes in the detail sketching stage.\nIn the detail sketching stage, users may further craft finegrained surface details through sketching on the rendered coarse sketch image Sc. To generate detailed geometry Mf from the fine sketch Sf , as shown in Fig. 5, we propose IDGMM, which learns a progressive deformation from Mc to Mf , under the guidance of both the learned implicit field (SDF) and the learned depth map from Sf ."
        },
        {
            "heading": "4.1 Preliminary",
            "text": "Before introducing the proposed model, we will briefly review some relevant concepts and building blocks. Pix2Pix. Given a source image Is, Pix2Pix [23] learns a mapping from Is to a target image It, i.e., f : Is \u2192 It in an adversarial manner. Commonly, a U-Net is adopted to model this translation, and the conditional GAN loss and the reconstruction loss (L1 or L2 loss) are used for training. In our model, the Pix2Pix module is adopted for translations among sketch images, depth maps, and normal maps. Implicit Learning. Recently, various deep representations have been used for 3D shape reconstruction, e.g., voxels, point clouds, meshes, and implicit fields. Among them, implicit field-based methods achieve state-of-the-art performance [66], [67], [68]. There are two commonly used formulations to model implicit surfaces: occupancy and signed distance function (SDF). Occupancy is a continuous function go that maps a query point p \u2208 R3 to a binary status o \u2208 {0, 1}, indicating inside/outside of the surface. SDF is a function gs that maps p to its signed distance s to the underlying surface. A multi-layer perception (MLP) is usually adopted for approximating go or gs. PIFu. Among the works relevant to single image 3D shape reconstruction, pixel-aligned implicit function (PIFu) outperforms its counterparts in generating results better matching input images. Specifically, PIFu models a function h to map p \u2208 R3 with a projected pixel-aligned feature fp to an occupancy o or SDF value d, i.e., h : {p, fp} \u2192 o/d. Firstly, an hourglass architecture [69] is applied on I to obtain a feature map If . Then, p is projectd onto If to obtain fp. MLP is used to model h. Our system also requires input-aligned results, so we adopt PIFu as the base module for shape inference from sketch images. Our method uses SDF-based PIFu since it is more suitable for providing deformation guidance. PIFu with Normal Input. As a follow-up work of PIFu, PIFuHD [13] proposed a coarse-to-fine pixel-aligned implicit shape learning pipeline to generate more geometry details. More specifically, it utilizes PIFu as the coarse-level learning and adopts generated normal maps for fine-level learning. Inspired by PIFuHD, we infer normal maps from the input sketch images with Pix2Pix to assist in learning fine-grained surface details. Similar to the design\nproposed in PIFuHD, we maintain a tiny MLP that extracts local image features from the inferred normal maps to generate highfrequency details. In the following sections, we will use PIFu-N to denote our PIFu with normal input."
        },
        {
            "heading": "4.2 Coarse Modeling",
            "text": "In the coarse stage, users only need to draw a rough outline for a desired face, i.e., the face contour and attachment contours (e.g., ears). A straightforward way to generate a coarse model from the outline sketch Sr is to use PIFu, which maps Sr to an implicit field. Subsequently, Marching Cubes [70] can be adopted to extract a mesh from the implicit field. However, as the attachments and the face are represented with a single mesh, users cannot directly manipulate the layout for the attachments, thus significantly weakening users\u2019 control over modeling results."
        },
        {
            "heading": "4.2.1 Part-separated PIFu",
            "text": "To boost the controllability of our system, we present a novel part-separated PIFu. Let\u2019s first consider a common scenario where a face contains a left ear and a right ear. As shown in Fig. 4, three different PIFu modules are used to model the three parts separately. They use different MLPs but share a common encoder that maps Sr to feature maps. In our implementation, the number of parts is fixed. The MLPs designed for ear parts can also be used to generate ear-like attachments, such as horns.\nThe 3D location of each ear is kept without any normalization during training, which makes the network learn the layout of ears automatically. After obtaining the implicit field of each part, we extract separated meshes from them (for better efficiency, 643 resolution is used for marching cube). After users manipulate 3D ear placements, those meshes are merged into a single one with a corefine-and-compute-union operation provided by CGAL 1. After this step, we apply a remeshing method [71] to get Mc.\n1. CGAL: the Computational Geometry Algorithms Library. https://www.cgal.org/.\nAlthough our curvature-aware strokes contain a \u201cdepth\u201d attribute for depth controlling, it can only model local depth. Thus we provide a profile sketching tool for global depth editing (as seen in Fig. 2). Specifically, the profile contour is treated as the handle to define a Laplacian deformation [72]. Since Mc in the coarse stage is in a low resolution, the Laplacian deformation can be performed in real-time."
        },
        {
            "heading": "4.2.2 Training",
            "text": "The part-separated PIFu is trained in a fully-supervised manner. For each character face mesh M in the dataset, we render its contours as a sketch image input. To prepare the ground truth data for training our part-separated PIFu used in the coarse stage, we smooth faces meshes M , and then segment them into distinct parts (i.e., faces and attachments). The ground-truth SDF values for each part are calculated in the world coordinates. During training, we use the L1 metric to measure the difference between the predicted SDF values and the ground truth."
        },
        {
            "heading": "4.3 IDGMM: Implicit and Depth Guided Mesh Modeling",
            "text": "In the fine stage, Mc is first rendered into a new contour map Sc. Then users will draw curvature-aware strokes over Sc, and we denote the updated sketch image as Sf . This section discusses the method to map Sf to a model denoted as Mf . It resembles the shape of Sc but contains local geometric details reflected by Sf , as illustrated in Fig. 5.\nRecently, many deep-learning-based methods [12], [13], [66], [67], [68] have been proposed to map a sketch image to a 3D model. A straightforward solution is to apply PIFu-based methods [12], [13] and extract the surface mesh with Marching Cubes (MC) [70]. However, MC is time-consuming (5 seconds 2563 iso-value grid) when extracting high-resolution surfaces and fails to meet the requirements for interactive editing. To this end, we apply the field-guided deformation formula to speed up the extraction of detailed surfaces from implicit fields.\nSpecifically, our method takes Mc and Sf as input and learns the displacement for each vertex on Mc with the help of both the implicit and depth-map representations. Before conducting deformation, we subdivide the regions [71] where detail strokes are drawn to better afford geometric details. Note that the sketching process is iterative, and the input Mc at the n-th step is the resulting mesh at step (n-1). For simplicity, we still use Mc to represent the input coarse mesh of each step."
        },
        {
            "heading": "4.3.1 Implicit-guided Mesh Updating",
            "text": "Inspired by the work [73], SimpModeling [11] proposed a strategy for mesh deformation under the guidance of implicit fields, but it is inefficient: 1) SimpModeling utilizes an occupancy field and needs to determine the updated vertices by a dense sampling way; 2) to stabilize the deformation, the Laplacian deformation technique [72] is adopted.\nIn contrast, we update Mc directly with the guidance of the continuous SDF field to keep robustness during deformation, which dramatically reduces the computational cost of the Laplacian deformation (i.e., updating each vertex v \u2208 Mc via v\u2032 = v + gs(v)n, where n indicates the normal of v and gs(v) is the SDF value of v). The above updating mechanism could be performed iteratively for multiple times, but its enhancement was slight. Hence, we only perform one iteration to reduce the computational burden and leave the remaining detail enhancement work to the depth-guided deformation stage. We denote the new mesh after updating as M \u2032c.\nA direct way to learn the SDF function from Sf is by applying PIFu-N on Sf . However, It may lead to a misalignment between the generated SDF field and the coarse mesh Mc, thus challenging the deformation. Therefore, as illustrated in Fig. 5, we render Mc into a depth map Dc, and feed Dc and Sf together into a Pix2Pix module to infer a normal map N for conducting PIFu-N."
        },
        {
            "heading": "4.3.2 Depth-guided Mesh Refinement",
            "text": "Although normal-assisted PIFu can model details better than other existing methods, generating details as reflected in the normal map is still practically challenging. Our experiments found that the learned depth maps contain richer geometric details than the learned implicit fields. Thus we propose a depth-guided deformation method to enhance M \u2032c further. Specifically, as illustrated in Fig. 5, we first render M \u2032c into a depth map D \u2032 c and feed it together with N into a new Pix2Pix module for generating a depth map Df with sharper details than D\u2032c. Here, we use N instead of Sf since N has already captured the geometric information from Sf and can ease the learning procedure. Without Depth Alignment. To transfer geometric details from Df to M \u2032c, a straightforward way is to first convert Df to a point cloud P and then fit M \u2032c to P . Specifically, for each vertex v of M \u2032 c, we retrieve K closest points in P and employ the inverse distance weighting algorithm [74] to directly update the position of v. Flow-based Local Depth Alignment. Although the design of the method discussed above well guarantees global alignment between P and M \u2032c, there is no guarantee for local alignment. Implicitguided mesh updating is hard to ensure the alignment of local geometry (e.g., nose) between the M \u2032c and Sf (thus, both N and Df may also suffer from misalignment). Directly fitting M \u2032c to Df tends to cause artifacts due to the local misalignment between them, as shown in Fig. 6. Multiple iterations and extensive smoothings are required to obtain stable results, which is inefficient and may result in blurry geometric details. To address this issue, we\npropose a flow-based alignment method. More specifically, we train a FlowNet [75] to take Df and D\u2032c as input and output a warping field. The warping field is applied to align Df to M \u2032c and generate an aligned/warped depth D\u2032f . Then a high-quality point cloud P can be extracted from D\u2032f . Thus, P is also locally aligned with M \u2032c. The good alignment between P and M \u2032 c facilitates the registration of local geometric details from P to M \u2032c. As a result, the final mesh Mf is close to M \u2032c but with more local details, instead of being completely aligned with Sf . The alignment of the sketch, depth maps, and normal map used in Fig. 5 is shown in Fig. 7. Although a minor global misalignment exists between Mf and Sf , the resulting mesh is still plausible and convincing, as illustrated in Fig. 9. Thanks to the local alignment, we found that one iteration of the depth-guided mesh refinement is enough to reconstruct vivid details stably (the improvement of multiple iterations is slight), reducing the computational cost."
        },
        {
            "heading": "4.3.3 Training",
            "text": "IDGMM is backed by four learning-based models: Pix2Pix-1 that maps Sf \u2295Dc (\u2295 indicates concatenation) to N , Pix2Pix-2 that maps D\u2032c \u2295N to Df , PIFu-N and FlowNet. All the networks are trained separately and in a fully-supervised manner: 1) To train Pix2Pix-1, for each ground-truth mesh M (which contains rich details), we render its ridge and valley lines as input fine strokes, using the tool provided by Suggestive Contours [65]. The stroke types are encoded by the channel of red or green colors, and the depth is encoded with the shades of the color. Specifically, the ridge is encoded in (c, 0, 0) and the valley in (0, c, 0), c = 255\u2212 |k|, where k is the curvature of a line segment. Thus the smaller value of c, the visually greater color depth (i.e., visually darker), representing the higher ridge or deeper valley. In our experiments, the trained model can generalize well to strokes of varying widths, though the strokes in the training set are in a constant width. 2) We smooth M to be Ms and use it as Mc to render depth maps as Dc for training Pix2Pix-1 (N is rendered from M ). 3) We put M into a 1283 SDF field (noted as g128M ) and extract the mesh Ml. Then we render Ml into a depth map to approximate D\u2032c for training Pix2Pix-2. 4) We subdivide M to get M \u2032 with dense points and deform M \u2032 under the guidance of g128M to generate a new mesh Mg . We render M \u2032 and Mg to depth maps to approximate Df and D\u2032c. As Mg and M\n\u2032 are topologically consistent, it is easy to obtain a dense flow as supervision to train FlowNet."
        },
        {
            "heading": "5 RESULTS AND EVALUATION",
            "text": "In this section, we will evaluate our system from two aspects, namely, system usability (Section 5.1) and algorithm effectiveness (Section 5.2)."
        },
        {
            "heading": "5.1 Evaluation on System Usability",
            "text": "Apparatus. Our user interface was implemented with QT and deployed on a desktop workstation with one Intel i5 @2.7GHz CPU and 8GB of memory. Users can interact with the system with a computer mouse or a pen tablet. The neural-network-based backend model was implemented with Pytorch 1.8.1 and deployed on a server with one Intel i7 @4.2GHz CPU, 16 GB of memory, and one NVIDIA GTX 3090 GPU graphics card. To support the training of our proposed algorithms for modeling high-fidelity 3D heads, we merged the existing datasets of 3DAnimalHead [11] and 3DCaricShop [76], resulting in 4,528 high-quality models in total. Then we split these data into 9:1 for training and testing in our experiments. Please refer to our supplemental materials for the implementation details of the neural networks. Participants. Our key objective is to create a 3D modeling system that is easy to use for amateur users without 3D modeling experience. To verify this, we invited 16 subjects (P1-P16, aged 18 to 32) to participate in this evaluation session, none of whom had experience in 3D modeling. Six of them (P2, P3, P6, P7, P8, P12) had professional 2D drawing experience, and the remaining had limited drawing experience. Before the modeling session, each participant was given 10 minutes to watch an video showing the basic operations of our system. After the tutorial, each user had 20 minutes to get familiar with our system. All the participants were asked to perform comparison and usability studies."
        },
        {
            "heading": "5.1.1 Comparison Study",
            "text": "We first conducted a comparison study on different modeling systems to demonstrate the superiority of our system. After thoroughly reviewing existing sketch-based character modeling systems, we chose DeepSketch2Face [10] and SimpModeling [11] for comparison since these systems can be easily accessed. For DeepSketch2Face, its released system was used. We asked the authors of SimpModeling to provide their system to us. ZBrush is a powerful commercial software for assisting professional artists in creating arbitrary 3D models. We also added ZBrush to our informal comparison on face modeling. For a fair comparison, all 16 subjects were also given 10 minutes to learn through a tutorial and 20 minutes to get familiar with each of the other systems before the formal user study. In the formal session, each user was given a shading image of a 3D model as a reference. She/he was requested to create 3D models referring to the given image using the four compared systems (i.e., DeepSketch2Face, SimpModeling, SketchMetaFace, and ZBrush) in random order. Note that all the tools provided by SimpModeling and ZBrush are 3D\ninteractive operations, while most operations of DeepSketch2Face and SketchMetaFace focus on the 2D canvas.\nFig. 8 shows the reference images, the created models with the four systems, and the corresponding modeling time. Compared to DeepSketch2Face and SimpModeling, our system supported users to create more appealing shapes and craft more vivid surface details. The geometric shape and surface details created by our system are closer to the reference models. Compared to ZBrush, our system took less time for users to create visually reasonable 3D models. To complete each model, each user took around 2- 5 minutes to use DeepSketch2Face, around 7-15 minutes with SimpModeling, around 5-9 minutes with our system, and around 10-18 minutes with ZBrush. Most participants complained that DeepSketch2Face was hard to use as it could only output human faces (mainly because of the limited parametric space of the human face). They mentioned that SimpModeling could create coarse shapes and some minor details, but it was challenging to learn and use. We observed that most subjects got stuck in the coarse shape modeling process with SimpModeling and ZBrush. Some even gave up adjusting coarse shapes and directly turned to sculpting surface details. \u201cThe 3D operations are difficult to use, and I need to speed time adjusting the shape. I am disappointed with SimpModleing and ZBrush\u201d, as commented by P8. \u201c3D interactions are extremely unfriendly to me. I need to switch perspectives frequently. These frequent switching operations make me irritable\u201d (P11). Most subjects enjoyed the modeling process defined by SketchMetaFace. Some participants reported that SketchMetaFace was user-friendly and allowed for creating vivid avatar heads easily. They also pointed out that our system saved much time and labor in generating 3D heads. \u201cSketchMetaFace is much better than SimModeling. The coarse shape modeling provided by SketchMetaFace is easier and can save me a lot of time. The curvature-aware strokes allow me to craft details freely in an intuitive way\u201d (P6). \u201cIt is very cool to create 3D models by drawing sketches. I am looking forward to using SketchMetaFace in the future.\u201d P1 suggested that the 3D sculpting tools (e.g., smooth and crease) provided by ZBrush could be added to the fine stage, supporting users in further fine-tuning geometric details."
        },
        {
            "heading": "5.1.2 Usability Study",
            "text": "In this study, each participant was asked to freely create at least one model without restrictions on result diversity, result quality, or time duration. Fig. 9 shows a gallery of models created by these participants, which reflect the expressiveness of our system. It can be seen from this figure that our system supports amateurs in geometrical modeling to create character faces with diversified\nshapes and rich geometric details. All of the participants felt that our system was powerful in creating diversified avatar heads, and they were deeply impressed by the simplicity, intuitiveness, and controllability of our system. It is worth mentioning that two of the participants said they enjoyed the process very much and expressed their desire to learn 3D modeling.\nMost of the participants liked the intuitive stroke suggestion tool, which was quite helpful for them in figuring out the meaning of curvature-aware strokes. We observed that the participants with great drawing skills (i.e., P2, P3, P6, P7, P8, and P12) quickly became used to working with the curvature-aware strokes thanks to the suggestion tool. Once grasping curvature-aware strokes, they preferred to paint each part of the model from scratch and customize desired details by themselves instead of searching for a specific structure using the stroke suggestion module. P6 commented \u201cThe stroke suggestion tool is a very nice and useful function for assisting me in understanding the usage of curvatureaware strokes.\u201d We received similar positive comments from P7 and P12: \u201cWith the help of the stroke suggestion function, I can easily understand how to depict geometric structures using curvature-aware strokes\u201d (P7); \u201cThe curvature-aware strokes are useful and powerful for carving models\u2019 details, like wrinkles\u201d (P12). Other participants tended to use the stroke suggestion function throughout the whole modeling process due to their limited drawing skills. \u201cThe suggestion module is easy and intuitive to use. I do not need to spend much time thinking about how to paint a correct sketch. It avoids frequent modifying operations\u201d (P1). \u201cThe suggestion module is convenient and friendly for me. It reduces a lot of manual operations and allows me to create diversified results in a very easy way\u201d (P5). \u201cI can make funny and realistic results by simply searching and integrating different parts in minutes (two eyes, a nose, and a mouth)\u201d (P10).\nThe participants also provided some constructive comments. For example, P4 said, \u201cIt would be better to allow me to search for a suitable head contour in the coarse modeling stage, just like searching for a nose or a mouth in the fine stage.\u201d One potential solution is collecting a coarse shape database and applying the retrieval mechanism in the coarse-shape modeling stage. \u201cAlthough\nthe profile depth editing tool allows me to adjust models in the side view, the system still fails to create an elephant\u2019s nose. I do not know how to create an elephant\u2019s nose using the tools provided by SketchMetaFace.\u201d said P2. Enlarging our datasets and adopting multi-view drawing in the coarse stage would be a possible solution for this problem."
        },
        {
            "heading": "5.1.3 Questionnaire Analysis",
            "text": "At the end of the comparison study, each participant was required to complete a System Usability Scale (SUS) questionnaire and a NASA Task Load Index (NASA-TLX) questionnaire to evaluate the usability and workload of our system. We found that the overall SUS score of our system was 79, out of a scale of 100 (DeepSketch2Face: 64, SimpModeling: 38, ZBrush: 41), indicating the good usability of our system [77]. In Fig. 10(a), we show the mean scores for all the individual SUS questions. For the questions with the odd numbers, the higher the SUS scores, the better; for the rest of the questions, the lower the SUS scores, the better. The scores of Q1 and Q9 suggest that the participants appreciated our system and were satisfied with the models created by our system. From Q2-4, Q7-8, and Q10, we can conclude that our system supported amateur users creating desired 3D head models easily and intuitively, indicating the good user efficiency and usability of our system. The scores of Q5-6 show that the participants also recognized our system\u2019s well-designed modeling pipeline and tools. Although the high scores of Q3 and Q7 indicate that DeepSketch2Face is easy to use, the participants were disappointed with its results, leading to low scores for Q1 and Q9. The high scores of Q2, Q4, Q6, Q8, and Q10 and the low scores of Q3, Q7, and Q9 all suggest that SimpModleing and ZBrush are unfriendly for these amateur uses. Grasping these two systems is extremely hard for them.\nFig. 10(b) illustrates the average score for each question in the NASA-FLX questionnaire. The results of our systems are also positive. Compared to SimpModeling and ZBrush, our system\u2019s mental demand, physical demand, temporal demand, effort, and frustration are at an extremely low level. It implies that our system does not require users to pay a lot of concentration and effort\nwhen using it. The higher performance score of our system reflects that the participants were also more satisfied with their modeling results with our system. The lower performance score and the higher frustration score of SimpModeling and ZBrush than those of our system suggest that it was hard for the participants to create desired results using 3D operations. The lower performance score of DeepSketch2Face demonstrates that the participants were unsatisfied with the results generated by its algorithm, which also leads to a high frustration level.\nWe conducted a subjective user study to evaluate the faithfulness (i.e., the degree of fitness to reference images/models) of synthesized results. We randomly chose a set of results from the comparison study, containing 15 reference models and the corresponding results created by the participants using the four above systems. We invited 50 subjects to participate in this subjective evaluation through an online questionnaire. Most subjects had no 3D modeling experience, and none had participated in the previous studies. We showed the participants five images for each case (15 cases in total), including the input sketch and the four modeling results by the compared systems, placed side by side in random order. Each participant was asked to score each result based on the faithfulness to the reference model (1 denoting the lowest fitness and 10 for the highest fitness). Fig. 10(c) shows the mean score of each system for this study. This figure shows that the 3D models created by amateurs with our system in the comparison study received relatively higher marks than the counterpart systems, implying that our system could assist novice users in creating desired 3D heads. Statistical analysis also showed that the scores significantly differed across the compared systems. Specifically, we ran Shapiro-Wilk normality tests on the collected data and found non-normality distributions (p < 0.001). We thus conducted Kruskal-Wallis tests on the faithfulness scores and found significant effects. Paired tests between our system and each of the compared ones confirmed that our system (mean: 6.28) could effectively support amateurs in creating significantly more faithful results to the reference models than the other systems, i.e., DeepSketch2Face (mean: 1.96, p < 0.001), SimpModeling (mean: 3.64, p < 0.001) and ZBrush (mean: 5.82, p = 0.008). More details can be found in our supplementary material."
        },
        {
            "heading": "5.2 Evaluation on Algorithm Effectiveness",
            "text": "Comparison on Part-separated Mesh Inference. There are some alternative methods [21], [57], [78] for inferring part-separated meshes from an input sketch. To verify the generalization ability of part-separated PIFu, we choose two representative alternative methods for comparison. One is a retrieval-based method [57], denoted as Retrieval and the other one is a deformation-based method [21], denoted as Pixel2Mesh. The qualitative comparisons are presented in Fig. 11, where we can see that our results align much better with the input sketches. Comparisons on Sketch2Mesh. The core problem of our system is to learn the mapping from Sf to a detailed mesh. To evaluate the superiority of IDGMM, we selected four existing representative methods for comparison: 3D-R2N2 [79], Pixel2Mesh [21], DeepSDF [67] and PIFuHD [13] (the method used by SimpModeling). All these methods took Sf and Dc as input for fairness. Fig. 12 and Tab. 1 show the results of this comparison. Both qualitative and quantitative results demonstrate the superiority of our method. Although PIFuHD performs not badly on quantitative measurements, the qualitative results show that our proposed\nalgorithm (IDGMM) performs much better than PIFuHD on geometric details synthesis. Meanwhile, PIFuHD requires a timeconsuming mesh extraction process from an implicit field (around 5.0s for one result generation). SimpModeling slightly reduces PIFuHD\u2019s time consumption by sampling points along the normal directions and applying local Laplacian deformation (1.0s for one result generation). Our IDGMM combines the advantages of mesh, continuous SDF, and depth map representations, making it very powerful not only in generating detailed 3D geometry but also in inference efficiency (around 0.5s for one result generation).\nAblation Study on Implicit/Depth Guidance. There are two key components in our proposed IDGMM: implicit-guided mesh updating and depth-guided mesh refinement. To verify the indispensability of these two modules, we compared IDGMM with two alternative settings: 1) without implicit guidance - we use Dc and N as input to generate Df and corresponding warped P , which is then used to guide the deformation from Mc. 2) without depth guidance, i.e., M \u2032c shown in Fig. 5. Qualitative\nFig. 13: Ablation study on implicit/depth guidance. From left to right: (a) input sketch; (b) coarse mesh (i.e., Mc in Fig. 5); (c) resulting mesh with only depth guidance (without implicit guidance); (d) resulting mesh with only implicit guidance (without depth guidance, i.e., M \u2032c in Fig. 5); (e) resulting mesh with both guidance (i.e., Mf in Fig. 5).\n(a) without curvature-aware strokes (b) with curvature-aware strokes\nFig. 14: Ablation study on without/with curvature-aware strokes. Using curvature-aware strokes significantly helps enhance the quality of the generated geometric details.\nresults are shown in Fig. 13. The resulting meshes with both implicit and depth guidance outperform the other two options on surface detail generation, implying the necessity of the implicitguided and depth-guided modules. Ablation Study on Curvature-aware Strokes. The common option\nto represent sketches is using strokes without any curvature-aware attributes (e.g., DeepSketch2Face and SimpModeling), which is hard to depict complex surface details, as seen in the left part of Fig. 14. The right part of Fig. 14 shows the great capability of curvature-aware strokes in representing rich geometric details. Perceptive Evaluation Study. To further evaluate the effectiveness and superiority of our proposed algorithm (part-separated PIFu and IDGMM), we conducted another perceptive evaluation study. We selected 10 samples from the experiments of Comparison on Part-separated Mesh Inference (like Fig. 11), Comparisons on Sketch2Mesh (like Fig. 12), and Ablation Study on Implicit/Depth Guidance (like Fig. 13) respectively, resulting in three questionnaires. Each case in the questionnaires showed the input sketch and the results generated by different algorithms, placed side by side in random order. The 50 subjects mentioned above were also asked to evaluate each synthesized model\u2019s faithfulness (i.e., the degree of fitness to input sketches) on a ten-point Likert scale (1 = lowest fitness to 10 = highest fitness). Fig. 10(d) shows that the results generated by part-separated PIFu fit the input sketches better than Retrieval and Pixel2Mesh. Fig. 10(e) suggests that IDGMM could synthesize richer, more vivid, and more realistic geometric details than the other methods. Fig. 10(f) indicates the necessity and superiority of combining implicit and depth guidance for detailed geometry generation. For statistical analysis, we first performed Shapiro-Wilk normality tests, respectively, for the three collected data and found that all of them followed non-normality distributions (p < 0.001). Therefore, we conducted a KruskalWallis test on the faithfulness scores for each perceptive evaluation, and the results also showed significance across different comparisons. For the evaluation of coarse shape modeling, paired tests showed that our method (mean: 8.60) performs significantly better on diverse shape generation than both Retrieval (mean: 3.85, p < 0.001) and Pixel2Mesh (mean: 5.38, p < 0.001). For the evaluation of surface detail generation, the results indicated that IDGMM (mean: 8.90) led to significantly more faithful results than the other methods, i.e., 3D-R2N2 (mean: 3.25, p < 0.001), Pixel2Mesh (mean: 3.89, p < 0.001), DeepSDF (mean: 5.43, p < 0.001), and PIFuHD (mean: 6.63, p < 0.001). For the evaluation of implicit/depth guidance, the tests suggested that depth&implicit guidance (mean: 8.55) significantly performs better on geometric detail synthesis than the alternative options, i.e., only implicit guidance (mean: 6.23, p < 0.001) and only depth guidance (mean: 5.95, p < 0.001). It is worth mentioning that the difference between depth and implicit guidance was not distinct (p = 0.169). This is consistent with our expectation, since both only using depth refinement and only using implicit refinement can synthesize minor details. But they fail to depict high-quality geometric details, further confirming the significant positive effect of incorporating implicit and depth refinement. All these statistical results confirmed that all our proposed algorithms significantly outperform the corresponding alternative options. More details about evaluation are provided in our supplementary material."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we presented an easy-to-use sketching system for amateur users to create and high-fidelity 3D face models. Both the user interface and the algorithm are carefully designed. Firstly, curvature-aware strokes are utilized to assist users in easily carving geometric details. Secondly, a coarse-to-fine interface is designed. In the coarse stage, users only need to model face contours and\nthe 3D layout of ears. Then, in the fine stage, all interactions are operated on a 2D canvas for detail drawing. Thirdly, to support the accuracy and usability of the user interface, a novel method, named Implicit and Depth guided Mesh Modeling (IDGMM), is proposed. It combines the advantages of the implicit (SDF), mesh, and depth representations, and reaches a good balance between output quality and inference efficiency. Both evaluations of the system and algorithm demonstrate that our system is of better usability than existing systems and the proposed IDGMM also outperforms existing methods.\nAlthough our system is able to create 3D models with diversified shapes and rich details, it also has some limitations (Fig. 15): a) As we only focus on frontal-view sketching for detail carving, some organs with complex depth changing are hard to model, such as the nose of an elephant; b) When the strokes are densely placed, it cannot produce reasonable geometric details as a large number of vertices are required in this scenario, which our current system does not support. In the future, we will enlarge our dataset to support users in modeling shapes with other categories, such as cartoon character bodies and human garments. We will also try to take multi-view sketches as input to further support the creation of complex models, such as elephants. Meanwhile, we will explore the possibilities to carve high-resolution models efficiently and support richer detail crafting effectively. Acknowledgements. The work was supported in part by NSFC62172348, the Basic Research Project No. HZQB-KCZYZ2021067 of Hetao Shenzhen-HK S&T Cooperation Zone, the National Key R&D Program of China with grant No. 2018YFB1800800, the Shenzhen Outstanding Talents Training Fund 202002, the Guangdong Research Projects No. 2017ZT07X152 and No. 2019CX01X104, the Guangdong Provincial Key Laboratory of Future Networks of Intelligence (Grant No. 2022B1212010001), the Shenzhen Key Laboratory of Big Data and Artificial Intelligence (Grant No. ZDSYS201707251409055), and the Key Area R&D Program of Guangdong Province with grant No. 2018B030338001. It was also partially supported by Outstanding Yound Fund of Guangdong Province with No. 2023B1515020055, Shenzhen General Project with No. JCYJ20220530143604010, Hong Kong Research Grants Council under General Research Funds (HKU17206218), grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CityU 11212119) and the Centre for Applied Computing and Interactive Media (ACIM) of School of Creative Media, CityU."
        }
    ],
    "title": "SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling",
    "year": 2023
}