{
    "abstractText": "Pre-training has been an important ingredient in developing strong monocular depth estimation models in recent years. For instance, self-supervised learning (SSL) is particularly effective by alleviating the need for large datasets with dense groundtruth depth maps. However, despite these improvements, our study reveals that the later layers of the SOTA SSL method are actually suboptimal. By examining the layer-wise representations, we demonstrate significant changes in these later layers during fine-tuning, indicating the ineffectiveness of their pre-trained features for depth estimation. To address these limitations, we propose MeSa, a comprehensive framework that leverages the complementary strengths of masked, geometric, and supervised pre-training. Hence, MeSa benefits from not only general-purpose representations learnt via masked pre-training but also specialized depth-specific features acquired via geometric and supervised pre-training. Our CKA layer-wise analysis confirms that our pre-training strategy indeed produces improved representations for the later layers, overcoming the drawbacks of the SOTA SSL method. Furthermore, via experiments on the NYUv2 and IBims-1 datasets, we demonstrate that these enhanced representations translate to performance improvements in both the in-distribution and out-of-distribution settings. We also investigate the influence of the pre-training dataset and demonstrate the efficacy of pre-training on LSUN, which yields significantly better pre-trained representations. Overall, our approach surpasses the masked pre-training SSL method by a substantial margin of 17.1% on the RMSE. Moreover, even without utilizing any recently proposed techniques, MeSa also outperforms the most recent methods and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Muhammad Osama Khan"
        },
        {
            "affiliations": [],
            "name": "Junbang Liang"
        },
        {
            "affiliations": [],
            "name": "Chun-Kai Wang"
        }
    ],
    "id": "SP:b8cde0f8fe34040c1e9682cb6220fe40675bed22",
    "references": [
        {
            "authors": [
                "S.F. Bhat",
                "I. Alhashim",
                "P. Wonka"
            ],
            "title": "Adabins: Depth estimation using adaptive bins",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018,",
            "year": 2021
        },
        {
            "authors": [
                "S.F. Bhat",
                "R. Birkl",
                "D. Wofk",
                "P. Wonka",
                "M. M\u00fcller"
            ],
            "title": "Zoedepth: Zero-shot transfer by combining relative and metric depth",
            "venue": "arXiv preprint arXiv:2302.12288,",
            "year": 2023
        },
        {
            "authors": [
                "J.-W. Bian",
                "H. Zhan",
                "N. Wang",
                "T.-J. Chin",
                "C. Shen",
                "I. Reid"
            ],
            "title": "Auto-rectify network for unsupervised indoor depth estimation",
            "venue": "IEEE PAMI,",
            "year": 2021
        },
        {
            "authors": [
                "J.-W. Bian",
                "H. Zhan",
                "N. Wang",
                "Z. Li",
                "L. Zhang",
                "C. Shen",
                "M.-M. Cheng",
                "I. Reid"
            ],
            "title": "Unsupervised scale-consistent depth learning from video",
            "venue": "IJCV,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Cao",
                "Z. Xie",
                "B. Liu",
                "Y. Lin",
                "Z. Zhang",
                "H. Hu"
            ],
            "title": "Parametric instance classification for unsupervised visual feature learning",
            "venue": "Advances in Neural Information Processing Systems, 33,",
            "year": 2020
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Chen",
                "C. Schmid",
                "C. Sminchisescu"
            ],
            "title": "Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera",
            "venue": "ICCV, pages 7063\u20137072,",
            "year": 2019
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "C. Doersch",
                "A. Gupta",
                "A.A. Efros"
            ],
            "title": "Unsupervised visual representation learning by context prediction",
            "venue": "ICCV, pages 1422\u20131430,",
            "year": 2015
        },
        {
            "authors": [
                "D. Eigen",
                "C. Puhrsch",
                "R. Fergus"
            ],
            "title": "Depth map prediction from a single image using a multi-scale deep network",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "H. Fu",
                "M. Gong",
                "C. Wang",
                "K. Batmanghelich",
                "D. Tao"
            ],
            "title": "Deep ordinal regression network for monocular depth estimation",
            "venue": "CVPR, pages 2002\u20132011,",
            "year": 2018
        },
        {
            "authors": [
                "R. Garg",
                "V.K. BG",
                "G. Carneiro",
                "I. Reid"
            ],
            "title": "Unsupervised cnn for single view depth estimation: Geometry to the rescue",
            "venue": "ECCV. Springer,",
            "year": 2016
        },
        {
            "authors": [
                "S. Gidaris",
                "P. Singh",
                "N. Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "arXiv preprint arXiv:1803.07728,",
            "year": 2018
        },
        {
            "authors": [
                "C. Godard",
                "O. Mac Aodha",
                "G.J. Brostow"
            ],
            "title": "Unsupervised monocular depth estimation with left-right consistency",
            "venue": "CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "C. Godard",
                "O. Mac Aodha",
                "M. Firman",
                "G.J. Brostow"
            ],
            "title": "Digging into self-supervised monocular depth prediction",
            "venue": "ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "J.-B. Grill",
                "F. Strub",
                "F. Altch\u00e9",
                "C. Tallec",
                "P. Richemond",
                "E. Buchatskaya",
                "C. Doersch",
                "B. Avila Pires",
                "Z. Guo",
                "M. Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "V. Guizilini",
                "R. Ambrus",
                "S. Pillai",
                "A. Raventos",
                "A. Gaidon"
            ],
            "title": "3d packing for self-supervised monocular depth estimation",
            "venue": "CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Guo",
                "N.U. Islam",
                "M.B. Gotway",
                "J. Liang"
            ],
            "title": "Discriminative, restorative, and adversarial learning: Stepwise incremental pretraining",
            "venue": "Domain Adaptation and Representation Transfer: 4th MICCAI Workshop, DART 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings, pages 66\u201376. Springer,",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR, pages 770\u2013778,",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "H. Fan",
                "Y. Wu",
                "S. Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Chen",
                "S. Xie",
                "Y. Li",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009,",
            "year": 2022
        },
        {
            "authors": [
                "M. Jaderberg",
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Spatial transformer networks",
            "venue": "In NeurIPS,",
            "year": 2015
        },
        {
            "authors": [
                "A. Kirillov",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Panoptic feature pyramid networks",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6399\u20136408,",
            "year": 2019
        },
        {
            "authors": [
                "S. Kornblith",
                "M. Norouzi",
                "H. Lee",
                "G. Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "International Conference on Machine Learning, pages 3519\u20133529. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "I. Laina",
                "C. Rupprecht",
                "V. Belagiannis",
                "F. Tombari",
                "N. Navab"
            ],
            "title": "Deeper depth prediction with fully convolutional residual networks",
            "venue": "3DV,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "V. Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692,",
            "year": 2019
        },
        {
            "authors": [
                "R. Mahjourian",
                "M. Wicke",
                "A. Angelova"
            ],
            "title": "Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints",
            "venue": "CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "N. Mayer",
                "E. Ilg",
                "P. Fischer",
                "C. Hazirbas",
                "D. Cremers",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "What makes good synthetic training data for learning disparity and optical flow estimation",
            "venue": "International Journal of Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "B. Neyshabur",
                "H. Sedghi",
                "C. Zhang"
            ],
            "title": "What is being transferred in transfer learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "J. Ning",
                "C. Li",
                "Z. Zhang",
                "Z. Geng",
                "Q. Dai",
                "K. He",
                "H. Hu"
            ],
            "title": "All in tokens: Unifying output space of visual tasks via soft token",
            "venue": "arXiv preprint arXiv:2301.02229,",
            "year": 2023
        },
        {
            "authors": [
                "A. Pasad",
                "J.-C. Chou",
                "K. Livescu"
            ],
            "title": "Layer-wise analysis of a self-supervised speech representation model",
            "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 914\u2013921. IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "A. Pasad",
                "B. Shi",
                "K. Livescu"
            ],
            "title": "Comparative layer-wise analysis of self-supervised speech models",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE,",
            "year": 2023
        },
        {
            "authors": [
                "V. Patil",
                "C. Sakaridis",
                "A. Liniger",
                "L. Van Gool"
            ],
            "title": "P3depth: Monocular depth estimation with a piecewise planarity prior",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1610\u20131621,",
            "year": 2022
        },
        {
            "authors": [
                "R. Ranftl",
                "K. Lasinger",
                "D. Hafner",
                "K. Schindler",
                "V. Koltun"
            ],
            "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
            "venue": "IEEE PAMI,",
            "year": 2020
        },
        {
            "authors": [
                "R. Ranftl",
                "A. Bochkovskiy",
                "V. Koltun"
            ],
            "title": "Vision transformers for dense prediction",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12179\u2013 12188,",
            "year": 2021
        },
        {
            "authors": [
                "N. Silberman",
                "D. Hoiem",
                "P. Kohli",
                "R. Fergus"
            ],
            "title": "Indoor segmentation and support inference from rgbd images",
            "venue": "ECCV,",
            "year": 2012
        },
        {
            "authors": [
                "L. Sun",
                "J.-W. Bian",
                "H. Zhan",
                "W. Yin",
                "I. Reid",
                "C. Shen"
            ],
            "title": "Sc-depthv3: Robust self-supervised monocular depth estimation for dynamic scenes",
            "venue": "arXiv preprint arXiv:2211.03660,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wang",
                "A.C. Bovik",
                "H.R. Sheikh",
                "E.P. Simoncelli"
            ],
            "title": "Image Quality Assessment: from error visibility to structural similarity",
            "venue": "IEEE TIP,",
            "year": 2004
        },
        {
            "authors": [
                "Z. Wu",
                "Y. Xiong",
                "S.X. Yu",
                "D. Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "CVPR, pages 3733\u20133742,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Xie",
                "Y. Lin",
                "Z. Zhang",
                "Y. Cao",
                "S. Lin",
                "H. Hu"
            ],
            "title": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Xie",
                "Z. Geng",
                "J. Hu",
                "Z. Zhang",
                "H. Hu",
                "Y. Cao"
            ],
            "title": "Revealing the dark secrets of masked image modeling",
            "venue": "arXiv preprint arXiv:2205.13543,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xie",
                "Z. Zhang",
                "Y. Cao",
                "Y. Lin",
                "J. Bao",
                "Z. Yao",
                "Q. Dai",
                "H. Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9653\u20139663,",
            "year": 2022
        },
        {
            "authors": [
                "W. Yin",
                "J. Zhang",
                "O. Wang",
                "S. Niklaus",
                "L. Mai",
                "S. Chen",
                "C. Shen"
            ],
            "title": "Learning to recover 3d scene shape from a single image",
            "venue": "CVPR, pages 204\u2013213,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Yin",
                "J. Shi"
            ],
            "title": "GeoNet: Unsupervised learning of dense depth, optical flow and camera pose",
            "venue": "CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "F. Yu",
                "A. Seff",
                "Y. Zhang",
                "S. Song",
                "T. Funkhouser",
                "J. Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
            "venue": "arXiv preprint arXiv:1506.03365,",
            "year": 2015
        },
        {
            "authors": [
                "W. Yuan",
                "X. Gu",
                "Z. Dai",
                "S. Zhu",
                "P. Tan"
            ],
            "title": "New crfs: Neural window fully-connected crfs for monocular depth estimation",
            "venue": "arXiv preprint arXiv:2203.01502,",
            "year": 2022
        },
        {
            "authors": [
                "R. Zhang",
                "P. Isola",
                "A.A. Efros"
            ],
            "title": "Colorful image colorization",
            "venue": "Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 649\u2013666. Springer,",
            "year": 2016
        },
        {
            "authors": [
                "W. Zhao",
                "Y. Rao",
                "Z. Liu",
                "B. Liu",
                "J. Zhou",
                "J. Lu"
            ],
            "title": "Unleashing text-to-image diffusion models for visual perception",
            "venue": "arXiv preprint arXiv:2303.02153,",
            "year": 2023
        },
        {
            "authors": [
                "T. Zhou",
                "M. Brown",
                "N. Snavely",
                "D.G. Lowe"
            ],
            "title": "Unsupervised learning of depth and egomotion from video",
            "venue": "CVPR,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Monocular depth estimation is an important computer vision problem, with applications ranging from self-driving cars to augmented reality and robotics. Initially, supervised learning methods [10, 11, 25] were developed to tackle this problem, utilizing annotated depth data for training the models. However, collecting diverse real-world datasets with precise ground-truth depth is extremely challenging. Hence, self-supervised methods were developed that learn depth from stereo image pairs [12, 14] or monocular videos [49] without relying on depth annotations. Particularly, self-supervised monocular depth estimation from videos is especially appealing for real-world applications, as it only requires a single camera for data collection. These self-supervised methods typically employ view synthesis as the main supervision signal and are trained via the photometric loss [49].\nPreprint. Under review.\nar X\niv :2\n31 0.\n04 55\n1v 1\n[ cs\n.C V\n] 6\nIn recent years, pre-training has emerged as an important factor in developing strong depth estimation models. For instance, Ranftl et al. [34] noticed that randomly initialized models tend to be about 35% worse than the same models pre-trained on ImageNet. Building on this, Xie et al. [41] recently obtained SOTA results by directly fine-tuning a SimMIM [42] pre-trained network for depth estimation. SimMIM [42], a masked pre-training [21, 42] method, learns self-supervised representations via the following pretext task: given a partially masked input image, the network reconstructs the masked portions of the image. Interestingly, this relatively simple pre-training task obtains SOTA performance on depth estimation without utilizing any geometric priors.\nDespite these pre-training algorithms yielding significant improvements, we demonstrate that the last layers of the SOTA SSL method [41] are actually suboptimal. To investigate this, we compare the similarity of the pre-trained representation and fine-tuned representation for each layer (Figure 2). Intuitively, a higher similarity means that the pre-trained representation is well-suited for the downstream task and hence requires minimal updates during the fine-tuning process and vice versa [29]. Our analysis reveals significant changes in these later layers during fine-tuning, indicating the ineffectiveness of the pre-trained features for depth estimation.\nBased on the intuition that different pre-training strategies capture distinct types of features, we propose a comprehensive pre-training framework, MeSa, that addresses the aforementioned limitations. MeSa leverages the complementary strengths of three types of learning strategies: masked, geometric, and supervised pre-training. Via this synergy, our framework benefits from not only general-purpose representations learnt via masked pre-training but also specialized depth-specific features acquired via geometric and supervised pre-training.\nOur layer-wise analysis confirms that our pre-training strategy indeed produces improved representations for the later layers, thereby successfully overcoming the drawbacks of the SOTA SSL method. Furthermore, via experiments on the NYUv2 and IBims-1 datasets, we demonstrate that these enhanced representations translate to performance improvements in both the in-distribution and out-of-distribution settings. Additionally, we investigate the impact of the pre-training dataset and demonstrate the superiority of pre-training on the LSUN dataset, resulting in significantly improved pre-trained representations. Moreover, via benefiting from the 3D projective geometry during pre-training, our method helps eliminate the artifacts observed in the SOTA SSL model that lacks geometric priors.\nTo sum up, our main contributions include:\n\u2022 A qualitative analysis of pre-training effectiveness based on layer-wise feature similarities that uncovers the insight that the later layers of the SOTA SSL method are not optimally pre-trained.\n\u2022 A novel pre-training pipeline, MeSa, that effectively pre-trains the entire network by leveraging the complementary strengths of masked, geometric, and supervised pre-training, thereby benefiting from both general-purpose as well as specialized depth-specific features.\n\u2022 Our pre-trained model surpasses the SOTA masked pre-training method by a substantial margin of 17.1% on the RMSE. Moreover, even without utilizing any recently proposed techniques, it also outperforms the most recent methods and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Monocular Depth Estimation",
            "text": "Depth estimation was initially treated as a supervised learning problem [10, 11, 25], requiring groundtruth depth to train the networks. Since collecting ground-truth depth is a challenging problem in itself, some methods explored using synthetic data [28]. However, it is also not trivial to generate large varied synthetic datasets containing diverse scenes. A promising alternative is to train depth estimation networks using self-supervised learning, where view synthesis forms the main supervision signal and depth is synthesized as an intermediate step. Zhou et al. [49] developed one of the first monocular self-supervised algorithms, where they trained joint depth estimation and pose estimation networks using monocular videos. Since then, several methods [17, 44, 7, 14, 27] have been proposed\nto improve various components. For instance, Monodepth2 [15] proposed a simple model to elegantly handle occlusions as well as reduce visual artifacts.\nSC-DepthV1 [4] proposed a geometry consistency loss in order to learn scale-consistent depth maps across the video. SC-DepthV2 [3] built on top of this and proposed an auto-rectify network to alleviate training instabilities due to rotation of handheld cameras, thereby enabling better depth estimation on indoor scenes. Following this, SC-DepthV3 [37] used an external pre-trained depth estimation network to generate pseudo-depth, which was used to improve the depth estimation performance in dynamic scenes. Remarkably, Xie et al. [41] recently achieved SOTA performance by directly fine-tuning a masked pre-trained SimMIM [42] model for depth estimation without utilizing any geometric constraints used in previous methods. In this paper, we analyze the layer-wise representations of this SOTA model and leverage the 3D projective geometry in order to overcome its drawbacks, thereby designing a strong self-supervised learning algorithm for monocular depth estimation."
        },
        {
            "heading": "2.2 Self-supervised Learning",
            "text": "Self-supervised learning (SSL) is a powerful approach for learning representations from unlabeled data without requiring human annotation. SSL learns by leveraging various pretext tasks such as relative patch prediction [9], rotation prediction [13], and colorization [47]. A popular class of SSL methods is contrastive learning [39, 20, 6, 5, 16, 40] which learns transformation invariant representations via maximizing the similarity of positive pairs and minimizing the similarity of negative pairs. Masked image modeling (MIM) has recently gained traction in vision [21, 42] following the success of masked pre-training in NLP [8, 26]. Although these MIM approaches have obtained SOTA results in several tasks, a comprehensive understanding of the pre-trained representations is still lacking, making the development of new SSL methods challenging. Recently, a few methods [41, 31, 32] have attempted to understand these pre-trained presentations. Our work follows in this direction and uses the insights from our analysis to design a better self-supervised learning algorithm that effectively pre-trains the entire network for depth estimation."
        },
        {
            "heading": "3 Method",
            "text": "As illustrated in Figure 1, our framework consists of three pre-training strategies \u2013 1) Masked, 2) Geometric, and 3) Supervised pre-training. By synergistically integrating these three strategies, MeSa facilitates effective representation learning that benefits from both general-purpose representations (via masked pre-training) as well as depth-specific features (via geometric and supervised pretraining). Hence, MeSa effectively pre-trains the entire network including the later layers, thereby addressing the limitations of the SOTA SSL method. In the following subsections, we briefly introduce masked, geometric, and supervised pre-training in Sections 3.1, 3.2, and 3.3 respectively. This is then followed by the fine-tuning and implementation details in Section 3.4. Lastly, Section 3.5 outlines the techniques used for the layer-wise analysis to qualitatively understand the representations learnt via the three pre-training strategies."
        },
        {
            "heading": "3.1 Masked Pre-training",
            "text": "Masked pre-training learns representations by masking a region of the input image and reconstructing the masked region based on the partially masked input. In this work, we utilize the SimMIM framework proposed by Xie et al. [42] for masked pre-training. However, other pre-training methods could be used as well since our framework is agnostic to the exact choice of pre-training method. We use a simple masking strategy and randomly mask out some patches of the input image. The decoder consists of a single linear layer to predict the pixels of the masked regions. Finally, the network is trained using a plain \u21131 loss applied to the masked regions:\nL = 1\n|xM | \u2225xM \u2212 g(f(x\u0302))M\u22251 (1)\nwhere f is the encoder, g is the decoder, x is the raw image, and x\u0302 is the partially masked image input to the encoder. The subscript M denotes the masked subset of pixels whereas | \u00b7 | denotes the number of pixels. After training, the decoder g is usually discarded and the encoder f is used for downstream tasks."
        },
        {
            "heading": "3.2 Geometric Pre-training",
            "text": "Geometric pre-training learns representations via utilizing the 3D projective geometry. In this work, we use the SC-DepthV2 [3] framework for geometric pre-training, where depth and pose networks are trained jointly on monocular videos. Given consecutive image pairs Ia, Ib from a video, we first generate the predicted depths Da, Db of the two views and the relative camera pose Pab between them:\nDa = g \u2032(f(Ia)), Db = g \u2032(f(Ib)), Pab = h(Ia, Ib) (2) where f and g\u2032 are the depth estimation encoder and decoder respectively whereas h is the pose estimation network.\nView synthesis forms the main supervision signal (i.e., given one view, the task is to predict a view of the same scene from a different viewpoint). Given the predicted depth Da, relative camera pose Pab, and the source view Ib, differentiable bilinear interpolation [22] is used to generate a prediction of the reference view Ia\u2032 . Hence, depth is actually synthesized as an intermediate step and photometric loss between the generated Ia\u2032 and actual Ia images is used to train the network:\nLP = 1 |V| \u2211 p\u2208V ( \u03bb \u2225Ia(p)\u2212 I \u2032a(p)\u22251 + (1\u2212 \u03bb) 1\u2212 SSIMaa\u2032(p) 2 ) (3)\nwhere V is the set of valid points projected from Ia to Ib and SSIMaa\u2032 is the similarity between Ia and Ia\u2032 computed via the SSIM function [38].\nMoreover, the geometry consistency loss is used to ensure that the two depth maps (Da and Db) are consistent in terms of 3D geometry:\nLG = 1 |V| \u2211 p\u2208V Ddiff(p) (4)\nwhere Ddiff is the depth inconsistency map between Da and Db. For further details, please refer to SC-Depth [4]."
        },
        {
            "heading": "3.3 Supervised Pre-training",
            "text": "Supervised pre-training learns representations via using off-the-shelf supervised pre-trained networks for depth estimation. In this work, we use the SC-DepthV3 [37] framework for supervised pre-training, which uses the LeReS [43] pre-trained network to generate pseudo-depth. This pseudo-depth is used to improve performance on dynamic objects via the confident depth ranking loss (LCDR) and on object boundaries via the egde-aware relative normal loss (LERN ). Hence, the overall loss for joint training via geometric and supervised pre-training is given by:\nL = \u03b1LMP + \u03b2LG + \u03b3LN + \u03b4LCDR + \u03f5LERN (5)\nwhere LMP is the weighted photometric loss, LG is the geometry consistency loss, LN is the normal matching loss, LCDR is the confident depth ranking loss, and LERN is the edge-aware relative normal loss."
        },
        {
            "heading": "3.4 Implementation Details",
            "text": "We pre-train via the three learning strategies sequentially in order to avoid training instability associated with concurrent training [18]. Moreover, this also allows us to significantly reduce the pre-training time since we can leverage existing pre-trained models, thereby eliminating the need for training the entire pipeline from scratch whenever a new pre-training method is added.\nIn the masked pre-training phase, we employ a Swin-v2-L network as the encoder f and a single linear layer as the decoder g. For the subsequent geometric and supervised pre-training stages, we keep the pre-trained encoder f and replace the decoder with a DispNet [49] g\u2032. Additionally, we utilize a ResNet18 [19] backbone in the pose network h to compute the relative camera pose between the concatenated input images.\nWe use the LSUN [45] dataset for masked pre-training whereas we utilize the training split of the NYUv2 [36] dataset for geometric pre-training. For supervised pre-training, we leverage the pretrained network provided by LeReS [43] to generate the pseudo-depth. We follow the training details outlined in the official methods (SimMIM [42] and SC-Depth [37]), with the exception of using an image size of 480\u00d7480 instead of 256\u00d7320 for geometric and supervised pre-training on NYUv2. To evaluate all the pre-trained methods, we follow the SOTA SSL method [41] and fine-tune them on the training split of the NYUv2 dataset [36]. We fine-tune for 75 epochs on 8 A100 GPUs, using a polynomial learning rate schedule with a 0.9 factor and a min lr of 10\u22125 and a max lr of 10\u22124."
        },
        {
            "heading": "3.5 Layer-wise Analysis",
            "text": "Following previous work [41, 29], we use CKA (centered kernel alignment) [24] for our analysis, which is a metric that allows us to compare the representations of various layers within a network.\nCKA similarity values range from 0 to 1, with increasing values meaning that the two representations are more similar. We compare the layer-wise representations of nine layers within the Swin-v2-L architecture. For convenience, we refer to them as layers 0-8, with the exact layers detailed in the supplementary material.\nWe perform two types of analyses to evaluate the efficacy of the pre-trained layer-wise representations. Firstly, we compare the representations of layers 1-8 in the network to layer 0 (Figure 2 left) in order to study the changes in representations as we delve deeper into the network. Secondly, for each layer, we compare the similarity of its pre-trained representation to its fine-tuned representation (Figure 2 right) in order to understand how each layer evolves during the fine-tuning process. Intuitively, a higher similarity means that the pre-trained representation is well-suited for the downstream task and hence requires minimal updates during the fine-tuning process and vice versa [29]."
        },
        {
            "heading": "4 Experiments",
            "text": "Firstly, we analyze the layer-wise representations of the SOTA SSL model in order to illustrate the suboptimal pre-trained representations of the later layers (Section 4.1). To address this shortcoming, we propose a novel pre-training pipeline, MeSa, that not only achieves improved quantitative performance in both in-distribution (Section 4.2) and out-of-distribution (Section 4.3) settings but also learns better layer-wise representations (Section 4.4). Lastly, we investigate the impact of different pre-training datasets on the layer-wise features (Section 4.5) and conclude with a comprehensive comparison against the SOTA depth estimation models (Section 4.6)."
        },
        {
            "heading": "4.1 Layer-wise Analysis of the SOTA SSL Model",
            "text": "Although the SOTA SSL model yields excellent performance, it is unclear which parts of the pretrained model contribute to this improvement. In order to understand this, we analyze the layer-wise representations of this masked pre-trained model using the techniques discussed in Section 3.5.\nBased on the analysis in Figure 2 (left), we discover a U-shaped pattern in the representation space, where the representations initially start getting farther apart but then become more similar to the initial layers towards the end of the network. The top row of Figure 2 (left) compares the representation of layer 0 against the representations of layers 0-8. The similarity of layer 0\u2019s representation with itself is 1 as expected. Thereafter, for layers 1-4, the representations begin to diverge from layer 0\u2019s representation, as indicated by the decreasing similarities (darker colors). However, interestingly, past layer 4, the similarity increases (lighter colors) suggesting that the representations are becoming more similar to the layer 0 representation. This U-shaped pattern is likely a result of the pre-training objective, which is to reconstruct masked portions of the image. Hence, the later layers end up having similar representations to the first few layers. A similar pattern has also been observed in [31, 32] in the context of speech processing.\nWhereas this U-shaped pattern is useful for solving the self-supervised pretext task, it is not obvious if it aids in learning effective representations for downstream tasks. To investigate this, we compare the similarity of the pre-trained and fine-tuned representations of each layer in Figure 2 (right). From the diagonal line (top-left to bottom-right), we observe that the similarities are quite small for the later layers (5-8). Lower similarities imply that these layers undergo significant changes during fine-tuning, indicating that they are not effective for the downstream depth estimation task [29]."
        },
        {
            "heading": "4.2 MeSa",
            "text": "To address the aforementioned drawbacks, we propose a novel pre-training pipeline, MeSa, that utilizes the complementary benefits of masked, geometric, and supervised pre-training.\nTable 1 shows the depth estimation results of the three pre-training strategies on the NYUv2 dataset. The results demonstrate significant performance improvements achieved by all three learning strategies, highlighting their complementary strengths. Overall, our approach surpasses the SOTA SSL model, which relies solely on masked pre-training, by 10.2% on absolute relative error (0.083\u21920.074) and 7.64% on RMSE (0.287\u21920.265). Figure 3 presents qualitative results of the three pre-training strategies on the NYUv2 dataset, demonstrating significant improvements through the integration of geometric and supervised pre-training. When relying solely on masked pre-training, as in the SOTA SSL method, artifacts appear at the top and bottom of the predicted depth maps due to the absence of ground-truth depth (during fine-tuning) in these regions. By utilizing geometric pre-training, our method effectively eliminates these artifacts since the photometric loss utilizes 3D projective geometry and learns accurate depth across the entire image without relying on ground-truth. Moreover, geometric and supervised pre-training also help enhance the sharpness of the predicted depth maps."
        },
        {
            "heading": "4.3 Out-of-distribution Performance",
            "text": "In this section, we evaluate the performance of the three pre-training strategies in the out-ofdistribution (OOD) setting. To this end, we evaluate the models trained on the NYUv2 dataset directly on the IBims-1 dataset without any additional fine-tuning.\nTable 2 presents the OOD results on the IBims-1 dataset. In addition to the overall depth estimation accuracy (measured via AbsRel), it is also important to improve the accuracy of depth boundaries (measured via \u03b5accDBE and \u03b5 comp DBE) as well as planar regions (measured via \u03b5 plan PE and \u03b5 orie PE ), both of which are critical for many real-world applications. Our results highlight that we surpass the SOTA masked pre-training method on all five metrics in the OOD setting. Notably, we achieve substantial\nimprovements of 35.8% (30.05\u219219.31) and 33.4% (3.26\u21922.17) on depth boundary completeness (\u03b5compDBE) and planarity (\u03b5 plan PE ), respectively.\nFigure 4 visualizes the OOD performance of the three pre-training strategies. From the results, we observe a significant improvement in the sharpness of depth maps when leveraging geometric and supervised pre-training. Moreover, the localization of depth boundaries is also greatly enhanced."
        },
        {
            "heading": "4.4 Layer-wise Analysis of MeSa",
            "text": "In this section, we analyze layer-wise representations of the MeSa pre-trained network to ascertain that our pre-training strategy indeed produces improved representations for the later layers, thereby successfully overcoming the drawbacks of the SOTA SSL method.\nSimilar to Section 4.1, for each layer, we compare the similarity of its pre-trained representation to its fine-tuned representation (Figure 5). As mentioned earlier, when using masked pre-training (MP) alone, the later layers (5-8) are not particularly beneficial for depth estimation since they undergo significant changes during fine-tuning, illustrated by the low similarity values. On the other hand, incorporating geometric pre-training (GP) alongside MP (i.e., MP+GP) allows for more effective representations to be learned deeper into the network, as shown by the higher similarities (lighter colors) until layer 6. However, layers 7-8 are still not optimally utilized. By leveraging all three pre-training strategies (i.e., MP+GP+SP), even these last two layers have comparatively higher similarities. Hence, MeSa effectively pre-trains the entire network, including the last few layers, for\ndownstream depth estimation. The Appendix shows more CKA layer-wise analyses illustrating the efficacy of MeSa pre-training."
        },
        {
            "heading": "4.5 Impact of Different Pre-training Datasets on Layer-wise Features",
            "text": "In this section, we study the impact of different pre-training datasets on the layer-wise pre-trained features. To investigate this, we compare the representations of two networks pre-trained on LSUN and ImageNet respectively. LSUN is less diverse (featuring mostly indoor scenes) in comparison to ImageNet, allowing us to understand the impact of pre-training on these two datasets of varying diversities. To ensure a fair comparison, we use the same size of pre-training dataset (1200K) in both cases. For ImageNet, we use the pre-trained model provided by Xie et al. [42, 41] whereas for LSUN, we pre-train a Swin-v2-L network using SimMIM [42] on a subset of the LSUN dataset. The subset consists of 300K images each from the bedroom, dining room, living room, and kitchen categories.\nSimilar to the analysis in Section 4.1, Figure 9 (in the Appendix) compares the representations of various layers within the pre-trained networks, whereas Figure 10 compares the representations of the pre-trained networks and the (depth estimation) fine-tuned networks. The analysis in Figure 9 (top row) reveals that the intermediate layers of the LSUN pre-trained network exhibit lower similarities (darker colors) with layer 0 compared to the similarities of the corresponding layers of the ImageNet pre-trained network. Since earlier layers (e.g., layer 0) capture low-level features, this suggests that pre-training with LSUN learns more high-level representations in the middle layers that are beneficial for tasks requiring higher-level reasoning, as illustrated by the superior depth estimation results on NYUv2. This is further corroborated by Figure 10, which demonstrates that the LSUN pre-trained features are more effective for the downstream task than the ImageNet pre-trained features, as evidenced by the higher similarities (lighter colors) between the pre-trained and fine-tuned features along the diagonal line for LSUN compared to ImageNet. Since the images in LSUN have a smaller diversity compared to those in ImageNet, we hypothesize that pre-training on LSUN forces the network to reconstruct masked portions of similar images, thereby encouraging it to utilize more layers for higher level reasoning than for RGB reconstruction (Figure 9)."
        },
        {
            "heading": "4.6 Comparison with SOTA Methods",
            "text": "Table 3 compares MeSa against state-of-the-art methods on the NYUv2 dataset. As in recent works, we primarily focus on RMSE since performance on the other metrics is highly saturated. Our method is most directly comparable to [41], which we surpass by a large margin of 17.1%. Moreover, even without utilizing any recently proposed techniques, such as soft token and mask augmentation [30], metric bins module [2] or pre-trained text-to-image diffusion model [48], our approach also outperforms the most recent methods. Since our method is agnostic to these techniques, it could also be used in conjunction with them to yield further improvements. Overall, we surpass the current best approach [48] by 6.3%, establishing a new SOTA on the challenging NYUv2 dataset."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose MeSa, a novel pre-training pipeline, to overcome the limitations of the SOTA SSL method, which fails to optimally pre-train the later layers. Via integrating the complementary strengths of masked, geometric, and supervised pre-training, MeSa learns effective layer-wise representations across the entire network. Moreover, MeSa leads to a substantial improvement of 17.1% on the RMSE compared to the SOTA SSL method and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset."
        },
        {
            "heading": "A NYUv2 Visualizations",
            "text": "Figure 6 shows more visualizations of the three pre-training strategies on the NYUv2 dataset.\nB IBims-1 Visualizations\nFigure 7 shows more visualizations of the three pre-training strategies in the OOD setting on the IBims-1 dataset."
        },
        {
            "heading": "C CKA Layers",
            "text": "Similar to previous studies [41, 29], we adopt the CKA (centered kernel alignment) [24] metric to compare the representations learnt via the various pre-training methods. The CKA similarity values range from 0 to 1, where higher values indicate greater similarity between the representations. In our investigation, we examine the layer-wise representations of nine layers within the Swin-v2-L architecture. For convenience, we refer to them as layers 0-8, with the exact layers detailed in Table 4."
        },
        {
            "heading": "D MeSa Layer-wise Analysis",
            "text": "Here, we analyze the layer-wise representations of the MeSa pre-trained network to show that our pre-training strategy effectively pre-trains the entire network, including the later layers. To this end, we conduct a comparison between the layer 0 representation and the representations of layers 1-8, as illustrated in Figure 8 (top row). When utilizing masked pre-training (MP) alone, the later layers (5-8) do not provide significant benefits for downstream depth estimation since they become specialized for the reconstruction task, as evidenced by the increasing similarity values (lighter colors) past layer 4 in the top row. However, incorporating geometric pre-training in addition to masked pre-training (i.e., MP+GP) enables more effective representations to be learned deeper into the network, as indicated by the decreasing similarities (darker colors) up to layer 6. Nevertheless, layers 7-8 are still not fully utilized. By leveraging all three pre-training strategies (i.e., MP+GP+SP), we observe a monotonous decrease in similarities (darkening of colors) up to layer 8, indicating that the entire network learns distinct and high-level representations compared to layer 0. Hence, MeSa effectively pre-trains the entire network, including the later layers."
        },
        {
            "heading": "E Layer-wise Analysis of Different Pre-training Datasets",
            "text": "In this section, we study the impact of different pre-training datasets on the layer-wise pre-trained features. For detailed configurations of the ImageNet and LSUN pre-trained networks, please refer to Section 4.5 in the main paper.\nSimilar to the analysis in Section 4.1, Figure 9 compares the representations of various layers within the pre-trained networks, whereas Figure 10 compares the representations of the pre-trained networks and the (depth estimation) fine-tuned networks. The analysis in Figure 9 (top row) reveals that the intermediate layers of the LSUN pre-trained network exhibit lower similarities (darker colors) with layer 0 compared to the similarities of the corresponding layers of the ImageNet pre-trained network. Since earlier layers (e.g., layer 0) capture low-level features, this suggests that pre-training with LSUN learns more high-level representations in the middle layers that are beneficial for tasks requiring higher-level reasoning, as illustrated by the superior depth estimation results on NYUv2. This is further corroborated by Figure 10, which demonstrates that the LSUN pre-trained features are more effective for the downstream task than the ImageNet pre-trained features, as evidenced by the higher similarities (lighter colors) between the pre-trained and fine-tuned features along the diagonal line for LSUN compared to ImageNet. Since the images in LSUN have a smaller diversity compared to those in ImageNet, we hypothesize that pre-training on LSUN forces the network to reconstruct masked portions of similar images, thereby encouraging it to utilize more layers for higher level reasoning than for RGB reconstruction (Figure 9)."
        }
    ],
    "title": "MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth Estimation",
    "year": 2023
}