{
    "abstractText": "Motor imagery (MI) is a popular paradigm for controlling electroencephalogram (EEG) based Brain-Computer Interface (BCI) systems. Many methods have been developed to attempt to accurately classify MI-related EEG activity. Recently, the development of deep learning has begun to draw increasing attention in the BCI research community because it does not need to use sophisticated signal preprocessing and can automatically extract features. In this paper, we propose a deep learning model for use in MI-based BCI systems. Our model makes use of a convolutional neural network based on a multi-scale and channeltemporal attention module (CTAM), which called MSCTANN. The multi-scale module is able to extract a large number of features, while the attention module includes both a channel attention module and a temporal attention module, which together allow the model to focus attention on the most important features extracted from the data. The multi-scale module and the attention module are connected by a residual module, which avoids the degradation of the network. Our network model is built from these three core modules, which combine to improve the recognition ability of the network for EEG signals. Our experimental results on three datasets (BCI competition IV 2a, III IIIa and IV 1) show that our proposed method has better performance than other state-of-the-art methods, with accuracy rates of 80.6%, 83.56% and 79.84%. Our model has stable performance in decoding EEG signals and achieves efficient classification performance while using fewer network parameters than other comparable state-ofthe-art methods.",
    "authors": [],
    "id": "SP:6693bda408840b5afd9f04470e87d4c5c123b9dd",
    "references": [
        {
            "authors": [
                "J. Jin",
                "Z.Q. Wang",
                "R. Xu"
            ],
            "title": "Robust similarity measurement based on a novel time filter for SSVEPs detection",
            "venue": "IEEE Trans. Rehabil. Eng., 2021, DOI: 10.1109/ TBME.2021.3118468.",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "J. Jin",
                "S. Li"
            ],
            "title": "Evaluation of color modulation in visual P300-speller using new stimulus patterns",
            "venue": "Cognit. Neurodyn., vol. 15, pp. 873-886, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Jochumsen",
                "T.A.M. Janjua",
                "J.C. Arceo"
            ],
            "title": "Induction of Neural Plasticity Using a Low-Cost Open Source Brain-Computer Interface and a 3D-Printed Wrist Exoskeleton",
            "venue": "Sens., vol. 21, no. 2, Jan 2021, Art no. 572.",
            "year": 2021
        },
        {
            "authors": [
                "N. Kobayashi",
                "M. Nakagawa"
            ],
            "title": "BCI\u2010based control of electric wheelchair using fractal characteristics of EEG",
            "venue": "IEEJ Trans. Electr. Electron. Eng., vol. 13, no. 12, pp. 1795-1803, 2018.",
            "year": 1803
        },
        {
            "authors": [
                "A. Kreilinger",
                "H. Hiebel",
                "G.R. M\u00fcller-Putz"
            ],
            "title": "Single versus multiple events error potential detection in a BCIcontrolled car game with continuous and discrete feedback",
            "venue": "IEEE Trans. Biomed. Eng., vol. 63, no. 3, pp. 519-529, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Q. Gao",
                "X. Zhao",
                "X. Yu"
            ],
            "title": "Controlling of smart home system based on brain-computer interface",
            "venue": "Technol. Health Care, vol. 26, no. 5, pp. 769-783, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "C.N. Munyon"
            ],
            "title": "Neuroethics of non-primary braincomputer interface: focus on potential military applications",
            "venue": "Front. Neurosci., vol. 12, no. 5, pp. 696, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Liang",
                "J. Jin",
                "I. Daly"
            ],
            "title": "Novel channel selection model based on graph convolutional network for motor imagery",
            "venue": "Cognit. Neurodyn., pp. 1-14, 2022, DOI:10.1007/s11571-022-09892-1.",
            "year": 2022
        },
        {
            "authors": [
                "D. Coyle",
                "T.M. McGinnity",
                "G. Prasad"
            ],
            "title": "Improving the separability of multiple EEG features for a BCI by neural-time-series-prediction-preprocessing",
            "venue": "Biomed. Signal Process. Control , vol. 5, no. 3, pp. 196-204, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S.S. Gupta",
                "R.R. Manthalkar"
            ],
            "title": "Detection of Motor Activity in Visual Cognitive Task Using Autoregressive Modelling and Deep Recurrent Network",
            "venue": "Lect. Notes Electr. Eng., 2022, pp. 371-381.",
            "year": 2022
        },
        {
            "authors": [
                "M.Z. Al Faiz",
                "A.A. Al-Hamadani"
            ],
            "title": "Online brain computer interface based five classes EEG to control humanoid robotic hand",
            "venue": "Int. Conf. Telecommun. Signal Process., TSP, 2019: IEEE, pp. 406-410.",
            "year": 2019
        },
        {
            "authors": [
                "A.C. Subrata",
                "M.A. Riyadi",
                "T. Prakoso"
            ],
            "title": "EEGbased BMI using Multi-Class Motor Imagery for Bionic Arm",
            "venue": "MECnIT - Int. Conf. Mech., Electron., Comput., Ind. Technol., 2020: IEEE, pp. 255-260.",
            "year": 2020
        },
        {
            "authors": [
                "M.N. Alam",
                "M.I. Ibrahimy",
                "S. Motakabber"
            ],
            "title": "Feature extraction of EEG signal by power spectral density for motor imagery based BCI",
            "venue": "Proc. Int. Conf. Comput. Commun. Eng., ICCCE, 2021: IEEE, pp. 234- 237.",
            "year": 2021
        },
        {
            "authors": [
                "J. Jin",
                "T.N. Qu",
                "R. Xu"
            ],
            "title": "Motor Imagery EEG Classification Based on Riemannian Sparse Optimization and Dempster-Shafer Fusion of Multi- Time-Frequency Patterns",
            "venue": "IEEE Trans. Neural Syst. Rehabil. Eng.,2022,DOI10.1109/TNSRE.2022.3217573.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Miao",
                "J. Jin",
                "I. Daly"
            ],
            "title": "Learning common timefrequency-spatial patterns for motor imagery classification",
            "venue": "IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29, pp. 699-707, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Geng",
                "S. Xue",
                "P. Yu"
            ],
            "title": "A Fusion Algorithm for EEG Signal Processing Based on Motor Imagery Brain- Computer Interface,\"Wireless",
            "venue": "Commun. Mobile Comput.,",
            "year": 2022
        },
        {
            "authors": [
                "X. Gong",
                "S. Chen",
                "Y. Ban",
                "M. Wang"
            ],
            "title": "Feature Processing of Multi-classification Motor Imagery EEG based on improved ICA and SVM",
            "venue": "Proc. - Int. Conf. Intell. Comput. Human-Comput. Interact., ICHCI, 2021: IEEE, pp. 318-321.",
            "year": 2021
        },
        {
            "authors": [
                "X. Gao",
                "D. Xu",
                "M. Cheng"
            ],
            "title": "A BCI-based environmental controller for the motion-disabled,\"IEEE Trans",
            "venue": "Neural Syst. Rehabil. Eng., vol. 11,",
            "year": 2003
        },
        {
            "authors": [
                "Z. Wu",
                "D. Yao"
            ],
            "title": "Frequency detection with stability coefficient for steady-state visual evoked potential (SSVEP)-based BCIs,\"J",
            "venue": "Neural Eng.,",
            "year": 2007
        },
        {
            "authors": [
                "C. Brunner",
                "M. Naeem",
                "R. Leeb"
            ],
            "title": "Spatial filtering and selection of optimized components in four class motor imagery EEG data using independent components analysis",
            "venue": "Pattern Recognit. Lett., vol. 28, no. 8, pp. 957- 964, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "J. Jin",
                "R. Xiao",
                "I. Daly"
            ],
            "title": "Internal feature selection method of CSP based on L1-norm and Dempster\u2013Shafer theory",
            "venue": "IEEE Trans. Neural Networks Learn. Syst., vol. 32, no. 11, pp. 4814-4825, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Du",
                "R. Xu",
                "J. Zhang"
            ],
            "title": "Motor Imagery Analysis Based on Filter Bank Common Spatial Pattern",
            "venue": "Proc. - Int. Conf. Artif. Intell. Comput. Eng., ICAICE, 2021: IEEE, pp. 660-665.",
            "year": 2021
        },
        {
            "authors": [
                "R. Chatterjee",
                "T. Bandyopadhyay"
            ],
            "title": "EEG based motor imagery classification using SVM and MLP",
            "venue": "Proc. Int. Conf. Comput. Intell. Netw. CINE, 2016: IEEE, pp. 84-89.",
            "year": 2016
        },
        {
            "authors": [
                "S.K. Mandal",
                "M. Naskar"
            ],
            "title": "Meta heuristic assisted automated channel selection model for motor imagery brain-computer interface",
            "venue": "Multimedia Tools Appl., vol. 81, no. 12, pp. 17111-17130, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.K. Mandal",
                "M.N.B. Naskar"
            ],
            "title": "Algorithmic Analysis on Automated Channel Selection Framework for Motor Imagery BCI",
            "venue": "Proc. Int. Conf. Trends Electron. Informatics, ICOEI, 2021: IEEE, pp. 32-39.",
            "year": 2021
        },
        {
            "authors": [
                "A. Ramadhani",
                "H. Fauzi",
                "I. Wijayanto"
            ],
            "title": "The Implementation of EEG Transfer Learning Method Using Integrated Selection for Motor Imagery Signal",
            "venue": "Lect. Notes Electr. Eng., pp. 457-466."
        },
        {
            "authors": [
                "M. Rashid",
                "B.S. Bari",
                "M.J. Hasan"
            ],
            "title": "The classification of motor imagery response: an accuracy enhancement through the ensemble of random subspace k-NN,\"PeerJ",
            "venue": "Comput. Sci.,",
            "year": 2021
        },
        {
            "authors": [
                "B. Ta\u015far",
                "O. Yaman"
            ],
            "title": "EEG Signals Based Motor Imagery and Movement Classification for BCI Applications",
            "venue": "Int. Conf. Decis. Aid Sci. Appl., DASA, 2022: IEEE, pp. 1425-1429.",
            "year": 2022
        },
        {
            "authors": [
                "C. Uyanik",
                "M.A. Khan",
                "I.C. Brunner"
            ],
            "title": "Machine Learning for Motor Imagery Wrist Dorsiflexion Prediction in Brain-Computer Interface Assisted Stroke Rehabilitation",
            "venue": "Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS, 2022: IEEE, pp. 715-719.",
            "year": 2022
        },
        {
            "authors": [
                "H. Altaheri",
                "G. Muhammad",
                "M. Alsulaiman"
            ],
            "title": "Deep learning techniques for classification of electroencephalogram (EEG) motor imagery (MI) signals: a review,\"Neural",
            "venue": "Comput. Appl.,",
            "year": 2021
        },
        {
            "authors": [
                "W.Q. Liu",
                "Y. Zeng"
            ],
            "title": "Motor Imagery Tasks EEG Signals Classification Using ResNet with Multi-Time- Frequency Representation",
            "venue": "Int. Conf. Intell. Comput. Signal Process., ICSP, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wang",
                "L. Wang",
                "S. Xu"
            ],
            "title": "A Novel Motor Imagery EEG Classification Approach Based on Time-Frequency Analysis and Convolutional Neural Network",
            "venue": "J. Neural Eng., pp. 329-346, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.F. Hwaidi",
                "T.M. Chen"
            ],
            "title": "Classification of Motor Imagery EEG Signals",
            "venue": "Based on Deep Autoencoder and Convolutional Neural Network Approach,\"IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "C. Shalu",
                "T. Sachin",
                "B. Varun"
            ],
            "title": "Convolutional neural network based approach towards motor imagery tasks EEG signals classification",
            "venue": "IEEE Sens. J., vol. 19, no. 12, pp. 4494-4500, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhu",
                "P. Li",
                "C. Li"
            ],
            "title": "Separated channel convolutional neural network to realize the training free motor imagery BCI systems",
            "venue": "Biomed. Signal Process. Control, vol. 49, pp. 396-403, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Wu",
                "Y. Niu",
                "F. Li"
            ],
            "title": "A parallel multi-scale filter bank convolutional neural networks for motor imagery EEG classification,\"Front",
            "venue": "Neurosci., vol. 13,",
            "year": 2019
        },
        {
            "authors": [
                "S. Roy",
                "K. McCreadie",
                "G. Prasad"
            ],
            "title": "Can a single model deep learning approach enhance classification accuracy of an eeg-based brain-computer interface",
            "venue": "IEEE Syst. Man Cybern. Mag., 2019: IEEE, pp. 1317- 1321.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "X.-R. Zhang",
                "B. Zhang"
            ],
            "title": "A channelprojection mixed-scale convolutional neural network for motor imagery EEG decoding,\"IEEE",
            "venue": "Trans. Neural Syst. Rehabil. Eng., vol. 27,",
            "year": 2019
        },
        {
            "authors": [
                "J.Z. Liu",
                "F.F. Ye",
                "H. Xiong"
            ],
            "title": "Convolutional neural network-based EEG signal recognition for multi-class motor imagery",
            "venue": "J. Zhejiang Univ.-Sci. A, vol. 55, no. 11, pp. 2054-2066, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z.Y. Jia",
                "Y.F. Lin",
                "T.H. Liu"
            ],
            "title": "A method for motor imagery classification based on multi-scale feature extraction and Squeeze-and-Excitation model",
            "venue": "J Comput. Res. Dev., vol. 57, no. 12, pp. 2481-2489, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Woo",
                "J. Park",
                "J.-Y. Lee",
                "I.S. Kweon"
            ],
            "title": "CBAM: Convolutional block attention module",
            "venue": "Lect. Notes Comput. Sci., 2018, pp. 3-19.",
            "year": 2018
        },
        {
            "authors": [
                "M. Tangermann",
                "K-R. M\u00fcller",
                "A. Aertsen"
            ],
            "title": "Review of the BCI competition IV",
            "venue": "Front. Neurosci., pp. 55, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "B. Blankertz",
                "K-R. Muller",
                "D.J. Krusienski"
            ],
            "title": "The BCI competition III: Validating alternative approaches to actual BCI problems.",
            "venue": "IEEE Trans. Neural Syst. Rehabil. Eng., vol. 14,",
            "year": 2006
        },
        {
            "authors": [
                "H. Bashashati",
                "R.K. Ward",
                "A. Bashashati"
            ],
            "title": "Usercustomized brain-computer interfaces using Bayesian optimization,\"J",
            "venue": "Neural Eng., vol. 13,",
            "year": 2016
        },
        {
            "authors": [
                "V.J. Lawhern",
                "A.J. Solon",
                "N.R. Waytowich"
            ],
            "title": "EEGNet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces,\"J",
            "venue": "Neural Eng., vol. 15,",
            "year": 2018
        },
        {
            "authors": [
                "X. Deng",
                "B. Zhang",
                "N. Yu",
                "K. Liu",
                "K. Sun"
            ],
            "title": "Advanced TSGL-EEGNet for motor imagery EEGbased brain-computer interfaces",
            "venue": "IEEE Access, vol. 9, pp. 25118-25130, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Mane",
                "N. Robinson",
                "A.P. Vinod"
            ],
            "title": "A Multi-view CNN with Novel Variance Layer for Motor Imagery Brain Computer Interface,'",
            "venue": "Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS,",
            "year": 2021
        },
        {
            "authors": [
                "G.A. Altuwaijri",
                "G. Muhammad",
                "H. Altaheri"
            ],
            "title": "A Multi-Branch Convolutional Neural Network with Squeeze-and-Excitation Attention Blocks for EEG- Based Motor Imagery Signals Classification",
            "venue": "Diagn., vol. 12, no. 4, pp.995, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.M. Ingolfsson",
                "M. Hersche",
                "X. Wang"
            ],
            "title": "EEG- TCNet: An Accurate Temporal Convolutional Network for Embedded Motor-Imagery Brain\u2013Machine Interfaces",
            "venue": "IEEE Syst. Man Cybern. Mag., 2010: IEEE, pp. 2958-2965. This article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3294815 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n1\n Abstract\u2014Motor imagery (MI) is a popular paradigm for controlling electroencephalogram (EEG) based Brain-Computer Interface (BCI) systems. Many methods have been developed to attempt to accurately classify MI-related EEG activity. Recently, the development of deep learning has begun to draw increasing attention in the BCI research community because it does not need to use sophisticated signal preprocessing and can automatically extract features. In this paper, we propose a deep learning model for use in MI-based BCI systems. Our model makes use of a convolutional neural network based on a multi-scale and channeltemporal attention module (CTAM), which called MSCTANN. The multi-scale module is able to extract a large number of features, while the attention module includes both a channel attention module and a temporal attention module, which together allow the model to focus attention on the most important features extracted from the data. The multi-scale module and the attention module are connected by a residual module, which avoids the degradation of the network. Our network model is built from these three core modules, which combine to improve the recognition ability of the network for EEG signals. Our experimental results on three datasets (BCI competition IV 2a, III IIIa and IV 1) show that our proposed method has better performance than other state-of-the-art methods, with accuracy rates of 80.6%, 83.56% and 79.84%. Our model has stable performance in decoding EEG signals and achieves efficient classification performance while using fewer network parameters than other comparable state-ofthe-art methods.\nIndex Terms\u2014Motor imagery; EEG; Multi-scale convolution; Convolution neural network; Attention module.\nThis work was supported by STI 2030-major projects 2022ZD0208900 and the Grant National Natural Science Foundation of China under Grant 62176090; in part by Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX, in part by the Program of Introducing Talents of Discipline to Universities through the 111 Project under Grant B17017; in part by the ShuGuang Project supported by the Shanghai Municipal Education Commission and the Shanghai Education Development Foundation under Grant 19SG25; in part by the Polish National Science Center under Grant UMO-2016/20/W/NZ4/00354. This research is also supported by National Government GuidedSpecial Funds for Local Science and Technology Development (Shenzhen, China) (No. 2021Szvup043) and by Project of Jiangsu Province Science and Technology Plan Special Fund in 2022 (Key research and development plan industry foresight and key core technologies) under Grant BE2022064-1. (Corresponding author: Jing Jin.)\nI. INTRODUCTION\nrain-Computer Interface (BCI) uses artificial intelligence to decode signals from the brain in order to provide a communication pathway between the\nbrain and the world [1]. The original purpose of BCI technology is to identify the intention of human activities by analyzing EEG signals and converting them into commands to control external auxiliary devices, to assist people with motor disabilities to interact with the external environment [2]. As a result of continuous development of research, BCI technology is gradually being applied to increasing numbers of fields such as the medical industry [3, 4], entertainment [5], smart homes [6], and the military [7]. Commonly used BCI paradigms include steady-state visual evoked potentials (SSVEPs), P300 potentials, and motor imagery. Among these paradigms, motor imagery requires no additional stimulation apparatus, instead users modulate their EEG simply through the imagination of movement. In contrast to other BCI systems, the motor imagery paradigm can directly map a user\u2019s movement intention to an action. This allows participants to complete specific tasks by imagining limb movements. Furthermore, BCIs based on motor imagery can be spontaneous. In other words, participants can generate EEG signals through motor imagery without the need for external cues or other stimuli. Consequently, motor imagery BCIs have become one of the most popular paradigms.\nWhen imagining movement, the activity of specific frequency bands within the brain changes. Specifically, activity in the mu band, from 8-12 Hz, and the beta band, from 13-30\nRunze Wu, Jing Jin, and Xingyu Wang are with the Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, Shanghai 200237, P.R. China (e-mail: epoch_666@163.com; jinjingat@gmail.com; xywang@ecust.edu.cn). Jing Jin is also with Shenzhen Research Institute of East China University of Science and Technology, Shenzhen, 518063, PR China.\nIan Daly is with the Brain-Computer Interfacing and Neural Engineering Laboratory, School of Computer Science and Electronic Engineering, University of Essex, Wivenhoe Park, Colchester, Essex CO4 3SQ, UK(e-mail: i.daly@essex.ac.uk).\nAndrzej Cichockia is with the Laboratory for Advanced Brain Signal Processing, RIKEN Brain Science Institute, Japan and the Systems Research Institute of Polish Academy of Science, 01-447 Warsaw, Poland, Department of Informatics, Nicolaus Copernicus University, 87-100 Torun, Poland (e-mail: a.cichocki@riken.jp).\nClassification of motor imagery based on multi-scale feature extraction and the channel-\ntemporal attention module Runze Wu, Jing Jin* Senior Member, IEEE, Ian Daly, Xingyu Wang, and Andrzej Cichocki, Fellow,\nIEEE\nB\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n2\nHz, are known to change during motor imagery. While performing an imagined task, such as the right-hand movement, the contralateral hemisphere of the brain exhibits a phenomenon of reduced low-frequency activity, termed eventrelated desynchronization (ERD), while the ipsilateral hemisphere of the brain produces a phenomenon of increased activity, termed the event-related synchronization (ERS) [8].\nTo accurately recognize the ERD/S, considerable research efforts have focused on combinations of feature extraction methods combined with machine learning to complete the classification task. Within the feature extraction step a form of transformation method (usually a linear transformation) is employed to identify and extract some important features from the EEG signals. The important feature information is retained and the influence of noisy additional information is reduced or removed, thus transforming the originally complex highdimensional EEG signals into a lower-dimensional less noisy domain [9]. Feature extraction is usually conducted from four perspectives: the time domain [10,11], the frequency domain [12,13], the time-frequency domain [14,15], and the spatial domain [16,17]. Typical feature extraction methods include, but are not limited to, the Fast Fourier Transform (FFT) [18], the wavelet transform (WT) [19], principal component analysis (PCA) [20], and common spatial patterns (CSP) [21]. Among these methods, the CSP algorithm is the most widely used in BCI systems. Its basic principle is to identify a spatial filter that maximizes the variance between two categories. Recently, the basic CSP algorithm has been extended in a number of ways to meet particular challenges within the BCI domain. Among these extensions, the filter bank common space mode (FBCSP) [22] is one promising method that uses the frequency domain characteristics of MI to optimize the spatial filter. Specifically, within the FBCSP algorithm, the MI signal is divided into multiple frequency sub-bands and then each sub-band is filtered by CSP to extract an optimal feature set.\nThe feature extraction step is typically followed by feature classification. The classification task makes use of machine learning methods to identify user intention from the extracted features. Common machine learning methods applied within the BCI field include, but are not limited to, support vector machines (SVMs) [23,24], linear discriminant analysis classifiers (LDAs) [25,26], K-nearest neighbours classifiers (KNNs) [27,28], and naive Bayes classifiers (NBs) [29,30].\nHowever, traditional machine learning methods work most effectively only when the features they are applied to have been carefully chosen and pre-processed to maximize the signal-tonoise ratio. An alternative approach, that many investigators have recently begun to focus on, is deep learning. Deep learning models can effectively capture a high-dimensional feature representation from the EEG signals as well as the potential relationships between internal features through a nonlinear deep structure. For EEG signals with complex information content and strong time-varying characteristics, a deep feature representation can be extracted through deep learning models. Deep learning models do not require complex processing of the input data, and some models can even directly use the original data as their input without any pre-processing or feature\nextraction [31]. Many different deep learning models have been developed over recent years to classify EEG data. For example, Liu [32] et al. proposed a multi-feature fusion method based on ResNet to extract features. This model classified EEG with accuracies which were 39.65% higher than those achieved by a model using single features. For feature extraction and classification of MI-EEG signals, Wang [33] et al. combined a Squeeze-and-Excitation convolution neural network (SECNN) with the time-varying autoregressive model (TVAR) and power spectral density (PSD) time-frequency analysis, to improve the accuracy of MI-BCI systems. Hwaidi [34] et al. trained deep neural networks by combining a deep autoencoder (DAE) and CNN architectures to classify EEG MI signals. The results of the model show that it outperforms current CNN-based approaches and several traditional machine-learning approaches.\nDeep learning does not require complex feature extraction methods, but simple data preprocessing steps have been shown to further improve their classification results. For example, Shalu [35] et al. achieved relatively good results by transforming continuous wavelet transforms into timefrequency plots and feeding them into a deep CNN for classification. Zhu [36] et al. designed a separate channel convolution network to encode the multi-channel data of CSP, preserving the time-varying information that is helpful for distinguishing tasks. However, some studies have proved that deep learning can even directly extract features from raw EEG data. For example, Wu [37] et al. proposed a parallel multi-scale filter bank CNN for MI classification. The extracted output features are connected to the spatial convolution layer to complete the fusion of multimodal features from EEG signals. Roy [38] et al. explored the use of different fusion models to automatically complete multimodal feature extraction and classification from raw EEG signals. The model achieves 80.32% accuracy on the BCI competition IV 2b dataset. Li [39] et al. used amplitude interference as a means of data enhancement to expand the dataset and constructed a channel projection mixedscale CNN to decode EEG. This framework used the original multi-channel EEG signal as the input, and achieved an average accuracy of 67.17% in a four-class classification task.\nDeep learning has considerable potential to improve the performance of MI-EEG BCIs, but some problems still remain: 1) Most methods developed to date are intended for binary classification tasks, or if they are applied to multi-class problems, convert the multi-class problem into multiple binary tasks. However, there is a growing need for effective multiclass solutions for MI BCIs. 2) The process of feature extraction and selection is very time-consuming. Ideally, we would like to take full use of the advantages of automatic learning of deep neural networks. To do this we need to extract more effective features by either adopting data enhancement methods or by designing more complex network models. However, complex networks will increase the run time and the number of parameters that need to be trained in the network. 3) There are individual differences in the EEG signals of different participants but the single-scale convolution kernel can only use a single set of weights when extracting features. 4) Although\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n3\nthe neural network can automatically extract features, the extracted features are not necessarily effective in all cases. Extracting features without emphasis will not only increase the calculation cost but also lead to feature redundancy.\nThe literatures on MI mentioned above are all based on the use of 2D inputs to the network, while there are far fewer studies on the use of 1D inputs. Liu [40] et al. compared the classification results of both one-dimensional (1D) and twodimensional (2D) input forms based on public datasets and the results indicate that the 1D form of input can lead to higher classification accuracies and converge faster. Jia [41] et al. used a 1D input, but the convolution was performed in the time dimension and finally reached an average classification accuracy rate of 78% on the four classified published datasets, demonstrating that a 1D input can also be helpful for MI classification tasks.\nTo tackle the problems listed above, a deep learning-based multi-class MI signal recognition method is proposed in this paper. This model utilizes preprocessed EEG signals to realize end-to-end automatic learning without the need for manually designed feature extraction methods. The main contributions of this study are as follows:\n1) To investigate multi-class tasks, this paper carries out experiments on two BCI competition datasets, each containing four-classes of movement tasks. To demonstrate the performance of the model we add a dataset with 2-categories of MI tasks.\n2) To improve the efficiency of feature extraction, this paper proposes an end-to-end neural network and uses the proposed data augmentation to enrich the feature information.\n3) In view of the differences in EEG signals recorded from different participants, a multi-scale module is designed to extract richer features, which increases the range of the network to learn features and improves the classification accuracy.\n4) To address the problem that the neural network may learn features that are not focused, the information over different channels and over time is learned through two modules, a channel attention module and a temporal attention module, to attempt to improve the classification result.\nThe rest of the paper is organized as follows: the details of the methods are described in Section II. The experiments and results are presented in Section III. The factors influencing the experimental and future work are discussed in Section IV. Finally, we conclude our research in Section V."
        },
        {
            "heading": "II. METHODS",
            "text": "The model proposed in this paper is a neural network with a multi-scale module and two attention modules. The overall framework of the MSCTANN model is shown in Figure 1. The model includes three core parts: the multi-scale module, the residual module and the channel-temporal attention module (CTAM). Augmentation of the training data is used to provide more information for the neural network. Multi-scale modules then automatically extract features from this augmented data and different extraction levels solve the problem of differences in EEG signals among participants. The residual module is used to fuse the features transmitted by the multi-scale module, and the introduction of the residual module avoids network degradation caused by an excessive number of network layers. The CTAM is used to automatically select the fused features, effectively avoiding information redundancy, and automatically learning the importance of different features, thus improving the classification result for MI-EEG signals."
        },
        {
            "heading": "A. Data augmentation",
            "text": "For neural networks, the data that need to be used for training needs to be sufficiently larger. However, most EEG datasets cannot satisfy this requirement to support training the network. Therefore, data enhancement is necessary. Each sample of EEG\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n4\ndata based on MI in this paper is represented as a 2D matrix of dimensions \ud835\udc36 \u00d7 \ud835\udc47 (channel \u00d7 time), where rows represent data collected from different electrodes and columns represent data at different sample points. In this paper, a head-to-tail extended data augmentation method is proposed. The schematic diagram of the head-to-tail augmentation method is shown in Figure 1(a). For each trial, the signal head is extracted at a certain length and filled into the tail. This extraction process is continuously cycled until the cycle of the whole signal is completed. The length of the loop is an adjustable parameter and one of the influential factors affecting the final classification outcome. If the length of the loop is too long, the network cannot obtain enough information to overcome the over-fitting problem. Conversely, the difference between different samples will be very small."
        },
        {
            "heading": "B. The multi-scale module",
            "text": "There are several challenges still to be overcome when designing MI-BCIs. One of the most important of these challenges is the difference in EEG signals between different participants. As a result of this, if a single extraction method is used, it will not only limit the extraction of each participant's information but also ignore the individual differences between different participants, resulting in a poor final classification result. How to use the information provided to extract more features is a problem that still needs a solution. Therefore, this paper designs a multi-scale structure, which automatically extracts features from the original EEG signals based on multiscale convolution and pooling. Its structure is shown in Figure 1(b).\nThe multi-scale structure proposed in this paper is designed according to related methods in the field of signal processing. Conv1 is a convolutional layer with a smaller kernel, which can effectively collect fine-grained local information. Conv2 is a convolutional layer with a medium kernel, which can retain relatively coarse-grained feature information. Conv3 is a convolutional layer with a larger kernel, which can capture the overall characteristics of the EEG signals. Three different sizes of convolutional kernels can extract more adequate features from a multi-scale perspective by the multi-scale structure. In order to extract features it is necessary to reduce the matrix parameters and feature dimensions through the pooling layer, thereby reducing the number of parameters in the last fully connected layer. The incorporation of the pooling layer can also\nspeed up calculations and prevent overfitting effects. Most of the existing studies used a single-scale pooling layer, which increased the possibility of information loss to some extent. Although this method can remove redundant information, the criteria for information redundancy are not fixed for different participants. If only a single scale is used for extraction and processing, it greatly increases the possibility of loss of important information. Therefore, our method added multiscale pooling to multi-scale convolution, and two multi-scale structures were combined to better process the MI-EEG signals."
        },
        {
            "heading": "C. The residual module",
            "text": "The residual module can fuse the extracted features. Furthermore, the introduction of this module avoids the problem of network degradation produced by an excessive number of network layers. The structure of the residual module is shown in Figure 1(c). The connection line on the right side of the module is called the identity shortcut connection, which adds neither extra parameters nor computational complexity. The identity shortcut connection can solve the problem that the newly added layer does not work effectively by allowing the module to skip one or more layers if needed. This module is formed by combining multiple 1D convolutional layers and batch normalization (BN) layers with the superimposed residual connection. The definition of this module can be expressed as:\n\ud835\udc4c = \ud835\udc4b + \ud835\udc4b 1 where \ud835\udc4b and \ud835\udc4b represent the input and the output of the residual module, and \ud835\udc4c represents the total output. The features learned by the shallow network can be passed to the deep network by the residual connectivity module, thus avoiding network degradation."
        },
        {
            "heading": "D. Channel-temporal attention module (CTAM)",
            "text": "Convolutional block attention module (CBAM) is a lightweight attention module that can conduct attention training in both channel and spatial dimensions [42]. Inspired by the CBAM module, this article constructs a channel-temporal attention module, called CTAM. The overall structure of our CTAM module is shown in Figure 1(d), and the specific structure is shown in Figure 2. The CTAM module includes a channel attention module and a temporal attention module, which complement both channel attention and temporal attention, achieving considerable performance improvement\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n5\nwhile keeping the computational overhead small. Further directed screening of features can be performed to automatically learn more important features, thus achieving the goal of boosting the MI-EEG signal classification performance.\n1) Channel Attention Module (CAM): The channel attention module compresses the temporal dimension without changing the channel dimension. This module focuses on useful information of the target. The specific flow of the channel attention module is shown in Figure 2(a). It utilizes two parallel max pooling layers and average pooling layers for feature selection and compression. Then the number of channels is compressed to 1 channel per reduction fold of the original by sharing the MLP module. After the Relu activation function gets the result of two activations, the channel number is expanded to the original number. These two output results are added element by element to get the output result of the channel attention via a sigmoid activation function. The formula for channel attention is as follows:\n\ud835\udc40 (\ud835\udc4c) = \ud835\udf0e \ud835\udc40\ud835\udc3f\ud835\udc43 \ud835\udc34\ud835\udc63\ud835\udc54\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc4c) + \ud835\udc40\ud835\udc3f\ud835\udc43 \ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc4c)\n= \ud835\udf0e \ud835\udc4a \ud835\udc4a \ud835\udc4c + \ud835\udc4a \ud835\udc4a (\ud835\udc4c ) 2\nwhere \ud835\udc40 (\ud835\udc4c) represents the convolution result of the CAM, \ud835\udf0e represents the activation function. \ud835\udc4a and \ud835\udc4a represent the weights of the MLP and \ud835\udc4c and \ud835\udc4c represent features output by different pooling layers under the channel attention module.\n2) Temporal Attention Module (TAM): TAM compresses the channel dimension without changing the temporal dimension. This module focuses on the location information of the target. The specific flow of the temporal attention module is shown in Figure 2(b). The TAM produces two feature maps by maximum pooling and average pooling of the output from the channel attention module. Then the two feature maps are combined and turned into a single-channel feature map by the convolution operation. The feature map of temporal attention is obtained through the sigmoid function. Finally, the output is multiplied by the original map. The formula for temporal attention is as follows:\n\ud835\udc40 (\ud835\udc4c) = \ud835\udf0e \ud835\udc53([\ud835\udc34\ud835\udc63\ud835\udc54\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc4c); \ud835\udc40\ud835\udc4e\ud835\udc65\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc4c)])\n= \ud835\udf0e \ud835\udc53 \ud835\udc4c ; \ud835\udc4c 3\nwhere \ud835\udc40 (\ud835\udc4c) represents the convolution result of the TAM, \ud835\udc53 represents the active convolutions, and \ud835\udc4c and \ud835\udc4c represent the output features of the different pooling layers under the temporal attention module.\n3) Combined channel and temporal attention module: Attention modules can increase the representativeness of the network by focusing on important features, and suppressing unnecessary ones. The CTAM module emphasizes temporal information while simultaneously reinforcing channel information.\nThe CTAM module consists of the CAM and TAM modules. Through the CAM module, the input feature \ud835\udc4c multiplies the result by the original input, and the result \ud835\udc4c is the input of the TAM module. Finally, the output result of the TAM module is multiplied with \ud835\udc4c :\n\ud835\udc4c = \ud835\udc40 (\ud835\udc4c) \u2297 \ud835\udc4c\n\ud835\udc4c = \ud835\udc40 (\ud835\udc4c\u2032) \u2297 \ud835\udc4c 4\nwhere \ud835\udc4c is the original input feature, \ud835\udc4c is the result of multiplying the CAM convolution output with the original map, and \ud835\udc4c is the result after multiplying the TAM convolution output with 'Y . This is also the CTAM final output result."
        },
        {
            "heading": "III. EXPERIMENTS AND RESULTS",
            "text": ""
        },
        {
            "heading": "A. EEG Data",
            "text": "This paper uses three well-regarded datasets in the field of MI classification: Dataset 1: BCI competition IV 2a dataset [43], Dataset 2: BCI competition III IIIa dataset [44] and Dataset 3: BCI competition IV 1 dataset [43].\nDataset 1: The BCI competition IV 2a dataset contains data from nine participants performing four classes of MI tasks (involving the left hand, right hand, foot, and tongue). This dataset records EEG signals from an EEG setup placed according to the international 10-20 system with 25 electrodes (22 EEG channels and 3 EOG channels). The sampling frequency is 250 Hz and a 0.5-100 Hz band-pass filter and 50 Hz power frequency notch filter are used for filtering. Each participant completed two sessions, each of which contained six runs with 48 trials per run, while one session contained 288 trials.\nThe timing pattern of Dataset 1 is shown in Figure 3(a). In the experiment, the beginning of the trial is a fixation cross for the first 1s. Then a cue of direction shows for 1.25s. At t=3s, participants are asked to imagine the corresponding movement until they finished the task at t=6s. The acquisition of this dataset uses a feedback-free experimental paradigm, intercepting the time of the signal from 0.5s after the start of the cue to the end of MI, that is, from 2.5-6s, with a total intercept of 3.5s. Since MI is most commonly associated with changes in\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n6\nthe \u0251 (8-12 Hz) and \u03b2 (13-30 Hz) frequency bands the data is band-pass filtered at 8-30 Hz using a Butterworth filter. Dataset 2: The BCI competition III IIIa dataset contains data from 3 participants performing a four-category MI task. The task type is the same as used in Dataset 1. This dataset includes EEG signals recorded with 60 electrodes. The sampling frequency used was 250 Hz. Participant K3 in this dataset completed 360 MI trials while the other two participants each completed 240 trials. The sample data is equal for each category.\nThe timing paradigm used to record Dataset 2 is shown in Figure 3(b). In the experiment, the beginning of the trial is a black screen for the first 2s. Then a fixation cross \u201c+\u201d is displayed for 1s. A directional arrow is then displayed for 1s. At the same time, the participant is asked to imagine the corresponding movement until the fixation cross disappears at t=7s. Each of the 4 cues is displayed 10 times within each run in a randomized order. The trials for Dataset 2 are extracted from 3.5-6.5s, and the filter settings used remain consistent with Dataset 1.\nDataset 3: The BCI competition IV 1 dataset contains data from 7 participants performing a 2-category MI task. This dataset includes EEG signals recorded with 59 electrodes. The sampling frequency used was 100 Hz. The data from participants labeled c, d, and e from this dataset were not used, because they are artificially generated. Each participant was asked to complete 200 trials.\nThe timing paradigm used to record Dataset 3 is shown in Figure 3(c). In the experiment, the beginning of the trial is a fixation cross for the first 2s. Then a directional arrow is displayed for 4s. At the same time, the participant is asked to imagine the corresponding movement until the cross disappears at t=6s. The trials for Dataset 3 are extracted from 2.5-5.5s, and the filter settings used remain consistent with Dataset 1."
        },
        {
            "heading": "B. Experimental setup",
            "text": "This experiment adopts within-subject classification. We employ a fivefold cross-validation approach to perform network training by optimizing the cross-entropy loss function, with the number of training iterations set to 50. The neural network is trained using the Adam optimizer, which updates the network weights more efficiently than the classical random gradient descent method. Additionally, it also accelerates the convergence of the neural network. The initial learning rate of the network is set to 1\u00d710-3 and then adjusted through a cosine annealing attenuation strategy, which means that the learning\nrate will be readjusted and restored after decay to a certain value, jumping out of the current local optimal solution and searching for the global optimal solution again. To prevent overfitting problems, a dropout rate of 0.4 is set in the final fully connected layer. More network parameter settings are detailed in Table 1 (note, the term N in the table denotes the batch size)."
        },
        {
            "heading": "C. Overall Comparison",
            "text": "The model presented in this paper was compared with several classical and state-of-the-art models on the BCI IV 2a, BCI III IIIa and BCI IV 1 datasets. Table 2 compares the effect of our proposed method with other state-of-the-art methods on Dataset 1. The numbers highlighted in bold in the table indicate the participants' best outcomes.\nWe compared our model to the following methods: 1. FBCSP [45]: A model that manually extracts features. This model is often used as a baseline method to classify MI-EEG signals. It has yielded good results in several previous EEG decoding studies. It performs task classification by extracting CSP features from different frequency bands and then using the SVM model to classify the features.\n2. EEGNet [46]: A deep learning model that uses 2D temporal convolution, deep convolution, and separable convolution to accomplish classification.\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n7\n3. DeepConvNet [47]: A deep learning model that is deeper than the ShallowConvNet. It consists of four convolutional and Max pooling layer blocks, followed by a soft Max layer.\n4. FBCNet [48]: A deep learning model using EEG bandpass filtering to create multi-frequency bands. It consists of two trainable layers.\n5. MBEEGSE [49]: A deep learning model for decoding MI known as a multi-branch EEGNet with squeeze-and-excitation blocks.\n6. EEG - TCNet [50]: A deep learning model with temporal convolutional network. It involves dilated convolution and residual modules.\nAs shown in Table 2, our proposed MSCTANN model can achieve an average recognition accuracy of 80.6%. Compared to other methods, our proposed MSCTANN method achieves a statistically significantly higher mean classification accuracy over participants. In terms of recognition accuracy, MSCTANN achieves an accuracy that is, on average, 10.92% higher than FBCSP, which proves that our model can extract more effective information than FBCSP. The five other deep-learning models, EEGNet, DeepConvNet, FBCNet, MBEEGSE and EEGTCNet are generally more effective than FBCSP, which demonstrates the advantages of deep learning. However, our proposed MSCTANN method is, on average, 8.16%, 8.58%, 7.4%, 2.71% and 3.98% more accurate than these five models, which demonstrates the effectiveness of our proposed model\ndesign. The structure of the deep learning model is very important for feature extraction, and selecting an appropriate model can result in better classification performance. The reason our proposed MSCTANN model performs better than other state-of-the-art methods may be due to the use of multiscale design elements and the role of the attention module. Specifically, our proposed MSCTANN model incorporates both a multi-scale model and attention modules, which makes feature extraction and screening more reasonable and leads to improved classification performance.\nFigure 4 shows the results of our proposed MSCTANN model and the other state-of-the-art models on Dataset 2 and Dataset 3. In Dataset 2, our MSCTANN model achieves the highest accuracy, 83.56%. The performance of our MSCTANN model is 16.2% higher than FBCSP. When comparing the deep learning models, our MSCTANN model achieves an average accuracy that is 7.87%, 11.62%, 4.9%, 1.1% and 3.23% higher than those five models, indicating that targeted extraction of features can improve model performance. In Dataset 3, our MSCTANN model achieves the highest accuracy, 79.88%. The performance of our MSCTANN model is 17.13% higher than FBCSP. Compared to the five other deep learning models, our model produces a performance which is 8.75%, 13.88%, 6.13%, 0.88% and 2%, better respectively. When considering all three datasets, our model has a more stable performance than the other methods we compare against. In addition, the single training time for the three datasets is 64.98s, 65.8s, and 25.61s, respectively."
        },
        {
            "heading": "IV. DISCUSSION",
            "text": "The performance of our proposed MSCTANN model is affected by several factors: 1) To demonstrate the advantages of multi-scale kernels, ablation experiments corresponding to single scale kernels are conducted. 2) In the multi-scale module, the different sizes of the convolution kernels will affect the content of the information in the extracted features. 3) The validity of the CTAM layer for feature learning and selection. 4) The abundance of features as a result of the data augmentation and feature augmentation methods.\nA. Influence of multi-scale kernel\nMulti-scale kernels can extract features at different scales at the same time. To highlight the advantages of multi-scale convolution kernels, we perform corresponding ablation experiments on the optimal combination of multi-scale kernels for each dataset. The results of the three datasets are shown in figures 5(a), (b) and (c), respectively. We use a radar chart to display the results. As can be seen from the figure, the single scale kernels of almost all participants are not as performant as multi-scale kernels. In dataset 1, compared to single scale kernels, the accuracy of multi-scale kernels for all participants improved by 5.57%, 5.27%, and 3.05%, respectively. In dataset 2, the accuracy improved by 2.82%, 6.94%, and 8.19%, respectively. In dataset 3, the accuracy improved by 4.25%, 2.13%, and 3.52%, respectively. This result also verifies that multi-scale kernel can extract more information than single\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n8\nscale kernel. In addition, we added a significance test for the single scale kernel (in Figure 5(d)). From a statistical point of view, the single scale kernels and multi-scale kernels have significant differences in results."
        },
        {
            "heading": "B. Multi-scale kernel size",
            "text": "In this section, we use different combinations of convolution kernel sizes to explore the effect of kernel size on model performance. Due to the different number of channels that are available in the two datasets, kernel combinations need to be considered separately for each of the three datasets. The results for each dataset are shown in Figure 6. Due to the close number of EEG channels in dataset 2 and dataset 3, the same kernel combination is adopted for both these datasets. It can be seen that the most suitable kernel combination for Dataset 1 (in Figure 6(a)) is (3, 11, 19), the most suitable kernel combination for Dataset 2 (in Figure 6(b)) is (7, 21, 35) and the most suitable kernel combination for Dataset 3 (in Figure 6(c)) is (3, 11, 19). For the three datasets, the accuracy of the best kernel combination was 5.95%, 5.32%, and 3.04% higher than that of the worst kernel combination, respectively. From the experimental results achieved with the three datasets, it is not difficult to see that the performance difference between different kernel combinations is still relatively large. In this article, we were only able to consider a limited number of combinations, so there may still be better kernel combinations that we have not yet discovered. Indeed, from our current results, it is not yet possible to analyze helpful optimization rules and this, and related aspects, need to be further explored in the future.\nC. Influence of the CTAM\nTo explore the effects of the CTAM layer on the classification results, we perform ablation experiments. As shown in Figure 7. The experimental results on all datasets illustrate that the classification accuracy appears to be reduced by different amounts for each participant without the CTAM layer. If the participants with high and low accuracy rates are divided by 78.89% (the average accuracy of all subjects), the results show that the effect of improvement of low accuracy rate participants is more obvious, with an average increase in performance of 3.45%, and the best improvement effect reaching 5.95%. Thus, the validity of the CTAM layer in our proposed model is demonstrated. The features extracted by the original EEG signals through the multi-scale module and the residual module are different. The CTAM layer can automatically learn the importance of different features, and then improve the classification result for the MI tasks.\nD. Influence of data augmentation\nThe purpose of data augmentation is to optimize the training process by overcoming the problem of insufficient training data. To demonstrate the validity of our head-to-tail data augmentation method, the experiments are repeated with a dataset that does not use data augmentation. Figure 8 shows the test results achieved with this un-augmented data from each of the three datasets (the blank groups). The use of data augmentation has improved the classification performance of the three datasets by 7.99%, 6.58%, and 3.63%, respectively.\nIn the head-to-tail data augmentation method, the\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n9\naugmentation length determines the final amount of training data, which will, in turn, affect the training of the model. If the augmentation is too short, it may result in the reuse of data, which not only does not provide additional useful information but also increases the computational cost of training. If the augmentation is too long, it may again reduce the utilization of the data and prevent the extraction of additional useful information. To investigate the relationship between the expansion length and the final training effect, we conducted a comparison test at different amplification lengths. Considering computational cost and time, the expansion length is only set from 20 to 100 in steps of 10. The results of this test on each of the two datasets are shown in Figure 8. In Dataset 1, the best effect is the combination with a length of 50. This results in a 3.39% improvement over the worst combination. In Dataset 2,\nthe best effect is the combination with a length of 80. This results in an improvement of 1.38% over the worst combination. In Dataset 3, the best effect is the combination with a length of 40. This results in an improvement of 1.75% over the worst combination. This illustrates that there is not a clear relationship between the classification results and the augmentation length. A shorter augmentation length will obtain more training data, but it does not bring better results, and even requires more network computing power."
        },
        {
            "heading": "E. Future work",
            "text": "In the future, we will investigate the application of lightweight networks for the classification of motor imagery, reducing the number of parameters in the neural networks and improving the operational efficiency of the neural networks. In addition, our future work will further investigate which network architectures are more suitable for processing MI-EEG signals and try to utilize fewer channels to achieve better results.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n> IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING <\n10"
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we propose a neural network model called MSCTANN. It is a deep learning-based signal recognition method for multi-class MI classification. The multi-scale module of our MSCTANN model is able to automatically extract and screen features, which can extract rich feature information for the differences in MI-EEG signals. The CTAM layer in our proposed MSCTANN model is able to automatically learn channel and temporal valid information from the data, thus making the network more targeted for learning. Additionally, this paper also proposes a data augmentation method to increase the training data samples, which provides more information for our MSCTANN model. The validity of the method on two four-classification datasets is verified by experiments. Our MSCTANN model provides ideas for the model architecture of deep learning and makes contributions to the recognition task."
        }
    ],
    "title": "Classification of motor imagery based on multi-scale feature extraction and the channeltemporal attention module",
    "year": 2023
}