{
    "abstractText": "\u00a92023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Abstract\u2014Increasing the model capacity is a known approach to enhance the adversarial robustness of deep learning networks. On the other hand, various model compression techniques, including pruning and quantization, can reduce the size of the network while preserving its accuracy. Several recent studies have addressed the relationship between model compression and adversarial robustness, while some experiments have reported contradictory results. This work summarizes available evidence and discusses possible explanations for the observed effects.",
    "authors": [
        {
            "affiliations": [],
            "name": "Svetlana Pavlitska"
        },
        {
            "affiliations": [],
            "name": "Hannes Grolig"
        },
        {
            "affiliations": [],
            "name": "J. Marius Z\u00f6llner"
        }
    ],
    "id": "SP:51823eef9470a5ee79e989a23acdcbbc9591aa72",
    "references": [
        {
            "authors": [
                "I.J. Goodfellow",
                "J. Shlens",
                "C. Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "International Conference on Learning Representations (ICLR), 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "International Conference on Learning Representations (ICLR), 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Shafahi",
                "M. Najibi",
                "M.A. Ghiasi",
                "Z. Xu",
                "J. Dickerson",
                "C. Studer",
                "L.S. Davis",
                "G. Taylor",
                "T. Goldstein"
            ],
            "title": "Adversarial training for free!",
            "venue": "Advances in Neural Information Processing Systems (NIPS),",
            "year": 2019
        },
        {
            "authors": [
                "T. Pang",
                "X. Yang",
                "Y. Dong",
                "H. Su",
                "J. Zhu"
            ],
            "title": "Bag of Tricks for Adversarial Training",
            "venue": "International Conference on Learning Representations (ICLR), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Maini",
                "E. Wong",
                "J.Z. Kolter"
            ],
            "title": "Adversarial robustness against the union of multiple perturbation models",
            "venue": "International Conference on Machine Learning (ICML), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Schott",
                "J. Rauber",
                "M. Bethge",
                "W. Brendel"
            ],
            "title": "Towards the first adversarially robust neural network model on mnist",
            "venue": "International Conference on Learning Representations (ICLR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Athalye",
                "L. Engstrom",
                "A. Ilyas",
                "K. Kwok"
            ],
            "title": "Synthesizing Robust Adversarial Examples",
            "venue": "International Conference on Machine Learning (ICML), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M.S. Bernstein",
                "A.C. Berg",
                "L. Fei-Fei"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "Int. J. Comput. Vis., 2015.",
            "year": 2015
        },
        {
            "authors": [
                "H. Salman",
                "A. Ilyas",
                "L. Engstrom",
                "A. Kapoor",
                "A. Madry"
            ],
            "title": "Do adversarially robust imagenet models transfer better?",
            "venue": "Advances in Neural Information Processing Systems (NIPS),",
            "year": 2020
        },
        {
            "authors": [
                "C. Xie",
                "M. Tan",
                "B. Gong",
                "J. Wang",
                "A.L. Yuille",
                "Q.V. Le"
            ],
            "title": "Adversarial examples improve image recognition",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Andriushchenko",
                "N. Flammarion"
            ],
            "title": "Understanding and improving fast adversarial training",
            "venue": "Advances in Neural Information Processing Systems (NIPS), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Madry",
                "A. Makelov",
                "L. Schmidt",
                "D. Tsipras",
                "A. Vladu"
            ],
            "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
            "venue": "International Conference on Learning Representations (ICLR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "L. Rice",
                "E. Wong",
                "J.Z. Kolter"
            ],
            "title": "Overfitting in adversarially robust deep learning",
            "venue": "International Conference on Machine Learning (ICML), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Li",
                "A. Kadav",
                "I. Durdanovic",
                "H. Samet",
                "H.P. Graf"
            ],
            "title": "Pruning filters for efficient convnets",
            "venue": "International Conference on Learning Representations (ICLR), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Han",
                "H. Mao",
                "W.J. Dally"
            ],
            "title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding",
            "venue": "International Conference on Learning Representations (ICLR), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "P. Stock",
                "A. Joulin",
                "R. Gribonval",
                "B. Graham",
                "H. J\u00e9gou"
            ],
            "title": "And the bit goes down: Revisiting the quantization of neural networks",
            "venue": "International Conference on Learning Representations (ICLR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Galloway",
                "G.W. Taylor",
                "M. Moussa"
            ],
            "title": "Attacking binarized neural networks",
            "venue": "International Conference on Learning Representations (ICLR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A.S. Rakin",
                "J. Yi",
                "B. Gong",
                "D. Fan"
            ],
            "title": "Defend deep neural networks against adversarial examples via fixed and dynamic quantized activation functions",
            "venue": "arXiv preprint arXiv:1807.06714, 2018.",
            "year": 1807
        },
        {
            "authors": [
                "A.W. Wijayanto",
                "J.J. Choong",
                "K. Madhawa",
                "T. Murata"
            ],
            "title": "Towards robust compressed convolutional neural networks",
            "venue": "IEEE International Conference on Big Data and Smart Computing (BigComp), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Lin",
                "C. Gan",
                "S. Han"
            ],
            "title": "Defensive quantization: When efficiency meets robustness",
            "venue": "International Conference on Learning Representations (ICLR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proc. IEEE, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "N. Carlini",
                "D. Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "IEEE Symposium on Security and Privacy, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Zagoruyko",
                "N. Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "British Machine Vision Conference (BMVC), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "2009.",
            "year": 2009
        },
        {
            "authors": [
                "N. Papernot",
                "P. McDaniel",
                "S. Jha",
                "M. Fredrikson",
                "Z.B. Celik",
                "A. Swami"
            ],
            "title": "The limitations of deep learning in adversarial settings",
            "venue": "IEEE European symposium on security and privacy (EuroS&P), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "P. Chen",
                "H. Zhang",
                "Y. Sharma",
                "J. Yi",
                "C. Hsieh"
            ],
            "title": "ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "venue": "ACM Workshop on Artificial Intelligence and Security, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A.G. Howard",
                "M. Zhu",
                "B. Chen",
                "D. Kalenichenko",
                "W. Wang",
                "T. Weyand",
                "M. Andreetto",
                "H. Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "CoRR, vol. abs/1704.04861, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Zhou",
                "A. Yao",
                "Y. Guo",
                "L. Xu",
                "Y. Chen"
            ],
            "title": "Incremental network quantization: Towards lossless cnns with low-precision weights",
            "venue": "International Conference on Learning Representations (ICLR), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Guo",
                "A. Yao",
                "Y. Chen"
            ],
            "title": "Dynamic network surgery for efficient dnns",
            "venue": "Advances in Neural Information Processing Systems (NIPS), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Kurakin",
                "I.J. Goodfellow",
                "S. Bengio"
            ],
            "title": "Adversarial examples in the physical world",
            "venue": "International Conference on Learning Representations (ICLR) - Workshops, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "International Conference on Learning Representations (ICLR), 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Netzer",
                "T. Wang",
                "A. Coates",
                "A. Bissacco",
                "B. Wu",
                "A.Y. Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "2011.",
            "year": 2011
        },
        {
            "authors": [
                "W. Xu",
                "D. Evans",
                "Y. Qi"
            ],
            "title": "Feature squeezing: Detecting adversarial examples in deep neural networks",
            "venue": "Proceedings 2018 Network and Distributed System Security Symposium, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Gorsline",
                "J. Smith",
                "C. Merkel"
            ],
            "title": "On the adversarial robustness of quantized neural networks",
            "venue": "Proceedings of the 2021 on Great Lakes Symposium on VLSI, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Varghese",
                "C. H\u00fcmmer",
                "A. B\u00e4r",
                "F. H\u00fcger",
                "T. Fingscheidt"
            ],
            "title": "Joint optimization for dnn model compression and corruption robustness",
            "venue": "Deep Neural Networks and Data for Automated Driving, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Chen",
                "Y. Zhu",
                "G. Papandreou",
                "F. Schroff",
                "H. Adam"
            ],
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "venue": "European Conference on Computer Vision (ECCV), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Cordts",
                "M. Omran",
                "S. Ramos",
                "T. Rehfeld",
                "M. Enzweiler",
                "R. Benenson",
                "U. Franke",
                "S. Roth",
                "B. Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T. Stauner",
                "F. Blank",
                "M. F\u00fcrst",
                "J. G\u00fcnther",
                "K. Hagn",
                "P. Heidenreich",
                "M. Huber",
                "B. Knerr",
                "T. Schulik",
                "K. Lei\u00df"
            ],
            "title": "Synpeds: A synthetic dataset for pedestrian detection in urban traffic scenes",
            "venue": "Computer Science in Cars Symposium, CSCS, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Wang",
                "G.W. Ding",
                "R. Huang",
                "Y. Cao",
                "Y.C. Lui"
            ],
            "title": "Adversarial robustness of pruned neural networks",
            "venue": "Preprint, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Guo",
                "C. Zhang",
                "C. Zhang",
                "Y. Chen"
            ],
            "title": "Sparse dnns with improved adversarial robustness",
            "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Neklyudov",
                "D. Molchanov",
                "A. Ashukha",
                "D.P. Vetrov"
            ],
            "title": "Structured bayesian pruning via log-normal multiplicative noise",
            "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.-M. Moosavi-Dezfooli",
                "A. Fawzi",
                "P. Frossard"
            ],
            "title": "Deepfool: A simple and accurate method to fool deep neural networks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Jordao",
                "H. Pedrini"
            ],
            "title": "On the effect of pruning on adversarial robustness",
            "venue": "International Conference on Computer Vision (ICCV), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Sandler",
                "A. Howard",
                "M. Zhu",
                "A. Zhmoginov",
                "L.-C. Chen"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Zhang",
                "M. Ciss\u00e9",
                "Y.N. Dauphin",
                "D. Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. DeVries",
                "G.W. Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Yun",
                "D. Han",
                "S.J. Oh",
                "S. Chun",
                "J. Choe",
                "Y. Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6023\u20136032.",
            "year": 2019
        },
        {
            "authors": [
                "J.T.C. Min",
                "M. Motani"
            ],
            "title": "Dropnet: reducing neural network complexity via iterative pruning",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, 2020, pp. 9356\u20139366.",
            "year": 2020
        },
        {
            "authors": [
                "M. Lin",
                "R. Ji",
                "Y. Wang",
                "Y. Zhang",
                "B. Zhang",
                "Y. Tian",
                "L. Shao"
            ],
            "title": "Hrank: Filter pruning using high-rank feature map",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J.-H. Luo",
                "J. Wu"
            ],
            "title": "Neural network pruning with residual-connections and limited-data",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Jordao",
                "F. Yamada",
                "W.R. Schwartz"
            ],
            "title": "Deep network compression based on partial least squares",
            "venue": "Neurocomputing, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Hendrycks",
                "T.G. Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "International Conference on Learning Representations (ICLR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Liao",
                "S. Wang",
                "L. Xiang",
                "N. Ye",
                "S. Shao",
                "P. Chu"
            ],
            "title": "Achieving adversarial robustness via sparsity",
            "venue": "Machine Learning, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. van der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Gui",
                "H. Wang",
                "H. Yang",
                "C. Yu",
                "Z. Wang",
                "J. Liu"
            ],
            "title": "Model compression with adversarial robustness: A unified optimization framework",
            "venue": "Advances in Neural Information Processing Systems (NIPS), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Sinha",
                "H. Namkoong",
                "J.C. Duchi"
            ],
            "title": "Certifying some distributional robustness with principled adversarial training",
            "venue": "International Conference on Learning Representations (ICLR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Ye",
                "K. Xu",
                "S. Liu",
                "H. Cheng",
                "J.-H. Lambrechts",
                "H. Zhang",
                "A. Zhou",
                "K. Ma",
                "Y. Wang",
                "X. Lin"
            ],
            "title": "Adversarial robustness vs. model compression, or both?",
            "venue": "in International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "S. Han",
                "J. Pool",
                "J. Tran",
                "W. Dally"
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 28, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "V. Sehwag",
                "S. Wang",
                "P. Mittal",
                "S. Jana"
            ],
            "title": "Hydra: Pruning adversarially robust neural networks",
            "venue": "Advances in Neural Information Processing Systems (NIPS), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Hu",
                "T. Chen",
                "H. Wang",
                "Z. Wang"
            ],
            "title": "Triple wins: Boosting accuracy, robustness and efficiency together by enabling input-adaptive inference",
            "venue": "International Conference on Learning Representations (ICLR), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Huang",
                "N. Wang"
            ],
            "title": "Data-driven sparse structure selection for deep neural networks",
            "venue": "European Conference on Computer Vision (ECCV), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Weng",
                "H. Zhang",
                "P. Chen",
                "J. Yi",
                "D. Su",
                "Y. Gao",
                "C. Hsieh",
                "L. Daniel"
            ],
            "title": "Evaluating the robustness of neural networks: An extreme value theory approach",
            "venue": "International Conference on Learning Representations (ICLR), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Carmon",
                "A. Raghunathan",
                "L. Schmidt",
                "J.C. Duchi",
                "P.S. Liang"
            ],
            "title": "Unlabeled data improves adversarial robustness",
            "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Kaya",
                "S. Hong",
                "T. Dumitras"
            ],
            "title": "Shallow-deep networks: Understanding and mitigating network overthinking",
            "venue": "International Conference on Machine Learning (ICML), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Frankle",
                "M. Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "International Conference on Learning Representations (ICLR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Liu",
                "M. Sun",
                "T. Zhou",
                "G. Huang",
                "T. Darrell"
            ],
            "title": "Rethinking the value of network pruning",
            "venue": "International Conference on Learning Representations (ICLR), 2019.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n31 1.\n15 78\n2v 1\n[ cs\n.L G\n] 2\n7 N\n\u00a92023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\nIndex Terms\u2014model compression, adversarial robustness\nI. INTRODUCTION AND RELATED WORK\nGoodfellow et al. [1] and Szegedy et al. [2] first brought up the risk of adversarial attacks, small perturbations (often imperceptible by humans) that are carefully crafted and added to the input of state-of-the-art (SOTA) deep neural networks (DNNs). Without specific DNN training or mitigation measures, these attacks lead to high-confidence wrong outputs of SOTA DNNs and convolutional neural networks (CNNs). This inherent vulnerability of DNNs poses an especially high risk when applying them in autonomous driving, facial recognition, or medical domains.\nAdversarial defenses attempt to robustify neural networks artificially, but robustly solving a task fundamentally increases its difficulty. However, simply scaling model sizes is not always an option and is quickly restricted by technical and financial factors. Model compression approaches such as quantization and pruning can significantly reduce model size while preserving comparable performance levels.\nThe impact of model compression on adversarial robustness has been a focus of several recent studies. However, to the best of our knowledge, no analysis of the existing publications to summarize the state of the art has been performed so far. Our work aims at closing this research gap. We have reviewed existing works that either explored the effect of model compression methods on the adversarial vulnerability of the networks or tried to combine both goals in a single training algorithm. We group the existing evidence from the experiments and make conclusions based on these."
        },
        {
            "heading": "A. Adversarial Training",
            "text": "Adversarial training (AT) remains among the most successful defenses against adversarial examples [3]\u2013[7]. Salman et al. showed that adversarially trained ImageNet [8]-classifiers show better transferability [9], which is consistent with the hypothesis that adversarially trained robust networks provide better feature representations. Gong et al. showed that AT can improve image recognition models by preventing overfitting [10]. Andriushchenko et al. [11] stated that performing AT efficiently is important because it is the crucial algorithm for robust deep learning. The idea is intuitive: DNNs are trained by handing them data and correct labels to learn their decision boundaries. In AT, adversarial examples and their correct labels are precautiously augmented into the training process to train a more robust model. Madry et al. proposed the prime baseline for AT with a Projected Gradient Descent (PGD) attack [12], which was later improved by [13] using early-stopping."
        },
        {
            "heading": "B. Model Compression",
            "text": "DNN and CNN architectures have become increasingly deep and complex and can require millions of parameters, which leads to slow inference. Many techniques have been developed to speed up inference, including quantization and pruning.\nPruning aims at selecting insignificant parameters that can be removed to make the model smaller while maintaining high prediction accuracy. The simplest approach, magnitudebased pruning, removes weights below a specified threshold value. Instead of pruning individual weights, it is also possible to prune at a higher level of granularity by removing entire feature maps or filters in a CNN. Filters can be removed using data-independent pruning methods based on properties such as their L1 norm [14]. Correct pruning can help to speed up the inference without impacting accuracy [15]. Quantization is another method that reduces the precision of the model parameters, e.g., from 32-bit floating point to 8-bit integers. It can be performed on scalars or vectors as demonstrated in [16], where the reconstruction error of the activations rather than the weights is minimized."
        },
        {
            "heading": "III. RELATIONSHIP BETWEEN QUANTIZATION AND ROBUSTNESS",
            "text": "Quantization has so far been a focus of only a few works exploring adversarial robustness (see Table I). Our search has revealed a total of four papers [17]\u2013[20], all of which consider both white-box and black-box attacks, while PGD [12] is a method used in all works.\nOne of the first works regarding quantization and adversarial robustness is from Galloway et al. [17]. The authors focused on binarized neural networks where both weights and activations in the hidden layers are quantized to \u00b11. Randomized quantization was used. They compared full-precision networks to their respective binarized network. It was observed that AT is a balancing act with binary models, whereas scaled binary models can benefit from AT. Overall, they concluded that binarized networks can slightly improve the robustness against certain attacks. In terms of efficiency, they observed an advantage of the binarized networks over their full-precision equivalents.\nIn [18], Rakin et al. proposed a novel approach where activations are quantized to increase the adversarial robustness of DNNs. The approach integrates the quantized activation functions into AT. They proposed a fixed as well as a dynamic activation quantization method. For experiments, adversarially trained baseline networks were used. Then, the authors trained LeNet [21] and ResNet-18 [22] with the fixed and dynamic quantization techniques. The models were quantized with different quantization levels (1-, 2- and 3-bit activation). The robustness of the fixed and dynamic quantized networks against various attacks (PGD [12], FGSM [1], Carlini and Wagner (C&W) attack [23]) was compared with the robustness of the baseline networks. The authors concluded that fixed and dynamic quantization can increase the robustness.\nA further work by Wijayanto et al. [19] proposed an adversarial-aware compression framework for DNNs. This framework combines pruning, quantization, and encoding. In their experiments, the approach is compared to pruned and quantized networks. It was observed that quantization can improve robustness.\nAnother novel quantization method is proposed by Lin et al. [20], where an empirical study regarding quantization and robustness was conducted. The authors quantized the activations and compared the naive quantized models to their respective full-precision models. They observed that the conventional quantization method is not robust and that input image quantization applied to hidden layers worsens the robustness. The proposed defensive quantization approach achieved higher robustness than their full-precision counterparts and improved the accuracy without adversarial attack.\nGorsline et al. investigated the effect of weight quantization on robustness in [35]. They experimented on MNIST [21] and a two-spiral classification problem.They concluded with the observation that quantization does not affect robustness if the adversarial attack exceeds a critical strength.\nFinally, Varghese et al. [36] introduced a novel hybrid compression approach that combines pruning and quantiza-\ntion and studied the relationships between robustness and compression. They investigated the more complex task of semantic segmentation for automated driving. In contrast to the other works, the authors investigated corruption robustness, not adversarial robustness. By corruption, they refer to augmentations caused by real-world events (e.g., noise, blur, or weather conditions). They observed improved robustness of the compressed DeepLabv3+ [37] network compared to the reference network.\nIn summary, naive quantization without AT has demonstrated both negative [20] and positive [19] impact on adversarial robustness. If quantization was combined with AT, a positive effect was observed in several works [17], [19], [20]. Moreover, AT was shown to improve quantization itself [20]."
        },
        {
            "heading": "IV. RELATIONSHIP BETWEEN PRUNING AND ROBUSTNESS",
            "text": "An overview of the works that focus on pruning and robustness is given in Table II. We divide the considered approaches into three groups: (1) works that examine the intrinsic relationships between pruning and robustness, (2) works proposing novel approaches via a combination of static pruning with robust training, and (3) the dynamic pruning approach, incorporating adversarial robustness as a training objective."
        },
        {
            "heading": "A. Effects of Pruning on Robustness",
            "text": "The first group of works aims at studying the general effects of pruning on adversarial robustness. In the theoretical and empirical analyses, particular attention was paid to the question of whether pruning offers inherent protection against adversarial attacks.\nWang et al. [40] conducted the first analysis regarding the adversarial robustness of pruned deep neural networks. The work was not published because the experimental evidence was not grounded enough. The effects of pruning on robustness and the impact of AT on pruned networks were investigated. Naturally trained models were compared to their original networks. The accuracy of a pruned model was similar to the accuracy of an original network. The robustness of a pruned network under FGSM and Papernot\u2019s attacks was worse than the robustness of an original network. Neither the pruned nor the original model could withstand the PGD attack. The authors suspected, that pruning reduces the network capacity, which in turn reduces its robustness. Then, the authors performed AT with FGSM and PGD along with the network pruning procedure and compared these models to their respective adversarially trained original networks. They observed that highly pruned networks can become considerably robust, while weight pruning allows more compression than filter pruning, and PGD leads to more robust models than FGSM.\nIn additional experiments with a Wide ResNet [24] on CIFAR-10 [25], the authors observed an interesting result. The PGD-trained network that was moderately pruned (less than 50% of the parameters) was slightly more accurate and more robust than the respective original network. The\nrobustness of the highly pruned network (80% to 94% of the weights) was higher than the original, but the accuracy on natural images dropped simultaneously. With an increasing compression rate, the robustness of the model drops earlier than the classification accuracy. The authors observed that with the training procedures applied, a model cannot be both highly robust and pruned simultaneously.\nAnother early work that studied the intrinsic relationships between the sparsity achieved through weight and activation pruning and the adversarial robustness of DNNs is by Guo et al. [41]. Their analysis is one of the few works that examine the effects of pure pruning without AT on adversarial robustness. The authors trained different architectures and evaluated their robustness under various l2 and l\u221e white-box attacks. For the evaluation of the robustness of the models, the authors suggested two metrics that describe the ability to resist l2 and l\u221e attacks, respectively. First, they pruned the weights of the dense reference networks and compared the robustness of the pruned networks to the original ones. Sparse DNNs are prone to be more robust against l\u221e (FGSM and rFGSM [1])\nand l2 (DeepFool [43], C&W L2 [23]) attacks until the sparsity reaches some thresholds, above which the capacity of the pruned models degrades. This observation is consistent with the observations from [40] described above. The authors verified their results additionally with the attack-agnostic CLEVER [63] scores. They observed positive correlations between activation sparsity in a certain range and robustness. The authors suggested taking care and avoiding sparsity rates that are too high and concluded that sparse nonlinear DNNs can be more robust than their dense counterparts if the sparsity is within a certain range.\nSimilar to the work by Guo et al. [41], Jordao and Pedrini [44] studied the intrinsic effect of pruning on the adversarial robustness of deep convolutional networks without AT. However, unlike [40], [41], the authors did not examine the trade-off between robustness, accuracy, and compression but the relationship between generalization and robustness. They observed that pruning preserves generalization. The authors pruned filters and layers from several reference architectures based on different pruning criteria. After pruning, they fine-\ntuned the compressed networks with augmented data. First, they compared the accuracy and robustness of the dense reference networks to their pruned counterparts (filters, layers, and both) under different attacks. Overall, they observed that pruning improves robustness without sacrificing generalization. Similar to [41], the authors did not use the PGD attack in their experiments.\nFurthermore, they could not observe a superior pruning strategy with respect to all attacks. Then, they demonstrated that removing single filters can improve the robustness without adjusting the network parameters. They also observed that fine-tuning leads to increased adversarial robustness than training from scratch. When comparing the pruned network to other defense mechanisms, they observed that pruning obtained one of the best average improvements. They suggested combining pruning with other defense mechanisms to achieve more robust and efficient networks. The authors concluded that pruning filters or layers (or both) increase the adversarial robustness of convolutional networks.\nIn summary, both negative [40], [56] and positive [41], [44] effect of pruning on robustness were seen in the experiments, although studies leading to the latter provided significantly more empirical evidence. Both papers observing positive effects [41], [44] have used retraining \u2013 this confirms again that omitted retraining strongly weakens robustness. On the other hand, these works did not provide results for the PGD, making comparing the pieces of evidence difficult."
        },
        {
            "heading": "B. Combined Compression-Robustness Methods",
            "text": "Various combined compression-robustness approaches were proposed, with network pruning performed before, after, or alternately with AT. Liao et al. [54] theoretically proved the correlation between weight sparsity and adversarial robustness and showed in experiments that weight sparsity improves robustness with AT. They showed that pruning does not affect the model robustness negatively in some adversarial settings. Furthermore, they demonstrated, that the robustness can be improved with AT after pruning. Overall, the proposed novel AT method that includes pruning was shown to lead to sparse networks with better performance than their dense counterparts.\nIn [56] the authors stated, that they describe the first framework that connects model compression with adversarial robustness. They proposed their Adversarially Trained Model Compression (ATMC) framework, which includes pruning, quantization, and AT. ATMC was compared to adversarially trained, pruned, adversarially trained, and pruned, as well as adversarially trained, pruned, and adversarially retrained models. Their results support the existence of a trilateral trade-off between robustness, accuracy, and compression. Analogously to [40], [41], the authors concluded, that if robustness is taken into account, model compression can maintain accuracy and robustness, whereas naive model compression may decrease adversarial robustness.\nA similar approach is proposed by Ye et al [58]. The authors proposed a framework of concurrent AT and weight\npruning. To compare weight pruning and training from scratch, they adversarially trained models of different architectures with various scaling factors. Then, the authors pruned the filters of each network with the proposed framework. Each reference network was pruned to the respective smaller scaling factors. The authors summarized that pruned networks can have high accuracy and robustness, which can be lost if a network with a comparable size is adversarially trained from scratch. Framework evaluation under different pruning schemes and transfer attacks has demonstrated, that irregular pruning performs the best and filter pruning performs the worst. Interestingly, the pruned model turned out to be more robust to transfer attacks than the respective dense network.\nIn [60], pruning is formulated as an empirical risk minimization problem, while the minimization problem can be integrated with various robust training objectives like AT. The authors demonstrated that pruning after training helps to achieve state-of-the-art accuracy and robustness. The proposed method (HYDRA) incorporates the AT approach by Carmon et al. [64], although other robust training objectives are possible. The authors observed improved compression, accuracy, and robustness compared to the baseline networks and previous work like the ADMM [59]-based approach by Ye et al. [58]. The authors advocated for formulating pruning as an optimization problem that integrates the robust training objective. They identified the performance gap between non-pruned and pruned networks as an open challenge.\nIn summary, two works [58], [60] observed a significantly higher robustness of pruned networks compared to compact networks of comparable size. Furthermore, the authors concluded that pruned networks can, after all, exhibit similar robustness to their dense reference networks.\nFurthermore, the results overall indicate that the effect of pruning on robustness varies in magnitude depending on whether we are comparing networks of the same capacity or networks of different capacities. Retraining the pruned models seems to be a crucial factor in that view. It was observed that most networks show a higher robustness when retrained after pruning, compared to the networks for which no retraining was performed."
        },
        {
            "heading": "C. Dynamic Pruning and Robustness",
            "text": "Hu et al. [61] proposed the first dynamic approach to improve network efficiency, accuracy, and robustness and called it Robust Dynamic Inference Networks (RDI Nets). These networks are based on the work of Kaya et al. [65]. RDI-nets stop inference in early layers. In their experiments, the authors evaluated three adversarially (PGD) trained models against their respective RDI nets using three white-box attack algorithms, which were executed in three proposed attack forms. Then the authors compared the RDI-nets to defended sparse networks, i.e., networks that were compressed with a state-of-the-art network pruning method Sparse Structure Selection (SSS) [62] and then adversarially retrained (PGD). Furthermore, they compared their RDI nets to the latest ATMC algorithm [56]. The pruning + defense baseline has demon-\nstrated superior robustness compared to the respective dense reference network. The authors concluded with the statement that they achieved better accuracy, stronger robustness, and computational savings of up to 30%. It should be noted, however, that dynamic pruning does not reduce the model size, but can only achieve efficiency gains in terms of the required computing resources."
        },
        {
            "heading": "D. Connection to the Lottery Ticket Hypothesis",
            "text": "The lottery ticket hypothesis by Frankle et al. [66] states that randomly initialized networks contain subnetworks (\u201dthe winning tickets\u201d). When trained in isolation, these subnetworks can reach test accuracies comparable to the reference network in a less or equal number of iterations. The initial weights of these winning tickets make training particularly effective. The only meaning of weight pruning is thus the effective initialization of the final pruned model.\nIn contrast, Liu et al. [67] observed that the winning ticket initialization does not bring improvement over random initialization. They showed that training from scratch gave comparable or better performance than SOTA pruning algorithms, thus making the original network\u2019s inherited weights useless. The meaning of weight pruning is thus the pruned architecture itself. They suggested that pruning can be a useful architecture search paradigm, but the pruned network should be trained with random initialized values.\nA few works examined these hypotheses with respect to adversarial robustness. In particular, Ye et al. [58] observed that training from scratch cannot achieve robustness and accuracy simultaneously, even with inherited initialization, which contradicts the lottery ticket hypothesis. In contrast, Liao et al. [54] concluded that preferable adversarial robustness can be achieved through the lottery ticket settings. They argue that they search for the winning ticket by iterative global unstructured pruning, while Ye et al. [58] used filter pruning. Jordao et al. [44] showed that fine-tuning leads to better robustness than the winning ticket.\nFinally, Sehwag et al. [60] demonstrated the existence of hidden sub-networks that are more robust than the original network. They showed that highly robust sub-networks exist even within non-robust networks."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work, we reviewed and compared the existing works exploring the relationship between model compression methods (quantization and pruning) and adversarial robustness.\nThroughout all experiments, it was shown that naive pruning and quantization can reduce robustness. Furthermore, as long as networks are compressed within certain limits, pruning may preserve or even improve robustness, especially when comparing compressed and compact models of the same size.\nMoreover, the reviewed works showed that combining model compression and robustness in AT is possible. However, a trade-off exists between compression ratio, accuracy, and robustness. It was observed relatively consistently that once a critical compression ratio is exceeded, first the robustness\nand then the accuracy decrease. Some authors explain that robustness thus requires a greater capacity than accuracy.\nOverall, many reviewed works agree that compression must be performed carefully. Simple, straightforward compression can also have negative effects on robustness; some authors, therefore, also suggest that robustness should be taken into account in the evaluation of new compression methods."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "This research is funded by the German Federal Ministry of Education and Research within the project \u201dGreenEdge-FuE\u201c, funding no. 16ME0517K."
        }
    ],
    "title": "Relationship between Model Compression and Adversarial Robustness: A Review of Current Evidence",
    "year": 2023
}