{
    "abstractText": "Actions description languages (ADLs), such as STRIPS, PDDL, and RDDL specify the input format for planning algorithms. Unfortunately, their syntax is foreign to most potential users of planning technology. Moreover, this syntax limits the ability to describe complex and large domains. We argue that programming languages (PLs), and more specifically, probabilistic programming languages (PPLs), provide a more suitable alternative. PLs are familiar to all programmers, support complex data types and rich libraries for their manipulation, and have powerful constructs, such as loops, sub-routines, and local variables with which complex, realistic models and complex objectives can be simply and naturally specified. PPLs, specifically, make it easy to specify distributions, which are essential for stochastic models. The natural objection to this proposal is that PLs are opaque and too expressive, making reasoning about them difficult. However, PPLs also come with efficient inference algorithms, which, coupled with a growing body of work on sampling-based and gradient-based planning, imply that planning and execution monitoring can be carried out efficiently in practice. We expand on this proposal, illustrating its potential with examples.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ronen I. Brafman"
        },
        {
            "affiliations": [],
            "name": "David Tolpin"
        },
        {
            "affiliations": [],
            "name": "Or Wertheim"
        }
    ],
    "id": "SP:00a974e6b337da4e27ddfb952edfcd9dabb98f47",
    "references": [
        {
            "authors": [
                "P. Auer",
                "N. Cesa-Bianchi",
                "P. Fischer"
            ],
            "title": "Finitetime Analysis of the Multiarmed Bandit Problem",
            "venue": "Machine Learning, 47(2-3): 235\u2013256.",
            "year": 2002
        },
        {
            "authors": [
                "C. B\u00e4ckstr\u00f6m",
                "B. Nebel"
            ],
            "title": "Complexity Results for SAS+ Planning",
            "venue": "Comput. Intell., 11: 625\u2013656.",
            "year": 1995
        },
        {
            "authors": [
                "A.G. Baydin",
                "B.A. Pearlmutter",
                "A.A. Radul",
                "J.M. Siskind"
            ],
            "title": "Automatic Differentiation in Machine Learning: a Survey",
            "venue": "Journal of Machine Learning Research, 18(153): 1\u201343.",
            "year": 2018
        },
        {
            "authors": [
                "J. Bhandari",
                "D. Russo"
            ],
            "title": "On the Linear Convergence of Policy Gradient Methods for Finite MDPs",
            "venue": "Banerjee, A.; and Fukumizu, K., eds., Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "E. Bingham",
                "J.P. Chen",
                "M. Jankowiak",
                "F. Obermeyer",
                "N. Pradhan",
                "T. Karaletsos",
                "R. Singh",
                "P. Szerlip",
                "P. Horsfall",
                "N.D. Goodman"
            ],
            "title": "Pyro: deep universal probabilistic programming",
            "venue": "Journal of Machine Learning Research, 20(28): 1\u20136.",
            "year": 2019
        },
        {
            "authors": [
                "P.E. Bulychev",
                "A. David",
                "K.G. Larsen",
                "M. Mikucionis",
                "D.B. Poulsen",
                "A. Legay",
                "Z. Wang"
            ],
            "title": "UPPAALSMC: Statistical Model Checking for Priced Timed Automata",
            "venue": "Wiklicky, H.; and Massink, M., eds., Proceedings 10th Workshop on Quantitative Aspects of Program-",
            "year": 2012
        },
        {
            "authors": [
                "A. Camacho",
                "R.T. Icarte",
                "T.Q. Klassen",
                "R. Valenzano",
                "S.A. McIlraith"
            ],
            "title": "LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI).",
            "year": 2019
        },
        {
            "authors": [
                "B. Carpenter",
                "A. Gelman",
                "M. Hoffman",
                "D. Lee",
                "B. Goodrich",
                "M. Betancourt",
                "M. Brubaker",
                "J. Guo",
                "P. Li",
                "A. Riddell"
            ],
            "title": "Stan: a probabilistic programming language",
            "venue": "Journal of Statistical Software, Articles, 76(1): 1\u201332.",
            "year": 2017
        },
        {
            "authors": [
                "M. Chen",
                "E. Frazzoli",
                "D. Hsu",
                "W.S. Lee"
            ],
            "title": "POMDP-lite for robust robot planning under uncertainty",
            "venue": "2016 IEEE International Conference on Robotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, 5427\u20135433.",
            "year": 2016
        },
        {
            "authors": [
                "R.E. Fikes",
                "N. Nilsson"
            ],
            "title": "STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving",
            "venue": "Artificial Intelligence, 2: 189\u2013208.",
            "year": 1971
        },
        {
            "authors": [
                "M. Fox",
                "D. Long"
            ],
            "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning",
            "venue": "Domains. JAIR,",
            "year": 2003
        },
        {
            "authors": [
                "A. Gelman",
                "J. Carlin",
                "H. Stern",
                "D. Rubin"
            ],
            "title": "Bayesian Data Analysis, Third Edition",
            "venue": "Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. ISBN 9781420057294.",
            "year": 2013
        },
        {
            "authors": [
                "A. Gerevini",
                "P. Haslum",
                "D. Long",
                "A. Saetti",
                "Y. Dimopoulos"
            ],
            "title": "Deterministic planning in the fifth international planning competition: PDDL3 and experimental evaluation of the planners",
            "venue": "Artif. Intell., 173(5-6): 619\u2013668.",
            "year": 2009
        },
        {
            "authors": [
                "Z. Ghahramani"
            ],
            "title": "Learning Dynamic Bayesian Networks",
            "venue": "Giles, C. L.; and Gori, M., eds., Adaptive Processing of Sequences and Data Structures, International Summer School on Neural Networks, \u201dE.R. Caianiello\u201d, Vietri sul Mare, Salerno, Italy, September 6-13, 1997, Tutorial Lec-",
            "year": 1997
        },
        {
            "authors": [
                "N.D. Goodman",
                "V.K. Mansinghka",
                "D.M. Roy",
                "K. Bonawitz",
                "J.B. Tenenbaum"
            ],
            "title": "Church: a language for generative models",
            "venue": "Proceedings of Uncertainty in Artificial Intelligence.",
            "year": 2008
        },
        {
            "authors": [
                "N.D. Goodman",
                "A. Stuhlm\u00fcller"
            ],
            "title": "The Design and Implementation of Probabilistic Programming Languages",
            "venue": "Electronic; retrieved 2019/3/29.",
            "year": 2014
        },
        {
            "authors": [
                "D. Henriques",
                "J.G. Martins",
                "P. Zuliani",
                "A. Platzer",
                "E.M. Clarke"
            ],
            "title": "Statistical Model Checking for Markov Decision Processes",
            "venue": "Ninth International Conference on Quantitative Evaluation of Systems, QEST 2012, London, United Kingdom, September 17-20, 2012, 84\u201393. IEEE",
            "year": 2012
        },
        {
            "authors": [
                "A. Kucukelbir",
                "D. Tran",
                "R. Ranganath",
                "A. Gelman",
                "D.M. Blei"
            ],
            "title": "Automatic differentiation variational inference",
            "venue": "J. Mach. Learn. Res., 18(1): 430\u2013474.",
            "year": 2017
        },
        {
            "authors": [
                "N. Lipovetzky",
                "H. Geffner"
            ],
            "title": "Width and Serialization of Classical Planning Problems",
            "venue": "Raedt, L. D.; Bessiere, C.; Dubois, D.; Doherty, P.; Frasconi, P.; Heintz, F.; and Lucas, P. J. F., eds., ECAI 2012 - 20th European Conference on Artificial Intelligence. Including Prestigious",
            "year": 2012
        },
        {
            "authors": [
                "V. Mansinghka",
                "D. Selsam",
                "Y.N. Perov"
            ],
            "title": "Venture: a higher-order probabilistic programming platform with programmable inference",
            "venue": "CoRR, abs/1404.0099.",
            "year": 2014
        },
        {
            "authors": [
                "D. McDermott",
                "M. Ghallab",
                "A. Howe",
                "C. Knoblock",
                "A. Ram",
                "M. Veloso",
                "D. Weld",
                "D. Wilkins"
            ],
            "title": "PDDL \u2013 The Planning Domain Definition Language \u2013 Version 1.2",
            "venue": "Technical report,",
            "year": 1998
        },
        {
            "authors": [
                "N. Meuleau",
                "L. Peshkin",
                "K.-E. Kim",
                "L.P. Kaelbling"
            ],
            "title": "Learning finite-state controllers for partially observable environments",
            "venue": "conference on Uncertainty in artificial intelligence (UAI), 427\u2013436.",
            "year": 1999
        },
        {
            "authors": [
                "T. Minka",
                "J. Winn",
                "J. Guiver",
                "D. Knowles"
            ],
            "title": "Infer .NET 2.4, microsoft research cambridge",
            "year": 2010
        },
        {
            "authors": [
                "L.M. Murray",
                "T.B. Sch\u00f6n"
            ],
            "title": "Automated learning with a probabilistic programming language: Birch",
            "venue": "Annual Reviews in Control, 46: 29 \u2013 43.",
            "year": 2018
        },
        {
            "authors": [
                "B. Paige",
                "F. Wood",
                "A. Doucet",
                "Y. Teh"
            ],
            "title": "Asynchronous anytime sequential Monte Carlo",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2014
        },
        {
            "authors": [
                "L. P\u00e9ret",
                "F. Garcia"
            ],
            "title": "On-Line Search for Solving Markov Decision Processes via Heuristic Sampling",
            "venue": "Proceedings of the 16th European Conference on Artificial Intelligence, 530\u2013534.",
            "year": 2004
        },
        {
            "authors": [
                "P. Poupart",
                "C. Boutilier"
            ],
            "title": "Bounded Finite State Controllers",
            "venue": "Proceedings of the 16th International Conference on Neural Information Processing Systems. MIT Press.",
            "year": 2004
        },
        {
            "authors": [
                "M.L. Puterman"
            ],
            "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
            "venue": "Wiley.",
            "year": 2005
        },
        {
            "authors": [
                "T. Rainforth",
                "C.A. Naesseth",
                "F. Lindsten",
                "B. Paige",
                "J.-W. van de Meent",
                "A. Doucet",
                "F. Wood"
            ],
            "title": "Interacting particle Markov chain Monte Carlo",
            "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "S. Sanner"
            ],
            "title": "Relational Dynamic Influence Diagram Language (RDDL): Language Description",
            "venue": "Unpublished. Available from Author.",
            "year": 2010
        },
        {
            "authors": [
                "D. Silver",
                "J. Veness"
            ],
            "title": "Monte-Carlo Planning in Large POMDPs",
            "venue": "NIPS\u201910.",
            "year": 2010
        },
        {
            "authors": [
                "S. Srivastava",
                "S. Russell",
                "P. Ruan",
                "X. Cheng"
            ],
            "title": "First-Order Open-Universe POMDPs",
            "venue": "Zhang, N. L.; and Tian, J., eds., Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27, 2014, 742\u2013751. AUAI",
            "year": 2014
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "venue": "Cambridge, MA: MIT Press.",
            "year": 1998
        },
        {
            "authors": [
                "D. Tolpin",
                "S.E. Shimony"
            ],
            "title": "MCTS based on simple regret",
            "venue": "Proceedings of The 26th AAAI Conference on Artificial Intelligence, 570\u2013576.",
            "year": 2012
        },
        {
            "authors": [
                "J.-W. van de Meent",
                "B. Paige",
                "D. Tolpin",
                "F. Wood"
            ],
            "title": "Black-box policy search with probabilistic programs",
            "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "J.-W. van de Meent",
                "H. Yang",
                "V. Mansinghka",
                "F. Wood"
            ],
            "title": "Particle Gibbs with ancestor sampling for probabilistic programs",
            "venue": "In Artificial Intelligence and Statistics",
            "year": 2015
        },
        {
            "authors": [
                "O. Wertheim",
                "D. Suissa",
                "R.I. Brafman"
            ],
            "title": "Towards Plug\u2019nPlay Task-Level Autonomy for Robotics Using POMDPs and Generative Models",
            "venue": "Proceedings of the AREA22 Workshop.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wingate",
                "N.D. Goodman",
                "D.M. Roy",
                "L.P. Kaelbling",
                "J.B. Tenenbaum"
            ],
            "title": "Bayesian policy search with policy priors",
            "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence, 1565\u20131570.",
            "year": 2011
        },
        {
            "authors": [
                "D. Wingate",
                "T. Weber"
            ],
            "title": "Automated Variational Inference in Probabilistic Programming",
            "venue": "CoRR, abs/1301.1299.",
            "year": 2013
        },
        {
            "authors": [
                "F. Wood",
                "J.-W. van de Meent",
                "V. Mansinghka"
            ],
            "title": "A new approach to probabilistic programming inference",
            "venue": "In Artificial Intelligence and Statistics",
            "year": 2014
        },
        {
            "authors": [
                "L. Yang",
                "P. Hanrahan",
                "N.D. Goodman"
            ],
            "title": "Generating efficient MCMC kernels from probabilistic programs",
            "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, 1068\u20131076.",
            "year": 2014
        },
        {
            "authors": [
                "Q. Yang",
                "K. Wu",
                "Y. Jiang"
            ],
            "title": "Learning action models from plan examples using weighted MAX-SAT",
            "venue": "Artificial Intelligence, 171(2): 107\u2013143.",
            "year": 2007
        },
        {
            "authors": [
                "N. Ye",
                "A. Somani",
                "D. Hsu",
                "W.S. Lee"
            ],
            "title": "DESPOT: Online POMDP Planning with Regularization",
            "venue": "J. Artif. Intell. Res., 58: 231\u2013266.",
            "year": 2017
        },
        {
            "authors": [
                "H. Younes",
                "M. Littman"
            ],
            "title": "PPDDL1.0: An Extension to PDDL for Expressing Planning Domains with Probabilistic Effects",
            "venue": "Technical Report CMU-CS-04-167,",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Introduction Action description languages (ADLs), such as STRIPS (Fikes and Nilsson 1971), PDDL (McDermott et al. 1998) and RDDL (Sanner 2010) specify the input format to planning algorithms. Unfortunately, their syntax is familiar to planning experts only, and not to potential users of planning technology. This has been recognized as a detriment to wider adoption of planning technology, and has led to much interest in the topic of knowledge acquisition for planning with an annual workshop.\nThe syntax of PDDL or RDDL can be mastered with some practice, but it comes with inherent expressive limitations. These make the task of modeling large and complex realworld planning problems particularly challenging. In this paper we argue for the use of programming languages (PLs), and more specifically, probabilistic programming languages (PPLs) (Goodman et al. 2008; Mansinghka, Selsam, and Perov 2014; Wood, van de Meent, and Mansinghka 2014; Goodman and Stuhlmu\u0308ller 2014), as more suitable alternatives to classical ADLs. Copyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nFirst, programmers know quite well how to use PLs to describe what they conceive in their minds. Second, it is easier to write complex models in PLs because they support complex data types and come with rich and efficient libraries for their manipulation. Describing arrays, linked-lists, vectors, trees, tensors etc. is very difficult and cumbersome within a classic ADL. Such a description would not be transparent to anyone who reads it, is likely to be lengthy, and algorithms that use it would have no knowledge of its special properties. PLs also come with powerful constructs, such as loops, subroutines, and local variables that farther enhance our ability to naturally specify complex, realistic models and complex objectives. More specifically, PPLs make it easy to specify distributions essential for modeling uncertainty about action effects and about the world\u2019s state \u2014 essential components of many, if not most, real-world models. Moreover, most ADLs are restricted to closed models, whereas open models are easy to specify using PPLs.\nTwo related objections arise at this point: First, isn\u2019t this essentially telling us to use simulators? Second, we know from KR that expressive models are difficult to reason with, and more specifically, in the case of full-fledged PLs, many inference tasks would be undecideable. Moreover, PLs are likely not structured enough to support good algorithms.\nIndeed, simulation will play a major role in planning algorithms based on PPL models. Simulation based methods such as algorithms for bandit problems (Auer, CesaBianchi, and Fischer 2002), MDPs (Puterman 2005) and POMDPs (Silver and Veness 2010; Ye et al. 2017), RL algorithms (Sutton and Barto 1998), and Novelty-based methods (Lipovetzky and Geffner 2012) have been very successful, recently. And by using code to describe the model, we make it very easy to automatically generate efficient simulators (Wertheim, Suissa, and Brafman 2022) that can also utilize various libraries for manipulating complex data types.\nBut PPL-based ADLs offer more than fast simulation. We believe that future work can exploit the growing body of supported inference algorithms in service of planning and (no less important) execution monitoring. Specifically, PPLs make it easy to assess the likelihood of new observations, allowing us to detect rare and unlikely events. They support more involved inference algorithms that can allow us to validate our plans and verify various properties. PPLs also support annotation methods that are then used by the inference\nalgorithms to make the computation more efficient. One can use such annotations to construct more structured abstractions of the model that can guide computation (e.g., bias the sampling process towards better actions, provide heuristic estimates), or to automatically detect components with special structure (e.g., deterministic variables) and exploit them.\nPerhaps more importantly, PPLs can exploit automatic differentiation methods (Baydin et al. 2018). This implies that recent efficient gradient-based planning methods, such as (Bhandari and Russo 2021) could be used to solve them.\nWe believe the above makes the case for exploiting PPLsbased ADL in planning clear. The rest of this paper provides some background, expands on some of the above issues, and describes a number of examples that illustrate the points made above. We end with a suggested research agenda.\nBackground"
        },
        {
            "heading": "Action Description Languages",
            "text": "The basic components of most planning ADLs can be traced to the STRIPS language (Fikes and Nilsson 1971). Propositions are used to describe the state of the world. These propositions are obtained by instantiating typed variables within a given set of predicates, by objects. For example On(BlockA,BlockB) instantiates the binary predicate On using two block objects, which yields a proposition that is either true or false. Actions are usually specified as schema, e.g., Pickup(?x), with its variables as place holders for appropriate objects. The action description specifies an applicability condition in the form of the precondition list \u2014 a list of predicates with objects or variables that appear in the action schema, and a similar list describing its effects.\nPlanning languages have evolved to gradually more complex descriptions, including quantification over variables, conditional effects, resources, cost, preferences, constraints, axioms, non-Boolean finite-domain variables and much more (Fox and Long 2003; Gerevini et al. 2009; Ba\u0308ckstro\u0308m and Nebel 1995). However, the constrained structure of the languages, their reliance on simple data types and lack of iterators makes it difficult to express complex models.\nMoreover, none of the above languages support probabilistic models \u2014 an essential component of realistic models. These models require numeric information and, often, variables with continuous domains. While an extension of PDDL to probabilistic domains (PPDDL) (Younes and Littman 2004) exists, it is mainly useful for transforming existing classical domains to probabilistic ones.\nThere are two notable exceptions. RDDL (Sanner 2010) is a language developed for describing dynamic Bayesian networks (DBNs) (Ghahramani 1997) that capture the transition and observation functions of MDPs and POMDPs. RDDL can be viewed as restricted, special purpose probabilistic programming language. DBNs describe the conditional probability of post-action or observation values given the pre-action variables\u2019 values. Additional layers in between the pre-action and post-action layer allow intermediate computations. RDDL specs are declarative and explicitly specify the conditional probabilities of each variable value. These values can be described using a broad set of sup-\nported mathematical and logical functions and expressions. A variable in one layer can be conditioned on the previous layer, only. This can be viewed as a generative model that describes how each layer\u2019s values are generated from the previous layer. Writing RDDL specifications requires mastering their syntax, and complex distributions may be difficult to specify. Intermediate computations are accomplished by adding intermediate DBN layers, which may be tedious since each variable can only be assigned once. As we show later, a piece of code with local variables (possibly assigned multiple times) and advanced control structures like loops can be much easier to specify, is more compact and is easier to understand. It also enables much more efficient sampling by simply using the code itself, whereas RDDL specification must be parsed and sampled by a generic piece of code. And finaly, RDDL can describe graphical models, only. Hence, it cannot capture open-world domains.\nBLOG (Srivastava et al. 2014) was an important step in the direction we are espousing worth highlighting. Its syntax is PL-like, and it supports set objects using which it is able to model open universes. However, it is short of a fullfledged PPL as it does not support rich data structure and itreserators, and lacks the inference capabilities of modern PPLs."
        },
        {
            "heading": "Probabilistic Programming",
            "text": "Probabilistic programming (Goodman et al. 2008; Mansinghka, Selsam, and Perov 2014; Wood, van de Meent, and Mansinghka 2014; Goodman and Stuhlmu\u0308ller 2014) represents statistical models as programs written in an otherwise general programming language that provides syntax for the definition and conditioning of random variables. This means that programmers familiar with languages such as C++, Python, and more, can use them with almost no additional effort. Inference can be performed on probabilistic programs to obtain the posterior distribution or point estimates of the variables. Inference algorithms are provided by the PPL framework, and each algorithm is usually applicable to a wide class of probabilistic programs in a black-box manner. Probabilistic programs may contain loops, conditional statements, recursion, and operate on built-in and user defined data structures. The algorithms include MetropolisHastings(Mansinghka, Selsam, and Perov 2014; Yang, Hanrahan, and Goodman 2014), Hamiltonian Monte Carlo (Carpenter et al. 2017), expectation propagation (Minka et al. 2010), extensions of Sequential Monte Carlo (Wood, van de Meent, and Mansinghka 2014; van de Meent et al. 2015; Paige et al. 2014; Rainforth et al. 2016; Murray and Scho\u0308n 2018), variational inference (Wingate and Weber 2013; Kucukelbir et al. 2017), gradient-based optimization (Carpenter et al. 2017; Bingham et al. 2019), and others.\nExpressive Power We compare PPL-based specification with RDDL ones on two examples, seeking to illustrate that that PPL-based specification are easier to write and understand and can describe models that RDDL (and most other ADLs) cannot describe, and this is done without a need to learn a new language. We\nfocus on RDDL because it is the richest available ADL actively used in planning that supports probabilistic models.\nLocusts Swarm \u2014 Multi-Stage Exogenous Events In this domain, aside from the agent\u2019s action, there are exogenous events that take place in parallel. To capture these exogenous events, RDDL needs to introduce intermediate layers in the DBN. If the event is complex, i.e., develops in multiple stages, then one layer is needed for each stage since every variable in a DBN can be assigned once only. With code, on the other hand, one can considerably simplify the domain description by using intermediate variables that describe the process and are reassigned multiple times.\nIn the Locusts Swarm domain, a swarm of locusts has invaded a nature reserve. The reserve is divided into nine primary cells that host a variety of endangered plants (10 tons of plants in each cell). The swarm moves from cell to cell twice during the night, eating half the plants of each cell it encounters. It rests in the last cell visited at night during the following day. The swarm moves stochastically depending on how many plants it smells in neighboring cells. The reserve management has a crop duster that can spray pesticides at a single cell at night. These materials fade away after one day and need a whole day to disable the swarm. The only chance to stop this catastrophe is to spray, in advance, the cell in which the swarm will rest the following day.\nRDDL requires a description that grows linearly because increasing the number of nightly swarm transitions increases the number of required RDDL layers. Using a PPL, we can provide a fixed-size description. The verified RDDL code appears in Listing 3 and the pseudo PPL code in Listing 1.\nThe Open World Room Cleaning Domain Using code, it is easy to describe open-world domains, i.e., domains in which the set of objects is not constant. This is very natural in many cases, and was demonstrated in BLOG (Srivastava et al. 2014). Below is a simple example of a domain describing a robot cleaning a children\u2019s room. Such a domain does not correspond to a graphical model, because the size of the graph changes. Hence, it cannot be described in RDDL, PDDL, and similar languages.\nIn the Room Cleaning domain, a robot cleans the children\u2019s room and should put all the toys in their place. Each toy has a size and a level of difficulty to grasp, and a price. The robot is initially aware of only three toys, some already in place. It can place one toy per minute, yet it may drop and break a toy. Every minute it receives positive reward for ordered toys based on their size, and a negative reward for unordered toys. If it breaks a toy, it receives a one-time penalty for its price. Every minute the robot is cleaning, it may find up to two new toys needing attention. The mission only ends when all toys are in place, and there is no maximum number of toys (See pseudo code in Listing 2)."
        },
        {
            "heading": "Reward Machines and Factorization",
            "text": "Recently, specification of complex, non-Markovian reward functions (i.e., ones depending on the entire past), captured by automata, and known as reward machines have become\npopular (Camacho et al. 2019). By taking the Cartesian product of the underlying MDP and the reward-machine automaton, we obtain a new, product MDP w.r.t which the reward is Markovian.\nTechnically, reward machines allow us to decompose the \u201dtrue\u201d product MDP into smaller components. This is good for learning, representation, and planning. This is particularly important when MDPs are represented using explicit tables. Probabilistic programs generalize this idea. First, factored structure can be reflected directly in the code \u2014 both the probabilistic part and the reward part. Second, one can describe much more elaborate reward functions using code. In fact, utilities behave much like log-probabilities (Wingate et al. 2011; van de Meent et al. 2016), and the same machinery can be applied to them.\nInference, Planning and Verification One advantage of PPLs is the (growing) body of inference algorithms they support. Sampling algorithms are obtained immediately, as the PPL code can be executed to yield simulated values. This is an important advantage over existing ADLs in which the description must be parsed and compiled to an executable format. Below we describe a number of additional inference tasks that can be carried out using existing infra-structure."
        },
        {
            "heading": "Computing Policy Parameters \u2014 Sailing Domain",
            "text": "In many applications, we seek to find optimal parameters for a policy with fixed structure. A classic example is a fixed-size finite-state controller, where we seek to find the structure of the best controller for a POMDP with bounded size (Meuleau et al. 1999; Poupart and Boutilier 2004). The Sailing Domain provides an example of such a problem (Pe\u0301ret and Garcia 2004; Tolpin and Shimony 2012).\nA sailing boat must travel between the opposite corners A and B of a square lake of a given size. At each step, the boat can head in 8 directions to adjacent squares. It always moves one unit-distance, called a leg. The unit distance cost of movement depends on the wind, which can also blow in 8 directions. The cost of sailing into the wind is prohibitively high, upwind is the highest feasible, and away from the wind is the lowest. When the angle between the boat and the wind changes sign, the sail must be tacked, which incurs an additional cost. The wind is assumed to follow a random walk, either staying the same or switching to an adjacent direction, with a known probability.\nFor any given lake size, there is a non-parametric stochastic policy that tabulates the distribution of legs for each combination of location, tack, and wind. However, such policy does not generalize well \u2014 if the lake surface area increases, due to a particularly rainy year, for example, the policy is not applicable to the new parts of the lake. Instead, we can define a generalizable parametric policy balancing between hesitation in anticipation for a better wind and rushing to the goal at any cost. The policy chooses a leg with log-probability equal, up to a constant, to the sum of the leg cost and of the Euclidean distance between the position after the leg and the goal, multiplied by the policy parameter \u03b8 (the leg directed into the wind is excluded from choices). The greater\nthe \u03b8, the higher is the probability that a leg bringing the boat closer to the goal will be chosen:\nlog Pr(leg) = leg-cost+\u03b8\u00b7distance(next-location, goal)+C (1)\nIn a probabilistic programming language, (1) can be expressed as just sampling from a categorical distribution:\nleg \u223c Categorical({leg i : Pr(leg i)}) (2)\nInferring the policy, either offline or online, can be accomplished using out-of-the-box inference algorithms for models expressed by probabilistic programs. Note, that this policy is differentiable with respect to \u03b8. This allows to leverage automatic differentiation capabilities of modern probabilistic programming languages for efficient optimization.\nVerification Probabilistic model-checking (a.k.a. statistical model checking in the verification community) (Henriques et al. 2012) refers to the problem of verifying certain properties of probabilistic transition systems. Special purpose systems for this task such as PRISM (Kwiatkowska, Norman, and Parker 2011) and UPPAAL-SMC (Bulychev et al. 2012) have been developed. The basic query in such systems is what is the probability of a future event satisfying some property. There are several settings in which the query is posed: (a) for the fixed policy \u2014 to check the robustness of a chosen policy, (b) for a broad class (a distribution) of policies \u2014 to check the robustness of the agent in an average case, and (c) for the worst-case policy with respect to the query \u2014 to check robustness of the agent in an adversarial setting. For the above mentioned sailing domain, for example, we may query about the probability of visiting the complementary corners of the square lake, where bulrush grows, or about the probability of travel cost execeeding a certain threshold.\nThis is a standard query in probabilistic programming. From the probabilistic inference point of view, these three settings involve analysis of the predictive posterior of the model and differ only in the way the model is conditioned. For (a), the predictive posterior is obtained for the model conditioned on the chosen policy parameters. For (b), a Bayesian prior reflecting assumptions on the class of policies is imposed on latent variables, and the predictive posterior is obtained under the prior. For (c), applicable to stochastic domains only, the model is conditioned on the occurence of the event of interest. The posterior is most often represented by samples, allowing for convenient and efficient calculation of quantiles and compatibility intervals; alternatively, variational inference may allow performing probabilistic queries on the posterior in closed form.\nMoreover, unlike purpose-built tools, which are often confined to a particular type of queries, the whole arsenal of Bayesian model checking, evaluation, and comparison (Gelman et al. 2013, Chapters 6\u20137) becomes available when probabilistic programming is used to define the model.\nResearch Agenda The discussion above suggests natural research questions:\nPlanning Algorithms The most important direction is continuing to develop planning algorithms that can exploit PPL-based action descriptions. This includes: \u2022 Efficient sampling algorithms. \u2022 Model-based gradient descent algorithms that exploit our\nability to auto-diff probabilistic programs. \u2022 Techniques for guiding planners, including heuristics\n(such as novelty (Lipovetzky and Geffner 2012)), helpful-actions, as used by POMCP (Silver and Veness 2010) and Despot (Ye et al. 2017) to perform roll-outs, and other techniques.\n\u2022 Algorithms that recognize domains with special properties, e.g., deterministic, fully observable, allowing the use of special-purpose planners\n\u2022 Algorithms that exploit component structure within a program for planning, such as deterministic components (as in reward machines and POMDP-Lite (Chen et al. 2016)), fully observable components, etc."
        },
        {
            "heading": "Learning Algorithms",
            "text": "A long line of work, going back at least to (Yang, Wu, and Jiang 2007) is concerned with learning action models for planning. This becomes even more crucial when we allow more complex descriptions. This should be of interest to the PPL community, interested in learning PPLs in general, as it offers a more constrained setting with many potential examples. If fact, learning quantitative information with program structure fixed is already supported by existing PPLs."
        },
        {
            "heading": "Compilation",
            "text": "Methods for compiling programs into existing formalisms (e.g., to RDDL) could be used to exploit the strength of current planning algorithms. Approximate compilation methods can be used to provide approximate solutions, and these could also be used to guide algorithms that solve the original problem description, e.g., as heuristics.\nSummary In this paper we argued for the use of PPLs as a language for specifying stochastic planning models. PPL makes it easier to express complex models, and their code can be exploited by planning algorithms. Indeed, models of various realistic systems cannot be captured compactly using existing formalism, nor do associated methods scale to handle planning in them. Moreover, PPL inference algorithms can be used by planning algorithms to support various useful inference tasks. This, of course, does not detract from the usefulness of existing methods with their associated planning algorithms. They remain useful as potential models, when appropriate, and as tools that can be exploited by richer models to support efficient planning.\nAppendix: Code Examples In the following pages we provide PPL pseudo-code for the Locust Swarm and Room Cleaning domains and verified RDDL code for the Locust Swarm domain.\nListing 1 The Locusts Swarm domain in pseudo PPL code. 1: c o o r d i n a t e s enum {x , y} 2: d i r e c t i o n s enum {up , down , l e f t , r i g h t} 3: o v e r n i g h t t r a n s i t i o n s =2 / / The number o f n i g h t l y swarm movements . 4: 5: / / s t a t e v a r i a b l e s : 6: t o n s o f p l a n t s i n c e l l s [ 3 , 3 ] =10 / / 1 0 t o n s i n each of 3x3 c e l l s 7: s w a r m l o c a t i o n = (2 , 2 ) / / The ( x , y ) l o c a t i o n o f t h e swarm . 8: s w a r m d i s a b l e d = f a l s e / / De te rmine i f t h e swarm i s d i s a b l e d and no l o n g e r damages t h e p l a n t s . 9:\n10: / / \u2019 d i s a b l e s w a r m a c t i o n \u2019 i s t h e l o c a t i o n we want t o s p r a y t h e p e s t i c i d e s . \u2019 s \u2019 i s t h e s t a t e we want t o sample from . 11: f u n c t i o n s a m p l e n e x t s t a t e ( d i s a b l e s w a r m a c t i o n , s ) 12: f o r move i n 1 t o o v e r n i g h t t r a n s i t i o n s / / Repea t by t h e number o f t h e swarm n i g h t l y t r a n s i t i o n s . 13: t o t a l w e i g h t =0 14: w e i g h t s [ 4 ] 15: 16: / / C a l c u l a t e t h e p r o b a b i l i t y f o r t h e swarm t o go i n each d i r e c t i o n based on t h e amounts o f p l a n t s t h e r e . 17: w e i g h t s [ up ] = i f s . s w a r m l o c a t i o n [ y ] > 2 t h e n 0 . 0 e l s e s . t o n s o f p l a n t s i n c e l l s [ s . s w a r m l o c a t i o n [ x ] , s . s w a r m l o c a t i o n [ y ] +1 ] 18: w e i g h t s [ down ] = i f s . s w a r m l o c a t i o n [ y ] < 2 t h e n 0 . 0 e l s e s . t o n s o f p l a n t s i n c e l l s [ s . s w a r m l o c a t i o n [ x ] , s . s w a r m l o c a t i o n [ y ] \u22121 ] 19: w e i g h t s [ r i g h t ] = i f s . s w a r m l o c a t i o n [ x ] > 2 t h e n 0 . 0 e l s e s . t o n s o f p l a n t s i n c e l l s [ s . s w a r m l o c a t i o n [ x ] +1 ] , s . s w a r m l o c a t i o n [ y ] ] 20: w e i g h t s [ l e f t ] = i f s . s w a r m l o c a t i o n [ x ] < 2 t h e n 0 . 0 e l s e s . t o n s o f p l a n t s i n c e l l s [ s . s w a r m l o c a t i o n [ x ] \u22121 ] , s . s w a r m l o c a t i o n [ y ] ] 21: t o t a l w e i g h t = w e i g h t s [ up ] + w e i g h t s [ down ] + w e i g h t s [ r i g h t ] + w e i g h t s [ l e f t ] 22: 23: f o r m o v e d i r e c t i o n i n d i r e c t i o n s / / Normal i ze t h e w e i g h t s . 24: w e i g h t s [ m o v e d i r e c t i o n ] = w e i g h t s [ m o v e d i r e c t i o n ] / t o t a l w e i g h t 25: m o v e d i r e c t i o n = s a m p l e d i s c r e t e ( w e i g h t s ) / / Sample t h e swarm movement d i r e c t i o n . 26: 27: / / C a l c u l a t e t h e new swarm l o c a t i o n based on t h e sampled d i r e c t i o n . 28: s . s w a r m l o c a t i o n [ x ] = s w i t c h ( m o v e d i r e c t i o n ){ 29: c a s e r i g h t : s . s w a r m l o c a t i o n [ x ] +1 30: c a s e l e f t : s . s w a r m l o c a t i o n [ x ] \u22121 31: d e f a u l t : s . s w a r m l o c a t i o n [ x ] 32: } 33: s . s w a r m l o c a t i o n [ y ] = s w i t c h ( m o v e d i r e c t i o n ){ 34: c a s e up : s . s w a r m l o c a t i o n [ y ] +1 35: c a s e down : s . s w a r m l o c a t i o n [ y ] \u22121 36: d e f a u l t : s . s w a r m l o c a t i o n [ y ] 37: } 38: i f n o t s . s w a r m d i s a b l e d / / I f t h e swarm i s n o t d i s a b l e d , i t e a t s h a l f t h e food i n i t s c u r r e n t l o c a t i o n . 39: s . t o n s o f p l a n t s i n c e l l s [ s w a r m l o c a t i o n ] = 0 . 5 * s . t o n s o f p l a n t s i n c e l l s [ s w a r m l o c a t i o n ] 40: 41: i f s . d i s a b l e s w a r m a c t i o n == s . s w a r m l o c a t i o n / / The swarm i s d i s a b l e d i f t h e c rop d u s t e r s p r a y e d i t s l a s t l o c a t i o n . 42: s . s w a r m d i s a b l e d = t r u e 43: 44: / / The reward i s t h e t o t a l number o f p l a n t s l e f t i n t h e n a t u r e r e s e r v e . 45: r eward =0 46: f o r e a c h t o n s i n c e l l i n s . t o n s o f p l a n t s i n c e l l s 47: r eward = reward + t o n s i n c e l l 48: r e t u r n reward , s / / R e t u rn t h e reward and t h e sampled n e x t s t a t e .\nListing 2 The Room Cleaning domain in pseudo PPL code. 1: t o y s p r o p e r t i e s enum { s i z e , i n p l a c e , p r i c e , g r a s p i n g d i f f i c u l t y} 2: t o y s = [{0 . 1 , t r u e , 0 . 3 , 0 . 1} ,{0 . 8 , f a l s e , 0 . 2 , 0 . 1} ,{6 . 8 , f a l s e , 1 . 5 , 0 . 2} ] \\\\The \u2019 toys \u2019 l i s t i s t h e o n l y s t a t e v a r i a b l e . 3: 4: / / \u2019 o r d e r t o y a c t i o n \u2019 i s t h e i n d e x of t h e t o y t h a t s h o u l d be o r d e r e d . \u2019 s \u2019 i s t h e s t a t e we want t o sample ( t h e n e x t s t a t e and reward )\nfrom . 5: f u n c t i o n s a m p l e n e x t s t a t e ( o r d e r t o y a c t i o n , s ) 6: r eward = 0 7: t o y = s . t o y s [ o r d e r t o y a c t i o n ] / / Get t h e t o y t h e r o b o t s h o u l d h a n d l e . 8: i f n o t t o y [ i n p l a c e ] / / I f t o y n o t i n p l a c e 9: i f s a m p l e b e r n o u l l i ( t o y [ g r a s p i n g d i f f i c u l t y ] ) / / Sample i f t h e t o y f a l l s and b r e a k s based on i t s g r a s p i n g d i f f i c u l t y .\n10: r eward = \u2212 t o y [ p r i c e ] / / Give a n e g a t i v e reward based on t h e t o y p r i c e . 11: s . t o y s . remove ( t o y ) / / Remove t h e broken t o y from t h e t o y s l i s t . 12: e l s e 13: t o y [ i n p l a c e ] = t r u e / / I f t h e r o b o t c o u l d g r a s p t h e t o y , i t i s s a i d t o be i n p l a c e . 14: 15: f o r e a c h t o y i n s . t o y s / / Give a p o s i t i v e ( n e g a t i v e ) reward f o r each t o y ( n o t ) i n p l a c e by i t s s i z e . 16: i f t o y [ i n p l a c e ] 17: r eward = reward + t o y [ s i z e ] 18: e l s e 19: r eward = reward \u2212 t o y [ s i z e ] 20: 21: n u m b e r o f t o y s t o a d d = s a m p l e d i s c r e t e ( 0 . 4 , 0 . 3 , 0 . 3 ) / / Sample t h e number o f new t o y s found ( z e r o : 0 . 4 , one : 0 . 3 , o r two : 0 . 3 ) . 22: f o r 1 t o n u m b e r o f t o y s t o a d d / / Add t o y s as t h e amount found . 23: new toy = { a b s o l u t e v a l u e ( s a m p l e n o r m a l (1 , 1 ) , s a m p l e b e r n o u l l i ( 0 . 2 ) , 24: s a m p l e u n i f o r m (0 , 4 ) , s a m p l e u n i f o r m (0 , 0 . 4 )} / / Sample t h e new t o y p r o p e r t i e s . 25: s . t o y s . append ( new toy ) / / Add t h e new t o y t o t h e t o y s l i s t . 26: r e t u r n reward , s / / R e t u rn t h e reward and t h e sampled n e x t s t a t e .\nListing 3 The Locusts Swarm domain in RDDL code. 1: / / s w a r m o f l o c u s t s : c e l l s map 2: / / 3: / / 3|@c1|@c2|@c3| 4: / / 2|@c4|@c5|@c6| 5: / / 1|@c7|@c8|@c9| 6: / / \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 7: / / y / x 1 2 3 8: domain s w a r m o f l o c u s t s 2 { 9: t y p e s { c e l l : o b j e c t ; d i r e c t i o n : {@up , @down , @le f t , @r igh t} ;} ;\n10: 11: p v a r i a b l e s { 12: x l o c a t i o n ( c e l l ) : {non \u2212 f l u e n t , i n t , d e f a u l t = \u22121} ; 13: y l o c a t i o n ( c e l l ) : {non \u2212 f l u e n t , i n t , d e f a u l t = \u22121} ; 14: t o n s o f p l a n t s ( c e l l ) : { s t a t e \u2212 f l u e n t , r e a l , d e f a u l t = 1 0 . 0} ; 15: s w a r m d i s a b l e d : { s t a t e \u2212 f l u e n t , boo l , d e f a u l t = f a l s e} ; 16: s w a r m l o c a t i o n x : { s t a t e \u2212 f l u e n t , i n t , d e f a u l t = 1} ; s w a r m l o c a t i o n y : { s t a t e \u2212 f l u e n t , i n t , d e f a u l t = 1} ; 17: t o t a l n e i g h b o r s p l a n t s 1 : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 1} ; 18: swarm move1 : { i n t e rm \u2212 f l u e n t , d i r e c t i o n , l e v e l = 2} ; 19: s w a r m l o c a t i o n 1 x : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 3} ; 20: s w a r m l o c a t i o n 1 y : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 3} ; 21: t o n s o f p l a n t s 1 ( c e l l ) : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 4} ; 22: t o t a l n e i g h b o r s p l a n t s 2 : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 5} ; 23: swarm move2 : { i n t e rm \u2212 f l u e n t , d i r e c t i o n , l e v e l = 6} ; 24: s w a r m l o c a t i o n 2 x : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 7} ; 25: s w a r m l o c a t i o n 2 y : { i n t e rm \u2212 f l u e n t , r e a l , l e v e l = 7} ; 26: d i s a b l e s w a r m a c t ( c e l l ) : { a c t i o n \u2212 f l u e n t , boo l , d e f a u l t = 0} ;} ; 27: 28: c p f s{ 29: t o t a l n e i g h b o r s p l a n t s 1 = sum {? c : c e l l} [ i f ( ( ( s w a r m l o c a t i o n x +1) == x l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n y == y l o c a t i o n ( ? c ) ) | 30: ( ( s w a r m l o c a t i o n x \u2212 1) == x l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n y == y l o c a t i o n ( ? c ) ) | 31: ( ( s w a r m l o c a t i o n y + 1) == y l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n x == x l o c a t i o n ( ? c ) ) | 32: ( ( s w a r m l o c a t i o n y \u22121) == y l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n x == x l o c a t i o n ( ? c ) ) ) t h e n t o n s o f p l a n t s ( ? c ) e l s e 0 . 0 ] ; 33: swarm move1 = D i s c r e t e ( d i r e c t i o n , 34: @up : i f ( s w a r m l o c a t i o n y == 3) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n y + 1) == y l o c a t i o n ( ? c ) \u02c6 35: s w a r m l o c a t i o n x == x l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s ( ? c ) e l s e 0 . 0 ] ) / t o t a l n e i g h b o r s p l a n t s 1 , 36: @down : i f ( s w a r m l o c a t i o n y == 1) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n y \u2212 1) == y l o c a t i o n ( ? c ) \u02c6 37: s w a r m l o c a t i o n x == x l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s ( ? c ) e l s e 0 . 0 ] / t o t a l n e i g h b o r s p l a n t s 1 ) , 38: @lef t : i f ( s w a r m l o c a t i o n x == 1) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n x \u2212 1) == x l o c a t i o n ( ? c ) \u02c6 39: s w a r m l o c a t i o n y == y l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s ( ? c ) e l s e 0 . 0 ] / t o t a l n e i g h b o r s p l a n t s 1 ) , 40: @right : i f ( s w a r m l o c a t i o n x == 3) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n x + 1) == x l o c a t i o n ( ? c ) \u02c6 41: s w a r m l o c a t i o n y == y l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s ( ? c ) e l s e 0 . 0 ] / t o t a l n e i g h b o r s p l a n t s 1 ) ) ; 42: s w a r m l o c a t i o n 1 x = s w i t c h ( swarm move1 ){c a s e @up : s w a r m l o c a t i o n x , c a s e @down : s w a r m l o c a t i o n x , 43: c a s e @right : s w a r m l o c a t i o n x + 1 , c a s e @le f t : s w a r m l o c a t i o n x \u2212 1} ; 44: s w a r m l o c a t i o n 1 y = s w i t c h ( swarm move1 ){c a s e @up : s w a r m l o c a t i o n y + 1 , c a s e @down : s w a r m l o c a t i o n y \u2212 1 , 45: c a s e @right : s w a r m l o c a t i o n y , c a s e @le f t : s w a r m l o c a t i o n y} ; 46: t o n s o f p l a n t s 1 ( ? c ) = i f ( s w a r m l o c a t i o n 1 y == y l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n 1 x == x l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s ( ? c ) * 0 . 5 47: e l s e t o n s o f p l a n t s ( ? c ) ; 48: t o t a l n e i g h b o r s p l a n t s 2 =sum {? c : c e l l} [ i f ( ( ( s w a r m l o c a t i o n 1 x +1) == x l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n 1 y == y l o c a t i o n ( ? c ) ) | 49: ( ( s w a r m l o c a t i o n 1 x \u2212 1) == x l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n 1 y == y l o c a t i o n ( ? c ) ) | 50: ( ( s w a r m l o c a t i o n 1 y + 1) == y l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n 1 x == x l o c a t i o n ( ? c ) ) | 51: ( ( s w a r m l o c a t i o n 1 y \u22121) == y l o c a t i o n ( ? c ) \u02c6 s w a r m l o c a t i o n 1 x == x l o c a t i o n ( ? c ) ) ) t h e n t o n s o f p l a n t s 1 ( ? c ) e l s e\n0 . 0 ] ; 52: 53: swarm move2 = D i s c r e t e ( d i r e c t i o n , 54: @up : i f ( s w a r m l o c a t i o n 1 y == 3) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n 1 y + 1) == y l o c a t i o n ( ? c ) \u02c6 55: s w a r m l o c a t i o n 1 x == x l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s 1 ( ? c ) e l s e 0 . 0 ] ) / t o t a l n e i g h b o r s p l a n t s 2 , 56: @down : i f ( s w a r m l o c a t i o n 1 y == 1) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n 1 y \u2212 1) == y l o c a t i o n ( ? c ) \u02c6 57: s w a r m l o c a t i o n 1 x == x l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s 1 ( ? c ) e l s e 0 . 0 ] / t o t a l n e i g h b o r s p l a n t s 2 ) , 58: @lef t : i f ( s w a r m l o c a t i o n 1 x == 1) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n 1 x \u2212 1) == x l o c a t i o n ( ? c ) \u02c6 59: s w a r m l o c a t i o n 1 y == y l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s 1 ( ? c ) e l s e 0 . 0 ] / t o t a l n e i g h b o r s p l a n t s 2 ) , 60: @right : i f ( s w a r m l o c a t i o n 1 x == 3) t h e n 0 . 0 e l s e ( sum {? c : c e l l} [ i f ( ( s w a r m l o c a t i o n 1 x + 1) == x l o c a t i o n ( ? c ) \u02c6 61: s w a r m l o c a t i o n 1 y == y l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s 1 ( ? c ) e l s e 0 . 0 ] / t o t a l n e i g h b o r s p l a n t s 2 ) ) ; 62: s w a r m l o c a t i o n 2 x = s w i t c h ( swarm move2 ){c a s e @up : s w a r m l o c a t i o n 1 x , c a s e @down : s w a r m l o c a t i o n 1 x , 63: c a s e @right : s w a r m l o c a t i o n 1 x + 1 , c a s e @ le f t : s w a r m l o c a t i o n 1 x \u2212 1} ; 64: s w a r m l o c a t i o n 2 y = s w i t c h ( swarm move2 ){c a s e @up : s w a r m l o c a t i o n 1 y + 1 , c a s e @down : s w a r m l o c a t i o n 1 y \u2212 1 , 65: c a s e @right : s w a r m l o c a t i o n 1 y , c a s e @ le f t : s w a r m l o c a t i o n 1 y} ; 66: s w a r m l o c a t i o n x \u2019= s w a r m l o c a t i o n 2 x ; 67: s w a r m l o c a t i o n y \u2019= s w a r m l o c a t i o n 2 y ; 68: t o n s o f p l a n t s \u2019 ( ? c ) = i f ( s w a r m d i s a b l e d ) t h e n t o n s o f p l a n t s ( ? c ) e l s e ( i f ( s w a r m l o c a t i o n 2 y == y l o c a t i o n ( ? c ) \u02c6 69: s w a r m l o c a t i o n 2 x == x l o c a t i o n ( ? c ) ) t h e n t o n s o f p l a n t s 1 ( ? c ) * 0 . 5 e l s e t o n s o f p l a n t s 1 ( ? c ) ) ; 70: s w a r m d i s a b l e d \u2019 = i f ( s w a r m d i s a b l e d ) t h e n s w a r m d i s a b l e d e l s e e x i s t s {? c : c e l l} ( ( d i s a b l e s w a r m a c t ( ? c ) == 1) \u02c6 71: x l o c a t i o n ( ? c ) == s w a r m l o c a t i o n 2 x \u02c6 y l o c a t i o n ( ? c ) == s w a r m l o c a t i o n 2 y ) ; 72: } ; 73: r eward = [ sum {? c : c e l l} ( t o n s o f p l a n t s \u2019 ( ? c ) ) ] ; 74: } 75: 76: non \u2212 f l u e n t s s w a r m o f l o c u s t s 2 n f { 77: domain = s w a r m o f l o c u s t s 2 ; 78: o b j e c t s { c e l l : { c e l l 1 , c e l l 2 , c e l l 3 , c e l l 4 , c e l l 5 , c e l l 6 , c e l l 7 , c e l l 8 , c e l l 9} ;} ; 79: non \u2212 f l u e n t s { 80: x l o c a t i o n ( c e l l 1 ) =1; x l o c a t i o n ( c e l l 2 ) =2; x l o c a t i o n ( c e l l 3 ) =3 ; x l o c a t i o n ( c e l l 4 ) =1 ; x l o c a t i o n ( c e l l 5 ) =2; x l o c a t i o n ( c e l l 6 ) =3; 81: x l o c a t i o n ( c e l l 7 ) =1; x l o c a t i o n ( c e l l 8 ) =2; x l o c a t i o n ( c e l l 9 ) =3 ; y l o c a t i o n ( c e l l 1 ) =3 ; y l o c a t i o n ( c e l l 2 ) =3; y l o c a t i o n ( c e l l 3 ) =3; 82: y l o c a t i o n ( c e l l 4 ) =2; y l o c a t i o n ( c e l l 5 ) =2; y l o c a t i o n ( c e l l 6 ) =2 ; y l o c a t i o n ( c e l l 7 ) =1 ; y l o c a t i o n ( c e l l 8 ) =1; y l o c a t i o n ( c e l l 9 ) =1;} ; 83: } 84: 85: i n s t a n c e i n s t s w a r m o f l o c u s t s 2 { 86: domain = s w a r m o f l o c u s t s 2 ; non \u2212 f l u e n t s = s w a r m o f l o c u s t s 2 n f ; 87: i n i t \u2212 s t a t e { s w a r m l o c a t i o n x =2; s w a r m l o c a t i o n y =2;} ; 88: max\u2212nondef \u2212 a c t i o n s = 1 ; h o r i z o n = 9 ; d i s c o u n t = 0 . 9 ;}\nAcknowledgements This work was supported by the Israel Ministry of Science and Technology Grant 3-15626, ISF Grant 1651/19 and by the Lynn and William Frankel Center for Computer Science.\nReferences Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finitetime Analysis of the Multiarmed Bandit Problem. Machine Learning, 47(2-3): 235\u2013256. Ba\u0308ckstro\u0308m, C.; and Nebel, B. 1995. Complexity Results for SAS+ Planning. Comput. Intell., 11: 625\u2013656. Baydin, A. G.; Pearlmutter, B. A.; Radul, A. A.; and Siskind, J. M. 2018. Automatic Differentiation in Machine Learning: a Survey. Journal of Machine Learning Research, 18(153): 1\u201343. Bhandari, J.; and Russo, D. 2021. On the Linear Convergence of Policy Gradient Methods for Finite MDPs. In Banerjee, A.; and Fukumizu, K., eds., Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, 2386\u20132394. PMLR. Bingham, E.; Chen, J. P.; Jankowiak, M.; Obermeyer, F.; Pradhan, N.; Karaletsos, T.; Singh, R.; Szerlip, P.; Horsfall, P.; and Goodman, N. D. 2019. Pyro: deep universal probabilistic programming. Journal of Machine Learning Research, 20(28): 1\u20136. Bulychev, P. E.; David, A.; Larsen, K. G.; Mikucionis, M.; Poulsen, D. B.; Legay, A.; and Wang, Z. 2012. UPPAALSMC: Statistical Model Checking for Priced Timed Automata. In Wiklicky, H.; and Massink, M., eds., Proceedings 10th Workshop on Quantitative Aspects of Programming Languages and Systems, QAPL 2012, Tallinn, Estonia, 31 March and 1 April 2012, volume 85 of EPTCS, 1\u201316. Camacho, A.; Icarte, R. T.; Klassen, T. Q.; Valenzano, R.; and McIlraith, S. A. 2019. LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI). Carpenter, B.; Gelman, A.; Hoffman, M.; Lee, D.; Goodrich, B.; Betancourt, M.; Brubaker, M.; Guo, J.; Li, P.; and Riddell, A. 2017. Stan: a probabilistic programming language. Journal of Statistical Software, Articles, 76(1): 1\u201332. Chen, M.; Frazzoli, E.; Hsu, D.; and Lee, W. S. 2016. POMDP-lite for robust robot planning under uncertainty. In 2016 IEEE International Conference on Robotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, 5427\u20135433. Fikes, R. E.; and Nilsson, N. 1971. STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving. Artificial Intelligence, 2: 189\u2013208. Fox, M.; and Long, D. 2003. PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains. JAIR,"
        },
        {
            "heading": "20: 61\u2013124.",
            "text": "Gelman, A.; Carlin, J.; Stern, H.; and Rubin, D. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. ISBN 9781420057294.\nGerevini, A.; Haslum, P.; Long, D.; Saetti, A.; and Dimopoulos, Y. 2009. Deterministic planning in the fifth international planning competition: PDDL3 and experimental evaluation of the planners. Artif. Intell., 173(5-6): 619\u2013668.\nGhahramani, Z. 1997. Learning Dynamic Bayesian Networks. In Giles, C. L.; and Gori, M., eds., Adaptive Processing of Sequences and Data Structures, International Summer School on Neural Networks, \u201dE.R. Caianiello\u201d, Vietri sul Mare, Salerno, Italy, September 6-13, 1997, Tutorial Lectures, volume 1387 of Lecture Notes in Computer Science, 168\u2013197. Springer.\nGoodman, N. D.; Mansinghka, V. K.; Roy, D. M.; Bonawitz, K.; and Tenenbaum, J. B. 2008. Church: a language for generative models. In Proceedings of Uncertainty in Artificial Intelligence.\nGoodman, N. D.; and Stuhlmu\u0308ller, A. 2014. The Design and"
        },
        {
            "heading": "Implementation of Probabilistic Programming Languages.",
            "text": "Electronic; retrieved 2019/3/29.\nHenriques, D.; Martins, J. G.; Zuliani, P.; Platzer, A.; and Clarke, E. M. 2012. Statistical Model Checking for Markov Decision Processes. In Ninth International Conference on Quantitative Evaluation of Systems, QEST 2012, London, United Kingdom, September 17-20, 2012, 84\u201393. IEEE Computer Society.\nKucukelbir, A.; Tran, D.; Ranganath, R.; Gelman, A.; and Blei, D. M. 2017. Automatic differentiation variational inference. J. Mach. Learn. Res., 18(1): 430\u2013474.\nKwiatkowska, M.; Norman, G.; and Parker, D. 2011. PRISM 4.0: Verification of Probabilistic Real-time Systems. In Proc. 23rd International Conference on Computer Aided Verification (CAV\u201911), volume 6806 of LNCS, 585\u2013591. Springer.\nLipovetzky, N.; and Geffner, H. 2012. Width and Serialization of Classical Planning Problems. In Raedt, L. D.; Bessiere, C.; Dubois, D.; Doherty, P.; Frasconi, P.; Heintz, F.; and Lucas, P. J. F., eds., ECAI 2012 - 20th European"
        },
        {
            "heading": "Conference on Artificial Intelligence. Including Prestigious Applications of Artificial Intelligence (PAIS-2012) System Demonstrations Track, Montpellier, France, August 27-31 ,",
            "text": "2012, volume 242 of Frontiers in Artificial Intelligence and Applications, 540\u2013545. IOS Press.\nMansinghka, V.; Selsam, D.; and Perov, Y. N. 2014. Venture: a higher-order probabilistic programming platform with programmable inference. CoRR, abs/1404.0099.\nMcDermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram, A.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. PDDL \u2013 The Planning Domain Definition Language \u2013 Version 1.2. Technical report, TR-98-003, Yale Center for Computational Vision and Contro.\nMeuleau, N.; Peshkin, L.; Kim, K.-E.; and Kaelbling, L. P. 1999. Learning finite-state controllers for partially observable environments. In conference on Uncertainty in artificial intelligence (UAI), 427\u2013436.\nMinka, T.; Winn, J.; Guiver, J.; and Knowles, D. 2010. Infer .NET 2.4, microsoft research cambridge.\nMurray, L. M.; and Scho\u0308n, T. B. 2018. Automated learning with a probabilistic programming language: Birch. Annual Reviews in Control, 46: 29 \u2013 43. Paige, B.; Wood, F.; Doucet, A.; and Teh, Y. 2014. Asynchronous anytime sequential Monte Carlo. In Advances in Neural Information Processing Systems. Pe\u0301ret, L.; and Garcia, F. 2004. On-Line Search for Solving Markov Decision Processes via Heuristic Sampling. In Proceedings of the 16th European Conference on Artificial Intelligence, 530\u2013534. Poupart, P.; and Boutilier, C. 2004. Bounded Finite State Controllers. In Proceedings of the 16th International Conference on Neural Information Processing Systems. MIT Press. Puterman, M. L. 2005. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley. Rainforth, T.; Naesseth, C. A.; Lindsten, F.; Paige, B.; van de Meent, J.-W.; Doucet, A.; and Wood, F. 2016. Interacting particle Markov chain Monte Carlo. In Proceedings of the 33rd International Conference on Machine Learning, volume 48 of JMLR: W&CP. Sanner, S. 2010. Relational Dynamic Influence Diagram Language (RDDL): Language Description. Unpublished. Available from Author. Silver, D.; and Veness, J. 2010. Monte-Carlo Planning in Large POMDPs. In NIPS\u201910. Srivastava, S.; Russell, S.; Ruan, P.; and Cheng, X. 2014. First-Order Open-Universe POMDPs. In Zhang, N. L.; and Tian, J., eds., Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, UAI 2014, Quebec City, Quebec, Canada, July 23-27, 2014, 742\u2013751. AUAI Press. Sutton, R. S.; and Barto, A. G. 1998. Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press. Tolpin, D.; and Shimony, S. E. 2012. MCTS based on simple regret. In Proceedings of The 26th AAAI Conference on Artificial Intelligence, 570\u2013576. van de Meent, J.-W.; Paige, B.; Tolpin, D.; and Wood, F. 2016. Black-box policy search with probabilistic programs. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, 1195\u20131204. van de Meent, J.-W.; Yang, H.; Mansinghka, V.; and Wood, F. 2015. Particle Gibbs with ancestor sampling for probabilistic programs. In Artificial Intelligence and Statistics. Wertheim, O.; Suissa, D.; and Brafman, R. I. 2022. Towards Plug\u2019nPlay Task-Level Autonomy for Robotics Using POMDPs and Generative Models. In Proceedings of the AREA22 Workshop. Wingate, D.; Goodman, N. D.; Roy, D. M.; Kaelbling, L. P.; and Tenenbaum, J. B. 2011. Bayesian policy search with policy priors. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence, 1565\u20131570. Wingate, D.; and Weber, T. 2013. Automated Variational Inference in Probabilistic Programming. CoRR, abs/1301.1299.\nWood, F.; van de Meent, J.-W.; and Mansinghka, V. 2014. A new approach to probabilistic programming inference. In Artificial Intelligence and Statistics. Yang, L.; Hanrahan, P.; and Goodman, N. D. 2014. Generating efficient MCMC kernels from probabilistic programs."
        },
        {
            "heading": "In Proceedings of the Seventeenth International Conference",
            "text": "on Artificial Intelligence and Statistics, 1068\u20131076. Yang, Q.; Wu, K.; and Jiang, Y. 2007. Learning action models from plan examples using weighted MAX-SAT. Artificial Intelligence, 171(2): 107\u2013143. Ye, N.; Somani, A.; Hsu, D.; and Lee, W. S. 2017. DESPOT: Online POMDP Planning with Regularization. J. Artif. Intell. Res., 58: 231\u2013266. Younes, H.; and Littman, M. 2004. PPDDL1.0: An Extension to PDDL for Expressing Planning Domains with Probabilistic Effects. Technical Report CMU-CS-04-167, Carnegie Mellon University."
        }
    ],
    "title": "Probabilistic Programs as an Action Description Language",
    "year": 2023
}