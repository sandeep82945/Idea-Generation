{
    "abstractText": "We consider the controllability of large-scale linear networked dynamical systems when complete knowledge of network structure is unavailable and knowledge is limited to coarse summaries. We provide conditions under which average controllability of the fine-scale system can be well approximated by average controllability of the (synthesized, reduced-order) coarse-scale system. To this end, we require knowledge of some inherent parametric structure of the fine-scale network that makes this type of approximation possible. Therefore, we assume that the underlying fine-scale network is generated by the stochastic block model (SBM)\u2014often studied in community detection. We then provide an algorithm that directly estimates the average controllability of the fine-scale system using a coarse summary of SBM. Our analysis indicates the necessity of underlying structure (e.g., in-built communities) to be able to quantify accurately the controllability from coarsely characterized networked dynamics. We also compare our method to that of the reduced-order method and highlight the regimes where both can outperform each other. Finally, we provide simulations to confirm our theoretical results for different scalings of network size and density, and the parameter that captures how much community-structure is retained in the coarse summary.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nafiseh Ghoroghchian"
        },
        {
            "affiliations": [],
            "name": "Rajasekhar Anguluri"
        },
        {
            "affiliations": [],
            "name": "Gautam Dasarathy"
        },
        {
            "affiliations": [],
            "name": "Stark C. Draper"
        },
        {
            "affiliations": [],
            "name": "Edward S. Rogers"
        }
    ],
    "id": "SP:1c6c30b0f08353d5cbd2882a75cbec6aeec6176d",
    "references": [
        {
            "authors": [
                "R.F. Betzel",
                "D.S. Bassett"
            ],
            "title": "Multi-scale brain networks",
            "venue": "Neuroimage, vol. 160, pp. 73\u201383, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Gu",
                "F. Pasqualetti",
                "M. Cieslak",
                "Q.K. Telesford",
                "B.Y. Alfred",
                "A.E. Kahn",
                "J.D. Medaglia",
                "J.M. Vettel",
                "M.B. Miller",
                "S.T. Grafton"
            ],
            "title": "Controllability of structural brain networks",
            "venue": "Nature Communications, vol. 6, no. 1, pp. 1\u201310, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T.M. Karrer",
                "J.Z. Kim",
                "J. Stiso",
                "A.E. Kahn",
                "F. Pasqualetti",
                "U. Habel",
                "D.S. Bassett"
            ],
            "title": "A practical guide to methodological considerations in the controllability of structural brain networks",
            "venue": "Journal of Neural Eng., vol. 17, no. 2, p. 026031, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C.N. Heck",
                "D. King-Stephens",
                "A.D. Massey",
                "D.R. Nair",
                "B.C. Jobst",
                "G.L. Barkley",
                "V. Salanova",
                "A.J. Cole",
                "M.C. Smith",
                "R.P. Gwinn"
            ],
            "title": "Two-year seizure reduction in adults with medically intractable partial onset epilepsy treated with responsive neurostimulation: Final results of the rns system pivotal trial",
            "venue": "Epilepsia, vol. 55, no. 3, pp. 432\u2013441, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "P. Benner",
                "S. Gugercin",
                "K. Willcox"
            ],
            "title": "A survey of projection-based model reduction methods for parametric dynamical systems",
            "venue": "SIAM Review, vol. 57, no. 4, pp. 483\u2013531, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "F. Pasqualetti",
                "S. Zampieri",
                "F. Bullo"
            ],
            "title": "Controllability metrics, limitations and algorithms for complex networks",
            "venue": "IEEE Trans. on Control of Network Syst., vol. 1, no. 1, pp. 40\u201352, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Z. Yuan",
                "C. Zhao",
                "Z. Di",
                "W.-X. Wang",
                "Y.-C. Lai"
            ],
            "title": "Exact controllability of complex networks",
            "venue": "Nature Communications, vol. 4, no. 1, pp. 1\u20139, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "G. Baggio",
                "F. Pasqualetti",
                "S. Zampieri"
            ],
            "title": "Energy-aware controllability of complex networks",
            "venue": "Annual Review of Control, Robotics, and Autonomous Systems, vol. 5, no. 1, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Lindmark"
            ],
            "title": "Controllability of Complex Networks at Minimum Cost. PhD thesis, Link\u00f6ping University Electronic Press, 2020",
            "venue": "(visited on 04/24/2022)",
            "year": 2022
        },
        {
            "authors": [
                "G. O\u2019Leary",
                "D.M. Groppe",
                "T.A. Valiante",
                "N. Verma",
                "R. Genov"
            ],
            "title": "NURIP: Neural interface processor for brain-state classification and programmable-waveform neurostimulation",
            "venue": "IEEE Journal of Solid-State Circuits, vol. 53, no. 11, pp. 3150\u2013 3162, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Kassiri",
                "S. Tonekaboni",
                "M.T. Salam",
                "N. Soltani",
                "K. Abdelhalim",
                "J.L.P. Velazquez",
                "R. Genov"
            ],
            "title": "Closed-loop neurostimulators: A survey and a seizure-predicting design example for intractable epilepsy treatment",
            "venue": "IEEE Trans. on Biomedical Circuits and Syst., vol. 11, no. 5, pp. 1026\u20131040, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M.T. Schaub",
                "S. Segarra",
                "J.N. Tsitsiklis"
            ],
            "title": "Blind identification of stochastic block models from dynamical observations",
            "venue": "SIAM Journal on Mathematics of Data Science, vol. 2, no. 2, pp. 335\u2013367, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Xing",
                "X. He",
                "H. Fang",
                "K.H. Johansson"
            ],
            "title": "Community detection for gossip dynamics with stubborn agents",
            "venue": "2020 59th IEEE Conference on Decision and Control, pp. 4915\u20134920, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "E. Abbe"
            ],
            "title": "Community detection and stochastic block models: Recent developments",
            "venue": "The Journal of Machine Learning Research, vol. 18, no. 1, pp. 6446\u20136531, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R.F. Betzel",
                "J.D. Medaglia",
                "D.S. Bassett"
            ],
            "title": "Diversity of meso-scale architecture in human and non-human connectomes",
            "venue": "Nature Communications, vol. 9, no. 1, pp. 1\u201314, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X. Mao",
                "P. Sarkar",
                "D. Chakrabarti"
            ],
            "title": "Estimating mixed memberships with sharp eigenvector deviations",
            "venue": "Journal of the American Statistical Association, no. just-accepted, pp. 1\u201324, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Kalman"
            ],
            "title": "On the general theory of control systems",
            "venue": "IFAC Proceedings Volumes, vol. 1, no. 1, pp. 491\u2013502, 1960. 1st Int. IFAC Congress on Automatic and Remote Control, Moscow, USSR, 1960.",
            "year": 1960
        },
        {
            "authors": [
                "J. Klamka"
            ],
            "title": "Controllability and Minimum Energy Control. Studies in Systems, Decision and Control",
            "year": 2018
        },
        {
            "authors": [
                "S. Pequito",
                "S. Kar",
                "A.P. Aguiar"
            ],
            "title": "Minimum cost input/output design for large-scale linear structural systems",
            "venue": "Automatica, vol. 68, pp. 384\u2013391, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Olshevsky"
            ],
            "title": "Minimal controllability problems",
            "venue": "IEEE Trans. on Control of Network Syst., vol. 1, no. 3, pp. 249\u2013258, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "V. Tzoumas",
                "M.A. Rahimian",
                "G.J. Pappas",
                "A. Jadbabaie"
            ],
            "title": "Minimal actuator placement with bounds on control effort",
            "venue": "IEEE Trans. on Control of Network Syst., vol. 3, no. 1, pp. 67\u201378, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A.S.A. Dilip"
            ],
            "title": "The controllability gramian, the Hadamard product, and the optimal actuator/leader and sensor selection problem",
            "venue": "IEEE Control Syst. Lett., vol. 3, no. 4, pp. 883\u2013888, 2019. 29 Ghoroghchian, Anguluri, et al.: Coarse Controllability of Linear Systems",
            "year": 2019
        },
        {
            "authors": [
                "P.V. Chanekar",
                "E. Nozari",
                "J. Cort\u00e9s"
            ],
            "title": "Energy-transfer edge centrality and its role in enhancing network controllability",
            "venue": "IEEE Trans. on Network Science and Eng., vol. 8, no. 1, pp. 331\u2013346, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Vosughi",
                "C. Johnson",
                "M. Xue",
                "S. Roy",
                "S. Warnick"
            ],
            "title": "Target control and source estimation metrics for dynamical networks",
            "venue": "Automatica, vol. 100, pp. 412\u2013416, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "I. Klickstein",
                "F. Sorrentino"
            ],
            "title": "Control distance and energy scaling of complex networks",
            "venue": "IEEE Trans. on Network Science and Eng., vol. 7, no. 2, pp. 726\u2013736, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "F.L. Cortesi",
                "T.H. Summers",
                "J. Lygeros"
            ],
            "title": "Submodularity of energy related controllability metrics",
            "venue": "53 IEEE Conference on Decision and Control, pp. 2883\u2013288, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M.V. Srighakollapu",
                "R.K. Kalaimani",
                "R. Pasumarthy"
            ],
            "title": "Optimizing network topology for average controllability",
            "venue": "Syst. & Control Lett., vol. 158, p. 105061, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F.L. Cortesi",
                "T.H. Summers",
                "J. Lygeros"
            ],
            "title": "Submodularity of energy related controllability metrics",
            "venue": "53 IEEE Conference on Decision and Control, pp. 2883\u20132888, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "V.M. Preciado",
                "M.A. Rahimian"
            ],
            "title": "Controllability gramian spectra of random networks",
            "venue": "2016 American Control Conference, pp. 3874\u20133879, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S.D. Bopardikar"
            ],
            "title": "A randomized approach to sensor placement with observability assurance",
            "venue": "Automatica, vol. 123, p. 109340, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.C. Antoulas"
            ],
            "title": "Approximation of Large-Scale Dynamical Systems",
            "venue": "Society for Industrial and Applied Mathematics,",
            "year": 2005
        },
        {
            "authors": [
                "B. Moore"
            ],
            "title": "Principal component analysis in linear systems: Controllability, observability, and model reduction",
            "venue": "IEEE Trans. on Automatic Control, vol. 26, no. 1, pp. 17\u201332, 1981.",
            "year": 1981
        },
        {
            "authors": [
                "X. Cheng",
                "J. Scherpen"
            ],
            "title": "Model reduction methods for complex network systems",
            "venue": "Annual Review of Control, Robotics, and Autonomous Syst., vol. 4, pp. 425\u2013453, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Cheng",
                "J.M. Scherpen"
            ],
            "title": "Robust synchronization preserving model reduction of Lur\u2019e networks",
            "venue": "European Control Conference, pp. 2254\u20132259, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Monshizadeh",
                "H.L. Trentelman",
                "M. Kanat Camlibel"
            ],
            "title": "Stability and synchronization preserving model reduction of multi-agent systems",
            "venue": "Syst. & Control Lett., vol. 62, no. 1, pp. 1\u201310, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "H.-J. Jongsma",
                "P. Mlinari\u0107",
                "S. Grundel",
                "P. Benner",
                "H.L. Trentelman"
            ],
            "title": "Model reduction of linear multi-agent systems by clustering with H2 and H\u221e error bounds",
            "venue": "Mathematics of Control, Signals, and Syst., no. 6, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P. Benner",
                "S. Grundel"
            ],
            "title": "Mlinari\u0107, Clustering-Based Model Order Reduction for Nonlinear Network Systems, pp. 75\u201396",
            "venue": "Cham: Springer Int. Publishing,",
            "year": 2021
        },
        {
            "authors": [
                "X. Cheng",
                "Y. Kawano",
                "J.M.A. Scherpen"
            ],
            "title": "Reduction of second-order network systems with structure preservation",
            "venue": "IEEE Trans. on Automatic Control, vol. 62, no. 10, pp. 5026\u20135038, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "N. Monshizadeh",
                "H.L. Trentelman",
                "M.K. Camlibel"
            ],
            "title": "Projection-based model reduction of multi-agent systems using graph partitions",
            "venue": "IEEE Trans. on Control of Network Systems, vol. 1, no. 2, pp. 145\u2013154, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Dulac",
                "E. Gaussier",
                "C. Largeron"
            ],
            "title": "Mixed-membership stochastic block models for weighted networks",
            "venue": "Conference on Uncertainty in Artificial Intelligence, pp. 679\u2013688, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Funke",
                "T. Becker"
            ],
            "title": "Stochastic block models: A comparison of variants and inference methods",
            "venue": "PloS one, vol. 14, no. 4, p. e0215296, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Ghoroghchian",
                "S.C. Draper",
                "R. Genov"
            ],
            "title": "A hierarchical graph signal processing approach to inference from spatiotemporal signals",
            "venue": "2018 29th Biennial Symp. on Commun., pp. 1\u20135, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Ghoroghchian",
                "D.M. Groppe",
                "R. Genov",
                "T.A. Valiante",
                "S.C. Draper"
            ],
            "title": "Node-centric graph learning from data for brain state identification",
            "venue": "IEEE Trans. on Signal and Inf. Process. over Networks, vol. 6, pp. 120\u2013132, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.C. Gilbert",
                "K. Levchenko"
            ],
            "title": "Compressing network graphs",
            "venue": "Proc. of the LinkKDD workshop at the 10th ACM Conference on KDD, vol. 124, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "K.J. Ahn",
                "S. Guha",
                "A. McGregor"
            ],
            "title": "Graph sketches: Sparsification, spanners, and subgraphs",
            "venue": "Proc. of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Syst., pp. 5\u201314, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "G. Dasarathy",
                "P. Shah",
                "B.N. Bhaskar",
                "R.D. Nowak"
            ],
            "title": "Sketching sparse matrices, covariances, and graphs via tensor products",
            "venue": "IEEE Trans. on Inf. Theory, vol. 61, no. 3, pp. 1373\u20131388, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "N. Ghoroghchian",
                "G. Dasarathy",
                "S. Draper"
            ],
            "title": "Graph community detection from coarse measurements: Recovery conditions for the coarsened weighted stochastic block model",
            "venue": "Int. Conference on Artificial Intelligence and Statistics, pp. 3619\u20133627, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. M\u00fcller",
                "H. Weber"
            ],
            "title": "Analysis and optimization of certain qualities of controllability and observability for linear dynamical systems",
            "venue": "Automatica, vol. 8, no. 3, pp. 237\u2013246, 1972.",
            "year": 1972
        },
        {
            "authors": [
                "G. Yan",
                "J. Ren",
                "Y.-C. Lai",
                "C.-H. Lai",
                "B. Li"
            ],
            "title": "Controlling complex networks: How much energy is needed",
            "venue": "Physical Review Lett., vol. 108, no. 21, p. 218703, 2012. 30 Ghoroghchian, Anguluri, et al.: Coarse Controllability of Linear Systems",
            "year": 2012
        },
        {
            "authors": [
                "T.H. Summers",
                "F.L. Cortesi",
                "J. Lygeros"
            ],
            "title": "On submodularity and controllability in complex dynamical networks",
            "venue": "IEEE Trans. on Control of Network Syst., vol. 3, no. 1, pp. 91\u2013101, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Ikeda",
                "K. Kashima"
            ],
            "title": "Sparsity-constrained controllability maximization with application to time-varying control node selection",
            "venue": "IEEE Control Syst. Lett., vol. 2, no. 3, pp. 321\u2013326, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Pirani",
                "J.A. Taylor"
            ],
            "title": "Controllability of ac power networks with dc lines",
            "venue": "IEEE Trans. on Power Syst., vol. 36, no. 2, pp. 1649\u20131651, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.L. McGowan",
                "L. Parkes",
                "X. He",
                "O. Stanoi",
                "Y. Kang",
                "S. Lomax",
                "M. Jovanova",
                "P.J. Mucha",
                "K.N. Ochsner",
                "E.B. Falk"
            ],
            "title": "Controllability of structural brain networks and the waxing and waning of negative affect in daily life",
            "venue": "Biological Psychiatry Global Open Science, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Fang",
                "B. Godlewska",
                "R.Y. Cho",
                "S.I. Savitz",
                "S. Selvaraj",
                "Y. Zhang"
            ],
            "title": "Effects of escitalopram therapy on functional brain controllability in major depressive disorder",
            "venue": "Journal of Affective Disorders, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Deng",
                "S. Gu"
            ],
            "title": "Controllability analysis of functional brain networks",
            "venue": "arXiv preprint arXiv:2003.08278, 2020.",
            "year": 2003
        },
        {
            "authors": [
                "B. Recht",
                "M. Fazel",
                "P.A. Parrilo"
            ],
            "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
            "venue": "SIAM review, vol. 52, no. 3, pp. 471\u2013501, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "C.-Y. Chi",
                "W.-C. Li",
                "C.-H. Lin"
            ],
            "title": "Convex optimization for signal processing and communications: From fundamentals to applications",
            "venue": "CRC press,",
            "year": 2017
        },
        {
            "authors": [
                "O. Sporns",
                "R.F. Betzel"
            ],
            "title": "Modular brain networks",
            "venue": "Annual review of psychology, vol. 67, pp. 613\u2013640, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Galhotra",
                "A. Mazumdar",
                "S. Pal",
                "B. Saha"
            ],
            "title": "The geometric block model",
            "venue": "Proc. of the AAAI Conference on Artificial Intelligence, vol. 32, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "G. Rossetti",
                "L. Milli",
                "R. Cazabet"
            ],
            "title": "Cdlib: A python library to extract, compare and evaluate communities from complex networks",
            "venue": "Applied Network Science, vol. 4, no. 1, p. 52, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Aicher",
                "A.Z. Jacobs",
                "A. Clauset"
            ],
            "title": "Learning latent block structure in weighted networks",
            "venue": "Journal of Complex Networks, vol. 3, no. 2, pp. 221\u2013248, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Lin"
            ],
            "title": "On the Dirichlet distribution",
            "venue": "Department of Mathematics and Statistics, Queens University, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Liu",
                "J. Zhang",
                "Y. Liu"
            ],
            "title": "Trace inequalities for matrix products and trace bounds for the solution of the algebraic riccati equations",
            "venue": "Journal of Inequalities and Applications, vol. 2009, pp. 1\u201317, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "S. Boucheron",
                "G. Lugosi",
                "P. Massart"
            ],
            "title": "Concentration inequalities: A nonasymptotic theory of independence",
            "venue": "Oxford university press,",
            "year": 2013
        },
        {
            "authors": [
                "E.E. Tyrtyshnikov"
            ],
            "title": "A brief introduction to numerical analysis",
            "venue": "Springer Science & Business Media,",
            "year": 1997
        },
        {
            "authors": [
                "R. Bardenet",
                "O.-A. Maillard"
            ],
            "title": "Concentration inequalities for sampling without replacement",
            "venue": "Bernoulli, vol. 21, no. 3, pp. 1361\u20131385, 2015. 31",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In this paper we study controllability of networks with discrete-time linear dynamics (hereafter, LTI systems) when our knowledge of system structure is limited to coarse summaries. We are motivated by myriad real-world settings, ranging from power and water systems to brain networks, where system identification must be performed based upon data collected by low-resolution instruments unable to probe fine-scale structure. Our motivating real-world example is the human brain. While efforts are under way to produce a map of a canonical human brain, our knowledge of the brain as an interconnected, network system is not yet to the level of the individual neurons of the entire brain [1]. And yet, modern medical technologies aim to control the brain using coarse MRI based network connectivity [2,3]. For example, novel brain implants designed for epilepsy patients steer the brain away from states that correspond to seizures [4]. The inaccessibility of fine-scale network information may result in wrong control decisions that diverge greatly from those derived from the fine-scale network.\nOur goal is to quantify the controllability of a fine-scale networked LTI system using its coarse summaries and a few of the fine-scale network properties such as community structure (see below). In the presence of full knowledge of the fine-scale system, existing works address this goal by first obtaining a lower-dimensional (coarse-scale) system using various methods under the umbrella term of model order reduction (MOR). Then, they relate the controllability of this coarse-scale system to that of the original fine-scale system [5]. But this approach is inapplicable in settings wherein one does not have access to the fine-scale network. Nonetheless, we may fit a dynamical system for coarse network\n\u2217This paper is based upon work supported by by the Natural Sciences and Engineering Research Council (NSERC) of Canada, including through a Discovery Research Grant; the National Science Foundation under awards OAC-1934766, CCF-2029044, and CCF-2048223; and the National Institutes of Health under the award 1R01GM140468-01.\nar X\niv :2\n20 6.\n10 56\n9v 1\n[ ee\nss .S\nY ]\n2 1\ndata and study the controllability of this system as a proxy for the controllability of the fine-scale system. However, it is not clear when such an approximation is sufficiently accurate. In Section 3, we study this approximation in detail. The second contrast between classical MOR and our setting is best understood in terms of active versus passive MOR. Classical MOR is active in that it determines how to coarsen the system to yield the best reduction. But for us, our knowledge is restricted by the precision of our observations (e.g., owing to the resolution of measurement devices or to anatomical considerations), thereby limiting our ability to devise best reduction schemes. So, passively collected network data is our starting point.\nTo realize our goal, we consider a suitable controllability Gramian based metric. It is well known that the Gramian based metrics help compare the control effort exerted either by two individual nodes or groups of nodes [6\u20139]. We focus on the latter to address the following question. For certain groups of nodes that we can control globally (i.e., not actuating each node individually) suppose that we have access only to their coarse summary; that is, certain combinations of weights of network edges in and across the groups of nodes. Then, we ask which groups influence (or require minimum energy) in controlling the fine-scale system? This joint measuring and actuating is required in epilepsy applications to determine the device (instrument) position to collapse the unstable brain-state oscillations that can lead to seizures [10, 11].\nTo leverage fine-scale network properties (e.g., community structure, sparsity, and node degree) along with coarse summaries, we assume that the fine-scale network is generated by a stochastic block model (SBM). These models find applications in studying communities in social networks and recently a few groups have considered SBMs for network dynamics [12, 13]. It is known that SBMs naturally allow for generating networks with different community structures (e.g., assortative and dis-assortative) [14, 15]. Interestingly, we show that they also naturally allow to study the the relationship between controllability and the synchronization\u2014a measure that quantifies how well the coarse network data are synchronized (or overlapped) with communities. The higher the synchronization, the less information the coarse summary contains regarding the fine-scale network (see Section 2).\nTo quantify the controllability of the fine-scale LTI system, we consider the average controllability metric given by the trace of the controllability Gramian of a discrete-time system with non-negative system matrices [16]. Specifically, we characterize the average controllability vector of a set of systems. Each entry of the vector corresponds to the average controllability of a system with specific group of input (control) nodes as described above. We then devise two competing schemes (see below) to estimate this average controllability vector and compare the true and estimated vectors. Our main contributions are as follows.\n1. In Section 3, we consider an auxiliary, fictitious, linear dynamical system based on the coarse data, which we term the passively reduced-order model (PROM).2 Each node in the fictitious network corresponding to PROM is associated with a group of nodes in the fine-scale network that map to the same coarse measurement. We use the average controllability vector of the PROM to approximate that of the fine-scale system. Notice that this implicitly couples measurement and actuation. We derive a tight upper bound on the approximation-error which goes to zero as the coarse and fine network sizes grow; the network becomes dense; the coverage of the fine network by coarse measurements expands; and the synchronization between the coarse summary data and the underlying community structure increases. If synchronization is not sufficiently high, the error may not approach zero even as the network size increases.\n2. In Section 4, we learn the average controllability vector of the fine-scale system directly from the coarse data without considering an auxiliary system. We develop a novel learning-based algorithm with its roots in a mixed-membership method proposed by [17] for unsupervised learning of the parameters and the community structure in an SBM. We derive a tight upper bound on the estimation error and characterize its convergence in terms of network parameters. Using this bound, we show that the estimation error approaches zero as long as the community structure parameter estimates are accurate and the coverage of the fine network by coarse measurements is sufficiently rich. Although the learning-based technique\u2019s estimate quality is implicitly dependent on synchronization, it has more resilience than the PROM method in low-synchronized scenarios.\n3. In Section 5, we present multiple numerical simulations to show that the estimation errors and their qualitative behavior (in terms of the various network and coarsening parameters) are comparable to the tight error bounds and their qualitative nature we derived in Sections 3 and 4. Importantly, consistent with the theory, simulations show that the synchronization has less impact on the estimation error of the learning-based method. Finally, our empirical investigation reveals that both the approaches studied can outperform each other in terms of estimation error. Specifically, the learning-based approach outperforms its counterpart in regions for which the network density and the coarse network size are not too small, and the coarse measurements are not fully synchronized with communities.\n2The prefix \"passive\" in PROM emphasizes that the auxiliary system is not a reduction of the fine-scale model. Rather the auxiliary system is a low-dimensional fictitious model that we fit to the coarse data."
        },
        {
            "heading": "1.1 Related work",
            "text": "Controllability of Complex Networks: Controllability of dynamical systems has a rich, long history since its inception in Kalman\u2019s work [18]. Several articles and textbook entries were written about its characterizations and manifestations in several applications [19]. In studies concerning network controllability, the two areas that received great attention are determining (a) the minimal set of control nodes to steer a network to a desired state [20\u201324]; and (b) the mimimum control energy associated with a set of (driver) nodes to steer a network to a desired state [6, 8, 9, 25, 26]. The control energy is often quantified using certain scalar metrics of the spectrum of controllability Gramian matrix. Our work falls into the second category and we focus on the average controllability metric. Capitalizing on the sub-modularity property of several control energy metrics, including average energy, a few studies developed greedy algorithms to design network topology that has minimum control energy (see [27, 28]). Further, in [2, 29], the authors studied which group of nodes, when actuated as inputs, can be used to steer the network to an arbitrary target state, and at what cost. Different from the aforementioned works, which focus only on a metric for a group or full set of nodes, we consider a vector of such scalar metrics (specifically, average energy) to study the comparative influence of different groups of nodes. Ours is the first work to characterize this type of error bounds for the controllability of coarse networks, and we contribute to the nascent field of analyzing controllability/obseravability metrics for random linear systems [30, 31].\nMOR for Complex Networks: There is a matured theory for MOR methods for dynamical systems [5, 32, 33]. However, MOR methods for network dynamics are limited. We briefly review some of the recent developments and recommend [34] for further reading. Methods based on reducing the dimension of individual subsystems in a network were described in [35, 36], where as methods based on reducing the number of nodes in a network using clustering and projection were described in [37\u201340]. Specifically, the latter methods typically rely on Petrov-Galerkin approximation, where projection matrices are constructed to retain certain structural aspects (e.g., clusters) in the reduced network. Although our coarsening operation relies on a given projection matrix, our goal is not to obtain a reduced-order system that well-approximate, or retains some spatial structure of, the high-dimensional system. Rather we quantify the average controllability of the fine-scale network using a coarse network only. Finally, in the literature, the model reduction error is quantified using the system H2 and H\u221e norms [37]. Instead, we use the `1-norm to compare the average controllability of a group of nodes in fine-scale and their counterpart in the coarse network.\nCoarsened SBM as a generative process: The SBM and its variants provide a powerful modeling framework to facilitate fundamental understanding of graph community organization and have found several applications, including social and power networks (see [14, 41, 42] and references therein). Several studies assume the knowledge of fine-scale network, which is a reasonable assumption for networks with a few thousand nodes. Nonetheless, there are several networks (eg., brain networks) with millions of nodes, and not all the network connections are available for a analyst to do inference (e.g., control or estimation). However, analyst can still have access to summaries of connections (for eg., restrictions in brain signal acquisition from macro-contacts, and imaging data [43,44]). While the study of recovering graph structures from coarsened measurements has received some attention in the past [45\u201347], the study of extracting community structure from coarse summaries is recent. In [48], the authors extended the SBM for community detection [14], to lay out a framework for a coarsened, weighted variant of the SBM. We build off those results in this paper.\nNotation: We denote vectors and matrices using bold faced small and upper case letters. For Z = [Zuv] \u2208 Rn\u00d7n, define \u2016Z\u2016\u221e = max1\u2264u\u2264n \u2211n v=1 |Zuv|; \u2016Z\u2016max = maxu,v |Zuv|; \u2016Z\u20161,1 = \u2211n u=1 \u2211n v=1 |Zuv|; and \u2016Z\u20162 =\u221a\n\u03bbmax(ZTZ), where \u03bbmax is the maximum eigenvalue. Denote the spectral radius of Z by \u03c1(Z) = maxi{|\u03bbi(Z)|}, where \u03bbi is the i-th eigenvalue; diag(Z) = [Z11, . . . ,Znn]T \u2208 Rn; and Diag(Z) sets the off-diagonal entries of Z to zero. The matrix inequality Z1 \u2264 Z2 implies element wise inequality. Denote n dimensional all ones and all zero vectors by 1n and 0n. For z = [zu] \u2208 Rn, define supp(z) as the set of indices i for which zi 6= 0; \u2016z\u20161 = \u2211n u=1 |zu|. For a positive integer n, we denote [n] , {1, . . . , n}. The cardinality of a set V is denoted by |V|. Vector ei is a standard basis vector containing all 0\u2019s except for its i-th element that is 1. The Hadamard (element-wise) product is denoted by . We write f(n) = O(h(n)) iff there exist positive reals c0 and n0 such that |f(n)| \u2264 c0h(n) for all n \u2265 n0. Conversely, f(n) = \u2126(h(n)) iff there exist c0, n0 > 0 such that |f(n)| \u2265 c0h(n) for all n \u2265 n0. [a, b] shows the interval (set of real numbers) between scalar values a and b."
        },
        {
            "heading": "2 Preliminaries and Problem Statement",
            "text": "In this section, we introduce a class of stable LTI systems that evolve on networks with randomly generated edge weights. We then describe how we use the SBM to generate random fine networks with community structure, and how we model the coarse measurements of these networks. We then formulate our problem statement."
        },
        {
            "heading": "2.1 Dynamical systems and controllability",
            "text": "A network is an un-directed graph G , (V, E), with node set V , {1, . . . , n} and edge set E \u2286 V \u00d7 V . Each edge (u, v) \u2208 E has a weight Auv = Avu \u2208 R. The weighted symmetric adjacency matrix of G is A , [Auv], where Auv = Auv = 0 whenever Auv /\u2208 E . A random network is an un-directed graph where the [Auv] are random variables. For a network G with adjacency matrix A, associate state xi[t] \u2208 R to the i-th node at time t. The states evolve according to LTI dynamics as\nx[t+ 1] = Anomx[t] + Bu[t] , \u2200 t \u2208 N. (1)\nThe state x[t] = [x1[t], . . . , xn[t]]T is steered by the input u[t] \u2208 Rn. The normalized state adjacency matrix Anom , A/( + \u03c1(A)) is such that the spectral radius \u03c1(Anom) < 1,\u2200 > 0; see [3]. The input matrix B = Diag(b) with b \u2208 {0, 1}n, where we callK = supp(b) \u2282 [n] the control node set, determines which nodes receive control input. Note that Bu[t] is equal to ZuK[t], where Z is a rectangular matrix composed of columns of B indexed by K, and uK is a K-length vector. We stick with the former notation in (1) to enhance readability of notations in the next sections. For instance, for B = Diag(1n1 ,0n\u2212n1), the input enters the network through control nodes set K = {1, . . . , n1}. Average controllability is a widely studied metric of controllability that quantifies the difficulty of controlling a network [2, 6, 49] defined as tr [CT (Anom,B)], where\nCT (Anom,B) = T\u22121\u2211 `=0 (Anom) `BBT(Anom) ` (2)\nis the T -step controllability Gramian of (1). By construction CT (Anom,B) 0, and hence, tr [CT (Anom,B)] \u2265 0. When the context is clear, we drop the arguments Anom and B from CT (Anom,B) and write CT . For ease of exposition, we work with the average controllability for the infinite time horizon Gramian C = limT\u2192\u221e CT , which exists when \u03c1(Anom) < 1.\nThe inverse of the average controllability is a lower bound on the minimum energy (in expectation) required to drive the system in (1) from the origin to any target state on the unit sphere. This energy would be \u222b S xTC\u22121x dx/ \u222b S dx = n\u22121tr(C\u22121), where S = {x : \u2016x\u20162 = 1} [8, 49]. It follows that tr(C\u22121) \u2265 1/tr(C). The tightness of the bound is numerically explored in [3, 50]. The higher tr(C) is for a given set of control nodes (indexed by the non-zero columns in B), the smaller the average energy, and the lower the influence that set has on the network. In this light, tr(C) can be interpreted as the overall network controllability proxy in all directions in the state space [51]. Owing to the above discussion and given the ill-posedness of C for high-dimensional networks, the average controllability provides an easily computable proxy for the average energy [28, 52, 53]. Furthermore, because tr(C) equals the energy of the impulse response (or equivalently, the H2-norm of the network system), the average controllability quantifies the capability of grouped input nodes to drive the network to easy-to-reach state with minimum effort [54\u201356].\nWe can interpret the average controllability using the nuclear norm which is the sum of singular values, i.e., the trace of a positive semi-definite matrix [57]. The nuclear norm is the convex envelope of the rank function (that is, the number of rank-one matrices needed to obtain a given matrix [58, page 71]). The higher the nuclear norm, meaning the large average controllability, the harder will be to approximate the Gramian or metrics of the Gramian by those of its reduced-order model. This is one reason why our learning-based approach (see Section 4) that directly estimates the average controllability can perform better."
        },
        {
            "heading": "2.2 The stochastic block model",
            "text": "The stochastic block model (SBMs) is a probabilistic model that produces random graphs with communities. We use this model, as hinted in the introduction, to reveal the role of community structure in the quality of controllability estimation from coarse measured graphs. Formally, let G , (V, E) be the un-directed graph (i.e., the fine graph) with n nodes and random edge weights generated according to the general GSBM(n,Q,d):\nDefinition 1. (General SBM) Given a natural number K, vector d \u2208 [0, 1]K , and a matrix Q \u2208 [0, 1]K\u00d7K , the GSBM(n,Q,d) is a distribution on a graph G of size n such that 1) the G\u2019s node set, denoted by V where |V| = n, is partitioned into K disjoint communities V = \u222aKk=1Vk. The communities have relative sizes d = [d1, . . . dK ] where |Vk| = ndk; and 2) two nodes u \u2208 Vk and v \u2208 Vk\u2032 are joined by an edge with weight Auv \u2208 {0, 1} drawn with probability Qkk\u2032 independently from other edges, for all k, k\u2032 \u2208 [K].\nThe GSBM(n,Q,d) generates a random un-directed graph with K communities with non-identical intra- and crosscommunity connection probabilities specified by Q \u2208 [0, 1]K\u00d7K . An operational interpretation of the General SBM is\nprovided by the community membership matrix P \u2208 RK\u00d7n:\nPkv = { 1 ifv \u2208 Vk 0 otherwise.\n(3)\nFor any k, k\u2032 \u2208 [K], it follows that Auv \u223c Bernoulli(Qk,k\u2032), if Pku = Pk\u2032v = 1. (4)\nIt is immediate that PPT = Diag (|V1|, . . . , |VK |), where |Vk| is the size of the K-th community. Let D , 1nPP T = Diag (d) be the diagonal matrix of relative community sizes.\nTo facilitate analysis and to harness the power of SBM generated networks, we work with high-dimensional networks. We make some assumptions on the scaling of the parameters of the underlying SBM as n\u2192\u221e. Assumption 1. (SBM scaling [14]) There exists a sequence \u03c1n \u2208 (0, 1) and a constant K \u00d7K matrix Q\u25e6 \u2265 0 such that\nQ = \u03c1nQ\u25e6, (5)\nwhere 0 \u2264 [Q\u25e6]kk\u2032 \u2264 1, and \u03c1n \u221a n\u2192\u221e as n\u2192\u221e.\nThe parameter \u03c1n in (5) allows us to regulate the density (or sparsity) of connections in and across communities. For several real-world networks, \u03c1n typically decreases with n [14]. Remark 1. The relative community sizes of the GSBM in Def. 1 are set to d. Another common model is to choose community memberships in an i.i.d. manner from some distribution p. In this model, relative community sizes become random with E[d] = p. The analysis of this paper will hold for such a model except that, when evoking the expectation of the fine matrix A, an outer expectation should be added to account for the randomness in community sizes."
        },
        {
            "heading": "2.3 Fine Graph and Coarse Measurements",
            "text": "In this part, we describe the modeling choice and notations for the coarse measuring process. We assume that the fine graph Gfine with adjacency matrix A is drawn from the General SBM distribution in Definition 1. Moreover, (1) is the fine-scale system dynamics, and we refer to it as Sfine, i.e.\nSfine : x[t+ 1] = Anomx[t] + Bu[t] , \u2200 t \u2208 N. (6) We define the coarse-scale summary of A as\nA\u0303 , WAWT \u2208 Rm\u00d7m, (7)\nwhere W \u2208 Rm\u00d7n is the coarse-measurement matrix whose rows indicate which parts of the graph are measured. In this paper we assume (a) The i-th row of the coarse-measurement matrix W has ri equal non-zero terms which sum to\n1. These non-zero indices are the fine nodes that are measured as a group; and (b) WWT = Diag([1/r1, \u00b7 \u00b7 \u00b7 , 1/rm]), meaning the sets of fine nodes corresponding to each coarse measurement are disjoint. The coarsened matrix A\u0303 is the starting point of our analysis.\nThe matrix A\u0303 can be interpreted as the adjacency matrix of an un-directed coarse graph Gcoarse with m fictitious nodes\u2014referred to as the (coarse or) c-nodes. This interpretation is helpful as we discuss the network dynamics of A\u0303 in Section 3. The regime where the total coverage size \u2211 i\u2208[m] ri n indicates that c-nodes cover the fine graph only\nsparsely. In other words, there may exist (several) fine nodes that do not contribute to A\u0303 (see Fig. 1). Similar coarsening models have been used in the literature (see [47, 48])."
        },
        {
            "heading": "2.4 Problem Statement",
            "text": "Our objective is to quantify the average controllability of Gfine using only Gcoarse (see (7)), and the knowledge of Gfine\u2019s community structure in Definition 1. Furthermore, we do not have access to the way the coarse graph is acquired at the time of decision making; that is, the coarse-measurement matrix W is unknown, meaning we do not know which group of fine nodes are mapped to each c-node.\nLet Anom = A/( + \u03c1(A)) in (6) be such that A is drawn from General SBM given by (4). The input matrix BK , B is diagonal by definition. Let Ki = supp(wi), for i \u2208 [m], be the set (hereafter, group) of nodes coarsened by the i-th row wi of W in (7). As a result, the control node groups {K1, . . . ,Km} match the measured groups.\nRecall that wi is the coarse measuring vector mapping to the i-th c-node. For the Ki-th group with BKi = Diag(wTi ), i \u2208 [m], consider the average controllabity metric \u03b8(i)group,A , tr[C(Anom,BKi)]. Define the m-dimensional group average controllability vector as\n\u03b8group,A , [ \u03b8 (1) group,A, . . . ,\u03b8 (m) group,A ]T . (8)\nNote that \u03b8(i)group,A \u2265 0, due to the positive semi-definiteness of the Gramian C(\u00b7). This vector summarizes the average controllability metrics for all control nodes sets {K1, . . . ,Km}. Thus, \u03b8group,A helps us deduce several quantitative properties that are key to being able to control (6) via BKi . For example, \u03b8group tells us which Kis can drive the network to the desired target state with least control effort.\nUsing only the knowledge of A\u0303 in (7), our goal is to estimate the vector \u03b8group,A in (8). We will develop two approaches. The first, Passively Reduced-Order Model (PROM), is based on the traditional model order reduction (MOR) approach. Here we rely upon a passively reduced-order auxiliary system Scoarse in (10), which is governed by A\u0303, to infer \u03b8group,A. Our second approach is learning-based. We directly estimate \u03b8group,A using a clustering-based mixed-membership community-learning algorithm (see Section 4). In the latter case, we bypass the intermediate step of dealing with a reduced-order system. Fig. 1 illustrates the proposed approaches. We use the `1-norm difference between the true- and the estimated controllability vectors to compare the performance of these competing methods (see (12)). Our main results in Sections 3 and 4 are probabilistic because A is a random matrix.\nWe introduce the expected quantities A\u0304 , E[A] and \u00af\u0303A , E[A\u0303]. We later use these to place some mild regulations on Gcoarse. From (4) and (7), the expected quantities evaluate to\nA\u0304 = PTQP and \u00af\u0303 A = \u03a6Q\u03a6T, (9)\nwhere \u03a6 , WPT \u2208 Rm\u00d7K is the coarse community membership matrix, and \u03a6ik captures how much the i-th c-node overlaps with the K-th community. To make the analysis less cumbersome, in what follows, we let the coarsening matrix has fixed row support (r). The example below illustrates the definition of the matrices P, D, \u03a6, and W. Example 1. For the network shown in Fig. 2-(a), the fine community membership matrix is\nP = [ 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 ] \u2208 {0, 1}3\u00d78.\nThe relative fine-scale community sizes are\nd = [ 3\n8 ,\n1 2 , 1 8 ], D = Diag(d) =  38 0 00 12 0 0 0 18  \u2208 R3\u00d73.\nThe coarse-measurement matrix where r = 2 is\nW =  1 2 1 2 0 0 0 0 0 0 0 0 12 1 2 0 0 0 0\n0 0 0 0 12 1 2 0 0 0 0 0 0 0 0 12 1 2  \u2208 R4\u00d78. The coarse community membership matrix tells us what fractions of each coarse node\u2019s group of fine nodes belong to each underlying community,\n\u03a6 = WPT =  1 0 012 12 00 1 0 0 12 1 2  \u2208 R4\u00d73. Finally, we note that the rows of \u03a6T determine that each community contains what portion of the coarse nodes\u2019 groups of fine nodes. Hence,\n\u03a6T\u03a6\nm =\n 516 116 01 16 6 16 1 16\n0 116 1 16  \u2208 R3\u00d73 is a measure of the correlation of (and cross-correlation between) the (coarse membership) representations of communities (i.e., the rows of \u03a6T). Matrix \u03a6\nT\u03a6 m plays a role in the error bound of Theorem 1 and is discussed in Section 3. Each\nc-node in {1, 3} covers one community; each c-nodes in {2, 4} overlap with two. Remark 2. (Synchronization of community and coarsening): The coarsening operation in Section 2.3 is oblivious to the community structure of Gfine. Thus, the i-th c-node may contain information about multiple communities if |supp(\u03a6i)| > 1 (the subscript denotes the i-th row). Perfect synchronization is the situation in which |supp(\u03a6i)| = 1. This occurs when each c-node covers nodes from only one community."
        },
        {
            "heading": "3 Approximating Group Average controllability: PROM-based Approach",
            "text": "In this section, we define PROM (Passively Reduced-Order Model) as an auxiliary or fictitious network linear dynamics associated with Gcoarse. We then analyze how well the average controllability vector of this reduced system approximates \u03b8group,A in (8). Formally, consider the coarse system Scoarse defined by the dynamics\nScoarse : x\u0303[t+ 1] = A\u0303nomx\u0303[t] + B\u0303u\u0303[t] , (10)\nwhere A\u0303nom = A\u0303/( + \u03c1(A\u0303)), A\u0303 is given by (7). Note that x\u0303[t] \u2208 Rm is not a compressed version of the state x[t] in Sfine. Rather, x\u0303[t] is a fictitious state controlled by the dynamics of the (scaled) matrix A\u0303. This fictitious state is controlled by the input B\u0303u\u0303[t] \u2208 Rm. We refer to this coarse-scale system modeling from the fine-scale system as PROM because the dimension of the coarse system can be much less than that of the fine system, i.e. m n.\nSince each group of nodes that receive control input maps to one c-node, B\u0303i = ei is the coarsened input matrix. Also let \u03b8(i)\ncoarse,A\u0303 ,tr[C(A\u0303nom, B\u0303i)]\u22651 be the average controllability for the i-th node in Gcoarse, which is, by definition,\nlower-bound by 1. Define the coarse average controllability vector: \u03b8coarse,A\u0303 , [ \u03b8 (1) coarse,A\u0303 , . . . ,\u03b8 (m) coarse,A\u0303 ]T \u2208 Rm. (11)\nWe show in Theorem 1 that the \u03b8coarse,A\u0303 well approximates \u03b8group,A under some conditions. To quantify the approximation, we define the `1-error metric:\n\u2206(A, A\u0303), \u2225\u2225\u2225\u2225\u2225 r\u03b8group,A \u2212 1m\u2016r\u03b8group,A \u2212 1m\u20161\u2212 \u03b8coarse,A\u0303 \u2212 1m\u2016\u03b8coarse,A\u0303 \u2212 1m\u20161 \u2225\u2225\u2225\u2225\u2225\n1\n. (12)\nWe prefer \u2206(A, A\u0303) to the usual metric \u2016\u03b8group,A\u2212\u03b8coarse,A\u0303\u20161 for the following reasons. First, the factor of r in the first term of \u2206(A, A\u0303) in (12) accounts for the additional scaling by 1/r in the fine system input matrix BKi = Diag(w T i ) (see Lemma 2 in the appendix). Second, the shift by \u22121 discounts the inherent +1 shift in the average controllability definition (see (24) in the appendix). Third, the denominators in (12) ensure that the error is scale-invariant, i.e. the error is unaffected if all elements of either of the controllability vectors are multiplied by a common scalar factor. Moreover we chose \u2016.\u20161 over other vector norms (like \u2016.\u2016max) as it aggregates the errors when choosing a singular or any combinations of c-nodes to stimulate sequentially.\nThe following matrix will appear in Theorems 1 and contributes to upper bounding the controllability estimation errors\n\u03a5 , D\u2212 1 2 ([ I\u2212 (\u03b9D 12 Q\u25e6D 1 2 )2 ]\u22121 \u2212 I ) D\u2212 1 2 \u2208 RK\u00d7K , (13)\nwhere \u03b9 = 1/( \u03c1nn + \u03c1(Q\u25e6D)) is the normalization factor 1/( + \u03c1(A\u0304)) in (6) after substituting A\u0304 with A, with \u03c1(\u00b7) is the spectral radius, \u03c1n and Q\u25e6 are in Assumption 1. We set the normalized matrix Z = \u03b9D 1 2 Q\u25e6D\n1 2 . Hence, the term[ I\u2212 (\u03b9D 12 Q\u25e6D 1 2 )2 ]\u22121\nin (13) becomes the resolvent of Z, i.e. (zI\u2212Z2)\u22121, evaluated at z = 1. Also, note that matrix D 1 2 Q\u25e6D 1 2 is the intra- and cross-community probability matrix that is re-weighted according to the relative community sizes. Recall that the resolvent captures the direct and indirect effects (via the sum of the powers) of D 1 2 Q\u25e6D 1 2 . As a result, matrix \u03a5 in (13) encapsulates the size-weighted community-connection probability matrix\u2019s direct and indirect impacts. For more information on the significance and the properties of resolvents see [59]. Later, in Lemma 1, we see how [\u03a5]kk moves in tandem (i.e. increase and decrease together) with both \u03b8 (i) group,A\u0304 and \u03b8(i)\ncoarse, \u00af\u0303 A\nfor i-th c-nodes that\nare perfectly synchronized with community K. The aformentioned definitions coupled with Lemma 1 in Section 4, contribute to the proof of the next Theorem.\nTheorem 1. (`1 bound on \u03b8group,A \u2212 \u03b8coarse,A\u0303): Let \u2206(A, A\u0303) be defined as in (12) for A \u2208 {0, 1} n\u00d7n and A\u0303 \u2208 Rm\u00d7m, and 0 < \u03b4 < 1 be a constant. Then, under Assumption 1 \u2203n0,m0 \u2208 N such that for n > n0,m > m0\n\u2206(A,A\u0303)=O \u03b7+\u03b7\u0303 \u03c1n + \u221a n mr \u221a log(1\u03b4) 2 + \u2016L\u03a6,\u03a5\u20161 m + \u2225\u2225\u2225\u2225D9\u03a6T\u03a6m \u2225\u2225\u2225\u2225 max  with probability at least 1\u2212 3\u03b4. Here \u03b7 = \u221a log(n 2 \u03b4 )/(2n)\n\u2016QDQ\u2016min ; \u03b7\u0303 =\n\u221a log(m 2\n\u03b4 )/(2m)\n\u2016Q \u03a6T\u03a6m Q\u2016min ; L\u03a6,\u03a5 = \u03a6diag(\u03a5)\u2212 diag(\u03a6\u03a5\u03a6T);\nD = (1/n)PPT; \u03a5 is defined in (13); \u03a6 is defined in (9) and Q in Def. 1.\nTheorem 1 indicates that \u2206(A, A\u0303) approaches zero as: 1) the total coverage proportion, i.e. mr/n, increases; 2) \u03b7 and \u03b7\u0303, that guarantee the bound on \u2206(A, A\u0303) exists with large probability, become negligible; 3) 1m\u2016\u03a6diag(\u03a5) \u2212 diag(\u03a6\u03a5\u03a6T)\u20161 and \u2016D \u2212 \u03a6 T\u03a6 m \u2016max decrease. The latter two terms have to do with synchronization (or overlap), i.e. the extent to which coarse measurements are synchronized (or overlapped) with communities (see Remark 2). The first term 1m\u2016\u03a6diag(\u03a5) \u2212 diag(\u03a6\u03a5\u03a6\nT)\u20161 is equal to zero if perfect synchronization holds. The relationship between the second term, \u2016D\u2212 \u03a6\nT\u03a6 m \u2016max, and community overlap is less straightforward. To understand this, note\nthat matrix \u03a6 T\u03a6 m (see Example 1) quantifies a measure of correlation between (coarse membership) representations of communities. This matrix reveals two natural tradeoffs in the coarsening process. The first is overlap: the offdiagonal elements, [\u03a6\nT\u03a6 m ]kk\u2032 = \u2211 i\u2208[m] \u03a6ik\u03a6ik\u2032/m for k 6= k\u2032, show the extent of synchronization between coarse\nmeasurements and communities (c.f., Remark 2 and Example 1). The second is balancedness: the diagonal elements,\n[\u03a6 T\u03a6 m ]kk = \u2211 i\u2208[m] \u03a6 2 ik/m for all k \u2208 [K], reflect the relative community sizes after coarse measuring. Separately considering diagonal and off-diagonal elements, helps demystify the role of the error term \u2016D\u2212 \u03a6 T\u03a6 m \u2016max in Theorem 1:\u2223\u2223\u2223\u2223[D\u2212 \u03a6T\u03a6m ]k,k\u2032\n\u2223\u2223\u2223\u2223 = { \u2211i\u2208[m] \u03a6ik\u03a6ik\u2032/m if k 6= k\u2032|dk \u2212\u2211i\u2208[m] \u03a62ik/m| if k = k\u2032. (14) From (14) we infer that \u2016D\u2212 \u03a6\nT\u03a6 m \u2016max approaches zero when c-nodes overlap with fewer communities, and c-nodes\nare more evenly spread over communities. The latter means that larger communities get more coarse measurements, and vice versa.\nOverall, we conclude that the PROM-based estimate \u03b8coarse,A\u0303 will well approximate \u03b8group,A when the graph density \u03c1n and number of fine- and c-nodes are sufficiently large, when coarse measurements are sufficiently synchronized with the communities, and when coarse measurements are distributed across communities in a balanced way. In many settings such conditions will be (roughly) satisfied in practice, in particular when communities and coarse measurements are spatially (geographically) localized [60, 61]. This is because nearby nodes tend to be connected and form communities (e.g., social networks of friends and neighbours, regions with distinct functional and structural features in the brain). Moreover, coarse-measuring is likely local (e.g. parcellation of brain imaging data, electrophysiological recording of the brain, aggregation of geographically adjacent nodes). Also, coarse measurements tend to be uniformly distributed across a network and, hence, larger communities are typically measured more frequently."
        },
        {
            "heading": "4 Approximating Group Average Controllability: Learning-based Approach",
            "text": "We now present our learning-based approach to estimate \u03b8group,A. Unlike the PROM-based approach that relies on Scoarse, we directly estimate elements in \u03b8group,A using a mixed-membership (MM) community-learning algorithm [17,62,63]. Specifically, we work with the mixed-membership algorithm of [17] which is not only numerically efficient but comes with strong theoretical guarantees.\nAs mentioned in Section 3, the following lemma is instrumental in the proof of both of our primary results, Theorems 1 and 2. This lemma also provides motivation for our candidate estimator in (16). For these reasons, we delayed the presentation of Lemma 1 until now. In essence, this lemma characterizes the group-average controllability vector associated with the expected matrix A\u0304 in (9).\nLemma 1. (Group Controllability of Expected Dynamics): Define \u03b8group,A\u0304 by replacing A in (8) with the expected matrix A\u0304 given by (9). Let \u03a6 be as in (9), P be in (3), and r be the coverage size of W in (7). Under Assumption 1, we have\n\u03b8group,A\u0304 = 1\nr\n( 1m + 1\nn \u03a6diag(\u03a5)\n) , (15)\nwhere n and m are the dimensions of A and A\u0303; moreover \u03a5 is defined in (13).\nLemma 1 gives us a formula to compute the group average controllability vector associated with the expected matrix A\u0304. Recall that \u03a6i,k = |{v : v \u2208 Vk \u2229 supp(wi)}|/|supp(wi)| (k \u2208 [K], i \u2208 [m]) is the fraction of the i-th c-node\u2019s overlap with community K. From Lemma 1, it follows that \u03b8(i) group,A\u0304 \u221d \u2211 k\u2208[K] \u03a6i,k\u03a5kk. In view of this observation, we infer that c-nodes that have the most overlap with communities of largest \u03a5kk are the most controllable. Moreover, Theorem 6 (see Appendix) shows that \u2206(A, A\u0304) can be arbitrarily small with high probability. This fact coupled with Lemma 1, leads us to propose our candidate estimator\n\u03b8\u0302group , 1m + \u03a6\u0302diag(\u03a5\u0302), (16)\nwhere\n\u03a5\u0302 , D\u0302\u2212 1 2 ([ I\u2212 (\u03b9\u0302D\u0302 12 Q\u0302\u25e6D\u0302 1 2 )2 ]\u22121 \u2212 I ) D\u0302\u2212 1 2 ,\n\u03b9\u0302 = /n+ \u03c1(Q\u0302D\u0302). (17)\nNote that the candidate estimator in (16) is similar to the true group controllability vector in (15), but there are two important differences. First, the true terms \u03a6,Q, and D are replaced with their estimated (hatted) counterparts. Second, the constant coefficients 1/r and 1/n in (15) are omitted since they affect neither the error metric (see (18) wherein 1/r is compensated and 1/n cancels out in both the numerator and denominator) nor the order of nodes when assessed\nbased on their controllability value. The hatted quantities \u03a6\u0302 and Q\u0302 are obtained from Algorithm 2, which takes as input A\u0303 and the number of communities K3. We obtain D\u0302 from Algorithm 1. Importantly, \u03b8\u0302group in (16) is obtained from coarsened matrix A\u0303 and not from the fine scale matrix A. The following assumption is required by [17] for the recovery of \u03a6\u0302, Q\u0302 using their proposed Algorithm 2.\nAssumption 2. (Existence of a perfectly-synchronized c-node): For all k \u2208 [K], there exist at least one coarse node i \u2208 [m] such that \u03a6ik = 1.\nThis means that Gcoarse has at least one perfectly-synchronized (or equivalently \u201cpure\u201d) c-node per community k \u2208 [K]. In real-world networks communities and coarse measurements are often spatially or geographically localized (see Section 3 for a more detailed explanation). Hence, it is reasonable to expect that there is at least one coarse measurement per community that does not overlap with multiple communities. The next theorem bounds the error of our proposed learning-based estimation in (16) using the following metric:\n\u2206\u0302(A\u0303) = \u2225\u2225\u2225\u2225\u2225 \u03b8\u0302group \u2212 1m\u2016\u03b8\u0302group \u2212 1m\u20161 \u2212 r\u03b8group,A \u2212 1m\u2016r\u03b8group,A \u2212 1m\u20161 \u2225\u2225\u2225\u2225\u2225\n1\n. (18)\nTheorem 2. (`1 error bound on \u03b8\u0302group \u2212 \u03b8group,A): Let \u2206\u0302(A\u0303) be defined as in (18) for A \u2208 {0, 1}n\u00d7n and A\u0303 \u2208 Rm\u00d7m, and 0 < \u03b4 < 1 be a constant. Then, under Assumptions 1,2, \u2203n0 \u2208 N such that for n > n0\n\u2206\u0302(A\u0303)=O  \u03b7 \u03c1n + \u221a n mr \u221a log(1 \u03b4 ) 2 + \u2016L\u03a6\u20161,1 m +\u2016LQ\u2016max+\u2016LD\u2016max  holds with probability at least 1\u2212 2\u03b4, where L\u03a6 = \u03a6\u0302\u2212\u03a6, LQ = Q\u0302\u2212Q, and LD = D\u0302\u2212D; \u03b7 = \u221a log(n 2 \u03b4 )/(2n)\n\u2016QDQ\u2016min ; n\nand m are the dimensions of A and A\u0303; \u03a6 is defined in (9) and Q in Def. 1; and D = (1/n)PPT.\nTheorem 2 suggests that for sufficiently large n (smaller \u03b7 which yields high probabilistic guarantees for the bound on \u2206\u0302(A\u0303)) and large total coverage proportion mrn , the estimate \u03b8\u0302group approximates \u03b8group,A to arbitrary precision under the following conditions. First, the graph is dense enough (sufficiently large \u03c1n). Second, the coarse community membership matrix is well estimated (smaller \u2016L\u03a6\u20161,1/m). The third condition needed to yield small \u2206\u0302(A\u0303) is that cross-community probability estimation does not suffer from high error (smaller \u2016LQ\u2016max), and the relative community sizes estimated from the coarse graph are close to the ones in the fine graph (smaller \u2016LD\u2016max). The authors of [17] show that \u2016L\u03a6\u20161,1/m and \u2016LQ\u2016max in Theorem 2 approach zero under some conditions as m \u2192 \u221e. While it is unclear whether these theoretical conditions hold in our setting (given our coarsening process), our simulations show that \u2206\u0302(A\u0303) decreases with an increase in m. The study and characterization of the behavior of \u2016L\u03a6\u20161,1/m and \u2016LQ\u2016max with respect to graph scaling (i.e. \u03c1n) is left for future work.\nThe result of Theorem 2 is important because one can directly infer the most influential control node groups, Ki (one with high average controllability) via the most influential i-th c-nodes, and vice versa. In contrast to the PROM-based error characterization in Theorem 1, the learning-based method\u2019s error does not suffer explicitly from non-synchronization of coarse measurements and communities, although the error is implicitly influenced by the estimation quality of \u03a6,Q,D. Hence, learning-based estimation of group controllability is likely to be more robust than that of the PROM-based when community overlap is present. This hypothesis is validated via simulation in Section 5.\nAlgorithm 1: Direct Inference of the Group Average Controllability\nRequire: estimates \u03a6\u0302 and Q\u0302 from Algorithm 2, and the number of communities K 1: compute D\u0302 = Diag( \u03a6\u0302\nT1m 1K\u03a6\u0302T1m ) and \u03b9\u0302 = n + \u03c1(Q\u0302D\u0302) 2: return \u03b8\u0302group =1m+\u03a6\u0302diag ( D\u0302\u2212 1 2 ([ I\u2212(\u03b9\u0302D\u0302 12 Q\u0302\u25e6D\u0302 1 2 )2 ]\u22121 \u2212I ) D\u0302\u2212 1 2 ) )\n3Algorithm 2, a mixed-membership algorithm, is adapted from [17]. It is a type of spectral clustering method that first performs eigen decomposition of A\u0303 to find the overlapping membership (\u03a6ik) of the fine nodes. A pruning step is included (see steps 4 and 5 in Algorithm 2) that improves algorithm performance.\nAlgorithm 2: Mixed-Membership Community Estimation Algorithm [17]\nRequire: Coarse adjacency matrix A\u0303, number of communities K 1: compute the highest K spectral-decomposition of A\u0303 as V\u0302 \u039b\u0302V\u0302 T and set Spruned = Prune(V\u0302 ) 2: set X = V\u0302 ([m]\\Spruned, :) and compute Spure = Successive Projection Algorithm(XT) 3: set Xpure = X(Spure, :) and compute un-normalized \u03a6\u0302un\u2212nom = V\u0302 X\u22121pure 4: \u03a6\u0302un\u2212nomik \u2190 0 if\u03a6\u0302 un\u2212nom ik < e\n\u221212,\u2200i \u2208 [m], k \u2208 [K] 5: return \u03a6\u0302 = Diag\u22121(\u03a6\u0302un\u2212nom1K)\u03a6\u0302un\u2212nom and Q\u0302 = Xpure\u039b\u0302XTpure"
        },
        {
            "heading": "5 Simulations",
            "text": "We validate our theoretical results by plotting the errors \u2206(A, A\u0303) and \u2206\u0302(A\u0303) and show that these errors are comparable to the bounds we obtained in Theorems 1 and 2 4. We generate A\u223cGSBM(n,Q,d) and then determine A\u0303 = WAWT. For a realization of A\u0303, we obtain the support of each row of W independently, by following a 3-step process. First, we draw a vector of i.i.d elements % \u2208 [K]m from the distribution with probability vector [dk]Kk=1. The i-th element %i \u2208 [K] represent the community with which the i-th c-node has the most overlap. Second, we generate a set of random vectors \u03c0 \u223c Dir(\u03b2, \u00b7 \u00b7 \u00b7 , 1\ufe38\ufe37\ufe37\ufe38\n%ith\n, \u00b7 \u00b7 \u00b7 , \u03b2) \u2208 [0, 1]K , where Dir(.) is the Dirichlet multivariate probability distribution\nparameterized by 0 < \u03b2 < 1 [64]. Dir(\u03b2, \u00b7 \u00b7 \u00b7 , 1\ufe38\ufe37\ufe37\ufe38 %ith , \u00b7 \u00b7 \u00b7 , \u03b2) outputs a vector of size K whose %i-th component is the largest, the strength of other components is proportionate to \u03b2, and its elements sum to 1. The Dirichlet distribution is a distribution over a probability simplex, whose output allocates probabilities to K distinct categories (here communities). Finally, for each communityK where \u03c0k > 0, br \u00b7\u03c0kc fine nodes are randomly chosen from Vnon\u2212selk (i.e., non-selected fine indices in community K) and set as the support of wi. The chosen indices are removed from Vnon\u2212selk . This process continues until the support of all wis are selected. The first step ensures that the community balancedness criterion illustrated in Section 3 is maintained, i.e. the coverage of each community after coarse-measuring remains (in expectation) proportionate to the fine-scale community sizes. The second step controls the level of community overlap (or synchronization); the smaller \u03b2 is chosen, the more coarse nodes get synced with communities, and vice versa (c.f., Remark 2).\nWe set number of fine nodes n = 5000, the overlap parameter \u03b2 = 0.05, and the number of communitiesK = 4. Finally, for Q = \u03c1nQ\u25e6 \u2208 RK\u00d7K , we set [Q\u25e6]kk = p = 0.5, [Q\u25e6]kk\u2032 = q = 0.1 (for k 6= k\u2032). If not specified, the number of c-nodes m = 100, the density scaling parameter \u03c1n = 0.1, and the coverage size per c-node r = 10. Fig. 3a-3c illustrate the qualitative behavior of the errors with respect to changes in the degree of (non-)synchronization in coarse nodes (i.e., \u03b2), m, and \u03c1n. To fairly compare these errors, we also consider a baseline random vector \u00b5 \u2208 [1, 2]m whose elements are generated (drawn) independently from the uniform distribution on the interval [1, 2], independently of A\u0303. We then define the baseline error as\n\u2225\u2225\u2225 \u00b5\u22121m\u2016\u00b5\u22121m\u20161 \u2212 r\u03b8group,A\u22121m\u2016r\u03b8group,A\u22121m\u20161 \u2225\u2225\u22251. We make the following observations. First, both the learning and PROM-based errors are consistently better than the random baseline. Second, the learning-based approach has consistently smaller error than that of the PROM-based approach for large parametric regimes. Third, Fig. 3a shows that all errors monotonically decrease as m increases. Fourth, Fig. 3b shows that errors decrease as \u03c1n increases. This is expected because larger values of \u03c1n result in more distant intra- and cross-community edge densities. This makes community representation extraction and controllability estimation easier. Finally, Fig. 3c demonstrates the higher tolerance of the Learning approach to situations wherein coarse measurements are less synchronized (larger \u03b2), in comparison to the PROM method. From Fig. 3c we observe that in the perfectly synchronized regime on the left of the x-axis, PROM performs very well. As the overlap increases up to the point around \u03b2 ' 0.1, measurements become less synchronized with communities and both estimation errors goes up. After this point, the overlap becomes so high that the controllability values essentially become closer and closer to one another and result in a slight decrease in the PROM estimation error. These results are consistent with our bounds in Theorems 1 and 2.\n4All the results presented in this paper are reproducible. The theoretical findings are annotated and step-by-step elaborated in the appendix. The data generation process and the parameter values used for numerical simulations are fully explained. In addition, the Python code from which the simulation figures are generated is available upon request. The code is also on Github and the repository will go public upon submission acceptance."
        },
        {
            "heading": "6 Conclusion and Future work",
            "text": "We introduced a learning-based framework that exploits the power of community-based representation learning to infer average controllability of fine graphs from coarse summary data. We compared the performance of this approach with that of the Passively Reduced-Order Model (PROM) approach. For both these methods, we derived high probability error bounds on the deviation between the error estimate and ground truth, and validated the theory with numerical simulations. Our results highlight the role of fine- and coarse-network sizes, graph density, and measurement synchronization with communities in modulating the estimation errors. Interestingly, for the latter approach, we show that the estimation error decreases with network size albeit the synchronization bias, which is not the case with the PROM-based approach. For future, we plan to implement our theory to study the role of coarsening, community structures, and synchronization aspects on the controllability of brain networks. Extending the proposed tools and techniques to controllability metrics other than average controllability is left for future work."
        },
        {
            "heading": "A Appendix",
            "text": "This section contains proofs for the main results in Sections 3 and 4.\nNotations: 1K,K denotes an all-one matrix of dimension K by K.\nUseful Matrix Norm Equivalence [65\u201367]: For two real, square, and symmetric matrices Z,Y \u2208 Rn\u00d7n, the following equality and inequalities hold:\n1. If Z,Y are invertible: Z\u22121 \u2212Y\u22121 = Z\u22121(Y \u2212 Z)Y\u22121. 2. tr(ZY) \u2264 \u2016Z\u20162tr(Y). 3. For all norms: if \u2016Z\u2016 < 1\u21d2 \u2016(I\u2212 Z)\u22121\u2016 < 11\u2212\u2016Z\u2016 .\n4. \u2016ZY\u2016max\u2264\u2016Z\u2016\u221e\u2016Y\u2016max. 5. \u2016Z\u2016\u221e \u2264 n\u2016Z\u2016max. 6. \u2016Z\u20162 \u2264 n\u2016Z\u2016max. 7. \u2016ZY\u2016max \u2264 n\u2016Z\u2016max\u2016Y\u2016max\nNote that when Z is square and symmetric, the 2-norm and the spectral radius coincide and can be used interchangeably, i.e. \u03c1(Z) = \u2016Z\u20162. Proposition 3. (Hoeffding\u2019s Inequality) [68]: Let {z`}|z|`=1 be bounded independent random variables. Then the following holds with probability at least 1\u2212 \u03b4, for some \u03b4 > 0:\u2223\u2223\u2223\u2223\u2223\u2223 |z|\u2211 `=1 z`\u2212E  |z|\u2211 `=1 z` \u2223\u2223\u2223\u2223\u2223\u2223\u2264 \u221a |z| 2 log ( 1 \u03b4 ) (max(z)\u2212min(z)) (19)\n\u2264 \u221a |z| 2 log ( 1 \u03b4 ) max(z)\u2212min(z) E [\u2211|z|\n`=1 z` ] E  |z|\u2211 `=1 z`  (20) \u2264 \u221a |z| 2 log( 1 \u03b4 ) max(z)\u2212min(z) min(E [\u2211|z|\n`=1 z`\n] ) E  |z|\u2211 `=1 z` , (21) where max(z) and min(z) respectively denote the maximum and minimum possible values of {z`}|z|`=1.\nWe need the following results to prove Lemma 5 and Theorem 2. Proposition 4. (Monotone Matrix Norms [69]): Suppose the element-wise inequality Z \u2265 Y holds. Then, \u2016Z\u2016p \u2265 \u2016Y\u2016p, for a positive integer p. Proposition 5. (An upper bound on the `1 norm of the difference between two `1-normalized vectors): Let z,y \u2208 Rm be such that z \u2265 1m and y \u2265 1m, where 1m is the all-ones vector. Then,\u2225\u2225\u2225\u2225 z\u2212 1m\u2016z\u22121m\u20161\u2212 y \u2212 1m\u2016y \u2212 1m\u20161 \u2225\u2225\u2225\u2225 1 \u22642 \u2016z\u2212y\u20161 max (\u2016z\u22121m\u20161, \u2016y\u22121m\u20161) .\nProof. Using the triangle inequality, we have:\n\u2225\u2225\u2225\u2225 z\u2212 1\u2016z\u2212 1m\u20161 \u2212 y \u2212 1\u2016y \u2212 1m\u20161 \u2225\u2225\u2225\u2225 1 = m\u2211 i=1 \u2223\u2223\u2223\u2223zi \u2212 1\u2212 (yi \u2212 1) + (yi \u2212 1)\u2016z\u2212 1m\u20161 \u2212 yi \u2212 1\u2016y \u2212 1m\u20161 \u2223\u2223\u2223\u2223\n= m\u2211 i=1 \u2223\u2223\u2223\u2223 zi \u2212 yi\u2016z\u2212 1m\u20161 \u2212 [yi \u2212 1] [\n1 \u2016y \u2212 1m\u20161 \u2212 1 \u2016z\u2212 1m\u20161\n]\u2223\u2223\u2223\u2223\n= m\u2211 i=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 zi \u2212 yi \u2016z\u2212 1m\u20161 \u2212 [yi \u2212 1]  m\u2211 j=1 (zj \u2212 yj) \u2016y \u2212 1m\u20161 \u2016z\u2212 1m\u20161  \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264\nm\u2211 i=1 |zi \u2212 yi|\n\u2016z\u2212 1m\u20161 1 + m\u2211 i=1 |yi \u2212 1| \u2016y \u2212 1m\u20161  = 2 \u2016z\u2212 y\u20161\u2016z\u2212 1m\u20161 . (22)\nBy exchanging the roles of zi and yi, we find a similar bound that combined to (22) gives the inequality in the statement of Lemma.\nA.1 Lemma and Proof: Relationship Between Fine- and Group- Average Controllability\nLet Z be an n\u00d7n system matrix that could be either A or A\u0304 (the expected quantity). Recall the controllability Gramian of the LTI system in (1) has been defined in (2). We define the n-dimensional fine average controllability vector \u03b8fine,Z similar to (8) as\n\u03b8Tfine,Z , [tr[C(Znom, e1)] . . . tr[C(Znom, en)]] , (23)\nwhere ei is the i-th canonical basis vector in Rn. By definition in (2), the following holds:\n\u03b8 (i) fine,Z = tr [C(Znom, ei)] = \u221e\u2211 \u03c4=0 tr(Z\u03c4nomeie T i Z \u03c4 nom)\n= \u221e\u2211 \u03c4=0 [diag(Z2\u03c4nom)]i = 1 + [diag(Z 2 nom)]i + \u00b7 \u00b7 \u00b7\n= [diag ( (I\u2212 Z2nom)\u22121 ) ]i. (24)\nThe series converges to the final equality because \u03c1(Z) \u2264 1. Putting (24) into a vector form gives:\n\u03b8fine,Z =diag (C(Znom, In\u00d7n))=diag ( (I\u2212 Z2nom)\u22121 ) . (25)\nThe following lemma states the group average controllability vector in (8) is a linear mapping of the fine average controllability vector defined above.\nLemma 2. With the notation defined above for Z consider the average controllability vector in (8). Then\n\u03b8group,Z = 1\nr W\u03b8fine,Z.\nProof. We begin by simplifying the group average controllability using its definition in (8) and the definition of Gramian in (2), for a general matrix Z:\n\u03b8 (i) group,Z = tr [ C(Znom,diag(wTi )) ] = tr\n[ \u221e\u2211 \u03c4=0 Z\u03c4nomdiag(w T i )diag(w T i ) TZ\u03c4nom ]\n= tr [ \u221e\u2211 \u03c4=0 Z\u03c4nomdiag((wi wi)T)Z\u03c4nom ]\n(a) =\n1\nr2 tr  \u221e\u2211 \u03c4=0 Z\u03c4nom( \u2211\nv\u2208supp(wi)\neve T v )Z \u03c4 nom  = 1\nr2 \u2211 v\u2208supp(wi) tr [ \u221e\u2211 \u03c4=0 Z\u03c4nomeve T vZ \u03c4 nom ] \ufe38 \ufe37\ufe37 \ufe38\n\u03b8 (v) fine,Z\n= (wi wi)T\u03b8fine,Z, (26)\nwhere (a) is due to the assumption of supp(wi) = r at the end of Section 2.4, for all i \u2208 [m]; denotes the Hadamard product; and \u03b8fine,Z has been defined in (25) in which A is substituted with Z. Putting (26) in vector form concludes the proof.\nThe above relationship states that the group controllability \u03b8group,Z is the mean value of the fine controllability of the groups of nodes that map to each c-node, weighted by the factor 1/r (that is rooted in the definition of the coarse-measurement matrix W).\nThe following results are instrumental in proving Lemma 1 and Lemma 5. Lemma 3. (spectral radius of A\u0304): Let D = 1nPP\nT with \u03c1n be defined as in (5). Then the spectral radius of A\u0304 = PTQP in (9) is given as\n\u03c1(A\u0304) = n\u03c1(QD) = n\u03c1n\u03c1(Q\u25e6D).\nProof. Recall that the eigenvalues of AB and BA coincide for any two square matrices A and B. Thus,\n\u03c1(A\u0304) = \u03c1(QPPT) = n\u03c1(QD) = n\u03c1n\u03c1(Q\u25e6D),\nwhere for the last equality we use the relation in (5).\nThe result below gives an equivalent expression for the spectral radius of the expected coarse-scale adjacency matrix \u00af\u0303A.\nLemma 4. (spectral radius of \u00af\u0303A): The spectral radius of \u00af\u0303A can be expanded as\n\u03c1( \u00af\u0303 A) = \u03c1(\u03a6Q\u03a6T) = m\u03c1 ( Q \u03a6T\u03a6\nm\n) . (27)\nProof. Similar to Lemma 3. Details are omitted.\nA.2 Lemma and proof: Error between the Gramians of random and expected LTI systems\nLet Sfine and Scoarse denote the expected dynamics of LTI (6) when A and A\u0303 are replaced with the expected quantities A\u0304 and \u00af\u0303A. The following result provides an error bound on the respective differences of the fine- and coarse- average controllability vectors, which is equivalent to bound the difference between the Gramians of Sfine and Sfine and that of Scoarse and Scoarse.\nLemma 5. (Error between the Gramians of random and expected LTI systems): Let \u03b8fine,A,\u03b8fine,A\u0304 be defined as in (25), and \u03b8coarse,A\u0303,\u03b8coarse, \u00af\u0303A as in (11). Under Assumption 1, the following holds with probability at least 1\u2212 \u03b4:\n\u03b1n , \u2225\u2225\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2225\u22251 (28)\n\u2264 \u03b7\n[ 1/\u03c1n+c1 c2\u25e6 + ( 12 +2\u03b7) [c1+ 1\u03c1n ] c2\u25e6 ( 1+ \u221a 1 + \u03b7 \u221a 1+ 1/\u03c1nnc2\u25e6 + 2 /\u03c1nnc\u25e6 )] [ 1\u2212 (\nc\u25e6 /\u03c1n+c\u25e6\n)2] [ 1\u2212 (\n\u03c1(A) +\u03c1(A) )2] (29) = O( \u03b7\n\u03c1n ), (30)\nand with probability at least 1\u2212 \u03b4\u0303: \u03b1\u0303m , \u2225\u2225\u2225\u03b8coarse,A\u0303 \u2212 \u03b8coarse, \u00af\u0303A\u2225\u2225\u22251 (31)\n\u2264 \u03b7\u0303\n[ 1/\u03c1n+c\u03031\nc\u03032\u25e6 + ( 12 + 2\u03b7\u0303) [c\u03031+ 1\u03c1n ] c\u03032\u25e6\n( 1+ \u221a 1 + \u03b7\u0303 \u221a\n1 + 1/\u03c1nmc\u03032\u25e6 + 2 /\u03c1nmc\u0303\u25e6 )] [ 1\u2212 ( c\u0303\u25e6\n/\u03c1n+c\u0303\u25e6\n)2] [ 1\u2212 ( \u03c1(A\u0303)\n+\u03c1(A\u0303) )2] (32) = O( \u03b7\u0303\n\u03c1n ), (33)\nwhere\n\u03b7 =\n\u221a log(n 2\n\u03b4 )/(2n)\n\u2016QDQ\u2016min , \u03b7\u0303 =\n\u221a log(m 2\n\u03b4\u0303 )/(2m)\n\u2016Q\u03a6T\u03a6m Q\u2016min , c\u25e6 = \u03c1(Q\u25e6D), c1 = tr ( (DQ\u25e6) 2 )\nc\u0303\u25e6 = \u03c1 ( Q\u25e6 \u03a6T\u03a6\nm\n) , c\u03031 = tr ( ( \u03a6T\u03a6\nm Q\u25e6)\n2 ) . (34)\nNote that C(Z, I) = (I\u2212 Z2)\u22121, where Z can take Anom, A\u0304nom, A\u0303nom, or \u00af\u0303 Anom. Lemma 5 is effectively bounding the difference of resolvents (zI\u2212 Z2)\u22121 evaluated at z = 1. For A = A\u0303, we have n = m and \u03a6 T\u03a6 m = D and hence, both \u03b1n and \u03b1\u0303n coincide. Lemma 5 is basically a concentration result for the Gramians of Sfine (or Scoarse) and Sfine (or Scoarse). However the rate at which the difference goes to zero is different for the fine- and coarse systems.\nProof. We start by simplifying \u03b1n in (28) (explanations for each step succeed the equations): \u03b1n = \u2225\u2225\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2225\u22251\n(a) = \u2225\u2225diag [C(Anom, In\u00d7n)\u2212 C(A\u0304nom, In\u00d7n)]\u2225\u22251\n(b) = tr (\u2223\u2223C(Anom, In\u00d7n)\u2212 C(A\u0304nom, In\u00d7n)\u2223\u2223) (c) = tr\n(\u2223\u2223(I\u2212A2nom)\u22121 \u2212 (I\u2212 A\u03042nom)\u22121\u2223\u2223) (d) = tr ( (I\u2212A2nom)\u22121|A2nom\u2212A\u03042nom|(I\u2212A\u03042nom)\u22121\n) (e)\n\u2264 \u2225\u2225(I\u2212A2nom)\u22121\u20162tr (\u2223\u2223A2nom \u2212 A\u03042nom\u2223\u2223) \u2016(I\u2212 A\u03042nom)\u22121\u2225\u22252\n(f) \u2264 1 1\u2212 \u2016A2nom\u20162\n1 1\u2212 \u2016A\u03042nom\u20162 tr (\u2223\u2223A2nom \u2212 A\u03042nom\u2223\u2223) , (35)\nwhere step (a) is due to the equivalent derivation of the fine controllability vector in (25); (b) is due to the equivalence of the of trace of the absolute value of a matrix and the `1 norm of its diagonal entries; (d)-(f) follow respectively from\nitems 1-3 in the matrix equivalence list at the beginning of the appendix; finally, (c) follows from the definition of the Gramian matrix in (2) and the Neumann series formula such that the limit below exists\nC(Anom, In\u00d7n) , lim T\u2192\u221e CT (Anom, In\u00d7n)\n= lim T\u2192\u221e T\u22121\u2211 t=0 (Anom) t(ATnom) t\n= lim T\u2192\u221e T\u22121\u2211 t=0 (Anom) 2t = (I\u2212A2nom)\u22121, (36)\nbecause A is a symmetric matrix and \u2016Anom\u20162 = \u03c1(A) +\u03c1(A) < 1. Similarly, we have \u2016A\u0304nom\u20162 = \u03c1(A\u0304) +\u03c1(A\u0304) < 1, and hence\nC(A\u0304nom, In\u00d7n) , lim T\u2192\u221e CT (A\u0304nom, In\u00d7n) = (I\u2212 A\u03042nom)\u22121.\nIn (35), the two terms \u2016A2nom\u20162 < 1, \u2016A\u03042nom\u20162 < 1 by definition. In the following, we will find a bound on the remaining term tr(|A2nom \u2212 A\u03042nom|). Using the Hoeffding\u2019s inequality in Proposition 3, the following holds for i, j \u2208 [n] and i 6= j, with probability at least 1\u2212 \u03b4:\n\u2223\u2223[A2]ij \u2212 E[A2]ij\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 `\u2208[n] Ai`Aj` \u2212 E[ \u2211 Ai`Aj`] \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u221a n log(1\u03b4)\n2\nmax(Ai`) 9 min(Ai`)\nmin([A\u03042]ij) [A\u03042]ij\n(a) \u2264\n\u221a log (\n1 \u03b4 ) 2n\n1\n\u2016QDQ\u2016min [A\u03042]ij , (37)\nwhere (a) follows by definitions of D,P in Section 2.2 coupled with the fact that\nA\u03042 = PQPTPQPT = nPQDQPT.\nSimilarly, for the diagonal elements, the following inequality holds with probability at least 1\u2212 \u03b4: \u2223\u2223[A2]ii \u2212 E[A2]ii\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 `\u2208[n] A2i` 9 E[A2i`] \u2223\u2223\u2223\u2223\u2223\u2223= \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 `\u2208[n] Ai` 9 E[Ai`] \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u221a n log(1\u03b4)\n2\nmax(Ai`) 9 min(Ai`) min(E [A2]ii) E[A2]ii\n(a) \u2264\n\u221a log (\n1 \u03b4 ) 2n\n1\n\u2016Qd\u2016min E[A2]ii, (38)\nwhere (a) is due to the identity E [ A2 ] ii\n= PiQP1n = PiQd. By merging (37), (38) and putting them in a matrix form, and by defining \u03b7 as\n1\u221a 2n max\n \u221a log(n 2\u2212n \u03b4 ) \u2016QDQ\u2016min , \u03c1n \u221a log(n\u03b4 ) \u2016Qd\u2016min \u2264 \u221a log ( n2 \u03b4 ) 2n\n1\n\u2016QDQ\u2016min ,\u03b7,\nthe following element-wise inequality holds with probability at least 1\u2212 \u03b4:\nA2 \u2264 (1 + \u03b7)E[A2] \u2264 (1 + \u03b7)(A\u03042 \u2212Diag[nPTQDQP] + nDiag[PTQd] = (1+\u03b7)n ( PTQDQP+Diag[PT(Q (1K,K\u2212Q)d)] ) . (39)\nSimilarly, for the lower bound we get: A2 \u2265 (1\u2212\u03b7)n ( PTQDQP+Diag [ PT(Q (1K,K\u2212Q)d) ]) . (40)\nFrom (39), (40), and Proposition 4, we obtain several matrix norm inequalities which we use later:\n\u03c12(A) = \u03c1(A2) \u2265(1 9 \u03b7) ( \u03c1(A\u0304)2+n\u03c1(Diag[PT(Q (1K,K 9 Q)d)]) ) = (1\u2212 \u03b7) ( n2\u03c12(QD) + n\u2016Q (1K,K \u2212Q)d\u2016max ) , (41)\nand:\n\u03c12(A) \u2264 (1 +\u03b7)n ( n\u03c12(QD)+\u2016Q (1K,K\u2212Q)d\u2016max ) \u2264 (1 + \u03b7)n2\u03c12n ( \u03c12(Q\u25e6D) + 1\nn\u03c1n\n) , (42)\nby employing the triangle inequality on matrix norm. In addition, (9) and the invariance of trace under cyclic permutation yield the following inequality\ntr ( A\u03042 ) = n2tr ( (DQ)2 ) . (43)\nCombining (43) and (39), gives tr ( A2 ) \u2264 tr [ (1+\u03b7)n ( PTQDQP+Diag[PT(Q (1K,K 9Q)d)] )] \u2264 (1 + \u03b7)n2 [ tr ( (DQ)2 ) + \u2016Qd\u2016max ] (44)\nMoreover, Lemma 3 and (41) give\n\u03c1(A) \u03c1(A\u0304) \u2264\n\u221a (1 + \u03b7)n2\u03c12n ( \u03c12(Q\u25e6D) + 1 n\u03c1n ) n\u03c1n\u03c1(Q\u25e6D)\n\u2264 \u221a 1 + \u03b7 \u221a 1 +\n1\nn\u03c1n\u03c12(Q\u25e6D) . (45)\nSimilarly, by combining Lemma 3 and (42), we have\n\u03c1(A) \u03c1(A\u0304) \u2265\n\u221a (1\u2212 \u03b7) + (1\u2212 \u03b7)\u2016Q\u25e6 (1\u2212Q)d\u2016max\nn\u03c1n\u03c12(Q\u25e6D) \u2265 \u221a 1\u2212 \u03b7. (46)\nCombining (39)-(46), gives the following upper bound for the tr (\u2223\u2223A2nom \u2212 A\u03042nom\u2223\u2223) term:\ntr (\u2223\u2223A2nom \u2212 A\u03042nom\u2223\u2223) (a)= tr(\u2223\u2223\u2223\u2223 A2 \u2212 A\u03042( + \u03c1(A\u0304))2 + ( A2 + \u03c1(A) \u2212 A2 + \u03c1(A\u0304) ) \u2223\u2223\u2223\u2223) (b)\n\u2264 tr (\u2223\u2223\u2223\u2223\u03b7A\u03042 + \u03b7nDiag[PTQ (1\u2212Q)d]( + \u03c1(A\u0304))2 +( A2( + \u03c1(A))2\u2212 A2( + \u03c1(A\u0304))2 ) \u2223\u2223\u2223\u2223) (c)\n\u2264 \u03b7 tr ( A\u03042 )\n( + \u03c1(A\u0304))2 + \u03b7n\ntr ( Diag[PTQ (1\u2212Q)d] ) ( + \u03c1(A\u0304))2 + \u2223\u2223\u2223\u2223 1( + \u03c1(A))2 \u2212 1( + \u03c1(A\u0304))2 \u2223\u2223\u2223\u2223 tr (A2)\n(d) \u2264 \u03b7 n2tr\n( (DQ)2 ) ( + n\u03c1(QD)) 2 + \u03b7 n2\u2016Q (1\u2212Q)d\u2016max ( + n\u03c1(QD))2\n+ \u2223\u2223\u2223\u22231\u2212 \u03c1(A)\u03c1(A\u0304) \u2223\u2223\u2223\u2223 (1 + \u03c1(A) + 2 \u03c1(A\u0304) ) (1 + \u03b7)n2 [ tr ( (DQ)2 ) + \u2016Qd\u2016max ] (1\u2212 \u03b7) (n2\u03c12(QD) + n\u2016Q (1\u2212Q)d\u2016max)\n(e) \u2264 \u03b7 1/\u03c1n + tr ((DQ\u25e6)2) \u03c12(Q\u25e6D) + 1 2 (1 + 4\u03b7) [ tr ( (DQ\u25e6) 2 ) + 1\u03c1n ] \u03c12(Q\u25e6D)\n\u00d7 ( 1+ \u221a 1+\u03b7 \u221a 1 +\n1\nn\u03c1n\u03c12(Q\u25e6D) +\n2\nn\u03c1n\u03c1(Q\u25e6D)\n)] , (47)\nwhere step (a) follows from the normalization factors defined in (1) and an additive-subtractive term; (b) is by replacing from (39); (c) is due to triangle inequality and the linearity of trace; (d) is because of (43), (44), (42), the definition of P in (3), and the following inequality\u2223\u2223\u2223\u2223 1( +\u03c1(A))2 9 1( +\u03c1(A\u0304))2 \u2223\u2223\u2223\u2223= \u2223\u2223\u2223\u2223 ( +\u03c1(A\u0304))2 9 ( +\u03c1(A))2( +\u03c1(A))2( +\u03c1(A\u0304))2 \u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223 (\u03c1(A\u0304) + \u03c1(A) + 2 )(\u03c1(A\u0304)\u2212 \u03c1(A))( + \u03c1(A))2( + \u03c1(A\u0304))2 \u2223\u2223\u2223\u2223\n\u2264\n[ 1 + \u03c1(A)+2\n\u03c1(A\u0304) ] \u2223\u2223\u22231\u2212 \u03c1(A)\u03c1(A\u0304) \u2223\u2223\u2223 ( + \u03c1(A))2 ; (48)\nfinally, (e) follows from (46), (45), and two easy-to-validate inequalities \u221a\n1\u2212 \u03b7 \u2265 1\u2212\u03b7/2 and 1+\u03b71\u2212\u03b7 \u2264 1+4\u03b7,\u2200\u03b7 < 1/2. From (47), and using the normalization factor definition when A and its expectation are the dynamics of the LTI system in (6), we can further simplify (35) as\n\u03b1n \u2264 \u03b7\n[ 1/\u03c1n+c1\nc2\u25e6 + ( 12 + 2\u03b7) [c1+ 1\u03c1n ] c2\u25e6\n( 1+ \u221a 1 + \u03b7 \u221a\n1 + 1/\u03c1nnc2\u25e6 + 2 /\u03c1nnc\u25e6 )] [ 1\u2212 ( c\u25e6\n/\u03c1n+c\u25e6\n)2] [ 1\u2212 ( \u03c1(A) +\u03c1(A) )2] (49) (a) = O( \u03b7\n\u03c1n ). (50)\nIn the above c\u25e6 = \u03c1(Q\u25e6D), c1 = tr ( (DQ\u25e6) 2 ) , and step (a) holds in the asymptotic scenario when D,Q are constant and \u221a n\u03c1n is very large and hence, \u03b7 becomes very small.\nelaborated in the following.\nFollowing similar lines as those in (35), the upper bound on \u03b1\u0303m (31) can be simplified to\n\u03b1\u0303m \u2264 1 1\u2212 \u2016A\u03032nom\u20162 1 1\u2212 \u2016 \u00af\u0303A 2 nom\u20162 tr(|A\u03032nom \u2212 \u00af\u0303 A 2 nom|). (51)\nTo get (51), one should note that (2) and the Neumann series yield\nC(A\u0303nom, Im\u00d7m) , lim T\u2192\u221e CT (A\u0303nom, Im\u00d7m)\n= lim T\u2192\u221e T\u22121\u2211 t=0 A\u0303tnom ( A\u0303Tnom )t = lim T\u2192\u221e T\u22121\u2211 t=0 A\u03032tnom = (I\u2212 A\u03032nom)\u22121, (52)\nsince \u2016A\u0303nom\u20162 = \u03c1(A\u0303) +\u03c1(A\u0303) < 1. Next, we simplify the term tr(|A\u0303 2 nom \u2212\n\u00af\u0303 A\n2 nom|) in (51) using similar concentration\nbounds as those in (47). From Proposition 3, Lemma 4, and the definition of \u00af\u0303A in (9), the following holds for i, j \u2208 [m] and i 6= j with probability at least 1\u2212 \u03b4\u0303:\n|[A\u03032]ij \u2212 E[A\u03032]ij | = | \u2211 `\u2208[m] A\u0303i`A\u0303j` \u2212 E[ \u2211\nA\u0303i`A\u0303j`]\ufe38 \ufe37\ufe37 \ufe38 [ \u00af\u0303 A 2 ]ij |\n\u2264 \u221a\u221a\u221a\u221am log ( 1\u03b4\u0303) 2 max(A\u0303i`A\u0303j`)\u2212min(A\u0303i`A\u0303j`) min(\u03a6Ti Q\u03a6 T\u03a6Q\u03a6j) E[A\u03032]ij\n\u2264 \u221a\u221a\u221a\u221a log ( 1\u03b4\u0303) 2m\n1\u2225\u2225\u2225Q\u03a6T\u03a6m Q\u2225\u2225\u2225 min E[A\u03032]ij . (53)\nSimilarly for the diagonal elements, we have: \u2223\u2223\u2223[A\u03032]ii \u2212 E[A\u03032ii]\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 `\u2208[m] A\u03032i` \u2212 E[A\u03032i`] \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u221a\u221a\u221a\u221am log ( 1\u03b4\u0303) 2 max(A\u03032i`)\u2212min(A\u03032i`) min(E[\n\u2211 `\u2208[m]\nA\u03032i`]) E[ \u2211 `\u2208[m] A\u03032i`]. (54)\nNext, we simplify the term E[A\u03032i`] in (54). From Section 2.2, we can verify that\nr2A\u0303ij \u223c Poisson-Binomial({Qkk\u2032 \u00b7 \u00b7 \u00b7Qkk\u2032\ufe38 \ufe37\ufe37 \ufe38 r2\u03a6ik\u03a6jk\u2032 : k, k\u2032 \u2208 [K]}), (55)\nsince r2A\u0303ij is a sum of independent (but not necessarily identically) Bernoulli random variables, parameterized by Qkk\u2032s for r2\u03a6ik\u03a6jk\u2032 of the variables contributing to the sum. We can denote (55) concisely by:\nr2A\u0303ij \u223c Poisson-Binomial({vec(Q)} vec(r 2\u03a6i\u03a6 T j )), (56)\nwhere vec(Z) outputs a vector containing all the elements in its input matrix Z. We use {vec(Z)} vec(Y) to denote a vector that contains all the elements of Z, where Zij has Yij occurrences. From (56), and using the variance definition of Poisson-Binomial, the term E[A\u03032i`] in (54) can be represented as\nE[A\u03032i`] = E[A\u0303i`]2 + var(A\u0303i`)\n= (\u03a6Ti Q\u03a6`) 2 +\n1\nr4 (1\u2212{vec(Q)} vec(r\n2\u03a6i\u03a6 T ` ))T{vec(Q)} vec(r 2\u03a6i\u03a6 T ` ) (57)\n=(\u03a6TiQ\u03a6`) 2+\n1\nr4 vec(r2\u03a6i\u03a6 T ` ) T[vec(Q) (1K2\u2212vec(Q))]\n= \u03c12n(\u03a6 T i Q\u25e6\u03a6`) 2+ \u03c1n r2 vec(\u03a6i\u03a6 T ` ) Tvec(Q\u25e6 (1K,K\u2212Q)). (58)\nWe then sum over the ` index of (57) to obtain the E[A\u03032ii] term in (54): E[A\u03032ii] = E[ \u2211 `\u2208[m] A\u03032i`] (59)\n=\u03a6TiQ\u03a6 T\u03a6Q\u03a6i+\n1\nr2 vec(\u03a6i 1Tm\u03a6\ufe37 \ufe38\ufe38 \ufe37\u2211 `\u2208[m] \u03a6T`) Tvec(Q (1K,K\u2212Q))\n= \u03a6Ti Q\u03a6 T\u03a6Q\u03a6i+\n1\nr2 vec(\u03a6i1\nT m\u03a6) Tvec(Q (1K,K\u2212Q))\n\u2265m\u2016Q\u03a6 T\u03a6\nm Q\u2016min. (60)\nSimilarly an upper bound for (59) is obtained: E[A\u03032ii] = E[ \u2211 `\u2208[m] A\u03032i`]\n= m\u03c12n\u03a6 T i Q\u25e6\n\u03a6T\u03a6\nm Q\u25e6\u03a6i + \u03c1n 4r2 vec(\u03a6i1 T\u03a6)T1\n\u2264 m\u03c12n\u2016Q\u25e6 \u03a6T\u03a6\nm Q\u25e6\u2016max + \u03c1n 4r2 \u2211 `\u2208[m],k,k\u2032\u2208[K]\n\u03a6ik\u03a6`k\u2032\ufe38 \ufe37\ufe37 \ufe38 =m\n(61)\nFrom (59), and the fact 0 \u2264 A\u0303i,` \u2264 1 by definition in (7), the upper bound on (54) can be further simplified as\n|[A\u03032]ii \u2212 E[A\u03032ii]| \u2264 \u221a\u221a\u221a\u221a log ( 1\u03b4\u0303) 2m\n1 \u2016Q\u03a6T\u03a6m Q\u2016min E[ \u2211 `\u2208[m] A\u03032i`]. (62)\nBy merging (53) and (62) and putting [A\u03032]ij\u2019s in a matrix form, we get\nA\u03032 \u2264 (1 + \u03b7\u0303)E[A\u03032] \u2264 (1 + \u03b7\u0303) ( \u00af\u0303 A\n2 + m\u03c1n 4r2 I ) = (1 + \u03b7\u0303) ( (\u03a6Q\u03a6T)2 +\nm\u03c1n 4r2\nI )\n= (1 + \u03b7\u0303)m ( \u03a6Q \u03a6T\u03a6\nm Q\u03a6T + \u03c1n 4r2 I\n) , (63)\nand\nA\u03032 \u2265 (1\u2212 \u03b7\u0303)m\u03a6Q\u03a6 T\u03a6\nm Q\u03a6T, (64)\nwhich both hold with probability at least 1\u2212 \u03b4\u0303 for \u03b7\u0303 in 34. Similar to the \u03b1n derivations, further norm inequalities are obtained from (63) and (64), which will be then used to simplify the upper bound on \u03b1\u0303m in (51). Using Proposition 4), \u03c1(A\u0303)2 can be bounded from below by\n\u03c1(A\u0303)2 = \u03c1(A\u03032) \u2265 (1\u2212 \u03b7\u0303)m2\u03c12n\u03c12 ( Q\u25e6 \u03a6T\u03a6\nm\n) , (65)\nand bounded from above by\n\u03c1(A\u0303)2 \u2264 (1 + \u03b7\u0303)m[m\u03c12n\u03c12 ( Q\u25e6 \u03a6T\u03a6\nm\n) +\n\u03c1n 4r2 ]. (66)\nEquation (65), coupled with Lemma 4, gives\n\u03c1(A\u0303) \u03c1( \u00af\u0303 A) \u2264\n\u221a (1 + \u03b7\u0303)m[m\u03c12n\u03c1 2 ( Q\u25e6 \u03a6T\u03a6 m ) + \u03c1n4r2 ]\nm\u03c1 ( Q\u03a6\nT\u03a6 m ) \u2264 \u221a 1 + \u03b7\u0303\n\u221a\u221a\u221a\u221a1 + 1 4r2m\u03c1n\u03c12 ( Q\u25e6 \u03a6T\u03a6 m\n) . (67) Similarly, with the help of (66), we obtain\n\u03c1(A\u0303) \u03c1( \u00af\u0303 A) \u2265 \u221a\n1\u2212 \u03b7\u0303 \u221a\u221a\u221a\u221a1 + 1\n4r2m\u03c1n\u03c12 ( Q\u25e6 \u03a6T\u03a6 m ) \u2265 \u221a\n1\u2212 \u03b7\u0303. (68) In addition, (9) and the invariance of trace under cyclic permutation, yield the following inequality\ntr ( \u00af\u0303 A 2 ) = m2tr ( ( \u03a6T\u03a6\nm Q)2\n) . (69)\nCombining (69) and (63), gives tr ( A\u03032 ) \u2264 tr [ (1 + \u03b7\u0303)m ( \u03a6Q \u03a6T\u03a6\nm Q\u03a6T + \u03c1n 4r2 I )] \u2264 (1 + \u03b7\u0303)m2 [ tr ( ( \u03a6T\u03a6\nm Q)2\n) +\n\u03c1n 4r2\n] . (70)\nSimilar to (49), we bound the term tr(|A\u03032nom \u2212 \u00af\u0303 A\n2\nnom|) to finalize the upper bound simplification in (51) (inner steps are removed due to redundancy):\n\u03b1\u0303m \u2264 \u03b7\u0303\n[ 1/\u03c1n+c\u03031\nc\u03032\u25e6 + ( 12 + 2\u03b7\u0303) [c\u03031+ 1\u03c1n ] c\u03032\u25e6\n( 1+ \u221a 1 + \u03b7\u0303 \u221a\n1 + 1/\u03c1nmc\u03032\u25e6 + 2 /\u03c1nmc\u0303\u25e6 )] [ 1\u2212 ( c\u0303\u25e6\n/\u03c1n+c\u0303\u25e6\n)2] [ 1\u2212 ( \u03c1(A\u0303)\n+\u03c1(A\u0303) )2] (71) = O( \u03b7\u0303\n\u03c1n ), (72) where c\u0303\u25e6 = \u03c1 ( Q\u25e6 \u03a6T\u03a6 m ) , c\u03031 = tr ( (\u03a6 T\u03a6 m Q\u25e6) 2 ) . The last scaling function is similarly found as that of (49), for the asymptotic scenario when \u221a m\u03c1n is very large and hence \u03b7\u0303 becomes very small. The proof is now complete.\nA.3 Proof of Lemma 1\nWe aim to find the fine controllability Sfine, i.e. when (6) has expected dynamics. To do so, we write down the definition of \u03b8fine,A\u0304 by substituting Z in (23) with A\u0304 (explanation for each step succeeds the equations):\n\u03b8fine,A\u0304 = diag ( \u221e\u2211 \u03c4=0 A\u03042\u03c4nom ) (a) = diag ( \u221e\u2211 \u03c4=0 ( \u03b9 n PTQ\u25e6P) 2\u03c4 ) = diag ( I + ( \u03b9\nn )2PTQ\u25e6PP TQ\u25e6P\n+( \u03b9\nn )4PTQ\u25e6PP TQ\u25e6PP TQ\u25e6PP\nTQ\u25e6P + \u00b7 \u00b7 \u00b7 )\n= 1n + 1\nn diag\n( PTD\u2212 1 2 [ (\u03b9D 1 2 Q\u25e6D 1 2 )2 + \u00b7 \u00b7 \u00b7 ] D\u2212 1 2 P )\n= 1n + 1\nn diag\n( PT\u03a5P ) (b) = 1n + 1\nn PTdiag(\u03a5), (73)\nwhere as defined in (13)\n\u03a5 = D\u2212 1 2 ([ I\u2212 (\u03b9D 12 Q\u25e6D 1 2 )2 ]\u22121 \u2212 I ) D\u2212 1 2\n= \u03b92Q\u25e6DQ\u25e6(I\u2212 (\u03b9Q\u25e6DQ\u25e6)2)\u22121, (74) and \u03b9 , 1\n\u03c1nn +\u03c1(Q\u25e6D)\n= O(1) since n\u03c1n n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 0. Step (a) follows from the definition of A\u0304 in (9) and Lemma 3; (b) is\ndue to the special structure of P in (3), where for an arbitrary matrix Z of appropriate size the following equality holds: [diag(PTZP)]i = \u2211\nk,k\u2032\u2208[K] PkiZk,k\u2032Pk\u2032i= \u2211 k\u2208[K] PkiZk,kPki\n= \u2211 k\u2208[K] P2kiZk,k (c) = Pidiag(Z).\nStep (c) is true because P is binary. Combining Lemma 2 and (73) with the definition of \u03a6 in (9), yields:\n\u03b8group,A\u0304 = 1\nr W\n( 1n + 1\nn PTdiag(\u03a5)) ) = 1\nr\n( 1m + 1\nn \u03a6diag(\u03a5)\n) . (75)\nA.4 Theorem and Proof: `1 error bound between the group controllability of the true and expected A\nDefine the error \u2206(A, A\u0304) similar to (12):\n\u2206(A, A\u0304) , \u2225\u2225\u2225\u2225 r\u03b8group,A \u2212 1m\u2016r\u03b8group,A \u2212 1m]\u20161 \u2212 r\u03b8group,A\u0304 \u2212 1m\u2016r\u03b8group,A\u0304 \u2212 1m\u20161 \u2225\u2225\u2225\u2225\n1\n. (76)\nThe following theorem bounds the difference between the group controllability of the true and expected A in (76). As hinted in Section 4, this difference, when small, will allow the introduction of the candidate estimator in (16) only as a function of \u03a6,Q, and D, which are retrievable from the coarse-scale matrix A\u0303.\nTheorem 6. (`1 error bound between \u03b8group,A and \u03b8group,A\u0304): Let \u2206(A, A\u0304) be defined as above for A \u2208 {0, 1}n\u00d7n and A\u0303 \u2208 Rm\u00d7m, and 0 < \u03b4\u25e6, \u03b4 < 1 be two constants. Then, under Assumption 1, \u2203n0 \u2208 N such that for n > n0\n\u2206(A, A\u0304)\u22642\n(\n\u03c1nn +\u03c1(Q\u25e6D) )2 \u2016Q\u25e6DQ\u25e6\u2016min 1+\u221a nmr \u221a\u221a\u221a\u221a log ( 1\u03b4\u25e6) 2 \u03b1n\n=O  \u03b7\u03c1n + \u221a n mr \u221a\u221a\u221a\u221a log ( 1\u03b4\u25e6) 2 , (77) holds with probability at least (1\u2212 \u03b4\u25e6)(1\u2212 \u03b4), where \u03b7 = \u221a log(n 2 \u03b4 )/(2n)\n\u2016QDQ\u2016min ; n and m are the dimensions of A and A\u0303; Q\nis defined in Def. 1; and D = (1/n)PPT.\nProof. We start by providing an upper bound on \u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2016max which will be shortly used to bound (77). Using the triangle inequality on matrix norm, we have:\n\u2016\u03b8fine,A\u2212\u03b8fine,A\u0304\u2016max \u2264 \u2016\u03b8fine,A\u22121m\u2016max+\u2016\u03b8fine,A\u0304\u22121m\u2016max (a)\n\u2264 \u2016(I\u2212A2nom)\u22121 \u2212 I\u2016max + \u2016 1\nn PTdiag(\u03a5)\u2016max\n(b) \u2264 \u221e\u2211 `=1 \u2016(A2)`\u2016max ( + \u03c1(A))2` + 1 n \u2016diag(\u03a5)\u2016max\n(c) \u2264 \u221e\u2211 `=1 (1 + \u03b7)`n` f(`) \u03c12`(A) + 1 n \u2016\u03a5\u2016max, (78)\nwhere (a) follows from (25) and (73); (b) is because P is binary; and finally (c) is due to (39) and defining f(`) , \u2016 ( PTQDQP + Diag[PT(Q (1K,K\u2212Q)d)] )` \u2016max. We, now, have f(1) = \u2016QDQ\u2016max + \u2016Qd\u2016max, and for all ` > 1\nf(`) (a) \u2264 \u2016PTQDQP + Diag[PT(Q (1K,K\u2212Q)d)]\u2016\u221e\u2016 ( PTQDQP+Diag[PT(Q (1K,K\u2212Q)d)] ) \u2212\u03001\u2016max (b)\n\u2264 (\u2016PTQDQP\u2016\u221e+\u2016Q (1K,K\u2212Q)d)\u2016max)f(`\u22121) (c) \u2264 (n\u2016QDQ\u2016max+\u2016Qd\u2016max)f(`\u2212 1), (79)\nwhere (a) is due to item 4 in the matrix norm equivalence list at the beginning of the appendix; (b) follows from the triangle inequality on matrix norms matrix, P being binary, and the definition of f(`) prior to (79); and finally (c) is because of item 5 of the list at the start of the appendix. Using (79), we upper bound the summation term in (78):\n\u221e\u2211 `=1 (1 + \u03b7)`n` f(`) \u03c12`(A) (a) \u2264 \u221e\u2211 `=1 ( 1+\u03b7 1\u2212\u03b7 )`n`\n(n\u2016QDQ\u2016max+\u2016Qd\u2016max)`\n(n2\u03c12(QD)+n\u2016Q (1K,K\u2212Q)d\u2016max)`\n= \u221e\u2211 `=1\n( 1+\u03b7\n1\u2212\u03b7 \u2016Q\u25e6DQ\u25e6\u2016max + 1n\u03c1n \u2016Q\u25e6d\u2016max\n\u03c12(Q\u25e6D)+ 1 n\u03c1n \u2016Q\u25e6 (1K,K\u2212Q)d\u2016max\n)` , (80)\nUsing (80), we can further simplify (78) as\n\u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2016max \u2264 1+\u03b7 1\u2212\u03b7\n\u2016Q\u25e6DQ\u25e6\u2016max+ 1n\u03c1n \u2016Q\u25e6d\u2016max \u03c12(Q\u25e6D)+\n1 n\u03c1n \u2016Q\u25e6 (1K,K\u2212Q)d\u2016max\n1\u2212 1+\u03b71\u2212\u03b7 \u2016Q\u25e6DQ\u25e6\u2016max+ 1n\u03c1n \u2016Q\u25e6d\u2016max\n\u03c12(Q\u25e6D)+ 1 n\u03c1n \u2016Q\u25e6 (1K,K\u2212Q)d\u2016max\n+ 1\nn \u2016\u03a5\u2016max\n(a) = O(1). (81)\n(a) in (81) is because \u03b7 is very small when n is very large, and \u2016\u03a5\u2016max is a constant (c.f. (74)): \u2016\u03a5\u2016max = \u2225\u2225\u2225\u2225D\u2212 12 ([I\u2212 (\u03b9D 12 Q\u25e6D 12 )2]\u22121 \u2212 I)D\u2212 12 \u2225\u2225\u2225\u2225\nmax\n= O(1). (82) We, now, return to bounding \u2206(A, A\u0304). Using (76) and Lemma 2, we obtain:\n\u2206(A, A\u0304) = m\u2211 i=1 | r\u03b8 (i) group,A \u2212 1 \u2016r\u03b8group,A \u2212 1m\u20161 \u2212 r\u03b8 (i) group,A\u0304 \u2212 1 \u2016r\u03b8group,A\u0304 \u2212 1m\u20161 |\n= m\u2211 i=1 \u2223\u2223\u2223\u2223 wi\u03b8fine,A \u2212 1\u2016W\u03b8fine,A \u2212 1m\u2016 \u2212 wi\u03b8fine,A\u0304 \u2212 1\u2016W\u03b8fine,A\u0304 \u2212 1m\u20161 \u2223\u2223\u2223\u2223\n(a) \u2264 2 \u2016W\u03b8fine,A \u2212W\u03b8fine,A\u0304\u20161 max ( \u2016W\u03b8fine,A \u2212 1\u20161, \u2016W\u03b8fine,A\u0304 \u2212 1\u20161 ) (b)\n\u2264 2 1 r\u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u20161,(rm) max ( \u2016W\u03b8fine,A \u2212 1\u20161, \u2016W\u03b8fine,A\u0304 \u2212 1\u20161 ) (c)\n\u2264 2 1 r [ mr n \u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u20161+\n\u221a mr log( 1\u03b4\u25e6 )\n2 \u2016\u03b8fine,A\u2212\u03b8fine,A\u0304\u2016max] max ( \u2016W\u03b8fine,A\u22121\u20161, \u2016W\u03b8fine,A\u0304 \u2212 1\u20161 ) = 2 [mn \u03b1n + \u221a m log( 1\u03b4\u25e6 )\n2r \u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2016max] max ( \u2016W\u03b8fine,A \u2212 1\u20161, \u2016W\u03b8fine,A\u0304 \u2212 1\u20161 ) (d)\n\u2264 2 [mn \u03b1n +\n\u221a m log( 1\u03b4\u25e6 )\n2r \u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2016max] m n 1\n\u03c1nn +\u03c1(Q\u25e6D)\n\u2016diag(\u03a5)\u2016min\n(e) \u2264 2 ( \u03c1nn + \u03c1(Q\u25e6D))\n2\n\u2016Q\u25e6DQ\u25e6\u2016min [\u03b1n +\n1 + \u03b7\n1\u2212 \u03b7\n\u221a n\nmr \u221a\u221a\u221a\u221a log ( 1\u03b4\u25e6) 2 \u2016\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2016max]\n(f) = O  \u03b7\u03c1n + \u221a n mr \u221a\u221a\u221a\u221a log ( 1\u03b4\u25e6) 2  (83) that holds with probability at least (1\u2212 \u03b4)(1\u2212 \u03b4\u25e6) \u2265 1\u2212 \u03b4\u25e6 \u2212 \u03b4 (c.f. probability of joint events), for sufficiently large n, i.e. \u2203n0 : n \u2265 n0. (a) follows from Proposition 5 where z is replaced with W\u03b8fine,A and y with W\u03b8fine,A\u0304; (c) uses (81); (d) is from (75) which proves:\n\u2016r\u03b8(i) group,A\u0304 \u2212 1\u20161 = \u2225\u2225\u2225\u2225 1n\u03a6diag(\u03a5) \u2225\u2225\u2225\u2225 1\n\u2265 1 n \u2016\u03a61K\u20161 \u2016\u03a5\u2016min \u2265 m n \u2016\u03a5\u2016min\n(g) = \u2126 (m n ) , (84)\nwhere (g) in (84) follows from \u2016\u03a5\u2016min = \u2225\u2225\u2225\u2225D\u2212 12 ([I\u2212(\u03b9D 12 Q\u25e6D 12 )2]\u22121\u2212I)D\u2212 12 \u2225\u2225\u2225\u2225\nmin\n= \u2126(1). (85)\n(e) uses the definition of \u03a5 in (13); (f) due to the definition of \u03b1n in (28) and an easy-to-verify inequality 1+\u03b71\u2212\u03b7 \u2264 1 + 4\u03b7 for \u03b7 < 1/2 that is satisfied for sufficiently large n; and finally, (b) is the result of the properties of the coarsemeasurement matrix in (7), the following notation definition\u2225\u2225\u03b8fine,A \u2212 \u03b8fine,A\u0304\u2225\u22251,(rm) , \u2211\ni\u2208 \u222aj\u2208[m]Kj\n\u2223\u2223\u2223\u03b8(i)fine,A \u2212 \u03b8(i)fine,A\u0304\u2223\u2223\u2223\nfor Ki = supp(wi) defined in Section 2.4, and invoking (87) which is explained below. Sum of Random Samples from a set of Data Points: Using a Hoeffding\u2019s bound equivalence for the sum of random samples of a set of data points {zi}i\u2208I indexed by I, we have [70, Proposition 1.2]\nP{| 1 |I| \u2211 i\u2208I |zi| \u2212 1 |z| \u2211 i\u2208[|z|] |zi|| \u2264 \u03b7\u25e6} \u2265 1\u2212 exp ( \u2212 2|I|\u03b7 2 \u25e6 (max(|z|)\u2212min(|z|))2 )\n\u2265 1\u2212 exp ( \u2212 2|I|\u03b7 2 \u25e6\nmax2|z|\n) (86)\nwhich means with probability at least 1\u2212 \u03b4\u25e6\n\u2016z\u20161,(rm) \u2264 |I| |z| \u2016z\u20161 + \u221a\u221a\u221a\u221a |I| log ( 1\u03b4\u25e6) 2 max|z|\n\u2264 [ |I| |z| + \u221a\u221a\u221a\u221a |I| log ( 1\u03b4\u25e6) 2 \u2016z\u2016max \u2016z\u20161 ]\u2016z\u20161. (87)\nTo simplify the notations, we set \u03b4\u25e6 = \u03b4. This concludes the proof.\nA.5 Proof of Theorem 1\nThe proof of the theorem makes use of Lemma 5. Let i \u2208 [m], and recall that Bi = diag(wTi ) and B\u0303i = rWdiag(wTi ). Consider the following bound using the triangle inequality for matrix norms:\n\u2206(A, A\u0303) , \u2225\u2225\u2225\u2225\u2225 r\u03b8group,A\u22121m\u2016r\u03b8group,A\u22121m\u20161\u2212 \u03b8coarse,A\u0303\u22121m\u2016\u03b8coarse,A\u0303 \u2212 1m\u20161 \u2225\u2225\u2225\u2225\u2225\n1 \u2264 \u2225\u2225\u2225\u2225 r\u03b8group,A\u22121m\u2016r\u03b8group,A \u2212 1m\u20161\u2212 r\u03b8group,A\u0304\u22121m\u2016r\u03b8group,A\u0304 \u2212 1m\u20161 \u2225\u2225\u2225\u2225 1\ufe38 \ufe37\ufe37 \ufe38\n=\u2206(A,A\u0304)\n+ \u2225\u2225\u2225\u2225\u2225 \u03b8coarse,A\u0303 \u2212 1m\u2016\u03b8coarse,A\u0303\u22121m\u20161\u2212 \u03b8 coarse, \u00af\u0303 A \u2212 1m \u2016\u03b8 coarse, \u00af\u0303 A \u22121m\u20161 \u2225\u2225\u2225\u2225\u2225 1\ufe38 \ufe37\ufe37 \ufe38\n=\u2206(A\u0303, \u00af\u0303 A)\n+ \u2225\u2225\u2225\u2225\u2225 r\u03b8group,A\u0304 \u2212 1m\u2016r\u03b8group,A\u0304\u22121m\u20161\u2212 \u03b8 coarse, \u00af\u0303 A \u2212 1m \u2016\u03b8 coarse, \u00af\u0303 A \u22121m\u20161 \u2225\u2225\u2225\u2225\u2225 1\ufe38 \ufe37\ufe37 \ufe38\n=\u2206(A\u0304, \u00af\u0303 A)\n. (88)\nwhere \u03b8group,A and \u03b8coarse,A\u0303 are defined respectively in (8) and (11). Similarly, \u03b8group,A\u0304 and \u03b8coarse, \u00af\u0303A are defined by replacing A with A\u0304 in (8), and A\u0303 with \u00af\u0303A in (11). An upper bound on \u2206(A, A\u0304) has been provided in Theorem 6. We now derive an upper bound on \u2206(A\u0303, \u00af\u0303A) with the help of Proposition 5. Replacing Z with \u00af\u0303A in (25) yields\n\u03b8 coarse, \u00af\u0303 A = diag\n( (I\u2212 \u00af\u0303A 2 nom) \u22121 )\n. Hence\u2225\u2225\u2225\u03b8 coarse, \u00af\u0303 A \u2212 1 \u2225\u2225\u2225\n1 = tr\n[ (I\u2212 \u00af\u0303A 2\nnom) \u22121 \u2212 I ] = tr [ \u00af\u0303 A 2 nom(I\u2212 \u00af\u0303 A 2 nom) \u22121 ] \u2265 tr [ \u00af\u0303 A 2\nnom\n] \u2265\ntr [( \u03a6Q\u03a6T )2]\n( + \u03c1(\u03a6Q\u03a6T))2 \u2265 tr [ (\u03a6 T\u03a6 m Q\u25e6) 2 ] (\nm\u03c1n + \u03c1\n( Q\u25e6 \u03a6T\u03a6 m ))2 , (89)\nfor the proof of which (9) and the normalization factor in (10) are used. Replacing z and y in Proposition 5 with \u03b8coarse,A\u0303 and \u03b8coarse, \u00af\u0303A yields\n\u2206(A\u0303, \u00af\u0303 A) = \u2225\u2225\u2225\u2225\u2225 \u03b8coarse,A\u0303\u22121m\u2016\u03b8coarse,A\u0303\u22121m\u20161\u2212 \u03b8 coarse, \u00af\u0303 A \u22121m \u2016\u03b8 coarse, \u00af\u0303 A \u22121m\u20161 \u2225\u2225\u2225\u2225\u2225 1\n\u2264 2 \u2016\u03b8coarse,A\u0303 \u2212 \u03b8coarse, \u00af\u0303A\u20161 max ( \u2016\u03b8coarse,A\u0303 \u2212 1m\u20161, \u2016\u03b8coarse, \u00af\u0303A \u2212 1m\u20161 ) (a) \u2264 2 \u03b1\u0303m \u2016\u03b8\ncoarse, \u00af\u0303 A \u2212 1m\u20161\n(b) \u2264 2\n( m\u03c1n +\u03c1 ( Q\u25e6 \u03a6T\u03a6 m )) 2\ntr((\u03a6 T\u03a6 m Q\u25e6) 2)\ufe38 \ufe37\ufe37 \ufe38 O(1)\n\u03b1\u0303m (c) =O\n( \u03b7\u0303\n\u03c1n\n) , (90)\nwith probability at least 1\u2212 \u03b4\u0303. Steps (a) and (b) are by substitution respectively from (31) and (89); and (c) follows from (28) and the assumption that n,m are sufficiently large.\nSimilar to (77) and (90), we find an upper bound for the remaining error term \u2206(A\u0304, \u00af\u0303A) in (88). We first derive a closed-form formula for \u03b8\ncoarse, \u00af\u0303 A similar to that of \u03b8group,A\u0304 obtained in Lemma 2 (middle steps are removed due to redundancy):\n\u03b8 coarse, \u00af\u0303 A =diag ( \u221e\u2211 \u03c4=0 \u00af\u0303 A 2\u03c4 nom ) =diag ( \u221e\u2211 \u03c4=0 ( \u03b9\u0303 m \u03a6Q\u25e6\u03a6 T)2\u03c4 )\n=1m + 1\nm diag\n( \u03a6\u03a5\u0303\u03a6T ) , (91)\nwhere\n\u03a5\u0303, \u03b9\u03032Q\u25e6 \u03a6T\u03a6\nm Q\u25e6(I\u2212(\u03b9\u0303\n\u03a6T\u03a6\nm Q\u25e6)\n2)\u22121, and \u03b9\u0303 , 1/ (\n\u03c1nm\n+\u03c1 ( Q\u03a6\nT\u03a6 m\n)) . By substituting (75) and (91) into \u2206(A\u0304, \u00af\u0303A) in (88), we get\n\u2206(A\u0304, \u00af\u0303 A) = \u2225\u2225\u2225\u2225\u2225 r\u03b8group,A\u0304 \u2212 1\u2016r\u03b8group,A\u0304 \u2212 1m\u20161 \u2212 \u03b8 coarse, \u00af\u0303 A \u2212 1 \u2016\u03b8 coarse, \u00af\u0303 A \u2212 1m\u20161 \u2225\u2225\u2225\u2225\u2225 1\n= \u2225\u2225\u2225\u2225\u2225 1n\u03a6diag(\u03a5)\u2016 1n\u03a6diag(\u03a5)\u20161 \u2212 1 mdiag(\u03a6\u03a5\u0303\u03a6 T) \u2016 1mdiag(\u03a6\u03a5\u0303\u03a6T)\u20161 \u2225\u2225\u2225\u2225\u2225 1\n(a) \u2264 2 \u2016\u03a6diag(\u03a5)\u2212 diag(\u03a6\u03a5\u0303\u03a6 T)\u20161 max ( \u2016\u03a6diag(\u03a5)\u20161, \u2016diag(\u03a6\u03a5\u0303\u03a6T)\u20161 ) \u2264 2\u2016\u03a6diag(\u03a5)\u2212 diag(\u03a6(\u03a5\u0303\u2212\u03a5 + \u03a5)\u03a6\nT)\u20161 m\u2016\u03a5\u2016min\n\u2264 2\u2016\u03a6diag(\u03a5)\u2212 diag(\u03a6\u03a5\u03a6 T)\u20161 +m\u2016\u03a5\u0303\u2212\u03a5\u2016max m\u2016\u03a5\u2016min (b) =O ( \u2016\u03a6diag(\u03a5)\u2212diag(\u03a6\u03a5\u03a6T)\u20161\nm +\u2016D\u2212\u03a6\nT\u03a6 m \u2016max\n) , (92)\nwhere step (a) follows from similar logic as in Proposition 5; (b) is proved similar to a later elaborate derivation in Section A.6 which results in (103), except that Q\u0302 in (103) is replaced with Q (i.e. LQ = 0) and D\u0302 in (103) is substituted by \u03a6\nT\u03a6 m (i.e. LD = D\u2212 \u03a6T\u03a6 m ). The statement of the theorem follows with probability at least (1\u2212 \u03b4\u25e6)(1\u2212 \u03b4)(1\u2212 \u03b4\u0303)\n(the probability of joint events) that satisfies\n(1\u2212 \u03b4\u25e6)(1\u2212 \u03b4)(1\u2212 \u03b4\u0303) \u2265 1\u2212 \u03b4\u25e6 \u2212 \u03b4 \u2212 \u03b4\u0303, (93) and by substituting (77), (90), and (92) into (88). To simplify the notations, we set \u03b4\u25e6 = \u03b4\u0303 = \u03b4. The proof is now complete.\nA.6 Proof of Theorem 2\nWe start by using the triangle inequality for matrix norms in order to find an upper bound for \u2206\u0302(A\u0303):\n\u2206\u0302(A\u0303) = \u2225\u2225\u2225\u2225\u2225 \u03b8\u0302group \u2212 1m\u2016\u03b8\u0302group \u2212 1m\u20161 \u2212 r\u03b8group,A \u2212 1m\u2016r\u03b8group,A \u2212 1m\u20161 \u2225\u2225\u2225\u2225\u2225\n1 \u2264 \u2225\u2225\u2225\u2225 r\u03b8group,A\u0304 \u2212 1m\u2016r\u03b8group,A\u0304 \u2212 1m\u20161 \u2212 r\u03b8group,A \u2212 1m\u2016r\u03b8group,A \u2212 1m\u20161 \u2225\u2225\u2225\u2225 1\ufe38 \ufe37\ufe37 \ufe38\n=\u2206(A,A\u0304)\n+ \u2225\u2225\u2225\u2225\u2225 \u03b8\u0302group \u2212 1m\u2016\u03b8\u0302group \u2212 1m\u20161 \u2212 r\u03b8group,A\u0304 \u2212 1m\u2016r\u03b8group,A\u0304 \u2212 1m\u20161 \u2225\u2225\u2225\u2225\u2225\n1\ufe38 \ufe37\ufe37 \ufe38 =\u2206\u0302(A\u0303,A\u0304)\n. (94)\nThe first term on the RHS of (94) has already been bounded in (77). In the following, we bound the second term in (94) and then combine the two bounds. We substitute \u03b8\u0302 (i)\ngroup from the output of Algorithm 1, and \u03b8 (i) group,A\u0304 from (75), and\n\u03a5 defined in (13):\n\u2206\u0302(A\u0303, A\u0304) = \u2225\u2225\u2225\u2225\u2225 \u03a6\u0302diag(\u03a5\u0302)\u2016\u03a6\u0302diag(\u03a5\u0302)\u20161 \u2212 \u03a6diag(\u03a5)\u2016\u03a6diag(\u03a5)\u20161 \u2225\u2225\u2225\u2225\u2225\n1\n. (95)\nWe set \u03a6\u0302 = \u03a6 + L\u03a6, Q\u0302 = Q + LQ, and D\u0302 = D + LD as the error matrices of appropriate sizes. Substitution of these error matrices into (95), successive application of the triangle inequality, and the use of Proposition 5 gives\n\u2206\u0302(A\u0303, A\u0304) \u2264 2 \u2016|\u03a6\u0302diag(\u03a5\u0302)\u2212\u03a6diag(\u03a5)\u20161 max ( \u2016\u03a6\u0302diag(\u03a5\u0302)\u20161, \u2016\u03a6diag(\u03a5)\u20161 ) \u2264 2\u2016|(\u03a6 + L\u03a6)diag(\u03a5\u0302)\u2212\u03a6diag(\u03a5)\u20161\n\u2016\u03a6diag(\u03a5)\u20161\n\u2264 2\u2016\u03a6diag(\u03a5\u0302\u2212\u03a5)\u20161 + \u2016|L\u03a6diag(\u03a5\u0302)\u20161 m\u2016diag(\u03a5)\u2016min\n\u2264 2\u2016\u03a5\u0302\u2212\u03a5\u2016max + \u2016\u03a5\u0302\u2016max\u2016L\u03a6\u20161,1/m \u2016\u03a5\u2016min . (96)\nTo continue the simplification of \u2016\u03a5\u0302\u2212\u03a5\u2016max in (96), we bring the two error matrices LQ and LD introduced in the statement of the theorem into play:\n(Q\u25e6 + LQ)(D + LD)(Q\u25e6 + LQ) = Q\u25e6DQ\u25e6 + \u03931,\n(D + LD)(Q\u25e6 + LQ) = DQ\u25e6 + \u03932,\n(D 1 2 + L\n1 2\nD)(Q + LQ)(D 1 2 + L\n1 2\nD) = D 1 2 QD 1 2 + \u03933,\n(DQ\u25e6 + \u03932) 2` = (DQ\u25e6) 2` + \u03934, (97)\nwhere we define\n\u03931 , Q\u25e6DLQ + Q\u25e6LDQ\u25e6 + Q\u25e6LDLQ + LQDQ\u25e6 + LQDLQ + LQLDQ\u25e6 + LQLDLQ,\n\u03932 , DLQ + LDQ\u25e6 + LDLQ,\nThe exact definitions of \u03933 and \u03934 can be retrieved by simple expansion, where \u03933 = O(LQ + LD),\u03934 = O(\u03932) when LD,LQ are small. From (97), we have\n|\u03c1(QD + \u03933)\u2212 \u03c1(QD)| (a) \u2264 \u2016\u03933\u20162 (b) \u2264 K\u2016\u03933\u2016max, (98)\nwhere (a) is due to Proposition 4, and (b) uses item 6 of the list in beginning of the appendix. Substituting (97), (98), (74), and the equivalence of (17) (similar to (74)) into (96) yields:\n\u2016\u03a5\u0302\u2212\u03a5\u2016max =\u2016\u03b9\u03022Q\u0302D\u0302Q\u0302(I\u2212(\u03b9\u0302D\u0302Q\u0302)2)\u22121\u2212\u03b92Q\u25e6DQ\u25e6(1\u2212(\u03b9DQ\u25e6)2)\u22121\u2016max = \u03b9\u03022\u2016(Q\u25e6DQ\u25e6 + \u03931)(I\u2212 (\u03b9\u0302D\u0302Q\u0302)2)\u22121 \u2212 ( \u03b9\n\u03b9\u0302 )2Q\u25e6DQ\u25e6(1\u2212 (\u03b9DQ\u25e6)2)\u22121\u2016max\n= \u03b9\u03022\u2016(Q\u25e6DQ\u25e6)[(I\u2212 (\u03b9\u0302D\u0302Q\u0302)2)\u22121 \u2212 (I\u2212 (\u03b9DQ\u25e6)2)\u22121]\n+ \u03931(I\u2212(\u03b9\u0302D\u0302Q\u0302)2)\u22121\u2212 \u03b92\u2212 \u03b9\u03022\n\u03b9\u03022 Q\u25e6DQ\u25e6(I\u2212(\u03b9DQ\u25e6)2)\u22121\u2016max\n(c) \u2264 [ \u2016Q\u25e6DQ\u25e6\u2016max\u2016\u03935\u2016max+\u2016\u03931\u2016max\u2016(1\u2212(\u03b9DQ\u25e6)2)\u22121\u2016max\n+ ( 2\u03c1(QD) + \u03c1(\u03933) + 2\nn\n) \u2016\u03933\u20162(\nn + \u03c1(QD) )2 \u2016Q\u25e6DQ\u25e6\u2016max\u2016(I\u2212 (\u03b9DQ\u25e6)2)\u22121\u2016max]K\u03b9\u03022 (d) = O(\u2016\u03935\u2016max + \u2016\u03931\u2016max + \u2016\u03933\u2016max), (99)\nwhere \u03935 , (I\u2212 (\u03b9\u0302D\u0302Q\u0302)2)\u22121 \u2212 (I\u2212 (\u03b9DQ\u25e6)2)\u22121; (c) follows from the triangle inequality of matrix norms, multiple use of item 7 in the list at the beginning of the appendix, (13), (17), and inequality (a) of (98); and (d) is due to inequality (b) of (98), Q\u25e6,D being constant values, and for when \u03931,\u03933, and \u03935 are small. Next, we further simplify (99) by finding the max norms of \u03931,\u03933, and \u03935. First, we upper bound \u2016\u03935\u2016max in (99) with the help of Neumann series, (97), and triangle inequality on matrix norms:\n\u2016\u03935\u2016max= \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 `\u22651 [(\u03b9\u0302D\u0302Q\u0302)2` \u2212 (\u03b9DQ\u25e6)2`] \u2225\u2225\u2225\u2225\u2225\u2225 max\n= \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 `\u22651 \u03b9\u03022`\u03934 \u2212 (\u03b92` \u2212 \u03b9\u03022`)(DQ\u25e6)2` \u2225\u2225\u2225\u2225\u2225\u2225\nmax \u2264 \u2225\u2225\u03b9\u03022(1\u2212 \u03b9\u03022)\u22121\u03934\u2225\u2225max+\u2225\u2225(DQ\u25e6)2[1\u2212(DQ\u25e6)2]\u22121\u2225\u2225max max`\u22651 |\u03b92`\u2212 \u03b9\u03022`|\n(a) = O (\u2016\u03934\u2016max+|\u03b99 \u03b9\u0302|) (b) =O(\u2016\u03932\u2016max+\u2016\u03933\u2016max), (100)\nwhere (a) holds when \u03932,\u03933 are small, and because \u03b9, \u03b9\u0302 are constant. Similar to (100), and by using triangle inequality on matrix norms as well as item 7 in the list at the beginning of the appendix, \u2016\u03931\u2016max, \u2016\u03932\u2016max are bounded from above by\n\u2016\u03931\u2016max \u2264K2 (\u2016Q\u25e6D\u2016max\u2016LQ\u2016max+\u2016Q\u25e6\u2016max\u2016LD\u2016max\u2016Q\u2016max +\u2016Q\u25e6\u2016max\u2016LD\u2016max\u2016LQ\u2016max+\u2016LQ\u2016max\u2016DQ\u25e6\u2016max\n+\u2016LQ\u2016max\u2016D\u2016max\u2016LQ\u2016max+\u2016LQ\u2016max\u2016LD\u2016max\u2016Q\u25e6\u2016max + \u2016LQ\u2016max\u2016LD\u2016max\u2016LQ\u2016max) \u2264K2(\u2016LQ\u2016max+\u2016LD\u2016max+\u2016LD\u2016max\u2016LQ\u2016max+\u2016LQ\u2016max\n+\u2016LQ\u20162max + \u2016LQ\u2016max\u2016LD\u2016max + \u2016LQ\u20162max\u2016LD\u2016max )\n= O(\u2016LQ\u2016max + \u2016LD\u2016max), (101)\nand\n\u2016\u03932\u2016max = \u2016DLQ + LDQ\u25e6 + LDLQ\u2016max \u2264 K(\u2016D\u2016max\u2016LQ\u2016max + \u2016LD\u2016max\u2016Q\u25e6\u2016max + \u2016LD\u2016max\u2016LQ\u2016max) (b)\n\u2264K(\u2016LQ\u2016max+\u2016LD\u2016max+\u2016LD\u2016max\u2016LQ\u2016max) = O(\u2016LQ\u2016max + \u2016LD\u2016max), (102)\nwhere (b) in (102) is because of the definitions of D and Q\u25e6 respectively in Assumption 1 and the paragraph following (4); and the scaling behaviour both in (101) and (102) is for when LQ,LD are small.\nBy substituting (100), (101), and (102) into (99), we get:\n\u2016\u03a5\u0302\u2212\u03a5\u2016max = O(\u2016LQ\u2016max + \u2016LD\u2016max), (103)\nSubstituting (103) into the original error in (96), yields: \u2206\u0302(A\u0303, A\u0304) = O ( \u2016L\u03a6\u20161,1 m + \u2016LQ\u2016max + \u2016LD\u2016max ) , (104)\nbecause \u2016\u03a5\u2016min = \u2126(1) and \u2016\u03a5\u0302\u2016max = O(1), per their definitions in (13) and (17). The proof concludes by merging (104) and Theorem 6 and substituting into (94)."
        }
    ],
    "year": 2022
}