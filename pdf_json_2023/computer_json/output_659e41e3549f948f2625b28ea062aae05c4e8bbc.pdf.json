{
    "abstractText": "Existing language and vision models achieve impressive performance in image-text understanding. Yet, it is an open question to what extent they can be used for language understanding in 3D environments and whether they implicitly acquire 3D object knowledge, e.g. about different views of an object. In this paper, we investigate whether a state-of-the-art language and vision model, CLIP, is able to ground perspective descriptions of a 3D object and identify canonical views of common objects based on text queries. We present an evaluation framework that uses a circling camera around a 3D object to generate images from different viewpoints and evaluate them in terms of their similarity to natural language descriptions. We find that a pre-trained CLIP model performs poorly on most canonical views and that fine-tuning using hard negative sampling and random contrasting yields good results even under conditions with little available training data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Henrik Voigt"
        },
        {
            "affiliations": [],
            "name": "Jan Hombeck"
        },
        {
            "affiliations": [],
            "name": "Monique Meuschke"
        },
        {
            "affiliations": [],
            "name": "Kai Lawonn"
        },
        {
            "affiliations": [],
            "name": "Sina Zarrie\u00df"
        }
    ],
    "id": "SP:7d2ac9cbdbbcd908cbb07acd9f40187ee05bb408",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Judy Fan",
                "Robert Hawkins",
                "Noah Goodman",
                "Leonidas J Guibas."
            ],
            "title": "Shapeglot: Learning language for shape differentiation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8938\u20138947.",
            "year": 2019
        },
        {
            "authors": [
                "Tal Arbel",
                "Frank P Ferrie."
            ],
            "title": "Viewpoint selection by navigation through entropy maps",
            "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision, volume 1, pages 248\u2013254. IEEE.",
            "year": 1999
        },
        {
            "authors": [
                "Alberto Baldrati",
                "Marco Bertini",
                "Tiberio Uricchio",
                "Alberto Del Bimbo."
            ],
            "title": "Effective conditioned and composed image retrieval combining clip-based features",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "Xavier Bonaventura",
                "Miquel Feixas",
                "Mateu Sbert",
                "Lewis Chuang",
                "Christian Wallraven."
            ],
            "title": "A survey of viewpoint selection methods for polygonal models",
            "venue": "Entropy, 20(5):370.",
            "year": 2018
        },
        {
            "authors": [
                "Udeepta D Bordoloi",
                "H-W Shen."
            ],
            "title": "View selection for volume rendering",
            "venue": "VIS 05. IEEE Visualization, 2005., pages 487\u2013494. IEEE.",
            "year": 2005
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012",
            "year": 2015
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Xuefeng Du",
                "Zhaoning Wang",
                "Mu Cai",
                "Yixuan Li."
            ],
            "title": "Vos: Learning what you don\u2019t know by virtual outlier synthesis",
            "venue": "arXiv preprint arXiv:2202.01197.",
            "year": 2022
        },
        {
            "authors": [
                "Yue Fan",
                "Winson Chen",
                "Tongzhou Jiang",
                "Chun Zhou",
                "Yi Zhang",
                "Xin Eric Wang."
            ],
            "title": "Aerial vision-and-dialog navigation",
            "venue": "arXiv preprint arXiv:2205.12219.",
            "year": 2022
        },
        {
            "authors": [
                "Han Fang",
                "Pengfei Xiong",
                "Luhui Xu",
                "Yu Chen."
            ],
            "title": "Clip2video: Mastering video-text retrieval via image clip",
            "venue": "arXiv preprint arXiv:2106.11097.",
            "year": 2021
        },
        {
            "authors": [
                "Samir Yitzhak Gadre",
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Ludwig Schmidt",
                "Shuran Song."
            ],
            "title": "Clip on wheels: Zero-shot object navigation as object localization and exploration",
            "venue": "arXiv preprint arXiv:2203.10421.",
            "year": 2022
        },
        {
            "authors": [
                "Michael Goldberg."
            ],
            "title": "A class of multi-symmetric polyhedra",
            "venue": "Tohoku Mathematical Journal, First Series, 43:104\u2013108.",
            "year": 1937
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy."
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572.",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Tomihisa Kamada",
                "Satoru Kawai."
            ],
            "title": "A simple method for computing general position in displaying three-dimensional objects",
            "venue": "Computer Vision, Graphics, and Image Processing, 41(1):43\u201356.",
            "year": 1988
        },
        {
            "authors": [
                "Apoorv Khandelwal",
                "Luca Weihs",
                "Roozbeh Mottaghi",
                "Aniruddha Kembhavi."
            ],
            "title": "Simple but effective: Clip embeddings for embodied ai",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14829\u201314838.",
            "year": 2022
        },
        {
            "authors": [
                "Solomon Kullback",
                "Richard A Leibler."
            ],
            "title": "On information and sufficiency",
            "venue": "The annals of mathematical statistics, 22(1):79\u201386.",
            "year": 1951
        },
        {
            "authors": [
                "Teng-Yok Lee",
                "Oleg Mishchenko",
                "Han-Wei Shen",
                "Roger Crawfis."
            ],
            "title": "View point evaluation and streamline filtering for flow visualization",
            "venue": "2011 IEEE Pacific Visualization Symposium, pages 83\u201390. IEEE.",
            "year": 2011
        },
        {
            "authors": [
                "George Leifman",
                "Elizabeth Shtrom",
                "Ayellet Tal."
            ],
            "title": "Surface regions of interest for viewpoint selection",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 38(12):2544\u20132556.",
            "year": 2016
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Ming Gong",
                "Daxin Jiang."
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11336\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi."
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Monique Meuschke",
                "Wito Engelke",
                "Oliver Beuing",
                "Bernhard Preim",
                "Kai Lawonn."
            ],
            "title": "Automatic viewpoint selection for exploration of timedependent cerebral aneurysm data",
            "venue": "Bildverarbeitung fuer die Medizin 2017, pages 352\u2013357.",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Mockus."
            ],
            "title": "Application of bayesian approach to numerical methods of global and stochastic optimization",
            "venue": "Journal of Global Optimization, 4(4):347\u2013365.",
            "year": 1994
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Amit H Bermano."
            ],
            "title": "Clipcap: Clip prefix for image captioning",
            "venue": "arXiv preprint arXiv:2111.09734.",
            "year": 2021
        },
        {
            "authors": [
                "Konrad M\u00fchler",
                "Mathias Neugebauer",
                "Christian Tietjen",
                "Bernhard Preim."
            ],
            "title": "Viewpoint selection for intervention planning",
            "venue": "EuroVis, pages 267\u2013 274.",
            "year": 2007
        },
        {
            "authors": [
                "Mathias Neugebauer",
                "Kai Lawonn",
                "Oliver Beuing",
                "Philipp Berg",
                "Gabor Janiga",
                "Bernhard Preim."
            ],
            "title": "Amnivis\u2013a system for qualitative exploration of near-wall hemodynamics in cerebral aneurysms",
            "venue": "Computer Graphics Forum, volume 32, pages",
            "year": 2013
        },
        {
            "authors": [
                "Gustavo Olague",
                "Roger Mohr."
            ],
            "title": "Optimal camera placement for accurate reconstruction",
            "venue": "Pattern recognition, 35(4):927\u2013944.",
            "year": 2002
        },
        {
            "authors": [
                "Dimitri Plemenos",
                "Dmitry Sokolov."
            ],
            "title": "Viewpoint quality and scene understanding",
            "venue": "Eurographics Symposium on Virtual Reality, pages 67\u2013",
            "year": 2006
        },
        {
            "authors": [
                "Joshua Podolak",
                "Philip Shilane",
                "Aleksey Golovinskiy",
                "Szymon Rusinkiewicz",
                "Thomas Funkhouser."
            ],
            "title": "A planar-reflective symmetry transform for 3d shapes",
            "venue": "ACM SIGGRAPH 2006 Papers, pages 549\u2013559.",
            "year": 2006
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "DR Roberts",
                "A David Marshall."
            ],
            "title": "Viewpoint selection for complete surface coverage of three dimensional objects",
            "venue": "BMVC, pages 1\u201311. Citeseer.",
            "year": 1998
        },
        {
            "authors": [
                "Joshua Robinson",
                "Ching-Yao Chuang",
                "Suvrit Sra",
                "Stefanie Jegelka."
            ],
            "title": "Contrastive learning with hard negative samples",
            "venue": "arXiv preprint arXiv:2010.04592.",
            "year": 2020
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki."
            ],
            "title": "Laion-400m: Open dataset of clipfiltered 400 million image-text pairs",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Shen",
                "Liunian Harold Li",
                "Hao Tan",
                "Mohit Bansal",
                "Anna Rohrbach",
                "Kai-Wei Chang",
                "Zhewei Yao",
                "Kurt Keutzer"
            ],
            "title": "How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383",
            "year": 2021
        },
        {
            "authors": [
                "Jun Tao",
                "Jun Ma",
                "Chaoli Wang",
                "Ching-Kuang Shene."
            ],
            "title": "A unified approach to streamline selection and viewpoint selection for 3d flow visualization",
            "venue": "IEEE Transactions on Visualization and Computer Graphics, 19(3):393\u2013406.",
            "year": 2012
        },
        {
            "authors": [
                "Jesse Thomason",
                "Michael Murray",
                "Maya Cakmak",
                "Luke Zettlemoyer."
            ],
            "title": "Vision-and-dialog navigation",
            "venue": "Conference on Robot Learning, pages 394\u2013 406. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Jesse Thomason",
                "Mohit Shridhar",
                "Yonatan Bisk",
                "Chris Paxton",
                "Luke Zettlemoyer."
            ],
            "title": "Language grounding with 3d objects",
            "venue": "Conference on Robot Learning, pages 1691\u20131701. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Bart Thomee",
                "David A Shamma",
                "Gerald Friedland",
                "Benjamin Elizalde",
                "Karl Ni",
                "Douglas Poland",
                "Damian Borth",
                "Li-Jia Li."
            ],
            "title": "Yfcc100m: The new data in multimedia research",
            "venue": "Communications of the ACM, 59(2):64\u201373.",
            "year": 2016
        },
        {
            "authors": [
                "Pere-Pau V\u00e1zquez",
                "Miquel Feixas",
                "Mateu Sbert",
                "Wolfgang Heidrich."
            ],
            "title": "Viewpoint selection using viewpoint entropy",
            "venue": "VMV, volume 1, pages 273\u2013280. Citeseer.",
            "year": 2001
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Legg Yeung",
                "Mojtaba Seyedhosseini",
                "Yonghui Wu."
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "venue": "arXiv preprint arXiv:2205.01917.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advancements in pre-training large-scale language and vision (L&V) models, such as CLIP (Radford et al., 2021), have led to exceptional performance on benchmarks and leaderboards in 2D image-text retrieval (Shen et al., 2021; Fang et al., 2021; Baldrati et al., 2022). However, the image-text data in these benchmarks have specific properties and biases (Thomason et al., 2022) that may limit the language grounding capabilities of existing L&V models and their robustness in realword scenarios (Khandelwal et al., 2022; Gadre et al., 2022). A fundamental bias in existing L&V data comes from the fact that images generally show single, human-centric views of different objects. This raises a simple but intriguing question: to what extent can a model acquire knowledge about the concept of viewpoints and identify different views on the same object? Figure 1 il-\nlustrates this challenge, showing the top-3 images retrieved by CLIP for two basic viewpoint descriptions, car/airplane from the bottom, in the LAION5B (Schuhmann et al., 2021) data set: the airplane images mostly correspond to the correct view, but none of the car images shows a bottom view. It suggests that the model does not generalize the meaning of viewpoint descriptions across different objects,1 and may fail to acquire visual-linguistic knowledge that would be needed in more realistic 3D scenarios, such as when instructing a drone to take a picture of an object from a specific viewpoint (Thomason et al., 2020; Fan et al., 2022). This opens the door for a systematic examination of the capabilities of L&V models for grounding viewpoint descriptions, delving into the question of why, despite their excellent zero-shot capabilities, a model like CLIP struggles when it comes to representing perspectives of the same object.\nIn this paper, we investigate whether language understanding in pre-trained L&V models generalizes to simple text-viewpoint descriptions of common objects. We propose a new task \u2013 textviewpoint retrieval \u2013 and a framework for analyz-\n1When searching the LAION-5B dataset via image embeddings of cars from the bottom, dozens of relevant results can be provided, which shows that these views exist in the data.\n2https://rom1504.github.io/clip-retrieval/\nar X\niv :2\n30 2.\n10 28\n2v 1\n[ cs\n.C V\n] 1\n3 Fe\nb 20\n23\ning and scaling image-text models with 3D data. We implement a Paparazzi agent that circles a spherical camera around a 3D object, samples images, and scores pairs of image-viewpoint descriptions using a pre-trained image-text matching model. In this framework, we evaluate and analyze whether CLIP, as a representative image-textmatching model with excellent zero-shot capabilities, systematically retrieves images of views of 3D shapes, regardless of potential reporting biases in 2D L&V data sets.\nTo successfully interpret viewpoint descriptions like car from the bottom, models need to connect concepts in natural language to visual representations and basic knowledge of object geometry. To investigate this, our approach is deliberately simple: we use 3D shapes from five categories of common objects in ShapeNet that have visually distinct canonical views (front, back, left, right, top, bottom). Based on Goldberg polyhedrons (Goldberg, 1937), that divide a sphere into hexagonal shapes, we analyze whether CLIP provides an adequate embedding for the viewpoint space around an object. Our analysis suggests that basic viewpoint understanding is indeed a systematic gap in the pre-trained CLIP model, as it achieves very poor performance in scoring view-description pairs and even retrieves nonsensical, non-human-centric views. Furthermore, we find that this problem is not fixed by standard fine-tuning. Thus, we propose a procedure for fine-tuning CLIP that extends the contrastive learning approach to viewpoints and descriptions generated from 3D visualizations. We find that a small amount of training data and extended fine-tuning is successful in scaling CLIP to basic viewpoint understanding in 3D."
        },
        {
            "heading": "2 Related Work",
            "text": "Vision, View, and Language. To date, research on grounding language in vision focuses on connecting language to visual representations of 2D human-centric views of scenes and objects based on, e.g., large image-caption data sets (Thomee et al., 2016; Schuhmann et al., 2021). Retrieval models in L&V usually rank a fixed set of images showing single views of different objects and scenes given a textual query or vice versa (Li et al., 2020a,b; Baldrati et al., 2022). Common understanding models process pairs of texts or questions and single-view images and predict labels for them, typical generation models process single-view im-\nages and generate descriptions for them (Mokady et al., 2021; Yu et al., 2022). In this paper, we propose a new L&V retrieval task where the model needs to search for a specific view, represented as an image, of a 3D object given a textual query. In our task, the space of possible view-images is not restricted to a human-centric view.\nLanguage Grounding in 3D. Achlioptas et al. (2019) present pioneering work in this area, with a referring expression data set designed for learning the language of shape for chair objects in ShapeNet, the most well-known resource for 3D object models (Chang et al., 2015). They build a neural resolution model that predicts which chair is referred to by a given shape description. Their encoder combines an autoencoder for point clouds of 3D shapes and a pre-trained image encoder for a single view of the object. As Achlioptas et al. (2019) collected descriptions of the 3D objects in a static environment with a fixed camera perspective, their approach does not account for dynamic viewpoints in 3D. Thomason et al. (2022) present a larger data set for expressions referring to ShapeNet objects and build a model that relies on image-text matching via the CLIP architecture, similar to ours. Their model takes images of eight fixed viewpoints of the object as input and integrates a component that estimates the viewing angle of an image. They evaluate on resolution accuracy and do not explicitly test viewpoint understanding in the CLIP model. In contrast to these existing works, the input to our model does not specify a fixed set of camera positions, and the output is an explicit, specific viewpoint of an object represented as an image.\nCamera Position Estimation. Viewpoint selection in a 3D environment is a well-known problem in other areas (Kamada and Kawai, 1988; Roberts and Marshall, 1998; Arbel and Ferrie, 1999; V\u00e1zquez et al., 2001; Plemenos and Sokolov, 2006; Podolak et al., 2006; M\u00fchler et al., 2007). Work in photogrammetry investigates camera position estimation minimizing the error in 3D measurements and reconstruction (Olague and Mohr, 2002). Systems in visualization aim to find an optimized viewpoint with the least possible occlusion and maximum information content for polygonal data (V\u00e1zquez et al., 2001; Neugebauer et al., 2013; Meuschke et al., 2017), volumetric data (Bordoloi and Shen, 2005) and vector fields (Lee et al., 2011; Tao et al., 2012). A key challenge in these\nareas is the definition of what actually constitutes a good viewpoint (Bonaventura et al., 2018). Most algorithms aim to find a viewpoint that is of high interest to the user (Leifman et al., 2016; Neugebauer et al., 2013), but do not yet incorporate textual descriptions of viewpoints. In addition, most of these algorithms require expensive annotated mesh representations of 3D objects. L&V models pre-trained on raw image-text data constitute an extremely promising direction here, provided that they are capable of viewpoint understanding."
        },
        {
            "heading": "3 Text-Viewpoint Retrieval Task",
            "text": "We study viewpoint understanding from descriptions and describe a framework for text-viewpoint retrieval. We present a task definition, the set-up of the 3D environment and the camera, and our approach to evaluation and analysis."
        },
        {
            "heading": "3.1 Task Definition",
            "text": "We define the input of our viewpoint retrieval task to consist of a 3D scene with a single object O, a search query describing a viewpoint q, and an orbital camera C circling the object. The camera returns single views of the object v that are represented as RGB images. The retrieval model\u2019s task is to find a viewpoint v that matches the query q. In this work, we implement retrieval via a scoring function S that passes pairs of images v (taken by the camera) and queries q to a pre-trained textimage matching model. The parameterization of the orbiting camera C determines the space of possible viewpoints V that the retrieval model has to search. The parameter setup we used in this work is explained in detail below.\nThis setting leverages the well-understood image-text matching in 2D for language grounding in 3D. Our retrieval model does not have a symbolic or explicit representation of the object\u2019s geometry but can perceive it by taking images from various perspectives. This framework is independent of different types of 3D data and only requires an engine that renders images of 3D environments."
        },
        {
            "heading": "3.2 Camera Set-up",
            "text": "For the purpose of this study, we restrict the viewpoint space V to views that contain the object of interest. We use a spherical camera system where the center of the object defines its center, as shown in Figure 2. The camera in orbit can be navigated around the desired object using polar coordinates.\nThe position of the camera towards the object is defined by (r, \u03b8, \u03d5) for the radial distance, the azimuthal angle, and the polar angle. The center of the object is defined by the center of its bounding box. The camera\u2019s local x and y axes are used to adjust the camera\u2019s viewing angles. Rotation around the local z-axis of the camera is disabled in this work, as the results would be the same, only with a rotated output image. In summary, the exact camera position and rotation along the sphere can be described by five parameters: (r, \u03b8, \u03d5, x, y).\nTo create equidistant sample points for camera positions along the sphere, we use a Goldberg polyhedron (Goldberg, 1937). It divides a sphere into mostly hexagonal shapes, including a small finite number of pentagons, and creates a nearly equidistant sample space (see Figure 2). The centers of the hexagons give us a discrete number of sample points, which reduce the possible configurations of our camera setup to a finite number. The hexagon centers can be approached for different radii r . The polyhedron used in this work initially yields 1002 sample points per radius. This discretization of the sample space is fine enough to allow benchmarking and analysis of viewpoint retrieval models.\nThe object O lies at the origin of the Cartesian space (0, 0, 0), which is also the center of the surrounding hypersphere. The radius r is clipped relatively to the size of the object. We estimate the extent of the object based on its bounding box. We\ndetermine the extent of the bounding box based on the minimum rmin and maximum radius rmax of the surrounding orbital spheres. In our experiments, we set rmin to two times the edge length of the bounding box and rmax to ten times the edge length of the bounding box."
        },
        {
            "heading": "3.3 Evaluation and Analysis",
            "text": "Common Objects and Canonical Views. To systematically evaluate language-view understanding in CLIP, we limit the set of viewpoint descriptions Q in our experiments to the six canonical views front, back, right, left, top, bottom defined by Chang et al. (2015). We choose 3D models of common object categories in ShapeNet (Chang et al., 2015). From the available 55 categories, we selected five categories where all canonical views are visually distinct: cars, airplanes, motorbikes, mugs and benches.3 As ShapeNet provides an aligned representation of all 3D models, these restrictions yield a fully controllable experimental setup where training and test data with pairs of queries and views can be generated automatically. The experimental setup is general enough to be transferable to arbitrary object domains and various forms of textual viewpoint descriptions.\nViewpoint Quality Evaluation. To assess the quality of text-viewpoint retrieval, we use the KL divergence (Kullback and Leibler, 1951) of a model\u2019s scoring function against a gold standard scoring distribution as well as the classical retrieval metrics precision@k and retrieval@k. We use KL divergence in addition since retrieval metrics only reflect performance on gold standard viewpoints and do not allow us to infer the global performance needed to find out why models fail on certain queries, as discussed in Section 5. We define the gold standard score distribution with respect to a particular viewpoint as a discrete normal distribution around the gold standard viewpoint, which is the mean of the distribution. The three polygonal rings around the mean are assigned the normalized score value at one, two, or three times the standard deviation of the normal distribution. The scores for all these viewpoints sum to 1. The scores for all other viewpoints around the sphere are set to zero. The setup is illustrated in Figure 2. To visually\n3Many object categories like bottle, ball, table, etc. do not have this property. For instance, the front and back views of a bottle are not or much less distinct than the front and back views of a car.\nanalyze the goodness of a scoring function over a sphere, we unfold the polyhedron and upsample it, as shown in the small map at the bottom right of Figure 2. In this way, we can visualize the difference between the gold standard and the predicted score distribution for an object.\nSearch Performance Evaluation. When searching a 3D scene, there are many possible viewpoints to consider. A scoring function that works well on a subset of pre-selected viewpoints may yield a good result in retrieval metrics, but in practical usage, it may lead the search algorithm to an unexpected or nonsensical viewpoint. Therefore, to evaluate the performance of a model, we need to consider not only how well it performs on the gold standard viewpoint images, but also how well it can guide a search algorithm to find the right viewpoint in the scene. We compare the performance of different search algorithms under different configurations of the scoring function to understand the impact of the shape of the scoring function on search performance. We compute search performance as follows: a search is considered successfully completed if the found viewpoint is within a certain radius of the respective gold standard viewpoint. We define the radius discretely based on the hexagonal rings around a gold standard viewpoint on the Goldberg polyhedron. In our experiments, we consider a search to be solved if a viewpoint is found within the first two rings around the gold standard viewpoint (see Figure 2). We compare performance in terms of the number c of calls to the scoring function required by the search algorithm to solve the search problem described above. We restrict the search length to a maximum number cmax of 300 viewpoints to visit. To obtain a robust comparison, we run the procedure n times at randomly selected starting positions on the hypersphere around the object. In our experiments, we set n to ten. Then, the number of calls cn is averaged."
        },
        {
            "heading": "4 Model",
            "text": ""
        },
        {
            "heading": "4.1 Scoring Function",
            "text": "The heart of our retrieval model is a function S that outputs matching scores for pairs of images and queries (v, q). Pre-trained L&V models like CLIP (Radford et al., 2021) embed (v, q) pairs into a common subspace, resulting in latent vector representations zv and zq, e.g., of size 512 in the original CLIP. The output of the scoring function S\nis the cosine similarity of the latent representations of the viewpoint image and the search query:\nS(v, q) = cos(zv, zq) = zv\u00b7zq |zv||zq| = \u2211N i=1 zvizqi\u221a\u2211N\ni=1 z 2 vi \u221a\u2211N i=1 z 2 qi\n(1) To evaluate a given viewpoint with respect to a query, both are encoded into their latent representations zv and zq, and the cosine similarity of their latent representations is used as a score for how well the view matches the query."
        },
        {
            "heading": "4.2 Objective Functions",
            "text": "To achieve high similarity between associated texts and images, Radford et al. (2021) apply a contrastive learning paradigm. In a training batch of N image-text pairs, a cosine similarity score is computed for each possible text-image combination. This leads to N \u00d7N scores over which a cross-entropy loss is calculated across the rows and columns. For corresponding text-image pairs, the maximum class score is expected, while for all other pairs, a minimum score is targeted.\nWe extend this contrastive learning paradigm for fine-tuning CLIP with 3D data by minimizing the combination of three different loss objectives: a) for negative examples, b) for random examples, and c) for hard negative examples.\nCross-Entropy Loss on Negative Examples is calculated and summed for both queries q and viewpoints v as Lv,q. The parameter \u03c4 is a learnable parameter for scaling the logits:\nLv,q = \u2212 1\nN N\u2211 i=1 log exp (cos (zvi , zqi) /\u03c4)\u2211N j=1 exp ( cos ( zvi , zqj ) /\u03c4 )\n(2)\nCross-Entropy Loss on Random Examples is denoted as Lr and computed between annotated viewpoints and randomly generated viewpoints of the 3D scene. Lr is computed exactly as in equation (2), but the contrastive examples are random images from the scene in this case.\nCross-Entropy Loss on Hard Negative Examples referenced as Lh uses images that have a different annotation but appear to be similar in latent space (Li et al., 2021). Robinson et al. (2020) present a sampling method that rescales the loss of negative examples based on their similarity to the gold standard sample. Following this, the loss Lh is calculated as the weighted contrastive loss\nbetween the positive samples x+ and the hard negative samples x\u2212 drawn from the modified negative sampling distribution q:\nLh = Ex+\u223cp+x\n[ \u2212 log e f(x)T f(x+)\nef(x) T f(x+)+GEx\u2212\u223cq\n[ ef(x) T f(x\u2212) ] ]\n(3) In notation, p+ is the marginal distribution of positive examples in the overall distribution of samples p. q is the distribution of negative samples. x is a single sample, x+ and x\u2212 are the respective positive and negative samples. f is a similarity measure, in our case it is cosine similarity. G is a weighting parameter that can be used to adjust the hardness of the negative sampling.\nThe total loss is parameterized as the weighted sum of the three objectives:\nLtotal = \u03b1Lv,q + \u03b2Lr + \u03b3Lh (4)\nThe ablations resulting from the different combinations presented above are evaluated in Section 6. The parameters \u03b1, \u03b2, and \u03b3 are chosen based on the respective experiment."
        },
        {
            "heading": "4.3 Search Algorithms",
            "text": "At inference time, our retrieval model requires a search algorithm A, a function that optimizes the output of the scoring function S given the space of viewpoints V and a query q. We compare the performance of two search algorithms. Greedy search starts with a grid-based approach on the Goldberg polyhedron and tries to find the optimum by moving greedily in the direction of the neighboring region with the highest score in each iteration. Bayesian search samples positions on the hypersphere based on incrementally obtained function values, attempting to sample with higher probability in regions that contain optima (Mockus, 1994). See appendix A for implementation details."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "Training. For each of the six canonical view query types and five object categories, we generate 1,000 training images in a Unity scene on randomly selected objects from the ShapeNet training set. This results in 6,000 image and text pairs per object category, which is tiny as compared to the 15 million images in the YFCC100M (Thomee et al., 2016) data set for training the original CLIP.\nTest Set. For evaluating the retrieval quality for each object category we randomly select three 3D shapes from the ShapeNet test set. Then we compute the normalized score distribution on synthetic images around the sphere with radius five for all selected objects of a category, compute the KLDivergence and average the results per viewpoint query (see Table 1). To assess the performance on real-world data, we carefully curated a data set of 600 images (5 categories \u00d7 6 viewpoints \u00d7 20 images) by retrieving visually similar images for a seed image using image similarity on LAION-5B. Synthetic gold standard views are obtained from the sampled spheres (see Table 2).\nModels. From the official CLIP repository (OpenAI), we select ResNet-101 (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) as image encoder and pre-trained BERT model (Devlin et al., 2018) as query encoder. We compare the following models: (i) PRE-TRained CLIP, without further fine-tuning, (ii) CLIP-FT, a version of CLIP fine-tuned on the training data with standard crossentropy loss, (iii) CLIP-RC-HNS, fine-tuned with extended loss objectives explained in Section 4."
        },
        {
            "heading": "5.2 Viewpoint Quality Results",
            "text": "Table 1 shows the results for the quality of viewpoint retrieval with different models, objects, and viewpoints. We find that a pre-trained CLIP model shows a high divergence from the gold standard\ndistribution for all object categories under investigation. The fine-tuned model performs slightly better, but still shows large differences from the gold standard. The use of random contrasting and hard negative sampling brings the score distribution closer to the gold standard distribution. This shows that standard CLIP pre-training and finetuning on human-centered 2D images do not produce a suitable scoring function for the viewpoint space around a 3D object.\nEvaluating performance on real data using KL divergence is not possible in a similar way as on synthetic data because we do not have access to images from arbitrary viewpoints. Therefore, we compare precision@k and recall@k between synthetic images from ShapeNet and real images at the gold standard viewpoints in Table 2. The results show that pre-trained CLIP performs poorly in grounding viewpoints on both synthetic data and real data. Fine-tuning the model on synthetic data greatly improves the retrieval metrics for both synthetic and real data. RC-HNS performs well on synthetic data that is within the distribution, however, it yields slightly lower scores on real-world data in comparison to FT. This may result from the fact that RC-HNS forces the model to generally score out-of-distribution data lower, thereby making the scoring function more sensitive to differences between synthetic and real-world images. In traditional 2D benchmarks, this may seem like a disadvantage compared to FT, but it proves to be advantageous in 3D viewpoint search, as demonstrated in the following section. Here, the FT model loses performance due to unpredictable scoring behavior in regions far from the gold standard viewpoints."
        },
        {
            "heading": "5.3 Search Performance Results",
            "text": "We test search performance in 3D as described in Section 3.3 for all six queries. Table 3 illustrates the\nperformance for Greedy and Bayes search. Both algorithms perform significantly better than an exhaustive search on the Goldberg polyhedron (= 1002 sample points, fixed radius). Bayesian search is much faster than greedy search, when using a finetuned scoring function (FT, RC-HNS), and it is more affected by the shape of the scoring function since it samples it strategically: it is fastest with the smoothest scoring function RC-HNS and very slow with pretrained CLIP. This is in line with the viewpoint quality results in Section 5.2, showing\nthat pretrained CLIP has a poor representation of the viewpoint space around an object."
        },
        {
            "heading": "6 Analysis",
            "text": "This section takes a closer look at how well the textviewpoint embeddings capture understanding of different viewpoints. Specifically, we will explore whether the scoring functions correctly identify viewpoints that align with the linguistic description, while providing lower scores for those that do not."
        },
        {
            "heading": "6.1 Exhaustive Viewpoint Space Analysis",
            "text": "Based on the polyhedron that defines the viewpoint space of the camera, we carry out an exhaustive analysis of the scoring function over this space for specific objects and queries. We select a car from the test set of the ShapeNet data set and plot the scores of the evenly distributed samples from the surface of the Goldberg polyhedron at a radius of five for the six canonical viewpoint queries. We examine five different configurations of the loss objective shown in Equation (4). Figure 3a) illustrates\nthe target region on the hexagon diagram, which contains the optimal viewpoint for a given query. It can be seen in Figure 3b) that a pre-trained CLIP model even if trained on a large data set, is not able to discriminate between different viewpoints and that the scoring function has multiple optima. Finetuning the CLIP model (3c) on synthetic images improves viewpoint discriminability. Nevertheless, apart from the absolute gold standard regions, the function shows problematic local optima and in particular the left and right side views of the car are difficult to distinguish. In (d), we fine-tune the CLIP model by applying the hard negative sampling strategy proposed by Robinson et al. (2020). The results show that the gold standard viewpoints can be distinguished much more effectively when compared to previous experiments. However, the transition between viewpoints is quite sudden, making it challenging for a search algorithm to reach the optimum. In (e), a combination of negative contrastive loss Lv,q and random contrastive loss Lr is applied. The results show that the additional objective makes the scoring function much more stable in regions farther away from known canonical viewpoints. In experiment (f), we combine hard negative sampling Lh with the idea of random contrasting. The plot of the scoring function shows that for each canonical viewpoint, the function increases steadily toward the optimal view."
        },
        {
            "heading": "6.2 Nonsensical Viewpoints",
            "text": "A further problem we noticed is that CLIP predicts high scores for nonsensical views that do not relate to the query, but rather seem to activate certain features to drive up the score, similar to adversarial examples (Goodfellow et al., 2014). Such behavior of models on unseen images has also been described by Du et al. (2022) and should be considered when using CLIP representations in continuous 3D environments, especially for vision-and-language navigation tasks, as in Khandelwal et al. (2022). Figure 4 shows retrieved nonsensical viewpoint images among the top-5 for car from the front."
        },
        {
            "heading": "6.3 Data Set Size Ablations",
            "text": "To test how the scoring function is affected when only a small amount of training data is available, we gradually reduce the number of training samples from 1,000 to 1 for the best-performing model CLIP-RC-HNS. Access to 1,000 training examples per viewpoint, as shown in 5a), leads to a smooth function. Reducing the training data by 90 percent to 100 examples per viewpoint keeps good performance for the target viewpoints. Compared to the full data set, smoothness suffers slightly. Reducing the training data by 99 percent to ten samples per viewpoint still allows good results in the target regions. However, the surrounding regions become less smooth and drop more abruptly. Surprisingly, when breaking down the training data to one example per viewpoint, the target viewpoint areas still lead to global optima in all search queries. However, the transitions are no longer smooth but rather abrupt, especially for the front and back."
        },
        {
            "heading": "7 Conclusion",
            "text": "We developed a new framework to assess the capabilities of L&V models to ground viewpoint descriptions. Through our research, we discovered that a standard CLIP model struggles to distinguish between different viewpoints. To address this, we explored a combination of different loss objectives on synthetic data to make it easier to retrieve viewpoints from language descriptions. Our experiments revealed that incorporating random contrasting leads to a more accurate and seamless scoring function, as compared to using only text and human-centric images. Our framework thus offers a promising approach to scale L&V models trained on large-scale image-text datasets for applications that involve interaction in the 3D world.\nLimitations We deliberately opted for a simple controllable setup in order to gain a precise understanding of viewpoint representation in CLIP. Our experiments are restricted to canonical views and canned descriptions since they are easy to generate and evaluate automatically. Extending the data to other views and to human-like descriptions is the obvious avenue for future research. In particular, with the advent of NERF models in computer vision, we look forward to integrating these types of models into our framework, as this would allow the generation of near-realistic images in a controlled 3D setup, which would allow for even better evaluation of scoring functions in text viewpoint retrieval. Varying the level of detail of the 3D shapes, especially in complex 3D scenes where large objects consist of smaller parts is another interesting direction. Another restriction of our set-up is the fact that we consider context-free retrieval of viewpoints, whereas in many human-like descriptions such as the right front tire of a car, the viewpoint may not be visually unique and depend on the context of the scene, such as the relative position of the viewpoint to other viewpoints. The same applies to views that need to be delivered to a user in a task-oriented interaction, and are likely to be more complex and diverse than the canonical and synthetic ones used in this work. In conclusion, we believe that our framework has the potential to provide a more comprehensive understanding of reporting biases in image-text data used for pretraining L&V models. By conducting a 360-degree analysis of the scoring function, our framework allows for a more thorough examination of these biases, as everything is visible and nothing can be hidden from the investigator, unlike when evaluating against a set of gold-standard viewpoints.\nEthics Statement\n3D models from the ShapeNet dataset are available for research and non-commercial purposes as well as the LAION-5B data set. We did not collect any personal information from any annotators. We clearly state the intended use of our models, which is to support human-centric interaction with AI models in the 3D world."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the Michael Stifel Center Jena for funding this work, which is part of the Carl Zeiss\nFoundation-funded project \u2019A Virtual Workshop for Digitization in the Sciences\u2019 (062017-02)."
        },
        {
            "heading": "A Experiment Details",
            "text": "This section provides additional details on our experimental setup. Section A.1 contains further visualizations of the experiments discussed in section 5. Section A.2 provides details about the implementation of the search algorithms used in our benchmark.\nA.1 Scoring Function Analysis The following plots illustrate the score distributions obtained with the different model ablations CLIPPRE-TR, CLIP-FT, and CLIP-RC-HNS.\nScoring Function PRETR. Figure 6a shows the score distribution of the PRE-TRained CLIP model over 3D objects from the test set of the ShapeNet dataset.\nScoring Function FT. Figure 6b depicts the scoring distribution of the CLIP-FT model over 3D objects from the test set of the ShapeNet dataset.\nScoring Function RC-HNS. Figure 7a illustrates the score distribution of the CLIP-RC-HNS model over 3D objects from the test set of the ShapeNet dataset.\nComparison of Score Distributions for Object Only Queries. To understand which viewpoints CLIP scores best on an object-only query such as a picture of a car, we compare these objectonly queries for all object categories tested on respective 3D objects from the test set. This tells us which viewpoints CLIP associates most with a given object category. Figure 8a indicates that a PRE-TRained CLIP model is not able to distinguish specific viewpoint queries from pure object queries.\nComparison of Optimal Viewpoints. Figure 8b shows the viewpoint images obtained from the optima of the scoring distributions generated by a CLIP model and a CLIP-RC-HNS model. The images illustrate that descriptions of viewpoints are indeed a bias in CLIP.\nFigure 7b illustrates the viewpoints resulting from the global optima of the scoring functions obtained from the CLIP-RC-HNS model.\nA.2 Search Algorithm Analysis In our work, we are particularly interested in the impact of the shape of the scoring function on the performance of various search algorithms. Section A.2.1 provides details on the implementation\nof greedy search. Section A.2.3 illustrates how the search algorithms listed above perform their task on a sphere.\nA.2.1 Greedy Search Implementation Details We implement a greedy search algorithm as a representative for gradient-based approaches. The greedy search starts with a grid-based approach on the Goldberg polyhedron and always follows the region with the highest score. It tries to find the optimum by greedily selecting the highest scoring regions at each iteration and searching in their neighboring regions at the next iteration. The search is initialized with k randomly selected starting points (here k = 6) from the Goldberg polyhedron. In addition, a cutoff value c must be chosen to determine how many grid points will be considered in the next iteration of the search. The cutoff value can be described as a relative percentage or as an absolute cutoff value. After evaluating all viewpoints with respect to the given query, the next iteration is started by selecting the locations with the highest scores considering the selected cutoff. All obtained scores and their neighboring sample points from the Goldberg polyhedron are added to the list of investigated viewpoints. After that, the next iteration is started. The neighborhood range n, which specifies the number of neighborhood grid points to be examined, can be adjusted. The search can be terminated after i iterations or when no new items have been added to the list of investigated viewpoints. In summary, the greedy search is parameterized by: (k, c, n, i). We chose greedy search as a test algorithm for our benchmark to see how much gradient-based methods as candidate algorithms for the text-viewpoint retrieval task in a 3D environment depend on a smooth structure of the scoring function in their performance. We use a greedy nearest-neighbour heuristic, since the function is only defined at a fixed number of points due to the discretization of the search space.\nA.2.2 Bayesian Search Implementation Details\nBayesian optimization (Mockus, 1994) is used to estimate the optimum of a black-box function that is costly to evaluate. The algorithm updates its Bayesian prior based on the stepwise function values obtained, increasing the certainty that the regions are likely to be optima and therefore more likely to be explored than other regions of the black box function. Then, the number of samples from\nthe regions of interest is increased accordingly. We construct the search problem as a Bayesian optimization as follows: The input of the search algorithm is a vector of size five describing the camera position on the hypersphere around the target object: r, \u03b8, \u03d5, x, y. In this parameterization, \u03b8 and \u03d5 are spherical coordinates, r is the distance to the center of the 3D object, and x and y are the orientations of the camera along the horizontal and vertical axes. The location of the optimum of the scoring function with respect to a query q depends on the rotation of the 3D object, which we only know is centered around (0, 0, 0). Therefore, Bayesian search tries to find the optimum of the scoring function with respect to the properties of the 3D object at hand given the search query q. For our benchmarks, we use the implementation of the Bayesian optimization algorithm in Head et al. (2021).\nA.2.3 Search Algorithm Behavior on Sphere The experiments in Section 5 have shown that a smooth scoring function is advantageous for search algorithms in text-viewpoint retrieval. This section visually analyzes why this is the case by examining how the algorithms perform on a sphere around a target object.\nFigure 8c illustrates how the different algorithms approach the regions with higher scores differently. The greedy search with a low cutoff spreads across the sphere in waves, starting from the initial points. Once it touches a high point, it remains attached to it. In this respect, a good initialization is impor-\ntant, e.g., through a high number of random starting points. Bayesian search also starts from randomly initialized starting points around the hypersphere. Compared to greedy search, it reaches the optimum much faster and more purposefully, since sampling is not bound to any local constraints, such as neighboring regions. Another advantage over greedy search is that random starting points have much lower cost than in greedy search, since they do not cause additional computations in the following iteration. The figure shows that the focus of sampling from random starting points across the sphere leads to small, concentrated regions with high scores. In terms of success rate, Bayesian search is less prone to confounding optima, since a certain number of samples are drawn randomly from different regions anyway. Therefore, the approach is more robust to cases with multiple optima, as is the case with the CLIP-FT model. Despite these obstacles, a solution is reached relatively quickly. However, if the scoring function has a ragged structure like the CLIP-PRETR model, even a sampling-based approach has difficulty identifying the optimal regions due to the raggedness and non-uniformity of the function.\nA.3 Retrieval Metrics Analysis Table 4 shows the precision and recall metrics on synthetic data broken down by object category. Table 5 shows the precision and recall metrics on real data obtained from the LAION-5B data set.\n(a) Scoring Function Distribution of CLIP PRE-TR model on cars, motorbikes, airplanes, benches, and mugs for the six canonical viewpoint queries.\n(b) Scoring Function Distribution of the CLIP-FT model on cars, motorbikes, airplanes, benches, and mugs for the six canonical viewpoint queries.\nFigure 6: Scoring Function Distributions on CLIP PRE-TR and CLIP-FT.\n(a) Scoring Function Distribution of the CLIP-RC-HNS model on cars, motorbikes, airplanes, benches, and mugs for the six canonical viewpoint queries."
        }
    ],
    "title": "Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions",
    "year": 2023
}