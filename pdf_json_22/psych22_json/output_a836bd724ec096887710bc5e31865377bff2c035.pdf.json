{
    "abstractText": "Reinforcement learning (RL) models have been influential in characterizing human learning and decision making, but few studies apply them to characterizing human spatial navigation and even fewer systematically compare RL models under different navigation requirements. Because RL can characterize one\u2019s learning strategies quantitatively and in a continuous manner, and one\u2019s consistency of using such strategies, it can provide a novel and important perspective for understanding the marked individual differences in human navigation and disentangle navigation strategies from navigation performance. One-hundred and fourteen participants completed wayfinding tasks in a virtual environment where different phases manipulated navigation requirements. We compared performance of five RL models (3 model-free, 1 model-based and 1 \u201chybrid\u201d) at fitting navigation behaviors in different phases. Supporting implications from prior literature, the hybrid model provided the best fit regardless of navigation requirements, suggesting the majority of participants rely on a blend of model-free (route-following) and model-based (cognitive mapping) learning in such navigation scenarios. Furthermore, consistent with a key prediction, there was a correlation in the hybrid model between the weight on model-based learning (i.e., navigation strategy) and the navigator\u2019s exploration vs. exploitation tendency (i.e., consistency of using such navigation strategy), which was modulated by navigation task requirements. Together, we not only show how computational findings from RL align with the spatial navigation literature, but also reveal how the relationship between navigation strategy and a person\u2019s consistency using such strategies changes as navigation requirements change.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qiliang He"
        },
        {
            "affiliations": [],
            "name": "Jancy Ling Liu"
        },
        {
            "affiliations": [],
            "name": "Lou Eschapasse"
        },
        {
            "affiliations": [],
            "name": "Elizabeth H. Beveridge"
        },
        {
            "affiliations": [],
            "name": "Thackery I. Brown"
        }
    ],
    "id": "SP:c2549b411ab5d62e1a728117b8f9c2e05b6a81ea",
    "references": [
        {
            "authors": [
                "Collins",
                "A.G.E. Reinforcement learning"
            ],
            "title": "Bringing together computation and cognition",
            "venue": "Curr. Opin. Behav. Sci. 29, 63\u201368",
            "year": 2019
        },
        {
            "authors": [
                "M.K. Eckstein",
                "L. Wilbrecht",
                "A.G. Collins"
            ],
            "title": "What do reinforcement learning models measure? Interpreting model parameters in cognition and neuroscience",
            "venue": "Curr. Opin. Behav. Sci",
            "year": 2021
        },
        {
            "authors": [
                "S.J. Gershman",
                "Daw",
                "N.D. Reinforcement learning",
                "episodic memory in humans",
                "animals"
            ],
            "title": "An integrative framework",
            "venue": "Annu. Rev. Psychol. 68, 101\u2013128",
            "year": 2017
        },
        {
            "authors": [
                "P.L. Lockwood",
                "Klein-Fl\u00fcgge",
                "M.C. Computational modelling of social cognition",
                "behaviour"
            ],
            "title": "A reinforcement learning primer",
            "venue": "Soc. Cogn. Affect. Neurosci. https:// doi. org/ 10. 1093/ scan/ nsaa0 40",
            "year": 2020
        },
        {
            "authors": [
                "R.S. Sutton",
                "A.G. Barto"
            ],
            "title": "Reinforcement Learning, Second Edition: An Introduction",
            "year": 2018
        },
        {
            "authors": [
                "Thorndike",
                "E.L. Animal intelligence"
            ],
            "title": "An experimental study of the associative processes in animals",
            "venue": "Psychol. Rev. Monogr. Suppl. 2, 1\u2013109",
            "year": 1898
        },
        {
            "authors": [
                "M.K. Eckstein",
                "A.G.E. Collins"
            ],
            "title": "Computational evidence for hierarchically structured reinforcement learning in humans",
            "venue": "PNAS 117,",
            "year": 2020
        },
        {
            "authors": [
                "A.G.E. Collins",
                "M.J. Frank"
            ],
            "title": "How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis",
            "venue": "Eur. J. Neurosci",
            "year": 2012
        },
        {
            "authors": [
                "A.R. Otto",
                "S.J. Gershman",
                "A.B. Markman",
                "Daw",
                "N.D. The curse of planning"
            ],
            "title": "Dissecting multiple reinforcement-learning systems by taxing the central executive",
            "venue": "Psychol. Sci. 24, 751\u2013761",
            "year": 2013
        },
        {
            "authors": [
                "I. van de Vijver",
                "R. Ligneul"
            ],
            "title": "Relevance of working memory for reinforcement learning in older adults varies with timescale of learning",
            "venue": "Aging Neuropsychol. Cogn",
            "year": 2019
        },
        {
            "authors": [
                "N.D. Daw",
                "S.J. Gershman",
                "B. Seymour",
                "P. Dayan",
                "R.J. Dolan"
            ],
            "title": "Model-based influences on humans\u2019 choices and striatal prediction errors",
            "venue": "Neuron 69,",
            "year": 2011
        },
        {
            "authors": [
                "D.A. Simon",
                "N.D. Daw"
            ],
            "title": "Neural correlates of forward planning in a spatial decision task in humans",
            "venue": "J. Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "G. Jocham",
                "T.A. Klein",
                "M. Ullsperger"
            ],
            "title": "Dopamine-mediated reinforcement learning signals in the striatum and ventromedial prefrontal cortex underlie value-based choices",
            "venue": "J. Neurosci",
            "year": 2011
        },
        {
            "authors": [
                "Vikbladh",
                "O. M"
            ],
            "title": "Hippocampal contributions to model-based planning and spatial memory",
            "venue": "Neuron 102,",
            "year": 2019
        },
        {
            "authors": [
                "W. Schultz"
            ],
            "title": "Behavioral dopamine signals",
            "venue": "Trends Neurosci",
            "year": 2007
        },
        {
            "authors": [
                "N.D. Daw",
                "Y. Niv",
                "P. Dayan"
            ],
            "title": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control",
            "venue": "Nat. Neurosci",
            "year": 2005
        },
        {
            "authors": [
                "J. Gl\u00e4scher",
                "N. Daw",
                "P. Dayan",
                "O\u2019Doherty",
                "J.P. States versus rewards"
            ],
            "title": "Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning",
            "venue": "Neuron 66, 585\u2013595",
            "year": 2010
        },
        {
            "authors": [
                "D. Anggraini",
                "S. Glasauer",
                "K. Wunderlich"
            ],
            "title": "Neural signatures of reinforcement learning correlate with strategy adoption during spatial navigation",
            "venue": "Sci. Rep. 8,",
            "year": 2018
        },
        {
            "authors": [
                "Q. He",
                "T.P. McNamara",
                "B. Bodenheimer",
                "A. Klippel"
            ],
            "title": "Acquisition and transfer of spatial knowledge during wayfinding",
            "venue": "J. Exp. Psychol. Learn. Mem. Cogn",
            "year": 2019
        },
        {
            "authors": [
                "A.R. Otto",
                "C.M. Raio",
                "A. Chiang",
                "E.A. Phelps",
                "N.D. Daw"
            ],
            "title": "Working-memory capacity protects model-based learning from stress",
            "venue": "PNAS 110,",
            "year": 2013
        },
        {
            "authors": [
                "A. Radulescu",
                "R. Daniel",
                "Y. Niv"
            ],
            "title": "The effects of aging on the interaction between reinforcement learning and attention",
            "venue": "Psychol. Aging 31,",
            "year": 2016
        },
        {
            "authors": [
                "Q. He",
                "T.P. McNamara",
                "T.I. Brown"
            ],
            "title": "Manipulating the visibility of barriers to improve spatial navigation efficiency and cognitive mapping",
            "venue": "Sci. Rep. 9,",
            "year": 2019
        },
        {
            "authors": [
                "Q. He",
                "E.H. Beveridge",
                "J. Starnes",
                "S.C. Goodroe",
                "T.I. Brown"
            ],
            "title": "Environmental overlap and individual encoding strategy modulate memory interference in spatial navigation",
            "venue": "Cognition 207,",
            "year": 2021
        },
        {
            "authors": [
                "E.R. Chrastil",
                "Warren",
                "W.H. Active",
                "passive spatial learning in human navigation"
            ],
            "title": "Acquisition of survey knowledge",
            "venue": "J. Exp. Psychol. Learn. Mem. Cogn. 39, 1520\u20131537",
            "year": 2013
        },
        {
            "authors": [
                "T. Ishikawa",
                "Montello",
                "D.R. Spatial knowledge acquisition from direct experience in the environment"
            ],
            "title": "Individual differences in the development of metric knowledge and the integration of separately learned places",
            "venue": "Cogn. Psychol. 52, 93\u2013129",
            "year": 2006
        },
        {
            "authors": [
                "S.M. Weisberg",
                "V.R. Schinazi",
                "N.S. Newcombe",
                "T.F. Shipley",
                "Epstein",
                "R.A. Variations in cognitive maps"
            ],
            "title": "Understanding individual differences in navigation",
            "venue": "J. Exp. Psychol. Learn. Mem. Cogn. 40, 669\u2013682",
            "year": 2014
        },
        {
            "authors": [
                "M. Hegarty",
                "A.E. Richardson",
                "D.R. Montello",
                "K. Lovelace",
                "I. Subbiah"
            ],
            "title": "Development of a self-report measure of environmental spatial ability",
            "venue": "Intelligence 30,",
            "year": 2002
        },
        {
            "authors": [
                "A.P. Boone",
                "X. Gong",
                "M. Hegarty"
            ],
            "title": "Sex differences in navigation strategy and efficiency",
            "venue": "Mem. Cogn. https:// doi. org/ 10",
            "year": 2018
        },
        {
            "authors": [
                "S.A. Marchette",
                "A. Bakker",
                "Shelton",
                "A.L. Cognitive mappers to creatures of habit"
            ],
            "title": "Differential engagement of place and response learning mechanisms predicts human navigational behavior",
            "venue": "J. Neurosci. 31, 15264\u201315268",
            "year": 2011
        },
        {
            "authors": [
                "A.P. Boone",
                "B. Maghen",
                "Hegarty",
                "M. Instructions matter"
            ],
            "title": "Individual differences in navigation strategy and ability",
            "venue": "Mem. Cogn. https:// doi. org/ 10. 3758/ s13421- 019- 00941-5",
            "year": 2019
        },
        {
            "authors": [
                "Kuliga",
                "S.F. et al. Exploring individual differences",
                "building complexity in wayfinding"
            ],
            "title": "The case of the Seattle central library",
            "venue": "Environ. Behav. https:// doi. org/ 10. 1177/ 00139 16519 836149",
            "year": 2019
        },
        {
            "authors": [
                "Q. He",
                "T.P. McNamara"
            ],
            "title": "Spatial updating strategy affects the reference frame in path integration",
            "venue": "Psychon. Bull. Rev",
            "year": 2018
        },
        {
            "authors": [
                "T.I. Brown",
                "S.A. Gagnon",
                "A.D. Wagner"
            ],
            "title": "Stress disrupts human hippocampal-prefrontal function during prospective spatial navigation and hinders flexible behavior",
            "venue": "Curr. Biol",
            "year": 2020
        },
        {
            "authors": [
                "T.I. Brown",
                "A.S. Whiteman",
                "I. Aselcioglu",
                "C.E. Stern"
            ],
            "title": "Structural differences in hippocampal and prefrontal gray matter volume support flexible context-dependent navigation ability",
            "venue": "J. Neurosci",
            "year": 2014
        },
        {
            "authors": [
                "Q. He",
                "T.I. Brown"
            ],
            "title": "Heterogeneous correlations between hippocampus volume and cognitive map accuracy among healthy young adults",
            "venue": "Cortex 124,",
            "year": 2020
        },
        {
            "authors": [
                "E.R. Chrastil",
                "K.R. Sherrill",
                "I. Aselcioglu",
                "M.E. Hasselmo",
                "C.E. Stern"
            ],
            "title": "Individual differences in human path integration abilities correlate with gray matter volume in retrosplenial cortex, hippocampus, and medial prefrontal cortex. ENeuro https:// doi",
            "venue": "www.nature.com/scientificreports/",
            "year": 2022
        },
        {
            "authors": [
                "Sherrill",
                "K. R"
            ],
            "title": "Functional connections between optic flow areas and navigationally responsive brain regions during goaldirected navigation",
            "venue": "Neuroimage 118,",
            "year": 2015
        },
        {
            "authors": [
                "V.D. Bohbot",
                "J. Lerch",
                "B. Thorndycraft",
                "G. Iaria",
                "A.P. Zijdenbos"
            ],
            "title": "Gray matter differences correlate with spontaneous strategies in a human virtual navigation",
            "venue": "task. J. Neurosci",
            "year": 2007
        },
        {
            "authors": [
                "K.J. Blacker",
                "S.M. Weisberg",
                "N.S. Newcombe",
                "Courtney",
                "S.M. Keeping track of where we are"
            ],
            "title": "Spatial working memory in navigation",
            "venue": "Vis. Cogn. 25(7\u20138), 691\u2013702",
            "year": 2017
        },
        {
            "authors": [
                "A. Nazareth",
                "X. Huang",
                "D. Voyer",
                "N. Newcombe"
            ],
            "title": "A meta-analysis of sex differences in human navigation skills",
            "venue": "Psychon. Bull. Rev. https:// doi",
            "year": 2019
        },
        {
            "authors": [
                "Q. He",
                "T.P. McNamara",
                "J.W. Kelly"
            ],
            "title": "Reference frames in spatial updating when body-based cues are absent",
            "venue": "Mem. Cogn",
            "year": 2018
        },
        {
            "authors": [
                "Q. He",
                "T.P. McNamara"
            ],
            "title": "Virtual orientation overrides physical orientation to define a reference frame in spatial updating",
            "venue": "Front. Hum. Neurosci. 12,",
            "year": 2018
        },
        {
            "authors": [
                "R.L. Klatzky",
                "J.M. Loomis",
                "A.C. Beall",
                "S.S. Chance",
                "R.G. Golledge"
            ],
            "title": "Spatial updating of self-position and orientation during real, imagined, and virtual locomotion",
            "venue": "Psychol. Sci",
            "year": 1998
        },
        {
            "authors": [
                "R.F. Wang",
                "J.R. Brockmole",
                "R.A. Abdul-Salaam"
            ],
            "title": "Spatial updating across environments",
            "venue": "J. Vis",
            "year": 2002
        },
        {
            "authors": [
                "R.F. Wang",
                "J.R. Brockmole"
            ],
            "title": "Simultaneous spatial updating in nested environments",
            "venue": "Psychon. Bull. Rev. 10,",
            "year": 2003
        },
        {
            "authors": [
                "A.W. Siegel",
                "S.H. White"
            ],
            "title": "The development of spatial representations of large-scale environments",
            "venue": "Adv. Child Dev. Behav. 10,",
            "year": 1975
        },
        {
            "authors": [
                "E.C. Tolman"
            ],
            "title": "Cognitive maps in rats and men",
            "venue": "Psychol. Rev. 55,",
            "year": 1948
        },
        {
            "authors": [
                "Newman",
                "E.L. et al. Learning your way around town"
            ],
            "title": "How virtual taxicab drivers learn to use both layout and landmark information",
            "venue": "Cognition 104, 231\u2013253",
            "year": 2007
        },
        {
            "authors": [
                "S.F. Feng",
                "S. Wang",
                "S. Zarnescu",
                "R.C. Wilson"
            ],
            "title": "The dynamics of explore\u2013exploit decisions reveal a signal-to-noise mechanism for random exploration",
            "venue": "Sci. Rep. 11,",
            "year": 2021
        },
        {
            "authors": [
                "J. Cohen"
            ],
            "title": "Statistical Power Analysis for the Behavioral Sciences (Routledge, 1988). https:// doi",
            "year": 1988
        },
        {
            "authors": [
                "Virtanen",
                "P. et al. SciPy 1.0"
            ],
            "title": "Fundamental algorithms for scientific computing in Python",
            "venue": "Nat. Methods 17, 261\u2013272",
            "year": 2020
        },
        {
            "authors": [
                "Q. He",
                "A.T. Han",
                "T.A. Churaman",
                "T.I. Brown"
            ],
            "title": "The role of working memory capacity in spatial learning depends on spatial information integration difficulty in the environment",
            "venue": "J. Exp. Psychol. Gen. https:// doi",
            "year": 2020
        },
        {
            "authors": [
                "B. Diedenhofen",
                "Musch",
                "J. cocor"
            ],
            "title": "A comprehensive solution for the statistical comparison of correlations",
            "venue": "PLoS ONE 10, e0121945",
            "year": 2015
        },
        {
            "authors": [
                "Hunter",
                "J.D. Matplotlib"
            ],
            "title": "A 2D graphics environment",
            "venue": "Comput. Sci. Eng. 9, 90\u201395",
            "year": 2007
        },
        {
            "authors": [
                "S.M. Weisberg",
                "Newcombe",
                "N.S. Cognitive maps"
            ],
            "title": "Some people make them, some people struggle",
            "venue": "Curr. Dir. Psychol. Sci. https:// doi. org/ 10. 1177/ 09637 21417 744521",
            "year": 2018
        },
        {
            "authors": [
                "T. Wolbers",
                "M. Hegarty"
            ],
            "title": "What determines our navigational abilities",
            "venue": "Trends Cogn. Sci",
            "year": 2010
        },
        {
            "authors": [
                "T. Wolbers",
                "Wiener",
                "J.M. Challenges for identifying the neural mechanisms that support spatial navigation"
            ],
            "title": "The impact of spatial scale",
            "venue": "Front. Hum. Neurosci. 8, 571 (2014). Author contributions Q.H., J.L., L.E. and T.B. designed the experiment. J.L., L.E. and E.B. collected the data. Q.H. performed the data analysis. Q.H. wrote the main manuscript text and prepare the figures. All authors revised and reviewed the manuscript. Funding Funding was provided by Warren Alpert Foundation to Q.H. and National Institutes of Health"
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nwww.nature.com/scientificreports"
        },
        {
            "heading": "A comparison of reinforcement",
            "text": "learning models of human spatial navigation Qiliang He1*, Jancy Ling Liu2, Lou Eschapasse1, Elizabeth H. Beveridge1 & Thackery I. Brown1*\nReinforcement learning (RL) models have been influential in characterizing human learning and decision making, but few studies apply them to characterizing human spatial navigation and even fewer systematically compare RL models under different navigation requirements. Because RL can characterize one\u2019s learning strategies quantitatively and in a continuous manner, and one\u2019s consistency of using such strategies, it can provide a novel and important perspective for understanding the marked individual differences in human navigation and disentangle navigation strategies from navigation performance. One-hundred and fourteen participants completed wayfinding tasks in a virtual environment where different phases manipulated navigation requirements. We compared performance of five RL models (3 model-free, 1 model-based and 1 \u201chybrid\u201d) at fitting navigation behaviors in different phases. Supporting implications from prior literature, the hybrid model provided the best fit regardless of navigation requirements, suggesting the majority of participants rely on a blend of model-free (route-following) and model-based (cognitive mapping) learning in such navigation scenarios. Furthermore, consistent with a key prediction, there was a correlation in the hybrid model between the weight on model-based learning (i.e., navigation strategy) and the navigator\u2019s exploration vs. exploitation tendency (i.e., consistency of using such navigation strategy), which was modulated by navigation task requirements. Together, we not only show how computational findings from RL align with the spatial navigation literature, but also reveal how the relationship between navigation strategy and a person\u2019s consistency using such strategies changes as navigation requirements change.\nReinforcement learning (RL) has made tremendous progress in the past two decades in computer science, psychology and neuroscience1\u20135. RL describes a learning mechanism in which behaviors are shaped through approaching rewards and avoiding punishments, which has a long history dating back to the nineteenth century6. In psychology, RL studies show that RL can be hierarchically structured7 and it interacts with other cognitive functions such as working memory in decision-making8\u201310. In neuroscience, RL studies have sought to reveal the neural substrates representing RL parameters (e.g., prediction error in the RL model), which include the striatum11,12, the ventromedial prefrontal cortex13, hippocampus14 and the firing behavior of dopamine neurons15. Most of the paradigms used in human RL studies focus on decision-making in 2D environments, so it remains understudied whether these findings can be generalized to scenarios important for survival like spatial navigation, which takes place in 3D environments. Moreover, modeling human spatial navigation behaviors via RL models could provide additional insight into the underlying cognitive mechanisms for a given navigational route compared to traditional methods (e.g., computing a route\u2019s length) by using a person\u2019s own navigational history to quantify that individual\u2019s navigation strategies and their consistency of using such strategies (discussed more below). The current study aims to address these gaps and opportunities in the literature.\nTo maximize rewards while minimizing effort/cost, we learn the associations between behaviors and rewards mainly through two types of RL: model-free\u2014which fosters rigid repetitions of previously rewarded actions, or model-based\u2014which fosters a mental model of the environment or task structure to flexibly select goal-directed actions11,12,16,17. In spatial navigation, model-free learning corresponds to response learning, or merely learning the landmark-action associations (e.g., turn right at the second traffic light) without learning the overall layout of the environment. Model-based learning, on the other hand, corresponds to place learning or learning the configuration of the environment12,18,19. From the RL literature, there is evidence showing that model-free and\nOPEN\n1School of Psychology, Georgia Institute of Technology, Atlanta, USA. 2School of Economics, Georgia Institute of Technology, Atlanta, USA. *email: duncan.heqiliang@gmail.com; thackery.brown@psych.gatech.edu\n2 Vol:.(1234567890) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nmodel-based learning operate and compete in parallel9,11,20,21. From the human spatial cognition literature, there is a plethora of evidence showing there are substantial individual differences in human spatial navigation, including wayfinding performance19,22\u201328 and navigation strategy29\u201334. Such individual differences are found to be correlated with structural and functional differences of the brain34\u201339, working memory capacity23,27,40, gender29,41 and task instructions31,33. Taking the findings from RL and spatial cognition together, it is reasonable to hypothesize that whereas the navigation behaviors of some people are best fit by either the model-free or the model-based learning, the majority of them should be best fit by a hybrid model, which is a combination or more-continuous balance of model-free and model-based learning.\nSurprisingly, there were very few studies lending support to this idea that such a hybrid RL model provides the best fit to characterize human spatial navigation. Using a spatial navigation task where the layout of the environment changed continuously, Simon and Daw12 found that the model-based learning fit the data better than the model-free, but whether a hybrid model provides a better fit than the model-based model remains unknown. Using several spatial navigation tasks, Anggraini et\u00a0al.18 compared the neural substrates which tracked modelfree, model-based and hybrid parameters, but which model provides the best fit to the navigation data themselves remains unknown. We believe that comparing the model-free, model-based and the hybrid models is crucial for efforts to investigate how the results from computational modeling, such as RL, fit the well-established individual differences and strategy-preference findings in the spatial cognition literature.\nIf the results from RL are in line with the findings from the literature, then we can harness the parameters generated from RL models to better understand human spatial navigation with much more confidence. As noted above, one unique contribution of applying RL to understanding human spatial navigation is that RL models can reveal one\u2019s navigation strategy quantitatively and as a function of one\u2019s prior experiences. In recent literature, we and others have used the dual-solution task30 to quantify individuals\u2019 navigation strategies towards following a familiar route vs. taking a shortcut. In this approach, participants first follow a pre-defined route to a destination several times, and then they can navigate to the destination freely. Using this task, individual navigation strategy is quantified via the solution index, which is equal to the number of trials in which a novel shortcut is taken divided by the number of successful trials in which either a shortcut or the learned route is taken. Studies using this task show marked individual differences in navigation tendency, such that some individuals primarily use shortcuts, some primarily use familiar routes, and many fall between these extremes30,31,34. However, classifying the navigation strategy of an episode into either familiar route following or shortcut taking in this manner may simplify many complex navigation behaviors observed in our daily life. It is often possible that one first follows the first few sections of the learned route and then takes a shortcut from there; similarly, the cognitive demands of a \u201cshortcut\u201d may depend heavily on how much of the route sequence has been previously traversed (i.e., to what degree is the route a novel construct). The weight parameter of a RL hybrid model can reveal a navigator\u2019s reliance on cognitive map vs. route-following on a trial-by-trial basis, which provides a finer grained characterization of one\u2019s navigation strategy in an objective and continuous manner, based on their prior navigation history. Furthermore, RL models can also estimate the consistency of using a navigation strategy across different navigation episodes. These two parameters from RL provide important insight into how one adapts their navigation strategies under different navigation requirements, which is a great complement to the solution index of the dual-solution task.\nTo select the appropriate RL algorithms for the current study, we used temporal difference (TD) learning9,11,12,18,21, which is one of the most commonly used model-free algorithms in the RL literature. TD learning assumes that agents learn the future reward value following an action, and adjusts predictions continuously before obtaining the reward5. In the current study, we compared three types of TD models: TD(0), TD(\u03bb) and TD(1). The major difference between TD(\u03bb) and TD(0) is that TD(\u03bb) adds an eligibility trace (see \u201cMethods\u201d), which adds the assumption that all the values of the visited locations are updated over time and the amount of updating depends on the visitation frequency. TD(1) is a special case of TD(\u03bb) such that once a location is visited, the updated value at a location would never diminish over time even if it is not visited again. In other words, TD(1) assumes that there is no forgetting of the importance of the visited locations and therefore there is no need for a navigator whose cognition resembles this framework to revisit them to retain their relevance in wayfinding. We target these TD models because they differ from each other on how the importance of the visited locations changes over time\u2014essentially, they make different assumptions about memory updating in spatial navigation. The topic of memory updating in spatial navigation has been studied extensively in experimental approaches33,42\u201346, but rarely through this computational lens. Note that these three TD algorithms predict that participants select different paths to reach destination based on their navigation history, but do not necessarily predict different navigation performance [e.g., one who favors TD(1) does not necessarily have to have better performance than the one who favors TD(0)].\nIn addition to model-free learning, we constructed a model-based model for spatial navigation. People who completely rely on a model-based system are assumed to have a perfect cognitive map47,48, and therefore one model-based model is sufficient. Because human performance relying purely on idealized cognitive maps may be implausible (at least for the vast majority of people) in many navigational scenarios, a hybrid model was constructed to reflect a heterogeneity or balance within an individual between map-like and more experience-bound knowledge. This hybrid model was developed by combing the best performing TD model with the model-based model, and adds a free weight parameter (\u03c9) to capture the individual\u2019s relative reliance on the model-based learning. As mentioned earlier, we hypothesized that the hybrid model was the best performing model in fitting human spatial navigation data.\nThe results from the model comparisons can inform us how well RL models fit the human spatial navigation literature, and as mentioned above, there are at least two parameters (navigation strategies and the consistency of using such strategies) generated from the RL models which could let us gain additional insight on human spatial navigation compared to traditional methods. To this end, we created navigation tasks with different requirements\n3 Vol.:(0123456789) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nand investigated how these different navigational scenarios modulated navigation strategies and the consistency of using such strategies, which reflected how humans adapted to the ever-changing environment12 but has rarely been examined in the literature. Specifically, our design considers the fact that not every goal-directed navigation problem is best approached in the same way, and this creates a dynamic in which individual differences can also be understood in terms of how people shift their learning/behavioral model under different demands. In the current study, participants first found different objects in a virtual environment from a fixed starting location (the Fixed phase), and then found the same objects in the same environment but from various random locations (the Random phase). We were interested in how the relative model-based weight (\u03c9) and the exploration\u2013exploitation parameter (\u03b8) changed as a function of learning experience and these different task requirements. In our hybrid model, \u03c9 represented the mean of an individual\u2019s navigation strategy across a number of navigation trials, and \u03b8 represents the consistency of using this strategy in these trials (i.e., the degree to which an individual persists with a particular way of approaching the tasks in the face of feedback and changing demands).\nExamining \u03c9 and \u03b8 separately and jointly would shed important light on how humans adapted to navigation scenarios of different requirements. Based on our manipulation of the navigation requirements, we hypothesized that (1) \u03c9, the reliance on model-based system or the cognitive map, would increase from the Fixed to the Random phases due to increasing familiarity of the environment19,22,23,49 and the demands of the Random phase encouraging greater reliance on map-like knowledge. (2) Participants would be more exploratory or deviate from their default strategy more in the Random phase due to the randomness and uncertainty introduced50. Therefore, we hypothesized that \u03b8 would increase from the Fixed to the Random phases. (3) The correlations between \u03c9 and \u03b8 would be different in the Fixed phase from the Random phase\u2014in the Fixed phase, where the starting location was always the same, there was no need to vary navigation strategy from trial to trial. In the Random phase, on the other hand, a more efficient strategy would be to rely on the model-free system when starting from a familiar location but to rely on the model-based system when starting from an unfamiliar location (thus favoring variation of navigation strategy). In other words, we theorized that better navigators would use one strategy more consistently in deterministic navigational scenarios, whereas they would vary their strategy more often in probabilistic navigational scenarios. From this theoretical perspective, we hypothesized that the correlation between \u03c9 and \u03b8 would be positive in the Fixed phase (i.e., better cognitive mappers would stick with one strategy more often than non-cognitive mappers), but the correlation would become negative in the Random phase (i.e., cognitive mappers may be more flexible in how they approach spatial problems). In this way, the hybrid RL model allows us to test a very specific but important prediction about the cognitive basis of human navigation performance. Finally, to show that \u03c9 was indeed reflective of spatial navigation ability, we correlated \u03c9 with objectively measured wayfinding performance, with the hypothesis that these two factors were significantly correlated.\nTo foreshadow our results, we found that the model-free model outperformed the model-based model in the Fixed phase, but vice versa in the Random phase. The hybrid model, on the other hand, was the best model of human navigation in both phases. Participants relied on cognitive maps more and deviated from their default strategy more in the Random than in the Fixed phases. Supporting our theoretical framework, the correlations between model-based reliance and exploration\u2013exploitation were different between the Fixed and Random phases. Lastly, wayfinding performance was correlated with model-based reliance."
        },
        {
            "heading": "Methods",
            "text": "Participants. One hundred and twenty-six participants from Georgia Institute of Technology and the Atlanta community participated in this experiment, either for course credits or monetary compensation. Participants spent between 80 to 140\u00a0min completing the experiment. Twelve participants felt motion sensitive and did not finish the experiment. As a result, one hundred and fourteen participants (forty-six females) were included in the data analysis. A sensitivity power analysis showed that the smallest effect size our study could detect was r = 0.26 given our final sample size (114), targeting statistical power (0.8) and alpha level (0.05), which was sensitive enough to detect small (0.2) to medium (0.5) effects according to Cohen\u2019s guidelines51. All participants (age 18\u201333) gave written consent and informed consent was obtained from all participants. The research was approved by the Institutional Review Board of Georgia Institute of Technology (IRB approval Code: H17456). All procedures were performed in accordance with the institutional guidelines.\nMaterials and procedure. Navigation in virtual environment. Participants completed a practice session in a 4 \u00d7 4 grid of rooms to familiarize with the control scheme and the objective of the navigation task. The 3D virtual environment was created in Sketchup (www. sketc hup. com) and the navigation task was rendered and implemented in Unity 3D video game engine (https:// unity. com/). Each room was a square of 10 \u00d7 10 virtual meters in size with a wall of 3 virtual meter. There was a penetrable door in each side of the room except for the rooms at the boundary. Movement in the virtual environment was enabled by keyboard, which provided selfpaced and continuous translation and rotation. After the practice session, participants started the Fixed phase.\nFixed phase. To assess navigational learning and model it using RL algorithms, participants learned to navigate to hidden locations in a 6 \u00d7 6 grid of virtual rooms (Fig.\u00a01). Each room had a unique reference object (toys, furniture, vehicles, etc.) served as local landmark which could only be seen within the room but not from other rooms. No distal or global landmark was available. Over the course of nine trials in the Fixed phase participants were instructed to find three specific goal objects repeatedly (apple, banana and watermelon; three trials per goal). These goal objects remained in the same rooms throughout the experiment but only the to-be-found goal object was invisible in a specific trial (e.g., all reference objects would be visible in their rooms, but if the goal object was \u201capple\u201d in this trial, the banana would not appear even if participants traversed across the banana\u2019s\n4 Vol:.(1234567890) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nroom). This helped avoid blending learning of different goal-destination pairings in the same trial. Once participants had found the goal object, they were teleported to the starting location and were instructed to find the next goal object. To make this Fixed phase amenable to model-free learning, participants were always brought back to the same starting position with the same facing direction, and each goal object was to be found in the same order across participants (i.e., apple-banana-watermelon and then repeat for all participants). This is akin to learning the outbound paths from one\u2019s new home to the grocer, movie theater, etc. We limited the Fixed phase to nine trials to minimize the transfer of spatial learning, that is, many participants may start deriving shortcuts through model-based learning in the Fixed phase when they become extensively familiar with the environment19,30,34, potentially suppressing our ability to delineate interesting individual differences.\nRandom phase. Participants then underwent a \u201cRandom phase\u201d in the same virtual environment as in the Fixed phase. Importantly, our implementation of a small number of trials in the Fixed phase in our design not only ensured participants did not derive and habitualize \u201cshortcuts\u201d during the Fixed phase but also still had room to improve their precise configural knowledge of the environment and continue learning at this point. Therefore, the Random phase represented a critical period involving a probe of (1) spatial transfer and flexible perspective adoption from the Fixed phase and then (2) continued environmental learning under new procedures. The Random phase was almost identical to the Fixed phase except that participants\u2019 starting location and orientation were randomized in each trial (goal object locations were excluded in the possible starting locations). This is akin to finding the same grocer, movie theater, etc. from variable locations in the neighborhood. In addition, the order of goal objects was pseudorandomized such that each goal object was to be found once in every three trials but not in a predictable order (e.g., banana-apple-watermelon-apple-watermelon-banana\u2026). There were seventy-two trials in the Random phase.\nFor our study, it was critical that the Fixed phase always preceded the Random for each participant. First, exposure to the Random phase prior to the Fixed phase may encourage participants to default to a model-based strategy and the performance in the Fixed phase could be at the floor level. Second, the Fixed phase\u2014by repeating start-goal location pairings\u2014enabled participants to develop one (or several) routes to a goal that would then be familiar in the Random phase and could be strategically exploited from a familiar landmark/room (enabling participants to exhibit shifts in strategy in Random where otherwise they would only have one [model-based] to go from).\nAnalyses pipeline outline. Described in detail below, our analysis pipeline was as follows: we first fit each participants\u2019 navigation behaviors with the three model-free models and selected the besting performing one (Fig.\u00a0 3). We then created a hybrid model by combining the winning model-free model and a model-based model. Finally, we compared the performance of the model-free, model-based and hybrid models (Fig.\u00a04) and chose the best performing model for subsequent individual differences analyses using its parameters (Table\u00a01).\nReinforcement learning models. As mentioned in the Instruction, we used five different reinforcement learning models to fit navigation behaviors, separately for the Fixed and the Random phases and separately for each participant. We modelled the sequence of participants\u2019 choices (which rooms to enter) by comparing them step by step to those predicted by various models. As we had a 6 \u00d7 6 grid, the navigation task consisted of 36 states (rooms) and in each state, subjects could have up to four action choices (up, down, left or right). The navigation task consisted of three rewards (three goal objects), and the objective for all models was to learn the state-action value function Q(s,a) at each state-action pair (i.e., which direction to go when in a specific room to maximize reward) for each goal object (Fig.\u00a02). We assumed no interference or generalization among the (implicit) rewards of the three goal objects, and thus each algorithm was subdivided into three independent task sets and value functions, one for each goal object.\nModel\u2011free reinforcement learning. To provide further insight of model-free behaviors in human spatial navigation and chose the best one for the hybrid model, we created three TD models: TD(0), TD(\u03bb) and TD(1). We\nFigure\u00a01. Experimental materials. (A) Layout of the environment. S indicates the fixed starting location in the Fixed phase, G1 ~ 3 indicate the goal object locations. (B, C) Actual view of landmark objects and rooms from the participants. Note that participants were brought back to the same starting location after finding a goal object during the Fixed phase.\n5 Vol.:(0123456789) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nfirst describe and provide the equations for TD(0), and then explain the differences between the three models. The equations for how the Q values were updated in the TD model were as follows5:\nHere, t denoted the current state and action, and t + 1 denoted the future state and action chosen by the softmax function (see below). Equation\u00a0(1) showed that the Q value associated with the current state (Q(s(t), a(t))) was updated by an error \u03b4, adjusted by the learning rate \u03b1. Equation\u00a0(2) showed that the error \u03b4 was determined by the reward associated with the future state (r(t+1)) plus the difference between the Q values associated with the future and current states. The Q value of each state-action pair was initialized to be 0 in the beginning of the experiment, and the Q values were carried across trials and phases.\n(1)QTD(st , at) = QTD(st , at)+ \u03b1\u03b4\n(2)where\u03b4 = rt+1 + QTD(st+1, at+1)\u2212 QTD(st , at)\nFigure\u00a02. Model-free (A) and model-based (B) reinforcement learning models. The numbers in the figure are state values, showing how a navigator decides to move along the route in a given state/room. (A) Model-free valuation based on the TD algorithm. After finding an object this algorithm updates the values only along the traversed path. (B) Model-based valuations derived from dynamic programming. The model-based algorithm assumes a perfect cognitive map and the values in the entire environment are precomputed (See Model-based reinforcement learning). S starting location, G1 Goal object # 1.\nFigure\u00a03. Model comparison in the Fixed (A) and the Random (B) phases. BIC Bayesian Information Criterion. n.s. not significant. ***p < 0.001.\n6 Vol:.(1234567890) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nTo determine which action to take based on Q values associated with the future states and actions, we computed the probability of the action selection based on the softmax function:\n\u03b8 was the inverse temperature controlling the degree of randomness in participants\u2019 action selection, and a\u2032 denoted the possible future actions from the current state. \u03b8 was constrained between 1 to 15 based, and the higher the \u03b8, the more deterministic of the action selection and therefore more exploitative.\nCompared to TD(0), TD(\u03bb) added the eligibility trace e to the Q value updating5, which was a temporary record of how frequently each state was visited. The eligibility trace for each state-action pair was set to be 0 in the beginning of each trial. The equations for how the Q values were updated in TD(\u03bb) were as follows:\nI was the indicator function, which was equal to 1 when the condition inside it was true and 0 otherwise. \u03bb was constrained between 0 and 1, so Eq.\u00a0(5) indicated that the less frequently a state was visited, the smaller the updating of the Q value associated with that state. TD(1) was a special case of TD(\u03bb), which forced every visited state to get the same amount of updating regardless of how often they were visited. When relating these RL algorithms to human memory systems, TD(0) assumed that memory updating, which was represented in the Q value updating, occurred only in the most recent visited location, whereas TD(\u03bb) assumed that memory updating occurred in all previously visited locations continuously and such updating scaled with the frequency of visitation. TD(1) differed from TD(\u03bb) that memory updating did not scale with the frequency of visitation.\nModel\u2011based reinforcement learning. For the model-based algorithm, we used dynamic programing5 which learned the layout of the environment (i.e., cognitive map) by computing the Q values via traversing all possible rooms and directions to locate the goal (Fig.\u00a02). We computed the Q values based on a \u2018sweeping\u2019 process termi-\n(3)pt+1 = exp\u03b8Q(st+1,a) \u2211\na \u2032 \u2208Aexp\n\u03b8Q(st+1,a\u2032)\n(4)QTD( )(st , at) = QTD( )(st , at)+ \u03b1\u03b4et(s, a)\n(5)where et(s, a) = et\u22121(s, a)+ I(St = s,At = a)\nFigure\u00a04. Model comparison in the Fixed (A) and the Random (B) phases. BIC Bayesian Information Criterion. **p < 0.01, ***p < 0.001.\n7 Vol.:(0123456789) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nnating at goal locations. We first initialized all QMB(s) to 0 in the beginning of the Fixed phase. Then, for all states and adjacent state-action pairs we iteratively performed the following:\nwhere (a|s) was the probability to take action a from state s following the exploration vs. exploitation policy. p ( s \u2032 , r|s, a )\nwas the probability to end up in state s\u2032 and receive reward r given the current state and action. The algorithm had one fixed parameter \u03b3 set at 0.8. The final model-based values (QMB) were the values after the algorithm converged (i.e., the difference between each of the QMB in the current iteration and the previous iteration was smaller than 0.0001). Conceptually, the model-based values reflected the state-action values as if one had a perfect cognitive map, and therefore the Q values did not get updated and they were the same for all participants.\nHybrid model. We implemented a hybrid model as a weighted linear combination of the values from the best performing model-free algorithm across participants and the model-based algorithm:\nwhere \u03c9 represented the balance between the model-free and model-based behaviors. The higher the \u03c9, the better the navigation behaviors could be characterized as model-based or cognitive map guided.\nModel fitting and comparison. For each algorithm, we computed the negative log-likelihood (NLL) of the observed choices ( at ) by the summing over the log of Eq.\u00a0(3), for the action chosen on each of the n trials, as follows:\nwhere vector X denotes the free parameters of the model, and the NLL was computed separately in Fixed and Random phases. The best fitting parameters were then computed as those that minimize the negative log likelihood:\nModel fitting was performed using the the optimization function from SciPy52. Model comparison was performed by computing the Bayes Information Criterion (BIC) for each model for each participant, separately in the Fixed and Random phases.\nwhere k is the number of free parameters in the model and n is the number of trials in the data. There were two free parameters (\u03b1 and \u03b8) in the TD(0) and TD(1) models, and three free parameters (\u03b1, \u03b8 and \u03bb) in TD(\u03bb). There was one free parameter \u03b8 in the model-based model, and four free parameters (\u03b1, \u03b8, \u03bb and \u03c9) in the hybrid model.\nExcessive distance. Excessive distance (ED) has been a widely used index to indicate wayfinding efficiency19,22,49,53 which was defined as:\nAn ED of 0 indicated perfect wayfinding performance (actual traversed distance equals to optimal distance) and an index of 1 indicated the actual traversed distance was 100% longer than the optimal distance. In our study, because states and state transitions were compartmentalized by rooms, we used the number of rooms to represent distance."
        },
        {
            "heading": "Results",
            "text": "We used JASP (JASP Team, 2021) and Cocor54 for statistical analyses, and Matplotlib55 and Seaborn for data visualization.\nTD(\u03bb) outperformed other TD models in fitting spatial navigation behavior. We first compared the TD family algorithms in modeling navigation behavior in the Fixed and Random phases, separately (Fig.\u00a03). In the Fixed phase, the one-way repeated ANOVA, with the three TD models as independent variable and BIC as dependent variable, was significant (F(2,226) = 49.77, p < 0.001, \u03b72 = 0.30). Paired t-test showed that TD(\u03bb) outperformed TD(1) (t(113) = \u2212\u00a07.04, p < 0.001, Cohen\u2019s d = \u2212\u00a00.66), and was similar to the TD(0) model (t(113) = 0.94, p = 0.35, Cohen\u2019s d = 0.09). The TD(0) model outperformed the TD(1) model (t(113) = \u2212\u00a0 7.59, p < 0.001, Cohen\u2019s d = \u2212\u00a00.711; Fig.\u00a03A). In the Random phase, the one-way repeated ANOVA was also significant (F(2,226) = 10.48, p < 0.001, \u03b72 = 0.09). Paired t-test showed that TD(\u03bb) outperformed TD(1) (t(113) = \u2212\u00a0 3.62, p < 0.001, Cohen\u2019s d = \u2212\u00a00.34), and the TD(0) model (t(113) = \u2212\u00a04.52, p < 0.001, Cohen\u2019s d = \u2212\u00a00.42). There was no significant difference between the TD(0) and the TD(1) models (t(113) = 1.36, p = 0.18, Cohen\u2019s d = 0.13; Fig.\u00a03B).\n(6)QMB(s) \u2190 \u2211\na \u03c0(a|s)+\n\u2211\ns\u2032 ,r p ( s\u2032, r|s, a )[ r + \u03b3QMB ( s\u2032 )]\n(7)Qhybrid = (1\u2212 \u03c9)QMF + \u03c9QMB\n(8)NLL(X) = \u2212 n \u2211\nt=1\nlogp(at |X)\n(9)XMLE = arg minx NLL(X)\n(10)BIC = klogn+ 2XMLE\n(actual traversed distance\u2212 optimal distance)/optimal distance.\n8 Vol:.(1234567890) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nOverall, the TD(\u03bb) was the best performing model among our selection of model-free models, and therefore we used it for the hybrid model.\nThe hybrid model outperformed the model-free and model-based models in both phases. We next compared which model, namely the model-free, model-based and the hybrid, provided the best fit in the Fixed and Random phases, respectively (Fig.\u00a04). In the Fixed phase, the one-way repeated ANOVA was significant (F(2,226) = 105.09, p < 0.001, \u03b72 = 0.48). Paired t-test showed that the hybrid model outperformed the modelfree (t(113) = \u2212\u00a0 4.07, p < 0.001, Cohen\u2019s d = \u2212\u00a0 0.38), and the model-based models (t(113) = \u2212\u00a0 17.93, p < 0.001, Cohen\u2019s d = \u2212\u00a01.68). Furthermore, the model-free model outperformed the model-based model (t(113) = \u2212\u00a08.34, p < 0.001, Cohen\u2019s d = \u2212\u00a00.78; Fig.\u00a04A). In the Random phase, the one-way repeated ANOVA was also significant (F(2,226) = 114.74, p < 0.001, \u03b72 = 0.50). Paired t-test showed that the hybrid model outperformed the model-free (t(113) = \u2212\u00a011.62, p < 0.001, Cohen\u2019s d = \u2212\u00a01.09), and the model-based models (t(113) = \u2212\u00a03.07, p = 0.003, Cohen\u2019s d = \u2212\u00a00.29). Contrary to the results in the Fixed phase, the model-based model outperformed the model-free model in the Random phase (t(113) = \u2212\u00a010.59, p < 0.001, Cohen\u2019s d = \u2212\u00a00.99; Fig.\u00a04B). Clearly, the hybrid model the best performing model in both navigational phases.\nThe correlation between navigation strategy (\u03c9) and exploration\u2013exploitation tendency (\u03b8) was modulated by navigation requirement. Lastly, we asked whether navigation strategy (\u03c9) and exploration\u2013exploitation (\u03b8) was modulated by navigation requirements. \u03c9 was significantly smaller in the Fixed than in the Random phases (t(113) = \u2212\u00a017.56, p < 0.001, Cohen\u2019s d = \u2212\u00a01.64), suggesting that in general, participants\u2019 navigation behaviors reflected more model-free in the Fixed than in the Random phases. On the other hand, \u03b8 was significantly larger in the Fixed than in the Random phases (t(113) = \u2212\u00a07.75, p < 0.001, Cohen\u2019s d = 0.73), suggesting that in general, participants used the same navigation strategy more consistently in the Fixed than in the Random phases. We then compared the correlations \u03c9 and \u03b8 in the Fixed and Random phases (Table\u00a01). In the Fixed phase, this correlation was significantly positive (r(114) = 0.25, p = 0.007), suggesting that in the Fixed phase, the cognitive mappers tended to exploit or used the same navigation strategy more consistently than the route followers. In the Random phase, on the other hand, this correlation became significantly negative (r(114) = \u2212\u00a0 0.35, p < 0.001), suggesting that in the Random phase, the cognitive mappers tended to explore or vary their navigation strategy more than the route followers. Together, these results supported our theoretical framework of one important way that cognitive mappers differ from route followers: cognitive mappers are flexible and efficient not only by virtue of making use of cognitive map-based strategies, but by adaptively avoiding or embracing strategy change based on different navigational requirements.\nTo demonstrate that \u03c9 was also correlated with objectively observed performance, we correlated \u03c9 and excessive distance (ED). We found that \u03c9 was correlated with ED significantly in both Fixed and Random phases (rs(114) < \u2212\u00a00.51, ps < 0.001), supporting the prediction that regardless of navigational requirements, more modelbased behavior is indicative of being a better, more spatially-efficient navigator."
        },
        {
            "heading": "Discussion",
            "text": "The current study compared five RL models in characterizing human behaviors in navigation tasks with different requirements, and we found that a hybrid model, consisting of both model-free and model-based learning, provided the best fit in both navigation tasks, despite being penalized (in model comparison) for its greater complexity. Furthermore, through individual differences analyses, we found that the reliance on the model-based system (\u03c9) and the variability of using the default strategy (\u03b8) increased as the randomness of the wayfinding increased. Interestingly, the correlation between \u03c9 and \u03b8 was modulated by task requirements, such that individuals who relied more on model-based learning were more likely to stick with one navigation strategy when wayfinding was from the same starting location, but were more likely to vary their navigation strategy when wayfinding was from an unpredictable starting location.\nWe first compare three model-free models, namely the TD(0), TD(\u03bb) and TD(1), to determine the role of memory updating in spatial navigation. As mentioned in the Introduction and Methods, TD(0) assumes memory updating only occurs in the most recent visited location. On the other hand, the eligibility trace in TD(\u03bb) assumes that memory updating occurs in all previously visited locations and the amount of updating decreased over time if such locations were not visited again. TD(1) is the special case of TD(\u03bb) that memory updating is the same in all previously visited locations regardless of their visitation frequency. Our results show that although the TD(\u03bb) model is not better than the TD(0) model in the Fixed phase, it does outperform the other model-free models of our navigators\u2019 cognition in the Random phase. As evidenced by the superiority of the hybrid model in this phase over a purely model-based approach, and by virtue of TD(\u03bb)\u2019s properties, these findings suggest that to the extent that people exhibit TD-like profiles their spatial memory updating typically occurs in a more continuous manner across all previously visited locations and scales with visitation frequency in spatial navigation, especially when wayfinding is not completely deterministic (i.e., the Random phase). Our findings not only complement the literature on memory updating in spatial navigation33,42\u201346, but also extend these findings via a computational approach.\nAs stated in the Introduction, the increasing familiarity of the environment and the demands of the Random phase would encourage participants to rely on map-like knowledge in a greater extent in the Random phase. Indeed, when compared the performance of model-free learning against model-based learning, we find that the model-free learning outperforms model-based in the Fixed phase, but is outperformed by model-based in the Random phase, which validates our modeling methods. The hybrid model, on the other hand, outperforms the model-free and model-based models in both learning phases, suggesting that the majority of the individuals did not entirely rely on either the model-free or model-based learning systems, in either scenario, but instead fell\n9 Vol.:(0123456789) Scientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1\nsomewhere in between. These findings are aligned with the well-established findings of the substantial individual differences in spatial navigation, such that\u2014although some individuals have little or near perfect configural knowledge of their environment\u2014most of fall somewhere in between on various objective measures22\u201324,26,36,56. To the best of our knowledge, this is the first study showing that a hybrid RL model significantly outperforms the model-free and model-based models in human spatial navigation explicitly.\nAfter confirming that the results from the hybrid model were in line with the findings in the literature, we extracted the two key parameters to give a new understanding of individual differences in spatial navigation in such tasks. A large portion of the literature in human spatial navigation investigates what makes a good navigator57, and a critical component is navigation strategy (route-following or cognitive mapping)29\u201331,34. As mentioned in the \u201cIntroduction\u201d, the most widely used method of measuring one\u2019s navigation strategy is the dual-solution task, which may simplify the complex navigation behaviors observed in humans. The parameter \u03c9 of the hybrid model indicates the proportion which people use a cognitive map in spatial navigation relative to following familiar routes, and it is significantly correlated with, but importantly is not identical to, navigation performance. The critical difference between \u03c9 and the solution index from the dual-solution task is that the solution index indicates a general navigation strategy as a trait over a number of trials, but \u03c9 can be used to indicate a general strategy (like what we did in the current study), but also be used in a trial-by-trial basis where each trial has its own \u03c9. In other words, the RL models can provide a finer grained measure of one\u2019s navigation strategy compared to traditional methods.\nAnother important and unique contribution of the RL models is the parameter of the exploration\u2013exploitation tendency (\u03b8), which reflects how consistently one uses their default strategy and is very difficult to estimate with traditional methods. The combination of \u03c9 and \u03b8 provides a powerful and unique way to further understand what contribute to the substantial individual differences in spatial navigation. For example, in our study the change in correlation between \u03c9 and \u03b8 reveals that cognitive mappers use different strategies more often when the randomness in the task increases, which is a novel finding on what makes a good navigator57. This finding is also important because although model-based behavior itself captures navigational flexibility, we also see that people with good cognitive maps and an ability to engage in model-based behavior are also the same people who more often appropriately shift between frameworks For example, imagine rounding a corner and a familiar set of cues becomes visible\u2014it may be that a familiar path forward from this point, aligning with model-free behavior, is in fact the most efficient option, and our computational approach reveals how a good cognitive mapper may make this switch. Taken together, these findings not only give us a new perspective for understanding the individual differences in human spatial navigation ability, through the lens of navigation strategies and the consistency of using such strategies, but also have important implications for how one might improve spatial navigation ability in humans as well as artificial agents."
        },
        {
            "heading": "Limitations",
            "text": "In the current study, the Fixed phase always preceded the Random phase. As elaborated in the \u201cMethods\u201d, this was particularly important for the design and current research questions\u2014however, an obvious step for subsequent studies would be to leverage a longer design that includes switches back to Fixed trials (either interleaved with Random or blocked) at different stages of learning, in order to understand how the experience of navigating our environment in more flexible ways (Random) may influence our internal model used when navigating more familiar relationships between locations.\nAnother important design consideration was that the environment layout was a regular grid-shape without any global landmarks. These attributes combine to make orientation in the global space more difficult and dependent on learning relationships between adjacent rooms. On the one hand, we view this as a strength of the design, especially in the context of studying state-to-state associations in a reinforcement learning model. And indeed, there are many real-world scenarios that share features with our virtual environment\u2014navigating enclosed spaces, such as the interior of a hotel, mall, or hospital, and to a lesser extent subway maps. In addition, when nestled down among tall buildings in city streets our view of landmarks are more likely to be constrained to vista space and a local scale58, somewhat akin to the present task\u2019s constraints (but not wholly). Nonetheless, another obvious step forward from this research is to investigate whether the shape of the environment and the presence of global landmarks affects the pattern of results reported in the current study. One might hypothesize that\u2014by facilitating shortcutting\u2014global landmarks would reduce the propensity of some people to exploit familiar route segments in lieu of exploration at the Random phase transition. On the other hand, pivoting from some of our other recent findings22, it may also be the case that better navigators are those who flexibly take advantage of these additional cues more-so than poorer navigators, in which case an intriguing prediction from the present findings is that such cues may exacerbate differences between more and less-model-based individuals, and their variability in strategy, according to both task demands and cue availability. Future studies could test these complementary ideas."
        },
        {
            "heading": "Conclusions",
            "text": "In the current study we compare five reinforcement learning models in fitting human spatial navigation behaviors. We find that the hybrid model provides the best fit to the data regardless of task requirements, and it sheds important light on how task requirement modulates the navigation strategy (the balance between model-free and model-based), the consistency of using, and the interaction between these two factors. All in all, we show that reinforcement learning models provide a finer grained characterization of navigation strategy in a continuous manner based on individual\u2019s prior navigation history, which not only complements and extends the existing methods of studying individual differences in spatial navigation, but also suggests that the consistency of using a navigation strategy based on navigation requirements is an important factor of what makes a good navigator.\n10\nVol:.(1234567890)\nScientific Reports | (2022) 12:13923 | https://doi.org/10.1038/s41598-022-18245-1"
        },
        {
            "heading": "Data availability",
            "text": "The data that support the findings of this study and the analysis code are available from the corresponding authors upon request.\nReceived: 4 April 2022; Accepted: 8 August 2022"
        },
        {
            "heading": "Author contributions",
            "text": "Q.H., J.L., L.E. and T.B. designed the experiment. J.L., L.E. and E.B. collected the data. Q.H. performed the data analysis. Q.H. wrote the main manuscript text and prepare the figures. All authors revised and reviewed the manuscript."
        },
        {
            "heading": "Funding",
            "text": "Funding was provided by Warren Alpert Foundation to Q.H. and National Institutes of Health (Grant No. 1-R21AG063131) to T.B."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Correspondence and requests for materials should be addressed to Q.H.\u00a0or\u00a0T.I.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2022"
        }
    ],
    "title": "A comparison of reinforcement learning models of human spatial navigation",
    "year": 2022
}