{
    "abstractText": "forcing: effects of intrinsic noise and common input Jonas Ranft and Benjamin Lindner 1)Institut de Biologie de l\u2019ENS, Ecole Normale Sup\u00e9rieure, CNRS, Inserm, Universit\u00e9 PSL, 46 rue d\u2019Ulm, 75005 Paris, France 2)Bernstein Center for Computational Neuroscience Berlin, Philippstra\u00dfe 13, Haus 2, 10115 Berlin, Germany and Physics Department of Humboldt University Berlin, Newtonstra\u00dfe 15, 12489 Berlin, Germany",
    "authors": [
        {
            "affiliations": [],
            "name": "Jonas Ranft"
        },
        {
            "affiliations": [],
            "name": "Benjamin Lindner"
        }
    ],
    "id": "SP:c6ebabab0bb3bc53dce5f699d6d3acc5c5e855da",
    "references": [
        {
            "authors": [
                "Sompolinsky",
                "A. Crisanti",
                "H.J. Sommers"
            ],
            "title": "Chaos in random neural networks,",
            "venue": "ena,\u201d Rev Mod Phys,",
            "year": 2005
        },
        {
            "authors": [
                "A. Rusakov",
                "L.P. Savtchenko",
                "P.E. Latham"
            ],
            "title": "Noisy Synaptic Conductance: Bug or a Feature?",
            "venue": "Neurons (Oxford University Press,",
            "year": 2004
        }
    ],
    "sections": [
        {
            "heading": "A self-consistent analytical theory for rotator networks under stochastic",
            "text": "forcing: effects of intrinsic noise and common input\nJonas Ranft1 and Benjamin Lindner2 1)Institut de Biologie de l\u2019ENS, Ecole Normale Supe\u0301rieure, CNRS, Inserm, Universite\u0301 PSL, 46 rue d\u2019Ulm, 75005 Paris, France 2)Bernstein Center for Computational Neuroscience Berlin, Philippstra\u00dfe 13, Haus 2, 10115 Berlin, Germany and Physics Department of Humboldt University Berlin, Newtonstra\u00dfe 15, 12489 Berlin, Germany\n(*Electronic mail: jonas.ranft@ens.psl.eu)\n(Dated: April 15, 2022)\nDespite the incredible complexity of our brains\u2019 neural networks, theoretical descriptions of neural dynamics have led to profound insights into possible network states and dynamics. It remains challenging to develop theories that apply to spiking networks and thus allow one to characterize the dynamic properties of biologically more realistic networks. Here, we build on recent work by van Meegen & Lindner who have shown that \u201crotator networks,\u201d while considerably simpler than real spiking networks and therefore more amenable to mathematical analysis, still allow to capture dynamical properties of networks of spiking neurons. This framework can be easily extended to the case where individual units receive uncorrelated stochastic input which can be interpreted as intrinsic noise. However, the assumptions of the theory do not apply anymore when the input received by the single rotators is strongly correlated among units. As we show, in this case the network fluctuations become significantly non-Gaussian, which calls for a reworking of the theory. Using a cumulant expansion, we develop a self-consistent analytical theory that accounts for the observed non-Gaussian statistics. Our theory provides a starting point for further studies of more general network setups and information transmission properties of these networks.\nNot surprisingly, networks of recurrently connected neurons , such as e.g. found in the human cortex, are capable to support a very rich repertoire of dynamical states. A remarkable feature of such networks is that they can exhibit asynchronous, irregular activity even in the case of deterministic dynamics, where the disorder due to the random connectivity between neurons gives rise to temporal fluctuations of their activity. Understanding the statistics and dynamical properties of network states by mathematical analysis of models of recurrent networks remains a challenging problem especially for networks of spiking neurons, for which analytical expressions e.g. for the network noise autocorrelation function are hard to obtain. For these reasons, we investigate here a much simpler model class of recurrently connected oscillators that nevertheless has been shown to give a reasonable account of the statistics of spiking networks, and that allows one to obtain in comparison much simpler self-consistent equations for the network statistics. Using this model, we study the effects of intrinsic noise uncorrelated among oscillators and shared common noise. Interestingly, we find that in the presence of common noise the network fluctuations become nonGaussian even if both the self-generated network noise in the absence of noise and the additional common noise are Gaussian. To account for the non-Gaussian statistics, we develop a self-consistent theory that includes higher cumulants of the network fluctuations. Our theory provides a satisfactory description of the observed power spectra and autocorrelation functions of the network fluctuations and rotator dynamics, and can serve as a starting point for further investigations of networks in which non-Gaussian statistics due to input correlations might play a role.\nI. INTRODUCTION\nNetworks of recurrently coupled oscillators have been studied in a wide variety of contexts, representing spatially inhomogeneous chemical reactions1, neural networks2\u20134, or power grids5. A classic paradigm has been and continues to be the Kuramoto model, which allowed many analytical insights in e.g. the synchronization dynamics depending on the disorder in the system1,6,7. More generally, networks of recurrently connected units can be in a regime where they exhibit apparently noisy dynamics despite being entirely deterministic, a feature which has received a lot of attention in the context of neural networks8\u201316. While such noisy dynamics can easily be observed e.g. in spiking network models where it manifests itself in the \u201casynchronous state\u201d17, it is considerably harder to analytically determine the effective noise characteristics for such models. In order to thoroughly characterize the internally-generated noise as a function of network parameters and understand its impact e.g. on the statistics of individual units in the network, it seems important to have models at one\u2019s disposal that capture the phenomenon while still allowing one to gain analytical insights.\nRecently, van Meegen & Lindner14 introduced the so-called \u201crotator network\u201d of recurrently coupled rotators, in which the coupling between the oscillating units is an arbitrary function of the \u201cpresynaptic\u201d oscillator phases. Despite being a strongly simplified description of real spiking networks, the authors showed that the model still allows to capture dynamical properties of model networks of spiking neurons. Importantly, for the rotator network one can self-consistently calculate the statistics of the internally-generated fluctuations (network noise). Here, we will build upon this work to investigate the influence of additional intrinsic noise sources at the level\nar X\niv :2\n20 4.\n06 63\n1v 1\n[ q-\nbi o.\nN C\n] 1\n3 A\npr 2\n02 2\n2 of the individual units as well as shared common input. The additional input leads to modifications of the self-consistent equation for the autocorrelation function of the network noise and impacts the individual units\u2019 statistics. When the external inputs are uncorrelated across the units, the required modifications of the theory are straightforward. However, if there are substantial correlations in the input across units, basic assumptions of the existing theory break down and require a more substantial revision with regard to the non-Gaussian statistics of the network noise. Extending the theory accordingly, we aim to go a step further towards increased biological realism in the context of neural networks while maintaining the analytical tractability of the model.\nAfter briefly introducing the basic model and reviewing previous results in section II, we extend the rotator network model to the case where individual units are subject to additional intrinsic noise in section III. Intrinsic noise can e.g. be caused by stochastic ion channel openings in the case of neural networks, or by fluctuating inputs or loads in power grids. In section IV, we discuss the effect of shared external input on all units. Whereas the internally-generated network noise remains Gaussian in the presence of intrinsic noise only, it becomes non-Gaussian in the presence of correlated inputs. This is captured by a modified self-consistent equation that takes into account higher cumulants of the internallygenerated noise. We conclude in section V with a summary of our results and a brief discussion of their implications."
        },
        {
            "heading": "II. PREVIOUS RESULTS: NETWORK DYNAMICS WITHOUT EXTERNAL INPUTS",
            "text": "In a previous study, van Meegen & Lindner introduced a simple model of a recurrent rotator network, where individual units are described as phase variables \u03b8m, each driven by an intrinsic oscillation frequency \u03c9m and receiving input from the other units in the network,\n\u03b8\u0307m = \u03c9m +\u2211 n Kmn f (\u03b8n), (1)\nwhere the recurrent input is specified by the matrix of connection weights Kmn between units n and m, and the coupling function f (\u00b7) that depends only on the \u2018input phases\u2019 \u03b8n. To be specific, we will assume Kmn to be a Gaussian random matrix with \u3008Kmn\u3009= 0 and \u3008KmnKm\u2032n\u2032\u3009= K 2\nN \u03b4mm\u2032\u03b4nn\u2032 unless specified otherwise, but see section V for a discussion of other choices for the connectivity. Based on dynamical mean-field theory, Eq. (1) can be interpreted as a stochastic dynamics for the \u03b8m with an internally generated network noise \u03bem,\n\u03bem = \u2211 n Kmn f (\u03b8n), (2)\nthe statistics of which has to be determined self-consistently. The relative simplicity of the model allows one to derive an analytical solution for the noise autocorrelation function C\u03be (\u03c4)\u2261 \u3008\u03bem(t)\u03bem(t+\u03c4)\u3009. To this end, following van Meegen & Lindner, we introduce the auxiliary function\n\u039b(\u03c4) = \u222b \u03c4\n0 dt(\u03c4\u2212 t)C\u03be (t), (3)\nwhich implies\nC\u03be (\u03c4) = \u039b\u0308(\u03c4). (4)\nIn the case of the autonomous network described by Eq. (1), the function \u039b(\u03c4) is a solution of the ordinary differential equation\n\u039b\u0308(\u03c4) = K2 \u2211 l |Al |2\u03a6(l\u03c4)exp[\u2212l2\u039b(\u03c4)]. (5)\nHere, Al are the coefficients of a Fourier expansion of the coupling function, f (\u03b8) = \u2211l Al exp[il\u03b8 ], and \u03a6(x) = \u3008exp[i\u03c9nx]\u3009\u03c9n is the characteristic function of the intrinsic frequencies. The above equation for \u039b follows from a selfconsistency condition for the noise autocorrelation, where the average is taken over realizations of the connectivity matrix Kmn, intrinsic frequencies \u03c9n, initial conditions, and the effective noise \u03bem in a self-consistent manner.\nWe would like to emphasize that, while the relations of \u039b(\u03c4) in Eqs. (3) and (4) hold true throughout this manuscript, the dynamical equation that one obtains for \u039b will depend on the situation considered (absence or presence of intrinsic and/or common noise) and, for the most involved case of common noise, on the level of approximation used. It would be possible to add a label to the function \u039b that would distinguish these different cases, but we abstain from doing so for the ease of notation.\nWe quickly recapitulate the derivation of van Meegen & Lindner for reference, also because our new results will be obtained along similar lines. Using the Gaussian statistics when averaging over the connectivity matrix, one first obtains\nC\u03be (\u03c4) = \u2211 n,n\u2032 \u3008KmnKmn\u2032 f (\u03b8n(t)) f (\u03b8n\u2032(t + \u03c4))\u3009K,\u03c9,\u03b80 (6a)\n= \u2211 n,n\u2032 \u3008KmnKmn\u2032\u3009K\u3008 f (\u03b8n(t)) f (\u03b8n\u2032(t + \u03c4))\u3009K,\u03c9,\u03b80 (6b)\n= K2\nN \u2211n \u3008 f (\u03b8n(t)) f (\u03b8n(t + \u03c4))\u3009K,\u03c9,\u03b80 . (6c)\nNote that the disorder of the connectivity matrix in principle still contributes to the fluctuating dynamics of the \u03b8n(t). The self-consistency ansatz for statistics of the internallygenerated network fluctuations now consists in considering the effective stochastic dynamics \u03b8\u0307m = \u03c9m + \u03bem for the rotators in the network and to average over realizations of the network noise \u03bem(t) in the following. Using the Fourier expansion of f (\u00b7) and \u03b8m(t) = \u03b8m(t0) + \u222b t t0 dt \u2032\u03b8\u0307m(t \u2032), one thus\n3 obtains\nC\u03be (\u03c4) K2/N = \u2211 n \u2211 l,l\u2032 AlAl\u2032\u3008eil\u03b8n(t)+il \u2032\u03b8n(t+\u03c4)\u3009\u03be,\u03c9,\u03b80 (7a)\n= \u2211 n \u2211 l,l\u2032\nAlAl\u2032\u3008ei(l+l \u2032)\u03b8n(t0)\u3009\u03b80\u00d7\n\u3008eil \u222b t t0 dt \u2032\u03b8\u0307n(t \u2032)+il\u2032 \u222b t+\u03c4 t0 dt \u2032\u03b8n(t \u2032)\u3009\u03be,\u03c9 (7b)\n= \u2211 n \u2211 l |Al |2\u3008eil\u03c9n\u03c4\u3009\u03c9\u3008eil\n\u222b t+\u03c4 t dt\n\u2032\u03ben(t \u2032)\u3009\u03be (7c)\n= \u2211 n \u2211 l |Al |2\u03a6(l\u03c4)\u3008eilyn(\u03c4;t)\u3009 (7d)\n= \u2211 n \u2211 l |Al |2\u03a6(l\u03c4)e\u2212\nl2 2 \u3008yn(\u03c4;t) 2\u3009 (7e)\n= N \u2211 l |Al |2\u03a6(l\u03c4)e\u2212l\n2 \u222b \u03c4 0 dt\n\u2032(\u03c4\u2212t \u2032)C\u03be (t \u2032). (7f)\nA few remarks are in order: To ensure that the result does not depend on a particular initialization, one can average over the absolute phases \u03b8n(t0) at a given (arbitrary) reference time t0 and require that this average can be taken independently of the average over the \u03ben. For uniformly distributed phases \u03b8n(t0), one immediately obtains \u3008ei(l+l\n\u2032)\u03b8n(t0)\u3009\u03b8n(t0) = \u03b4l,l\u2032 . We furthermore introduced the new variable yn(\u03c4; t) =\u222b t+\u03c4\nt dt \u2032\u03ben(t \u2032). In the limit of a large network (N 1), one can assume that the \u03ben are Gaussian distributed, and thus that yn(\u03c4; t) obeys the relation \u3008eilyn\u3009= e\u2212l\n2\u3008y2n\u3009/2. The second moment \u3008y2n\u3009 can be found according to\n\u3008yn(\u03c4; t)2\u3009= \u222b t+\u03c4 t dt1 \u222b t+\u03c4 t dt2 \u3008\u03ben(t1)\u03ben(t2)\u3009\n= 2 \u222b \u03c4\n0 dt(\u03c4\u2212 t)C\u03be (t) (8)\nafter a change of variables and using the symmetry of C\u03be (t) around the origin. Since all units are statistically equivalent and their correlation functions the same, the sum over n then becomes trivial.\nThe differential equation for \u039b (Eq. (5)) is eventually obtained with the substitution Eq. (3), and the noise autocorrelation follows as\nC\u03be (\u03c4) = K2 \u2211 l |Al |2\u03a6(l\u03c4)exp[\u2212l2\u039b(\u03c4)]. (9)\nThe autocorrelation function Cxm(t) of the pointer xm(t) = ei\u03b8m(t) for a given rotator m is then rather straightforwardly obtained as\n\u3008xm(t)\u2217xm(t + \u03c4)\u3009= \u3008ei(\u03b8m(t+\u03c4)\u2212\u03b8m(t))\u3009\n= ei\u03c9m\u03c4\u3008ei \u222b t+\u03c4 t dt \u2032\u03bem(t \u2032)\u3009\n= ei\u03c9m\u03c4\u2212\u039b(\u03c4).\nOf note, Cxm(t) explicitly depends on the particular intrinsic frequency \u03c9m of the rotator m. Averaged over all rotators, the autocorrelation becomes Cx(t) = \u03a6(\u03c4)e\u2212\u039b(\u03c4)."
        },
        {
            "heading": "III. ROTATOR NETWORK WITH INDIVIDUAL NOISE SOURCES",
            "text": "Focussing on recurrent input, van Meegen & Lindner did not address the effect of external inputs or other noise sources acting on individual units. Typically, however, in many applications, e.g. in biological neural networks, the single units are subject to intrinsic noise, e.g. channel noise or unreliable synaptic transmission in the case of neural networks18\u201320.\nIt seems important to understand the degree to which these additional inputs may shape the dynamics of the network and individual units. There is a rich literature on the Kuramoto model driven by individual and common noise beginning with Kuramoto himself, see e.g.1,21,22, but how noise affects the rotator dynamics considered here is not known. In this section, we will consider the case of intrinsic noise sources \u03b7m that are uncorrelated between different units (correlated inputs will be addressed in section IV). The model then reads\n\u03b8\u0307m = \u03c9m +\u2211 n Kmn f (\u03b8n)+\u03b7m, (10)\nthe recurrent input again being specified by the connectivity matrix Kmn and coupling function f (\u00b7). We consider the intrinsic fluctuations to be independent Gaussian noise sources with \u3008\u03b7m\u3009= 0, \u3008\u03b7m(t)\u03b7n(t + \u03c4)\u3009= \u03b4mnC\u03b7(\u03c4)."
        },
        {
            "heading": "A. Derivation of the self-consistent correlation functions in the presence of intrinsic noise",
            "text": "We aim to determine how the presence of additional noise may shape the rotator autocorrelation function Cxm(\u03c4) of rotator m, the effective stochastic dynamics of which is given by\n\u03b8\u0307m = \u03c9m +\u03bem +\u03b7m, (11)\nwhere the statistics of the network noise \u03bem is still to be determined. First of all, it is not difficult to show that the network noise is not correlated with the external noise, as seen as follows:\n\u3008\u03bem(t)\u03b7m(t \u2032)\u3009= \u2211 n \u3008Kmn\u3009K\u3008 f (\u03b8n(t))\u03b7m(t \u2032)\u3009= 0 (12)\nsince \u3008Kmn\u3009K = 0. We now turn to the problem of calculating the autocorrelation function of the network noise. When all rotators are subject to intrinsic noise, we find a modified self-consistency condition for C\u03be (t) as follows. Along the lines of the derivation in the case without noise, Eq. (7c) now becomes\nC\u03be (\u03c4) K2/N = \u2211 n \u2211 l |Al |2\u3008eil\u03c9n\u03c4\u3009\u03c9\u3008eil\n\u222b t+\u03c4 t dt\n\u2032[\u03ben(t \u2032)+\u03b7n(t \u2032)]\u3009\u03be,\u03b7\n= \u2211 n \u2211 l |Al |2\u03a6(l\u03c4)\u3008eilyn(\u03c4;t)\u3009, (13)\nwhere we redefined the variable yn(\u03c4; t) = \u222b t+\u03c4\nt dt \u2032[\u03ben(t \u2032)+\u03b7n(t \u2032)]. (14)\n4 Assuming that yn(\u03c4; t) is Gaussian, we now find\n\u3008eilyn(\u03c4)\u3009= e\u2212 l2 2 \u3008yn(\u03c4) 2\u3009 (15a)\n= e\u2212l 2 \u222b \u03c4 0 dt(\u03c4\u2212t)\u3008[\u03ben(t \u2032)+\u03b7n(t \u2032)][\u03ben(t \u2032+t)+\u03b7n(t \u2032+t)]\u3009 (15b) = e\u2212l 2 \u222b \u03c4 0 dt(\u03c4\u2212t)[C\u03be (t)+C\u03b7 (t)]. (15c)\nPutting everything together, one obtains\nC\u03be (\u03c4) = K2 \u2211 l |Al |2\u03a6(l\u03c4)exp\n( \u2212l2[\u039b(\u03c4)+\u039b\u03b7(\u03c4)] ) . (16)\nHere \u039b\u03b7(\u03c4) = \u222b \u03c4\n0 dt(\u03c4 \u2212 t)C\u03b7(t) is given by the autocorrelation function C\u03b7(t) of the intrinsic noise sources23 and the function \u039b(\u03c4) = \u222b \u03c4 0 dt(\u03c4 \u2212 t)C\u03be (t), defined as before, now obeys the ordinary differential equation\n\u039b\u0308(\u03c4) = K2 \u2211 l |Al |2\u03a6(l\u03c4)exp\n( \u2212l2[\u039b(\u03c4)+\u039b\u03b7(\u03c4)] ) . (17)\nOnce we have calculated \u039b(t), we know the autocorrelation function of the network noise C\u03be (t) via Eq. (16), but can now also determine the autocorrelation function Cxm(t) of the m-th oscillator (the complex pointer xm = ei\u03b8m ):\nCxm(\u03c4) = exp[i\u03c9m\u03c4\u2212\u039b(\u03c4)\u2212\u039b\u03b7(\u03c4)] . (18)\nSimilarly to the case without intrinsic noise, this can be derived using \u03b8m(t) = \u03b8m(t0) + \u222b t t0 dt\n\u2032\u03b8\u0307m(t \u2032) with Eq. (11) and using Eq. (15) for l = 1. Eqs. (16-18) constitute the theory for the self-consistent correlation functions for a rotator network with individual intrinsic Gaussian noise.\nSo far we have not made any assumptions about the temporal correlations of the intrinsic noise, i.e., our theory holds for a general colored noise \u03b7m(t). For our numerical examples in the following, we consider for simplicity Gaussian white noise, for which C\u03b7(\u03c4) = 2D\u03b7 \u03b4 (\u03c4), implying \u039b\u03b7(\u03c4) = D\u03b7 \u03c4 ."
        },
        {
            "heading": "B. Effects of intrinsic white noise on the rotator autocorrelation",
            "text": "In order to test our theory, we simulated the rotator network with additional intrinsic white noise for various combinations of noise intensity D\u03b7 and coupling strength K. The corresponding theoretical predictions for Cxm(t) obtained from Eqs. (17) and (18) are nicely confirmed for all combinations used, see Fig. 1, although the number of units in the network (N = 100) is not excessively large. For a white noise, Eq. (18) reads\nCxm(\u03c4) = exp[i\u03c9m\u03c4\u2212\u039b(\u03c4)\u2212D\u03b7 \u03c4] , (19)\nfrom which we can expect a damping of the autocorrelation function with increasing noise strength D\u03b7 . This is indeed observed in the case of both weak (Fig. 1a) and strong (Fig. 1b) network noise.\nThe network-mediated influence of the intrinsic noises \u03b7n, n 6= m, on the autocorrelation function of rotator m enters the\n(a)\nexpression for Cxm(t) via the modified network-noise autocorrelation function C\u03be (t). In order to assess this indirect effect of the intrinsic noise, we compare our result to the case where only a specific, single unit m receives additional intrinsic noise \u03b7m according to Eq. (10), while all other units of the network evolve according to Eq. (1). For the particular rotator m that is subject to intrinsic noise, Cxm(t) is then still given by Eq. (18) with the exponential damping term e\u2212D\u03b7 t , but where the network-noise autocorrelation C\u03be (t) is the \u201cintrinsic-noise free\u201d version C\u03be (t) = \u039b\u0308(t) with \u039b obeying Eq. (5).\nStochastic network simulations and theoretical predictions for Cx(t) again nicely agree (not shown) but look very similar to the case where all units are subject to intrinsic noise shown in Fig. 1. We nevertheless identify two distinct effects due to the network-mediated influence of the other intrinsic noise sources, as shown in Fig. 2: For weak network noise (Fig. 2a), we find that the intrinsic noise on the other units in the network further decorrelates oscillator m relative to the case of a single noisy unit with \u201cintrisic-noise free\u201d recurrent input. Interestingly, in the case of strong network noise (Fig. 2b), the\n5 (a)\n(b)\nFigure 2. The network-mediated effect of noise in \u201cbackground\u201d units on rotator autocorrelation function Cxm of noisy unit m (solid lines) can be assessed by comparison with the autocorrelation function Cxm of unit m in the case where only unit m is subject to noise (dashed lines). The autocorrelation in the absence of any noise is also shown (thin lines). (a) Intermediate noise level with K = 0.5, D\u03b7 = 0.2; (b) strong noise level with K = 2, D\u03b7 = 0.5. All curves are obtained from the theory, with dt = 0.001.\nopposite effect can be observed, as the autocorrelation of the specific rotator m decays faster in the case where it is the only one subject to intrinsic noise while the rest of the network does evolve without additional intrinsic noise."
        },
        {
            "heading": "IV. ROTATOR NETWORK WITH COMMON",
            "text": "EXTERNAL INPUT\nSo far we extended the rotator network model of Ref. 14 to the case where individual units are subject to intrinsic noise, and assumed independence of the noise sources for different units. In the context of neural networks, possible origins of such a noise term could be the neurons\u2019 stochastic ion channel kinetics and spontaneous release of neurotransmitters18, for which the assumption of independence seems clearly satisfied. Another possible interpretation of the noise term \u03b7m is that it results from external (as opposed to recurrent) synaptic inputs that are uncorrelated across units.\nMore generally however, neurons may also receive correlated external inputs, which could stem e.g. from other brain areas that project broadly to a local recurrently connected network. Here, we will thus relax the assumption that any additional inputs to the individual units are independent and study the potential effects on the network dynamics. Regarding\nthe theory, one might be tempted to think that it can be extended to this new case without major changes and that its main assumptions, e.g. the Gaussian distribution of the network noise, are still valid. We will show however that the network noise ceases to be Gaussian-distributed if the external noise is significantly correlated across units. Consequently, the self-consistent theory of Ref. 14 cannot easily be extended to this case. Instead, we have to more substantially rework the theory. By taking higher-order correlation functions of the network noise into account, we can arrive at an approximate theory that captures the resulting non-Gaussian network statistics surprisingly well.\nTo be specific, we consider the following model, where \u03b7c is the additional input common to all units:\n\u03b8\u0307m = \u03c9m +\u2211 n Kmn f (\u03b8n)+\u03b7m +\u03b7c. (20)\nWe assume that \u03b7c is a Gaussian noise which is uncorrelated with the intrinsic noise \u03b7m of each unit m, \u3008\u03b7c(t)\u03b7m(t +\u03c4)\u3009 \u2261 0, and that its temporal statistics are described by the autocorrelation function \u3008\u03b7c(t)\u03b7c(t + \u03c4)\u3009 = Cc(\u03c4). Furthermore, along the same lines as sketched for the intrinsic noise \u03b7m, Eq. (12), we note that \u03b7c is uncorrelated with the network noise \u03bem, \u3008\u03b7c(t)\u03bem(t + \u03c4)\u3009 = 0, by virtue of the averaging properties of Kmn.\nTo develop the theory for the case with common input, we start again by expressing the autocorrelation \u3008\u03bem(t)\u03bem(t + \u03c4)\u3009 in terms of the connectivity matrix Kmn, coupling function f (\u00b7), and rotator phases \u03b8m. The additional average over independent realizations of the common noise \u03b7c is the only departure from the case considered in the previous section and does not interfere with the initial steps of Eqs. (6) and (7), which now lead to\nC\u03be (\u03c4) = K2\nN \u2211N \u2211l |Al |2\u3008eil\u03c9n\u03c4\u3009\u03c9\u00d7\n\u3008eil \u222b t+\u03c4 t dt \u2032[\u03ben(t \u2032)+\u03b7n(t \u2032)+\u03b7c(t \u2032)]\u3009\u03be,\u03b7,\u03b7c . (21)\nBy redefining the variable yn(\u03c4; t) according to\nyn(\u03c4; t) = \u222b t+\u03c4\nt dt \u2032[\u03ben(t \u2032)+\u03b7n(t \u2032)+\u03b7c(t \u2032)] (22)\nand noting that \u3008eil\u03c9n\u03c4\u3009\u03c9 = \u03a6(l\u03c4), expression (21) can again be written more conveniently as\nC\u03be (\u03c4) = K2\nN \u2211n \u2211l |Al |2\u03a6(l\u03c4)\u3008eilyn(\u03c4;t)\u3009\u03be,\u03b7,\u03b7c . (23)\nA nave generalization of our previous approach would yield a simple modification of Eqs. (16-18). Under the assumption that the yn(\u03c4; t) were still Gaussian-distributed, it would follow that the respective expressions hold with the autocorrelation function C\u03b7(\u03c4) of the intrinsic noise simply being replaced by the sum of the noise autocorrelation functions C\u03b7(\u03c4)+Cc(\u03c4). Consider the special situation in which intrinsic noise and common noise share the same autocorrelation function and we can describe by a parameter c how much of\n6 the input noise is common, i.e. C\u03b7(\u03c4) = (1\u2212 c)Ctot(\u03c4) and Cc(\u03c4) = cCtot(\u03c4). In this case, the output statistics would not depend on c, implying that the network statistics is unaffected by the presence or absence of correlations among the additional stochastic forcings of the rotators. However, this lack of dependence is not observed in stochastic network simulations of Eq. (20), demonstrating that the nave generalization sketched above indeed fails to account for the effective stochastic dynamics of the system.\nIn order to derive a correct self-consistent expression for the network-noise autocorrelation function C\u03be (\u03c4), we thus need to determine the only remaining unknown term \u3008eilyn(\u03c4;t)\u3009\u03be,\u03b7,\u03b7c in Eq. (23). Let us consider the general case of a stochastic variable y. By definition of the cumulant-generating function, we have\n\u3008eily\u3009y = exp\n( \u221e\n\u2211 k=1\n(il)k\nk! \u03bak\n) . (24)\nHere, the \u03bak are the cumulants of y which we can calculate as a function of its moments \u3008y j\u3009y, j \u2264 k. If y is zero-centered and Gaussian-distributed, one immediately finds \u3008eily\u3009y = e\u2212l 2\u03ba2/2 = e\u2212l 2\u3008y2\u3009y/2, which led to the results obtained in the previous sections. We will see that, while yn(\u03c4; t) as defined in Eq. (22) still has a vanishing mean value in the presence of common input, it is no longer Gaussian-distributed and higher cumulants need to be taken into account.\nAssuming that the deviations from Gaussian statistics are small, our strategy is the following: First, we approximate the average \u3008eily\u3009y by taking only the first few cumulants according to Eq. (24) into account, i.e.,\n\u3008eily\u3009y \u2248 exp ( \u2212l2 \u03ba2\n2 \u2212 il3 \u03ba3 6 + l4 \u03ba4 24\n) . (25)\nHere, the higher-order cumulants \u03ba3 and \u03ba4 are finite only in the presence of correlations between the inputs of different units, and should vanish with vanishing correlations. As we will show, these higher-order cumulants have in turn themselves Gaussian and non-Gaussian contributions in a sense that will be made explicit below. In line with our assumption of a small departure from Gaussian statistics, we thus, in a second approximation, calculate each of these cumulants to zeroth order, that is, assuming purely Gaussian statistics of composite variables of the type yn+\u03b7c and yn+ym. This strategy allows us to obtain a self-consistent equation for C\u03be (\u03c4) in the presence of common input that reproduces measured autocorrelation functions as well as higher-order cumulants of yn in stochastic network simulations."
        },
        {
            "heading": "A. Calculation of the cumulants of yn(\u03c4; t)",
            "text": "The first cumulant is identical to the first moment and vanishes,\n\u3008yn(\u03c4; t)\u3009= t+\u03c4\u222b t dt \u2032[\u3008\u03ben(t \u2032)\u3009+ \u3008\u03b7n(t \u2032)\u3009+ \u3008\u03b7c(t \u2032)\u3009] = 0, (26)\nas \u3008\u03ben\u3009 = 0 because of the average over the Knn\u2032 and \u3008\u03b7n\u3009 = \u3008\u03b7c\u3009 = 0 by definition. If the first moment vanishes, the second cumulant \u03ba2, i.e. the variance, is identical to the second moment. Its calculation is analogous to the case of purely intrinsic noise discussed above, and given by\n\u3008yn(\u03c4)2\u3009= \u03c4\u222b\n0\ndt(\u03c4\u2212 t)[C\u03be (t)+C\u03b7(t)+Cc(t)]. (27)\nThe new non-trivial contributions in this theory will be the third and fourth cumulants which can also be expressed in terms of the moments; in the following we will calculate and use\n\u03ba3 = \u3008y3n\u3009, \u03ba4 = \u3008y4n\u3009\u22123\u3008y2n\u30092,\nto approximate the characteristic function in Eq. (23)."
        },
        {
            "heading": "1. Third cumulant",
            "text": "Inserting the definition of yn, we obtain for the third cumulant\n\u03ba3(\u03c4) = \u3008y3n(\u03c4; t)\u3009\n=\n\u2329 3\n\u220f j=1 t+\u03c4\u222b t dt j[\u03ben(t j)+\u03b7n(t j)+\u03b7c(t j)] \u232a . (28)\nNote that \u03ba3 does neither depend on t nor on any neuron index n, as these dependences disappear after averaging over uniform initial conditions for the phases and the disorder of the connectivity, respectively.\nBefore evaluating the triple integral, we consider separately the different contributions that arise from the correlator of the different noise sources, such as \u3008\u03ben(t1)\u03b7n(t2)\u03b7c(t3)\u3009 or \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u3009. Because \u03b7n and \u03b7c are Gaussian and uncorrelated by construction, all terms in which only these two appear vanish. Furthermore, averaging over the zero-centered, Gaussian Kmn implies that all terms with odd powers of \u03ben must also vanish, see e.g. Eq. (12). As all three integrals in Eq. (28) cover the same domain, we can arbitrarily permute the variables t j, and retain the following remaining contributions:\n\u2329 3 \u220f j=1 [\u03ben(t j)+\u03b7n(t j)+\u03b7c(t j)] \u232a /3 =\n\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u3009+ \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u3009, (29)\nwhere the equality holds under the triple integral of Eq. (28). We next aim to express both contributions in terms of the correlation functions C\u03be , C\u03b7 , and Cc, where the latter two are known and C\u03be remains to be determined self-consistently. To be specific and in line with our treatment of \u03b7m in the previous section, we will consider both \u03b7m and \u03b7c to be Gaussian white noises with respective intensities D\u03b7 and Dc for the remainder\n7 of this section. The correlation functions are then given by C\u03b7(\u03c4) = 2D\u03b7 \u03b4 (\u03c4), Cc(\u03c4) = 2Dc\u03b4 (\u03c4), which decidedly simplifies calculations. Using the definition of \u03bem, we can expand the first contribution of Eq. (29) as\n\u3008\u03bem(t1)\u03bem(t2)\u03b7m(t3)\u3009 = \u2211\nn,n\u2032 \u3008KmnKmn\u2032 f (\u03b8n(t1)) f (\u03b8n\u2032(t2))\u03b7m(t3)\u3009\n= \u2211 n,n\u2032 \u3008KmnKmn\u2032\u3009K\u3008 f (\u03b8n(t1)) f (\u03b8n\u2032(t2))\u03b7m(t3)\u3009\n= K2\nN \u2211n \u3008 f (\u03b8n(t1)) f (\u03b8n(t2))\u03b7m(t3)\u3009. (30)\nAgain along the lines of Eqs. (6) and (7) but with \u03b8\u0307m = \u03c9m + \u03bem +\u03b7m +\u03b7c, we now obtain\n\u3008\u03bem(t1)\u03bem(t2)\u03b7m(t3)\u3009= K2\nN \u2211n \u2211l |Al |2\u03a6(l(t2\u2212 t1))\u00d7\n\u3008eilyn(t2\u2212t1;t1)\u03b7m(t3)\u3009\u03be,\u03b7,\u03b7c . (31)\nInstead of the characteristic function \u3008eilyn\u3009, we now need to find an expression for the average \u3008eilyn\u03b7m\u3009.\nWe can distinguish the cases n 6= m and n = m. When n 6= m, \u3008yn(t2 \u2212 t1)\u03b7m(t3)\u3009 = 0, since \u03ben, \u03b7n, and \u03b7c are all uncorrelated with \u03b7m; therefore, \u3008eilyn(t2\u2212t1;t1)\u03b7m(t3)\u3009 = \u3008eilyn(t2\u2212t1;t1)\u3009\u3008\u03b7m(t3)\u3009 = 0. When n = m, the correlation \u3008eilyn(t2\u2212t1;t1)\u03b7n(t3)\u3009 does not vanish but should remain bounded for large N. We thus expect that, according to Eq. (31), \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u3009 scales like 1/N and vanishes for N\u2192 \u221e. Hence we are allowed to neglect the first term on the r.h.s. of Eq. (29).\nIn order to calculate the last remaining term \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u3009 of Eq. (29), we need to find the average \u3008eilyn \u03b7c\u3009 in analogy to Eq. (31) for \u03b7c. We exploit the following relation for stochastic variables a and b:\n\u3008eiab\u3009=\u2212i (\nd dr\n\u2329 ei(a+rb) \u232a) r=0 . (32)\nIn our case, a= lyn(t2\u2212t1), b= \u03b7c(t3). In Eq. (32), the expectation \u3008eiab\u3009 is expressed as the derivative of a characteristic function \u3008eiuz\u3009z evaluated at u = 1, where z = a+ rb. This characteristic function can again be expressed in terms of the cumulants of z,\n\u3008eiuz\u3009z = exp\n( \u221e\n\u2211 k=1\n(iu)k\nk! \u03ba\u0302k\n) , (33)\nwhere \u03ba\u0302k are the cumulants of z. The first cumulant \u03ba\u03021 of z vanishes. The second cumulant can be calculated from the variances and covariances of yn and \u03b7c. Above, we assumed that yn is (weakly) non-Gaussian in the presence of common input, and thus in turn z cannot be Gaussian either. However, we make here a Gaussian approximation and neglect higherorder cumulants of z, as these appear as the corrections to the corrections in Eq. (25). We then find\n\u3008eiuz\u3009z \u2248 exp ( \u2212u 2 2 ( \u3008a2\u3009+2r\u3008ab\u3009+ r2\u3008b2\u3009 )) , (34)\nand consequently for the required expression\n\u3008eilyn\u03b7c\u3009 \u2248 il\u3008yn\u03b7c\u3009e\u2212l 2\u3008y2n\u3009/2. (35)\nThis term can now be inserted in the analogous version of Eq. (31) for \u03b7c,\n\u3008\u03bem(t1)\u03bem(t2)\u03b7c(t3)\u3009\n= K2\nN \u2211n \u2211l |Al |2\u03a6(l(t2\u2212 t1))\u00d7\n\u3008eilyn(t2\u2212t1;t1)\u03b7c(t3)\u3009\u03be,\u03b7,\u03b7c \u2248 K2 \u2211\nl |Al |2il\u03a6(l(t2\u2212 t1))\u3008yn(t2\u2212 t1)\u03b7c(t3)\u3009\u00d7\ne\u2212l 2\u3008yn(t2\u2212t1)2\u3009/2. (36)\nHere, we furthermore used that \u3008eilyn\u03b7c\u3009 does not depend on neuron index n and therefore all n terms of the sum contribute equally. The covariance of yn and \u03b7c can be calculated directly using the definition of yn in Eq. (22) and is given by\n\u3008yn(t2\u2212 t1)\u03b7c(t3)\u3009= 2Dc\u0398(t3\u2212 t1)\u0398(t2\u2212 t3). (37)\nWe can now evaluate the triple integral of Eq. (28) to calculate the third cumulant \u03ba3(\u03c4), which based on the above can be expressed as\n\u03ba3(\u03c4)\u2248 12DcK2 \u2211 l |Al |2il t+\u03c4\u222b t dt1 t+\u03c4\u2212t1\u222b 0 d\u03c4 \u2032\u03c4 \u2032\u00d7\n\u03a6(l\u03c4 \u2032)exp(\u2212l2\u3008yn(\u03c4 \u2032)2\u3009/2). (38)\nHere, we furthermore used that t1 and t2 can be permuted to constrain the integral over t2 to t2 \u2265 t1, which gives an additional factor 2, and introduced the variable \u03c4 \u2032 = t2\u2212 t1. By changing the order of integration, we can eliminate the integral over t1, and using also the result for the second moment of Eq. (27) with the by now familiar definition \u039b(\u03c4) =\u222b \u03c4\n0 dt(\u03c4\u2212 t)C\u03be (t), we eventually obtain\n\u03ba3(\u03c4)\u2248 12DcK2 \u2211 l |Al |2il \u03c4\u222b 0 d\u03c4 \u2032(\u03c4\u2212 \u03c4 \u2032)\u03c4 \u2032\u00d7\n\u03a6(l\u03c4 \u2032)exp(\u2212l2[\u039b(\u03c4 \u2032)+(D\u03b7 +Dc)\u03c4 \u2032]). (39)\nIn order to facilitate the numerical calculations and to avoid evaluating the integral over \u03c4 \u2032, we follow a similar strategy as in the cases without common noise, and compute the second time derivative of \u03ba3(\u03c4),\n\u03ba\u03083(\u03c4)\u2248 12DcK2 \u2211 l |Al |2il\u03c4\u03a6(l\u03c4)e\u2212l 2[\u039b(\u03c4)+(D\u03b7+Dc)\u03c4], (40)\nwhich can be used to calculate \u03ba3(\u03c4) along with \u039b(\u03c4) by integrating numerically the coupled ordinary differential equations for both quantities. To be precise, if we stopped our approximation for \u3008eilyn\u3009 at the third cumulant, Eq. (23) would become\n\u039b\u0308(\u03c4)\u2248 K2 \u2211 l |Al |2\u03a6(l\u03c4)e\u2212l\n2[\u039b(\u03c4)+(D\u03b7+Dc)\u03c4]\u2212 il 3\n6 \u03ba3(\u03c4). (41)\n8 Eqs. (40) and (41) together with the initial conditions \u03ba3(0) = \u03ba\u03073(0) = \u039b(0) = \u039b\u0307(0) = 0 (found from Eqs. (39) and (3)) would then constitute the self-consistent theory for the autocorrelation function of the network noise C\u03be (\u03c4) = \u039b\u0308(\u03c4) up to third order."
        },
        {
            "heading": "2. Fourth cumulant",
            "text": "In order to improve our description of the non-Gaussian statistics of yn(\u03c4), we also consider the fourth cumulant in the approximation of the average \u3008eilyn\u3009, see Eq. (25). The fourth cumulant of yn(\u03c4) is given by\n\u03ba4(\u03c4) = \u3008y4n(\u03c4; t)\u3009\u22123\u3008y2n(\u03c4; t)\u30092. (42)\nWe first focus on the fourth moment\n\u3008y4n(\u03c4; t)\u3009=\n\u2329 4\n\u220f j=1 t+\u03c4\u222b t dt j[\u03ben(t j)+\u03b7n(t j)+\u03b7c(t j)] \u232a . (43)\nFollowing the same reasoning as above for our calculation of the third cumulant, several terms below the quadruple integral can be discarded. First, all terms with odd powers of \u03ben have to vanish because of the averaging properties of the connectivity matrix Kmn. In addition, all terms \u3008\u03b7 pn \u03b7qc \u3009, p+q = 4, either vanish for odd powers p and q or are exactly compensated by the corresponding terms in the expression 3\u3008y2n(\u03c4)\u30092. (Note that \u03b7n and \u03b7c are Gaussian variables for which the fourth cumulant vanishes.) The remaining terms that potentially contribute to the fourth cumulant therefore are\n\u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009, \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009, \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009, \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009.\n(44)\nWe will briefly sketch the calculation for these terms but spare the reader the rather laborious calculations, and refer to appendix A for the details.\nFirst, we use again that the \u03ben can be expressed in terms of the rotator phases,\n\u03ben(t) = \u2211 p,k Knp|Ak|2 exp(ik\u03b8p(t)), (45)\nand that \u03b8p(t) = \u03b8p(t0) + \u222b t\nt0 dt \u2032(\u03c9p + \u03bep + \u03b7p + \u03b7c). For\nall correlators containing the product \u03ben(ti)\u03ben(t j), one eventually obtains expressions that involve the integrated input to the rotators yp(t j \u2212 ti), giving rise to averages of the type \u3008eikyp(t2\u2212t1)eilyq(t4\u2212t3)\u3009, \u3008eikyp(t2\u2212t1)\u03b7n(t3)\u03b7n(t4)\u3009, \u3008eikyp(t2\u2212t1)\u03b7c(t3)\u03b7c(t4)\u3009, and \u3008eikyp(t2\u2212t1)\u03b7n(t3)\u03b7c(t4)\u3009, respectively. For the last three contributions, we next use a similar identity as used above for the third cumulant (cf. Eq. (32)):\n\u3008eiabc\u3009=\u2212 (\nd dr d ds\n\u2329 ei(a+rb+sc) \u232a) r=0,s=0 . (46)\nIn all four cases of Eq. (44), we therefore have to evaluate averages \u3008eiz\u3009, where for the first case z = kyp(t2\u2212 t1)+ lyq(t4\u2212 t3) for the first and z = a+ rb+ sc with a = kyp(t2\u2212 t1) and b = \u03b7n/c(t3), c = \u03b7n/c(t4) for the last three contributions. To compute these averages, we then make the second simplifying approximation discussed at the beginning of this section, effectively assuming Gaussian statistics for the stochastic variable z. In this approximation, \u3008eiz\u3009\u2248 e\u2212\u3008z2\u3009/2, where only variances and covariances of yp, yq, \u03b7n, and \u03b7c appear in \u3008z2\u3009, depending on the contribution we consider. For the last three terms, the Gaussian approximation directly leads to\n\u3008eiabc\u3009 \u2248 (\u3008bc\u3009\u2212\u3008ab\u3009\u3008ac\u3009)e\u2212\u3008a2\u3009/2 (47)\nfollowing relation (46), with appropriately chosen a, b, and c. Let us consider the first contribution, \u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009, for which we have\n\u3008z2\u3009= k2\u3008y2p(t2\u2212 t1)\u3009+2kl\u3008yp(t2\u2212 t1)yq(t4\u2212 t3)\u3009 + l2\u3008y2q(t4\u2212 t3)\u3009. (48)\nFor p 6= q, the covariance \u3008yp(t2\u2212 t1)yq(t4\u2212 t3)\u3009 scales with Dc and vanishes in the absence of common input:\n\u3008yp(t2\u2212 t1)yq(t4\u2212 t3)\u3009= 2Dc t2\u222b\nt1\ndt \u2032 t4\u222b\nt3\ndt \u2032\u2032\u03b4 (t \u2032\u2032\u2212 t \u2032) (49)\nFor p = q, one can show that the covariances \u3008yp(t2\u2212 t1)yq(t4\u2212 t3)\u3009 do not contribute to the final expression for \u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009 as the average \u3008ei(yp+yq)\u3009 appears in a double sum over p and q and eventually scales with 1/N, see appendix A.\nEventually, the first contribution can be written as\n\u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009 \u2248 3K4 \u2211 k,l gk(t2\u2212 t1)gl(t4\u2212 t3)\n\u00d7 exp(\u22122klDc t2\u222b\nt1\ndt \u2032 t4\u222b\nt3\ndt \u2032\u2032\u03b4 (t \u2032\u2032\u2212 t \u2032)), (50)\nwhere we used the definition\ngk(\u03c4) = |Ak|2\u03a6(k\u03c4)e\u2212k 2[\u039b(\u03c4)+(D\u03b7+Dc)\u03c4] (51)\nto simplify the expression. For the fourth cumulant, we are eventually interested in the (integrated) difference\n\u03ba4,I = t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4 ( \u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009\u2212\n3\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03ben(t3)\u03ben(t4)\u3009 ) . (52)\nAfter a somewhat lengthy calculation (see appendix A), we obtain an expression for the second derivative of \u03ba4,I that can\nbe used for the computation of the fourth cumulant,\nd2\nd\u03c42 \u03ba4,I = 24K4 \u2211\nk,l { \u03c4\u222b\n0\ndt(\u03c4\u2212 t)gk(\u03c4)gl(t) [ e\u22122klDct \u22121 ] +\n\u03c4\u222b 0 dta \u03c4\u222b \u03c4\u2212ta dtbgk(ta)gl(tb) [ e\u22122klDc(ta+tb\u2212\u03c4)\u22121 ]} . (53)\nAlthough not as conveniently integrated as the expression for the third cumulant, it still allows numerical solution.\nBased on relation (47), we quickly discuss the three remaining contributions in Eq. (44). For \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009, since \u3008yp\u03b7n\u3009 = 0 for p 6= n (the covariances \u3008ab\u3009 and \u3008ac\u3009 of Eq. (47)), one can show that this term actually does not contribute to the fourth cumulant as it is exactly canceled by its \u2018counterpart\u2019 in 3\u00b522 , see appendix A. The average \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009 does not have a corresponding term in 3\u00b522 , but vanishes nevertheless, as \u3008yp\u03b7n\u3009 = 0 for p 6= n (the covariance \u3008ab\u3009 of Eq. (47)) and in addition \u3008\u03b7n\u03b7c\u3009 = 0 (the covariance \u3008bc\u3009 of Eq. (47)). Thus, the only remaining contribution to the fourth cumulant comes from \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009, for which we have to sum over terms of the type( \u3008\u03b7c(t3)\u03b7c(t4)\u3009\u2212 k2\u3008yp(t2\u2212 t1)\u03b7c(t3)\u3009\u3008yp(t2\u2212 t1)\u03b7c(t4)\u3009\n) \u00d7 e\u2212 k2\u3008y2p(t2\u2212t1)\u3009 2 , (54)\nsee Eq. (47). While the first term with the covariance \u3008\u03b7c(t3)\u03b7c(t4)\u3009 is exactly cancelled by the corresponding product in 3\u00b522 , the second term does contribute to the fourth cumulant. After another somewhat lengthy calculation (see appendix A), the second derivative of this contribution to \u03ba4 is eventually found to be\nd2\nd\u03c42 \u03ba4,II =\u221248D2cK2 \u2211 k k2\u03c42gk(\u03c4). (55)\nAs expected, this contribution vanishes in the absence of common noise (Dc = 0), consistent with deviations from Gaussian statistics being induced solely by input common to all units. The initial conditions for each of the two contributions to the fourth cumulant, \u03ba4,I(0) = 0 and \u03ba4,II(0) = 0 can be deduced from the defining quadruple integral; the first time derivatives of \u03ba4,I and \u03ba4,II vanish as well for \u03c4 = 0, see appendix A.\nFor the convenience of the reader, we provide below the final set of equations that define the self-consistent theory for the network noise autocorrelation function, including approximate corrections for non-Gaussian statistics up to the fourth cumulant:\n10\n\u039b\u0308(\u03c4) = K2 \u2211 l\ngl(\u03c4)exp ( \u2212 il 3\n6 \u03ba3(\u03c4)+\nl4 24 \u03ba4(\u03c4)\n) (56)\n\u03ba\u03083(\u03c4) = 12DcK2 \u2211 l il\u03c4gl(\u03c4) (57)\n\u03ba\u03084(\u03c4) = 24K4 \u2211 k,l { \u03c4\u222b 0 dt(\u03c4\u2212 t)gk(\u03c4)gl(t) [ e\u22122klDct \u22121 ] + \u03c4\u222b 0 dta \u03c4\u222b \u03c4\u2212ta dtbgk(ta)gl(tb) [ e\u22122klDc(ta+tb\u2212\u03c4)\u22121 ]} \u221248D2cK2 \u2211\nk k2\u03c42gk(\u03c4), (58)\nwith the short-hand notation and initial conditions, respectively gl(\u03c4) = |Al |2\u03a6(l\u03c4)exp ( \u2212l2[\u039b(\u03c4)+(D\u03b7 +Dc)\u03c4] ) , (59)\n\u039b(0) = \u039b\u0307(0) = \u03ba3(0) = \u03ba\u03073(0) = \u03ba4(0) = \u03ba\u03074(0) = 0. (60)\nThe network noise autocorrelation is then given as before by C\u03be (\u03c4) = \u039b\u0308(\u03c4), see Eq. (3)."
        },
        {
            "heading": "B. Effects of common Gaussian white noise",
            "text": "We now turn to the effects that are specific to the common noise and are captured with our modified theory. For a fixed total noise intensity D\u03b7 +Dc = D = const, we thus compare the cases of purely intrinsic noise (D\u03b7 =D, Dc = 0) and purely common noise (Dc = D, D\u03b7 = 0). We will restrict ourselves to a pure sine coupling f (\u00b7) = sin(\u00b7) and vary the coupling strength K and total noise intensity D."
        },
        {
            "heading": "1. Common Gaussian white noise induces non-Gaussian network fluctuations",
            "text": "We have already seen in our calculation that the network fluctuations cannot be any longer assumed to be Gaussian in the presence of input shared among the different units. In Fig. 3 we confirm this theoretical finding by numerical simulations, and also illustrate that our approximations for the third and fourth cumulants work reasonably well for the considered parameters. The cumulants are functions of the time argument \u03c4 , and are shown in Fig. 3 in a rescaled version that capture the deviation from Gaussianity relative to the (timedependent) standard deviation \u03c3(\u03c4) = \u03ba1/22 (\u03c4) of the distribution of the yn. This corresponds to rewriting the expansion Eq. (24) as\n\u3008eilyn(\u03c4)\u3009= exp\n( \u221e\n\u2211 k=1 (il\u03c3(\u03c4))ksk\n) ,\nwhere sk = \u03bak/(\u03ba k/2 2 k!) are the rescaled cumulants shown in Fig. 3. Because \u3008yn\u3009= 0 and by the definition of the variance, we have s1 \u2261 0 and s2 \u2261 1/2. For k > 2, the sk can be thought\nof as generalizations of the skewness \u03ba3/\u03ba 3/2 2 and excess kurtosis \u03ba4/\u03ba22 to higher orders, weighted by 1/k! with which the kth cumulant enters the series Eq. (24). For common noise, s3(\u03c4), s4(\u03c4) and s5(\u03c4) (solid lines) are different from zero, illustrating that the deviation from Gaussianity is significant. However, they remain always small compared to s2 \u2261 1/2, and especially s5 is small. This lends further support to the basic idea of our approximation, i.e., the expansion around the Gaussian case and the neglect of even higher cumulants.\nFor purely intrinsic noise, the magnitudes of the rescaled higher-order cumulants (dotted lines in Fig. 3) are extremely small \u2013 expected fluctuations due to finite-size effects cannot even be resolved on the scale of this figure and are zero for all practical purposes. This is a striking confirmation of our assumption of Gaussian network noise statistics for the case of purely individual noise.\nOur theory that involves a number of approximations provides a quantitatively reasonable description of the numerically measured cumulants for a variety of parameters, irrespective of the intensity of the common noise D and the strength of the coupling K, Fig. 3. In particular, the theory reproduces the pronounced oscillations at low total noise (e.g. K = 0.5 and D = 0.05 of Fig. 3e) and the respective maxima of the time-dependent functions at all parameters considered. Small deviations are observed at intermediate times.\nTo provide a more comprehensive overview of the dependence on the parameters, we plot the maxima of |s3(\u03c4)| and |s4(\u03c4)| as functions of K and D, in Figs. 4 and 5, respectively. The quality of our approximation becomes apparent from the comparison of the simulation results (panels a) and the theoretical predictions (panels b). Remarkably, these functions display global maxima at non-vanishing but finite values of K and D, indicating a maximal deviation from Gaussianity at this parameter combination (Kmax \u2248 0.6 and Dmax \u2248 0.1 within our resolution for both |s3(\u03c4)| and |s4(\u03c4)| and both simulations and theory). Specifically, s3 and s4 seem to vanish for all limiting cases of vanishing or infinite K or D, which is consistent with the the integrated input yn becoming effectively\n11\nGaussian. In the following, we argue why this is so.\nFor D\u2192 0, yn becomes Gaussian because here the system approaches the previously studied purely autonomous network, in which fluctuations are Gaussian14. For K \u2192 0, the integrated inputs yn is completely dominated by the common noise that is Gaussian by assumption. Thus, it is also plausible that the cumulants vanish in this limit. For D\u2192 \u221e at fixed K, a similar argument holds true: The common noise will dominate the integrated input, in which the network noise is limited in amplitude, hence also in this limit the fluctuations will be Gaussian. Finally, for K\u2192\u221e at fixed D, the dynamics of the network approaches that of a purely autonomous network without external input, simply because the network fluctuations are orders of magnitude stronger than the common noise.\nIn summary, significantly non-vanishing values of s3 and s4 appear as predicted by the theory when the network is subject to a common noise, whereas a purely individual noise cannot evoke any non-Gaussian features of the network fluctuations. Our theory captures the non-Gaussian statistics reasonably well."
        },
        {
            "heading": "2. Common Gaussian white noise increases spectral low-frequency power and shifts peaks to higher frequencies",
            "text": "We can use the computed cumulants to obtain the correlation functions and, by an additional Fourier transform, the power spectra of the network noise \u03bei and the individual pointers xi. We look at those statistics for two selected points in the parameter plane (K,D), respectively (0.8, 0.2) and (0.5,0.1), see Fig. 6. Two effects of the common noise can be observed if we compare to the case of purely individual noise of the same strength. Firstly, there is an increase of power around zero frequency, both for the network fluctuations \u03bem(t) (left) and the individual rotators xm (right). This increase is roughly a factor of two for K = 0.5, D = 0.1, but smaller for the other set. Because the external noise has power for all frequencies, this increase in low frequency power is a non-trivial prediction of our theory that is confirmed in the simulations. Power at low frequencies corresponds to slow fluctuations, which thus seem to become more important in the presence of common noise. The described effect stands in marked contrast to that observed in populations of spiking neurons with global inhibition, in which common noise can induce a spectral peak at non-vanishing frequencies and a reduction of power around zero frequency24,25.\nThere is a second effect of the common noise on the spectra. For K = 0.8, D = 0.2, we observe that the main spectral peak is shifted from the intrinsic rotator frequency \u03c90 to a some-\n12\nwhat higher frequency; the same effect is in principle also present at the other parameter set but much weaker. The increase in frequency corresponds to a slightly faster oscillation of the units. It is also predicted by our theory and confirmed by the simulations.\nFor completeness, in Fig. 7 we show for the two parameter sets the self-consistent autocorrelation functions C\u03be (\u03c4) of the network fluctuations and Cx(\u03c4) of the rotators, which allow for another comparison between theory and simulations. Indeed, for both parameter sets, deviations of the simulations from the theory appear to be minor, and taking into account only the first four cumulants yields a good agreement. We also take the opportunity to show how the theory would work if we would only include the third cumulant. Deviations from the simulations are stronger when only \u03ba3 is used, and there is a clear benefit to include the fourth cumulant."
        },
        {
            "heading": "V. SUMMARY & OUTLOOK",
            "text": "In this paper, we have studied a network of randomly coupled phase oscillators which are driven by intrinsic and external (common) noise. We have generalized the theory of self-consistent correlation functions developed in Ref. 14 for the case of the autonomous network (i.e. in the absence of stochastic forcing). It turned out that this generalization is rather straightforward in the case of purely intrinsic (private)\nnoise. Here, the network fluctuations are still to a very good approximation Gaussian \u2013 in fact, they approach Gaussian statistics in the strict thermodynamic limit of an infinite network. Our theory works very well in describing the autocorrelation functions of the network noise and of the individual rotators. Furthermore, we also compared the situations in which all rotators receive a private noise of a given intensity and in which only the observed rotator is subject to noise of that intensity. The network-mediated effect of the private noise on all rotators may have nontrivial consequences on the correlation statistics of the observed rotator. The coherence of the oscillation may decrease or increase by stochastic forcing of the rest of the network, depending on the chosen parameters and the coupling function.\nWhen the rotators are subject to common noise, the picture becomes more complicated. We have shown that, somewhat paradoxically, the driving with a common Gaussian noise turns a Gaussian network noise into a non-Gaussian one. This is reminiscent of earlier observations made in the case of coupled chaotic maps, where the mean-field ceases to be Gaussian in the thermodynamic limit26,27; an effect which is reproduced when independent maps are subject to a common noise28. For the rotator network, the non-Gaussian statistics demand a serious revision of the theory developed by van Meegen & Lindner14. Here we put forward a perturbative approach based on a cumulant expansion around the Gaussian case that leads to a system of few equations for the network noise autocorrela-\n13\ntion and the third and fourth (time-dependent) cumulants of the integrated network noise. The latter measure the deviations from Gaussianity and vanish in the absence of common noise. We have evaluated these equations for large ranges of coupling strength K and noise intensity Dc for a simple sine coupling. We have found that our theory describes the network statistics in most cases surprisingly well. Notably, looking at the temporal maximum of the absolute values of third and fourth cumulants, we found non-monotonic dependences of these statistics on K and Dc. Specifically, there are nonvanishing values K\u2217 and D\u2217c for which the rescaled skewness s3 and excess kurtosis s4 are maximal. Nonetheless, even in this case, the non-Gaussian deviations are moderate and still captured by our theory. We recall that the latter is based on two important assumptions: i) We consider higher-order cumulants only up to the fourth cumulant, and ii) assume Gaussian statistics in the calculations of the (non-Gaussian) third and fourth cumulants of the integrated network noise. In principle, it is possible to relax these assumptions and to calculate the fifth cumulant and/or consider non-Gaussian contributions in the calculation of the cumulants. However, the calculations would become even more involved and, at least for our system, the agreement between predicted and measured correlation functions (the actual statistics of interest) is already sat-\nisfactory with the approximations made. We note in passing that the expansion in cumulants we propose here is somehow remindful of the approach presented in Refs. 29 and 30, where circular cumulants were used to improve the prediction of the order parameter for Kuramoto oscillators with intrinsic noise and Lorentz-distributed frequencies. These works were not concerned with the statistics of the fluctuating \u201crecurrent\u201d input however, and the first Kuramoto order parameter being finite, only the second cumulant was considered as a correction in applications.\nWhat are the effects of the non-Gaussian network fluctuations induced by common noise at the level of the correlation functions and power spectra? We found two effects on the spectra. For once, the common noise may shift the main peak to higher frequencies. Secondly, the common Gaussian white, i.e. temporally uncorrelated, noise gives rise to increased low-frequency power in both network noise and individual rotator spectra. Both of these effects are not easily explained, but are well captured by our self-consistent theory of non-Gaussian fluctuations. It is still an open problem to extract these features from our set of equations by analytical techniques, e.g. by an appropriately simplified Fourier transformation of the equations in simple limit cases as was done in14. We note that slow fluctuations have also been observed to arise spontaneously in recurrent networks of LIF neurons with sufficiently strong (current-based) synaptic coupling in the absence of common noise13,31 and one may speculate how common input may further increase low frequency power in those networks. On a different note, the increase in lowfrequency power in our system stands in marked contrast to the reduction of low-frequency power due to common noise in populations of integrate-and-fire neurons with global inhibitory feedback24,25.\nOur theory may serve as a template for calculations of the self-consistent correlation statistics for other systems of interest. A straightforward generalization of our results is possible, for instance if the common noise is temporally correlated, as characterized by a correlation function Cc(\u03c4). More complicated will be the case of a coupling matrix that respects Dale\u2019s law, i.e. that a given unit j is either excitatory (Ki j > 0 for all i) or inhibitory (Ki j < 0), possibly combined with a given connection probability p between units. Preliminary simulation results show that in this case, the statistics of network fluctuations cease to be Gaussian. Given the importance of this constraint in the neural context, a generalization of our theory to this case is a particularly promising topic of future research."
        },
        {
            "heading": "Appendix A: Detailed calculation of the fourth cumulant",
            "text": "Here we aim to provide some details of the calculation of the fourth cumulant \u03ba4(\u03c4) of the stochastic variable yn(\u03c4). Since \u3008yn(\u03c4)\u3009 = 0, the fourth cumulant is given by \u03ba4 = \u3008y4n\u3009\u22123\u3008y2n\u30092. For the fourth moment, we have\n14\n\u3008y4n(\u03c4)\u3009=\n\u2329 4\n\u220f j=1 t+\u03c4\u222b t dt j[\u03ben(t j)+\u03b7n(t j)+\u03b7c(t j)] \u232a (A1)\n= t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4 ( \u3008 4 \u220f j=1 \u03ben(t j)\u3009+ \u3008 4 \u220f j=1 \u03b7n(t j)\u3009+ \u3008 4 \u220f j=1 \u03b7c(t j)\u3009+12\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009\n+6\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009+6\u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009+6\u3008\u03b7n(t1)\u03b7n(t2)\u03b7c(t3)\u03b7c(t4)\u3009 ) , (A2)\nwhere we already used that odd powers of \u03ben vanish when averaging over the Gaussian, zero-centered connectivity matrix Kmn. (We also used that \u3008\u03b73n \u03b7c\u3009 = \u3008\u03b73n \u3009\u3008\u03b7c\u3009 = 0, \u3008\u03b7n\u03b73c \u3009 =\n\u3008\u03b7n\u3009\u3008\u03b73c \u3009 = 0 as both noise sources are uncorrelated by definition.) Of note, we can easily match all terms but one to a corresponding term in 3\u3008y2n\u30092:\n3\u3008y2n(\u03c4)\u30092 = 3 t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4\n\u2329 2\n\u220f j=1 [\u03ben(t j)+\u03b7n(t j)+\u03b7c(t j)]\n\u232a\u2329 4\n\u220f j=3 [\u03ben(t j)+\u03b7n(t j)+\u03b7c(t j)]\n\u232a (A3)\n= t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4 ( 3\u3008 2 \u220f j=1 \u03ben(t j)\u3009\u3008 4 \u220f j=3 \u03ben(t j)\u3009+3\u3008 2 \u220f j=1 \u03b7n(t j)\u3009\u3008 4 \u220f j=3 \u03b7n(t j)\u3009+3\u3008 2 \u220f j=1 \u03b7c(t j)\u3009\u3008 4 \u220f j=3 \u03b7c(t j)\u3009\n+6\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7n(t3)\u03b7n(t4)\u3009+6\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7c(t3)\u03b7c(t4)\u3009+6\u3008\u03b7n(t1)\u03b7n(t2)\u3009\u3008\u03b7c(t3)\u03b7c(t4)\u3009 ) . (A4)\nFor a process yn(\u03c4) with perfectly Gaussian statistics, this is exactly the expression the fourth moment would boil down to, and \u00b54\u2212 3\u00b522 = 0. Here, we are looking for a difference arising in the fourth moment; this is slightly different from the third cumulant which\u2014for a centered process\u2014is directly\ngiven by the third moment. All terms that only involve powers of \u03b7n and \u03b7c are purely Gaussian by definition, and the differences of the corresponding terms vanish. We are left with four potentially finite contributions,\n\u3008y4n(\u03c4)\u3009\u22123\u3008y2n(\u03c4)\u30092 = t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4 [ \u3008 4 \u220f j=1 \u03ben(t j)\u3009\u22123\u3008 2 \u220f j=1 \u03ben(t j)\u3009\u3008 4 \u220f j=3\n\u03ben(t j)\u3009\ufe38 \ufe37\ufe37 \ufe38 I +12\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009\ufe38 \ufe37\ufe37 \ufe38 II\n+6 ( \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7n(t3)\u03b7n(t4)\u3009\ufe38 \ufe37\ufe37 \ufe38\nIII\n) +6 ( \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7c(t3)\u03b7c(t4)\u3009\ufe38 \ufe37\ufe37 \ufe38\nIV\n)] ,\n(A5)\nwhich we discuss separately in the following. We just mention that we can in the following assume t2 > t1 for all terms without loss of generality, and consider the fourfold integral over\nthe domain t+\u03c4\u222b t dt1 t+\u03c4\u222b t1 dt2 t+\u03c4\u222b t dt3 t+\u03c4\u222b t dt4 with the integrands being multiplied by two. For term I, we can and will additionally assume t4 > t3 without loss of generality for the same symmetry reasons, and accordingly the multiply the integrand by four. 1. \u3008\u220f4j=1 \u03ben(t j)\u3009\u22123\u3008\u220f2j=1 \u03ben(t j)\u3009\u3008\u220f4j=3 \u03ben(t j)\u3009\nBy expressing the network noise again in terms of the couplings Kmn and phases \u03b8n based on the Fourier decomposition\nof the coupling function,\n\u03ben(t) = \u2211 m Knm \u2211 l\nAle ik ( \u03b8p(t0)+ \u222b t t0 dt \u2032\u03b8\u0307m(t \u2032) ) , (A6)\nwe obtain the following expression for the first term:\n15\n\u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009 = \u2329 \u2211\nm,o,p.q KnmKnoKnpKnq \u2211 k,l,r,s AkAlArAse\nik [ \u03b8m(t0)+ \u222b t1 t0 dt \u2032\u03b8\u0307m(t \u2032) ] \u00b7 \u00b7 \u00b7eis [ \u03b8q(t0)+ \u222b t4 t0 dt \u2032\u03b8\u0307q(t \u2032) ]\u232a\n(A7)\n= \u2211 m,o,p,q \u2211 k,l,r,s\nAkAlArAs \u2329 KnmKnoKnpKnq \u232a K \u2329 eik [ \u03b8m(t0)+ \u222b t1 t0 dt \u2032\u03b8\u0307m(t \u2032) ] \u00b7 \u00b7 \u00b7eis [ \u03b8q(t0)+ \u222b t4 t0 dt \u2032\u03b8\u0307q(t \u2032) ]\u232a . (A8)\nWe separately average over the disorder, which in the case of Gaussian couplings gives\n\u2329 KnmKnoKnpKnq \u232a K = K4\nN2 (\u03b4mo\u03b4pq +\u03b4mp\u03b4oq +\u03b4mq\u03b4op) .\n(A9)\nAfter eliminating the \u03b4\u00b7\u00b7\u03b4\u00b7\u00b7 by summing over two of the four corresponding indices m, o, p and q and appropriate renaming of the remaining indices, one thus obtains\n\u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009= K4\nN2 \u2211p,q \u2211k,l,r,s AkAlArAs [ \u2329\neik [ \u03b8p(t0)+ \u222b t1 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eil [ \u03b8p(t0)+ \u222b t2 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eir [ \u03b8q(t0)+ \u222b t3 t0 dt \u2032\u03b8\u0307q(t \u2032) ] eis [ \u03b8q(t0)+ \u222b t4 t0 dt \u2032\u03b8\u0307q(t \u2032) ]\u232a\n+\u2329 eik [ \u03b8p(t0)+ \u222b t1 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eil [ \u03b8p(t0)+ \u222b t3 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eir [ \u03b8q(t0)+ \u222b t2 t0 dt \u2032\u03b8\u0307q(t \u2032) ] eis [ \u03b8q(t0)+ \u222b t4 t0 dt \u2032\u03b8\u0307q(t \u2032) ]\u232a\n+\u2329 eik [ \u03b8p(t0)+ \u222b t1 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eil [ \u03b8p(t0)+ \u222b t4 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eir [ \u03b8q(t0)+ \u222b t2 t0 dt \u2032\u03b8\u0307q(t \u2032) ] eis [ \u03b8q(t0)+ \u222b t3 t0 dt \u2032\u03b8\u0307q(t \u2032) ]\u232a] . (A10)\nThe three terms differ in their combinations of the time arguments t1, t2, t3 and t4, and in principle cannot be subsumed into a single term. As they appear below a quadruple integral\nover all four time arguments, we can for the ease of notation and without loss of generality permute time indices separately for the three terms however, and consider\n\u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009= 3 K4\nN2 \u2211p,...,s AkAlArAs\n\u2329 eik [ \u03b8p(t0)+ \u222b t1 t0 dt \u2032\u03b8\u0307p(t \u2032) ] eil [ \u03b8p(t0)+ \u222b t2 t0 dt \u2032\u03b8\u0307p(t \u2032) ] \u00b7 \u00b7 \u00b7eis [ \u03b8q(t0)+ \u222b t4 t0 dt \u2032\u03b8\u0307q(t \u2032) ]\u232a\n= 3 K4\nN2 \u2211p,...,s AkAlArAs\n\u2329 ei(k+l)\u03b8p(t0)eik \u222b t1 t0 dt \u2032\u03b8\u0307p(t \u2032)eil \u222b t2 t0 dt \u2032\u03b8\u0307p(t \u2032)ei(r+s)\u03b8q(t0)eir \u222b t3 t0 dt \u2032\u03b8\u0307q(t \u2032)eis \u222b t4 t0 dt \u2032\u03b8\u0307q(t \u2032) \u232a . (A11)\nWe next average over random initial phases \u03b8p(t0), \u03b8p(t0) at the reference time t0, with \u2329 ei(k+l)\u03b8p(t0) \u232a\n\u03b8p(t0) = \u03b4k,\u2212l and\nthe analogous relation for \u03b8q(t0), leading to\n\u3008\u03bem(t1) \u00b7 \u00b7 \u00b7\u03bem(t4)\u3009= 3 K4\nN2 \u2211p,q\u2211k,l |Ak|2|Al |2\n\u2329 eik \u222b t2 t1 dt \u2032\u03b8\u0307p(t \u2032)eil \u222b t4 t3 dt \u2032\u03b8\u0307q(t \u2032) \u232a . (A12)\nUsing \u03b8\u0307m = \u03c9m +\u03bem +\u03b7m +\u03b7c and averaging separately over the intrinsic frequencies, \u3008eix\u03c9i\u3009\u03c9 = \u03a6(x) being the character-\n16\nistic function of the \u03c9i, one eventually obtains\n\u3008\u03bem(t1) \u00b7 \u00b7 \u00b7\u03bem(t4)\u3009= 3 K4\nN2 \u2211p,q\u2211k,l |Ak|2|Al |2\n\u2329 eik\u03c9p(t2\u2212t1) \u232a\u2329 eil\u03c9q(t4\u2212t3) \u232a \u00d7\u2329\neik \u222b t2 t1 dt \u2032[\u03bep(t \u2032)+\u03b7p(t \u2032)+\u03b7c(t \u2032)]eil \u222b t4 t3 dt \u2032[\u03beq(t \u2032)+\u03b7q(t \u2032)+\u03b7c(t \u2032)] \u232a\n(A13)\n= 3 K4\nN2 \u2211p,q\u2211k,l |Ak|2|Al |2\u03a6(k(t2\u2212 t1))\u03a6(l(t4\u2212 t3))\u3008ei[kyp(t2\u2212t1;t1)+lyq(t4\u2212t3;t3)]\u3009, (A14)\nwhere we furthermore used the definition of Eq. (22) for the yp, yq that appear in the average in the last equation.\nIn principle, the average \u3008ei[kyp(t2\u2212t1;t1)+lyq(t4\u2212t3;t3)]\u3009 can again be expressed in terms of the cumulants of z = kyp(t2\u2212 t1; t1)+ lyq(t4\u2212 t3; t3), in perfect analogy to Eq. (24). However, we will make here again the second approximation of our theory and consider only the second cumulant of z when evaluating the average \u3008eiz\u3009, effectively assuming that the deviations from Gaussianity of the yp and yq are small and can be neglected in the calculation of the higher-order corrections to the average \u3008eiyn\u3009, i.e. the third and fourth cu-\nmulant of yn. This approximation allows one to express \u3008ei[kyp(t2\u2212t1;t1)+lyq(t4\u2212t3;t3)]\u3009 only in terms of the variances and covariances of yp and yq,\n\u3008eiz\u3009= e\u2212 l2 2 \u3008y 2 p(t2\u2212t1;t1)\u3009e\u2212 k2 2 \u3008y 2 q(t4\u2212t3;t3)\u3009\u00d7\ne\u2212kl\u3008yp(t2\u2212t1;t1)yq(t4\u2212t3;t3)\u3009, (A15)\nwhich we can calculate within the self-consistent theory for the correlation function of the network noise. In particular, the covariance is given by\n\u3008yp(t2\u2212 t1; t1)yq(t4\u2212 t3; t3)\u3009= t2\u222b\nt1\ndt \u2032 t4\u222b\nt3\ndt \u2032\u2032\u3008[\u03bep(t \u2032)+\u03b7p(t \u2032)+\u03b7c(t \u2032)][\u03beq(t \u2032\u2032)+\u03b7q(t \u2032\u2032)+\u03b7c(t \u2032\u2032)]\u3009 (A16)\n= t2\u222b t1 dt \u2032 t4\u222b t3 dt \u2032\u2032 [ \u03b4pq ( C\u03be (t \u2032\u2032\u2212 t \u2032)+C\u03b7(t \u2032\u2032\u2212 t \u2032) ) +Cc(t \u2032\u2032\u2212 t \u2032) ] . (A17)\n(Note that the crosscorrelation \u3008\u03bep(t \u2032)\u03beq(t \u2032\u2032)\u3009 vanishes for p 6= q, as can be seen from \u2211m,n\u3008KpmKqn f (\u03b8m(t \u2032)) f (\u03b8n(t \u2032\u2032))\u3009 = \u2211m,n\u3008KpmKqn\u3009\u3008 f (\u03b8m(t \u2032)) f (\u03b8n(t \u2032\u2032))\u3009 and \u3008KpmKqn\u3009 = 0 when p 6= q.)\nThe double sum over p and q of Eq. (A14) contains N terms with p = q and N(N\u2212 1) terms with p 6= q, while the sum has a prefactor 1/N2. In the large-N limit, we neglect all contributions that scale with 1/N and keep thus only the last term in Eq. (A17). With Cc(t \u2032\u2032\u2212 t \u2032) = 2Dc\u03b4 (t \u2032\u2032\u2212 t \u2032) and \u3008y2{p,q}(\u03c4, t)\u3009 = 2\u039b(\u03c4)+ 2(D\u03b7 +Dc)\u03c4 (see Eq. (27)), we thus\nobtain\n\u3008\u03ben(t1) \u00b7 \u00b7 \u00b7\u03ben(t4)\u3009= 3K4 \u2211 k,l gk(t2\u2212 t1)gl(t4\u2212 t3)\u00d7\ne \u22122klDc t2\u222b t1 dt \u2032 t4\u222b t3 dt \u2032\u2032\u03b4 (t \u2032\u2032\u2212t \u2032) , (A18)\nwhere we used the definition for the gi(t) given by Eq. (51). Along the same lines, it is straightforward to show that the corresponding term from \u3008y2n(\u03c4)\u30092 is given by\n\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03ben(t3)\u03ben(t4)\u3009= K4 \u2211 k,l gk(t2\u2212 t1)gl(t4\u2212 t3),\n(A19) so that we eventually obtain the following expression for the first contribution to the fourth cumulant:\n17\n\u03ba4,I(\u03c4) = t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4 ( \u3008 4 \u220f j=1 \u03ben(t j)\u3009\u22123\u3008 2 \u220f j=1 \u03ben(t j)\u3009\u3008 4 \u220f j=3 \u03ben(t j)\u3009 )\n= 12K4 \u2211 k,l t+\u03c4\u222b t dt1 t+\u03c4\u222b t1 dt2 t+\u03c4\u222b t dt3 t+\u03c4\u222b t3 dt4gk(t2\u2212 t1)gl(t4\u2212 t3)\ne\u22122klDc t2\u222b t1 dt \u2032 t4\u222b t3 dt \u2032\u2032\u03b4 (t \u2032\u2032\u2212t \u2032) \u22121  . (A20)\nWith the variable transformations ta = t2\u2212 t1, tb = t4\u2212 t3, and tc = t3\u2212 t1 we can rewrite this expression as\n\u03ba4,I(\u03c4) = 12K4 \u2211 k,l \u03c4\u222b 0 dta \u03c4\u222b 0 dtb \u03c4\u2212tb\u222b ta\u2212\u03c4 dtc gk(ta)gl(tb)\ne\u22122klDc ta\u222b0 dt \u2032 tc+tb\u222btc dt \u2032\u2032\u03b4 (t \u2032\u2032\u2212t \u2032)\u22121  tupp(\u03c4,ta,tb,tc)\u222b\ntlow(\u03c4,ta,tb,tc)\ndt1, (A21)\nwhere care has to be taken in identifying the correct integration limits tlow(\u03c4, ta, tb, tc) and tupp(\u03c4, ta, tb, tc) for t1 when\nchanging the order of integration. Making first a case distinction for ta > tb and tb > ta, respectively, and then considering tc relative to ta\u2212 tb and 0, we eventually find\ntupp(\u03c4,ta,tb,tc)\u222b tlow(\u03c4,ta,tb,tc) dt1 =  \u03c4\u2212 ta + tc ta\u2212 \u03c4 < tc < min(ta\u2212 tb,0) \u03c4\u2212max(ta, tb) min(ta\u2212 tb,0)< tc < max(ta\u2212 tb,0) \u03c4\u2212 tb\u2212 tc max(ta\u2212 tb,0)< tc < \u03c4\u2212 tb . (A22)\nWhile the numerical evaluation of the resulting triple integral is possible a priori, we try and simplify the expression further by taking the second time derivative with respect to \u03c4 . For the first time derivatives w.r.t. the integration limits of the\ntriple integral over ta, tb, and tc, one can see that they vanish when we use the respective values for ta, tb, and tc with Eq. (A22). The remaining time derivative of \u222b tupp tlow dt1 is surprisingly simple and equal to 1, as can be seen as well from Eq. (A22). Thus,\nd d\u03c4 \u03ba4,I(\u03c4) = 12K4 \u2211 k,l \u03c4\u222b 0 dta \u03c4\u222b 0 dtb \u03c4\u2212tb\u222b ta\u2212\u03c4 dtc gk(ta)gl(tb) ( e\u22122klDcG(ta,tb,tc)\u22121 ) , (A23)\nwhere we furthermore introduced the short-hand notation\nG(ta, tb, tc) = \u222b ta 0 dt \u2032 \u222b tc+tb tc dt \u2032\u2032\u03b4 (t \u2032\u2032\u2212 t \u2032). (A24)\nThe second time derivative is then given by\n18\nd2\nd\u03c42 \u03ba4,I(\u03c4) = 12K4 \u2211\nk,l\n[\u222b \u03c4 0 dtb \u222b \u03c4\u2212tb 0 dtcgk(\u03c4)gl(tb) ( e\u22122klDcG(\u03c4,tb,tc)\u22121 ) +\n\u222b \u03c4 0 dta \u222b 0 ta\u2212\u03c4 dtcgk(ta)gl(\u03c4) ( e\u22122klDcG(ta,\u03c4,tc)\u22121 ) +\u222b \u03c4\n0 dta \u222b \u03c4 0 dtbgk(ta)gl(tb) ( e\u22122klDcG(ta,tb,\u03c4\u2212tb)\u22121 ) +\u222b \u03c4\n0 dta \u222b \u03c4 0 dtbgk(ta)gl(tb) ( e\u22122klDcG(ta,tb,ta\u2212\u03c4)\u22121 )] . (A25)\nLet us consider the four contributions separately.\n1) ta = \u03c4 : tc > 0, tc + tb < \u03c4 \u21d2 G(\u03c4, tb, tc) = tb\u222b \u03c4 0 dtb \u222b \u03c4\u2212tb 0 dtcgk(\u03c4)gl(tb) ( e\u22122klDcG(\u03c4,tb,tc)\u22121 ) = \u222b \u03c4 0 dtb(\u03c4\u2212 tb)gk(\u03c4)gl(tb) ( e\u22122klDctb \u22121 ) (A26)\n2) tb = \u03c4 : tc < 0, tc + \u03c4 > ta \u21d2 G(ta,\u03c4, tc) = ta\u222b \u03c4 0 dta \u222b 0 ta\u2212\u03c4 dtcgk(ta)gl(\u03c4) ( e\u22122klDcG(ta,\u03c4,tc)\u22121 ) = \u222b \u03c4 0 dta(\u03c4\u2212 ta)gk(ta)gl(\u03c4) ( e\u22122klDcta \u22121 ) (A27)\nNote that this term is completely analogous to the previous one and because of the k, l symmetry both can be combined by simply computing one of them and taking it twice.\n3) tc = \u03c4\u2212 tb : tc > 0, tc + tb > ta \u21d2 G(ta, tb,\u03c4\u2212 tb) = { 0 ta + tb < \u03c4 (tc > ta) ta + tb\u2212 \u03c4 ta + tb > \u03c4\u222b \u03c4\n0 dta \u222b \u03c4 0 dtbgk(ta)gl(tb) ( e\u22122klDcG(ta,tb,\u03c4\u2212tb)\u22121 ) = \u222b \u03c4 0 dta \u222b \u03c4 \u03c4\u2212ta dtbgk(ta)gl(tb) ( e\u22122klDc(ta+tb\u2212\u03c4)\u22121 ) (A28)\n4) tc = ta\u2212 \u03c4 : tc < 0, tc + tb < ta \u21d2 G(ta, tb, ta\u2212 \u03c4) = { 0 ta + tb < \u03c4 (tc + tb < 0) ta + tb\u2212 \u03c4 ta + tb > \u03c4\u222b \u03c4\n0 dta \u222b \u03c4 0 dtbgk(ta)gl(tb) ( e\u22122klDcG(ta,tb,ta\u2212\u03c4)\u22121 ) = \u222b \u03c4 0 dta \u222b \u03c4 \u03c4\u2212ta dtbgk(ta)gl(tb) ( e\u22122klDc(ta+tb\u2212\u03c4)\u22121 ) (A29)\nThis term is obviously identical to the previous one.\nBased on the above, the four terms can be regrouped into a simple integral from 0 . . .\u03c4 and a double integral,\nd2\nd\u03c42 \u03ba4,I(\u03c4) = 24K4 \u2211\nk,l  \u03c4\u222b 0 dtb(\u03c4\u2212 tb)gk(\u03c4)gl(tb) ( e\u22122klDctb \u22121 ) + \u03c4\u222b 0 dta \u03c4\u222b \u03c4\u2212ta dtbgk(ta)gl(tb) ( e\u22122klDc(ta+tb\u2212\u03c4)\u22121 ) , (A30)\nwhich is the first contribution to the expression given in Eq. (58).\n2. \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009\nWe can express the \u03ben(t j) in terms of the coupling matrix Kn p and the phases \u03b8p, and arrive along the very same lines\n19\nas above at the following expression:\n\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009= K2\nN \u2211p \u2211k |Ak|2\u03a6(k(t2\u2212 t1))\u00d7\n\u3008eikyp(t2\u2212t1;t1)\u03b7n(t3)\u03b7c(t4)\u3009. (A31)\nTo evaluate the remaining average \u3008eikyp(t2\u2212t1;t1)\u03b7n(t3)\u03b7c(t4)\u3009, we first use relation (46) to express it as a derivative w.r.t. r and s of an average of the type \u3008eiz\u3009, where z = yp(t2 \u2212 t1; t1) + r\u03b7n(t3) + s\u03b7c(t4). Here, we make again our second approximation and neglect higher-order cumulants of z when evaluating \u3008eiz\u3009, effectively treating z as Gaussian for the calculation of the higher-order cumulants of yn(\u03c4). We can than express \u3008eikyp(t2\u2212t1;t1)\u03b7n(t3)\u03b7c(t4)\u3009 in terms of the variances and\ncovariances of yp(t2\u2212 t1; t1), \u03b7n(t3), and \u03b7c(t4), see Eq. (47), and obtain\n\u3008eikyp(t2\u2212t1;t1)\u03b7n(t3)\u03b7c(t4)\u3009= ( \u3008\u03b7n(t3)\u03b7c(t4)\u3009\u2212\nk2\u3008yp(t2\u2212 t1)\u03b7n(t3)\u3009\u3008yp(t2\u2212 t1)\u03b7c(t4)\u3009 )\n\u00d7 e\u2212 k2\u3008y2p(t2\u2212t1)\u3009 2 . (A32)\nHowever, upon closer inspection one finds that none of these terms contributes in the large-N limit, as \u3008\u03b7n(t3)\u03b7c(t4)\u3009 = 0 by definition and\n\u3008yp(t2\u2212 t1)\u03b7n(t3)\u3009= \u03b4pn2D\u03b7 \u0398(t3\u2212 t1)\u0398(t2\u2212 t3). (A33)\nThe sum over p in Eq. (A31) thus reduces to a single term that scales with 1/N,\n\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7c(t4)\u3009= 4D\u03b7 Dc K2\nN \u2211k gk(t2\u2212 t1)k2\u0398(t3\u2212 t1)\u0398(t2\u2212 t3)\u0398(t4\u2212 t1)\u0398(t2\u2212 t4), (A34)\nand vanishes as N\u2192 \u221e.\n3. \u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7n(t3)\u03b7n(t4)\u3009\nWe can directly apply the intermediate results from the previous contribution, in particular relation (47) also discussed in\nthe main text, and write\n\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009= K2\nN \u2211p \u2211k |Ak|2\u03a6(k(t2\u2212 t1))\u3008eikyp(t2\u2212t1;t1)\u03b7n(t3)\u03b7n(t4)\u3009 (A35)\n= K2\nN \u2211p \u2211k gk(t2\u2212 t1)\n( \u3008\u03b7n(t3)\u03b7c(t4)\u3009\u2212 k2\u3008yp(t2\u2212 t1; t1)\u03b7n(t3)\u3009\u3008yp(t2\u2212 t1; t1)\u03b7n(t4)\u3009 ) . (A36)\nThe first term is exactly canceled by \u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7n(t3)\u03b7n(t4)\u3009 which we need to subtract\nto compute the cumulant. The averages lryp(t2\u2212 t1; t1)\u03b7n(t3) contribute only for p = n as discussed above, and we are left with\n\u3008\u03ben(t1)\u03ben(t2)\u03b7n(t3)\u03b7n(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7n(t3)\u03b7n(t4)\u3009\n=\u22124D2\u03b7 K2\nN \u2211k k2gk(t2\u2212 t1)k2\u0398(t3\u2212 t1)\u0398(t2\u2212 t3)\u0398(t4\u2212 t1)\u0398(t2\u2212 t4). (A37)\nThis term also vanishes in the limit of N \u2192 \u221e and does not contribute to the fourth cumulant.\n20\n4. \u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7c(t3)\u03b7c(t4)\u3009\nThis last term follows again straightforwardly from the previous contribution, where we have simply to replace \u03b7n by \u03b7c. However, the averages \u3008yp(t2\u2212 t1; t1)\u03b7n(t j)\u3009 contribute for each p in the sum which is the equivalent of Eq. (A36), and we eventually obtain\n\u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7c(t3)\u03b7c(t4)\u3009 =\u22124D2cK2 \u2211\nk k2gk(t2\u2212 t1)\u0398(t3\u2212 t1)\u0398(t2\u2212 t3)\u0398(t4\u2212 t1)\u0398(t2\u2212 t4). (A38)\nThe contribution to the fourth cumulant is accordingly given by\n\u03ba4,IV(\u03c4) = 6 t+\u03c4\u222b t dt1 \u00b7 \u00b7 \u00b7 t+\u03c4\u222b t dt4 (\u3008\u03ben(t1)\u03ben(t2)\u03b7c(t3)\u03b7c(t4)\u3009\u2212\u3008\u03ben(t1)\u03ben(t2)\u3009\u3008\u03b7c(t3)\u03b7c(t4)\u3009)\n=\u221248D2cK2 \u2211 k k2 t+\u03c4\u222b t dt1 t+\u03c4\u222b t1 dt2gk(t2\u2212 t1)(t2\u2212 t1)2. (A39)\nTo further simplify the calculation of \u03ba4,IV(\u03c4), we again take the second time derivative w.r.t. \u03c4 . After the variable substitution ta = t2\u2212 t1 and changing the integration to\u222b t+\u03c4\nt dt1 \u222b t+\u03c4\u2212t1 0 dta, it is straightforward to obtain\nd d\u03c4 \u03ba4,IV(\u03c4) =\u221248D2cK2 \u2211 k\nk2 \u03c4\u222b\n0\ndt \u2032gk(t \u2032)t \u20322, (A40)\nwhere we furthermore used the substitution t \u2032= t+\u03c4\u2212t1. The second time derivative then follows immediately as\nd2\nd\u03c42 \u03ba4,IV(\u03c4) =\u221248D2cK2 \u2211 k k2gk(\u03c4)\u03c42, (A41)\nwhich is the second contribution to the expression given in Eq. (58)."
        }
    ],
    "title": "A self-consistent analytical theory for rotator networks under stochastic forcing: effects of intrinsic noise and common input",
    "year": 2022
}