{
    "abstractText": "We consider two modifications of the Arrow-Hurwicz (AH) iteration for solving the incompressible steady Navier-Stokes equations for the purpose of accelerating the algorithm: grad-div stabilization, and Anderson acceleration. AH is a classical iteration for general saddle point linear systems and it was later extended to Navier-Stokes iterations in the 1970\u2019s which has recently come under study again. We apply recently developed ideas for grad-div stabilization and divergence-free finite element methods along with Anderson acceleration of fixed point iterations to AH in order to improve its convergence. Analytical and numerical results show that each of these methods improves AH convergence, but the combination of them yields an efficient and effective method that is competitive with more commonly used solvers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pelin G. Geredeli"
        },
        {
            "affiliations": [],
            "name": "Leo G. Rebholz"
        },
        {
            "affiliations": [],
            "name": "Duygu Vargun"
        },
        {
            "affiliations": [],
            "name": "Ahmed Zytoon"
        }
    ],
    "id": "SP:b126cc119b213031070ef7d91f5709e20326173e",
    "references": [
        {
            "authors": [
                "H. An",
                "X. Jia",
                "H. Walker"
            ],
            "title": "Anderson acceleration and application to the three-temperature energy equations",
            "venue": "Journal of Computational Physics,",
            "year": 2017
        },
        {
            "authors": [
                "D.G. Anderson"
            ],
            "title": "Iterative procedures for nonlinear integral equations",
            "venue": "J. Assoc. Comput. Mach.,",
            "year": 1965
        },
        {
            "authors": [
                "K.J. Arrow",
                "L. Hurwicz"
            ],
            "title": "Gradient method for concave programming I: Local results",
            "venue": "Studies in Linear and Nonlinear Programming,",
            "year": 1958
        },
        {
            "authors": [
                "M. Benzi",
                "G. Golub",
                "J. Liesen"
            ],
            "title": "Numerical solution of saddle point problems",
            "venue": "Acta Numerica,",
            "year": 2005
        },
        {
            "authors": [
                "M. Benzi",
                "M. Olshanskii"
            ],
            "title": "An augmented Lagrangian-based approach to the Oseen problem",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2006
        },
        {
            "authors": [
                "S. B\u00f6rm",
                "S. Le Borne"
            ],
            "title": "H-LU factorization in preconditioners for augmented Lagrangian and grad-div stabilized saddle point systems",
            "venue": "Internat. J. Numer. Methods Fluids,",
            "year": 2012
        },
        {
            "authors": [
                "P. Chen",
                "J. Huang",
                "H. Sheng"
            ],
            "title": "Solving steady incompressible Navier-Stokes equations by the Arrow-Hurwicz method",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 2017
        },
        {
            "authors": [
                "P. Chen",
                "J.H. P",
                "H. Sheng"
            ],
            "title": "Some Uzawa methods for steady incompressible Navier-Stokes equations discretized by mixed element methods",
            "venue": "J. Comput. Appl. Math.,",
            "year": 2015
        },
        {
            "authors": [
                "R. Codina"
            ],
            "title": "An iterative penalty method for the finite element solution of the stationary Navier- Stokes equations",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 1993
        },
        {
            "authors": [
                "C. Evans",
                "S. Pollock",
                "L. Rebholz",
                "M. Xiao"
            ],
            "title": "A proof that Anderson acceleration improves the convergence rate in linearly converging fixed-point methods (but not in those converging quadratically)",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "L. Franca",
                "T. Hughes"
            ],
            "title": "Two classes of mixed finite element methods",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 1988
        },
        {
            "authors": [
                "A. Fu",
                "J. Zhang",
                "S. Boyd"
            ],
            "title": "Anderson accelerated Douglas-Rachford splitting",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2020
        },
        {
            "authors": [
                "U. Ghia",
                "K.N. Ghia",
                "C.T. Shin"
            ],
            "title": "High-Re solutions for incompressible flow using the Navier- Stokes equations and a multigrid method",
            "venue": "J. Comput. Phys.,",
            "year": 1982
        },
        {
            "authors": [
                "V. Girault",
                "P.-A"
            ],
            "title": "Raviart. Finite element methods for Navier-Stokes equations, volume 5 of Springer Series in Computational Mathematics",
            "venue": "Theory and algorithms",
            "year": 1986
        },
        {
            "authors": [
                "P. Gresho",
                "R. Lee"
            ],
            "title": "Don\u2019t suppress the wiggles. - they\u2019re telling you something",
            "venue": "Computers and Fluids,",
            "year": 1981
        },
        {
            "authors": [
                "M. Gunzburger"
            ],
            "title": "Finite element methods for viscous incompressible flows: A guide to theory, practice, and algorithm (Computer Science and Scientific Computing",
            "venue": "Academic Press Inc.,Boston,MA,",
            "year": 1989
        },
        {
            "authors": [
                "M. Gunzburger"
            ],
            "title": "Iterative penalty methods for the Stokes and Navier-Stokes equations",
            "venue": "Proceedings from Finite Element Analysis in Fluids conference,",
            "year": 1989
        },
        {
            "authors": [
                "J. Guzman",
                "L. Scott"
            ],
            "title": "The Scott-Vogelius finite elements revisited",
            "venue": "Math. Comp.,",
            "year": 2019
        },
        {
            "authors": [
                "T. Heister",
                "G. Rapin"
            ],
            "title": "Efficient augmented Lagrangian-type preconditioning for the Oseen problem using grad-div stabilization",
            "venue": "Int. J. Numer. Meth. Fluids,",
            "year": 2013
        },
        {
            "authors": [
                "N. Higham",
                "N. Strabic"
            ],
            "title": "Anderson acceleration of the alternating projections method for computing the nearest correlation matrix",
            "venue": "Numerical Algorithms,",
            "year": 2016
        },
        {
            "authors": [
                "V. John",
                "A. Liakos"
            ],
            "title": "Time dependent flow across a step: the slip with friction boundary condition",
            "venue": "International Journal for Numerical Methods in Fluids,",
            "year": 2006
        },
        {
            "authors": [
                "V. John",
                "A. Linke",
                "C. Merdon",
                "M. Neilan",
                "L.G. Rebholz"
            ],
            "title": "On the divergence constraint in mixed finite element methods for incompressible flows",
            "venue": "SIAM Review,",
            "year": 2017
        },
        {
            "authors": [
                "C. Kelley"
            ],
            "title": "Numerical methods for nonlinear equations",
            "venue": "Acta Numerica,",
            "year": 2018
        },
        {
            "authors": [
                "W. Layton"
            ],
            "title": "An Introduction to the Numerical Analysis of Viscous Incompressible Flows",
            "venue": "SIAM, Philadelphia,",
            "year": 2008
        },
        {
            "authors": [
                "J. Loffeld",
                "C. Woodward"
            ],
            "title": "Considerations on the implementation and use of Anderson acceleration on distributed memory and GPU-based parallel computers",
            "venue": "Advances in the Mathematical Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "P.A. Lott",
                "H.F. Walker",
                "C.S. Woodward",
                "U.M. Yang"
            ],
            "title": "An accelerated Picard method for nonlinear systems related to variably saturated flow",
            "venue": "Adv. Water Resour.,",
            "year": 2012
        },
        {
            "authors": [
                "H. Morgan",
                "L. Scott"
            ],
            "title": "Towards a unified finite element method for the stokes equations",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2018
        },
        {
            "authors": [
                "M.A. Olshanskii",
                "A. Reusken"
            ],
            "title": "Grad-Div stabilization for the Stokes equations",
            "venue": "Math. Comp.,",
            "year": 2004
        },
        {
            "authors": [
                "Y. Peng",
                "B. Deng",
                "J. Zhang",
                "F. Geng",
                "W. Qin",
                "L. Liu"
            ],
            "title": "Anderson acceleration for geometry optimization and physics simulation",
            "venue": "ACM Transactions on Graphics,",
            "year": 2018
        },
        {
            "authors": [
                "S. Pollock",
                "L. Rebholz"
            ],
            "title": "Anderson acceleration for contractive and noncontractive operators",
            "venue": "IMA Journal of Numerical Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "S. Pollock",
                "L. Rebholz",
                "M. Xiao"
            ],
            "title": "Anderson-accelerated convergence of Picard iterations for incompressible Navier-Stokes equations",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 2019
        },
        {
            "authors": [
                "S. Pollock",
                "L. Rebholz",
                "M. Xiao"
            ],
            "title": "Acceleration of nonlinear solvers for natural convection problems",
            "venue": "Journal of Numerical Mathematics,",
            "year": 2021
        },
        {
            "authors": [
                "L. Rebholz",
                "D. Vargun",
                "M. Xiao"
            ],
            "title": "Enabling fast convergence of the iterated penalty Picard iteration with o(1) penalty parameter for incompressible Navier-Stokes via Anderson acceleration",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "L. Rebholz",
                "A. Viguerie",
                "M. Xiao"
            ],
            "title": "Efficient nonlinear iteration schemes based on algebraic splitting for the incompressible Navier-Stokes equations",
            "venue": "Math. Comp.,",
            "year": 2019
        },
        {
            "authors": [
                "L. Rebholz",
                "M. Xiao"
            ],
            "title": "On reducing the splitting error in Yosida methods for the Navier-Stokes equations with grad-div stabilization",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2015
        },
        {
            "authors": [
                "P. Stasiak",
                "M. Matsen"
            ],
            "title": "Efficiency of pseudo-spectral algorithms with anderson mixing for the SCFT of periodic block-copolymer phases",
            "venue": "Eur. Phys. J. E,",
            "year": 2011
        },
        {
            "authors": [
                "R. Temam"
            ],
            "title": "Navier-Stokes equations. Theory and numerical analysis",
            "venue": "Studies in Mathematics and its Applications,",
            "year": 1977
        },
        {
            "authors": [
                "A. Toth",
                "C. Kelley",
                "S. Slattery",
                "S. Hamilton",
                "K. Clarno",
                "R. Pawlowski"
            ],
            "title": "Analysis of Anderson acceleration on a simplified neutronics/thermal hydraulics system",
            "venue": "Proceedings of the ANS MC2015 Joint International Conference on Mathematics and Computation (M&C), Supercomputing in Nuclear Applications (SNA) and the Monte Carlo (MC) Method, ANS MC2015",
            "year": 2015
        },
        {
            "authors": [
                "H. Uzawa"
            ],
            "title": "Iterative methods for concave programming",
            "venue": "Studies in Linear and Nonlinear Programming,",
            "year": 1958
        },
        {
            "authors": [
                "H.F. Walker",
                "P. Ni"
            ],
            "title": "Anderson acceleration for fixed-point iterations",
            "venue": "SIAM J. Numer. Anal.,",
            "year": 2011
        },
        {
            "authors": [
                "D. Wicht",
                "M. Schneider",
                "T. Bohlke"
            ],
            "title": "Anderson-accelerated polarization schemes for fast Fourier transform-based computational homogenization",
            "venue": "International Journal for Numerical Methods in Engineering,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "We consider two modifications of the Arrow-Hurwicz (AH) iteration for solving the incompressible steady Navier-Stokes equations for the purpose of accelerating the algorithm: grad-div stabilization, and Anderson acceleration. AH is a classical iteration for general saddle point linear systems and it was later extended to Navier-Stokes iterations in the 1970\u2019s which has recently come under study again. We apply recently developed ideas for grad-div stabilization and divergence-free finite element methods along with Anderson acceleration of fixed point iterations to AH in order to improve its convergence. Analytical and numerical results show that each of these methods improves AH convergence, but the combination of them yields an efficient and effective method that is competitive with more commonly used solvers.\nKeywords: Anderson acceleration, Arrow-Hurwicz, Navier-Stokes equations, Finite element method (FEM)"
        },
        {
            "heading": "1. Introduction",
            "text": "We consider in this paper solving the incompressible steady Navier-Stokes equations (NSE) with the Arrow-Hurwicz (AH) iteration. The steady NSE defined on a domain \u2126 \u2282 Rd (d=2 or 3) are given by\n\u2212\u03bd\u2206u+ (u \u00b7 \u2207)u+\u2207p = f in \u2126, (1.1a) \u2207 \u00b7 u = 0 in \u2126, (1.1b)\nu = 0 on \u2202\u2126, (1.1c)\nwhere u and p represent the unknown velocity and pressure, f a given forcing, and \u03bd the kinematic viscosity which is inversely proportional to the Reynolds number Re. Extension of this work to one time step in a temporal discretization of the time dependent NSE is straight-forward.\nAmong various novel iterative methods for solving saddle point systems, the AH algorithm for the steady NSE was seemingly first studied by Temam in 1977 in [37], and also more recently in [7]. The AH iteration is given with the following decoupled equations:\n\u22121 \u03c1 \u2206(um+1 \u2212 um)\u2212 \u03bd\u2206um + um \u00b7 \u2207um+1 +\u2207pm = f,\n\u03b1(pm+1 \u2212 pm) + \u03c1\u2207 \u00b7 um+1 = 0,\nwith \u03c1 and \u03b1 being user determined parameters. We note that if \u03c1 = \u03bd\u22121 then AH is exactly the modified Uzawa algorithm from [8]. This iteration is interesting because it is efficient since the two equations decouple, with the second equation being simple and the first equation requiring a typical\nPreprint submitted to Elsevier March 4, 2022\nar X\niv :2\n20 3.\n01 53\n4v 1\n[ m\nat h.\nN A\n] 3\nM ar\nconvection diffusion solver where one controls the diffusion coefficient (in each iteration) with \u03c1. Hence from an implementation (i.e. linear algebraic) point of view, the cost of one AH iteration is very cheap compared to that of a typical Picard or Newton iteration which needs to resolve a saddle point system. However, a serious drawback of the AH method is that its convergence properties are not particularly good and even though each iteration is cheap, the total number of iterations can be very large. The purpose of this paper is to improve the AH algorithm so that it is a competitive and even attractive method for efficient computing of accurate steady NSE solutions. We enhance the AH method with two recently developed ideas, one from computational fluid dynamics (grad-div stabilization) and the other from nonlinear solver theory (Anderson acceleration). Indeed, we show that the combination of these two improvements theoretically and computationally yields that AH method can become a very good solver.\nThe classical AH iteration developed in 1958 by Arrow and Hurwicz [3] is a stationary iterative method to solve saddle point linear systems. As noted in [4], this linear algebraic AH iteration can be regarded as an inexpensive alternative to the (linear algebraic) Uzawa method [39] whenever solves with the matrix arising from the convection-diffusion operators are expensive. Temam seems to be the first to export the AH iteration ideas to Galerkin methods for solving the steady NSE, and was able to prove convergence (although without a rate) under certain choices of parameters. The iteration was (finally) proven to be contractive in 2017 [7], where it was shown to be linearly convergent under very small data and certain choices of parameters. While this was a big step forward for AH and the convergent rate was proven less than 1, the exact rate was not easy to decipher and in practice could be very close to 1. The numerical tests in [7] for some relatively easy problem revealed that the AH method could be made to converge with good parameter choices, but the number of iterations could be very large (e.g. over 700 iterations with Re = 100 for a 2d driven cavity problem, and over 10,000 for a 2d steady flow past a cylinder). While iterations of AH would likely be 5-20 times cheaper than one iteration of usual Picard (e.g. if Krylov solvers with preconditioners such as those in [19, 6, 5] to solve the saddle point linear systems at each iteration), such high iteration counts still make AH uncompetitive.\nHerein we aim to improve the convergence properties of the AH method by enhancing it with two techniques. The first is the addition of grad-div stabilization which refers to consistent penalization term that adds 0 = \u2212\u03b3\u2207(\u2207 \u00b7 u) to the NSE momentum equation before discretizing, where \u03b3 > 0 is a user defined parameter (how large it should be depends on many factors, see e.g. [22]). It was first proposed by Hughes and Franca in 1988 [11], and has been shown to improve accuracy of finite element approximations [28], improve saddle point linear solvers [28, 5, 19], and help various NSE nonlinear iterative solvers converge faster, e.g. [35, 34]. The grad-div stabilized AH iteration takes the form\n\u22121 \u03c1 \u2206(um+1 \u2212 um)\u2212 \u03bd\u2206um + um \u00b7 \u2207um+1 \u2212 \u03b3\u2207(\u2207 \u00b7 um+1) +\u2207pm = f,\n\u03b1(pm+1 \u2212 pm) + \u03c1\u2207 \u00b7 um+1 = 0.\nWe show that the existing convergence theory can dramatically be improved with the use of grad-div stabilization theory. Moreover, we show that under a certain choice of parameters and in a particular (but commonly used) discrete setting, grad-div stabilized AH method is equivalent to the classical iterated penalty Picard iteration. By establishing this connection, we are able to bring to bear the long established theory for this classical iteration to the AH setting, which establishes a linear convergence rate close to that of Picard for sufficiently large \u03b3.\nThe second enhancement we provide to the AH method is Anderson acceleration (AA). AA is an extrapolation technique used to improve convergence of fixed point iterations. It was first developed in 1965 by D.G. Anderson [2], and its use has exploded in the last decade after the paper of Walker and Ni in 2011 showed how effective AA can be on a wide range of problems [40]. It has recently been used to improve convergence and robustness of solvers for various types of flow problems [26, 31, 32], geometry optimization [29], radiation diffusion and nuclear physics [1, 38], molecular interaction [36],\nand many others e.g. [40, 23, 25, 26, 12, 41, 20]. In [33], AA was shown to significantly improve the convergence and robustness for the IPP method for the NSE and allow for a much wider range of penalty parameter choices. Given the success AA has had in improving other types of nonlinear iterations for the NSE, applying it to the AH method seems a natural next step. Moreover, due to its dramatic improvement of the IPP method in [33] and our showing the strong connection of graddiv stabilized AH method to IPP method, applying AA to grad-div stabilized AH seems an optimal combination to improve AH convergence behavior. A general convergence framework was developed for AA in [10] and then sharpened in [30] which allows for theoretical justification of improved linear convergence from AA, if the associated fixed point function satisfies sufficient smoothness properties. We will set up the AH iteration as a fixed point problem and prove that its fixed point operator satisfies the assumptions needed to apply the AA convergence theory. Furthermore, extensive computations of AH with AA are performed, and AA is observed to provide a dramatic improvement in convergence behavior, with the best convergence coming from combining AA with grad-div stabilization.\nThis article is arranged as follows. Section 2 provides the necessary notation and mathematical preliminaries used throughout the paper. In section 3 we consider the theoretical improvement provided by grad-div stabilization, while in section 4 we show how the fixed point operator associated with the AH iteration allows for the AA theory from [30] to be applied. Finally, in section 5, we give results of several numerical tests that show AH method enhanced with AA and grad-div stabilization can be a very effective nonlinear solver for the steady NSE."
        },
        {
            "heading": "2. Preliminaries",
            "text": "In this section we provide some mathematical preliminaries and notation that will be used throughout the paper. We begin by defining the following function spaces on a domain \u2126 that either has smooth boundary or is a convex polygon:\nL2(\u2126) := {w : \u2126 7\u2192 R : \u2016w\u2016L2(\u2126) := (\u222b\n\u2126\n|w|2 dx )1/2 <\u221e},\nHm(\u2126) := {w : \u2126 7\u2192 R : \u2016w\u2016Hm(\u2126) :=  \u2211 |\u03b2|\u2264m \u2016D\u03b2w\u20162L2(\u2126) 1/2 <\u221e}. Throughout this paper, (., .) and \u2016 \u00b7 \u2016 denote the inner product and norm on L2(\u2126), respectively. All other norms will be denoted with subscripts. Also, we define the following natural spaces for NSE:\nQ := {w \u2208 L2(\u2126) : \u222b\n\u2126\nw dx = 0},\nX := {w \u2208 H 1(\u2126) : w|\u2202\u2126 = 0},\nand we do not distinguish vector and scalar valued spaces, as it will be clear from context. The skew-symmetric trilinear form b\u2217 : X \u00d7X \u00d7X \u2192 R is defined by\nb\u2217(u, v, w) = 1\n2 (((u \u00b7 \u2207)v, w)\u2212 (u \u00b7 \u2207)w, v)), (2.1)\nand it can easily be observed that b\u2217(u, v, v) = 0. (2.2)\nWe will also utilize the well known bound resulting from Ho\u0308lder\u2019s and Sobolev inequalties [24]:\n|b\u2217(u, v, w)| \u2264M\u2016\u2207u\u2016\u2016\u2207v\u2016\u2016\u2207w\u2016, (2.3)\nwhere M is a constant depending only on \u2126.\n2.1. Finite element preliminaries\nLet Xh\u00d7Qh \u2282 X\u00d7Q be conforming and finite dimensional finite element spaces for the velocity and pressure. Then a finite element method for (1.1), based on the standard velocity-pressure formulation and equipped with grad-div stabilization seeks (u, p) \u2208 Xh\u00d7Qh such that \u2200(v, q) \u2208 Xh\u00d7Qh we have\n\u03bd(\u2207u,\u2207v) + \u03b3(\u2207 \u00b7 u,\u2207 \u00b7 v) + b\u2217(u, u, v)\u2212 (\u2207 \u00b7 v, p) = (f, v) (2.4a) (\u2207 \u00b7 u, q) = 0. (2.4b)\nHere, \u03b3 \u2265 0 is referred to as the grad-div stabilization parameter. The discrete problem (2.4) is well-posed if the pair satisfies the inf-sup condition\nsup 0 6=v\u2208Xh (\u2207 \u00b7 v, q) \u2016\u2207v\u2016 \u2265 \u03b2\u2016q\u2016 \u2200q \u2208 Qh,\nfor some \u03b2 > 0, and the small data condition \u03ba := M\u03bd\u22122\u2016f\u2016\u22121 < 1 holds. The small data condition is needed for uniqueness (although precisely how sharp it is remains an open question), although existence and boundedness can be proven for any given data. Common choices that satisfy the infsup condition with \u03b2 independent of h, and the ones we make herein are Xh \u00d7 Qh = Pk(\u03c4h) \u2229 X \u00d7 Pk\u22121(\u03c4h) \u2229 C0(\u2126) Taylor Hood elements (with \u03c4h representing a regular conforming mesh of \u2126) and Xh\u00d7Qh = Pk(\u03c4h)\u2229X\u00d7Pk\u22121(\u03c4h)\u2229Q Scott-Vogelius elements with appropriate k and mesh structure (see e.g. [18, 22] for more details).\nWe will utilize the bound \u2016\u2207u\u2016 \u2264 \u03bd\u22121\u2016f\u2016\u22121, (2.5)\nwhich is proven in [14, 24] and can be easily deduced from (2.4).\n2.2. The Arrow-Hurwicz method\nWe recall the Arrow-Hurwicz (AH) method from [7] for steady Navier-Stokes equations. The method is given in [37] with a slight change of parameter variables.\nAlgorithm 2.1. Let \u03c1, \u03b1 > 0 be user selected parameters, then\n1. Let u0 \u2208 Xh and p0 \u2208 Qh be the solution of the mixed formulation: \u2200(v, q) \u2208 Xh \u00d7Qh :\n(\u2207u0,\u2207v)\u2212 (\u2207 \u00b7 v, p0) = (f, v) (2.6a) (\u2207 \u00b7 u0, q) = 0, (2.6b)\n2. For m \u2265 0, we define um+1 \u2208 Xh to be the solution of the following variational formulation : \u2200v \u2208 Xh, um+1 satisfies\n1 \u03c1 (\u2207(um+1 \u2212 um),\u2207v) + \u03bd(\u2207um,\u2207v) + b(um;um+1, v)\u2212 (\u2207 \u00b7 v, pm) = (f, v), (2.7)\nand pm+1 \u2208 Qh to be the solution of the following variational formulation : \u2200q \u2208 Qh, pm+1 satisfies\n\u03b1((pm+1 \u2212 pm), q) + \u03c1(\u2207 \u00b7 um+1, q) = 0. (2.8)\nRemark 2.2. The well-posedness and convergence of the AH scheme was shown in [37], under some assumptions on the parameter choices for \u03c1 and \u03b1. Under similar choices and additional data restrictions beyond the small data condition, the AH method was shown to be contractive in [7]. While contractive, the linear convergence rate is rather hard to decipher from the analysis, which is rather technical (although still an important step forward). Indeed the rate could be very close to 1, and in the computations with the AH method in [7], it appears that it often is."
        },
        {
            "heading": "3. Convergence analysis of a grad-div stabilized AH method",
            "text": "In this section we consider the following grad-div stabilized AH method, and will show that it has improved convergence properties over the usual AH method.\nAlgorithm 3.1. Let \u03c1, \u03b1 > 0 be parameters, then\n1. Let u0 \u2208 Xh and p0 \u2208 Qh be the solution of the Stokes problem (2.6). 2. For m \u2265 0, we define um+1 \u2208 Xh to be the solution of: \u2200v \u2208 Xh, um+1 satisfies\n1 \u03c1 (\u2207(um+1 \u2212 um),\u2207v) + \u03bd(\u2207um,\u2207v) + b(um;um+1, v)\n+ \u03b3(\u2207 \u00b7 um+1,\u2207 \u00b7 v)\u2212 (\u2207 \u00b7 v, pm) = (f, v) (3.1)\nwhere the grad-div parameter \u03b3 > 0 is a user selected parameter, and pm+1 \u2208 Qh to be the solution of: \u2200q \u2208 Qh, pm+1 satisfies\n\u03b1((pm+1 \u2212 pm), q) + \u03c1(\u2207 \u00b7 um+1, q) = 0. (3.2)\nWe show in this section how grad-div stabilization can provide improved convergence for AH . First we show linear convergence through a connection to the classical iterated Picard penalty method, and then we show the classical convergence analysis with the grad-div term included. Throughout this section u and p represent the solution of (2.4), and we assume the small data condition \u03ba < 1 holds.\n3.1. Linear convergence of the grad-div stabilized AH iteration via a connection to the iterated Picard penalty method\nIn this section we show that with certain discretizations, linear convergence for the grad-div stabilized AH iteration can be established under a small data condition. In particular, we consider the case of velocity and pressure spaces satisfying both the inf-sup stability condition and \u2207 \u00b7Xh = Qh. For example, (P2, P disc 1 ) Scott-Vogelius elements on Alfeld splits satisfy this property [22]. We prove that with the right parameters, linear convergence that is equivalent to that of the iterated Picard penalty (IPP) method is achieved (which in practice is close to that of the well known Picard method [14, 9, 27, 33]).\nThe IPP method for the steady NSE is a classical method that has been well studied and extensively used [17, 27, 35, 9], and is defined [9] by: Given uk \u2208 Xh, pk \u2208 Qh, solve for uk+1 \u2208 Xh, pk+1 \u2208 Qh satisfying\nb\u2217(um, um+1, v)\u2212 (pm+1,\u2207 \u00b7 v) + \u03bd(\u2207um+1,\u2207v) = (f, v) \u2200v \u2208 Xh, (3.3) (pm+1, q) + (\u2207 \u00b7 um+1, q) = (pm, q) \u2200q \u2208 Qh, (3.4)\nwhere > 0 is a penalty parameter which is generally taken small. It is proven by Codina in [9] that if the penalty parameter < \u03bd\u03b2 2\nM2 and small data condition \u03ba < 1 holds then both \u2016p m+1 \u2212 p\u2016 and\n\u2016\u2207(um \u2212 u)\u2016 converge linearly to 0, with rate at most\nrateIPP =\n( 1\n2 +\n1\n2\n( 1 + 2\nr\n)1/2)( \u03ba+ \u03bd\u22121\u03b2\u22122(MC1 + r\u03bd) 2 ) , (3.5)\nwhere C1 = ( \u03bd\u22121\u2016f\u2016\u22121 + 1/2\u03bd\u22121/2 ( \u03b1\u03bd\u03b2\u22121\u2016\u2207(u\u2212 u0)\u2016+ \u2016p\u2212 p0\u2016+ \u2016p\u2016 )) / ( 1\u2212 1/2\u03bd\u22121/2\u03b2\u22121M )\nand r \u2265 2 (the optimal r \u2265 2 will depend on the other parameters). While this is a complex expression, a rate closer to \u03ba \u2013the convergence rate of the usual Picard iteration [14] which is recovered when = 0\u2013 is typically observed. However, even with larger penalty parameter such as = 1, IPP enhanced with Anderson acceleration can be very effective, even for larger data [33].\nWe now establish that if \u03b3 = \u03c1\u03b1\u22121, \u03c1 = \u03bd\u22121 and \u03b1 = \u03bd , then the grad-div stabilized AH method is identical to IPP. From (3.2), since \u2207 \u00b7 Xh = Qh, we observe that pm+1 \u2261 pm \u2212 \u03c1\u03b1\u22121\u2207 \u00b7 um+1 \u2261 pm \u2212 \u03b3\u2207 \u00b7 um+1, by taking \u03b3 = \u03c1\u03b1\u22121. Substituting into (3.1), we obtain\n1 \u03c1 (\u2207(um+1 \u2212 um),\u2207v) + \u03bd(\u2207um,\u2207v) + b\u2217(um, um+1, v)\u2212 (pm+1,\u2207 \u00b7 v) = (f, v),\nand so setting \u03c1 = \u03bd\u22121 yields\n\u03bd(\u2207um+1,\u2207v) + b\u2217(um, um+1, v)\u2212 (pm+1,\u2207 \u00b7 v) = (f, v).\nNote that the grad-div stabilized AH momentum equation now exactly matches with the IPP momentum equation (3.3). Substituting \u03c1 = \u03bd\u22121 into (3.2) we obtain\n\u03bd\u03b1(pm+1 \u2212 pm, q) + (\u2207 \u00b7 um+1, q) = 0.\nFinally setting \u03b1 = \u03bd , we recover (3.4), thus establishment of the grad-div stabilized AH and IPP methods are equivalent when parameters are chosen so that \u03b3 = \u03c1\u03b1\u22121, \u03c1 = \u03bd\u22121 and \u03b1 = \u03bd . With this connection, we have proved the following theorem for grad-div stabilized AH method.\nTheorem 3.2. Suppose grad-div stabilized AH is computed with parameters \u03c1 = \u03bd\u22121, \u03b1 = \u03bd and \u03b3 = \u03c1\u03b1\u22121 = \u22121, with user selected penalty parameter < \u03bd\u03b2 2\nM2 . Then, under the small data condition \u03ba < 1, grad-div stabilized AH method converges linearly with a rate at most rateIPP defined in (3.5).\nRemark 3.3. Just as with IPP, the convergence rate of \u03ba will typically be observed in practice for sufficiently small . Moreover, with AA, larger penalty parameters such as = 1 can be used and yield a very efficient and effective iteration even for \u03ba > 1 as shown in [33].\nRemark 3.4. While we do not prove it in our current manuscript, we expect that the parameters \u2018near\u2019 those in the theorem will still provide a linear convergence by continuity. We believe that it can be proved by using the similar argument followed in [9], however the additional terms will create a quite challenging theory that is beyond the scope of this paper. Our numerical tests show that grad-div stabilized AH is effective with a rather wide range of parameter choices, especially for smaller and with AA.\n3.2. Improvement to classical analysis of AH method via grad-div stabilization\nWe now consider the improvements to the classical convergence arguments for AH method, without assuming certain choices of finite elements or meshes other than Xh \u00d7 Qh which satisfy the inf-sup condition. Begin the analysis by adding and subtracting the true solution (u, p) from (3.1) - (3.2), and then subtracting (2.4) yields\n1 \u03c1 (\u2207(um+1 \u2212 u+ u\u2212 um),\u2207v) + \u03bd(\u2207(um \u2212 u),\u2207v) + b\u2217(um, um+1, v)\n\u2212 b\u2217(u, u, v) + \u03b3(\u2207 \u00b7 (um+1 \u2212 u),\u2207 \u00b7 v)\u2212 (\u2207 \u00b7 v, pm \u2212 p) = 0, (3.6)\nand \u03b1((pm+1 \u2212 p+ p\u2212 pm), q) + \u03c1(\u2207 \u00b7 (um+1 \u2212 u+ u), q) = 0, (3.7)\nrespectively. Denote em+1 = um+1 \u2212 u and em+1p = pm+1 \u2212 p. Taking v = em+1 and q = em+1p in (3.6)-(3.7) respectively gives us\n1 \u03c1 (\u2207(em+1 \u2212 em),\u2207em+1) + \u03bd(\u2207em,\u2207em+1) + b\u2217(um, um+1, em+1)\n\u2212 b\u2217(u, u, em+1) + \u03b3(\u2207 \u00b7 em+1,\u2207 \u00b7 em+1)\u2212 (\u2207 \u00b7 em+1, emp ) = 0, (3.8)\n\u03b1((em+1p \u2212 emp ), epm+1) + \u03c1(\u2207 \u00b7 em+1, em+1p ) = 0. (3.9)\nSince b\u2217 is skew-symmetric with respect to its last two arguments,\nb\u2217(um, um+1, em+1)\u2212 b\u2217(u, u, em+1) = b\u2217(um, um+1, um+1)\u2212 b\u2217(um, um+1, u)\u2212 b\u2217(u, u, um+1), = b\u2217(em, u, um+1),\n= b\u2217(em, u, um+1)\u2212 b\u2217(em, u, u), = b\u2217(em, u, em+1).\nNext adding (3.8)-(3.9), using the polarization identity, rearranging and simplifying the terms, we get\n1 2\u03c1 (\u2016\u2207(em+1 \u2212 em)\u20162 + \u2016\u2207em+1\u20162) + \u03bd\u2016\u2207em+1\u20162 + \u03b3\u2016\u2207 \u00b7 em+1\u20162 + \u03b1 2\u03c1 (\u2016em+1p \u2212 emp \u20162 + \u2016em+1p \u20162)\n= 1 2\u03c1 \u2016\u2207em\u20162 + \u03b1 2\u03c1 \u2016emp \u20162 + \u03bd(\u2207(em+1 \u2212 em),\u2207em+1)\u2212 b\u2217(em, u, em+1) + (\u2207 \u00b7 em+1, emp \u2212 em+1p ).\n(3.10)\nTo bound the nonlinear term on the right hand side of last relation, we use (2.3) and (2.5):\n| \u2212 b\u2217(em, u, em+1)| \u2264M\u2016\u2207em\u2016\u2016\u2207u\u2016\u2016\u2207em+1\u2016 \u2264M\u03bd\u22121\u2016f\u2016\u22121\u2016\u2207em\u2016\u2016\u2207em+1\u2016 = \u03ba\u03bd\u2016\u2207em\u2016\u2016\u2207em+1\u2016.\nFor the third and the last terms in (3.10), we utilize Cauchy-Schwarz and Young\u2019s inequalities via\n\u03bd(\u2207(em+1 \u2212 em),\u2207em+1) + (\u2207 \u00b7 em+1, emp \u2212 em+1p )\n\u2264 1 4\u03c1 \u2016\u2207(em+1 \u2212 em)\u20162 + \u03c1\u03bd2\u2016\u2207em+1\u20162 + \u03b1 4\u03c1 \u2016emp \u2212 em+1p \u20162 + \u03c1 \u03b1 \u2016\u2207 \u00b7 em+1\u20162. (3.11)\nUsing Young\u2019s and the triangle inequalities provides\n\u03ba\u03bd\u2016\u2207em\u2016\u2016\u2207em+1\u2016 \u2264 \u03ba\u03bd(\u2016\u2207(em+1\u2212em)\u2016+\u2016\u2207em+1\u2016)\u2016\u2207em+1\u2016 \u2264 \u03ba\u03bd 2 \u2016\u2207(em+1\u2212em)\u20162+3\u03ba\u03bd 2 \u2016\u2207em+1\u20162,\nand now combining the above bounds, we obtain( 1\n2\u03c1 \u2212 \u03ba\u03bd 2\n) \u2016\u2207(em+1 \u2212 em)\u20162 + ( 1\n2\u03c1 + \u03bd \u2212 \u03c1\u03bd2 \u2212 3\u03ba\u03bd 2\n) \u2016\u2207em+1\u20162\n+ ( \u03b3 \u2212 \u03c1\n\u03b1\n) \u2016\u2207 \u00b7 em+1\u20162 + \u03b1\n2\u03c1 \u2016em+1p \u20162 +\n\u03b1 4\u03c1 \u2016emp \u2212 em+1p \u20162 \u2264 1 2\u03c1 \u2016\u2207em\u20162 + \u03b1 2\u03c1 \u2016emp \u20162. (3.12)\nProvided that \u03c1 \u2264 \u03bd\u22121 max { \u03ba\u22121, 1 } and \u03b3 \u2265 \u03c1\u03b1 along with the additional small data assumption \u03ba < 23 , the estimate (3.12) is sufficient to provide convergence of the grad-div stabilized AH method. Comparing to analysis without the grad-div term from [7], we observe that with grad-div the coefficient of the left hand side term \u2016\u2207em+1\u20162 is larger and there are less restrictions on the parameters (including no restriction now on \u03b1). The key difference arises from utilizing the left hand side term \u2016\u2207 \u00b7 em+1\u2016, allowing for a larger coefficient of \u2016\u2207 \u00b7 em+1\u2016. Since this is not a proof of contraction, it offers less of a comparison of rates than the previous section did. Note that if \u2207 \u00b7Xh \u2282 Qh then we can follow the proof of [7] to prove a contraction, however these results would be similar to that of [7] and not nearly as strong as what is proven above in Theorem 3.2."
        },
        {
            "heading": "4. Anderson Acceleration applied to the grad-div stabilized AH Method",
            "text": "In this section, we show that the Anderson acceleration (AA) method can be applied to the graddiv stabilized AH method (Algorithm 3.1) and will improve its linear convergence rate. We begin this section with a review of AA and recent theoretical results. We will proceed to show how the grad-div stabilized AH method fits into this framework, which in turn allows for invoking the AA theory. Throughout this section, we assume that the data is sufficiently small and parameters are chosen so that grad-div stabilized AH provides a contractive iteration; we specify this assumption below precisely, after we give some notation.\n4.1. Anderson acceleration\nIn this subsection, we provide AA procedure and its convergence properties. Consider a fixedpoint operator g : Y \u2192 Y where Y is a Hilbert space equipped with induced norm \u2016 \u00b7 \u2016Y , and denote wj = g(xj\u22121) \u2212 xj\u22121 as the nonlinear residual, also sometimes is called the update step. Then, the AA algorithm with depth m (if m = 0, it returns to usual Picard iteration) applied to the fixed-point problem g(x) = x, reads as follows.\nAlgorithm 4.1. (Anderson acceleration with depth m and damping factors \u03b2k) Step 0: Choose x0 \u2208 Y. Step 1: Find w1 \u2208 Y such that w1 = g(x0)\u2212 x0. Set x1 = x0 + w1. Step k: For k = 2, 3, . . . Set mk = min{k \u2212 1,m}.\n[a.] Find wk = g(xk\u22121)\u2212 xk\u22121. [b.] Solve the minimization problem for the Anderson coefficients {\u03b1kj } k\u22121 k\u2212mk\n{\u03b1kj }k\u22121k\u2212mk = argmin \u2225\u2225\u2225\u2225\u2225 ( 1\u2212 k\u22121\u2211\nj=k\u2212mk \u03b1kj\n) wk +\nk\u22121\u2211 j=k\u2212mk \u03b1kjwk\u2212j \u2225\u2225\u2225\u2225\u2225 Y . (4.1)\n[c.] For damping factor 0 < \u03b2k \u2264 1, set\nxk = (1\u2212 k\u22121\u2211\nj=k\u2212mk \u03b1kj )xk\u22121 +\n\u2211k\u22121 j=k\u2212mk \u03b1 k jxj\u22121 + \u03b2k ( (1\u2212 k\u22121\u2211 j=k\u2212mk \u03b1kj )wk + k\u22121\u2211 j=k\u2212mk \u03b1kjwk\u2212j ) . (4.2)\nTo understand how AA improves convergence, we define the optimization gain factor \u03b8k by\n\u03b8k =\n\u2225\u2225\u2225\u2225\u2225(1\u2212 k\u22121\u2211j=k\u2212mk \u03b1kj )wk + k\u22121\u2211 j=k\u2212mk \u03b1kjwk\u2212j \u2225\u2225\u2225\u2225\u2225 Y\n\u2016wk\u2016Y ,\nwhich characterizes the improvement in fixed-point convergence rate as proposed in [30, 31]. The following assumptions from [30] provide sufficient conditions on the fixed point operator g for the convergence and acceleration results.\nAssumption 4.2. Assume g \u2208 C1(Y ) has a fixed point x\u2217 in Y , and there are positive constants C0 and C1 with\n1. \u2016g\u2032(x)\u2016Y \u2264 C0 for all x \u2208 Y , and 2. \u2016g\u2032(x)\u2212 g\u2032(y)\u2016Y \u2264 C1\u2016x\u2212 y\u2016Y for all x, y \u2208 Y .\nAssumption 4.3. Assume there is a constant \u03c3 > 0 for which the differences between consecutive residuals and iterates satisfy\n\u2016wk+1 \u2212 wk\u2016Y \u2265 \u03c3\u2016xk \u2212 xk\u22121\u2016Y , k \u2265 1. (4.3)\nAssumption 4.2 will be verified for Picard fixed-point operator grad-div stabilized AH method in next sections. Also, Assumption 4.3 can be verified easily for this method which is contractive under small data and particular parameter choices. Under Assumptions 4.2 and 4.3, the following result from [30], generates a bound on the residual \u2016wk+1\u2016 in terms of the previous residual \u2016wk\u2016.\nTheorem 4.4 (Pollock et al., 2021). Let Assumptions 4.2 and 4.3 hold, and suppose the direction sines between each column j of matrix\nFj := ( (wj \u2212 wj\u22121)(wj\u22121 \u2212 wj\u22122) . . . (wj\u2212mj+1 \u2212 wj\u2212mj ) ) and the subspace spanned by the preceeding columns satisfy | sin(fj,i, span {fj,1, . . . , fj,i\u22121})| \u2265 cs > 0, for j = k\u2212mk, . . . , k\u22121. Then the residual wk+1 = g(xk)\u2212xk from Algorithm 4.1 (depth m) satisfies the following bound.\n\u2016wk+1\u2016 \u2264 \u2016wk\u2016 ( \u03b8k((1\u2212 \u03b2k) + C0\u03b2k) + CC1 \u221a\n1\u2212 \u03b82k 2\n( \u2016wk\u2016h(\u03b8k)\n+ 2 k\u22121\u2211 n=k\u2212mk+1 (k \u2212 n) \u2016wn\u2016h(\u03b8n) +mk \u2016wk\u2212mk\u2016h(\u03b8k\u2212mk) )) , (4.4)\nwhere each h(\u03b8j) \u2264 C \u221a\n1\u2212 \u03b82j + \u03b2j\u03b8j, and C depends on cs and the implied upper bound on the direction cosines.\nIn (4.4), the optimization gain \u03b8k is scaling the first-order term, which is residual in the standard fixed-point iteration. On the other hand, the higher-order terms are scaled by a factor of \u221a 1\u2212 \u03b82k, which implies if the optimization works, the relative weight of the higher-order terms increase, otherwise the relative weight of the first-order term increase in (4.4).\n4.2. Grad-div stabilized AH method as a fixed point iteration\nIn this subsection, we define the fixed-point operator G which is associated with grad-div stabilized AH iteration. Note that in this section, u, p are generic functions and are not the steady NSE solution as in the previous section.\nDefinition 4.5. Define mapping G : (Xh, Qh) \u2192 (Xh, Qh), G(u, p) = (G1(u, p), G2(u, p)) such that for any (v, q) \u2208 (Xh, Qh)\n\u03c1\u22121(\u2207(G1(u, p)\u2212 u),\u2207v) + \u03bd(\u2207u,\u2207v) + b(u;G1(u, p), v) +\u03b3(\u2207 \u00b7G1(u, p),\u2207 \u00b7 v)\u2212 (p,\u2207 \u00b7 v) = (f, v), (4.5)\n\u03b1(G2(u, p)\u2212 p, q) + \u03c1(\u2207 \u00b7G1(u, p), q) = 0. (4.6)\nNow, we show thatG is well-defined and bounded with respect to the norm \u2016(v, q)\u2016H =: \u221a \u2016\u2207v\u20162 + \u03b1\u2016q\u20162\non Xh \u00d7Qh.\nLemma 4.6. The operator G is well-defined. Moreover, if G(u, p) = (G1(u, p), G2(u, p)) is the solution of (4.5)-(4.6), then the following inequality holds\n\u2016G(u, p)\u2016H \u2264 \u221a (1 + 4\u03c12\u03bd2)\nN \u2016\u2207u\u2016+\n\u221a \u03b1\nN \u2016p\u2016+\n\u221a 4\u03c12\nN \u2016f\u2016\u22121, (4.7)\nwhere N := min{ 12 \u2212 \u03b1 \u22121\u03c12, 1}.\nProof. Assume that a solution exists. Then, choosing v = G1(u, p) and q = G2(u, p) eliminates the nonlinear term and yields\n1\n2\n( \u2016\u2207G1(u, p)\u20162 \u2212 \u2016\u2207u\u20162 + \u2016\u2207(G1(u, p)\u2212 u)\u20162 ) + \u03c1\u03b3\u2016\u2207 \u00b7G1(u, p)\u20162 \u2212 \u03c1(p,\u2207 \u00b7G1(u, p))\n= \u03c1(f,G1(u, p))\u2212 \u03c1\u03bd(\u2207u,\u2207G1(u, p)), \u03b1\n2\n( \u2016G2(u, p)\u20162 \u2212 \u2016p\u20162 + \u2016G2(u, p)\u2212 p\u20162 ) + \u03c1(\u2207 \u00b7G1(u, p), G2(u, p)) = 0.\nthanks to the polarization identity. Then, combining the above equations gives that\n1\n2\n( \u2016\u2207G1(u, p)\u20162 + \u2016\u2207(G1(u, p)\u2212 u)\u20162 ) + \u03b1\n2\n( \u2016G2(u, p)\u20162 + \u2016G2(u, p)\u2212 p\u20162 ) + \u03c1\u03b3\u2016\u2207 \u00b7G1(u, p)\u20162\n= 1 2 \u2016\u2207u\u20162 + \u03b1 2 \u2016p\u20162 + \u03c1(f,G1(u, p))\u2212 \u03c1\u03bd(\u2207u,\u2207G1(u, p))\u2212 \u03c1(G2(u, p)\u2212 p,\u2207 \u00b7G1(u, p)).\nMultiplying the both sides of the last relation by 2, dropping positive terms \u2016\u2207(G1(u, p) \u2212 u)\u20162, \u03c1\u03b3\u2016\u2207\u00b7G1(u, p)\u20162, and \u2016G2(u, p)\u2212p\u20162 on the left hand side, and using Ho\u0308lder\u2019s and Young\u2019s inequalities produce(\n1 2 \u2212 \u03b1\u22121\u03c12\n) \u2016\u2207G1(u, p)\u20162 + \u03b1\u2016G2(u, p)\u20162 \u2264 (1 + 4\u03c12\u03bd2)\u2016\u2207u\u20162 + \u03b1\u2016p\u20162 + \u03c124\u2016f\u20162\u22121.\nLetting N := min{ 12 \u2212 \u03b1 \u22121\u03c12, 1}, and dividing both sides by N , we get\n\u2016G(u, p)\u20162 \u2264 (1 + 4\u03c1 2\u03bd2) N \u2016\u2207u\u20162 + \u03b1 N \u2016p\u20162 + 4\u03c1 2 N \u2016f\u20162\u22121.\nThen, taking the square root of both sides reduces it to (4.7). Since G is linear and finite dimensional, showing that the solution G(u, p) is bounded continuously by the data implies solution uniqueness and thus existence as well.\nWe now rewrite the grad-div stabilized AH method in terms of a mapping G : (Xh, Qh)\u2192 (Xh, Qh) that satisfies for m \u2265 0\nG(um, pm) = (G1(u m, pm), G2(u m, pm)) := (um+1, pm+1),\nwhere (um, pm) is the mth iteration of the A-H method described in Algorithm 3.1.\n4.3. Applying AA to the grad-div stabilized AH iteration\nIn this subsection, we show the sufficient smoothness properties of the associated fixed point operator G for the grad-div stabilized AH iteration to apply AA theory. Now, we show Lipschitz continuity of G.\nLemma 4.7. For any (u, p), (w, z) \u2208 (Xh, Qh), we have\n\u2016G(u, p)\u2212G(w, z)\u2016H \u2264 CL\u2016(u, p)\u2212 (w, z)\u2016H (4.8) where CL = max{ \u221a 2 K , ( 1\u2212\u03c1\u03bd\u221a K + \u03c1\u221a KN ((1 + 2\u03c1\u03bd)\u2016\u2207u\u2016+ \u221a \u03b1\u2016p\u2016+ 2\u03c1\u2016f\u2016\u22121) ) }.\nRemark 4.8. Note that we have already discussed that in case of sufficiently small data and particularly chosen parameters, Algorithm 3.1 is contractive. In this section we assume to be in the contractive setting, and thus we have that the Lipschitz constant CL in Lemma 4.7 is less than 1.\nProof. Subtracting (4.5) with (w, z) from (4.5) with (u, p) gives\n(\u2207(G1(u, p)\u2212G1(w, z)),\u2207v)\u2212 (\u2207(u\u2212 w),\u2207v) + \u03c1\u03bd(\u2207(u\u2212 w),\u2207v) + \u03c1b(u;G1(u, p)\u2212G1(w, z), v) +\u03c1b(u\u2212 w;G1(w, z), v) + \u03c1\u03b3(\u2207 \u00b7 (G1(u, p)\u2212G1(w, z)),\u2207 \u00b7 v) = \u03c1(p\u2212 z,\u2207 \u00b7 v),\n\u03b1(G2(u, p)\u2212G2(w, z), q)\u2212 \u03b1(p\u2212 z, q) + \u03c1(\u2207 \u00b7 (G1(u, p)\u2212G1(w, z)), q) = 0.\nThen, setting v = G1(u, p) \u2212 G1(w, z) and q = G2(u, p) \u2212 G2(w, z) which eliminates the first nonlinear term on the left hand side of the first equation, and combining these equations provide\n\u2016\u2207(G1(u, p)\u2212G1(w, z))\u20162 + \u03b1\u2016G2(u, p)\u2212G2(w, z)\u20162 + \u03c1\u03b3\u2016\u2207 \u00b7 (G1(u, p)\u2212G1(w, z))\u20162\n= \u03c1(p\u2212 z,\u2207 \u00b7 (G1(u, p)\u2212G1(w, z))) + (1\u2212 \u03c1\u03bd)(\u2207(u\u2212 w),\u2207(G1(u, p)\u2212G1(w, z))) \u2212 \u03c1b(u\u2212 w;G1(w, z), G1(u, p)\u2212G1(w, z)) + \u03b1(p\u2212 z,G2(u, p)\u2212G2(w, z)) \u2212 \u03c1(\u2207 \u00b7 (G1(u, p)\u2212G1(w, z)), G2(u, p)\u2212G2(w, z)).\nBy dropping positive term \u03c1\u03b3\u2016\u2207 \u00b7 (G1(u, p) \u2212 G1(w, z))\u20162 on the left hand side and using CauchySchwarz and Young\u2019s inequalities, Lemma 4.6 and (2.3) , we get(\n1 2 \u2212 3\u03c1\n2\u03b1\u22121\n4\n) \u2016\u2207(G1(u, p)\u2212G1(w, z))\u20162 + \u03b1\n4 \u2016G2(u, p)\u2212G2(w, z)\u20162 \u2264 2\u03b1\u2016p\u2212 z\u20162\n+ ( (1\u2212 \u03c1\u03bd)2 + \u03c12 ( (1 + 4\u03c12\u03bd2)\nN \u2016\u2207u\u20162 + \u03b1 N \u2016p\u20162 + 4\u03c1\n2\nN \u2016f\u20162\u22121\n)) \u2016\u2207(u\u2212 w)\u20162.\nDefining K = min{ (\n1 2 \u2212\n3\u03c12\u03b1\u22121\n4 ) , 14} and then dividing both sides by K yields (4.8).\nNext, we define an operator G\u2032 and show it is indeed the Fre\u0301chet derivative of the operator of G.\nDefinition 4.9. Given (u, p) \u2208 Xh \u00d7Qh, define an operator G\u2032(u, p; \u00b7, \u00b7) : Xh \u00d7Qh \u2192 Xh \u00d7Qh by\nG\u2032(u, p;w, s) =: (G\u20321(u, p;w, s), G \u2032 2(u, p;w, s))\nsatisfying for all (w, s) \u2208 Xh \u00d7Qh.\n(\u2207(G\u20321(u, p;w, s)\u2212 w),\u2207v) + \u03c1\u03bd(\u2207w,\u2207v)+\u03c1b(w;G1(u, p), v) + \u03c1b(u;G\u20321(u, p;w, s), v) \u03c1\u03b3(\u2207 \u00b7G\u20321(u, p;w, s),\u2207 \u00b7 v)\u2212 \u03c1(s,\u2207 \u00b7 v) = 0,\n\u03b1(G\u20322(u, p;w, s)\u2212 s, q) + \u03c1(\u2207 \u00b7G\u20321(u, p;w, s), q) = 0. (4.9)\nLemma 4.10. The operator G\u2032 is well-defined for all (u, p), (w, s) \u2208 Xh \u00d7Qh such that\n\u2016G\u2032(u, p;w, s)\u2016H \u2264 CL\u2016(w, s)\u2016H (4.10)\nProof. Adding equations in (4.9) and setting v = G\u20321(u, p;w, s) and q = G \u2032 2(u, p;w, s) produces\n\u2016\u2207G\u20321(u, p;w, s)\u20162 + \u03c1\u03b3\u2016\u2207 \u00b7G\u20321(u, p;w, s)\u20162 + \u03b1\u2016G\u20322(u, p;w, s)\u20162\n= (1\u2212 \u03c1\u03bd)(\u2207w,\u2207G\u20321(u, p;w, s)) + \u03b1(s,G\u20322(u, p;w, s))\u2212 \u03c1b(w;G1(u, p), G\u20321(u, p;w, s)) + \u03c1(s,\u2207 \u00b7G\u20321(u, p;w, s))\u2212 \u03c1(\u2207 \u00b7G\u20321(u, p;w, s), G\u20322(u, p;w, s)).\n(4.11)\nThen, by dropping positive term \u03c1\u03b3\u2016\u2207\u00b7G\u20321(u, p;w, s)\u20162 on the left hand side, applying Cauchy-Schwarz and Young\u2019s inequalities produces\n\u2016\u2207G\u20321(u, p;h, s)\u20162 + \u03b1\u2016G\u20322(u, p;h, s)\u20162\n\u2264 (1\u2212 \u03c1\u03bd)2\u2016\u2207h\u20162 + 1 4 \u2016\u2207G\u20321(u, p;h, s)\u20162 + \u03b1\u2016s\u20162 + \u03b1 4 \u2016G\u20322(u, p;h, s)\u20162\n+ \u03c12M2 ( (1 + 4\u03c12\u03bd2)\nN \u2016\u2207u\u20162 + \u03b1 N \u2016p\u20162 + 4\u03c1\n2\nN \u2016f\u20162\u22121\n) \u2016\u2207h\u20162 + 1\n4 \u2016\u2207G\u20321(u, p;h, s)\u20162\n+ \u03b1\u2016s\u20162 + \u03c1 2\u03b1\u22121\n4 \u2016\u2207G\u20321(u, p;h, s)\u20162 +\n\u03c12\u03b1\u22121\n2 \u2016\u2207G\u20321(u, p;h, s)|2 +\n\u03b1 2 \u2016G\u20322(u, p;h, s)\u20162\nthanks to the Lemma 4.6 and (2.3). After rearranging the terms, we get( 1\n2 \u2212 3\u03c1\n2\u03b1\u22121\n4\n) \u2016\u2207G\u20321(u, p;h, s)\u20162 + \u03b1\n4 \u2016G\u20322(u, p;h, s)\u20162\n\u2264 ( (1\u2212 \u03c1\u03bd)2 + \u03c12M2 ( (1 + 4\u03c12\u03bd2)\nN \u2016\u2207u\u20162 + \u03b1 N \u2016p\u20162 + 4\u03c1\n2\nN \u2016f\u20162\u22121\n)) \u2016\u2207h\u20162 + 2\u03b1\u2016s\u20162\nTaking the square root of both sides and considering constant K which is defined in proof of Lemma 4.7 finishes the proof.\nNow, we prove that G\u2032 is Fre\u0301chet derivative operator of G.\nLemma 4.11. For any (u, p) \u2208 Xh \u00d7Qh\n\u2016 G(u+ w, p+ s)\u2212G(u, p)\u2212G\u2032(u, p;w, s)\u2016H \u2264 \u03c1MCL\u221a 1\u2212 \u03c1\u03b1\u22121 \u2016(w, s)\u20162H . (4.12)\nProof. Denote \u03b71 = G1(u + w, p + s) \u2212 G1(u, p) \u2212 G\u20321(u, p;w, s), \u03b72 = G2(u + w, p + s) \u2212 G2(u, p) \u2212 G\u20322(u, p;w, s). Subtracting the sum of (4.9) and (4.5) from the equation (4.5) with (u+w, p+s) yields\n(\u2207\u03b71,\u2207v) + \u03c1\u03b3(\u2207 \u00b7 \u03b71,\u2207 \u00b7 v) + \u03c1b(u; \u03b71, v) + \u03c1b(w;G1(u+ w, p+ s)\u2212G1(u, p), v) = 0, \u03b1(\u03b72, q) + \u03c1(\u2207 \u00b7 \u03b71, q) = 0.\nChoosing v = \u03b71 and q = \u03b72 vanishes first nonlinear term on the left hand side, and combining these equations gives\n\u2016\u2207\u03b71\u20162 + \u03b1\u2016\u03b72\u20162 + \u03c1\u03b3\u2016\u2207 \u00b7 \u03b71\u20162 = \u2212\u03c1(\u2207 \u00b7 \u03b71, \u03b72)\u2212 \u03c1b(w;G1(u+ w, p+ s)\u2212G1(u, p), \u03b71). (4.13)\nApplying Cauchy-Schwarz on the right hand side and dropping \u03c1\u03b3\u2016\u2207\u00b7\u03b71\u20162 on the left hand side yields\n\u2016\u2207\u03b71\u20162 + \u03b1\u2016\u03b72\u20162 \u2264 \u03c1\u2016\u2207 \u00b7 \u03b71\u2016\u2016\u03b72\u2016+M\u03c1\u2016\u2207w\u2016\u2016\u2207G1(u+ w, p+ s)\u2212G1(u, p)\u2016\u2016\u2207\u03b71\u2016.\nNow with Young\u2019s inequality and Lemma 4.7, we obtain using \u2016 \u00b7 \u2207\u2016 \u2264 \u2016\u2207\u2016 and \u2016\u2207w\u20162 \u2264 \u2016\u2207w\u20162 + \u03b1\u2016s\u20162 = \u2016(w, s)\u20162H that\n\u2016\u2207\u03b71\u20162 + \u03b1\u2016\u03b72\u20162 \u2264 \u03c12\u03b1\u22121\n2 \u2016\u2207\u03b71\u20162 +\n\u03b1 2 \u2016\u03b72\u20162 +\n\u03c12\n2 M2C2L\u2016(w, s)\u20164H +\n1 2 \u2016\u2207\u03b71\u20162.\nThen, rearrange and obtain the following\n( 1\n2 \u2212 \u03c1\n2\u03b1\u22121\n2\n) \u2016\u2207\u03b71\u20162 + \u03b1\n2 \u2016\u03b72\u20162 \u2264\n\u03c12\n2 M2C2L\u2016\u2207w\u20162\u2016\u2207(G1(u+ w, p+ s)\u2212G1(u, p))\u20162\n\u2264 \u03c1 2\n2 M2C2L\u2016(w, s)\u20164H ,\nwhere we used (2.3). Then, applying the definition of \u03b71 and \u03b72, dividing both sides by ( 1 2 \u2212 \u03c12\u03b1\u22121 2 ) and taking square roots give that G\u2032 is indeed the Fre\u0301chet derivative of G which satisfies (4.12).\nWe now proceed to show that G\u2032 is Lipschitz continuous over Xh \u00d7Qh. Lemma 4.12. G is Lipschitz continuously differentiable on Xh \u00d7Qh, such that for all u,w, \u03b8 \u2208 Xh and p, s, \u03be \u2208 Qh,\n\u2016G\u2032(u+ w, p+ s; \u03b8, \u03be)\u2212G\u2032(u, p; \u03b8, \u03be)\u2016H \u2264 (\n4\u03c12M2C2L 1\u2212 \u03c12\u03b1\u22121\n)1/2 \u2016(\u03b8, \u03be)\u2016H\u2016(w, s)\u2016H . (4.14)\nProof. Subtracting (4.9) with G\u2032(u, p; \u03b8, \u03be) from (4.9) with G\u2032(u+ w, p+ s; \u03b8, \u03be) and denoting e1 =: G \u2032 1(u+ w, p+ s; \u03b8, \u03be)\u2212G\u20321(u, p; \u03b8, \u03be) and e2 =: G\u20322(u+ w, p+ s; \u03b8, \u03be)\u2212G\u20322(u, p; \u03b8, \u03be) yield\n(\u2207e1,\u2207v) + \u03c1b(\u03b8;G1(u+ w, p+ s)\u2212G1(u, p), v) + \u03c1b(u; e1, v) +\u03c1b(w;G\u20321(u+ w, p+ s; \u03b8, \u03be), v) + \u03c1\u03b3(\u2207 \u00b7 e1,\u2207 \u00b7 v) = 0,\n\u03b1(e2, q) + \u03c1(\u2207 \u00b7 e1, q) = 0.\nSetting v = e1 and q = e2 vanishes the third term on the left hand side of the first equality and adding these equations provide\n\u2016\u2207e1\u20162 + \u03b1\u2016e2\u20162 + \u03c1\u03b3\u2016\u2207 \u00b7 e1\u20162\n= \u2212\u03c1(\u2207 \u00b7 e1, e2)\u2212 \u03c1b(\u03b8;G1(u+ w, p+ s)\u2212G1(u, p), e1)\u2212 \u03c1b(w;G\u20321(u+ w, p+ s; \u03b8, \u03be), e1).\nDropping the positive term \u03c1\u03b3\u2016\u2207\u00b7e1\u20162 on the left hand side, applying Cauchy-Schwarz and Young\u2019s inequalities and (2.3), using Lemma 4.7 and 4.10, we get(\n1 2 \u2212 \u03c1\n2\u03b1\u22121\n2\n) \u2016\u2207e1\u20162 + \u03b1\n2 \u2016e2\u20162 \u2264 \u03c12M2C2L\u2016\u2207\u03b8\u20162\u2016(w, s)\u20162H + \u03c12M2C2L\u2016\u2207w\u20162\u2016(\u03b8, \u03be)\u20162\n\u2264 2\u03c12M2C2L\u2016(\u03b8, \u03be)\u20162H\u2016(w, s)\u20162H . Dividing both sides by (\n1 2 \u2212\n\u03c12\u03b1\u22121\n2\n) gives\n\u2016(e1, e2)\u20162H = \u2016\u2207e1\u20162 + \u03b1\u2016e2\u20162 \u2264 4\u03c12M2C2L 1\u2212 \u03c12\u03b1\u22121 \u2016(\u03b8, \u03be)\u20162H\u2016(w, s)\u20162H .\nThen taking the square roots of both sides gives that G\u2032 is Lipschitz continuous, and (4.14) holds.\n4.4. Convergence of the Anderson Accelerated AH algorithm for steady NSE In previous subsection, we proved that the solution operator G associated with grad-div stabilized AH iteration (4.5)-(4.6) satisfies Assumption 4.2 which is the one of sufficient conditions to apply the one-step residual bound of [30]. Also, Assumption 4.3 is satisfied since G is contractive under small data condition and certain parameter choices.\nUnder these assumptions and with Lemmas 4.7, 4.11, 4.12 and Theorem 4.4, we have established the convergence of (4.5)-(4.6) where G is the solution operator associated with grad-div stabilized AH iteration.\nTheorem 4.13. For any step k > m with \u03b1km 6= 0, the following bound holds for the grad-div stabilized AH iteration (4.5)-(4.6)\n\u2016(wk+1, zk+1)\u2016H \u2264\u03b8k(1\u2212 \u03b2k + \u03b2kCL)\u2016(wk, zk)\u2016H + C \u221a 1\u2212 \u03b82k\u2016(wk, zk)\u2016H m\u2211 j=1 \u2016(wk\u2212j+1, zk\u2212j+1)\u2016H ,\nfor the residual (wk, zk), where \u03b8k is the gain from the optimization problem, CL is the Lipschitz constant of G defined in Lemma 4.7, and C depending on \u03b8k, \u03b2k, CL.\nThis theorem tells us that (4.5)-(4.6), with a good initial guess, converges linearly with rate \u03b8k(1\u2212\u03b2k+\u03b2kCL) < 1, which improves on Algorithm 3.1 due to the scaling \u03b8k and the damping factor \u03b2k. In the case G is contractive, i.e. CL < 1, then the optimal choice for relaxation is \u03b2k = 1."
        },
        {
            "heading": "5. Numerical Experiments",
            "text": "In this section, we perform several numerical tests to illustrate the theory above and to show how the grad-div stabilized, Anderson accelerated AH algorithm can be an effective and efficient solver for the steady NSE. The stopping criteria for all of our tests is \u2016uk \u2212 uk\u22121\u2016 \u2264 10\u22126.\n5.1. Lid-driven cavity\nWe first test the AH method for steady NSE on the lid-driven cavity problem. The domain for the problem is the unit square \u2126 = (0, 1)2 and we impose Dirichlet boundary conditions by u|y=1 = (1, 0)T and u = 0 everywhere else. We choose the parameter \u03b1 = \u03bd\u22121. We first illustrate how grad-div stabilization improves the AH method, and show the dramatic improvement offered by (P2, P disc 1 ) Scott-Vogelius (SV) over (P2, P1) Taylor-Hood (TH). The second test shows even further dramatic improvement by incorporating AA.\n5.1.1. The effect of grad-div stabilization and comparison of Scott-Vogelius vs Taylor-Hood\nOur analysis above suggests that convergence of AH will be improved from using grad-div stabilization, and also from using SV elements instead of TH since the connection to the iterated Picard penalty method is only made for SV elements. Hence we now compare the AH method for both element choices, with and without grad-div stabilization (using parameter \u03b3 = 1). We run the tests for varying \u03c1 (to try to find a good choice of parameter \u03c1) and with varying Re = \u03bd\u22121. For these tests a uniform mesh with h = 1/32 is used.\nResults are shown in Figure 1. We observe the best AH results clearly come from using SV instead of TH, and using \u03b3 = 1 with SV gives by far the best results. In most cases, TH fails to converge for any \u03c1. Based on these results, we use SV elements for the rest of the numerical tests in this paper.\n5.1.2. Anderson accelerated grad-div stabilized AH method with SV elements\nWe now consider the same test problem, using the AH iteration only with SV elements and \u03b3 = 1, but now adding AA. We test AA depths m = 0 (no acceleration),1, 5 and 10. Figure 2 and 3 show convergence results obtained by Anderson accelerated grad-div stabilized AH method for Re = 100 and 1000, respectively, for varying \u03c1. As depth increases, the number of iterations decreases significantly. The fastest convergence is obtained with depths m = 50, but there is not much improvement past m = 5.\nWe also note that for optimally chosen \u03c1 in this setting (i.e. \u03c1 = 20 for Re = 100 and \u03c1 = 50 for Re = 1000 based on test above) with SV elements and grad-div stabilization, there is not much difference in convergence from AA. However, for slightly non optimal \u03c1, there can be a dramatic improvement from AA. Since one often does not know optimal \u03c1 a priori, the expected case in practice is using a non-optimal \u03c1.\nComparing to existing literature, for Re = 100 driven cavity it is reported in [7] that 731 iterations of AH were needed to converge to the same tolerance used herein and with Taylor-Hood elements, \u03c1 = 1.2 and \u03b1 = 70. Our results with Taylor-Hood elements and no grad-div stabilization were similarly bad, see figure 1 in row 1; in fact, that they got convergence at all for this test is rather extraordinary. With SV elements and grad-div stabilization, figure 1 shows that with \u03b1 = 100 and \u03c1 = 20, convergence is achieved in 80 iterations. With less optimal parameter choices, AA can still keep the total iteration count low, see figure 2.\n5.1.3. Driven cavity with Re=5,000 and Re=10,000\nAs a final test with the driven cavity, we consider the case of Re=5,000 and Re=10,000 with h=1/64. This is a difficult problem for nonlinear solvers [31, 30], and we show now that with the right parameter choices, the AH method can be effective for this problem. For 5,000, \u03c1 = 100, \u03b3 = 1, m = 100 were used to obtain convergence in 464 iterations. For 10,000, \u03c1 = 150, \u03b3 = 10, m = 100 was used to obtain convergence in 217 iterations. Streamlines for both solutions are shown in Figure 4, and they are in good agreement with those from [13] even though we use a coarser mesh. We note that, to date, the highest Re for successful lid driven cavity computations in the literature is 1000 in [7].\n5.2. Channel flow past a step\nFor our last test, we consider 2D channel flow past a step with Re = \u03bd\u22121 = 100. The domain for this problem is a 40 \u00d7 10 rectangular channel, with a 1 \u00d7 1 \u2018step\u2019 placed 5 units into the channel at the bottom. The triangulation we use is shown in Figure 5, along with the Re = 100 solution found with our solver (which is consistent with solutions from the literature [21, 15, 16]). The discretization uses (P2, P disc 1 ) SV elements that provided 32,682 velocity degrees of freedom.\nFirst we consider \u03b3 = 10 = \u03b5\u22121, noting that obtaining convergence with \u03b3 = 1 proved very difficult and we were not able to find a parameter set that gave convergence. With \u03b3 = 10, we computed four parameter sets: (\u03c1 = 50, \u03b1 = \u03b5\u03bd ) , (\u03c1 = 50, \u03b1 = 1 \u03bd ), (\u03c1 = 100, \u03b1 = \u03b5 \u03bd ) - which is exactly the iterated penalty Picard method, and (\u03c1 = 100, \u03b1 = 1\u03bd ). Convergence plots for each of these parameter sets and varying m are shown in figure 6, and we observe that m = 100 is the best choice for AA for all cases, and that AH with parameters chosen to match IPP performs significantly worse than other parameter choices. The choice \u03c1 = 50 and \u03b1 = \u03bd\u22121 with m = 100 was very effective. Results improve with \u03b3 = 100 for all parameter sets, see figure 7. Here, again AH improves on IPP, with \u03c1 = 50 and \u03c1 = 100 withe m = 100 were very effective parameter choices."
        },
        {
            "heading": "6. Conclusions",
            "text": "This paper developed multiple improvements to the AH method for solving the steady NavierStokes equations, and showed that with grad-div stabilization, SV elements and Anderson acceleration, the AH method can be a very effective and efficient solver. SV elements and grad-div stabilization allowed us to connect AH to the well known iterated penalty Picard method, which has good convergence properties under small data [33]. We also proved that the AH iteration, under certain conditions on the data and parameters, fits into the Anderson acceleration analysis framework developed in [30] and thus AA improves the linear convergence rate of the AH method by the gain of the underlying AA optimization problem. We also gave results of several numerical tests that show how each of these\nimprovements is important for good convergence behavior, and when used together the AH method can be very effective."
        },
        {
            "heading": "7. Acknowledgment",
            "text": "Author PG acknowledges partial support from National Science Foundation grant DMS 1907823. Authors LR and DV acknowledge partial support from NSF grant DMS 2011490."
        }
    ],
    "title": "Improved convergence of the Arrow-Hurwicz iteration for the Navier-Stokes equation via grad-div stabilization and Anderson acceleration",
    "year": 2022
}