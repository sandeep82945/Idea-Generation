{
    "abstractText": "We explore unique considerations involved in fitting machine learning (ML) models to data with very high precision, as is often required for science applications. We empirically compare various function approximation methods and study how they scale with increasing parameters and data. We find that neural networks (NNs) can often outperform classical approximation methods on high-dimensional examples, by (we hypothesize) auto-discovering and exploiting modular structures therein. However, neural networks trained with common optimizers are less powerful for low-dimensional cases, which motivates us to study the unique properties of neural network loss landscapes and the corresponding optimization challenges that arise in the high precision regime. To address the optimization issue in low dimensions, we develop training tricks which enable us to train neural networks to extremely low loss, close to the limits allowed by numerical precision.",
    "authors": [
        {
            "affiliations": [],
            "name": "Friedhelm Schwenker"
        },
        {
            "affiliations": [],
            "name": "Eric J. Michaud"
        },
        {
            "affiliations": [],
            "name": "Ziming Liu"
        }
    ],
    "id": "SP:c0a7067731b98c0db63f2041a0dfc22101b46393",
    "references": [
        {
            "authors": [
                "S. Gupta",
                "A. Agrawal",
                "K. Gopalakrishnan",
                "P. Narayanan"
            ],
            "title": "Deep Learning with Limited Numerical Precision",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "P. Micikevicius",
                "S. Narang",
                "J. Alben",
                "G. Diamos",
                "E. Elsen",
                "D. Garcia",
                "B. Ginsburg",
                "M. Houston",
                "O. Kuchaiev",
                "G Venkatesh"
            ],
            "title": "Mixed precision training",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "D. Kalamkar",
                "D. Mudigere",
                "N. Mellempudi",
                "D. Das",
                "K. Banerjee",
                "S. Avancha",
                "D.T. Vooturi",
                "N. Jammalamadaka",
                "J. Huang",
                "H Yuen"
            ],
            "title": "A study of BFLOAT16 for deep learning training",
            "venue": "arXiv 2019,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wang",
                "C.Y. Lai",
                "J. G\u00f3mez-Serrano",
                "T. Buckmaster"
            ],
            "title": "Asymptotic self-similar blow up profile for 3-D Euler via physics-informed neural networks. arXiv 2022",
            "year": 2022
        },
        {
            "authors": [
                "V. Jejjala",
                "D.K.M. Pena",
                "C. Mishra"
            ],
            "title": "Neural network approximations for Calabi-Yau metrics",
            "venue": "J. High Energy Phys",
            "year": 2022
        },
        {
            "authors": [
                "J. Martyn",
                "D. Luo",
                "K. Najafi"
            ],
            "title": "Applying the Variational Principle to Quantum Field Theory with Neural-Networks; Bulletin of the American Physical Society; American Physical Society",
            "year": 2023
        },
        {
            "authors": [
                "D. Wu",
                "L. Wang",
                "P. Zhang"
            ],
            "title": "Solving statistical mechanics using variational autoregressive networks",
            "venue": "Phys. Rev. Lett",
            "year": 2019
        },
        {
            "authors": [
                "S.M. Udrescu",
                "M. Tegmark"
            ],
            "title": "AI Feynman: A physics-inspired method for symbolic regression",
            "venue": "Sci. Adv. 2020,",
            "year": 2631
        },
        {
            "authors": [
                "S.M. Udrescu",
                "A. Tan",
                "J. Feng",
                "O. Neto",
                "T. Wu",
                "M. Tegmark"
            ],
            "title": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "venue": "Adv. Neural Inf. Process. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "R.B. Leighton",
                "M. Sands"
            ],
            "title": "The Feynman Lectures on Physics",
            "year": 1965
        },
        {
            "authors": [
                "IEEE St"
            ],
            "title": "754-2019 (Revision of IEEE 754-2008); IEEE Standard for Floating-Point Arithmetic",
            "venue": "IEEE: Piscataway, NJ, USA,",
            "year": 2019
        },
        {
            "authors": [
                "I. G\u00fchring",
                "M. Raslan",
                "G. Kutyniok"
            ],
            "title": "Expressivity of deep neural networks",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "J. Hestness",
                "S. Narang",
                "N. Ardalani",
                "G. Diamos",
                "H. Jun",
                "H. Kianinejad",
                "M. Patwary",
                "M. Ali",
                "Y. Yang",
                "Y. Zhou"
            ],
            "title": "Deep learning scaling is predictable, empirically",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "J. Kaplan",
                "S. McCandlish",
                "T. Henighan",
                "T.B. Brown",
                "B. Chess",
                "R. Child",
                "S. Gray",
                "A. Radford",
                "J. Wu",
                "D. Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "T. Henighan",
                "J. Kaplan",
                "M. Katz",
                "M. Chen",
                "C. Hesse",
                "J. Jackson",
                "H. Jun",
                "T.B. Brown",
                "P. Dhariwal",
                "S Gray"
            ],
            "title": "Scaling laws for autoregressive generative modeling",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "D. Hernandez",
                "J. Kaplan",
                "T. Henighan",
                "S. McCandlish"
            ],
            "title": "Scaling laws for transfer",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "B. Ghorbani",
                "O. Firat",
                "M. Freitag",
                "A. Bapna",
                "M. Krikun",
                "X. Garcia",
                "C. Chelba",
                "C. Cherry"
            ],
            "title": "Scaling laws for neural machine translation",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "M.A. Gordon",
                "K. Duh",
                "J. Kaplan"
            ],
            "title": "Data and parameter scaling laws for neural machine translation",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Punta Cana, Dominican Republic,",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhai",
                "A. Kolesnikov",
                "N. Houlsby",
                "L. Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "J. Hoffmann",
                "S. Borgeaud",
                "A. Mensch",
                "E. Buchatskaya",
                "T. Cai",
                "E. Rutherford",
                "D.d.L. Casas",
                "L.A. Hendricks",
                "J. Welbl",
                "A Clark"
            ],
            "title": "Training Compute-Optimal Large Language Models",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A. Clark",
                "D. de Las Casas",
                "A. Guy",
                "A. Mensch",
                "M. Paganini",
                "J. Hoffmann",
                "B. Damoc",
                "B. Hechtman",
                "T. Cai",
                "S Borgeaud"
            ],
            "title": "Unified scaling laws for routed language models",
            "venue": "In Proceedings of the International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "U. Sharma",
                "J. Kaplan"
            ],
            "title": "A neural scaling law from the dimension of the data manifold",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Bahri",
                "E. Dyer",
                "J. Kaplan",
                "J. Lee",
                "U. Sharma"
            ],
            "title": "Explaining neural scaling laws",
            "venue": "arXiv 2021,",
            "year": 2021
        },
        {
            "authors": [
                "R. Arora",
                "A. Basu",
                "P. Mianjy",
                "A. Mukherjee"
            ],
            "title": "Understanding deep neural networks with rectified linear units",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "K. Eckle",
                "J. Schmidt-Hieber"
            ],
            "title": "A comparison of deep networks with ReLU activation function and linear spline-type methods",
            "venue": "Neural Netw",
            "year": 2019
        },
        {
            "authors": [
                "P. Virtanen",
                "R. Gommers",
                "T.E. Oliphant",
                "M. Haberland",
                "T. Reddy",
                "D. Cournapeau",
                "E. Burovski",
                "P. Peterson",
                "W. Weckesser",
                "J Bright"
            ],
            "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python",
            "venue": "Nat. Methods",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv 2014,",
            "year": 2014
        },
        {
            "authors": [
                "Y.A. LeCun",
                "L. Bottou",
                "G.B. Orr",
                "K.R. M\u00fcller"
            ],
            "title": "Efficient backprop. In Neural Networks: Tricks of the Trade",
            "year": 2012
        },
        {
            "authors": [
                "T. Poggio"
            ],
            "title": "Compositional Sparsity: A Framework for ML; Technical Report; Center for Brains, Minds and Machines (CBMM)",
            "year": 2022
        },
        {
            "authors": [
                "W. Dahmen"
            ],
            "title": "Compositional Sparsity, Approximation Classes, and Parametric Transport Equations",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "T. Poggio",
                "H. Mhaskar",
                "L. Rosasco",
                "B. Miranda",
                "Q. Liao"
            ],
            "title": "Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review",
            "venue": "Int. J. Autom. Comput",
            "year": 2017
        },
        {
            "authors": [
                "M. Kohler",
                "S. Langer"
            ],
            "title": "On the rate of convergence of fully connected deep neural network regression estimates",
            "venue": "Ann. Stat",
            "year": 2021
        },
        {
            "authors": [
                "B. Bauer",
                "M. Kohler"
            ],
            "title": "On deep learning as a remedy for the curse of dimensionality in nonparametric regression",
            "venue": "Ann. Stat",
            "year": 2019
        },
        {
            "authors": [
                "F. Lekien",
                "J. Marsden"
            ],
            "title": "Tricubic interpolation in three dimensions",
            "venue": "Int. J. Numer. Methods Eng",
            "year": 2005
        },
        {
            "authors": [
                "H.W. Lin",
                "M. Tegmark",
                "D. Rolnick"
            ],
            "title": "Why does deep and cheap learning work so well",
            "venue": "J. Stat. Phys",
            "year": 2017
        },
        {
            "authors": [
                "L. Sagun",
                "L. Bottou",
                "Y. LeCun"
            ],
            "title": "Eigenvalues of the hessian in deep learning: Singularity and beyond",
            "venue": "arXiv 2016,",
            "year": 2023
        },
        {
            "authors": [
                "L. Sagun",
                "U. Evci",
                "V.U. Guney",
                "Y. Dauphin",
                "L. Bottou"
            ],
            "title": "Empirical analysis of the hessian of over-parametrized neural networks",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "G. Gur-Ari",
                "D.A. Roberts",
                "E. Dyer"
            ],
            "title": "Gradient descent happens in a tiny subspace",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data",
            "year": 2016
        },
        {
            "authors": [
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Training methods for adaptive boosting of neural networks. Adv",
            "venue": "Neural Inf. Process. Syst. 1997,",
            "year": 1997
        },
        {
            "authors": [
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Boosting neural networks",
            "venue": "Neural Comput",
            "year": 2000
        },
        {
            "authors": [
                "S. Badirli",
                "X. Liu",
                "Z. Xing",
                "A. Bhowmik",
                "K. Doan",
                "S.S. Keerthi"
            ],
            "title": "Gradient boosting neural networks: Grownet",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Citation: Michaud, E.J.; Liu, Z.;\nTegmark, M. Precision Machine\nLearning. Entropy 2023, 25, 175.\nhttps://doi.org/10.3390/e25010175\nAcademic Editor:\nFriedhelm Schwenker\nReceived: 28 November 2022\nRevised: 5 January 2023\nAccepted: 12 January 2023\nPublished: 15 January 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: machine learning; ML for science; scaling laws; optimization"
        },
        {
            "heading": "1. Introduction",
            "text": "Most machine learning practitioners do not need to fit their data with much precision. When applying machine learning to traditional tasks in artificial intelligence such as those in computer vision or natural language processing, one typically does not desire to bring training loss all the way down to exactly zero, in part because training loss is just a proxy for some other performance measure like accuracy that one actually cares about, or because there is intrinsic uncertainty which makes perfect prediction impossible, e.g., for language modeling. Accordingly, to save memory and speed up computation, much work has gone into reducing the numerical precision used in models without sacrificing model performance much [1\u20133]. However, modern machine learning methods, and deep neural networks in particular, are now increasingly being applied to science problems, for which being able to fit models very precisely to (high-quality) data can be important [4\u20137]. Small absolute changes in loss can make a big difference, e.g., for the symbolic regression task of identifying an exact formula from data [8,9]. It is therefore timely to consider what, if any, unique considerations arise when attempting to fit ML models very precisely to data, a regime we call Precision Machine Learning (PML). How does pursuit of precision affect choice of method? How does optimization change in the high-precision regime? Do otherwise-obscure properties of model expressivity or optimization come into focus when one cares a great deal about precision? In this paper, we explore these basic questions."
        },
        {
            "heading": "1.1. Problem Setting",
            "text": "We study regression in the setting of supervised learning, in particular the task of fitting functions f : Rd \u2192 R to a dataset of D = {(~xi, yi = f (~xi)} |D| i=1. In this work, we mostly restrict our focus to functions f which are given by symbolic formulas. Such functions are appropriate for our purpose, of studying precision machine learning for science applications, since they (1) are ubiquitous in science, fundamental to many fields\u2019 descriptions of nature,\nEntropy 2023, 25, 175. https://doi.org/10.3390/e25010175 https://www.mdpi.com/journal/entropy\n(2) are precise, not introducing any intrinsic noise in the data, making extreme precision possible, and (3) often have interesting structure such as modularity that sufficiently clever ML methods should be able to discover and exploit. We use a dataset of symbolic formulas from [8], collected from the Feynman Lectures on Physics [10]. Just how closely can we expect to fit models to data? When comparing a model prediction f\u03b8(~xi) to a data point yi, the smallest nonzero difference allowed is determined by the numerical precision used. IEEE 754 64-bit floats [11] have 52 mantissa bits, so if yi and f\u03b8(~xi) are of order unity, then the smallest nonzero difference between them is e0 = 2\u221252 \u223c 10\u221216. We should not expect to achieve relative RMSE loss below 10\u221216, where relative RMSE loss, on a dataset D, is:\n`rms \u2261 ( \u2211 |D| i=1 | f\u03b8(~xi)\u2212 yi| 2\n\u2211 |D| i=1 y 2 i\n) 1 2\n= | f\u03b8(~xi)\u2212 yi|rms\nyrms . (1)\nIn practice, precision can be bottlenecked earlier by the computations performed within the model f\u03b8 . The task of precision machine learning is to try to push the loss down many orders of magnitude, driving `rms as close as possible to the numerical noise floor e0."
        },
        {
            "heading": "1.2. Decomposition of Loss",
            "text": "One can similarly define relative MSE loss `mse \u2261 `2rms, as well as non-relative (standard) MSE loss Lmse( f ) = 1|D| \u2211 D i=1( f\u03b8(~xi)\u2212 yi)2, and Lrms = \u221a Lmse. Minimizing `rms, `mse, Lrms, Lmse are equivalent up to numerical errors. Note that (relative) expected loss can be defined on a probability distribution P(Rd ,R), like so:\n`Prms =\n( E(~x,y)\u223cP[( f\u03b8(~x)\u2212 y)2]\nE(~x,y)\u223cP[y2]\n) 1 2\n. (2)\nWhen we wish to emphasize the distinction between loss on a dataset D (empirical loss) and a distribution P (expected loss), we write `D and `P. In the spirit of [12], we find it useful to decompose sources of error into different sources, which we term optimization error, sampling luck, the generalization gap, and architecture error. A given model architecture parametrizes a set of expressible functionsH. One can define three functions of interest withinH:\nf bestP \u2261 argmin f\u2208H {`P( f )}, (3)\nthe best model on the expected loss `P,\nf bestD \u2261 argmin f\u2208H {`D( f )}, (4)\nthe best model on the empirical loss `D, and\nf usedD = A(H, D, L), (5)\nthe model found by a given learning algorithm A which performs possibly imperfect optimization to minimize empirical loss L on D.\nWe can therefore decompose the empirical loss as follows:\n`D( f usedD ) = [` D( f usedD )\u2212 `D( f bestD )]\ufe38 \ufe37\ufe37 \ufe38\noptimization error\n+ [`D( f bestD )\u2212 `P( f bestD )]\ufe38 \ufe37\ufe37 \ufe38 sampling luck +\n[`P( f bestD )\u2212 `P( f bestP )]\ufe38 \ufe37\ufe37 \ufe38 generalization gap + `P( f bestP )\ufe38 \ufe37\ufe37 \ufe38 architecture error , (6)\nwhere all terms are positive except possibly the sampling luck, which is zero on average, has a standard deviation shrinking with data size |D| according to the Poisson scaling |D|\u22121/2, and will be ignored in the present paper. The generalization gap has been extensively studied in prior work, so this paper will focus exclusively on the optimization error and the architecture error. To summarize: the architecture error is the best possible performance that a given architecture can achieve on the task, the generalization gap is the difference between the optimal performance on the training set D and the architecture error, and the optimization error is the error introduced by imperfect optimization\u2014the difference between the error on the training set found by imperfect optimization and the optimal error on the training set. When comparing methods and studying their scaling, it useful to ask which of these error sources dominate. We will see that both architecture error and optimization error can be quite important in the high-precision regime, as we will elaborate on in Section 2, Section 3 and Section 4, respectively."
        },
        {
            "heading": "1.3. Importance of Scaling Exponents",
            "text": "In this work, one property that we focus on is how methods scale as we increase parameters or training data. This builds on a recent body of work on scaling laws in deep learning [13\u201321] which has found that, on many tasks, loss decreases predictably as a power-law in the number of model parameters and amount of training data. Attempting to understand this scaling behavior, [22,23] argue that in some regimes, cross-entropy and MSE loss should scale as N\u2212\u03b1, where \u03b1 & 4/d, N is the number of model parameters, and d is the intrinsic dimensionality of the data manifold of the task. Consider the problem of approximating some analytic function f : [0, 1]d \u2192 R with some function which is a piecewise n-degree polynomial. If one partitions a hypercube in Rd into regions of length e and approximates f as a n-degree polynomial in each region (requiring N = O(1/ed) parameters), absolute error in each region will be O(en+1) (given by the degree-(n + 1) term in the Taylor expansion of f ) and so absolute error scales as N\u2212 n+1 d . If neural networks use ReLU activations, they are piecewise linear (n = 1) and so we may expect `rmse(N) \u221d N\u2212 2 d . Sharma & Kaplan [22] argue that if neural networks map input data onto an intermediate representation, where representations lie on a manifold of intrinsic dimension d\u2217 < d, and then perform a piecewise linear fit to a function on this manifold, then RMSE error should scale as N\u22122/d \u2217 . If one desires very low loss, then the exponent \u03b1, the rate at which methods approach their best possible performance (The best possible performance can be determined either by precision limits or by noise intrinsic to the problem, such as intrinsic entropy of natural language) matters a great deal. Sharma & Kaplan [22] note that 4/d (2/d for RMSE loss) is merely a lower-bound on the scaling rate\u2014we consider ways that neural networks can improve on this bound. Understanding model scaling is key to understanding the feasibility of achieving high precision."
        },
        {
            "heading": "1.4. Organization",
            "text": "This paper is organized as follows: In Section 2 we discuss piecewise linear approximation methods, comparing ReLU networks with linear simplex interpolation. We discover that neural networks can sometimes substantially outperform linear simplex interpolation, and hypothesize that NNs do this by exploiting the modularity of the problem, which we call the modularity hypothesis. In Section 3 we discuss nonlinear methods, including neural networks with nonlinear activation functions, and find that optimization error, rather than approximation error, can be a major bottleneck to achieving high-precision fits. In Section 4 we discuss the optimization challenge of high-precision neural network training \u2013 how optimization difficulties can often make total error far worse than the limits of what architecture error allows. We attempt to develop optimization methods for overcoming these problems and describe their limitations, then conclude in Section 5."
        },
        {
            "heading": "2. Piecewise Linear Methods",
            "text": "We first consider approximation methods which provide a piecewise linear fit to data. We focus on two such methods: linear simplex interpolation and neural networks with ReLU activations. To review, linear simplex interpolation works as follows: given our dataset of |D| input-output pairs {(~xi, yi)} |D| i=1, linear simplex interpolation first computes a Delaunay triangulation from ~x1, . . . ,~x|D| in the input space Rd, partitioning the space into a collection of d-simplices, each with d + 1 vertices, whose union is the convex hull of the input points. Since d + 1 points determine a linear (affine) function Rd \u2192 R, the function f can be approximated within each d-simplex as the unique linear function given by the value of the function f at the vertices. This gives a piecewise linear function on the convex hull of the training points. Linear simplex interpolation needs to store N = |D|(d + 1) parameters: |D|d values for the vertices ~xi, and |D| values for the corresponding function values yi. Neural networks with ReLU activations also give a piecewise linear fit f\u03b8 . We consider only fully-connected feedforward networks, a.k.a. multilayer perceptrons (MLPs). Such networks consist of a sequence of alternating affine transformations T : ~x 7\u2192W~x + b and element-wise nonlinearities \u03c3(~x)i = \u03c3(~xi) for an activation function \u03c3 : R\u2192 R:\nf\u03b8 = Tk+1 \u25e6 \u03c3 \u25e6 Tk \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 T2 \u25e6 \u03c3 \u25e6 T1.\nFollowing [24], we define the depth of the network as the number of affine transformations in the network, which is one greater than the number of hidden layers k. As shown in [24], any piecewise linear function on Rd can be represented by a sufficiently wide ReLU NN with at most dlog2(d + 1)e+ 1 depth. Therefore, sufficiently wide and deep networks are able to exactly express functions given by linear simplex interpolation. A natural question then is: given the same amount of data and parameters, how do the two methods compare? We discover that simplex interpolation performs better on 1D and 2D problems, but that neural networks can outperform simplex interpolation on higher-dimensional problems. So although simplex interpolation and ReLU NNs both parametrize the same function class (piecewise linear functions), their performance can differ significantly in practice (Ref. [25] analyzed the expressivity of ReLU NNs in comparison with multivariate adaptive regression splines. Our work conducts a more extensive set of experiments to study how approximators scale in practice. We also restrict our focus to fitting physics functions, which have modular structure). In our experiments, we use the implementation of simplex interpolation from SciPy [26]. When training neural networks, we use the Adam optimizer [27] with a learning rate of 10\u22123, and train for 20k steps. We use a batch size of min(|D|, 104). While we report loss using RMSE, we train using MSE loss. Training points are sampled uniformly from intervals specified by the AI Feynman dataset [8] (typically [1, 5] \u2282 R for each input), but when training neural networks, we normalize the input points [28] so that they have zero mean and unit variance along each dimension. We estimate test loss on datasets of 30k samples (Project code can be found at https://github.com/ejmichaud/precision-ml, accessed on 11 January 2023). In Section 1, we show for 1D and 2D problems the linear regions given both by simplex interpolation and by neural networks trained with a comparable number of parameters . For 2D problems, Figure 1 illustrates the importance of normalizing input data for ReLU networks. We see that there is a far higher density of linear regions around the data when input data is normalized, which leads to better performance. Neural networks, with the same number of parameters and trained with the same low-dimensional data, often have fewer linear regions than simplex interpolation.\nIn Figure 2, we show how the precision of linear simplex interpolation and neural networks scale empirically. Since simplex interpolation is a piecewise linear method, from the discussion in Section 1.3, we expect its RMSE error to scale as N\u22122/d, and find that this indeed holds (Scaling as N\u22122/d only holds when the model is evaluated on points not too close to the boundary of the training set. At the boundary, simplices are sometimes quite large, leading to a poor approximation of the target function close to the boundary, large errors, and worse scaling. In our experiments, we therefore compute test error only for points at least 10% (of the width of the training set in each dimension) from the boundary of the training set). To provide a fair comparison with simplex interpolation when evaluating neural networks on a dataset of size D, we give it the same number of parameters N = |D|(d + 1). From Figure 2, we see that simplex interpolation outperforms neural networks on low dimensional problems but that neural networks do better on higher-dimensional problems. For the 1D example in Figure 2 (top left), we know that the amount by which the neural networks under-perform simplex interpolation is entirely due to optimization error. This is because any 1D piecewise linear function f (x) with m corners at x1, . . . , xm can trivially be written as a linear combination of m functions ReLU(x\u2212 xi). We hypothesize that the reason why neural networks are able to beat simplex interpolation when fitting functions given by symbolic equations is that neural networks learn to exploit the inherent modularity of these problems. We term this the modularity hypothesis. This property of the regression functions we study, which we call modularity, has been referred to by other authors as \u201ccompositional sparsity\u201d [29,30] or the property of being \u201chierarchically local compositional\u201d [31]. Such functions consist of many low-dimensional functions composed with each other. For instance, given three two-dimensional functions f1, f2, f3, one can construct a modular four-dimensional function g(x1, x2, x3, x4) = f3( f1(x1, x2), f2(x3, x4)).\nMany natural functions have this structure, notably those we are interested in given by symbolic equations, which compose together binary functions (+,\u2212,\u00d7,\u00f7) and unary functions (ex, ln(x), etc.). We are particularly interested in modular functions where the computation graph describing the function has low maximum arity\u2014that the constituent functions composing it individually have low input dimension. For symbolic equations, the maximum arity is two.\nOn modular problems, it is a theoretical possibility [32,33] that sufficiently deep neural networks could scale as if the problem dimension was the maximum arity of the problem computation graph, rather than the input dimension. This is possible if networks learn a modular solution to the problem, approximating each node in the computation graph and internally composing together these elements according to the computation graph. The error of ReLU NNs should therefore be able to scale as N\u22122/d \u2217 where d\u2217 is the maximum arity of the computation graph of the target function (d\u2217 = 2 for symbolic equations). Scaling in this manner requires excess capacity to be allocated towards improving the approximation of all nodes in the graph, while continuing to compose them properly, rather than approximating a single function on a fixed data manifold. We observe two tentative pieces of evidence for the modularity hypothesis as the reason why NNs beat linear simplex interpolation. First, we see from Figure 2, at least early in the scaling curves, that neural networks usually appear to scale not as N\u22122/d, but rather as N\u22122/d \u2217 where d\u2217 = 2, the maximum arity of the symbolic regression problems we train on. As an additional test, we train networks where we hard-code the modularity of the problem into the architecture, as depicted in Figure 3. Figure 3 indeed reveals how models for which we enforce the modularity of the problem perform and scale similarly to same-depth dense neural networks without modularity enforced. A modular architecture can be created from a dense one by forcing weight matrices to be block-diagonal (where we do not count off-diagonal entries towards the number of model parameters), but in practice we create\nmodular architectures by creating a separate MLP for each node in the symbolic expression computation graph and connecting them together in accordance with the computation graph. See the diagrams in Figure 3 for an illustration of the modular architecture. In Figure 3, we plot modular and dense network performance against number of model parameters, but we also find that holding width constant, rather than number of parameters, modular networks still slightly outperform their dense counterparts. For instance, depth-6 width-100 modular networks outperform dense networks of the same width and depth, despite dense networks having \u22482.5\u00d7 fewer parameters. Such \u201cless is more\u201d results are to be expected if the optimal architecture is in fact modular, in which case a fully connected architecture wastes resources training large numbers of parameters that should be zero.\nOur observation that neural networks sometimes appear to scale as if the problem dimension was the maximum arity of the computational graph generalizes and improves upon neural scaling results from Sharma & Kaplan [22], which argued that the effective problem dimension d\u2217 is the intrinsic dimension of a fixed data manifold. In the view from [22], excess capacity is allocated towards approximating a function on this fixed manifold with greater resolution, rather than allocating capacity towards many internal modules and composing them in an appropriate manner as we have discussed. It is widely believed in the ML community that the dimensionality of the data manifold or the power-law statistics of the data covariance matrix spectra sets the neural scaling exponent (see for example the excellent treatment by Sharma & Kaplan [22] and Bahri et al. [23]), however, our results show that scaling exponents can be better under the modularity hypothesis, where networks exploit modularity to scale in the maximum arity of a problem\u2019s computation graph."
        },
        {
            "heading": "3. Nonlinear Methods",
            "text": "We now turn our attention to approximation methods that are thoroughly nonlinear (as opposed to piecewise linear). As discussed in the introduction, methods approximating the target function f by a piecewise polynomial have a scaling exponent \u03b1 = n+1d where n is the degree of the polynomial. In Figure 4, we plot the performance of approximation methods which are piecewise polynomial, for 1D, 2D and 3D problems. For 1D and 2D problems, we use splines of varying order. For 3D problems, we use the cubic spline interpolation method of [34]. We see empirically that these methods have scaling exponent \u03b1 = (n + 1)/d. If the order of the spline interpolator is high enough, and the dimension low enough, we see that relative RMSE loss levels out at e0 \u2248 10\u221216 at the precision limit. Unfortunately, a basic limitation of these methods is that they are limited to low-dimensional problems.\nThere are relatively few options for high-dimensional nonlinear interpolation (One method, which we have not tested, is radial basis function interpolation). A particularly interesting parametrization of nonlinear functions is given by neural networks with nonlinear activation functions (and not piecewise linear like ReLUs). In Figure 5, we show how neural networks with tanh activations scale in increasing width. We observe that on some problems, they do better than the ideal scaling achievable with linear methods (shown as a green dashed line). However, in our experiments, they can sometimes scale worse, perhaps the result of imperfect optimization. Also, we find that scaling is typically not nearly as clean as a power law as it was for ReLU networks. For some problems, one can show theoretically that architecture error can be made arbitrarily low, and that the loss is due entirely to optimization error and the generalization gap. As shown in [35], a two-layer neural network with only four hidden units can perform multiplication between two real numbers, provided that a twice-differentiable activation function is used. See Figure 6b for a diagram of such a network, taken from [35]. Note that this network becomes more accurate in the limit that some of its parameters become very small and others become very large. This result, that small neural networks can express multiplication arbitrarily well, implies that neural network architecture error is effectively zero for some problems. However, actually learning this multiplication circuit in practice is challenging since it involves some network parameters diverging\u2192 \u221e while others\u2192 0 in a precise ratio. This means that for some tasks, neural network performance is mainly limited not by architecture error, but by optimization error. Indeed, on some problems, a failure to achieve high precision can be blamed entirely on the optimization error. In Figure 6a, we show neural network scaling on the equation f (x1, x2, x3, y1, y2, y3) = x1y1 + x2y2 + x3y3. For this problem, a 2-layer network with 12 hidden units (implementing three multiplications in parallel, with their results added in the last layer) can achieve \u22480 architecture error. Yet we see a failure to get anywhere near that architecture error or the noise floor set by machine precision. Instead, as one scales up the network size on this task, we see that despite architecture error abruptly dropping to zero early on, the actually attained loss continues to scales down smoothly.\nIt is therefore important to analyze the problem of optimization for high precision, which we do in the next section."
        },
        {
            "heading": "4. Optimization",
            "text": "As seen above, when deep neural networks are trained with standard optimizers, they can produce significant optimization error, i.e., fail to find the best approximation. In this section, we discuss the difficulty of optimization in the high-precision regime and explore a few tricks for improving neural network training."
        },
        {
            "heading": "4.1. Properties of Loss Landscape",
            "text": "To understand the difficulty of optimizing in the high-precision regime, we attempt to understand the local geometry of the loss landscape at low loss. In particular, we compute the Hessian of the loss and study its eigenvalues. In Figure 7, we plot the spectrum of the\nHessian, along with the magnitude of the gradient projected onto each of the corresponding eigenvectors, at a point in the loss landscape found by training with the Adam optimizer for 30k steps in a teacher-student setup. The teacher is a depth-3, width-3 tanh MLP and the student is a depth-3, width-40 tanh MLP. In line with [36\u201338], we find that at low loss, the loss landscape has a top cluster of directions of high curvature (relatively large positive eigenvalues) and a bulk of directions of very low curvature. Furthermore, the gradient tends to point most strongly in directions of higher curvature, and has very little projection onto directions of low curvature magnitude.\nThe basic picture emerging from this analysis is that of a canyon, i.e., a very narrow, very long valley around a low-loss minimum. The valley has steep walls in high-curvature directions and a long basin in low-curvature directions. Further reducing loss in this environment requires either (1) taking very precisely-sized steps along high-curvature directions to find the exact middle of the canyon or (2) moving along the canyon in lowcurvature directions instead, almost orthogonally to the gradient. In this landscape, typical first-order optimizers used in deep learning may struggle to do either of these things, except perhaps if learning rates are chosen extremely carefully."
        },
        {
            "heading": "4.2. Optimization Tricks for Reducing Optimization Error",
            "text": "How can we successfully optimize in such a poorly-conditioned, low-loss regime? We first find that switching from first-order optimizers like Adam to second-order optimizers like BFGS [39] can improve RMSE loss by multiple orders of magnitude. Second-order methods often both (1) employ line searches, and (2) search in directions not strongly aligned with the gradient, allowing optimization to progress within low-curvature subspaces. However, methods like BFGS are eventually bottlenecked by numerical precision limitations. To further lower loss, we tested the following two methods:\n4.2.1. Low-Curvature Subspace Optimization\nWe find that by restricting our optimization to low-curvature subspaces, we can further decrease loss past the point where loss of precision prevented BFGS from taking further steps. Our method has a single hyperparameter \u03c4. The method is as follows: let g = \u2207\u03b8L be the gradient and H be the Hessian of the loss. Denote an eigenvector-eigenvalue pair of H by (ei, \u03bbi). Instead of stepping in the direction\u2212g, we instead compute g\u0302 = \u2211i:\u03bbi<\u03c4 ei(ei \u00b7 g). Essentially, we just project g onto the subspace spanned by eigenvalues of ei such that \u03bbi < \u03c4. We then perform a line search to minimize loss along the direction \u2212g\u0302, and repeat the process. Note that this requires computing eigenvectors and eigenvalues for the whole Hessian H. See Appendix B for training curves with low-curvature subspace training with varying \u03c4.\n4.2.2. Boosting: Staged Training of Neural Networks\nWe also investigated techniques related to the common practice of \u201cboosting\u201d in ML [40\u201343]. We found the following method to work quite well in our setting. Instead of training a full network to fit the target f , one can train two networks f (1)\u03b81 , f (2) \u03b82 sequentially:\nfirst train f (1)\u03b81 to fit f , then train f (2) \u03b82\nto fit the residual f\u2212 f (1)\u03b81\nc , where c 1 normalizes the residual to be of order unity. One can then combine the two networks into a single model f (x) \u2248 f (1)\u03b81 (x) + c f (2) \u03b82 (x). If networks f (1)\u03b81 , f (2) \u03b82\nhave widths w1, w2 respectively, then they can be combined into one network of width w1, w2, with block-diagonal weight matrices, and where the parameters of the last layer of f (2)\u03b82 are scaled down by c. While further boosting steps can be performed, we found that there are quickly diminishing returns to training on successive residuals beyond that of the first network. We find that, for low-dimensional problems, we can achieve substantially lower loss with these techniques. We use the following setup: we train width-40 depth-3 tanh MLPs to fit single-variable polynomials with the BFGS optimizer on MSE loss. The SciPy [26] BFGS implementation achieves 10\u22127 RMSE loss before precision loss prevents further iterations. Subsequently using low-curvature subspace training with a threshold \u03c4 = 10\u221216 can further lower RMSE loss a factor of over 2\u00d7. On similar low-dimensional problems, as shown in Figure 8, applying boosting, training a second network with BFGS on the residual of the first can lower RMSE loss further by five to six orders of magnitude. In Figure 8, we compare training runs with these tricks to runs with the Adam optimizer for a variety of learning rates. For our Adam training runs, we use width-40 tanh MLPs. When training with boosting, we train a width-20 network for f (1)\u03b81 and a width-20 network for f (2) \u03b82\n, for a combined width of 40. We also plot a width-40 network trained solely with BFGS for comparison. We find, unsurprisingly, that BFGS significantly outperforms Adam. With our tricks, particularly boosting, we can sometimes outperform even well-tuned Adam by eight orders of magnitude, driving RMSE loss down to \u224810\u221214, close to the machine precision limit. See Appendix A for a brief discussion of the origin of the benefit of boosting."
        },
        {
            "heading": "4.3. Limitations and Outlook",
            "text": "The techniques we described above are not a silver bullet for fitting neural networks to any data with high precision. Firstly, second-order optimizers like BFGS scale poorly with the number of model parameters N (since the Hessian is an N \u00d7 N matrix), limiting their applicability to small models. Also, we find that the gains from boosting diminish quickly as the input dimension of the problem grows. In Figure 9, we see that on the six-dimensional problem discussed earlier (Figure 6a), BFGS + boosting achieves only about a two-order-of-magnitude improvement, bringing the RMSE loss from 10\u22122 to 10\u22124.\nWhile boosting does not help much for high-dimensional problems, its success on low-dimensional problems is still noteworthy. By training two parts of a neural network separately and sequentially, we were able to dramatically improve performance. Perhaps there are other methods, not yet explored, for training and assembling neural networks in nonstandard ways to achieve dramatically better precision. The solutions found with boosting, where some network weights are at a much smaller scale than others, are not likely to be found with typical training. An interesting avenue for future work would be exploring new initialization schemes, or other ways of training networks sequentially, to discover better solutions in underexplored regions of parameter space."
        },
        {
            "heading": "5. Conclusions",
            "text": "We have studied the problem of fitting scientific data with a variety of approximation methods, analyzing sources of error and their scaling.\n\u2022 Linear Simplex Interpolation provides a piecewise linear fit to data, with RMSE loss scaling reliably as D\u22122/d. Linear simplex interpolation always fits the training points exactly, and so error comes from the generalization gap and the architecture error. \u2022 ReLU Neural Networks also provide a piecewise linear fit to data. Their performance (RMSE loss) often scales as D\u22122/d \u2217 , where d\u2217 is the maximum arity of the task (typically\nd\u2217 = 2). Accordingly, they can scale better than linear simplex interpolation when d > 2. Unfortunately, they are often afflicted by optimization error making them scale worse than linear simplex interpolation on 1D and 2D problems, and even in higher dimensions in the large-network limit.\n\u2022 Nonlinear Splines approximate a target function piecewise by polynomials. They scale as D\u2212(n+1)/d where n is the order of the polynomial. \u2022 Neural Networks with smooth activations provide a nonlinear fit to data. Quite small networks with twice-differentiable nonlinearities can perform multiplication arbitrarily well [35], and so for many of the tasks we study (given by symbolic formulas), their architecture error is zero. We find that their inaccuracy does not appear to scale cleanly as power-laws. Optimization error is unfortunately a key driver of the error of these methods, but with special training tricks, we found that we could reduce RMSE loss on 1D problems down within 2\u20134 orders of magnitude of the 64-bit machine precision limit e0 \u223c 10\u221216. For those seeking high-precision fits, these results suggest the following heuristics, summarized in Figure 10 as a \u201cUser\u2019s Guide to Precision\u201d: If data dimensionality d is low(d \u2264 2), polynomial spline interpolation can provide a fit at machine precision if you (1) have enough data and (2) choose a high enough polynomial order. Neural networks with smooth activations may in some cases also approach machine precision, possibly with less data, if they are trained with second-order optimizers like BFGS and boosted. For higher-dimensional problems (d \u2265 3), neural networks are typically the most promising choice, since they can learn compositional modular structure that allows them to scale as if the data dimensionality were lower.\nIn summary, our results highlight both advantages and disadvantages of using neural networks to fit scientific data. We hope that they will help provide useful building blocks for further work towards precision machine learning.\nAuthor Contributions: E.J.M. performed the experiments and made the figures. E.J.M., Z.L. and M.T. wrote the manuscript. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by The Casey Family Foundation, the Foundational Questions Institute, the Rothberg Family Fund for Cognitive Science, the NSF Graduate Research Fellowship (Grant No. 2141064), and IAIFI through NSF grant PHY-2019786.\nData Availability Statement: The data presented in this study can be reproduced with code openly available at https://github.com/ejmichaud/precision-ml (accessed on 4 January 2023).\nConflicts of Interest: The authors declare no conflict of interest."
        },
        {
            "heading": "Appendix A. Boosting Advantage Comes Not Just from Rescaling Loss",
            "text": "When one trains a second network on the residual of a first trained network during boosting, the scale of the loss, at least early in training, lies around order unity, much higher than the loss at the end of training the first network. Does the advantage of boosting then lie in increasing the value of the loss to avoid precision limits within BFGS? To investigate this, instead of performing boosting, we instead simply rescale the loss after training width-40\nnetworks with BFGS and further train the same network with BFGS. In Figure A1, we plot training curves for varying amounts of rescaling of the loss function. We find that the benefits of rescaling the loss are small compared with boosting. At best, we achieved an \u22484\u00d7 reduction in RMSE loss, compared with the 10,000\u2013100,000\u00d7 reduction from boosting.\nFigure A1. Rescaling loss has minimal benefit relative to boosting."
        },
        {
            "heading": "Appendix B. A Closer Look at Low-Curvature Subspace Optimization",
            "text": "Low-curvature subspace optimization tends to reduce loss much less than boosting; it is difficult to even see improvement from low-curvature subspace training in Figures 8 and 9. In Figure A2, we show training curves for low-curvature subspace optimization for a width-20 depth-3 tanh network after being trained with BFGS. We see that for appropriately chosen curvature thresholds \u03c4, we can reduce RMSE loss by 2\u20133\u00d7, although the method is admittedly expensive, requiring a full computation of the Hessian.\n0 50 100 150 200 steps (line search each step)\n3 \u00d7 10 7\n4 \u00d7 10 7\n6 \u00d7 10 7\nRM SE\nlo ss\nFitting y = x2, further reducing loss (after BFGS) with low-curvature subspace training\ncurv threshold 1.0e-06 1.0e-08 1.0e-10 1.0e-12 1.0e-14 1.0e-16 1.0e-18\nFigure A2. Low-curvature subspace training curves for varying thresholds \u03c4."
        },
        {
            "heading": "Appendix C. Neural Scaling of Tanh Networks with BFGS versus Adam Optimizer",
            "text": "In Section 3, we found that tanh networks sometimes scaled poorly, and suggested that this was due to challenges with optimization rather than the expressivity of the architecture. In Figure A3, we compare neural scaling curves when training with Adam versus the BFGS optimizer for the same problems as Figure 5.\n102 103 104 105\n10 4\n10 2\nRM SE\nTe st\nL os\ns\n= arcsin (dn ): d = 3\n102 103 104 105\n10 2\n10 1\n100\ns = (x2 x1)2 + (y2 y1)2 : d = 4\n102 103 104 105 Model Parameters N = |D|(d + 1)\n100\n102\nRM SE\nTe st\nL os\ns\nEn = Tkbnlog (V2V1 ): d = 5 Simplex Interpolation L N 2/d\nAdam tanh depth 2 Adam tanh depth 3 Adam tanh depth 4\nBFGS tanh depth 2 BFGS tanh depth 3 BFGS tanh depth 4\n102 103 104 105 Model Parameters N = |D|(d + 1)\n10 2\n100\nE = 3pdz x2 + y24 r5 : d = 6\nFigure A3. Scaling of tanh networks with the BFGS vs Adam optimizer. We use the same setup as in Figure 5, training tanh MLPs of depth 2\u20134 of varying width on functions given by symbolic equations. BFGS outperforms Adam on the 3-dimensional example shown (top left) and performs roughly similarly to Adam on the other problems."
        },
        {
            "heading": "Appendix D. Loss Landscape at Lower Loss",
            "text": "In Figure 7, we plotted the Hessian eigenvalues at the end of training with the Adam optimizer. In Figure A4, we do the same but for a point in the loss landscape found by the BFGS optimizer rather than Adam. The basic picture of a \u201ccanyon\u201d discussed in Section 4 is clearer at lower-loss points in the landscape found by BFGS\u2014there is a clear set of high-curvature directions which the gradient mostly points along.\n0 100 200 300 400 500 Most Positive Eigenvalue (ordered by value) Most Negative\n10 21\n10 16\n10 11\n10 6\n10 1\nEi ge\nnv al\nue M\nag ni\ntu de\n= 0\nHessian Eigenvalues and Gradient Direction\n10 17\n10 15\n10 13\n10 11\n10 9\nGr ad\nie nt\nP ro\nje ct\nio n\nM ag\nni tu\nde\nFigure A4. Eigenvalues (dark green) of the loss landscape Hessian (MSE loss) after training a width20, depth-3 network to fit y = x2 with the BFGS optimizer. Like in Figure 7, we also plot the magnitude of the gradient\u2019s projection onto each corresponding eigenvector (thin red line). The \u201ccanyon\u201d shape of the loss landscape is more obvious at lower-loss points in the landscape found by BFGS than Adam finds. There is a clear set of top eigenvalues corresponding to a few directions of much higher curvature than the bulk."
        }
    ],
    "title": "Precision Machine Learning",
    "year": 2023
}