{
    "abstractText": "The phase-transition behavior of the NP-hard vertex-cover (VC) combinatorial optimization problem is studied numerically by linear programming (LP) on ensembles of random graphs. As the basic Simplex (SX) algorithm suitable for such LPs may produce incomplete solutions for sufficiently complex graphs, the application of cutting-plane (CP) methods is sought. We consider Gomory and {0, 1 2 } cuts. We measure the probability of obtaining complete solutions with these approaches as a function of the average node degree c and observe transition between typically complete and incomplete phase regions. While not generally complete solutions are obtained for graphs of arbitrarily high complexity, the CP approaches still advance the boundary in comparison to the pure SX algorithm, beyond the known replica-symmetry breaking (RSB) transition at c = e \u2248 2.718. In fact, our results provide evidence for another algorithmic transition at c \u2248 2.90(2). Besides this, we quantify the transition between easy and hard solvability of the VC problem also in terms of numerical effort. Further we study the so-called whitening of the solution, which is a measure for the degree of freedom that single vertices experience with respect to degenerate solutions. Inspection of the quantities related to clusters of white vertices reveals that whitening is affected, only slightly but measurably, by the RSB transition.",
    "authors": [
        {
            "affiliations": [],
            "name": "G. Claussen"
        },
        {
            "affiliations": [],
            "name": "A. K. Hartmann"
        }
    ],
    "id": "SP:6f544eb7c59186cfa0b13231b77fde8758330b8a",
    "references": [
        {
            "authors": [
                "M. M\u00e9zard",
                "A. Montanari"
            ],
            "title": "Information",
            "venue": "Physics and Computation ",
            "year": 2009
        },
        {
            "authors": [
                "C. Moore",
                "S. Mertens"
            ],
            "title": "The Nature of Computation (Oxford",
            "year": 2011
        },
        {
            "authors": [
                "S. Kirkpatrick",
                "B. Selman"
            ],
            "title": "Science 264",
            "venue": "1297 ",
            "year": 1994
        },
        {
            "authors": [
                "I.P. Gent",
                "T. Walsh"
            ],
            "title": "in Proceedings of 12th European Conference on Artificial Intelligence",
            "venue": "ECAI \u201996, edited by W. Wahlster ",
            "year": 1996
        },
        {
            "authors": [
                "M. Weigt",
                "A.K. Hartmann"
            ],
            "title": "Phys",
            "venue": "Rev. Lett. 84, 6118 ",
            "year": 2000
        },
        {
            "authors": [
                "M. Davis",
                "G. Logemann",
                "D. Loveland"
            ],
            "title": "Commun",
            "venue": "ACM 5, 394 ",
            "year": 1962
        },
        {
            "authors": [
                "R. Sedgewick"
            ],
            "title": "Algorithms in C (Addison-Wesley",
            "venue": "Reading ",
            "year": 1990
        },
        {
            "authors": [
                "C.H. Papadimitriou"
            ],
            "title": "in Foundations of Computer Science",
            "venue": "Annual IEEE Symposium on, Vol. 32, edited by I. C. Society ",
            "year": 1991
        },
        {
            "authors": [
                "J. Ardelius",
                "E. Aurell"
            ],
            "title": "Phys",
            "venue": "Rev. E 74, 037702 ",
            "year": 2006
        },
        {
            "authors": [
                "M. M\u00e9zard",
                "G. Parisi",
                "R. Zecchina"
            ],
            "title": "Science 297",
            "venue": "812 ",
            "year": 2002
        },
        {
            "authors": [
                "M. M\u00e9zard",
                "R. Zecchina"
            ],
            "title": "Phys",
            "venue": "Rev. E 66, 056126 ",
            "year": 2002
        },
        {
            "authors": [
                "M. M\u00e9zard",
                "G. Parisi"
            ],
            "title": "and M",
            "venue": "Virasoro, Spin glass theory and beyond ",
            "year": 1987
        },
        {
            "authors": [
                "C.H. Papadimitriou",
                "K. Steiglitz"
            ],
            "title": "Combinatorial Optimization",
            "venue": "Algorithms and Complexity ",
            "year": 1998
        },
        {
            "authors": [
                "W.J. Cook",
                "W.H. Cunningham",
                "W.R. Pulleyblank"
            ],
            "title": "and A",
            "venue": "Schriever, Combinatorial Optimization ",
            "year": 1998
        },
        {
            "authors": [
                "T. Dewenter",
                "A.K. Hartmann"
            ],
            "title": "Phys",
            "venue": "Rev. E 86, 041128 ",
            "year": 2012
        },
        {
            "authors": [
                "N. Adler",
                "A.S. Hakkert",
                "J. Kornbluth",
                "T. Raviv",
                "M. Sher"
            ],
            "title": "Ann",
            "venue": "Oper. Res. 221, 9 ",
            "year": 2014
        },
        {
            "authors": [
                "M. Safar",
                "M. Taha"
            ],
            "title": "and S",
            "venue": "Habib, in 2007 IEEE/ACS International Conference on Computer Systems and Applications ",
            "year": 2007
        },
        {
            "authors": [
                "E. Boros",
                "V. Gurvich"
            ],
            "title": "Discrete Appl",
            "venue": "Math. ",
            "year": 2007
        },
        {
            "authors": [
                "L.E. Parker",
                "B.A. Emmons"
            ],
            "title": "in Proceedings of International Conference on Robotics and Automation",
            "venue": "Vol. 3 ",
            "year": 1997
        },
        {
            "authors": [
                "M. Weigt",
                "A.K. Hartmann"
            ],
            "title": "Phys",
            "venue": "Rev. Lett. 86, 1658 ",
            "year": 2001
        },
        {
            "authors": [
                "M. Weigt",
                "A.K. Hartmann"
            ],
            "title": "Phys",
            "venue": "Rev. E 63, 056127 ",
            "year": 2001
        },
        {
            "authors": [
                "M. Weigt",
                "A.K. Hartmann"
            ],
            "title": "Europhys",
            "venue": "Lett. 62, 533 ",
            "year": 2003
        },
        {
            "authors": [
                "M. Bauer",
                "O. Golinelli"
            ],
            "title": "Eur",
            "venue": "Phys. J. B 24, 339 ",
            "year": 2001
        },
        {
            "authors": [
                "W. Barthel",
                "A.K. Hartmann"
            ],
            "title": "Phys",
            "venue": "Rev. E 70, 066120 ",
            "year": 2004
        },
        {
            "authors": [
                "P.R. Thie",
                "G.E. Keough"
            ],
            "title": "An Introduction to Linear Programming and Game Theory",
            "year": 2008
        },
        {
            "authors": [
                "R.E. Gomory"
            ],
            "title": "Bull",
            "venue": "Am. Math. Soc. 64, 275 ",
            "year": 1958
        },
        {
            "authors": [
                "R. Parker",
                "R. Rardin"
            ],
            "title": "Discrete Optimization",
            "year": 1988
        },
        {
            "authors": [
                "G. Cornu\u00e9jols"
            ],
            "title": "Ann",
            "venue": "Oper. Res. 149, 63 ",
            "year": 2007
        },
        {
            "authors": [
                "R.E. Gomory"
            ],
            "title": "Ann",
            "venue": "Oper. Res. 149, 99 ",
            "year": 2007
        },
        {
            "authors": [
                "A. Caprara",
                "M. Fischetti"
            ],
            "title": "Math",
            "venue": "Program. 74, 221 ",
            "year": 1996
        },
        {
            "authors": [
                "A. Braunstein",
                "R. Zecchina"
            ],
            "title": "J",
            "venue": "Stat. Mech.: Theory Exp. 2004, P06007 ",
            "year": 2004
        },
        {
            "authors": [
                "S. Seitz",
                "M. Alava",
                "P. Orponen"
            ],
            "title": "J",
            "venue": "Stat. Mech.: Theory Exp. 2005, P06006 ",
            "year": 2005
        },
        {
            "authors": [
                "E. Maneva",
                "E. Mossel",
                "M.J. Wainwright"
            ],
            "title": "J",
            "venue": "ACM 54, 17 ",
            "year": 2007
        },
        {
            "authors": [
                "M. Alava",
                "J. Ardelius",
                "E. Aurell",
                "P. Kaski",
                "S. Krishnamurthy",
                "P. Orponen",
                "S. Seitz"
            ],
            "title": "Proc",
            "venue": "Natl. Acad. Sci. U.S.A. 105, 15253 ",
            "year": 2008
        },
        {
            "authors": [
                "A. Braunstein"
            ],
            "title": "L",
            "venue": "Dall\u2019Asta, G. Semerjian, and L. Zdeborov\u00e1, J. Stat. Mech.: Theory Exp. 2016, 053401 ",
            "year": 2016
        },
        {
            "authors": [
                "Y. Zhang",
                "M. Zhang",
                "G. Pu",
                "F. Song",
                "J. Li",
                "AI Commun"
            ],
            "title": "31",
            "venue": "267 ",
            "year": 2018
        },
        {
            "authors": [
                "J. Ardelius",
                "L. Zdeborov\u00e1"
            ],
            "title": "Phys",
            "venue": "Rev. E 78, 040101 ",
            "year": 2008
        },
        {
            "authors": [
                "R.J. Dakin"
            ],
            "title": "Comput",
            "venue": "J. 8, 250 ",
            "year": 1965
        },
        {
            "authors": [
                "D. Gade",
                "S. K\u00fc\u00e7\u00fckyavuz"
            ],
            "title": "Wiley Encyclopedia of Operations Research and Management Science",
            "venue": "1 ",
            "year": 2013
        },
        {
            "authors": [
                "A. Zanette",
                "M. Fischetti",
                "E. Balas"
            ],
            "title": "Math",
            "venue": "Program. 130, 153 ",
            "year": 2011
        },
        {
            "authors": [
                "W. Cook",
                "S. Dash",
                "R. Fukasawa",
                "M. Goycoolea",
                "INFORMS J. Comput"
            ],
            "title": "21",
            "venue": "641 ",
            "year": 2009
        },
        {
            "authors": [
                "A.M.C.A. Koster",
                "A. Zymolka",
                "M. Kutschka"
            ],
            "title": "Algorithmica 55",
            "venue": "375 ",
            "year": 2009
        },
        {
            "authors": [
                "H. Schawe",
                "R. Bleim",
                "A.K. Hartmann"
            ],
            "title": "PLOS ONE 14",
            "venue": "e0215309 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Leone",
                "F. Ricci-Tersenghi",
                "R. Zecchina",
                "J. Phys"
            ],
            "title": "A: Math",
            "venue": "Gener 34, 4615 ",
            "year": 2001
        },
        {
            "authors": [
                "A.K. Hartmann"
            ],
            "title": "Big Practical Guide to Computer Simulations (World Scientific, Singapore",
            "year": 2015
        },
        {
            "authors": [
                "P. Erd\u0151s",
                "A. R\u00e9nyi"
            ],
            "title": "Publ",
            "venue": "Math. Inst. Hungar. Acad. Sci. 5, 17 ",
            "year": 1960
        },
        {
            "authors": [
                "S. Khuller"
            ],
            "title": "SIGACT News 33",
            "venue": "31 ",
            "year": 2002
        },
        {
            "authors": [
                "X. Chen",
                "Z. Tang",
                "X. Xu",
                "S. Li",
                "G. Xia"
            ],
            "title": "and J",
            "venue": "Wang, in International Symposium on Neural Networks ",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "Cutting-Plane Algorithms and Solution Whitening for the Vertex-Cover Problem\nG. Claussen1\u2217 and A. K. Hartmann1\u2020 1 Institut fu\u0308r Physik, Universita\u0308t Oldenburg,\nCarl-von-Ossietzky-Stra\u00dfe 9\u201311, 26111 Oldenburg, Germany\n(Dated: June 1, 2022)\nThe phase-transition behavior of the NP-hard vertex-cover (VC) combinatorial optimization problem is studied numerically by linear programming (LP) on ensembles of random graphs. As the basic Simplex (SX) algorithm suitable for such LPs may produce incomplete solutions for sufficiently complex graphs, the application of cutting-plane (CP) methods is sought. We consider Gomory and {0, 1\n2 } cuts. We measure the probability of obtaining complete solutions with these approaches as a function of the average node degree c and observe transition between typically complete and incomplete phase regions. While not generally complete solutions are obtained for graphs of arbitrarily high complexity, the CP approaches still advance the boundary in comparison to the pure SX algorithm, beyond the known replica-symmetry breaking (RSB) transition at c = e \u2248 2.718. In fact, our results provide evidence for another algorithmic transition at c \u2248 2.90(2). Besides this, we quantify the transition between easy and hard solvability of the VC problem also in terms of numerical effort. Further we study the so-called whitening of the solution, which is a measure for the degree of freedom that single vertices experience with respect to degenerate solutions. Inspection of the quantities related to clusters of white vertices reveals that whitening is affected, only slightly but measurably, by the RSB transition.\nI. INTRODUCTION\nPhase transitions in random ensembles of combinatorial optimization problems [1\u20133] have been attracting the statistical physics community since more than two decades. In particular so called easy-hard transitions have been observed, where for a control parameter of the random ensemble there exist critical values such that, for a given algorithm, on the easy side a problem can be typically solved in polynomial time, while in the hard region an exponential effort is necessary. Phase transitions on suitably chosen ensembles of random instances were found, e.g., for the Satisfiability Problem (SAT) [4], the Traveling Salesperson Problem (TSP) [5] or the vertexcover problem (VC) [6]. Note that such transitions describe the relationship between optimization problems and algorithms to solve them. In the physics community, so far algorithms have predominately studied which are based on moving in or representing the space of feasible configurations. These algorithm are exact branchand-bound algorithms [7, 8], stochastic algorithms, like WalkSAT [9] or ASAT [10] and message-passing algorithms [11, 12], which are inspired by statistical mechanics methods like the cavity approach [13]. Instead, we follow here a different approach, namely linear programming (LP) [14] in connection with cutting planes (CP) [15]. Such approaches operate outside the space of feasible solutions for most of the computation time. Although for practical applications many CP implementations exist, e.g., based on commercial packages like CPLEX [16],\n\u2217 gunnar.claussen1@uni-oldenburg.de \u2020 alexander.hartmann@uni-oldenburg.de\nthe phase transition behavior with respect to easy-hard transitions has not been studied widely. Recently and for the first time to our knowledge, a phase transition in a problem-specific CP approach was observed and analyzed [17]. Based on this, we perform here a corresponding study for two very general CP approaches, so-called Gomory and {0, 12} cuts, for the VC problem on Erdo\u030bsRenyi (ER) random graphs.\nThe VC problem describes the intricate task of finding the vertices (also called nodes) of a graph that make up a subset called cover in such a way that all edges of the graph are adjacent to at least one vertex in the cover set. It forms a prime example of a combinatorial optimization problem[14], as the decision whether one vertex is part of the cover set or not is Boolean. As many problems of applications can be mapped onto graphs, it comes as no big surprise that also the VC problem has been applied to model real-world problems such as the number of guards needed in a museum [6], the stationing of police cars on road networks [18], the placement of sensor devices in wireless communication networks [19], vaccination strategies against disease spreading [20], or even cooperative robot surveillance [21].\nPreviously the phase transition behavior of VC was studied mostly [6, 22\u201324] in connection with algorithms based on the space of feasible solutions. For the case of ER random graphs, it turned out that the connectivity cc = e \u2248 2.718, i.e. the Eulerian number plays a crucial role. For c < e, VC is typically easy [25] and can analytically be solved by a replica-symmetric approach [6], hence the solutions landscape is dominated by one single cluster in the thermodynamic limit. For c > e, replicasymmetry breaking (RSB) appears, as visible by many clusters in the solution landscape [26], and the problem appears typically hard to solve by exact algorithms. Nevar X\niv :2\n20 5.\n15 92\n3v 1\n[ co\nnd -m\nat .s\nta t-\nm ec\nh] 3\n1 M\nay 2\n02 2\n2 ertheless, VC can be comfortably expressed as an LP, where variables xi = 0, 1 describe whether node i belongs to the cover subset or not, see below for a technical description. With this, a relaxed LP, i.e., with xi \u2208 [0, 1], can be solved through the Simplex (SX) algorithm [27]. A simple measure of the complexity of a graph is the connectivity c := 2MN , where M is the number of edges and N the number of vertices. As the VC problem is noterministic polynomial (NP)-hard [28], with increasing complexity of the graph, the plain SX algorithm fails to produce correct (also called complete) results with unambiguously decided variable values xi \u2208 {0, 1}. Instead the yielded solutions include an increasing amount of undecided or fractional values xi \u2208 [0, 1]\\{0, 1}. Such solutions are called incomplete, and generally the fraction p of complete solutions obtainable with the SX algorithm breaks down when increasing c. This represents, e.g. on the ensemble of random graphs, such an aforementioned phase transition between an \u201ceasy\u201d and a \u201chard\u201d optimization problem [17].\nThe SX algorithm can be improved by the introduction of cutting planes, i.e., sets of linear inequalities which reduce the volume of the space of possible solutions. For the study of phase transitions of VC, first problem-specific cutting planes were investigated, based on the topology of the problem [17]. There, it was reasoned that for closed cycles within a graph that consist of an odd number of vertices, the respective number of vertices covered within this cycle has to be equal to the the rounded-up half of the number of vertices. With this, additional constraints could be entered into the LP. With these measures a transition regarding the hardness of the problem was observed at a the connectivity of cc = e that has already been described earlier.\nWhile this approach was so to say tailored for the VC problem and its prerequisites, it is tempting to utilize more general approaches as well. A well-established one is the so-called Gomory cut, which produces new constraints from a given solution by regarding only the fractional parts of the coefficients in the SX tableau [29]. Originally is was viewed as a theoretical way only, also because of arising numerical instabilities [30]. Nevertheless, more recently it has since become more widely employed [31, 32] for practical optimization, especially in connection with branch-and-cut methods. Note that the aim of computer scientists is an engineering one, to provide as efficient algorithms as possible, typically tested for test-beds of problems, and leading usually to implementations of clever combinations of algorithms. In the present work, we are more interested in the behavior of certain isolated algorithms when applied to ensembles exhibiting easy-hard transitions. This means we are interested in the statistical mechanics properties and the relation to the computational complexity of certain algorithms, even if in some cases the algorithmic performance is bad.\nA similar CP method is the { 0, 12 }\n(zero-half) cut, which is a special case of the so-called Chva\u0301tal-Gomory\ncut [33]. Within this approach, linearly independent rows of this tableau are added up until an odd number of coefficients is either equal to 0 or can be divided to 12 respectively, hence the name. The right-hand side of the inequality can then be rounded up or down (depending on the sign) to the next integer. Note that these cuts are also noted to combine \u201ccyclic\u201d inequality constraints [34], thus the cycle-cut scheme employed by [17] can just be interpreted as an applied case of zero-half cuts.\nFor the present work, we again start by solving the VC of ER graphs by application of the plain SX algorithm, but we perform a thorough analysis of the results, which was lacking in previous work. Next, we expand the SX algorithm by allowing Gomory cuts, zero-half cuts or both combined. For comparison, we also employ an exact algorithm, but restricted in system size. We investigate the criticality of the VC problem\u2019s hardness with respect to the fraction of complete solutions, the fraction of covered vertices and the deterministic calculation time needed for obtaining the solution.\nA further property we put under scrutiny in the present work is the so-called whiteness of the solution. This is not directly related to cutting planes approaches, but gives also some insight into the solution landscape and has been, to our knowledge, not been studied before for VC. As the name indicates, the method was originally proposed for graph-coloring problems [35] and more prominently applied to the K-SAT problem later [36, 37]. Whitening was used to identify, e.g., clusters of solutions that differ in only one variable [36, 38]. A detailed study on the more general K-SAT problems was presented later on [39, 40]. Whitening procedures have been used also in the calculation of SAT backbones [41]. The interesting property to put under particular scrutiny with respect to solutions of the VC problem is the dependence of the \u201cwhiteness\u201d of the solution on the solvability of the respective problem. In the literature on whitening, this idea has been presented in variousways. First, there is the notion of a \u201cfreezing\u201d transition: With an increased number of clauses, i.e. constraints in the SAT problem, the fraction of \u201cfrozen\u201d solutions rises, i.e., the fraction of those variables which exhibit in all degenerate solutions the same value. For a low density of constraints, the solutions are nearly always \u201cwhite\u201d, also called \u201cunfrozen\u201d or \u201cfree\u201d in other references [40, 42]. Thus, it has been assumed that the presence of white solutions indicates easy solvability [39]. A particular mark of this property is the fact that the average whiteness depth (AWD), which expresses the average iteration number of the whitening procedure in which a variable of the model is marked as white, remains finite. It appears however that this property has not been inspected any further afterwards.\nNote that the whitening procedure is not applicable to VC solutions in its original form, but we present below a corresponding adjusted version. In the results section, we will investigate quantities related to the prevalence of white vertices in the graph and interconnected clusters\n3 of these. This paper is outlined as follows: Section II A introduces the VC problem formally and its LP formulation. Next, in Secs. II B,II C and II D we explain the numerical algorithms we have used. In section III we show our results including data for the typical computaional hardness. Ultimately, Sec. IV sums up the results briefly and gives an outlook to further desirable research on this topic."
        },
        {
            "heading": "II. METHODS",
            "text": "We first introduce the formal definition of the vertexcover problem and show how it can be written as a linear program. Next, we explain the CP approach. In the third section, we show our whitening procedure as adapted to VC. Finally, we define the finite-size scaling approach we have used to analyze the data.\nA. The vertex-cover problem as a linear program\nGenerally, the vertex-cover problem is defined on a graph G = (V,E), where V is a finte set of N vertices (or nodes) which are connected by M undirected edges {i, j} \u2208 E \u2282 V (2). In turn, the edge {i, j} is called incident to the nodes i and j, and these two edges are called adjacent to each other.\nNow, the VC problem on G consists of the following: Find a subset VC \u2286 V such that \u2200{i, j} \u2208 E : i \u2208 VC \u2228 j \u2208 VC . To put it less formal: Each edge present in E should be incident to at least one vertex in VC . The vertices present in VC are called covered, and the subset VC comprises the vertex cover of the graph G. Note that with the aforementioned condition, it is possible for an edge to be incident to two vertices of VC .\nTrivially, VC = V is already a vertex cover, but obviously a very large one. The main point in the VC problem is therefore to minimize the size |VC | of the vertex cover, which is an optimization problem. It belongs to the class of NP-hard problems [43], which means that currently only exact algorithms are known which require a worst-case running time which grows exponentially in the number N of nodes. This directly motivates the expression of the VC problem as a linear program (LP). In this, all vertices are represented as variables xi where xi = 1 means node i is covered while xi means i is not covered. Now, we have the program statement\nminimize \u2211 i\u2208V xi (1)\nsubject to: xi + xj \u2265 1 \u2200{i, j} \u2208 E (2) xi \u2208 {0, 1} .\nSuch a scheme is called integer linear program (ILP). Eq. (1) denotes the objective function of the program.\nThe total sum of variables is to be minimized, i.e. the number of elements xi in the vertex cover VC . But this minimization has to fulfill the expressions stated in Eq. (2), which are called constraints: Each edge {i, j} present in E is represented through an inequality, and this inequality is fulfilled if the variables corresponding to the vertices i and j sum up to at least 1, i.e., if at least one node is covered.\nBut unfortunately, such ideal results are only possible if the solution space is integer, i.e. ~x \u2208 {0, 1}N . With real values xi \u2208 R, which makes the problem feasible for polynomially-running LP solvers. One calls the constraint xi \u2208 {0, 1} relaxed, e.g., to xi \u2208 [0, 1]. However, the problem now arises that a constraint may also be fulfilled by e.g. xi = xj = 0.5. For example for a triangle graph, i.e., the complete graph with three nodes, x1 = x2 = x3 = 0.5 is the optimum solution of the relaxed problem, while a true optimum VC solution has\u22113\n1 xi = 2. Obtaining such a non-integer solution, in particular for large problems, gives no clear deciding statement whether i and j are actually part of the vertex cover. Solutions of the LP with such an outcome on any of the variable values xi are called incomplete.\nNote that actually solving ILPs is a research topic of its own, and has spurred the development of many nowcommon methods. Most prominent among these are treebased branching methods [44] and CP approaches [45]. Such approaches have an exponential worst-case running time. The latter approach and its phase transition behavior with respect to typically polynomial running times is the main focus of this work.\nB. Cutting-plane algorithms\nTo explain the approaches we used, we have to resort a bit to the theory of linear programming, for easy introductions see [14, 27]. Nevertheless, we will mention only those elements of LP, which are needed here.\nWe start considering the relaxed VC problem, i.e., with xi \u2208 [0, 1], the space ~x \u2208 [0, 1]N is the N -dimensional unit hypercube. The linear inequalities in Eq. (2) constitute dividing lines in the xi-xj plane. Thus, the relaxed solution space becomes limited in nearly all directions, and instead of a hypercube, it is now equivalent to a polytope.\nThe linearity of the optimization problems means that for any points inside the polytope there is a direction in which one can improve the objective function. Therefore, optimum solutions are found on the vertices or facets of the polytope. To apply the SX algorithm, the problem is slightly rewritten, by replacing the inequalities xi + xj \u2265 1 by equalities xi + xj \u2212 sm = 1 with an additional slack variable sm \u2264 0 for each constraint. Note that for VC, the constraints imply sm \u2208 [0, 1]. Since there are M edges, there are M constraints and therefore m = 1, . . . ,M slack variables sm. This set of equations is written in matrix form as\n4 A~x = ~b , (3)\nwith A being an M \u00d7 (N + M) matrix describing the equalities. For simplicity, we denote the combined vector of the problem variables x1, . . . , xN and the slack variables s1, . . . , sM as the (N + M) dimensional vector ~x. Note that here ~b = (1, 1, . . . , 1)T \u2208 RM and that A has rank M since all edges are independent in the graph.\nAs the theory of LP [14, 27] tells us, SX works by selecting so called basic feasible solutions. These are subsets B = {B(1), . . . ,B(M)} of indices B(i) \u2208 {1, . . . , N +M} such that the columns of A corresponding to these indices are linearly independent. Thus, these columns form a M \u00d7M matrix of rank M denoted as B. The other indices of the other columns of A are denoted as Z, forming the matrix Z. W.l.o.g., let the columns and rows of A and the corresponding entries of ~x and ~b be ordered in such a way that A = (B,Z) and ~x = (~xB , ~xZ) T . Thus, Eq. (3) can be written as A~x = B~xB + Z~xn = ~b. Since B has rank M , the inverse matrix B\u22121 exists, and one obtains B\u22121B~xB+B \u22121Z~xZ = B \u22121~b, i.e., ~xB+B \u22121Z~xZ = B\n\u22121~b. By selecting ~xZ \u2261 0 one obtains the basic feasible solution\n~xB = B \u22121~b , ~xZ \u2261 0 . (4)\nWe just mentioned that SX works by first determining an initial basic feasible solution, which is sometimes involved, and then exchanging variables in and out of B while assuring that the value of the objective function is not changed opposite to the desired direction. This is interated until no further improvement is possible, i.e., the optimum is found, which also fulfills Eq. (4).\nNow, since the problem is relaxed, i.e., ~x \u2208 [0, 1]N+M , there may be non-integer entries of ~x such that the original ILP Eqs.(1),(2) is not solved. Note that if the relaxed problem leads actually to an integer, i.e., complete solution, it is automatically a solution of the ILP and we are done.\nHence, we now consider the case of an incomplete solution, i.e., there are entries in ~xB which are, for our problem, in ]0, 1[. We next explain so called Gomory cuts [29, 32], which have the basic idea to generate inequalities which make the present incomplete version invalid, while not affecting any solution to the ILP. The starting point is to multiply Eq. (3) with B\u22121 leading to\nB\u22121A~x = B\u22121~b . (5)\nFollowing in our presentation[46], we denote the entries of B\u22121A = by a\u0304ij , and the entries of B \u22121~b by b\u0304i. Note that after again suitable reordering or rows and columns, B\u22121A = B\u22121(B,Z) = (E,B\u22121Z), where E is the unit matrix. According to Eq. (4) the right hand side just contains the non-zero entries of the solution of the relaxed\nLP, i.e., of the variables belonging to the optimum basic feasible solution. Since this solution is not integer, there must be at least one line i where the right hand side is non-integer. This line reads:\nxB(i) + \u2211 j\u2208Z a\u0304ijxj = b\u0304i . (6)\nSince all variables fulfill xi \u2265 0, by rounding up the coefficients on the left hand side to da\u0304ije we immediately get xB(i) + \u2211 j\u2208Zda\u0304ijexj \u2265 b\u0304i. If all variables xi on the left hand side, also the added slack variables, are required to be integer in the end, the full left-hand side must be integer. Thus, we can round up the right hand side as well, resulting in\nxB(i) + \u2211 j\u2208Z da\u0304ijexj \u2265 db\u0304ie , (7)\nFor the current solution xi = 0 for j \u2208 Z and b\u0304i is noninteger, which contradicts Eq. (4). Thus, the inequality is not fulfilled by the current solution. Such an inequality is called Gomory cut.\nIf not all coefficients a\u0304ij \u2208 Z, then one can equivalently subtract Eq. (6) from Eq. (7). By denoting the missing part of a real number r to its next integer as \u03c6(r) = dre \u2212 r, we arrive at\n\u2211 j\u2208Z \u03c6(a\u0304ij)xj \u2265 \u03c6(b\u0304i) . (8)\nFor the current solution xi = 0 for j \u2208 Z and b\u0304i is non-integer, thus \u03c6(b\u0304i) > 0. Thus, also Eq. (8), which is called Gomory fractional cut, is not fulfilled by the current solution.\nWith the addition of the the Gomory cut given by Eq. (6) or Eq. (8), the LP can be re-solved. Afterwards, theroblem either yields a complete solution or allows additional feasible Gomory cuts, and this scheme is iterated until either a complete solution is obtained or no more Gomory cuts can be generated.\nThe main notion behind the usage of Gomory cuts is, in the words of [14], that \u201cno integer feasible points are excluded\u201d, while the solution space is still curtailed in a meaningful way, therefore generally leading towards the correct, optimum solution of the LP. It can be proven [46] that in principle the iteration of adding Gomory cuts will terminate after a finite number of steps in an integer solution, if it exists. Here, in particular the so called lexicographical dual SX algorithm yields a good performance [47]. As mentioned before, for practical implementation it is at least possible that no further Gomory cuts be generated at some point \u2013 which then might terminate the execution of the algorithm without finding a complete solution. It has also been argued that Gomory cuts are generally prone to numerical instability [48], due to computational problems in floating-point arithmetic when representing fractional numbers and possibly also\n5 due to the rapid expansion of the LP size generated by the addition of constraints and slack variables. A possible remedy of the first problem could be to treat all constraint coefficients aij generally as fractions instead of floating-point numbers.\nThe other considered CP method are {0, 12} (zero-half) cuts. In order to understand this approach, we note that the Gomory cut Eq. (7) has the form\nd~\u03bbTAex \u2265 d~\u03bbT~be , (9)\nwhere the vector ~\u03bb \u2208 Rm originates from the corresponding entries of the inverse matrix B. Still, this holds for arbitrary vectors ~\u03bb, starting from A~x = ~b and also from A~x \u2265 ~b, if one uses the original starting LP Eq. (2) without slack variables, with the same argument when deriving Eq. (7) above. If Eq. (9) excludes the current optimal but non-integer solution, it can be useful for a CP scheme. For the {0, 12} cuts, one restricts oneself to vectors ~\u03bb \u2208 {0, 12}\nm. Note that useful inequalities arise only if actual rounding takes place, this means that in A and ~b odd entries must exist. But this can be guaranteed always, because if all entries in a line are even, one can divide by two as many times as needed until at least one entry becomes odd, without changing the meaning. Although the set of possible vectors ~v is exponentially large, there exist efficient methods to find cuts which are violated by the current non-integer solution [33, 49]. Since we use an external library for our simulations, we do not go into details here.\nIn the introduction, we mentioned that the earlier CP approach to the VC problem of [17] is a special case of the {0, 12} cut. This is to be understood in the following way: In the VC problem, the basic constraints as per Eq. (2) include only two variables, i.e., those denoting whether the respective vertices incident to the respective edge are covered or not. If one now considers a cycle L \u2282 E in the graph and sums up the inequalities corresponding to the edges in the cycle, each node of the cycle will contribute its corresponding variable twice, leading to 2 \u2211 i:\u2203e\u2208L:i\u2208e xi \u2265 |L|. After multiplying with 1 2 and rounding up, which may happen only at the right side, one arrives at nontrivial inequalities for odd cycle length |L|. Therefore, the application of the {0, 12} CP approach is a generalization of the cuts used before [17] allows for a comparison with the previous results.\nRegarding the actual implementation of those CP methods we used the IBM ILOG CPLEX optimization studio [16]. In CPLEX, we use the pre-defined methods supplied by CPLEX when defining the LP specifically as an ILP. This is done by changing the type of all present variables from floating-point numbers to integers. For some our our results shown below, we have used the full power of CPLEX for the solution of the ILP. But for our analyses of the effects of the cutting planes, we have deactivates manually other other cut types except Gomory cuts and zero-half cuts. We then deactivated also branch-\nand-cut methods, which is done by limiting the number of branching nodes to zero, and all other the heuristic methods, which are anyway not further specified by the CPLEX documentation.\nFor some performance tests and comparison, in particular for counting the number of generated cutting inequalities, we also used our own implementation of Gomory cuts while using CPLEX just as LP solver. Nevertheless, we observed the issues mentioned earlier regarding numerical stability, i.e., the difficulty of representing fractional values like 13 = 0.3 = 0.333... with sufficient precision. Thus, we have set a maximum number Ncut,max = 1000 of added CPs. If this threshold was reached, without finding a complete solution, the corresponding instance was counted as incomplete."
        },
        {
            "heading": "C. Whitening",
            "text": "We start by considering the whitening approach for KSAT [36, 37]. A SAT instance is a conjunction of logical clauses, where each clause contains disjunctions of possibly negated variables called literals. For K-SAT each clause contains K literals. The SAT problems is a decision problem which asks whether for a given instance there exists a satisfying \u201ctrue\u201d assignment. Whitening is meant to distinguish white and frozen variables within a solution, where flipping white variables affects the solution only locally, but not globally. Therefore, the whitening algorithm for SAT start by assigning the white state to all clauses which are satisfied by at least two literals, i.e., at least one variable can be flipped without changing the satisfied state of the clause. Next, iteratively, variables are assigned white which appear as satisfying only in white clauses. At the same time, more clauses can turn white if they contain a white variable. The iteration stops if no changes occur.\nUnfortunately, this algorithm cannot be directly transferred to VC: The clauses of SAT correspond to the edges of VC. Therefore, in the initial phase, a white state of an edge would be assigned if the edge is covered twice. Now, a node would be considered white if all its incident edges are white. But this would mean that the node and all its neighbors are covered, which is a contradiction to the minimum property of a minimum vertex cover.\nThus, we have adapted the approach for the VC problem. The basic idea is that for a given minimum vertex cover, a covered vertex i which has at most one uncovered neighboring vertex j, one can cover i instead of j. Thus the vertex i can be in two states, covered and uncovered, which is considered as white. Corresponding cases hold for white neighbors. The actual algorithm reads as follows:\n\u2022 Given: a vertex cover VC\n\u2022 Initialization: Mark every vertex as frozen, i.e., non-white.\n6 \u2022 Repeat until no vertex state changes:\n1. Mark all vertices i white that are covered (i \u2208 VC) and that have a maximum of one neighbor which is non-covered and frozen.\n2. Mark all vertices i white that are not covered (i /\u2208 VC), but adjacent to white vertices only\nThe notion behind this scheme is exemplified by the application shown in Fig. 1: In the first step, a single node could be declared as white, i.e., node 4. This is due to the degree of freedom attributed to this vertex in the solution. Flipping vertex 4 would leave only edge (1, 4) uncovered, while the other edges incident to vertex 4, i.e. (0, 4) and (4, 6) are also incident to other covered vertices. Hence, a complete solution after flipping vertex 4 can still be obtained by covering vertex 1. The solution V\u0303C = {0, 1, 3, 6} is then of the same size NC = 4 as the original solution V\u0303C = {0, 3, 4, 6} and hence a degenerate case. For the example graph and the given solution, not more white nodes are detected.\nNote that the whitening algorithm is able to gather information about the solution space structure, but it will typically not have access to all degenerate solutions. For the example shown in Fig. 1, there are more solutions: As mentioned, one could cover node 1 instead of node 4. If node 1 is covered, one can cover node 5 instead of node 3. This solution is not detected by the whitening algorithm. We have investigated other variants of algorithms for the whitening, with more relaxed options for a node to become white, e.g., make a node white if all neighbors are either white or covered. In the present example in Fig. 1, node 1 would immediately turn white. This holds also for more nodes. After some iterations even node 6 would become white as well, which clearly makes no sense since it is covered in all minimum vertex covers. All the variants we have tried, although they looked promising on the first sight, lead to such undesirable results.\nA particular question regarding the whitening procedure is the treatment of isolated vertices of the graph,\ni.e., those without neighbors. One might consider them frozen, as they do not play any role for the VC solution itself since they are never covered at all. However, they fulfill condition (2) of the whitening procedure if the expression \u201cadjacent to white vertices only\u201d is interpreted as \u201cof all their neighbors (zero), all (zero) are white\u201d. With defining isolated vertices as white it is also possible to achieve full whitening, i.e., all vertices are white. Thus, we find it justified to define isolated vertices as white here.\nD. Scaling analysis\nOur main interest is to determine phase transitions between regions where our algorithms result in complete solutions, i.e., where all variables are integer, and regions where no or few complete solutions are found. For this purpose we measure the fraction p of graphs with a complete solution as a function of the control parameter c, which is the average vertex degree here.\nIn order to determine the phase transition from the data we follow the scheme established by [50] for the 2-SAT problem. Note that this approach works in particular for the case when the transition is not visible by an intersection of the curves for different sizes N , but instead on one side of the transition, the data is almost size independent, as it was observed previously for other phase transitions in combinatorial optimization problems [51]. The scaling approach works in the following way. For each number of vertices N of the graph, we follow the course of p(c), seek out the point of the steepest decrease in the curve, align a tangent to it and calculate the intercept c0(N) with the c-axis. The behavior of c0(N) follows [50] the finite-size behavior of other standard phase transitions well, i.e., a power-law behavior as\nc0(N) = a \u00b7N\u2212b + c0,\u221e . (10)\nThus, the asymptotic N \u2192 \u221e critical value c0,\u221e can be obtained by standard least-square fitting to Eq. (10)."
        },
        {
            "heading": "III. RESULTS",
            "text": "We performed our numerical simulation [52] of vertex covers for the ensemble of Erdo\u030bs-Renyi graphs [53]. We analyzed graphs with the number of vertices ranging between N = 20 and N = 3000, respectively. For this purpose we generated graphs exhibiting exhibiting many values of the connectivity c = 2MN between c = 0.1 and at most c = 8. For each value of N and c we generated an number of realizations of random graphs ranging between 20000 for the smallest sizes N \u2264 500 to at least 2000 for the largest sizes.\n7 0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.8 1.6 2.4 3.2 4.0 4.8 5.6 6.4 7.2\np\nc\nN = 100 N = 200 N = 400 N = 900 c0,\u221e = 2.75(2)\n2.5\n3.0\n3.5\n4.0\n0 400 800 1200 c\n0\nN\nFIG. 2. The fraction p of complete solutions of the VC problem on ER graphs of N vertices as function of the connectivity c, obtained with the SX algorithm. A tangent is fitted to the the steepest point of each curve, shown as straight lines. The intercept c0(N) af the tangent with the c-axis quantity is plotted as function of N , as shown in the inset. A power-law fit according to Eq. (10), shown as a line in the inset, results in a = 3.8(3), b = 0.35(3) and an asymptotic critical value cSX \u2261 c0,\u221e = 2.67(5), which is indicated by the vertical solid line in the main plot."
        },
        {
            "heading": "A. Completeness of solutions",
            "text": "To start, we consider the behavior of SX algorithm itself, without any refinements. The result is shown in Fig. 2 and reveals that p begins to drop already for small values of c. While this behavior appears to be independent of graph size N , the individual curves start to spread and become separated around c \u2248 2.4. After a steep decrease of p, the curves flatten out, but nevertheless hit p = 0 on all occasions, albeit earlier for larger graphs than for smaller ones.\nWe have analyzed the curves as described in Sec. II D. The finite-size dependence of the intercepts c0(N) behave in a quite regular way and can be well fitted by the power law of Eq. (10), resulting in an asymptotic value cSX \u2261 c0,\u221e = 2.67(5). This is within error bars compatible with the known critical value of cc = e = 2.71828..., where replica symmetry-breaking in the analytical calculation appears [6, 23], where clustering of solutions can be observed numerically [26], and where the percolation of the leaf-removal core occurs [25]. Interestingly, at this point also the SX algorithm in combination with the cycle cutting planes stop to work successfully [17]. Thus, the comparison with the present result seems to show that\nthe application of cycle cutting planes leads to the same phase transition point. Nevertheless, a direct comparison of the rate p of complete solutions from Fig. 2 and Fig. 1 of Ref. [17] reveals, that p is always higher when including the cycle cuts, as one can expect it. These figures suggest the following interpretation: For the SX algorithm alone, the point c = e denotes the connectivity beyond which basically no complete solutions can be found at all, while for c < e, a finite fraction a graphs can be solved, but p \u2192 1 only for c \u2192 0. One the other hand, SX plus the cycle cuts allows one to find complete solutions for almost all graphs for c < e, in particular p\u2192 1 for about c \u2264 2.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.8 1.6 2.4 3.2 4.0 4.8 5.6 6.4 7.2\np de\nc = 1\n-p (0\n<x i<\n1)\nc\nN = 50 N = 150 N = 550 N = 1280 c0,\u221e = 3.2(2)\n3.0\n3.5\n4.0\n0 400 800 1200\nc 0\nN\nFIG. 3. Fraction of decided vertices pdec in solutions obtained with the SX algorithm as function of connectivity c. The inset shows the intercepts c0(N), the extrapolation yields a critical value cdecSX \u2261 c0,\u221e = 3.2(2). The vertical line indicates the extrapolated critical value c0,\u221e, while the other lines show the tangents used to determine the intercepts c0(N).\nOne might argue that the overall fraction of complete solutions is a too rigorous quantity, as a VC solution is incomplete already with only a small fraction of vertices being actually undecided. From the algorithmic point of view even obtaining a finite fraction of decided variables xi, i.e., xi = 0 or xi = 1, might help towards the overall solution, because for the vertices cover problem it has been shown [54] that there are always exact solutions for which the state of these variables is the same. This means, one can safely remove the nodes of these decided variables from the graph and apply a different, possibly exhaustive, algorithm for the remaining graph, which might be considerably smaller.\nTherefore, we consider next the fraction of decided vertices, i.e., pdec = |{i \u2208 V |xi = 0 \u2228 xi = 1}|/N . The\n8 result is shown in Fig. 3. It turns out that the fraction of decided vertices remains high for much more complex graphs, i.e. higher values of the connectivity c, in comparison to the result for p. It appears that indeed only small fractions of vertices are responsible for the rapid decrease of p in Fig. 2. In turn, the analysis based on estimating the corresponding intercepts c0(N) with respect to the graph size N yields a much higher critical value of cdecSX \u2261 c0,\u221e = 3.2(2). Note that the curves of pdec include all complete solutions as well, which enter into this averaged quantity with pdec = 1. The actual fraction of decided vertices in graphs with incomplete solutions may be considerably smaller. Anyway, the result means that there is an intermediate regime, where one can decide at least a fraction of the variables and the remaining problem to be solved by a complete algorithm is considerably smaller. As usual, more insight on this might be gained by examining the actual distribution of the number of decided or undecided vertices and not alone the behavior of the average, but this is beyond the scope of the current study.\nNext, we consider the application of the SX algorithm with Gomory cuts. The result for p(c) is shown in Fig. 4. The found curves of p(c) are similar to that of the SX case in that they also approach zero. Some other properties however differ from that case: Most notably, p(c) stays close to 1 over quite a range of c. It only starts to drop off at considerably large values around c \u2248 2.3, also this drop is steeper than in the SX case.\nAn extrapolation from the zero intercepts of the tangents on the steepest slopes yields a value of cGomory \u2261 c0,\u221e = 2.90(2), which however is well above the value\nfound for the SX case. Thus, it seems that Gomory cuts help the SX algorithm efficiently to obtain complete solutions for connectivities beyond the aforementioned critical value at cc = e \u2248 2.718. Hence, the results offer the possibility that there exists another structural transition beyond cc = e which could be responsible for the transition seen with the complete Gomory-cut algorithm. However, the extent of this change of cGomory as seen in the thermodynamic limit N \u2192 \u221e may appear a bit arguable, as the extrapolated value does not lie far above that for the SX algorithm. Still, as we will see in the next section, the results for the number of necessary cuts will show that there is indeed a change of the behavior significantly above c = e \u2248 2.718. Note that the results depend in principle on the limit Ncut,max for the number of applied Gomory cuts. Since the Gomory cuts are complete in theory, an infinite-precision CP algorithm would always lead to a complete solution if one allows for arbitrary long running time. Anyway for some test cases in the critical region, we did not see any relevant changes when raising this limit Ncut,max by a factor of ten. Note that if indeed raising the limit lead to any notable change, it would rather increase of p(c). This would rather increase cGomory than decrease it towards cc = e.\nWe have evaluated the fraction pdec of decided variables also for the SX + Gomory cuts approach. The figures looks similar to the previous shown results and is therefore omitted here. We have in the same way analyzed the interceptions co(N) and fitted it to the power law Eq. (10). We have obtained a extrapolated critical value cdecGomory \u2261 c0,\u221e = 3.48(5) for this case. Similar to the pure SX case, the critical point cdecGomory below which almost all variables can be decided is well above the critical point cGomory = 2.90(2) below which the problem can be completely solved. Thus, there is again an intermediate regime, where the Gomory-cut approach is able to determine many variables, leaving only a small fraction of the graph still to be solved by an algorithm which is complete in practice.\nThe value cdecGomory = 3.48(5) is near the critical value c3\u2212core \u2248 3.35 where the percolation of the 3-core appears for ER random graphs [1]. The q-core of a graph is the subgraph which remains when one iteratively removes all nodes and the adjacent edges for nodes with a degree smaller than q. Note that for q = 3 the transition is first order, often called \u201cexplosive percolation\u201d. Given this closeness, we have tried to find correlations between the set of non-integer nodes and between the 3-core by investigating realizations for c = 3.35 and N = 1000. But our results indicate only a small relationship: The size of the 3-core does not at all correlate with the number of noninteger variables (not shown), while it is known that the size of the leaf-removal core does [17], which is also confirmed by the data obtained in this study. Still, whenever there is a non-zero 3-core, the fraction of non-integer variables which are located on the 3-core is about 47%, while the 3-core size is only about 17% of the graph. Thus, one can say that the 3-core does not determine the number\n9 of non-integer variables, but non-integer variables are located preferentially on the 3-core.\nMoving over to the SX algorithm with {0, 12} cuts, the result is shown in Fig. 5. Note that here we rely completely on the CPLEX implementation of the cuts. Opposed to the pure SX algorithm, CPLEX then does not yield incomplete solutions here, but simply returns no solution at all and an error informing us that \u201cno solution exists\u201d. Therefore, the fraction of undecided variables can not be evaluated. Anyway, the result of p(c) displays similar qualities compared to Gomory cuts, including the tendency to remain near p(c) \u2248 1 until c \u2248 2.3. The critical point for the disappearance of complete solutions is determined by the fit to Eq. (10) and yields the extrapolated value c0, 12 \u2261 c0,\u221e = 2.8(1). This is statistically compatible with the critical values obtained for the SX approach alone, the SX + cylce cuts algorithm [17] and the critical value cc = e \u2248 2.718. Since, as discussed, the previously used cycle cuts are a, apparently powerful, subset of the zero-half cuts, it appears reasonable that the critical point is comparable, if not equal.\nIt might be tempting to allow Gomory and {0, 12} cuts at the same time. We actually carried out such calculations, yet it turned out that the found value for c0,\u221e did not go beyond those found for these two CP methods alone, respectively. Therefore, we omit these results here, and conclude that the applicability of cutting planes is\ncontrolled by the set of \u201cmost-powerful\u201d cuts and the corresponding critical points determined by the related structural changes of the graph ensemble."
        },
        {
            "heading": "B. Computational hardness",
            "text": "The next question we want to address is whether there is a connection between the behavior of the solvability and the typical computational hardness. Regarding the application of the pure SX algorithm, this question is not so interesting, because it is known that the SX algorithm typically runs always in polynomial time [14], although there is not formal polynomial bound for the worst-case running time. Note that the so called ellipsoid algorithm solves LP in guaranteed worst-case polynomial time, but it is more complicated and slower than SX.\nThus, we consider next the case of the Gomory cuts. We investigated the typical numberNcuts of cuts, as usual when considering running times, since it will not be influenced by statistical outliers which are very hard to solve. In particular, we evaluated the r = 0.7 percentile. Thus, 70% of all problem instances require a number of cuts smaller or equal to the shown value. Note the we could have used the median, i.e., the 0.5 percentile instead, but here the finite-size dependence is not very strong since about half of the problem instances require only few cuts. This means, for the median it is much harder to distinguish between a polynomial and an exponential growth. Note that when taking the percentile also instances which can not be solved within the selected maximum number of added cuts have no influence, if the fraction of solved instances is larger than r. This is also the reason why we have not chosen a larger percentile r, because then the number of unsolved instances would\n10\nmake more points invalid: For c = 2.9 and N = 3000, the fraction of unsolved instances has already grown to 0.25.\nFor small values of c, the problem is typically solved with no or very few additional cuts. Thus, we concentrate on the case of c near the critical value cc = e. In Fig. 6 the typical number Ncuts of Gomory cuts as a function of N is shown for three significant values of c. Please note the logarithmic scale for Ncuts. For c = 2.8, the data exhibits a curvature and a fit to a power law Ncut(N) = Ncut,0+\u03b1N\n\u03b2 works very well, with Ncut,0 = 1.6(2), \u03b1 = 0.012(7), and a small \u03b2 = 0.86(7). This shows that the problem is typically polynomial here, even slightly beyond c = e. Note that, as additional test, for c = 2.8 also the average number of Gomory cuts (not shown) exhibits a polynomial behavior with power as a function of the system size N\u03b2 with \u03b2 = 1.29(4).\nOn the other hand for c = 2.9 and c = 3.0, the data follows a straight line and fits well to an exponential Ncut(N) = \u03b3 exp(\u03b4N), with \u03b3 = 3.7(2), \u03b4 = 0.015(1) for c = 2.9 and \u03b3 = 3.1(4), \u03b4 = 0.033(1) for c = 3.0. This renders the problem typically exponentially hard for c \u2265 2.9. These results are compatible with the above determined critical threshold cGomory = 2.90(2) below which typically complete solutions are found by this approach. Note in particular the fact that the number of cuts at c = 2.8 increases clearly polynomially, at least for the system sizes we can access, confirms that that the critical value cGomory for the Gomory cuts is separated from the well known value cc = e where RSB appears.\nThe CPLEX package which we have used offers also a complete solver for ILPs through a number of means beyond those considered here in detail. Among these are branching, all kinds of different CP methods and not further specified heuristics, all of which are even enabled per default. This however comes at the price of a massive numerical effort, which can be estimated directly from the solver\u2019s \u201ctics\u201d. This is stated by CPLEX and measured versus a given counter during the execution of the code and can therefore be interpreted as deterministic and somehow independent of the respective computer system. Thus, we use this number of tics as calculation time ttotal of the complete solver and study its dependence on the graph size N at given values of the graph connectivity c. It turns out that the behavior of ttotal is well described through a power-law dependence, i.e. ttotal \u221d N b in any case as exemplified in the inset of Fig. 7. Note that we observe polynomial behavior for all values of c, even well beyond cc = e. This shows that the algorithm is in the range of accessible system sizes empirically very powerful. Still, it can not be excluded that for much larger sizes an exponential behavior becomes visible for large enough connectivity c. Still, for low values of c, the exponents b revolve around b \u2248 1.5, in the range of b = 1.4...1.6. However, the main plot of Fig. 7 exposes a minimum at c \u2248 2.6. The reason for the slight decrease beyond c \u2265 1, instead of a slight growth, is probably that CPLEX starts to use other means than the cutting planes studied here,\ne.g., activates branching, since this is actually more efficient. Beyond c \u2248 2.6 an excessive growth of b can be observed. Thus, although the typical behavior is polynomial for the complete CPLEX solver in the studied regime, the combination of algorithms CPLEX uses to attack VC is again mostly influenced by the structural change of the ER random graphs near cc, highlighting the importance of this critical point.\nNote that we could have measured also the running time for the zero-half cuts through the number of CPLEX tics. Nevertheless, the behavior of the zero-half cuts is very similar to the cycle cuts, as explained. Since the empirical computational hardness of the cycle cuts has already been studied [17] and did also show a pronounced change at cc = e, we do not expect different results here and thus did not consider the empirical time complexity of the zero-half cuts.\nC. Whiteness of vertex-cover solutions\nThe intention behind investigation of whitening in VC solutions is the idea that the aforementioned transition at cc = e might also affect quantities related to white or frozen vertices of these solution. The whitening procedure can only be applied properly on correct, complete solutions of the VC problem on a given graph. Therefore,\n11\nfor the corresponding simulations we simply utilized all options CPLEX has to offer, heedless of the drastic increase of computation time for larger connectivities. Still, it was sufficient to consider only graphs up to c = 4 where we were able to study graphs with N \u2264 700.\nOur results show that, even with definition of isolated vertices as white, full-white solutions are rare and only occur for graphs of small size N and low connectivity c, so the VC problem is generally characterized by largely frozen variables and only a certain fraction of white vertices or clusters of these.\nFirst, we considered the size dependence of the number Nwv(N) of white vertices. For a given connectivity c, the relative quantity nwv = Nwv/N converges asymptotically to a constant value according to a power-law like Eq. (10), i.e., nwv = nwv,\u221e + aN\n\u2212b, see inset of Fig. 8. These extrapolated values nwv,\u221e as function of c are shown see main plot in the figure. No particular influence of the transition at or near cc = e or elsewhere is visible. The asymptotic fraction nwv of white vertices matches well an exponential decrease as function of c. This resembles the exponential decrease e\u2212c of the isolated vertices, but not as fast. Already at c = 1, the fraction of isolated vertices is about 0.37, well below nwv,\u221e \u2248 0.64.\nStill, by taking a closer look, the RSB transition at cc = e is visible in the data. In the inset of Fig. 9 the relative size nmax,w = Nmax,w/N of the largest white cluster is shown as a function of the number N of nodes. The data follows well a power law with exponent b for all values of\nc. In the main plot of Fig. 9 the results for b from fits to power laws are shown as function of the connectivity c. For low values of c, b revolves around low values of ca. b \u2248 0.73. With c increasing above the threshold around cc = e, the value of the exponent then moves to a regime of considerably larger values around b \u2248 0.8. It is also possible to fit a stretched and shifted sigmoid function sig(c) = b0 + a 1+exp(c\u2212c\u2217) to the course of b(c) to model this transition and yield a center point of c\u2217 = 2.73(6), which which is compatible with the RSB transition at cc = e within the error range.\nFor some other quantities obtained through fits similar changes near cc = e are visible. For example, when considering the average size of all clusters except the largest one, and fitting power laws to the N -dependence, the prefactor of the resulting power law shows a decrease by about 15% (not shown) near c = cc, but the signature is a bit weaker as compared to the preceding case. Next, when analyzing the number of white clusters as function of system size, the approach to the limiting value is from below for about c \u2264 2.6 while it is from above for larger values of c. Nevertheless, such pronounced changes as function of c are not visible for all parameters involved in the finite-size behavior of parameters related to whitening. For example, we also considered the exponent b obtained in fitting nwv(N), as shown in\n12\nthe inset of Fig. 8. Here the value of b is up to a factor of 2 larger for smaller values of c \u2264 2 as compared to large values c \u2265 3 (not shown). But the data for b is rather noisy and the decrease seems to happen rather near c = 2 than near cc = e.\nIV. CONCLUSIONS\nWe have considered the NP-hard combinatorial optimization problem vertex cover on Erdo\u030bs-Renyi random graphs with connectivity c. We applied a represention as a linear program and sought to solve this through various variants of the simplex algorithm in particular enhanced with Gomory cuts. The main motivation behind this was the question how the structure of the ER graphs influences the ability of the SX algorithm augmented with a theoretically complete set of cutting planes to obtain integer solutions at the expense of computational effort. Also, due to numerical limitations originating from finite number precision, well known instabilities of Gomory cuts may lead to regions in graph space, where in practice no solutions can be obtained at all.\nBy applying the CPLEX library for LPs, we could study rather large graph sizes of up to N = 3000 nodes. We have found transitions between solvable and unsolvable phases for SX alone and for SX+Gomory cuts. By analyzing the finite-size dependence of the interceptions c0(N) of tangents of the solution probability p(c), we determined critical points by extrapolation N \u2192 \u221e. For the pure SX algorithm, the obtained value cSX = 2.67(5) is well compatible with the critical point cc = e \u2248 2.718 where the onset of RSB is located, a hierarchical clustering of the solution sets can be observed, and the ER ensemble exhibits a percolation transition of the leafremoval core. Interestingly, the same analysis for the SX+Gomory cuts approach yields a significantly higher value cGomory = 2.90(2). The analysis of the number Ncuts of necessary Gomory cuts supports the finding that this easy-hard transition is distinct from the RSB transition: For c \u2264 2.8, Ncuts grows like a power law with the system size, while for c \u2265 2.9 a clear exponential growth is visible. Thus, it seems that the structure of ER random graphs exhibits another, to our knowledge yet unknown, change of structure for c \u2208 [2.8, 2.9]. Nevertheless, since we study only graphs of finite sizes numerically, we cannot exclude that the apparent polynomial growth of the running time we have observed for c = 2.8 turns into an exponential growth at much larger system sizes, but this appears unlikely to us, given the clean fits we have observed. Anyway, it would be very interesting if our results motivate further analysis which confirm that VC indeed exhibits more than one transition with respect to the computational complexity, similar, e.g., to the rich behavior of the SAT problem.\nConcerning the technical implementation of Gomory cuts, it should be noted that they require very high numerical precision, since they rely on an accurate compu-\ntational representation of rational numbers. CPLEX actual operates with a finite numerical percision. At least, cross-checking with other publicly available LP solvers such as lp solve,[55] PuLP[56] or Python MIP[57] might give further insight to this issue, to see whether the observed easy-hard transition at a point above cc = e is present for other solvers as well.\nWe have also studied the fraction pdec of decided variables which results form the calculation of partial solutions. This is interesting, because for VC all integervalued variables within a incomplete solution can be left out before proceding with a complete algorithm. We have found also transitions from a low-connectivity phase where pdec > 0, but possibly pdec < 1 to a high-connectivity phase where pdec = 0. Interestingly, by again analyzing the finite-size behavior of the intercepts of tangents, we obtained for the SX and for the SX+Gomory cases of transition points around c = 3.3 which are located even more above the known RSB transition point cc = e. Our analysis revealed a weak relation to the appearance of the 3-core, but whether undecided variables exist at all in the solution seems basically to be independent of the existence of a 3-core.\nOur results show that the dynamics of LP based algorithms are harder to understand. The reason is probably that they operate outside the space of feasible solutions, in contrast to branch-and-bound algorithms, messagepassing techniques or approaches based on random walks, where only feasible solutions are considered. Thus, LPbased algorithms deserve a closer look by physicists in the future, in particular because such algorithms are applied in practical industrial applications.\nFurthermore, it turned out that { 0, 12 }\ncuts cannot be seen an alternative to Gomory cuts. The obtained phase transition rather coincides with that observed for the pure SX approach and the one seen in previous work for SX + cycle cuts. This seems plausible, since we argued that they technically are largely identical to the modelspecific CP approach of [17], which considered particular constraints for vertex cycles of odd length within the ER graph.\nRegarding whitening of the VC solutions, the findings for this problem differed well from those reported for SAT models in that the VC problem hardly ever showed full whitening. This however comes as no big surprise, as we had to modify the whitening procedure specifically for this problem since the original whitening procedure for SAT tended to mark the whole interconnected cluster of the graph as white. The asymptotic N \u2192 \u221e fraction of white vertices showed no pronounced change as a function of the connectivity c, just a smooth exponential decay. But instead, we found out that the criticality exposes itself in other properties related to whitening, namely in the exponent which controls the finite-size behavior of the size of the largest white cluster.\nTo summarize, whitening for VC, at least in the version we tried, displays visible effects related to the clustering phase transition at cc = e only within a deeper scaling\n13\nanalysis of the data. A more direct access to the clustering structure, e.g., by looking at neighboring solutions in solution space [26], seems so far more capable to provide insight into the solution landscape for VC.\nConcerning further work, it should be stressed that Gomory cuts are in principle able to solve any combinatorial optimization problem, by considering relaxed LPs, i.e., configurations which start non-feasible but optimal and are moved iteratively into the space of feasible solution. This should motivate more research, seeking a better understanding of the relation between computational complexity, problem instance properties and solution-space structure. Here, checking the VC problem through other LP solvers with Gomory cuts might add clarity, in particular for solvers which work with true infinite-precision rational number representations. Also it would certainly be worthwhile to consider other problems, like coloring, SAT, number partitioning or the traveling salesperson problem, to see whether known easy-hard transitions are visible when studying the behavior of Gomory cuts.\nAlso inspection of typical properties of complete solutions of VC and other optimization problems might give insight to similar CP methods. It could be even\nworthwhile to utilize machine-learning based methods for the selection of cutting pleanes, since such approaches to the general VC problem have already been considered in other ways [58\u201360].\nFinally, the work on the relation between out-ofconfiguration space LP cutting plane approaches and the structure of combinatorial optimization problems has been numerical so far, to our knowledge. It would therefore be very desirable if analytical approaches were attempted to better understand how the structure of the solution space affects such algorithms."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We acknowledge personal support by Christoph Norrenbrock. The simulations were performed at the the HPC cluster CARL, located at the University of Oldenburg (Germany) and funded by the DFG through its Major Research Instrumentation Program (INST 184/157-1 FUGG) and the Ministry of Science and Culture (MWK) of the Lower Saxony State.\n[1] A. K. Hartmann and M. Weigt, Whiley-VCH, Weinheim (2005). [2] M. Me\u0301zard and A. Montanari, Information, Physics and Computation (Oxford University Press, Oxford, 2009). [3] C. Moore and S. Mertens, The Nature of Computation (Oxford University Press, Oxford, 2011). [4] S. Kirkpatrick and B. Selman, Science 264, 1297 (1994). [5] I. P. Gent and T. Walsh, in Proceedings of 12th European\nConference on Artificial Intelligence. ECAI \u201996, edited by W. Wahlster (Wiley, Chichester, 1996) p. 170. [6] M. Weigt and A. K. Hartmann, Phys. Rev. Lett. 84, 6118 (2000). [7] M. Davis, G. Logemann, and D. Loveland, Commun. ACM 5, 394 (1962). [8] R. Sedgewick, Algorithms in C (Addison-Wesley, Reading (MA), 1990). [9] C. H. Papadimitriou, in Foundations of Computer Science, Annual IEEE Symposium on, Vol. 32, edited by I. C. Society (IEEE Computer Society, Los Alamitos, CA, USA, 1991) pp. 163\u2013169. [10] J. Ardelius and E. Aurell, Phys. Rev. E 74, 037702 (2006). [11] M. Me\u0301zard, G. Parisi, and R. Zecchina, Science 297, 812 (2002). [12] M. Me\u0301zard and R. Zecchina, Phys. Rev. E 66, 056126 (2002). [13] M. Me\u0301zard, G. Parisi, and M. Virasoro, Spin glass theory and beyond (World Scientific, Singapore, 1987). [14] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization. Algorithms and Complexity (Dover Publications, 1998). [15] W. J. Cook, W. H. Cunningham, W. R. Pulleyblank, and A. Schriever, Combinatorial Optimization (Wiley, New York, 1998).\n[16] \u201cIBM ILOG CPLEX optimizer,\u201d . [17] T. Dewenter and A. K. Hartmann, Phys. Rev. E 86,\n041128 (2012). [18] N. Adler, A. S. Hakkert, J. Kornbluth, T. Raviv, and\nM. Sher, Ann. Oper. Res. 221, 9 (2014). [19] M. Safar, M. Taha, and S. Habib, in 2007 IEEE/ACS\nInternational Conference on Computer Systems and Applications (IEEE, 2007) pp. 592\u2013598. [20] E. Boros and V. Gurvich, Discrete Appl. Math. (2007). [21] L. E. Parker and B. A. Emmons, in Proceedings of Inter-\nnational Conference on Robotics and Automation, Vol. 3 (IEEE, 1997) pp. 2082\u20132089. [22] M. Weigt and A. K. Hartmann, Phys. Rev. Lett. 86, 1658 (2001). [23] M. Weigt and A. K. Hartmann, Phys. Rev. E 63, 056127 (2001). [24] M. Weigt and A. K. Hartmann, Europhys. Lett. 62, 533 (2003). [25] M. Bauer and O. Golinelli, Eur. Phys. J. B 24, 339 (2001). [26] W. Barthel and A. K. Hartmann, Phys. Rev. E 70, 066120 (2004). [27] P. R. Thie and G. E. Keough, An Introduction to Linear Programming and Game Theory (John Wiley & Sons, Hoboken, New Jersey, 2008). [28] R. M. Karp, in Complexity of computer computations (Springer, 1972) pp. 85\u2013103. [29] R. E. Gomory, Bull. Am. Math. Soc. 64, 275 (1958). [30] R. Parker and R. Rardin, Discrete Optimization (Aca-\ndemic Press, New York, 1988). [31] G. Cornue\u0301jols, Ann. Oper. Res. 149, 63 (2007). [32] R. E. Gomory, Ann. Oper. Res. 149, 99 (2007). [33] A. Caprara and M. Fischetti, Math. Program. 74, 221\n(1996).\n14\n[34] B. Yu\u0308ceoglu, Branch-and-cut algorithms for graph problems (Maastricht University, 2015). [35] G. Parisi, arXiv preprint cs/0312011 (2003). [36] A. Braunstein and R. Zecchina, J. Stat. Mech.: Theory\nExp. 2004, P06007 (2004). [37] S. Seitz, M. Alava, and P. Orponen, J. Stat. Mech.:\nTheory Exp. 2005, P06006 (2005). [38] E. Maneva, E. Mossel, and M. J. Wainwright, J. ACM\n54, 17 (2007). [39] M. Alava, J. Ardelius, E. Aurell, P. Kaski, S. Krishna-\nmurthy, P. Orponen, and S. Seitz, Proc. Natl. Acad. Sci. U.S.A. 105, 15253 (2008). [40] A. Braunstein, L. Dall\u2019Asta, G. Semerjian, and L. Zdeborova\u0301, J. Stat. Mech.: Theory Exp. 2016, 053401 (2016). [41] Y. Zhang, M. Zhang, G. Pu, F. Song, and J. Li, AI Commun. 31, 267 (2018). [42] J. Ardelius and L. Zdeborova\u0301, Phys. Rev. E 78, 040101 (2008). [43] M. R. Garey and D. S. Johnson, Computers and intractability (W.H. Freemann, San Francisco, 1979). [44] R. J. Dakin, Comput. J. 8, 250 (1965). [45] J. E. Kelley Jr., SIAM J. Appl. Math. 8, 703 (1960). [46] D. Gade and S. Ku\u0308c\u0327u\u0308kyavuz, Wiley Encyclopedia of Op-\nerations Research and Management Science , 1 (2013).\n[47] A. Zanette, M. Fischetti, and E. Balas, Math. Program. 130, 153 (2011). [48] W. Cook, S. Dash, R. Fukasawa, and M. Goycoolea, INFORMS J. Comput. 21, 641 (2009). [49] A. M. C. A. Koster, A. Zymolka, and M. Kutschka, Algorithmica 55, 375 (2009). [50] H. Schawe, R. Bleim, and A. K. Hartmann, PLOS ONE 14, e0215309 (2019). [51] M. Leone, F. Ricci-Tersenghi, and R. Zecchina, J. Phys. A: Math. Gener 34, 4615 (2001). [52] A. K. Hartmann, Big Practical Guide to Computer Simulations (World Scientific, Singapore, 2015). [53] P. Erdo\u030bs and A. Re\u0301nyi, Publ. Math. Inst. Hungar. Acad. Sci. 5, 17 (1960). [54] S. Khuller, SIGACT News 33, 31 (2002). [55] \u201clp solve version 5.5.2.5,\u201d . [56] \u201cPuLP,\u201d . [57] \u201cPython-MIP,\u201d . [58] J. Ramanujam and P. Sadayappan, in ICNN (1988) pp.\n325\u2013332. [59] I. K. Evans, in International Conference on Evolutionary\nProgramming (Springer, 1998) pp. 377\u2013386. [60] X. Chen, Z. Tang, X. Xu, S. Li, G. Xia, and J. Wang, in\nInternational Symposium on Neural Networks (Springer, 2004) pp. 430\u2013435."
        }
    ],
    "title": "Cutting-Plane Algorithms and Solution Whitening for the Vertex-Cover Problem",
    "year": 2022
}