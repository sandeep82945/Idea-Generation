{
    "abstractText": "We use the Lambek Calculus with soft sub-exponential modalities to model and reason about discourse relations such as anaphora and ellipsis. A semantics for this logic is obtained by using truncated Fock spaces, developed in our previous work. We depict these semantic computations via a new string diagram. The Fock Space semantics has the advantage that its terms are learnable from large corpora of data using machine learning and they can be experimented with on mainstream natural language tasks. Further, and thanks to an existing translation from vector spaces to quantum circuits, we can also learn these terms on quantum computers and their simulators, such as the IBMQ range. We extend the existing translation to Fock spaces and develop quantum circuit semantics for discourse relations. We then experiment with the IBMQ AerSimulations of these circuits in a definite pronoun resolution task, where the highest accuracies were recorded for models when the anaphora was resolved.",
    "authors": [],
    "id": "SP:1ef6de254f5dcbecdec5296b441c7b1c46a88594",
    "references": [
        {
            "authors": [
                "Mehrnoosh Sadrzadeh Bob Coecke",
                "Edward Grefenstette"
            ],
            "title": "Lambek vs. lambek: Functorial vector space semantics and string diagrams for lambek calculus",
            "venue": "Ann. Pure and Applied Logic,",
            "year": 2013
        },
        {
            "authors": [
                "Alexandre Cl\u00e9ment",
                "Nicolas Heurtel",
                "Shane Mansfield",
                "Simon Perdrix",
                "Ben\u00f4\u0131t Valiron"
            ],
            "title": "Lov-calculus: A graphical language for linear optical quantum circuits, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Giovanni de Felice",
                "Bob Coecke"
            ],
            "title": "Quantum linear optics via string diagrams, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Giovanni de Felice",
                "Alexis Toumi",
                "Bob Coecke"
            ],
            "title": "DisCoPy: Monoidal categories in python",
            "venue": "Electronic Proceedings in Theoretical Computer Science,",
            "year": 2021
        },
        {
            "authors": [
                "Vojt\u011bch Hav\u013a\u0131\u010dek",
                "Antonio D. C\u00f3rcoles",
                "Kristan Temme",
                "Aram W. Harrow",
                "Abhinav Kandala",
                "Jerry M. Chow",
                "Jay M. Gambetta"
            ],
            "title": "Supervised learning with quantumenhanced feature spaces",
            "venue": "Nature, 567(7747):209\u2013212,",
            "year": 2019
        },
        {
            "authors": [
                "E. J"
            ],
            "title": "Humphreys. introduction to lie algebras and representation theory",
            "venue": "SpringerVerlag,",
            "year": 1972
        },
        {
            "authors": [
                "G. J\u00e4ger"
            ],
            "title": "A multi-modal analysis of anaphora and ellipsis",
            "venue": "University of Pennsylvania Working Papers in Linguistics,",
            "year": 1998
        },
        {
            "authors": [
                "G. J\u00e4ger"
            ],
            "title": "Anaphora and type logical grammar, volume 24",
            "venue": "Springer Science  Business Media,",
            "year": 2006
        },
        {
            "authors": [
                "M. Kanovich",
                "S. Kuznetsov",
                "V. Nigam",
                "A. Scedrov"
            ],
            "title": "Soft Subexponentials and Multiplexing",
            "venue": "In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),",
            "year": 2020
        },
        {
            "authors": [
                "M. Kanovich",
                "S. Kuznetsov",
                "A. Scedrov"
            ],
            "title": "Undecidability of the Lambek calculus with a relevant modality",
            "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),",
            "year": 2016
        },
        {
            "authors": [
                "Dimitri Kartsaklis",
                "Ian Fan",
                "Richie Yeung",
                "Anna Pearson",
                "Robin Lorenz",
                "Alexis Toumi",
                "Giovanni de Felice",
                "Konstantinos Meichanetzidis",
                "Stephen Clark",
                "Bob Coecke"
            ],
            "title": "lambeq: An efficient high-level python library for quantum nlp, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lafont"
            ],
            "title": "Soft linear logic and polynomial time",
            "venue": "Theoretical Computer Science,",
            "year": 2004
        },
        {
            "authors": [
                "Joachim Lambek"
            ],
            "title": "The Mathematics of Sentence Structure",
            "venue": "The American Mathematical Monthly,",
            "year": 1958
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern"
            ],
            "title": "The winograd schema challenge",
            "venue": "In Proceedings of the International Workshop on Temporal Representation and Reasoning,",
            "year": 2012
        },
        {
            "authors": [
                "Robin Lorenz",
                "Anna Pearson",
                "Konstantinos Meichanetzidis",
                "Dimitri Kartsaklis",
                "Bob Coecke"
            ],
            "title": "Qnlp in practice: Running compositional models of meaning on a quantum computer, 2021",
            "year": 2021
        },
        {
            "authors": [
                "L. McPheat",
                "H. Wazni",
                "M. Sadrzadeh"
            ],
            "title": "Vector space semantics for lambek calculus with soft subexponentials",
            "venue": "In Proceedings of the tenth international conference on Logical Aspect of Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Konstantinos Meichanetzidis",
                "Stefano Gogioso",
                "Giovanni De Felice",
                "Nicol\u00f2 Chiappori",
                "Alexis Toumi",
                "Bob Coecke"
            ],
            "title": "Quantum natural language processing on near-term quantum computers, 2020",
            "year": 2020
        },
        {
            "authors": [
                "M. Moortgat"
            ],
            "title": "Multimodal linguistic inference",
            "venue": "Journal of Logic, Language and Information,",
            "year": 1996
        },
        {
            "authors": [
                "Glyn Morrill",
                "Oriol Valent\u0301\u0131n"
            ],
            "title": "Computational coverage of tlg: Nonlinearity",
            "venue": "In Proceedings of NLCS\u201915. Third Workshop on Natural Language and Computer Science,",
            "year": 2015
        },
        {
            "authors": [
                "Glyn Morrill",
                "Oriol Valent\u0301\u0131n"
            ],
            "title": "On the logic of expansion in natural language. In Logical Aspects of Computational Linguistics. Celebrating 20 Years of LACL",
            "venue": "9th International Conference,",
            "year": 1996
        },
        {
            "authors": [
                "Altaf Rahman",
                "Vincent Ng"
            ],
            "title": "Resolving complex cases of definite pronouns: The winograd schema challenge",
            "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
            "year": 2012
        },
        {
            "authors": [
                "M. Sadrzadeh",
                "M. Moortgat",
                "G. Wijnholds"
            ],
            "title": "A frobenius algebraic analysis for parasitic gaps",
            "venue": "In Workshop on Semantic Spaces at the Intersection of NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Mehrnoosh Sadrzadeh",
                "Stephen Clark",
                "Bob Coecke"
            ],
            "title": "The frobenius anatomy of word meanings i: subject and object relative pronouns",
            "venue": "Journal of Logic and Computation,",
            "year": 2013
        },
        {
            "authors": [
                "Mehrnoosh Sadrzadeh",
                "Stephen Clark",
                "Bob Coecke"
            ],
            "title": "The Frobenius anatomy of word meanings II: Possessive relative pronouns",
            "venue": "Journal of Logic and Computation,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Shepherd",
                "Michael J. Bremner"
            ],
            "title": "Temporally unstructured quantum computation",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,",
            "year": 2009
        },
        {
            "authors": [
                "J.C. Spall"
            ],
            "title": "Implementation of the simultaneous perturbation algorithm for stochastic optimization",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems,",
            "year": 1998
        },
        {
            "authors": [
                "Gijs Wijnholds",
                "Mehrnoosh Sadrzadeh"
            ],
            "title": "Classical copying versus quantum entanglement in natural language: The case of VP-ellipsis",
            "venue": "Electronic Proceedings in Theoretical Computer Science,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Seminal work of Lambek in 1958 [15] showed that the simple logic of concatenation and its residuals form a Syntactic Calculus. This calculus, referred to as\nHadi Wazni University College London, e-mail: hadi.wazni.20@ucl.ac.uk\nKin Ian Lo University College London e-mail: kin.lo.20@ucl.ac.uk\nLachlan McPheat University College London e-mail: l.mcpheat@ucl.ac.uk\nMehrnoosh Sadrzadeh University College London e-mail: m.sadrzadeh@ucl.ac.uk\n1\nar X\niv :2\n20 8.\n05 39\n3v 1\n[ cs\n.C L\n] 1\n0 A\nug 2\nthe Lambek Calculus, could model and reason about grammatical structures of sentences of natural language. The atomic formulae of the logic model basic grammatical types, e.g. noun phrases n and declarative sentences s. Concatenations of formulae model compositions of types.\nSubsequent work of Moortgat in 1996 [20], and Jaeger in 1998 [9], Morrill in 2015, 2016 [21, 22], and Kanovich et. al in 2016, 2020 [12, 11] showed that adding modalities to the Lambek calculus increases its expressive power. Whereas Moortgat used these modalities to restrict associativity, Morrill was in favour of using them for island types and iterative conjunctives. Kanovitch et. al focused on parasitic gaps. All these models worked at the sentence level. The work of Jaeger, on the other hand, extended the application of Lambek Calculus to the discourse level and to model the relationship between two or more sentences. An example of such a relationship is coreference, that is, when words from different sentences refer to each other. This phenomena is observed in anaphora, when a pronoun such as \u2018he\u2019 refers to a noun phrase, such as \u2018John\u2019, in the discourse \u2018John slept. He snored.\u2019. Another example of coreference is ellipsis, when an ellipsis marker such as \u2018too\u2019 refers to a verb phrase such as \u2018slept\u2019 in the discourse \u2018John slept. Bill did too\u2019.\nIn previous work [18], we combined Jaeger\u2019s idea with Lambek Calculus with Soft Sub-Exponentials introduced by Kanovitch et. al in [11], since it had better meta logical properties, i.e. finite rules, cut elimination, strong normalisation, and decidability. We showed how it can model and reason about anaphora and ellipsis and developed a finite dimensional vector spaces for it. This development was in the style of the vector space semantics of Lambek calculus [3], but with a novel feature. The soft sub-exponentiated formulae were interpreted as truncated Fock spaces. A Fock space is the direct sum of all tensor powers of a vector space. Fixing the field to be F, the Fock space over a vector space V is\nF\u2295 V \u2295 (V \u2297 V )\u2295 (V \u2297 V \u2297 V \u2297 V )\u2295 \u00b7 \u00b7 \u00b7\nA truncated version of this space is obtained restricting the Fock space to its k0\u2019th tensor power, for k0 a fixed bound, defined as follows:\nF\u2295 V \u2295 (V \u2297 V )\u2295 (V \u2297 V \u2297 V \u2297 V )\u2295 (V \u2297 V \u2297 \u00b7 \u00b7 \u00b7 \u2297 V )\ufe38 \ufe37\ufe37 \ufe38 k0 .\nThe bound k0 is fixed by the logic. The soft sub-exponentiated formulae of the logic !A can be thought of as storages of formulae, which only contain k0 copies of a formula A. Access to these copies is obtained by a ! elimination rule in the logic and by projection to the right level of a truncated Fock space in the semantics. When modelling coreference relations such as anaphora, only a fixed number of pronouns refer to a noun phrase in any given discourse. An upper bound can easily be determined from these fixed numbers, e.g. by averaging.\nHistorically, Lambek Calculus has enjoyed a relational semantics and a question arises that \u2018what is the benefit of working with a vector space semantics?\u2019. The answer comes from recent advances in Natural Language Processing, where vector semantics are learnable via machine learning algorithms such as neural networks. Having a vector space semantics for Lambek Calculus and its modal extensions enables us to work with machine learnt vector representations of words, sentences, and discourse units in a structured way. In practice, however, these semantics rely on higher order tensors building which is computationally costly. A recent line of research [19, 17] known as Quantum Natural Language Processing (QNLP) argues that this problem is solvable by using Quantum computers. Tensors are native to Quantum computers and can be learnt by them using less resources.\nWe use the vector space-to-Quantum circuit translation of [19, 17] and turn our truncated Fock space semantics into Quantum circuits using a novel string diagrammatic calculus that we develop for truncated Fock spaces. We then use the DisCoPy and Lambeq tools [6, 13] to learn the parameters of these circuits on a definite pronoun anaphora resolution task inspired by the Winograd Schema Challenge [16]. This challenge consists of sentences with ambiguous anaphoric reference relations, where a definite pronoun in the second sentence can either refer to the subject or the object of the first sentence. The goal of this task is to disambiguate the reference relation. We implement the task as a classification task and train a model using the IBMQ AerSimulator [2]. We implemented eight different models in which we experimented with presence or lack of grammatical structure and discourse structure. All the models converged but the highest accuracies were recorded for models where anaphora is resolved, irregardless of the model encoding the grammatical structure or not. The lowest accuracies were observed for models when anaphora was not resolved and there was no notion of grammar.\nThe current results are obtained over a small dataset of 144 entries where each grammatical type is only modelled by a one qubit quantum state. Experimenting with a large scale dataset and solidifying these results is work in progress."
        },
        {
            "heading": "2 Lambek calculus with soft Subexponentials (SLLM) and its applications to modelling discourse structure",
            "text": "The formulae of Lambek calculus with soft Subexponentials, in short SLLM are generated using the following BNF.\nA,B ::= A \u2208 At | A \u00b7B | A\\B | A/B |!A | \u2207A\nThe set of atoms can be any set of indices. For the linguistic application purposes of this paper, we set the atoms to be {s, n} with s representing the\ngrammatical \u2018declarative sentence\u2019 and n the \u2018noun phrase\u2019. With the above BNF over this set of atoms we can generate the usual set of complex types, including the formula for an adjective n/n, the formula for an intransitive verb n\\s and the formula for a transitive verb n\\s/n.\nGiven the types of the calculus, we define its sequents to be pairs, written \u0393 \u2212\u2192 A, where \u0393 is a finite list of formulas, \u0393 = A1, A2, . . . , An and A is a formula. Given this notion of sequents, we define the logical rules of SLLM in a Gentzen calculus style below.\nIn linguistic applications, \u0393 often represents the grammatical types of a string of words, and A is the result of the composition of these types. An example of a grammatical rule of English is that a noun phrase such as (John, composes with an intransitive verb such as sleeps to form a sentence, that is John sleeps. This is modelled in SLLM via sequent n, n\\s \u2212\u2192 s, and its proof tree, which is as follows:\nn \u2212\u2192 n s \u2212\u2192 s n, n\\s \u2212\u2192 s \\L (1)\nThe modality ! is known as a soft sub-exponential. The banged formulae are thought of as storages and the !-modality itself as a projection operation.\nReading !L-structural rule from bottom to top, we are projecting from the storage !A which contains k0 formulae A, to n copies of A, for 1 \u2264 n \u2264 k0. This allows for the existence of a controlled implicit notion of copying in the syntax, which is not the same as the usual on-the-nose notion of copying: we are not replacing !A with copies of !A nor are we replacing A with copies of A. We are just projecting from a storage containing many copies of A to a smaller number of those copies. This implicit notion of copying comes from Soft Linear Logic [14] has a neat interpretation in the vector space semantics as we show in 3. The second modality, \u2207, is the one that allows its formulas to be permuted. This is seen in the rules perm and perm\u2032 in table 1.\nThe bounding of n is strictly necessary, as having an unbounded number of copies makes the calculus\u2019 derivability problem undecidable. This bound is not a problem in terms of modelling language, as the bound corresponds to the number of times one may refer to something in a discourse. A bad but symbolic bound then would be the number of words in the discourse you want to analyse.\nThe way we model discourse phenomena such as anaphora and ellipsis is similar to that of Ja\u0308ger\u2019s [9, 10]. In Ja\u0308ger\u2019s work the word that is being referred to is explicitly copied and moved to the site of the referring word, where it gets applied. In our framework, however, we do not have explicit copying and use a storage type for the word that is being referred to. For example, if we wish to model the discourse John sleeps. He snores., we need to first assign a storage type to John, then project two copies from it, and move one to the site of the type of He. Then apply the type of He to the moved copy of John. So we assign the type !\u2207n to John, and assign the type \u2207n\\n to the pronoun He. This latter represents a function which takes a projected-from-a-storage noun as its (left)input and returns a noun as its outputs. This assignment is summarised below:\n{(John : !\u2207n), (sleeps : n\\s), (He : \u2207n\\n), (snores : n\\s)}.\nThe discourse structure of John sleeps. He snores. is modelled by the following SLLM proof tree:\nn \u2212\u2192 n \u2207n \u2212\u2192 \u2207n\nn \u2212\u2192 n s, s \u2212\u2192 s, s s, n, n\\s \u2212\u2192 s, s \\L\ns, \u2207n, \u2207n\\n, n\\s \u2212\u2192 s, s \\L\nn, n\\s, \u2207n, \u2207n\\n, n\\s \u2212\u2192 s, s \\L\n\u2207n, n\\s, \u2207n, \u2207n\\n, n\\s \u2212\u2192 s, s \u2207L \u2207n, \u2207n, n\\s, \u2207n\\n, n\\s \u2212\u2192 s, s perm\n!\u2207n, n\\s, \u2207n\\n, n\\s \u2212\u2192 s, s !L\nReading the proof from bottom to top, we see that the application of !L replaces the type of John (!\u2207n) with two copies of John (\u2207n). The application of perm moves one of the copies next to He. The following \\L application identifies He with John, and the application of \u2207L \u2018forgets\u2019 that the other\ncopy of John is a copy. The sequent above the application of \u2207L corresponds to the typing of John sleeps. John snores. and the proof of it shows that this is indeed two sentences.\nIn order to show that the system can model more complex example, we consider the anaphoric reference: The ball hit the window and Bill caught it, directly from the pronoun resolution dataset [23]. Here, it refers to The ball. For brevity we may type the whole noun phrase The ball as !\u2207n, and the noun phrase the window as n1 , and as usual we type and as s\\s/s and both verbs hit and caught as n\\s/n. Finally, we type the pronoun it as \u2207n\\n. Note that this example consists of only one sentence. One can easily break replace the and with a full stop and work with two sentences. Since we already have a two-sentence example, we will model the one-sentence case, which is modelled in the following proof tree.\n\u2207n \u2212\u2192 \u2207n\nn \u2212\u2192 n n \u2212\u2192 n\ns \u2212\u2192 s n \u2212\u2192 n\nn \u2212\u2192 n s \u2212\u2192 s s \u2212\u2192 s s/s, s \u2212\u2192 s /L\ns/s, s/n, n \u2212\u2192 s /L\ns/s, n, n\\s/n, n \u2212\u2192 s \\L\ns, s\\s/s, n, n\\s/n, n \u2212\u2192 s \\L\ns/n, n, s\\s/s, n, n\\s/n, n \u2212\u2192 s /L\nn, n\\s/n, n, s\\s/s, n, n\\s/n, n \u2212\u2192 s \\L\n\u2207n, n\\s/n, n, s\\s/s, n, n\\s/n, n \u2212\u2192 s \u2207L\n\u2207n, n\\s/n, n, s\\s/s, n, n\\s/n,\u2207n,\u2207n\\n \u2212\u2192 s \\L \u2207n, n\\s/n, n, s\\s/s, n,\u2207n, n\\s/n,\u2207n\\n \u2212\u2192 s perm \u2207n, n\\s/n, n, s\\s/s,\u2207n, n, n\\s/n,\u2207n\\n \u2212\u2192 s perm \u2207n, n\\s/n, n,\u2207n, s\\s/s, n, n\\s/n,\u2207n\\n \u2212\u2192 s perm \u2207n, n\\s/n,\u2207n, n, s\\s/s, n, n\\s/n,\u2207n\\n \u2212\u2192 s perm \u2207n,\u2207n, n\\s/n, n, s\\s/s, n, n\\s/n,\u2207n\\n \u2212\u2192 s perm\n!\u2207n, n\\s/n, n, s\\s/s, n, n\\s/n,\u2207n\\n \u2212\u2192 s !L\n(2) Ellipsis is modelled similarly. Since we have only experimented with anaphoric reference in the current paper, we refer the reader for the typing and proof tree of an example, e.g. John plays guitar. Mary does too. to previous work [18].\nThe dog broke the vase. It was clumsy, which naturally has type s \u00b7 s in the following proof:\nthe dog :!\u2207n, broke : n\\s/n, the vase : n, It : \u2207n\\n, was : n\\s/(n/n), clumsy : n/n,\n1 The modelling can me made be more precise by typing all instances of the to n/n and typing ball to !\u2207n and window to n.\nn \u2212\u2192 n \u2207n \u2212\u2192 \u2207n \u2207R\nn \u2212\u2192 n n \u2212\u2192 n\nn \u2212\u2192 n\nn \u2212\u2192 n n \u2212\u2192 n n/n, n \u2212\u2192 n /L\nn/n \u2212\u2192 n/n /R s \u2212\u2192 s s \u2212\u2192 s s, s \u2212\u2192 s \u00b7 s \u00b7R\ns, s/(n/n), n/n \u2212\u2192 s \u00b7 s s, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \\L\ns/n, n, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s /L\nn, n\\s/n, n, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \\L\n\u2207n, n\\s/n, n, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \u2207L\n\u2207n, n\\s/n, n,\u2207n,\u2207n\\n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \\L \u2207n, n\\s/n,\u2207n, n,\u2207n\\n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s perm \u2207n,\u2207n, n\\s/n, n,\u2207n\\n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s perm\n!\u2207n, n\\s/n, n,\u2207n\\n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s !L\n(3) Similarly we can resolve object-anaphora, as in the example: The cat broke\nthe glass. It was fragile.\nthe cat : n, broke : n\\s/n, the glass :!\u2207n, It : \u2207n\\n, was : n\\s/(n/n), fragile : n/n,\n\u2207n \u2212\u2192 \u2207n\nn \u2212\u2192 n n \u2212\u2192 n\nn \u2212\u2192 n\nn \u2212\u2192 n n \u2212\u2192 n n/n, n \u2212\u2192 n /L\nn/n \u2212\u2192 n/n /R s \u2212\u2192 s s \u2212\u2192 s s, s \u2212\u2192 s \u00b7 s \u00b7R\ns, s/(n/n), n/n \u2212\u2192 s \u00b7 s /L\ns, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \\L\ns/n, n, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s /L\nn, n\\s/n, n, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \\L\nn, n\\s/n,\u2207n, n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \u2207L\nn, n\\s/n,\u2207n,\u2207n,\u2207n\\n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s \\L\nn, n\\s/n, !\u2207n,\u2207n\\n, n\\s/(n/n), n/n \u2212\u2192 s \u00b7 s !L\n(4)"
        },
        {
            "heading": "3 Truncated Fock space semantics of SLLM",
            "text": "In this section we recall the definition of the vector space semantics of SLLM, first defined in [18]. We define the semantics inductively on formulas, which are interpreted as vector spaces and proofs, which are interpreted as linear maps. We will use a semantic bracket notation [[ ]] : SLLM \u2192 FdVect to denote the semantics of formulas [[A]] and the semantics of proofs [[\u03c0]], where\nif \u03c0 is a proof of a sequent \u0393 \u2212\u2192 A, then [[\u03c0]] is a linear map from [[\u0393 ]] to [[A]].\n1. For the atomic formulas n, s we interpret them as some fixed vector spaces N := [[n]] and S := [[s]]. 2. Formulas of the form A \u00b7 B are interpreted using the tensor product [[A \u00b7 B]] := [[A]]\u2297 [[B]]. 3. Formulas of the form A\\B are interpreted using the vector space dual [[A\\B]] := [[A]]\u2217 \u2297 [[B]]. Similarly, we have [[B/A]] := [[B]]\u2297 [[A]]\u2217. 4. \u2207-formulas are interpreted trivially [[\u2207A]] := [[A]]. 5. !-formulas are interpreted using truncated Fock-spaces [[!A]] := Tk0 [[A]],\nwhere\nTk0 [[A]] = k0\u2295 i=0 [[A]]\u2297i = k\u2295[[A]]\u2295([[A]]\u2297[[A]])\u2295([[A]]\u2297[[A]]\u2297[[A]])\u2295\u00b7 \u00b7 \u00b7\u2295[[A]]\u2297k0 .\nAs a notational convenience, we will write [[\u0393 ]] for [[A1]]\u2297 [[A2]]\u2297\u00b7 \u00b7 \u00b7\u2297 [[An]] for \u0393 = A1, . . . , An.\nThe vector space semantics of proofs is as follows:\n1. The axiom A \u2212\u2192 A is simply interpreted as the identity matrix I[[A]] : [[A]]\u2192 [[A]]. 2. The \u00b7L rule is trivially interpretted, as the semantics does not distinguish between the , and the \u00b7. 3. The \u00b7R rule is interpreted as the tensor product of linear maps. That is, given proofs of the hypotheses of the \u00b7R-rule \u03c01 of \u03931 \u2212\u2192 A and \u03c02 of \u03932 \u2212\u2192 B we have the semantics of the new proof, ending with the \u00b7R rule as [[\u03c01]]\u2297 [[\u03c02]]. 4. The \\L and /L-rules are interpreted as application. That is, given proofs \u03c0 of \u0393 \u2212\u2192 A and \u03c4 of \u03a31, B,\u03a32 \u2212\u2192 C we have the semantics of the proof ending with the \\L-rule being\n[[\u03c4 ]] \u25e6 (I[[\u03a31]] \u2297 ev l [[A]],[[B]] \u2297 I[[\u03a32]]) \u25e6 (I[[\u03a31]] \u2297 [[\u03c0]]\u2297 I[[A\\B]] \u2297 I[[\u03a32]]).\nSimilarly, for the /L-rule with the same \u03c0 and \u03c4 as above, we have that the semantics of the new proof, now ending with /L is\n[[\u03c4 ]] \u25e6 (I[[\u03a31]] \u2297 ev r [[A]],[[B]] \u2297 I[[\u03a32]]) \u25e6 (I[[\u03a31]] \u2297 I[[B/A]] \u2297 [[\u03c0]]\u2297 I[[\u03a32]]).\n5. The \\R and /R-rules are interpreted as currying. That is, given a proof \u03c0 of A,\u0393 \u2212\u2192 B, the semantics of the proof beginning with \u03c0 and ending with \\R is the curried version of [[\u03c0]], which we denote as\n\u039bl[[\u03c0]] : [[\u0393 ]]\u2192 [[A]]\u2297 [[B]].\nSimilarly for the /R-rule, if we assume the sequent \u0393,A \u2212\u2192 B has proof \u03c4 , the new proof ending in /R has semantics\n\u039br[[\u03c4 ]] : \u0393 \u2192 [[B]]\u2297 [[A]].\n6. Both the \u2207L and \u2207R-rules are trivial, since we interpret \u2207 trivially in the vector space semantics. 7. The perm and perm\u2032-rules are interpreted using the symmetry, \u03c3, of the tensor product. Recall that for any two vector spaces V,W we have that \u03c3V,W : V \u2297W \u223c= W \u2297 V . Using this symmetry we can interpret a proof \u03c0 followed by the perm-rule as [[\u03c0]] \u25e6 (I[[\u03931]] \u2297 \u03c3[[A]],[[\u03932]] \u2297 I[[\u03933]]) 8. The !L-rule, the one that lets us copy, is interpreted using the projections from the truncated Fock space. Recall that whenever you have a direct sum V \u2295W of vector spaces V,W we have two canonical projections, namely pV : V \u2295 W \u2192 V and pW : V \u2295 W \u2192 W defined as pV (v, w) = v and pW (v, w) = w. These projections extend to any number of summands, which in our case is k0. Thus, if we consider a proof \u03c0 of a sequent \u03931, A,A, . . . , A, \u03932 \u2212\u2192 B (with n instances of A, say) which is followed by an application of the !L-rule, the semantics of the whole proof becomes\n[[\u03c0]] \u25e6 (I[[\u03931]] \u2297 pn \u2297 I[[\u03932]])\nwhere pn : Tk0 [[A]]\u2192 [[A]]\u2297n, is the nth projection map. 9. The !R-rule is interpreted as the application of Tk0 . That is, given a proof \u03c0 of the sequent A \u2212\u2192 B, which is followed by an application of !R to give !A \u2212\u2192!B, the semantics of the whole proof is Tk0 [[\u03c0]]. By this notation we mean the following map\n(Tk0 [[\u03c0]])( j\u2297 i=0 aji ) k0 j=0 := ( j\u2297 i=0 [[\u03c0]]aji ) k0 j=0\nwhere a0 is some element in the ground field, and a j i \u2208 [[A]] for all 1 \u2264 i \u2264 j \u2264 k0. That Tk0 [[\u03c0]] yields a linear map is not obvious, but it is a well known fact proven in any text on universal enveloping algebras, for instance [8]."
        },
        {
            "heading": "4 Diagrammatic Computations",
            "text": ""
        },
        {
            "heading": "4.1 Existing String Diagrammatic Calculus",
            "text": "We recall the standard graphical language for finite dimensional vector spaces. Vector spaces are denoted by labelled strings, as in figure 1a where we have drawn a vector space V . By convention, the 1-dimensional vector space is not drawn at all. The tensor product of two vector spaces is denoted by placing the corresponding strings side-by-side as in figure 1b, where we have drawn V \u2297W .\nLinear maps f : V \u2192 W are denoted as boxes on strings, with their domain feeding into the box from above, and the codomain coming out from below, as in figure 2a. The composition of linear maps is denoted by vertical superposition of boxes, as in 2b, where we have drawn the composition of maps f : V \u2192 W and g : W \u2192 U . The tensor product of linear maps is depicted by horizontal juxtaposition, as in figure 2c\nVectors v \u2208 V are in bijection with linear maps k \u2192 V , so we may think of vectors as a special kind of linear functions. Since we do not draw the ground field at all we may draw vectors as boxes with no output, as in figure 3a. We orient these boxes as we will need to manipulate diagrams with elementboxes and in doing so it is useful to keep track of which way round your diagram is drawn. Note that a linear functional, i.e. a linear map V \u2192 k is a box with no output, and so we may draw it as in figure 3b. We draw it with the opposite orientation to illustrate the self-duality of FdVect, where every vector v \u2208 V defines a functional via the inner product (v,\u2212) : V \u2192 k which maps w 7\u2192 (v, w).\nThe symmetry of the tensor product, i.e. V \u2297W \u223c= W \u2297 V for any vector spaces V,W , is drawn by crossing wires as in figure 4.\nInner products are depicted by cups, as drawn in 5. Dually, we have caps, which are maps k \u2192 V \u2297 V corresponding to the unit map 1 7\u2192 \u2211n i=1 vi \u2297 vi where we take V to have basis {vi}i=1,...,n.\nThese diagrams satisfy the usual string diagrammatic equations, e.g. the most important of which is the following known as yanking 6."
        },
        {
            "heading": "4.2 New Diagrams for Fock Spaces",
            "text": "Next we introduce now diagrams for our Fock spaces. We depict Fock spaces, i.e. vector spaces of the form Tk0V by bold strings, labelled with a V , see figure 7. The special diagrammatic structure we have on Fock spaces is the projection to the n-th layer, which is denoted by the usual linear map notation, which in this case we label by pn, and call a pn-box, as in figure 7.\nSimilar to vectors from vector spaces, vectors from Fock spaces Tk0V are depicted with linear maps v: k \u2192 Tk0V , see figure 8.\nA series of operation often performed on Fock spaces is accessing an element via the above linear map, followed by a projection to the n-th layer, see 9.\nAs an example, here is the diagram of the discourse \u2018John slept. He snored.\u2019."
        },
        {
            "heading": "5 Translating String Diagrams to Quantum Circuits",
            "text": "In [17] the process of representing a sentence as a quantum circuit is described in four steps. We apply these steps to the process of representing a discourse as a quantum circuit, where they become as follows:\n1. A discourse is parsed into a proof in SLLM. 2. The proof tree is turned into a string diagram. 3. The string diagram is simplified according to the rewrite rules of Lambeq,\nwhere we use an existing rule to replace the verb to be with a cap and introduce a new rule which also replaces the pronoun with a cap. 4. The simplified diagram is then normalised, again using Lambeq, where cups are removed, wires are stretched and boxes are rearranged; all of these are for the purpose of a faster execution on the IBMQ simulator and backends. 5. The resulting simplified and normalised diagrams are transformed into a quantum circuit based on a specific parameterisation scheme and choice of ansa\u0308tze2.\nWe elaborate on this process. The translation between string diagrams and quantum circuits is summarised in figure 11. Here, a triangle with a 0 in it, is a 0-state qubit. A box with an H in it is a Hadamard gate. We have only two operations that can act on a 0-state qubit after a Hadamard operation. One of them is a CNOT gate which is depicted with a dot connected to an \u2295. The other one is a controlled-Z-rotation on an angle, depicted as a box labelled as R\u03b1(\u03b8i) connected to a control qubit, where \u03b1 can be x, y or x and \u03b8 any angle from 0 to 2\u03c0. These are the native gates of the IBMQ quantum computers.\n2 A map that determines choices such as the number of qubits that every wire of a string diagram is associated with and the concrete parameterised quantum states that correspond to each word. For our experiments, we represented the noun wires and sentence wires by a one-qubit system.\nThe above steps only operate at the sentence level. We need Fock spaces to model the between-sentences discourse relations. Fock spaces do not have counterparts in the quantum circuits implemented by the IBMQ range. We observe that in the string diagrams of the discourse phenomena, Fock spaces are only manipulated using a series of two operations: an access to an element of a Fock space, followed by a projection to an n-th layer. We replace this sequence of operations by an order n tensor. This is depicted in Figure 12. Formally speaking, by doing so, we are restricting ourselves to only the n-th layer of a Fock space, which is nothing but an order n tensor, for which we have a quantum circuit counterpart in IBMQ.\nThis is mitigated by simplifying the string diagrams. For instance, a discourse with a subject anaphoric relation such as \u2018The dog broke the vase. It was clumsy.\u2019, is depicted into the string diagram of Figure 13 while an object anaphoric relation is depicted as in Figure 14.\nThe diagrams in Figures 13 and 14 are then simplified and normalised into the diagrams in Figure 15 and Figure 16 3.\n3 Although there are still cups in Figure 16, this is the most optimised version of the diagram. Removing the remaining two cups, stretching the wires, and rearranging the boxes will result in Figure 17, which not only has a cap and a cup, but also a crossing, thus is far from optimised.\nbroke\nclumsy\nvasedog\nN\nS\nN\nS\nN\nFig. 15\nbroke\ncat glass\nfragile\nN N\nS\nN N\nN S\nFig. 16\nThe simplified and normalised diagrams are then translated into the circuits in Figure 18 and in Figure 19 respectively.\n0\nH\n0\nH\nRz(\u2212\u03b81)\n0\nH\n0\nH\n0\nH\nRz(\u03b82)\n0\nH\nRx(\u2212\u03b84)\nRz(\u2212\u03b83)\n0\nRx(\u2212\u03b86)\nRz(\u2212\u03b85)\n0\nH\nRz(\u03b87)\nFig. 18\n0\nH\nRx(\u2212\u03b83)\n0\nRx(\u2212\u03b81)\nRz(\u2212\u03b82)\n0\nH\nRz(\u03b84)\n0\nH\nRz(\u03b85)\n0\nH\n0\nH\n0\nH\nRz(\u03b86)\n0\nH\n0 0\nH\n0\n0\nH\nRz(\u03b87)\nFig. 19"
        },
        {
            "heading": "6 Experiments",
            "text": "We train binary classification models to predict whether a pronoun refers to a subject or an object of a sentence. This training is a classical-quantum hybrid training scheme where a quantum computer is responsible for computing the meaning of the sentence by connecting the quantum states in a quantum\ncircuit. A classical computer is used to compute the loss function of the training. In each iteration, a new set of quantum states is computed based on the loss function of the previous iteration. We use DisCoPy[6] to produce diagrams and circuits and lambeq[13] to train our models. The code and the dataset are publicly available here45."
        },
        {
            "heading": "6.1 Dataset",
            "text": "We use the BERT6 large model with whole word masking to generate our dataset. This model uses the context words surrounding a mask token to predict what the masked word should be. We use it to to generate the words of the entries of our dataset.\nEach entry of the dataset contains a pair of sentences (S1, S2). The first sentence S1 is of the form Subject1 Verb1 Object and the second sentence S2 is of the form Subject2 Verb2 Adjective. The first sentence contains two candidate referents (Subject1, Object) and the second sentence contains one referring pronoun (Subject2 ). The pronoun agrees in gender, number, and semantic class with both referents and can in principle refer to either, but only one of the two reference relations is correct. If the referent relation is between the pronoun and Subject1, we have a case of subject anaphora and if it is between the pronoun and the Object, it is object anaphora. These two cases are the two classes of our binary classification task.\nWe start with an initial sentence The girls ate the apples. They were hungry. from the definite pronoun resolution dataset of [23]. We first mask the subject (girls) and let the model predict new subjects (men, children). We then mask the object (apples) and let the model predict new objects (cookies, pancakes). Next, we mask the verbs ate, were and the adjective (hungry) and apply a similar process to these to obtain more verbs and adjectives.\nThe final dataset has a vocabulary of size 16, It consists of 144 pairs of sentences. We divide these to 72 pairs for training, 36 for testing, and 36 for validation. Sentences with subject anaphora are labeled 0, while sentences with object anaphora are labeled 1. The dataset is balanced with respect to the two classes. A snapshot of the entries of the dataset is presented in Table 2.\n4 https://github.com/hwazni/anaphora 5 https://github.com/kinianlo/discopro/ 6 https://huggingface.co/bert-large-cased-whole-word-masking"
        },
        {
            "heading": "6.2 Preprocessing for binary classification",
            "text": "The two sentences (S1, S2) of each entry of the dataset are combined together to create a single output quantum state. This single state will be used as the input to our binary classifier. In principle, this can be any quantum map that takes two sentences as inputs and gives a sentence as the output. We experimented with two choices for this quantum map:\n1. A spider, implemented as a CNOT gate on a quantum circuit, which encodes a commutative Frobenius multiplication. The resulting combined quantum circuit is denoted by S1 S2. 2. A general quantum gate parameterised by the IQP ansatz which contains a controlled-Z rotation whose angle is learnt in the training. The resulting combined quantum circuit is denoted by S1RZS2.\nWhen there is no need to specify the combination operation, we use the general symbol \u00a9 to denote either.\nIn the language of string diagrams, the Frobenius multiplication is depicted by a black circle, see for example [25, 26, 24, 29]. The controlled-Z rotation is a depicted by a box, which we label as RZ , see Figures 20 and 21.\nAs before, we use Lambeq to produce the circuits corresponding to these diagrams and these are shown in Figures 22 and 23 respectively."
        },
        {
            "heading": "6.3 Binary classification training",
            "text": "We represent the combined sentences S1 \u00a9 S2 of each entry of the dataset by a single parametrised quantum circuit, where the combination circuit is the combined version of the circuits of S1 and S2 according to the operations listed above, in the previous subsection.\nWe first randomly initialise a set of parameters \u0398 = (\u03b81, \u03b82, ..., \u03b8k), such that every parameter used by the IQP ansatz [27, 7] of every word in the vocabulary is included. IQP stands for Instantaneous Quantum Polynomial, it is a method of developing circuits which interleaves layers of Hadamard quantum gates with diagonal unitaries.\nEvery combined circuit from the test dataset is then evaluated against \u0398. That means if the same word appears in two different circuits, it is represented as the same quantum state in both circuits. We denote the output state from the circuit S1\u00a9 S2 as |S1\u00a9 S2 (\u0398)\u3009. The expected prediction of the binary class of each entry of the dataset is then given by Born rule, i.e. as follows\nli\u0398(S1\u00a9 S2) := |\u3008i |S1\u00a9 S2 (\u0398)|\u3009| 2 +\nIn the above, i \u2208 {0, 1}, = 10\u22129 and l\u0398(S1\u00a9S2) is a probability distribution defined as follows\nl\u0398(S1\u00a9 S2) := (l0\u0398(S1\u00a9 S2), l1\u0398(S1\u00a9 S2))/ \u2211 i li\u0398(S1\u00a9 S2)\n20 Wazni, Lo, McPheat, Sadrzadeh Note that the sum \u2211 i |\u3008i |S1\u00a9 S2 (\u0398)|\u3009| 2 need not equal to 1 as there can be post-selections in the circuits, rendering some circuit runs are discarded. In some extreme cases, almost all circuit runs could be discarded. To avoid the possibility of dividing by zero, the small constant = 10\u22129 is added. The predicted binary class from the model is then obtained by rounding to the nearest integer bl\u0398(S1\u00a9 S2)e.\nThe class labels are represented as one-hot encoding [0,1] and [1,0], corresponding to subject anaphora and object anaphora respectively. The model is trained by comparing the predicted label with the training label using a binary cross-entropy loss function and minimized using a non-gradient based optimisation algorithm known as SPSA (Simultaneous Perturbation Stochastic Approximation) [28]. As a result, the system learns to classify the sentences by adjusting the parameters."
        },
        {
            "heading": "6.4 Models",
            "text": "First and foremost, we implement the SLLM model. This model encodes both the grammatical and discourse structures of pairs of sentences of our dataset. In other words, in the SLLM model, each sentence of the dataset has a circuit built according to the string diagram of its syntactic structure. Further, the two sentences of the dataset are connected to each other according to the discourse structure between them. For comparison reasons, we implement and experiment with three other models: (1) a model where there is notion of grammar and no notion of discourse, (2) a model where there is no notion of grammar, but anaphora is resolved, and (3) a model where we have a notion of grammar, but the anaphora relation between the sentence are not resolved.\nWe train a binary classification task for each of these four models (the SLLM model is our model no. 4), In order to do so, we have to combine the sentences of each entry of the dataset to obtain one single output state. We experiment with both the Frobenius and the Z Rotation operations, described in the previous susbection. As a result, in total we train eight binary classification models.\nWe elaborate on each of these models below and draw the string diagrams corresponding to the subject and object anaphora relations.. For reasons of space, we only provide the Frobenius combinations of sentences. The Z rotation combinations are obtained in a similar way, by only replacing the final (lower most) Frobenius with a Z rotation box.\n\u2022 Model 1: this model is a bag of words model where the words are connected to each other using the Frobenius multiplication operation. As this operation is commutative, we have no notion of grammar in the model. Further, we do not resolve the anaphora, i.e. we do not connect any of the noun phrases of the first sentence to the pronoun of the second sentence. We\nrefer to this model as the no-grammar, no-discourse model. This model is depicted in Figure 24.\n\u2022 Model 2: this model is similar to the first model in that, there is still no notion of grammar: the words are connected to each other using the Frobenius multiplication. It differs from it in that either the subject or object of the first sentence is connected to the pronoun of the second sentence. So we resolve the anaphora. We refer to this model as no grammar, discourse model. This model is depicted in Figure 25.\n\u2022 Model 3: Here we have a notion of grammar; each sentence has a circuit obtained from the string diagram of its syntactic structure. There is, however, no notion of discourse. The anaphora is not resolved and neither the subject or object of the first sentence is connected to the pronoun of\nthe second sentence. We refer to this model as the grammar, no-discourse model. This model is depicted in Figure 26.\n\u2022 Model 4: Finally, in this model we have the full SLLM syntax turned into string diagrams and then into circuits. Here, we both encode the grammatical structure of each sentence, as well as resolve the anaphora. We refer to this model as grammar, discourse model and depicted in Figure 27."
        },
        {
            "heading": "6.5 Results",
            "text": "Figs. 28, 29, 30, and 31 present the convergence of the models on the training dataset. Shown is the training loss (red, y1 axis) and the training accuracy (blue, y2 axis) where each line is from averaging over 20 runs of the optimisation with a random initial parameter point. For all the models, we fixed the hyper-parameters7 and the number of iterations (100) to have a fair comparison. As is clear from the plots, the training converges smoothly in all cases. We notice that in Models 2, 3 and 4, the cost function converges faster\n7 The initial learning rate a = 0.05, the initial parameter shift scaling factor c = 0.06, a stability constant A = 0.01 * number of training steps.\nwhen sentences S1 and S2 are connected with a Frobenius rather than the controlled-Rz gate. For instance, after 60 iterations, Model 2a converges to a loss of (0.775) while Model 2b to (0.824); Model 3a to (0.749) while Model 3b to (0.861); Model 4a to (0.39) while Model 4b to (0.695).\nTable 3 shows the test accuracy results for our 8 models. It\u2019s clear that models with S1 and S2 being connected with a Frobenius performed better. Model 1 achieved a 12% increase in accuracy; Model 2 a 32%; Model 3 a 23%; Model 4 a 14.71%. On the contrary, in the plots of models 1b and 3b show, the training accuracy remained around 0.5 without any increase. Models with a controlled-Rz gate may need more time and iterations to train.\nOn the other hand, we observe a remarkable improvement in test accuracy after connecting the pronoun with its anaphora. In Model 1a, the recorded value was (0.64) after 100 iterations. This accuracy increased about 35% in Model 2a. Similarly, the accuracy in Model 1b fluctuated around (0.51) while it increased to reach (0.67) in Model 2b. The same scenario is repeated when we compare Model 3a (0.72) with Model 4a (0.91) and Model 3b (0.48) with Model 4b (0.76). Also, we can see a boost in performance when we introduce the notion of grammar to the models. For instance, the accuracy increased 8.4% between Model 1a and Model 3a, and around 8.9% between Model 2b and Model 4b. However this wasn\u2019t the case between Models 2a and 4a even though both models performed well."
        },
        {
            "heading": "7 Summary, Conclusions, Future Work",
            "text": "This paper had two main parts. The first part was on the theory of a specific modal Lambek calculus and its applications to modelling discourse relations such as anaphora and ellipsis. In this part, we focused on the calculus of Kanovich et. al [11], which we denote by SLLM. The modalities of this calculus are in the style of the soft modalities of Linear Logic [14]. They have the advantage that the logic containing them remains decidable, despite allowing for an implicit form of copying \u2013 via the notion of storage and projections from it. A vector space semantics for this calculus was developed\nin previous work [18], where the modal formulae were interpreted as truncated Fock spaces, i.e. direct sums of tensor powers, but only up to a fixed given order k0. We end the first part by introducing a novel string diagrammatic semantics for this truncated Fock space semantics.\nIn the second part, we followed the line of work initiated in [19, 17] and translated our string diagrams to quantum circuits. Here, vector spaces are translated to quantum states, and the operations on them to quantum gates. We translate our truncated Fock space semantics into a quantum circuits by extending the existing translation. We then applied our setting to a definite pronoun resolution task, and (1) develop a smaller version of the original dataset of [16], elaborated on in [23], (2) model the pronominal anaphora relations with modal formulae, (3) build the corresponding Fock semantics of them and after depicting the reasoning in diagrams, (4) translate these to quantum circuits. We learn the parameters of the resulting circuits by simulating them in the AerSimulator [2]. Apart from the SLLM model, we implement and experiment with three other models which either had a notion of grammar, or a notion of discourse, or neither of the two. The highest accuracies were recorded for models with a notion of discourse (i.e. the anaphora was resolved), irregardless of presence of a grammatical structure. Furthermore, we experimented with two different ways of combining the sentences of each discourse. Our exploration of the combination operation between the sentences of each discourse revealed that the CNOT combination (Frobenius multiplication) achieves a higher accuracy than a controlled- RZ rotation.\nIt is tempting to immediately conclude that resolving discourse is more important than modelling grammatical structure, at least for definite pronoun resolution. There are, however, reasons to abstain from a quick conclusion. Most important of these is that our dataset is small; experimentation on a larger dataset is work in progress. We would also like to model other types of coreference relations such as ellipsis. Moving away from simulations and\ntraining our circuit parameters on the real quantum computers is another avenue for future work. Finally, efforts to develop Fock space quantum computers and developing NLP packages for them is work in progress, see for example [4, 5], and it would be more natural to use these computers for our task."
        }
    ],
    "title": "A Quantum Natural Language Processing Approach to Pronoun Resolution",
    "year": 2022
}