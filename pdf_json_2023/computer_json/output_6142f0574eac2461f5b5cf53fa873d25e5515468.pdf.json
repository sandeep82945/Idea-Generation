{
    "abstractText": "The integration of multi-modal data, such as pathological images and genomic data, is essential for understanding cancer heterogeneity and complexity for personalized treatments, as well as for enhancing survival predictions. Despite the progress made in integrating pathology and genomic data, most existing methods cannot mine the complex inter-modality relations thoroughly. Additionally, identifying explainable features from these models that govern preclinical discovery and clinical prediction is crucial for cancer diagnosis, prognosis, and therapeutic response studies. We propose PONETa novel biological pathway informed pathology-genomic deep model that integrates pathological images and genomic data not only to improve survival prediction but also to identify genes and pathways that cause different survival rates in patients. Empirical results on six of The Cancer Genome Atlas (TCGA) datasets show that our proposed method achieves superior predictive performance and reveals meaningful biological interpretations. The proposed method establishes insight into how to train biologically informed deep networks on multimodal biomedical data which will have general applicability for understanding diseases and predicting response and resistance to treatment.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lin Qiu"
        },
        {
            "affiliations": [],
            "name": "Aminollah Khormali"
        },
        {
            "affiliations": [],
            "name": "Kai Liu"
        }
    ],
    "id": "SP:c80e72224c9d87c94a60492f6f47debfe462a3f1",
    "references": [
        {
            "authors": [
                "Khaleel I Al-Obaidy",
                "John N Eble",
                "Mehdi Nassiri",
                "Liang Cheng",
                "Mohammad K Eldomery",
                "Sean R Williamson",
                "Wael A Sakr",
                "Nilesh Gupta",
                "Oudai Hassan",
                "Muhammad T Idrees"
            ],
            "title": "Recurrent kras mutations in papillary renal neoplasm with reverse polarity",
            "venue": "Modern Pathology,",
            "year": 2020
        },
        {
            "authors": [
                "John KC Chan"
            ],
            "title": "The wonderful colors of the hematoxylin-eosin stain in diagnostic surgical pathology",
            "venue": "International Journal of Surgical Pathology,",
            "year": 2014
        },
        {
            "authors": [
                "Anika Cheerla",
                "Olivier Gevaert"
            ],
            "title": "Deep learning with multimodal representation for pancancer prognosis",
            "venue": "prediction. Bioinformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Richard J Chen",
                "Ming Y Lu",
                "Wei-Hung Weng",
                "Tiffany Y Chen",
                "Drew FK Williamson",
                "Trevor Manz",
                "Maha Shady",
                "Faisal Mahmood"
            ],
            "title": "Multimodal co-attention transformer for survival prediction in gigapixel whole slide images",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Richard J Chen",
                "Ming Y Lu",
                "Jingwen Wang",
                "Drew FK Williamson",
                "Scott J Rodig",
                "Neal I Lindeman",
                "Faisal Mahmood"
            ],
            "title": "Pathomic fusion: An integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Richard J Chen",
                "Ming Y Lu",
                "Drew FK Williamson",
                "Tiffany Y Chen",
                "Jana Lipkova",
                "Zahra Noor",
                "Muhammad Shaban",
                "Maha Shady",
                "Mane Williams",
                "Bumjin Joo"
            ],
            "title": "Pan-cancer integrative histology-genomic analysis via interpretable multimodal deep learning",
            "year": 2022
        },
        {
            "authors": [
                "Jun Cheng",
                "Jie Zhang",
                "Yatong Han",
                "Xusheng Wang",
                "Xiufen Ye",
                "Yuebo Meng",
                "Anil Parwani",
                "Zhi Han",
                "Qianjin Feng",
                "Kun Huang"
            ],
            "title": "Integrative analysis of histopathological images and genomic data predicts clear cell renal cell carcinoma prognosis",
            "venue": "Cancer Research,",
            "year": 2017
        },
        {
            "authors": [
                "Elisa Cirillo",
                "Laurence D Parnell",
                "Chris T Evelo"
            ],
            "title": "A review of pathway-based analysis tools that visualize genetic variants",
            "venue": "Frontiers in Genetics,",
            "year": 2017
        },
        {
            "authors": [
                "Pierre Courtiol",
                "Charles Maussion",
                "Matahi Moarii",
                "Elodie Pronier",
                "Samuel Pilcer",
                "Meriem Sefta",
                "Pierre Manceron",
                "Sylvain Toldo",
                "Mikhail Zaslavskiy",
                "Nolwenn Le Stang"
            ],
            "title": "Deep learning-based classification of mesothelioma improves prediction of patient outcome",
            "venue": "Nature Medicine,",
            "year": 2019
        },
        {
            "authors": [
                "David R Cox"
            ],
            "title": "Regression models and life-tables",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 1972
        },
        {
            "authors": [
                "Haitham A Elmarakeby",
                "Justin Hwang",
                "Rand Arafeh",
                "Jett Crowdis",
                "Sydney Gang",
                "David Liu",
                "Saud H AlDubayan",
                "Keyan Salari",
                "Steven Kregel",
                "Camden Richter"
            ],
            "title": "Biologically informed deep neural network for prostate cancer",
            "venue": "discovery. Nature,",
            "year": 2021
        },
        {
            "authors": [
                "Antonio Fabregat",
                "Steven Jupe",
                "Lisa Matthews",
                "Konstantinos Sidiropoulos",
                "Marc Gillespie",
                "Phani Garapati",
                "Robin Haw",
                "Bijay Jassal",
                "Florian Korninger",
                "Bruce May"
            ],
            "title": "The reactome pathway knowledgebase",
            "venue": "Nucleic Acids Research,",
            "year": 2020
        },
        {
            "authors": [
                "Treg Grubb",
                "Smruthi Maganti",
                "John Michael Krill-Burger",
                "Cameron Fraser",
                "Laura Stransky",
                "Tomas Radivoyevitch",
                "Kristopher A. Sarosiek",
                "Francisca Vazquez",
                "William G. Kaelin Jr.",
                "Abhishek A. Chakraborty"
            ],
            "title": "A mesenchymal tumor cell state confers increased dependency on the bcl-xl antiapoptotic protein in kidney cancer",
            "venue": "bioRxiv,",
            "year": 2022
        },
        {
            "authors": [
                "Jing-Yi Guo",
                "Zuo-qian Jing",
                "Xue-jie Li",
                "Li-yuan Liu"
            ],
            "title": "Bioinformatic analysis identifying psmb 1/2/3/4/6/8/9/10 as prognostic indicators in clear cell renal cell carcinoma",
            "venue": "International Journal of Medical Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Zhezhu Han",
                "Dongxu Kang",
                "Yeonsoo Joo",
                "Jihyun Lee",
                "Geun-Hyeok Oh",
                "Soojin Choi",
                "Suwan Ko",
                "Suyeon Je",
                "Hye Jin Choi",
                "Jae J Song"
            ],
            "title": "Tgf-beta downregulation-induced cancer cell death is finely regulated by the sapk signaling cascade",
            "venue": "Experimental and Molecular Medicine,",
            "year": 2018
        },
        {
            "authors": [
                "Jie Hao",
                "Youngsoon Kim",
                "Tae-Kyung Kim",
                "Mingon Kang"
            ],
            "title": "Pasnet: pathway-associated sparse deep neural network for prognosis prediction from high-throughput data",
            "venue": "BMC Bioinformatics,",
            "year": 2018
        },
        {
            "authors": [
                "Frank E Harrell",
                "Robert M Califf",
                "David B Pryor",
                "Kerry L Lee",
                "Robert A Rosati"
            ],
            "title": "Evaluating the yield of medical tests",
            "venue": "Journal of the American Medical Association,",
            "year": 1982
        },
        {
            "authors": [
                "Achim Hekler",
                "Jochen S Utikal",
                "Alexander H Enk",
                "Wiebke Solass",
                "Max Schmitt",
                "Joachim Klode",
                "Dirk Schadendorf",
                "Wiebke Sondermann",
                "Cindy Franklin",
                "Felix Bestvater"
            ],
            "title": "Deep learning outperformed 11 pathologists in the classification of histopathological melanoma",
            "venue": "images. Europe Journal of Cancer,",
            "year": 2019
        },
        {
            "authors": [
                "Le Hou",
                "Dimitris Samaras",
                "Tahsin M Kurc",
                "Yi Gao",
                "James E Davis",
                "Joel H Saltz"
            ],
            "title": "Patch-based convolutional neural network for whole slide tissue image classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Le Hou",
                "Rajarsi Gupta",
                "John S Van Arnam",
                "Yuwei Zhang",
                "Kaustubh Sivalenka",
                "Dimitris Samaras",
                "Tahsin M Kurc",
                "Joel H Saltz"
            ],
            "title": "Dataset of segmented nuclei in hematoxylin and eosin stained histopathology images of ten cancer",
            "venue": "types. Scientific Data,",
            "year": 2020
        },
        {
            "authors": [
                "Osamu Iizuka",
                "Fahdi Kanavati",
                "Kei Kato",
                "Michael Rambeau",
                "Koji Arihiro",
                "Masayuki Tsuneki"
            ],
            "title": "Deep learning models for histopathological classification of gastric and colonic epithelial tumours",
            "venue": "Scientific Report,",
            "year": 2020
        },
        {
            "authors": [
                "Maximilian Ilse",
                "Jakub Tomczak",
                "Max Welling"
            ],
            "title": "Attention-based deep multiple instance learning",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Lv Jin",
                "Xiao-Yu Zuo",
                "Wei-Yang Su",
                "Xiao-Lei Zhao",
                "Man-Qiong Yuan",
                "Li-Zhen Han",
                "Xiang Zhao",
                "Ye-Da Chen",
                "Shao-Qi Rao"
            ],
            "title": "Pathway-based analysis tools for complex diseases: A review",
            "venue": "Genomics Proteomics Bioinformatics,",
            "year": 2014
        },
        {
            "authors": [
                "Onno Kampman",
                "Elham J Barezi",
                "Dario Bertero",
                "Pascale Fung"
            ],
            "title": "Investigating audio, video, and text fusion methods for end-to-end automatic personality prediction",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
            "year": 2018
        },
        {
            "authors": [
                "Jared L. Katzman",
                "Uri Shaham",
                "Alexander Cloninger",
                "Jonathan Bates",
                "Tingting Jiang",
                "Yuval Kluger"
            ],
            "title": "Deepsurv: personalized treatment recommender system using a cox proportional hazards deep neural network",
            "venue": "BMC Medical Research Methology,",
            "year": 2018
        },
        {
            "authors": [
                "Jin-Hwa Kim",
                "Kyoung-Woon On",
                "Woosang Lim",
                "Jeonghee Kim",
                "Jung-Woo Ha",
                "Byoung-Tak Zhang"
            ],
            "title": "Hadamard product for low-rank bilinear pooling",
            "venue": "In Proceedings of International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ruiqing Li",
                "Xingqi Wu",
                "Ao Li",
                "Minghui Wang"
            ],
            "title": "Hfbsurv: hierarchical multimodal fusion with factorized bilinear models for cancer survival",
            "venue": "prediction. Bioinformatics,",
            "year": 2022
        },
        {
            "authors": [
                "Yanming Li",
                "Bin Nan",
                "Ji Zhu"
            ],
            "title": "Multivariate sparse group lasso for the multivariate multiple linear regression with an arbitrary group",
            "venue": "structure. Biometrics,",
            "year": 2015
        },
        {
            "authors": [
                "Shuheng Lin",
                "Ana Negulescu",
                "Sirisha Bulusu",
                "Benjamin Gibert",
                "Jean-Guy Delcros",
                "Benjamin Ducarouge",
                "Nicolas Rama",
                "Nicolas Gadot",
                "Isabelle Treilleux",
                "Pierre Saintigny"
            ],
            "title": "Non-canonical notch3 signalling limits tumour angiogenesis",
            "venue": "Nature Communications,",
            "year": 2017
        },
        {
            "authors": [
                "Zhun Liu",
                "Ying Shen",
                "Paul Pu Liang",
                "Amir Zadeh",
                "Louis-Philippe Morency"
            ],
            "title": "Efficient low-rank multimodal fusion with modality-specific factors",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Ming Y Lu",
                "Drew FK Williamson",
                "Tiffany Y Chen",
                "Richard J Chen",
                "Matteo Barbieri",
                "Faisal Mahmood"
            ],
            "title": "Data-efficient and weakly supervised computational pathology on whole-slide images",
            "venue": "Nature Biomedical Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Sijie Mai",
                "Haifeng Hu",
                "Songlong Xing"
            ],
            "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Tejaswini Mallavarapu",
                "Youngsoon Kim",
                "Jung Hun Oh",
                "Mingon Kang"
            ],
            "title": "R-pathcluster: Identifying cancer subtype of glioblastoma multiforme using pathway-based restricted boltzmann machine",
            "venue": "IEEE International Conference on Bioinformatics and Biomedicine,",
            "year": 2017
        },
        {
            "authors": [
                "Pooya Mobadersany",
                "Safoora Yousefi",
                "Mohamed Amgad",
                "David A Gutman",
                "Jill S Barnholtz-Sloan",
                "Jos\u00e9 E Vel\u00e1zquez Vega",
                "Daniel J Brat",
                "Lee AD Cooper"
            ],
            "title": "Predicting cancer outcomes from histology and genomics using convolutional networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2018
        },
        {
            "authors": [
                "Zhenyuan Ning",
                "Weihao Pan",
                "Yuting Chen",
                "Qing Xiao",
                "Xinsen Zhang",
                "Jiaxiu Luo",
                "Jian Wang",
                "Yu Zhang"
            ],
            "title": "Integrative analysis of cross-modal features for the prognosis prediction of clear cell renal cell carcinoma",
            "year": 2020
        },
        {
            "authors": [
                "Behnaz Nojavanasghari",
                "Deepak Gopinath",
                "Jayanth Koushik",
                "Tadas Baltru\u0161aitis",
                "Louis-Philippe Morency"
            ],
            "title": "Deep multimodal fusion for persuasiveness prediction",
            "venue": "In Proceedings of the 18th ACM International Conference on Multimodal Interaction,",
            "year": 2016
        },
        {
            "authors": [
                "Xipeng Pan",
                "Lingqiao Li",
                "Huihua Yang",
                "Zhenbing Liu",
                "Jinxin Yang",
                "Lingling Zhao",
                "Yongxian Fan"
            ],
            "title": "Accurate segmentation of nuclei in pathological images via sparse reconstruction and deep convolutional networks",
            "year": 2017
        },
        {
            "authors": [
                "Soujanya Poria",
                "Iti Chaturvedi",
                "Erik Cambria",
                "Amir Hussain"
            ],
            "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
            "venue": "In Proceedings of International Conference on Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Guang Shan",
                "Tian Tang",
                "Huijun Qian",
                "Yue Xia"
            ],
            "title": "Expression of tiam1 and rac1 proteins in renal cell carcinoma and its clinical-pathological features",
            "venue": "International Journal of Clinical and Experimental Pathology,",
            "year": 2017
        },
        {
            "authors": [
                "Vaishnavi Subramanian",
                "Tanveer Syeda-Mahmood",
                "Minh N Do"
            ],
            "title": "Multimodal fusion using sparse cca for breast cancer survival prediction",
            "venue": "In Proceedings of IEEE 18th International Symposium on Biomedical Imaging (ISBI),",
            "year": 2021
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Joshua B Tenenbaum",
                "William T Freeman"
            ],
            "title": "Separating style and content with bilinear models",
            "venue": "Neural Computation,",
            "year": 2000
        },
        {
            "authors": [
                "Chunyu Wang",
                "Junling Guo",
                "Ning Zhao",
                "Yang Liu",
                "Xiaoyan Liu",
                "Guojun Liu",
                "Maozu Guo"
            ],
            "title": "A cancer survival prediction method based on graph convolutional network",
            "venue": "IEEE Trans Nanobioscience,",
            "year": 2020
        },
        {
            "authors": [
                "Xiyue Wang",
                "Sen Yang",
                "Jun Zhang",
                "Minghui Wang",
                "Jing Zhang",
                "Junzhou Huang",
                "Wei Yang",
                "Xiao Han"
            ],
            "title": "Transpath: Transformer-based self-supervised learning for histopathological image classification",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqin Wang",
                "Ruiqing Li",
                "Minghui Wang",
                "Ao Li"
            ],
            "title": "Gpdbn: deep bilinear network integrating both genomic data and pathological images for breast cancer prognosis",
            "venue": "prediction. Bioinformatics,",
            "year": 2021
        },
        {
            "authors": [
                "Martin W\u00f6llmer",
                "Felix Weninger",
                "Tobias Knaup",
                "Bj\u00f6rn Schuller",
                "Congkai Sun",
                "Kenji Sagae",
                "Louis-Philippe Morency"
            ],
            "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
            "venue": "IEEE Intelligent Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Ellery Wulczyn",
                "David F Steiner",
                "Zhaoyang Xu",
                "Apaar Sadhwani",
                "Hongwu Wang",
                "Isabelle FlamentAuvigne",
                "Craig H Mermel",
                "Po-Hsuan Cameron Chen",
                "Yun Liu",
                "Martin C Stumpe"
            ],
            "title": "Deep learning-based survival prediction for multiple cancer types using histopathology",
            "venue": "images. Plos One,",
            "year": 2020
        },
        {
            "authors": [
                "Chun-ming Yang",
                "Shan Ji",
                "Yan Li",
                "Li-ye Fu",
                "Tao Jiang",
                "Fan-dong Meng"
            ],
            "title": "B-catenin promotes cell proliferation, migration, and invasion but induces apoptosis in renal cell carcinoma",
            "venue": "OncoTargets and Therapy,",
            "year": 2017
        },
        {
            "authors": [
                "Zhou Yu",
                "Jun Yu",
                "Jianping Fan",
                "Dacheng Tao"
            ],
            "title": "Multi-modal factorized bilinear pooling with coattention learning for visual question answering",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Amir Zadeh",
                "Rowan Zellers",
                "Eli Pincus",
                "Louis-Philippe Morency"
            ],
            "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
            "venue": "IEEE Intelligent Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Amir Zadeh",
                "Minghai Chen",
                "Soujanya Poria",
                "Erik Cambria",
                "Louis-Philippe Morency"
            ],
            "title": "Tensor fusion network for multimodal sentiment analysis",
            "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Shekoufeh Gorgi Zadeh",
                "Matthias Schmid"
            ],
            "title": "Bias in cross-entropy-based training of deep survival networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Qiang Zhang",
                "Xiujuan Yin",
                "Zhiwei Pan",
                "Yingying Cao",
                "Shaojie Han",
                "Guojun Gao",
                "Zhiqin Gao",
                "Zhifang Pan",
                "Weiguo Feng"
            ],
            "title": "Identification of potential diagnostic and prognostic biomarkers for prostate cancer",
            "venue": "Oncology Letters,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "The integration of multi-modal data, such as pathological images and genomic data, is essential for understanding cancer heterogeneity and complexity for personalized treatments, as well as for enhancing survival predictions. Despite the progress made in integrating pathology and genomic data, most existing methods cannot mine the complex inter-modality relations thoroughly. Additionally, identifying explainable features from these models that govern preclinical discovery and clinical prediction is crucial for cancer diagnosis, prognosis, and therapeutic response studies. We propose PONET- a novel biological pathway informed pathology-genomic deep model that integrates pathological images and genomic data not only to improve survival prediction but also to identify genes and pathways that cause different survival rates in patients. Empirical results on six of The Cancer Genome Atlas (TCGA) datasets show that our proposed method achieves superior predictive performance and reveals meaningful biological interpretations. The proposed method establishes insight into how to train biologically informed deep networks on multimodal biomedical data which will have general applicability for understanding diseases and predicting response and resistance to treatment."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Manual examination of hematoxylin and eosin (H&E)-stained slides of tumor tissue by pathologists is currently the state-of-the-art for cancer diagnosis (Chan, 2014). The recent advancements in deep learning for digital pathology have enabled the use of whole-slide images (WSIs) for computational image analysis tasks, such as cellular segmentation (Pan et al., 2017; Hou et al., 2020), tissue classification and characterization (Hou et al., 2016; Hekler et al., 2019; Iizuka et al., 2020). While H&E slides are important and sufficient to establish a profound diagnosis, genomics data can provide a deep molecular characterization of the tumor, potentially offering the chance for prognostic and predictive biomarker discovery.\nCancer prognosis via survival outcome prediction is a standard method used for biomarker discovery, stratification of patients into distinct treatment groups, and therapeutic response prediction (Cheng et al., 2017; Ning et al., 2020). WSIs exhibit enormous heterogeneity and most approaches adopt a two-stage multiple instance learning-based (MIL) approach for the representation learning of WSIs. Firstly, instance-level feature representations are extracted from image patches in the WSI, and then global aggregation schemes are applied to the bag of instances to obtain a WSI-level representation for subsequent modeling purpose (Hou et al., 2016; Courtiol et al., 2019; Wulczyn et al., 2020; Lu et al., 2021). Therefore, multimodal survival prediction faces an additional challenge due to the large data heterogeneity gap between WSIs and genomics, and many existing approaches use simple multimodal fusion mechanisms for feature integration, which prevents mining important multimodal interactions (Mobadersany et al., 2018; Chen et al., 2022b;a).\nThe incorporation of biological pathway databases in a model takes advantage of leveraging prior biological knowledge so that potential prognostic factors of well-known biological functionality can be identified (Hao et al., 2018). Moreover, encoding biological pathway information into the neural\nar X\niv :2\n30 1.\n02 38\n3v 1\n[ q-\nbi o.\nQ M\n] 6\nJ an\n2 02\nnetworks achieved superior predictive performance compared with established models (Elmarakeby et al., 2021).\nBased on the current challenges in multimodal fusion of pathology and genomics and the potential prognostic interpretation to link pathways and clinical outcomes in pathway-based analysis, we propose a novel biological pathway-informed pathology-genomic deep model, PONET, that uses H&E WSIs and genomic profile features for survival prediction. The proposed method contains four major contributions: 1) PONET formulates a biological pathway-informed deep hierarchical multimodal integration framework for pathological images and genomic data; 2) PONET captures diverse and comprehensive modality-specific and cross-modality relations among different data sources based on the factorized bilinear model and graph fusion network; 3) PONET reveals meaningful model interpretations on both genes and pathways for potential biomarker and therapeutic target discovery; PONET also shows spatial visualization of the top genes/pathways which has enormous potential for novel and prognostic morphological determinants; 4) We evaluate PONET on six public TCGA datasets which showed superior survival prediction comparing to state-of-the-art methods. Fig. 1 shows our model framework."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Multimodal Fusion. Earlier works on multimodal fusion focus on early fusion and late fusion. Early fusion approaches fuse features by simple concatenation which cannot fully explore intra-modality dynamics (W\u00f6llmer et al., 2013; Poria et al., 2016; Zadeh et al., 2016). In contrast, late fusion fuses different modalities by weighted averaging which fails to model cross-modal interactions (Nojavanasghari et al., 2016; Kampman et al., 2018). The exploitation of relations within each modality has been successfully introduced in cancer prognosis via bilinear model (Wang et al., 2021b) and graph-based model (Subramanian et al., 2021). Adversarial Representation Graph Fusion (ARGF) (Mai et al., 2020) interprets multimodal fusion as a hierarchical interaction learning procedure where firstly bimodal interactions are generated based on unimodal dynamics, and then trimodal dynamics are generated based on bimodal and unimodal dynamics. We propose a new hierarchical fusion framework with modality-specific and cross-modality attentional factorized bilinear modules to mine the comprehensive modality interactions. Our proposed hierarchical fusion framework is different from ARGF in the following ways: 1) We take the sum of the weighted modality-specific representation as the unimodal representation instead of calculating the weighted average of the modality-specific representation in ARGF; 2) For higher level\u2019s fusion, ARGF takes the original embeddings of each modality as input while we use the weighted modality-specific representations; 3) We argue that ARGF takes redundant information during their trimodal dynamics.\nMultimodal Survival Analysis. There have been exciting attempts on multimodal fusion of pathology and genomic data for cancer survival prediction (Mobadersany et al., 2018; Cheerla & Gevaert, 2019; Wang et al., 2020). However, these multimodal fusion based methods fail to model the interaction between each subset of multiple modalities explicitly. Kronecker product considers pairwise interactions of two input feature vectors by producing a high-dimensional feature of quadratic expansion (Zadeh et al., 2017), and showed its superiority in cancer survival prediction (Wang et al., 2021b; Chen et al., 2022b;a). Despite promising results, using Kronecker product in multimodal\nfusion may introduce a large number of parameters that may lead to high computational cost and risk of overfitting (Kim et al., 2017; Liu et al., 2021), thus limiting its applicability and improvement in performance. To overcome this drawback, hierarchical factorized bilinear fusion for cancer survival prediction (HFBSurv) (Li et al., 2022) uses factorized bilinear model to fuse genomic and image features, dramatically reducing computational complexity. PONET differs from HFBSurv in two ways: 1) PONET\u2019s multimodal framework has three levels of hierarchical fusion module including unimodal, bimodal, and trimodal fusion while HFBSurv only considers within-modality and cross-modality fusion which we argue it is not adequate for mining the comprehensive interactions; 2) PONET leverages biological pathway informed network for better prediction and meaningful interpretation purposes.\nPathway-associated Sparse Neural Network. The pathway-based analysis is an approach that a number of studies have investigated to improve both predictive performance and biological interpretability (Jin et al., 2014; Cirillo et al., 2017; Hao et al., 2018; Elmarakeby et al., 2021). Moreover, pathway-based approaches have shown more reproducible analysis results than gene expression data analysis alone (Li et al., 2015; Mallavarapu et al., 2017). These pathway-based deep neural networks can only model genomic data which severely inhibits their applicability in current biomedical research. Additionally, the existing pathway-associated sparse neural network structures are limited for disease mechanism investigation: there is only one pathway layer in PASNet (Hao et al., 2018) which contains limited biological prior information to deep dive into the hierarchical pathway and biological process relationships; P-NET (Elmarakeby et al., 2021) calculates the final prediction by taking the average of all the gene and pathway layers\u2019 outputs, and this will bias the learning process because it will put more weights for some layers\u2019 outputs while underestimating the others."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM FORMULATION AND NOTATIONS",
            "text": "The model architecture of PONET is presented in Fig. 1, where three modalities are included as input: gene expression g \u2208 Rdg , pathological image p \u2208 Rdp , and copy number (CNV) + mutation (MUT) CNV + MUT \u2208 Rdc , with dp being the dimensionality of p and so on. We define a hierarchical factorized bilinear fusion model for PONET. We build a sparse biological pathway-informed embedding network for gene expression. A fully connected (FC) embedding layer for both preprocessed pathological image feature (fp) and the copy number + mutation (fc) to map feature into similar embedding space for alleviating the statistical property differences between modalities, the three network architecture details are in Appendix C.1. We label the three modality embeddings as hm, m \u2208 {g, p, c}, the superscript/subscript u, b, and t represents unimodal fusion, biomodal fusion and trimodal fusion. After that, the embeddings of each modality are first used as input for unimodal fusion to generate the modality-specific representation hum = \u03c9\nmh\u0302m, \u03c9m represent the modality-specific importance, the feature vector of the unimodal fusion is the sum of all modality-specific representations hu = \u2211 m h u m. In the bimodal fusion, modality-specific representations from the output of unimodal fusion are fused to yield cross-modality representations hbm1m2 = \u03c9m1m2 h\u0302m1m2 ,m1,m2 \u2208 {p, c, g} andm1 6= m2, \u03c9m1m2 represents the corresponding cross-modality importance. Similarly, the feature vector of bimodal fusion is calculated as hb = \u2211 m1,m2\nhbm1m2 . We propose to build a trimodal fusion to take each cross-modality representation from the output of bimodal fusion to mine the interactions. Similarly to the bimodal fusion architecture, the trimodal fusion feature vector will be ht = \u2211 m1,m2,m3\n\u03c9m1m2m3 h\u0302m1m2m3 ,m1,m2,m3 \u2208 {p, c, g} and m1 6= m2 6= m3, \u03c9m1m2m3 represents the corresponding trimodal importance. Finally, PONET concatenates hu, hb, ht to obtain the final comprehensive multimodal representation and pass it to the Cox proportional hazards model (Cox, 1972; Cheerla & Gevaert, 2019) for survival prediction. In the following sections we will describe our hierarchical factorized bilinear fusion framework, l, o, s represents the dimensionality of hm, zm, h\u0302m1m2."
        },
        {
            "heading": "3.2 SPARSE NETWORK",
            "text": "We design the sparse gene-pathway network consisting of one gene layer followed by three pathway layers. A patient sample of e gene expressions is formed as a column vector, which is denoted by X = [x1, x2, ..., xe], each node represents one gene. The gene layer is restricted to have connections\nreflecting the gene-pathway relationships curated by the Reactome pathway dataset (Fabregat et al., 2020). The connections are encoded by a binary matrix M \u2208 Ra\u00d7e, where a is the number of pathways and e is the number of genes, an element of M, mij , is set to one if gene j belongs to pathway i. The connections that do not exist in the Reactome pathway dataset will be zero-out. For the following pathway-pathway layers, a similar scheme is applied to control the connection between consecutive layers to reflect the parent-child hierarchical relationships that exist in the Reactome dataset. The output of each layer is calculated as\ny = f [(M \u2217W)TX + ] (1)\nwhere f is the activation function, M represents the binary matrix, W is the weights matrix, X is the input matrix, is the bias vector, and \u2217 is the Hadamard product. We use tanh for the activation of each node. We allow the information flow from the biological prior informed network starting from the first gene layer to the last pathway layer, and we label the last layer output embeddings of the sparse network for gene expression as hg ."
        },
        {
            "heading": "3.3 UNIMODAL FUSION",
            "text": "Bilinear models (Tenenbaum & Freeman, 2000) provide richer representations than linear models. Given two feature vectors in different modalities, e.g., the visual features x \u2208 Rm\u00d71 for an image and the genomic features y \u2208 Rn\u00d71 for a genomic profile, the bilinear model uses a quadratic expansion of linear transformation considering every pair of features:\nzi = x TWiy (2)\nwhere Wi \u2208 Rm\u00d7n is a projection matrix, zi \u2208 R is the output of the bilinear model. Bilinear models introduce a large number of parameters which potentially lead to high computational cost and overfitting risk. To address these issues, Yu et al. (2017) develop the Multi-modal Factorized Bilinear pooling (MFB) method, which enjoys the dual benefits of compact output features and robust expressive capacity.\nInspired by the MFB (Yu et al., 2017) and its application in pathology and genomic multimodal learning (Li et al., 2022), we propose unimodal fusion to capture modality-specific representations and quantify their importance. The unimodal fusion takes the embedding of each modality hm as input and factorizes the projection matrix Wi in Eq. (2) as two low-rank matrices:\nzi = h T mWihm = k\u2211 d=1 hTmum,dv T m,dhm\n= 1T (UTm,ihm \u25e6 V Tm,ihm),m \u2208 {p, c, g} (3)\nwe get the output feature zm:\nzm = SumPooling ( U\u0303Tmhm\u25e6V\u0303 Tmhm, k ) ,m \u2208 {p, c, g} (4)\nwhere k is the latent dimensionality of the factorized matrices. SumPooling (x, k) function performs sum pooling over x by using a 1-D non-overlapped window with the size k, U\u0303m \u2208 Rl\u00d7ko and V\u0303m \u2208 Rl\u00d7ko are 2-D matrices reshaped from Um and Vm, Um =[Um,1, . . . , Um,h] \u2208 Rl\u00d7k\u00d7o and Vm = [Vm,1, . . . , Vm,h] \u2208 Rl\u00d7k\u00d7o. Each modality-specific representation h\u0302m \u2208 Rl+o is obtained as:\nh\u0302m = hm\u00a9zm,m \u2208 {p, c, g} (5)\nwhere \u00a9 denotes vector concatenation. We also introduce a modality attention network Atten \u2208 Rl+o \u2192 R1 to determine the weight for each modality-specific representation to quantify its importance:\n\u03c9m = Atten(h\u0302m; \u0398Atten),m \u2208 {p, c, g} (6)\nwhere \u03c9m is the weight of modality m. In practice, Atten consists of a sigmoid activated dense layer parameterized by \u0398Atten. Therefore, the output of each modality in unimodal fusion, hum, is denoted as \u03c9mh\u0302m \u2208 Rl+o,m \u2208 {p, c, g}. Accordingly, the output of unimodal fusion, hu, is the sum of each weighted modality-specific representation \u03c9mh\u0302m,m \u2208 {p, c, g} which is different from ARGF (Mai et al., 2020) that used the weighted average of different modalities as the unimodal fusion output."
        },
        {
            "heading": "3.4 BIMODAL AND TRIMODAL FUSION",
            "text": "Bimodal fusion aims to fuse diverse information of different modalities and quantify different importance for them. After receiving the modality-specific representations hum from the unimodal fusion, we can generate the cross-modality representation h\u0302m1m2 \u2208 Rs similar to Eq. (4) :\nh\u0302m1,m2 = Sum Pooling ( U\u0303Tm1h u m1\u25e6V\u0303 T m2h u m2 , k ) ,\nm1,m2 \u2208 {p, c, g},m1 6= m2 (7)\nwhere U\u0303Tm1 \u2208 R (l+o)\u00d7ks and V\u0303 Tm2 \u2208 R (l+o)\u00d7ks are 2-D matrices reshaped from Um1 and Vm2 and Um1 = [Um1,1, . . . , Um1,s] \u2208 R(l+o)\u00d7k\u00d7s and Vm2 = [Vm2,1, . . . , Vm2,s] \u2208 R(l+o)\u00d7k\u00d7s. We leverage a bimodal attention network (Mai et al., 2020) to identify the importance of the crossmodality representation. The similarity Sm1m2 \u2208 R1 of hum1 and h u m2 is first estimated as follows:\nSm1,m2 = l+o\u2211 i=1 ( e\u03c9m1hum1,i\u2211l+o j=1 e \u03c9m1h u m1,j )( e\u03c9m2h u m2,i\u2211l+o j=1 e \u03c9m2h u m2,j ) (8)\nwhere the computed similarity is in the range of 0 to 1. Then, the cross-modality importance \u03c9m1m2 is obtained by:\n\u03c9m1m2 = e\u03c9\u0302mimj\u2211\nmi 6=mj e \u03c9\u0302mimj\n, \u03c9\u0302m1m2 = \u03c9m1 + \u03c9m2 Sm1m2 + S0\n(9)\nwhere S0 represents a pre-defined term controlling the relative contribution of similarity and modalityspecific importance, and here is set to 0.5. Therefore, the output of bimodal fusion, hb, is the sum of each weighted cross-modality representation \u03c9m1m2 h\u0302m1m2 ,m1,m2 \u2208 {p, c, g} and m1 6= m2. In trimodal fusion, each bimodal fusion output is fused with the unimodal fusion output that does not contribute to the formation of the bimodal fusion. The output for each corresponding trimodal representation is h\u0302m1m2m3 . In addition, trimodal attention was applied to identify the importance of each trimodal representation, \u03c9m1m2m3 . The output of the trimodal fusion, ht, is the sum of each weighted trimodal representation \u03c9m1m2m3 h\u0302m1m2m3 ,m1,m2,m3 \u2208 {p, c, g} and m1 6= m2 6= m3."
        },
        {
            "heading": "3.5 SURVIVAL LOSS FUNCTION",
            "text": "We train the model through the Cox partial likelihood loss (Cheerla & Gevaert, 2019) with l1 regularization for survival prediction, which is defined as:\n`(\u0398) = \u2212 \u2211 i:Ei=1 h\u0302\u0398 (xi)\u2212 log \u2211 j:Ti>Tj exp ( h\u0302\u0398 (xj) )+ \u03bb (\u2016\u0398\u20161) (10) where the values Ei, Ti and xi for each patient represent the survival status, the survival time, and the feature, respectively. Ei = 1 means event while Ei = 0 represents censor. h\u0302\u0398 is the neural network model trained for predicting the risk of survival, \u0398 is the neural network model parameters, and \u03bb is a regularization hyperparameter to avoid overfitting."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets. To validate our proposed method, we used six cancer datasets from The Cancer Genome Atlas (TCGA), a public cancer data consortium that contains matched diagnostic WSIs and genomic data with labeled survival times and censorship statuses. The genomic profile features (mutation status, copy number variation, RNA-Seq expression) are preprocessed by Porpoise 1 (Chen et al., 2022b). For this study, we used the following cancer types: Bladder Urothelial Carcinoma (BLCA) (n = 437), Kidney Renal Clear Cell Carcinoma (KIRC) (n = 350), Kidney Renal Papillary Cell Carcinoma (KIRP) (n = 284), Lung Adenocarcinoma (LUAD) (n = 515), Lung Squamous Cell Carcinoma (LUSC) (n = 484), Pancreatic adenocarcinoma (PAAD) (n = 180). We downloaded the same diagnostic WSIs from the TCGA website 2 that were used in Porpoise study to match the paired genomic features and survival times. The feature alignment table for all the cancer types is in Appendix A. For each WSI, automated segmentation of tissue was performed. Following segmentation, image patches of size 224 \u00d7 224 were extracted without overlap at the 20 X equivalent pyramid level from all tissue regions identified while excluding the white background and selecting only patches with at least 50% tissue regions. Subsequently, a visual representation of those patches is extracted with a vision transformer (Wang et al., 2021a) pre-trained on the TCGA dataset through a self-supervised constructive learning approach, such that each patch is represented as a 1 \u00d7 2048 vector. Fig. 2 shows the framework for the visual representation extraction by vision transformer (VIT). Survival outcome information is available at the patient level, we aggregated the patch-level feature into slide level feature representations based on an attention-based method (Lu et al., 2021; Ilse et al., 2018).\nBaselines. Using the same 5-fold cross-validation splits for evaluating PONET, we implemented and evaluated six state-of-the-art methods for survival outcome prediction. Additionally, we included three variations of PONET: a) PONET-O represents only genomic data, and pathway architecture for the gene expression are included in the model; b) PONET-OH represents only genomic and pathological image data but without pathway architecture in the model; c) PONET is our full model. For all methods, we use the same VIT feature extraction pipeline for WSIs, as well as identical training hyperparameters and loss functions for supervision. Training details and the parameters tuning can be found in Appendix C.2.\nCoxPH (Cox, 1972) represents the standard Cox proportional hazard models. DeepSurv (Katzman et al., 2018) is the deep neural network version of the CoxPH model. Pathomic Fusion (Chen et al., 2022a) as a pioneered deep learning-based framework for predicting survival outcomes by fusing pathology and genomic multimodal data, in which Kronecker product is taken to model pairwise feature interactions across modalities. GPDBN (Wang et al., 2021b) adopts Kronecker product to model inter-modality and intra-modality relations between pathology and genomic data for cancer prognosis prediction. HFBSurv (Li et al., 2022) extended GPDBN using the factorized bilinear model to fuse genomic and pathology features in a within-modality and cross-modalities hierarchical fusion. Porpoise (Chen et al., 2022b) applied the discrete survival model and Kronecker product to fuse pathology and genomic data for survival prediction (Zadeh & Schmid, 2020).\nEvaluation. For each cancer dataset, we used the cross-validated concordance index (C-Index) (Appendix B.1) (Harrell et al., 1982) to measure the predictive performance of correctly ranking the predicted patient risk scores with respect to overall survival."
        },
        {
            "heading": "4.2 RESULTS",
            "text": "Comparison with Baselines. In combing pathology image, genomics, and pathway network via PONET, our approach outperforms CoxPH models, unimodal networks, and previous deep learningbased approaches on pathology-genomic-based survival outcome prediction (Table 1). The results show that deep learning-based approaches generally perform better than the CoxPH model. PONET achieves superior C-index values in all six cancer types. All versions of PONET outperform Pathomic\n1https://github.com/mahmoodlab/PORPOISE 2https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga\nFusion by a big margin. Pathomic Fusion uses Kronecker product to fuse the two modalities, and that\u2019s also the reason why other advanced fusion methods, like GPDBN and HFBSurv, achieve better performance. Also, we argue that Pathomic Fusion extracts the region of interest of pathology image for feature extraction might limit the understanding of the tumor microenvironment of the whole slide. HFBSurv shows better performance than GPDBN and Pathomic Fusion which is consistent with their findings, and these results further demonstrate that the hierarchical factorized bilinear model can better mine the rich complementary information among different modalities compared to the Kronecker product. Porpoise performs similarly with PONET on TCGA-KIRC and TCGA-KIRP and outperformed HFBSurv in these two studies, this probably is due to Porpoise partitioned the survival time into different non-overlapping bins and parameterized it as a discrete survival model (Zadeh & Schmid, 2020) which works better for these two cancer types. In other cases, Porpoise performs similarly to HFBSurv. Note: the results of Porpoise are from their paper (Chen et al., 2022b).\nAdditionally, we can see that PONET consistently outperforms PONET-O and PONET-OH indicating the effectiveness of the biological pathway-informed neural network and the contribution of pathological image for the overall survival prediction.\nAblation Studies. To assess whether the impact of hierarchical factorized bilinear fusion strategy is indeed effective, we compare PONET with four single-fusion methods: 1) Simple concatenation: concatenate each modality embeddings; 2) Element-wise addition: element-wise addition from each modality embeddings; 3) Tensor fusion (Zadeh et al., 2017): Kronecker product from each modality embeddings. Table 2 shows the C-index values of different methods. We can see that PONET achieves the best performance and shows remarkable improvement over single-fusion methods on different cancer type datasets. For example, PONET outperforms the Simple concatenation by 8.4% (TCGA-BLCA), 27% (TCGA-KIRP), 15% (TCGA-LUAD), 8.0% (TCGA-LUSC), and 11.4% (TCGA-PAAD), etc.\nFurthermore, we adopted five different configurations of PONET to evaluate each hierarchical component of the proposed method: 1) Unimodal: unimodal fusion output as the final feature representation; 2) Bimodal: bimodal fusion output as the final feature representation; 3) Unimodal + Bimodal: hierarchical (include both unimodal and bimodal feature representation) fusion; 4) ARGF: ARGF (Mai et al., 2020) fusion strategy; 5) PONET: our proposed hierarchical strategy by incorporating unimodal, bimodal, and trimodal fusion output. As shown in Table 2, Unimodal + Bimodal performs better than Unimodal and Bimodal which demonstrates that Unimodal + Bimodal can capture the relations within each modality and across modalities. ARGF performs worse than Unimodal + Bimodal and far worse than PONET across all the cancer types. PONET outperforms\nUnimodal + Bimodal in 4 out of 5 cancer types indicating that three layers of hierarchical fusion can mine the comprehensive interactions among different modalities.\nTo evaluate our sparse gene-pathway network design, we compare PONET with PASNet (Hao et al., 2018) and P-NET (Elmarakeby et al., 2021) pathway architecture, PASNet performs the worst due to the fact that it only has one pathway layer in the network, and thus limited prior information was used to predict the outcome. PONET constantly outperforms P-NET across all the cancer types, which demonstrates that averaging all the intermediate layers\u2019 output for the final prediction cannot fully capture the prior information flow among the hierarchical biological structures.\nModel Interpretation. We discuss the model interpretation results for cancer type TCGA-KIRP here and the results for other cancer types are included in the Appendix C.3. To understand the interactions between different genes, pathways, and biological processes that contributed to the predictive performance and to study the paths of impact from the input to the outcome, we visualized the whole structure of PONET with the fully interpretable layers after training (Fig. 3 a). To evaluate the relative importance of specific genes contributing to the model prediction, we inspected the genes layer and used the Integrated Gradients attribution (Sundararajan et al., 2017) method to obtain the total importance score of genes, and the modified ranking algorithm details are included in the Appendix B.3. Highly ranked genes included KRAS, PSMB6, RAC1, and CTNNB1 which are known kidney cancer drivers previously (Yang et al., 2017; Shan et al., 2017; Al-Obaidy et al., 2020; Guo et al., 2022). GBN2, a member of the guanine nucleotide-binding proteins family, has been reported that the decrease of its expression reduced tumor cell proliferation (Zhang et al., 2019). A recent study identified a strong dependency on BCL2L1, which encodes the BCL-XL anti-apoptotic protein, in a subset of kidney cancer cells (Grubb et al., 2022). This biological interpretability revealed established and novel molecular features contributing to kidney cancer. In addition, PONET selected a hierarchy of pathways relevant to the model prediction, including downregulation of TGF-\u03b2 receptor signaling, regulation of PTEN stability and activity, the NLRP1 inflammasome, and noncanonical activation of NOTCH3 by PSEN1, PSMB6, and BCL2L1. TGF-\u03b2 signaling is increasingly recognized as a key driver in cancer, and in progressive cancer tissues TGF-\u03b2 promotes tumor formation, and its increased expression often correlates with cancer malignancy (Han et al., 2018). Noncanonical activation of NOTCH3 was reported to limit tumor angiogenesis and plays a vital role in kidney disease (Lin et al., 2017).\nTo further inspect the pathway spatial association with the WSI slide we adopted the co-attention survival method MCAT (Chen et al., 2021) between WSIs and genomic features on the top pathways of the second layer, visualized as a WSI-level attention heatmap for each pathway genomic embedding in Fig. 3 b (algorithm details are included in the Appendix B.4). We used the gene list from the top 4 pathways as the genomic features and trained MCAT on the TCGA-KIRP dataset for survival prediction. Overall, we observe that high attention in different pathways showed different spatial pattern associations with the slide. This heatmap can reflect genotype-phenotype relationships in cancer pathology. The high attention regions (red) of different pathways in the heatmap have positive associations with the predicted death risk while the low attention regions (blue) have negative associations with the predicted risk. By further checking the cell types in high attention patches we can gain insights of prognostic morphological determinants and have a better understanding of the complex tumor microenvironment.\nPatient Stratification. In visualizing the Kaplan-Meier survival curves of predicted high risk and low risk patient populations, we plot four variations of PONET in Fig. 4. PONET-ARGF represents the model that we use the hierarchical fusion strategy of ARGF in our pathway-informed PONET model. From the results, PONET enables easy separation of patients into low and high risk groups with remarkably better stratifica-\ntion (P-Value = 6.60e-7) in comparison to the others.\nComplexity Comparison. We compared PONET with Pathomic Fusion, GPDBN, and HFBSurv since both Pathomic Fusion and GPDBN are based on Kronecker product to fuse different modalities while GPDBN and HFBSurv modeled inter-modality and intra-modality relations which have similar consideration to our method. As illustrated in Table 3, PONET has 2.8M (M = Million) trainable parameters, which is approximately 1.6%, 3.4%, and 900% of the number of parameters of Pathomic Fusion, GPDBN, and HFBSurv. To assess the time complexity of PONET and the competitive methods, we calculate each method\u2019s floating-point operations per second (FLOPS) in testing. The results in Table 3 show that PONET needs 3.1G during testing, compared with 168G, 91G, and 0.5G in Pathomic Fusion, GPDBN, and HFBSurv. The main reason for fewer trainable parameters and the number of FLOPS lies in that PONET and HFBSurv perform multimodal fusion using the factorized bilinear model, and can significantly reduce the computational complexity and meanwhile obtain more favorable performance. PONET has one additional trimodal fusion which explains why it has more trainable parameters than HFBSurv."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this study, we pioneer propose a novel biological pathway-informed hierarchical multimodal fusion model that integrates pathology image and genomic profile data for cancer prognosis. In comparison to previous works, PONET deeply mines the interaction from multimodal data by conducting unimodal, bimodal and trimodal fusion step by step. Empirically, PONET demonstrates\nthe effectiveness of the model architecture and the pathway-informed network for superior predictive performance. Specifically, PONET provides insight on how to train biologically informed deep networks on multimodal biomedical data for biological discovery in clinic genomic contexts which will be useful for other problems in medicine that seek to combine heterogeneous data streams for understanding diseases and predicting response and resistance to treatment."
        },
        {
            "heading": "A DATA",
            "text": "Table 3 in Appendix A shows the number of patients with matched different data modalities: WSI (Whole slide image), CNV (Copy number), MUT (Mutation), RNA (RNA-Seq gene expression). For each TCGA dataset and each patient we have preprocessed data dimensions dg \u2208 R1\u00d72000 (RNA), dc \u2208 R1\u00d7227 (CNV + MUT), and dp \u2208 R1\u00d732 (WSI) which will be used for our multimodal fusion."
        },
        {
            "heading": "B METHODS",
            "text": ""
        },
        {
            "heading": "B.1 C-INDEX",
            "text": "We use concordance-index (C-index) (Harrell et al., 1982) to measure the performance of survival models. It evaluates the model by measuring the concordance of the ranking of predicted harzards with the true survival time of patients. The range of the C-index is [0, 1], and larger values indicate better performance with a random guess leading to a C-index of 0.5."
        },
        {
            "heading": "B.2 WSI REPRESENTATION LEARNING",
            "text": "It has been shown that the WSI visual representations extracted by self-supervised learning methods on histopathological images are more accurate and transferable than the supervised baseline models on domain-irrelevant datasets such as ImageNet. In this work, a pre-trained Vision Transformer (ViT) model (Wang et al., 2021a) that is trained on a large histopathological image dataset has been utilized for tile feature extraction. The model is composed of two main neural networks that learn from each other, i.e., student and teacher networks. Parameters of the teacher model \u03b8t are updated using the student network with parameter \u03b8s using the update rule represented in Eq. (11).\n\u03b8t \u2190 \u03c4\u03b8t + (1\u2212 \u03c4)\u03b8s (11)\nTwo different views of a given input H&E image x, uniformly selected from the training set I, are generated using random augmentations, i.e., u, v. Then, student and teacher models generate two different visual representations according to u and v as y1 = f\u03b8s (u) and y\u03022 = f\u03b8t (v), respectively. Finally, the generated visual representations are transformed into latent space using linear projection as p1 = g \u03b8s ( g\u03b8s (y1) ) and z\u03022 = g\u03b8t (y\u03022) for student and teacher networks, respectively. Similarly, feed-\ning v and u to student and teacher networks leads to y2 = f\u03b8s (v) , y\u03021 = f\u03b8t (u) , p2 = g\u03b8s ( g\u03b8s (y2) ) and z\u03021 = g\u03b8t (y\u03021). Finally, the symmetric objective function Lloss is optimized through minimizing the `2 \u2212 norm distance between student and teacher as Eq. (12)\nLloss = 1\n2 L (p1, z\u03022) +\n1 2 L (p2, z\u03021) (12)\nwhere L(p, z) = \u2212 p\u2016p\u20162 \u00b7 z \u2016z\u20162 and \u2016 \u00b7 \u20162 represents `2 \u2212 norm."
        },
        {
            "heading": "B.3 SPARSE NETWORK FEATURE INTERPRETATION",
            "text": "We use the Integrated Gradients attribution algorithm to rank the features in all layers. Inspired by PNET (Elmarakeby et al., 2021), to reduce the bias introduced by over-annotation of certain nodes (nodes that are members of too many pathways), we adjusted the Integrated Gradients scores using a graph informed function f that considers the connectivity of each node. The importance score of each node i, Cli is divided by the node degree d l i if the node degree is larger than the mean of node degrees plus 5\u03c3 where \u03c3 is the standard deviation of node degrees.\ndli = fan\u2212 inli + fan\u2212 outli\nadjusted Cli = f(x) =\n{ Cli dli , dli > \u00b5+ 5\u03c3\nCli , otherwise"
        },
        {
            "heading": "B.4 CO-ATTENTION BASED PATHWAY VISUALIZATION",
            "text": "After we got the ranking of top genes and pathways, we adopted the co-attention survival model (MCAT) (Chen et al., 2021) to show the spatial visualization of genomic features. We trained MACT on all our TCGA datasets, and MACT learns how WSI patches attend to genes when predicting patient survival. We define each WSI patch representation and pathway genomic features as Hbag and Gbag . The genomic features are the gene list values from the top pathways of each TCGA dataset. The model uses Gbag \u2208 RN\u00d7dg to guide the feature aggregation of Hbag \u2208 RN\u00d7dp into a clustered set of gene-guided visual concepts H\u0302bag \u2208 RN\u00d7dp , dg and dp represents the dimension for the pathway (number of genes involved in the pathway) and patch. Through the following mapping:\nCoAttnG\u2192H(G,H) = softmax ( QK>\u221a dp )\n= softmax\n( WqGH\n>W>s\u221a dp\n) WvH \u2192 Acoattn WvH \u2192 H\u0302\nwhere Wq,Ws,Wv \u2208 Rdp\u00d7dp are trainable weight matrices multiplied to the queries Gbag and key-value pair (Hbag , Hbag ), and Acoattn \u2208 RN\u00d7M is the co-attention matrix for computing the weighted average of Hbag . Here, M represents the number of patches in one slide, and N represents the number of pathways (We trained the top four pathways, so N = 4 in our study)."
        },
        {
            "heading": "C EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 NETWORK ARCHITECTURE",
            "text": "Sparse network for gene: The final gene expression embedding is hg \u2208 R1\u00d750. Pathology network: The slide level image feature representation is passed through an image embedding layer and encodes the embedding as hp \u2208 R1\u00d750. CNV + MUT network: Similarly as the pathology network, the patient level CNV + MUT feature representation is passed through an FC embedding layer and encodes the embedding as hc \u2208 R1\u00d750."
        },
        {
            "heading": "C.2 EXPERIMENTAL DETAILS",
            "text": "PONET. The latent dimensionality of the factorized matrices k is a very important tuning parameter. We tune k = [3, 5, 10, 20, 30, 50] based on the testing C-index value (Appendix Fig. 5) and the loss of training and testing plot (Appendix Fig. 6) for each dataset. We choose k to maximize the C-index value and also it should have stable convergence in both training and testing loss. For example, we choose k = 10 in TCGA-KIRP for the optimized results. We can see that in Appendix Fig. 5 the testing loss is quite volatile when k is less than 10. Similarly, we choose k = [20, 10, 20, 20, 10] for TCGA-BLCA, TCGA-KIRC, TCGA-LUAD, TCGA-LUSC, and TCGA-PAAD, respectively.\nThe learning rate and the regularization hyperparameter \u03bb for the Cox partial likelihood loss are also tunable parameters. The model is trained with Adam optimizer. For each training/testing pair, we first empirically preset the learning rate to 1.2e-4 as a starting point for a grid search during training, the optimal learning rate is determined through the 5-fold cross-validation on the training set, C-index was used for the performance metric. After that, the model is trained on all the training sets and evaluated on the testing set. We use 2e-3 through the experiments for \u03bb. The batch size is set to 16, and the epoch is 100. During the training process, we carefully observe the training and testing loss for convergence (Figure 4 in Appendix C.2). The server used for experiments is NVIDIA GeForce RTX 2080Ti GPU. CoxPH. We only include the age and gender for the survival prediction. Using CoxPHFitter from lifelines 3. DeepSurv 4. We concatenate preprocessed pathological image features, gene expression, and copy number + mutant data in a vector to train the DeepSurv model. L2 reg = 10.0, dropout = 0.4, hidden layers sizes = [25, 25], learning rate = 1e-05, learning rate decay = 0.001, momentum = 0.9. Pathomic Fusion 5. We use the pathomicSurv model which takes our preprocessed image feature, gene expression, and copy number + mutation as model input. k = 20, Learning rate is 2e-3, weight decay is 4e-4. The batch size is 16, and the epoch is 100. Drop out rate is 0.25. GPDBN 6. The learning rate is 2e-3, the batch size is 16, the weight decay is 1e-6, the dropout rate is 0.3, and the epoch is 100.\nHFBSurv 7. The learning rate is set to 1e-3, the batch size is 16, \u03bb = 3e-3, weight decay is 1e-6, and the epoch is 100.\n3https://github.com/CamDavidsonPilon/lifelines 4https://github.com/czifan/DeepSurv.pytorch 5https://github.com/mahmoodlab/PathomicFusion 6https://github.com/isfj/GPDBN 7https://github.com/Liruiqing-ustc/HFBSurv"
        },
        {
            "heading": "C.3 ADDITIONAL RESULTS",
            "text": ""
        }
    ],
    "title": "DEEP BIOLOGICAL PATHWAY INFORMED PATHOLOGY- GENOMIC MULTIMODAL SURVIVAL PREDICTION",
    "year": 2023
}