{
    "abstractText": "Deep spiking neural networks (SNNs) are promising neural networks for their model capacity from deep neural network architecture and energy efficiency from SNNs\u2019 operations. To train deep SNNs, recently, spatio-temporal backpropagation (STBP) with surrogate gradient was proposed. Although deep SNNs have been successfully trained with STBP, they cannot fully utilize spike information. In this work, we proposed gradient scaling with local spike information, which is the relation between preand post-synaptic spikes. Considering the causality between spikes, we could enhance the training performance of deep SNNs. According to our experiments, we could achieve higher accuracy with lower spikes by adopting the gradient scaling on image classification tasks, such as CIFAR10 and CIFAR100.",
    "authors": [
        {
            "affiliations": [],
            "name": "Seongsik Park"
        },
        {
            "affiliations": [],
            "name": "Jeonghee Jo"
        },
        {
            "affiliations": [],
            "name": "Jongkil Park"
        },
        {
            "affiliations": [],
            "name": "Yeonjoo Jeong"
        },
        {
            "affiliations": [],
            "name": "Jaewook Kim"
        },
        {
            "affiliations": [],
            "name": "Suyoun Lee"
        },
        {
            "affiliations": [],
            "name": "Joon Young Kwak"
        },
        {
            "affiliations": [],
            "name": "Inho Kim"
        },
        {
            "affiliations": [],
            "name": "Jong-Keuk Park"
        },
        {
            "affiliations": [],
            "name": "Kyeong Seok Lee"
        },
        {
            "affiliations": [],
            "name": "Gye Weon Hwang"
        },
        {
            "affiliations": [],
            "name": "Hyun Jae Jang"
        }
    ],
    "id": "SP:eb8e9fed821ca3bb7a4b08ec3e9ea244262bf9b0",
    "references": [
        {
            "authors": [
                "T. Bu",
                "W. Fang",
                "J. Ding",
                "P. Dai",
                "Z. Yu",
                "T. Huang"
            ],
            "title": "Optimal ann-snn conversion for high-accuracy and ultralow-latency spiking neural networks",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "M. Davies",
                "N. Srinivasa",
                "Lin",
                "T.-H",
                "G. Chinya",
                "Y. Cao",
                "S.H. Choday",
                "G. Dimou",
                "P. Joshi",
                "N. Imam",
                "S Jain"
            ],
            "title": "Loihi: A neuromorphic manycore processor with on-chip learning",
            "venue": "IEEE Micro,",
            "year": 2018
        },
        {
            "authors": [
                "S. Deng",
                "Y. Li",
                "S. Zhang",
                "S. Gu"
            ],
            "title": "Temporal efficient training of spiking neural network via gradient reweighting",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "P.U. Diehl",
                "M. Cook"
            ],
            "title": "Unsupervised learning of digit recognition using spike-timing-dependent plasticity",
            "venue": "Frontiers in computational neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "C. Duan",
                "J. Ding",
                "S. Chen",
                "Z. Yu",
                "T. Huang"
            ],
            "title": "Temporal effective batch normalization in spiking neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "B. Han",
                "G. Srinivasan",
                "K. Roy"
            ],
            "title": "Rmp-snn: Residual membrane potential neuron for enabling deeper highaccuracy and low-latency spiking neural network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "E.M. Izhikevich"
            ],
            "title": "Which model to use for cortical spiking neurons",
            "venue": "IEEE transactions on neural networks,",
            "year": 2004
        },
        {
            "authors": [
                "S. Kim",
                "S. Park",
                "B. Na",
                "S. Yoon"
            ],
            "title": "Spiking-yolo: Spiking neural network for energy-efficient object detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "A. Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Y. Li",
                "S. Deng",
                "X. Dong",
                "R. Gong",
                "S. Gu"
            ],
            "title": "A free lunch from ann: Towards efficient, accurate spiking neural networks calibration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "W. Maass"
            ],
            "title": "Networks of spiking neurons: the third generation of neural network models",
            "venue": "Neural Networks,",
            "year": 1997
        },
        {
            "authors": [
                "A. Morrison",
                "M. Diesmann",
                "W. Gerstner"
            ],
            "title": "Phenomenological models of synaptic plasticity based on spike timing",
            "venue": "Biological cybernetics,",
            "year": 2008
        },
        {
            "authors": [
                "B. Na",
                "J. Mok",
                "S. Park",
                "D. Lee",
                "H. Choe",
                "S. Yoon"
            ],
            "title": "Autosnn: towards energy-efficient spiking neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "S. Park",
                "S. Kim",
                "B. Na",
                "S. Yoon"
            ],
            "title": "T2fsnn: Deep spiking neural networks with time-to-first-spike coding",
            "venue": "In DAC,",
            "year": 2020
        },
        {
            "authors": [
                "S Park"
            ],
            "title": "Fast and efficient information transmission with burst spikes in deep spiking neural networks",
            "venue": "In DAC,",
            "year": 2019
        },
        {
            "authors": [
                "K. Roy",
                "A. Jaiswal",
                "P. Panda"
            ],
            "title": "Towards spike-based machine intelligence with neuromorphic",
            "venue": "computing. Nature,",
            "year": 2019
        },
        {
            "authors": [
                "A. Sengupta",
                "Y. Ye",
                "R. Wang",
                "C. Liu",
                "K. Roy"
            ],
            "title": "Going deeper in spiking neural networks: Vgg and residual architectures",
            "venue": "Frontiers in neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "M. Tan",
                "Q. Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wu",
                "L. Deng",
                "G. Li",
                "J. Zhu",
                "L. Shi"
            ],
            "title": "Spatiotemporal backpropagation for training high-performance spiking neural networks",
            "venue": "Frontiers in neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "Q. Yang",
                "J. Wu",
                "M. Zhang",
                "Y. Chua",
                "X. Wang",
                "H. Li"
            ],
            "title": "Training spiking neural networks with local tandem learning",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "S. Yun",
                "D. Han",
                "S.J. Oh",
                "S. Chun",
                "J. Choe",
                "Y. Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "L Zhang"
            ],
            "title": "Tdsnn: From deep neural networks to deep spike neural networks with temporal-coding",
            "venue": "In AAAI,",
            "year": 2019
        },
        {
            "authors": [
                "H. Zheng",
                "Y. Wu",
                "L. Deng",
                "Y. Hu",
                "G. Li"
            ],
            "title": "Going deeper with directly-trained larger spiking neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhou",
                "Y. Zhu",
                "C. He",
                "Y. Wang",
                "S. Yan",
                "Y. Tian",
                "L. Yuan"
            ],
            "title": "Spikformer: When spiking neural network meets transformer",
            "venue": "In ICLR,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Deep learning with deep neural networks (DNNs) have been rapidly advancing artificial intelligence (AI) technology in various fields (LeCun et al., 2015; Tan & Le, 2019). However, as AI technology continues to progress, it demands more energy and computing resources, raising concerns about sustainable development and application. Spiking neural networks (SNNs) have received considerable attention as a solution to this problem. SNNs, which have been considered third-generation artificial neural networks, enable event-based computing, resulting in sparse operations compared to DNNs (Maass, 1997). Furthermore, SNNs hold great importance as the basis for neuromorphic computing, which imitates the operations of the human brain for its exceptional energy efficiency (Davies et al., 2018; Roy et al., 2019).\n1Center for Neuromorphic Engineering, Korea Institute of Science and Technology, Seoul, Korea. Correspondence to: Seongsik Park <seong.sik.park@kist.re.kr>.\nIn ICML Workshop on Localized Learning (LLW), Honolulu, Hawaii, USA. 2023. Copyright 2023 by the author(s).\nDeep SNNs have been actively studied to combine the features of both DNNs and SNNs: the model capacity of the former and the energy efficiency of the latter. Deep SNNs have a similar synaptic topology to DNNs, with interconnected spiking neurons. While deep SNNs can leverage the advantages of both DNNs and SNNs, they face challenges in training. As an indirect training method for deep SNNs, DNN-to-SNN conversion has been proposed. Although this approach has enabled the implementation of various deep SNN models (Park et al., 2019; Kim et al., 2020; Park et al., 2020; Li et al., 2021; Bu et al., 2022), it has introduced issues, such as long inference latency (Han et al., 2020).\nRecently, to improve the training performance, a gradientbased training algorithm, which is a successful training approach for DNNs, has been applied to training deep SNNs, such as spatio-temporal backpropagation (STBP) (Wu et al., 2018). This method with surrogate gradient, which can handle the non-differentiability of spiking neurons, has proven to be effective in training deep SNNs. Based on the successful training, gradient-based training approaches have inspired further research about improving the training performance of deep SNNs (Zheng et al., 2021; Deng et al., 2022; Yang et al., 2022). Furthermore, it has enabled the expansion of deep SNNs in various applications and algorithms, including Transformer models (Zhou et al., 2023) and neural architecture search algorithms (Na et al., 2022).\nGradient-based training algorithms have allowed deep SNNs to utilize their model capacity sufficiently. However, these algorithms cannot exploit the dynamic characteristics of SNNs as they are derived from DNNs. Unlike DNNs, SNNs have spatio-temporal features, and spiking neurons transmit information in the form of spikes. Thus, to maximize the training performance of deep SNNs, we proposed a training algorithm, called gradient scale, that can consider the spike dynamics in SNNs. We were inspired by spiketiming-dependent plasticity (STDP), which is a biologically plausible training algorithm of SNNs with local spike causality (Diehl & Cook, 2015). While utilizing the training performance of gradient-based algorithms, we adjusted gradients depending on the local spike relationships, which can be defined by the causality between spikes of pre- and post-\nar X\niv :2\n30 8.\n00 55\n8v 1\n[ cs\n.N E\n] 1\nA ug\n2 02\n3\nsynaptic neurons. The proposed algorithm was evaluated on ResNet architectures (He et al., 2016) with image classification tasks, such as CIFAR10 and CIFAR100 (Krizhevsky, 2009)."
        },
        {
            "heading": "2. Related Works",
            "text": ""
        },
        {
            "heading": "2.1. Spiking Neural Networks",
            "text": "SNNs consist of spiking neurons and synapses that connect them. Mimicking the behavior of the brain, spiking neurons exchange information with binary spikes through synapses. Because of the spike-based operation, SNNs have been expected to enable event-driven computing, which is a nextgeneration and energy-efficient computing paradigm. Thus, SNNs are promising neural networks for energy-efficient artificial intelligence as a fundamental component of neuromorphic computing that mimics the operations of the human brain.\nAlthough there are various types of spiking neurons, such as izhikevich, leaky integrate-and-fire (LIF), and integrateand-fire (IF) neuron models (Izhikevich, 2004), neurons commonly operate in an integrate-and-fire manner. Spiking neurons integrate incoming information into the internal state, called membrane potential, and fire spikes whenever the potential exceeds a threshold voltage. Due to the low complexity of computation, most deep SNNs adopt relatively simple spiking neuron models, such as IF and LIF. Thus, in this work, we used an LIF neuron model, which is described as\nulj(t) = \u03c4u l j(t-1) + z l j(t), (1)\nwhere \u03c4 is a leak constant, ulj(t) and z l j(t) are the membrane potential and incoming information of the jth spiking neuron in lth layer at time step t, respectively. The incoming information, called post-synaptic potential (PSP), is caused by pre-synaptic spikes (input spikes) as\nzlj(t) = \u2211 i wlijs l-1 i (t) + b l j , (2)\nwhere w and b are the synaptic weight and bias, respectively. When the accumulated information on the membrane potential exceeds a certain threshold, spikes are generated, and the information is transmitted to adjacent neurons through synapses. Spike generation can be expressed as\nslj(t) = H(u l j(t)\u2212 vlth,j(t)), (3)\nwhere H is the Heaviside step function, and vth is a threshold voltage. When a spike is generated, the membrane potential is reset. There are mainly two reset methods: soft and hard reset can be stated as\nulj(t) = { ulj(t)\u2212 slj(t)vlth,j(t) (soft) (slj(t)\u2212 1)ulj(t) + slj(t)vlr,j(t) (hard), (4)\nwhere vr is a rest potential."
        },
        {
            "heading": "2.2. Training Methods of deep SNNs",
            "text": "Training algorithms of deep SNNs can be categorized into two approaches: indirect and direct training. Indirect training, which is represented by DNN-to-SNN conversion, transforms a pre-trained DNN model into deep SNN with the same topology, and the converted SNN only performs inference. This approach has been successfully applied to various neural network architectures (Sengupta et al., 2019; Han et al., 2020), applications (Kim et al., 2020), and neural codings (Park et al., 2019; Zhang et al., 2019; Park et al., 2020). However, it had drawbacks, such as long inference latency, due to disregarding features of SNNs during the training of DNNs. Certain studies have attempted to address these limitations with calibration (Li et al., 2021) and SNN-aware DNN training (Bu et al., 2022), but there still remain limitations that it is challenging to directly consider the dynamics of deep SNNs.\nDirect training is a promising approach for highperformance and efficient deep SNNs. It can be mainly divided into unsupervised and supervised learning; which are represented by STDP and stochastic gradient descent (SGD), respectively. STDP is a biologically plausible training algorithm that considers the causal relationship between the spikes of pre- and post-synaptic neurons (Diehl & Cook,\n2015). While it takes into account the characteristics of SNNs, its low training performance compared to other algorithms has limited its application in deep SNN training.\nThe gradient-based training algorithm of deep SNNs leverages successful training algorithms from DNNs, such as SGD and error backpropagation. One of the significant obstacles to training deep SNNs with a gradient-based algorithm was the non-differentiability of spiking neurons as depicted in Eq. 3. To overcome this, STBP with a surrogate gradient, which approximates the gradient, was proposed and could train deep SNNs successfully (Wu et al., 2018). Since then, subsequent studies on improving the training performance of deep SNNs have been published, such as threshold-dependent batch normalization (tdBN) (Zheng et al., 2021), temporal effective batch normalization (Duan et al., 2022), and temporal efficient training with timevariant target distribution (Deng et al., 2022). However, these training algorithms did not utilize local information that can improve the training performance. Recently, a study using local information for training was published, but it did not utilize relationships between spikes (Yang et al., 2022)."
        },
        {
            "heading": "3. Methods",
            "text": "With the introduction of scalable training algorithms, such as STBP, deep SNNs have become trainable with gradients. However, these existing gradient-based algorithms for deep SNNs have a limitation in that they do not effectively consider the causal relationship between spikes of pre- and post-synaptic neurons. Thus, in this work, we propose a method to exploit the local spike information in training deep SNNs with the gradient-based algorithm.\nBefore explaining the proposed method, we should define the relational expression of spikes. There are various representations for spike relation, but, in this work, we adopt\ntrace-based representation for its low computational complexity, which is suitable for deep SNNs (Morrison et al., 2008). An example of the representation is shown in Fig. 1. Spre and Spost indicate pre- and post-synaptic spike trains, respectively. The history of spike generation in each neuron is recorded in the spike trace x as follows:\nxli(t) = e -1xli(t-1) + s l i(t). (5)\nThe spike trace increases by a spike when the neuron fires and exponentially decreases at each time step of the forward (blue dotted line in Fig. 1) Each layer has pre- and postsynaptic traces (X lpre, X l post) according to its connection. With these two spike traces, we defined the relationship of spikes R as\nRl(t) = f l(X lpre(t), X l post(t)), (6)\nwhere f l is a relationship function of lth layer. During training, it is calculated in the backward path (orange dotted line in Fig. 1). We used convolution and outer product operations for the relationship function f of convolution and fully connected layers, respectively.\nWe proposed a gradient scale that adjusts the gradient of synaptic weight according to the local relationship of the spike. Inspired by STDP, the proposed algorithm encourages training with the gradient when there is a causal relationship between pre- and post-synaptic spikes. Otherwise, if there is less relationship, the algorithm hinders the training. We implemented this encouragement and hindrance by scaling the gradients of synaptic weights obtained from STBP as follows:\n\u2206W l = \u2212\u03b7g( \u03b4L \u03b4W l , Rl) = \u2212\u03b7(\u03b1 \u03b4L \u03b4W l \u25e6Rl+(1\u2212\u03b1) \u03b4L \u03b4W l\n), (7)\nwhere L is a loss, \u03b7 is a learning rate, g is a gradient scaling function, \u03b1 is a interpolation coefficient, and \u25e6 is elementwise multiplication (Hadamard product). The gradient scaling function g receives the gradients and spike relationship as inputs. As described in Eq. 7, we adopted a simple linear interpolation function for the scaling. In this work, we set the coefficient \u03b1 to 0.1 empirically."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Experimental Setup",
            "text": "To evaluate the effectiveness of the proposed gradient scaling, we set STBP (Wu et al., 2018) and tdBN (Zheng et al., 2021) as a baseline training algorithm. For each configuration, we trained deep SNN models for 300 epochs using SGD. We adopted a learning rate schedule in which the learning rate decreased to 0.1 times every 100 epochs. We used LIF neurons with the leak constant \u03c4 of 0.9, and the time step was fixed to four. We constructed deep SNN models based on ResNet20 and ResNet32 architectures and trained them on image classification datasets, such as CIFAR10 and CIFAR100. For data augmentation, Cutmix (Yun et al., 2019) was used, and for input encoding, real value encoding was applied as in other studies (Wu et al., 2018; Zheng et al., 2021)."
        },
        {
            "heading": "4.2. Experimental Results",
            "text": "The experimental results on CIFAR10 and CIFAR100 are presented in Tables 1 and 2, respectively. We compared the training results of the baseline and proposed methods in various configurations of the model architecture and reset method of spiking neurons. For fair and precise evaluations, we recorded the mean and maximum results for test accuracy and spike count after training four times on each configuration. For the accuracy on CIFAR10 dataset, the mean and maximum accuracy were improved when the proposed gradient scale was applied in all cases except for the case of ResNet20 with the hard reset, as shown in Table 1. Furthermore, the proposed approach can reduce the number\nof spikes in most cases. There are similar trends in training results on CIFAR100, as depicted in Table 2. The accuracy and spike counts are improved in most cases except ResNet32 with the soft reset and the hard reset, respectively, with the proposed methods.\nTable 3 presents the comparisons of the proposed method with other deep SNN training methods. For a fair comparison, we compared the results of a model structure similar to ResNet20 on the CIFAR10 dataset. Overall, the proposed approach shows higher training performance than the recent previous methods. In the case of soft reset, when the proposed method is applied, we achieve higher accuracy with shorter time steps than the conversion methods (Li et al., 2021; Bu et al., 2022) and local tandem learning (Yang et al., 2022). In the case of hard reset, it shows higher accuracy than tdBN (Zheng et al., 2021), but lower training performance than TET (Deng et al., 2022). It was difficult to compare other metrics, such as spike counts, in this study as they were not commonly reported in previous works."
        },
        {
            "heading": "5. Discussion",
            "text": "The proposed training algorithm can be further improved with optimization of relation function f , scaling function g, and hyperparameters, such as \u03b1 in Eq. 7. In this paper, a simple spike relation function, which only considers the positive relation between the spike traces, and scaling function was used to show the feasibility of enhancement training performance with local spike information. In order to the improvement, we can consider more complicated relation functions of pre- and post-synaptic spike traces, which consider a negative relation of the spike traces as the STDP learning rule. Furthermore, we can use other scaling functions based on theoretical analysis of deep SNNs, instead of linear interpolation as in this work."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we proposed a training method for deep SNNs with spike-dependent local information. The pro-\nposed method, which is compatible with gradient-based training algorithms, such as STBP, scales the gradient of synaptic weight according to the relationship between spike traces of adjacent neurons. We verified the effectiveness of the proposed approach with ResNet architecture on CIFAR datasets. In the future, we will improve the proposed algorithm through exploration and optimization of the spike relation function and gradient scaling function. In addition, we will evaluate the algorithm with other model architectures and datasets. We believe that by taking into account the characteristics of SNNs and utilizing local information, the training performance of deep SNNs can be improved."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the Korea Institute of Science and Technology (KIST) through 2E32260 and the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science and ICT) [NRF-2021R1C1C2010454]."
        }
    ],
    "title": "Gradient Scaling on Deep Spiking Neural Networks with Spike-Dependent Local Information",
    "year": 2023
}