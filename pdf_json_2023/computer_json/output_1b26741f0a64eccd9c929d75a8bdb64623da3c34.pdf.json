{
    "abstractText": "The problem of the low accuracy of Dissolved Gas Analysis (DGA) in diagnosing transformer faults is addressed by proposing an Improved Golden Jackal Optimization (IGJO) based Stochastic Configuration Network (SCN) method. The method of transformer fault diagnosis based on IGJO optimized SCN is proposed. Firstly, Kernel Principal Component Analysis (KPCA) is used to reduce the dimensionality of the gas data and extract the effective feature quantities. Secondly, the L2 parametric penalty term is introduced into the SCN to improve the generalisation ability of SCN in practical applications. The elite backward learning and golden sine algorithms are incorporated into the golden jackal algorithm, and the IGJO performance is tested using 13 typical test functions, demonstrating that the IGJO has greater stability and merit-seeking capability. The penalty term coefficient C of the SCN is optimised using the IGJO to develop a transformer fault diagnosis model with an Improved Golden Jackal algorithm optimised Random Configuration Network (IGJO-SCN). Finally, the feature quantities extracted by KPCA are used as the input set of the model and the different transformer fault diagnosis models are simulated and validated. The results show that the IGJO-SCN has higher transformer fault diagnosis accuracy compared to other models. INDEX TERMS Transformer; Fault diagnosis; Dissolved gas analysis; Improved Golden Jackal optimization algorithm",
    "authors": [
        {
            "affiliations": [],
            "name": "Wanjie Lu"
        },
        {
            "affiliations": [],
            "name": "Chun Shi"
        },
        {
            "affiliations": [],
            "name": "Hua Fu"
        },
        {
            "affiliations": [],
            "name": "Yaosong Xu"
        }
    ],
    "id": "SP:197a4ed0171946a3374659fb528eb862b910e187",
    "references": [
        {
            "authors": [
                "C. Zhang",
                "Y. He",
                "B. Du",
                "L. Yuan",
                "B. Li",
                "S. Jiang"
            ],
            "title": "Transformer fault diagnosis method using IoT based monitoring system and ensemble machine learning",
            "venue": "\u201d Future Generation Computer Systems, vol. 108, pp. 533-545",
            "year": 2020
        },
        {
            "authors": [
                "J. Dai",
                "H. Song",
                "G. Sheng",
                "X. Jiang"
            ],
            "title": "Dissolved gas analysis of insulating oil for power transformer fault diagnosis with deep belief network,",
            "venue": "IEEE Transactions on Dielectrics and Electrical Insulation,",
            "year": 2017
        },
        {
            "authors": [
                "W. Lu",
                "C. Shi",
                "H. Fu",
                "Y. Xu"
            ],
            "title": "Research on transformer fault diagnosis based on ISOMAP and IChOA\u2010LSSVM",
            "venue": "\u201d Iet Electr. Power Appl.",
            "year": 2023
        },
        {
            "authors": [
                "H. Wei",
                "Y. Wang",
                "L. Yang",
                "C. Yan",
                "Y. Zhang",
                "R. Liao"
            ],
            "title": "A new support vector machine model based on improved imperialist competitive algorithm for fault diagnosis of oil-immersed transformers, ",
            "venue": "J. Elect. Eng. Technol.,",
            "year": 2017
        },
        {
            "authors": [
                "H Malik",
                "S. Mishra"
            ],
            "title": "Application of gene expression programming (GEP) in power transformers fault diagnosis using DGA[J",
            "venue": "IEEE Transactions on Industry Applications,",
            "year": 2016
        },
        {
            "authors": [
                "C. Jin-qiang"
            ],
            "title": "Fault Prediction of a Transformer Bushing Based on Entropy Weight TOPSIS and Gray Theory,",
            "venue": "Comput. Sci. Eng., vol. 21,",
            "year": 2019
        },
        {
            "authors": [
                "P. LuW"
            ],
            "title": "Li and D",
            "venue": "Huang, \u201cTransformer fault diagnosis method based on graph theory and rough set,\u201d J. Intell. Fuzzy Syst., vol. 35, no. 1, pp. 223-230",
            "year": 2018
        },
        {
            "authors": [
                "J.I. Aizpurua"
            ],
            "title": "Power transformer dissolved gas analysis through Bayesian networks and hypothesis testing,",
            "venue": "Ieee Trns. Dielectr. Electr. Insul.,",
            "year": 2018
        },
        {
            "authors": [
                "A. Zou",
                "R. Deng"
            ],
            "title": "Q",
            "venue": "Mei and L. Zou, \u201cFault diagnosis of a transformer based on polynomial neural networks,\u201d Cluster Computing, vol. 22, no. S4, pp. 9941-9949",
            "year": 2019
        },
        {
            "authors": [
                "Z.M. \u017darkovi\u0107"
            ],
            "title": "Stojkovi\u0107, \u201cAnalysis of artificial intelligence expert systems for power transformer condition monitoring and diagnostics,",
            "venue": "Electr. Power Syst. Res.,",
            "year": 2017
        },
        {
            "authors": [
                "J. Li",
                "Q. Zhang",
                "K. Wang",
                "J. Wang"
            ],
            "title": "T",
            "venue": "Zhou and Y. Zhang, \u201cOptimal dissolved gas ratios selected by genetic algorithm for power transformer fault diagnosis based on support vector machine,\u201d Ieee Trns. Dielectr. Electr. Insul., vol. 23, no. 2, pp. 1198-1206",
            "year": 2016
        },
        {
            "authors": [
                "Y. Zhou",
                "X. Yang"
            ],
            "title": "L",
            "venue": "Tao and L. Yang, \u201cTransformer Fault Diagnosis Model Based on Improved Gray Wolf Optimizer and Probabilistic Neural Network,\u201d Energies, vol. 14, no. 11, pp. 3029",
            "year": 2021
        },
        {
            "authors": [
                "S K"
            ],
            "title": "A Semi-Supervised Autoencoder With an Auxiliary Task (SAAT) for Power Transformer Fault Diagnosis Using This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3265469 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License",
            "venue": "For more information,",
            "year": 2022
        },
        {
            "authors": [
                "M. Tahir",
                "S. Tenbohlen"
            ],
            "title": "Transformer Winding Condition Assessment Using Feedforward Artificial Neural Network and Frequency Response Measurements,",
            "venue": "Energies, vol. 14,",
            "year": 2021
        },
        {
            "authors": [
                "H.A. Illias",
                "W. Zhao Liang"
            ],
            "title": "Identification of transformer fault based on dissolved gas analysis using hybrid support vector machinemodified evolutionary particle swarm optimisation,",
            "venue": "Plos One,",
            "year": 1913
        },
        {
            "authors": [
                "X. Huang",
                "Y. Zhang",
                "J. Liu"
            ],
            "title": "H",
            "venue": "Zheng and K. Wang, \u201cA Novel Fault Diagnosis System on Polymer Insulation of Power Transformers Based on 3-stage GA\u2013SA\u2013SVM OFC Selection and ABC\u2013SVM Classifier,\u201d Polymers, vol. 10, no. 10, pp. 1096",
            "year": 2018
        },
        {
            "authors": [
                "X. Huang",
                "X. Huang"
            ],
            "title": "B",
            "venue": "Wang and Z. Xie, \u201cFault diagnosis of transformer based on modified grey wolf optimization algorithm and support vector machine,\u201d Ieej Trans. Electr. Electron. Eng., vol. 15, no. 3, pp. 409-417",
            "year": 2019
        },
        {
            "authors": [
                "N. Chopra",
                "M. Mohsin Ansari"
            ],
            "title": "Golden jackal optimization: A novel nature-inspired optimizer for engineering applications,",
            "venue": "Expert Syst. Appl.,",
            "year": 2022
        },
        {
            "authors": [
                "M. Khishe"
            ],
            "title": "Greedy opposition-based learning for chimp optimization algorithm,",
            "venue": "Artif. Intell. Rev.,",
            "year": 2022
        },
        {
            "authors": [
                "B.S. Yildiz",
                "N. Pholdee"
            ],
            "title": "S",
            "venue": "Bureerat, A.R. Yildiz and S.M. Sait, \u201cEnhanced grasshopper optimization algorithm using elite opposition-based learning for solving real-world engineering problems,\u201d Eng. Comput., vol. 38, no. 5, pp. 4207-4219",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xiao"
            ],
            "title": "and S",
            "venue": "Liu, \u201cStudy on elite opposition-based golden-sine whale optimization algorithm and its application of project optimization.\u201d Electronics Letters, vol. 47, no. 10, pp. 2177-2186",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yuan",
                "X. Mu",
                "X. Shao",
                "J. Ren"
            ],
            "title": "Y",
            "venue": "Zhao and Z. Wang, \u201cOptimization of an auto drum fashioned brake using the elite opposition-based learning and chaotic k-best gravitational search strategy based grey wolf optimizer algorithm,\u201d Appl. Soft. Comput., vol. 123, pp. 108947",
            "year": 2022
        },
        {
            "authors": [
                "R.O.K. S",
                "K.T.M.A.Z.A"
            ],
            "title": "Improved Harris Hawks Optimization Using Elite Opposition-Based Learning and Novel Search Mechanism for Feature Selection,",
            "venue": "Ieee Access,",
            "year": 2020
        },
        {
            "authors": [
                "E. TANYILDIZI",
                "G. DEMIR"
            ],
            "title": "Golden Sine Algorithm: A Novel Math-Inspired Algorithm,",
            "venue": "Adv. Electr. Comput. Eng., vol. 17,",
            "year": 2017
        },
        {
            "authors": [
                "G. Kaur",
                "S. Arora"
            ],
            "title": "Chaotic whale optimization algorithm,",
            "venue": "Journal of Computational Design and Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "A. Seyyedabbasi",
                "F. Kiani"
            ],
            "title": "Sand Cat swarm optimization: a nature-inspired algorithm to solve global optimization problems,",
            "year": 2022
        },
        {
            "authors": [
                "A. Faramarzi",
                "M. Heidarinejad"
            ],
            "title": "S",
            "venue": "Mirjalili and A.H. Gandomi, \u201cMarine Predators Algorithm: A nature-inspired metaheuristic,\u201d Expert Syst. Appl., vol. 152, pp. 113377",
            "year": 2020
        },
        {
            "authors": [
                "L. Abualigah",
                "A. Diabat"
            ],
            "title": "Advances in Sine Cosine Algorithm: A comprehensive survey,",
            "venue": "Artif. Intell. Rev.,",
            "year": 2021
        },
        {
            "authors": [
                "S. Mirjalili",
                "A.H. Gandomi",
                "S.Z. Mirjalili",
                "S. Saremi"
            ],
            "title": "H",
            "venue": "Faris and S.M. Mirjalili, \u201cSalp Swarm Algorithm: A bio-inspired optimizer for engineering design problems,\u201d Adv. Eng. Softw., vol. 114, pp. 163- 191",
            "year": 2017
        },
        {
            "authors": [
                "E. Nabil"
            ],
            "title": "A Modified Flower Pollination Algorithm for Global Optimization,",
            "venue": "Expert Syst. Appl.,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2022 1\ntransformer faults is addressed by proposing an Improved Golden Jackal Optimization (IGJO) based Stochastic Configuration Network (SCN) method. The method of transformer fault diagnosis based on IGJO optimized SCN is proposed. Firstly, Kernel Principal Component Analysis (KPCA) is used to reduce the dimensionality of the gas data and extract the effective feature quantities. Secondly, the L2 parametric penalty term is introduced into the SCN to improve the generalisation ability of SCN in practical applications. The elite backward learning and golden sine algorithms are incorporated into the golden jackal algorithm, and the IGJO performance is tested using 13 typical test functions, demonstrating that the IGJO has greater stability and merit-seeking capability. The penalty term coefficient C of the SCN is optimised using the IGJO to develop a transformer fault diagnosis model with an Improved Golden Jackal algorithm optimised Random Configuration Network (IGJO-SCN). Finally, the feature quantities extracted by KPCA are used as the input set of the model and the different transformer fault diagnosis models are simulated and validated. The results show that the IGJO-SCN has higher transformer fault diagnosis accuracy compared to other models.\nINDEX TERMS Transformer; Fault diagnosis; Dissolved gas analysis; Improved Golden Jackal optimization algorithm\nI. INTRODUCTION As an important electrical equipment in the power system, power transformers bear the heavy responsibility of voltage boost or voltage reduction [1]. Once a transformer failure occurs, it will seriously threaten the normal operation of the power system, so it is of great significance to improve the accuracy of transformer fault diagnosis for the stability and reliability of power system operation [2-3].\nCurrently, dissolved gas analysis in oil has become a viable method for transformer fault diagnosis in the power industry [4]. Based on the principles of Dissolved Gas Analysis (DGA), many scholars have proposed a series of\nmethods for fault diagnosis of transformers, such as the codeless ratio method [5], IEC triple ratio method [6], Duval triangle method [7], etc. Although these ratio discriminatory methods are simple, they often suffer from missing codes and lead to The accuracy of fault diagnosis is not high. Combined with mathematical models and theories, some scholars have also proposed transformer fault diagnosis methods based on grey correlation theory [8], fuzzy theory [9], rough set theory [10], Bayesian theory [11], and other aspects, but these methods generally have the disadvantage of low correct classification rate.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nIn recent years, with the development of artificial intelligence, a series of intelligent diagnosis methods using dissolved gas concentration values in oil as characteristic quantities have been generated. The literature [12] uses artificial neural networks for fault diagnosis, but the method has a long training time and is prone to fall into local optimal solutions in the process of finding the best solution; the expert system used in the literature [13] has poor autonomous learning capability and is not suitable for diagnosing overly complex systems. The literature [14] uses Support Vector Machine (SVM) for fault diagnosis, which can well solve the common problems such as small samples and non-linear classification, but the selection of its two parameters, i.e. penalty factor and kernel function parameters, has a great impact on the classification accuracy of SVM. A transformer fault diagnosis model based on Improved Grey Wolf Optimization (IGWO) and Probabilistic Neural Network (PNN) was proposed in the literature [15]. The accuracy of transformer fault diagnosis was significantly improved. However, the authors did not study the initialisation of Grey Wolf Optimization (GWO) and the improved update strategy alone does not consistently improve the GWO optimisation performance. The literature [16] proposes a DGA-based Semisupervised Autoencoder with an Auxiliary Task (SAAT) for power transformer fault diagnosis. The results show that the SAAT-based fault diagnosis method has good diagnostic performance. The proposed health feature space helps to monitor the health status of the transformer intuitively. The above method only reflects the health condition of the transformer and cannot identify specific fault types, therefore the proposed method cannot be applied for accurate fault identification. An intelligent monitoring and classification algorithm based on Frequency Response Analysis (FRA) is proposed in the literature [17] for detecting transformer winding faults. The model is able to accurately identify various winding states in transformers. However, they did not investigate the disadvantages of Artificial Neural Network (ANN), whose parameters are difficult to determine, or the inability to optimise the diagnostic accuracy of the proposed model using ANN alone.\nSynthesizing the above research methods in the literature [8]-[17], there are difficulties in solving multiclassification problems with SVM, while ANN requires a large amount of sample data, the model validity is not strong, and the stability and robustness of fuzzy theory is poor. Compared with other neural network architectures, SCN not only has good generalization characteristics of stochastic learning, but also the number of nodes in its implicit layer is generated gradually based on the supervision mechanism, which is a good solution to the problem that the number of nodes in the implicit layer of RVFL model is difficult to determine, and also has more\nextensive applications in the field of fault diagnosis. Therefore, the random configuration network is chosen as the basic model in this paper.\nThe value of the penalty term coefficient C of the SCN is also very elaborate and the parameters can be optimized using an optimization algorithm. Common optimisation algorithms include the particle swarm algorithm [18], the artificial bee colony algorithm [19] and the grey wolf algorithm [20]. The Golden Jackal Optimisation (GJO) algorithm, first proposed in 2022, is able to search the objective space better and has a higher success rate in solving optimisation problems [21]. Compared with traditional optimization algorithms, the Golden Jackal optimization algorithm has significant advantages in terms of global search capability and search accuracy, but similar to traditional methods, the drawback that the GJO is highly susceptible to local optimal solutions due to the reduction of population diversity in the late iteration still exists. Due to the high dimensionality, non-linearity and complexity of the original transformer fault data, higher diagnostic accuracy cannot be achieved with a single machine learning approach, so it is necessary to reduce the dimensionality of the fault data.\nThree shortcomings of SCN-based transformer fault diagnosis are summarised: 1) the SCN may be overfitted when the number of training samples is small and the value of the penalty coefficients is very delicate; 2) the noise of transformer fault data reduces the stability of the model; 3) the study of the optimization algorithm is not targeted and cannot significantly improve the optimization performance. Therefore, this paper proposes a transformer fault diagnosis method based on KPCA and IGJO-SCN methods. It is worth noting that the innovations and contributions of this paper are mainly divided into the following four improvement methods. Firstly, KPCA is used to extract the features of the DGA data to reduce the influence of noise on the diagnosis results. In addition, the GJO can be improved by the following two methods to obtain the IGJO. elite backward learning is proposed to improve the initial diversity of golden jackal populations. And a golden sine strategy is proposed to improve the convergence speed and accuracy of the GJO. The IGJO can then be obtained from the two improved methods mentioned above, and benchmark functions are used to test the optimization performance of the IGJO and other algorithms. The results show that the GJO has the best optimization performance. Finally, the introduction of L2 parametric penalty terms in SCNs is proposed to address the shortcomings of stochastic configuration networks. The elite backward learning, golden sine strategy is incorporated into the traditional golden jackal algorithm, and the performance of the improved golden jackal optimization algorithm is tested using a typical test function, demonstrating that the IGJO has better search capability and optimization finding accuracy. Afterward,\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nthe IGJO is used to dynamically optimize the parameters of the random configuration network, and the dimensionality-reduced feature quantities are fed into fault diagnosis models such as IGJO-SCN, GJO-SCN, WOASCN, MPA-SCN, and SCSO-SCN. The results show that the proposed method has the highest diagnostic accuracy, the shortest diagnostic time, the best effectiveness, and the strongest significance. These results can prove that the proposed method has the best diagnostic performance."
        },
        {
            "heading": "II. FAULT FEATURE EXTRACTION",
            "text": "In traditional intelligent diagnosis methods, the characteristic gases generally used include H2, CH4, C2H4, C2H6, C2H2, and the total hydrocarbon TH, which is the sum of the four gases CH4, C2H4, C2H6, and C2H2. The impact of data preprocessing on the correct fault diagnosis rate is ignored when these six gases are used as input parameters. Therefore, in this paper, these 6 gases are paired in pairs to generate a DGA ratio that may be used for transformer fault diagnosis. Since 2 volume fraction ratios of the same gas, e.g. H2/CH4 and CH4/H2, provide the same discriminability to the classifier, only 15 gas ratio combinations are listed, as shown in Table 1.\nTABLE 1. Ratio of dissolved gases in oil\nNumber Volume to fraction ratio\nX1 H2/CH4 X2 H2/C2H4 X3 H2/C2H6 X4 H2/C2H2 X5 CH4/C2H4 X6 CH4/C2H6 X7 CH4/C2H2 X8 C2H4/C2H6 X9 C2H4/C2H2 X10 C2H6/C2H2 X11 H2/TH X12 CH4/TH X13 C2H4/TH X14 C2H6/TH X15 C2H2/TH\nKPCA is a non-linear feature extraction method that effectively eliminates redundancy and spatial correlation between data and extracts non-linear feature primitives containing the main data information. It makes up for the shortcomings of traditional PCA algorithms in processing non-linear data, and is commonly used in the detection of anomalies in industrial control systems.\nWhen processing the non-linear DGA data, considering the high dimensional characteristics of the data, KPCA is used to reduce the dimensionality of the DGA data. KPCA uses kernel functions to achieve dimensionality reduction of non-linear data in a high-dimensional space, avoiding the problem of linear indistinguishability of the feature vectors. Therefore, in this paper, KPCA is used for feature extraction of 15 feature ratios, and the results are shown in Figure 1.\nAs can be seen from Figure 1, the first six principal components correspond to eigenvalues greater than one, and\nthe cumulative contribution rate has exceeded 95%, which can obtain the majority of the required feature information, so the dimensionality of the feature variable after dimensionality reduction is 6.\nTo illustrate the superiority of KPCA, it was compared with the traditional DGA-based transformer fault diagnosis methods (Rogers four-ratio and IEC three-ratio methods). The features of each DGA method were input into SCN for fault diagnosis, and the results of the comparison of model feature input and correct diagnosis rate are shown in Table 2.\nThe data before and after the dimensionality reduction of KPCA were used for fault diagnosis by SCN, and the running time and correct diagnosis rate were compared before and after the dimensionality reduction, as shown in Table 3."
        },
        {
            "heading": "III. GOLDEN JACKAL ALGORITHM",
            "text": "The Golden Jackal optimization (GJO) algorithm is a metaheuristic algorithm proposed in 2022 based on the process of prey predation by pairs of golden jackals, which has the characteristics of fast optimisation and good convergence.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nThe GJO algorithm is designed and optimised by modeling the specific hunting strategy of the paired golden jackal, and the algorithm is optimised as follows.\nStage 1: Exploration stage (prey search stage). The golden jackal searches and waits for its prey according to its nature and instincts, with the male leading and the female following the male. The couple follow their prey and update their position according to equation (1).\n( ) ( ) ( ) ( )\n( ) ( ) ( ) ( ) 1\n2\nPr\nPr\nM M\nFM FM\nY t Y t E Y t rl ey t\nY t Y t E Y t rl ey t\n = \u2212 \u2212  = \u2212 \u2212\n(1)\nIn equation (1): ( )Pr ey t is the location of the prey; ( )MY t and ( )FMY t are the current male and female jackal positions respectively; ( )1Y t and ( )2Y t are the positions of male and female jackals respectively after changing with the prey; E is the energy change of the prey when avoiding the grasp of the golden jackal, E is determined by equation (2)\n( )( )\n1 0\n0\n1 1\n*\n2* 1\n* 1 /\nE E E\nE rand\nE c t T\n =  = \u2212  = \u2212\n(2)\nIn equation (2): 0E is the initial energy of the prey; t is the current number of iterations; T is the maximum number of iterations; 1c is a constant with a constant value of 1.5; 1E gradually decreases from 1.5 to 0 during the iterations, indicating that the energy of the prey gradually decreases during the capture process. ( )Prrl ey t is a function that simulates the trajectory of the prey where rl is determined by equation (3)\n0.05 ( )rl LF y= (3)\nIn equation (3), ( )LF y is the levy flight distribution\nfunction.\nFinally, the prey position is updated by equation (4)\n( ) ( ) ( )( )1 21 / 2Y t Y t Y t+ = + (4)\nStage 2: The hunting stage (encirclement and capture of prey). When the prey is discovered by the golden jackal and starts to hide from the golden jackal, the energy gradually decreases during the hiding process and the male and female jackals start to surround the prey and capture it. This process is mathematically modelled as\n( ) ( ) ( ) ( )\n( ) ( ) ( ) ( ) 1\n2\nPr\nPr\nM M\nFM FM\nY t Y t E rl Y t ey t\nY t Y t E rl Y t ey t\n = \u2212 \u2212  = \u2212 \u2212\n(5)\nAll variables have the same meaning as in equation (1),\nand finally the prey position is updated by equation (4).\nThe condition for the golden jackal to switch from the exploration phase to the hunting phase is determined by E. When |E|>1, the golden jackal performs the exploration phase in different areas, and when |E|<1, the golden jackal surrounds and attacks the prey to capture it. The pseudo code of GJO is follows Table 4.\nTABLE 4. The pseudo code of GJO\nAlgorithm:GJO Initialize the random prey and its fitness Yi(i=1,2,\u2026,N)\nWhile(t<T)\nCalculate the fitness values of preys Y1 = best prey(Male Jackal position) Y2 = second best prey(Female Jackal position) for(each prey)\nUpdate the escape energy E according to Eq.(2) Update rl according to Eq.(3) if(|E|>=1) Update the prey position according to Eq.(1)(4) if(|E|<1) Update the prey position according to Eq.(4)(5)\nend for t=t+1\nend while\nIV. IMPROVED GOLDEN JACKAL ALGORITHM\nA. ELITE REVERSE LEARNING STRATEGIES\nThe main idea of Opposition-Based Learning (OBL) [22] is to simultaneously compute and determine the candidate solution and the corresponding inverse solution, and select the best candidate solution from them by its fitness value. The OBL strategy can improve the population diversity more effectively and prevent the algorithm from converging too early.\nDefinition 1 (Inverse solution): A feasible solution in a\npopulation is ( )1 2, , , Dx x x=X , ,j j jx a b   , 1 j D  ,\nthen its inverse solution is ( )1 2, , , Dx x x=X ,\n( ) jj j jx r a b x= + \u2212 , and r is a random number obeying a\nuniform distribution of [0,1].\nOBL can better extend the search range of the population and improve the performance of the algorithm. However, OBL has a certain degree of randomness in generating its opposite solution. Each randomly generated candidate has a 50% probability of being far away from the optimal solution of the problem compared to its opposite individual.\nThe main idea of Elite Opposition-Based Learning (EOBL)\n[23], which aims to improve the solution quality of reverse learning, is to generate more promising solutions by evaluating the opposite solutions of elite solutions. The opposite solution is more likely to be located where the global optimum is located. This mechanism has been successfully applied to improvements of the Whale algorithm (WOA) [24], the Grey Wolf algorithm (GWO) [25] and the Harris Hawk algorithm (HHO) [26], among others.\nDefinition 2 (elite inverse solution): The extreme value point of an ordinary individual in the population is defined as the corresponding elite individual,\n( )( )E E E E,1 ,2 ,, , , 1,2, ,i i i i DX X X X i N= = in the population,\nthen its elite inverse solution ( )E E E E,1 ,2 ,, , ,i i i i DX X X X= is defined as shown in equation (6):\nE , ,( ) E i j j j i jX lb ub E=  + \u2212 (6)\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nwhere,  is a random number of [0,1]; E\n, ,i j j jX lb ub   ;\n,min{ }j i jlb X= , ,max{ }j i jub X= , are the lower and upper bounds of the dynamic boundary, respectively. If , E i jX exceeds the bound, then the mathematical model is corrected using the stochastic method as shown in equation (7):\n, ( ) E i j j jX rand lb ub= + (7)\nB. GOLDEN SINE PREDATION MECHANISM\nThe golden sine algorithm (gloden-SA) [27] is a metaheuristic algorithm proposed by Tanyildizi in 2017. Based on the relationship between the sine function and the unit circle, gloden-SA can traverse all points on the sine function, i.e. traverse the entire unit circle. The scanning of the whole unit circle is similar to the search of the search space in an optimisation problem. At the same time, the introduction of the golden partition coefficient in its position updating process makes each iteration both narrow the search area and traverse the excellent solution region, which improves the local mining ability and solution accuracy while speeding up the convergence of the algorithm.\nThe core process of the gloden-SA algorithm is the way in which its positions are updated. First, n individual positions are randomly generated, each corresponding to an initial solution, and the position is updated for each individual by means of equation (8).\n( ) ( )1 1 2 1 1 2sin sin t t t t i i i ix x r r r c P c x + = \u2212 \u2212 (8)\nWhere: ,1 ,2 ,, , , t t t t i i i i dx x x x =   ,\n( ) ( )( )max, 1,2, , , 1,2, ,i n t t= = , ,ti dx denote the position of the ith individual in the dth dimension at the tth iteration;\nt\niP is the global optimal position at the tth iteration; r1, r2 are\nrandom numbers belonging to [0, 2\u03c0] and [0, \u03c0] respectively, r1 determines the distance of movement of individual i at the next iteration, and r2 determines the direction of movement of individual i at the next iteration [18]; c1, c2 are introduced as golden partition coefficients into the position update formula, ( )1 1c a g bg= \u2212 + , ( )2 1c ag b g= + \u2212 , the initial values of a and b are -\u03c0, \u03c0 respectively, and the golden partition numbers ( 5 1)/2g = \u2212 , c1, c2 narrow the search\nspace and guide the individual at the current position to the global optimal position during the iteration.\nC. IGJO ALGORITHM\nIn response to the shortcomings of the basic golden jackal optimisation algorithm, the paper proposes an elite reverse learning mechanism with golden sine for Golden Jackal Optimisation (IGJO). The basic GJO algorithm uses a random initialisation method to initialise the population without a priori knowledge, which is prone to the problem of poor diversity of the golden jackal population. The quality of the initial population has a large impact on the algorithm's optimisation search performance, and a good initial population is beneficial to global optimisation search.\nTherefore, the thesis introduces an elite backward learning mechanism to improve the golden jackal algorithm, using a population selection method to rank the current golden jackal population and its inverse population by their fitness values, and select the best golden jackal individuals as the new generation individuals, so as to improve the population quality of the GJO algorithm. Firstly, the GJO algorithm incorporates the EBOL strategy to increase the diversity of the initialized population, increase the search space and lay the foundation for global optimization; secondly, at each iteration, the EBOL strategy can generate the inverse solution far away from the local extremum, guiding the GJO algorithm to jump out of the local extremum and enhancing the ability of the algorithm to search globally. In addition, the EBOL strategy uses a dynamic boundary tracking search mode to locate individuals in a progressively smaller search region, improving the convergence accuracy and speed of the GJO algorithm.\nThe position of the prey is the guiding mark of the movement of the golden jackal individuals. In the golden jackal algorithm incorporating the golden sine algorithm, the golden jackal moves in a golden sine manner to arrest the prey, and in the process of each iteration, the ordinary individual will exchange information with the best individual, fully absorbing the information of the position gap with the best individual to improve the algorithm's performance and accuracy of the search for the best. In addition, the search area is gradually reduced according to the golden partition coefficient, and the update distance and direction of the golden jackal individuals are adjusted according to r1 and r2 to guide the individuals to quickly approach the optimal value in the high-quality solution area, which optimizes the traditional GJO algorithm's search method and improves the algorithm's search speed and exploitation capability. Figure 2 is the flowchart of IGJO.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. TIME COMPLEXITY ANALYSIS OF IGJO\nThe time complexity of the original golden jackal algorithm is O(N \u00d7 D \u00d7 T). In the IGJO algorithm, the time complexity of the elite reverse learning strategy is O(N \u00d7 D), the time\ncomplexity of calculating the fitness value is O(N \u00d7 D), and the time complexity of calculating the individual position update is O(N \u00d7 D). The time complexity of the IGJO algorithm is the same as that of the basic GJO algorithm, indicating that the 2 improved strategies do not increase the computational burden of the golden jackal algorithm."
        },
        {
            "heading": "V. SIMULATION EXPERIMENTS AND ANALYSIS OF",
            "text": "RESULTS\nA. INITIAL PARAMETER SETTING\nIn this paper, Whale Optimization Algorithm (WOA) [28], the Sand Cat Swarm Optimization (SCSO) [29], Marine Predators Algorithm (MPA) [30], Sine Cosine Algorithm (SCA) [31], Salp Swarm Algorithm (SSA) [32] and Golden Jackal Optimization Algorithm (GJO) were selected for comparison with the elite reverse learning and golden sine golden jackal algorithm IGJO. The initial population size was set to 30 and the maximum number of iterations was set to 500. the parameters of the above algorithms are shown in Table 5.\nTABLE 5. Parameter settings\nAlgorithm Parameters\nWOA The parameter \u03b1 changed from 2 to\n0, b = 1\nSCSO The sensitivity range rG changed from 2 to 0, R changed from \u22122rG\nto 2rG\nMPA \ud835\udc39\ud835\udc34\ud835\udc37\ud835\udc60 = 0.2 \ud835\udc43 = 0.5 SCA Constant \u03b1 = 2 and probability update was 0.5 SSA Probability update was 0.5 GJO \ud835\udc361 variable is varied from 1 to 2 IGJO \ud835\udc361 variable is varied from 1 to 2\nB. TEST FUNCTIONS\nIn this paper, 13 commonly used benchmarking functions are selected to test the performance of each algorithm, and the relevant properties of the functions are shown in Table 6.\n2\n6\n1\n( ) ( 0.5) n\ni\ni F x x = = + 30 [-100,100] 0\n4\n7\n1\n( ) [0,1) n\ni\ni F x ix rand = = + 30 [-1.28,1.28] 0\n2\n8\n1\n( ) 10 (2 ) 10 n\ni i\ni F x x s cos x = = \u2212 + 30 [-500,500] -418.9829xDim\n2\n9 1 ( ) 10 (2 ) 10\nn\ni ii F x x cos x = = \u2212 + 30 [-5.12,5.12] 0\n2\n10 1 1\n1 1 ( ) 20exp( 0.2 ) exp( cos(2 )) 20 n n\ni ii i F x x x e n n  = = = \u2212 \u2212 \u2212 + +  30 [-32,32] 0\n2\n11 1 1\n1 ( ) cos( ) 1\n4000\nn n i ii i x F x x\ni= = = \u2212 +  30 [-600,600] 0\n 2 212 1 11\n1\n( ) 10sin( ) ( 1) [1 10sin ( )]\n( ,10,100,4)\n( ) 1\n1 , ( , , , ) { 0 4\n( )\nn i ii\nn ii\nm\ni i\ni i i i\nm\ni i\nF x y y n\nu x\nk x a x a x\ny u x a k m a x a\nk x a x a\n   +=\n=\n= + \u2212 + +\n\u2212  +\n= + = \u2212  \n\u2212 \u2212  \u2212\n\n 30 [-50,50] 0\n2 2 2\n13 1\n2 2\n1\n( ) 0.1 {10sin 3 ( 1) [1 sin (3 1)]\n( 1) [1 sin (3 )]} ( ,5,100,4)\nn\ni i ii\nn\nn n ii\nF x x x x\nx x u x\n \n\n=\n=\n=  + \u2212 + +\n+ \u2212 + +\n  30 [-50,50] 0\nC. ANALYSIS OF EXPERIMENTAL RESULTS\nEach of the above algorithms was run 30 times on each benchmark test function to obtain the average value, and the experimental results are shown in Table 7.\n8 VOLUME XX, 2022\nStd 5.53E-01 8.70E-01 4.10E-01 1.15E+05 1.27E+03 7.74E-01 9.81E-02\nF6\nBest 5.22E-02 7.60E-01 1.37E-08 4.44E+00 3.35E-08 1.61E+00 6.93E-05\nWrost 9.47E-01 2.76E+00 2.37E-07 1.34E+02 4.91E-07 3.50E+00 1.14E-03\nAve 4.36E-01 1.97E+00 5.16E-08 1.97E+01 1.16E-07 2.62E+00 6.17E-04\nStd 2.31E-01 5.22E-01 4.70E-08 2.87E+01 9.57E-08 4.49E-01 2.38E-04\nF7\nBest 2.07E-04 4.92E-06 3.52E-04 4.63E-03 5.07E-02 1.78E-05 3.26E-06\nWrost 2.07E-02 1.52E-03 2.77E-03 2.06E+00 2.92E-01 2.96E-03 3.25E-04\nAve 5.16E-03 1.47E-04 1.37E-03 1.58E-01 1.71E-01 6.37E-04 9.15E-05\nStd 6.08E-03 2.73E-04 6.19E-04 3.70E-01 6.62E-02 6.25E-04 7.57E-05\nF8\nBest -1.26E+04 -9.28E+03 -1.02E+04 -4.57E+03 -9.74E+03 -6.46E+03 -6.78E+03\nWrost -7.71E+03 -5.51E+03 -7.98E+03 -3.38E+03 -6.27E+03 -2.44E+03 -2.90E+03\nAve -1.07E+04 -6.90E+03 -9.03E+03 -3.80E+03 -7.56E+03 -3.99E+03 -3.56E+03\nStd 1.78E+03 8.83E+02 5.06E+02 2.95E+02 7.17E+02 1.08E+03 1.39E+05\nF9\nBest 0.00E+00 0.00E+00 0.00E+00 3.45E-01 2.98E+01 0.00E+00 0.00E+00\nWrost 0.00E+00 0.00E+00 0.00E+00 1.04E+02 1.29E+02 0.00E+00 0.00E+00\nAve 0.00E+00 0.00E+00 0.00E+00 2.98E+01 5.71E+01 0.00E+00 0.00E+00\nStd 0.00E+00 0.00E+00 0.00E+00 3.11E+01 1.98E+01 0.00E+00 0.00E+00\nF10\nBest 8.88E-16 8.88E-16 4.77E-13 7.07E-02 5.68E-01 4.44E-15 8.88E-16\nWrost 7.99E-15 8.88E-16 3.44E-12 2.03E+01 4.34E+00 7.99E-15 8.88E-16\nAve 4.56E-15 8.88E-16 1.38E-12 1.12E+01 2.64E+00 7.40E-15 8.88E-16\nStd 2.72E-15 0.00E+00 7.20E-13 9.57E+00 9.24E-01 1.35E-15 0.00E+00\nF11\nBest 0.00E+00 0.00E+00 0.00E+00 3.14E-02 5.98E-04 0.00E+00 0.00E+00\nWrost 0.00E+00 0.00E+00 0.00E+00 2.11E+00 3.89E-02 0.00E+00 0.00E+00\nAve 0.00E+00 0.00E+00 0.00E+00 9.75E-01 1.48E-02 0.00E+00 0.00E+00\nStd 0.00E+00 0.00E+00 0.00E+00 3.86E-01 9.66E-03 0.00E+00 0.00E+00\nF12\nBest 5.54E-03 4.26E-02 1.66E-09 6.82E-01 4.09E+00 7.46E-02 1.29E-05\nWrost 4.91E-02 3.29E-01 1.26E-05 1.15E+06 1.77E+01 3.27E-01 4.58E-02\nAve 2.03E-02 1.21E-01 6.78E-07 4.31E+04 8.54E+00 1.92E-01 3.83E-03\nStd 1.03E-02 6.54E-02 2.60E-06 2.10E+05 3.40E+00 7.15E-02 9.38E-03\nF13\nBest 1.58E-01 1.51E+00 4.52E-08 4.67E+00 1.42E-02 1.27E+00 1.66E-05\nWrost 1.05E+00 2.88E+00 8.73E-02 2.11E+05 4.93E+01 2.12E+00 1.23E-02\nAve 4.98E-01 2.38E+00 1.34E-02 1.61E+04 1.73E+01 1.73E+00 1.59E-03\nStd 2.34E-01 3.78E-01 2.22E-02 5.01E+04 1.53E+01 2.11E-01 2.24E-03\nAs can be seen from Table 7, for the 12 test functions, IGJO outperforms WOA, SCSO, MPA, SCA, SSA and GJO in terms of the optimal value, mean and standard deviation. For the test functions F1 to F4, IGJO can calculate the optimal value of 0. The optimal value and average value are hundreds of orders of magnitude higher than the other five algorithms, and the standard deviation is 0, so the robustness is better. For the test functions F6 and F7, IGJO outperforms the other six algorithms. For functions F8 to F13, the accuracy and robustness of the IGJO optimization are optimal. The above shows that the two methods, elite backward learning and golden sine, are effective in optimizing the GJO.\nFigures 3 and 4 show the convergence curves of the test functions used, which more clearly demonstrate the effect of the IGJO on the search for an optimum. From Figures 3 and 4, it can be seen that the convergence accuracy and convergence speed of the improved IGJO are better than those of WOA, SCSO, MPA, SCA, SSA, and GJO, indicating that the IGJO with the introduction of two strategies outperforms the single-strategy improved Golden Jackal algorithm and has the excellent merit-seeking capability.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n(a) Convergence curves of F1\n(e) Convergence curves of F5\n(c) Convergence curves of F3\n(b) Convergence curves of F2\n(f) Convergence curves of F6\n(d) Convergence curves of F4\n8 VOLUME XX, 2022\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nA box plot is a statistical chart that uses five statistics in the data: minimum, upper quartile, median, lower quartile, and maximum to describe the data. The top and bottom segments of a box plot represent the maximum and minimum values of the data respectively. The top and bottom segments of the box plot represent the third quartile and first quartile respectively. The thick line in the middle of the box plot represents the median of the data. It provides a visual indication of the outliers, the dispersion of the distribution, and the symmetry of the data. The stability of the algorithms was statistically analyzed using the box plots, where the WOA, SCSO, MPA, GJO, and IGJO algorithms were\nselected for comparative testing and the results of 30 independent experiments were recorded. Figure 5 shows the statistical results of the algorithms on F1, F5, F7, and F10. On all the functions tested, the deviation between the minimum and maximum values obtained by the proposed IGJO algorithm is relatively small, while the deviation between the minimum and maximum values obtained by the WOA algorithm on F1 is larger and the deviation between the minimum and maximum values obtained by the MPA algorithm on F7 and F10 is larger. This indicates that the proposed IGJO is stable and can keep the deviations within a reasonable range with high performance.\nFIGURE 5. Box plot statistics results\nD. STATISTICAL TESTS AND MAE RANKING\nIn the comparison of the computational power of the above seven algorithms, it is not sufficient to use only the three indicators of the best value, the worst value, the mean and the standard deviation to evaluate them, so the Wilcoxon rank sum test was used in this paper to test the results, as shown in Table 8. When p<0.05, it means that there is a difference\nbetween the two algorithms in terms of the effect of finding the best value, and vice versa.\nTABLE 8. Wilcoxon rank sum test results\nfunctio\nn WOA SCSO MPA SCA SSA GJO\nF1 1.21E12 1.21E12 1.21E12 1.21E12 1.21E12 1.21E12 F2 6.48E-\n12\n6.48E-\n12\n6.48E-\n12\n6.48E-\n12\n6.48E-\n12\n6.48E-\n12\nF3 1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\nF4 1.21E12 1.21E12 1.21E12 1.21E12 1.21E12 1.21E12\nWOA SCSO MPA GJO IGJO -5.00E-23\n0.00E+00\n5.00E-23\n1.00E-22\n1.50E-22\n2.00E-22\n2.50E-22\nV al\nu e\nWOA SCSO MPA GJO IGJO\n(a) Box-plot of F1 (b) Box-plot of F5\nWOA SCSO MPA GJO IGJO -5\n0\n5\n10\n15\n20\n25\n30\n35\nV al\nu e\nWOA SCSO MPA GJO IGJO\nWOA SCSO MPA GJO IGJO -0.004\n-0.002\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\nV al\nu e\nWOA SCSO MPA GJO IGJO\nWOA SCSO MPA GJO IGJO -1.00E-12\n0.00E+00\n1.00E-12\n2.00E-12\n3.00E-12\n4.00E-12\nV al\nu e\nWOA SCSO MPA GJO IGJO\n(c) Box-plot of F7 (d) Box-plot of F10\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nF5 3.02E-\n11\n3.02E-\n11\n3.02E-\n11\n3.02E-\n11\n3.02E-\n11\n3.02E-\n11\nF6 3.02E11 3.02E11 3.02E11 3.02E11 3.02E11 3.02E11 F7 4.98E-\n11\n0.7171\n9\n3.02E-\n11\n3.02E-\n11\n3.02E-\n11\n1.47E-\n07\nF8 1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\nF9 NaN NaN NaN 1.21E12 1.21E12 NaN\nF10 1.26E-\n08 NaN\n1.21E-\n12\n1.21E-\n12\n1.21E-\n12\n1.18E-\n13\nF11 NaN NaN NaN 1.21E-\n12\n1.21E-\n12 NaN\nF12 1.31E08 3.34E11 3.02E11 3.02E11 3.02E11 3.02E11 F13 3.02E-\n11\n3.02E-\n11\n9.23E-\n01\n3.02E-\n11\n3.02E-\n11\n3.02E-\n11\nAs can be seen from Table 8, for all functions except F9, F10 and F11, the p-values between IGJO and the other algorithms are much less than 0.05, indicating that IGJO has better performance in finding the optimum. The presence of NaN in Table 7 indicates that the corresponding algorithms have found the global optimum.\nMean Absolute Error (MAE) [33] represents the average of the absolute values of the differences between the results and the actual values. The mathematical definition of MAE is shown in equation (14):\n1\nn i ii m k\nMAE n\n= \u2212 =  \u2223 \u2223\n(9)\nwhere mi is the average of the optimal solutions computed for each algorithm on each test function, ki is the theoretical optimal value of each test function, and n is the number of test functions. The smaller the value of MAE, the higher the computational performance of the algorithm. The results of the ranking of the algorithms according to the MAE values are shown in Table 9.\nTABLE 9. Sort results\nAlgorithm MAE Rank\nIGJO 2.74E+02 1\nGJO 3.10E+02 2\nSCSO 5.33E+02 3\nMPA 6.96E+02 4\nSSA 7.45E+02 5\nWOA 4.19E+03 6\nSCA 1.07E+04 7\nFrom Table 8, we can see that the MAE value of IGJO is the smallest, which means that IGJO has better performance in finding the best."
        },
        {
            "heading": "VI. REGULARISED RANDOM CONFIGURATION NETWORK",
            "text": "The stochastic configuration network (SCN) is a powerful class of stochastic learning models with a stronger generalization performance than traditional stochastic learning models, as its hidden layer structure can be generated adaptively based on training effects [34]. The basic\nidea is to start with a smaller network and then gradually add new hidden nodes with random parameters until an acceptable tolerance is reached.\nFor a dataset {( , )}i iD x y= , 1, ,i I= , 1 d ix R  ,\n1 m iy R  ; where ix denotes the dataset feature attribute data\nand iy denotes the data label attribute.\n( ) T 1\n1\n1 ( ) L l lL l l\nl f   = \u2212\n\u2212\n= +x w x b (10)\nwhere: ,1 ,2 ,[ , , , ]l l l l n   = - output weight; L- number of layers of the neural network, a positive integer; ( )l -\nactivation function of the lth hidden neuron;  , m d l v v   \u2212w -\nthe weight of the lth hidden neuron;  , m d lb v v   \u2212 - bias of\nthe lth hidden l neuron; 1 1 1,1 1, , L L L L m f f e e \u2212 \u2212 \u2212 \u2212  = \u2212 =   e -\nResiduals of the L-1st hidden layer node; f -Actual label type.\nIf the specified error tolerance is not reached, the model will generate new hidden layer nodes under the constraints and satisfy the trend of decreasing deviation as the number of nodes increases, eventually achieving lim 0L L\u2192 \u2212 =f f .\nwhere: 1\nL\nL L Li= =f \u03b2 \u03c3 .\nAt this point, the model outputs the following weights\nbased on the updated model:\n1 2 3\n1\na[ ] rgmin L\nL t l\nl\nf   \n=\n= \u2212 \u03b2 \u03b2, \u03b2,\u03b2 \u03b2 \u03c3, \u03b2 (11)\nWhere: ,1 ,2 ,, , ,l l l l m     =  \u03b2 \u03b2 \u03b2 \u03b2 . When the number of\ntraining samples is small, in order to avoid the phenomenon of overfitting in the SCN, the author introduces the L2 parametric penalty term in the objective function of the model; at the same time, the empirical risk and structural risk are minimized, so as to improve the generalization performance of the network. At this point, the objective function of the SCN is improved as follows:\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 2 3 1\ni[ ] arg m n l\nL\nL l\nl\nf C    \n=\n   +\n \u2212  =  \u03b2 \u03b2 \u03b2, ,\u03b2 \u03b2, \u03b2 \u03c3 (12)\nwhere:C - the penalty term weighting factor of the model. The output weights are defined according to the least\nsquares method as :\nT 1( ) \u2212= +\u03b2 GG I GT (13)\nwhere: ( )T= +G w x b - implied layer output.\nWhere: T\n1 2 3, , , L      =  \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 ."
        },
        {
            "heading": "VII. TRANSFORMER FAULT DIAGNOSIS EXAMPLE ANALYSIS",
            "text": "The data in this paper was obtained from a grid in the northeast of the State Grid and the IEC TC 10 database. The data set contains six different operating states of transformers: high energy discharge, low energy discharge, partial discharge, high-temperature overheating, medium and lowtemperature overheating, and normal, each operating state contains five fault gas characteristics such as H2, C2H2, C2H4, C2H6 and CH4, of which 720 data sets were randomly selected and scaled by 3: 1 ratio for training and testing. The six operating condition types were numbered 1-6: low energy discharge (D1), high energy discharge (D2), partial discharge (PD), medium to low-temperature overheating (T1), hightemperature overheating (T2), and normal condition (NC). The composition of the training and test sample data is shown in Table 10, and some data are given in Table 11.\nTABLE 10. Fault sample data composition\nFault type Code Training sets Test sets Total sample\nD1 1 111 37 148\nD2 2 81 27 108\nPD 3 96 32 128\nT1 5 78 26 104\nT2 4 102 34 136\nNC 6 72 24 96\nTotal 540 180 720"
        },
        {
            "heading": "59 10.4 4 10 12.7 T1",
            "text": ""
        },
        {
            "heading": "30 7.4 8.5 1.8 19 T1",
            "text": ""
        },
        {
            "heading": "176 206 47.7 75.7 68.7 T1",
            "text": ""
        },
        {
            "heading": "3.1 0.5 2.9 4.5 42.8 T2",
            "text": ""
        },
        {
            "heading": "32.5 28.7 6.6 16.2 108.5 T2",
            "text": ""
        },
        {
            "heading": "30.6 29.9 6.8 16.1 99.4 T2",
            "text": ""
        },
        {
            "heading": "242 6 1 4 0 PD",
            "text": ""
        },
        {
            "heading": "565 93 34 47 0 PD",
            "text": ""
        },
        {
            "heading": "27 90 63 42 0.2 D1",
            "text": ""
        },
        {
            "heading": "101 169 595 34 0 D1",
            "text": ""
        },
        {
            "heading": "73 520 140 1200 6 D2",
            "text": ""
        },
        {
            "heading": "5.5 35.9 28.1 156 0 D2",
            "text": "12 23 9 98 0 D2\n42 33.6 14.8 9 0.6 NC"
        },
        {
            "heading": "33.7 3 33.2 27.7 2.5 NC",
            "text": ""
        },
        {
            "heading": "39.2 24.5 18.4 11.4 6.5 NC",
            "text": "To avoid redundancy and overlap of fault feature information in the original samples, as well as to improve the efficiency and accuracy of the algorithm, and because a single fault feature cannot fully reflect the intrinsic connection with different state types, the five single fault feature data were extended, while the transformer fault features were dimensionally reduced using KPCA to obtain a new fault feature set consisting of six principal components. The new fault feature set was input into the model for training and testing respectively, and the diagnostic accuracy of the different models was analyzed. The simulation experiment platform for this paper is a computer with Windows 10 operating system, 16G of memory, and Matlab 2020b as the running environment."
        },
        {
            "heading": "A. COMPARISON OF DIAGNOSTIC PERFORMANCE",
            "text": "OF DIFFERENT MODELS WITH SCN\nTo verify the effectiveness and superiority of SCN, five common machine learning models, including Probabilistic Neural Network (PNN), Random Forest (RF), Support Vector Machine (SVM), Extreme Learning Machine (ELM), and eXtreme Gradient Boosting (XGBoost), were used as comparisons in this paper. The feature datasets obtained from KPCA dimensionality reduction were input into different models for training and classification, and the average accuracy was obtained by running the different models 10 times respectively, and the results are shown in Table 12.\nTABLE 12. Comparison of the average accuracy of differentdiagnostic models\nFault type\nAverage accuracy rate /%\nFIGURE 7. Comparison of the accuracy of different diagnostic models\nAs can be seen from Table 12 and Figure 7, the correct diagnosis rate using the SCN model was significantly higher than that of other models in the low-energy discharge, partial discharge, high-temperature overheating, and normal operation states; the correct diagnosis rate using XGBoost and SCN was the same in the high-energy discharge and medium-to-low-temperature overheating states. The combined diagnostic rate using the SCN model was the highest at 90%, while the accuracy rates using the PNN, RF, SVM, ELM and XGBoost models were 77.78%, 80.56%, 84.44%, 82.78%, and 86.11% respectively, all of which were lower than the SCN model. This indicates that the diagnostic performance of the SCN model is significantly better than other similar models.\nB. COMPARISON OF ALGORITHM OPTIMISATION\nAs the penalty term coefficients of the SCN have a significant impact on the training learning effect, the IGJO was used to find the optimization of the two hyperparameters of the LSSVM and to compare the optimization results with those of the WOA, SCSO, MPA, and GJO-SCN. The iterative curve with KPCA reduced data as the model input and diagnostic accuracy as the fitness value is shown in Figure 8.\nAs can be seen from Figure 8, the WOA algorithm is not effective in optimizing the parameters, resulting in the lowest fitness. The standard GJO algorithm reaches the global optimum after 32 iterations due to its weak global search capability and weak local exploitation capability. SCSO and MPA reach the global optimum in 31 and 36 iterations respectively and have low fitness. The IGJO with the EOBL and golden sine strategies converge in only 25 iterations and has the highest fitness value. This shows that it is feasible to use the IGJO to optimize the SCN-related hyperparameters to build a transformer fault diagnosis model.\nC. COMPARISON OF THE DIAGNOSTIC PERFORMANCE OF DIFFERENT HEURISTIC ALGORITHMS FOR OPTIMISING SCN\nTo verify the superiority of the IGJO superior SCN diagnostic model, the WOA-SCN, SCSO-SCN, MPA-SCN, GJO-SCN, and IGJO-SCN models were compared and analyzed. The average accuracies of the different diagnostic methods are shown in Table 13.\nD1 D2 PD T1 T2 NC\n60\n80\n100\nA cc\nu ra\ncy (%\n)\nPNN RF SVM ELM XGBoost SCN\n8 VOLUME XX, 2022\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 VOLUME XX, 2022\nAs can be seen from Figure 9 and Table 13, the IGJOSCN has the highest fault accuracy rate, where the diagnostic accuracy rates for low energy discharge, high-temperature overheating, and normal operating conditions are 100%, and the diagnostic accuracy rates for high energy discharge, partial discharge, and medium and low-temperature overheating are 92.59%, 96.88%, and 84.62% respectively, and the correct diagnostic rates for all six fault types are above 84%, for a total The fault diagnosis accuracy rate reached 96.11%. In addition, the fault diagnosis accuracy of GJO-SCN, SCSO-SCN, MPA-SCN, and WOA-SCN was 91.11%, 86.67%, 83.33%, and 76.67% respectively, and the fault diagnosis accuracy of IGJO-SCN was improved by 5%, 9.44%, 12.78%, and 19.44% respectively compared with the above four methods. The results indicate that the SCN fault diagnosis model optimized by IGJO has higher reliability after KPCA dimensionality reduction."
        },
        {
            "heading": "VIII. CONCLUSION",
            "text": "1) This paper introduces the L2 parametric penalty term in the random configuration network, which improves the generalization ability of SCN in practical applications. 2) Using KPCA to reduce the dimensionality of 15 features, the resulting six features can express valid information about transformer faults and avoid redundant information, while facilitating the learning and training of SCN models. The running time is improved by nearly 20s and the correct rate is improved by nearly 15% compared to that before dimensionality reduction. The complexity and running time of the model are reduced and the accuracy of the model is improved. 3) The IGJO algorithm overcomes the drawback that the GJO algorithm is prone to fall into local optimality, and has better stability and superiority-seeking ability compared with the WOA, SCSO, MPA, SCA and SSA algorithms. 4) The simulation results show that the parameter optimization of SCN using the IGJO algorithm can effectively improve the fault diagnosis accuracy of the transformer by 96.11%. the fault diagnosis accuracy of GJO-SCN, SCSO-SCN, MPA-SCN, and WOA-SCN is 91.11%, 86.67%, 83.33%, and 76.67% respectively. The fault diagnosis accuracy of IGJO-SCN improved by 5%, 9.44%, 12.78%, and 19.44% respectively compared to the above four methods.The IGJO-SCN model proposed in this paper verifies that its diagnostic performance is excellent, but there is still room for improvement in the selection of data features. Due to the small influence of gases such as CO and CO2, only five of the characteristic gases are\nselected as the original data in this paper, and the influence of these gases needs to be considered when more detailed fault diagnosis classification is carried out later, and more combinations of ratios can also be used to enrich the characteristic In addition, since different combination models have different diagnostic rates for different fault types, it is possible to try to compare the fault diagnostic rates of multiple models and select the optimal model to diagnose the corresponding faults."
        }
    ],
    "title": "Fault Diagnosis Method for Power Transformers Based on Improved Golden Jackal Optimization Algorithm and Random Configuration Network",
    "year": 2023
}