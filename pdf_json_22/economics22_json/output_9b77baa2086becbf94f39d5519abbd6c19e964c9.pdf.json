{
    "abstractText": "The most widely used integrated assessment model for studying the economics of climate change is the dynamic/regional integrated model of climate and economy (DICE/RICE) [2,3]. In this document, we first represent the RICE-2011 model as a dynamic game, termed the RICE game. Then, both cooperative and non-cooperative solutions to the RICE game are considered. Next, a description of how to use the repositoryRICE-GAME on GitHub is provided [6]. The repository RICE-GAME is a Matlab and CasADibased implementation of the RICE game and its cooperative and non-cooperative solutions. 1 Preliminary: Dynamic Games The theory of dynamic games lies in the interface between game theory and optimal control, which involves a dynamic decision process for multiple players, each of which tries to maximize a cumulative payoff function. An n-player discrete-time dynamic game over a finite horizon is defined as follows. Dynamic Game. The n players are indexed in V := {1, 2, . . . , n}; time is discrete with the steps indexed in T := {0, 1, . . . , T}. Each player can manipulate the game through its control decisions, and the control decision space of player i \u2208 V is denoted by Ui \u2286 R . At each time step t = 0, . . . , T , the decision executed by player i is denoted by ui(t) \u2208 Ui. We also use u(t) = [u \u22a4 1 (t); . . . ;u \u22a4 n (t)], Ui = [u \u22a4 i (0); . . . ;u \u22a4 i (T )] and U = [U1; . . . ;Un] to represent the all-player decision profile at time t, the player-i decision throughout the time horizon, and the decision profile for all players and for all time stpes. The control decisions of all players excluding player i at time step t is denoted by u\u2212i(t), and the control decisions of all players excluding player i over the entire horizon is represented by U\u2212i. For each t = 0, 1, . . . , T , the group of players are associated with a dynamical state x(t) \u2286 R denoting the state space, whose dynamics are described by x(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t = 0, . . . , T, (1) with x0 being the initial state. At each time t = 0, . . . , T , upon playing ui(t), the agent i receives a payoff gi(x(t),ui(t),u\u2212i(t)) \u2208 R given other players\u2019 actions u\u2212i(t) and the current state x(t), where gi(x(t),ui(t),u\u2212i(t)) is a continuous function with respect to x(t),ui(t), and u\u2212i(t). The cumulative payoff",
    "authors": [
        {
            "affiliations": [],
            "name": "Yijun Chen"
        }
    ],
    "id": "SP:d05014389534d889ca8ebda69ae1f4af081916fc",
    "references": [
        {
            "authors": [
                "J.A.E. Andersson",
                "J. Gillis",
                "G. Horn",
                "J.B. Rawlings",
                "M. Diehl"
            ],
            "title": "CasADi: a software framework for nonlinear optimization and optimal control",
            "venue": "Mathematical Programming Computation, vol. 11, no. 1, pp. 1\u201336, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "W.D. Nordhaus"
            ],
            "title": "DICE 2013R: Introduction and User\u2019s Manual",
            "venue": "Yale University Press, no. 1, pp. 1\u2013102, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "W.D. Nordhaus"
            ],
            "title": "Economic aspects of global warming in a post-Copenhagen environment",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 107, no. 26, pp. 11721\u201311726, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "T. Faulwasser",
                "C.M. Kellett",
                "S.R. Weller"
            ],
            "title": "DICE2013R-mc: Dynamic Integrated model of Climate and Economy 2013R - Matlab and CasADi\u201d, available at https://github.com/cmkellett/DICE2013R-mc",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n21 1.\n07 75\n0v 1\n[ ee\nss .S\nY ]\n1 4\nN ov\nThe most widely used integrated assessment model for studying the economics of climate change is the dynamic/regional integrated model of climate and economy (DICE/RICE) [2,3]. In this document, we first represent the RICE-2011 model as a dynamic game, termed the RICE game. Then, both cooperative and non-cooperative solutions to the RICE game are considered. Next, a description of how to use the repositoryRICE-GAME on GitHub is provided [6]. The repository RICE-GAME is a Matlab and CasADibased implementation of the RICE game and its cooperative and non-cooperative solutions."
        },
        {
            "heading": "1 Preliminary: Dynamic Games",
            "text": "The theory of dynamic games lies in the interface between game theory and optimal control, which involves a dynamic decision process for multiple players, each of which tries to maximize a cumulative payoff function. An n-player discrete-time dynamic game over a finite horizon is defined as follows.\nDynamic Game. The n players are indexed in V := {1, 2, . . . , n}; time is discrete with the steps indexed in T := {0, 1, . . . , T}. Each player can manipulate the game through its control decisions, and the control decision space of player i \u2208 V is denoted by Ui \u2286 R d. At each time step t = 0, . . . , T , the decision executed by player i is denoted by ui(t) \u2208 Ui. We also use u(t) = [u \u22a4 1 (t); . . . ;u \u22a4 n (t)], Ui = [u \u22a4 i (0); . . . ;u \u22a4 i (T )] and U = [U1; . . . ;Un] to represent the all-player decision profile at time t, the player-i decision throughout the time horizon, and the decision profile for all players and for all time stpes. The control decisions of all players excluding player i at time step t is denoted by u\u2212i(t), and the control decisions of all players excluding player i over the entire horizon is represented by U\u2212i.\nFor each t = 0, 1, . . . , T , the group of players are associated with a dynamical state x(t) \u2286 Rm denoting\nthe state space, whose dynamics are described by\nx(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t = 0, . . . , T, (1)\nwith x0 being the initial state. At each time t = 0, . . . , T , upon playing ui(t), the agent i receives a payoff gi(x(t),ui(t),u\u2212i(t)) \u2208 R given other players\u2019 actions u\u2212i(t) and the current state x(t), where gi(x(t),ui(t),u\u2212i(t)) is a continuous function with respect to x(t),ui(t), and u\u2212i(t). The cumulative payoff\nof agent i throughout the time horizon is therefore\nJi(X,Ui,U\u2212i) = T \u2211\nt=0\ngi(x(t),ui(t),u\u2212i(t)) (2)\nwhere X = (x\u22a4(0), . . . ,x\u22a4(T ))\u22a4. Each player\u2019s goal is to maximize its own payoff function by making best control decisions for the underlying dynamical system, and the system dynamics produces a terminal state x(T + 1) towards the end of the time horizon."
        },
        {
            "heading": "2 Description of RICE as a Dynamic Game",
            "text": "In what follows, we represent the RICE-2011 model as a dynamic game, termed the RICE game. Our presentation of the RICE game is based on the RICE-2011 model with slight modifications, but the nature of being a dynamic game is preserved.\nRegions. There are 12 regions in the RICE game. Each region is considered a player and the regions are indexed in V = {1, 2, . . . , n} with n = 12.\nTime Steps and Calendar Years. The RICE game operates in periods of \u2206 = 5 years, starting from the year 2020 as the initial year1. Taking the discrete time step index T = {0, 1, . . . , T}, the relation between an actual calendar year and the corresponding discrete time step is determined by\nyear(t) = year(0) + 5t, year(0) = 2020, (3)\nyielding calendar year 2020, 2025, 2030, . . . as desired.\nState Variables and Control Decisions. The RICE game has n + 5 state variables: two variables to model the temperature dynamics in the form of the temperature deviation in the atmosphere and in the lower ocean from the reference year 1750 (TAT and TLO, respectively), three variables to model the carbon dynamics in the form of average carbon mass in the atmosphere, the upper ocean and biosphere, and the deep ocean (MAT,MUP and MLO, respectively), and n variables to model the economic dynamics in the form of each region\u2019s capital (Ki, i \u2208 V). Control decisions of the RICE game are each region\u2019s emissionreduction rate \u00b5i and each region\u2019s saving rate si where the former represents the ratio of investment to the net economic output in each region, and the latter represents the rate at which CO2 emissions are reduced in each region.\nWe denote the dynamical state of the RICE game at time step t \u2208 T as\nx(t) = [TAT(t);TLO(t);MAT(t);MUP(t);MLO(t);K1(t); . . . ;Kn(t)] \u2208 R n+5. (4)\nWe also denote control decisions of region i \u2208 V at time step t \u2208 T by\nui(t) = [\u00b5i(t); si(t)] \u22a4 := [ui[1](t);ui[2](t)] \u22a4 \u2208 [0, 1]2. (5)\nConsequently, control decisions of the RICE dynamic game at time step t \u2208 T of all players are\nu(t) = [\u00b51(t); . . . , \u00b5N (t); s1(t); . . . ; sN (t)] \u2208 [0, 1] 24. (6)\n1The RICE-2011 model operates in periods of 10 years starting from 2005.\nFurther denote region i\u2019s control decisions over the time horizon by Ui := [\u00b5i(0); . . . , \u00b5i(T ); si(0); . . . ; si(T )]. The control decisions of all regions excluding region i at time step t is denoted by u\u2212i(t), and the control decisions of all regions excluding region i over the time horizon is represented by U\u2212i.\nRICE Dynamics. The RICE game is also driven by several exogenous and time-varying signals that evolve independently such as radiative forcing caused by greenhouse gases other than CO2 emissions F EX, each region\u2019s natural CO2 emissions due to land use E land i , each region\u2019s carbon intensity \u03c3i, each region\u2019s total factor productivity Ai and each region\u2019s population Li. The dynamics of x(t) can be written as\nx(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t \u2208 T, (7)\nwhere x0 represents the initial state of the underlying dynamical system. The transition functions f := [f1; f2; . . . ; fn+5] follow the RICE dynamics:\n[\nTAT(t+ 1) TLO(t+ 1)\n]\n=\n[\n\u03c611 \u03c612 \u03c621 \u03c622\n] [\nTAT(t) TLO(t)\n]\n+\n[\n\u03be2\n0\n]\nF (t), (8)\n\n  \nMAT(t+ 1) MUP(t+ 1) MLO(t+ 1)\n\n   =\n\n  \n\u03b611 \u03b612 0 \u03b621 \u03b622 \u03b623\n0 \u03b632 \u03b633\n\n  \n\n  \nMAT(t) MUP(t) MLO(t)\n\n   +\n\n  \n\u03be1\n0 0\n\n   E(t), (9)\nKi(t+ 1) = (1\u2212 \u03b4 K i ) 5Ki(t) + 5 ( 1\u2212 a [1] i T AT(t)\u2212 a [2] i T AT(t)a [3] i )\n\u00b7 (\n1\u2212 \u03b8 [1] i (t)\u00b5i(t)\n\u03b8 [2] i\n) Ai(t)Ki(t) \u03b3i (Li(t)\n1000\n)1\u2212\u03b3i si(t), (10)\nwhere total radiative forcing F (t) at time step t, each region\u2019s total emissions Ei(t) including natural emissions and industrial emissions at time step t and global emission E(t) at time step t are given by\nF (t) = \u03b7 log2\n( MAT(t)\nMAT,1750\n)\n+ FEX(t), (11)\nEi(t) = \u03c3i(t)(1\u2212 \u00b5i(t))Ai(t)Ki(t) \u03b3i(t)\n(Li(t)\n1000\n)1\u2212\u03b3i +Elandi (t), (12)\nE(t) =\nn \u2211\ni=1\nEi(t), (13)\n\u03b8 [1] i (t) =\npbi\n1000 \u00b7 \u03b8 [2] i\n(1\u2212 \u03b4pbi ) t \u00b7 \u03c3i(t). (14)\nPayoff Functions. For each region, the social welfare at time step t of the population Li(t) consuming the consumption Ci(t) is defined by the population-weighted utility of per capita consumption\ngi(Ci(t), Li(t)) = Li(t) \u00b7 (Ci(t) Li(t) )1\u2212\u03b1i \u2212 1\n1\u2212 \u03b1i , (15)\nwhere each region\u2019s consumption at time step t is given by\nCi(t) = ( 1\u2212 a [1] i T AT(t)\u2212 a [2] i T AT(t)a [3] i )( 1\u2212 \u03b8 [1] i (t)\u00b5i(t) \u03b8 [2] i ) Ai(t)Ki(t) \u03b3i (Li(t)\n1000\n)1\u2212\u03b3i si(t). (16)\nEach region\u2019s payoff function is defined as the cumulative social welfare of region i across the time horizon:\nJi = T \u2211\nt=0\ngi(Ci(t), Li(t))\n(1 + \u03c1i)5t\n=\nT \u2211\nt=0\n(\n[Ai(t)Li(t) 1+\u03b1i\u2212\u03b3i\n(1\u2212 \u03b1i)(1 + \u03c1i)5t\n( 1\u2212 ui[2](t) )( 1\u2212 a [1] i x1(t)\u2212 a [2] i x1(t) a [3] i )( 1\u2212 \u03b8 [1] i (t)ui[1](t) \u03b8 [2] i ) x5+i(t) \u03b3i ]\n\u2212 Li(t)\n(1\u2212 \u03b1i)(1 + \u03c1i)5t\n)\n. (17)\nFor each region i \u2208 V, naturally it will attempt to maximize its cumulative social welfare.\nWe have now formally presented the RICE game where the regions as players seek to plan their control\ndecisions in emission-reduction rate and saving rate for the entire time horizon\nUi := [\u00b5i(0); . . . , \u00b5i(T ); si(0); . . . ; si(T )]\nso as to maximize their payoff functions (17) subject to the underlying dynamical system (7), represented in (8)-(10).\nThe Social Cost of CO2. The regional SCC is then given by the ratio of the regional marginal welfare with respect to regional emissions and with respect to regional consumption\nSCCi(t) = \u22121000 \u00b7 \u2202Ji\n\u2202Ei(t) / \u2202Ji \u2202Ci(t)\n= \u22121000 \u00b7 \u2202Ci(t)\n\u2202Ei(t) . (18)\nParameters, Exogenous Signals and Initial State. The values for the parameters and initial state can be found in the tables at the end of this document. In the RICE game, the initial state are calibrated to match the data in the starting year 2020. The values for the parameters in the geophysical sector uses the latest updated values in the DICE-2016 model. The values for the exogenous and time-varying signals can be found in the Excel file named exogenous_states_long.mat in the repository RICE-GAME ."
        },
        {
            "heading": "3 Cooperative Decisions of the RICE Game",
            "text": "In this section, we study the solutions to the RICE game under cooperative settings. First of all, we revisit the classical RICE solution concept defined by a system-level social welfare maximization. Next, we move to Pareto solutions to the RICE game. Finally, we introduce a receding horizon approach to approximate the classical RICE solution."
        },
        {
            "heading": "3.1 Solution to RICE Social Welfare Maximization",
            "text": "RICE social welfare maximization is for a centralized climate policy planner to compute the Ui, i \u2208 V that maximize the sum of the weighed regional social welfare across all regions for a given initial condition x0 :\nmax Ui,i\u2208V\nn \u2211\ni=1\nciJi,\nsubject to x(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t \u2208 T\nu(t) \u2208 [0, 1]24, t \u2208 T.\n(19)\nwhere the values of ci, i \u2208 V can be found in Table 3."
        },
        {
            "heading": "3.2 Pareto Frontier between Developed and Developing Regions",
            "text": "The regions in the RICE game are classified into two clusters of regions: developed regions (US, EU, Japan and other high income countries) and developing regions (Russia, Non-Russian Eurasia, China, India, Middle East, Africa, Latin America and other Asian countries). We denote Vdeveloped = {1, 2, 3, 11} and Vdeveloping = {4, 5, 6, 7, 8, 9, 10, 12}. Corresponding, the social welfare of the two clusters of regions are defined as, respectively,\nWdeveloped = \u2211\ni\u2208Vdeveloped\nJi, Wdeveloping = \u2211\ni\u2208Vdeveloping\nJi.\nWe can calculate Pareto social welfare frontier between the developed and developing clusters by solving\nthe family of optimization problems for a given initial condition x0:\nmax Ui,i\u2208V\np \u00b7Wdeveloped + (1\u2212 p) \u00b7Wdeveloping\nsubject to x(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t \u2208 T\nu(t) \u2208 [0, 1]24, t \u2208 T.\n(20)\nwhere p is selected in the interval [0, 1]. For any fixed p \u2208 [0, 1], we obtain a Pareto solution, and their collection form the Pareto frontier between the developed and developing clusters."
        },
        {
            "heading": "3.3 Receding Horizon Solution to RICE Social Welfare Maximization",
            "text": "The work of [8] established a novel receding horizon solution to DICE, which provides robustness and computational efficiency compared to solving DICE in a long time horizon directly. We extend the idea of [8] to RICE.\nFor the receding horizon approach, we denote the prediction horizon by Trh and the simulation horizon\nby Tsim. We introduce l(t,x(t),u(t)) := \u2211N i=1 ci \u00b7 gi(Ci(t),Li(t)) (1+\u03c1i)5t . We assume a full measurement of the estimate of the state x(t) is available at each time step t \u2208 Tsim := {0, 1, . . . , Tsim}. We present the receding horizon approximation of (19) in Algorithm 1:\nAfter Algorithm 1, the control decision profile Urhw := [urhw(0)\u22a4, . . . ,urhw(Tsim) \u22a4]\u22a4 is the receding\nhorizon solution to RICE."
        },
        {
            "heading": "4 Non-cooperative Decisions of the RICE Game",
            "text": "In this section, we study the solutions to the RICE game under non-cooperative settings. We present the best-response dynamics for dynamic games, and receding horizon feedback decisions for dynamic games. It can be also implemented over the RICE game, since the RICE game is inherently a dynamic game."
        },
        {
            "heading": "4.1 Best-response Dynamics",
            "text": "In what follows, we establish the best-response recursions for the dynamic game introduced in Section 1 with n players over a finite horizon T . We assume the dynamic game is repeatedly and recursively played for N episodes, where each episode consists of T time steps. We thereby define the aggregated control decisions\nAlgorithm 1 MPC-RICE Input: simulation horizon Tsim; prediction horizon Trh.\n1: t\u2190 0 2: while t \u2264 Tsim do 3: observe x(t) 4: compute the optimal solution u\u2217(s), s \u2208 S := {t, t+ 1, . . . , t+ Trh}, to the following optimization\nproblem over the receding horizon S\nmax u(s),\u2200s\u2208S\nt+Trh \u2211\ns=t\nl(s,x(s),u(s))\nsubject to x(s+ 1) = f(s,x(s),u(s)), s \u2208 S,\nu(s) \u2208 [0, 1]24, s \u2208 S.\n(21)\n5: apply urhw(t) := u\u2217(t) to RICE dynamic game"
        },
        {
            "heading": "6: end while",
            "text": "of all players in episode k = 1, \u00b7 \u00b7 \u00b7 , N by U(k), and the decisions of player i in episode k by U (k) i . Similarly, the decisions of players excluding player i in episode k is denoted by U (k) \u2212i . The best-response recursion of the agents in the dynamic game over the N episodes are described in the following Recursive Best-response Algorithm for Dynamic Games (RBA-DG) as in Algorithm 2."
        },
        {
            "heading": "4.2 Receding Horizon Feedback Decisions",
            "text": "In what follows, we present a framework for dynamic games where in a single play over the time horizon, players observe the underlying dynamic process and other players\u2019 actions, and then apply a receding-horizon feedback decision making for a prediction horizon.\nConsider the dynamic game introduced in Section 1 with n players over a finite horizon T . The game is played only once, and the players take the following feedback decision process in the receding horizon sense. The players apply a receding horizon approach and compute their feedback decisions ui(t). At each time t = 0, . . . , T \u2212 1, each player i observes other players\u2019 played action u\u2212i(t) and the system state x(t). Then, every player i assumes that u\u2212i(t) will continue to be played over [t + 1, t + Trh], and therefore decides its best feedback decision plan uRHP i|t+1\u2192t+Trh (x(t),u\u2212i(t)), where u RHP i|t+1\u2192t+Trh maximizes the cumulative payoff of player i over the time horizon [t + 1, t + Trh] conditioned on that u\u2212i(t) be played over [t + 1, t + Trh]. Finally, each player i plays the first planned decision uRHP i|t+1 in u RHP i|t+1\u2192t+Trh for the step t+1, and the process moves forward recursively. Denoting uRHFi (t) as the actions generated by the receding horizon feedback decision process, clearly there is an underlying feedback law \u03c0i such that\nuRHFi (t) = \u03c0i(t,x(t),u RHF \u2212i (t))\nwhich are actually played at each time t. The resulting collective player decisions over the entire time horizon is written as URHF. The exact computational process of this receding horizon feedback decision framework is presented in the following Receding Horizon Feedback Algorithm for Dynamic Games (RHFA-DG).\nAlgorithm 2 Recursive Best-response Algorithm for Dynamic Games (RBA-DG) Input: Episodes N ; ci, i \u2208 V .\n1: compute an optimal cooperative solution Uc by the following problem\nmax U1,\u00b7\u00b7\u00b7 ,Un\n\u2211\ni\u2208V\nci \u00b7 Ji(X,Ui,U\u2212i)\nsubject to x(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t \u2208 T\nu(t) \u2208 [0, 1]24, t \u2208 T.\n2: let U (0) i = U c i ,\u2200i \u2208 V 3: k \u2190 0 4: while k \u2264 N do 5: for each player i \u2208 V do 6: observe U (k) \u2212i 7: compute U (k+1) i by solving the problem\nmax Ui Ji(X,Ui,U\u2212i), (22a)\ns.t. x(t+ 1) = f(t,x(t),u(t)), (22b)\nU\u2212i = U (k) \u2212i , (22c) x(0) = x0. (22d)"
        },
        {
            "heading": "8: end for",
            "text": "9: U(k+1) = [U (k+1) 1 ; \u00b7 \u00b7 \u00b7 ;U (k+1) N ]\n10: k \u2190 k + 1\n11: end while"
        },
        {
            "heading": "5 Description of Code",
            "text": "In what follows, a description of how to use the repository RICE-GAME on GitHub is provided [6]. The repository RICE-GAME is a Matlab and CasADi-based implementation of the RICE game and its cooperative and non-cooperative solutions. A Matlab implementation of DICE and receding horizon solution to DICE can be found in [7, 8].\nThe repository RICE-GAME consists of two folders:\n\u2022 /RICE-GAME/cooperative provides cooperative solutions to the RICE game under cooperative set-\ntings;\n\u2022 /RICE-GAME/non-cooperative provides non-cooperative solutions to the RICE game under non-\ncooperative settings."
        },
        {
            "heading": "5.1 Cooperative Solutions",
            "text": "Under cooperative settings, the following cooperative solutions are considered:\n\u2022 Solution to RICE social welfare maximization under the directory\n/RICE-GAME/cooperative/RICE SWM;\nAlgorithm 3 Receding Horizon Feedback Algorithm for Dynamic Games (RHFA-DG) Input: simulation horizon Tsim; prediction horizon Trh; ci, i \u2208 V.\n1: compute an optimal cooperative solution Uc by the following problem\nmax U1,\u00b7\u00b7\u00b7 ,Un\n\u2211\ni\u2208V\nci \u00b7 Ji(X,Ui,U\u2212i)\nsubject to x(t+ 1) = f(t,x(t),u(t)), x(0) = x0, t \u2208 T\nu(t) \u2208 [0, 1]24, t \u2208 T.\n2: let URHFi (0) = U c i (0),\u2200i \u2208 V 3: t\u2190 0 4: while t \u2264 Tsim do 5: for each player i \u2208 V do 6: Take action URHFi (t) 7: end for 8: for each player i \u2208 V do 9: observe x(t) and URHF(t)\n10: compute x(t+ 1) according to (1) 11: assume all players j \u2208 V/{i} will continue to play URHF\u2212i (t) over [t+ 1, t+ Trh] 12: compute its optimal solution uRHP i|t+1\u2192t+Trh to the following receding horizon optimization problem\nmax ui|t+1\u2192t+T\nrh\nt+Trh \u2211\ns=t+1\ngi(s,x(s),ui(s),u\u2212i(s)) (23a)\ns.t. x(s+ 1) = f(s,x(s),u(s)), (23b)\nu\u2212i(s) = U RHF \u2212i (t), s \u2208 [t+ 1, t+ Trh]. (23c)\n13: plan URHFi (t+ 1) = u RHP i|t+1 14: end for 15: end while\n\u2022 Pareto frontier between developed and developing regions under the directory\n/RICE-GAME/cooperative/Pareto Frontier;\n\u2022 Receding horizon solution to RICE social welfare maximization under the directory\n/RICE-GAME/cooperative/MPC-RICE.\nSolution to RICE Social Welfare Maximization. It consists of seven files and a folder:\n\u2022 RICE_SWM.m is the top-level file and calls the files of specify_parameters.m, solve_swm_problem.m,\ntest_rice_dynamics.m, rice_dynamics.m, and try_a_guess.m. It provides an implementation of RICE social welfare maximization.\n\u2022 specify_parameters.m specifies and returns the parameters and exogenous terms for the RICE game,\nin the structure Params.\n\u2022 solve_swm_problem.m is a function that solves RICE social welfare maximization problem (19) given\ninitial condition x0 and time horizon problem_horizon.\n\u2022 test_rice_dynamics.m is a function that takes all players\u2019 control decisions of double data type as\none of input arguments and calculates the dynamical states of the RICE game (all dynamical states belong to double data type). In addition, it also calculates the value of the payoff functions (belonging to double data type) and the quantities for each region\u2019s emissions and consumption (belonging to double data type).\n\u2022 rice_dynamics.m is a function that takes all players\u2019 control decisions of SX data type from CasADi\nas one of input arguments and calculates the dynamical states of the RICE game (all dynamical states belong to SX data type). In addition, it also calculates the value of the payoff functions (belonging to SX data type) and the quantities for each region\u2019s emissions and consumption (belonging to SX data type).\n\u2022 try_a_guess.m returns an initial guess as a starting point for RICE social welfare maximization\nproblem (19).\n\u2022 plot_result.m generates plots of the trajectories of the emission-reduction rate, saving rate, atmo-\nspheric temperature deviation and social cost of CO2.\n\u2022 ./results is a folder where the results computed from RICE_SWM.m are saved.\nPareto Frontier between Developed and Developing Regions. It consists of five files and a folder:\n\u2022 Pareto.m is the top-level file and calls the files of specify_parameters.m, rice_dynamics.m and\ntest_rice_dynamics.m. It takes 1001 linearly spaced values between 0 and 1 as the values of p, and solves the respective optimization problem (20) under each p.\n\u2022 specify_parameters.m, rice_dynamics.m and test_rice_dynamics.m are the same as those in\nRICE social welfare maximization.\n\u2022 plot_pareto.m generates plots of the social welfare Pareto frontier between developed regions and\ndeveloping regions, and the atmospheric temperature deviation at the final time step versus the parameter p.\n\u2022 ./results is a folder where the results computed from Pareto.m are saved.\nReceding Horizon Solution to RICE Social Welfare Maximization. It consists of seven files and a folder:\n\u2022 MPC_RICE.m is the top-level file and calls the files of specify_parameters.m, solve_swm_problem.m,\nrice_dynamics.m, test_rice_dynamics.m and try_a_guess.m. It provides an implementation of Algorithm 1. The simulation horizon is specified in T_simulation. The prediction horizon is set in T_prediction and problem_horizon.\n\u2022 specify_parameters.m, solve_swm_problem.m, rice_dynamics.m, test_rice_dynamics.m and try_\na_guess.m are the same as those in RICE social welfare maximization.\n\u2022 comparison_dg_mpc.m generates the plots of the comparison of the atmospheric temperature deviation\ntrajectories and optimal emission-reduction rates under the optimal control decisions solved from (19) and Algorithm 1 with different receding horizons.\n\u2022 ./results is a folder where the results computed from MPC_RICE.m are saved."
        },
        {
            "heading": "5.2 Non-cooperative Solutions",
            "text": "Under non-cooperative settings, the following planning processes are considered:\n\u2022 Best-response dynamics under the directory\n/RICE-GAME/non-cooperative/BR;\n\u2022 Receding horizon feedback decisions under the directory\n/RICE-GAME/non-cooperative/RHFA.\nBest-response Dynamics. There are six files and a folder:\n\u2022 BR_Algorithm.m is the top-level file and calls the files of specify_parameters.m, test_rice_dynamics.\nm, ith_rice_dynamics.m and solve_ith_problem.m. It provides an implementation of Algorithm 2.\n\u2022 specify_parameters.m and test_rice_dynamics.m are the same as those in RICE social welfare\nmaximization.\n\u2022 ith_rice_dynamics.m is a function that takes player i\u2019s control decisions of SX data type from CasADi\nand other players\u2019 control decisions of double data type as two of input arguments and calculates the dynamical states of the RICE game (all dynamical states belong to SX data type). In addition, it also calculates the value of the payoff functions (belonging to SX data type) and the quantities for each region\u2019s emissions and consumption (belonging to SX data type).\n\u2022 solve_ith_problem.m is a function that solves the optimization problem (22) in the iteration k + 1\nof the best-response dynamics given initial condition x0, time horizon problem_horizon and other players\u2019 control decisions U_br(:,:,k) in the iteration k.\n\u2022 comparison_br.m generates the plots of the convergence of \u2016U (k) i \u2212 U (k\u22121) i \u2016 versus iterations, and\nthe comparison of the atmospheric temperature deviation trajectories and optimal emission-reduction rates under the optimal control decisions solved from (19) and Algorithm 2 in the final iteration.\n\u2022 ./results is a folder where the results computed from BR_Algorithm.m are saved.\nReceding Horizon Feedback Decisions. There are six files and a folder:\n\u2022 RHF_Algorithm.m is the top-level file and calls the files of specify_parameters.m, test_rice_\ndynamics.m, solve_ith_problem.m and ith_rice_dynamics.m. It provides an implementation of Algorithm 3.\n\u2022 specify_parameters.m and test_rice_dynamics.m are the same as those in RICE social welfare\nmaximization\n\u2022 solve_ith_problem.m and ith_rice_dynamics.m are the same as those in the best-response dynam-\nics.\n\u2022 comparison_RHFA.m generates the plots of the comparison of the atmospheric temperature deviation\ntrajectories and optimal emission-reduction rates under the optimal control decisions solved from (19), Algorithm 2 in the final iteration K, and Algorithm 3 with different prediction horizons.\n\u2022 ./results is a folder where the results computed from RHF_Algorithm.m are saved."
        },
        {
            "heading": "6 Software Requirements",
            "text": "This implementation of RICE-GAME is implemented in the platform of Matlab [5] along with the CasADi framework for automatic differentiation and numeric optimization [1]. Version 3.5.5 of CasADi is used. The compatible Matlab version for different operating systems can be found on the CasADi website [4]. The repository RICE-GAME is distributed under the GNU General Public License v3.0."
        }
    ],
    "title": "A Matlab and CasADi-based Implementation of RICE Dynamic Game",
    "year": 2022
}