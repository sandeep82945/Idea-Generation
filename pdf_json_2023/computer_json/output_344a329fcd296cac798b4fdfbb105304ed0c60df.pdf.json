{
    "abstractText": "Estimating the number of clusters and cluster structures in unlabeled, complex, and high-dimensional datasets (like images) is challenging for traditional clustering algorithms. In recent years, a matrix reordering-based algorithm called Visual Assessment of Tendency (VAT), and its variants have attracted many researchers from various domains to estimate the number of clusters and inherent cluster structure present in the data. However, these algorithms face significant challenges when dealing with image data as they fail to effectively capture the crucial features inherent in images. To overcome these limitations, we propose a deep-learningbased framework that enables the assessment of cluster structure in complex image datasets. Our approach utilizes a self-supervised deep neural network to generate representative embeddings for the data. These embeddings are then reduced to 2-dimension using t-distributed Stochastic Neighbour Embedding (t-SNE) and inputted into VAT based algorithms to estimate the underlying cluster structure. Importantly, our framework does not rely on any prior knowledge of the number of clusters. Our proposed approach demonstrates superior performance compared to state-ofthe-art VAT family algorithms and two other deep clustering algorithms on four benchmark image datasets, namely MNIST, FMNIST, CIFAR-10, and INTEL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alokendu Mazumder"
        },
        {
            "affiliations": [],
            "name": "Tirthajit Baruah"
        },
        {
            "affiliations": [],
            "name": "Akash Kumar Singh"
        },
        {
            "affiliations": [],
            "name": "Pagadla Krishna Murthy"
        },
        {
            "affiliations": [],
            "name": "Vishwajeet Pattanaik"
        },
        {
            "affiliations": [],
            "name": "Punit Rathore"
        }
    ],
    "id": "SP:6fb8c4b547679fede8f63f21c3dc431bad013d0a",
    "references": [
        {
            "authors": [
                "A.K. Jain",
                "R.C. Dubes"
            ],
            "title": "Algorithms for Clustering Data",
            "venue": "USA: Prentice-Hall, Inc.,",
            "year": 1988
        },
        {
            "authors": [
                "B.S. Everitt"
            ],
            "title": "Graphical techniques for multivariate data",
            "year": 1978
        },
        {
            "authors": [
                "J. Bezdek",
                "R. Hathaway"
            ],
            "title": "Vat: a tool for visual assessment of (cluster) tendency",
            "venue": "Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN\u201902 (Cat. No.02CH37290), vol. 3, 2002, pp. 2225\u20132230 vol.3.",
            "year": 2002
        },
        {
            "authors": [
                "R.C. Prim"
            ],
            "title": "Shortest connection networks and some generalizations",
            "venue": "The Bell System Technical Journal, vol. 36, no. 6, pp. 1389\u20131401, 1957.",
            "year": 1957
        },
        {
            "authors": [
                "T.C. Havens",
                "J.C. Bezdek"
            ],
            "title": "An efficient formulation of the improved visual assessment of cluster tendency (ivat) algorithm",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 24, no. 5, pp. 813\u2013822, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P. Rathore",
                "D. Kumar",
                "J.C. Bezdek",
                "S. Rajasegarar",
                "M. Palaniswami"
            ],
            "title": "A rapid hybrid clustering algorithm for large volumes of high dimensional data",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 4, pp. 641\u2013654, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. Zhang",
                "Y. Zhu",
                "S. Rajasegarar",
                "G. Li",
                "G. Liu"
            ],
            "title": "Kernelbased ivat with adaptive cluster extraction",
            "venue": "2022. [Online]. Available: https://doi.org/10.21203/rs.3.rs-1483344/v1",
            "year": 2022
        },
        {
            "authors": [
                "P. Vincent",
                "H. Larochelle",
                "I. Lajoie",
                "Y. Bengio",
                "P.-A. Manzagol",
                "L. Bottou"
            ],
            "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.",
            "venue": "Journal of machine learning research,",
            "year": 2010
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ser. ICML\u201920. JMLR.org, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.C. Havens",
                "J.C. Bezdek",
                "M. Palaniswami"
            ],
            "title": "Scalable single linkage hierarchical clustering for big data",
            "venue": "2013 IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing, 2013, pp. 396\u2013401.",
            "year": 2013
        },
        {
            "authors": [
                "R.J. Hathaway",
                "J.C. Bezdek",
                "J.M. Huband"
            ],
            "title": "Scalable visual assessment of cluster tendency for large data sets",
            "venue": "Pattern Recognition, vol. 39, no. 7, pp. 1315\u20131324, 2006. [Online]. Available: https://www.sciencedirect.com/science/ article/pii/S0031320306000550",
            "year": 2006
        },
        {
            "authors": [
                "M. Johnson",
                "L. Moore",
                "D. Ylvisaker"
            ],
            "title": "Minimax and maximin distance designs",
            "venue": "Journal of Statistical Planning and Inference, vol. 26, no. 2, pp. 131\u2013148, 1990. [Online]. Available: https://www.sciencedirect.com/science/ article/pii/037837589090122B",
            "year": 1990
        },
        {
            "authors": [
                "L. Wang",
                "X. Geng",
                "J. Bezdek",
                "C. Leckie",
                "R. Kotagiri"
            ],
            "title": "Specvat: Enhanced visual cluster analysis",
            "venue": "2008 Eighth IEEE International Conference on Data Mining, 2008, pp. 638\u2013647.",
            "year": 2008
        },
        {
            "authors": [
                "J. Zbontar",
                "L. Jing",
                "I. Misra",
                "Y. LeCun",
                "S. Deny"
            ],
            "title": "Barlow twins: Self-supervised learning via redundancy reduction",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18\u201324 Jul 2021, pp. 12 310\u201312 320. [Online]. Available: https://proceedings.mlr.press/v139/zbontar21a.html",
            "year": 2021
        },
        {
            "authors": [
                "X. Chen",
                "K. He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 15 745\u2013 15 753.",
            "year": 2021
        },
        {
            "authors": [
                "J.-B. Grill",
                "F. Strub",
                "F. Altch\u00e9",
                "C. Tallec",
                "P.H. Richemond",
                "E. Buchatskaya",
                "C. Doersch",
                "B.A. Pires",
                "Z.D. Guo",
                "M.G. Azar",
                "B. Piot",
                "K. Kavukcuoglu",
                "R. Munos",
                "M. Valko"
            ],
            "title": "Bootstrap your own latent - a new approach to self-supervised learning",
            "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS\u201920. Red Hook, NY, USA: Curran Associates Inc., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. van den Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Parulekar",
                "L. Collins",
                "K. Shanmugam",
                "A. Mokhtari",
                "S. Shakkottai"
            ],
            "title": "Infonce loss provably learns cluster-preserving representations",
            "venue": "arXiv preprint arXiv:2302.07920, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Krizhevsky",
                "V. Nair",
                "G. Hinton"
            ],
            "title": "Cifar-10 (canadian institute for advanced research)",
            "venue": "2009. [Online]. Available: http://www.cs.toronto.edu/\u223ckriz/cifar.html",
            "year": 2009
        },
        {
            "authors": [
                "P. Bansal"
            ],
            "title": "Intel image classification",
            "venue": "2016. [Online]. Available: https://www.kaggle.com/datasets/puneet6060/ intel-image-classification",
            "year": 2016
        },
        {
            "authors": [
                "L. van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research, vol. 9, no. 86, pp. 2579\u20132605, 2008. [Online]. Available: http://jmlr.org/papers/v9/vandermaaten08a.html",
            "year": 2008
        },
        {
            "authors": [
                "Y. LeCun",
                "C. Cortes"
            ],
            "title": "MNIST handwritten digit database",
            "venue": "http://yann.lecun.com/exdb/mnist/, 2010. [Online]. Available: http://yann.lecun.com/exdb/mnist/",
            "year": 2010
        },
        {
            "authors": [
                "H. Xiao",
                "K. Rasul",
                "R. Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Wang",
                "C. Leckie",
                "K. Ramamohanarao",
                "J. Bezdek"
            ],
            "title": "Automatically determining the number of clusters in unlabeled data sets",
            "venue": "IEEE Transactions on knowledge and Data Engineering, vol. 21, no. 3, pp. 335\u2013350, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "L. Lov\u00e1sz",
                "M. Plummer"
            ],
            "title": "Matching theory, akad",
            "venue": "Kiad\u00f3, Budapest, 1986.",
            "year": 1986
        },
        {
            "authors": [
                "J. Xie",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "Unsupervised deep embedding for clustering analysis",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ser. ICML\u201916. JMLR.org, 2016, p. 478\u2013487.",
            "year": 2016
        },
        {
            "authors": [
                "S. Rebuffi",
                "S. Ehrhardt",
                "K. Han",
                "A. Vedaldi",
                "A. Zisserman"
            ],
            "title": "Lsd-c: Linearly separable deep clusters",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). Los Alamitos, CA, USA: IEEE Computer Society, oct 2021, pp. 1038\u20131046. [Online]. Available: https://doi.ieeecomputersociety.org/10. 1109/ICCVW54120.2021.00121",
            "year": 2021
        },
        {
            "authors": [
                "Y. You",
                "I. Gitman",
                "B. Ginsburg"
            ],
            "title": "Large batch training of convolutional networks",
            "venue": "2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Data clustering is a widely used unsupervised learning technique that involves dividing a collection of unlabeled objects into k groups of similar objects. Various clustering algorithms are available in the literature, such as hierarchical clustering, centroid-based approaches, density-based algorithms, and distribution-based clustering. Most clustering algorithms require k, the number of clusters to seek,\nas an input, which is the clustering tendency assessment problem. One common method to determine the number of clusters and their underlying structure is to visualize the data points using a 2D or 3D plot. However, this approach is only feasible for two- or three-dimensional datasets. For high-dimensional datasets such as images, time-series, visualizing and interpreting cluster structures using 2D or 3D visualization is not practical. Although various dimensionality reduction techniques, such as principal component analysis (PCA) and linear discriminant analysis (LDA), exist in the literature, these techniques often result in a lowdimensional representation of complex, high-dimensional datasets that may not fully reflect the inherent cluster structure due to information loss.\nThere are various formal (based on statistics) and informal (other approaches) techniques [1,2] available in the literature for cluster structure assessment, but they are not completely effective. In contrast, visual approaches [3] have been in use for many years and serve as the foundation for many visual data analysis methods. The Visual Assessment of Clustering Tendency (VAT) [4], a matrix reorderingbased visual-analytical method, is one of such algorithm which provides a visual way to assess the clustering tendency of various datasets. There are several variants of VAT available for different types of data, which are collectively known as the VAT family of algorithms. The VAT family of algorithms has become an acceptable and widely used tool in several domains like biomedical applications, speech processing, image segmentation, transportation applications, and etc for exploratory data analysis.\nVAT algorithm employs a variant of Prim\u2019s minimum spanning tree algorithm [5] to perform matrix reordering of the pairwise dissimilarity matrix to generate a reordered dissimilarity matrix. The reordered dissimilarity matrix can be viewed as a monochrome image called a Reordered Dissimilarity Image (RDI) or cluster heat map. The RDI displays\nar X\niv :2\n30 6.\n00 01\n1v 2\n[ cs\n.L G\n] 3\n1 Ju\na possible cluster structure of the data set by showing dark blocks (data points of low dissimilarity values) along the diagonal. One method to obtain an accurate estimate of the number of clusters (k) from the RDI in the data is to count the number of dark blocks along the diagonal of the RDI. That means VAT not only can be used for cluster tendency assessment but also can be used for subsequent clustering of the input datasets, without needing the number of clusters.\nThis method is particularly effective for datasets with well-separated, compact clusters since the dark blocks along the diagonal are easily identifiable. However, for complex datasets (e.g., images, time series) having overlapping cluster structures (which is the case for most real-life datasets), existing VAT approaches perform poorly as the RDI quality degrades and the contrast between dark blocks along the diagonal and the rest of the image decrease. This makes it difficult to count the dark blocks along the diagonal.\nThere have been some efforts [6\u20138] to improve the quality of VAT generated RDI to accurately estimate the number of clusters for various complex geometry datasets. The VAT family algorithms, commonly used for analyzing cluster structures, exhibit poor performance when applied to image datasets, especially those with overlapping clusters. In the typical workflow, images are flattened before employing the VAT algorithms, resulting in the loss of their crucial spatial features. Consequently, the pixel-wise Euclidean distance becomes less effective in accurately capturing similarities or dissimilarities between images due to the feature loss incurred during flattening and the curse of dimensionality. Figure 1 shows an improved Visual Assessment of Tendency (iVAT) [6] RDI for a synthetic, high-dimensional dataset (number of samples = 1000, dimensions= 100) having three well-separated Gaussian mixtures (so k=3) in View (a), and RDI for a sample of popular MNIST dataset (number of samples = 1000 dimensions= 784, k = 10 classes) in View (b). It is evident from the figure that iVAT performs well when the data has inherently well separated clusters as we can clearly see three dark blocks along the diagonal in its RDI representing three clusters. However, when it comes to image datasets like MNIST, it struggles to provide meaningful results, as the resulting RDI does not exhibit clear dark blocks along the diagonal. This limitation highlights the need for a VAT variant that can effectively preserve the essential features of images, enabling more accurate assessments of cluster structures in image datasets.\nAs unsupervised deep learning methods (like autoencoders [9] and contrastive learning [10]) excel at extracting robust features from complex images, it makes them wellsuited for developing a dedicated VAT algorithm for image datasets.\nTo address the above concerns, we propose a novel visual-analytical framework called DeepVAT. DeepVAT uti-\nlizes deep learning techniques to extract meaningful deep features from images, enabling more effective assessment of cluster structures. Unlike traditional VAT approaches, DeepVAT can uncover hidden cluster structures within image data, even in situations where ground truth labels or information about the number of classes are unavailable. Our major contributions are as follows:\n1. We proposed a deep, self-supervised learning framework, DeepVAT, that can provide visual evidence of the number of clusters present in complex image datasets.\n2. In our method, we did not incorporate any prior knowledge about the ground truth number of clusters of data.\n3. We performed experiments on four real-world, publicly available, large image datasets to show the superiority of DeepVAT over other state-of-the-art VAT family algorithms (proposed for high-dimensional data) in terms of quality of RDI, clustering accuracy, and normalized mutual information (NMI) score.\nTo the best of our knowledge, our work represents the first investigation in the literature exploring the utilization of deep features from images in the context of VAT methods. This contribution highlights the importance of incorporating deep learning techniques in the development of VAT models for accurate and insightful analysis of image datasets.\nHere is an outline of the rest of this article. Section 2 presents the preliminaries for the VAT/iVAT algorithm and reviews related work. The proposed algorithm, DeepVAT, is discussed in Section 3. Section 4 discusses the experiments and results, followed by conclusions in Section 5."
        },
        {
            "heading": "2. Preliminaries and Related work",
            "text": ""
        },
        {
            "heading": "2.1. VAT and iVAT",
            "text": "Consider we have a set of N objects, denoted as O = o1, o2, . . . , oN , where each object in O is described by a p-dimensional feature vector (\u2208 Rp). Alternatively, the data can be represented as a dissimilarity matrix, denoted\nas DN = [dij ], where dij indicates the dissimilarity between object oi and object oj computed using a suitable distance measure. The VAT algorithm considers the dissimilarity matrix, DN as input and reorders (by shuffling the rows and columns) using a modified Prim\u2019s algorithm. The image I(DN ) of the reordered distance matrix DN displays each pixel\u2019s intensity to indicate the dissimilarity between the corresponding row and column objects. When dark blocks appear along the diagonal, they might represent distinct clusters, ideally k (the original number of clusters in data) clusters. As single-linkage clusters are always diagonally aligned in VAT ordered images [11], kp aligned clusters can be obtained by cutting the largest (kp \u2212 1) edges (given by the MST cut magnitude order d) from the MST. Here, kp is the estimated number of clusters from VAT/iVAT RDI.\nThe improved-VAT (iVAT) [6] enhances the quality of VAT [4] RDI by using path-based distance transformation. The iVAT transformed matrix D \u2032\nN = [d \u2032\nij ] is generated using a path-based minimax distance [5]:\nd \u2032\nij = min p\u2208Pij max 1<h<|p| Dp[h]p[h+1] (1)\nwhere p \u2208 Pij is an acyclic path in the set of all acyclic paths between objects (oi) and (oj) (vertices i and j) in O."
        },
        {
            "heading": "2.2. VAT Variants for Large Volumes of HighDimensional Data",
            "text": "Although the VAT tool, discussed above, finds its usefulness in many applications, it can be computationally expensive as the size of the data set grows due to its O(N2) complexity. To understand the clustering structure for large volume datasets, a scalable version of VAT called scalable VAT (sVAT) was developed by Hathaway et al. [12], which utilizes a smart sampling based approach. To begin,\nsVAT extracts a smart sample of size n (where n << N ) from the large data set X using Maximin Random Sampling (MMRS) [13]. The extracted sample is then used to compute the distance matrix Dn, which is input into VAT.\nTo handle large volumes of high-dimensional datasets, Rathore et. al in [7] proposed FensiVAT, an ensemblebased, hybrid clustering framework that combines fast dataspace reduction using random projection with an intelligent sampling strategy to assess the clustering tendency of high-dimensional data. Recently, Zhang et al. [8] proposed another method that leverages a kernel-based dissimilarity matrix to refine the RDI further, called kernel-based iVAT (KernelVAT). They use a Gaussian kernel and Isolation kernel (data-dependent) to transform the RDI.\nThe SpecVAT [14] algorithm is another approach that improves the quality of the RDI produced by VAT. It utilizes spectral graph theory to transform the raw distance matrix into a graph embedding space using graph Laplacian. It then creates an alternative feature representation of the data by selecting the r most significant eigenvectors that correspond to the highest eigenvalues. VAT is then applied to this transformed representation, resulting in a much-improved RDI.\nTo our knowledge, none of the existing VAT family of algorithms, including those reviewed in this section, have been investigated thoroughly on image datasets. Moreover, they have been shown to perform poorly on image datasets in their numerical experiments. Below, we discuss our proposed framework, DeepVAT."
        },
        {
            "heading": "3. Proposed Framework: DeepVAT",
            "text": "In this paper, we propose a deep learning-based framework, DeepVAT, to advance the VAT family of algorithms for cluster structure assessment in complex image datasets.\nFigure 2 presents each step of our proposed framework. Below, we briefly explain each step of DeepVAT keyed to the blocks shown in figure 2."
        },
        {
            "heading": "3.1. Generating Image Embeddings",
            "text": "The first step in our framework is representation learning by employing deep learning architectures. The objective of this step is to attain a cluster-friendly representation of images, which involves bringing similar data points closer to each other and pushing dissimilar points further away. While deep learning architectures like autoencoders can be explored for this purpose, they may lack the inherent capability to produce a truly cluster-friendly representation.\nRecently, a wide range of self-supervised approaches such as contrastive learning based models has been proposed that can provide cluster-friendly representations for images using deep neural networks, without the need for ground truth information. These models include Simple Contrastive Learning of Representations (SimCLR) [10], Barlow Twins [15], Decoupled Contrastive Learning (DCL) [16], SimSiam [17], Bootstrap Your Own Latent (BYOL) [18], and many others.\nWe observed that incorporating SimCLR as the feature extractor in the DeepVAT pipeline led to significantly superior iVAT images compared to when an autoencoder was used as the feature extractor. The significant difference observed can be attributed to the inherent capabilities of SimCLR compared to basic autoencoders. SimCLR has the ability to effectively group similar points together and push dissimilar points apart, thanks to the InfoNCE loss it minimizes [19]. In contrast, basic autoencoders lack this inherent capability. Additionally, a recent study [20] suggests that the InfoNCE loss aids in learning cluster-preserving representations of images, further highlighting the suitability of SimCLR for DeepVAT. Hence, we chose SimCLR as our primary model for creating embeddings in our proposed framework.\nIn SimCLR, the first stage includes performing auxiliary tasks or a given batch of images, such as corrupting the data, adding noise, and creating augmented views of the same data. These transformations generate fresh views of the same images, effectively enlarging the training set. Grayscale images cannot undergo certain transformations such as color jitters. Instead, an affine stretch is utilized along with rotation, resizing, and blurring. Through these tasks, the models can acquire a rich and beneficial representation of the data.\nSimCLR consists of an encoder network and a non-linear projection head. The augmented images are fed into the encoder to extract high-level features. The encoder consists of several convolutional and fully connected layers and is trained using a contrastive loss function. The SimCLR framework utilizes an InfoNCE loss function [19] to\nmeasure the similarity between different views of an image. The model aims to maximize the similarity between the two views of the same image and minimize the similarity between views of different images. By doing so, SimCLR learns to extract valuable features robust to variations in the input data, which is helpful for generalization in realworld scenarios. The encoder projects the images into (say) d-dimensional space.\nThen, the projection head, a small neural network, further maps the encoded features (d-dimensional) to a (lower) m-dimensional set of embeddings, and then back to a lower-manifold of d-dimensional space, resulting in a rankdeficient weight matrix. This projection head is trained alongside the encoder during training. After training successfully, the projection head is discarded, and the data is passed through the trained encoder to generate embeddings. The projection head serves as an additional non-linear transformation that helps to increase the quality of the learned features."
        },
        {
            "heading": "3.2. Dimensional Reduction using t-SNE",
            "text": "Despite the fact that SimCLR embeddings (shown with a pink bar in figure 2) can be used to compute the dissimilarity matrix for VAT/iVAT, the high dimensionality of the SimCLR embeddings can lead to the curse of dimensionality problem, which can affect the quality of the resulting visualization. In our experiments, as discussed in Section 4, we observed that using SimCLR embeddings to compute the dissimilarity matrix did not result in a significant improvement in the quality of the resulting RDI for complex image datasets (CIFAR-10 [21] and INTEL [22]).\nOne way to tackle this issue is to apply t-SNE on a data representation obtained from SimCLR. Compared to the original flattened image data, t-SNE works better on SimCLR embeddings because SimCLR is a deep-layer architecture that can more efficiently represent the highly varying data manifold in multiple nonlinear layers [10,23]. The projections generated by SimCLR\u2019s projection head can identify highly varying manifolds better than a local method like t-SNE, resulting in a higher quality visualization compared to using t-SNE on the original high-dimensional data [23]. However, it is important to acknowledge that representing the complete structure of intrinsically high-dimensional data in just two or three dimensions is fundamentally impossible, highlighting a fundamental limitation."
        },
        {
            "heading": "3.3. Smart Sampling: Maximin Random Sampling",
            "text": "(MMRS)\nComputing and analyzing VAT RDI using t-SNE embeddings (shown with a pink bar in figure 2), generated in the last step, may be infeasible for image datasets with large samples (N) due to O(N2) complexity of VAT. To deal with large image datasets, we exploit a smart sampling ap-\nproach called Maximin and Random Sampling (MMRS). Let X = {xi}Ni=1 represent the set of t-SNE reduced embeddings obtained from the trained encoder, where xi \u2208 R2. The MMRS technique is an intelligent way to obtain samples in large batch data sets by combining MaxiMin (MM) and Random Sampling (RS). The MM sampling process starts by identifying a set of k \u2032 (an overestimate of k) distinguished objects, which are the farthest from each other in the input data X. Then each point in the set X is grouped with its nearest distinguished object using the nearest prototype rule (NPR) (mentioned in [7]), which divides the entire dataset into k\u2032 groups {Gi}k \u2032\ni=1 where Gi \u2286 X, \u2200i \u2208 {1, 2, . . . , k\u2032} by associating |Gi| points to ith MM sample, which represents each of the k \u2032 group. Finally, the sample S of size n << N is formed by selecting random data-points from each of the k \u2032 groups {Gi}k \u2032\ni=1. The number of points nj extracted from group Gj is proportional to the cardinality of Gj , i.e nj \u221d |Gj |. To be precise, nj = \u2308n\u00d7 |Gj |/N\u2309, where \u2308.\u2309 is the ceiling function. This step gives us a smart sample of size n << N in lower dimensional space. Rather than feeding a large number of embeddings directly into iVAT for visualization, we feed a smart sample of size n, obtained using MMRS."
        },
        {
            "heading": "3.4. Dissimilarity Matrix Computation for",
            "text": "VAT/iVAT\nThe reduced-dimension, smart samples are used to compute dissimilarity matrix Dn which is fed to the VAT/iVAT algorithm to obtain reordered dissimilarity matrix D \u2032\nn. The visualization of I(D \u2032\nn) suggests the number of clusters k present in the dataset."
        },
        {
            "heading": "4. Experiments",
            "text": "We performed experiments on four publicly available, real, image datasets. We evaluated the ability of DeepVAT to suggest the number of clusters in image datasets and compared its performance with other VAT family methods that are claimed to work better with high-dimensional data. We also compare DeepVAT with two well known deepclustering based methods. The experiments were conducted on a regular PC with the following configuration: OS: Ubuntu 22.04.2 LTS (64 bit); processor: Intel(R) Xeon(R) Gold 5220R CPU @ 2.20GHz; RAM: 62 GB; GPU: Nvidia Quadro RTX 6000, 24 GB."
        },
        {
            "heading": "4.1. Datasets",
            "text": "We performed our experiments on the following datasets:\n1. MNIST [24]: It has a total of 60, 000 grayscale training images of digits with a dimension of 28\u221728 ranging from 0 to 9, i.e., total 10 classes, with each class hav-\ning 6,000 images. The full training set is used in all experiments (60,000 images).\n2. FMNIST [25]: It has a total of 60,000 grayscale training images of fashion apparel with a dimension of 28 \u2217 28, i.e., it has a total of 10 classes, with each class having 6,000 images. The full training set is used in all experiments (60,000 images).\n3. CIFAR10 [21]: It has a total of 50, 000 natural RGB training images with a dimension of 32 \u2217 32 \u2217 3. It has a total of 10 classes, with each class having 5, 000 images. The full training set is used in all experiments (50,000 images).\n4. Intel Image Dataset [22]: It has 14, 034 natural RGB training images and 3,000 testing images with 6 classes. We clubbed both sets and used the final count of 17, 000 images to perform various experiments. Each image has a dimension of 32 \u2217 32 \u2217 3."
        },
        {
            "heading": "4.2. Evaluation Criteria",
            "text": "We show all (best) iVAT images with an estimated number of clusters (kp) for all the compared algorithms in Table 1. To estimate kp, we used the algorithm presented in [26]. As mentioned in section 2.1, kp clusters can be obtained by cutting (kp-1) edges in MST provided by VAT/iVAT algorithm. We used the predicted labels and ground truth information of each dataset to compute the partition accuracy (PA) for the estimated value of k (from iVAT image) and normalized mutual information (NMI). The PA of a clustering algorithm is the percentage (%) ratio of the number of samples with matching ground truth and algorithmic labels to the total number of samples in the dataset. To ensure consistent label mapping between the predicted and true labels, the Kuhn-Munkres algorithm [27] is employed to find the best mapping between the predicted and ground truth labels. A higher value of PA and NMI implies a better match to the ground truth partition."
        },
        {
            "heading": "4.3. Comparison of DeepVAT with other Models",
            "text": "In this section, we make a qualitative and quantitative comparison of DeepVAT with existing state-of-the-art VAT family methods that claim to work with high-dimensional and complex data (images when flattened can be seen as high-dimensional data). Specifically, we compare DeepVAT with the following methods:\n1. VAT family methods\n(a) FensiVAT: FensiiVAT [7] is applied on a small MMRS subset of the embeddings extracted from the trained encoder of SimCLR.\n(b) KernelVAT: KernelVAT [8]. is applied on a small MMRS subset of the embeddings extracted from the trained encoder of SimCLR.\n(c) SpecVAT: SpecVAT [14] is applied on a small MMRS subset of the embeddings extracted from the trained encoder of SimCLR.\n2. Deep-Clustering methods\n(a) DEC [28]: iVAT is applied to the t-SNE reduced embeddings, extracted from the trained encoder of DEC. Specifically, iVAT is applied to a smaller MMRS subset.\n(b) LSD-C [29]: The t-SNE reduced embeddings, extracted from the trained encoder of DEC, are utilized for applying iVAT. More specifically, iVAT is applied to a smaller MMRS subset.\n(c) Autoencoder + iVAT: We trained a vanilla autoencoder and obtained embeddings from the trained encoder network. Subsequently, t-SNE is applied to these embeddings, and iVAT is then applied specifically to a smaller MMRS subset of the reduced embeddings."
        },
        {
            "heading": "4.3.1 Parameter Settings",
            "text": "In DeepVAT, the SimCLR model was trained using the LARS optimizer [30] for each dataset, with 1, 000 epochs. The output dimension of the encoder network was set to d = 2, 048, and the projection head network was chosen to have m = 128. We performed each experiment five times on each dataset and reported the average results. We use a batch size of 700 for MNIST and FMNIST and 256 for CIFAR10 and Intel Image Dataset. The parameters for MMRS sampling are k\u2032 = 15 for MNIST, FMNIST, and CIFAR10, and 10 for INTEL, number of samples, n: 4,000 for all datasets.\nEuclidean distance is utilized as the metric to generate the RDI for the t-SNE reduced embeddings of the MNIST dataset. Likewise, in the ablation study discussed in Section 4.4, Euclidean distance is employed to generate the RDI for the t-SNE reduced embeddings of the raw-flattened MNIST data. For all other experiments, cosine dissimilarity is utilized as the dissimilarity measure to generate the RDI.\nThe input to all three VAT based methods is 2048- dimensional SimCLR embeddings as these methods transform the original data into a suitable embedding space/low dimensional space by virtue of their design. In KernelVAT, radial basis function (RBF) kernel is used, with the precision parameter (\u03b3) set to 0.05. In FensiVAT, the down-space (reduced) dimension for random projection is chosen 100 when FensiVAT is applied to a 2048-dimensional SimCLR embedding. In SpecVAT, we performed iterations over the\nparameter number of eigen-values (r) ranging from 1 to 10 and noted the best result.\nDEC [28] and LSD-C [29] heavily rely on prior information about the number of clusters in a dataset, while DeepVAT do not require this specific information. As stated in Section 4.3.1, we deliberately choose an overestimate for the number of clusters in all our experiments involving VAT algorithms. Consequently, for a fair comparison, we adopt the same overestimate (k \u2032 = 15 for MNIST, FMNIST, and CIFAR-10, and k \u2032\n= 10 for INTEL) for DEC (which requires the value of k for performing k-means) and LSD-C (where the linear layer after the encoder has the same number of neurons as the number of classes).\nTo ensure fairness in the assessment, just like DeepVAT utilized t-SNE reduced embeddings from the SimCLR encoder, we also apply t-SNE to reduce the embeddings generated from the encoders of DEC and LSD-C to 2 dimensions before generating the RDI.\nWe keep all the parameter settings the same unless stated otherwise."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": ""
        },
        {
            "heading": "4.4.1 Results and Discussions",
            "text": "Table 1 shows the comparison of all six models based on the RDI quality and their ability to estimate the underlying clusters (kp) accurately. Table 2 shows the comparison of DeepVAT with all the six methods mentioned above based on the PA and NMI.\nWe can see that the DeepVAT method generates much clearer and sharper dark blocks compared to the SpecVAT, KernelVAT, and FensiVAT models. Consequently, the number of dark blocks generated by DeepVAT (kp) is close to the original number of classes (k) in the dataset, making it the most accurate in estimating the potential number of clusters compared to other algorithms. When applying FensiVAT and KernelVAT directly to the high-dimensional embedding, we observed that they produced blurry RDI (the cluster count is good but the quality of RDI is poor) and only achieved moderate quantitative results in terms of PA and NMI (refer Table 2) for simple datasets such as MNIST and FMNIST. However, when dealing with complex datasets like CIFAR-10 and INTEL, both algorithms failed to generate high-quality RDI and quantitative results (refer Table 2). Note that clustering algorithms face significant challenges when dealing with these datasets, as the ground truth labels may not accurately reflect distinct clusters within the feature vector representation of the data points. These results suggest that our approach produces more visually appealing and informative representations of the data.\nBased on the results presented in Table 2, DeepVAT demonstrates a significant performance advantage over state-of-the-art VAT family methods in terms of both PA and\nNMI metrics. DeepVAT demonstrates its superiority over deep-clustering algorithms by achieving a 35% improvement in PA and 30% improvement in NMI. Furthermore, it outperforms simple autoencoders by an impressive 95% on PA and 203% on NMI metrics, clearly highlighting its remarkable performance. As a result, DeepVAT surpasses\nall six competitive models in both PA and NMI measures.\nThe success of DeepVAT can be attributed to the use of SimCLR and t-SNE. SimCLR is effective at generating a robust representation of the dataset by leveraging nonlinear functions, such as deep CNN encoders and projection heads, to approximate its intrinsic dimensionality. By ap-\nplying t-SNE on the representation produced by SimCLR, we obtain a better low-dimensional embedding, as SimCLR is better equipped to detect highly varying manifolds than t-SNE alone.\nTo examine the impact of various components in our model, we perform a three-part ablation study on all four datasets. We systematically eliminate one component at a time from the DeepVAT pipeline (Fig. 2) to assess its reliance within the complete pipeline. Our model is summarised as DeepVAT = SimCLR + t-SNE + MMRS + iVAT.\n1. DeepVAT minus SimCLR: We flatten each image in the dataset and apply t-SNE on top of them. Then we sample using MMRS and compute the final iVAT image for the samples. This will show that our model not only benefits from the t-SNE block.\n2. DeepVAT minus t-SNE: Images are passed through a trained SimCLR encoder, and we sample the learned high-dimensional embeddings using MMRS. The final iVAT image is computed on the sampled embeddings.\n3. DeepVAT minus MMRS: iVAT image is computed on full set of embeddings. However, as iVAT/VAT family algorithms require computation of dissimilarity matrix, which has a time complexity of order O(N2), it will take hours to get the results. Hence, due to such large time complexity and resource constraint, we are not reporting the results of this ablation.\n4. DeepVAT minus tSNE minus SimCLR: We apply iVAT directly on the MMRS sub-set of raw flattened images.\nThe findings of the ablation study (1) (Table 3) suggest that the generation of RDI by DeepVAT is not solely reliant on t-SNE. Although t-SNE applied directly to raw flattened images produces reasonably good results, it is not as accurate as DeepVAT. However, when dealing with complex datasets like CIFAR-10, utilizing t-SNE on raw flattened images fails to provide meaningful information about\nthe cluster structure. Additionally, the role of the SimCLR module in DeepVAT is investigated in the study (2). The results in Table 3 indicate that SimCLR alone does not yield satisfactory outcomes, although it still demonstrates limited interpretability for simple datasets like MNIST and FMNIST. Nevertheless, when iVAT is applied to SimCLR embeddings for complex datasets, it fails to convey meaningful results. This limitation may be attributed to the high dimensionality of the SimCLR embeddings (2048), which hinders the accurate inference of cluster presence by iVAT."
        },
        {
            "heading": "5. Conclusions and Future Work",
            "text": "This article proposes a deep, self-supervised learning based VAT framework, DeepVAT, for cluster structure assessment in image data. The self-supervised learning method SimCLR significantly improved the performance of iVAT both qualitatively and quantitatively. Our experimental results suggest that when t-SNE is used as dimensionality reduction on top of SimCLR embeddings, the iVAT yields a much sharper RDI, thus a more accurate estimate of the number of clusters. This is because SimCLR can capture the intrinsic dimensionality of image datasets which helped t-SNE in generating a good low dimensional representation. Based on our numerical experiments on four image datasets, we have also shown that DeepVAT significantly outperformed other VAT family methods (FensiVAT, KernelVAT and SpecVAT) and two deep clustering methods (DEC and LSD-C) based on clustering partition accuracy (PA) and NMI. We believe that deploying more deep learning based models like deep metric learning and semisupervised, which have partial access to labels can further improve the iVAT image for complex datasets.\nAt present, the training time for major self-supervised contrastive learning models is quite extensive. As part of our future work, we aim to focus on reducing the training time required for such models. Our objective is to develop methods that can generate high-quality iVAT images using self-supervised contrastive learning models in significantly less time."
        }
    ],
    "title": "DeepVAT: A Self-Supervised Technique for Cluster Assessment in Image Datasets",
    "year": 2023
}