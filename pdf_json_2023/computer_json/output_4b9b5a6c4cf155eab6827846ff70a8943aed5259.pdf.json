{
    "abstractText": "The presence of a large number of bots in Online Social Networks (OSN) leads to undesirable social effects. Graph neural networks (GNNs) are effective in detecting bots as they utilize user interactions. However, class-imbalanced issues can affect bot detection performance. To address this, we propose an over-sampling strategy for GNNs (OS-GNN) that generates samples for the minority class without edge synthesis. First, node features are mapped to a feature space through neighborhood aggregation. Then, we generate samples for the minority class in the feature space. Finally, the augmented features are used to train the classifiers. This framework is general and can be easily extended into different GNN architectures. The proposed framework is evaluated using three real-world bot detection benchmark datasets, and it consistently exhibits superiority over the baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuhao Shi"
        },
        {
            "affiliations": [],
            "name": "Kai Qiao"
        },
        {
            "affiliations": [],
            "name": "Jie Yang"
        },
        {
            "affiliations": [],
            "name": "Baojie Song"
        },
        {
            "affiliations": [],
            "name": "Jian Chen"
        },
        {
            "affiliations": [],
            "name": "Bin Yan"
        }
    ],
    "id": "SP:8e317a3a90a380a5f03c6b6dafc2d492fadb074a",
    "references": [
        {
            "authors": [
                "Stefano Cresci",
                "Roberto Di Pietro",
                "Marinella Petrocchi"
            ],
            "title": "The paradigm-shift of social spambots: Evidence, theories, and tools for the arms race",
            "venue": "Proceedings of WWW, 2017, pp. 963\u2013972.",
            "year": 2017
        },
        {
            "authors": [
                "Emilio Ferrara",
                "Onur Varol",
                "Clayton A. Davis",
                "Filippo Menczer",
                "Alessandro Flammini"
            ],
            "title": "The rise of social bots",
            "venue": "Communications of the ACM, vol. 59, pp. 96\u2013104, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Shangbin Feng",
                "Herun Wan",
                "Ningnan Wang",
                "Jundong Li",
                "Minnan Luo"
            ],
            "title": "Satar: A self-supervised approach to twitter account representation learning and its application in bot detection",
            "venue": "Proceedings of ACM CIKM, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Shangbin Feng",
                "Herun Wan",
                "Ningnan Wang",
                "Minnan Luo"
            ],
            "title": "Botrgcn: Twitter bot detection with relational graph convolutional networks",
            "venue": "Proceedings of IEEE/ACM ASONAM, pp. 236\u2013239, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Shangbin Feng",
                "Zhaoxuan Tan",
                "Rui Li",
                "Minnan Luo"
            ],
            "title": "Heterogeneity-aware twitter bot detection with relational graph transformers",
            "venue": "Proceedings of AAAI, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Shangbin Feng",
                "Zhaoxuan Tan",
                "Herun Wan",
                "Ningnan Wang",
                "Zilong Chen",
                "Binchi Zhang",
                "Qinghua Zheng",
                "Wenqian Zhang",
                "Zhenyu Lei",
                "Shujie Yang",
                "Xinshun Feng",
                "Qing Zhang",
                "Hongrui Wang",
                "Yuhan Liu",
                "Yuyang Bai",
                "Heng Wang",
                "Zijian Cai",
                "Yanbo Wang",
                "Lijing Zheng",
                "Zihan Ma",
                "Jundong Li",
                "Minnan Luo"
            ],
            "title": "Twibot-22: Towards graph-based twitter bot detection",
            "venue": "ArXiv, vol. abs/2206.04564, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Kai-Cheng Yang",
                "Onur Varol",
                "Pik-Mai Hui",
                "Filippo Menczer"
            ],
            "title": "Scalable and generalizable social bot detection through data selection",
            "venue": "Proceedings of AAAI, 2019, vol. 34, pp. 1096\u20131103.",
            "year": 2019
        },
        {
            "authors": [
                "Shuhao Shi",
                "Kai Qiao",
                "Jian Chen"
            ],
            "title": "Mgtab: A multi-relational graph-based twitter account detection benchmark",
            "venue": "ArXiv, vol. abs/2301.01123, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Adrian Rauchfleisch",
                "Jonas Kaiser"
            ],
            "title": "The false positive problem of automatic bot detection in social science research",
            "venue": "PLoS ONE, vol. 15, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Stefano Cresci",
                "Roberto Di Pietro",
                "Marinella Petrocchi",
                "Angelo Spognardi",
                "Maurizio Tesconi"
            ],
            "title": "Fame for sale: Efficient detection of fake twitter followers",
            "venue": "Decis. Support Syst., vol. 80, pp. 56\u201371, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Zafar Gilani",
                "Reza Farahbakhsh",
                "Gareth Tyson",
                "Liang Wang",
                "Jon A. Crowcroft"
            ],
            "title": "Of bots and humans (on twitter)",
            "venue": "Proceedings of IEEE/ACM ASONAM, pp. 349\u2013354, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Shangbin Feng",
                "Herun Wan",
                "Ningnan Wang",
                "Jundong Li",
                "Minnan Luo"
            ],
            "title": "Twibot-20: A comprehensive twitter bot detection benchmark",
            "venue": "Proceedings of ACM CIKM, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Fenyu Hu",
                "Liping Wang",
                "Q. Liu",
                "Shu Wu",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "title": "Graphdive: Graph classification by mixture of diverse experts",
            "venue": "Proceedings of IJCAI, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Tianxiang Zhao",
                "Xiang Zhang",
                "Suhang Wang"
            ],
            "title": "Graphsmote: Imbalanced node classification on graphs with graph neural networks",
            "venue": "Proceedings of WSDM, 2021, pp. 833\u2013841.",
            "year": 2021
        },
        {
            "authors": [
                "Joonhyung Park",
                "Jae gyun Song",
                "Eunho Yang"
            ],
            "title": "Graphens: Neighbor-aware ego network synthesis for class-imbalanced node classification",
            "venue": "Proceedings of ICLR, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Yijun Duan",
                "Xin Liu",
                "Adam Jatowt"
            ],
            "title": "Anonymity can help minority: A novel synthetic data over-sampling strategy on multi-label graphs",
            "venue": "ECML-PKDD."
        },
        {
            "authors": [
                "N. Chawla",
                "K. Bowyer",
                "Lawrence O. Hall",
                "W. Philip Kegelmeyer"
            ],
            "title": "Smote: Synthetic minority over-sampling technique",
            "venue": "ArXiv, vol. abs/1106.1813, 2002.",
            "year": 1813
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross B. Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, pp. 318\u2013327, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Yin Cui",
                "Menglin Jia",
                "Tsung-Yi Lin",
                "Yang Song",
                "Serge J. Belongie"
            ],
            "title": "Class-balanced loss based on effective number of samples",
            "venue": "Proceedings of CVPR, 2019, pp. 9260\u20139269.",
            "year": 2019
        },
        {
            "authors": [
                "Min Shi",
                "Yufei Tang",
                "Xingquan Zhu"
            ],
            "title": "Multi-class imbalanced graph convolutional network learning",
            "venue": "Proceedings of IJCAI, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Mahsa Ghorbani",
                "Anees Kazi",
                "Mahdieh Soleymani Baghshah",
                "Hamid R. Rabiee",
                "Nassir Navab"
            ],
            "title": "Ragcn: Graph convolutional network for disease prediction problems with imbalanced data",
            "venue": "Medical image analysis, vol. 75, pp. 102272, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "ArXiv, vol. abs/1609.02907, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "William L. Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Proceedings of NIPS, 2017, vol. 30.",
            "year": 2017
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio\u2019",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "ArXiv, vol. abs/1710.10903, 2017.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Graph neural networks, Bot detection, Class-imbalance, Over-sampling, Feature space\n1. INTRODUCTION\nOnline Social Networks (OSN) have been plagued by bots and malicious accounts, which has caused negative social effects [1, 2]. To address this issue, researchers have proposed various bot detection methods. Currently, the most effective bot detection methods [3, 4, 5, 6] use Graph Neural Networks (GNNs) to exploit user relationships.\nIn bot detection, users are classified as either bots or humans. The imbalance ratio \u03c1 = MN is used to measure class imbalance, where N and M represent majority and minority class sample numbers, respectively. Table 1 summarizes the existing Twitter bot detection datasets, which mostly have imbalanced bot and human user distributions.\nGraph-based bot detection methods have not adequately addressed the issue of class imbalance. When the training data is imbalanced, the model may struggle to learn enough features from the minority class, resulting in a tendency to classify most samples as belonging to the majority class. Figure 1 shows the results of bot detection using GNNs on the\n*Corresponding Author. This work was supported by the National Key Research and Development Project of China under Grants 2020YFC1522002.\nMGTAB [8] and TwiBot-20 [12] datasets. Despite high overall accuracy, there is a significant bias towards predicting the majority class, with the minority class being much less accurately classified [13]. This is especially the case when the degree of imbalance is large.\nIn this paper, we propose Over-Sampling Strategy in Feature Space for Graphs Neural Networks (OS-GNN) to address class imbalance in GNN models in bot detection. Our approach generates synthetic features for the minority class by oversampling in the feature space, without synthesizing node edges. We then use the rebalanced feature matrix for classification. Our method outperforms previous methods such as GraphSMOTE [14] and GraphEns [15] on various bot detection benchmark datasets with different GNN architectures.\n2. PRELIMINARY"
        },
        {
            "heading": "2.1. Notations and Problem Definition",
            "text": "The distribution of class scale for bot and human are imbalanced, as Table 1 shown, resulting in GNN classifiers\u2019 bias towards the majority class.\nLet G = (V,E,H) denote a social graph, where V = {v1, v2, . . . , vN} is a set of users and E is a set of relationships between them. The relationships include friends and followers. H \u2208 RN\u00d7d is the feature matrix, where d is the dimension of the node attribute. Y \u2208 Rn is the label for nodes in G. We denote the set of labeled nodes as VL and the set\nar X\niv :2\n30 2.\n06 90\n0v 2\n[ cs\n.C V\n] 1\n1 Se\np 20\n23\nof synthetic nodes as VS. In bot detection, labels are obtained through manual annotation, which can be costly. Thus, |VL| << |V| is typical. Our goal is to generate nodes for the minority class to improve accuracy on the minority class and overall model accuracy in class-imbalanced graphs.\nf ( VL, VS ,H) \u2192 Y (1)"
        },
        {
            "heading": "2.2. Graph Neural Networks",
            "text": "GNNs learn node representations according to the graph structure, and the process can be formulated as:\nh (l) v = COMBINE ( h (l) v ,AGGREGATE ({ h (l\u22121) u ,\u2200u \u2208 Nv })) ,\n(2) where hlv denotes feature of node v at l-th GNN layer and Nv represent the neighbor set of node v ; h (0) v is initialized with node attribute hv . AGGREGATE(\u00b7) and COMBINE(\u00b7) represent neighbor aggregation and combination functions, respectively. The aggregation function usually needs to be differentiable and permutation invariant.\n3. PROPOSED METHOD"
        },
        {
            "heading": "3.1. Motivation",
            "text": "Current over-sampling methods for GNNs [14, 16] create nodes in either the original feature space or the embedding space. However, the newly generated nodes lack connectivity relationships (edges), which must be constructed after over-sampling. This results in an approximation of the true distribution of edges and increases noise in the synthetic data, ultimately impacting the GNNs\u2019 performance.\nOS-GNN addresses this issue by generating embeddings for minority class nodes in feature space to balance the distribution of different classes."
        },
        {
            "heading": "3.2. Neighbor Aggregator",
            "text": "Neighborhood aggregation obtains node embeddings using neighborhood information. The original attribute of node vi is hi and the representation with neighborhood information qi is obtained through the neighbor aggregator. We use 2- hop neighborhood aggregation to prevent over-smoothing and over-fitting, i.e., qi = h2i , and formulate the neighbor aggregation as in Eq. (2).\nAs the original graph is imbalanced, some bot nodes may be largely connected to human nodes. Neighborhood aggregation may make the embeddings of these bot nodes similar to human nodes. To preserve the original node information, we concatenate qi with hi to obtain xi = [qi\u2225hi], the representation of node vi."
        },
        {
            "heading": "3.3. Synthetic Node Generator",
            "text": "After obtaining X = [xi, 0 \u2264 i \u2264 N ], we apply SMOTE [17] to generate synthetic minority nodes. In this way, we can make the distribution of different classes more balanced, making the trained classifier perform better on minority class samples initially under-represented.\nWe use the hyper-parameter \u03c9, over-sampling scale, to control the amount of nodes to be generated for minority class. For minority class node vi, let \u0393(\u00b7) denote the set of k-nearest neighbors measured by Euclidean distance in the feature space. A random node vu, with the same labels as vi, is selected from \u0393 (vi). A random point on the line between xu and xi is chosen as xk:\nxk = (1\u2212 \u03b4) \u00b7 xi + \u03b4 \u00b7 xu, (3)\nwhere xk is the embedding of the virtual node vk in the feature space. \u03b4 is a random variable, following uniform distribution, ranges from 0 to 1. vk has the same labels as vi and vu. Thus, we obtain labeled synthetic minority class samples that equalize the sample distribution."
        },
        {
            "heading": "3.4. GNN Classifier",
            "text": "Let X\u0303 be the augmented node representations in the feature space, concatenating X with the embedding of the synthetics nodes. As the embeddings already contain neighbourhood information, the generation of edges for the synthetic nodes is not required. For the GCN classifier, the formula is shown as:\nY\u0302 = softmax ( A\u03c3 ( AHW(1) ) W(2) ) . (4)\nUse the identity matrix E as the input adjacency matrix A of GNN classifier f(\u00b7). The labels and the augmented embed-\ndings matrix X\u0303 are input into f for training. The formula for our OS-GCN, using GCN as backbone, is shown as Eq. (5).\nY\u0302 = softmax ( \u03c3 ( X\u0303W(1) ) .W(2) ) (5)\nThe final objective function of GNN classifier can be written as:\nL (y\u2032) = \u2211\nvi\u2208V L loss (yi, y\n\u2032 i) + \u03bb \u2211 vj\u2208V s loss ( yj , y \u2032 j ) , (6)\nwhere y\u2032i denotes predicted label of node vi, and \u03bb is the parameter ranging from 0 to 1. The loss function measures the supervised loss between real and predicted labels. Different GNN models can be adopted for classifier, and the entire framework is efficient and easy to implement.\n4. EXPERIMENT"
        },
        {
            "heading": "4.1. Experimental Settings",
            "text": "Datasets We evaluate models on three Twitter bot detection datasets that with graph structures: Cresci-15 [10], TwiBot20 [12], MGTAB [8]. We conduct a 1:1:8 random partition as training, validation, and test set for all datasets, and use the most commonly used follower and friend relationships during evaluation. Evaluation Metrics We adopt three criteria: Accuracy (Acc), F1-macro scores (F1), as well as balanced accuracy (bAcc) for the class-imbalanced node classification. To further evaluate performance on minority class data, we adopt two criteria: true positive rate (TPR) and minority class accuracy (Minor Acc). Configurations All experiments were conducted on a server with 9 GPUs (RTX TITAN). All models were trained using\nthe Adam optimization algorithm and 2 layers for GNN. The learning rate was initialized to 1e-3 with weight decay being 5e-4. Models were trained until convergence, with a maximum epoch of 500. The minority class samples were generated to match the majority class using \u03c9 set to N\u2212MM ( 1\u2212\u03c1 \u03c1 ). Hyper-parameter \u03bb was set to 0.8 for optimal accuracy and TPR. \u03b2 \u2208 {0.9, 0.99, 0.999, 0.9999} for CB loss and \u03b1 was set to 0.25 for Focal loss. Focal loss and CB loss both used \u03b3 set to 2."
        },
        {
            "heading": "4.2. Baseline methods",
            "text": "To validate our method, we compare our model with a variety of baseline methods using different rebalance methods. We consider seven baseline methods, including vanilla (cross entropy), Focal loss [18], Class Balanced (CB) loss [19], DR-GCN [20], RA-GCN [21], GraphSMOTE [14], and GraphENS [15]."
        },
        {
            "heading": "4.3. Main Results",
            "text": "Bot detection performance We adopt three widely used homogeneous GNNs as backbone: GCN [22], SAGE [23], and GAT [24]. In Table 2, we report average results with standard deviation for the baselines and ours. All OS-GNN with different backbones showed significant improvements compared to the \u201cVanilla\u201d setting, in which no special algorithm is adopted. In most cases, OS-GNN outperforms all baselines in all datasets, demonstrating our method\u2019s effectiveness. Across all datasets, OS-GNN achieved a minor boost on the Cresci-15 dataset. This is due to the unimportance of edges in the Cresci-15 for node classification, reflected in heterogeneous GNNs only slightly outperform homogeneous GNNs. In addition, bot detection on Cresci-15 is a simple task with less negative effects of imbalance.\nTable 2. Experimental results of our OS-GNN and other baselines on three bot detection benchmark datasets. We report averaged accuracy (Acc), F1-macro score (F1), and balanced accuracy (bAcc) and with the standard errors for 5 repetitions on three representative GNN architectures.\nMethod TwiBot-20 (\u03c1 = 0.795, \u03c9 = 0.258) Cresci-15 (\u03c1 = 0.582, \u03c9 = 0.718) MGTAB (\u03c1 = 0.375, \u03c9 = 1.667) Acc F1 bAcc Acc F1 bAcc Acc F1 bAcc\nG C\nN Vanilla 68.76 \u00b10.60 68.30 \u00b10.51 68.29 \u00b10.62 96.50 \u00b10.36 96.20 \u00b10.42 95.95 \u00b10.53 82.69 \u00b10.76 74.85 \u00b11.32 72.32 \u00b11.29 Focal loss 69.83 \u00b10.95 69.68 \u00b10.86 68.37 \u00b10.79 96.61 \u00b10.31 96.27 \u00b10.82 96.23 \u00b10.65 84.43 \u00b10.68 78.39 \u00b11.17 76.31 \u00b11.54 CB loss 69.94 \u00b10.55 69.81 \u00b10.53 68.36 \u00b10.57 94.03 \u00b12.08 93.77 \u00b14.03 94.25 \u00b13.86 84.12 \u00b10.65 80.14 \u00b10.91 80.60 \u00b11.15 DR-GCN 76.40 \u00b11.05 75.50 \u00b11.34 76.23 \u00b10.72 93.96 \u00b12.84 93.64 \u00b13.37 94.01 \u00b12.20 83.23 \u00b12.93 72.58 \u00b15.10 76.32 \u00b13.25 RA-GCN 73.65 \u00b10.65 73.14 \u00b10.49 72.08 \u00b11.24 96.67 \u00b10.11 96.40 \u00b10.12 95.74 \u00b10.31 82.13 \u00b10.85 78.02 \u00b10.85 78.60 \u00b11.37 GraphSmote 76.40 \u00b10.79 71.50 \u00b11.47 75.25 \u00b11.01 96.56 \u00b10.22 96.26 \u00b10.27 95.98 \u00b10.31 83.28 \u00b10.61 79.21 \u00b10.73 81.96 \u00b11.32 GraphEns 76.12 \u00b10.79 74.39 \u00b10.38 76.13 \u00b10.76 96.61 \u00b10.17 96.31 \u00b10.19 96.17 \u00b10.23 82.83 \u00b10.85 77.21 \u00b12.43 78.63 \u00b11.43 Ours 83.44 \u00b10.40 83.18 \u00b10.35 83.12 \u00b10.24 96.73 \u00b10.30 96.46 \u00b10.18 96.43 \u00b10.19 85.84 \u00b10.92 83.27 \u00b10.80 85.81 \u00b10.33\nSA G\nE Vanilla 79.65 \u00b10.59 79.31 \u00b10.56 78.85 \u00b10.79 96.41 \u00b10.29 95.41 \u00b10.33 96.04 \u00b10.24 85.34 \u00b10.49 81.70 \u00b10.83 79.33 \u00b11.67 Focal loss 78.02 \u00b11.57 77.69 \u00b11.53 78.24 \u00b11.49 96.22 \u00b10.31 95.89 \u00b10.31 95.61 \u00b10.55 85.84 \u00b10.51 82.20 \u00b10.63 82.17 \u00b11.10 CB loss 78.49 \u00b10.48 77.23 \u00b10.45 77.12 \u00b10.70 96.04 \u00b10.32 95.73 \u00b10.35 95.70 \u00b10.57 86.02 \u00b10.76 82.76 \u00b10.87 83.92 \u00b10.75 DR-GCN 79.87 \u00b10.80 78.50 \u00b10.66 79.01 \u00b10.55 95.52 \u00b11.02 95.64 \u00b12.26 94.87 \u00b11.18 85.61 \u00b10.94 81.15 \u00b11.20 78.82 \u00b11.16 RA-GCN 77.65 \u00b10.44 78.14 \u00b10.67 76.65 \u00b11.03 96.32 \u00b10.27 96.11 \u00b10.42 96.16 \u00b10.24 85.23 \u00b10.85 78.02 \u00b10.85 78.06 \u00b11.12 GraphSmote 80.25 \u00b10.74 79.14 \u00b10.95 79.35 \u00b10.54 96.34 \u00b10.72 95.82 \u00b10.96 94.28 \u00b12.17 85.51 \u00b10.62 81.15 \u00b10.70 78.22 \u00b10.75 GraphEns 79.85 \u00b10.37 79.42 \u00b10.46 78.83 \u00b10.61 96.04 \u00b10.28 95.73 \u00b10.37 94.56 \u00b11.34 86.19 \u00b10.54 81.45 \u00b10.58 81.04 \u00b10.63 Ours 81.32 \u00b10.65 81.03 \u00b10.61 81.00 \u00b10.65 96.42 \u00b10.33 95.62 \u00b10.35 96.06 \u00b10.61 86.21 \u00b10.35 82.81 \u00b10.24 84.36 \u00b10.44\nG A\nT Vanilla 72.80 \u00b10.11 72.31 \u00b10.27 71.57 \u00b10.88 96.49 \u00b10.15 96.18 \u00b10.30 95.86 \u00b10.39 84.46 \u00b11.13 80.47 \u00b11.29 79.35 \u00b11.58 Focal loss 71.12 \u00b11.46 71.66 \u00b11.40 71.22 \u00b11.03 96.43 \u00b10.26 96.11 \u00b10.29 95.65 \u00b10.47 83.77 \u00b11.15 78.60 \u00b10.99 77.68 \u00b11.57 CB loss 72.13 \u00b10.73 71.52 \u00b10.66 71.33 \u00b11.80 96.47 \u00b10.32 96.18 \u00b10.34 95.98 \u00b10.42 84.58 \u00b10.72 81.12 \u00b10.70 82.35 \u00b11.04 DR-GCN 76.40 \u00b11.05 75.50 \u00b11.34 75.87 \u00b11.45 93.16 \u00b14.84 92.64 \u00b15.03 92.01 \u00b12.36 83.23 \u00b12.93 74.58 \u00b13.10 76.68 \u00b11.57 RA-GCN 76.17 \u00b10.65 75.47 \u00b11.23 75.81 \u00b10.85 94.15 \u00b10.25 93.80 \u00b10.25 94.32 \u00b10.23 84.76 \u00b10.65 80.28 \u00b12.51 78.98 \u00b11.02 GraphSmote 77.42 \u00b10.61 76.34 \u00b11.05 76.39 \u00b11.63 96.55 \u00b10.27 96.38 \u00b10.30 95.56 \u00b10.49 84.92 \u00b11.35 81.13 \u00b12.23 76.84 \u00b11.72 GraphEns 76.17 \u00b10.25 75.47 \u00b10.32 75.41 \u00b10.53 96.57 \u00b10.26 96.45 \u00b10.33 94.95 \u00b10.45 85.06 \u00b10.65 80.82 \u00b11.21 74.32 \u00b11.14 Ours 82.49 \u00b10.42 82.30 \u00b10.37 82.41 \u00b10.25 96.65 \u00b10.36 96.38 \u00b10.39 96.35 \u00b10.40 86.75 \u00b10.74 85.39 \u00b10.71 87.18 \u00b10.50\nVanilla GraphSMOTE OS-GNN\nTwiBot-20:GCN\n60\n65\n70\n75\n80\n85\nMinor Acc TPR\nVanilla GraphSMOTE OS-GNN\nTwiBot-20:GAT\n60\n65\n70\n75\n80\n85\nMinor Acc TPR\nFig. 3. Comparison of TPR and Minor Acc on the TwiBot-20 datasets. (a) and (b) use GCN and GAT as backbone model, respectively.\nImproving True Positives Rate In Figure 1, the minority class classification is low because the model\u2019s classification boundary overlaps with the minority class. OS-GNN significantly improves bAcc compared to baseline models, as shown in Table 2. We verify that OS-GNN improves GNN performance by reducing false positives, as seen in the steady increase of TPR and Minor Acc in Figure 3."
        },
        {
            "heading": "4.4. Parameters Sensitivity Analysis",
            "text": "We tested OS-GNN with different values of \u03bb in Eq. (6). We experimented with values from 0.1 to 1.0, running each test five times, and averaging the results shown in Figure 4.\nOur method is sensitive to \u03bb. When \u03bb increases, accuracy initially improves before decreasing. This is because \u03bb controls the weight of synthesized minority class samples in the loss function. As \u03bb increases, the model focuses more on the\n0.0 0.2 0.4 0.6 0.8 1.0 85.0\n85.5\n86.0\n86.5\n87.0\n87.5\n0.0 0.2 0.4 0.6 0.8 1.0 65.0\n70.0\n75.0\n80.0\n85.0\n90.0\nA cc ur ac y\nl\nGCN GAT SAGE TP\nR\nl\nFig. 4. Sensitivity to hyperparameters \u03bb. The picture on the left and right shows the accuracy and TPR change of OS-GNN as \u03bb increases, respectively.\nsynthesis of minority samples, resulting in a continuous improvement in TPR. However, an excessive \u03bb makes the model concentrate too much on synthetic samples, which reduces the performance of majority class samples and ultimately affects the overall classification accuracy.\n5. CONCLUSIONS\nIn this paper, we introduces Over-Sampling Strategy in Feature Space for Graphs Neural Networks (OS-GNN), an efficient method for tackling the challenge of imbalance in bot detection. Instead of constructing edges, OS-GNN generates embeddings in the feature space for the minority class, reducing noise and improving efficiency. Experimental results on bot detection benchmark datasets demonstrated OS-GNN\u2019s effectiveness.\n6. REFERENCES\n[1] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, et al., \u201cThe paradigm-shift of social spambots: Evidence, theories, and tools for the arms race,\u201d in Proceedings of WWW, 2017, pp. 963\u2013972.\n[2] Emilio Ferrara, Onur Varol, Clayton A. Davis, Filippo Menczer, and Alessandro Flammini, \u201cThe rise of social bots,\u201d Communications of the ACM, vol. 59, pp. 96\u2013104, 2014.\n[3] Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan Luo, \u201cSatar: A self-supervised approach to twitter account representation learning and its application in bot detection,\u201d in Proceedings of ACM CIKM, 2021.\n[4] Shangbin Feng, Herun Wan, Ningnan Wang, and Minnan Luo, \u201cBotrgcn: Twitter bot detection with relational graph convolutional networks,\u201d Proceedings of IEEE/ACM ASONAM, pp. 236\u2013239, 2021.\n[5] Shangbin Feng, Zhaoxuan Tan, Rui Li, and Minnan Luo, \u201cHeterogeneity-aware twitter bot detection with relational graph transformers,\u201d in Proceedings of AAAI, 2021.\n[6] Shangbin Feng, Zhaoxuan Tan, Herun Wan, Ningnan Wang, Zilong Chen, Binchi Zhang, Qinghua Zheng, Wenqian Zhang, Zhenyu Lei, Shujie Yang, Xinshun Feng, Qing Zhang, Hongrui Wang, Yuhan Liu, Yuyang Bai, Heng Wang, Zijian Cai, Yanbo Wang, Lijing Zheng, Zihan Ma, Jundong Li, and Minnan Luo, \u201cTwibot-22: Towards graph-based twitter bot detection,\u201d ArXiv, vol. abs/2206.04564, 2022.\n[7] Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, and Filippo Menczer, \u201cScalable and generalizable social bot detection through data selection,\u201d in Proceedings of AAAI, 2019, vol. 34, pp. 1096\u20131103.\n[8] Shuhao Shi, Kai Qiao, Jian Chen, et al., \u201cMgtab: A multi-relational graph-based twitter account detection benchmark,\u201d ArXiv, vol. abs/2301.01123, 2023.\n[9] Adrian Rauchfleisch and Jonas Kaiser, \u201cThe false positive problem of automatic bot detection in social science research,\u201d PLoS ONE, vol. 15, 2020.\n[10] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, and Maurizio Tesconi, \u201cFame for sale: Efficient detection of fake twitter followers,\u201d Decis. Support Syst., vol. 80, pp. 56\u201371, 2015.\n[11] Zafar Gilani, Reza Farahbakhsh, Gareth Tyson, Liang Wang, and Jon A. Crowcroft, \u201cOf bots and humans (on twitter),\u201d Proceedings of IEEE/ACM ASONAM, pp. 349\u2013354, 2017.\n[12] Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan Luo, \u201cTwibot-20: A comprehensive twitter bot detection benchmark,\u201d Proceedings of ACM CIKM, 2021.\n[13] Fenyu Hu, Liping Wang, Q. Liu, Shu Wu, Liang Wang, and Tieniu Tan, \u201cGraphdive: Graph classification by mixture of diverse experts,\u201d in Proceedings of IJCAI, 2022.\n[14] Tianxiang Zhao, Xiang Zhang, and Suhang Wang, \u201cGraphsmote: Imbalanced node classification on graphs with graph neural networks,\u201d in Proceedings of WSDM, 2021, pp. 833\u2013841.\n[15] Joonhyung Park, Jae gyun Song, and Eunho Yang, \u201cGraphens: Neighbor-aware ego network synthesis for class-imbalanced node classification,\u201d in Proceedings of ICLR, 2022.\n[16] Yijun Duan, Xin Liu, Adam Jatowt, et al., \u201cAnonymity can help minority: A novel synthetic data over-sampling strategy on multi-label graphs,\u201d in ECML-PKDD.\n[17] N. Chawla, K. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer, \u201cSmote: Synthetic minority over-sampling technique,\u201d ArXiv, vol. abs/1106.1813, 2002.\n[18] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dolla\u0301r, \u201cFocal loss for dense object detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, pp. 318\u2013327, 2017.\n[19] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie, \u201cClass-balanced loss based on effective number of samples,\u201d in Proceedings of CVPR, 2019, pp. 9260\u20139269.\n[20] Min Shi, Yufei Tang, Xingquan Zhu, et al., \u201cMulti-class imbalanced graph convolutional network learning,\u201d in Proceedings of IJCAI, 2020.\n[21] Mahsa Ghorbani, Anees Kazi, Mahdieh Soleymani Baghshah, Hamid R. Rabiee, and Nassir Navab, \u201cRagcn: Graph convolutional network for disease prediction problems with imbalanced data,\u201d Medical image analysis, vol. 75, pp. 102272, 2021.\n[22] Thomas Kipf and Max Welling, \u201cSemi-supervised classification with graph convolutional networks,\u201d ArXiv, vol. abs/1609.02907, 2016.\n[23] William L. Hamilton, Zhitao Ying, and Jure Leskovec, \u201cInductive representation learning on large graphs,\u201d in Proceedings of NIPS, 2017, vol. 30.\n[24] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio\u2019, and Yoshua Bengio, \u201cGraph attention networks,\u201d ArXiv, vol. abs/1710.10903, 2017."
        }
    ],
    "title": "OVER-SAMPLING STRATEGY IN FEATURE SPACE FOR GRAPHS BASED CLASS-IMBALANCED BOT DETECTION",
    "year": 2023
}