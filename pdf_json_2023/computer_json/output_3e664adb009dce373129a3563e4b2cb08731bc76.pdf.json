{
    "abstractText": "Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present POLYLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model\u2019s performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation. Extensive experiments show that POLYLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English. Our models, alone with the instruction data and multilingual benchmark, are available at: https://modelscope.cn/models/damo/nlp_ polylm_13b_text_generation.",
    "authors": [
        {
            "affiliations": [],
            "name": "GUAGE MODEL"
        },
        {
            "affiliations": [],
            "name": "Xiangpeng Wei"
        },
        {
            "affiliations": [],
            "name": "Haoran Wei"
        },
        {
            "affiliations": [],
            "name": "Huan Lin"
        },
        {
            "affiliations": [],
            "name": "Tianhao Li"
        },
        {
            "affiliations": [],
            "name": "Pei Zhang"
        },
        {
            "affiliations": [],
            "name": "Xingzhang Ren"
        },
        {
            "affiliations": [],
            "name": "Mei Li"
        },
        {
            "affiliations": [],
            "name": "Yu Wan"
        },
        {
            "affiliations": [],
            "name": "Zhiwei Cao"
        },
        {
            "affiliations": [],
            "name": "Binbin Xie"
        },
        {
            "affiliations": [],
            "name": "Tianxiang Hu"
        },
        {
            "affiliations": [],
            "name": "Shangjie Li"
        },
        {
            "affiliations": [],
            "name": "Binyuan Hui"
        },
        {
            "affiliations": [],
            "name": "Bowen Yu"
        },
        {
            "affiliations": [],
            "name": "Dayiheng Liu"
        },
        {
            "affiliations": [],
            "name": "Baosong Yang"
        },
        {
            "affiliations": [],
            "name": "Fei Huang"
        },
        {
            "affiliations": [],
            "name": "Jun Xie"
        }
    ],
    "id": "SP:61ec10d7f9c19f13d14c6721ace6bbc28dea660a",
    "references": [
        {
            "authors": [
                "Yuvanesh Anand",
                "Zach Nussbaum",
                "Brandon Duderstadt",
                "Benjamin Schmidt",
                "Andriy Mulyar"
            ],
            "title": "Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Anil",
                "Andrew M Dai",
                "Orhan Firat",
                "Melvin Johnson",
                "Dmitry Lepikhin",
                "Alexandre Passos",
                "Siamak Shakeri",
                "Emanuel Taropa",
                "Paige Bailey",
                "Zhifeng Chen"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403,",
            "year": 2023
        },
        {
            "authors": [
                "Lo\u0131\u0308c Barrault",
                "Magdalena Biesialska",
                "Ond\u0159ej Bojar",
                "Marta R. Costa-juss\u00e0",
                "Christian Federmann",
                "Yvette Graham",
                "Roman Grundkiewicz",
                "Barry Haddow",
                "Matthias Huck",
                "Eric Joanis",
                "Tom Kocmi",
                "Philipp Koehn",
                "Chi-kiu Lo",
                "Nikola Ljube\u0161i\u0107",
                "Christof Monz",
                "Makoto Morishita",
                "Masaaki Nagata",
                "Toshiaki Nakazawa",
                "Santanu Pal",
                "Matt Post",
                "Marcos Zampieri"
            ],
            "title": "Findings of the 2020 conference on machine translation (WMT20)",
            "venue": "Proceedings of the Fifth Conference on Machine",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "R\u00e9jean Ducharme",
                "Pascal Vincent"
            ],
            "title": "A neural probabilistic language model",
            "venue": "In Advances in neural information processing systems,",
            "year": 2000
        },
        {
            "authors": [
                "Yoshua Bengio",
                "J\u00e9r\u00f4me Louradour",
                "Ronan Collobert",
                "Jason Weston"
            ],
            "title": "Curriculum learning",
            "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009,",
            "year": 2009
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training and scaling",
            "venue": "arXiv preprint arXiv:2304.01373,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shouyuan Chen",
                "Sherman Wong",
                "Liangjian Chen",
                "Yuandong Tian"
            ],
            "title": "Extending context window of large language models via positional interpolation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yiran Chen",
                "Zhenqiao Song",
                "Xianze Wu",
                "Danqing Wang",
                "Jingjing Xu",
                "Jiaze Chen",
                "Hao Zhou",
                "Lei Li"
            ],
            "title": "Mtg: A benchmarking suite for multilingual text generation",
            "venue": "In NAACL-HLT,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Eunsol Choi",
                "Michael Collins",
                "Dan Garrette",
                "Tom Kwiatkowski",
                "Vitaly Nikolaev",
                "Jennimaria Palomaki"
            ],
            "title": "Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "year": 2019
        },
        {
            "authors": [
                "Yiming Cui",
                "Ziqing Yang",
                "Xin Yao"
            ],
            "title": "Efficient and effective text encoding for chinese llama and alpaca",
            "venue": "arXiv preprint arXiv:2304.08177,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Leo Gao",
                "Jonathan Tow",
                "Stella Biderman",
                "Sid Black",
                "Anthony DiPofi",
                "Charles Foster",
                "Laurence Golding",
                "Jeffrey Hsu",
                "Kyle McDonell",
                "Niklas Muennighoff",
                "Jason Phang",
                "Laria Reynolds",
                "Eric Tang",
                "Anish Thite",
                "Ben Wang",
                "Kevin Wang",
                "Andy Zou"
            ],
            "title": "A framework for few-shot language model evaluation, Sep 2021",
            "venue": "URL https://doi.org/10.5281/zenodo.5371628",
            "year": 2021
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Andrew S. Gordon",
                "Zornitsa Kozareva",
                "Melissa Roemmele"
            ],
            "title": "Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "In International Workshop on Semantic Evaluation,",
            "year": 2011
        },
        {
            "authors": [
                "Kenneth Heafield"
            ],
            "title": "Kenlm: Faster and smaller language model queries",
            "venue": "In Proceedings of the sixth workshop on statistical machine translation,",
            "year": 2011
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv: Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Felix Gimeno",
                "Andy Brock",
                "Oriol Vinyals",
                "Andrew Zisserman",
                "Jo\u00e3o Carreira"
            ],
            "title": "Perceiver: General perception with iterative attention",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning, ICML 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Matthijs Douze",
                "H\u00e9rve J\u00e9gou",
                "Tomas Mikolov"
            ],
            "title": "Fasttext. zip: Compressing text classification models",
            "venue": "arXiv preprint arXiv:1612.03651,",
            "year": 2016
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson"
            ],
            "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "arXiv preprint arXiv:1808.06226,",
            "year": 2018
        },
        {
            "authors": [
                "M. Pawan Kumar",
                "Benjamin Packer",
                "Daphne Koller"
            ],
            "title": "Self-paced learning for latent variable models",
            "venue": "In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems",
            "year": 2010
        },
        {
            "authors": [
                "Patrick Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk"
            ],
            "title": "MLQA: Evaluating cross-lingual extractive question answering",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Xi Victoria Lin",
                "Todor Mihaylov",
                "Mikel Artetxe",
                "Tianlu Wang",
                "Shuohui Chen",
                "Adam Lopez"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "S. Longpre",
                "Yi Lu",
                "Joachim Daiber"
            ],
            "title": "Mkqa: A linguistically diverse benchmark for multilingual open domain question answering",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V Le",
                "Barret Zoph",
                "Jason Wei"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "venue": "arXiv preprint arXiv:2301.13688,",
            "year": 2023
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Martin Karafi\u00e1t",
                "Luk\u00e1s Burget",
                "Jan Cernock\u00fd",
                "Sanjeev Khudanpur"
            ],
            "title": "Recurrent neural network based language model",
            "venue": "URL http://dblp.uni-trier.de/db/conf/interspeech/interspeech2010. html#MikolovKBCK10",
            "year": 2010
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "arXiv preprint arXiv:2211.01786,",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Annual Meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only",
            "venue": "arXiv preprint arXiv:2306.01116,",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao"
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277,",
            "year": 2023
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer"
            ],
            "title": "Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Edoardo M. Ponti",
                "Goran Glava s",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u2019c",
                "Anna Korhonen"
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "arXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Matt Post"
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "In Proceedings of the Third Conference on Machine Translation: Research Papers,",
            "year": 2018
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Mike Lewis"
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "URL https://s3-us-west-2. amazonaws. com/openaiassets/researchcovers/languageunsupervised/language understanding paper",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Jack W Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "venue": "arXiv preprint arXiv:2112.11446,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "venue": "arXiv preprint arXiv:2211.05100,",
            "year": 2022
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch"
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "arXiv preprint arXiv:1508.07909,",
            "year": 2015
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Shaden Smith",
                "Mostofa Patwary",
                "Brandon Norick",
                "Patrick LeGresley",
                "Samyam Rajbhandari",
                "Jared Casper",
                "Zhun Liu",
                "Shrimai Prabhumoye",
                "George Zerveas",
                "Vijay Korthikanti"
            ],
            "title": "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model",
            "venue": "arXiv preprint arXiv:2201.11990,",
            "year": 2022
        },
        {
            "authors": [
                "Jianlin Su",
                "Yu Lu",
                "Shengfeng Pan",
                "Ahmed Murtadha",
                "Bo Wen",
                "Yunfeng Liu"
            ],
            "title": "Roformer: Enhanced transformer with rotary position embedding",
            "venue": "CoRR, abs/2104.09864,",
            "year": 2021
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/stanford_alpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Tikhonov",
                "Max Ryabinin"
            ],
            "title": "It\u2019s all in the heads: Using attention heads as a baseline for cross-lingual transfer in commonsense reasoning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems 30,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language",
            "venue": "understanding. ArXiv,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "In Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Hamish Ivison",
                "Pradeep Dasigi",
                "Jack Hessel",
                "Tushar Khot",
                "Khyathi Raghavi Chandu",
                "David Wadden",
                "Kelsey MacMillan",
                "Noah A. Smith",
                "Iz Beltagy",
                "Hannaneh Hajishirzi"
            ],
            "title": "How far can camels go? exploring the state of instruction tuning on open resources, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ruibin Xiong",
                "Yunchang Yang",
                "Di He",
                "Kai Zheng",
                "Shuxin Zheng",
                "Chen Xing",
                "Huishuai Zhang",
                "Yanyan Lan",
                "Liwei Wang",
                "Tieyan Liu"
            ],
            "title": "On layer normalization in the transformer architecture",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel"
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "year": 2010
        },
        {
            "authors": [
                "Yinfei Yang",
                "Yuan Zhang",
                "Chris Tar",
                "Jason Baldridge"
            ],
            "title": "Paws-x: A cross-lingual adversarial dataset for paraphrase identification",
            "venue": "In EMNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Seonghyeon Ye",
                "Hyeonbin Hwang",
                "Sohee Yang",
                "Hyeongu Yun",
                "Yireun Kim",
                "Minjoon Seo"
            ],
            "title": "In-context instruction learning",
            "venue": "arXiv preprint arXiv:2302.14691,",
            "year": 2023
        },
        {
            "authors": [
                "Aohan Zeng",
                "Xiao Liu",
                "Zhengxiao Du",
                "Zihan Wang",
                "Hanyu Lai",
                "Ming Ding",
                "Zhuoyi Yang",
                "Yifan Xu",
                "Wendi Zheng",
                "Xiao Xia"
            ],
            "title": "Glm-130b: An open bilingual pre-trained model",
            "venue": "arXiv preprint arXiv:2210.02414,",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He"
            ],
            "title": "PAWS: Paraphrase Adversaries from Word Scrambling",
            "venue": "In NAACL,",
            "year": 2019
        },
        {
            "authors": [
                "Taori"
            ],
            "title": "We show the used prompt when constructing MULTIALPACA dataset",
            "year": 2023
        },
        {
            "authors": [
                "Marie Curie"
            ],
            "title": "\u00e9tait une physicienne et une chimiste fran\u00e7aise et polonaise, n\u00e9e \u00e0 Varsovie en 1867 et d\u00e9c\u00e9d\u00e9e \u00e0 Saint-Cloud en 1934",
            "venue": "Elle est surtout connue pour ses de\u0301couvertes dans le domaine de la radioactivite\u0301, qui ont re\u0301volutionne\u0301 notre compre\u0301hension du fonctionnement des e\u0301le\u0301ments chimiques. Examples",
            "year": 1934
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) are trained on vast amounts of data in a self-supervised fashion, which has shown promising performance in a variety of zero-shot and few-shot tasks (Brown et al., 2020; Chowdhery et al., 2022). Fine-tuning these models on a diverse set of tasks allows them to handle unseen tasks following natural language instructions (Ouyang et al., 2022; Longpre et al., 2023; Taori et al., 2023; Anand et al., 2023). These properties have attracted significant attention from the Artificial Intelligence community and offering a potential path towards artificial general intelligence. Unfortunately, most LLMs are developed for English, such as LLaMA (Touvron et al., 2023), BLOOM (Scao et al., 2022), Chinchilla (Hoffmann et al., 2022), OPT (Zhang et al., 2022). A main reason stems from recent findings that model performance is closely related to the scale of the training dataset (Kaplan et al., 2020; Rae et al., 2021; Biderman et al., 2023; Touvron et al., 2023), leading to predominant focus on resource-rich languages, particularly English.\nThe relatively high concentration of studies on English limits the research and usage of LLMs in other languages. For instance, Thai and Indonesian have over 300 million (M) speakers, yet the size of these two languages in common crawl-based dataset such as mC4 (Xue et al., 2020) is only 80 billion (B) tokens, comprising a mere 3% of the English data. Due to the insufficient high-quality internet data, LLM capabilities on low-resource languages fail to be easily improved through expanding their data size like English (Kaplan et al., 2020; Rae et al., 2021; Biderman et al., 2023). As a result, existing open-source LLMs such as XGLM (Lin et al., 2022), BLOOM (Scao\n\u2217Major contributors. \u2020Contribution during internship at Alibaba DAMO Academy. \u2021Corresponding authors: {liudayiheng.ldyh, yangbaosong.ybs}@alibaba-inc.com\n1\nar X\niv :2\n30 7.\n06 01\n8v 1\n[ cs\n.C L\n] 1\n2 Ju\nl 2 02\n3\net al., 2022), and LLaMA (Touvron et al., 2023) perform relatively poor on these languages, some of which are entirely overlooked. It is crucial to explore multilingual LLMs to bridge this gap and achieve academic and social significance.\nOur goal is to enhance the exploration and utilization of LLMs for non-native English speakers. In this work, we fill three significant gaps in this field: 1) the absence of an open-source multilingual LLM; 2) the inadequate availability of multilingual instruction data; and 3) the lack of a unified evaluation benchmark for multilingual settings.\nConcretely, we first develop an open-source multilingual LLM from scratch, called Polyglot Large Language Model (POLYLM, Section 3). Contrary to existing open-source multilingual LLMs that lack 13B model, we release POLYLM-13B and POLYLM-1.7B to facilitate its usage. To construct POLYLM, we leverage a massive dataset of 640B tokens, culled from publicly available sources such as Wikipedia, mC4 (Xue et al., 2020), CC-100 (Conneau et al., 2019). This dataset contains over 30% of non-English languages, specifically covering 18 of the most commonly spoken languages.1 To alleviate the problem of insufficient data for low-resource languages, we propose a curriculum learning strategy. The training schedule increases the amount of data available for training in English during the initial phases, then ramping up the ratio of high-quality, low-resource languages as training progresses. We expect the method to enable the transfer of general knowledge from English to other languages, leading to significant improvements in overall performance.\nIn light of the supervised fine-tuning (SFT) stage, we construct a multilingual instruction dataset termed MULTIALPACA with 132,701 samples (Section 4). At present, there is a dearth of highquality open-source multilingual SFT datasets. On the one hand, extant multilingual SFT datasets, e.g. xP3-MT (Muennighoff et al., 2022), are acquired via machine translation, which potentially yields a style of translationese, a lack of cultural nuances, as well as translation errors. On the other hands, manually annotating instructions is a laborious and costly process that does not lend itself well to the incorporation of creative flourishes. Drawing inspiration from recent advances in self-instruct (Wang et al., 2022; Taori et al., 2023), we devise a multilingual self-instruct method to automatically generate instruction data. Utilizing 175 English seeds as a starting point, our method leverage multilingual seed translation, instruction generation, and filtering mechanisms to deliver high quality multilingual instruction data.\nIn order to assess the multilingual capabilities of LLM, we curate a benchmark derived from existing multilingual tasks (Section 5.1), including QA (Clark et al., 2020), understanding (Conneau et al., 2019; Yang et al., 2019; Tikhonov & Ryabinin, 2021; Ponti et al., 2020), generation (Chen et al., 2021), and cross-lingual machine translation (Barrault et al., 2020). The benchmark is constructed with meticulously prompting and finally covers 10 tasks across 15 languages. Extensive experiments (Section 6) demonstrate that our pretrained model outperforms open-source models of comparable model size (e.g. BLOOM, LLaMA, etc.) in non-English languages. Through in-depth analyses, we identify finding that the proposed curriculum training strategy boosts the multilingual performance while maintain the English proficiency. In addition, the use of multilingual instruction data markedly enhances the ability of POLYLM to tackle multilingual zero-shot tasks."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "In this section, we begin with a review of the background on language modeling. We then examine previous research on knowledge transferring, and instruction learning of pre-trained LLMs, with a focus on their relevance to POLYLM. Finally, we outline our rationale for training POLYLM.\nLanguage Modeling refers to the process of estimating the probability of a sequence of tokens, i.e. p(x) = p(x1, x2, ..., xT ) = \u220fT t=1 p(xt|x<t). This is also commonly referred to as autoregressive sequence modeling, as it involves predicting the future token at each time-step based on the preceding context. The initial language models were predominantly n-gram models that evaluate the likelihood of a sequence of tokens based on the frequency of its occurrence in a training corpus. Over the last two decades, neural networks have proven to be effective in the task of language modeling, including feed-forward models (Mikolov et al., 2010) and recurrent neural networks (Bengio et al.,\n1According to https://www.ethnologue.com/insights/most-spoken-language/. Some languages with interchangeable and more widely used official languages are not given priority, such as Hindi, Wu Chinese, and Cantonese.\n2\n2000). More recently, Transformer (Vaswani et al., 2017), a self-attention based neural network, has shown unparalleled language model performance (Devlin et al., 2019; Radford et al., 2018), and become the de facto backbone of LLMs emerged in the past three years, such as GPT3 (Brown et al., 2020), Gopher (Rae et al., 2021), PaLM (Anil et al., 2023), BLOOM (Scao et al., 2022), Chinchilla (Hoffmann et al., 2022), GLM (Zeng et al., 2022) and LLaMA (Touvron et al., 2023).\nTransfer Learning is a rapidly evolving field of research that has garnered significant interest in recent years. In this scenario, models are initially trained on extensive unlabeled data, and then their acquired knowledge is applied to various downstream tasks through fine-tuning. Some of the most prominent works in this area include the ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) have demonstrated remarkable success. These developments subsequently prompt work (Raffel et al., 2020; Radford et al., 2019; Xue et al., 2020) on better results by adopting larger scale data and parameters to further improve model performance. Although pretraing-then-finetuning is still effective in achieving high performance with limited labeled data, recent advancements has shown that language models with extremely large scale parameters can perform tasks without further optimization. The most exemplary model is GPT3 (Brown et al., 2020), which utilizes a contextualized approach by incorporating multiple input-output demonstrations and presenting them alongside the query. This effectively stimulates the model to generate accurate predictions, showcasing encouraging outcomes in zero/few-shot situations.\nInstruction Learning aims to bring together various natural language processing tasks by framing them as question-answering exercises that operate over a given context. This approach enhances the value of LLMs by leveraging their existing knowledge. With the success of language models, there has been a growing interest in exploring their potential to comprehend and execute instructions. Several advanced researches (Ouyang et al., 2022; Wei et al., 2022; Peng et al., 2023; Ye et al., 2023; Zhou et al., 2023) have demonstrated a remarkable ability to generalize to new zero-shot tasks. However, they rely heavily on human-generated instruction data, which is frequently constrained in terms of quantity, diversity, and creativity, which is very time-consuming and labor-intensive. Wang et al. (2022) make an effort to construct a self-Instruct framework for improving the instructionfollowing capabilities of LLMs. Similarly, Xu et al. (2023) propose an evol-instruct framework to automatically rewrite simple human-written instructions step by step into more complex ones, to further improve instruction-followed LLMs.\nIn this paper, we propose POLYLM to address the following blanks and limitations in current LLM research, offering a comprehensive and innovative solution to advance this field.\n\u2022 We provide a 13B scale model that is proficient in the major non-English languages spoken worldwide, such as Spanish, Russian, Arabic, Japanese, Korean, Thai, Indonesian, and Chinese etc. It is a perfect complement to the existing open-source models, including: (1) LLaMA, English is predominant among the whole dataset. (2) BLOOM, lack of 13B version and fail to address languages spoken by significant populations, such as Japanese, Korean and Thai. (3) XGLM (Lin et al., 2022), the maximum version is 7B. (4) mGPT (Shliazhko et al., 2022), only 1.3B version is available.\n\u2022 We suggest an advanced curriculum learning approach that facilitates the transfer of commonsense knowledge, acquired mainly in English, to diverse non-English languages and specific NLP downstream tasks such as machine translation.\n\u2022 We propose MULTIALPACA to complement ALPACA (Taori et al., 2023) and CHINESEALPACA (Cui et al., 2023), making LLMs better follow multilingual instructions, particularly those coming from non-native English speakers."
        },
        {
            "heading": "3 POLYLM: A POLYGLOT LARGE LANGUAGE MODEL",
            "text": "In this section, we present the design of POLYLM, which includes a detailed description of its training dataset (Section 3.1), architecture (Section 3.2), and training process (Section 3.3)."
        },
        {
            "heading": "3.1 DATASET",
            "text": "The composition of the pre-training dataset used for POLYLM is shown in Table 1. Our pre-training dataset contains 640B tokens in total, of which English data accounts for 68%. To develop POLYLM\n3"
        },
        {
            "heading": "Language Tokens (B) Percentage (%) Language Tokens (B) Percentage (%)",
            "text": "with multilingual capabilities, the pre-training dataset has about 32% non-English multilingual data, which is a higher percentage of non-English data than most previous open-sourced large language models (Biderman et al., 2023; Zhang et al., 2022; Touvron et al., 2023; Penedo et al., 2023). To be concrete, the English data contains documents with 425B tokens from multiple sources, such as The Pile (Gao et al., 2020), mC4 (Xue et al., 2020), and Wikipedia. While the 204B multilingual data tokens come from CC-100 (Conneau et al., 2019), mC4 (Xue et al., 2020), Wikipedia. The multilingual data mainly covers the following languages: zh, ar, es, fr, de, it, nl, ru, id, pl, pt, ja, th, tr, he, ko, vi, with the distribution given in Table 2. To enable the model ability of code understanding and generation, we also incorporate code data of 7.5B tokens from GitHub with permissioned licenses into our pre-training dataset. In order to further improve the cross-lingual and multilingual ability of the POLYLM, similar to PaLM2 (Anil et al., 2023), we employ parallel multilingual data of 1B tokens into our pre-training dataset.\nTo build the pre-training dataset, we also develop a comprehensive data pre-processing pipeline that implements multiple techniques for data cleaning and filtering. The pipeline consists of the following stages:\n1) Language identification. We classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g., fastText (Joulin et al., 2016)).\n2) Rule-based filtering. Following Rae et al. (2021); Scao et al. (2022), we eliminate irrelevant or low-quality content using various rules and heuristics, including repetition removal (the document with the excessive line, paragraph, or n-gram repetitions is removed), document-wise filtering (removing outlier documents by overall length, symbol-to-word ratio, the ratio of ellipsis, invisible characters, numbers, and dates, etc.), and line-wise corrections (such as URL filtering, long words removal, and whitespace standardization).\n3) ML-based quality filtering. We further filter low-quality multilingual documents using several small n-gram-based language models (e.g., KenLM (Heafield, 2011)) for different languages trained on their gold-standard corpora. In addition, similar to Raffel et al. (2020); Smith et al. (2022), we also train a 2-gram fastText (Joulin et al., 2016) classifier to filter the low-quality English documents. This classifier uses Wikipedia, and Books from The Pile (Gao et al., 2020) as the positive samples\n4"
        },
        {
            "heading": "Architecture hyperparameters",
            "text": ""
        },
        {
            "heading": "Pretraining hyperparameters",
            "text": ""
        },
        {
            "heading": "Multilingul Self-instruction finetuning hyperparameters",
            "text": "and CommonCrawl web documents as the negative samples. To sum up, about 28.3% data are filtered with Rule-based filtering and ML-based quality filtering.\n4) Deduplication. In line with Raffel et al. (2020), we remove similar documents to reduce data redundancy with MinHashLSH-based fuzzy deduplication technology, where 23.1% English documents and 18.6% non-English documents are removed.\nBased on the POLYLM multilingual pre-training dataset, we derived a vocabulary with 256K token entries using Byte-Pair Encoding (BPE) (Sennrich et al., 2015) with the implementation from SentencePiece (Kudo & Richardson, 2018). To enhance the mathematical capabilities of our model, we follow Touvron et al. (2023) to split all numbers into individual digits. The unknown characters are fallback to byte encoding of UTF-8 to guarantee the coverage of rare words (e.g., emoji, and special symbols). For tokenizer training, we sample multilingual documents with a similar distribution as Conneau et al. (2019) used to increase the number of vocabulary tokens associated with low-resource languages and alleviate the bias towards high-resource languages. We compare the compression rate on different language corpora of different tokenizers. We use XLM-R (Conneau et al., 2019) tokenizer, which supports 100 languages, as the baseline (the compression rate of XLM-R tokenizer is set to 1). As shown in Figure 1, POLYLM has achieved significantly better compression rates in most covered languages, while maintaining the compression rate in English as\n5\nBLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023), GPT-2 (Radford et al., 2019), and GPT4 (OpenAI, 2023). Note that some open source models that are not friendly to language extensions, for example, LLaMA (Touvron et al., 2023) only contain a 32K size vocabulary mostly composed of English tokens, which is not friendly to non-Latin languages. When improving a certain non-Latin language ability, the vocabulary needs to be expanded like Chinese-LLaMA (Cui et al., 2023). On the contrary, POLYLM allows researchers to improve the model\u2019s ability in a covered language by simply continuing monolingual pre-training without expanding the vocabulary."
        },
        {
            "heading": "3.2 ARCHITECTURE",
            "text": "It has become apparent that the computational cost of exploring different architectural designs for LLMs is prohibitive. Therefore, we present the distinctive design options of POLYLM2 in this section.\nFollowing some endeavours on large language models, we develop a decoder-only autoregressive Transformer architecture detailed in Radford et al. (2019). To stabilize the training, we adopt PreLN (Xiong et al., 2020), i.e. y = x + LayerNorm(f(x)) (where f(\u00b7) indicates the layer function) for layer normalization, and apply the Xavier normal initialization (Glorot & Bengio, 2010) with bias terms are initialized to zero. To improve FFNs in Transformer, we replace ReLU with GeLU activation (Hendrycks & Gimpel, 2016).\nIn this paper we present two Transformer language models with 1.7 billion and 13 billion parameters, respectively. The architectural details are displayed in Table 3."
        },
        {
            "heading": "3.3 TRAINING",
            "text": "We train all models with a 2048 token context window, using the Adam (\u03b21 = 0.9, \u03b22 = 0.95) optimizer. We warm-up the learning rate from 1e\u22127 to the maximum learning rate over the first 2000 steps, and then decay it to 10% of the maximal learning rate using a cosine schedule. We use a weight decay of 0.1 and gradient clipping of 1.0.\n2Recent research indicates that Rotary Position Encoding (RoPE) (Su et al., 2021) yields superior performance. Accordingly, we will switch to the latest Megatron-LM branch and promptly release 13B and 1.7B versions featuring RoPE.\n6\nPOLYLM was trained using Megatron-LM 3 on a cluster of 32 A100 GPU (8\u00d780G) servers. We apply tensor model parallelism within a single node, setting tensor-model-parallel-size as 8. When training a 13B-parameter model, our code processes around 1170 tokens/sec/GPU, thus training over our dataset containing 640B tokens takes approximately 29 days. However, we faced numerous unforeseen spikes and deviations in losses, which prolonged the entire training process to a duration of two months. There are several possible conditions that result in training collapses, and our unique choices to enhance training stability.\nLower Maximal Learning Rate. Learning rate is an important hyperparameter in neural network models that controls the magnitude of parameter updates. In our first few attempts, we drew inspiration from previous research which indicated that smaller models tend to benefit from higher learning rates. As such, we opted to set the learning rate to 1\u00d7 10\u22124. Without exception, all attempts to train POLYLM-13B have resulted in loss spikes with this choice in early stage, which tend to occur more frequently as the training progresses, as illustrated in Figure 2a. We have noticed that the gradient norm shows significant fluctuations during the warm-up phase, when the learning rate is increasing linearly (see Figure 2b).\nThe fundamental issue with instability during training is that a large learning rate can cause the gradient to grow too large, surpassing the model\u2019s capacity and resulting in a gradient explosion that prevents parameter updates. The problem is handled via reducing learning rate to 6 \u00d7 10\u22125, i.e. a proper learning rate located before the step where the initial spike in loss occurs (Cf. Figure 2c).\nMixed-Precision. Despite the potential instabilities associated with training models using half precision (float16) activations and model parameters that arise from the limited numerical range, it has been proposed that the numbers represented by bfloat16 allow for training of models and can avoid performance degradation compared to full float32 training. Thus, we incorporate the bfloat16 numerical format to reduce memory and increase training efficiency. However, similar to OPT-175B (Zhang et al., 2022), BLOOM-176B (Scao et al., 2022) and GLM-130B (Zeng\n3https://github.com/NVIDIA/Megatron-LM\n7\net al., 2022), the training of POLYLM-13B still faces frequent loss spikes while lowering learning rate. We attempted to address such challenge via manually skipping data and restart the straining, it unfortunately tends to become increasingly severe as the training does on (Cf. Figure 3a).\nAfter conducting two weeks of investigation, we have come to the realization that the instabilities we are encountering may not be due to the training data under the mutlilingual scenario (with the vocabulary up to 256,000), but rather due to the model itself. Specifically, we suspect that there could be a risk of overflow in the attention or residual connectivity layers. Taking this into account, we have configured the residual connection and attention layers to have a numerical precision of float32 to ensure optimal performance, resulting in a highly stable training process (Cf. Figure 3b).\nCurriculum Learning. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is a significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. To address this issue, we adopt a curriculum learning strategy (Bengio et al., 2009; Kumar et al., 2010; Jaegle et al., 2021) that ramps up the ratio of high-quality and low-resource languages during training. Specifically, the training process is divided into two stages. In the first stage, we use the whole pre-training dataset to train a base model yields commonsense generalization ability. In the second stage, we transition to a subset of the pre-training dataset that boasts superior quality and a greater proportion of multilingual content, to further strengthen the model\u2019s multilingual capabilities. Figure 4 compares the language distribution of training data in two stages, indicating that the proportion of most low-resource languages has been increased in the sub-dataset.\nTo build the sub-dataset for curriculum learning, we first manually evaluate the quality of publicly available data source in the pre-training dataset, and sample about 97B tokens from the high-quality sources while increasing the proportion of languages other than Chinese and English. We also enhance the proportion of parallel data (OPUS) to facilitate the modeling of cross-lingual representation. The detail of the sub-dataset are illustrated in Figure 5. According to our established setup, the curriculum training process is highly stable (Cf. Figure 3c)."
        },
        {
            "heading": "4 MULTIALPACA: A MULTILINGUAL SELF-INSTRUCTION DATASET",
            "text": "Fine-tuning LLMs with instruction-based tasks has been proven effective in practice (Ouyang et al., 2022; Wei et al., 2022; Peng et al., 2023; Ye et al., 2023). By providing accurate task instructions during the SFT phase, LLMs can not only learn to understand the requirements of each task via the instruction part, but also show extensive abilities to cope with other types of tasks which are even unseen during training (Wei et al., 2022). Nevertheless, tuning multilingual LLMs is still troubled by the scarcity of current SFT datasets. On the one hand, most instruction-based datasets are mainly in resource-rich languages (e.g., English or Chinese). To the best of our knowledge, there is currently no high-quality multilingual instruction-based SFT dataset for LLM training. On the other hand, most instructions are manufactured by experienced language speakers (e.g., Wei et al., 2022). Although the quality of instructions is well preserved, the amount of tasks is rather scarce for fine-tuning LLMs.\nTo overcome these two drawbacks, we determine to extend the generality of our proposed POLYLM via creating a multilingual SFT dataset \u2013 MULTIALPACA (Figure 6). Following the self-instruct paradigm proposed by recent studies (Wang et al., 2022; Taori et al., 2023), we query the available LLM for responses, iteratively collecting and filtering self-instruct examples to build our dataset. MULTIALPACA delivers comprehensive support on multilingualism, covering up to 11 languages including Arabic (Ar), German (De), Spanish (Es), French (Fr), Indonesian (Id), Japanese (Ja), Korean (Ko), Portuguese (Pt), Russian (Ru), Thai (Th), and Vietnamese (Vi). For each language, the number of tasks in MULTIALPACA varies from 9,515 to 14,671, yielding 132,701 tasks in total."
        },
        {
            "heading": "4.1 TASK FORMAT",
            "text": "We first form the format of our tasks by referring to Taori et al. (2023), where each task contains three parts: 1) \u201cinstruction\u201d describes the requirements of the corresponding task; 2) \u201cinput\u201d can complement the \u201cinstruction\u201d to a complete question; and 3) \u201coutput\u201d is a correct answer of the question. We notice that, Taori et al. (2023) constructed their dataset where each\n8\n\u201cinstruction\u201d can be equipped with multiple \u201cinput-output\u201d instances. For simplicity, we only assign each \u201cinstruction\u201d with one \u201cinput-output\u201d instance."
        },
        {
            "heading": "4.2 MULTIALPACA CONSTRUCTION",
            "text": "As shown in Figure 7, we construct the MULTIALPACA dataset based on the following steps:4\nCollecting Multilingual Seed Tasks We first obtain 175 seed tasks from Taori et al. (2023) to construct the multilingual ones for MULTIALPACA. After manually checking them, we remove the cases where answering the questions requires cultural backgrounds (e.g., idiom explanation, character-level riddle, and lyrics generation). Then, we marked the cases whose original \u201cinput\u201d or \u201coutput\u201d should be reserved (e.g., single-choice question, translation, bias identification, and code generation), where those tasks will directly use the original \u201cinput\u201d or \u201coutput\u201d across different languages for MULTIALPACA. Finally, we filter out 13 inappropriate seed tasks, and modified 23 ones marked due to the reuse of \u201cinput\u201d or \u201coutput\u201d parts. We translate the remaining 162 tasks into the other 11 languages, yielding multilingual seed tasks for each language.\nIterative Progress We manage the MULTIALPACA dataset construction progress as an iterative one with multiple rounds. For each round, we manage the following five substeps in order:\n4See Appendix A for more details.\n9\n\u2022 Prompt Construction We follow Taori et al. (2023) to construct the prompts for MULTIALPACA when querying LLM for completion. When handling each involved language, for each prompt, we sample two seed tasks and one MULTIALPACA task as the demonstrations, and guide the LLM to complete the other 17 tasks in the response. For each round, we construct 100 prompts for querying the completion by LLM.5\n\u2022 Response Collection We collect the responses from CHATGPT via the OpenAI API service. The model we use is \u201cgpt-3.5-turbo-0301\u201d, which supports the processing of tokens up to 4,096.\n\u2022 Format Checking When checking the format, we first remove the last task if the response is stopped due to the exceeding of max sequence length. Then, we use the pre-defined task format to help split the response string, so as to make sure each of the tasks contains \u201cinstruction\u201d, \u201cinput\u201d, and \u201coutput\u201d parts.\n\u2022 Similarity Checking After that, to preserve the diversity of MULTIALPACA dataset, we further check the similarity between the tasks that are newly collected and those from the task pool. Following Taori et al. (2023), we compute the Rouge-L F-scores between the instruction of each newly collected task and those of all collected ones. For each newly collected task, it would be added to the task pool only if all the scores are lower than 0.7.\n\u2022 Task Pool Updating In the end, we update the task pool by adding the newly collected tasks, and arrange the next round for collecting MULTIALPACA self-instruct tasks.\nMULTIALPACA Dataset Export Totally, we arrange 10 rounds in the iterative progress when constructing the MULTIALPACA dataset. We export all tasks from the task pool as the MULTIALPACA dataset for SFT learning."
        },
        {
            "heading": "5 MULTILINGUAL BENCHMARK",
            "text": "We aim to assess the capabilities of POLYLM from various perspectives: 1) the ability of large language models (LLMs) to understand and generate natural languages, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks. Following the experiment design of previous work (Scao et al., 2022; Ahuja et al., 2023), we gather a subset of datasets from previous NLP tasks to construct a multilingual benchmark. The brief statistics of all datasets in the benchmark can be found in Table 4. The details of how we frame all the tasks with prompting are listed in Appendix B."
        },
        {
            "heading": "5.1 TASKS IN BENCHMARK",
            "text": "All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Natural Language Generation and Machine Translation. The details of each dataset that we use for benchmarking are given below.\nTo assess the comprehension capability of large models across various languages, we collect the multilingual versions of datasets from seberal wide-used NLP benchmarks (Wang et al., 2018; 2019).\n5Except for the first round where the task pool is empty, we arrange 10 prompts for completion due to the small number of available tasks for demonstrations.\n10\nXNLI (Conneau et al., 2019) serves as a benchmark to evaluate a model\u2019s proficiency in predicting textual entailment. The task entails the evaluation of whether two given sentences, A and B, convey the same meaning, are contradictory, or are unrelated. The dataset has been professionally translated into 14 languages from the original English XNLI dataset.\nPAWS-X (Yang et al., 2019) is a benchmark to evaluate the model\u2019s ability to judge whether one sentence is the paraphrase of another. It is professionally translated from the PAWS (Zhang et al., 2019) dataset into 6 diverse languages.\nXWinograd (Tikhonov & Ryabinin, 2021) serves as a benchmark to measure a model\u2019s common sense reasoning ability. Specifically, the task entails presenting the model with a brief contextual passage and requiring it to select the accurate term from a set of two options for a pronoun in the passage.\nXCOPA (Ponti et al., 2020) is another benchmark intended to assess the proficiency of models in commonsense reasoning across languages. The dataset comprises translations and re-annotations of the English COPA Gordon et al. (2011), spanning 11 languages around the globe. Based on the given premise and prompt, the task is to choose the more plausible response between two answer choices that can be inferred from the premise.\nTyDi QA (Clark et al., 2020) is a question-answering dataset covering 11 typologically diverse languages with 200K question-answer pairs. We use this dataset to evaluate the ability to grasp knowledge from natural text. Unlike previous datasets such as MLQA (Lewis et al., 2020) and MKQA (Longpre et al., 2020), this dataset is collected directly in each language without the use of translation. We select 5 languages out of 11 that are included in the pretraining corpora of POLYLM. Following the PaLM (Chowdhery et al., 2022), we evaluate models on the Gold passage task, which requires answering questions based on a passage that is guaranteed to contain the answer.\nMTG (Chen et al., 2021) is used to assess the efficacy of large language models in generating longer responses across diverse usage scenarios and multiple languages. MTG covers four different generation tasks: Story Ending Generation (SG), Title Generation (TG), Question Generation (QG), and Summarization (Summ). The datasets are originally written in English, subsequently extended into four other languages (German, French, Spanish, and Chinese) through the use of machine translation and human annotation. The effectiveness of LLM-generated responses is evaluated using the average of Rouge1, Rouge2, and RougeL.\nWMT20 (Barrault et al., 2020) is used to study the cross-lingual proficiency of large language models in accomplishing translation tasks, as the process of translation entails both comprehending the semantic of the input in one language and expressing it in another. We select translation tasks between English and each of the following languages as benchmark languages: German, Japanese, Russian, and Chinese. The results are evaluated using the SacreBLEU (Post, 2018) and the scores for BLEU (Papineni et al., 2002) on the test set are reported."
        },
        {
            "heading": "5.2 EVALUATION DESIGN",
            "text": "For metric evaluation, the tasks included in our benchmark can be divided into two categories: classification-style tasks and generation-style tasks.\nClassification-style tasks require selecting the correct option from several options, such as the XNLI dataset. To evaluate these tasks, following the way in Gao et al. (2021), we design the problem in the form of a cloze test, where each option is filled in to construct a complete sentence. We then choose the correct answer by separately calculating the log-likelihood of each completed sentence and selecting the one with the highest value.\nGeneration-style tasks, such as machine translation, require generating answers with several natural sentences. For these tasks, we adopt greedy decoding for deterministic results. Considering the efficiency of decoding, we restrict the maximum number of generated tokens to 256. For foundation models, we choose the result before the first \u2018\\n\u2019 as the answer, while for models that have undergone instruction tuning, we decode until the EOS token appears.\nIn evaluating foundation models, considering that models have not been able to understand instructions, we adopt in-context learning (Brown et al., 2020) to evaluate the model for generation-style tasks. We generally choose no more than five examples due to the model\u2019s context window limita-\n11\ntion. For tasks that have well-divided training/development sets, we randomly draw examples from them for each test sample. Otherwise, we draw examples randomly from the test sets except for the current sample."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "In this section, we provide separate comparison results for the pre-training and SFT models. Then, we analyze the effectiveness of our model in three aspects: curriculum learning, multilingual instruction finetuning, and the scaling for model size."
        },
        {
            "heading": "6.1 COMPARISONS BETWEEN PRE-TRAINED FOUNDATIONAL MODELS",
            "text": "For the pre-trained models, we selected two mainstream open-source models as our baselines.\n\u2022 LLaMA (Touvron et al., 2023) is a pre-trained language model released by MetaAI, which includes 7B, 13B, 30B, and 65B versions. The pre-training dataset is sourced from publicly\n12\navailable corpora. The 33B and 65B models are trained on 1.4 T tokens, while the 7B and 13B models are trained on 1 T tokens. To ensure an equal parameter count comparison with POLYLM, we mainly take the 13B version into consideration.\n\u2022 BLOOM (Scao et al., 2022) is a multilingual model that covers 46 natural languages and 13 programming languages with a maximum of 176B parameters. Since BLOOM has not released a 13B version, we opt for the BLOOM-7.1B model as our baseline.\nWe evaluate POLYLM across various multilingual tasks, covering natural language understanding (NLU), knowledge, natural language generation (NLG) and machine translation (MT). To make a clearer comparison of the multilingual capabilities of different models, we present the results using radar charts, with detailed results available in the C.\nNatural Language Understanding. Figure 8 shows the results on four NLU tasks under the zeroshot setting. POLYLM-13B shows comparable performance to the English-centric LLaMA-13B model in the English scenario. Moreover, it yields substantial improvements of 7.2% and 19.1% on PAWS-X and XNLI respectively. For languages other than English (the multilingual column), POLYLM-13B outperforms LLaMA-13B with average improvement up to 7.6%, 5.6%, 3%, and 11% on XCOPA, PAWS-X, XWinagrad, and XNLI, respectively. When compared to the multilingual language model BLOOM-7.1B, POLYLM-13B outperforms with an average improvement of\n13\n4.2%, 4.1%, 3.4%, and 4% points on the respective tasks. This improvement can be attributed to the higher percent of multilingual text during pre-training and curriculum learning strategy.\nKnowledge. We evaluate our model on grasping multilingual knowledge by using the TyDiQA benchmark in the one-shot setting. Upon careful analysis of Figure 9a, it is evident that BLOOM7.1B experiences significant performance drops in the Korean (ko) and Russian (ru) language directions, whereas LLaMA-13B and POLYLM-13B exhibit better balance across all five languages. Furthermore, POLYLM-13B has an additional advantage of an average 1.2-point lead over LLaMA13B.\nNatural Language Generation. Figure 9b displays the Rouge scores of four diverse NLG tasks in multilingual settings. From a multilingual perspective, POLYLM-13B outperforms all other models across four languages, namely Chinese (zh), Spanish (es), French (fr), and German (de). Moreover, in terms of task types, POLYLM-13B performs the best in question generation (QG) and summarization (Sum) tasks, while also showing comparable performance to the best model LLaMA-13B in the text generation (TG) task. Across all MTG tasks and languages, POLYLM-13B has an average score advantage of 1.6 and 2.3 compared to LLaMA-13B and BLOOM-7.1B, respectively.\nMachine Translation We focus on evaluating the translation performance on four typologically diverse languages from WMT20 datasets, including translation directions both from and to English. Results of Figure 9c show that POLYLM-13B achieves similar performance to LLaMA-13B in the multilingual to English directions and surpasses LLaMA-13B and BLOOM-7.1B with average BLEU scores of 5.4 and 15.8 in the English to multilingual directions.\n14"
        },
        {
            "heading": "6.2 COMPARISONS BETWEEN INSTRUCTION-FOLLOWED MODELS",
            "text": "This section focuses on evaluating the effectiveness of instruction-followed models founded on the pre-trained language models discussed in Section 6.1. We conduct a comparative analysis of POLYLM-MULTIALPACA-13B that is fine-tuned on POLYLM-13B using MULTIALPACA, against two other publicly available models:\n\u2022 BLOOMZ-MT-7B is initially pre-trained on BLOOM-7B, and later fine-tuned on the multilingual task mixture xP3-MT (Muennighoff et al., 2022).\n\u2022 LLaMA-Alpaca-13B is built based on the pre-trained model LLaMA-13B and fine-tuned on the English self-instruction dataset ALPACA (Taori et al., 2023).\nFigure 10 and 11 present the performance comparisons of instruction-followed models with the zeroshot setting, considering various tasks and language directions. The results indicate that POLYLMMULTIALPACA-13B is comparable or superior to LLaMA-Alpaca-13B on all English tasks, although the latter is primarily trained on English-only instructions. On other non-English tasks, POLYLM-MULTIALPACA-13B significantly outperforms LLaMA-Alpaca-13B. This superiority can be attributed to the inclusion of more well-balanced multilingual datasets during the pre-training and instruction fine-tuning. In comparison to BLOOMZ-MT-7B, POLYLM-MULTIALPACA-13B has demonstrated consistent improvements across all tasks and languages. We have observed an outlier MTG, and we speculate that this may be due to the fact that MTG testsets are part of the xP3\n15\ndataset. We plan to refine our instruction tuning process for POLYLM by utilizing the xP3 dataset in order to delve deeper into this inconsistency.\nNote that it is not feasible to fully assess the effectiveness of the model\u2019s performance through downstream NLP tasks after instruction fine-tuning. Therefore, we have presented selected examples for qualitative analysis, which are fully outlined in Appendix D."
        },
        {
            "heading": "6.3 ANALYSIS",
            "text": "Curriculum Learning. We validate the effectiveness of the curriculum learning strategy in NLU and MT tasks of multilingual benchmark (Section 5.1) by comparing the following variants:\n(1) w/o CL POLYLM-13B trained without curriculum learning, which is only optimized in pretrained dataset.\n(2) w/ CL POLYLM-13B trained with curriculum learning, using about 100B high-quality multilingual data selected from the pretrained dataset.\nPlease note that we only focus on the languages included during curriculum learning. Referring to Figure 12, the model with curriculum learning has achieved stable progress in mainly all languages in both NLU and MT tasks. First of all, the model performance is enhanced in most low-resource languages, indicating that the general knowledge can be effectively transferred to these languages through raising data proportion. Additionally, the model retains its superior performance in English,\n16\nwhich illustrates that improving data quality for high-resource languages can achieve competitive results to training with larger amounts of data. Finally, it is worth noting that introducing more multilingual parallel data during the curriculum learning significantly boost the model performance on translation task.\nMultilingual Self-instruction. Here we highlight the advantages of MULTIALPACA over Englishonly ALPACA (Taori et al., 2023), particularly in cross-lingual tasks (i.e., machine translation). As illustrated in Table 5, compared to the model fine-tuned only using ALPACA, POLYLMMULTIALPACA-13B exhibits substantial improvements in TyDiQA and multiple WMT20 translation tasks, with enhancements of +10 BLEU and +1.4% F1. These results suggest that MULTI-\n17\nALPACA is capable of simulating the cross-lingual alignment ability of the foundational, as well as facilitating the comprehension of multilingual instructions.\nScaling for Model Size. In addition to the 13B model, we also release a smaller 1.7B model. Recent studies highlight the critical role of model size in the performance of large language models (LLMs), with much of this work focusing on English (Kaplan et al., 2020; Rae et al., 2021; Biderman et al., 2023; Touvron et al., 2023). In this section, we present results for POLYLM-13B and POLYLM-1.7B to investigate the impact of model size on multilingual abilities. Consistent with the aforementioned experimental setup for the validation of base model, we compare the two models using a one-shot setting. As illustrated in Figure 13, the 13B model significantly outperforms the 1.7B model across all compared multilingual tasks. We posit that multilingual problems are more complex than their monolingual counterparts and may depend more heavily on the model\u2019s throughput. Moving forward, we plan to release additional models of varying sizes, with the ultimate goal of refining the scaling law for multilingualism."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "Multilingualism poses an inevitable challenge for LLM due to the scarcity of resources. In this work, we release POLYLM \u2013 a new multilingual LLM, alone with MULTIALPACA \u2013 a multilingual instruction dataset, and a multilingual benchmark. Quantitative and qualitative analyses demonstrate the superiority of POLYLM over open-source models in non-English languages. We find that incorporating curriculum learning strategy can boost the performance of LLM on non-English languages, without impeding its English proficiency. In addition, fine-tuning LLM with multilingual instruction data can considerably improve zero-shot performance on these languages.\nThere is still ample opportunity for refinement in our work endeavors. For instance, while we briefly assess the model\u2019s capacity to comprehend multilingual instructions, there is potential for further optimization through the amalgamation of data sources (Wang et al., 2023; Longpre et al., 2023), evolutionary methods (Xu et al., 2023) and diversification strategies (Zhou et al., 2023). Moreover, in our current version, we adopt absolute position encoding, which adheres to the early default configuration in Megatron toolkit (Shoeybi et al., 2020). Future iterations should incorporate techniques that facilitate the expansion of window size, such as rotary position encoding (Su et al., 2021; Chen et al., 2023) or ALiBi (Press et al., 2022).\nLanguage serves as a conduit for culture, and the unique contributions of various languages enrich and diversify our global community. Nevertheless, the advancement of LLM may inadvertently amplify the influence of prominent languages and present a formidable obstacle for low-resource languages. In light of these concerns, we aspire that our research will motivate further inquiry and innovation in the field of multilingual LLM."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "In this paper, we propose POLYLM, an LLM which offers a wider support on non-English languages. Our contributions are fully methodological: adding the support of multilingualism to LLM during training and SFT phases. However, when building our POLYLM model, it is unavoidable that our POLYLM might exhibit several common deficiencies of language models, e.g., hallucination and toxicity. Specifically, as the collected MULTIALPACA dataset are generated by CHATGPT, the pseudo tasks might give inappropriate pseudo tasks which are hardly filtered out, e.g., hallucinated reasoning and anti-fact statements (Brown et al., 2020; OpenAI, 2023). Besides, POLYLM may deliver toxic texts, which might be gender- or race-biased like other existing LLMs (Taori et al., 2023; Cui et al., 2023).\nDespite the ethical concerns above, we think that those problems are of vital importance to the AI community to study the deficiencies of LLMs. We recommend that the users of POLYLM and MULTIALPACA deploy our released materials only for research proposals. Besides, we suggest the users better identify the deficiencies of those contents, and welcome the following researchers to facilitate further research on the alignment between the LLM outputs and human values with POLYLM and MULTIALPACA materials.\n18"
        },
        {
            "heading": "A DETAILED SETTING FOR MULTIALPACADATASET CONSTRUCTION",
            "text": ""
        },
        {
            "heading": "A.1 PROMPT FOR MULTIALPACA DATASET CONSTRUCTION",
            "text": "We show the used prompt when constructing MULTIALPACA dataset in Table 6. We mainly refer to Taori et al. (2023), and adopt our prompt to multilingual scenarios after minor revisions. Briefly, in the prompt, we list several requirements of the self-instruct tasks in the prompt, i.e., the used language, the format, the diversity, and the lengths of tasks within each single response. We also add three demonstrations to help the model generate the tasks which follow the pre-defined format.\n23\n24"
        },
        {
            "heading": "Language # tasks Ratio (%)",
            "text": ""
        },
        {
            "heading": "A.2 FORMAT AND SIMILARITY CHECKING",
            "text": "After collecting the pseudo tasks for each language, we first remove the cases which contain website links. Then, we tokenize the \u201cinstruction\u201d, \u201cinput\u201d, and \u201coutput\u201d with available tokenization toolkits.6\nBesides, we found that some of the tasks may give redundant information within both \u201cinstruction\u201d and \u201cinput\u201d part. We revise those examples to make them available as much as possible. In detail, if the \u201cinput\u201d part can be a sub-string of the \u201cinstruction\u201d, we mark the \u201cinput\u201d part as an empty one (using the placeholder \u201c<noinput>\u201d). Otherwise, we compute the Rouge-L F-score between the \u201cinstruction\u201d and the \u201cinput\u201d part, filtering out the tasks whose result is over 0.5. Specially, for the Ko, Vi, and Ar tasks, we determine the threshold of Rouge-L F-score as 0.3, 0.3, and 0.2 for higher diversity of tasks, respectively.\nWe show the percentage of remaining examples after similarity checking in Figure 14. For each language, we show the number of self-instruct tasks in MULTIALPACA in Table 7.\n6nltk.tokenize.wordpunct tokenize for Ar, De, Es, Fr, Id, Pt, Ru, and Vi; kytea for Ja; Okt for Ko; thai tokenizer for Th.\n25"
        },
        {
            "heading": "B DETAILS OF TASK FORMATTING",
            "text": "We list the detailed format of all the tasks in our multilingual benchmark in the following tables."
        },
        {
            "heading": "C DETAILS OF EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 RESULTS OF PRETRAINED LANGUAGE MODELS",
            "text": ""
        },
        {
            "heading": "C.2 RESULTS OF SFT MODELS",
            "text": ""
        },
        {
            "heading": "D DEMONSTRATIONS FOR QUALITATIVE ANALYSIS",
            "text": "26\n27\n28\n29\n30\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u8bf7\u7ed9\u51fa\u2f00\u4e2a\u8ba1\u7b97\u6807\u51c6\u5dee\u7684\u516c\u5f0f\u3002\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n\u6807\u51c6\u5dee\u662f\u8861\u91cf\u6570\u636e\u79bb\u6563\u7a0b\u5ea6\u7684\u2f00\u4e2a\u7edf\u8ba1\u91cf\uff0c\u5176\u8ba1\u7b97\u516c\u5f0f\u4e3astd = sqrt(mean(abs(x - x.mean())**2))\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n31\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u547d\u540d\u2f00\u79cd\u2edd\u7269\u3002\n\u8fd9\u79cd\u2edd\u7269\u7531\u4ee5\u4e0b\u914d\u6599\u5236\u6210\uff1a\n- \u9e21\u86cb - \u2faf\u7c89 - \u5976\u6cb9 -\u2f81\u677e\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n\u8fd9\u79cd\u2edd\u7269\u88ab\u79f0\u4e3a\u2f81\u677e\u86cb\u7cd5\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n32\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u4ece\u4ee5\u4e0b\u53e5\u2f26\u4e2d\u9009\u62e9\u4e0e\u7ed9\u51fa\u53e5\u2f26\u542b\u4e49\u76f8\u540c\u7684\u53e5\u2f26\u3002\n\u8fc7\u5b8c\u8fd9\u4e2a\u5468\u672b\u540e\uff0c\u8fd9\u4e2a\u9879\u2f6c\u5e94\u8be5\u5b8c\u6210\u3002\n\u9009\u62e9\uff1a\na) \u8fd9\u4e2a\u9879\u2f6c\u5f88\u5feb\u5c31\u4f1a\u7ed3\u675f\u4e86\u3002 b) \u8fd9\u4e2a\u9879\u2f6c\u8fc7\u4e86\u8fd9\u4e2a\u5468\u672b\u5c31\u5b8c\u6210\u4e86\u3002 c) \u8fd9\u4e2a\u9879\u2f6c\u4e0d\u4f1a\u5728\u5468\u672b\u524d\u7ed3\u675f\u3002\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nb) \u8fd9\u4e2a\u9879\u2f6c\u8fc7\u4e86\u8fd9\u4e2a\u5468\u672b\u5c31\u5b8c\u6210\u4e86\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n33\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u5728\u6307\u5b9a\u7684\u8bed\u5883\u4e0b\uff0c\u4e3a\u2f00\u7ec4\u5355\u8bcd\u9009\u62e9\u76f8\u5e94\u7684\u4e2d\u2f42\u7ffb\u8bd1\u3002\n\u4e0b\u2faf\u662f\u2f00 \u7ec4\u5355\u8bcd\uff0c\u4e3a\u8fd9\u4e9b\u5355\u8bcd\u9009\u62e9\u6b63\u786e\u7684\u4e2d\u2f42\u7ffb\u8bd1\uff0c\u8bed\u5883\u4e3a\u5546\u4e1a\u9886\u57df\uff1a - deadline - board meeting - marketing strategy\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n- \u622a\u2f4c\u2f47\u671f - \u8463\u4e8b\u4f1a\u4f1a\u8bae - \u5e02\u573a\u6218\u7565\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%"
        },
        {
            "heading": "Input Text",
            "text": "tokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n34\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u5728\u7ed9\u5b9a\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u627e\u5230\u6240\u6709\u5305\u542b\u67d0\u4e2a\u5173\u952e\u8bcd\u7684\u6761\u2f6c\u3002\n\u6570\u636e\u96c6\uff1a \u6807\u9898 \u2f47\u671f \u4f5c\u8005 SARS\u75ab\u60c5\u4e2d\u7684\u795e\u7ecf\u75c5\u7406 2003\u5e745\u2f49 \u738b\u2f7c\u5e08 \u2f09\u7ae5\u80a5\u80d6\u95ee\u9898\u7684\u7814\u7a76 2005\u5e749\u2f49 \u5f20\u533b\u2f63 123\u4e2a\u5206\u7c7b\u95ee\u9898\u7684\u5206\u6790 2008\u5e7411\u2f49 \u674e\u6559\u6388 \u5305\u542b\u5173\u952e\u8bcd\u201c\u7814\u7a76\u201d\u7684\u6570\u636e\u6761\u2f6c\u5982\u4e0b\uff1a\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n2005\u5e749\u2f49\uff0c\u5f20\u533b\u2f63\u7684\u8bba\u2f42\u300a\u2f09\u7ae5\u80a5\u80d6\u95ee\u9898\u7684\u7814\u7a76\u300b\u4e2d\u5305\u62ec\u4e86\u5173\u952e\u8bcd\u201c\u7814\u7a76\u201d\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n35\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u5728\u4ee5\u4e0b\u53e5\u2f26\u4e2d\u627e\u5230\u8fde\u8bcd\u201c\u548c\u201d\u7684\u4f7f\u2f64\uff0c\u5e76\u8bf4\u660e\u5176\u4f5c\u2f64\u662f\u8fde\u63a5\u54ea\u4e9b\u8bcd\u8bed\uff1a\u6211\u4eec\u4eca\u5929\u65e9\u4e0a\u8d77\u5f97\u5f88 \u665a\uff0c\u548c\u6628\u5929\u2f00\u6837\u3002\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n\u5728\u8fd9\u53e5\u8bdd\u4e2d\uff0c\u8fde\u8bcd\u201c\u548c\u201d\u8fde\u63a5\u4e86\u4e24\u4e2a\u8bcd\u8bed\u201c\u4eca\u5929\u65e9\u4e0a\u201d\u548c\u201c\u6628\u5929\u201d\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n36\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\nHow does American cuisine vary by region?\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nAmerican cuisine is characterized by its regional variations. Some of these variations include the types of cuisine that dominate di erent parts of the country, such as Southern cooking in the South or Midwest American cooking in the Midwest. Additionally, regional variations include local ingredients, cooking techniques, and even dishes. The cuisine of the American South is known for its use of corn, while the Midwest is known for its emphasis on meat dishes like barbecue and steak. Additionally, regional variations include dishes like pizza in the Northeast and barbecue in the South.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n37\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\nHow do foreign nationals seek medical treatment in the United States?\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nForeign nationals seeking medical treatment in the United States typically have to obtain a medical visa. The type of visa they need depends on the purpose of the visit and the type of medical treatment they need. They typically need a B-1/B-2 visa for visiting the United States for leisure, business, or medical treatment, and a J visa for medical treatments. The application process typically involves a medical exam and proof of health insurance. Once approved, they can begin their medical treatment.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%"
        },
        {
            "heading": "Input Text",
            "text": "tokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n38\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\nEscribe una oraci\u00f3n con el verbo \"caminar\" conjugado en pret\u00e9rito imperfecto. Las mariposas ________ de flor en flor.\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nLas mariposas caminaban de flor en flor.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n39\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\nCompleta la serie num\u00e9rica siguiente: 2, 4, 6, 8, __, 12\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n10\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%"
        },
        {
            "heading": "Input Text",
            "text": "tokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n40\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u00c9crivez une courte biographie de Marie Curie.\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nMarie Curie \u00e9tait une physicienne et une chimiste fran\u00e7aise et polonaise, n\u00e9e \u00e0 Varsovie en"
        },
        {
            "heading": "1867 et d\u00e9c\u00e9d\u00e9e \u00e0 Saint-Cloud en 1934. Elle est surtout connue pour ses d\u00e9couvertes dans le domaine de la radioactivit\u00e9, qui ont r\u00e9volutionn\u00e9 notre compr\u00e9hension du fonctionnement des \u00e9l\u00e9ments chimiques.",
            "text": "Examples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%"
        },
        {
            "heading": "Input Text",
            "text": "tokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n41\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\nDonnez-moi l'URL du site web de l'Universit\u00e9 de Cambridge.\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nL'URL du site web de l'Universit\u00e9 de Cambridge est http://www.cam.ac.uk.\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%"
        },
        {
            "heading": "Input Text",
            "text": "tokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n42\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u300c\u904e\u53bb\u306b\u2f8f\u308f\u308c\u305f\u2f47\u672c\u306e\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u306e\u5e74\u306f\u4f55\u3067\u3059\u304b\uff1f\u300d\u3092\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n2020\u5e74\u306e\u6771\u4eac\u30aa\u30ea\u30f3\u30d4\u30c3\u30af\u306f\u30012021\u5e74\u306b\u5ef6\u671f\u3055\u308c\u307e\u3057\u305f\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%"
        },
        {
            "heading": "Input Text",
            "text": "tokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n43\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u660e\u6cbb\u6642\u4ee3\u306b\u304a\u3051\u308b\u6d0b\u9928\u5efa\u7bc9\u306e\u4ee3\u8868\u4f8b\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n\u660e\u6cbb\u6642\u4ee3\u306b\u304a\u3051\u308b\u6d0b\u9928\u5efa\u7bc9\u306e\u4ee3\u8868\u4f8b\u306e\u2f00\u3064\u306f\u3001\u6771\u4eac\u306e\u4e0a\u91ce\u306e\u56fd\u2f74\u535a\u7269\u9928\u3067\u3059\u3002\u3053\u306e\u5efa \u7269\u306f\u3001\u7f8e\u3057\u3044\u30c7\u30b6\u30a4\u30f3\u3001\u8d05\u6ca2\u306a\u5185\u88c5\u3001\u304a\u3088\u3073\u512a\u308c\u305f\u6280\u8853\u3092\u5099\u3048\u3066\u3044\u307e\u3059\u3002\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n44\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u0e32 \"epiphany\" \u0e2b\u0e21\u0e32\u0e22 \u0e07\u0e2d\u0e30\u0e44\u0e23?\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\nepiphiny \u0e2b\u0e21\u0e32\u0e22 \u0e07 \"\u0e01\u0e32\u0e23 \u0e01 \u0e32\u0e44 \u0e19\u0e1e\u0e1a \u0e07 \u0e15\u0e19\u0e2d\u0e22\u0e32\u0e01 \" \u0e07\u0e40 \u0e19 \u0e07 \u0e32\u0e1b\u0e23\u0e30 \u0e1a\u0e43\u0e08\u0e21\u0e32\u0e01"
        },
        {
            "heading": "Examples",
            "text": "The Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n45\nUse via API \u00b7 Built with Gradio\n\u6e05\u9664 \u63d0\u4ea4\n\u0e41\u0e1b\u0e25\u0e07 \u0e27\u0e40\u0e25\u0e02\u0e17\u0e28 \u0e22\u0e21\u0e40 \u0e19\u0e40\u0e1b\u0e2d \u0e40\u0e0b\u0e19 \u0e27\u0e40\u0e25\u0e02\u0e17\u0e28 \u0e22\u0e21: 0.75\n2048\ntemperature 1\ntopk 6\ntopp 0\n\u6807\u8bb0\n\u0e27\u0e40\u0e25\u0e02\u0e17\u0e28 \u0e22\u0e21\u0e40\u0e2b \u0e32 \u0e19\u0e41\u0e1b\u0e25\u0e07\u0e40 \u0e19\u0e40\u0e1b\u0e2d \u0e40\u0e0b\u0e19 \u0e44 \u0e32 75 \u0e40\u0e1b\u0e2d \u0e40\u0e0b\u0e19\nExamples\nThe Moon's orbit around Earth has\nThe smooth Borealis basin in the Northern Hemisphere covers 40%\nInput Text\ntokens_to_generate\ntemperature\ntop_k\ntop_p\nGenerated Text\n46"
        }
    ],
    "title": "POLYLM: AN OPEN SOURCE POLYGLOT LARGE LAN-",
    "year": 2023
}