{
    "abstractText": "This paper describes our system used in the SemEval-2022 Task5 Multimedia Automatic Misogyny Identification (MAMI). This task is to use the provided text-image pairs to classify emotions. In this paper, We propose a multi-label emotion classification model based on pre-trained LXMERT. We use FasterRCNN to extract visual representation and utilize LXMERT\u2019s cross-attention for multimodal alignment. Then we use the Bilinearinteraction layer to fuse these features. Our experimental results surpass the F1 score of baseline. For Sub-task A, our F1 score is 0.662 and Sub-task B\u2019s F1 score is 0.633. The code of this study is available on GitHub1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chao Han"
        },
        {
            "affiliations": [],
            "name": "Jin Wang"
        },
        {
            "affiliations": [],
            "name": "Xuejie Zhang"
        }
    ],
    "id": "SP:384b9b7deb553bee8ad325471bd872fef23fec02",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "2018 IEEE Conference on Computer Vision and Pat-",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "ELECTRA: pre-training text encoders as discriminators rather than generators",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Ad-",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Elisabetta Fersini",
                "Francesca Gasparini",
                "Giulia Rizzi",
                "Aurora Saibene",
                "Berta Chulvi",
                "Paolo Rosso",
                "Alyssa Lees",
                "Jeffrey Sorensen."
            ],
            "title": "SemEval-2022 Task 5: Multimedia automatic misogyny identification",
            "venue": "Proceedings of the 16th International Work-",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Tongwen Huang",
                "Zhiqi Zhang",
                "Junlin Zhang."
            ],
            "title": "Fibinet: combining feature importance and bilinear feature interaction for click-through rate prediction",
            "venue": "Proceedings of the 13th ACM Conference on Recommender Systems, RecSys 2019, Copenhagen,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Ming Gong",
                "Daxin Jiang."
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
            "venue": "The Thirty-Fourth AAAI Conference",
            "year": 2020
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "CoRR, abs/1908.03557.",
            "year": 2019
        },
        {
            "authors": [
                "Wei Li",
                "Can Gao",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Hao Liu",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "UNIMO: towards unified-modal understanding and generation via cross-modal contrastive learning",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural In-",
            "year": 2019
        },
        {
            "authors": [
                "Bo Peng",
                "Jin Wang",
                "Xuejie Zhang."
            ],
            "title": "Adversarial learning of sentiment word representations for sentiment analysis",
            "venue": "Information Sciences, 541:426\u2013 441.",
            "year": 2020
        },
        {
            "authors": [
                "Di Qi",
                "Lin Su",
                "Jia Song",
                "Edward Cui",
                "Taroon Bharti",
                "Arun Sacheti."
            ],
            "title": "Imagebert: Crossmodal pre-training with large-scale weak-supervised image-text data",
            "venue": "CoRR, abs/2001.07966.",
            "year": 2020
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross B. Girshick",
                "Jian Sun."
            ],
            "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137\u2013 1149.",
            "year": 2017
        },
        {
            "authors": [
                "Weijie Su",
                "Xizhou Zhu",
                "Yue Cao",
                "Bin Li",
                "Lewei Lu",
                "Furu Wei",
                "Jifeng Dai."
            ],
            "title": "VL-BERT: pretraining of generic visual-linguistic representations",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April",
            "year": 2020
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "LXMERT: learning cross-modality encoder representations from transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat-",
            "year": 2019
        },
        {
            "authors": [
                "Junfeng Tian",
                "Min Gui",
                "Chenliang Li",
                "Ming Yan",
                "Wenming Xiao."
            ],
            "title": "Mind at semeval-2021 task 6: Propaganda detection using transfer learning and multimodal fusion",
            "venue": "pages 1082\u20131087. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Jin Wang",
                "Liang-Chih Yu",
                "K. Robert Lai",
                "Xuejie Zhang."
            ],
            "title": "Investigating dynamic routing in treestructured lstm for sentiment analysis",
            "venue": "pages 3430\u2013 3435. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Jin Wang",
                "Liang Chih Yu",
                "K. Robert Lai",
                "Xuejie Zhang."
            ],
            "title": "Tree-structured regional cnnlstm model for dimensional sentiment analysis",
            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing, 28.",
            "year": 2020
        },
        {
            "authors": [
                "Li Yuan",
                "Jin Wang",
                "Xuejie Zhang."
            ],
            "title": "Ynu-hpcc at semeval-2020 task 8: Using a parallel-channel model for memotion analysis",
            "venue": "pages 916\u2013921. International Committee for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Xingyu Zhu",
                "Jin Wang",
                "Xuejie Zhang."
            ],
            "title": "YNU-HPCC at semeval-2021 task 6: Combining ALBERT and text-cnn for persuasion detection in texts and images",
            "venue": "Proceedings of the 15th International Workshop on Semantic Evalua-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 748 - 755 July 14-15, 2022 \u00a92022 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "In social networks, meme is mainly used to express the emotion of netizen. It usually consists of text and images. But at the same time, memes also convey some negative emotions, such as negative comments about women. SemEval-2022 Task5: Multimedia Automatic Misogyny Identification (MAMI) (Fersini et al., 2022) focuses on identifying whether meme conveys negative emotions towards women.\n\u2022 Sub-task A: a basic task about misogynous meme identification, where a meme should be categorized either as misogynous or not misogynous;\n\u2022 Sub-task B: an advanced task, where the type of misogyny should be recognized among potential overlapping categories such as stereotype, shaming, objectification, and violence.\nSince the Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models were proposed, researchers have begun to work on image\n1https://github.com/HC-super/ SemEval-2022-Task-5\nand text multi-modality work in recent years, in addition to using one modality such as only image or text. Nowadays, for multimodal models, they can be divided into two categories, singlestream model and dual-stream model. In the single-stream model, language information and vision information are fused at the beginning and directly input into the encoder. Some representative single-stream models include ImageBERT (Qi et al., 2020), Unicoder VL (Li et al., 2020), VL-BERT (Su et al., 2020), VisualBERT (Li et al., 2019), etc. In the dual-stream model, in addition to the LXMERT, we will introduce below, there were ViLBert (Lu et al., 2019) and UNIMO (Li et al., 2021), etc.\nAs for emotion recognition, in previous tasks, there are also emotion classification tasks based on multi-modal graphics and text, such as Zhu et al. (2021) used text-CNN and ALBERT to Identify the persuasion skills of Meme. Peng et al. (2020) used the adversarial learning of sentiment word representations for sentiment analysis. A tree-structured regional CNN-LSTM (Wang et al., 2020) and dynamic routing in a tree-structured LSTM (Wang et al., 2019) were used for dimensional sentiment analysis. In previous SemEval competitions, Tian et al. (2021) extracted heterogeneous visual representations (i.e., face features, OCR features, and multimodal representations) and explored various multimodal fusion strategies to combine the textual and visual representations. In addition, in multimodal analysis combining images and text, Yuan et al. (2020) proposed a parallel channel ensemble model combining BERT embedding, BiLSTM, attention and CNN, and ResNet for sentiment analysis of memes.\nThe main difficulty of multi-modality is how to extract the two modalities\u2019 features and express the semantics more accurately, which involves the representation of multi-modality, the alignment between multi-modality, and the fusion of multi-\n748\nmodality. For the multi-modal task of text and image, the previous practice is to input the text and image into two different pre-training models for processing the text and image modalities respectively, and then concatenate the output features and predict the emotion. However, this method lacks the processing of the alignment relationship between modalities. The proposed model considers the above three problems in the multi-modality field. Inspired by LXMERT, we use it as the main framework of our model. We use Faster R-CNN (Ren et al., 2017) to extract image RoI features and their position. For texts, we use BERT to extract text embedding. Then our system uses LXMERT (Tan and Bansal, 2019) to deal with the multimodal alignment of text and image. After when two modalities are processed by LXMERT, we use the learnable integration mechanism Bilinearinteraction layer to fuse these features.\nThe remainder of this paper is organized as follows. In section 2, we described LXMERT and our fusion method in detail. The experimental results are presented in section 3. Finally, a conclusion is drawn in section 4."
        },
        {
            "heading": "2 System Overview",
            "text": "Task A and Task B are very similar in model structure except for the output layer. Therefore, we in-\ntroduce the model we proposed as a whole. This model can be divided into four parts. They are the embedding layer for image and text preprocessing, the encoder for multi-modal presentation and alignment, the feature fusion layer, and the final output layer. The proposed model is as shown in Figure 2."
        },
        {
            "heading": "2.1 Embedding",
            "text": "For images, LXMERT does not simply use a convolutional neural network to output feature map but uses (Anderson et al., 2018) to extract objects from images. The image processing of LXMERT is similar to text processing inspired by BERT. The specific idea is to use Faster R-CNN to select 36 RoI (region of interest) boxes with high confidence for each image and use these boxes as the features of the image. Similar to the text processing of BERT, the model also considers the position of each box and embeds the corresponding position. 36 objects are extracted by Faster R-CNN as {o1, . . . , o36}. fj is the 2048 dimension RoI features of oj , and pj is its position. As is shown in figure 2, the processing of these variables is as follows:\nf\u0302j = LayerNorm (WFfj + bF)\np\u0302j = LayerNorm (WPpj + bP) vj = ( f\u0302j + p\u0302j ) /2\n(1)\nwhere WF and WP are the trainable weights of fully connected layer in matrix format. Moreover, bF and bP are the bias of the layer. f\u0302j and p\u0302j are the output of the layer-normalization.\nFor the text, sentences are converted into tokens whose length is equal to the length of scent according to the practice of WordPiece tokenizer (Wu et al., 2016). For instance, when the length of the sentence is n, the word tokens are {w1, ..., wn}. Then wordwi and its index i (the absolute position of wi) are projected to vectors by embedding sublayers.The specific structure of embedder is shown in Figure 1. Then added to the index-aware word embedding:\nw\u0302i = WordEmbed (wi)\nu\u0302i = IdxEmbed (i)\nhi = LayerNorm (w\u0302i + u\u0302i)\n(2)\nThe specific structure of embedder is shown in Figure 1."
        },
        {
            "heading": "2.2 Attention layer",
            "text": "In this subsection, we will give a brief description of the attention mechanism. The principle of the attention mechanism is to give a request vector x and its context vector yj , then, calculate the correlation between x and each yj , and get a correlation score. The correlation score used in LXMERT is the dot product of vector x and vector yj . After calculating the scores of all relevant context vectors yj for x, LXMERT uses softmax to convert each score into a probability \u03b1j to obtain the at-\ntention distribution.\naj = score (x, yj) \u03b1j = exp (aj) / \u2211\nk\nexp (ak) (3)\nAttX\u2192Y (x, {yj}) = \u2211\nj\n\u03b1jyj (4)\nThe output of the layer is the weighted sum of all probabilities with yi.\nThe self-attention layer in LXMERT is implemented in a similar way to the attention layer, except that the query vector x in self-attention comes from the context-dependent vector yi."
        },
        {
            "heading": "2.3 Encoder",
            "text": "The processing of image modality and text modality is shown in Figure 2. After embedding two modalities, LXMERT uses the two transformer single-modality encoders. One is a text encoder and another is an image encoder. Each layer in a single-modality encoder contains a self-attention (\u2018Self\u2019) sub-layer and a feed-forward (\u2018FF\u2019) sublayer, where the feed-forward sub-layer is further composed of two fully-connected sub-layers. We take NL and NR layers in the language encoder and the object-relationship encoder, respectively. We add a residual connection and layer normalization (annotated by the \u2018+\u2019 sign in Figure 2) after each sub-layer as in Transformer (Vaswani et al., 2017). The features processed by a singlemodality encoder will be first sent to another encoder called the cross-modality layer. Its main function is to align the features of the two modalities. The bi-directional cross-attention sublayer\ncontains two unidirectional cross-attention sublayers, one from image to text and the other from text to the image. LXMERT stacks them Nx times, the input of k-th layer is the output of the previous(k \u2212 1)-th layer. Similarly, the query and context vectors are the outputs of the (k \u2212 1)-th layer. The method of processing the text features hk\u22121i and the image features v k\u22121 j in unidirectional cross-attention sub-layers is as follows:\nh\u0302ki = CrossAttL\u2192R ( hk\u22121i , { vk\u221211 , . . . , v k\u22121 m }) v\u0302kj = CrossAttR\u2192L ( vk\u22121j , { hk\u221211 , . . . , h k\u22121 n }) (5)\nwhere h\u0302ki and v\u0302 k j are the output of the crossattenton layer. Then LXMERT further inputs the features processed by the cross-modality sublayer to the selfattention sublayer. This method aimed to further construct the internal connection of each modality after alignment. The specific treatment is:\nh\u0303ki = Self AttL\u2192L ( h\u0302ki , { h\u0302k1 , . . . , h\u0302 k n }) v\u0303kj = Self AttR\u2192R ( v\u0302kj , { v\u0302k1 , . . . , v\u0302 k m }) (6)\nh\u0302ki , v\u0302 k j then processed by self-attention to h\u0303 k i and v\u0303kj , which will be further input to an \u2018FF\u2019 sublayer, connected through a residual, and input to the normalization to obtain the final output hki , v k j . For each text in the data, the model will generate a Pooler output. We use the Pooler of each sentence as the output of the text modality."
        },
        {
            "heading": "2.4 Fusion",
            "text": "The method of this layer is inspired by Sina\u2019s paper FiBiNET by Huang et al. (2019). After LXMERT outputs two modality features, we need to further process its output. The dimension of image features is 36 \u00d7 768, while the dimension of text features is 768. To better integrate the two modalities, we flatten the image features and change its dimension to 768 through a feedforward layer. Then, each modality will be normalized through layer normalization. Then, the\nfeatures of each modality are sent to the Bilinearinteractive layer.\nThe idea of the Bilinear-interactive layer is as shown in Figure 3. We establish a k-order square matrixW , which is trainable. To fuse the information of various modalities, 768-dimensional image features will first inner product with W . Then, for text features, we use Hadamard product to multiply the previous matrix. We finally use the dropout layer to improve the generalization ability of the model."
        },
        {
            "heading": "2.5 Output layer",
            "text": "\u2022 Sub-task A: this task is a binary classification\ntask, so in the output layer, we use a shape of 768 \u00d7 1 full connection layer and use sigmoid as the activation function to process the results.\n\u2022 Sub-task B: this task is a multi-label classification task. Therefore, in the output layer, we use a full connection layer whose shape is 768 \u00d7 5. Since each label classification is equivalent to binary classification, we use sigmoid as the activation function to process the results during output."
        },
        {
            "heading": "3 Experiments and Evaluation",
            "text": ""
        },
        {
            "heading": "3.1 Dataset",
            "text": "The task organizer provided 10000 pieces of data for training, including meme images with image serial numbers and text descriptions corresponding to the image. In the training dataset, there\nare 10000 images and an excel table to record the text corresponding to the images and supervise the learning of the corresponding labels.\nWhen analyzing the data, we found that different labels account for different proportions in the number of their respective classifications. For the misogynous tag, both 0 and 1 categories account for 50%, so the data sample tag is more balanced for a supervision task. However, for the other four labels such as sharing, the proportion of label 1 is only 12.74%. Among 10000 samples, the label of violence accounts for only 9.53%. Table 1 shows the proportion of each label in the training dataset. As shown in Table 1, we find that the proportion of labels of different categories is very different, and there is data imbalance. This will make the model have a strong learning effect on a large classification label and easy to classify. However, for the low proportion of classification tags, it is difficult to learn and classify.\nBased on this, we use Focal loss by (He et al., 2016) as the loss function of our model."
        },
        {
            "heading": "3.2 Experimental configuration",
            "text": "Our model is based on TensorFlow platform version 2.5.0. The main model adopts LXMERT from the Hugging Face transformers toolkit. We first useUNC-NLP/LXMERT-base-uncased tokenizerFast to process our text to embeddings, and we also use UNC-NLP/LXMERT-base-uncased pretrained model as our base model LXMERT\u2019s pretrained model. The Adam optimizer (Kingma and Ba, 2015) was used to update all trainable parameters. The Hyper-parameters configuration used in the model is shown in Table 2: We use Faster RCNN to extract features of images, which is based on the paper by (Anderson et al., 2018). In this task, we use an open-source docker image airsplay / bottom-up attention and use a Faster R-CNN pretraining model based on ResNet101 to extract 36 RoI feature boxes and their corresponding position."
        },
        {
            "heading": "3.3 Evaluation Metrics",
            "text": "Sub-task A Systems will be evaluated using macro-average F1-score. In particular, for each class label (i.e. misogynous and not misogynous) the corresponding F1-score will be computed, and the final score will be estimated as the arithmetic mean of the two F1-score. Sub-task B Systems will be evaluated using weighted-average F1-score. In particular, the F1-score will be com-\nputed for each label and then their average will be weighted by support, i.e, the number of true instances for each label.\nprecision = TP\nTP + FP\nrecall = TP\nTP + FN\n(7)\nTP is the number of true positives classified by the model. FN is the number of false negatives classified by the model. FP is the number of false positives classified by the model.\nF1-score = 2\u00d7 precision \u00d7 recall\nprecision + recall (8)\nF1-score is the harmonic average of recall and precision."
        },
        {
            "heading": "3.4 Hyperparametric selection",
            "text": "In this section, we mainly introduce the hyperparametric selection of focal loss of the model. We adjust the two hyperparameters \u03b3 and \u03b1 of Focal loss and train the model. In the training dataset, we randomly take 90 % data for the training model and the remaining 10 % as the test dataset to test\nthe performance of the model. The loss function focal loss is modified based on the standard crossentropy loss.\nThis function can reduce the easy-to-classify samples so that the model can more focus on the samples that are difficult to classify in training. pt in cross-entropy loss function reflects the recognition ability of the model to this sample (i.e. how well the knowledge is mastered). We define pt is:\npt = { p if y = 1 1\u2212 p otherwise\n(9)\nThe smaller the pt is, the more difficult it is to classify, so contribution should be improved to the loss function when calculating the loss. Therefore, the specific method of Focal loss is to multiply a weight with pt before the entropy loss function. \u03b1 is balancing factor,\u03b1 \u2208 [0, 1], \u03b3 is modulating factor, \u03b3 \u2208 [0, 5]. The Focal loss is as:\nFocal_loss(pt) = \u2212\u03b1(1\u2212 pt)\u03b3log(pt) (10)\nThus, when \u03b1 = 1, gamma = 0, focal loss is similar to the cross-entropy loss function. By changing the values of \u03b3 and \u03b1, we found that when \u03b1 = 0.25 and \u03b3 = 3, for sub-task B, the weighted F1 score of our model reached 0.662 and 0.633. See Figure 4."
        },
        {
            "heading": "3.5 Model comparison",
            "text": "We compare our model to a baseline and a model that combines two pre-trained models based on ELECTRA (Clark et al., 2020) and ResNet-101 (Ren et al., 2017) in this section. ELECTRA deals with text modality and ResNet is used to deal with image modality. The methods of feature fusion are compared with the Bilinear-interactive layer and\nconcatenate layer using direct concatenate. The specific task is based on sub-task B. See Table 3 for details."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this task, we design an image and text multi-modality model based on LXMERT for multi-modality representation and alignment, and modality fusion based on the Bilinear-interaction layer. Compared with the traditional method of stitching two pre-training models for each modality then concatenating two features to predict emotion, this model considers the representation, alignment, and fusion of multi-modality, and achieves better results than the baseline method.\nAt the same time, we found that after adding the Bilinear-interaction layer, the performance of the model is better than using only feature concatenate. See Table 3. Meanwhile, when analyzing the data, we found that the background of the meme graph and some characters in the graph were not used as the target input model by Faster R-CNN, which may affect the accuracy of the model. Meanwhile, the size of the meme image is too small to include multiple targets, and the target is relatively single, which may affect the performance of the model."
        },
        {
            "heading": "5 Acknowledgement",
            "text": "This work was supported by the National Natural Science Foundation of China (NSFC) under Grants No. 61966038. The authors would like to thank the anonymous reviewers for their constructive comments."
        }
    ],
    "title": "YNU-HPCC at SemEval-2022 Task 5: Multi-Modal and Multi-label Emotion Classification Based on LXMERT",
    "year": 2022
}