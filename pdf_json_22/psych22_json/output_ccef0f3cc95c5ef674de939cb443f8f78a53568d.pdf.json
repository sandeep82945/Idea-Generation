{
    "abstractText": "Robots are used in various social interactions that require them to be perceived as credible agents (e.g., as product recommenders in shopping malls). To be rated credible (i.e., competent, trustworthy, and caring) a robot\u2019s mentalizing abilities have shown to be beneficial because they allow a robot to infer users\u2019 inner states, thus serving as a prerequisite for understanding their beliefs and attitudes. However, social robots are often deployed by private and thus profit-oriented companies. In such cases where an organization\u2019s implied manipulative intent is salient, the effect of robots\u2019 mentalizing abilities might be reversed. The reason for this is that mentalizing abilities could pose a persuasive threat to users rather than a feature for better understanding, thereby decreasing credibility attributions. These assumptions were tested in a three (robot\u2019s mentalizing abilities) by two (external manipulative intent) between-subjects, pre-registered, laboratory experiment during which participants interacted with a social robot that recommended experience vouchers as potential gifts for participants\u2019 target persons. Contrary to our assumptions, inferential statistical results revealed no significant differences in explicit or indirect credibility attributions caused by the experimental manipulation. The external manipulative intent of an organization using the robot caused no differences in participants\u2019 behavioral intentions or evaluations of it. Furthermore, only participants\u2019 attribution of empathic understanding to the robot varied significantly between the three mentalizing conditions. Our results suggest that people focus more on the robot than on the organization using it, causing potential opportunities for such organizations to hide their economic interests from the users.",
    "authors": [
        {
            "affiliations": [],
            "name": "Judee K. Burgoon"
        },
        {
            "affiliations": [],
            "name": "Grzegorz Marcin W\u00f3jcik"
        }
    ],
    "id": "SP:b0503e89bb646728ae89a8d65c173da2dfa078c6",
    "references": [
        {
            "authors": [
                "S. Andrist",
                "E. Spannan",
                "B. Mutlu"
            ],
            "title": "Rhetorical robots: Making robots more effective speakers using linguistic cues of expertise,",
            "venue": "Proceedings of the ACM/IEEE international conference on human-robot interaction; March",
            "year": 2013
        },
        {
            "authors": [
                "J. Banks"
            ],
            "title": "Theory of mind in social robots: replication of five established human tests",
            "venue": "Int. J. Soc. Robot",
            "year": 2020
        },
        {
            "authors": [
                "J. Banks"
            ],
            "title": "Of like mind: the (mostly) similar mentalizing of robots and humans",
            "venue": "Technol., Mind,",
            "year": 2021
        },
        {
            "authors": [
                "K. Baraka",
                "P. Alves-Oliveira",
                "T. Ribeiro"
            ],
            "title": "An extended framework for characterizing social robots,",
            "venue": "in Springer series on bio-and Neurosystems. 12th Edn. ed. C. Jost (Cham: Springer),",
            "year": 2020
        },
        {
            "authors": [
                "S. Baron-Cohen"
            ],
            "title": "The autistic child's theory of mind: a case of specific developmental delay",
            "venue": "J. Child Psychol. Psychiatr",
            "year": 1989
        },
        {
            "authors": [
                "S. Baron-Cohen",
                "A.M. Leslie",
                "U. Frith"
            ],
            "title": "Does the autistic child have a \u201ctheory of mind\u201d? Cognition",
            "year": 1985
        },
        {
            "authors": [
                "S. Basciano"
            ],
            "title": "A robot employed to combat spread of Coronavirus in supermarkets springwise intelligence",
            "venue": "Available at https://www.springwise.com/ innovation/retail/edeka-pepper-robot-coronavirus (Accessed May",
            "year": 2020
        },
        {
            "authors": [
                "W.O. Bearden",
                "D.M. Hardesty",
                "R.L. Rose"
            ],
            "title": "Consumer selfconfidence: Refinements in conceptualization and measurement",
            "venue": "J. Consum. Res",
            "year": 2001
        },
        {
            "authors": [
                "B. Benninghoff",
                "P. Kulms",
                "L. Hoffmann",
                "N.C. Kr\u00e4mer"
            ],
            "title": "Theory of mind in human-robot communication: appreciated or not? Kognit",
            "year": 2013
        },
        {
            "authors": [
                "B. Chang"
            ],
            "title": "MSC Cruises\u2019 newest ship will have a robot bartender that can make any drink you want \u2014 meet rob,",
            "venue": "in Business Insider. Available at: https://www. businessinsider.com/msc-cruises-robot-bartender-new-ship-virtuosa-2021-2 (Accessed May",
            "year": 2021
        },
        {
            "authors": [
                "F. Correia",
                "C. Guerra",
                "S. Mascarenhas",
                "F.S. Melo",
                "A. Paiva"
            ],
            "title": "Exploring the impact of fault justification in human-robot trust,",
            "venue": "Proceedings of the 17th international conference on autonomous agents and multiagent systems (AAMAS",
            "year": 2018
        },
        {
            "authors": [
                "N. Epley",
                "A. Waytz",
                "J.T. Cacioppo"
            ],
            "title": "On seeing human: a three-factor theory of anthropomorphism",
            "venue": "Psychol. Rev",
            "year": 2007
        },
        {
            "authors": [
                "F. Faul",
                "E. Erdfelder",
                "Lang",
                "A.-G",
                "A. Buchner"
            ],
            "title": "G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences",
            "venue": "Behav. Res. Methods",
            "year": 2007
        },
        {
            "authors": [
                "M. Finkel",
                "N.C. Kr\u00e4mer"
            ],
            "title": "Humanoid robots \u2013 artificial. Human-like. Credible? Empirical comparisons of source credibility attributions between humans, humanoid robots, and non-human-like devices",
            "venue": "Int. J. Soc. Robot",
            "year": 2022
        },
        {
            "authors": [
                "M. Friestad",
                "P. Wright"
            ],
            "title": "The persuasion knowledge model: how people cope with persuasion attempts",
            "venue": "J. Consum. Res",
            "year": 1994
        },
        {
            "authors": [
                "A.I. Goldman"
            ],
            "title": "Theory of mind,",
            "year": 2012
        },
        {
            "authors": [
                "O.C. G\u00f6r\u00fcr",
                "B.S. Rosman",
                "G. Hoffman",
                "S. Albayrak"
            ],
            "title": "Toward integrating theory of mind into adaptive decision-making of social robots to understand human intention,\u201d in Workshop on the role of intentions in human-robot interaction at the international conference on human-robot interaction; March 6, 2017; (Austria)",
            "year": 2017
        },
        {
            "authors": [
                "A.F. Hayes"
            ],
            "title": "Introduction to mediation, moderation, and conditional process analysis: A regression-based approach",
            "year": 2017
        },
        {
            "authors": [
                "F. Heider",
                "M. Simmel"
            ],
            "title": "An experimental study of apparent behavior",
            "venue": "Am. J. Psychol",
            "year": 1944
        },
        {
            "authors": [
                "A.C. Horstmann",
                "N.C. Kr\u00e4mer"
            ],
            "title": "Great expectations? Relation of previous experiences with social robots in real life or in the media and expectancies based on qualitative and quantitative assessment",
            "year": 2019
        },
        {
            "authors": [
                "A.C. Horstmann",
                "N.C. Kr\u00e4mer"
            ],
            "title": "Expectations vs. actual behavior of a social robot: an experimental investigation of the effects of a social robot\u2019s interaction skill level and its expected future role on people\u2019s evaluations",
            "venue": "PLoS One 15:e0238133. doi: 10.1371/journal.pone.0238133",
            "year": 2020
        },
        {
            "authors": [
                "S. Kopp",
                "N.C. Kr\u00e4mer"
            ],
            "title": "Revisiting human-agent communication: the importance of joint co-construction and understanding mental states",
            "venue": "Front. Psychol. 12:580955",
            "year": 2021
        },
        {
            "authors": [
                "J.C. McCroskey",
                "J.J. Teven"
            ],
            "title": "Goodwill: A reexamination of the construct and its measurement",
            "venue": "Commun. Monogr",
            "year": 1999
        },
        {
            "authors": [
                "M.J. Metzger",
                "A.J. Flanagin"
            ],
            "title": "Credibility and trust of information in online environments: the use of cognitive heuristics",
            "venue": "J. Pragmat",
            "year": 2013
        },
        {
            "authors": [
                "W. Mou",
                "M. Ruocco",
                "D. Zanatto",
                "A. Cangelosi"
            ],
            "title": "When would you trust a robot? A study on trust and theory of mind in human-robot interactions,",
            "venue": "Proceedings of the 29th IEEE international conference on robot and human interactive communication;",
            "year": 2020
        },
        {
            "authors": [
                "C. Nass",
                "Y. Moon"
            ],
            "title": "Machines and mindlessness: social responses to computers",
            "venue": "J. Soc. Issues",
            "year": 2000
        },
        {
            "authors": [
                "K. Pillay"
            ],
            "title": "Robots at reception: South African hotel turns to machines to beat pandemic",
            "venue": "Reuters. Available at: https://www.reuters.com/article/ us-health-coronavirus-safrica-hotel-idUSKBN2AF0QX (Accessed May",
            "year": 2021
        },
        {
            "authors": [
                "D. Premack",
                "G. Woodruff"
            ],
            "title": "Does the chimpanzee have a theory of mind",
            "venue": "Behav. Brain Sci",
            "year": 1978
        },
        {
            "authors": [
                "B. Reeves",
                "C.I. Nass"
            ],
            "title": "The media equation: How people treat computers, television, and new media like real people and places",
            "year": 1996
        },
        {
            "authors": [
                "S. Rodgers"
            ],
            "title": "Effects of sponsorship congruity on e-sponsors and e-newspapers",
            "venue": "Journal. Mass Commun. Q",
            "year": 2007
        },
        {
            "authors": [
                "M. Schurz",
                "J. Radua",
                "M.G. Tholen",
                "L. Maliske",
                "D.S. Margulies",
                "Mars",
                "R. B"
            ],
            "title": "Toward a hierarchical model of social cognition: a neuroimaging metaanalysis and integrative review of empathy and theory of mind",
            "year": 2021
        },
        {
            "authors": [
                "R.M. Seyfarth",
                "D.L. Cheney"
            ],
            "title": "Affiliation, empathy, and the origins of theory of mind",
            "venue": "Proc. Nat. Acad. Sci",
            "year": 2013
        },
        {
            "authors": [
                "C.J. Stanton",
                "C.J. Stevens"
            ],
            "title": "Don\u2019t stare at me: the impact of a humanoid robot\u2019s gaze upon trust during a cooperative human-robot visual task",
            "venue": "Int. J. Soc. Robot",
            "year": 2017
        },
        {
            "authors": [
                "S. Sturgeon",
                "A. Palmer",
                "J. Blankenburg",
                "D. Feil-Seifer"
            ],
            "title": "Perception of social intelligence in robots performing false-belief tasks,",
            "venue": "Proceedings of the 28th IEEE international conference on robot and human interactive communication; August -September",
            "year": 2019
        },
        {
            "authors": [
                "S.S. Sundar",
                "C. Nass"
            ],
            "title": "Conceptualizing sources in online news",
            "venue": "J. Commun",
            "year": 2001
        },
        {
            "authors": [
                "S. Thellman",
                "A. Silvervarg",
                "T. Ziemke"
            ],
            "title": "Some adults fail the falsebelief task when the believer is a robot,",
            "venue": "in Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction; March 23\u201326,",
            "year": 2020
        },
        {
            "authors": [
                "S. Thellman",
                "T. Ziemke"
            ],
            "title": "The intentional stance toward robots: conceptual and methodological considerations,",
            "venue": "Proceedings of the 41st annual conference of the cognitive science society; July",
            "year": 2019
        },
        {
            "authors": [
                "S. Vinanzi",
                "M. Patacchiola",
                "A. Chella",
                "A. Cangelosi"
            ],
            "title": "Would a robot trust you? Developmental robotics model of trust and theory of mind",
            "venue": "Phil. Trans. R. Soc. B",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "E.G. Krumhuber"
            ],
            "title": "Mind perception of robots varies with their economic versus social function",
            "year": 2018
        },
        {
            "authors": [
                "F. Yamaoka",
                "T. Kanda",
                "H. Ishiguro",
                "N. Hagita"
            ],
            "title": "Interacting with a human or a humanoid robot?",
            "venue": "Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems; October- November",
            "year": 2007
        }
    ],
    "sections": [
        {
            "text": "Frontiers in Psychology 01 frontiersin.org\nThe robotic mentalist \u2013 On the influences of robots\u2019 mentalizing abilities and external manipulative intent on people\u2019s credibility attributions Marcel\u00a0Finkel * and Nicole\u00a0C.\u00a0Kr\u00e4mer\nChair of Social Psychology: Media and Communication, Department of Computer Science and Applied Cognitive Science, University of Duisburg-Essen, Duisburg, Germany\nRobots are used in various social interactions that require them to\nbe\u00a0perceived as credible agents (e.g., as product recommenders in shopping\nmalls). To be\u00a0rated credible (i.e., competent, trustworthy, and caring) a robot\u2019s\nmentalizing abilities have shown to be\u00a0beneficial because they allow a robot to\ninfer users\u2019 inner states, thus serving as a prerequisite for understanding their\nbeliefs and attitudes. However, social robots are often deployed by private\nand thus profit-oriented companies. In such cases where an organization\u2019s\nimplied manipulative intent is salient, the effect of robots\u2019 mentalizing abilities\nmight be\u00a0reversed. The reason for this is that mentalizing abilities could pose\na persuasive threat to users rather than a feature for better understanding,\nthereby decreasing credibility attributions. These assumptions were tested\nin a three (robot\u2019s mentalizing abilities) by two (external manipulative intent)\nbetween-subjects, pre-registered, laboratory experiment during which\nparticipants interacted with a social robot that recommended experience\nvouchers as potential gifts for participants\u2019 target persons. Contrary to our\nassumptions, inferential statistical results revealed no significant differences\nin explicit or indirect credibility attributions caused by the experimental\nmanipulation. The external manipulative intent of an organization using the\nrobot caused no differences in participants\u2019 behavioral intentions or evaluations\nof it. Furthermore, only participants\u2019 attribution of empathic understanding to\nthe robot varied significantly between the three mentalizing conditions. Our\nresults suggest that people focus more on the robot than on the organization\nusing it, causing potential opportunities for such organizations to hide their\neconomic interests from the users.\nKEYWORDS\nhuman-robot interaction, mentalizing abilities, credibility, manipulative intent, sourceness\nTYPE Original Research PUBLISHED 26 October 2022 DOI 10.3389/fpsyg.2022.993302"
        },
        {
            "heading": "OPEN ACCESS",
            "text": ""
        },
        {
            "heading": "EDITED BY",
            "text": "Judee K. Burgoon, University of Arizona, United\u00a0States"
        },
        {
            "heading": "REVIEWED BY",
            "text": "Andrzej Tadeusz Kawiak, Marie Curie-Sklodowska University, Poland Grzegorz Marcin W\u00f3jcik, Marie Curie-Sklodowska University, Poland\n*CORRESPONDENCE Marcel Finkel marcel.finkel@uni-due.de"
        },
        {
            "heading": "SPECIALTY SECTION",
            "text": "This article was submitted to Human-Media Interaction, a section of the journal Frontiers in Psychology\nRECEIVED 18 July 2022 ACCEPTED 12 September 2022 PUBLISHED 26 October 2022"
        },
        {
            "heading": "CITATION",
            "text": "Finkel M and Kr\u00e4mer NC (2022) The robotic mentalist \u2013 On the influences of robots\u2019 mentalizing abilities and external manipulative intent on people\u2019s credibility attributions. Front. Psychol. 13:993302. doi: 10.3389/fpsyg.2022.993302"
        },
        {
            "heading": "COPYRIGHT",
            "text": "\u00a9 2022 Finkel and Kr\u00e4mer. This is an openaccess article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\nFrontiers in Psychology 02 frontiersin.org"
        },
        {
            "heading": "Introduction",
            "text": "Social robots are expected to serve as people\u2019s daily companions, assistants, guides, or other service-related agents soon. All these roles have in common that a robot would need to understand users\u2019 intentions and desires to communicate effectively. For instance, a robot being used as a product recommender needs to understand users\u2019 personal preferences and interests for different product attributes. Such focus on personal preferences is important because social encounters are characterized by unique individuals and a complex structure of context-dependent requirements.\nOne way to increase such value for human-robot interactions is through the robots\u2019 constantly advancing cognitive abilities which allow for more and more user-adapted interactions in the future. Thus, to ensure meaningful social conversations a social robot needs sufficient mentalizing abilities to engage in perspective-taking, i.e., understanding and considering the beliefs and interests of individual users (Kopp and Kr\u00e4mer, 2021). Some studies already show that the perception of such mentalizing abilities can have beneficial effects on people\u2019s evaluations of social robots (Sturgeon et\u00a0al., 2019; Mou et\u00a0al., 2020), although they can be\u00a0extended in terms of more differentiation between theoretical degrees of mentalizing.\nHowever, in many cases, social robots are utilized by private companies with profit-oriented interests such as cruise lines (Chang, 2021), retail stores (Basciano, 2020), and hotels (Pillay, 2021). Thus, people\u2019s credibility attributions related to the respective robot might decline because they perceive the robot to be\u00a0a marketing tool \u2013 potentially to the extent that they know that the costs of buying and maintaining a robot must be\u00a0 compensated by increases in the companies\u2019 sales or profit. If people think that higher profit is achieved by using a robot against their own interest, e.g., by persuasively recommending more high-profit products to them via the robot, it will impair future interactions with the robot. Although people will most likely assume that such economic interest does not originate from the robot itself but from the robot-using company, it will most likely influence the robot\u2019s credibility because of its association with the company. In such cases where the manipulative intent of an organization as the source behind the robot becomes salient, the beneficial influence of mentalizing abilities might therefore turn negative. In such cases, it can be\u00a0assumed that the robot\u2019s mentalizing abilities to understand its users are rather viewed as a manipulative threat. Because this assumed interaction of perceived mentalizing abilities and manipulative intent of a provider using the robot lacks empirical support, we\u00a0investigate the effects of social robots\u2019 mentalizing abilities in a laboratory experiment by manipulating them in combination with the presence or absence of a company\u2019s manipulative intent."
        },
        {
            "heading": "Mentalizing social robots",
            "text": "Robots that are used for creating social encounters are often humanoid robots, meaning their exterior is meant to resemble the\nphysique of a human being. Thus, these robots can easier make use of human communication strategies such as gesturing or facial expressions. A more fundamental and unique human ability however is the Theory of Mind ability. \u201cTheory of Mind (ToM) [or mentalizing] refers to the cognitive capacity to attribute mental states to self and others.\u201d (Goldman, 2012, p.\u00a0402). It\u2019s an ability that allows people to make inferences about other persons\u2019 inner states and helps to understand that their behavior is not a predetermined output to a certain input, but rather shaped by their personal beliefs and desires. Phrased differently, mentalizing is the engagement \u201cin meta-representational sense-making associated with inferring others\u2019 mental states\u201d (Banks, 2020, p.\u00a0403). Thus, it is a fundamental prerequisite for communication, especially regarding perspective-taking, and is closely linked and similar to the effects of empathy (Seyfarth and Cheney, 2013).\nAlthough research in computer science has already conceptualized how to implement ToM-approaches in technical manners (e.g., G\u00f6r\u00fcr et\u00a0al., 2017; Vinanzi et\u00a0al., 2019), most social robots do not have mentalizing abilities yet. They cannot infer their users\u2019 minds and thus are not able to behave as complex as humans do in social interactions. Despite robots\u2019 lack of mentalizing abilities people can attribute these qualities to a robot. People are often falsely attributing a mind to non-human and/or unliving entities, for instance, animals (Premack and Woodruff, 1978) or geometric shapes (Heider and Simmel, 1944), even if they know about the irrationality of this attribution. They do so autonomously because they have the need to understand the environment around them. Thus, people engage in sensemaking, relying on their knowledge about humans, in this case, that others are intentional beings (Epley et\u00a0al., 2007). In other words, people make use of folk psychology to develop an intentional stance toward robots (Thellman and Ziemke, 2019). They use their knowledge and experiences to extrapolate a mental model of how the robot works and why it acts the way it does. These mental models about robots are often influenced by mass media but also by personal encounters with robots (Horstmann and Kr\u00e4mer, 2019). Furthermore, they can be\u00a0distinguished by the degree of mentalizing abilities they assume a robot to have. If people acknowledge intentionality and regard a robot as having its own beliefs and desires, they have first-order beliefs about it. In these cases, the robot\u2019s abilities can be\u00a0 described as first-ordermentalizing abilities, that limit it from being able to infer other people\u2019s beliefs and desires. In contrast, second-order beliefs include the attribution of first-order beliefs to the robot but also the attribution of it being able to make inferences about other people\u2019s minds (Baron-Cohen, 1989). Phrased differently, the robots are perceived as understanding people as intentional beings and thus as having a theory of mind (second-order mentalizing).\nManipulations of a robot\u2019s mentalizing abilities have often been implemented by making use of known assessments for humans, for example via false-belief-tasks such as the Sally-Anne test introduced by Baron-Cohen et\u00a0al. (1985) and used for instance by Sturgeon et\u00a0al. (2019) and Mou et\u00a0al. (2020), (see Banks (2020), for an overview of replicated tests). These (modified) tests in\nFrontiers in Psychology 03 frontiersin.org\ncombination with a robot\u2019s answers/behaviors that are shown to the participants allow to make inferences about the robot\u2019s mentalizing abilities. For example, if the robot has a theory of mind, it should be\u00a0able to recognize other persons\u2019 false beliefs which must be\u00a0identified during the Sally-Anne false belief task to answer correctly. Based on these stimuli, people can then adjust their mental models of a robot\u2019s abilities.\nBut what are the consequences of robots having mentalizing abilities? Because having a ToM is a mandatory ability to ensure understanding in human communication it seems reasonable to postulate a positive impact of mentalizing abilities on humanrobot interactions. A robot that understands its users can create more meaningful and thus better-rated social interactions. This assumption receives support from empirical studies that manipulated the robot\u2019s mentalizing abilities. Researchers such as Mou et\u00a0al. (2020) could show that a robot having a theory of mind raised people\u2019s trust in it. In their experiment, people revised their decisions more often due to disagreement with the robot in cases when it had ToM abilities and rated the robot\u2019s credibility higher. These results are in line with other studies on mentalizing abilities in human-robot interactions. For example, Benninghoff et\u00a0 al. (2013) observed a similar positive influence of a robot\u2019s ToM-abilities on its social attractiveness. Another example is the experiment conducted by Sturgeon et\u00a0al. (2019) who similarly to Mou et\u00a0al. (2020) manipulated a robot\u2019s ToM-abilities by letting it pass or fail a false belief task. Those robots who passed the false belief task were evaluated significantly better regarding their social intelligence (including recognizing/predicting human cognitions and adapting to/predicting human behavior). However, Sturgeon et\u00a0al. (2019) state in their limitations that future investigations may include differentiations between first-and second-order ToM behavior to extend their findings.\nIn sum, previous studies showed the benefits of implementing and demonstrating robots\u2019 mentalizing abilities. As a result, a given robot will likely be\u00a0evaluated more socially competent and credible, which are important requirements for many servicerelated tasks.\nIn addition, there seems to be\u00a0a difference between implicit and explicit attributions of ToM (Banks, 2021). While the former refers to \u201csubconscious, automatic, spontaneous, nonconceptual, and procedural\u201d mentalizing (Banks, 2021, p.\u00a0 2), the latter is oppositely characterized by more conscious and controlled attribution processes. Banks (2021) results demonstrate that implicit mentalizing processes might be\u00a0similar between robots and humans and that most differences in mentalizing originate from explicit attempts of mind ascription. If the robot\u2019s mentalizing abilities are not salient enough, previous research supports the assumption that users\u2019 explicit attribution of ToM-abilities is lower for robots compared to human interaction partners (Banks, 2020; Thellman et\u00a0al., 2020; Finkel and Kr\u00e4mer, 2022). This observation is similar to the basic assumptions of media equation theory which postulates social but unaware reaction patterns to media entities if they make use of social cues (e.g., natural language use, interactivity, taking social roles)\n(Reeves and Nass, 1996; Nass and Moon, 2000). Likewise, to the different results of implicit and explicit mentalizing, the media equation cannot be\u00a0 measured via explicit measures, since participants produce these social reactions to media automatically and mindlessly (Nass and Moon, 2000).\nBased on the theoretical assumptions about mentalizing processes and previous empirical findings in human-robot interaction, it seems reasonable to postulate a positive influence of a robot\u2019s mentalizing abilities on its credibility ratings. Especially theory-of-mind abilities should create this positive impact, because unlike to first-order beliefs (and no mentalizing abilities) this concept includes the understanding of users\u2019 beliefs and desires, thus making the robot potentially able to take them into consideration for its behavior during user interactions. In order to consider the potential difficulty of explicit users\u2019 responses to robots, we\u00a0will distinguish between explicit and more indirect measures of credibility.\nH1: Social Robots with theory of mind-abilities are rated more credible (indirectly measured [a] and explicitly measured [b]) than robots without ToM-abilities."
        },
        {
            "heading": "Multidimensional source concept",
            "text": "Due to the multilayered communication process enabled by media technologies (such as social robots), the term source has been stretched (Sundar and Nass, 2001). It can for example refer to the producers of a message (such as journalists), to the communication channels (such as websites and newsletters), or even to the receiver of a message as more and more customizations allow for personal selection to which messages one wants to get exposed to. In their experiment, Sundar and Nass (2001) tested for differences in the content evaluation of news stories depending on these source concept types (here: news editors, computers, other users, self). Their results show that although the content remained similar between conditions, participants\u2019 evaluations of it were different. One significant difference was the higher quality rating of the content in combination with overall higher mean values for credibility, liking, and representativeness when the computer was listed as the source compared to news editors. Although not explicitly mentioned by Sundar and Nass (2001), these insights are relevant for interactions with robots as well, because like a computer, social robots can be\u00a0 viewed as the source or the medium of communication.\nMuch of the human-robot interaction research focuses on the robot as a source perspective and analyzes how different behaviors such as for instance fault justification (Correia et\u00a0al., 2018), use of linguistic cues (Andrist et\u00a0al., 2013), or gaze (Stanton and Stevens, 2017) influence the evaluation of social robots as senders of information. Here, the robot is analyzed independently from any intentions of the person(s) providing the robot. Of\nFrontiers in Psychology 04 frontiersin.org\ncourse, there are studies that investigated the difference between source and medium by manipulating the existence of a programmer or person manually steering the robot and people\u2019s orientation toward this person (programmer\u2019s thought). One of them is the study by Yamaoka et\u00a0al. (2007), who described their robot as being controlled either by a human or acting autonomously. Autonomy refers to \u201cthe extent to which a robot can operate in the tasks it was designed for (or that it creates for itself) without external intervention\u201d (Baraka et\u00a0al., 2020, p.\u00a024). This, however, does not include independence from a provider\u2019s intentions or corporate alignments. A completely autonomously acting robot can still be\u00a0 programmed to comply with the demands of the person/company providing it and to only act autonomously within corporate restrictions. Thus, similar to the differentiation of sources made by Sundar and Nass (2001) it needs to be\u00a0distinguished between the programmer/the person steering the robot and the person(s) or organizations providing and using the robot for their interests. To our knowledge, no study has yet addressed this concept of a robot provider\u2019s interests\u2019 influence on the perception and evaluation of an autonomous robot\u2019s credibility. Minor exceptions are the studies by Wang and Krumhuber (2018) who addressed this concept by manipulating a robot\u2019s value/function via textual information. They used a distinction between social value/function and economic value/function to frame the description of the experiment\u2019s humanoid robot. While the social functions described the robot\u2019s dedication to social purposes, the economic function was described as \u201cthe tendency to make financial profits and benefits for the corporate world\u201d (Wang and Krumhuber, 2018, p.\u00a03). Their results consistently showed that robots intended for social purposes were attributed higher emotional abilities than those labeled with an economic function. An explanation for these findings might be\u00a0the manipulative intent that comes into play if economic interests are involved. Companies that buy a robot expect higher revenue/profit because otherwise, they would not have invested in it. Thus, like a digital screen used for advertisement, autonomous, social robots will likely be\u00a0used as marketing tools and for instance, recommend a company\u2019s products to customers. Although such interest for financial profit cannot stem from the robot itself but only from the organization in charge of it, the underlying external manipulative intent may nevertheless reduce people\u2019s attributions of trustworthiness and goodwill toward the robot. Such economically motivated influence has already been investigated in other domains, showing that perceived manipulative intent can exert a negative influence on people\u2019s credibility ratings. Metzger and Flanagin (2013) described that if persuasive intent is perceived, attributions of credibility can decline due to a perception of potential biasedness of the source (persuasive intent heuristic). Building on these insights we\u00a0are interested to see if a robot that is used by an organization with manipulative intent will be\u00a0rated less credible than a robot without these cues of economic interest. To clarify that we\u00a0 are referring to the interests of the person(s) or organization(s) using the robot and not a robot\u2019s or programmer\u2019s\ninterests, we\u00a0will refer to this concept by using the term external manipulative intent.\nH2: External manipulative intent of an organization behind a robot leads to lower levels of attributed credibility for the robot (indirectly measured [a] and explicitly measured [b]) compared to a robot without external manipulative intent.\nAs outlined before, a robot is expected to be\u00a0rated less credible if it is provided by an organization with manipulative intent. But the negative influence of external manipulative intent might be\u00a0even more severe if a robot\u2019s mentalizing abilities are higher pronounced. Although higher mentalizing abilities have been shown to improve evaluations of robots, the salience of external manipulative intent could turn advantages of better user understanding into a perceived benefit for an organization using the robots for exploiting this user understanding. Because of this potential downside of mentalizing abilities being available to organizations with interests different from the users\u2019 we\u00a0postulate the following interaction effect.\nH3: Social robots with higher mentalizing abilities and external manipulative intent of a sponsoring company are rated least credible (indirectly measured [a] and explicitly measured [b]).\nNot all people are affected by persuasion attempts in the same way. Previous works explain that there are interpersonal differences regarding knowledge about sales tactics and manipulative intentions, known as a person\u2019s general persuasion knowledge (Friestad and Wright, 1994). This knowledge is described as helping people to see through persuasion attempts (for instance the strategies of salespeople), thus protecting them from being led to behave in ways they should not want to. Therefore, we\u00a0assume that especially individuals who estimate their general persuasion knowledge to be\u00a0higher will be\u00a0influenced more negatively in their credibility attributions by the presence of an external manipulative intent because they are more likely to detect a manipulation attempt.\nH4: Self-estimated persuasion knowledge moderates the external manipulative intent\u2019s influence on people\u2019s credibility ratings for the robot (indirectly measured [a] and explicitly measured [b]).\nThe concept of mentalizing abilities shows similarities to the concept of empathy as both are focused on gaining insights into people\u2019s inner workings. Schurz et\u00a0 al. (2021) argue that both concepts are related to one another and often co-occur to generate understanding in human interactions. But while mentalizing abilities are focused on cognitive understanding, empathy refers to the understanding of another person\u2019s emotional status. Because these concepts seem intertwined in human development, the question arises, if people draw inference-based conclusions about\nFrontiers in Psychology 05 frontiersin.org\na robot\u2019s empathic understanding based on its mentalizing abilities alone (implicating they assume a relation between both constructs that is similar to the one for humans). Thus, we\u00a0want to investigate to what degree people infer these additional attributions about a social robot\u2019s abilities if only given information about its mentalizing abilities. Thus, we\u00a0ask:\nRQ1: Are a robot\u2019s mentalizing abilities affecting attributed empathic understanding?\nIn addition, we\u00a0want to explore whether giving information on the company providing a robot shifts the source focus away from the robot and to the respective organization. As demonstrated by Sundar and Nass (2001) source perceptions of a computer-mediated message may vary. In cases where the robot is branded by the company\u2019s logo, it seems reasonable to assume that the inferred corporate, manipulative intent raises the salience of the robot as a medium rather than the robot being the source of information. In contrast, if no manipulative intent becomes salient, the robot should be\u00a0regarded more as an adviser than a channel for exerting manipulation. Because of a lack of empirical basis for this assumption, we\u00a0 aim to test this connection as a research question.\nRQ2: Is a robot regarded less as \u201cthe source\u201d and more as \u201cthe medium\u201d in conditions with external manipulative intent?"
        },
        {
            "heading": "Materials and methods",
            "text": ""
        },
        {
            "heading": "Design",
            "text": "To test the hypotheses and research questions a laboratory experiment was conceptualized and conducted at a German university. The laboratory experiment used a fully crossed three (robot\u2019s mentalizing abilities) \u00d7 two (external manipulative intent) between-subjects design, thus having six different conditions. The study was built around a dialogue-based recommendation task during which a humanoid robot (Pepper from Softbank Robotics) had to find a suiting experience voucher for a participant\u2019s selfchosen target person."
        },
        {
            "heading": "Sample",
            "text": "Based on our 3\u00d72 between-subjects design and an expected moderate effect size (Mou et\u00a0al., 2020) power analyses calculated with G*Power 3.1 (Faul et\u00a0 al., 2007) prior to the experiment revealed that about 200 participants are needed for testing our hypotheses on a 95% confidence interval.\nIn total, 203 participants were recruited for the experiment. However, due to server problems, one participant could not take part in the study because the questionnaire\u2019s content was not loaded. Furthermore, one participant was excluded due to a failed awareness check and a third because of serious problems with understanding what to do during the study. For these reasons, the final sample size includes N = 200 participants, equally and\nrandomly assigned to the six experimental groups (n = 33 to n = 34). The majority of participants were female (72%), currently enrolled as students (90%), and mostly undergraduate (72%). Participants\u2019 mean age was M = 23.77 (SD = 6.88) and ranged between 17 and 67 years."
        },
        {
            "heading": "Procedure",
            "text": "The study started in November 2021 and finished in April 2022 after the preregistered termination criterion of 200 valid data cases was met. Approval of the local ethic commission was received and the study was pre-registered at OSF.io.1 Recruiting took place via social media, flyers on the university campus, and interpersonal communication. There were no study-specific participation requirements, besides being at least 18 years old (or having parental permission to participate) as well as a sufficient understanding of the German language.\nAll participants were randomly assigned to one of the six experimental conditions (not blinded). Participants arrived at the laboratory and were briefed about the study\u2019s procedure, signed informed consent, and were seated in front of a laptop showing the first page of the questionnaire (see Figure\u00a01, left). Then they were left alone with the robot in the laboratory as the experimenter (always the same person) left into the neighboring room. The study used a Wizard-of-Oz scenario to create realistic interactions with the robot, i.e., the experimenter controlled the robot from the other room by activating fitting interaction scripts using the software Choregraph 2.5.11.\nThe study\u2019s first task included the demonstration of the robot\u2019s manipulated mentalizing abilities. Therefore, participants read an information text and answered three questions on how the robot will react in three different scenarios illustrated via picture stories (see section \u201cMentalizing abilities\u201d). After finishing all three picture stories, participants were informed about an experience voucher recommendation task. They were instructed to think about a person they want to buy a present for, so that the robot could ask them questions about this target person to find fitting experience vouchers. Right before participants had to turn to the robot to start this interaction, they were informed about it being supported by a third-party organization (company or student research project, see section \u201cExternal manipulative intent\u201d). After reading this vignette participants started the interaction with the robot by moving in front of it (see Figure\u00a0 1, right). First, the robot introduced itself and reminded participants of the respective organization supporting the interaction. During the interaction\u2019s recommendation process, the robot asked each participant five questions about their target person to find good event vouchers for them (e.g., \u201cHow artistically active is your target person?\u201d; see Table\u00a01). The robot\u2019s questions as well as the recommended\n1 https://osf.io/pnmwd/\nFrontiers in Psychology 06 frontiersin.org\nevent vouchers were kept very generic to present the same three experience vouchers to all participants (dinner in the dark, floating, and city trip). These events were chosen because they are suitable for many people and are not restricted to a specific sociodemographic group such as athletic persons. Participants answered the robot\u2019s questions one after another until the robot presented the three experience vouchers as the recommended gifts to buy. After the robot presented and showed the recommended vouchers on its display, it told the participants to continue the study at the computer, where they had to answer three questions referring to the recommended vouchers.\nOn the next pages of the questionnaire, participants were introduced to an alleged quiz game, during which they had to decide for or against help from the robot before answering four true-or-false questions about different facts concerning experience vouchers. When they made their decisions, participants were told that the quiz will be\u00a0postponed and were instructed to instead answer the items on the following pages (sourceness, source credibility, empathic understanding, manipulative intent, and sociodemographic questions). After they finished, instructions appeared on the screen to inform the experimenter. Participants then received the debriefing as well as half an hour of course credit or a small financial compensation and left the laboratory. On average, participants needed about 20 to 25 min to complete the study."
        },
        {
            "heading": "Independent variables",
            "text": ""
        },
        {
            "heading": "Mentalizing abilities",
            "text": "The manipulation of the robot\u2019s mentalizing abilities was implemented via information texts and three picture stories based on different known strategies to demonstrate the extent of mentalizing abilities (false-belief task, white lie scenario, and behavioral intention task; Banks, 2020). The information texts differed between conditions in terms of describing what the\nrobot can do. It either described the robot as having no mentalizing abilities and acting as a mere stimulus\u2013response machine (no mentalizing abilities), it being able to act in its own interest but without understanding other people\u2019s inner states (first-order mentalizing abilities), or it being able to understand that different persons have different inner states and beliefs (second-order mentalizing abilities/theory-of-mind). In addition to these descriptive information texts, three picture stories were used to demonstrate the robot\u2019s mentalizing abilities accordingly. The first picture story was a variation of the Sally-Anne-Test (false-belief-task; see Figure\u00a02), the second was a behavioral intention task, and the third a white lies scenario. After reading each picture story participants had to answer a question about the robot\u2019s next reactions. All three picture stories were shown in the same way in all conditions. However, the answers which were set as correct to the questions were manipulated to be\u00a0fitting for each condition.\nAll picture stories can be\u00a0 seen on OSF.io and will be\u00a0exemplified by the first story here (see Table\u00a02 and Figure\u00a02). In conditions without mentalizing abilities the correct answer for the false-belief-task stated that the robot is not able to tell where person A will look for the muffin because itself cannot see any muffin. In first-order mentalizing conditions the correct answer included the robot saying that person A will look at the correct place of the hidden muffin (turquoise box) and thus not recognizing person A\u2019s false beliefs. Lastly, theory of mind conditions assumed the robot to know about the false beliefs of person A. For this reason, the correct answer was statement number three declaring that the robot expects person A to look in the (wrong) white box for the muffin. To make sure participants internalized what the robot can understand in each condition, these questions were programmed to reload the page if a false answer was given. This way participants were forced to rethink their answers and thus learned about the robot\u2019s mentalizing abilities until they got the correct answer to continue with the next picture story.\nFIGURE\u00a01 Laboratory setting: Participants worked on the questionnaire sitting at the desk and turned to the robot for the recommendation task while the experimenter was in the room next door.\nFrontiers in Psychology 07 frontiersin.org"
        },
        {
            "heading": "External manipulative intent",
            "text": "The manipulation of the external manipulative intent was realized via vignettes about a fictitious organization called experiencevouchers.com. (translated from German for publication) supporting the robot\u2019s recommendation of experience vouchers for a self-chosen target person. In half of the conditions, this fictitious organization was described as a private and profit-oriented company selling experience vouchers and in the other half, it was labeled as a students\u2019 research project that focuses on helping customers to find good experience vouchers. In both cases, participants had to read a brief information text about the respective organization prior to the recommendation task. To further raise awareness, the respective organization\u2019s logo was shown on the robot\u2019s tablet (1280\u00d7800 pixels) during the interaction (see Figure\u00a0 3). Furthermore, the robot was programmed to mention the organization several times during its introduction and final presentation of the recommended experience vouchers.\nThe second part of the external manipulative intent manipulation included the marking of the robot\u2019s recommended vouchers as being exclusively from the private company (see text footnote 2) in conditions with external manipulative intent or from different companies/brands in conditions without external manipulative intent (see Figure\u00a04). In all conditions it was stated multiple times that the robot will consider vouchers from different\nvendors and that the only criterion of choice is a voucher\u2019s fit for the target person. This way, participants would only receive experience vouchers from the sponsoring company and from no other vendor, thus creating the impression of a manipulative intent. This manipulation was successfully supported by a pre-test (N = 69). External manipulative intent was significantly higher in conditions with a private company (M = 5.30, SD = 0.76) than in conditions with a student research project supporting the interaction with the robot (M = 4.30, SD = 1.23) (p = 0.007) measured via four items on a 7-point Likert scale (experiencevouchers.com supported the interaction with the robot primarily to promote its products\u201d/\u201c\u2026 had mostly good intentions in supporting the interaction with the robot.\u201d/\u201d\u2026 supported the interaction with the robot primarily because they care about the selection of suitable gifts.\u201d/\u201d\u2026 supported the interaction with the robot primarily to advance own goals.\u201d; 1 \u2013 do not agree at all to 7 \u2013 totally agree), adapted from Rodgers (2007)."
        },
        {
            "heading": "Measurements",
            "text": ""
        },
        {
            "heading": "Explicit credibility measures",
            "text": "Participants\u2019 perceptions of the robot\u2019s credibility were measured explicitly using the Source Credibility Measures (McCroskey and Teven, 1999). Its three subscales competence\nTABLE\u00a01 Translated script of the robot\u2019s statements and questions during the main interaction task.\n\u201cHello, my name is Pepper. Today I\u00a0have the task to select suitable experience vouchers for your target person\u201d\nExternal manipulative intent: \u201cI am\u00a0supported by provider of experience vouchers of all\nkinds. Of course, the recommendation process will also take into account experience\nvouchers from other companies if they are suitable for your target person.\u201d\nNo external manipulative intent: \u201cI am\u00a0supported by a research project by students,\nwhich is intended to support consumers in the purchase of experience vouchers. In\nthe recommendation process, experience vouchers from the range of different\ncompanies are therefore considered equally if they seem suitable for your target\nperson.\u201d\n\u201cIn order to be\u00a0able to recommend something to you, I\u00a0will ask you\u00a0questions about your target person, which you\u00a0should answer. Let us start with the first question. Please\ndescribe in a few sentences: how artistically active is your target person.\u201d\n[Participant answers]\n\u201cOkay, thank you\u00a0very much for your answer. Next question: how athletic is your target person? Again, please describe your target person in a few sentences, this time\nreferring to the target\u2019s person athleticism.\u201d\n[Participant answers]\n\u201cOkay, that\u2019s enough of an answer, thank you\u00a0very much. Let us move on to the third question: how much do you\u00a0like your target person? Or in other words, do you\u00a0like\nspending time with him/her.\u201d\n[Participant answers]\n\u201cAgain, thank you. We\u2019re getting there, too. But please tell me first: how old is your target.\u201d\n[Participant answers]\n\u201cAgain, thank you\u00a0for your answer. Now let us get to the last question: how much experience does your target already have with experience vouchers? Has she perhaps\nalready received experience vouchers as a gift.\u201d\n[Participant answers]\n\u201cThank you\u00a0very much for your answer. This is enough information to recommend suitable vouchers. The voucher recommendations will pop up on my screen in a few\nseconds.\u201d\nExternal manipulative intent: \u201cThe recommendations are as follows: A voucher for a\ndinner in the pitch dark. A saltwater relaxation bath or a 2-day city trip, all from the\noffer. Quite clearly the vouchers are most suitable as gift for your target person, in\ncomparison to the vouchers of experience voucher sellers.\u201d\nNo external manipulative intent: \u201cThe recommendations are as follows: A voucher\nfor a dinner in the pitch dark. A saltwater relaxation bath or a 2-day city trip, all\nfrom the offer of experiencevouchers.com.\u201d\n\u201cWell, that concludes the interaction task. Now please go back to the computer and click \u2018Next\u2019 to continue the study. I\u00a0wish you\u00a0much enjoyment.\u201d\nFrontiers in Psychology 08 frontiersin.org\n(\u03b1 = 0.76), trustworthiness (\u03b1 = 0.77), and goodwill (\u03b1 = 0.69) were measured with 6 items each (18\u00a0in total) via a semantic differential\n[e.g., (1) Incompetent \u2013 (7) Competent, (1) Untrustworthy \u2013 (7) Trustworthy, (1) Self-centered \u2013 (7) Not self-centered].\nFIGURE\u00a02 First of three picture stories shown to demonstrate the robot\u2019s mentalizing abilities (false-belief-task).\nFrontiers in Psychology 09 frontiersin.org"
        },
        {
            "heading": "Indirect credibility measures",
            "text": "Three indirect credibility measures were added to measure participants\u2019 evaluations of the robot\u2019s recommendations. This included participants\u2019 ratings of the recommended vouchers\u2019 fit for their target person (\u201cHow likely is it that the experience vouchers recommended by Pepper might appeal to your target person?\u201d) and their purchase intention \u201cHow likely is it (...) that you\u00a0would consider one of Pepper\u2019s recommended experience vouchers as a gift for your target person?,\u201d \u201cHow likely is it (...) that you\u00a0would buy one of Pepper\u2019s recommended experience vouchers as a gift for your target person?,\u201d (\u03b1 = 0.84). All three items were answered on a 7-point Likert scale (1 \u2013 very unlikely to 7 \u2013 very likely).\nIn addition to these metric measures, participants\u2019 willingness to get help from the robot during a quiz game was assessed. This quiz game included four yes-or-no questions related to the topic of experience vouchers. Participants were told to receive 10 points for answering a question correctly (40 points in total) and that for every 10 points gathered, 1\u20ac will be\u00a0 donated to a local animal shelter (4\u20ac in total). Since all questions were estimation questions (e.g., \u201cAre approx. 50% of all skydive-vouchers given away not redeemed (as of 2019)?\u201d),\nthe average rate of correct answers in case of guessing was two out of four (i.e., 20 points/2\u20ac). After these rules were explained, the robot was offered to serve as a joker, giving hints for each of the four questions in exchange for 10 minus points in advance. So, participants needed to decide whether they expect the robot to offer helpful tips to answer all questions (and thus getting 40\u201310 = 30 points/3\u20ac) or to rely on their own guesses to beat the average of 20 points. Their willingness to get help from the robot was measured via a binary yes-or-no question (\u201cBefore answering the questions above, would you\u00a0like to give 10 points to get hints from robot Pepper on all 4 questions?\u201d \u2013 Yes/No)."
        },
        {
            "heading": "Dispositional persuasion knowledge",
            "text": "Dispositional Persuasion Knowledge was measured via a six-item scale taken from Bearden et\u00a0al. (2001), e.g., \u201cI have no trouble understanding the bargaining tactics used by salespersons.\u201d (7-point Likert scale: 1 \u2013 extremely uncharacteristic for me to 7 \u2013 extremely characteristic for me; \u03b1 = 0.66)."
        },
        {
            "heading": "Empathic understanding",
            "text": "Empathic understanding was measured via eight items from Charrier\u2019s and colleagues\u2019 RoPE scale (2019), e.g., \u201cThe robot appreciates exactly how the things I\u00a0 experience feel to me.\u201d (7-point-Likert scale: 1 \u2013 do not agree at all to 7 \u2013 totally agree; \u03b1 = 0.81). The items had to be\u00a0answered together with four filler items created by the scale\u2019s authors (e.g., \u201cThe robot is responsible for its actions.\u201d)."
        },
        {
            "heading": "Sourceness",
            "text": "Sourceness was measured via a self-created sentencecompletion task. Participants had to complete three sentences with one out of seven constant answering options. The third question included the relevant sentence to be\u00a0completed: \u201c\u2026 made the decision which vouchers were presented to me.\u201d (brand name 1, brand name 2, the robot, experiencevouchers.com, the university, the sponsor, nobody). Here, participants had to choose if the robot acted as the decision maker or if these decisions were made by an institution only making use of the robot to communicate them."
        },
        {
            "heading": "Sociodemographic questions",
            "text": "Lastly, participants were asked sociodemographic questions about their age, sex, education, and occupation. Furthermore, they had to indicate if they had interacted with the study\u2019s robot already before this study and if yes indicate on which occasion (s) in an open text field."
        },
        {
            "heading": "Manipulation and awareness check",
            "text": "The external manipulative intent of the organization supporting the recommendation task was measured as a manipulation check via the same four items used during the pre-test (Rodgers, 2007; \u03b1 = 0.63). Unfortunately, due to a mistake, the scale was only included in one of the six\nTABLE\u00a02 Question, possible answers, and related conditions for the first picture story (Figure\u00a02).\nWhat answer is the robot likely to give to the following question? \u201cWhere will person A look first for the muffin?\u201d\nAnswer Condition in which this answer is correct\n1. Pepper: \u201cThere is no muffin to\nbe\u00a0seen. No answer possible to the\nquestion\u201d\nNo mentalizing\n2. Pepper: \u201cSince the muffin is in the\nturquoise box, person A will look\nhere\u201d\nFirst-order mentalizing\n3. Pepper: \u201cSince person A did not see\nwhere the muffin was put, person A\nwill probably look in the white box\u201d\nSecond-order mentalizing\nFIGURE\u00a03 Logos of the two fictitious organizations which were shown on the robot\u2019s tablet to raise awareness for their pretended support of the robot interaction. Company (top) vs. student research project (bottom).\nFrontiers in Psychology 10 frontiersin.org\nexperimental conditions for the first 111 participants. Thus, only from participant number 112 onward this scale was integrated and answered by participants from all conditions. In addition to this manipulation check, at the end of the study, participants were asked to indicate the kind of organization they were confronted with during the robot interaction (company, student research project, website of the consumer protection center, do not know anymore).\nLastly, to identify inattentive participants the study included an awareness check: \u201cIf you\u00a0are working on the study attentively, please select \u201c5\u2033 here.,\u201d embedded into the empathic understanding scale near the end of the questionnaire."
        },
        {
            "heading": "Results",
            "text": "All analyses were calculated using SPSS 27 and were tested on a 5% level of significance."
        },
        {
            "heading": "Manipulation check",
            "text": "The manipulation check for the external manipulative intent was successful. A t-test for independent samples revealed\nsignificant differences in the perceived intention of the organization using the robot being a student research project recommending different brands (M = 4.32) or a company recommending only its own products (M = 4.82) [t (89) = \u22122.59, p = 0.011]. In addition, 146 of 200 participants remembered the organization\u2019s type correctly on the last page of the study\u2019s questionnaire (company or student research project respectively).\nHypotheses 1 to 3\nHypotheses 1 and 2 assumed main effects of the experiment\u2019s independent variables mentalizing abilities and external manipulative intent on the explicitly and indirectly measured credibility constructs, with mentalizing abilities (especially theory-of-mind abilities) supporting and external manipulative intent impairing credibility attributions. Hypothesis 3 extended these assumptions by describing an interaction effect between these two independent variables, expecting higher mentalizing abilities to cause negative effects on credibility attributions given that an external manipulative intent is present. To test for these main and interaction effects a two-factor MANOVA was calculated including the credibility constructs competence, goodwill, and\nFIGURE\u00a04 Recommended experience vouchers for conditions with external manipulative intent of the supporting organization (top) and for conditions without (bottom). Adapted/reproduced from https://pixabay.com.\nFrontiers in Psychology 11 frontiersin.org\ntrustworthiness as well as expected voucher fit, purchase intention as dependent variables and persuasion knowledge as a covariate. The inferential statistical results revealed neither a significant main effect of mentalizing abilities [H1: F(10,382) = 0.950, p = 0.487, Wilk\u2019s \u03bb = 0.952, partial \u03b72 = 0.02], nor of external manipulative intent [H2: F(5,190) = 1.026, p = 0.307, Wilk\u2019s \u03bb = 0.974, partial \u03b72 = 0.03]. See Table\u00a03 for an overview of the mean values for the metric dependent measures.\nIn addition, a Chi2-test was calculated to analyze for differences in the willingness to get help from the robot for the quiz game between conditions. Table\u00a04 shows the distributions of Yes counts for each condition. Again, no significant effect of the two experimental conditions was detected (p = 0.448). Thus, in sum, no support for Hypotheses 1 and 2 was found.\nHypothesis 3 assumed an interaction effect between mentalizing abilities and external manipulative intent. Two significant interactions related to attributions of goodwill (p = 0.034, partial \u03b72 = 0.03) and trustworthiness (p = 0.038, partial \u03b72 = 0.03) were found during the calculated two-factor MANOVA (see Figures\u00a0 5, 6). Attributions of goodwill and trustworthiness were slightly higher expressed in first-order than in no mentalizing conditions if the external manipulative intent was absent (inversely in conditions with external manipulative intent). In second-order mentalizing conditions, however, the mean values between conditions were almost similar. Thus, although the results slightly point in the direction of the hypothesis, the inconclusive interaction pattern forces us to dismiss Hypothesis 3.\nHypothesis 4\nHypothesis 4 assumed a moderation effect of persuasion knowledge on the relationship between external manipulative intent and credibility, expected voucher fit, and purchase intention. Using the PROCESS v.4.1 macro (Hayes, 2017) we\u00a0 analyzed this relationship for each dependent variable. However, none of the analyzed moderation models showed a\nsignificant influence of people\u2019s self-estimated persuasion knowledge on credibility or its related constructs competence (p = 0.469), goodwill (p = 0.343), trustworthiness (p = 0.296), expected voucher fit (p = 0.524), and buying attention (p = 0.349). Thus, like the previous investigation on the main effects, the anticipated moderation described in Hypothesis 4 receives no support from our experiment\u2019s data (see Table\u00a05 for an overview).\nTABLE\u00a03 Summary of the mean values for the metric, dependent measures.\nNo EMI EMI No EMI EMI No EMI EMI\nNo mentalizing First-order mentalizing Second-order mentalizing\nCredibility Comp. 5.64 5.41 5.80 5.53 5.80 5.77\nTrustworth. 4.86 4.90 5.02 4.42 4.98 4.90\nGoodwill 4.30 4.45 4.59 4.33 4.79 4.47\nVoucher fit 5.47 5.48 5.67 4.88 5.30 5.39\nPurchase intention 5.33 5.50 5.41 4.74 5.54 5.33\nEmpathic understanding 3.14 2.98 3.42 3.20 3.50 3.63\nEMI, ext. manipulative intent.\nTABLE\u00a04 Distribution of yes responses referring to the question if participants want help from the robot.\n\u201cWould you like to give 10 points to get tips from robot Pepper on all 4 questions?\u201d\nNo ext. manipulative Intent Ext. manipulative intent\nNo mentalizing Yes = 20 Yes = 16\nFirst-order\nmentalizing\nYes = 26 Yes = 24\nSecond-order\nmentalizing\nYes = 19 Yes = 26\n1\n2\n3\n4\n5\n6\n7\nNo External\nManipulative Intent\nExternal Manipulative\nIntent\nNo Mentalizing Abilites (MA) First-Order MA Second-Order MA\nFIGURE\u00a05 Mean values for participants\u2019 attribution of goodwill to the robot.\nFrontiers in Psychology 12 frontiersin.org"
        },
        {
            "heading": "Research questions",
            "text": "The first research question addressed the relationship between a robot\u2019s alleged mentalizing abilities and users\u2019 attributions of\nempathic understanding to the robot. An analysis of variance with mentalizing abilities as a single factor and emphatic understanding as dependent measure revealed significant differences between these three experimental conditions [F = (2,197) = 6.42, p = 0.002, \u03b72 = 0.06; Mno mentalizing = 3.08, Mfirst-order mentalizing = 3.45, Msecond-order mentalizing = 3.70]. Subsequent post-hoc tests revealed that especially second-order and no mentalizing conditions varied significantly, indicating that with a robot\u2019s increasing mentalizing abilities participants associated higher empathic understanding with it. In addition, empathic understanding was (mediocrely) correlated with the study\u2019s main dependent measures credibility competence (r = 0.41), goodwill (r = 0.62), trustworthiness (r = 0.48) as well as participant\u2019s expected voucher fit (r = 0.25), and purchase intention (r = 0.26).\nFor the second research question, it was investigated whether the external manipulative intent caused any differences in participants\u2019 sourceness perceptions related to the robot. We\u00a0anticipated the robot to be\u00a0regarded more as a medium than as the source of information if the voucher recommendation task was framed as being supported by a profit-oriented company. By using a Chi2-test to analyze the distribution of the given answers of the sentence-completion task, however, no significant differences could be\u00a0found between conditions with or without external manipulative intent (p > 0.05, see Table\u00a06 for an overview of the selected answers). Thus, no support for our anticipated difference in source perception was obtained."
        },
        {
            "heading": "Discussion",
            "text": "The study\u2019s aim was to investigate how people\u2019s evaluations of a robot with different mentalizing abilities change when confronted with a situation during which the robot is used for the interest of an organization with own financial interests. For this research aim, we\u00a0designed a laboratory experiment during which we\u00a0let 200 participants interact with a social humanoid robot as a recommender for experience vouchers. This interaction with the robot was either framed as the robot being used by a student research project or by a fictitious company recommending only its own experience vouchers via the robot.\nIn sum, we\u00a0 could not identify differences in credibility attributions toward the robot based on the hypothesized main\nTABLE\u00a06 Distribution of given answers for the question who decided which voucher will be\u00a0presented.\nQuestion 3: \u201c\u2026 made the decision which vouchers were presented to me.\u201d\nNo ext. manipulative\nintent\nExt. manipulative intent\nThe robot n\u00a0= 85 n\u00a0= 83\nThe university n\u00a0= 2 n\u00a0= 3\nThe sponsor n\u00a0= 0 n\u00a0= 2\nNobody n\u00a0= 2 n\u00a0= 0\n1\n2\n3\n4\n5\n6\n7\nNo External\nManipulative Intent\nExternal Manipulative\nIntent\nNo Mentalizing Abilites (MA)\nFirst-Order MA\nSecond-Order MA\nFIGURE\u00a06 Mean values for participants\u2019 attribution of trustworthiness to the robot.\nTABLE\u00a05 Summary of results for the tests of hypotheses."
        },
        {
            "heading": "Hypotheses Confirmed/Rejected",
            "text": "H1: Social Robots with theory of mind-\nabilities are rated more credible\n(indirectly measured [a] and explicitly\nmeasured [b]) than robots without\nToM-abilities\nRejected (p\u00a0> 0.05)\nH2: External manipulative intent of an\norganization behind a robot leads to\nlower levels of attributed credibility for\nthe robot (indirectly measured [a] and\nexplicitly measured [b]) compared to a\nrobot without external manipulative\nintent\nRejected (p\u00a0> 0.05)\nH3: Social robots with higher\nmentalizing abilities and external\nmanipulative intent of a sponsoring\ncompany are rated least credible\n(indirectly measured [a] and explicitly\nmeasured [b])\nRejected (p\u00a0> 0.05)\nH4: Self-estimated persuasion\nknowledge moderates the external\nmanipulative intent\u2019s influence on\npeople\u2019s credibility ratings for the robot\n(indirectly measured [a] and explicitly\nmeasured [b])\nRejected (p\u00a0> 0.05)\nFrontiers in Psychology 13 frontiersin.org\neffects of mentalizing abilities or external manipulative intent, neither regarding explicit measures nor indirect ones. Although the study successfully created the impression of an external manipulative intent, this setting did not cause differences in credibility ratings, expected product fit, participants\u2019 purchase intention, or willingness to get help from the robot in a subsequent task. Similarly, the manipulation of the robot\u2019s mentalizing abilities did not result in significant differences with respect to these measurements as well. Only a small interaction effect between mentalizing abilities and external manipulative intent was observed regarding the robot\u2019s goodwill and trustworthiness. However, due to its inconclusive nature, this effect might be\u00a0the product of chance and does not allow us to draw meaningful conclusions.\nIn contrast to the rejected main hypotheses, the first research question revealed a significant difference in attributed empathic understanding to the robot depending on the extent of its mentalizing abilities. However, no differences in terms of source perception induced by the external manipulative intent manipulation could be\u00a0detected."
        },
        {
            "heading": "Theoretical implications",
            "text": "As summarized before and contrary to previous studies (Sturgeon et\u00a0al., 2019; Mou et\u00a0al., 2020) no effects of a robot\u2019s mentalizing abilities on credibility attributions were found. Despite a prior power analysis and more participants in single conditions compared to those previous experiments, we\u00a0could not detect the assumed effects of credibility-supporting mentalizing abilities. However, it is important to consider that the manipulation of mentalizing abilities did cause differences in attributions of empathic understanding related to the robot, with higher abilities being related to higher attributions of empathic understanding. Most likely participants used the depiction of the robot\u2019s mental processes as a reference for how much empathy they ascribe to it. Literature has pointed out that both these constructs are highly related, and their influence can be\u00a0intertwined (Schurz et\u00a0al., 2021), supporting the associative results found in this study. In addition, we\u00a0observed moderate correlations between empathic understanding and explicit and indirect credibility attributions. On the one hand, these correlations are plausible as we\u00a0 assumed a robot\u2019s ability to understand its users to affect credibility ratings, on the other hand, this partly contradicts the nonsignificant influence of mentalizing abilities on credibility we\u00a0found as well.\nA possible explanation for this result is, that the robot\u2019s credibility was rather rated on interaction-based criteria (for example its choice of words or the fit of its reactions to participants\u2019 answers) and not on the demonstration of its mentalizing abilities via text and picture stories prior to the interaction. It has been pointed out before that observing a robot\u2019s actual behavior is of higher importance for evaluating it than secondary sources (Horstmann and Kr\u00e4mer, 2020). Hence, a live demonstration of\nthe robot\u2019s mentalizing abilities could assumably have had a greater effect on people\u2019s credibility ratings and behavioral intentions. This might imply, that learning moderately important information about a robot before an interaction with it (especially if it\u2019s for the first time), is a non-recommendable procedure to ensure that this information gets considered by users.\nHowever, the studies by Sturgeon et\u00a0 al. (2019) and Mou et\u00a0 al. (2020) found an impact of mentalizing abilities on credibility using this procedure of prior demonstration via video. One reason for this difference to the current study could be\u00a0 the different types of interaction that followed the initial exposure to information about the robot. While the study conducted by Sturgeon et\u00a0al. (2019) was an online experiment, that only included the manipulation videos and no interactive task, the study by Mou et\u00a0al. (2020) additionally used a price estimation game as the main task where participants had to make several small price decisions to which the robot either agreed or disagreed. In contrast, our main task included a dialogue during which the robot asked the participants mostly open-ended questions (see Table\u00a01). Thus, our interaction was less-repetitive and routinized, forcing participants to decide on their own what type of answer they give in terms of length, details, and complexity when the robot asked its questions (e.g., regarding the question \u201cHow athletic is your target person?\u201d some participants simply answered by referring to the questions\u2019 wording and said, \u201cnot athletic\u201d or \u201cvery athletic,\u201d while other participants explained in detail, what kind of sport their target person is doing currently in combination with details on frequency). Thus, this task was probably more attentiondemanding task which could be\u00a0 a possible reason why the information learned beforehand did not become as relevant as during the study of Mou et\u00a0al. (2020).\nIn addition, because this study used the Wizard-of-Oz technique, the robot\u2019s answers to all questions were pre-scripted and thus identical and generally applicable to almost all participants\u2019 answers. The robot always thanked the participants for each answer and continued with the next question, regardless of the answer\u2019s length, detail, or complexity. Our intention was to keep the dialogue as similar as possible between all participants and still create realistic social interactions. However, participants most likely updated their mental model of the robot differently based on its reaction to their answers. If a participant delivered a complex answer and experienced the robot to understand it, this probably led to higher expectations of its abilities. On the other side, participants who gave the shortest possible answers only (e.g., \u201cnot athletic\u201d) were less likely to form the same expectations of the robot\u2019s abilities. For this reason, the study\u2019s main task may have created too much unsystematic variance because it left too much room for different, unintentional experiences while using the robot. However, social interactions are far more complex than our scripted interaction. Therefore, the effect of prior information on a robot\u2019s abilities might be\u00a0even less effective in real-world scenarios, unless they are highly important for the interaction (such as safety information).\nFrontiers in Psychology 14 frontiersin.org\nRegarding the manipulation of external manipulative intent, we\u00a0did not detect participants to behave differently toward a social robot being associated with an either profit-or non-profit organization using it, even despite considering their general persuasion knowledge during our calculations. These results suggest again that participants blanked out background information when they engaged with the social robot and put a higher focus on the robot and the interaction itself than on the organization making use of it. As social robots are rarely seen in people\u2019s everyday life (over 90% of participants did not interact with this robot before the study) this could have caused their attention to be\u00a0mainly focused on the robot itself, despite each organization\u2019s logo being continuously shown on the robot\u2019s screen as a reminder of the manipulation. The higher focus on the robot would also help to explain why 84% of all participants considered the robot itself to have made the decision which vouchers were recommended (thus being seen as the source of information), independent from the manipulation.\nParticipants\u2019 indifference in their reactions seems concerning regarding the influential potential of social robots as marketing tools. If people are equally likely to consider recommendations from a company or from non-profit organizations through the medium \u2018robot\u2019, this allows for manipulative advertising strategies. However, two counterarguments must be\u00a0considered. First, participants did detect higher external manipulative intent when it was present in our study. Secondly, the salience of the organization was not as high as it would be\u00a0during non-experimental human-robot interactions. The study took place in a neutral laboratory at a university and many of its students participated it (this argument receives support from the slightly higher mean values for the company\u2019s manipulative intent during the pre-test compared to the laboratory study). People might pay more attention to the organization using the robot if the robot is used in the organization\u2019s facilities (e.g., the store where the recommended products are sold) and the salience of the organization using the robot is therefore high.\nIn addition, no conclusive interaction effect was found, in line with our assumption that higher mentalizing abilities are becoming detrimental in situations with external manipulative intent. This is because, although there are small differences for example in trustworthiness between first-order mentalizing conditions with or without external manipulative intent, we\u00a0cannot explain the non-appearance of this effect in no or second-order mentalizing conditions. Thus, the significant interaction effects regarding trustworthiness and goodwill do not allow to claim that either higher or lower mentalizing abilities are beneficial in situations with or without external manipulative intent. Especially due to the nonsignificant main effects, the low effect sizes, and small mean differences we\u00a0conclude that these interaction effects are more likely the product of chance than signs of a meaningful interaction pattern."
        },
        {
            "heading": "Limitations",
            "text": "One major limitation of our study is the fact that the robot\u2019s comments on participants\u2019 given answers during the voucher recommendation task were the same for each condition. Its sentences were created to be\u00a0 suited for every mentalizing condition, not given the participants any different information about the robot\u2019s mental abilities than already shown before in the information texts and picture stories. However, this led to some minor problems during the conversation, e.g., the robot continuously referring to \u201cthe target person\u201d even if participants mentioned their target person\u2019s name or their relationship status. In such cases, the robot might have been not living up to the expectations of a robot that was shown to have a theory of mind.\nFurthermore, the robot\u2019s mentalizing abilities were only indirectly demonstrated by the information texts and picture stories. People had to guess the robot\u2019s next behavior/thoughts in each picture story\u2019s narrative based on the respective information text and could only continue the study when they made the right guess about what the robot is going to do/think (otherwise they had to answer each question again). Thus, our participants only indirectly learned about the robot\u2019s abilities but did not see or experience the robot\u2019s mentalizing behavior themselves. Additionally, the study took place in a university lab without a real company, real products to purchase and only limited consequences for participants to experience disadvantages from relying on the presented recommendations. Finally, the study\u2019s convenience sample included majorly undergraduate female students which restricts us from drawing too general conclusions from our experiment."
        },
        {
            "heading": "Conclusion",
            "text": "An experimental, laboratory study was conducted to investigate if social robots\u2019 mentalizing abilities that have been shown to increase credibility attributions in past research can have opposite, detrimental effects on a robot if it is being used by an organization with manipulative intent. Contrary to our assumptions, statistical results suggest that mentalizing abilities only affected empathic understanding but neither explicit nor indirect credibility attributions to our humanoid robot. Additionally, despite a successful manipulation check, the external manipulative intent of an organization using the robot did not create any differences in participants\u2019 evaluations or behavioral intentions related to the robot. Thus, in contrast to previous empirical findings, our research supports the assumption, that mentalizing abilities and credibility evaluations are not as closely related as expected. Furthermore, we\u00a0did not detect a negative interaction effect when high mentalizing abilities and an external manipulative intent are present in combination. This lets us suggest that people rather consider primary information communicated by the robot than secondary\nFrontiers in Psychology 15 frontiersin.org\nmeta-information about the organization behind it or the robot\u2019s abilities."
        },
        {
            "heading": "Data availability statement",
            "text": "The datasets (analyzed) for this study and its pre-study can be found in the OSF Repository (https://osf.io/pnmwd/)."
        },
        {
            "heading": "Ethics statement",
            "text": "The study, involving human participants, was reviewed and approved by the Ethics Committee of the Department of Computer Science and Applied Cognitive Science, Faculty of Engineering. Written informed consent to participate in this study was provided by each participant (and in one case the legal guardian). The individual(s) provided their written informed consent for the publication of any identifiable images or data presented in this article."
        },
        {
            "heading": "Author contributions",
            "text": "MF conceptualized the experiment, collected the data, carried out the data analyses, and wrote the paper. NK supported the experiment by advising and revising during the whole process\nfrom conception to submission. All authors contributed to the article and approved the submitted version."
        },
        {
            "heading": "Funding",
            "text": "The study only received internal financial support from the budget of the social psychology chair (University of DuisburgEssen) where the experiment was conducted."
        },
        {
            "heading": "Conflict of interest",
            "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be\u00a0construed as a potential conflict of interest."
        },
        {
            "heading": "Publisher\u2019s note",
            "text": "All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher."
        }
    ],
    "title": "The robotic mentalist \u2013 On the influences of robots\u2019 mentalizing abilities and external manipulative intent on people\u2019s credibility attributions",
    "year": 2022
}