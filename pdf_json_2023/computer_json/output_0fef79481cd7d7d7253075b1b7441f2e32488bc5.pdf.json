{
    "abstractText": "Given a (machine learning) classifier and a collection of unlabeled data, how can we efficiently identify misclassification patterns presented in this dataset? To address this problem, we propose a human-machine collaborative framework that consists of a team of human annotators and a sequential recommendation algorithm. The recommendation algorithm is conceptualized as a stochastic sampler that, in each round, queries the annotators a subset of samples for their true labels and obtains the feedback information on whether the samples are misclassified. The sampling mechanism needs to balance between discovering new patterns of misclassification (exploration) and confirming the potential patterns of classification (exploitation). We construct a determinantal point process, whose intensity balances the explorationexploitation trade-off through the weighted update of the posterior at each round to form the generator of the stochastic sampler. The numerical results empirically demonstrate the competitive performance of our framework on multiple datasets at various signal-to-noise ratios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bao Nguyen"
        },
        {
            "affiliations": [],
            "name": "Viet Anh Nguyen"
        }
    ],
    "id": "SP:b2791301fce51ea00b421086d4feddbac33ab4d0",
    "references": [
        {
            "authors": [
                "Raja Hafiz Affandi",
                "Emily Fox",
                "Ryan Adams",
                "Ben Taskar"
            ],
            "title": "Learning the parameters of determinantal point process kernels",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "David Alvarez-Melis",
                "Nicolo Fusi"
            ],
            "title": "Geometric dataset distances via optimal transport",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jordan T Ash",
                "Chicheng Zhang",
                "Akshay Krishnamurthy",
                "John Langford",
                "Alekh Agarwal"
            ],
            "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
            "year": 1906
        },
        {
            "authors": [
                "Alexei Borodin",
                "Eric M Rains"
            ],
            "title": "Eynard\u2013Mehta theorem, Schur process, and their Pfaffian analogs",
            "venue": "Journal of Statistical Physics,",
            "year": 2005
        },
        {
            "authors": [
                "Maria R Brito",
                "Edgar L Ch\u00e1vez",
                "Adolfo J Quiroz",
                "Joseph E Yukich"
            ],
            "title": "Connectivity of the mutual k-nearestneighbor graph in clustering and outlier detection",
            "venue": "Statistics & Probability Letters,",
            "year": 1997
        },
        {
            "authors": [
                "Samuel Budd",
                "Emma C Robinson",
                "Bernhard Kainz"
            ],
            "title": "A survey on active learning and human-in-the-loop deep learning for medical image analysis",
            "venue": "Medical Image Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Simon Caton",
                "Christian Haas"
            ],
            "title": "Fairness in machine learning: A survey",
            "venue": "arXiv preprint arXiv:2010.04053,",
            "year": 2020
        },
        {
            "authors": [
                "Laming Chen",
                "Guoxin Zhang",
                "Eric Zhou"
            ],
            "title": "Fast greedy MAP inference for determinantal point process to improve recommendation diversity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sangwoo Cho",
                "Chen Li",
                "Dong Yu",
                "Hassan Foroosh",
                "Fei Liu"
            ],
            "title": "Multi-document summarization with determinantal point processes and contextualized representations",
            "year": 1910
        },
        {
            "authors": [
                "Ali Civril",
                "Malik Magdon-Ismail"
            ],
            "title": "On selecting a maximum volume sub-matrix of a matrix and related problems",
            "venue": "Theoretical Computer Science,",
            "year": 2009
        },
        {
            "authors": [
                "Greg d\u2019Eon",
                "Jason d\u2019Eon",
                "James R Wright",
                "Kevin Leyton-Brown"
            ],
            "title": "The spotlight: A general method for discovering systematic errors in deep learning models",
            "venue": "ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2022
        },
        {
            "authors": [
                "Greg d\u2019Eon",
                "Jason d\u2019Eon",
                "James R. Wright",
                "Kevin Leyton-Brown"
            ],
            "title": "The spotlight: A general method for discovering systematic errors in deep learning models",
            "venue": "ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2022
        },
        {
            "authors": [
                "Sabri Eyuboglu",
                "Maya Varma",
                "Khaled Saab",
                "Jean-Benoit Delbrouck",
                "Christopher Lee-Messer",
                "Jared Dunnmon",
                "James Zou",
                "Christopher R\u00e9"
            ],
            "title": "Domino: Discovering systematic errors with cross-modal embeddings",
            "venue": "arXiv preprint arXiv:2203.14960,",
            "year": 2022
        },
        {
            "authors": [
                "Mike Gartrell",
                "Ulrich Paquet",
                "Noam Koenigstein"
            ],
            "title": "Lowrank factorization of determinantal point processes",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Boqing Gong",
                "Wei-Lun Chao",
                "Kristen Grauman",
                "Fei Sha"
            ],
            "title": "Diverse sequential subset selection for supervised video summarization",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Xinru Hua",
                "Truyen Nguyen",
                "Tam Le",
                "Jose Blanchet",
                "Viet Anh Nguyen"
            ],
            "title": "Dynamic flows on curved space generated by labeled data",
            "venue": "In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Kulesza",
                "Ben Taskar"
            ],
            "title": "Determinantal point processes for machine learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Hui Lin",
                "Jeff Bilmes"
            ],
            "title": "Learning mixtures of submodular shells with application to document summarization",
            "venue": "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Odile Macchi"
            ],
            "title": "The coincidence approach to stochastic point processes",
            "venue": "Advances in Applied Probability,",
            "year": 1975
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "James Melville"
            ],
            "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
            "venue": "arXiv preprint arXiv:1802.03426,",
            "year": 2018
        },
        {
            "authors": [
                "Ninareh Mehrabi",
                "Fred Morstatter",
                "Nripsuta Saxena",
                "Kristina Lerman",
                "Aram Galstyan"
            ],
            "title": "A survey on bias and fairness in machine learning",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "K.P. Murphy"
            ],
            "title": "Machine Learning: A Probabilistic Perspective",
            "year": 2012
        },
        {
            "authors": [
                "Besmira Nushi",
                "Ece Kamar",
                "Eric Horvitz"
            ],
            "title": "Towards accountable AI: Hybrid human-machine analyses for characterizing system failure",
            "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
            "year": 2018
        },
        {
            "authors": [
                "Dana Pessach",
                "Erez Shmueli"
            ],
            "title": "A review on fairness in machine learning",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2022
        },
        {
            "authors": [
                "Neoklis Polyzotis",
                "Steven Whang",
                "Tim Klas Kraska",
                "Yeounoh Chung"
            ],
            "title": "Slice finder: Automated data slicing for model validation",
            "venue": "In Proceedings of the IEEE International Conference on Data Engineering (ICDE),",
            "year": 2019
        },
        {
            "authors": [
                "Pengzhen Ren",
                "Yun Xiao",
                "Xiaojun Chang",
                "Po-Yao Huang",
                "Zhihui Li",
                "Brij B Gupta",
                "Xiaojiang Chen",
                "Xin Wang"
            ],
            "title": "A survey of deep active learning",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "Cynthia Rudin",
                "Berk Ustun"
            ],
            "title": "Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice",
            "year": 2018
        },
        {
            "authors": [
                "Svetlana Sagadeeva",
                "Matthias Boehm"
            ],
            "title": "Sliceline: Fast, linear-algebra-based slice finding for ML model debugging",
            "venue": "In Proceedings of the 2021 International Conference on Management of Data,",
            "year": 2021
        },
        {
            "authors": [
                "J. Schur"
            ],
            "title": "Bemerkungen zur theorie der beschr\u00e4nkten bilinearformen mit unendlich vielen ver\u00e4nderlichen",
            "venue": "Journal fu\u0308r die reine und angewandte Mathematik,",
            "year": 1911
        },
        {
            "authors": [
                "Ozan Sener",
                "Silvio Savarese"
            ],
            "title": "Active learning for convolutional neural networks: A core-set approach",
            "venue": "arXiv preprint arXiv:1708.00489,",
            "year": 2017
        },
        {
            "authors": [
                "Arash Shaban-Nejad",
                "Martin Michalowski",
                "John S Brownstein",
                "David L Buckeridge"
            ],
            "title": "Guest editorial explainable ai: towards fairness, accountability, transparency and trust in healthcare",
            "venue": "IEEE Journal of Biomedical and Health Informatics,",
            "year": 2021
        },
        {
            "authors": [
                "Sahil Singla",
                "Besmira Nushi",
                "Shital Shah",
                "Ece Kamar",
                "Eric Horvitz"
            ],
            "title": "Understanding failures of deep networks via robust feature extraction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Nimit Sohoni",
                "Jared Dunnmon",
                "Geoffrey Angus",
                "Albert Gu",
                "Christopher R\u00e9"
            ],
            "title": "No subclass left behind: Fine-grained robustness in coarse-grained classification problems",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yunsheng Song",
                "Jing Zhang",
                "Chao Zhang"
            ],
            "title": "A survey of large-scale graph-based semi-supervised classification algorithms",
            "venue": "International Journal of Cognitive Computing in Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Zixing Song",
                "Xiangli Yang",
                "Zenglin Xu",
                "Irwin King"
            ],
            "title": "Graph-based semi-supervised learning: A comprehensive review",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "John Urschel",
                "Victor-Emmanuel Brunel",
                "Ankur Moitra",
                "Philippe Rigollet"
            ],
            "title": "Learning determinantal point processes with moments and cycles",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Michael Wagner",
                "Philip Koopman"
            ],
            "title": "A philosophy for developing trust in self-driving cars",
            "venue": "In Road Vehicle Automation",
            "year": 2015
        },
        {
            "authors": [
                "Xingjiao Wu",
                "Luwei Xiao",
                "Yixuan Sun",
                "Junhang Zhang",
                "Tianlong Ma",
                "Liang He"
            ],
            "title": "A survey of human-in-theloop for machine learning",
            "venue": "Future Generation Computer Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Doris Xin",
                "Litian Ma",
                "Jialin Liu",
                "Stephen Macke",
                "Shuchen Song",
                "Aditya Parameswaran"
            ],
            "title": "Accelerating human-inthe-loop machine learning: Challenges and opportunities",
            "venue": "In Proceedings of the second workshop on data management for end-to-end machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "diagnosis [Shaban-Nejad et al",
                "Rudin",
                "Ustun",
                "Albahri"
            ],
            "title": "2023], or autonomous vehicles [Glomsrud et al., 2019",
            "venue": "Mehrabi et al.,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Given a (machine learning) classifier and a collection of unlabeled data, how can we efficiently identify misclassification patterns presented in this dataset? To address this problem, we propose a human-machine collaborative framework that consists of a team of human annotators and a sequential recommendation algorithm. The recommendation algorithm is conceptualized as a stochastic sampler that, in each round, queries the annotators a subset of samples for their true labels and obtains the feedback information on whether the samples are misclassified. The sampling mechanism needs to balance between discovering new patterns of misclassification (exploration) and confirming the potential patterns of classification (exploitation). We construct a determinantal point process, whose intensity balances the explorationexploitation trade-off through the weighted update of the posterior at each round to form the generator of the stochastic sampler. The numerical results empirically demonstrate the competitive performance of our framework on multiple datasets at various signal-to-noise ratios."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Over the past few years, algorithmic predictive models have claimed many successful stories in real-world applications, ranging from healthcare and finance to jurisdiction and autonomous driving. These successes often take place in an invariant environment where the training data and the test data come from sufficiently similar distributions. If this similarity condition does not hold, then it is well-known that the performance of the algorithmic prediction can deteriorate significantly in the deployment phase. This performance deterioration may trigger subsequent concerns, especially in\nconsequential domains such as self-driving cars and healthcare, where the algorithmic predictions may affect system reliability and human safety. When a predictive model performs unsatisfactorily in a systematic manner, then it is called a failure pattern. For example, if an object detection system fails to capture the object systematically under low-light conditions, then it is a failure pattern. Detecting and correcting the failure patterns is arguably one of the most daunting challenges in developing the future analytics systems.\nDetecting failure patterns is also beneficial for many steps in the life-cycle of an analytics product. For example, it is very common to develop analytics systems using data collected from one country, say the United States, but the systems can be deployed in another country, say Canada. Before field deployment, the systems need to undergo intensive fine-tuning and recalibration, and information regarding the failures can significantly reduce the time and efforts spent on this step. Further, as foundation models are increasingly used as a building block of future analytics systems, assessing the blindspots of these pre-trained models is of crucial importance for product development, deployment and continuous performance evaluation.\nDetecting failure patterns, unfortunately, requires the true outcome or label of the data samples. If the dataset is cleanly labeled, then the problem of failure pattern detection can be simplified to a search problem. The situation becomes more cumbersome if we have access to only an unlabeled dataset. It is thus natural herein to incorporate the role of a human annotator into the detection procedure. However, in many applications that require high-skilled annotators, such as healthcare, it is time-consuming and cost-intensive to query the annotator. Given a dataset containing unlabeled samples, we are interested in designing an efficient routine to query these samples for their true outcome or label, so that we can identify as many failure patterns as possible with minimal annotation queries.\nContributions. We propose in this paper a directed sam-\nAccepted for the 39th Conference on Uncertainty in Artificial Intelligence (UAI 2023).\nar X\niv :2\n30 6.\n00 76\n0v 1\n[ cs\n.L G\n] 1\nJ un\n2 02\npling mechanism with the goal of detecting failure patterns from an unlabeled dataset. This mechanism has two main components:\n\u2022 a Gaussian process to model the predictive belief about the misclassification probability for each unlabeled sample,\n\u2022 a determinantal point process sampling that balances the trade-off between exploration and exploitation by taking a mixture between a similarity matrix (reflecting exploration) and a posterior probability of misclassification matrix (reflection exploitation).\nUltimately, we propose a human-machine collaborative framework that relies on the machine\u2019s proposed queries and the corresponding annotator\u2019s feedback to detect failure patterns, and consequently improve the reliability of the predictive algorithm in variant environments.\nThis paper unfolds as follows: In Section 2, we survey existing efforts to identify failure patterns in a given dataset. Section 3 formalizes our problem and introduces the necessary notations. Section 4 depicts our approach using Gaussian processes to build the surrogate for the Value-of-Interest, which represents the belief about the misclassification probability for the unlabeled samples. Section 5 focuses on the determinantal point process sampler, which blends the Valueof-Interest (highlighting exploitation) with a diversity term (highlighting exploration). Section 6 discusses how we can choose the bandwidth, which is critical given the unsupervised nature of the failure identification problem. Finally, Section 7 provides extensive numerical results to demonstrate the superior performance of our directed sampling mechanism in detecting failure patterns for various datasets.\nIn almost all related work about failure identification, a failure mode (termed as \u201cslice\u201d in Eyuboglu et al. [2022]) of a pre-trained classifier on a dataset is a subset of the dataset that meets two conditions: (i) the classifier performs poorly when predicting samples in that subset, and (ii) the subset captures a distinct concept or attribute that would be recognizable to domain experts. If the true labels of all samples in that subset are available, criterion (i) can be easily confirmed using popular performance metrics for classification tasks. However, condition (ii) is subjective and implicit. For instance, two medical experts may interpret a group of misclassified brain scan images in two different ways, and both interpretations may be reasonable and acceptable. For unlabeled data, it is difficult to employ the existing definitions of the failure patterns.\nArguably, a failure mode is a subjective term that depends on the machine learning task, on the users, and on the datasets. We do not aim to provide a normative answer to the definition of a failure mode in this paper. Our paper takes a pragmatic approach: given a choice of failure pattern definition of users, we develop a reasonable method that can efficiently find the failure patterns from the unlabeled data.\nNotations. We use Rd to denote the space of d-dimensional vectors, and Sd+ to denote the set of d-by-d symmetric, positive semidefinite matrices. The transposition of a matrix A is denoted by A\u22a4, and the Frobenius norm of a matrix A is denoted by \u2225A\u2225F ."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Detecting failure patterns of a machine learning algorithm on a given dataset is an emergent problem in the literature. d\u2019Eon et al. [2022b] formulates a single-stage optimization problem to identify the systematic error. Sohoni et al. [2020] mitigates hidden stratification by detecting sub-class labels before solving a group distributionally robust optimization problem(GDRO). Eyuboglu et al. [2022] provides an evaluation method for slice discovery methods (SDMs): given a trained classifier and a labeled dataset of the form (xi, yi) N i=1, it outputs a set of slicing functions that partitions the dataset into overlapping subgroups on which the model underperforms. Nushi et al. [2018] proposes a hybrid approach to analyze the failures of AI systems in a more accountable and transparent way. The hybrid approach involves two main phases: (1) machine analysis to identify potential failure modes, and (2) human analysis to validate and explain the identified failure modes. Singla et al. [2021] concentrates on understanding the failures of deep neural networks. The paper proposes a method for robust feature extraction, which aims to identify the most important features that contribute to the output of a deep neural network. Polyzotis et al. [2019] proposed SliceFinder that automatically identifies subsets of the data that cause the model to fail or perform poorly. It repeatedly draws random subsets of the data and re-trains the model on each subset, then measures the model\u2019s performance. Finally, it employs a statistical test to identify data slices that are significantly different from the overall distribution. Sagadeeva and Boehm [2021] build a method using linear algebra techniques to speed up the slice-finding process for identifying problematic subsets of data that cause a model to fail.\nOur paper is also closely related to the field of active learning. Active learning examines how to obtain the largest amount of performance gains by labeling as few samples as possible. Comprehensive surveys on active learning can be found in Ren et al. [2021] and Budd et al. [2021]. There is, however, a critical difference between the settings of our paper and those of active learning: in our paper, we focus on identifying the failure pattern for a fixed (invariant) classifier, whereas active learning focuses on improving the model performance with model retraining after each batch of recommendations. The numerical experiments in Section 7 demonstrate empirically that active learning algorithms do not perform competitively at the failure identification task."
        },
        {
            "heading": "3 PROBLEM STATEMENT",
            "text": "We suppose that a user is given a classification algorithm and a set of unlabeled dataset. The classifier is denoted by C : X \u2192 Y , which admits a feature space X = Rd as input, and outputs labels in the set Y = {1, . . . , C}. The unlabeled dataset consists of N samples, which can be represented by N vectors xi \u2208 Rd for i = 1, . . . , N . For each sample xi, the predicted label (termed the pseudolabel) that is recommended by the algorithm is denoted by y\u0302i = C(xi); while its true label is denoted by ytruei . The i-th sample in the dataset is accurately classified if y\u0302i = ytruei , and it is misclassified if y\u0302i \u0338= ytruei .\nThe main object of this paper is not the individual misclassified samples. Our goal in this paper is to study the group effect of misclassified samples: when they are sufficiently close to each other, they form a failure pattern or a failure cluster, and together, they signal a systematic error of the predictive algorithm on the given dataset. To give a formal definition of a failure pattern, we need to construct a graph representation of the data. To do this, we suppose the available data can form an abstract undirected graph G = (V, E), where V is the set of nodes whereas each node represents one sample from the dataset, and E is the set of edges. Let Gmis = (Vmis, Emis) be the pruned graph of G after removing the nodes representing samples that are classified accurately. We can now rigorously define a general failure pattern.\nDefinition 3.1 (M -C failure pattern). Given a graph Gmis, an integer size threshold M , and a connectivity criterion C, a failure pattern is a subgraph Gfail = (V fail, E fail) of Gmis that satisfies the connectivity criterion C and the cardinality of the set V fail is at least M .\nOur framework gives the user tremendous flexibility in spec-\nifying the problem and leveraging our tool to identify the related failure patterns:\n\u2022 First, the user can specify the semantic similarity between two nodes represented by an edge connecting them. Moreover, this user-based graph construction can capture the inherently subjective characteristics of the definition of failure, which depends on the user\u2019s perspective and the domain of data. For a concrete example, if the user is confident that the embedding space is rich enough to capture the similarity between samples, then one possible approach is to construct G by the mutually nearest neighbor graph under the Euclidean distance between embedding representations.\n\u2022 Secondly, users can specify preferred connectivity criteria among samples in a failure pattern. For instance, the criterion of \u201ccompleteness\u201d can be employed as a candidate for C. This criterion is one of the most stringent: it mandates that each misclassified node must exhibit an edge (indicating semantic similarity) with every other misclassified node in the pattern. However, users can adjust the stringency by employing looser criteria, such as \u201cSuper-\u03ba connectivity\u201d or \u201cconnected connectivity\u201d.\n\u2022 Third, the users can specify M which depicts the amount of evidence required in order to confirm a failure pattern. A large value of M means a high level of evidence required (represented by a high number of clustered misclassified samples) to pinpoint a pattern. Moreover, one can also see that given a fixed graph of misclassified samples Gmis, the number of failure patterns in Gmis is non-increasing in M . Taking this perspective, one can also view M as a parameter that is inverse-proportional to the user\u2019s degree of risk aversion: a low value of M indicates that the user believes there are many misclassified patterns in the dataset.\nWhile identifying the failure patterns from an unlabeled dataset is becoming increasingly pertinent in the current practice of machine learning deployment, this problem is inherently challenging due to two main factors:\n1. Annotation cost: to determine if a data point is misclassified, we need to have complete information about its true label, which is often obtained by querying a team of human annotators. Unfortunately, in many consequential domains such as healthcare and law enforcement, the cost of annotation can be enormous for a large dataset.\n2. Signal-to-noise ratio: suppose that there are F failure patterns represented in the data graph Gmis and they are denoted by subgraphs Gfailf = (V failf , E failf ) for f = 1, . . . , F . The union \u222afV failf gives a collection of all misclassifed samples that belong to the failure patterns, and the remainder set Vmis\\\u222afV failf contains all samples that are misclassified but do not belong to any failure pattern. Because the goal of the user is to identify failure\npatterns, the user can view the cardinality of the union set \u222afV failf as a measure of the amount of signal in the dataset, and the cardinality of the remainder set Vmis\\\u222af V failf as a measure of the amount of noise therein. As such, we observe a typical signal-to-noise ratio (SNR) indication: if the SNR is high, then the problem tends to be easy; on the contrary, if the SNR is low then the problem tends to be hard.\nUnfortunately, crucial information to identify the failure patterns is not known to the user ex-ante: for example, the user does not know how many patterns there are in the dataset, nor does the user know the number of misclassified samples and the SNR. To proceed, we make the assumptions:\nAssumption 3.2. We assume the followings:\n\u2022 the number of unlabeled samples N in the given dataset is large and the cost of using an annotator is expensive, thus it may be prohibitive to annotate the whole dataset,\n\u2022 the inference cost of the predictive algorithm is negligible for any feature x \u2208 X , which means we can obtain the pseudolabels y\u0302i = C(xi) for all data points,\n\u2022 the predictive algorithm C remains invariant throughout the failure identification process, and thus the failure patterns do not change over time.\nThe first assumption is critical because if the cost of annotating the whole dataset is small, then the user does not need to use our proposed sampling mechanism because the benefit of annotating the whole dataset outweighs the resulting cost. The second assumption is reasonable in most practical settings as it requires feed-forwarding the entire given dataset through the predictive algorithm once. The last assumption ensures that our targets, the failure patterns, do not alter over time and that the collected information and the updated belief remain relevant for recommending the next batch of samples for annotation.\nWe are interested in a dynamic (sequential) recommendation setting that consists of T rounds. In each round t = 1, . . . , T , the system recommends to the user a set of s unlabeled samples to query the annotator for the true label. After the true labels are obtained, the user can recognize which newly-annotated samples are misclassified, and the user, based on this arrival of information, can (i) identify if a new failure pattern can be identified by verifying the conditions in Definition 3.1, and then (ii) update the user\u2019s own belief about where the remaining failure pattern may locate. The posterior belief is internalized to the recommendation mechanism to suggest another set of s unlabeled samples from the remaining pool to the next round.\nWe note that task (i) described above is purely algorithmic: after the arrival of the misclassification information, the user can run an algorithm to search for the newly-formed maximally connected subgraphs of misclassified samples\nwith size at least M . Task (ii), on the contrary, is more intricate because it requires a dynamic update of belief. To achieve task (ii), we will employ a Bayesian approach with Gaussian processes to form a belief about the locations of the failure patterns, and this belief will also be used for the sampling mechanism.\nRemark 3.3 (Semantics of the failure pattern). Herein, the failure patterns are defined using the closeness in the feature of the samples. This is a convenient abstraction to alleviate the dependence of the definition on specific tasks (such as image, text, or speech classification). Defining patterns based on the feature space is also a common practice in failure identification, see Eyuboglu et al. [2022] and d\u2019Eon et al. [2022b]."
        },
        {
            "heading": "4 GAUSSIAN PROCESS FOR THE VALUE-OF-INTEREST",
            "text": "In this section, we describe our construction of a surrogate function called the Value-of-Interest (VoI). Mathematically, we define VoI : X \u00d7 Y \u2192 [0, 1] to quantify the degree of interest assigned to each pair of feature-pseudolabel (xi, y\u0302i) data point. The notion of VoI aims to capture the exploitation procedure: it emphasizes recommending samples with a high tendency (or intensity) to confirm a misclassification pattern. Thinking in this way, we aim to predict the probability that the feature-pseudolabel pair will be part of a yet-to-confirmed failure pattern. For any generic sample (x, y\u0302), we model VoI using a sigmoid function of the form\nVoI(x, y\u0302) = 1\n1 + exp(\u2212g(x, y\u0302)) ,\nfor some function g : X \u00d7 Y \u2192 R. While VoI is bounded between 0 and 1, the function g can be unbounded, and it is more suitable to model our belief about g using a Gaussian process (GP). In particular, we let g \u223c GP(m, K), where m is the mean function and K is the kernel or covariance function, both are defined on X \u00d7 Y . A GP enables us to model the neighborhood influence among different samples through the covariance function K."
        },
        {
            "heading": "4.1 POSTERIOR UPDATE OF THE PREDICTIVE PROBABILITY",
            "text": "Using the Gaussian process model, we have\ng\u0303 =  g(x1, y\u03021)... g(xN , y\u0302N )  \u223c N (0,K), where for a slight abuse of notation, m is a vector of mean values and the covariance matrix K \u2208 SN+ is the Gram matrix with the components\nKij = K((xi, y\u0302i), (xj , y\u0302j)) \u2200(i, j). (1)\nWe can re-arrange g into another vector (g\u0303[t], g\u0303\u2217[t]) where the subvector g\u0303[t] = (g(xi, y\u0302i))i\u2208I[t] represents the value of g\u0303 at all samples that are queried by time t, while g\u0303\u2217[t] = (g(xi, y\u0302i))i\u2208I\u2217\n[t] represents the value of g\u0303 at all samples that\nare not queried yet by time t. By a similar decomposition of the matrix K, we can rewrite[\ng\u0303[t] g\u0303\u2217[t]\n] \u223c N (( m[t] m\u2217[t] ) , [ K[t] K \u2217 [t] (K\u2217[t]) \u22a4 K\u2217\u2217[t] ]) .\nBy the law of conditional distribution for joint Gaussian distributions [Murphy, 2012, \u00a715.2.1], we have the distribution of g\u0303\u2217[t] conditional on g\u0303[t] = g observe [t] is also a Gaussian distribution:\ng\u0303\u2217[t]|g observe [t] \u223c N (m \u2217 t ,\u03a3 \u2217 t ),\nwhere the conditional mean is determined by\nm\u2217t = m \u2217 [t] + (K \u2217 [t]) \u22a4K\u22121[t] (g observe [t] \u2212m[t]) (2a)\nand the conditional variance is computed as\n\u03a3\u2217t = K \u2217\u2217 [t] \u2212 (K \u2217 [t]) \u22a4K\u22121[t] K \u2217 [t]. (2b)\nThe vector m\u2217t captures the posterior mean of the misclassification probability of samples that are not yet queried by time t.\nWe now discuss how to estimate the expected value of VoI(xi, y\u0302i) for the unqueried sample i. Note that VoI(xi, y\u0302i) is a nonlinear function of g(xi, y\u0302i), we can use a secondorder Taylor expansion of VoI around the conditional mean of g, and we obtain1\nE[VoI(xi, y\u0302i)|gobserve[t] ] \u2248 \u03b1i + 1 2 \u03a3\u2217t,i\u03b2i \u225c \u03b3t,i, (3)\nwith \u03b1i and \u03b2i being computed as\n\u03b1i = (1 + exp(\u2212m\u2217t,i))\u22121, \u03b2i = \u03b1i(1\u2212 \u03b1i)(1\u2212 2\u03b1i).\nIn the above formulas, m\u2217t,i and \u03a3 \u2217 t,i are the mean and the variance component of the vector m\u2217t and matrix \u03a3 \u2217 t corresponding to sample i. A disadvantage of the approximation (3) is that the value \u03b3t,i may become negative due to the possible negative value of \u03b2i. If this happens, we can resort to the first-order approximation:\nE[VoI(xi, y\u0302i)|gobserve[t] ] \u2248 \u03b1i,\nwhich guarantees positivity due to the definition of \u03b1i."
        },
        {
            "heading": "4.2 COVARIANCE SPECIFICATION.",
            "text": "Given any two samples (x, y\u0302) and (x\u2032, y\u0302\u2032), the covariance between g(x, y\u0302) and g(x\u2032, y\u0302\u2032) is dictated by\nCov(g(x, y\u0302), g(x\u2032, y\u0302\u2032)) = K((x, y\u0302), (x\u2032, y\u0302\u2032)). 1See Appendix B.2 for detailed derivation.\nThe bivariate function K is constructed using a kernel on the feature-pseudolabel space X \u00d7 Y . We impose a product kernel on X \u00d7 Y of the form\nK((x, y\u0302), (x\u2032, y\u0302\u2032)) = KX (x, x\u2032)KY(y\u0302, y\u0302\u2032). (4)\nIn order to express a covariance function, we construct K as a positive definite kernel.\nDefinition 4.1 (Positive definite (pd) kernel). Let Z be any set. A symmetric function KZ : Z \u00d7 Z \u2192 R is positive definite if for any natural number n and any choices of (zi) n i=1 \u2208 Z and (\u03b1i)ni=1 \u2208 R, we have\nn\u2211 i=1 n\u2211 j=1 \u03b1i\u03b1jKZ(zi, zj) \u2265 0.\nIf KX and KY are positive definite kernels, then K is also a positive definite kernel according to Schur product theorem [Schur, 1911, Theorem VII]. Thus, it now suffices to construct individual pd kernel for KX and KY . We choose KX as the Gaussian kernel\nKX (x, x\u2032) = exp ( \u2212 1\n2h2X \u2225x\u2212 x\u2032\u222522\n) , (5)\nwhere hX > 0 is the kernel width.\nThe main difficulty encountered when specifying the kernel is the categorical nature of the pseudolabel. Imposing a kernel on Y hence may require a significant effort to pin down a similarity value between any pair of labels. To alleviate this difficulty, we first represent each label y by the respective conditional first- and second-moments, in which the moments are estimated using the samples and their pseudolabels. More specifically, we collect all samples whose pseudolabel is y, and we estimate the feature mean vector and the feature covariance matrix as follows\n\u00b5\u0302y = 1\nNy \u2211 i:y\u0302i=y xi \u2208 Rd, and\n\u03a3\u0302y = 1\nNy \u2211 i:y\u0302i=y (xi \u2212 \u00b5\u0302y)(xi \u2212 \u00b5\u0302y)\u22a4 \u2208 Sd+,\nwhere Ny is the number of samples with pseudolabel y. We now anchor the kernel on Y through the Gaussian kernel on the product space Rd \u00d7 Sd+ of the mean-covariance representation, that is, for any generic label y and y\u2032: KY(y, y\u2032) = exp ( \u2212\u2225\u00b5\u0302y \u2212 \u00b5\u0302y\n\u2032\u222522 2h2Y\n) exp ( \u2212\u2225\u03a3\u0302y \u2212 \u03a3\u0302y\n\u2032\u22252F 2h2Y\n) .\n(6) Notice that this kernel is specified by a single bandwidth parameter hY > 0. By combining Jayasumana et al. [2013, Theorems 4.3 and 4.4] and by the fact that the product of pd kernels is again a pd kernel, we conclude that the kernel K defined using the formulas (4)-(6) is a pd kernel.\nRemark 4.2 (Feature-label metric). Measuring the distance between two (categorical) classes by measuring the distance between the corresponding class-conditional distributions was previously used in Alvarez-Melis and Fusi [2020] and Hua et al. [2023]. Under the Gaussian assumptions therein, the 2-Wasserstein distance between conditional distributions simplifies to an explicit formula involving a Euclidean distance between their mean vectors and a Bures distance between their covariance matrices. Unfortunately, constructing a Gaussian kernel with the Bures distance on the space of symmetric positive semidefinite matrices may not lead to a pd kernel. To ensure that KY is a pd kernel, we need to use the Frobenius norm for the covariance part of (6).\nRemark 4.3 (Intuition on using pseudolabel). Exploiting the pseudolabel serves the following intuition: Suppose that an input xi is misclassified. If there exists another input x\u2032i which is close to xi, and its pseudolabel y\u0302\u2032i is also close to y\u0302i, then it is also likely that x\u2032i will also be misclassified. Thus, our construction of the VoI implicitly relies on the assumption that the pseudolabel is also informative to help predict failure patterns. If a user finds this assumption impractical, it is straightforward to remove this information from the specification of the kernel, and the construction of the Gaussian process still holds with minimal changes.\nRemark 4.4 (Dimensionality reduction). If the feature space is high-dimensional (d is large), we can apply a dimensionality reduction mapping \u03c6(xi) to map the features to a space of smaller dimensions d\u2032 \u226a d. The kernel KY will be computed similarly with \u00b5\u0302y and \u03a3\u0302y are all d\u2032 dimensional."
        },
        {
            "heading": "4.3 VALUE ADJUSTMENT AND PATTERN NEIGHBORHOOD UPDATE",
            "text": "At time t, the algorithm recommends querying the true label of the samples whose indices are in the set It. If for i \u2208 It, the sample xi is misclassified, that is y\u0302i \u0338= ytruei , then we should update the observed probability to VoI(xi, y\u0302i) = 1. However, this would lead to updating g(xi, y\u0302i) = +\u221e. Similarly, if xi is classified correctly then we should update g(xi, y\u0302i) = \u2212\u221e. To alleviate the infinity issues, we set a finite bound \u2212\u221e < L < 0 < U < +\u221e, and we update using the rule\ngobserve(xi, y\u0302i) = { U if xi is misclassified, L otherwise.\nMoreover, suppose that at time t, the annotator identifies that some samples form a pattern. In that case, we intentionally re-calibrate all the values for the samples in the pattern using\ngobserve(xi, y\u0302i) = L if i belongs to a failure pattern.\nIn doing so, we intentionally adjust the misclassified sample i to be a correctly classified (or \u2018good\u2019) sample.\nRemark 4.5 (Interpretation of VoI updates). The VoI aims at capturing the probability that an unlabeled sample belongs to an unidentified failure mode. There are three exemplary cases to guide the design of the VoI: (i) if an unlabeled sample is near the misclassified samples and those misclassified samples do not form a failure mode yet, then VoI should be high. This signifies exploitation for confirmation: it is better to concentrate on the region surrounding this unlabeled sample to confirm this failure mode; (ii) if an unlabeled sample is near the correctly-classified samples, then VoI should be low. This is an exploration process: this sample may be in the correctly-classified region, and we should query in other regions; (iii) if an unlabeled sample is near the misclassified samples and those misclassified samples already formed a failure mode, then VoI should be low. Again, this depicts an exploration process: it is better to search elsewhere for other failure modes."
        },
        {
            "heading": "5 DETERMINANT POINT PROCESS SAMPLER FOR ANNOTATION RECOMMENDATION",
            "text": "Determinantal Point Processes (DDPs) is a family of stochastic models that originates from quantum physics: DPPs are particularly useful to model the repulsive behavior of Fermion particles [Macchi, 1975]. Recently, DPPs have gained attraction in the machine learning community [Kulesza and Taskar, 2012, Affandi et al., 2014, Urschel et al., 2017] thanks to its successes in recommendation systems [Chen et al., 2018, Wilhelm et al., 2018, Gartrell et al., 2017] and text and video summarization [Lin and Bilmes, 2012, Cho et al., 2019, Gong et al., 2014], to name a few. Given N samples, the corresponding DPP can be formalized as follows.\nDefinition 5.1 (L-ensemble DPP). Given a matrix L \u2208 SN+ , an L-ensemble DPP is a distribution over all 2N index subsets J \u2286 {1, . . . , N} such that\nProb(J) = det(LJ)/ det(I + L),\nwhere LJ denotes the |J |-by-|J | submatrix of L with rows and columns indexed by J .\nIn this paper, we construct a DPP to help select the next batch of samples to query the annotator for their true labels. We design the matrix L that can balance between exploration (querying distant samples in order to identify new potential failure patterns) and exploitation (querying neighborhood samples to confirm plausible failure patterns). Given N samples, the exploration is determined by a similarity matrix S \u2208 SN+ that captures pairwise similarity of the samples\nSij = \u03ba(xi, xj) \u2200(i, j)\nfor some similarity metric \u03ba. For example, we can use \u03ba \u2261 KX , where KX is prescribed in (5) with the same bandwidth parameter hX .\nExploration. At time t, conditioned on the samples that are already queried I[t], we can recover a conditional similarity matrix S\u2217t for the set of unqueried samples following [Borodin and Rains, 2005, Proposition 1.2]. Let |I[t]| be the number of samples that have been drawn so far, then S\u2217t is a (N \u2212 |I[t]|)-dimensional positive (semi)definite matrix, calculated as\nS\u2217t = ([(S + II\u2217[t]) \u22121]I\u2217 [t] )\u22121 \u2212 I.\nThus, the matrix S\u2217t will serve as a diversity-promoting term of the conditional DPP at time t.\nExploitation. The exploitation is determined by a probability matrix P \u2208 SN\u2212|I[t]|+ . At time t, we can use P \u2217t = diag(\u03b3t,i), where \u03b3t,i is the posterior probability of being misclassified defined in (3). Notice that if \u03b3t,i is negative, we can replace \u03b3t,i by the first-order approximation to guarantee that P \u2217t is a diagonal matrix with strictly positive diagonal elements. The matrix P \u2217t will induce exploitation because it promotes choosing samples with a high posterior probability of misclassification with the goal of confirming patterns."
        },
        {
            "heading": "Exploration-Exploitation Balancing Conditional DPP.",
            "text": "We impose an additional parameter \u03d1 \u2208 [0, 1] to capture the exploration-exploitation trade-off. At time t, we use a DPP with the kernel matrix L\u03d1 defined as\nL\u03d1t = \u03d1S \u2217 t + (1\u2212 \u03d1)P \u2217t\nfor some mixture weight \u03d1 \u2208 [0, 1]. In particular, when \u03d1 is equal to zero, the algorithm\u2019s approach is entirely exploitative, and its primary objective is to confirm failure patterns in the dataset. Conversely, when \u03d1 is equal to one, the algorithm is entirely explorative, and its main aim is to recommend a diverse set of samples from the dataset. It is worth noting that the algorithm\u2019s behavior can be adjusted by modifying the value of \u03d1 to achieve an appropriate trade-off between exploration and exploitation depending on the specific problem at hand. Because both S\u2217t and P \u2217 t are positive semidefinite matrices, the weighted matrix L\u03d1t is also positive semidefinite, and specifies a valid DPP.\nQuery Suggestion. We choose a set of unlabeled samples for annotation using a maximum a posteriori (MAP) estimate of the DPP specified by L\u03d1t . We then find the s samples from the unlabeled data by solving the following problem\nmax { det(Lz) : z \u2208 {0, 1}N\u2212|I[t]|, \u2225z\u22250 = s } , (7)\nwhere Lz is a submatrix of L\u03d1t \u2208 S N\u2212|I[t]| + restricted to rows and columns indexed by the one-components of z. It is well-known that the solution to problem (7) coincides with the MAP estimate of the DPP with a cardinality constraint [Kulesza and Taskar, 2012].\nUnfortunately, problem (7) is NP-hard [Kulesza and Taskar, 2012]. We thus use heuristics to find a good solution to\nthe problem in a high-dimensional setting with a low running time. A common greedy algorithm to solve the MAP estimation problem is to iteratively find an index that maximizes the marginal gain to the incumbent set of chosen samples z. We then add the index j to the set of samples until reaching the cardinality constraint of s prototypes. This greedy construction algorithm has a complexity cost of O(s2N) time for each inference. An implementation of this algorithm is provided in Chen et al. [2018]. The greedy algorithm has been shown to achieve an approximation ratio of O( 1s! )[Civril and Magdon-Ismail, 2009]. Finally, to boost the solution quality, we add a 2-neighborhood local search that swaps one element from the incumbent set with one element from the complementary set. This local search is performed until no further improvement is found."
        },
        {
            "heading": "6 BANDWIDTH SELECTION",
            "text": "The product kernel K defined in (4) on the featurepseudolabel label space requires the specification of two hyper-parameters: the bandwidth for the feature hX and the bandwidth for the pseudolabels hY . Given N featurepseudolabel pairs (xi, y\u0302i) for i = 1, . . . , N and the kernel K defined as in Section 4, we denote the Gram matrix by K \u2208 SN+ with the components of K satisfying (1). If the bandwidth parameters are too small compared to the feature distance \u2225x \u2212 x\u2032\u22252 and the pseudolabel distance\u221a \u2225\u00b5\u0302y \u2212 \u00b5\u0302y\u2032\u222522 + \u2225\u03a3\u0302y \u2212 \u03a3\u0302y\u2032\u22252F in the dataset, then the matrix K tends toward an N -by-N identity matrix IN . Notice that when K is an identity matrix, the matrix multiplication (K\u2217)\u22a4K\u22121 turns into a matrix of zeros, and the updates (2) become m\u2217t = m \u2217 [t] and \u03a3 \u2217 t = K \u2217\u2217 [t] . This means that all observed information from previous queries is ignored. To alleviate this, we impose a restriction on hX and hY so that\n\u2225K(hX , hY)\u2212 IN\u2225F \u2265 \u03b4\u2225IN\u2225F (8)\nfor some value of \u03b4 > 0. In the above equation, we make explicit the dependence of the Gram matrix K on the hyperparameters hX and hY , and the norm \u2225 \u00b7 \u2225F is the Frobenius norm. Condition (8) imposes that the Gram matrix needs to be sufficiently different from the identity matrix, where the magnitude of the difference is controlled by \u03b4. The next proposition provides the condition to choose hX and hY to satisfy this condition.\nProposition 6.1 (Hyper-parameter values). For a fixed value of \u03b4 \u2208 (0, \u221a N \u2212 1), the condition (8) is satisfied if\nDX h2X + DY h2Y \u2264 ln N \u2212 1 \u03b42 ,\nwhere DX and DY are calculated based on the dataset as\nDX =\n\u2211 i>j \u2225xi \u2212 xj\u222522(\nN 2 ) , and DY = \u2211 i>j \u2225\u00b5\u0302y\u0302i \u2212 \u00b5\u0302y\u0302j\u222522 + \u2225\u03a3\u0302y\u0302i \u2212 \u03a3\u0302y\u0302j\u22252F(\nN 2 ) . We suggest choosing hX and hX to equalize the components\nDX h2X = DY h2Y = 1 2 ln N \u2212 1 \u03b42 .\nWe also notice that the value of \u03b4 = \u221a 2\u00d7 10\u22126 is reasonable for most practical cases encountered in the numerical experiments of this paper. Hence, without other mention stated, we set \u03b4 to \u221a 2\u00d7 10\u22126."
        },
        {
            "heading": "7 NUMERICAL EXPERIMENTS",
            "text": "Datasets. For the numerical experiments, we utilize 15 realworld datasets adapted from Eyuboglu et al. [2022]2. Each dataset in Eyuboglu et al. [2022] consists of a pre-trained classifier and a collection of data points. Each data point has three features: Activation (a 512-dimensional embedding features of the image with respect to the pre-trained classifier), True Label (an integer in the range {0, . . . , C} that represents the true class of the data point), Probs (a C-dimensional vector that indicates the probability of each class). By taking the argmax of the Probs vector for each data point, we can determine the predicted label (pseudolabel) for that data point.\nWe use the following construction of a failure pattern: Two samples are connected by an edge if each sample is in the knn-nearest neighbors of the other. Because X is a feature space, we measure the distance between two samples by taking the Euclidean distance between xi and xj . Notice that knn is a parameter that is chosen by the user. Criterion C in Definition 3.1 is chosen as maximally connected subgraphs. Further discussion about this specific selection of the user is provided in Appendix A.1. Indicating the failure mode as above requires the user to input two hyper-parameters, knn and M . The discussion about choosing values of knn and M in practical problems is in Appendix A.3.\nFor each dataset, we construct a dataset that is suited for the task of failure identification as follows: we choose different values of knn to construct the graph (cf. Section 3) and the evidence threshold M , then we generate the ground truth information about the true failure patterns by finding maximally connected components in Gmis. Each sample in the dataset is now augmented to have four features: Activation, True Label, Pseudo Label, and Pattern, where Pattern is an\n2Datasets are publicly available at https://dcbench. readthedocs.io/en/latest\ninteger in the range {\u22121, 1, . . . , P}, where \u22121 means that the sample does not belong to any failure pattern, and P is the number of failure patterns in the dataset.\nDuring the experiment, the true labels and patterns of samples are hidden: true labels are only accessible by querying the annotator, while pattern information is used to compute the performance ex-post. Our 15 generated datasets are classified into three classes based on the level of SNR: Low, Medium, and High. The details are in Appendix A.2.\nComparison. We compare the following baselines:\n\u2022 Active learning algorithms: We consider two popular active learning methods, namely BADGE [Ash et al., 2019] and Coreset [Sener and Savarese, 2017]. Because the classifier is fixed in our setting, the retraining stage in these active learning algorithms is omitted.\n\u2022 Uniform Sampling (US) algorithm: At iteration t with the set of remaining unlabeled samples I\u2217[t], we pick a size s subset of I\u2217[t] with equal probability. This algorithm is a stochastic algorithm; hence we take results from 30 random seed numbers and calculate the average.\n\u2022 Five variants of our Directed Sampling (DS_\u03d1) algorithm, with \u03d1 chosen from {0, 0.25, 0.5, 0.75, 1}. At \u03d1 = 0, our algorithm is purely exploitative, emphasizing the confirmation of failure patterns. At \u03d1 = 1, our algorithm is purely exploration, emphasizing recommending diverse samples from the dataset.\nThroughout, the batch size is set to s = 25 for all competing methods. Codes and numerical results are available at https://github.com/ nguyenngocbaocmt02/FPD.\nExperiment 1 (Sensitivity). The goal of this experiment is to measure the sensitivity of different recommendation algorithms. Hereby, sensitivity is defined as the fraction between the number of queried samples until the detection of the first failure pattern in the dataset and the total number of samples. This value measures how slowly we identify the first failure pattern: a lower sensitivity is more desirable.\nWe observed that the two active learning algorithms have the lowest performance. We suspect that the objective of active learning algorithms is to refine the decision boundaries to obtain better performance (accuracy), whereas the primary concern herein is to isolate misclassified clusters. Thus, active learning methods may not be applicable to the problem considered in this paper.\nWe could see from Table 1 that the sensitivity of all methods decreases as the SNR increases. All DS variants except the extreme with \u03d1 = 1.0 outperform the US method and active learning methods. The poor performance of DS_1.0 is attributed to the lack of an exploitative term, which is also the knowledge gathered from previous queries. Moreover, we notice that our proposed algorithm, DS_0.25 and DS_0\nhave the smallest sensitivity of 0.11 overall. While DS_0.25 achieves the highest performance in datasets with low noise magnitude, DS_0 is more effective in medium and high SNR contexts. This can be attributed to the fact that when the SNR is low, there are many noise samples. Consequently, if DS_0 gets trapped in a noisy misclassified region, it may take considerable time to confirm whether this area contains any patterns because the algorithm is purely exploitative when \u03d1 = 0. In contrast, DS_0.25 overcomes this issue by incorporating an exploration term that avoids focusing too much on any area.\nExperiment 2 (Effectiveness). This experiment aims to confirm the ability to recommend methods in detecting failure patterns subject to a limited number of annotation queries. More specifically, we allow the number of queries to be maximally 10% and 20% of the total number of samples in the dataset, and we measure effectiveness as the percentage of detected patterns up to that cut-off. A higher value of effectiveness indicates a higher capacity for failure pattern identification.\nWhen the maximum permitted number of queries is low (e.g., 10%), there is no significant difference in the overall performance of all algorithms because the queried information about the dataset is insufficient to confirm most patterns, see Table 2. However, all versions of DS perform equally well and are more effective than the US, BADGE, and Coreset. As the number of queries increases to 20% of the dataset in Table 3, all DS variants significantly outperform US and active learning methods: the DS methods manage to detect more than a third of all failure patterns. In high SNR datasets, DS_0.5 can even detect over half of the patterns on average.\nConclusions. We proposed a sampling mechanism for the purpose of failure pattern identification. Given a classifier and a set of unlabeled data, the method sequentially suggests a batch of samples for annotation and then consolidates the information to detect the failure patterns. The sampling mechanism needs to balance two competing criteria: explo-\nration (querying diverse samples to identify new potential failure patterns) and exploitation (querying neighborhood samples to collect evidence to confirm failure patterns). We constructed a Gaussian process to model the exploitative evolution of our belief about the failure patterns and used a DPP with a weighted matrix to balance the explorationexploitation trade-off. The numerical experiments demonstrate that our sampling mechanisms outperform the uniform sampling method in both sensitivity and effectiveness measures.\nAcknowledgments. We gratefully acknowledge the generous support from the CUHK\u2019s Improvement on Competitiveness in Hiring New Faculties Funding Scheme and the CUHK\u2019s Direct Grant Project Number 4055191."
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A.1 MOTIVATION FOR THE SPECIFIC USER\u2019S DEFINED FAILURE MODE DEFINITION",
            "text": "In this section, we provide the motivation, theoretical justification, and practical effectiveness of the failure mode definition based on mutual nearest neighbor graph3 on embedding space.\n\u2022 We make a similar assumption with d\u2019Eon et al. [2022a] and Sohoni et al. [2020] that the classifier\u2019s activations layer contains essential information about the semantic features used for classification. The proximity between two points in this embedding space could indicate their semantic similarity. Hence, issuing an edge between two points as in the mutual nearest neighbor graph likely guarantees that two connected points have much more semantic similarity than other pairs. This would ensure semantic cohesion for the points within a failure mode according to our definition.\n\u2022 Regarding the theoretical aspect, we use the mutual nearest neighbor graph, which is effective in clustering and outliers detection (see Song et al. [2022b], Song et al. [2022a] and Brito et al. [1997]). Moreover, Brito et al. [1997, Theorem 2.2] stated that with the reasonable choice of knn, connected components (a.k.a. maximally connected subgraphs) in a mutual knn-graph are consistent for the identification of its clustering structure.\n\u2022 In terms of more visual representation, we show images of four failure patterns of dataset id_1 in Figure A.2 to show the effectiveness of this definition on detecting semantic-cohesion clusters. We can observe that each failure pattern has a common concept recognizable by humans and includes images that are all misclassified. The top-left mode includes images of blonde-haired girls with tanned skin. The top-right mode includes images of girls wearing earrings. The bottom-left mode contains photos with tilted angles, and the bottom-right mode contains images with dark backgrounds."
        },
        {
            "heading": "A.2 DATASETS AND IMPLEMENTATION DETAILS",
            "text": "We describe fifteen datasets used in our work in Table 4.\nPreprocessing: We single out 15 datasets from Eyuboglu et al. [2022], each includes three features: Activation, True Label, and Pseudo Label. After that, we preprocess the data using a standard scaler for the Activation feature.\nGround truth generation: It is necessary to assign values of knn and M to each preprocessed dataset. The value of M expresses the level of evidence required for confirming the failure patterns. A higher value of M indicates a greater emphasis on the patterns that exist most frequently in the dataset. As M decreases to 1, the problem transforms into identifying misclassified data points, where each failure data point constitutes a pattern. Moreover, the users choose M so that they can perceive the shared concept of M samples. If M is too small, then the concept may not be distinctive enough between clusters, while if M is too large, the users may have a bottleneck in identifying the shared concept. The value of knn signifies the coherence required for data points within a pattern. The users choose a smaller knn if they need strong tightness between samples in a failure mode. Brito et al. [1997] recommended choosing knn of order log(N) for consistent identification\n3A mutual knn-nearest neighbor graph is a graph where there is an edge between xi and xj if xi is one of the knn nearest neighbors of xj and xj is one of the knn nearest neighbors of xi.\nof the clustering structure. A smaller value of knn imposes a more stringent condition to create an edge in the knn graph. When knn = 0, each data point is only connected with itself. If knn is sufficiently high, all misclassified data points merge to form a single failure pattern. From Figure A.3, we notice that as the increase of knn and SNR, there is a tendency to appear big patterns with a large number of data points. We could explain it as follows. When increasing knn, more edges are additionally created, which could initially connect separate patterns or augment more data points into the patterns. In practical applications of this problem, it is important to note that the two parameters knn and M rely heavily on the users, the machine learning tasks, and the nature of the dataset. In this study, we have established a fixed value of M equal to 10 for all datasets, and we have varied the value of knn to generate diverse scenarios of Signal-to-Noise Ratio (SNR). With the defined value of knn, we have constructed the knn graph of the re-scaled Activation feature. Subsequently, we have employed a simple Depth First Search algorithm on the sub-graph of only misclassified data points to collect all maximally connected components with cardinality greater than M . These components represent patterns that are the focus of the recommending algorithms. We add one additional feature named Pattern to each data point which indicates the pattern of it or \u22121 if it does not belong to any patterns.\nFinally, the complete dataset for our problem consists of four information: Activation, True Label, Pseudo Label, and Pattern."
        },
        {
            "heading": "A.3 ADDITIONAL NUMERICAL RESULTS",
            "text": "In the main paper, we present the numerical results for groups categorized into three levels of Signal-to-Noise Ratio (SNR). In this section, we offer a comprehensive breakdown of the results for each individual dataset in Tables 5, 6, and 7, respectively.\nWe also provide charts that illustrate the progress of algorithms over iterations in dataset id_10, as depicted in Figure A.4. The blue line represents the percentage of queried samples, which appears linear due to the fixed size of the queried batch at each iteration. The orange line indicates the percentage of detected misclassified samples out of the total misclassified ones in the dataset. The green line represents the percentage of detected failure modes out of the total number of failure modes in the dataset. It is evident that the orange line, corresponding to methods that incorporate our exploiting component (Gaussian process component) such as DS_0.0, DS_0.25, DS_0.5, and DS_0.75, consistently outperforms the blue lines significantly. This trend clearly demonstrates the effectiveness of our exploiting term in identifying misclassified samples.\nHowever, DS_0.0 shows inferior performance, as evidenced by the green line consistently falling below the blue line throughout the iterations, despite its effectiveness in identifying misclassified samples. In contrast, DS_0.25, DS_0.5, and DS_0.75 exhibit superb performance in detecting all failure patterns within approximately 100 iterations (40% of the dataset samples). This difference can be attributed to the absence of the exploration term in DS_0.0 when dealing with a high SNR level in dataset id_10."
        },
        {
            "heading": "A.4 ANALYSIS OF SAMPLING COMPLEXITY",
            "text": "Each iteration in our framework consists of two main phases. The first phase determines which samples to be labeled next, the most costly computation in this phase is the matrix inversion and computing matrix determinant. The maximum size of the matrix is N , so the time complexity is O(N3). If we use the optimized CW-like algorithm for matrix inversion, then the complexity can be as low as O(N2.373). The second phase includes updating information and confirming detected failure modes. Updating information involves matrix inversions and multiplications, with cost O(N2.373). A low-cost Depth First Search is implemented to check detected failure modes, which costs O(N). In conclusion, the cost of an iteration is O(N2.373)."
        },
        {
            "heading": "A.5 PRINCIPAL HYPER-PARAMETERS AND USER-DEFINED HYPER-PARAMETERS",
            "text": "Our proposed framework is applied to human-machine cooperation systems. Therefore, some terms depend on the user such as the failure mode definition which is defined by two factors: (i) how to determine whether two samples have a common concept; (ii) what the structure of a failure pattern is. In our experiments, we consider the case that the user defines an edge (common concept) by using the mutual knn-graph under the Euclidean distance on the embedding space. The connectivity criterion is maximally connected subgraphs (a.k.a. connected components). With this indication, the user also provides two hyper-parameters knn and M . The meaning of knn and M are mentioned in Appendix A.2. From the algorithmic viewpoint, our approach depends mainly on one main hyper-parameter \u03d1. The parameter \u03d1 regulates the exploration-exploitation trade-off in the sampling procedure (\u03d1 = 0 means pure exploitation, \u03d1 = 1 means pure exploration). We experimented with five values of \u03d1 throughout the paper."
        },
        {
            "heading": "B PROOFS",
            "text": ""
        },
        {
            "heading": "B.1 PROOFS OF PROPOSITION 6.1",
            "text": "Proof of Proposition 6.1. We first show that the value of \u03b4 should be upper-bounded by \u221a N \u2212 1. To see this, note that K(hX , hY) is a Gram matrix, so its diagonal elements are all ones, and the off-diagonal elements are in the range (0, 1]. We have an upper bound that:\n\u2225K(hX , hY)\u2212 IN\u2225F \u2264 \u221a N(N \u2212 1).\nTo ensure the existence of hX , hY , the value of \u03b4 must fulfill: \u03b4\u2225IN\u2225F < \u221a N(N \u2212 1) =\u21d2 \u03b4 < \u221a N \u2212 1.\nNext, we show that condition for hX and hY . Squaring both sides of (8) gives\n\u2225K(hX , hY)\u2212 IN\u22252F \u2265 \u03b42\u2225IN\u22252F = \u03b42N.\nBecause the diagonal elements of K(hX , hY) are all ones, the above condition is equivalent to\u2211 i>j exp ( \u2212 \u2225xi \u2212 xj\u2225 2 2 h2X \u2212 \u2225\u00b5\u0302y\u0302i \u2212 \u00b5\u0302y\u0302j\u222522 + \u2225\u03a3\u0302y\u0302i \u2212 \u03a3\u0302y\u0302j\u22252F h2Y ) \u2265 \u03b4 2N 2 . (9)\nUsing Jensen inequality for the exponential function, which is convex, we have the following lower bound:\n1( N 2 ) \u2211 i>j exp ( \u2212 \u2225xi \u2212 xj\u2225 2 2 h2X \u2212 \u2225\u00b5\u0302y\u0302i \u2212 \u00b5\u0302y\u0302j\u222522 + \u2225\u03a3\u0302y\u0302i \u2212 \u03a3\u0302y\u0302j\u22252F h2Y ) \u2265 exp ( \u2212 \u2211\ni>j \u2225xi \u2212 xj\u222522 h2X ( N 2 ) \u2212 \u2211i>j \u2225\u00b5\u0302y\u0302i \u2212 \u00b5\u0302y\u0302j\u222522 + \u2225\u03a3\u0302y\u0302i \u2212 \u03a3\u0302y\u0302j\u22252F h2Y ( N 2 ) ). Therefore, if hX and hY satisfy\nexp ( \u2212 \u2211\ni > j\u2225xi \u2212 xj\u222522 h2X ( N 2 ) \u2212 \u2211i>j \u2225\u00b5\u0302y\u0302i \u2212 \u00b5\u0302y\u0302j\u222522 + \u2225\u03a3\u0302y\u0302i\u03a3\u0302y\u0302j\u22252F h2Y ( N 2 ) ) \u2265 \u03b42 N \u2212 1 ,\nthen they also satisfy the condition (9). Defining the quantities DX and DY as in statement of the proposition, we find that hX and hY should satisfy\n\u21d4 DX h2X + DY h2Y \u2264 ln N \u2212 1 \u03b42 .\nThis completes the proof."
        },
        {
            "heading": "B.2 TAYLOR EXPANSION FOR VALUE-OF-INTEREST VOI",
            "text": "We first use a second-order Taylor expansion to approximate f(X) = VoI(X) = (1 + exp(\u2212g(X))\u22121 around the point X = \u00b5:\nf(X) = f(\u00b5) + (X \u2212 \u00b5)\u22a4\u2207f(\u00b5) + 1 2 (X \u2212 \u00b5)\u22a4\u22072f(\u00b5)(X \u2212 \u00b5) +O(\u2225\u2206X\u22253)\n= f(\u00b5) + (X \u2212 \u00b5)\u22a4\u2207f(\u00b5) + 1 2 Tr[\u22072f(\u00b5)(X \u2212 \u00b5)(X \u2212 \u00b5)\u22a4] +O(\u2225\u2206X\u22253).\nMoreover, we set \u00b5 as the expected value E[X], and taking expectations on both sides of the above equation gives\nE[f(X)] = E [ f(\u00b5) ] + E [ (X \u2212 \u00b5)\u22a4\u2207f(\u00b5) ] + 1 2 E [ Tr[\u22072f(\u00b5)(X \u2212 \u00b5)(X \u2212 \u00b5)\u22a4] ] +O(\u2225\u2206\u22253)\n= f(\u00b5) + 1\n2 \u03a3\u2217t,i\u22072f(\u00b5) +O(\u2225\u2206\u22253),\nwhere the second equality follows from the relationship E [ (X \u2212 \u00b5)\u22a4\u2207f(\u00b5) ] = E [ (X \u2212 \u00b5) ]\u22a4\u2207f(\u00b5) = (E[X]\u2212 \u00b5)]\u22a4\u2207f(\u00b5) = 0, and from the definition of the covariance matrix\nE [ (X \u2212 \u00b5)(X \u2212 \u00b5)\u22a4 ] = \u03a3\u2217t,i.\nIt now suffices to verify the expressions for \u03b1i and \u03b2i. Note that \u03b1i = f(\u00b5) = (1 + exp(\u2212\u00b5))\u22121 and \u03b2i is the second-order derivative\n\u03b2i = \u22072f(\u00b5) = \u03b1i(1\u2212 \u03b1i)(1\u2212 2\u03b1i),\nwhere the second equality follows from the property of the sigmoid function."
        },
        {
            "heading": "C SOCIAL IMPACT",
            "text": "One important social impact of this research lies in its potential to improve the accuracy and reliability of machine learning classifiers. By identifying misclassification patterns, the framework enables the refinement and improvement of classifiers, reducing the likelihood of wrong predictions in various domains. This can have wide-ranging implications, such as improving the performance of automated systems in critical areas where accurate classification is of utmost importance like healthcare diagnosis [Shaban-Nejad et al., 2021, Rudin and Ustun, 2018, Albahri et al., 2023], or autonomous vehicles [Glomsrud et al., 2019, Wagner and Koopman, 2015].\nAnother significant social impact of this research is its potential to address biases and fairness issues in machine learning systems [Caton and Haas, 2020, Mehrabi et al., 2021, Pessach and Shmueli, 2022]. By identifying misclassification patterns, the framework can shed light on potential biases in the data or algorithmic models. This knowledge is crucial for developing fairer and more equitable machine learning systems which are obligatory for bringing machine learning models to practical implementations.\nMoreover, the collaborative nature of the framework promotes human-machine interaction, fostering a symbiotic relationship that combines human expertise and algorithmic capabilities. This approach not only empowers human annotators by involving them in the decision-making process but also allows them to contribute their domain knowledge and intuition [Wu et al., 2022, Xin et al., 2018]."
        }
    ],
    "title": "Efficient Failure Pattern Identification of Predictive Algorithms",
    "year": 2023
}