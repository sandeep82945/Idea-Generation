{
    "abstractText": "The strength of modern generative models lies in their ability to be controlled through textbased prompts. Typical \u201chard\u201d prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also \u201csoft\u201d prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxin Wen"
        },
        {
            "affiliations": [],
            "name": "Neel Jain"
        },
        {
            "affiliations": [],
            "name": "John Kirchenbauer"
        },
        {
            "affiliations": [],
            "name": "Micah Goldblum"
        },
        {
            "affiliations": [],
            "name": "Jonas Geiping"
        },
        {
            "affiliations": [],
            "name": "Tom Goldstein"
        }
    ],
    "id": "SP:be0eda69a5a7c2ccf2a08a661a187a5cde399184",
    "references": [
        {
            "authors": [
                "S. Gray",
                "B. Chess",
                "J. Clark",
                "C. Berner",
                "S. McCandlish",
                "A. Radford",
                "I. Sutskever",
                "D. Amodei"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "In 34th Conference on Neural Information Processing Systems (NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "M. Cherti",
                "R. Beaumont",
                "R. Wightman",
                "M. Wortsman",
                "G. Ilharco",
                "C. Gordon",
                "C. Schuhmann",
                "L. Schmidt",
                "J. Jitsev"
            ],
            "title": "Reproducible scaling laws for contrastive language-image learning",
            "venue": "arXiv preprint arXiv:2212.07143,",
            "year": 2022
        },
        {
            "authors": [
                "H.W. Chung",
                "L. Hou",
                "S. Longpre",
                "B. Zoph",
                "Y. Tay",
                "W. Fedus",
                "E. Li",
                "X. Wang",
                "M. Dehghani",
                "S Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "M. Courbariaux",
                "Y. Bengio",
                "David",
                "J.-P"
            ],
            "title": "Binaryconnect: Training deep neural networks with binary weights during propagations",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "B. Lester",
                "R. Al-Rfou",
                "N. Constant"
            ],
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "venue": "[cs], September 2021b. URL http://arxiv.org/abs/2104.08691",
            "year": 2021
        },
        {
            "authors": [
                "H. Li",
                "S. De",
                "Z. Xu",
                "C. Studer",
                "H. Samet",
                "T. Goldstein"
            ],
            "title": "Training quantized nets: A deeper understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "J. Li",
                "D. Li",
                "C. Xiong",
                "S. Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "P. Luo",
                "X. Wang",
                "X. Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "J. McAuley",
                "J. Leskovec"
            ],
            "title": "Hidden factors and hidden topics: Understanding rating dimensions with review text",
            "venue": "In Proceedings of the 7th ACM Conference on Recommender Systems,",
            "year": 2013
        },
        {
            "authors": [
                "A. Prasad",
                "P. Hase",
                "X. Zhou",
                "M. Bansal"
            ],
            "title": "Grips: Gradient-free, edit-based instruction search for prompting large language models",
            "venue": "arXiv preprint arXiv:2203.07281,",
            "year": 2022
        },
        {
            "authors": [
                "A. Radford",
                "J. Wu",
                "R. Child",
                "D. Luan",
                "D. Amodei",
                "I. Sutskever"
            ],
            "title": "Language Models are Unsupervised Multitask Learners",
            "venue": "OpenAI, pp",
            "year": 2019
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "J. Rando",
                "D. Paleka",
                "D. Lindner",
                "L. Heim",
                "F. Tram\u00e8r"
            ],
            "title": "Red-teaming the stable diffusion safety",
            "venue": "filter. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "M. Rastegari",
                "V. Ordonez",
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "Xnor-net: Imagenet classification using binary convolutional neural networks",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "G. Santana"
            ],
            "title": "Gustavosta/Stable-Diffusion-Prompts \u00b7 Datasets at Hugging Face, December 2022. URL https://huggingface.co/datasets/ Gustavosta/Stable-Diffusion-Prompts",
            "year": 2022
        },
        {
            "authors": [
                "C. Schuhmann",
                "R. Beaumont",
                "R. Vencu",
                "C. Gordon",
                "R. Wightman",
                "M. Cherti",
                "T. Coombes",
                "A. Katta",
                "C. Mullis",
                "M Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "arXiv preprint arXiv:2210.08402,",
            "year": 2022
        },
        {
            "authors": [
                "N. Shazeer",
                "M. Stern"
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "W. Shi",
                "X. Han",
                "H. Gonen",
                "A. Holtzman",
                "Y. Tsvetkov",
                "L. Zettlemoyer"
            ],
            "title": "Toward human readable prompt tuning: Kubrick\u2019s the shining is a good movie, and a good prompt too",
            "venue": "arXiv preprint arXiv:2212.10539,",
            "year": 2022
        },
        {
            "authors": [
                "T. Shin",
                "Y. Razeghi",
                "R.L. Logan IV",
                "E. Wallace",
                "S. Singh"
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "R. Socher",
                "A. Perelygin",
                "J. Wu",
                "J. Chuang",
                "C.D. Manning",
                "A. Ng",
                "C. Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "P. Zhang",
                "X. Li",
                "X. Hu",
                "J. Yang",
                "L. Zhang",
                "L. Wang",
                "Y. Choi",
                "J. Gao"
            ],
            "title": "Vinvl: Revisiting visual representations in vision-language models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "S. Zhang",
                "S. Roller",
                "N. Goyal",
                "M. Artetxe",
                "M. Chen",
                "S. Chen",
                "C. Dewan",
                "M. Diab",
                "X. Li",
                "Lin",
                "X. V"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhang",
                "J. Zhao",
                "Y. LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification."
        },
        {
            "heading": "1. Introduction",
            "text": "Prompt engineering is the art of creating instructions to guide generative models. It is the key to unlocking the power of large models for both image generation and language tasks. As it stands today, prompt engineering methods can be coarsely divided into two camps. First, there are hard prompting methods, which use hand-crafted sequences of interpretable tokens to elicit model behaviors. Hard prompt discovery is a specialized alchemy, with many good prompts being discovered by trial and error, or sheer\n*Equal contribution. Code is available at https://github. com/YuxinWenRick/hard-prompts-made-easy.\nintuition. Then there are soft prompts, which consist of continuous-valued language embeddings that do not correspond to any human-readable tokens. Soft prompt discovery is a mathematical science; gradient-based optimizers and large curated datasets are used to generate highly performant prompts for specialized tasks.\nDespite the difficulty of engineering hard prompts, they have their advantages. Hard prompts and the tricks they exploit can be mixed, matched, and mutated to perform a range of different tasks, while soft prompts are highly specialized. Hard prompts are portable; they can be discovered using\nar X\niv :2\n30 2.\n03 66\n8v 2\n[ cs\n.L G\n] 1\nJ un\n2 02\n3\none model and then deployed on another. This portability is impossible with soft prompts due to differences in embedding dimension and representation space between models. Finally, hard prompts can be used when only API access to a model is available and it is not possible to control the embeddings of inputs.\nThis work explores the use of efficient gradient methods to optimize and learn discrete text, with an emphasis on applications to prompt engineering. In doing so, we unlock the ability to learn hard prompts via optimization. Learned hard prompts combine the ease and automation of soft prompts with the portability, flexibility, and simplicity of hard prompts. Our primary contributions are summarized as follows:\n\u2022 We propose a simple scheme for learning hard prompts using continuous optimization. The scheme builds on existing gradient reprojection schemes for optimizing text, and adapts lessons learned from the large-scale discrete optimization literature for quantized networks.\n\u2022 We show that this optimization method can be used to learn hard prompts for image generation, giving us a general tool to create prompts that elicit specific image styles, objects, and appearances. The learned prompts perform competitively with highly specialized prompt generation tools, despite using far fewer tokens and containing no hand-crafted components.\n\u2022 We also show that our learned hard prompts perform well on language classification tasks, out-performing other text optimization schemes. The learned prompts transfer well across networks, and this transfer is enhanced when they are regularized with fluency constraints to improve interpretability.\nIn addition to capturing the quantifiable benefits of learned prompts, the proposed schemes can be used to facilitate prompt exploration and discovery, as optimization often recovers words and tokens that are simultaneously highly interpretable and also highly non-obvious."
        },
        {
            "heading": "2. Related Works",
            "text": "Prompting in Language Models. Brown et al. (2020) was one of the first to demonstrate the power of prompting for task adaption of pre-trained language models. This \u201cinstruction tuning\u201d paradigm has since become a standard way to increase the ability of large models to follow complex, taskspecific instructions (Sanh et al., 2022; Chung et al., 2022). However, automatically finding suitable sets of text prompts, i.e. hard prompts, for these purposes remains an open challenge. Lester et al. (2021b) simplified the \u201cprefix tuning\u201d technique presented in Li & Liang (2021) to establish\nthe procedure referred to as standard soft \u201cprompt-tuning\u201d where they optimize sequences of continuous-valued embeddings prepended to the real embeddings of the input tokens. However, subsequent work by Khashabi et al. (2022) showed that the sequences of embeddings produced by this technique could map to token sequences with limited semantic scrutability. To address these limitations, in this work we construct a method for hybridizing the continuous soft-prompt optimization with hard vocabulary constraints, resulting in task-specific, interpretable tokens.\nDiscrete Optimization for Language. AutoPrompt (Shin et al., 2020) was one of the first discrete prompt optimization frameworks for transformer language models and subsequent approaches have included a gradient-free phrase editing method (Prasad et al., 2022), an embedding optimization approach based on Langevin dynamics (Shi et al., 2022) and a reinforcement learning approach (Deng et al., 2022).\nWe consider two gradient-based methods as baselines: FluentPrompt and AutoPrompt (Shi et al., 2022; Shin et al., 2020). AutoPrompt, which utilizes HotFlip proposed by Ebrahimi et al. (2018), greedily chooses the optimal token for each location in the prompt utilizing the gradient to find a selection of good candidates. However, AutoPrompt can become expensive very quickly. For each gradient step, the method requires an evaluation of each candidate at each location in the prompt, adding numerous additional forward passes. To avoid the additional forward passes, we originally considered AutoPromptk=1 with and without an added fluency constraint, but found that AutoPromptSGD with a fluency constraint outperformed its counterparts as seen in Figure 12, and thus we use SGD version of AutoPrompt as our other baseline similar to Shi et al. (2022). FluentPrompt differs from AutoPrompt by utilizing Langevin dynamics (Kumar et al., 2022) to optimize the prompt embeddings, as well as adding a fluency penalty.\nFor the baselines discussed above, at the end of every update step, the optimized prompt embeddings are projected onto their nearest neighbor embeddings to ensure that optimization is performed on the discrete set of natural language tokens. However, if the nearest neighbors are far away from the embeddings and the learning rate is not tuned properly, the embeddings may become stagnant, which can require extensive hyperparameter tuning as demonstrated in Figure 8. The cost of such a constraint is a loss of flexibility in the solutions the optimization can find. On the other hand, while soft prompts are not as limited in this way, just clamping a welltrained soft prompt to the nearest discrete prompt strongly degrades performance as observed in Khashabi et al. (2022).\nPrompt Discovery from Images. The process of extracting rich information from images and conveying it through natural language texts is known as image captioning. Zhang et al. (2021), Hu et al. (2022), and Li et al. (2022) achieve this\ngoal by training large captioning models on image-text pairs. However, these captions are often generic and may not accurately reflect new or unseen objects. In Gal et al. (2022), the authors propose a method that utilizes a soft prompt to optimize a text-guided diffusion model, allowing for the generation of similar visual concepts to those in the original image. In this case, though the final soft prompt is effective, optimization through a diffusion model is very expensive, and the prompts are neither interpretable nor portable.\nDiscrete Optimization. Discrete optimizers have long been used to train neural networks with quantized (e.g. binary) weights. In that context, the approach of re-projecting between gradient steps is known as stochastic rounding. However, it is known that this approach lacks the convergence guarantees of continuous optimization (Li et al., 2017). Over the last decade, stochastic rounding has been replaced by newer optimizers that maintain a continuous, rather than discrete, representation of the weights (Courbariaux et al., 2015). These optimizers consistently result in higher accuracy (Rastegari et al., 2016; Courbariaux et al., 2016) and avoid local minima (Li et al., 2017).\nWe take inspiration from these lessons learned in the binary networks community and adapt them to refine and simplify discrete optimizers for language."
        },
        {
            "heading": "3. Methodology",
            "text": "Learning Hard Prompts. We now present our effective and easy-to-use technique for discrete prompt optimization. The process requires the following inputs: a frozen model, \u03b8, a sequence of learnable embeddings, P = [ei, ...eM], ei \u2208 Rd, where M is the number of \u201ctokens\u201d worth of vectors to optimize, and d is the dimension of the embeddings. Additionally, we employ an objective function L. The discreteness of the token space is realized using a projection function, ProjE, that takes the individual embedding vectors ei in the prompt and projects them to their nearest neighbor in the embedding matrix E|V |\u00d7d where |V | is the vocabulary size of the model, and we denote the result of this operation as P\u2032 = ProjE(P) := [ProjE(ei), ...ProjE(eM)]. Additionally, we define a broadcast function, B : R(M\u00d7d) \u2192 R(M\u00d7d\u00d7b) that repeats the current prompt embeddings (P) in the batch dimension b times.\nFormally, to learn a hard prompt, we minimize the following risk by measuring the performance of P on the task data: R(P\u2032) = ED(L(\u03b8(B(P,X)),Y)).\nOur Method. We propose a simple but efficient gradientbased discrete optimization algorithm that combines the advantages of the baseline discrete optimization methods and soft prompt optimization. The steps of our scheme, which we call PEZ, are concretely defined in Algorithm 1. The method maintains continuous iterates, which in our\nAlgorithm 1 Hard Prompts made EaZy: PEZ Algorithm Input: Model \u03b8, vocabulary embedding E|V |, projection function Proj, broadcast function B, optimization steps T , learning rate \u03b3, Dataset D Sampled from real embeddings: P = [ei, ...eM] \u223c E|V | for 1, ..., T do\nRetrieve current mini-batch (X,Y ) \u2286 D. Forward Projection: P\u2032 = ProjE(P) Calculate the gradient w.r.t. the projected embedding: g = \u2207P\u2032Ltask(B(P\u2032, Xi), Yi, \u03b8) Apply the gradient on the continuous embedding: P = P\u2212 \u03b3g\nend for Final Projection: P = ProjE[P] return P\napplications corresponds to a soft prompt. During each forward pass, we first project the current embeddings P onto the nearest neighbor P\u2032 before calculating the gradient. Then, using the gradient of the discrete vectors, P\u2032, we update the continuous/soft iterate, P."
        },
        {
            "heading": "4. Prompt Inversion with CLIP",
            "text": "Our method for learning hard prompts is perfectly suited to multimodal vision-language models. With these models, like CLIP (Radford et al., 2021), we can use PEZ to discover captions which describe one or more target images. In turn, these discovered captions can be deployed as prompts for image generation applications. Since most text-guided diffusion models utilize pre-trained text encoders, such as the CLIP text encoder, and freeze them during training, we can discover prompts using these pre-trained text encoders that are directly relevant for downstream diffusion models. For instance, we can optimize a caption which describes an image and use this caption as a prompt for a diffusion model to generate other images with the same content.\nSince the CLIP model has its own image encoder, we can leverage it as a loss function to drive our PEZ method. This way we are optimizing prompts only for their cosine similarity to the CLIP image encoder, and avoiding gradient calculations on the full diffusion model altogether.\nFormally, given a text encoder function f and an image encoder function g, we optimize the hard prompt embedding P corresponding to a target image x by minimizing the following objective: L(P, x) = 1\u2212 S(f(P), g(x)), where S is the cosine similarity between two vectors."
        },
        {
            "heading": "4.1. Experimental Setting",
            "text": "We conduct experiments on four datasets with diverse distributions: LAION (Schuhmann et al., 2022), MS COCO (Lin et al., 2014), Celeb-A (Liu et al., 2015), and Lexica.art (Santana, 2022). LAION comprises over 5 billion diverse images scraped from the internet, including photos and paintings. MS COCO mainly contains real-life photographs with multiple common objects, whereas Celeb-A consists of celebrity portraits. Lexica.art is a set of AI-generated paintings with their prompts.\nWe measure the quality of the prompt via image similarity between original (target) image, and an image generated using the learned hard prompt. To do so, we use a larger reference CLIP model, OpenCLIP-ViT/G, that was not used during optimization and serves as a neutral metric for se-\nmantic similarity between the images.\nWe choose Stable Diffusion-v2 (Rombach et al., 2022) as our generative model, and the open-source CLIP model, OpenCLIP-ViT/H (Cherti et al., 2022) for crafting the prompt, as both share the same text encoder. During the prompt optimization process, we use a generic learning rate of 0.1 and run 3000 optimization steps using the AdamW optimizer (Loshchilov & Hutter, 2017). For Stable Diffusionv2, we set the guidance scale to 9 and the number of inference steps to 25. For each dataset, we randomly sample 100 data points and average CLIP scores over 5 runs with different random seeds.\nA natural baseline for hard prompt discovery with CLIP\nis the CLIP Interrogator1. To generate a descriptive hard prompt, this tool first uses a pre-trained captioning model, BLIP (Li et al., 2022) to create a caption of the target image. Then, top-k keywords from a pre-collected bank of keywords are appended to the caption based on CLIP scores between the keywords and the target image. These keywords were collected from various sources, including 5,265 artist names like \u201cVan Gogh\u201d and 100,970 phrases from prompt engineering, resulting in a diverse set. We find this keyword bank to contain most of the phrases from the Lexica.art dataset. CLIP Interrogator then greedily samples keywords until the prompt reaches CLIP\u2019s token length limit of 77."
        },
        {
            "heading": "4.2. Results",
            "text": "We show example hard prompts learned using our method and corresponding generations in Figure 2. The generated\n1https://github.com/pharmapsychotic/ clip-interrogator\nimages clearly show that the prompts effectively capture the semantic features of the target images. Further, the generations are highly similar to the original images as measured by CLIP score and under visual inspection. Additionally, the hard prompts do not overfit to the original target image and produce a diverse set of generated images given different random seeds.\nPrompts are human readable, containing a mix of real words and gibberish (non-word token sequences). However, the valid words that are included in the prompts provide a significant amount of information about the image. For example, in the first row, we can see the words \u201cmilkyway\u201d and \u201ccampfire,\u201d which are the two main elements in the target image. Interestingly, the optimized prompts may also include emojis, like present in the second row. represents the trees on the side and also the color theme of the image. The optimization process seems to choose these emojis to include useful information while keeping the prompt concise.\nFurther, we present quantitative evaluations in Table 1. Our method performs consistently across all four datasets and outperforms other gradient-based optimization baselines (full table can be found in Table 7). Notably, we can achieve similar performance to CLIP Interrogator, which has the highest CLIP score on LAION, MS COCO, Lexica.art, but not Celeb-A (The keyword bank in CLIP Interrogator does not include many words related to real human faces). However, CLIP Interrogator uses a large curated prompt dataset, the image captioning model BLIP, and a large number of tokens (as many as 77), while our proposed method only uses the CLIP model for prompt discovery and 8 tokens in total demonstrating its simultaneous simplicity and strength.\nWe ablate each of these differences. To do so, we include the keyword bank in our optimization method and only allow projections onto tokens from the keyword bank. Overall, we find that when adding this constraint to our model, and disabling BLIP to compare both methods on equal footing, we recover most of the quantitative difference between the methods on LAION and Lexica.art. Additionally, reducing the token length for the CLIP Interrogator, leads to a sharp drop in performance, again, particularly when normalizing by comparing both approaches at equal token lengths of 8. We note that even though Stable Diffusion and CLIP share the same text encoder, soft prompts do not transfer well compared to all hard prompt methods in our evaluation.\nPrompt Length. We further ablate the optimal number of tokens. In Figure 5, we find that longer prompts do not necessarily produce better results when generating with Stable Diffusion, even though they strictly reduce loss on the CLIP image encoder. Long prompts thus overfit and are less transferable, and we empirically find a length of 16 to\nresult in the most generalizable performance."
        },
        {
            "heading": "4.3. Style Transfer",
            "text": "The proposed approach can also be easily adapted to style transfer. We follow the setting investigated with soft prompts in Gal et al. (2022) but with our hard prompts. Given several examples that share the same style, we extract their shared style characteristics into a single hard prompt and use this prompt to apply the style to new objects or scenes. Figure 3 presents two examples of style transfer, showing that our method can easily embed the shared style elements in the prompt and apply them to novel concepts. Templates and learned prompts can be found in Appendix A.1."
        },
        {
            "heading": "4.4. Prompt Concatenation",
            "text": "Learned hard prompts are also very useful as composable building blocks for intricate scenes. We test this in Figure 4, where we separately generate prompts for two unrelated images, and then fuse both images by concatenating their prompts. We find that even different concepts, such as painted horses on a beach and a realistic sunset in a forest can be combined via their generated prompts."
        },
        {
            "heading": "4.5. Prompt Distillation",
            "text": "Another application where we can use our prompt optimization method is prompt distillation, reducing the length of prompts while preserving their capability. Distillation is useful in situations where the text encoder of the diffusion model has a limited maximum input length, such as the CLIP model, which has a maximum input length of 77 tokens. Also, long prompts may contain redundant and unimportant information, especially when hand-crafted, so we aim to distill their essence, preserving only important information in the prompt. We optimize a shorter prompt to match the features of the longer prompt simply based on its text encoder f . Given a target prompt\u2019s embedding Ptarget and learnable embedding e, we simply modify our loss into: L = 1\u2212 Sim(f(Ptarget), f(P)). We define the distillation ratio by |P|/|Ptarget|.\nIn Figure 6, we show images generated by the original prompts and the distilled prompts with four different dis-\ntillation ratios: 0.7, 0.5, 0.3, 0.1. We see here that even with only 3 or 4 tokens, the hard prompts can still generate images very similar in concept to the original, successfully distilling the longer human-made instructions."
        },
        {
            "heading": "5. Discrete Prompt Tuning with Language Models",
            "text": "In the text-to-text setting, the goal of Algorithm 1 is to discover a discrete sequence of tokens, the hard prompt, that will prompt the language model to predict the outcome of a classification task. Since an important property of text is its fluency, Shi et al. (2022) find that fluency can increase a prompt\u2019s readability and performance. Thus, we define the optimization objective in this section as a weighted function of task loss and fluency loss,\nL = (1\u2212 \u03bbfluency)Ltask + \u03bbfluencyLfluency.\nWe set \u03bb = 0.003 similar to Shi et al. (2022) for all methods, and we ablate our method without fluency (\u03bb = 0), which we denote as no fluency. We set out to show that hard prompts generated by this approach are successful both when transferring between a number of transformer-based language models, and also when used to discover prompts in few-shot settings. An attractive quality of these prompts, especially for language applications, is that they can be optimized on smaller language models and then transferred to other, much larger models."
        },
        {
            "heading": "5.1. Datasets and Setup",
            "text": "We evaluate Algorithm 1 against related algorithms on three classification tasks, two sentiment analysis tasks, SST-2 (Socher et al., 2013) and Amazon Polarity (McAuley & Leskovec, 2013), and a 4-way classification task, AGNEWS (Zhang et al., 2015). We build on the setting explored in Ding et al. (2022) and optimize hard prompts using GPT-2 Large (774M parameters) (Radford et al., 2019) with the Adafactor optimizer (Shazeer & Stern, 2018) and a batch size of 32 (Lester et al., 2021a). We provide details for prompt templates and verbalizers in Table 4.\nTransferability Set-up. To test transferability, we generate prompts from GPT-2 Large for 5000 steps. We then select the five prompts with the highest average validation accuracy for each technique and test them on larger models. We test the transferred text on: GPT-2 XL, T5-LM-XL, OPT-2.7B, and OPT-6B (Radford et al., 2019; Lester et al., 2021b; Zhang et al., 2022), verifying the reliability of the proposed algorithm over related techniques and testing whether the hard prompt can reliably boost performance. Thus, we also consider a baseline of empty prompts, with only the template.\nFew-Shot Setup. For the few-shot setting, we optimize each prompt for 100 epochs on GPT-2 Large on the AGNEWS dataset, where we sample two examples (k = 2) and four examples (k = 4) from each class to obtain the training set. Additionally, we create a holdout set of the same size, and finally validate the prompts on the entire validation set."
        },
        {
            "heading": "5.2. Results",
            "text": "We verify that our method is comparable to other methods in the sentiment analysis setting and outperforms the other methods on AGNEWS by about 2%. See Table 5 for details.\nPrompt Transferability. Table 2 shows for each method the five prompts trained on GPT-2 Large transferred to other LLMs. Interestingly, simply scaling a model\u2013with no additional training\u2013does not guarantee that performance will scale accordingly.2 We see that all gradient-based\n2A quick experiment with and without the template on GPT2 Large and XL showed that the template boosts performance\nmethods are able to transfer compared to evaluating just the template, finding that our prompts trained with the fluency constraint transfer better than the other prompts. Additionally, we can see the largest boost from OPT-6.7B with our fluent method with about a 14% increase over just the template baseline. Additionally, we see our AGNEWS prompts are able to transfer from GPT-2 Large to GPT-2 XL in Table 6 of the Appendix.\nPrompt Discovery. Table 3 shows that even with just a few shots, we can achieve high validation accuracy compared to our prepended counterparts. It is worth noting that each few-shot run takes about 5 minutes.\nWe run 100 seeds where the training set contains k samples from each class and also qualitatively examine the top prompts. Although many of the prompts are non-interpretable, many are also coherent. For example, even for k = 2, some of the prompts included news sources like \u201cBBC\u201d, while other prompts find new approaches to the news classification task considering the text coming from a blog: \u201cBrian blog,\u201d or \u201cBlog Revolution analyze.\u201d Due to the efficiency of these gradient-based methods, these methods can allow new ways for prompt engineers to discover novel prompts."
        },
        {
            "heading": "6. Safety Concerns",
            "text": "Token or word-level content filters are often used in textto-image diffusion model APIs to prevent the generation of\ndifferently for different models.\nNSFW or copyrighted content. For instance, the image generation API Midjourney has banned prompts containing the substring \u201cAfghan\u201d due to a copyright issue with the famous photo of an Afghan girl 3.\nHowever, prompt optimization can be used as a mechanism to bypass simple rule-based content filters. PEZ can generate a prompt that avoids banned tokens, yet still matches textual features with the original target prompt \u201cAfghan girl.\u201d Figure 7 shows the output of Midjourney using an optimized prompt which successfully reproduces the banned image without containing the banned word \u201cAfghan.\u201d Note that the prompt seems to incorrectly associate the subject of the image, Sharbat Gula, with the Taliban.\nEven if a defender now iterates the block-list and bans additional words from the adversarial prompt, an attacker can consistently optimize around addition content restrictions, as we show in supplementary material Figure 10. Overall, we suspect that only complete feature-based content detectors have the potential to mitigate these concerns for model owners (Rando et al., 2022)."
        },
        {
            "heading": "7. Conclusion",
            "text": "We propose a new method that utilizes continuous embeddings to reliably optimize hard prompts. The key advantage of our method is the use of continuous, i.e. soft, prompts as intermediate variables during the optimization of hard prompt tokens, leveraging gradient-based optimization. This way, the algorithm selects locations in embedding space where discrete embeddings are useful, rather than simply optimizing a soft prompt and later projecting onto nearby token embeddings in the hopes that these nearby hard prompts will perform well too. Additionally, as our\n3https://en.wikipedia.org/wiki/Afghan_ Girl\nmethod utilizes gradients across all steps by accumulating them into the soft prompt, this process makes optimization more robust to learning rates and potential noise in the data.\nAlthough our work makes progress toward prompt optimization, the community\u2019s understanding of language model embedding space is still in its infancy, and a deeper understanding of the geometry of the embedding space will likely enable even stronger prompt optimization in the future.\nOverall, we show through our experiments that hard prompts can be easily generated and flexibly used in practical applications. Yet, a limitation of hard prompts is that even though they are human-readable, they may still contain several un-interpretable tokens. Additionally, hard prompts may possibly extract harmful phrases or sensitive content from a language model\u2019s training data. Even though we did not observe specific instances of this behavior, it is a concern that should be taken into account in future applications."
        },
        {
            "heading": "8. Acknowledgements",
            "text": "This work was made possible by the Office of Naval Research (N000142112557), the ONR MURI program, the National Science Foundation (IIS-2212182), and Capital One Bank."
        },
        {
            "heading": "A. Appendix",
            "text": "A.1. Additional Results for Prompt Inversion with CLIP\nWe provide more qualitative results in Figure 9.\nFor each example in Figure 3, we use the following templates respectively: \u201ca tiger in the style of {}\u201d, \u201cthe streets of Paris in the style of {}\u201d, \u201ca calculator in the style of {}\u201d, \u201ca rocket in the style of {}\u201d, where {} is replaced with the hard prompts:\nresonvillains stargazing illustration tutorials sma internationalwomensday watercolor fiberlilycamila yokohama -sorrow fluids latest\nnpr anime novels pureibanganesha irvin paints encapsulmondo illustrillustroversized sultanconan \u00a2\nfor experiments 1 and 2, respectively.\nA.2. Additional Experiments and Details for Text-to-Text Hard Prompting\nBaseline Objective Formulations Formally, we define a AutoPromptSGD step as,\nPi+1 = ProjE[Pi \u2212 \u03b7\u2207PiL(B(Pi, Xi), Yi, \u03b8)]\nAdditionally, define FluentPrompt updates follows,\nPi+1 = ProjE[Pi \u2212 \u03b7\u2207PiL(B(Pi, Xi), Yi, \u03b8) + \u221a 2\u03b7\u03b2iz]\nDetails for Section 5 For Table 5, we report the best validation accuracy across three learning rates (0.1, 0.3, and 0.5), and for FluentPrompt and AutoPromptSGD we used the learning reported (1, 3, and 10) and follow Shi et al. (2022) for the remaining hyperparameters for FluentPrompt. For these experiments, we prepend our 10 token prompt to each input text. We employ early stopping for all methods using a hold-out set of 5000 examples for each dataset, evaluating every 100 steps.\nTable 5 shows that we are comparable to other methods in sentiment analysis and outperform the other methods on AGNEWS by about 2%. Examining the prompts, we find prompts are not coherent English for any of the methods. However, it does produce relevant tokens and phrases. For example, our method for SST-2 with the fluency constraint produced \u201cnegative vibeThis immatureollywood MandarinollywoodThis energetic screenplay.\u201d 4 This suggests the\n4Although we initialize the tokens with the label tokens, when examining the prompt over the optimization process, all tokens moved away from the initial tokens. This suggests that the process was able to relearn the class label.\noptimization process is finding relevant words to the task but lacks the ability to create full sentences."
        }
    ],
    "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery",
    "year": 2023
}