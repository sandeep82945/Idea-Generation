{
    "abstractText": "In this study, we address the emerging field of Streaming Federated Learning (SFL) and propose local cache update rules to manage dynamic data distributions and limited cache capacity. Traditional federated learning relies on fixed data sets, whereas in SFL, data is streamed, and its distribution changes over time, leading to discrepancies between the local training dataset and long-term distribution. To mitigate this problem, we propose three local cache update rules First-InFirst-Out (FIFO), Static Ratio Selective Replacement (SRSR), and Dynamic Ratio Selective Replacement (DRSR) that update the local cache of each client while considering the limited cache capacity. Furthermore, we derive a convergence bound for our proposed SFL algorithm as a function of the distribution discrepancy between the long-term data distribution and the client\u2019s local training dataset. We then evaluate our proposed algorithm on two datasets: a network traffic classification dataset and an image classification dataset. Our experimental results demonstrate that our proposed local cache update rules significantly reduce the distribution discrepancy and outperform the baseline methods. Our study advances the field of SFL and provides practical cache management solutions in federated learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Heqiang Wang"
        },
        {
            "affiliations": [],
            "name": "Jieming Bian"
        },
        {
            "affiliations": [],
            "name": "Jie Xu"
        }
    ],
    "id": "SP:84de327fa14d9441bad882a41a2acbee1b011ce1",
    "references": [
        {
            "authors": [
                "T. Li",
                "A.K. Sahu",
                "A. Talwalkar",
                "V. Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50\u201360, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W.Y.B. Lim",
                "N.C. Luong",
                "D.T. Hoang",
                "Y. Jiao",
                "Y.-C. Liang",
                "Q. Yang",
                "D. Niyato",
                "C. Miao"
            ],
            "title": "Federated learning in mobile edge networks: A comprehensive survey",
            "venue": "IEEE Communications Surveys & Tutorials, vol. 22, no. 3, pp. 2031\u20132063, 2020.",
            "year": 2031
        },
        {
            "authors": [
                "Q. Yang",
                "Y. Liu",
                "T. Chen",
                "Y. Tong"
            ],
            "title": "Federated machine learning: Concept and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), vol. 10, no. 2, pp. 1\u201319, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "O.A. Wahab",
                "A. Mourad",
                "H. Otrok",
                "T. Taleb"
            ],
            "title": "Federated machine learning: Survey, multi-level classification, desirable criteria and future directions in communication and networking systems",
            "venue": "IEEE Communications Surveys & Tutorials, vol. 23, no. 2, pp. 1342\u20131397, 2021. 9",
            "year": 2021
        },
        {
            "authors": [
                "S.U. Stich"
            ],
            "title": "Local sgd converges fast and communicates little",
            "venue": "International Conference on Learning Representations, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Yu",
                "S. Yang",
                "S. Zhu"
            ],
            "title": "Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 5693\u20135700.",
            "year": 2019
        },
        {
            "authors": [
                "J. Wang",
                "G. Joshi"
            ],
            "title": "Cooperative sgd: A unified framework for the design and analysis of communication-efficient sgd algorithms",
            "venue": "ICML Workshop on Coding Theory for Machine Learning, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.P. Karimireddy",
                "S. Kale",
                "M. Mohri",
                "S. Reddi",
                "S. Stich",
                "A.T. Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "International Conference on Machine Learning. PMLR, 2020, pp. 5132\u20135143.",
            "year": 2020
        },
        {
            "authors": [
                "X. Li",
                "K. Huang",
                "W. Yang",
                "S. Wang",
                "Z. Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "arXiv preprint arXiv:1907.02189, 2019.",
            "year": 1907
        },
        {
            "authors": [
                "H. Yang",
                "X. Zhang",
                "P. Khanduri",
                "J. Liu"
            ],
            "title": "Anarchic federated learning",
            "venue": "International Conference on Machine Learning. PMLR, 2022, pp. 25 331\u201325 363.",
            "year": 2022
        },
        {
            "authors": [
                "D. Jhunjhunwala",
                "P. Sharma",
                "A. Nagarkatti",
                "G. Joshi"
            ],
            "title": "Fedvarp: Tackling the variance due to partial client participation in federated learning",
            "venue": "Uncertainty in Artificial Intelligence. PMLR, 2022, pp. 906\u2013916.",
            "year": 2022
        },
        {
            "authors": [
                "H. Yang",
                "M. Fang",
                "J. Liu"
            ],
            "title": "Achieving linear speedup with partial worker participation in non-iid federated learning",
            "venue": "International Conference on Learning Representations, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Lu",
                "A. Liu",
                "F. Dong",
                "F. Gu",
                "J. Gama",
                "G. Zhang"
            ],
            "title": "Learning under concept drift: A review",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 12, pp. 2346\u20132363, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T.R. Hoens",
                "R. Polikar",
                "N.V. Chawla"
            ],
            "title": "Learning from streaming data with concept drift and imbalance: an overview",
            "venue": "Progress in Artificial Intelligence, vol. 1, no. 1, pp. 89\u2013101, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Y. Chen",
                "Z. Chai",
                "Y. Cheng",
                "H. Rangwala"
            ],
            "title": "Asynchronous federated learning for sensor data with concept drift",
            "venue": "2021 IEEE International Conference on Big Data (Big Data). IEEE, 2021, pp. 4822\u20134831.",
            "year": 2021
        },
        {
            "authors": [
                "G. Canonaco",
                "A. Bergamasco",
                "A. Mongelluzzo",
                "M. Roveri"
            ],
            "title": "Adaptive federated learning in presence of concept drift",
            "venue": "2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021, pp. 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "E. Jothimurugesan",
                "K. Hsieh",
                "J. Wang",
                "G. Joshi",
                "P.B. Gibbons"
            ],
            "title": "Federated learning under distributed concept drift",
            "venue": "arXiv preprint arXiv:2206.00799, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F.E. Casado",
                "D. Lema",
                "M.F. Criado",
                "R. Iglesias",
                "C.V. Regueiro",
                "S. Barro"
            ],
            "title": "Concept drift detection and adaptation for federated and continual learning",
            "venue": "Multimedia Tools and Applications, vol. 81, no. 3, pp. 3397\u20133419, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Jin",
                "L. Jiao",
                "Z. Qian",
                "S. Zhang",
                "S. Lu"
            ],
            "title": "Budget-aware online control of edge federated learning on streaming data with stochastic inputs",
            "venue": "IEEE Journal on Selected Areas in Communications, vol. 39, no. 12, pp. 3704\u20133722, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Gong",
                "Z. Zheng",
                "F. Wu",
                "B. Li",
                "Y. Shao",
                "G. Chen"
            ],
            "title": "Ode: A data sampling method for practical federated learning with streaming data and limited buffer",
            "venue": "arXiv preprint arXiv:2209.00195, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A.W. Moore",
                "K. Papagiannaki"
            ],
            "title": "Toward the accurate identification of network applications",
            "venue": "International workshop on passive and active network measurement. Springer, 2005, pp. 41\u201354.",
            "year": 2005
        },
        {
            "authors": [
                "M. Finsterbusch",
                "C. Richter",
                "E. Rocha",
                "J.-A. Muller",
                "K. Hanssgen"
            ],
            "title": "A survey of payload-based traffic classification approaches",
            "venue": "IEEE Communications Surveys & Tutorials, vol. 16, no. 2, pp. 1135\u20131156, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "V.F. Taylor",
                "R. Spolaor",
                "M. Conti",
                "I. Martinovic"
            ],
            "title": "Robust smartphone app identification via encrypted network traffic analysis",
            "venue": "IEEE Transactions on Information Forensics and Security, vol. 13, no. 1, pp. 63\u201378, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Liu",
                "L. He",
                "G. Xiong",
                "Z. Cao",
                "Z. Li"
            ],
            "title": "Fs-net: A flow sequence network for encrypted traffic classification",
            "venue": "IEEE INFOCOM 2019- IEEE Conference On Computer Communications. IEEE, 2019, pp. 1171\u20131179.",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhang",
                "F. Li",
                "F. Ye",
                "H. Wu"
            ],
            "title": "Autonomous unknown-application filtering and labeling for dl-based traffic classifier update",
            "venue": "IEEE IN- FOCOM 2020-IEEE Conference on Computer Communications. IEEE, 2020, pp. 397\u2013405.",
            "year": 2020
        },
        {
            "authors": [
                "H. Mun",
                "Y. Lee"
            ],
            "title": "Internet traffic classification with federated learning",
            "venue": "Electronics, vol. 10, no. 1, p. 27, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Peng",
                "M. He",
                "Y. Wang"
            ],
            "title": "A federated semi-supervised learning approach for network traffic classification",
            "venue": "arXiv preprint arXiv:2107.03933, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "J. Xu"
            ],
            "title": "Friends to help: Saving federated learning from client dropout",
            "venue": "arXiv preprint arXiv:2205.13222, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION Federated learning (FL) is a distributed machine learning paradigm that enables a set of clients with decentralized data to collaborate and learn a shared model under the coordination of a centralized server. In FL, data is stored on edge devices in a distributed manner, which reduces the amount of data that needs to be uploaded and decreases the risk of user privacy leakage. While FL has gained popularity in the field of distributed deep learning, most research on FL has been conducted under ideal conditions and has not fully accounted for real-world constraints and features. Given that the client in FL is typically an edge device, we highlight two features that are more aligned with reality. The first feature, called Streaming Data, acknowledges that clients often consist of edge devices that continually receive and record data samples on-the-fly. Therefore, FL must operate on dynamic datasets that are built on incoming streaming data, rather than static ones. The second feature, called Limited Storage, recognizes that edge devices such as network routers and IoT devices have limited storage space allocated for each service and application. As a result, only a restricted amount of space can be reserved for FL training without compromising the quality of other services. This paper aims to address the lack of consideration for these two real-world features in current FL research.\nHeqiang Wang, Jieming Bian and Jie Xu are with the Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL 33146, USA.\nTo address the problem presented above, we investigate a new FL problem, called Streaming Federated Learning (SFL), where the local models of clients are trained based on dynamic datasets rather than static ones. SFL involves three different types of data distributions. The first one is the long-term (underlying) label distribution, which pertains to the data distribution of the client following a prolonged period of streaming data reception. This distribution cannot be anticipated during the training process and, due to storage capacity constraints, it is unfeasible to obtain an accurate longterm distribution by recording the whole data stream. The second type is the short-term (empirical) label distribution, which corresponds to the distribution of the client\u2019s currently received data. Short-term distributions are noisy and may vary over time, and they may not necessarily approximate the long-term distribution. The discrepancy between the long-term distribution and short-term distributions is illustrated in Fig. 1. The third type is the cached label distribution, which is the distribution of the dataset currently stored in the client and is governed by the local cache update rule. The aforementioned three distributions suggest that the primary challenge of SFL is the discrepancy between the a priori unknown long-term distribution and the distribution of cached data for training, as the training data is continually gathered from the stream. As a result, a proper local dataset update rule is essential to produce a cached distribution based on the short-term distributions so that it captures the long-term distribution as accurately as possible, thereby enhancing the learning performance. Our main contributions are summarized as follows:\n1) We formulate the SFL problem and propose new FL algorithms for SFL. Unlike conventional FL, training in SFL must be conducted on a dynamic dataset based on streaming data rather than a static dataset. The clients in SFL also only have limited storage capacity, making storing all incoming data impossible. 2) We propose and investigate three different local dataset update rules and theoretically analyze the discrepancy between the resulting cached distributions and the longterm distribution. Based on this discrepancy analysis, we further prove a convergence bound of our proposed SFL algorithm. 3) We apply SFL to address a practical problem, namely online training of network traffic classifiers. Our experiments, which use both a network traffic classification dataset and the FMNIST dataset, demonstrate that our proposed update rules outperform benchmarks in the SFL framework. ar X\niv :2\n30 3.\n16 34\n0v 1\n[ cs\n.L G\n] 2\n8 M\nar 2\n02 3\n2 The rest of this paper is organized as follows. In Section II, we discuss related works on FL and network traffic classification. Section III presents the system model and formulates the SFL problem. In Section IV, we introduce the SFL workflow, propose three local dataset update rules, and analyze discrepancies. Section V presents the convergence analysis of SFL on non-i.i.d. data and a non-convex function. The experimental results of SFL are presented in Section VI. Finally, Section VII concludes the paper."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "In recent years, FL has emerged as a promising framework for decentralized deep learning. Several works, such as [1]\u2013 [4], have provided a comprehensive introduction to FL and its research problems. Among the various challenges in FL, the convergence analysis of FedAvg and its variants stands out as particularly crucial. Early works focused on FL under the assumptions of i.i.d. datasets and full client participation, as demonstrated in [5]\u2013[7]. Most of the theoretical works suggest that convergence occurs linearly given a sufficiently large number of learning rounds. However, this assumption may not always hold in real-world FL scenarios, leading to an increasing number of works [8]\u2013[12] investigating convergence proofs of FedAvg and its variants under non-i.i.d. datasets. Although the proposed SFL differs from conventional FL, our proof is primarily inspired by [12], which is based on non-convex functions and non-i.i.d. datasets.\nRecently, there has been a growing interest in exploring non-stationary and continually evolving local datasets that are known as concept drift problems [13], [14]. Some researchers have begun to investigate the FL with concept drift [15]\u2013[18]. To address the concept drift problem under the FL setting, various approaches have been proposed, such as adjusting learning rates [16], incorporating regularization terms [15], [18], or training multiple models separately [17]. However, although SFL considered in our paper has some connection to the concept drift problem, there is a fundamental difference. In the SFL problem, the global objective function remains constant depending on the constant albeit unknown long-term label distribution. However, in the concept drift problem, the underlying distribution changes, leading to a change in the global objective function. Apart from the concept drift problems, two works [19] and [20] consider a similar streaming data structure that is more relevant to our proposed SFL. In [19], the authors propose an online approach to control local model updates on streaming data and global model aggregations of FL, with the aim of preventing training load congestion after the preparation of the entire training data and ensuring that model training is spread out with the arrival of streaming data. Meanwhile, our study focuses on exploring the discrepancy between longterm label distribution and cached label distribution that arises from FL with streaming data and limited storage. The authors in [20] introduce an online data selection framework for FL with streaming data. They aim to allow the server to exert control in a way that gradually regulates the data distribution of all clients to approach an i.i.d. distribution by facilitating additional information exchange between the server and the\nclients. However, in our work, we consider a more practical scenario where the clients themselves are responsible for data selection.\nIn terms of application, we applied SFL to online training of network traffic classifiers. Network traffic classification involves categorizing network traffic data into different types or classes based on certain characteristics of the data. There are two main categories of methods for performing network traffic classification: traditional methods and machine learningbased methods. Traditional methods mainly rely on port [21] or payload [22] traffic classification approaches. However, these methods can fail when faced with port translation or encrypted network packets [23]. With the growing popularity of deep learning, some recent approaches have utilized neural networks for traffic classification. For instance, in [24], the authors proposed FS-Net based on recurrent neural networks and autoencoder for traffic classification and packet feature mining. Another approach, described in [25], uses a DLbased autonomous learning framework for traffic classification, which can also handle unknown classes. Nonetheless, these approaches typically rely on centralized deep learning models, which may not be the optimal choice for distributed scenarios involving edge devices such as routers. Some recent works [26], [27] have suggested using the FL approach to address the issue of traffic classification. However, their proposed solutions do not take into account the gradual arrival of data in network traffic problems or the limited storage capacity of edge devices like routers."
        },
        {
            "heading": "III. PROBLEM FORMULATION",
            "text": ""
        },
        {
            "heading": "A. System Model",
            "text": "Let us consider a network consisting of one server and K clients. Unlike conventional FL frameworks that use static datasets, every client in the considered system gradually acquires data from its online data source, and each of these online data sources has a long-term (underlying) label distribution. To facilitate exposition, we discrete time into periods (each of which corresponds to a learning round as we will define shortly) and assume that each client k \u2208 {1, ...,K} receives a set Skt of Bs labeled data samples from its online data source in each period t. Each client k has a finite cache Lk of size B > Bs. For analytical simplicity, we assume that B is a multiple of Bs and denote M = BBs \u2208 Z+. Because the client cache is limited, not all labeled data samples can be stored and used for learning at the same time.\nEach client k has a long-term label distribution \u03c0k = [\u03c0k,1 \u03c0k,2 ... \u03c0k,R], where R is the total number of label classes and \u03c0k,r represents the probability that class r appears in client k, which is unknown by the client beforehand. However, the short-term (empirical) label distribution can be different from the long-term label distribution and nonstationary over time as shown in Fig. 1. For example, in network traffic classification, productivity applications may take up a large portion of network traffic in the daytime while entertainment applications are more popular at night. As a result, the application label distribution of Skt in one period is noisy and biased due to not only the finite number\n3 Long-Term\nLabel Distribution\nShort-Term\nLabel Distribution\nFig. 1: The trajectory of the long-term label distribution and the short-term label distribution.\nof instances but also the non-stationary application usage patterns. Furthermore, the short-term label distribution often does not change abruptly but exhibits temporal correlations. In other words, the application label distributions in the received labeled dataset may be similar in adjacent periods. Let nk,rt be the number of instances with label r in Skt and we denote ukt = [u k,1 t u k,2 t ... u k,R t ] as the short-term label distribution of Skt where u k,r t = n k,r t /Bs. We make the following assumptions on ukt .\nAssumption 1 (Limited Temporal Correlation). There exists an integer \u0393 > 0 such that (1) for any \u03c4 \u2264 \u0393 we have 0 < maxk,r,t E[(uk,rt \u2212\u03c0k,r)(u k,r t\u2212\u03c4\u2212\u03c0k,r)] \u2264 \u03b42 for some constant \u03b42; (2) for any \u03c4 > \u0393 we have E[(uk,rt \u2212\u03c0k,r)(u k,r t\u2212\u03c4\u2212\u03c0k,r)] = 0.\nAssumption 1 states that the temporal correlation of the label distribution is confined in a neighborhood of \u0393 periods. For analytical simplicity, we assume the same \u03b42 for any \u03c4 \u2264 \u0393 but practically it makes sense that \u03b42 is larger for smaller \u03c4 since closer periods exhibit stronger correlation. This generalization is straightforward in our framework.\nBecause the client has a finite cache, we also define the cached label distribution at client k in period t, denoted by vkt = [v k,1 t v k,2 t ... v k,R t ], as the label distribution of data currently in the cache. The cached label distribution is a joint result of both the short-term distribution and the local cache update rule.\nTo better understand these concepts, consider the network traffic classification problem. Each local area network (LAN) k connects to the network via a router/access point k, which monitors the application usage in the LAN. These routers act as the client in FL. Suppose there are a total number of R possible applications and network traffic classification aims to identify the application y \u2208 {1, ..., R} based on the data packet feature x. In our problem, we consider that labeled data packets continuously arrive at the routers depending on the application usage pattern in the LAN for training the DLbased network traffic classifier. The labeled data packets may be manually labeled with delay and the number is kept small relative to the total data traffic in order to reduce the labeling overhead and complexity. It is important to note that the network traffic classifier problem represents only one instance\nof the broader SFL problem. In utilizing the network traffic classifier problem to illustrate SFL, our aim is simply to aid the reader\u2019s comprehension of the problem."
        },
        {
            "heading": "B. Learning Objective",
            "text": "Our goal is to train a machine learning model using the limited number of labeled data samples received by the different clients. Without loss of generality, we assume that the data arrival rate to all clients is the same. Therefore, the longterm label distribution of the overall network is simply the average of that of each client, i.e., \u03c0 = 1K \u2211K k=1 \u03c0\nk. We define the loss function as f(w) = E\u03be\u223c\u03c0F (w; \u03be) where F (w; \u03be) is the objective function with data sample/s \u03be, \u03be represents the sample/s drawn from the long-term label distribution, and the loss function can further be decomposed into a weighted sum of local loss functions as follows\nf(w) = 1\nK K\u2211 k=1 fk(w) = 1 K K\u2211 k=1 E\u03be\u223c\u03c0kF k(w; \u03be) (1)\nwhere fk(w) = E\u03be\u2208\u03c0kF k(w; \u03be) is the local loss function of client k. Thus, training the machine learning model is equivalent to solving for the optimal parameter w that minimizes the loss function, i.e., minw f(w).\nBecause of the distributed nature of the network, it is impractical to send all the labeled data samples to a central location to train the model. Privacy concerns can also be another reason that forbids clients from directly exchanging data with each other. In this paper, we take the FL approach to train the machine learning model in a distributed manner assisted by a parameter server, where clients train local models based on their local data and periodically exchange the local models with a parameter server to derive the global model. However, compared to conventional FL systems where local models are trained on static local datasets, the online machine learning model must be trained on time-varying dynamic data. As the labeled data samples are received gradually over time at the clients, the clients do not have access to the long-term label distribution at the beginning but must continuously update their finite local cache for the incoming training instances. The cached label distribution in the local cache may diverge from the long-term label distribution because of the short-term non-stationarity, thereby degrading the FL performance.\nIn the next sections, we introduce the SFL architecture for online machine learning model training and investigate how different local cache updating rules affect learning performance."
        },
        {
            "heading": "IV. SFL ARCHITECTURE AND LOCAL CACHE UPDATE RULES",
            "text": ""
        },
        {
            "heading": "A. SFL Architecture",
            "text": "In the proposed SFL system, learning is organized into a series of iterative learning rounds. As previously mentioned, one period corresponds to a learning round. Each learning round t comprises the following four steps.\n1) Global Model Download. Each client k downloads the current global model wt from the parameter server.\n4 2) Local Model Update. Each client k uses wt as the initial model to train a new local model wkt+1 based on the current training data samples in its local cache Lk. Because the local cache is finite and usually small, we consider local training performs E steps of full-batch gradient descent (GD). Specifically, the local model is updated as\nwkt,0 = wt (2) wkt,\u03c4+1 = w k t,\u03c4 \u2212 \u03b7Lgkt,\u03c4 ,\u2200\u03c4 = 1, ..., E (3) wkt+1 = w k t,E (4)\nwhere gkt,\u03c4 = \u2207F k(wkt,\u03c4 ;Lkt ) is the gradient computed on the local dataset currently stored in the local cache Lkt , and \u03b7L is the local learning rate. Note that because of the short-term non-stationarity and finite cache space, \u2207F k(w;Lkt ) = E\u03be\u223c\u03c0kF (w; \u03be) does not hold. 3) Local Model Upload. Clients then upload their local model updates to the server. Typically, instead of uploading the local model wkt+1, client k may upload only the local model update \u2206kt , which is defined as the total model difference as follows:\n\u2206kt = 1\n\u03b7L (wkt,E \u2212 wkt,0) = \u2212 E\u22121\u2211 \u03c4=0 gkt,\u03c4 (5)\n4) Global Model Update. The server updates the global model by using the aggregated local model updates from the clients:\nwt+1 = wt + \u03b7\u03b7L\u2206t,where \u2206t = 1\nK K\u2211 k=1 \u2206kt (6)\nwhere \u03b7 is the global learning rate."
        },
        {
            "heading": "B. Local Cache Update",
            "text": "The key difference between conventional FL and SFL is how the local model update is performed, specifically, what data the local model is trained on. In conventional FL, the local model is trained on a static local dataset (using either all data or sampled data) whereas in SFL, the local dataset must be continuously updated as new data is received and old data\nis removed. Therefore, the local cache update rule will affect what data is used for training local models and consequently the global learning performance.\nWe illustrate the streaming data arrival and local cache updating in Fig. 2. Between two consecutive local model updates, new labeled data is received by the clients. In particular, client k receives a labeled dataset Skt by the local cache update step in round t. Then client k updates the local cache Lkt using the new data Skt and the existing data in the local cache according to some update rule \u03a6 as follows\nLkt \u2190 \u03a6(Lkt\u22121,Skt ) (7)\nThe updated local cache is then used for local model training at client k. Next, we introduce several local cache update rules.\n1) First-In First-Out (FIFO): A straightforward local cache update rule is FIFO, which is also used as a baseline for many other caching systems. Specifically, the FIFO update rule uses queuing logic to remove the oldest data so that a newly received data instance can be added. In our problem, client k simply removes the Bs oldest labeled data instances, denoted by Hkt\u22121, to make room for the new Bs labeled data instances in Skt . Mathematically,\nLkt \u2190 Lkt\u22121\\Hkt\u22121 \u222a Skt (8)\nThe cached label distribution changes as a result of the updated local cache as follows:\nvk,rt = nr(Lkt\u22121)\u2212 nr(Hkt\u22121) + nr(Skt )\nB ,\u2200r = 1, ..., R (9)\nwhere nr(X ) is the number of data instances with label r in a set X . We characterize the discrepancy between the cached distribution and the long-term distribution below.\nProposition 1. The discrepancy between the cached label distribution and the long-term label distribution by using FIFO is bounded as follows,\nE[(vk,rt \u2212 \u03c0k,r)2] \u2264 1\nM (min{2\u0393 + 1,M})\u03b42 (10)\nProof. The proof can be found in Appendix A of supplementary materials.\nProposition 1 shows that FIFO update rule can reduce the distribution discrepancy in the local cache by a factor at most min{ 2\u0393+1M , 1} compared to the short-term label distribution depending on how the short-term label distributions are temporally correlated (i.e., \u0393) and the size of the cache (i.e., M ). In particular, if the short-term label distributions are independent across time (i.e., \u0393 = 0), then FIFO is able to reduce distribution discrepancy by 1/M . Moreover, as the local cache size increases to infinity, the discrepancy diminishes asymptotically, i.e., limM\u2192\u221e E[(vk,rt \u2212 \u03c0k,r)2]\u2192 0.\nAn obvious issue with the FIFO update rule is that the cached distribution vkt can still fluctuate significantly because of the short-term non-stationarity and the finite cache size, especially when the short-term label distribution is strongly temporally correlated and the cache size is small. Next, we propose two new cache update rules tailored to SFL.\n5 2) Static Ratio Selective Replacement (SRSR): The reason why FIFO may result in a large fluctuation in the short-term label distribution is that it is unable to use historical data instances and their label distribution information. The goal of SRSR is to smooth out the short-term label distribution and make it approximate the long-term label distribution by using a moving average type of update rule. Specifically, SRSR comprises two steps.\nStep 1. SRSR computes a weighted average of the number of data instances with label r in the local cache and that of the newly received data, i.e.,\nn\u0303r = (1\u2212 Bs B \u03b8)nr(Lkt\u22121) + \u03b8nr(Skt ) (11)\nThis will be the target number of data instances with the label r in the updated local cache. Here, the scalar Bs/B ensures that the size constraint of the local cache is always satisfied since one can easily verify that for any \u03b8 \u2208 [0, 1] we have\nR\u2211 r=1 n\u0303r = (1\u2212 Bs B \u03b8)B + \u03b8Bs = B (12)\nStep 2. SRSR performs selective replacement to meet the target label numbers while utilizing the new data as much as possible. Specifically, there are two cases depending on the values of n\u0303r and nr(Skt ).\n1) Case 1: n\u0303r \u2264 nr(Skt ). In this case, SRSR removes all data with label r in Lkt\u22121 and uniformly randomly selects n\u0303r data instances with label r from Skt to insert into the local cache. 2) Case 2: n\u0303r > nr(Skt ). In this case, SRSR uniformly randomly removes nr(Lkt\u22121)+nr(Skt )\u2212 n\u0303r existing data instances with label r from the local cache and inserts all data instances with label r from Skt into the local cache.\nSince the target label numbers are met, the cached label distribution by using SRSR is thus\nvk,rt = 1\nB\n( (1\u2212 Bs\nB \u03b8)nr(Lkt\u22121) + \u03b8nr(Skt ) ) = \u03b8\nB t\u2211 \u03c4=0 (1\u2212 Bs B \u03b8)\u03c4nr(Skt\u2212\u03c4 ) (13)\nThe above equation shows that the cached label distribution takes all historical data distribution into account but discounts old information at a rate 1\u2212 BsB \u03b8.\nProposition 2. The discrepancy between the cached label distribution and the long-term label distribution by using SRSR is bounded as follows\nE[(vk,rt \u2212 \u03c0k,r)2] \u2264 2(1\u2212 \u03b8\nM )2t(\u03c0k,r)2\n+ 2(1\u2212 (1\u2212 \u03b8 M\n)2t) (1\u2212 \u03b8M ) \u2212\u0393 \u2212 (1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42 (14)\nProof. The proof can be found in Appendix B of supplementary materials.\nCorollary 1. By choosing \u03b8 sufficiently small, the bound on E[(vk,rt \u2212 \u03c0k,r)2] decreases over t. Moreover,\nlim t\u2192\u221e\nE[(vk,rt \u2212 \u03c0k,r)2] \u2264 2 (1\u2212 \u03b8M ) \u2212\u0393 \u2212 (1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42\n(15)\nProof. The proof can be found in Appendix C of supplementary materials.\nProposition 2 and Corollary 1 imply that by choosing a small \u03b8 and with a large cache size M , SRSR can achieve a small label distribution discrepancy after sufficiently many rounds. On the other hand, the convergence to that small discrepancy is slower with a smaller \u03b8. Moreover, even in the limit t \u2192 \u221e, the discrepancy bound does not vanish unless M \u2192\u221e, i.e., the local cache has an infinity capacity.\n3) Dynamic Ratio Selective Replacement (DRSR): Now, we propose the DRSR update rule that overcomes the drawbacks of FIFO and SRSR. The goal of DRSR is to maintain the cached label distribution in the cache as the time-average short-term label distribution up to the current period. To this end, DRSR first uses a dynamic weight to compute the target numbers of data instances with different labels following a formula similar to Eq.(11) in SRSR, i.e.,\nn\u0303r = (1\u2212 Bs B \u03b8t)nr(Lkt\u22121) + \u03b8tnr(Skt ) (16)\nwhere \u03b81, ..., \u03b8t is the sequence of dynamic weights. Once n\u0303r,\u2200r is computed, DRSR follows the exact same Step 2 as in SRSR to perform the selective replacement.\nProposition 3. The discrepancy between the cached label distribution and the long-term label distribution by using DRSR with \u03b8t = BBst is bounded as follows\nE[(vk,rt \u2212 \u03c0k,r)2] \u2264 2\u0393 + 1\nt \u03b42 (17)\nProof. The proof can be found in Appendix D of supplementary materials.\nProposition 3 shows that the discrepancy decreases over time, and the cached label distribution converges to the longterm label distribution at a rate of O(1/t)."
        },
        {
            "heading": "V. CONVERGENCE ANALYSIS",
            "text": "In this section, we analyze the convergence of SFL. Because of the mismatch between the long-term label distribution \u03c0k and the cached label distribution vkt of the local cache Lkt , the gradient gkt,\u03c4 = \u2207F k(wkt,\u03c4 ;Lkt ) computed in the local model update steps differs from the desired gradient on the long-term label distribution, i.e. \u2207fk(wkt,\u03c4 ). Thanks to the full batch gradient descent, we are able to characterize the difference between gkt,\u03c4 and \u2207fk(wkt,\u03c4 ) through an intermediate variable, which we name the virtual local gradient and denote as g\u0302kt,\u03c4 . Specifically, g\u0302kt,\u03c4 is defined as follows:\ng\u0302kt,\u03c4 = R\u2211 r=1 \u03c0k,r\u2207F kr (wkt,\u03c4 ;L k,r t ) (18)\n6 where \u2207F kr (wkt,\u03c4 ;L k,r t ) is the gradient computed on only the subset of data instances with label r, denoted by Lk,rt , in the current local cache Lkt . We note that g\u0302kt,\u03c4 is only imaginary since neither it is actually computed nor it can be realistically computed. This is because our algorithm does not actually divide Lkt into R subsets Lk,1, ...,Lk,R and compute the gradients on each of these sets. Instead, only a single local gradient \u2207F k(wkt,\u03c4 ;Lkt ) is computed. More critically, even with \u2207F kr (wkt,\u03c4 ;L k,r t ),\u2200r = 1, ..., R, computing g\u0302kt,\u03c4 requires the knowledge of the long-term label distribution \u03c0k, which is unknown by the algorithm.\nBefore we move on to establish the connection between gkt,\u03c4 and \u2207fk(wkt,\u03c4 ) and prove the convergence of the proposed SFL algorithm under different local cache update rules, we make the following standard assumptions.\nAssumption 2 (Lipschitz Smoothness). The local objective function is Lipschitz smooth, i.e., \u2203L > 0, such that \u2016\u2207fk(x)\u2212\u2207fk(y)\u2016 \u2264 L\u2016x\u2212 y\u2016, \u2200x, y \u2208 Rd and \u2200k.\nAssumption 3 (Unbiased Gradient Estimator). For each client k, the label-wise local gradient is unbiased, i.e., ELk,r\u2207F k(x;Lk,r) = \u2207fk,r(x) , E\u03ber\u2207F (x; \u03ber) where \u03b6r is a instance with label r.\nAssumption 4 (Bounded Dissimilarity). There exists constants \u03c3G > 0 and A \u2265 0 so that\n\u2016fk(x)\u20162 \u2264 (A2 + 1)\u2016f(x)\u20162 + \u03c32G,\u2200x,\u2200k (19)\nwhen the local loss functions are identical, A2 = 0 and \u03c32G = 0.\nAssumption 5 (Gradient Bound). The label-wise local gradient is bounded,\nE [ \u2016\u2207F k(x;Lk,r)\u20162 ] \u2264 \u03c32M ,\u2200k, \u2200r, \u2200Lk,r (20)\nSimilar assumptions are commonly used in both the nonconvex optimization and FL literature [10]\u2013[12], [28]. We adapted some of the assumptions for the label-wise local gradient.\nIn the previous section, we established the upper bound on the cached label distribution and the long-term label distribution for different local update rules. To facilitate the exposition, we introduce a unified notation \u03bbt to represent the upper bounds. Specifically,\nE[(vk,rt \u2212 \u03c0k,r)2] \u2264 \u03bb2t (21)\nThe specific forms of \u03bbt can be found in Propositions 1, 2 and 3 for FIFO, SRSR and DRSR, respectively.\nWe begin by introducing some necessary lemmas to help us with the theorem that follows.\nLemma 1. The expectations of the difference between the real local gradient gkt,\u03c4 and virtual local gradient g\u0302 k t,\u03c4 is upper bounded as:\nE[|gkt,\u03c4 \u2212 g\u0302kt,\u03c4 |2] \u2264 R2\u03bb2t\u03c32M (22)\nThe difference between the virtual local gradient g\u0302kt,\u03c4 and expected gradient \u2207fk(wkt,\u03c4 ) is upper bounded as:\nE[|g\u0302kt,\u03c4 \u2212\u2207fk(wkt,\u03c4 )|2] \u2264 2R2\u03c02\u03c32M (23)\nwhere \u03c0 = maxk,r \u03c0k,r.\nProof. The proof can be found in Appendix E of supplementary materials.\nThe following result is on the upper bound for the \u03c4 -step SGD in the full participation case with Lemma 1.\nLemma 2. For any step-size satisfying \u03b7L \u2264 18LE , we have: \u2200\u03c4 = 0, ..., E \u2212 1\nE[\u2016wkt,\u03c4 \u2212 wt\u20162] \u2264 5E\u03b72LR2\u03bb2t\u03c32M + 60E2\u03b72LR2\u03c02\u03c32M + 30E2\u03b72L\u03c3 2 G + 30E 2\u03b72L(A 2 + 1)\u2016\u2207f(x)\u20162 (24)\nProof. The proof can be found in Appendix F of supplementary materials.\nBy defining \u2206t = \u2206\u0304t + et, where \u2206t = \u2212 1K \u2211K k=1 \u2211E\u22121 \u03c4=0 g k t,\u03c4 and \u2206\u0304t = \u2212 1K \u2211K k=1 \u2211E\u22121 \u03c4=0 g\u0302 k t,\u03c4 we can obtain the convergence bound of SFL with full client participation as follows:\nTheorem 1. Let constant local and global learning rates \u03b7L and \u03b7 be chosen as such that \u03b7L \u2264 min ( 1\u221a\n60(A2+1)EL , 18LE ) and \u03b7\u03b7L \u2264 14EL . Under Assump-\ntion (2)-(5) with full client participation, the sequence of model wt in real sequence satisfies\nmin t=0,...,T\u22121 E\u2016\u2207f(wt)\u20162 \u2264 f0 \u2212 f\u2217 c\u03b7\u03b7LET + \u03a6G + \u03a6M + \u03a6L\n(25)\nwhere c is a constant, f0 , f(w0), f\u2217 , f(w\u2217), w\u2217 is the optimal model and\n\u03a6G = 30E2\u03b72LL 2\nc \u03c32G (26)\n\u03a6M = 60\u03b72LE 2L2R2\u03c02\nc \u03c32M (27)\n\u03a6L =\n( 5\u03b72LEL 2 + 3\u03b7\u03b7LLE + 1 ) R2\u03c32M\ncT\nT\u22121\u2211 t=0 \u03bb2t (28)\nProof. The proof can be found in Appendix G of supplementary materials.\nThe above convergence bound contains four parts: a vanishing term f0\u2212f\u2217c\u03b7\u03b7LET as T increases, a constant term \u03a6G whose size depends on the problem instance parameters and is independent of T , a third term \u03a6M is affected by the number of classes R and maximum ratio \u03c0, and a final term \u03a6L that depends on the cumulative gap between the real and virtual sequences. The key insight derived by Theorem 1 is that the SFL convergence bound depends on two additional terms \u03a6M and \u03a6L when compare to the conventional FL. For each client, if we could use the long-term label distribution, the cumulative ratio gap 1T \u2211T\u22121 t=0 \u03bb 2 t = 0. Consequently, the convergence bound is simply f0\u2212f\u2217c\u03b7\u03b7LET +\u03a6G+\u03a6M . However, this gap cannot be eliminated since the client cannot directly use the long-term\n7 label distribution in the local model updating. By applying the specific learning rate, with T \u2192\u221e, we can get the following corollary for the general convergence rate: Corollary 2. With learning rates \u03b7L = 1\u221aTE and \u03b7 = \u221a EK, the convergence rate of the general case under full client participation is:\nO( 1\u221a EKT ) +O(\u03c3 2 G T )\ufe38 \ufe37\ufe37 \ufe38\n\u03a6G\n+O(R 2\u03c02\u03c32M T\n)\ufe38 \ufe37\ufe37 \ufe38 \u03a6M\n+O( \u03c32M\n\u2211T\u22121 t=0 \u03bb 2 t\nT )\ufe38 \ufe37\ufe37 \ufe38\n\u03a6L\nBased on the corollary 2 above, \u03a6L is the major factor that determines whether the results converge to a stationary point without any constant terms. By substituting the values of \u03bb2t for the three update rules mentioned earlier, we can derive the corresponding final convergence rates. Furthermore, it is shown that under the DRSR update rule the SFL can eventually converge to a stationary point."
        },
        {
            "heading": "VI. EXPERIMENTS",
            "text": "Setup. Our experiments are based on two datasets: FMNIST and the network traffic classification dataset (NTC) extracted from ISCXVPN2016 as in [25]. FMNIST is a commonly used dataset for image classification tasks, while NTC is a specialized dataset for network traffic classification. It contains 45000 network packets that are divided into 10 classes, each representing a different application such as YouTube or Skype, which are encrypted traffic samples using various methods. The packet vectors can be reshaped to 39 \u00d7 39 bytes gray images. For both datasets, we use LeNet [29] as the backbone model, specifically modified according to the different datasets. All experiment results reported are the average of 10 independent runs.\nData Stream Generation. The SFL system consists of 10 clients, and every client receives training data samples from C classes in the long-term distribution, which are noni.i.d between clients. To simulate the time-varying short-term distributions, we generate 10 possible distributions for each client. In each time slot, the client receives one distribution as the short-term distribution. To capture the temporal correlation of the short-term distribution, the transition between any two short-term distributions is governed by a probability determined by the Kullback\u2013Leibler (K-L) divergence [30] between these two label distributions. The probability of distribution transition between two distributions is higher when the K-L divergence between them is lower. The long-term distribution is obtained as the stationary distribution of these short-term distributions, based on the transition matrix, which is unknown to the client in advance.\nBenchmarks. In the experiment, the following two benchmarks are used for performance comparison.\n1) Full Information (FULL). In this ideal scenario, each client has a local training dataset with a distribution the same as the long-term distribution. 2) Lazy Updates (LAZY). In this scenario, the client keeps the initial training dataset in the cache and does not update its dataset. The client then conducts local training by utilizing this static local dataset.\nAs we have analyzed in Section IV, both FIFO and SRSR update rules can converge to a stationary point with infinite cache capacity. However, since infinite cache capacity is impractical in real-world scenarios, we will only conduct experiments under finite cache capacity.\nPerformance comparison. We first compare the convergence performance between our proposed update rules and benchmarks with parameters{ B = 300, Bs = 150, C = 3, \u03b8 = 2 3 } . Fig. 3(a) and (b) plot the convergence curves on the NTC dataset and the FMNIST dataset with full client participation, respectively. Several observations are made as follows. First, DRSR, SRSR and FIFO outperform LAZY in terms of test accuracy and convergence speed, particularly in the later stages. DRSR and SRSR achieve performance close to FULL on the NTC dataset due to their ability to gradually approximate the long-term label distribution. Second, DRSR and SRSR outperform FIFO in the entire learning process, mainly attributed to their ability to retain the knowledge of past data streams. Third, the learning performance of DRSR, SRSR and FIFO is significantly better than LAZY on the NTC dataset, while the performance gain is less significant on the FMNIST dataset. Overall, DRSR and SRSR are better than FIFO on both datasets. More comparisons between DRSR and SRSR will be given later.\nDistribution discrepancy. The learning performance of SFL depends on how well it can approximate the long-term label distribution. In this part, we examine how the distribution discrepancy changes during the training process for different local dataset update rules. The per-slot discrepancy is defined\n8 as \u03c8t = \u2211 k\u2208K \u2211 r\u2208R(v k,r t \u2212\u03c0k,r)2 and the accumulated dis-\ncrepancy is defined as \u03c8 = \u2211 t\u2208T \u2211 k\u2208K \u2211 r\u2208R(v k,r t \u2212\u03c0k,r)2. Fig. 4 shows the per-slot discrepancy and the accumulated discrepancy for the different update rules. From Fig. 4(a), we can see that DRSR and SRSR exhibit lower discrepancy and fewer fluctuations compared to FIFO, and the discrepancy of DRSR decreases over time. Fig. 4(b) demonstrates that DRSR has the lowest cumulative discrepancy throughout the learning process and performs better than both SRSR and FIFO.\nImpact of streaming data size Bs. The proposed update rules, DRSR and SRSR were examined for their learning performance when trained with different values of Bs and a constant B = 300. A larger value of Bs corresponds to a larger streaming packet per round. The NTC dataset was used with two different streaming data sizes, Bs \u2208 {30, 150}, to investigate the effects of varying Bs. The results, shown in Fig. 5, indicate that both DRSR and SRSR are capable of achieving the desired level of test accuracy. The variance of DRSR decreases with training, particularly in the later stages, where it is significantly smaller than the variance of SRSR.\nImpact of cache capacity B. In this set of experiments, we investigate the impact of varying the cache capacity, represented by B, on the learning performance while keeping the ratio (BsB ) constant at 0.1. We use the NTC dataset and tested two different values of B \u2208 {100, 300}. Our results, as shown in Fig. 6, suggest that increasing B leads to higher test accuracy and faster training rates. For example, at 200 rounds, the test accuracy is 0.72 for B = 100 and 0.83 for B = 300. However, in practical situations, the cache capacity of a client\nis often limited, despite the potential for better performance with larger values of B.\nImpact of parameter \u03b8. In this section, we examine the impact of \u03b8, which tunes the amount of incoming data to put in the cache, on the learning performance of SRSR. The rest parameters are {B = 300, Bs = [30, 150], C = 3}. The experiment results are shown in Fig. 7. When Bs is large, a smaller \u03b8 leads to better learning performance. This occurs because larger \u03b8 can cause a substantial shift in the cached label distribution, leading to an increase in variation (as seen in Fig. 7(b)). However, when Bs is small, choosing a small value for \u03b8 leads to a degradation of learning performance. The reason for this is that the ratio in the cached label distribution changes at a slow pace at the beginning (as seen in Fig. 7(a)). Consequently, determining the appropriate \u03b8 beforehand is a challenging task. However, this issue can be resolved using the DRSR update rule, which reduces \u03b8 gradually over time."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "Our paper presents a novel Federated Learning (FL) framework named SFL, which differs from traditional FL by operating on a dynamic dataset. This dynamic nature of the data, coupled with the limited cache capacity on clients, results in discrepancies between the local training dataset and the longterm data distribution. We propose three update rules for the local cache update process in the SFL problem and provide a thorough theoretical analysis and experimental comparison to support our work. Future research will focus on developing more effective update rules for SFL to accelerate the training convergence speed and explore the potential of applying SFL to other practical scenarios."
        },
        {
            "heading": "APPENDIX A PROOF OF PROPOSITION 1",
            "text": "First notice vk,rt is essentially the average short-term label distribution of periods t\u2212M + 1 through t, thus\nE[(vk,rt \u2212 \u03c0k,r)2] = E[( 1\nM M\u2211 m=1 uk,rt\u2212m+1 \u2212 \u03c0k,r)2] (29)\n= 1\nM2 M\u2211 m=1 E[(uk,rt\u2212m+1 \u2212 \u03c0k,r) M\u2211 m\u2032=1 (uk,rt\u2212m\u2032+1 \u2212 \u03c0 k,r)]\n(30)\n\u2264 1 M (min{2\u0393 + 1,M})\u03b42 (31)"
        },
        {
            "heading": "APPENDIX B PROOF OF PROPOSITION 2",
            "text": "The discrepancy can be bounded as follows.\nE[(vk,rt \u2212 \u03c0k,r)2] = E[( \u03b8\nM t\u2211 \u03c4=0 (1\u2212 \u03b8 M )\u03c4uk,rt\u2212\u03c4 \u2212 \u03c0k,r)2]\n(32)\n=E[( \u03b8\nM t\u2211 \u03c4=0 (1\u2212 \u03b8 M )\u03c4 (uk,rt\u2212\u03c4 \u2212 \u03c0k,r) + (1\u2212 \u03b8 M )t(\u2212\u03c0k,r))2]\n(33)\n\u22642E[( \u03b8 M t\u2211 \u03c4=0 (1\u2212 \u03b8 M )\u03c4 (uk,rt\u2212\u03c4 \u2212 \u03c0k,r))2] + 2(1\u2212 \u03b8 M )2t(\u03c0k,r)2\n(34)\n\u2264 2\u03b8 2\nM2 t\u2211 \u03c4=0 (1\u2212 \u03b8 M )\u03c4E[(uk,rt\u2212\u03c4 \u2212 \u03c0k,r)\nt\u2211 \u03c4 \u2032=0 (1\u2212 \u03b8 M )\u03c4 \u2032 (uk,rt\u2212\u03c4 \u2032 \u2212 \u03c0 k,r)] + 2(1\u2212 \u03b8 M )2t(\u03c0k,r)2\n(35)\n\u2264 2\u03b8 2\nM2 t\u2211 \u03c4=0 (1\u2212 \u03b8 M )\u03c4 (1\u2212 \u03b8 M )\u03c4 \u0393\u2211 i=\u2212\u0393 (1\u2212 \u03b8 M )i\u03b42\n+ 2(1\u2212 \u03b8 M )2t(\u03c0k,r)2 (36)\n=2 \u03b8\nM\n1\u2212 (1\u2212 \u03b8M ) 2t 1\u2212 (1\u2212 \u03b8M )2\n( (1\u2212 \u03b8\nM )\u2212\u0393 \u2212 (1\u2212 \u03b8 M )\u0393+1\n) \u03b42\n+ 2(1\u2212 \u03b8 M )2t(\u03c0k,r)2 (37)\n=2(1\u2212 (1\u2212 \u03b8 M\n)2t) (1\u2212 \u03b8M ) \u2212\u0393 \u2212 (1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42\n+ 2(1\u2212 \u03b8 M )2t(\u03c0k,r)2 (38)\nwhere the Eq.(33) uses \u03b8M \u2211t \u03c4=0(1 \u2212 \u03b8 M ) \u03c4 + (1 \u2212 \u03b8M ) t = 1; Eq.(34) is a result of the triangle inequality; Eq.(36) uses Assumption 1."
        },
        {
            "heading": "APPENDIX C PROOF OF COROLLARY 1",
            "text": "It is easy to see that E[(vk,rt \u2212 \u03c0k,r)2] is a weighted sum of (\u03c0k,r)2 and (1\u2212 \u03b8 M ) \u2212\u0393\u2212(1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42 where the weight\n(1 \u2212 \u03b8M ) 2t decreases with t. Moreover, it is easy to prove that (1\u2212 \u03b8 M ) \u2212\u0393\u2212(1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42 is increasing in \u03b8/M . Thus, by\nchoosing \u03b8 sufficiently small, (1\u2212 \u03b8 M ) \u2212\u0393\u2212(1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42 can be\nmade smaller than (\u03c0k,r)2. Therefore, the weighted sum decreases with time and approaches (1\u2212 \u03b8 M ) \u2212\u0393\u2212(1\u2212 \u03b8M ) \u0393+1\n2\u2212 \u03b8M \u03b42 in\nthe limit."
        },
        {
            "heading": "APPENDIX D PROOF OF PROPOSITION 3",
            "text": "We bound the discrepancy as follows. First plugging \u03b8t = B Bst into Eq.(16), we have\nn\u0303r = t\u2212 1 t nr(Lkt\u22121) + B Bst nr(Skt ) (39)\n= t\u2212 1 t ( t\u2212 2 t\u2212 1 nr(Lkt\u22122) + B Bs(t\u2212 1) nr(Skt\u22121) ) + B\nBst nr(Skt ) (40)\n= B\nBst t\u2211 \u03c4=1 nr(Sk\u03c4 ) (41)\nso we can obtain vk,rt = 1 t \u2211t \u03c4=1 u k,r t , then we will get\nE[(vk,rt \u2212 \u03c0k,r)2]\n=E[( 1\nt t\u2211 \u03c4=1 uk,r\u03c4 \u2212 \u03c0k,r)2] = E[( 1 t t\u2211 \u03c4=1 (uk,r\u03c4 \u2212 \u03c0k,r))2] (42)\n= 1\nt2 t\u2211 \u03c4=1 E [ (uk,r\u03c4 \u2212 \u03c0k,r) t\u2211 \u03c4 \u2032=1 (uk,r\u03c4 \u2032 \u2212 \u03c0 k,r) ] \u2264 2\u0393 + 1 t \u03b42\n(43)\nwhere the last inequality uses Assumption 1."
        },
        {
            "heading": "APPENDIX E PROOF OF LEMMA 1",
            "text": "The difference between the real local gradient and virtual local gradient can be bounded as follows:\nE[|gkt,\u03c4 \u2212 g\u0302kt,\u03c4 |2] (44)\n= E[| R\u2211 r=1 vk,rt \u2207F k(wkt,\u03c4 ;L k,r t )\u2212 R\u2211 r=1 \u03c0k,r\u2207F k(wkt,\u03c4 ;L k,r t )|2]\n(45)\n= E[| R\u2211 r=1 (vk,rt \u2212 \u03c0k,r)\u2207F k(wkt,\u03c4 ;L k,r t )|2] (46)\n\u2264 R R\u2211 r=1 E[|(vk,rt \u2212 \u03c0k,r)\u2207F k(wkt,\u03c4 ;L k,r t )|2] (47)\n\u2264 R R\u2211 r=1 E[|(vk,rt \u2212 \u03c0k,r)|2]E[|\u2207F k(wkt,\u03c4 ;L k,r t )|2] (48) \u2264 R2\u03bb2t\u03c32M (49)\nThen the proof of the second inequality is as follows:\nE[|g\u0302kt,\u03c4 \u2212\u2207fk(wkt,\u03c4 )|2] (50)\n2 = E[| R\u2211 r=1 \u03c0k,r\u2207F k(wkt,\u03c4 ;L k,r t )\u2212 R\u2211 r=1 \u03c0k,r\u2207fk,r(wkt,\u03c4 )|2]\n(51)\n= E[| R\u2211 r=1 \u03c0k,r(\u2207F k(wkt,\u03c4 ;L k,r t )\u2212\u2207fk,r(wkt,\u03c4 ))|2] (52)\n\u2264 R\u03c02 R\u2211 r=1 E[|(\u2207F k(wkt,\u03c4 ;L k,r t )\u2212\u2207fk,r(wkt,\u03c4 ))|2] (53) \u2264 2R2\u03c02\u03c32M (54)\nwhere \u03c0 = maxk,r \u03c0k,r is the maximum ratio of the long-term label distribution."
        },
        {
            "heading": "APPENDIX F PROOF OF LEMMA 2",
            "text": "In this subsection, we will get the local updates bound,\nE[\u2016wkt,\u03c4 \u2212 wt\u20162] (55) =E[\u2016wkt,\u03c4\u22121 \u2212 wt \u2212 \u03b7Lgkt,\u03c4\u22121\u20162] (56) =E[\u2016wkt,\u03c4\u22121 \u2212 wt \u2212 \u03b7L(gkt,\u03c4\u22121 \u2212 g\u0302kt,\u03c4\u22121 + g\u0302kt,\u03c4\u22121 \u2212\u2207fk(wkt,\u03c4\u22121) +\u2207fk(wkt,\u03c4\u22121)\u2212\u2207fk(wt) +\u2207fk(wt))\u20162]\n(57) \u2264 ( 1 + 1\n2E \u2212 1\n) E\u2016wkt,\u03c4\u22121 \u2212 wt\u20162 + \u03b72LE\u2016gkt,\u03c4\u22121 \u2212 g\u0302kt,\u03c4\u22121\u20162\n+6E\u03b72LE\u2016g\u0302kt,\u03c4\u22121 \u2212\u2207fk(wkt,\u03c4\u22121)\u20162 +6E\u03b72LE\u2016\u2207fk(wkt,\u03c4\u22121)\u2212\u2207fk(wt)\u20162 + 6E\u03b72LE\u2016\u2207fk(wt)\u20162 (58)\n\u2264 ( 1 + 1\n2E \u2212 1\n) E\u2016wkt,\u03c4\u22121 \u2212 wt\u20162 + \u03b72LR2\u03bb2t\u03c32M\n+12E\u03b72LR 2\u03c02\u03c32M + 6E\u03b7 2 LL 2E\u2016wkt,\u03c4\u22121 \u2212 wt\u20162 +6E\u03b72L\u03c3 2 G + 6E\u03b7 2 L(A 2 + 1)\u2016\u2207f(x)\u20162 (59)\n\u2264 ( 1 + 1\nE \u2212 1\n) E\u2016wkt,\u03c4\u22121 \u2212 wt\u20162 + \u03b72LR2\u03bb2t\u03c32M\n+12E\u03b72LR 2\u03c02\u03c32M + 6E\u03b7 2 L\u03c3 2 G + 6E\u03b7 2 L(A 2 + 1)\u2016\u2207f(x)\u20162 (60)\nUnrolling the recursion, we get:\n1\nK K\u2211 k=1 E[\u2016wkt,\u03c4 \u2212 wt\u20162] (61)\n\u2264 \u03c4\u22121\u2211 p=0 ( 1 + 1 E \u2212 1 )p [\u03b72LR 2\u03bb2t\u03c3 2 M + 12E\u03b7 2 LR 2\u03c02\u03c32M +6E\u03b72L\u03c3 2 G + 6E\u03b7 2 L(A 2 + 1)\u2016\u2207f(x)\u20162] (62)\n\u2264(E \u2212 1) [ (1 + 1\nE \u2212 1 )E \u2212 1\n] [\u03b72LR 2\u03bb2t\u03c3 2 M\n+12E\u03b72LR 2\u03c02\u03c32M + 6E\u03b7 2 L\u03c3 2 G + 6E\u03b7 2 L(A 2 + 1)\u2016\u2207f(x)\u20162] (63)\n\u22645E\u03b72LR2\u03bb2t\u03c32M + 60E2\u03b72LR2\u03c02\u03c32M +30E2\u03b72L\u03c3 2 G + 30E 2\u03b72L(A 2 + 1)\u2016\u2207f(x)\u20162 (64)\nThis completes the proof of lemma 2."
        },
        {
            "heading": "APPENDIX G PROOF OF THEOREM 1",
            "text": "In this section, we give the proofs in detail. Due to the smoothness in Assumption (2), taking expectation of f(wt+1) over the randomness in round t, we have\nEt[f(wt+1)] (65)\n\u2264f(wt) + \u3008\u2207f(wt),Et[wt+1 \u2212 wt]\u3009+ L\n2 Et[\u2016wt+1 \u2212 wt\u20162]\n(66) =f(wt) + \u3008\u2207f(wt),Et[\u03b7\u03b7L\u2206t + \u03b7\u03b7LE\u2207f(wt)\n\u2212\u03b7\u03b7LE\u2207f(wt)]\u3009+ L\n2 \u03b72\u03b72LEt[\u2016\u2206t\u20162] (67)\n=f(wt)\u2212 \u03b7\u03b7LE\u2016\u2207f(wt)\u20162\n+\u03b7 \u3008\u2207f(wt),E[\u03b7L\u2206t + \u03b7LE\u2207f(wt)]\u3009\ufe38 \ufe37\ufe37 \ufe38 A1 + L 2 \u03b72\u03b72L Et[\u2016\u2206t\u20162]\ufe38 \ufe37\ufe37 \ufe38 A2\n(68)\nNote that the term A1 can be bounded as follows:\nA1 = \u3008\u2207f(wt),Et[\u03b7L\u2206t + \u03b7LE\u2207f(wt)]\u3009 (69) =\u3008\u2207f(wt),Et[\u03b7L\u2206\u0304t + \u03b7Let + \u03b7LE\u2207f(wt)]\u3009 (70)\n=\u3008\u2207f(wt),Et[\u2212 1\nK K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u03b7L\u2207F k(wkt,\u03c4 )\n+\u03b7Let + \u03b7LE 1\nK K\u2211 k=1 \u2207F k(wt)]\u3009 (71)\n=\u3008 \u221a \u03b7LE\u2207f(wt),\u2212 \u221a \u03b7L\nK \u221a E Et[ K\u2211 k=1 E\u22121\u2211 \u03c4=0 (\u2207F k(wkt,\u03c4 )\n\u2212\u2207F k(wt))\u2212Ket]\u3009 (72) (a1) = \u03b7LE\n2 \u2016\u2207f(wt)\u20162\n+ \u03b7L\n2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 (\u2207F k(wkt,\u03c4 )\u2212\u2207F k(wt))\u2212Ket \u2225\u2225\u2225\u2225\u2225 2\n\u2212 \u03b7L 2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2\n(73)\n(a2)\n\u2264 \u03b7LE 2 \u2016\u2207f(wt)\u20162 + \u03b7L EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 (\u2207F k(wkt,\u03c4 )\u2212\u2207F k(wt)) \u2225\u2225\u2225\u2225\u2225 2\n\u2212 \u03b7L 2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 + \u03b7LEt\u2016et\u20162 E\n(74)\n(a3)\n\u2264 \u03b7LE 2 \u2016\u2207f(wt)\u20162 + \u03b7L K K\u2211 k=1 E\u22121\u2211 \u03c4=0 Et \u2225\u2225\u2207F k(wkt,\u03c4 )\u2212\u2207F k(wt)\u2225\u22252\n\u2212 \u03b7L 2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 + \u03b7LEt\u2016et\u20162 E\n(75)\n(a4)\n\u2264 \u03b7LE 2 \u2016\u2207f(wt)\u20162 +\n\u03b7LL 2\nK K\u2211 k=1 E\u22121\u2211 \u03c4=0 Et \u2225\u2225wkt,\u03c4 \u2212 wt\u2225\u22252\n3 \u2212 \u03b7L 2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 + \u03b7LEt\u2016et\u20162 E\n(76) (a5)\n\u2264 \u03b7LE( 1\n2 + 30(A2 + 1)\u03b72LE 2L2)\u2016\u2207f(wt)\u20162 +5\u03b73LEL 2 ( R2\u03bb2t\u03c3 2 M + 12ER 2\u03c02\u03c32M + 6E\u03c3 2 G ) \u2212 \u03b7L\n2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 + \u03b7LEt\u2016et\u20162 E\n(77)\nwhere (a1) follows from that \u3008x,y\u3009 = 12 [\u2016x\u2016 2 +\u2016y\u20162\u2212\u2016x\u2212 y\u20162], (a2) is due to that E\u2016x1 + x2\u20162 \u2264 2E[\u2016x1\u20162 + \u2016x2\u20162], (a3) is due to that E\u2016x1 + ...+xn\u20162 \u2264 nE[\u2016x1\u20162 + ...\u2016xn\u20162], (a4) is due to Assumption (2) and (a5) follows from Lemma 1.\nThe term A2 can be bounded as\nA2 =Et[\u2016\u2206t\u20162] = Et[\u2016\u2206\u0304t + et\u20162] (78) (a6)\n\u2264 2Et\u2016\u2206\u0304t\u20162 + 2Et\u2016et\u20162 (79)\n\u2264 2 K2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 g\u0302kt,\u03c4 \u2225\u2225\u2225\u2225\u2225 2 + 2Et\u2016et\u20162 (80)\n(a7)\n\u2264 2 K2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 (g\u0302kt,\u03c4 \u2212\u2207F k(wkt,\u03c4 )) \u2225\u2225\u2225\u2225\u2225 2 \n+ 2\nK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 ) \u2225\u2225\u2225\u2225\u2225 2 + 2Et\u2016et\u20162 (81)\n(a8)\n\u2264 4E K R2\u03c02\u03c32M + 4 K2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 \n+ 4\nK2 Et \u2016Ket\u20162 + 2Et\u2016et\u20162 (82)\n\u22644E K R2\u03c02\u03c32M\n+ 4\nK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 + 6Et\u2016et\u20162\n(83)\nwhere both (a6) is due to that E\u2016x1 + x2\u20162 \u2264 2E[\u2016x1\u20162 + \u2016x2\u20162], (a7) follows the fact that E[\u2016x\u20162] = E[\u2016x\u2212Ex\u20162] + \u2016Ex\u20162, and (a8) is due to Assumption (3)\nSubstituting the inequalities of A1 and A2 into the original inequality, we have:\nEt[f(wt+1)] (84) \u2264 f(wt)\u2212 \u03b7\u03b7LE\u2016\u2207f(wt)\u20162\n+ \u03b7 \u3008\u2207f(wt),E[\u03b7L\u2206t + \u03b7LE\u2207f(wt)]\u3009\ufe38 \ufe37\ufe37 \ufe38 A1 + L 2 \u03b72\u03b72L Et[\u2016\u2206t\u20162]\ufe38 \ufe37\ufe37 \ufe38 A2\n(85)\n\u2264 f(wt)\u2212 \u03b7\u03b7LE\u2016\u2207f(wt)\u20162\n+ \u03b7\u03b7LE( 1\n2 + 30(A2 + 1)\u03b72LE 2L2)\u2016\u2207f(wt)\u20162\n+ 5\u03b7\u03b73LE 2L2 ( R2\u03bb2t\u03c3 2 M + 12ER 2\u03c02\u03c32M + 6E\u03c3 2 G ) \u2212 \u03b7\u03b7L\n2EK2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2\n+ \u03b7\u03b7LEt\u2016et\u20162\nE + 2EL\u03b72\u03b72L K R2\u03c02\u03c32M\n+ 2L\u03b72\u03b72L K2 Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2 + 3\u03b72\u03b72LLEt\u2016et\u20162\n(86)\n= f(wt)\u2212 \u03b7\u03b7LE( 1\n2 \u2212 30(A2 + 1)\u03b72LE2L2)\u2016\u2207f(wt)\u20162\n+ 5\u03b7\u03b73LE 2L2R2\u03bb2t\u03c3 2 M + 60\u03b7\u03b7 3 LE 3L2R2\u03c02\u03c32M + 30\u03b7\u03b7 3 LE 3L2\u03c32G\n+ (\u03b7\u03b7L E + 3\u03b72\u03b72LL ) Et \u2225\u2225\u2225\u2225\u2225 1K K\u2211 k=1 E\u22121\u2211 \u03c4=0 ( g\u0302kt,\u03c4 \u2212 gkt,\u03c4 )\u2225\u2225\u2225\u2225\u2225 2\n\u2212 (\n\u03b7\u03b7L 2EK2 \u2212 2L\u03b7 2\u03b72L K2\n) Et \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 E\u22121\u2211 \u03c4=0 \u2207F k(wkt,\u03c4 )\u2212Ket \u2225\u2225\u2225\u2225\u2225 2\n(87) (a9)\n\u2264 f(wt)\u2212 c\u03b7\u03b7LE\u2016\u2207f(wt)\u20162 + 60\u03b7\u03b73LE3L2R2\u03c02\u03c32M + 30\u03b7\u03b73LE 3L2\u03c32G + ( 5\u03b7\u03b73LE 2L2 + \u03b7\u03b7LE + 3\u03b7 2\u03b72LLE 2 ) R2\u03bb2t\u03c3 2 M\n(88) where (a9) follows from ( \u03b7\u03b7L 2EK2 \u2212 2L\u03b72\u03b72L K2 ) > 0 if \u03b7\u03b7L \u2264\n1 4EL , and that there exits a constant c > 0 satisfying ( 1 2 \u2212 30(A2 + 1)\u03b72LE 2L2) > c > 0 if \u03b7L < 1\u221a\n60(A2+1)EL .\nRearranging and summing from t = 0, ..., T \u2212 1, we have: T\u22121\u2211 t=0 c\u03b7\u03b7LEE\u2016\u2207f(wt)\u20162 (89)\n\u2264f(w0)\u2212 f(wT ) + E\u03b7\u03b7L T\u22121\u2211 t=0 60\u03b72LE 2L2R2\u03c02\u03c32M\n+E\u03b7\u03b7L T\u22121\u2211 t=0 30\u03b72LE 2L2\u03c32G\n+E\u03b7\u03b7L T\u22121\u2211 t=0 ( 5\u03b72LEL 2 + 3\u03b7\u03b7LLE + 1 ) R2\u03bb2t\u03c3 2 M (90)\nwhich implies,\nmin t=0,...,T\u22121 E\u2016\u2207f(wt)\u20162 \u2264 f0 \u2212 f\u2217 c\u03b7\u03b7LET + \u03a6G + \u03a6M + \u03a6L\n(91)\nwhere\n\u03a6G = 30E2\u03b72LL 2\nc \u03c32G (92)\n\u03a6M = 60\u03b72LE 2L2R2\u03c02\nc \u03c32M (93)\n\u03a6L =\n( 5\u03b72LEL 2 + 3\u03b7\u03b7LLE + 1 ) R2\u03c32M\ncT\nT\u22121\u2211 t=0 \u03bb2t (94)\nThis completes the proof."
        }
    ],
    "title": "On the Local Cache Update Rules in Streaming Federated Learning",
    "year": 2023
}