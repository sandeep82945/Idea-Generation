{
    "abstractText": "The peer-review process is currently under stress due to the increasingly large number of submissions to top-tier venues, especially in Artificial Intelligence (AI) and Machine Learning (ML). Consequently, the quality of peer reviews is under question, and dissatisfaction among authors is not uncommon but rather prominent. In this work, we propose \"ReviVal\" (expanded as \"REVIew eVALuation\"), a system to automatically grade a peer-review report for its informativeness. We define review informativeness in terms of its Exhaustiveness and Strength, where Exhaustiveness signifies how exhaustively the review covers the different sections and qualitative aspects1 of the paper and Strength signifies how sure the reviewer is of their evaluation. We train ReviVal, a multitask deep network for review informativeness prediction on the publicly available peer reviews, which we curate from the openreview2 platform. We annotate the review sentence(s) with labels for (a) which sections and (b) what quality aspects of the paper those refer. We automatically annotate our data with the reviewer\u2019s sentiment intensity to capture the reviewer\u2019s conviction. Our approach significantly outperforms several intuitive baselines for this novel task. To the best of our knowledge, our work is a first-of-its-kind to automatically estimate the informativeness of a peer review report.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rajeev Verma"
        },
        {
            "affiliations": [],
            "name": "Tirthankar Ghosal"
        },
        {
            "affiliations": [],
            "name": "Saprativa Bhattacharjee"
        },
        {
            "affiliations": [],
            "name": "Asif Ekbal"
        },
        {
            "affiliations": [],
            "name": "Pushpak Bhattacharyya"
        }
    ],
    "id": "SP:2654338803b1dbdc2a894967050e0da305b43a22",
    "references": [
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan"
            ],
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association",
            "year": 2019
        },
        {
            "authors": [
                "Prabhat Bharti",
                "Tirthankar Ghosal",
                "Mayank Agarwal",
                "Asif Ekbal"
            ],
            "title": "A Method for Automatically Estimating the Informativeness of Peer Reviews",
            "venue": "In Proceedings of the 19th International Conference on Natural Language Processing (ICON). Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Prabhat Kumar Bharti",
                "Tirthankar Ghosal",
                "Mayank Agarwal",
                "Asif Ekbal"
            ],
            "title": "BetterPR: A Dataset For Estimating The Constructiveness Of Peer Review Comments",
            "venue": "In Linking Theory and Practice of Digital Libraries: 26th International Conference on Theory and Practice of Digital Libraries, TPDL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Prabhat Kumar Bharti",
                "Tirthankar Ghosal",
                "Mayank Agarwal",
                "Asif Ekbal"
            ],
            "title": "PEERRec: An AI-based approach to automatically generate recommendations and predict decisions in peer review",
            "venue": "International Journal on Digital Libraries",
            "year": 2023
        },
        {
            "authors": [
                "Prabhat Kumar Bharti",
                "Tirthankar Ghosal",
                "Mayank Agrawal",
                "Asif Ekbal"
            ],
            "title": "How Confident Was Your Reviewer? Estimating Reviewer Confidence From Peer Review Texts",
            "venue": "In Document Analysis Systems: 15th IAPR International Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "Prabhat Kumar Bharti",
                "Asheesh Kumar",
                "Tirthankar Ghosal",
                "Mayank Agrawal",
                "Asif Ekbal"
            ],
            "title": "Can a Machine Generate a Meta-Review? How Far Are We",
            "venue": "In Text, Speech, and Dialogue,",
            "year": 2022
        },
        {
            "authors": [
                "Prabhat Kumar Bharti",
                "Shashi Ranjan",
                "Tirthankar Ghosal",
                "Mayank Agrawal",
                "Asif Ekbal"
            ],
            "title": "PEERAssist: Leveraging On Paper-Review Interactions To Predict Peer Review Decisions",
            "venue": "Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Lutz Bornmann",
                "H-D Daniel"
            ],
            "title": "Reliability of reviewers\u2019 ratings when using public peer review: a case study",
            "venue": "Learned Publishing 23,",
            "year": 2010
        },
        {
            "authors": [
                "Elise S Brezis",
                "Aliaksandr Birukou"
            ],
            "title": "Arbitrariness in the peer review process",
            "venue": "Scientometrics",
            "year": 2020
        },
        {
            "authors": [
                "Daniela Calvetti",
                "Lothar Reichel"
            ],
            "title": "Tikhonov regularization of large linear problems",
            "venue": "BIT Numerical Mathematics 43,",
            "year": 2003
        },
        {
            "authors": [
                "Kevin Cohen",
                "Kar\u00ebn Fort",
                "Margot Mieskes",
                "Aur\u00e9lie N\u00e9v\u00e9ol"
            ],
            "title": "Reviewing Natural Language Processing Research. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
            "venue": "Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Tirthankar Ghosal",
                "Sandeep Kumar",
                "Prabhat Kumar Bharti",
                "Asif Ekbal"
            ],
            "title": "Peer review analyze: A novel benchmark resource for computational analysis of peer reviews",
            "venue": "PLOS ONE 17,",
            "year": 2022
        },
        {
            "authors": [
                "Tirthankar Ghosal",
                "Kamal Kaushik Varanasi",
                "Valia Kordoni"
            ],
            "title": "HedgePeer: A Dataset for Uncertainty Detection in Peer Reviews",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries (JCDL)",
            "year": 2022
        },
        {
            "authors": [
                "Tirthankar Ghosal",
                "Rajeev Verma",
                "Asif Ekbal",
                "Pushpak Bhattacharyya"
            ],
            "title": "DeepSentiPeer: Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "Douglas Heaven"
            ],
            "title": "AI peer reviewers unleashed to ease publishing grind",
            "year": 2018
        },
        {
            "authors": [
                "Janine Huisman",
                "Jeroen Smits"
            ],
            "title": "Duration and quality of the peer review process: the author\u2019s perspective",
            "venue": "Scientometrics 113,",
            "year": 2017
        },
        {
            "authors": [
                "Amy C Justice",
                "Mildred K Cho",
                "Margaret A Winker",
                "Jesse A Berlin",
                "Drummond Rennie",
                "Peer Investigators"
            ],
            "title": "Does masking author identity improve peer review quality?: A randomized controlled trial",
            "venue": "Jama 280,",
            "year": 1998
        },
        {
            "authors": [
                "Dongyeop Kang",
                "Waleed Ammar",
                "Bhavana Dalvi",
                "Madeleine van Zuylen",
                "Sebastian Kohlmeier",
                "Eduard Hovy",
                "Roy Schwartz"
            ],
            "title": "A dataset of peer reviews (peerread): Collection, insights and nlp applications",
            "year": 2018
        },
        {
            "authors": [
                "Dongyeop Kang",
                "Waleed Ammar",
                "Bhavana Dalvi",
                "Madeleine van Zuylen",
                "Sebastian Kohlmeier",
                "Eduard H. Hovy",
                "Roy Schwartz"
            ],
            "title": "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018",
            "year": 2018
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal",
                "Roberto Cipolla"
            ],
            "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2018
        },
        {
            "authors": [
                "Asheesh Kumar",
                "Tirthankar Ghosal",
                "Saprativa Bhattacharjee",
                "Asif Ekbal"
            ],
            "title": "Investigations On Meta Review Generation From Peer Review Texts Leveraging Relevant Sub-Tasks In the Peer Review Pipeline",
            "venue": "In Linking Theory and Practice of Digital Libraries: 26th International Conference on Theory and Practice of Digital Libraries, TPDL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Asheesh Kumar",
                "Tirthankar Ghosal",
                "Saprativa Bhattacharjee",
                "Asif Ekbal"
            ],
            "title": "Towards automated meta-review generation via an NLP/ML pipeline in different stages of the scholarly peer review process",
            "venue": "International Journal on Digital Libraries",
            "year": 2023
        },
        {
            "authors": [
                "Asheesh Kumar",
                "Tirthankar Ghosal",
                "Asif Ekbal"
            ],
            "title": "ADeepNeural Architecture for Decision-Aware Meta-Review Generation",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries (JCDL). 222\u2013225",
            "year": 2021
        },
        {
            "authors": [
                "Sandeep Kumar",
                "Hardik Arora",
                "Tirthankar Ghosal",
                "Asif Ekbal"
            ],
            "title": "Deep- ASPeer: Towards an Aspect-level Sentiment Controllable Framework for Decision Prediction from Academic Peer Reviews",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries (JCDL)",
            "year": 2022
        },
        {
            "authors": [
                "Sandeep Kumar",
                "Tirthankar Ghosal",
                "Prabhat Kumar Bharti",
                "Asif Ekbal"
            ],
            "title": "Sharing is Caring! Joint Multitask Learning Helps Aspect-Category Extraction and Sentiment Detection in Scientific Peer Reviews",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries (JCDL). 270\u2013273",
            "year": 2021
        },
        {
            "authors": [
                "Sandeep Kumar",
                "Tirthankar Ghosal",
                "Asif Ekbal"
            ],
            "title": "DeepMetaGen: an unsupervised deep neural approach to generate template-based meta-reviews leveraging on aspect category and sentiment analysis from peer reviews",
            "venue": "International Journal on Digital Libraries (2023),",
            "year": 2023
        },
        {
            "authors": [
                "John Langford",
                "Mark Guzdial"
            ],
            "title": "The arbitrariness of reviews, and advice for school administrators",
            "venue": "Commun. ACM 58,",
            "year": 2015
        },
        {
            "authors": [
                "John Langford",
                "Mark Guzdial"
            ],
            "title": "The arbitrariness of reviews, and advice for school administrators",
            "venue": "Commun. ACM 58,",
            "year": 2015
        },
        {
            "authors": [
                "Heidi Ledford",
                "Richard Van Noorden"
            ],
            "title": "High-profile coronavirus retractions raise concerns about data oversight. https://www.nature.com/articles/ d41586-020-01695-w",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "year": 2019
        },
        {
            "authors": [
                "Maciej J Mrowinski",
                "Piotr Fronczak",
                "Agata Fronczak",
                "Marcel Ausloos",
                "Olgica Nedic"
            ],
            "title": "Artificial intelligence in peer review: How can evolutionary computation support journal editors",
            "venue": "PloS one 12,",
            "year": 2017
        },
        {
            "authors": [
                "Abby Olena"
            ],
            "title": "How to Make Scientists into Better Peer Reviewers",
            "year": 2018
        },
        {
            "authors": [
                "Simon Price",
                "Peter A Flach"
            ],
            "title": "Computational support for academic peer review: a perspective from artificial intelligence",
            "venue": "Commun. ACM 60,",
            "year": 2017
        },
        {
            "authors": [
                "Anna Rogers"
            ],
            "title": "Peer review in NLP: reject-if-not-SOTA",
            "venue": "https:// hackingsemantics.xyz/2020/reviewing-models/. (Accessed on 09/02/2020)",
            "year": 2020
        },
        {
            "authors": [
                "D Sculley",
                "Jasper Snoek",
                "Alex Wiltschko"
            ],
            "title": "Avoiding a Tragedy of the Commons in the Peer Review Process",
            "year": 2018
        },
        {
            "authors": [
                "Nihar B Shah",
                "Behzad Tabibian",
                "Krikamol Muandet",
                "Isabelle Guyon",
                "Ulrike Von Luxburg"
            ],
            "title": "Design and analysis of the nips 2016 review process",
            "venue": "The Journal of Machine Learning Research",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Tomkins",
                "Min Zhang",
                "William D. Heavlin"
            ],
            "title": "Reviewer bias in single- versus double-blind peer review",
            "venue": "Proc. Natl. Acad. Sci. USA 114,",
            "year": 2017
        },
        {
            "authors": [
                "Susan Van Rooyen",
                "Fiona Godlee",
                "Stephen Evans",
                "Nick Black",
                "Richard Smith"
            ],
            "title": "Effect of open peer review on quality of reviews and on reviewers\u2019 recommendations: a randomised trial",
            "venue": "Bmj 318,",
            "year": 1999
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Rajeev Verma",
                "Rajarshi Roychoudhury",
                "Tirthankar Ghosal"
            ],
            "title": "The lack of theory is painful: Modeling Harshness in Peer Review Comments. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
            "venue": "Association for Computational Linguistics, Online only,",
            "year": 2022
        },
        {
            "authors": [
                "Rajeev Verma",
                "Kartik Shinde",
                "Hardik Arora",
                "Tirthankar Ghosal"
            ],
            "title": "Attend to Your Review: A Deep Neural Network to Extract Aspects from Peer Reviews",
            "venue": "In Neural Information Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Caiming Xiong",
                "Victor Zhong",
                "Richard Socher"
            ],
            "title": "Dynamic Coattention Networks For Question Answering",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Wenting Xiong",
                "Diane Litman"
            ],
            "title": "Automatically predicting peer-review helpfulness. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 502\u2013507",
            "venue": "Received x XXXX 2023; revised x XXXX",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Computing methodologies\u2192 Information extraction.\nKEYWORDS peer-review evaluation, peer review informativeness, multitasking, deep neural network\n\u2217Both authors contributed equally to this research. They carried out this work while they were at IIT Patna. 1aspects of a paper like novelty, soundness, clarity, impact, substance, etc. Please see section 3.2 for details. 2https://openreview.net\nPublication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of the United States government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SIGIR-AP \u201923, November 26\u201328, 2023, Beijing, China \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0408-6/23/11. . . $15.00 https://doi.org/10.1145/3624918.3625341\nACM Reference Format: Rajeev Verma, Tirthankar Ghosal, Saprativa Bhattacharjee, Asif Ekbal, and Pushpak Bhattacharyya. 2023. ReviVal: Towards Automatically Evaluating the Informativeness of Peer Reviews. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP \u201923), November 26\u201328, 2023, Beijing, China. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3624918.3625341"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Peer review is the central system of scientific research validation. However, it is not undebated that the system is without flaws. Several studies highlight the problems like bias [38], inconsistencies [28, 37], arbitrariness [9] of the process, thus, degrading the integrity and trust on the central system of research validation. Hence, it is not uncommon that because of these issues, sometimes good research gets ignored, and sub-par papers find a place. One such unfortunate example is the retractions of referred COVID19 articles [30]. Area chairs of conferences or editors of journals are responsible for mitigating these issues by assigning papers to relevant, expert reviewers and then evaluating those reviewers\u2019 comments to make informed decisions. However, the exponential rise in paper submissions has put the peer-review system under severe stress in recent years, leading to a dearth of experienced reviewers. Sometimes, the chairs or editors are left with no other option than to delegate this critical yet voluntary job to inexperienced reviewers to the extent of even to graduate students3. Many reviewers evaluate the submitted research based on some poor indicators, e.g., whether the work is SOTA or not [35]. Fortunately, the Natural Language Processing (NLP) community has proactively paid attention to these problems. They have been arguing for changes to the current reviewing practices (e.g., the introduction of ACL Rolling Review to enforce good reviewing practises 4, EMNLP 2020/2023 releasing strict rubrics for research evaluation [12]), organizing tutorials [11] for researchers to write high-quality reviews [33]. However, peer review suffers another major problem: reviewers\u2019 tendency to invest less time in this critical voluntary job [17]. Sometimes, the reviewers themselves are unsure of their judgment on the merit of the work, which becomes evident from their poor reviews [8] [29]. Thus, these reform programs, which are primarily targeted\n3https://www.the-scientist.com/critic-at-large/opinion--exorcising-ghostwritingfrom-peer-review-66940 4https://www.aclweb.org/adminwiki/index.php?title=ACL_Rolling_Review_ Proposal\ntowards improving the reviewing practices of human reviewers, might not be sufficient.\nAn important direction towards re-establishing trust in the peerreview process should be an attempt to establish confidence in the peer-review reports. Therefore, while promoting good quality and constructive reviews, a means to detect and penalize inferior reviews should also exist. Given the ever-increasing submission load and the importance of this problem, an automated system to judge reviews for quality would largely help. In this work, we propose a mechanism to detect the surface-level indicators of good and bad reviews. Our approach takes a review and grades it for informativeness. Defining peer-review quality is not straightforward [36]; it will vary across domains. In this work, we assert that: a good review should comment on the critical sections (e.g., Problem Statement, Methodology, Experiments, Results/Analysis, etc.) and address the critical aspects of the paper (e.g., Novelty, Theoretical Soundness, etc.) while clearly bringing out the reviewer\u2019s stand on the work. As we mostly fixate our attention towards estimating the inclusiveness of the review (in terms of section, aspect, sentiment), instead of quality, we instead call our investigation a means to determine the informativeness of the review. Here, our contributions are two-fold:\n\u2022 We define informativeness via two components: Exhaustiveness and Strength. Our deep neural architecture takes as input the full-paper text and the full review text, and we train in a multi-task setting. We employ reviewer confidence score prediction and review recommendation score as scaffolds for Exhaustiveness and Strength, respectively. \u2022 We propose a scoring mechanism to manifest exhaustiveness of a review via its section and aspect coverage of the paper."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Although we did not find any prior work addressing the exact task, we do find literature on AI-support for the peer review system [15, 19, 32, 34]. Additionally, [36] enlists some major points to address in a peer review. There have been numerous discussions on the implications of AI in the peer review pipeline. Authors [39] study the effect of revealing the identity of the reviewer in the peer review process and if it can enhance the quality of peer reviews whereas [18] study the effect of blind review model. However, all these are studies on different peer review models to improve review quality. In another relevant work [44], authors predicts the helpfulness of a peer-review using manual features. Their objective is somewhat similar to ours in term of Exhaustive component.\nOur work in this direction is in line with the recent efforts to incorporate Artificial Intelligence (AI) in the peer review pipeline [13, 15, 32, 34] till generation of decision-awaremeta-reviews [6, 22\u2013 24, 27]. In our earlier work, we try to model the harshness [41], constructiveness [3], informativeness [2], uncertainty [14] of peer reviews and confidence of reviewers [5] from a computational perspective. We also investigated aspect extraction [26, 42] and decision/recommendation score prediction [4, 7, 25] from peer reviews, which served as subtasks in our current work.\nPublishers and allied stakeholders are already considering AIassistants in the peer-review cycle [16]. An AI that could grade the reviews based on some quality standards would help regulating\na thrust on poor reviews. It would also help human reviewers to take up their job seriously. Editors would be able to discard trivially written reviews and ask emergency reviewers to step in. It could also be a step towards building a reviewer profile data of their review quality, thereby increasing trust in reviewers."
        },
        {
            "heading": "3 PEER REVIEW INFORMATIVENESS",
            "text": "As we discussed earlier, quality of a review is a somewhat subjective term and may encompass many other aspects. However, our definition of informativeness builds upon the expectations from a good review in general AI/ML venues, which are probably similar across venues from other domains. As per the rubrics defined in [36], we expect a good review should demonstrate the reviewer\u2019s understanding of the work with a concise summary, an evaluation of the writing quality (clarity), comments on the novelty of the proposal, and a fair evaluation of the experiments/results, and all the other major sections of the paper. To this end, we define review informativeness in terms of its Exhaustiveness and implicit Strength. We view Exhaustiveness as to how detailed the review is, and Strength as how opinionated it is in reflecting the reviewer\u2019s perspective. Eventually, we want to distinguish reviews written by experienced reviewers vs. those written by novice/non-expert/slack reviewers. A good review should be detailed and also high on the opinion. It should be comprehensive enough to discuss the merit of the work based on aspects like Impact, Empirical Soundness, etc., and opinionated enough to bring out the reviewer\u2019s stand on the work. However, the review\u2019s exhaustiveness and opinionatedness can textually reflect in varied proportions across different reviews. For example, when the work is excellent, we do not expect to see many comments about the work in general, but the opinion will be strongly positive. Our objective is to detect non-informative reviews, which are unfortunately very common in academia. To this end, our definition of Exhaustiveness has two components. Review Exhaustiveness component serves as a simple heuristic for whether the reviewer has read/understood the paper. If the reviewer has understood the paper, we assert that the review would be more detailed with comments on several parts of the paper, be it the problem statement, the proposed methodology, or the proposed results etc. This hypothesis works against the kind of reviews illustrated in R1 below.\nR1: This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.\nICLR 2018, id=BJjquybCW\nAnother essential role of the review is to assess the impact of the work in a broader academic context. Reviewers assess the proposed work for its novelty, soundness of the research methodology, etc. Aspect Exhaustiveness score puts thrust on this critical component. A review should have information that not only discusses the research but also validates it. Hence, we split the Exhaustiveness of the review into two components: Section Exhaustiveness and Aspect Exhaustiveness. We define the third component, Review Strength, as a measure of the degree of review opinionatedness and hence\nquantify the intensity of the review. With intensity, we do not emphasize the review\u2019s positive or negative sentiment but how strong a particular sentiment is."
        },
        {
            "heading": "3.1 Section Exhaustiveness",
            "text": "Wedefine set\ud835\udc38 = {Abstract(ABS), Introduction(INT), RelatedWorks(RWK), ProblemDefinition/Idea(PDI), Data/Datasets(DAT), Methodology(MET), Experiments(EXP), Results(RES), Tables and Figures(TNF), Analysis(ANA), Future Work(FWK), Overall(OAL), Bibliography(BIB), External(EXT)}. Here, we consider the general components in a typical AI/ML paper with obvious exceptions. Considering \ud835\udc38, we score section exhaustiveness of review \ud835\udc5f as:\n\ud835\udc46\ud835\udc52 =\n( \u2211 |\ud835\udc38 | \ud835\udc59\ud835\udc56 \u2208\ud835\udc38 \ud835\udc64\ud835\udc59\ud835\udc56 \u00d7 \ud835\udc53\ud835\udc59\ud835\udc56 ) \u00d7 \ud835\udc50 \u00d7 \ud835\udc5b\ud835\udc60\u2211 |\ud835\udc38 |\n\ud835\udc59\ud835\udc56 \u2208\ud835\udc38 \ud835\udc53\ud835\udc59\ud835\udc56 (1)\nwhere,\ud835\udc64\ud835\udc59\ud835\udc56 and \ud835\udc53\ud835\udc59\ud835\udc56 denote the relative importance and the frequency of the label \ud835\udc59\ud835\udc56 \u2208 \ud835\udc38 in \ud835\udc5f . \ud835\udc5b\ud835\udc60 equals total number of sentences in the review, and \ud835\udc50 denotes the coverage of the review defined as\n\ud835\udc50 = |{\ud835\udc59 \u2208 \ud835\udc38, \ud835\udc53\ud835\udc59 \u2260 0}|\n|\ud835\udc38 | (2)\nWe consider relative importance in our scoring function to give more importance to certain sections, like Methodology, Experiments, etc., discussed in almost all the reviews. This is intuitive as ICLR is an empirical venue; it is imperative for the reviewers to talk about methodology, experiments, related work for comparison, etc. Hence, we agree that the proposed scoring depends on the venue (whether theoretical or empirical)."
        },
        {
            "heading": "3.2 Aspect Exhaustiveness",
            "text": "For Aspect Exhaustiveness, we define \ud835\udc46 = {Appropriateness (APR), Originality/Novelty (NOV), Significance/Impact (IMP), Meaningful Comparison (CMP), Presentation and Formatting (PNF), Recommendation (REC), Empirical/Theoretical Soundness (EMP), Substance (SUB), Clarity(CLA)} as the set containing this reviewer subjectivity aspects. We again score a review r for aspect exhaustiveness as:\n\ud835\udc46\ud835\udc60 =\n( \u2211 |\ud835\udc46 | \ud835\udc59\ud835\udc56 \u2208\ud835\udc46 \ud835\udc64\ud835\udc59\ud835\udc56 \u00d7 \ud835\udc53\ud835\udc59\ud835\udc56 ) \u00d7 \ud835\udc50 \u00d7 \ud835\udc5b\ud835\udc60\u2211 |\ud835\udc46 |\n\ud835\udc59\ud835\udc56 \u2208\ud835\udc46 \ud835\udc53\ud835\udc59\ud835\udc56 (3)\nSymbols carry the same meaning as before."
        },
        {
            "heading": "3.3 Review Strength",
            "text": "Since the sentiment intensity scores could vary arbitrarily across annotators, we do automatic annotation via pre-trained models. To get Review Strength score, we use the pre-trained RoBERTa [31] model, specifically RoBERTa-large. We get the sentiment intensity of each of the sentences in the review. Then, we average them to define the intensity of the review. The intensity values lie in the range [\u22121, 1]."
        },
        {
            "heading": "4 DATASET DESCRIPTION",
            "text": "To proceed with our investigation, we require the papers and reviews with their confidence and recommendation scores. To do a fair evaluation, we also consider rejected paper reviews. However,\npeer review data are sensitive and not very straightforward to obtain, especially that of rejected papers. We collect papers, reviews, recommendation scores, and confidence scores corresponding to the three editions of the International Conference on Learning Representations (ICLR) from the openreview platform.We take only the official ICLR-appointed reviewer comments for our experiments. We also study a subset of these reviews and score these reviews based on our scoring mechanism in Section 2. The dataset details are in Table 1."
        },
        {
            "heading": "4.1 Annotation Process",
            "text": "We appointed three annotators for the taskwhowere duly paid. Two annotators hold a master\u2019s degree in engineering and are currently Ph.D. students (NLP/ML). The third annotator is one of the coauthors. We intentionally chose our annotators from a technical background as they would understand the scholarly texts better. We resolved the confusion in team meetings with the primary investigators. We also conduct a one-month training exercise with the annotators on the peer review process, review aspects, general machine learning paper content, and the corresponding labels. The annotation period lasted for six months. We annotate 1002 reviews from ICLR 2018 against sections and aspects of the paper. The objective was to identify to which section and to which aspect of the paper the review text corresponds.\n\u2022 Review-Paper Section Correspondence Section 3.1 enlists the section labels common in AI/ML papers. The labels are simple and obvious if one is familiar with ML papers. Not all the labels are prominent in the review texts (e.g., ABS, INT, FWK), as the reviewers do not generally comment on certain sections unless there are some explicit issues to\nhighlight. We find that some labels like MET, EXP, RES, and ANA are very interlinked and sometimes can be hard to distinguish in the context of a review text. \u2022 Review-Paper Aspect CategoryWe follow the ACL 2016 reviewing guidelines for our aspect labels (Section 3.2) [20]. Even though ICLR reviewing criteria do not explicitly command these aspects, an ideal review is expected to more or less address those. Please note that it may happen that a review sentence may not conform to any of the aspects prescribed in the ACL 2016 guidelines. We leave out those instances.\nWe adhere to certain additional guidelines:\n(1) We perform multi-label sentence-wise annotation for each review document across sections and aspects. However, for certain exceptions (long, compound sentences addressing multiple aspects and sections), we select a text segment to label (but do sentence-level annotations in most cases). (2) We consider sentence context when a single review sentence does not make sense if considered in isolation. e.g., What was the need for this? This statement is unclear if we do not consider the preceding context. (3) We put the confusing instances in the CANNOT DECIDE category and leave out the ambiguous instances from the annotation process after discussion.\nFigure 3 shows annotation for a sample review. Figure 1 shows the distribution of the scores for these 1002 reviews. We also measure the inter-annotator agreement on a subset of data (100 full reviews for the two exhaustiveness annotations). Considering a multi-label scenario, Inter-Annotator Agreement Krippendorf\u2019s Alpha for Section Exhaustiveness is 0.73 and for Aspect Exhaustiveness is 0.70."
        },
        {
            "heading": "5 KEY OBSERVATIONS AND MOTIVATIONS",
            "text": "Our initial analysis shows an interplay among the recommendation score, confidence score and review sentiment. Some key observations are:\n\u2022 When the recommendation score, confidence score and the review sentiment (positive) are high, the number of annotations \u2211 |\ud835\udc38 | \ud835\udc59\ud835\udc56\n\ud835\udc53\ud835\udc59\ud835\udc56 from Eq. 1 is low. This seems intuitive, as there is not much to discuss when the paper is outstanding. (Eg. https://openreview.net/forum?id=BJJLHbb0-&noteId= S1f48huxz) \u2022 When the recommendation score is low, the confidence score is high, and the review sentiment (negative) is high, this also correlates with fewer annotations. When the reviewer feels the paper is inferior, they become less interested in highlighting all the deficits. \u2022 One interesting trend is when the recommendation score is high and the sentiment is positive, but the confidence score is relatively low (1-3). Even in such cases, the total number of annotations is less. One possible explanation could be non-expert reviewers writing the review without clearly understanding the work, hence less attention to detail. (E.g. https://openreview.net/forum?id=H1pri9vTZ&noteId= rkeYOm_lM)\nFor the prediction of the informativeness scores, we use the scores from the 1002 annotated reviews for supervision. However, the data is less. Given this, exploiting the relationship between the recommendation score, confidence score, sentiment, and the informativeness scores seems promising. Sadly, the recommendation and confidence scores have flaws and can hint at non-desirable relationships, as evident from the third observation above. We exploit the interplay between the recommendation score, confidence score and the informativeness scores in a multi-task framework. Such a multi-task framework can help reduce the homoscedastic aleatoric uncertainty associated with each task, thus nullifying the noisy or undesirable relationships. Our model takes a paper and a review as input, and we train it to predict the confidence score, recommendation score and the informativeness scores. In essence, our main task is to predict the review informativeness scores, and the recommendation and confidence score predictions are our scaffold tasks."
        },
        {
            "heading": "6 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "6.1 Pre-processing",
            "text": "We first convert the PDFs into .json using the ScienceParse5 library. We do not consider the figures tables in the processing as these entities are not correctly parsed. We consider only paper full-text sentences and strip off the headings, metadata, and other nonrelevant information. We show our model architecture in Figure 2."
        },
        {
            "heading": "6.2 Encoder",
            "text": "The inputs to our model are the full paper and the review text. Let us denote the paper \ud835\udc43 = (\ud835\udc60\ud835\udc5d1 , \ud835\udc60 \ud835\udc5d 2 , ..., \ud835\udc60 \ud835\udc5d \ud835\udc5b\ud835\udc5d ) and the review \ud835\udc45 = (\ud835\udc60\ud835\udc5f1, \ud835\udc60 \ud835\udc5f 2, ..., \ud835\udc60 \ud835\udc5f \ud835\udc5b\ud835\udc5f ) as a sequence of their respective sentences. For a sentence \ud835\udc60\ud835\udc5d \ud835\udc56 we get a \ud835\udc51 dimensional embedding vector s\ud835\udc5d \ud835\udc56 \u2208 R\ud835\udc51 using SciBERT encoder [1]. We use the pretrained SciBERT model to have rich representations of scientific texts, as this language model is trained on a large collection of scientific papers. We get the paper representation P \u2208 R\ud835\udc5b\ud835\udc5d\u00d7\ud835\udc51 by concatenating these vectors.\nP = s\ud835\udc5d1 \u2295 s \ud835\udc5d 2 \u2295 ... \u2295 s \ud835\udc5d \ud835\udc5b\ud835\udc5d , P \u2208 R \ud835\udc5b\ud835\udc5d\u00d7\ud835\udc51 (4)\nSimilarly, we get the review representation R \u2208 R\ud835\udc5b\ud835\udc5f \u00d7\ud835\udc51 as\nR = s\ud835\udc5f1 \u2295 s \ud835\udc5f 2 \u2295 ... \u2295 s \ud835\udc5f \ud835\udc5b\ud835\udc5f ,R \u2208 R\ud835\udc5b\ud835\udc5f \u00d7\ud835\udc51 (5)"
        },
        {
            "heading": "6.3 Context Modeling",
            "text": "We employ the basic co-attention module [43] to extract the relative representation of the paper and review with respect to each other.\nWe get the affinity matrix E \u2208 R\ud835\udc5b\ud835\udc5f \u00d7\ud835\udc5b\ud835\udc5d as follows:\nE\ud835\udc56 \ud835\udc57 = 1 \u221a \ud835\udc51 F {(s\ud835\udc5f\ud835\udc56 ) \ud835\udc47 }F {s\ud835\udc5d \ud835\udc57 }, E\ud835\udc56 \ud835\udc57 \u2208 R (6)\nHere F is a linear layer (w\ud835\udc47 x + b). Thus, E\ud835\udc56 \ud835\udc57 gives the measure of similarity between the sentence \ud835\udc60\ud835\udc5f\n\ud835\udc56 and \ud835\udc60\ud835\udc5d \ud835\udc57 , which we convert to\nattention weights using the following normalizations:\n\ud835\udc50\ud835\udc56 \ud835\udc57 = expE\ud835\udc56 \ud835\udc57\u2211\ud835\udc5b\ud835\udc5d \ud835\udc58=1 expE\ud835\udc56\ud835\udc58 , \ud835\udc50\ud835\udc56 \ud835\udc57 \u2208 R (7)\n5https://github.com/allenai/science-parse\nWe get the relative representation of review sentence s\ud835\udc5f \ud835\udc56 with\nrespect to paper \ud835\udc43 by:\nr\ud835\udc5d \ud835\udc56 = \ud835\udc5b\ud835\udc5d\u2211\ufe01 \ud835\udc57=1 \ud835\udc50\ud835\udc56 \ud835\udc57 s\ud835\udc5f\ud835\udc56 , r \ud835\udc5d \ud835\udc56 \u2208 R\ud835\udc51 (8)\nSimilarly, we get the paper representation given the review sentence s\ud835\udc5f\n\ud835\udc56 by:\np\ud835\udc5f\ud835\udc56 = \ud835\udc5b\ud835\udc5d\u2211\ufe01 \ud835\udc57=1 \ud835\udc50\ud835\udc56 \ud835\udc57 s \ud835\udc5d \ud835\udc57 , p\ud835\udc56 \u2208 R\ud835\udc51 (9)\nWe concatenate all the corresponding vectors in r\ud835\udc5d \ud835\udc56 , p\ud835\udc5f \ud835\udc56 to get\nR\ud835\udc5d \u2208 R\ud835\udc5b\ud835\udc5f \u00d7\ud835\udc51 , P\ud835\udc5f \u2208 R\ud835\udc5b\ud835\udc5f \u00d7\ud835\udc51 ."
        },
        {
            "heading": "6.4 Sequential Feature Extractor",
            "text": "Now we do the individual processing of the R\ud835\udc5d , R, and R\ud835\udc50 = R\ud835\udc5d \u2295 P\ud835\udc5f \u2295 R in our coder module. In simple terms, a coder module is a collection of \ud835\udc3e attention modules. To encode extracted useful information from the review R, a coder module can be described in terms of individual sentences of the reviews s\ud835\udc5f1, s \ud835\udc5f 2, ...s \ud835\udc5f \ud835\udc5b\ud835\udc5f . The \ud835\udc58\ud835\udc61\u210e code extraction can be done as\nh\ud835\udc5f\ud835\udc56 = ReLU(w \ud835\udc47 s\ud835\udc5f\ud835\udc56 + \ud835\udc4f) (10)\n\ud835\udc64\ud835\udc58\ud835\udc56 = exp(h\ud835\udc5f \ud835\udc56 .c\ud835\udc58 )\u2211\ud835\udc5b\ud835\udc5f\n\ud835\udc57=1 exp(h \ud835\udc5f \ud835\udc57 .c\ud835\udc58 )\n(11)\n\ud835\udc66\ud835\udc58 = \ud835\udc5b\ud835\udc5f\u2211\ufe01 \ud835\udc57=1 \ud835\udc64\ud835\udc58\ud835\udc57 h \ud835\udc5f \ud835\udc57 (12)\nThus, we get \ud835\udc661, \ud835\udc662, ..., \ud835\udc66\ud835\udc3e features representations of a review \ud835\udc45, each encoded using a special trainable coder c\ud835\udc58 which is randomly initialized at first. This module is a shallow approximation to the popular self-attention operation [40] in NLP. We perform this operation sequentially for\ud835\udc5a times. We get outputs c\ud835\udc5d , c\ud835\udc45 and cRc for each of R\ud835\udc5d ,R,Rc respectively. Note that we are working with full paper and review text; thus, a shallow attention mechanism is used. However, any feasible contextual feature representation method can be used here if computation and resources are not constraints."
        },
        {
            "heading": "6.5 Feedforward Prediction Layers",
            "text": "We concatenate the outputs from the\ud835\udc5a\ud835\udc61\u210e coder layer i.e. c\ud835\udc5d , c\ud835\udc45 and cRc together in one flattened vector c. We pass c to the two coder modules specific to Recommendation Score Prediction and Confidence Score Prediction to get c\ud835\udc5f\ud835\udc52\ud835\udc50. and c\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53 . respectively. Each task has its own prediction layer, which is a multi-layered Perceptron (MLP) layer. For the scaffold tasks prediction, each of these c\ud835\udc60 are passed to their corresponding MLPs. For the main task prediction, we pass c \u2295 c\ud835\udc5f\ud835\udc52\ud835\udc50. \u2295 c\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53 . to another MLP for the main task. Thus, the last layers of the model have task-specific trainable parameters, while the parameters in the lower layers are shared."
        },
        {
            "heading": "6.6 Training and Experimental Setup",
            "text": "To account for the different numerical ranges of the loss values across multiple scaffold tasks, we combine recommendation task lossL\ud835\udc5f\ud835\udc52\ud835\udc50. and confidence task lossL\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53 . using a method proposed in [21] to get L\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc53 \ud835\udc53 \ud835\udc5c\ud835\udc59\ud835\udc51 as\nL\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc53 \ud835\udc53 \ud835\udc5c\ud835\udc59\ud835\udc51 = 1\n2\ud835\udf0e2\ud835\udc5f\ud835\udc52\ud835\udc50. L\ud835\udc5f\ud835\udc52\ud835\udc50. + 1 2\ud835\udf0e2 \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53 . L\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53 . + log\ud835\udf0e\ud835\udc5f\ud835\udc52\ud835\udc50.\ud835\udf0e\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc53 . (13)\nSimilarly, for L\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b ,\nL\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b = 1\n2\ud835\udf0e2 \ud835\udc52\ud835\udc65\u210e.\nL\ud835\udc52\ud835\udc65\u210e. + 1\n2\ud835\udf0e2 \ud835\udc60\ud835\udc62\ud835\udc4f \ud835\udc57 .\nL\ud835\udc60\ud835\udc62\ud835\udc4f \ud835\udc57 . + 1\n2\ud835\udf0e2 \ud835\udc56\ud835\udc5b\ud835\udc61 .\nL\ud835\udc56\ud835\udc5b\ud835\udc61 .\n+ log\ud835\udf0e\ud835\udc52\ud835\udc65\u210e.\ud835\udf0e\ud835\udc60\ud835\udc62\ud835\udc4f \ud835\udc57 .\ud835\udf0e\ud835\udc56\ud835\udc5b\ud835\udc61 . (14) Here, \ud835\udf0e\ud835\udc60 are the trainable parameters to optimize during training. L\ud835\udc56 is the mean-squared error loss. We also normalize the Section Exhaustiveness, Aspect Exhaustiveness, and Review Strength scores to lie in the range [1,10]. The normalization is done using\n\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 = (\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212\ud835\udc5a\ud835\udc56\ud835\udc5b ) \u2217 9 + 1 (15)\nThis normalization of the Review Strength scores means that a larger value indicates strong positive sentiment and a lesser value indicates strong negative sentiment."
        },
        {
            "heading": "6.7 Hyperparameters details",
            "text": "The dimensionality of the sentence embeddings is 768. We use a sequence of 3 coder layers in the Sequential coder having 64, 32, 8 codes (c\ud835\udc58 ). The linear layer in each coder layer transforms the input to a 256-dimensional space. The coder in the prediction layers has eight codes. We train using a batch size of 8, with a learning rate of 0.009, using Adam optimizer. We use l2-penalty of 0.009. These\nhyperparameters were found using multiple runs with different combinations and picking the best validation set performance. The training of our main task and scaffold task are done concurrently, i.e., we have L = L\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b + L\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc53 \ud835\udc53 \ud835\udc5c\ud835\udc59\ud835\udc51 (16) where L\ud835\udc5a\ud835\udc4e\ud835\udc56\ud835\udc5b and L\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc53 \ud835\udc53 \ud835\udc5c\ud835\udc59\ud835\udc51 are defined as above."
        },
        {
            "heading": "7 EVALUATION",
            "text": "As mentioned earlier, our main task is to predict the Informativeness scores for each review. This task is a regression problem, and so are our scaffolds. We evaluate the model on the held-out test set. We split the 1002 reviews into 800/100/102 train/validation/test sets. The training process is monitored using the validation set. Since there is no gold-standard data for evaluating the main task, we use simple baselines as comparing systems. Also, since we have much less data with gold annotated scores, we cannot compare with other sophisticated deep-learning techniques as they are dataintensive. Our primary contribution to the prediction task is using multi-task learning to facilitate the predictions even using less data. We employ the following simple baselines:\n(1) Mean Baseline: As most deep neural network models for regression tasks are susceptible to predicting the mean of the prediction target, we use this baseline as a comparing system to see if our model is not suffering from the same. (2) Ridge Regression : We perform simple Ridge Regression with average sentence embeddings as the input to the model. The model for Ridge Regression [10] is a two-layer feedforward network with an L2 regularizer. We keep the model shallow and carefully tune the L2 regularizer weight to prevent overfitting (the chances of which are high due to less training data). (3) Ridge Regression with Score Fusion: In this baseline, we additionally give recommendation score and confidence score as the input to the model, along with the average sentence embeddings described in the previous baseline. (4) CNN as a Sequential Coder: We also experiment by replacing the sequential Feature Extractor (section 6.4) with a Convolutional Neural Network with max-pooling in the multi-task framework. (5) Length Baseline: As both \ud835\udc46\ud835\udc52 and \ud835\udc46\ud835\udc60 are linearly dependent on \ud835\udc5b\ud835\udc60 , it\u2019s intuitive to think that \ud835\udc5b\ud835\udc60 itself is a good measure of the scores. Also, it is common in academia to regard longer reviews as informative and good reviews. We, therefore, use a simple ridge regression model to predict the scores given only the review length as input."
        },
        {
            "heading": "8 RESULTS",
            "text": "Table 2 shows the results for all the systems. Given the long-tail distribution of the scores, it seems reasonable that the mean baseline performs better. Due to fewer reviews with informativeness scores, ridge regression performs approximately the same as the Mean Baseline. This also means that the Ridge Regression method is under-parameterized to the complexity of the task. Fusing the recommendation and confidence scores into the Ridge Regression leads to improvement, further hinting towards our motivation for multi-task learning. Following the intuition, the length baseline\ngives competitive performance among all the comparing systems. We observe that length is somewhat an important metric for the informativeness of the review. However, we also opine that it is an uninformed metric.\nEach scaffold individually also leads to good improvements to the mean baseline. Combining both tasks in a single model substantially improves the other comparing systems. Interestingly, CNN, as a sequential coder, also performs better. Furthermore, we observe that the Recommendation task results in good improvements in Review Strength scores, whereas the Confidence task has improved performance for Section Exhaustiveness. We can see this as a piece of evidence that Recommendation scores are a proxy for Review Strength, while Confidence scores are the proxy for Review Exhaustiveness. This also seems intuitive (except for the reviewer\u2019s inconsistency and implicit subjectivity in translating their views of the research work to numerical scores), as a high recommendation score means that the review text has a positive polarity (negative for otherwise). Similarly, the confidence score is expected to be high when the reviewer has thoroughly understood/read the paper and has written the review in detail. Thus, Recommendation Score and Confidence Score prediction tasks can also be seen as\nscaffolds for Exhaustiveness and Strength of the review, respectively. We hypothesize that they can take care of the inconsistency in a symbiotic fashion by drawing information from the other task. This somewhat simulates the case when area chairs/editors pay lesser importance to the review, which received extreme recommendation scores (pos/neg) from the reviewers, but the reviewers themselves have low confidence.\nFigure 3 visualizes the attentions for review representation R in the first layer of the sequential coder. We observe that the code representation c\ud835\udc58 learns to encode useful information about the review, and the weighted sum then acts as a feature extractor. One can also observe that the sample review has a lesser Section Exhaustiveness and Aspect Exhaustiveness score as the review text is less detailed (with less coverage). It only comments very briefly on the Novelty, Meaningful Comparison, and Empirical/Theoretical soundness aspects. Most of the review text is about summarizing the research work with minimal reviewer scrutiny."
        },
        {
            "heading": "9 CONCLUSIONS",
            "text": "Automatically grading peer reviews for their quality is challenging. There is ongoing research on what defines a good review, and we\nagree that the definition would vastly vary across domains. The current work is to quantify peer-review informativeness standing on the shoulders of two questions: how exhaustive was the review? and how strong was the review?. We design a scoring mechanism to manifest exhaustiveness in terms of coverage of the review across paper sections and aspects. We then propose a multi-task deep network to predict the informativeness of the peer review with scaffolds for exhaustiveness and strength. We demonstrate that our scoring mechanism is intuitive, and the proposed approach yields encouraging performance. We still maintain that strong quality indicators should be researched further, like modelling whether the reviewer\u2019s comments are justified, whether the review text demonstrates domain expertise, etc. We will explore these parameters in our future work."
        },
        {
            "heading": "10 ETHICAL ISSUES",
            "text": "We understand that delegating this sensitive task, which is of paramount importance to scientific progress, to an AI which suffers from issues like interpretability, bias, and reliability is not without unwarranted risks and potential misuses. However, our work is intended as a proof-of-concept to automatically eliminate some of the bias in the peer-review process. As such, we want to motivate research on quantifying and judging the quality of peer reviews. We want to monitor what kind of indicators are used for that validation. We ascertain that flawed and unreasonable indicators should not be the driving force of scientific progress. We use the publicly available data from openreview.net, so no violation of confidentiality with respect to data or authors is made. We hired three full-time annotators to develop our dataset who are experienced with NLP/ML paper discourse and paper reviewing and are paid on par with a research scholar in India."
        }
    ],
    "title": "ReviVal: Towards Automatically Evaluating the Informativeness of Peer Reviews",
    "year": 2023
}