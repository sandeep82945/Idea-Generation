{
    "abstractText": "In the field of decision trees, most previous studies have difficulty ensuring the statistical optimality of a prediction of new data and suffer from overfitting because trees are usually used only to represent prediction functions to be constructed from given data. In contrast, some studies, including this paper, used the trees to represent stochastic data observation processes behind given data. Moreover, they derived the statistically optimal prediction, which is robust against overfitting, based on the Bayesian decision theory by assuming a prior distribution for the trees. However, these studies still have a problem in computing this Bayes optimal prediction because it involves an infeasible summation for all division patterns of a feature space, which is represented by the trees and some parameters. In particular, an open problem is a summation with respect to combinations of division axes, i.e., the assignment of features to inner nodes of the tree. We solve this by a Markov chain Monte Carlo method, whose step size is adaptively tuned according to a posterior distribution for the trees.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuta Nakahara"
        },
        {
            "affiliations": [],
            "name": "Naoki Ichijo"
        }
    ],
    "id": "SP:c6b2f774093cc2ef4ea3e2aa439a7a23834762e5",
    "references": [
        {
            "authors": [
                "Leo Breiman",
                "Jerome Friedman",
                "Charles J Stone",
                "Richard A Olshen"
            ],
            "title": "Classification and Regression Trees",
            "venue": "CRC press,",
            "year": 1984
        },
        {
            "authors": [
                "Jerome H. Friedman"
            ],
            "title": "Greedy function approximation: A gradient boosting machine",
            "venue": "The Annals of Statistics,",
            "year": 2001
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "XGBoost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "James O. Berger"
            ],
            "title": "Statistical Decision Theory and Bayesian Analysis",
            "year": 1985
        },
        {
            "authors": [
                "Tota Suko",
                "Ryo Nomura",
                "Toshiyasu Matsushima",
                "Shigeichi Hirasawa"
            ],
            "title": "Prediction algorithm for decision tree model. IEICE technical report",
            "venue": "Theoretical foundations of Computing,",
            "year": 2003
        },
        {
            "authors": [
                "Nao Dobashi",
                "Shota Saito",
                "Yuta Nakahara",
                "Toshiyasu Matsushima"
            ],
            "title": "Meta-tree random forest: Probabilistic data-generative model and Bayes optimal prediction",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Bishop"
            ],
            "title": "Pattern Recognition and Machine Learning",
            "year": 2006
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "Tie-Yan Liu"
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Hugh A. Chipman",
                "Edward I. George",
                "Robert E. McCulloch"
            ],
            "title": "Bayesian cart model search",
            "venue": "Journal of the American Statistical Association,",
            "year": 1998
        },
        {
            "authors": [
                "Hugh A. Chipman",
                "Edward I. George",
                "Robert E. McCulloch"
            ],
            "title": "BART: Bayesian additive regression trees",
            "venue": "The Annals of Applied Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "T. Matsushima",
                "S. Hirasawa"
            ],
            "title": "A Bayes coding algorithm using context tree",
            "venue": "IEEE International Symposium on Information Theory,",
            "year": 1994
        },
        {
            "authors": [
                "Robert H. Swendsen",
                "Jian-Sheng Wang"
            ],
            "title": "Replica monte carlo simulation of spin-glasses",
            "venue": "Phys. Rev. Lett.,",
            "year": 1986
        },
        {
            "authors": [
                "Yuta Nakahara",
                "Toshiyasu Matsushima"
            ],
            "title": "Batch updating of a posterior tree distribution over a meta-tree",
            "venue": "arXiv preprint arXiv:2303.09705,",
            "year": 2023
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Rodney Sparapani",
                "Charles Spanbauer",
                "Robert McCulloch"
            ],
            "title": "Nonparametric machine learning and efficient computation with Bayesian additive regression trees: The BART R package",
            "venue": "Journal of Statistical Software,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Cason"
            ],
            "title": "Titanic data (titanic3)",
            "venue": "Vanderbilt Biostatistics Datasets,",
            "year": 1999
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Studies of decision trees have been primarily focused on the problem of overfitting, i.e., improvement of prediction accuracy for new data. It is because, while decision trees can be easily constructed to perfectly fit the training data by selecting appropriate features and increasing the tree\u2019s depth, such models often show poor performance when predicting new data. Therefore, various methods have been proposed to improve the accuracy of predicting new data, e.g., pruning (e.g., [1]), ensemble learning (e.g., [2, 3]), and introducing regularization terms in the cost function (e.g., [4]). These methods have been successfully applied to various prediction problems on real-world data.\nNonetheless, it is challenging to prove the prediction made by these methods is statistically optimal or to discuss its room for improvement. In our opinion, a critical cause of this is that most prior studies have used trees solely to represent the prediction function (or hypotheses) to be constructed from given data and have not used to represent a stochastic data observation process behind the given data. Herein, we call such trees representing the predictive function function trees. In principle, it is\nPreprint. Under review.\nar X\niv :2\n30 6.\n07 06\n0v 1\n[ cs\n.L G\nimpossible to directly minimize a prediction error between the new data point and a predicted value without any assumption that both the training data and the new data point are observed according to a similar stochastic mechanism. More specifically, it is impossible to consider the optimal prediction based on the statistical decision theory (see, e.g., [5]) without any assumption of stochastic data observation processes.\nIn contrast, few studies utilized trees to represent stochastic data observation processes [6, 7]. We call such trees representing the data observation processes model trees herein. A model tree represents a division pattern of the feature space, i.e., the shape of the model tree represents the number of divisions and the features assigned to the inner nodes represent the division axes, in a similar manner to usual function trees. However, unlike the function trees, stochastic models are assigned to the leaf nodes of the model tree, and we assume the objective variables are observed according to them. Although the shape of the model tree, the features assigned to the inner nodes, and the parameters of the stochastic models on the leaf nodes are usually unobservable, assuming prior distributions on all these amounts and applying the Bayesian decision theory (see, e.g., [5]), [6, 7] provided a framework to directly minimize the expectation of the prediction error for new data instead of any cost function on the training data so that the overfitting could be avoided. Moreover, they theoretically derived the formula strictly minimizing that. The prediction made by this formula is called the Bayes optimal prediction.\nHowever, this formula involves expectations for the tree\u2019s shape, the features on the inner nodes, and the parameters of stochastic models on the leaf nodes under their posterior distributions. Although an algorithm to efficiently and exactly calculate the expectations for the tree\u2019s shape and the parameters on the leaf nodes are proposed in [6, 7], the expectation for the features on the inner nodes is still an open problem. Although an approximative method was proposed in [7], it loses the Bayes optimality.\nTherefore, we solve this by a Markov chain Monte Carlo (MCMC) method (see, e.g., [8]) and propose an algorithm to predict new data. Our method retains the Bayes optimality after sufficient MCMC iterations. Therefore, it is the first prediction algorithm achieving the direct minimization of the expectation of the prediction error for new data instead of any cost function on the training data, which cannot be achieved by any function tree based approaches mentioned in the first paragraph. In our method, we adaptively tune the step size of the MCMC method according to the posterior distribution for trees. We confirm its effectiveness by numerical experiments. As a result, our method showed better prediction performance than some state-of-the-art methods [4, 9].\nAs another related work, the first MCMC method for the model trees was reported by [10]. Although it seems not to be motivated by the Bayesian decision theoretical optimality but rather a stochastic search of the decision trees, it is able to be immediately applied to the Bayes optimal decision because it enables us to calculate the expectation for the model trees. However, this method approximates the expectations for both the features on the inner nodes and the shape of the tree by the MetropolisHastings (MH) method (see, e.g. [8]). In contrast, our method exactly calculates the expectation for the shape of the tree and approximates only the expectation for the features in the inner nodes. Therefore, our method is also regarded as an idea to accelerate or leverage the MCMC method of [10]. An effect from this perspective will be demonstrated in numerical experiments. [10] is further extended to a model called BART in [11], in which data are observed according to a sum of multiple model trees. Therefore, our model will be extended in a similar manner. However, we focus on the single tree model in this paper."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Basic Notations",
            "text": "Let the dimension of the continuous features be p \u2208 N := {1, 2, 3, . . .}. Let the dimension of the binary features be q \u2208 N. Let x = (x1, x2, . . . , xp, xp+1, . . . , xp+q) \u2208 Rp \u00d7 {0, 1}q \u2282 Rp+q be an explanatory variable, where x1, . . . , xp take continuous values and xp+1, . . . , xp+q take binary values. Also, Y denotes a set of possible values of objective variables. Our discussion can be applied to both a discrete set (e.g., Y = {0, 1}) and a continuous set (e.g., Y = R). Let Y be a random variable taking values in Y and y \u2208 Y be a realization of Y .\nRegarding a tree, we use the following notations. See also Fig. 1. Let Dmax \u2208 N be the maximum depth of trees. The perfect1 binary tree whose depth is Dmax is denoted by Tmax. The set of all nodes of Tmax is denoted by Smax. The set Smax can be divided into two disjoint subsets: Lmax \u2282 Smax and Imax \u2282 Smax, where Lmax is the set of the leaf nodes of Tmax and Imax is the set of the inner nodes of Tmax. In this paper, we consider a rooted tree, i.e., a tree that has a root node s\u03bb \u2208 Smax. Let T be a full (also called proper) subtree of Tmax, where T \u2019s root node is s\u03bb and all inner nodes have exactly two children. The set of all nodes of T is denoted by ST \u2282 Smax. It can be divided into LT \u2282 ST and IT \u2282 ST , where LT is the\nset of the leaf nodes of T and IT is the set of the inner nodes of T . The set of all full subtrees T is denoted by T . As we will describe later in detail, a feature index ks \u2208 {1, 2, . . . , p+ q} is assigned to an inner node s \u2208 Imax, and a feature assignment vector is denoted by k := (ks)s\u2208Imax \u2208 K := {1, 2, . . . , p+ q}|Imax|. Also as we will describe later in detail, a node s \u2208 Smax has a parameter \u03b8s. We use the notation \u03b8 := (\u03b8s)s\u2208Smax . The set of \u03b8 is denoted by \u0398."
        },
        {
            "heading": "2.2 Stochastic Data Observation Process",
            "text": "We assume the following probability distribution on an objective variable y given an explanatory variable x. Note that \u03b8, T , and k are unobservable parameters and their posterior should be calculated later in a Bayesian manner. First, we define the following subspace division procedure and a leaf node corresponding to a given explanatory variable. Note that this procedure is not a tree construction method from given data but a definition of stochastic data observation process behind the given data.\nDefinition 1 (Xk(s) and sk,T (x)). Given k, let Xk(s) for s \u2208 Smax denote a subspace of Rp+q , which is recursively defined in the following manner (see also Fig. 2).\nFirst, for the root node s\u03bb, we assume\nXk(s\u03bb) = [a1,s\u03bb ,b1,s\u03bb)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ap+q,s\u03bb , bp+q,s\u03bb). (1)\nHere, ak,s\u03bb , bk,s\u03bb \u2208 R are just initial values to determine thresholds and they do not restrict the acceptable range of the features. At this point, we use temporary minimum and maximum values for continuous features and ak,s\u03bb = 0 and bk,s\u03bb = 1 for binary features.\nNext, if the following holds for any inner node s \u2208 Imax,\nXk(s) = [a1,s, b1,s)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ap+q,s, bp+q,s), (2)\nthen the subspace assigned to the left child sl and the right child sr of s is defined as follows, based on the feature index ks assigned to s.\nXk(sl) = {x \u2208 Xk(s) | aks,s \u2264 xks < (aks,s + bks,s)/2}, (3) Xk(sr) = {x \u2208 Xk(s) | (aks,s + bks,s)/2 \u2264 xks < bks,s}. (4)\n1All inner nodes have exactly two children and all leaf nodes have the same depth.\nIn other words, the threshold is deterministically placed at a midpoint of the assigned subspace.\nLastly, we replace ak,s with \u2212\u221e for any k \u2208 {1, . . . , p+ q} and s \u2208 Smax such that ak,s = ak,s\u03bb . Similarly, we replace bk,s with\u221e for any k \u2208 {1, . . . , p+ q} and s \u2208 Smax such that bk,s = bk,s\u03bb . By this procedure, each s \u2208 Smax is assigned to a subspace of Rp+q and the following holds: for any T \u2208 T , \u22c3 s\u2208LT Xk(s) = R\np+q, and for any s, s\u2032 \u2208 LT , s \u0338= s\u2032 \u21d2 Xk(s) \u2229 Xk(s\u2032) = \u2205. Therefore, given k and T , we can uniquely determine a node s \u2208 LT such that x \u2208 Xk(s), for any x \u2208 Rp+q. Let sk,T (x) represents this node.\nUsing the above notation, we impose the following assumptions on the probability distribution of an objective variable y given an explanatory variable x. Assumption 1. Given k and T , let sk,T (x) \u2208 LT denote the leaf node defined in Def. 1, which is uniquely and deterministically obtained from the explanatory variable x. Then, we assume\np(y|x,\u03b8, T,k) = p(y|\u03b8sk,T (x)). (5)\nThat is, we assume that y is independent of any other parameter than that assigned to sk,T (x). Assumption 2. We assume the prior distribution on \u03b8 has the following form: p(\u03b8) = \u220f s\u2208Smax p(\u03b8s). In addition, we assume each prior p(\u03b8s) is a conjugate prior for p(y|\u03b8s) and we can calculate its predictive distribution p(y) = \u222b p(y|\u03b8s)p(\u03b8s)d\u03b8s with an acceptable cost.\nThe following examples fulfill the above assumptions. Example 1. For example, when Y is finite, we can assume the categorical distribution Cat(y|\u03c0s) and the Dirichlet prior Dir(\u03c0s|\u03b1). When y is a count data, i.e., Y = {0, 1, . . . }, we can assume the Poisson distribution Po(y|\u03bds) and the gamma prior Gam(\u03bds|\u03b1, \u03b2). When Y is continuous, we can assume the normal distribution N (y|\u00b5s, \u03c32s) and the normal-gamma prior N (\u00b5s|m, \u03b3\u03c32s)Gam(1/\u03c32s |\u03b1, \u03b2). Further, we can also assume a more complicated model, e.g., linear regression (LR) model N (y|w\u22a4s x, \u03c32s) and the normal-gamma prior N (ws|m,\u039b/\u03c32s)Gam(1/\u03c32s |\u03b1, \u03b2), as long as it satisfies Assumption 2. This flexibility is one of advantages of our model.\nWe assume the following prior distribution of T \u2208 T , which has been used in [13, 6, 7, 12]. Assumption 3. Given Dmax, we assume the following probability distribution on the set T of full trees T , which are subtrees of the perfect binary tree Tmax whose depth is Dmax:\np(T ) := \u220f s\u2208IT gs \u220f s\u2032\u2208LT (1\u2212 gs\u2032), (6)\nwhere gs \u2208 [0, 1] is a given hyperparameter representing an edge spreading probability of a node s. For s \u2208 Lmax, we assume gs = 0.\nProperties of this distribution are discussed in [12], e.g., Eq. (6) satisfies \u2211\nT\u2208T p(T ) = 1. Example 2. Figure 3 shows an example of p(T ). The hyperparameter gs represents the edge spreading probability under the condition that all the ancestor nodes of s extend their edges. In other words, the data observation process includes the explanatory variable xks with the prior probability gs under the\ncondition that it includes all the explanatory variables assigned to the ancestor nodes of s (see also Remark 2 of [12]). Therefore, the prior probability that an explanatory variable on a node is included in the model decreases exponentially with its depth. In the learning phase, this property of the prior distribution prevents overfitting.\nLastly, the prior distribution of k is as follows. Assumption 4. We assume that ks is independently assigned to each s \u2208 Imax with probability 1/(p+ q), that is p(k) is the uniform distribution on K.\nIn other words, each feature can be assigned multiple times on a path from the root node to a leaf node. We assume this for simplicity, and we can also restrict the number of feature assignments."
        },
        {
            "heading": "2.3 Problem Setup",
            "text": "We deal with a prediction problem of a new objective variable yn+1 \u2208 Y corresponding to an explanatory variable xn+1 \u2208 Rp \u00d7 {0, 1}q from given training data (xn, yn) := {(xi, yi)}i\u2208{1,2,...n} \u2208 (Rp \u00d7 {0, 1}q \u00d7 Y)n, where n \u2208 N is the sample size and we assume yi independently follows (5) given xi. We assume we know the maximum depth Dmax of the trees, the initial range [ak,s\u03bb , bk,s\u03bb) of the subspace for any k \u2208 K (see Definition 1), and the hyperparameters (gs)s\u2208Smax of p(T ) (see Assumption 3), while \u03b8, T , and k are unknown. We also make Assumptions 1, 2, 3, and 4."
        },
        {
            "heading": "2.4 Bayes Optimal Prediction for New Data Point",
            "text": "We formulate the optimal prediction under the Bayesian decision theory (see, e.g., [5]). As we have described, we would like to predict the value of the new objective variable yn+1 corresponding to xn+1 given training data (xn, yn). Hence, the decision function \u03b4, which outputs a predicted value, is defined as \u03b4 : Rp\u00d7{0, 1}q\u00d7 (Rp\u00d7{0, 1}q\u00d7Y)n \u2192 Y , and the Bayes risk function BR(\u03b4) based on the 0-1 loss \u21130\u22121(\u03b4(xn+1,xn, yn), yn+1) is defined as follows:\nBR(\u03b4) := \u2211 k\u2208K \u2211 T\u2208T \u222b \u0398 p(k, T,\u03b8) \u222b Yn p(yn|xn,\u03b8, T,k) \u222b Y p(yn+1|xn+1,\u03b8, T,k)\n\u00d7 \u21130\u22121(\u03b4(xn+1,xn, yn), yn+1)dyn+1dynd\u03b8. (7)\nWhen Y is a finite set, the integral with respect to y in (7) is replaced by the summation. Note that we can also assume other usual loss functions in the Bayes decision theory, e.g., the squared loss.2\nThe Bayes risk function BR(\u03b4) is an evaluation criterion of \u03b4, and it is known that the optimal decision \u03b4\u2217 that minimizes BR(\u03b4) is given as follows. Proposition 1 ([7]). The optimal decision \u03b4\u2217(xn+1,xn, yn) that minimizes (7) is\n\u03b4\u2217(xn+1,x n, yn) = arg max\nyn+1\u2208Y \u2211 k\u2208K \u2211 T\u2208T \u222b \u0398 p(yn+1|xn+1,\u03b8, T,k)p(\u03b8, T,k|xn, yn)d\u03b8. (8)\nIn this paper, we call \u03b4\u2217(xn+1,xn, yn) the Bayes optimal prediction. For the readers not familiar with Bayesian decision theory, the proof of Proposition 1 is given in the supplementary materials. For more detail, see e.g., [5].\nIn order to see the calculation of (8), we decompose it into three components as follows: q(yn+1|xn+1,xn, yn, T,k) := \u222b \u0398 p(yn+1|xn+1,\u03b8, T,k)p(\u03b8|xn, yn, T,k)d\u03b8, (9)\nq\u0303(yn+1|xn+1,xn, yn,k) := \u2211 T\u2208T p(T |xn, yn,k)q(yn+1|xn+1,xn, yn, T,k), (10)\n\u02dc\u0303q(yn+1|xn+1,xn, yn) := \u2211 k\u2208K p(k|xn, yn)q\u0303(yn+1|xn+1,xn, yn,k). (11)\n2Herein, we regard the explanatory variables xn and xn+1 are given constants. We can also regard them as random variables. In such a case, an additional expectation for p(xn,xn+1) is required to define BR(\u03b4). However, the Bayes optimal decision \u03b4\u2217 will be the same as (8).\nBy using these notations, (8) is rewritten as follows:\n\u03b4\u2217(xn+1,x n, yn) = arg max yn+1\u2208Y \u02dc\u0303q(yn+1|xn+1,xn, yn). (12)\nWe can efficiently calculate (9) under Assumption 2. To calculate (10), [6] and [7] introduced the following notion called meta-tree.\nMT,k := {(k\u2032, T \u2032) \u2208 K \u00d7 T | k\u2032 = k and T \u2032 is a subtree of T}. (13)\nIts hierarchical structure enables calculating the summation of p(T |xn, yn,k)q(yn+1|xn+1,xn, yn, T,k) in the meta-tree MT,k under Assumption 3. If T of the meta-tree MT,k equals Tmax, such summation is equivalent to calculating (10). Moreover, its computational cost is only O(Dmaxn) and retains the Bayes optimality.\nTherefore, we focus on the efficient calculation of (11), that is, the summation with respect to the meta-trees MTmax,k, which has not been established yet. Although an approximative method to calculate (11) has been proposed in [7], it loses the Bayes optimality.\nNote that we do not learn the thresholds for subspace partitioning because they are deterministiclly derived from k in our setup (see Definition 1). In other words, we regard the problem of threshold learning as the problem of learning how many times the same k is assigned on a path from the root node to a leaf node, and optimally solve it in Bayesian manner."
        },
        {
            "heading": "3 Meta-Tree Markov Chain Monte Carlo Methods",
            "text": "This section describes our main results. We propose a Markov chain Monte Carlo (MCMC) method to calculate (11) and construct an algorithm to predict new data, i.e., we approximate (12) as follows:\n\u03b4\u2217(xn+1,x n, yn) \u2248 arg max\nyn+1\u2208Y\n1\ntend tend\u2211 t=1 q\u0303(yn+1|xn+1,xn, yn,k(t)), (14)\nwhere tend \u2208 N is the maximum number of the MCMC iteration and {k(t)}tendt=1 denotes a sample following p(k|xn, yn), which is obtained by our MCMC method. We call this method meta-tree Markov chain Monte Carlo (MTMCMC) method. Specifically, we propose a MH algorithm (see, e.g., [8]) and extend it to a replica exchange Monte Carlo (REMC) method (e.g., [14]) to deal with multimodality of the posterior distribution. Herein, we only describe the underlying MH method. The extension to REMC method is described in supplementary materials.\nAs usual MH methods, we generate k\u2217 from a proposal distribution q(k\u2217|k(t\u22121)) in the tth iteration of our algorithm. Then, it will be accepted according to the following acceptance probability.\nA(k\u2217,k(t\u22121)) = min { 1,\np(k\u2217|xn, yn)q(k(t\u22121)|k\u2217) p(k(t\u22121)|xn, yn)q(k\u2217|k(t\u22121))\n} . (15)\nIf k\u2217 is accepted, we make k(t) \u2190 k\u2217, otherwise k(t) \u2190 k(t\u22121). Proposition 2. If q(k\u2217|k(t\u22121)) is time-invariant and q(k\u2217|k(t\u22121)) > 0 holds for any k\u2217 and k(t\u22121) through this process, then the detailed balance is satisfied and an empirical distribution of the obtained sample {k(t)}tendt=1 converges to the objective distribution p(k|xn, yn) after sufficient iteration.\nFor the readers not familiar with MCMC method, we briefly prove this proposition in the supplementary materials. For more detail, see e.g., Chapter 11 of [8]\nIn our case, from the Bayes\u2019 theorem and Assumption 4, Eq. (15) is further transformed as follows:\nA(k\u2217,k(t\u22121)) = min { 1,\np(yn|xn,k\u2217)q(k(t\u22121)|k\u2217) p(yn|xn,k(t\u22121))q(k\u2217|k(t\u22121))\n} . (16)\nIn general, p(yn|xn,k\u2217) and p(yn|xn,k(t\u22121)) in (16) cannot be efficiently calculated since it requires marginalization for T and \u03b8. However, we can calculate them by an algorithm proposed in [15]. Therefore, at this point, the rest of the problem is design of the proposal distribution q(k\u2217|k(t\u22121))."
        },
        {
            "heading": "3.1 Design of Proposal Distribution",
            "text": "Asymptotically, we can use any time-invariant distribution that satisfies q(k\u2217|k(t\u22121)) > 0 for any k\u2217 and k(t\u22121), e.g., the uniform distribution q(k\u2217|k(t\u22121)) = (p + q)\u2212|Imax|. However, its design crucially affects the performance on a finite MCMC sample. This can be explained from a viewpoint of an analogy of the MH algorithm and a neighborhood searching algorithm. In the MH algorithm, k\u2217 is proposed from a kind of neighborhood of k(t\u22121) according to q(k\u2217|k(t\u22121)). Roughly speaking, it will be accepted if it increases the probability of the objective distribution, i.e., p(k\u2217|xn, yn) > p(k(t\u22121)|xn, yn). Since the entropy of q(k\u2217|k(t\u22121)) corresponds to the step size of neighborhood search, it should be larger to accelerate the search but it should be smaller to increase the acceptance ratio. Therefore, a desirable q(k\u2217|k(t\u22121)) should induce many changes in the elements of k(t\u22121) when p(k(t\u22121)|xn, yn) is small and a few changes when p(k(t\u22121)|xn, yn) is large. Note that k is discrete and hierarchically structured. Therefore, we cannot use the derivative of p(k(t\u22121)|xn, yn), and any Gibbs sampler for our model has not been reported to our best knowledge. Then, we use the posterior distribution p(T |xn, yn,k(t\u22121)) as a heuristic to design q(k\u2217|k(t\u22121)). First, we have the following proposition. Proposition 3 ([6, 7, 12]). For any xn, yn, and k, the posterior distribution of T is represented as follows:\np(T |xn, yn,k) = \u220f s\u2208IT gs|xn,yn,k \u220f s\u2032\u2208LT (1\u2212 gs\u2032|xn,yn,k), (17)\nwhere gs|xn,yn,k \u2208 [0, 1] is a posterior parameter calculated from xn, yn, and k for each s \u2208 Smax.\nTherefore, in the tth iteration, we can represent the posterior distribution p(T |xn, yn,k(t\u22121)) in the same form as the prior distribution p(T ) with a posterior edge spreading probability gs|xn,yn,k(t\u22121) of a node s. In other words, given k(t\u22121), the data observation process includes an explanatory variable x\nk (t\u22121) s assigned to s with a posterior probability gs|xn,yn,k(t\u22121) under the condition that it includes the explanatory variables assigned to all the ancestor nodes of s (see also Fig. 3). We use this probability gs|xn,yn,k(t\u22121) as a heuristic to determine the fixed elements of k\u2217, that is, the smaller gs|xn,yn,k(t\u22121) the node s has, the more frequently k(t\u22121)s is changed. Consequently,\nwe generate k\u2217 according to the following procedure, see also Fig. 4. (The initial value k(0) is generated from the uniform distribution on K.)\n1. T\u0303 is generated according to q(T\u0303 |xn, yn,k(t\u22121)) := \u220f s\u2208IT\u0303 min{gs|xn,yn,k(t\u22121) , g\u0304} \u220f s\u2032\u2208LT\u0303 (1\u2212min{gs|xn,yn,k(t\u22121) , g\u0304}), (18)\nwhere g\u0304 \u2208 [0, 1] is predetermined in a burn-in phase (see also the supplementary materials).\n2. For s \u2208 IT\u0303 , k (t\u22121) s is fixed and k\u2217s = k (t\u22121) s .\n3. For s \u2208 LT\u0303 \u2229Imax, k (t\u22121) s is changed according to the uniform distribution on {1, 2, . . . , p+ q} \\ {k(t\u22121)s }. 4. The others are changed according to the uniform distribution on {1, 2, . . . , p+ q}.\nNote that T\u0303 is uniquely determined from k\u2217 and k(t\u22121) as the maximum tree that satisfies k\u2217s = k (t\u22121) s for all s \u2208 IT\u0303 . Therefore, the proposal distribution q(k\u2217|k(t\u22121)) is represented as follows:\nq(k\u2217|k(t\u22121)) = q(T\u0303 |xn, yn,k(t\u22121))(p+ q \u2212 1)\u2212|LT\u0303\u2229Imax|(p+ q)\u2212|Imax\\ST\u0303 |. (19)\nMoreover, the following theorem holds.\nTheorem 1. By using the MCMC sample obtained by the MH method based on the proposal distribution (19) and the acceptance probability (16), the MTMCMC method defined in (14) minimizes the Bayes risk function (7), i.e., achieves the Bayes optimality, after sufficient MCMC iterations.\nProof. If k(t\u22121) = k(t \u2032\u22121) holds, then q(k\u2217|k(t\u22121)) = q(k\u2217|k(t\u2032\u22121)) clearly holds for any k\u2217 even when t \u0338= t\u2032. Therefore, a Markov chain of k(t) induced from q(k\u2217|k(t\u22121)) and A(k\u2217,k(t\u22121)) is timeinvariant. Moreover, q(k\u2217|k(t\u22121)) > 0 holds for any k\u2217 and k(t\u22121). Therefore, the induced Markov chain of k(t) is ergodic. Then, q(k\u2217|k(t\u22121)) satisfies the condition of Proposition 2. Therefore, empirical distribution of the obtained sample converges to p(k|xn, yn). Lastly, the right-hand side of (14) converges to the left-hand side, which is the decision function strictly minimizing the Bayes risk function, after sufficient MCMC iterations because of the law of large numbers.\nRemark 1. g\u0304 is an additional parameter to control the entropy of q(k\u2217|k(t\u22121)). When n is large, gs|xn,yn,k(t\u22121) for the nodes near the root s\u03bb numerically equals to 1. Then, ks for them tends to be fixed and ergodicity will be collapsed. Introducing g\u0304, all the elements of k(t\u22121) are refreshed with the probability 1\u2212 g\u0304 and the ergodicity is ensured. This induces a \u201cjump\u201d of k\u2217 and has some effects to deal with multimodality of the posterior distribution. A more effective approach to multimodality is extending our MH method to the REMC method, which is described in the supplementary materials. Remark 2. Because of the uniqueness of T\u0303 , transition from k(t\u22121) to k\u2217 cannot occur through any other tree than T\u0303 , and vice versa. Therefore, q(k(t\u22121)|k\u2217) is represented as follows.\nq(k(t\u22121)|k\u2217) = q(T\u0303 |xn, yn,k\u2217)(p+ q \u2212 1)\u2212|LT\u0303\u2229Imax|(p+ q)\u2212|Imax\\ST\u0303 |, (20)\nwhere T\u0303 is same as that in (19). As a result, we can efficiently evaluate (16) because many terms in the numerator and the denominator of (16) are canceled by substituting (19) and (20). Further complexity reduction and complexity analysis are described in the supplementary materials."
        },
        {
            "heading": "4 Experiments",
            "text": "Herein, we introduce only two experiments. In the supplementary materials, we described the others, e.g., confirmation of convergence of the approximated posterior to the true posterior; comparison with the uniform proposal distribution and the other tree posterior based proposal distribution; and confirmation of behavior of likelihood during the MCMC sampling.\nFirst, we summarize the methods used in this section and their abbreviations. Most methods are used with their default hyperparameters (see the supplementary materials for detail).\nMTMCMC-Be-100(50) etc.: the method proposed in this paper. The letters next to MTMCMC mean a stochastic model of y assigned to each leaf node (see also Example 1). Be, Po, and LR means the Bernoulli distribution, the Poisson distribution, and the LR model, respectively. The numbers at the end mean the number of MCMC iterations and the length of burn-in, e.g., 100(50) means we make 150 proposals and remove the first 50 of them. MTRF-Be etc.: the meta-tree random forest [7] implemented in [16]. The letters next to MTRF have a similar meaning to MTMCMC. RF: The random forest [2] implemented in [17]. XGBoost: the XGBoost [4]. LightGBM: the light GBM implemented in [9]. BART100(50) etc.: the BART [11] implemented in [18]. The number of trees in the BART model is assumed to be one for comparison with our method under the same condition. It can be specified by ntree option. The number at the end has a similar meaning to MTMCMC."
        },
        {
            "heading": "4.1 Experiment 1: Bayes Optimality of Prediction",
            "text": "Purpose: we confirm the Bayes optimality of our prediction method. Under the Bayes criterion, our method is expected to outperform any other methods for synthetic data generated from the assumed stochastic model. In particular, our method cannot be outperformed by any function tree based methods such as RF, XGBoost, and LightGBM. Our method will also outperform MTRF because it approximates (11).\nConditions: we assume p = 0 and q = 20. Therefore, all the explanatory variables are binary. Y is also the binary set {0, 1}. We assume Dmax = 10. p(k) is the uniform distribution on K. p(T ) is the tree distribution of (6) with gs = 0.75 for any s \u2208 Smax. We generate k and T 100 times.\nSubsequently, we generate \u03b8, xn, and yn 100 times for each k and T . \u03b8s is independently distributed with p(\u03b8s) = Beta(\u03b8s|0.5, 0.5). The ith explanatory variable xi is independently generated from the uniform distribution on {0, 1}q . The data observation model p(y|\u03b8s) is the Bernoulli distribution Bern(y|\u03b8s). Each method is trained with the generated data up to the size of 200. The size of test data is 100. The other conditions are given in the supplementary materials.\nResults: Figure 5 shows the approximated Bayes risk of the prediction, i.e., the average of the classification error ratio for all the generated models, parameters, and data. As expected, our proposed method outperforms any other methods. Notably, our method outperformed the BART, which is also based on Bayesian inference. We consider it is because the model assumed in the BART is slightly different from our model. While the binary objective variable is directly output from a distribution on a leaf node in our model, it follows a logit-transformed distribution of a continuous output from a leaf node in the BART model."
        },
        {
            "heading": "4.2 Experiment 2: Real-World Example",
            "text": "Purpose: We confirm the performance of our method on real-world example.\nConditions: We apply our method to a binary classification task on data about the Titanic [19]3 and a regression task on data about abalones from the UCI repository [20]. Note that Y of the abalone data is Z\u22650. We perform the five-fold cross-validation for both data. The other conditions are given in the supplementary materials. In this experiment, we use the REMC method to deal with multimodality of the poseterior distribution. For more detail, see the supplementary materials. Only in this experiment, we used the sum of tree models, i.e., BART model with a default ntree option, for comparison, although our method is based on a single model tree. It will be represented by BART-Multi in figures.\nResults: Figures 6 and 7 show the box plot of the prediction error ratio for each validation data of the Titanic and the mean squared error for each validation data of the abalones, respectively. On average, our method showed comparable performance with state-of-the-art methods such as XGBoost and LightGBM. Although the sum of tree models (BART-Multi) showed the best performance in Fig. 7, the gap between our method and the sum of tree models can be decreased by extending our model to a sum of meta-tree models.\n3Data obtained from http://hbiostat.org/data courtesy of the Vanderbilt University Department of Biostatistics.\nAnother interesting insight from these results would be the difference that comes from the number of MCMC iterations. For BART, decreasing the number of iterations, the error ratio and the mean squared error were increased. In contrast, those of our method were not so increased. This indicates the efficiency of our sampling method compared with that of BART.\nRegarding the computational cost of our method, using Python on a normal labtop, each MCMC iteration on a single chain requires approximately 46 msec for abalone data, where K = 2, p = 8, q = 3, Dmax = 10, and LR models are assumed on the leaf nodes (the most complex setting in this experiment). For more detail, see the supplementary materials. Note that our motivation for the Bayes optimal prediction is to solve the overfitting for small data, and scalability is less important.\nIn summary, as shown in the above result, our model flexibly expresses the data observation processes by assuming different models on the leaf nodes. It supports both categorical and continuous objective variables with categorical, continuous, or mixed explanatory variables. Moreover, we can prevent overfitting because their parameters can be learned Bayes optimally. Further, the computational cost is not so expensive. Therefore, we believe that our method should be at least a possible choice for real-world tasks."
        },
        {
            "heading": "A Proof of Proposition 1",
            "text": "From Bayes\u2019 theorem, we have\nBR(\u03b4) = \u222b Yn (\u222b Y p(yn+1|xn+1,xn, yn)\u21130\u22121(\u03b4(xn+1,xn, yn), yn+1)dyn+1 ) p(yn|xn)dyn,\n(21) where\np(yn+1|xn+1,xn, yn) := \u2211 k\u2208K \u2211 T\u2208T \u222b \u0398 p(yn+1|xn+1,\u03b8, T,k)p(\u03b8, T,k|xn, yn)d\u03b8. (22)\nIf we find the decision function \u03b4 that minimizes the brackets in (21), then this is the optimal decision \u03b4\u2217, and it is easy to see that\n\u03b4\u2217(xn+1,x n, yn) = arg max yn+1\u2208Y p(yn+1|xn+1,xn, yn). (23)\nHence, from (22) and (23), we complete the proof of Proposition 1."
        },
        {
            "heading": "B Proof of Proposition 2",
            "text": "Herein, we prove Proposition 2 in general, i.e., we consider a general process to obtain an MCMC sample {z(t)}tendt=1 from an objective distribution p(z) by the MH algorithm. (In our case, z and p(z) should be replaced with k and p(k|xn, yn), respectively.) In the tth iteration of the MH algorithm, z\u2217 is generated from a proposal distribution q(z\u2217|z(t\u22121)). Then, it will be accepted according to the following acceptance probability\nA(z\u2217, z(t\u22121)) := min { 1,\np(z\u2217)q(z(t\u22121)|z\u2217) p(z(t\u22121))q(z\u2217|z(t\u22121))\n} . (24)\nIf z\u2217 is accepted, we make z(t) \u2190 z\u2217, otherwise z(t) \u2190 z(t\u22121).\nAccording to the above process, the transition probability T (z(t)|z(t\u22121)) is represented as follows.\nT (z(t)|z(t\u22121)) = { q(z(t)|z(t\u22121))A(z(t), z(t\u22121)), z(t) \u0338= z(t\u22121) 1\u2212 \u2211 z \u0338=z(t) q(z|z(t\u22121))A(z, z(t\u22121)), z(t) = z(t\u22121)\n(25)\nTherefore, if q(z\u2217|z(t\u22121)) is time-invariant4, then T (z(t)|z(t\u22121)) is also time-invariant. Moreover, if q(z\u2217|z(t\u22121)) > 0 holds for any z\u2217 and z(t\u22121), the Markov chain of z(t) is ergodic. It is known that any time-invariant and ergodic Markov chain has a unique stationary distribution p\u2217(z). It is also known that if any distribution p\u0303(z) satisfies the following condition called detailed balance, p\u0303(z)T (z\u2032|z) = p\u0303(z\u2032)T (z|z\u2032) for any z and z\u2032, (26) then p\u0303(z) is the stationary distribution, i.e., p\u0303(z) = p\u2217(z). (It is a sufficient condition but not a necessary condition.)\nThen, we prove the objective distribution p(z) is the stationary distribution of the Markov chain whose transition probability is T (z(t)|z(t\u22121)) by showing the objective distribution satisfies the detailed balance. If z = z\u2032, the equation in (26) clearly holds. If z \u0338= z\u2032, we have\np(z)T (z\u2032|z) = p(z)q(z\u2032|z)A(z\u2032, z) (27) = p(z)q(z\u2032|z)min { 1,\np(z\u2032)q(z|z\u2032) p(z)q(z\u2032|z)\n} (28)\n= min{p(z)q(z\u2032|z), p(z\u2032)q(z|z\u2032)} (29) = p(z\u2032)q(z|z\u2032)min {\np(z)q(z\u2032|z) p(z\u2032)q(z|z\u2032) , 1\n} (30)\n= p(z\u2032)q(z|z\u2032)A(z, z\u2032) (31) = p(z\u2032)T (z|z\u2032). (32)\n4For t \u0338= t\u2032, if z(t\u22121) = z(t \u2032\u22121) holds, then q(z\u2217|z(t\u22121)) = q(z\u2217|z(t \u2032\u22121)) holds for any z\u2217.\nTherefore, the objective distribution p(z) is the stationary distribution of Markov chain induced from the aforementioned process. Consequently, the empirical distribution {z(t)}t=1,2,... converges to the objective distribution p(z) after sufficient iteration."
        },
        {
            "heading": "C Tuning Algorithm of Additional Parameter of Meta-Tree Markov Chain Monte Carlo Methods",
            "text": "The additional parameter g\u0304 in (18) should be tuned in the burn-in phase. We did it by Algorithm 1. In our experiments, we set robj = 0.3, \u03c1 = 0.99, and \u03d5 = 0.999.\nAlgorithm 1 Tuning algorithm of g\u0304 Require: robj \u2208 [0, 1], \u03c1 \u2208 [0, 1], \u03d5 \u2208 [0, 1] Ensure: g\u0304 \u2208 [0, 1]\n1: g\u0304 \u2190 0 2: Naccept \u2190 1 3: Npropose \u2190 1 4: N \u2032propose \u2190 1 5: while Burn-in phase do 6: Propose k\u2217 7: if k\u2217 is accepted then 8: Naccept \u2190 \u03c1Naccept + 1 9: else\n10: Naccept \u2190 \u03c1Naccept 11: end if 12: Npropose \u2190 \u03c1Npropose + 1 13: r\u0302 \u2190 Naccept/Npropose 14: if r\u0302 > robj then 15: g\u0302tmp \u2190 g\u0302 \u00b7 robj/r\u0302 16: else 17: g\u0302tmp \u2190 1\u2212 (1\u2212 g\u0302)(1\u2212 robj)/(1\u2212 r\u0302) 18: end if 19: N \u2032propose \u2190 \u03d5N \u2032propose + 1 20: g\u0302 \u2190 (\u03d5g\u0302 + g\u0302tmp)/N \u2032propose 21: end while 22: return g\u0302"
        },
        {
            "heading": "D Other Examples of Proposal Distributions",
            "text": "We show other examples of the proposal distributions of k\u2217. Their effectiveness will be numerially compared in the next section."
        },
        {
            "heading": "D.1 Uniform Proposal Distribution",
            "text": "For comparison, we utilize the uniform distribution on K as the proposal distribution q(k\u2217|k(t\u22121)) = (p + q)\u2212|Imax|. For this type of proposal distribution, the acceptance probability that satisfies the detailed balance is derived as follows:\nA(k\u2217,k(t\u22121)) = min { 1,\np(yn|xn,k\u2217) p(yn|xn,k(t\u22121))\n} . (33)"
        },
        {
            "heading": "D.2 Tree Prior Based Proposal Distribution",
            "text": "We can utilize the tree prior (6) to generate T\u0303 instead of (18). Then, the proposal distribution q(k\u2217|k(t\u22121)) is represented as follows:\nq(k\u2217|k(t\u22121)) = p(T\u0303 )(p+ q \u2212 1)\u2212|LT\u0303\u2229Imax|(p+ q)\u2212|Imax\\ST\u0303 |. (34)\nThe acceptance probability that satisfies the detailed balance for this proposal distribution is the same as (33)."
        },
        {
            "heading": "D.3 Other Examples of Tree Posterior Based Proposal Distribution",
            "text": "In (18), we truncated the hyperparameter gs|xn,yn,k(t\u22121) by g\u0304 to ensure the ergodicity and induce a jump. We also utilize a reduced one, such as,\nq(T\u0303 |xn, yn,k(t\u22121)) = \u220f s\u2208IT\u0303 \u03b1gs|xn,yn,k(t\u22121) \u220f s\u2032\u2208LT\u0303 (1\u2212 \u03b1gs\u2032|xn,yn,k(t\u22121)), (35)\nwhere \u03b1 is in the range of [0, 1].\nFurther, not only reducing the large gs|xn,yn,k(t\u22121) , we can also amplify the small gs|xn,yn,k(t\u22121) as follows.\nq(T\u0303 |xn, yn,k(t\u22121)) = \u220f s\u2208IT\u0303 ( (gs + \u03b1(gs|xn,yn,k(t\u22121) \u2212 gs) ) \u220f s\u2032\u2208LT\u0303 ( 1\u2212 ( gs\u2032 + \u03b1(gs\u2032|xn,yn,k(t\u22121) \u2212 gs\u2032) )) , (36)\nwhere gs is the hyperparameter of the prior (6).\nFor (35) and (36), the acceptance probability is defined in a similar manner to (16). Many terms in the numerator and the denominator of (16) are canceled in a similar manner to Remark 2. We can tune \u03b1 in the same algorithm as Algorithm 1."
        },
        {
            "heading": "E Computationally Efficient Proposal Distribution",
            "text": "As described in Remark 2, we can efficiently evaluate the acceptance probability (16) by calculating q(T\u0303 |xn, yn,k(t\u22121)) and q(T\u0303 |xn, yn,k\u2217), which can be obtained from gs|xn,yn,k(t\u22121) and gs|xn,yn,k\u2217 only for s \u2208 ST\u0303 . Therefore, the computational cost to evaluate the acceptance probability is O(|ST\u0303 |). However, to sample k\u2217, we have to remember k(t\u22121)s for all the node s \u2208 STmax . It is because s \u2208 LT\u0303 holds with non-zero probability for all s \u2208 STmax , and if s \u2208 LT\u0303 holds, then k\u2217s must be different from k(t\u22121)s . In this section, we describe a method to reduce this complexity. Although the explanation is based on the proposal distribution (19) in the main article, this method is also applied for other proposal distributions based on (35) or (36).\nFirst, let Txn,k denote the minimal tree that contains the paths from the root node s\u03bb to the leaf node sk,Tmax(xi) for all i \u2208 {1, 2, . . . , n}. In other words, Txn,k is the minimal tree used during the observation of yn for given xn and k. Since the length of these paths is Dmax, |STxn,k | \u2264 nDmax holds.\nNext, we define the following parameter for all s \u2208 STmax :\ng\u0303s|xn,yn,k := { min{gs|xn,yn,k, g\u0304}, s \u2208 ITxn,k , 0, otherwise.\n(37)\nThen, we generate T\u0303 according to the following distribution instead of (18). q(T\u0303 |xn, yn,k(t\u22121)) = \u220f s\u2208IT\u0303 g\u0303s|xn,yn,k(t\u22121) \u220f s\u2032\u2208LT\u0303 (1\u2212 g\u0303s\u2032|xn,yn,k(t\u22121)). (38)\nLastly, we generate k\u2217 as follows. For s \u2208 IT\u0303 , k (t\u22121) s is fixed and k\u2217s = k (t\u22121) s . For s \u2208 LT\u0303 \u2229 IT xn,k(t\u22121)\n, k(t\u22121)s is changed according to the uniform distribution on {1, 2, . . . , p+ q}\\{k(t\u22121)s }. For the other nods, k(t\u22121)s is changed according to the uniform distribution on {1, 2, . . . , p+ q}.\nTherefore, we do not require k(t\u22121)s on s /\u2208 IT\u0303 xn,k(t\u22121) to generate k\u2217. Moreover, since T\u0303 is uniquely determined from k(t\u22121) and k\u2217, q(k\u2217|k(t\u22121)) is represented as follows.\nq(k\u2217|k(t\u22121)) = q(T\u0303 |xn, yn,k(t\u22121))(p+ q \u2212 1) \u2212 \u2223\u2223\u2223\u2223LT\u0303\u2229ITxn,k(t\u22121) \u2223\u2223\u2223\u2223 (p+ q) \u2223\u2223\u2223\u2223Imax\\(ST\u0303\u2229ITxn,k(t\u22121) )\u2223\u2223\u2223\u2223 .\n(39)\nFurther, IT\u0303 \u2282 ITxn,k\u2217 holds for any k\u2217 generated through T\u0303 . Similarly, (LT\u0303 \u2229 ITxn,k(t\u22121) ) = (LT\u0303 \u2229 ITxn,k\u2217 ) and (ST\u0303 \u2229 ITxn,k(t\u22121) ) = (ST\u0303 \u2229 ITxn,k\u2217 ) hold. Therefore, the transition from k \u2217 to k(t\u22121) cannot occur through any tree other than T\u0303 , and we can cancel the denominator and the numerator of the acceptance probability in a similar manner to Remark 2.\nIn summary, by using the proposal distribution in (39), we can sample k\u2217 without using k(t\u22121)s on s /\u2208 IT\nxn,k(t\u22121) and we can evaluate q(T\u0303 |xn, yn,k(t\u22121)) and q(T\u0303 |xn, yn,k\u2217) using only the\nparameters on the nodes in ST\u0303 ."
        },
        {
            "heading": "F Complexity Analysis of Meta-Tree Markov Chain Monte Carlo Methods",
            "text": "In this section, we summarize the computational complexity of MTMCMC methods. First, let ys|k denote the set of objective variables of data points that pass through s in the data generating process for given k and Tmax. Therefore, \u22c3 s\u2208L(T ) ys|k = y\nn holds for any T in the meta-tree MTmax,k. In each iteration of the MTMCMC methods, we have to do the following procedure.\n1. Calculate \u222b p(ys|k(t\u22121) |\u03b8s)p(\u03b8s)d\u03b8s for all s \u2208 STxn,k(t\u22121) .\n2. Calculate p(T |xn, yn,k(t\u22121)).\n3. Generate k\u2217 according to q(k\u2217|k(t\u22121)).\n4. Evaluate A(k\u2217,k(t\u22121)).\nAfter tend iterations, we calculate (14). In the following, we evaluate the computational complexity of these procedures.\nCalculation of \u222b p(ys|k(t\u22121) |\u03b8s)p(\u03b8s)d\u03b8s for all s \u2208 STxn,k(t\u22121) : we consider the worst case\nwhere Txn,k(t\u22121) = Tmax. For each node s \u2208 STmax , the computational cost to calculate\u222b p(ys|k(t\u22121) |\u03b8s)p(\u03b8s)d\u03b8s is usually proportional to the number of data points when p(ys|k(t\u22121) |\u03b8s) is an usual exponential family distribution. Although the number of data points assigned to each node s depends on k(t\u22121), the sum of the number of data points assigned to all the nodes at each depth is always n. Therefore, the computational cost to calculate \u222b p(ys|k(t\u22121) |\u03b8s)p(\u03b8s)d\u03b8s for all s \u2208 STmax is O(nDmax). We can calculate p(\u03b8s|ys|k(t\u22121)) simultaneously.\nCalculation of p(T |xn, yn,k(t\u22121)): using the method in [15], we can calculate p(T |xn, yn,k(t\u22121)) with a complexity of O(|ST\nxn,k(t\u22121) |). Note that |ST xn,k(t\u22121) | \u2264 nDmax always holds. We can\ncalculate p(yn|xn,k(t\u22121)) simultaneously.\nGeneration of k\u2217 according to q(k\u2217|k(t\u22121)): using the proposal distribution described in the previous section, we need not generate k\u2217s for s /\u2208 STxn,k\u2217 . Therefore, the computational complexity is O(|STxn,k\u2217 |).\nEvaluation of A(k\u2217,k(t\u22121)): to evaluate A(k\u2217,k(t\u22121)), we have to calculate p(yn|xn,k(t\u22121)), p(yn|xn,k\u2217), q(T\u0303 |xn, yn,k(t\u22121)), and q(T\u0303 |xn, yn,k\u2217). p(yn|xn,k(t\u22121)) is already calculated. p(yn|xn,k\u2217) can be calculated in a similar manner to p(yn|xn,k(t\u22121)) by using the method in [15]. Its computational complexity is O(|STxn,k\u2217 |). The computational complexity to calculate q(T\u0303 |xn, yn,k(t\u22121)) is O(|ST\u0303 |) as described in the previous section. When using the proposal distribution described in the previous section, O(|ST\u0303 |) = O(|STxn,k(t\u22121) |). The computational cost to calculated q(T\u0303 |xn, yn,k\u2217) is similarly evaluated.\nCalculation of (14): at this point, p(\u03b8s|ys|k(t)) and p(T |xn, yn,k(t)) are already calculated for all t \u2208 {1, 2, . . . , tend}. Therefore, by using the method in [6, 7], we can calculate q\u0303(yn+1|xn+1,xn, yn,k(t)), i.e., (9) and (10), with a complexity of O(Dmax). To calculate (14), we have to take summation of them for t \u2208 {1, 2, . . . , tend}. Therefore, the complexity is O(tendDmax) Consequently, the total complexity of the MTMCMC method is roughly O(tendnDmax)."
        },
        {
            "heading": "G Extension to Replica Exchange Monte Carlo Methods",
            "text": "In this section, we extend our MH method to REMC methods (e.g., [14]) to deal with multimodality of the posterior distribution. First, we define the following joint distribution over KJ for J \u2208 N.\nq(k1,k2, . . . ,kJ) := J\u220f j=1 qj(kj) := J\u220f j=1 p(kj |xn, yn)\u03b2j\u2211 kj\u2208K p(kj |x n, yn)\u03b2j , (40)\nwhere 0 \u2264 \u03b21 < \u03b22 < \u00b7 \u00b7 \u00b7 < \u03b2J = 1. Since \u03b2J = 1, the marginal distribution qJ(kJ) is equivalent to the posterior distribution p(k|xn, yn) required to calculate the Bayes optimal prediction. Therefore, we construct an MCMC method for this joint distribution q(k1,k2, . . . ,kJ) and use the sample for only kJ , ignoring those for k1,k2, . . . ,kJ\u22121. In REMC methods, the sample from the joint distribution is obtained as follows:\n1. For each q(kj), run the MH method and obtain the sample k (1) j ,k (2) j , . . . . The proposal distribution and the acceptance probability are similar to those in the usual MH method described in the main article.\n2. Let m \u2208 N be a predetermined number. For every m iterations of the MH method, we randomly choose j \u2208 {0, 1, . . . , J \u2212 1} and exchange k(t)j and k (t) j+1 with probability\nqj(k (t) j+1)qj+1(k (t) j )\nqj(k (t) j )qj+1(k (t) j+1)\n= p(yn|xn,k(t)j+1)\u03b2jp(yn|xn,k (t) j ) \u03b2j+1\np(yn|xn,k(t)j+1)\u03b2j+1p(yn|xn,k (t) j )\n\u03b2j , (41)\nwhere we used the Bayes\u2019 theorem and Assumption 4. (This procedure can be applied multiple times at the same tth iteration of the MH method.)\nIt is known that the above procedure satisfies the detailed balance condition and the obtained sample asymptotically follows q(k1,k2, . . . ,kJ) after sufficient iterations. Since \u03b2j is monotonically increasing, the effect of multimodality of p(k|xn, yn) is reduced for small j. Therefore, k(t)j for small j tends to move over the multiple modes. Exchange these sample with probability (41), the REMC methods ensure the detailed balance and provide the sample from the multiple modes."
        },
        {
            "heading": "H Detailed Conditions of Experiments in the Main Article",
            "text": ""
        },
        {
            "heading": "H.1 Detailed Condition of Experiment 1",
            "text": "MTMCMC: g\u0304 in (18) was adaptively tuned in the burn-in phase by the algorithm described in this supplementary material. The other hyperparameters, e.g., Dmax, gs, etc., were the same as those used to generate the true model and data.\nMTRF: The number of meta-trees used for the prediction was 100, which was the default value of the library [16]. The other hyperparameters, e.g., Dmax, gs, etc., were the same as those used to generate the true model and data.\nRF: The maximum depth was 10. The other hyperparameters were default values of the library [17].\nXGBoost: All the hyperparameters were default values of the library [4].\nLightGBM: All the hyperparameters were default values of the library [9].\nBART: We used the lbart (logit BART) function implemented in [18]. The ntree option was set at 1 because the true model was represented by a single tree. The other hyperparameters were default values of the library."
        },
        {
            "heading": "H.2 Detailed Condition of Experiment 2",
            "text": ""
        },
        {
            "heading": "H.2.1 Conditions for Classification",
            "text": "Data set: The data set was about the sinking of the Titanic [19]. Each data point (xi, yi) was a pair of the information about the ith passenger and his or her survival. In other words, we performed a binary classification. The used explanatory variables are \"pclass\", \"age\", \"sibsp\", \"parch\", \"fare\", \"sex\", and\n\"embarked\". We encoded \"sex\" into 0 or 1, and \"embarked\" into 001, 010, and 100 (one-hot vectors). Then, the number of continuous features is p = 5 and the number of categorical features is q = 4. Missing values were filled with the mode of each variable. The sample size was 1309.\nMTMCMC: We had Dmax = 10 and gs = 0.75 for any s \u2208 Smax. The distribution of y assigned at each node s and its prior distribution were assumed to be the Bernoulli distribution Bern(y|\u03b8s) and the beta distribution Beta(\u03b8s|0.5, 0.5), respectively. g\u0304 in (18) was fixed at 0.8. The number of replicas in the REMC method was 8. The replica exchange procedure was made every 10 iterations of the MH method. In each replica exchange procedure, randomly selected 4 replicas are sequentially tried to exchange.\nMTRF: We had Dmax = 10 and gs = 0.75 for any s \u2208 Smax. The distribution of y assigned at each node s and its prior distribution were assumed to be the Bernoulli distribution Bern(y|\u03b8s) and the beta distribution Beta(\u03b8s|0.5, 0.5), respectively. The number of meta-trees used for the prediction was 100, which was the default value of the library [16].\nRF: The maximum depth was 10. The other hyperparameters were default values of the library [17].\nXGBoost: All the hyperparameters were default values of the library [4].\nLightGBM: The maximum depth of trees was fexed at 10. The other the hyperparameters were default values of the library [9]. (This setting showed a better result than default maximum depth setting.)\nBART: We used the lbart (logit BART) function implemented in [18]. The ntree option was set at 1 for comparison with our method under the same condition. The other hyperparameters were default values of the library."
        },
        {
            "heading": "H.2.2 Conditions for Regression",
            "text": "Data set: The data set was about abalones from UCI repository [20]. Each data point (xi, yi) was a pair of physical measurements of abalones and its age. Therefore, the set of objective variable Y was Z\u22650. We used all the explanatory variables. We encoded \"Sex\", which consists of \"M\", \"F\", and \"I\" (infant), into 001, 010, and 100 (one-hot vectors). Then, the number of continuous features is p = 7 and the number of categorical features is q = 3. (When we assume a linear regression model at each leaf node of model trees, we had p = 8 because constant term was added.) The sample size was 4177.\nMTMCMC: We had Dmax = 10 and gs = 0.75 for any s \u2208 Smax. In MTMCMC-Po, the distribution of y assigned at each node s and its prior distribution were assumed to be the Poisson distribution Po(y|\u03bds) and the gamma distribution Gam(\u03bds|1, 1), respectively. In MTMCMC-LR, the distribution of y assigned at each node s and its prior distribution were assumed to be linear regression model N (y|w\u22a4s x, \u03c32s) and the normal-gamma prior N (ws|0, I/\u03c32s)Gam(1/\u03c32s |1, 1), respectively. g\u0304 in (18) was tuned in the burn-in phase. The number of replicas in the REMC method was 8. The replica exchange procedure was made every 10 iterations of the MH method. In each replica exchange procedure, randomly selected 4 replicas are sequentially tried to exchange.\nMTRF: We had Dmax = 10 and gs = 0.75 for any s \u2208 Smax. The number of meta-trees used for the prediction was 100, which was the default value of the library [16].\nRF: The maximum depth was 10. The other hyperparameters were default values of the library [17].\nXGBoost: All the hyperparameters were default values of the library [4].\nLightGBM: All the hyperparameters were default values of the library [9].\nBART: We used the gbart (generalized BART) function implemented in [18]. The ntree option was set at 1 for comparison with our method under the same condition. The other hyperparameters were default values of the library.\nBART-Multi: BART with default ntree option.\nI In-Depth Experiments on Algorithm Behavior"
        },
        {
            "heading": "I.1 Experiment 3: Convergence to Exact Posterior",
            "text": "Purpose: we confirm the convergence of the MCMC sample distribution to the exact posterior distribution. Since our proposal distributions satisfy the detailed balance and ergodicity, the approximated posteriors are expected to converge to the exact one. Further, we confirm the effectiveness of the design policy of the proposal distribution, compared with the uniform proposal distribution.\nConditions: to calculate the exact posterior distribution, we fix a true model with small p, q and Dmax. Specifically, we perform the experiment under the following conditions. We assume p = 0 and q = 5. Therefore, all the explanatory variables are binary. Y is also the binary set {0, 1}. We assume Dmax = 3. Then, we have |K| = 57 = 78125. Specific values of k, T , and \u03b8 are shown in Fig. 8. The data generative model p(y|\u03b8s) is the Bernoulli distribution Bern(y|\u03b8s). The ith explanatory variable xi is independently generated according to the uniform distribution on {0, 1}q. Then, yi is generated from the model shown in Fig. 8. The sample size n is 100 and the number of generated samples is 10.\nFor posterior learning, we independently assume the beta distribution Beta(\u03b8s|0.5, 0.5) as the prior distribution for each \u03b8s. The hyperparameter of p(T ) is fixed at gs = 0.5 for each s \u2208 Imax. Herein, we utilize two proposal distributions: the uniform distribution and (19). The tuning parameter g\u0304 in (18) of the tree posterior based proposal distribution q(k\u2217|k(t\u22121)) is fixed at 0.75. The burn-in length is 500 and the MCMC process is continued until 1000 samples are accepted.\nResults: we evaluate the distance d(p, p\u0302) between the exact posterior distribution p(k|xn, yn) and the approximated posterior distribution p\u0302(k|xn, yn) obtained from the MCMC sample by the following\nJensen-Shannon divergence.5\nd(p, p\u0302) := 1\n2 \u2211 k\u2208K p(k|xn, yn) log p(k|x n, yn) r(k|xn, yn) + 1 2 \u2211 k\u2208K p\u0302(k|xn, yn) log p\u0302(k|x n, yn) r(k|xn, yn) , (42)\nwhere r(k|xn, yn) := (p(k|xn, yn) + p\u0302(k|xn, yn))/2 and we use the convention that 0 log 0 = 0. Figure 9 shows the transition of the distance d(p, p\u0302) for the increase of the number of the accepted tests. Both of the approximated posteriors converge to the exact one as expected. The convergence speed of the tree posterior based proposal distribution is faster than that of the uniform proposal distribution. In addition, the acceptance ratio of the tree posterior based proposal distribution was 0.274, while that of the uniform proposal distribution was 0.0118. These results support the effectiveness of our design policy of the proposal distribution. We also obtained similar results for other data generative models described in the next subsection."
        },
        {
            "heading": "I.2 Experiment 4: Comparison of Convergence",
            "text": "We compare the convergence of the MCMC sample distribution obtained from the aforementioned four proposal distributions q(k\u2217|k(t\u22121)): the uniform distribution, Eq. (34), Eq. (19) and Eq. (36). We assumed three models shown in the upper side of Fig. 10. Herein, we assumed p = 0. The other hyperparameters are the same as those for Experiment 3. Resuls are shown in the lower side of Fig. 10 and Table 1. The tree posterior based proposal distributions (19) and (36) showed better\n5Since p\u0302(k|xn, yn) is a empirical distribution and takes 0 on some points in K, the usual Kullback\u2013Leibler divergence cannot be evaluated.\nperformances, i.e., they showed faster convergence and higher acceptance ratio than the others. In particular, the uniform proposal distribution and Eq. (34) showed extremely low acceptance ratio for Model B, which has an unbalanced shape."
        },
        {
            "heading": "I.3 Experiment 5: Confirmation of Likelihood Behavior",
            "text": "We confirm the behavior of our MCMC method from a perspective of likelihood. If our design policy of the proposal distribution q(k\u2217|k(t\u22121)) works, the likelihood p(yn|xn,k(t)) should increase in the early phase of the MCMC iterations and stay high.\nActually, we observed the desirable behavior for the real-world example [19] used in Experiment 2 in the main paper as shown in Fig. 11."
        },
        {
            "heading": "J Computing Resources",
            "text": "The main computing resources used in our experiments are as follows:\n\u2022 For Experiment 1 \u2013 Desktop 1\n* CPU: Intel(R) Xeon(R) Gold 6128 CPU @ 3.40GHz * Memory: 64GB * OS: Windows 10 Pro\n\u2013 Desktop 2 * CPU: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz * Memory: 64GB * OS: Windows 10 Pro\n\u2022 For Experiment 2 \u2013 Laptop 1\n* CPU: Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz * Memory: 8GB * OS: Windows 11 Pro"
        }
    ],
    "title": "Prediction Algorithms Achieving Bayesian Decision Theoretical Optimality Based on Decision Trees as Data Observation Processes",
    "year": 2023
}