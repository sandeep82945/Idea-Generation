{
    "abstractText": "Object recognition and object identification are multifaceted cognitive operations that require various brain regions to synthesize and process information. Prior research has evidenced the activity of both visual and temporal cortices during these tasks. Notwithstanding their similarities, object recognition and identification are recognized as separate brain functions. Drawing from the two-stream hypothesis, our investigation aims to understand whether the channels within the ventral and dorsal streams contain pertinent information for effective model learning regarding object recognition and identification tasks. By utilizing the data we collected during the object recognition and identification experiment, we scrutinized EEGNet models, trained using channels that replicate the two-stream hypothesis pathways, against a model trained using all available channels. The outcomes reveal that the model trained solely using the temporal region delivered a high accuracy level in classifying four distinct object categories. Specifically, the object recognition and object identification models achieved an accuracy of 89% and 85%, respectively. By incorporating the channels that mimic the ventral stream, the model\u2019s accuracy was further improved, with the object recognition model and object identification model achieving an accuracy of 95% and 94%, respectively. Furthermore, the Grad-CAM result of the trained models revealed a significant contribution from the ventral and dorsal stream channels toward the training of the EEGNet model. The aim of our study is to pinpoint the optimal channel configuration that provides a swift and accurate brain-computer interface system for object recognition and identification.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniel Leong"
        },
        {
            "affiliations": [],
            "name": "Thomas (Tien-Thong"
        }
    ],
    "id": "SP:247e1fe443f0b7d0b33e7f5e899c304f0dbd3a96",
    "references": [
        {
            "authors": [
                "M.A. Goodale",
                "A.D. Milner"
            ],
            "title": "Separate visual pathways for perception and action",
            "venue": "Trends in neurosciences, vol. 15, no. 1, pp. 20\u2013 25, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "R.M. Cichy",
                "D. Pantazis",
                "A. Oliva"
            ],
            "title": "Similarity-based fusion of MEG and fMRI reveals spatio-temporal dynamics in human cortex during visual object recognition",
            "venue": "Cerebral Cortex, vol. 26, no. 8, pp. 3563\u2013 3579, 2016. This article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3339698 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9",
            "year": 2016
        },
        {
            "authors": [
                "T.F. Tafreshi",
                "M.R. Daliri",
                "M. Ghodousi"
            ],
            "title": "Functional and effective connectivity based features of EEG signals for object recognition",
            "venue": "Cognitive neurodynamics, vol. 13, no. 6, pp. 555\u2013566, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Rizkallah",
                "P. Benquet",
                "A. Kabbara",
                "O. Dufor",
                "F. Wendling",
                "M. Hassan"
            ],
            "title": "Dynamic reshaping of functional brain networks during visual object recognition",
            "venue": "Journal of neural engineering, vol. 15, no. 5, p. 056022, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Dyck",
                "M.B. Brodeur"
            ],
            "title": "Erp evidence for the influence of scene context on the recognition of ambiguous and unambiguous objects",
            "venue": "Neuropsychologia, vol. 72, pp. 43\u201351, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Truman",
                "L. Mudrik"
            ],
            "title": "Are incongruent objects harder to identify? the functional significance of the n300 component",
            "venue": "Neuropsychologia, vol. 117, pp. 222\u2013232, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "G. Lu",
                "G. Hou"
            ],
            "title": "Effects of semantic congruence on sign identification: an erp study",
            "venue": "Human Factors, vol. 62, no. 5, pp. 800\u2013811, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.I. Coco",
                "A. Nuthmann",
                "O. Dimigen"
            ],
            "title": "Fixation-related brain potentials during semantic integration of object\u2013scene information",
            "venue": "Journal of Cognitive Neuroscience, vol. 32, no. 4, pp. 571\u2013589, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N.K. Logothetis",
                "D.L. Sheinberg"
            ],
            "title": "Visual object recognition",
            "venue": "Annual review of neuroscience, vol. 19, no. 1, pp. 577\u2013621, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "A.M. Chan",
                "E. Halgren",
                "K. Marinkovic",
                "S.S. Cash"
            ],
            "title": "Decoding word and category-specific spatiotemporal representations from MEG and EEG",
            "venue": "Neuroimage, vol. 54, no. 4, pp. 3028\u20133039, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "C. Wang",
                "S. Xiong",
                "X. Hu",
                "L. Yao",
                "J. Zhang"
            ],
            "title": "Combining features from ERP components in single-trial EEG for discriminating fourcategory visual objects",
            "venue": "Journal of neural engineering, vol. 9, no. 5, p. 056013, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Y. Qin",
                "Y. Zhan",
                "C. Wang",
                "J. Zhang",
                "L. Yao",
                "X. Guo",
                "X. Wu",
                "B. Hu"
            ],
            "title": "Classifying four-category visual objects using multiple ERP components in single-trial ERP",
            "venue": "Cognitive neurodynamics, vol. 10, no. 4, pp. 275\u2013285, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "B. Choi",
                "S. Jo"
            ],
            "title": "A low-cost EEG system-based hybrid braincomputer interface for humanoid robot navigation and recognition",
            "venue": "PloS one, vol. 8, no. 9, p. e74583, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "F. Arrichiello",
                "P. Di Lillo",
                "D. Di Vito",
                "G. Antonelli",
                "S. Chiaverini"
            ],
            "title": "Assistive robot operated via p300-based brain computer interface",
            "venue": "2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 6032\u20136037.",
            "year": 2017
        },
        {
            "authors": [
                "K. Kaspar",
                "U. Hassler",
                "U. Martens",
                "N. Trujillo-Barreto",
                "T. Gruber"
            ],
            "title": "Steady-state visually evoked potential correlates of object recognition",
            "venue": "Brain research, vol. 1343, pp. 112\u2013121, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "X. Chen",
                "B. Zhao",
                "Y. Wang",
                "X. Gao"
            ],
            "title": "Combination of highfrequency SSVEP-based BCI and computer vision for controlling a robotic arm",
            "venue": "Journal of neural engineering, vol. 16, no. 2, p. 026012, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S.-K. Chen",
                "C.-S. Chen",
                "Y.-K. Wang",
                "C.-T. Lin"
            ],
            "title": "An SSVEP Stimuli Design using Real-time Camera View with Object Recognition",
            "venue": "2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 2020, pp. 562\u2013567.",
            "year": 2020
        },
        {
            "authors": [
                "V.J. Lawhern",
                "A.J. Solon",
                "N.R. Waytowich",
                "S.M. Gordon",
                "C.P. Hung",
                "B.J. Lance"
            ],
            "title": "EEGNet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces",
            "venue": "Journal of neural engineering, vol. 15, no. 5, p. 056013, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "H. Ahmed",
                "R.B. Wilbur",
                "H.M. Bharadwaj",
                "J.M. Siskind"
            ],
            "title": "Object classification from randomized EEG trials",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3845\u20133854.",
            "year": 2021
        },
        {
            "authors": [
                "R. Shi",
                "Y. Zhao",
                "Z. Cao",
                "C. Liu",
                "Y. Kang",
                "J. Zhang"
            ],
            "title": "Categorizing objects from MEG signals using EEGNet",
            "venue": "Cognitive Neurodynamics, pp. 1\u201313, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Komolovait\u0117",
                "R. Maskeli\u016bnas",
                "R. Dama\u0161evi\u010dius"
            ],
            "title": "Deep convolutional neural network-based visual stimuli classification using electroencephalography signals of healthy and alzheimer\u2019s disease subjects",
            "venue": "Life, vol. 12, no. 3, p. 374, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Huang",
                "X. Wang",
                "J. Liu",
                "J. Li",
                "W. Tang"
            ],
            "title": "Virtual reality safety training using deep EEG-net and physiology data",
            "venue": "The visual computer, vol. 38, no. 4, pp. 1195\u20131207, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Borra",
                "F. Bossi",
                "D. Rivolta",
                "E. Magosso"
            ],
            "title": "Deep learning applied to EEG source-data reveals both ventral and dorsal visual stream involvement in holistic processing of social stimuli",
            "venue": "Scientific Reports, vol. 13, no. 1, p. 7365, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "M. Sim\u00f5es",
                "D. Borra",
                "E. Santamar\u0131\u0301a-V\u00e1zquez",
                "GBT-UPM",
                "M. Bittencourt-Villalpando",
                "D. Krzemi\u0144ski",
                "A. Miladinovi\u0107",
                "Neural Engineering Group",
                "T. Schmid",
                "H. Zhao"
            ],
            "title": "Bciautp300: A multi-session and multi-subject benchmark dataset on autism for p300-based brain-computer-interfaces",
            "venue": "Frontiers in Neuroscience, vol. 14, p. 568104, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Borra",
                "E. Magosso"
            ],
            "title": "Deep learning-based EEG analysis: investigating P3 ERP components",
            "venue": "Journal of Integrative Neuroscience, vol. 20, no. 4, pp. 791\u2013811, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Borra",
                "E. Magosso",
                "M. Castelo-Branco",
                "M. Sim\u00f5es"
            ],
            "title": "A bayesianoptimized design for an interpretable convolutional neural network to decode and analyze the p300 response in autism",
            "venue": "Journal of Neural Engineering, vol. 19, no. 4, p. 046010, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Jonas",
                "A.O. Rossetti",
                "M. Oddo",
                "S. Jenni",
                "P. Favaro",
                "F. Zubler"
            ],
            "title": "EEG-based outcome prediction after cardiac arrest with convolutional neural networks: Performance and visualization of discriminative features",
            "venue": "Human brain mapping, vol. 40, no. 16, pp. 4606\u20134617, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "H. Yang",
                "J. Li",
                "D. Chen",
                "M. Du"
            ],
            "title": "EEG-based intention recognition with deep recurrent-convolution neural network: Performance and channel selection by Grad-CAM",
            "venue": "Neurocomputing, vol. 415, pp. 225\u2013 233, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Griffin",
                "A. Holub",
                "P. Perona"
            ],
            "title": "Caltech-256 object category dataset",
            "venue": "2007.",
            "year": 2007
        },
        {
            "authors": [
                "A. Delorme",
                "S. Makeig"
            ],
            "title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis",
            "venue": "Journal of neuroscience methods, vol. 134, no. 1, pp. 9\u201321, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "L. Pion-Tonachini",
                "K. Kreutz-Delgado",
                "S. Makeig"
            ],
            "title": "Iclabel: An automated electroencephalographic independent component classifier, dataset, and website",
            "venue": "NeuroImage, vol. 198, pp. 181\u2013197, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D.-A. Clevert",
                "T. Unterthiner",
                "S. Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "venue": "arXiv preprint arXiv:1511.07289, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "P.-T. De Boer",
                "D.P. Kroese",
                "S. Mannor",
                "R.Y. Rubinstein"
            ],
            "title": "A tutorial on the cross-entropy method",
            "venue": "Annals of operations research, vol. 134, pp. 19\u201367, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "R. Caruana",
                "S. Lawrence",
                "C. Giles"
            ],
            "title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping",
            "venue": "Advances in neural information processing systems, vol. 13, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "Proceedings of the IEEE international conference on computer vision, 2017, pp. 618\u2013626.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Kourtzi",
                "N. Kanwisher"
            ],
            "title": "Representation of perceived object shape by the human lateral occipital complex",
            "venue": "Science, vol. 293, no. 5534, pp. 1506\u20131509, 2001.",
            "year": 2001
        },
        {
            "authors": [
                "A. Amedi",
                "G. Jacobson",
                "T. Hendler",
                "R. Malach",
                "E. Zohary"
            ],
            "title": "Convergence of visual and tactile shape processing in the human lateral occipital complex",
            "venue": "Cerebral cortex, vol. 12, no. 11, pp. 1202\u20131212, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "A. Martin"
            ],
            "title": "The representation of object concepts in the brain",
            "venue": "Annu. Rev. Psychol., vol. 58, pp. 25\u201345, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "H. Karimi-Rouzbahani",
                "M. Shahmohammadi",
                "E. Vahab",
                "S. Setayeshi",
                "T. Carlson"
            ],
            "title": "Temporal variabilities provide additional categoryrelated information in object category decoding: a systematic comparison of informative EEG features",
            "venue": "Neural Computation, vol. 33, no. 11, pp. 3027\u20133072, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.M. Proverbio",
                "F. Riva"
            ],
            "title": "Rp and n400 erp components reflect semantic violations in visual processing of human actions",
            "venue": "Neuroscience letters, vol. 459, no. 3, pp. 142\u2013146, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Abreu",
                "L. Fern\u00e1ndez-Aguilar",
                "F. Ferreira-Santos",
                "C. Fernandes"
            ],
            "title": "Increased n250 elicited by facial familiarity: An erp study including the face inversion effect and facial emotion processing",
            "venue": "Neuropsychologia, p. 108623, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Enge",
                "F. S\u00fc\u00df",
                "R.A. Rahman"
            ],
            "title": "Instant effects of semantic information on visual perception",
            "venue": "Journal of Neuroscience, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "E.L. Mazerolle",
                "R.C. D\u2019Arcy",
                "Y. Marchand",
                "R.B. Bolster"
            ],
            "title": "Erp assessment of functional status in the temporal lobe: Examining spatiotemporal correlates of object recognition",
            "venue": "International Journal of Psychophysiology, vol. 66, no. 1, pp. 81\u201392, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "I.D. Rus",
                "P. Marc",
                "M. Dinsoreanu",
                "R. Potolea",
                "R.C. Muresan"
            ],
            "title": "Classification of EEG signals in an object recognition task",
            "venue": "2017 13th IEEE International Conference on Intelligent Computer Communication and Processing (ICCP). IEEE, 2017, pp. 391\u2013395.",
            "year": 2017
        },
        {
            "authors": [
                "A. Das",
                "A. Mandel",
                "H. Shitara",
                "T. Popa",
                "S.G. Horovitz",
                "M. Hallett",
                "N. Thirugnanasambandam"
            ],
            "title": "Evaluating interhemispheric connectivity during midline object recognition using EEG",
            "venue": "PloS one, vol. 17, no. 8, p. e0270949, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.R. Brotchie",
                "R.A. Andersen",
                "L.H. Snyder",
                "S.J. Goodman"
            ],
            "title": "Head position signals used by parietal neurons to encode locations of visual stimuli",
            "venue": "Nature, vol. 375, no. 6528, pp. 232\u2013235, 1995. This article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3339698 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 10",
            "year": 1995
        },
        {
            "authors": [
                "M.T. de Schotten",
                "M. Urbanski",
                "H. Duffau",
                "E. Volle",
                "R. L\u00e9vy",
                "B. Dubois",
                "P. Bartolomeo"
            ],
            "title": "Direct evidence for a parietal-frontal pathway subserving spatial awareness in humans",
            "venue": "Science, vol. 309, no. 5744, pp. 2226\u20132228, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Wamain",
                "F. Gabrielli",
                "Y. Coello"
            ],
            "title": "EEG \u03bc rhythm in virtual reality reveals that motor coding of visual objects in peripersonal space is task dependent",
            "venue": "Cortex, vol. 74, pp. 20\u201330, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "E. Freud",
                "J.C. Culham",
                "D.C. Plaut",
                "M. Behrmann"
            ],
            "title": "The large-scale organization of shape processing in the ventral and dorsal pathways",
            "venue": "elife, vol. 6, p. e27576, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V. Ayzenberg",
                "C. Simmons",
                "M. Behrmann"
            ],
            "title": "Temporal asymmetries and interactions between dorsal and ventral visual pathways during object recognition",
            "venue": "Cerebral Cortex Communications, vol. 4, no. 1, p. tgad003, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "V. Ayzenberg",
                "M. Behrmann"
            ],
            "title": "The dorsal visual pathway represents object-centered spatial relations for object recognition",
            "venue": "Journal of Neuroscience, vol. 42, no. 23, pp. 4693\u20134710, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.K. Jeong",
                "Y. Xu"
            ],
            "title": "Behaviorally relevant abstract object identity representation in the human parietal cortex",
            "venue": "Journal of Neuroscience, vol. 36, no. 5, pp. 1607\u20131619, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Wang",
                "T. Zhou",
                "M. Qiu",
                "A. Du",
                "K. Cai",
                "Z. Wang",
                "C. Zhou",
                "M. Meng",
                "Y. Zhuo",
                "S. Fan"
            ],
            "title": "Relationship between ventral stream for object vision and dorsal stream for spatial vision: An fmri+ erp study",
            "venue": "Human Brain Mapping, vol. 8, no. 4, pp. 170\u2013181, 1999.",
            "year": 1999
        },
        {
            "authors": [
                "H.-O. Karnath",
                "J. R\u00fcter",
                "A. Mandler",
                "M. Himmelbach"
            ],
            "title": "The anatomy of object recognition\u2014visual form agnosia caused by medial occipitotemporal stroke",
            "venue": "Journal of Neuroscience, vol. 29, no. 18, pp. 5854\u2013 5862, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "P. Janssen",
                "B.-E. Verhoef",
                "E. Premereur"
            ],
            "title": "Functional interactions between the macaque dorsal and ventral visual pathways during threedimensional object vision",
            "venue": "Cortex, vol. 98, pp. 218\u2013227, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "U. G\u00fc\u00e7l\u00fc",
                "M.A. van Gerven"
            ],
            "title": "Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream",
            "venue": "Journal of Neuroscience, vol. 35, no. 27, pp. 10 005\u201310 014, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R.M. Cichy",
                "A. Khosla",
                "D. Pantazis",
                "A. Torralba",
                "A. Oliva"
            ],
            "title": "Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence",
            "venue": "Scientific reports, vol. 6, no. 1, p. 27755, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C. Zhuang",
                "S. Yan",
                "A. Nayebi",
                "M. Schrimpf",
                "M.C. Frank",
                "J.J. DiCarlo",
                "D.L. Yamins"
            ],
            "title": "Unsupervised neural network models of the ventral visual stream",
            "venue": "Proceedings of the National Academy of Sciences, vol. 118, no. 3, p. e2014196118, 2021.",
            "year": 2014
        },
        {
            "authors": [
                "G.M. Doniger",
                "J.J. Foxe",
                "M.M. Murray",
                "B.A. Higgins",
                "J.G. Snodgrass",
                "C.E. Schroeder",
                "D.C. Javitt"
            ],
            "title": "Activation timecourse of ventral visual stream object-recognition areas: high density electrical mapping of perceptual closure processes",
            "venue": "Journal of cognitive neuroscience, vol. 12, no. 4, pp. 615\u2013621, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "M. Behroozi",
                "M.R. Daliri",
                "B. Shekarchi"
            ],
            "title": "EEG phase patterns reflect the representation of semantic categories of objects",
            "venue": "Medical & biological engineering & computing, vol. 54, no. 1, pp. 205\u2013221, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A.X. Stewart",
                "A. Nuthmann",
                "G. Sanguinetti"
            ],
            "title": "Single-trial classification of EEG in a visual object task using ICA and machine learning",
            "venue": "Journal of neuroscience methods, vol. 228, pp. 1\u201314, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D.J. Kravitz",
                "K.S. Saleem",
                "C.I. Baker",
                "L.G. Ungerleider",
                "M. Mishkin"
            ],
            "title": "The ventral visual pathway: an expanded neural framework for the processing of object quality",
            "venue": "Trends in cognitive sciences, vol. 17, no. 1, pp. 26\u201349, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "H. Takemura",
                "A. Rokem",
                "J. Winawer",
                "J.D. Yeatman",
                "B.A. Wandell",
                "F. Pestilli"
            ],
            "title": "A major human white matter pathway between dorsal and ventral visual cortex",
            "venue": "Cerebral cortex, vol. 26, no. 5, pp. 2205\u20132214, 2016. This article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3339698 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Brain-Computer Interfaces (BCIs); object identification; object recognition; electroencephalogram (EEG); EEGNet; Deep learning; Ventral stream; Dorsal stream\nI. INTRODUCTION In our daily routines, we are faced with the task of decoding a large amount of changing visual information. In order to effectively engage with our surroundings, it is essential for our visual system to rapidly identify and interpret the visual information present in our environment. Impressively, our\nThis work was supported in part by the Australian Research Council (ARC) under discovery grants DP180100656 and DP210101093. Research was also sponsored in part by the Australia Defense Innovation Hub under Contract No. P18-650825, US Office of Naval Research Global under Cooperative Agreement Number ONRG - NICOP - N62909-19-1-2058, and AFOSR \u2013 DST Australian Autonomy Initiative agreement ID10134. We also thank the NSW Defense Innovation Network and NSW State Government of Australia for financial support in part of this research through grants DINPP2019 S103/09 and PP21-22.03.02.\nD. L., T. D. and C.-T. L. are with the University of Technology Sydney, Faculty of Engineering and Information Technology, Australian Artificial Intelligence Institute and GrapheneX-UTS Human-centric Artificial Intelligence Centre, 15 Broadway, Ultimo, New South Wales 2007, Australia.\n*Corresponding authors: Chin-Teng Lin (e-mail: ChinTeng.Lin@uts.edu.au)\nbrains demonstrate an exceptional ability to search for and perceive intricate images from nature with both speed and precision. Despite numerous investigations aiming to decode the functionality of our visual system, there is still a lack of comprehensive understanding of this intricate network. Nowadays, numerous theories and hypotheses have been proposed to explain how our brains recognize objects. The widely recognized two-streams hypothesis [1] is currently regarded as the prevailing model that explains the brain\u2019s visual processing mechanisms. This hypothesis suggests that when the occipital lobe, the brain\u2019s visual processing region, receives visual data, it splits it into two processing routes: the ventral and dorsal streams. The ventral stream sends its information to the temporal lobe, where the object is identified and recognized. The dorsal stream, on the other hand, is responsible for processing visual-spatial information and determining the object\u2019s location relative to the observer. This information is then relayed from the occipital lobe to the parietal lobe. Through fMRI and MEG studies, the activity of the ventral stream has been observed, thus highlighting its role in object recognition in the human brain [2].\nNumerous EEG-based studies on object recognition have similar research designs, where participants are expected to respond by pressing a button when a target stimulus is displayed. The main focus of these studies lies in these target stimuli. They are often keywords related to the object, such as its category [3], or they are used to assess the object\u2019s meaningfulness [4] or ambiguity [5]. Nevertheless, a handful of studies have modified their experimental design to focus on object identification in the brain instead of object recognition. For example, some studies necessitate participants to view congruent and incongruent scenes where a key object remains constant, and they are asked to identify that critical object among other objects in the scene [6], [7]. Another study requires participants to spot a target object that is either semantically consistent or inconsistent within a scene and press a button whenever the target object alters its identity, location, or both [8]. These studies highlight that the differentiation between object recognition and identification primarily hinges on the number of objects a person is presented with. When exposed to a single object, individuals will use object recognition. Conversely, when asked to distinguish among multiple objects, they resort to object identification. Despite the similarities between object recognition and identification, object identification is\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 perceived as a unique process, with different brain regions engaged in processing the information [9].\nSeveral researchers have endeavoured to develop BrainComputer Interface (BCI) systems for object recognition and identification by using salient EEG features. Event-Related Potential (ERP), a prevalent EEG feature, is the brain\u2019s response to a specific event or stimulus. In the context of a BCI system for object recognition, ERPs are recorded when an object enters the participant\u2019s visual field and subsequently classified according to the object\u2019s category ([10], [11], [12]). Conversely, an object identification BCI system relies on a visual stimulus, such as a flash or multiple flashes over the selected object, to provoke the ERP response. For instance, the P3-based BCI identifies objects based on the P3 peak, which occurs approximately 300-500 ms following event onset ([13], [14]). Beyond the P3 peak, the steady-state visually evoked potential (SSVEP) is another feature commonly used in BCI-based object identification systems. This technique involves placing a flicker at a specific frequency over the chosen object ([15], [16], [17]). However, despite the progress made in developing BCI systems that distinguish between object recognition and identification, the majority are not yet ready for practical application. The challenge lies in the systems\u2019 inability to discern the user\u2019s intention behind targeting an object: whether the user intends for the BCI to recognize the object or whether the user wants the BCI to select an object from their environment.\nWhile Event-Related Potential (ERP) is a prevalent feature utilized in EEG/BCI studies due to the insightful data it provides about cognitive processes and neural activities associated with specific events, its identification and recognition by the naked eye can be challenging, as it can differ significantly across individuals. As a result, machine learning algorithms are employed to facilitate a more precise, efficient, and objective analysis of ERP. Among all machine learning algorithms applied to EEG data analysis, EEGNet has displayed encouraging results in various EEG analysis tasks [18]. EEGNet is a compacted convolutional neural network incorporating depthwise and separable convolutions, enabling the effective capture of both spatial and temporal information in EEG signals. Several studies have confirmed the effectiveness of EEGNet in analyzing object-related ERP ([19], [20], [21], [22], [23]) and other EEG features such as P300 [24]. Nonetheless, comprehending the effectiveness of the EEGNet model requires an explanation of how the model learned from EEG data. Consequently, the utilization of explanation techniques has gained prominence as a means to visualize EEGNet models. Notably, various studies have utilized explanation techniques, including saliency maps [25], [26] and Grad-CAM [27], [28], to highlight the noteworthy EEG channels within the trained EEG models. For better classification results of object-related ERP, researchers often aim to utilize as many channels as possible within the region of interest. However, increasing the number of channels also escalates the complexity and latency of the BCI system, which isn\u2019t practical for real-time applications.\nThe objective of our study is to identify the best channel configuration for a fast and accurate BCI system for object recognition and identification. We examined the model trained using channels that emulate the pathways of the two-stream hypothesis compared to the model trained using all channels. The aim is to determine whether the channels within the ventral and dorsal streams contain information that could facilitate effective learning of the model on tasks related to object recognition and identification."
        },
        {
            "heading": "II. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A. Participant and Data Recording",
            "text": "In this study, a total of 25 participants, with an average age of 32.5 \u00b1 10.4 years and either normal or corrected-to-normal vision, were involved. The participants undertook 600 trials each, conducted at the Computation Intelligence and BrainComputer Interface (CIBCI) Centre situated at the University of Technology Sydney (UTS). Prior to the experiment, the participants were briefed on the instructions and were required to sign a consent form after being informed. The University of Technology Sydney granted ethical approval for this study under the ethics ID ETH20-5519.\nWe recorded the brain activities of the participants using a 64-channel EEG system produced by Neuroscan Compumedics Australia. This medical-grade device, known for its high-density EEG recordings and high precision, has been extensively utilised in previous neuroscience and neurodiagnostics research. The EEG electrodes were positioned according to the extended 10-20 international system, and the data was referenced to an electrode closest to the standard position FCZ. We maintained the electrode impedance below 5 k\u2126 and digitally sampled the EEG recordings at a rate of 1000 Hz."
        },
        {
            "heading": "B. Experimental Design",
            "text": "In the course of the experiment, participants were asked to undertake two tasks: object recognition and object identification. The object recognition task consisted of presenting randomly selected images from four categories of the Caltech-256 dataset [29], namely animals, flowers, food, and vehicles. Each category contained five distinct objects, with ten images per object, yielding a total of 200 images used in the experiment. At the beginning of the trial, participants were displayed a target image for a duration of 1 second and then prompted to answer if it was part of the specified category (see Figure 1). The objective of this task was to assess the participant\u2019s accuracy in recognizing the target image.\nFollowing the object recognition task, participants were asked to perform an object identification task. For this task, four images were randomly chosen from the dataset and presented in a four-image configuration (up, down, left, and right). However, at least one of the objects displayed was from the same category and subtype as in the preceding object\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3 recognition task. Participants were directed to select the image most closely related to the target image by pressing a button corresponding to the up, down, left or right directions within 3 seconds. Note that, due to the random selection of images, more than one image from the same category and subtype could be presented as options. Each trial lasted a total of 6 seconds, with a fixation cross appearing for 300 ms to mark the start of the trial. An example trial is illustrated in Figure 1. Each participant will perform 600 trials of the task, resulting in a total of 15000 trials over 25 participants."
        },
        {
            "heading": "C. EEG Analysis",
            "text": "The processing of EEG signals was carried out using EEGLAB v14.1.2 [30], a MATLAB toolbox. The unprocessed EEG data underwent filtering through a finite impulse response (FIR) filter, consisting of a 1 Hz high-pass and a 50 Hz low-pass filter. Channels identified as noisy were excluded using the EEGALB function \u2019clean channels\u2019 (3 \u00b1 2 channels per subject removed), and the data was re-referenced to the average. Following this, the adaptive mixed independent component analysis (AMICA) was implemented on the rereferenced data to decompose it into maximally independent components (ICs). These ICs represent statistically independent sources of EEG variance. Using the IClabel toolbox [31], we removed ICs associated with eye movement and muscle activity (3 \u00b1 1 ICs per subject removed). After discarding these undesired components, epochs were extracted. Each epoch spanned the entire trial duration, starting from 300 ms prior to the appearance of the target image (i.e. the event onset) and ending 5 seconds post-event onset. We identified and removed bad epochs by examining their data values and considering whether they exceeded the specified standard deviation threshold of 150 uV (394 \u00b1 57 trials per subject after removal). The epoched data were subsequently divided into two categories based on the object-related tasks. For both object recognition and identification tasks, a onesecond segment post-stimulation was extracted, resulting in a matrix of dimensions 60 (electrodes) \u00d7 1000(sampling points) x number of epochs."
        },
        {
            "heading": "D. EEGNet",
            "text": "1) EEGNet Structure: The EEGNet architecture utilized in this study adheres to a standard block structure comprising a temporal convolution layer, a depthwise convolution layer, and a separable convolution layer. The first layer, the temporal convolution layer, learns temporal filters by applying convolution operations on the input EEG data over time. It possesses filters that cover only a single EEG channel and multiple time points, preventing any mixing of data from different channels. The purpose of this layer is to learn time-dependent features, such as oscillations in the EEG signal, which represent changes in brain activity during object-related tasks. The next layer, the depthwise convolution layer, carries out depthwise convolutions. This layer applies a distinct set of filters to each input channel separately, possessing filters that span multiple channels and time points, allowing the model to learn spatial filters across\nchannels as they evolve over time. This layer\u2019s purpose is to learn spatial features, reflecting the distribution of brain activity across various brain areas or channels. These spatial features can aid in identifying patterns associated with specific brain states or tasks. Following the depthwise convolution layer, the model uses the separable convolution layer. This layer applies a depthwise spatial convolution followed by a pointwise convolution. Essentially, it applies a separate set of filters to each input channel and uses a 1x1 convolution to mix the output channels. This approach allows the model to learn more complex and abstract features that combine spatial and temporal information. This layer adds an extra layer of complexity to the learned features, potentially enhancing the model\u2019s accuracy.\nFollowing the depthwise convolution and separable convolution layers, batch normalization is used to boost the neural networks\u2019 speed, efficacy, and stability by normalizing the output from the previous layer. This step aids in learning stability and acceleration. Following batch normalization, the model utilizes the exponential linear unit (ELU) activation function. The ELU activation function\u2019s ability to introduce non-linearity into the model is vital for EEG data. Furthermore, the ELU activation function can hasten learning because it generates a balanced output with an average closer to zero and can mitigate the dead neuron issue [32]. The output of the ELU activation function is then subjected to an average pooling operation that reduces its dimensionality and offers a degree of translation invariance. A dropout layer follows, which helps prevent overfitting by providing a form of regularization. The output from the preceding layer is then reshaped via the Flatten layer and passed through a dense layer. This dense layer utilizes the features learned by the preceding layers for the final classification. Ultimately, the softmax function is applied to convert the network output into probability scores for each class. The overall structure is illustrated in Figure 2.\n2) Training procedure: After removing bad epochs, a total of 9835 epochs remained from the collective pool of 25 participants. This epoch dataset was divided into training, testing, and validation sets, with the training set comprising 80% of the entire dataset and the remaining 20% split evenly between testing and validation sets. The model was initially compiled using the Adam optimizer [33] and the categorical cross-entropy loss function [34], which provides the necessary tools and standards to modify the model\u2019s parameters during its training phase. Despite the EEGNet structure already incorporating elements designed to help prevent model overfitting, such as batch normalization and dropout, we introduced additional techniques to aid in model training. These included early stopping and a learning rate schedule. Early stopping uses a validation set to assess the model\u2019s performance following each epoch and halts the training when the performance on the validation set begins to decline [35]. On the other hand, learning rate schedules offer a mechanism for adjusting the learning rate throughout training by reducing the learning rate based on a predefined schedule, which for this training was an\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nexponential decay. After all, we ensured that the training loss and validation loss were approximately equal and relatively low before progressing to model prediction."
        },
        {
            "heading": "III. RESULTS",
            "text": ""
        },
        {
            "heading": "A. ERP of object recognition and object identification",
            "text": "Figure 3 presents the distinctive traits of the ERP signals for both object recognition and object identification tasks. These results were obtained by averaging the ERP outcomes from all 25 participants, with scalp topography visualized using EEGLAB\u2019s topoplot function. The upper portion of the figure demonstrates the average ERP throughout the trial for all channels, with ERPs corresponding to channels O1, P7, and T7 represented in blue, green, and red, respectively. The lower half of the figure displays the scalp topography across the trial for all four categories and their average. According to the scalp topography, the occipital area appears to be the most active region, from 100ms to 450ms during object recognition and from 300ms to 650ms during object identification."
        },
        {
            "heading": "B. Model comparisons",
            "text": "Figure 4 presents a comparative analysis of the EEGNet model trained with differing configurations of EEG channels for both the tasks of object recognition and object identification at both group and participant levels. The channel configurations are grouped as follows: the visual region (O1, OZ, O2), the temporal region (T7, TP7, TP8, T8), the ventral stream (T7, T8, TP7, TP8, P7, P8, PO7, PO8, O1, OZ, O2), the dorsal stream (CZ, CPZ, PZ, POZ, OZ), combine both stream and all channels. Figure 4A shows the model accuracy using the grouped data of every participant. For object recognition, the results indicate an accuracy rate of 64% when trained with the visual region, 89% with the temporal region, 95% with the ventral stream, 79% with the dorsal stream, 96% with the combined stream, and 99% when trained with all channels. Regarding object identification, the model reached an accuracy of 65% when trained using the visual region, 85% with the temporal region, 94% with the ventral stream, 82% with the dorsal stream, 96% with the combined stream, and 96% when trained with all channels. Subsequent to the group analysis, an examination of the EEGNet models at the individual participant level was conducted. The outcomes of this examination are shown in Figure 4B. For object recognition, the results demonstrate an average accuracy with standard deviation, as follows: 73.4 \u00b1 9.4% for models trained using the visual region, 80.7 \u00b1 7.4% with the temporal region, 93.2 \u00b1 5.8% with the ventral stream, 84.7 \u00b1 9.3% with the dorsal stream, 96.9 \u00b1 3% with the combined stream, and 99.6 \u00b1 0.2% when utilizing all channels. In terms of object identification, the model achieved an accuracy of 72.04 \u00b1 11.6% when trained with the visual region, 79.5 \u00b1 7% with the temporal region, 92.5 \u00b1 6.1% with the ventral stream, 88.3 \u00b1 5.6% with the dorsal stream, 96.6 \u00b1 2% with the combined stream, and 99.6 \u00b1 0.2% when all channels were utilized. A paired t-test was also implemented to indicate the statistically significant differences between the various EEGNet models.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\nFurthermore, the trained models were subjected to visualization through the Grad-CAM technique [36]. The resultant Grad-CAM findings were obtained by running the technique on the output generated by the temporal convolution layer. It should be noted that the ERP corresponding to tasks of object recognition and object identification occurs at varying temporal intervals, as illustrated in Figure 3. Consequently, we chose the time frames during which the brain\u2019s response exhibited maximum amplitude in both object recognition and object identification tasks. For the model focused on object recognition, data ranging from 0 ms to 500 ms was selected, whereas for the object identification model, the selected data spanned from 300 ms to 800 ms. Figure 5 presents the Grad-CAM visualizations for both models across all channels, targeting four distinct object categories. In the case of the object recognition model, the Grad-CAM visualizations indicate significant gradient scores primarily localized around the bilateral temporal and parietal regions for all categories, barring the flower category, where the significance is comparatively subdued. Conversely, the object identification model reveals pronounced gradient scores around the bilateral temporal and parietal regions for all categories, except for the vehicle category, where the significance is relatively reduced. Additionally, the results demonstrate heightened gradient scores in the frontal brain region across all categories, with the exception again being the vehicle category, where the importance is less notable."
        },
        {
            "heading": "IV. DISCUSSION",
            "text": "Object recognition and object identification play a pivotal role in numerous daily tasks. Both processes necessitate\nthe rapid and precise process of abundant dynamic visual information coupled with the swift retrieval of information from memory. This study delves into these cognitive processes by analyzing EEGNet models trained using diverse EEG channel configurations. The results indicate that models trained solely on visual channels underperformed relative to other configurations in both object recognition and object identification tasks. Although the visual region primarily processes visual inputs from the eyes, its interpretation focuses on basic features of the visual scene, such as edges, lines, and colours [37], [38]. Given that the visual channels lacked comprehensive information distinguishing between categories, the suboptimal performance of this model was anticipated. Conversely, the temporal lobe is instrumental in recognizing and identifying intricate visual stimuli, including objects [39]. Thus, channels located within the temporal region potentially contain data that aids the model in distinguishing between distinct categories. Our findings revealed a marked increase in accuracy when models were trained exclusively on channels from the temporal region for both object recognition and identification tasks compared to models trained on visual channels.\nUpon analyzing the ERP signals associated with object recognition and object identification used for model training, we identified several notable traits. As depicted in Figure 3, the average ERP within the visual channel for object recognition exhibited a pronounced double-peak potential approximately between 100 ms and 350 ms post-event onset. This double-peak potential can potentially be linked with the P2a and P2b responses related to object recognition as documented in prior research ([11], [12], [40]). A similar\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nresponse was identified in the average ERP within the visual channel during object identification. Nonetheless, the temporal scope of this double-peak potential was extended, ranging from approximately 250 ms to 650 ms post-event onset. Such a phenomenon might be attributable to the detection of multiple images, extending the response duration since participants were tasked with recognizing all displayed objects and pinpointing the target image among the object choices.\nBesides the visual channel, the temporal channel displayed a negative potential coinciding with the time frame of the double-peak potential. This is indicative of visual processing\npertinent to both object recognition and identification ([41], [42], [43]). Similar to the patterns observed in the visual channel, the temporal region also showed a protracted negative potential during the object identification task. In addition to ERP signals, scalp topography illustrated the cerebral responses during object recognition and identification tasks. Upon the onset of object-related ERP throughout the tasks, a synchronized activity was evident in the scalp topography, with the preponderance of the activity manifesting in the brain\u2019s occipital region. As the task progressed, activity was also discerned in the parietal and occipitotemporal regions. This observation accentuates the interrelation between ERP and the information related to objects.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nThe Grad-CAM findings, as delineated in Figure 5, reveal that multiple brain regions facilitated the training of the EEGNet model in distinguishing between four object categories. For both the object recognition and identification models, all categories exhibited pronounced gradient scores within the occipitotemporal region. This region encompasses channels from the ventral stream. Such a manifestation suggests that the model effectively assimilated pertinent information regarding the objects primarily through channels within the ventral stream, especially the temporal channels. This observation aligns with prior research indicating the significance of temporal channels in processing intricate visual stimuli [44], [45], [46]. Apart from the temporal channels, channels within the parietal region also displayed considerable contributions to the EEGNet model\u2019s training. The parietal brain region is widely recognized for its role in discerning an object\u2019s spatial attributes [47], [48], [49]. Given that objects were oriented in four distinct directions in our study, the spatial information is intrinsically vital for object identification. Hence, the pronounced significance of\nthe parietal region in the Grad-CAM results of the object identification model aligns with expectations. However, the emergence of parietal significance in the object recognition model was intriguing, particularly since the target object consistently appeared at the centre of the screen. This suggests that the parietal region\u2019s significance might be intricately linked to the spatial characteristics of the target object itself.\nIt is evidenced that the dorsal visual pathway plays an important role in supporting processes within the ventral pathway [50], [51]. However, the specifics of this interrelation remain relatively obscure. Ayzenberg et al. posited a hypothesis wherein the dorsal stream partakes in object recognition by processing spatial relations of the features of the object, subsequently constructing a global shape precept of the object [52]. This synthesized information is then relayed to the ventral pathway, bolstering object recognition processes. Moreover, research by Jeong & Xu [53] proposes that the dorsal stream recognizes an abstract representation of object identity, exhibiting a behaviorally pertinent role by closely tracking the perceived facial-identity similarity\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nobtained in behavioural tasks. This involvement of dorsal channels in object recognition might very well be echoed in our Grad-CAM results.\nThe exploration of the ventral and dorsal streams is by no means a novel undertaking, as many studies have endeavoured to discern the relationship between these streams and object-related tasks. These investigations span both anatomically [54], [55], [56] and computational approaches via deep learning algorithms [57], [58], [59]. Within the area of BCI research, there have been extensive endeavours to decode object-related information from the ventral stream using various EEG features such as ERP [60], power spectral density [45], EEG phase patterns [61], and independent components [62]. Our study, inspired by the two-stream hypothesis, trained models using channels reflecting the ventral and dorsal streams and subsequently combined both streams.\nModels emulating the ventral stream exhibited enhanced accuracy in both object recognition and object identification tasks when compared to models oriented around the visual and temporal regions. This improvement underscores the intrinsic value of information within the ventral stream, facilitating more efficient model learning. Conversely, models mimicking the dorsal stream didn\u2019t achieve the same efficacy as their ventral counterparts, although they outperformed the visual region models. As mentioned earlier, there\u2019s mounting evidence advocating the role of the dorsal stream in object recognition, suggesting that this stream encompasses visual data instrumental for the model\u2019s categorization capabilities. Given the growing body of literature on the symbiotic relationship between the ventral and dorsal streams, both anatomically and functionally [63], [64], we developed a model integrating channels from both streams. This composite model demonstrated marginally enhanced accuracy relative to the ventral stream model. Nevertheless, this slight enhancement is arguably attributable to the augmented channel data during model training rather than\nbeing a significant functional outcome."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this study, we embarked on a thorough examination of several EEGNet models using the data we collected for both object recognition and object identification tasks, each characterized by different channel configurations, to understand their effectiveness in object recognition and identification tasks. The findings reveal that the model trained utilizing the channels from the ventral stream outperforms those trained using regional channels. Notably, its efficacy is marginally surpassed by the model that was trained using all available channels. Furthermore, a modest enhancement in the model\u2019s performance was noted when channels from both the ventral and dorsal streams were combined. To delve into the intricacies of this observation, we used the Grad-CAM visualization technique on the trained model. The Grad-CAM result exposed a pronounced gradient score around the channels that form the ventral stream. Furthermore, a significant contribution from the parietal channels toward the EEGNet model\u2019s training was evident. This reinforces the prevailing understanding that the brain\u2019s dorsal stream is essential in tasks relating to object recognition and identification. Collectively, the results from our investigation underscore that the ventral and dorsal streams contain crucial information that can be harnessed for the efficient training of models on object recognition and identification tasks. This finding holds potential for the development of a rapid and precise BCI system designed for object recognition and identification."
        }
    ],
    "title": "Ventral and Dorsal Stream EEG Channels: Key Features for EEG-Based Object Recognition and Identification",
    "year": 2023
}