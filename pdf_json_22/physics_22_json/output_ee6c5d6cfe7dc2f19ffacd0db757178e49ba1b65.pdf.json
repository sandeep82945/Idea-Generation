{
    "abstractText": "Recently, a general data driven numerical framework has been developed for learning and modeling of unknown dynamical systems using fullyor partially-observed data. The method utilizes deep neural networks (DNNs) to construct a model for the flow map of the unknown system. Once an accurate DNN approximation of the flow map is constructed, it can be recursively executed to serve as an effective predictive model of the unknown system. In this paper, we apply this framework to chaotic systems, in particular the well-known Lorenz 63 and 96 systems, and critically examine the predictive performance of the approach. A distinct feature of chaotic systems is that even the smallest perturbations will lead to large (albeit bounded) deviations in the solution trajectories. This makes long-term predictions of the method, or any data driven methods, questionable, as the local model accuracy will eventually degrade and lead to large pointwise errors. Here we employ several other qualitative and quantitative measures to determine whether the chaotic dynamics have been learned. These include phase plots, histograms, autocorrelation, correlation dimension, approximate entropy, and Lyapunov exponent. Using these measures, we demonstrate that the flow map based DNN learning method is capable of accurately modeling chaotic systems, even when only a subset of the state variables are available to the DNNs. For example, for the Lorenz 96 system with 40 state variables, when data of only 3 variables are available, the method is able to learn an effective DNN model for the 3 variables and produce accurately the chaotic behavior of the system.",
    "authors": [],
    "id": "SP:9b62a764e75e271aee90f123de7e3e11503201b4",
    "references": [
        {
            "authors": [
                "M. Abadi",
                "A. Agarwal",
                "P. Barham",
                "E. Brevdo",
                "Z. Chen",
                "C. Citro",
                "G.S. Corrado",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "I. Goodfellow",
                "A. Harp",
                "G. Irving",
                "M. Isard",
                "Y. Jia",
                "R. Jozefowicz",
                "L. Kaiser",
                "M. Kudlur",
                "J. Levenberg",
                "D. Man\u00e9",
                "R. Monga",
                "S. Moore",
                "D. Murray",
                "C. Olah",
                "M. Schuster",
                "J. Shlens",
                "B. Steiner",
                "I. Sutskever",
                "K. Talwar",
                "P. Tucker",
                "V. Vanhoucke",
                "V. Vasudevan",
                "F. Vi\u00e9gas",
                "O. Vinyals",
                "P. Warden",
                "M. Wattenberg",
                "M. Wicke",
                "Y. Yu"
            ],
            "title": "and X",
            "venue": "Zheng, TensorFlow: Large-scale machine learning on heterogeneous systems",
            "year": 2015
        },
        {
            "authors": [
                "J. Bakarji",
                "K. Champion",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Discovering governing equations from partial measurements with deep delay autoencoders",
            "venue": "arXiv preprint arXiv:2201.05136, ",
            "year": 2022
        },
        {
            "authors": [
                "R. Bakker",
                "J.C. Schouten",
                "C.L. Giles",
                "F. Takens"
            ],
            "title": "and C",
            "venue": "M. Van den Bleek, Learning chaotic attractors by neural networks, Neural Computation, 12 ",
            "year": 2000
        },
        {
            "authors": [
                "U. Bhat",
                "S.B. Munch"
            ],
            "title": "Recurrent neural networks for partially observed dynamical systems",
            "venue": "Physical Review E, 105 ",
            "year": 2022
        },
        {
            "authors": [
                "G.E. Box",
                "G.M. Jenkins",
                "G.C. Reinsel",
                "G.M. Ljung"
            ],
            "title": "Time series analysis: forecasting and control",
            "venue": "John Wiley & Sons",
            "year": 2015
        },
        {
            "authors": [
                "S.L. Brunton",
                "B.W. Brunton",
                "J.L. Proctor",
                "E. Kaiser",
                "J.N. Kutz"
            ],
            "title": "Chaos as an intermittently forced linear system",
            "venue": "Nature Communications, 8 ",
            "year": 2017
        },
        {
            "authors": [
                "S.L. Brunton",
                "J.L. Proctor",
                "J.N. Kutz"
            ],
            "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "venue": "Proc. Natl. Acad. Sci. U.S.A., 113 ",
            "year": 2016
        },
        {
            "authors": [
                "K. Champion",
                "B. Lusch",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Data-driven discovery of coordinates and governing equations",
            "venue": "Proceedings of the National Academy of Sciences, 116 ",
            "year": 2019
        },
        {
            "authors": [
                "A. Chattopadhyay",
                "P. Hassanzadeh",
                "D. Subramanian"
            ],
            "title": "Data-driven predictions of a multiscale lorenz 96 chaotic system using machine-learning methods: reservoir computing",
            "venue": "artificial neural network, and long short-term memory network, Nonlinear Processes in Geophysics, 27 ",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "V. Churchill",
                "K. Wu",
                "D. Xiu"
            ],
            "title": "Deep neural network modeling of unknown partial differential equations in nodal space",
            "venue": "Journal of Computational Physics, 449 ",
            "year": 2022
        },
        {
            "authors": [
                "V. Churchill",
                "S. Manns",
                "Z. Chen",
                "D. Xiu"
            ],
            "title": "Robust modeling of unknown dynamical 16 systems via ensemble averaged learning",
            "venue": "arXiv preprint arXiv:2203.03458, ",
            "year": 2022
        },
        {
            "authors": [
                "P. Dubois",
                "T. Gomez",
                "L. Planckaert",
                "L. Perret"
            ],
            "title": "Data-driven predictions of the lorenz system",
            "venue": "Physica D: Nonlinear Phenomena, 408 ",
            "year": 2020
        },
        {
            "authors": [
                "X. Fu",
                "L.-B. Chang",
                "D. Xiu"
            ],
            "title": "Learning reduced systems via deep neural networks with memory",
            "venue": "J. Machine Learning Model. Comput., 1 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Han",
                "Z. Shi",
                "W. Wang"
            ],
            "title": "Modeling dynamic system by recurrent neural network with state variables",
            "venue": "International Symposium on Neural Networks, Springer",
            "year": 2004
        },
        {
            "authors": [
                "M. Han",
                "J. Xi",
                "S. Xu",
                "F.-L. Yin"
            ],
            "title": "Prediction of chaotic time series based on the recurrent predictor neural network",
            "venue": "IEEE transactions on signal processing, 52 ",
            "year": 2004
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "year": 2016
        },
        {
            "authors": [
                "S.H. Kang",
                "W. Liao",
                "Y. Liu"
            ],
            "title": "IDENT: Identifying differential equations with numerical time evolution",
            "venue": "arXiv preprint arXiv:1904.03538, ",
            "year": 2019
        },
        {
            "authors": [
                "H. Kim",
                "R. Eykholt",
                "J. Salas"
            ],
            "title": "Nonlinear dynamics",
            "venue": "delay times, and embedding windows, Physica D: Nonlinear Phenomena, 127 ",
            "year": 1999
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980, ",
            "year": 2014
        },
        {
            "authors": [
                "Z. Long",
                "Y. Lu"
            ],
            "title": "and B",
            "venue": "Dong, PDE-Net 2.0: Learning PDEs from data with a numericsymbolic hybrid deep network, arXiv preprint arXiv:1812.04426, ",
            "year": 2018
        },
        {
            "authors": [
                "Z. Long",
                "Y. Lu",
                "X. Ma",
                "B. Dong"
            ],
            "title": "PDE-net: Learning PDEs",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "E.N. Lorenz"
            ],
            "title": "Deterministic nonperiodic flow",
            "venue": "Journal of atmospheric sciences, 20 ",
            "year": 1963
        },
        {
            "authors": [
                "E.N. Lorenz"
            ],
            "title": "Predictability: A problem partly solved",
            "venue": "Proc. Seminar on predictability, vol. 1",
            "year": 1996
        },
        {
            "authors": [
                "E.N. Lorenz",
                "K.A. Emanuel"
            ],
            "title": "Optimal sites for supplementary weather observations: Simulation with a small model",
            "venue": "Journal of the Atmospheric Sciences, 55 ",
            "year": 1998
        },
        {
            "authors": [
                "L. Lu",
                "P. Jin",
                "G. Pang",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
            "venue": "Nature Machine Intelligence, 3 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Lu",
                "X. Meng",
                "Z. Mao",
                "G.E. Karniadakis"
            ],
            "title": "DeepXDE: A deep learning library for solving differential equations",
            "venue": "SIAM Review, 63 ",
            "year": 2021
        },
        {
            "authors": [
                "B. Lusch",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Deep learning for universal linear embeddings of nonlinear dynamics",
            "venue": "Nature communications, 9 ",
            "year": 2018
        },
        {
            "authors": [
                "T. Miyoshi",
                "H. Ichihashi",
                "S. Okamoto",
                "T. Hayakawa"
            ],
            "title": "Learning chaotic dynamics in recurrent rbf network",
            "venue": "Proceedings of ICNN\u201995-International Conference on Neural Networks, vol. 1, IEEE",
            "year": 1995
        },
        {
            "authors": [
                "H. Mori"
            ],
            "title": "Transport",
            "venue": "collective motion, and brownian motion, Progress of theoretical physics, 33 ",
            "year": 1965
        },
        {
            "authors": [
                "S. Pan",
                "K. Duraisamy"
            ],
            "title": "Data-driven discovery of closure models",
            "venue": "SIAM Journal on Applied Dynamical Systems, 17 ",
            "year": 2018
        },
        {
            "authors": [
                "S. Pawar",
                "O. San",
                "A. Rasheed",
                "I.M. Navon"
            ],
            "title": "A nonintrusive hybrid neural-physics modeling of incomplete dynamical systems: Lorenz equations",
            "venue": "GEM-International Journal on Geomathematics, 12 ",
            "year": 2021
        },
        {
            "authors": [
                "S.M. Pincus"
            ],
            "title": "Approximate entropy as a measure of system complexity",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 1991
        },
        {
            "authors": [
                "T. Qin",
                "Z. Chen",
                "J. Jakeman",
                "D. Xiu"
            ],
            "title": "Deep learning of parameterized equations with applications to uncertainty quantification",
            "venue": "Int. J. Uncertainty Quantification, ",
            "year": 2020
        },
        {
            "authors": [
                "T. Qin",
                "Z. Chen",
                "J. Jakeman",
                "D. Xiu"
            ],
            "title": "Data-driven learning of non-autonomous systems",
            "venue": "SIAM J. Sci. Comput., 43 ",
            "year": 2021
        },
        {
            "authors": [
                "T. Qin",
                "K. Wu",
                "D. Xiu"
            ],
            "title": "Data driven governing equations approximation using deep neural networks",
            "venue": "J. Comput. Phys., 395 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Raissi"
            ],
            "title": "Deep hidden physics models: Deep learning of nonlinear partial differential equations",
            "venue": "Journal of Machine Learning Research, 19 ",
            "year": 2018
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics informed deep learning (part 17 i): Data-driven solutions of nonlinear partial differential equations",
            "venue": "arXiv preprint arXiv:1711.10561, ",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations",
            "venue": "arXiv preprint arXiv:1711.10566, ",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Multistep neural networks for data-driven discovery of nonlinear dynamical systems",
            "venue": "arXiv preprint arXiv:1801.01236, ",
            "year": 2018
        },
        {
            "authors": [
                "M.T. Rosenstein",
                "J.J. Collins",
                "C.J. De Luca"
            ],
            "title": "A practical method for calculating largest lyapunov exponents from small data sets",
            "venue": "Physica D: Nonlinear Phenomena, 65 ",
            "year": 1993
        },
        {
            "authors": [
                "S.H. Rudy",
                "S.L. Brunton",
                "J.L. Proctor",
                "J.N. Kutz"
            ],
            "title": "Data-driven discovery of partial differential equations",
            "venue": "Science Advances, 3 ",
            "year": 2017
        },
        {
            "authors": [
                "S.H. Rudy",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Deep learning of dynamics and signal-noise decomposition with time-stepping constraints",
            "venue": "J. Comput. Phys., 396 ",
            "year": 2019
        },
        {
            "authors": [
                "S.H. Rudy",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Deep learning of dynamics and signal-noise decomposition with time-stepping constraints",
            "venue": "Journal of Computational Physics, 396 ",
            "year": 2019
        },
        {
            "authors": [
                "H. Schaeffer"
            ],
            "title": "Learning partial differential equations via data discovery and sparse optimization",
            "venue": "Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 473 ",
            "year": 2017
        },
        {
            "authors": [
                "H. Schaeffer",
                "S.G. McCalla"
            ],
            "title": "Sparse model selection via integral terms",
            "venue": "Phys. Rev. E, 96 ",
            "year": 2017
        },
        {
            "authors": [
                "H. Schaeffer",
                "G. Tran",
                "R. Ward"
            ],
            "title": "Extracting sparse high-dimensional dynamics from limited data",
            "venue": "SIAM Journal on Applied Mathematics, 78 ",
            "year": 2018
        },
        {
            "authors": [
                "S. Scher",
                "G. Messori"
            ],
            "title": "Generalization properties of feed-forward neural networks trained on lorenz systems",
            "venue": "Nonlinear processes in geophysics, 26 ",
            "year": 2019
        },
        {
            "authors": [
                "Y. Sun",
                "L. Zhang",
                "H. Schaeffer"
            ],
            "title": "NeuPDE: Neural network based ordinary and partial differential equations for modeling time-dependent data",
            "venue": "arXiv preprint arXiv:1908.03190, ",
            "year": 2019
        },
        {
            "authors": [
                "F. Takens"
            ],
            "title": "Detecting strange attractors in turbulence",
            "venue": "Dynamical systems and turbulence, Warwick 1980, Springer",
            "year": 1981
        },
        {
            "authors": [
                "I. The MathWorks"
            ],
            "title": "Econometrics Toolbox",
            "venue": "Natick, Massachusetts, United State",
            "year": 2022
        },
        {
            "authors": [
                "I. The MathWorks"
            ],
            "title": "Predictive Maintenance Toolbox",
            "venue": "Natick, Massachusetts, United State",
            "year": 2022
        },
        {
            "authors": [
                "J. Theiler"
            ],
            "title": "Efficient algorithm for estimating the correlation dimension from a set of discrete points",
            "venue": "Physical review A, 36 ",
            "year": 1987
        },
        {
            "authors": [
                "G. Tran",
                "R. Ward"
            ],
            "title": "Exact recovery of chaotic systems from highly corrupted data",
            "venue": "Multiscale Model. Simul., 15 ",
            "year": 2017
        },
        {
            "authors": [
                "G.M.A.P. Trischler"
            ],
            "title": "D\u2019Eleuterio, Synthesis of recurrent neural networks for dynamical system simulation",
            "venue": "Neural Networks,",
            "year": 2016
        },
        {
            "authors": [
                "P.R. Vlachas",
                "W. Byeon",
                "Z.Y. Wan",
                "T.P. Sapsis",
                "P. Koumoutsakos"
            ],
            "title": "Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks",
            "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474 ",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "N. Ripamonti",
                "J. Hesthaven"
            ],
            "title": "Recurrent neural network closure of parametric POD-Galerkin reduced-order models based on the Mori-Zwanzig formalism",
            "venue": "J. Comput. Phys., 410 ",
            "year": 2020
        },
        {
            "authors": [
                "K. Wu",
                "D. Xiu"
            ],
            "title": "Data-driven deep learning of partial differential equations in modal space",
            "venue": "J. Comput. Phys., 408 ",
            "year": 2020
        },
        {
            "authors": [
                "N. Wulkow",
                "P. Koltai",
                "V. Sunkara",
                "C. Sch\u00fctte"
            ],
            "title": "Data-driven modelling of nonlinear dynamics by barycentric coordinates and memory",
            "venue": "arXiv preprint arXiv:2112.06742, ",
            "year": 2021
        },
        {
            "authors": [
                "H. Zimmermann",
                "R. Neuneier"
            ],
            "title": "Modeling dynamical systems by recurrent neural networks",
            "venue": "WIT Transactions on Information and Communication Technologies, 25 ",
            "year": 2000
        },
        {
            "authors": [
                "R. Zwanzig"
            ],
            "title": "Nonlinear generalized langevin equations",
            "venue": "Journal of Statistical Physics, 9 ",
            "year": 1973
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 5.\n08 38\n4v 1\n[ cs\n.L G\n] 1\n2 M\nay 2\n02 2\nKey words. Deep neural networks, chaotic behavior, flow map\n1. Introduction. Due to recent advances in machine learning software and computing hardware combined with the availability of vast amounts of data, data-driven learning of unknown dynamical systems has been a very active research area in the past few years. One way to approach this problem is governing equation discovery, where a map is constructed from state variables to their time derivatives. Among other techniques, this can be achieved via sparse approximation, where under certain circumstances exact equation recovery is possible. See, for example, [7] and its many extensions in recovering both ODEs in [7, 17, 46, 47, 54] and PDEs in [42, 45]. Deep neural networks (DNNs) have also been used to construct this mapping. See, for example, ODE modeling in [26, 36, 40, 43], and PDE modeling in [20, 21, 25, 38, 39, 37, 49].\nAnother approach for learning unknown systems, our focus here, is flow map or evolution discovery, where a map is constructed between two system states separated by a short time to approximate the flow map of the system, [36]. Unlike governing equation discovery, approximating the flow map does not directly yield the specific terms in the underlying equations. Rather, if an accurate flow map is discovered, then an accurate predictive model can be defined for the evolution of the unknown system such that a new initial condition can be marched forward in time. This approach relies on a DNN, in particular residual network (ResNet) [16], to approximate the flow map. Since its introduction in [36] to model autonomous systems, a general framework has been developed for the flow map approximation of unknown systems from their trajectory data that extends to non-autonomous systems [35], parametric dynamical systems [34], partially observed dynamical systems [13], as well as partial differential equations [10, 58]. Of particular interest in this paper is [13], where a finite memory of the state variable time history is used to learn reduced systems where only\n\u2217Department of Mathematics, The Ohio State University, Columbus, OH 43210, USA. Emails: churchill.77@osu.edu, xiu.16@osu.edu Funding: This work was partially supported by AFOSR FA9550-22-1-0011.\nsome of the state variables are observed per the Mori-Zwanzig formulation.\nThe focus of this paper is to extend this flow map deep learning framework to chaotic systems and examine its performance for both fully and partially observed chaotic systems. Chaotic systems exhibit ultra sensitivity to perturbations of the system parameters and initial conditions. Hence, they represent a highly challenging case for any learning and modeling methods, particularly for long-term system behavior. Although well known chaotic systems, e.g., the Lorenz 63 system, have been adopted in the literature, they were mostly used as demonstrative examples of the proposed methods in the papers by using visual examination of phase plots. The nearly impossible task of matching the long term evolution of the true chaotic system is rarely addressed in the existing literature. With an exclusive focus on chaotic systems, the purpose of this paper is to systematically examine the long term predictive accuracy using a set of measures beyond phase plots. These include bounded pointwise error, histograms, autocorrelation functions, correlation dimension, approximate entropy, and Lyapunov exponent. Using these measures, we further establish that the flow map based deep learning method is capable of learning and modeling chaotic systems and producing accurate long term system predictions, for both fully- and partially-observed systems.\n1.1. Literature Review. The problem of learning fully- and partially-observed chaotic systems, especially the famous Lorenz 63 system, has been a popular topic particularly in the age of machine learning. Hence, we limit the scope of this review to papers particularly relevant to the subject of learning chaotic dynamics.\nThe recent paper [4] is most similar to this work in that the authors seek to learn partially-observed chaotic dynamics using memory. This paper considers observing just one variable while we consider many combinations of partially-observed variables, and focuses on optimizing the network parameters (e.g. number of neurons and memory length) of a particular recursive structure based on root mean squared error (RMSE), while we consider additional measures of chaotic dynamics and a general ResNet. There are also several other relevant papers including [55], which uses a recurrent neural network to predict chaotic systems including Lorenz 63. In [59], the authors discuss memory length and Takens\u2019 theorem [50] in the context of learning Lorenz 96. In addition, [48] considers approximating chaotic dynamical systems using limited data and external forcing. Also, [56, 32, 9, 12] use long short-term memory (LSTM) recurrent networks to study fully- and partially-observed Lorenz systems.\nThere are also several earlier papers, including [60] which proposes modeling dynamical systems via recurrent neural networks, [18] which deals with estimating time delays (memory length) in Lorenz and other systems, [3] that learns chaotic dynamics of reduced systems via neural networks, [29] which learns chaotic dynamics with neural networks, and [14, 15], which predict the chaotic Rossler system using a simple RNN. These papers are certainly relevant, but lack the extensive examples and systematic verification experiments that are now more easily achieved with today\u2019s computing systems.\nIt is also worth mentioning a large body of work that uses a different approach (governing equation discovery) to learning chaotic systems. In [7], the authors approximate the particular terms in chaotic systems (the right hand sides) through sparse optimization or network learning. Other work in this area includes [6, 27] which focus on learning delayed embeddings, [40] which uses physics-informed neural networks, [31], which explicitly learns the closure of the partially observed Lorenz 63 system via a NN, [8] which combines learning a coordinate transform from the delayed em-\nbedding coordinates with learning the dynamic coordinates of chaotic systems, [44] which learns the Lorenz system from noisy data, and [2] which learns the governing equations for Lorenz (or a Lorenz-like surrogate) from just one observation variable.\n1.2. Contributions. The chief contribution of this paper is a systematic and rigorous examination of learning the flow maps of fully- and partially-observed chaotic dynamical systems using an approachable and mathematically grounded DNN framework. In most papers dealing with this topic, pointwise error or a visual comparison of phase plots are used to assess the accuracy of network prediction. However, it is well-known that chaotic systems are extremely sensitive to perturbation, and therefore in the long run the model predictions will dramatically stray from the truth. This makes assessing the efficacy of the learned system, particularly for long-term, a challenging problem in and of itself. Hence, part of our contribution is the proposal of a more robust approach to the assessment of learning chaotic behavior not explored in the existing literature that includes standard techniques of error analysis such as pointwise error and phase plots as well as comparison of a variety of other measures that demonstrate accurate behavior including matching of histograms and autocorrelation, and statistics that quantify chaos such as correlation dimension, approximate entropy, and Lyapunov exponent. Finally, we contribute several ambitious numerical examples not explored in the literature. As a benchmark, we consider learning the well-known Lorenz systems with different combinations of observed variables. Of particular note is a 40-dimensional Lorenz 96 system with only 3 variables observed, where we demonstrate that the DNN method can learn the chaotic behavior in general despite training data being collected from a single long trajectory consisting of only 3 variables (out of 40).\n2. Flow Map Modeling of Unknown Chaotic Dynamical Systems. We are interested in constructing effective models for the evolution laws behind chaotic dynamical data. We follow the framework in [36, 13] for learning fully- and partiallyobserved dynamical systems via residual DNNs. The following review closely follows that of [11]. Throughout this paper our discussion will be on dynamical systems observed over discrete time instances with a constant time step \u2206t,\nt0 < t1 < \u00b7 \u00b7 \u00b7 , tn+1 \u2212 tn = \u2206t, \u2200n. (2.1)\nGenerality is not lost with the constant time step assumption, as variable time step can be treated as a separate entry to the DNN structure [35]. We will use a subscript to denote the time variable of a function, e.g., xn = x(tn).\n2.1. ResNet Modeling of Fully-Observed Systems. Consider an unknown autonomous system,\ndx dt = f(x), x \u2208 Rd, (2.2)\nwhere f : Rd \u2192 Rd is not known. Because it is autonomous, its flow map depends only on the time difference as opposed to the actual time, i.e., xn = \u03a6tn\u2212ts(xs). Thus, the solution having been marched forward one time step satisfies\nxn+1 = \u03a6\u2206t(xn) = xn + \u03a8\u2206t(xn), (2.3)\nwhere \u03a8\u2206t = \u03a6\u2206t \u2212 I, with I as the identity operator.\nWhen data for all of the state variables x over the time stencil (2.1) are available, they can be grouped into sequences\n{x(m)(0),x(m)(\u2206t), . . . ,x(m)(K\u2206t)}, m = 1, . . . ,M,\nwhere M is the total number of such data sequences and K + 1 is the length of each sequence (which is assumed to be a constant for notational convenience). This serves as the training data set. Inspired by basic numerical schemes for solving ODEs, one can model the unknown evolution operator using a residual network (ResNet) ([16]) in the form of\nyout = [I+N] (yin), (2.4)\nwhere N : Rd \u2192 Rd stands for the mapping operator of a standard feedforward fully connected neural network. The network is then trained by using the training data set and minimizing the recurrent mean squared loss function\n1\nM\nM \u2211\nm=1\nK \u2211\nk=1\n\u2225 \u2225 \u2225 x(m)(k\u2206t)\u2212 [I+N]k(x(m)(0)) \u2225 \u2225 \u2225\n2\n, (2.5)\nwhere [I+N]k indicates composition of the network function k times. Recurrent loss is used to increase the stability of the network approximation over long term prediction. The trained network thus accomplishes\nx(m)(k\u2206t) \u2248 [I+N]k(x(m)(0)), \u2200m = 1, . . . ,M, k = 1, . . . ,K.\nAfter the network is trained to a satisfactory accuracy level, it can then be used as a predictive model\nxn+1 = xn +N(xn), n = 0, 1, . . . , (2.6)\nstarting from any initial condition x(t0). This framework was proposed in [36] and was extended to parametric systems and time-dependent (non-autonomous) systems ([34, 35]).\n2.2. Memory-based ResNet Modeling of Partially-Observed Systems. A notable extension of the flow-map modeling in [36] is to partially-observed systems where some state variables are not observed at all. Let x = (z;w), where z \u2208 Rm and w \u2208 Rd\u2212m. Let z be the observables and w be the missing variables. That is, no information or data ofw are available. When data are available only for z, it is possible to derive a system of equations for z only via the Mori-Zwanzig (MZ) formulation ([30, 61]). However, the MZ formulation asserts that the reduced system for z requires a memory integral, whose kernel function, along with other terms in the formula, is unknown. By making a mild assumption that the memory is of finite (problemdependent) length, memory-based DNN structures were investigated in [57, 13]. While [57] utilized LSTM (long short-term memory) networks, [13] proposed a relatively simple DNN structure, in direct correspondence to the Mori-Zwanzig formulation, that takes the following mathematical form,\nzn+1 = zn +N(zn, zn\u22121, . . . , zn\u2212nM ), n \u2265 nM , (2.7)\nwhere nM \u2265 0 is the number of memory terms in the model. In this case, the DNN operator is N : Rm\u00d7(nM+1) \u2192 Rm, which corresponds to a ResNet with additional\ntime history inputs. The special case of nM = 0 corresponds to the standard ResNet model (2.6) for modeling fully-observed systems without missing variables (thus no need for memory).\nIn the case of m = 1, Takens\u2019 theorem [50] proves (non-constructively) the existence of a map from F : RnM+1 \u2192 R such that zn+1 = F (zn, . . . , zn\u2212nM ) provided nM \u2265 2d. This serves as inspiration that, given enough data, a universal approximator can find this map. However, in this paper we consider many values for m corresponding to different combinations of observed variables.\n3. Computational Framework. In the main task of this paper, we apply the flow map learning methods described in the previous section to chaotic systems. First, we review the setting including the DNN structure used as well as the specifics of the data generation and model training. Next, we discuss the qualitative and quantitative metrics to evaluate the flow map learning. We then present extensive numerical results for learning fully- and partially-observed Lorenz systems.\n3.1. DNN Structure. The structure of the DNNs used to achieve chaotic flow map learning is modeled by (2.7) ([13]). The network function N : Rm\u00d7(nM+1) \u2192 Rm maps the input time history of the observed variables to the output future time through a series of fully-connected (also known as dense) layers with ReLU activation. An illustration of this structure with memory length nM = 2 is shown in Figure 3.1. Note that this structure is general in that while it is built for partially-observed systems that require memory based on the Mori-Zwanzig formulation, setting the memory length nM = 0 returns a standard ResNet that is appropriate for learning fully-observed systems. Our extensive numerical experimentation and previous work with this framework indicate that particularly wide or deep networks are not typically necessary for learning with this structure, and hence most examples in this paper use 3 hidden layers with 20 neurons in each layer. The choice of memory length nM is in general problem-dependent and hinges on a number of factors including the time step and the relationship between the observed and missing variables, and is explored in detail in [13]. In the examples in this paper, typically nM = 10 time steps (equivalent to 0.1 seconds in time as discussed below).\n3.2. Data Generation and Model Training. For benchmarking purposes, in all examples the true chaotic systems we seek to approximate are known. However, these true models serve only two purposes: (1) to generate synthetic data with which to train the DNN flow map approximations; and (2) to generate reference solutions for comparison with DNN predictions in testing. Therefore, the knowledge of the true system does not in any way facilitate the DNN model approximation.\nData generation for both of these tasks is achieved by solving the true systems using a high-order numerical solver, and observing this reference solution at discrete time steps with \u2206t = 0.01 seconds. To generate the training data, a single initial condition x(0) generates a long trajectory from t = 0 to t = 10, 000 seconds (1, 000, 000 time steps). From this long trajectory, M sequences of length nM+K+1 are collected uniformly at random to form the training data set\n{x (m) n\u2212nM , . . . ,x (m) n\u22121,x (m) n ,x (m) n+1, . . . ,x (m) n+K}, m = 1, . . . ,M, (3.1)\nwhere nM is the memory length and K is the recurrent loss parameter. In our examples, typically M = 10, 000 and K = 10 time steps (equivalent to 0.1 seconds). For systems requiring memory, data is generated from the full true system for all state variables x, with only the data for the observed state variables z being kept and data for the missing variables w being discarded.\nOnce the training dataset has been generated, the learning task is achieved by training the DNN model (2.7) with the data set (3.1). In particular, the network hyperparameters (weights and biases) are trained by minimizing the recurrent loss function\n1\nM\nM \u2211\nm=1\nK \u2211\nk=1\n\u2225 \u2225 \u2225 x (m) n+k \u2212 [In +N] k(x(m)n , . . . ,x (m) n\u2212nM ) \u2225 \u2225 \u2225\n2\n, (3.2)\nwhere In(xn, . . . ,xn\u2212nM ) = xn, using the stochastic optimization method Adam [19]. In the examples below we typically train for 10, 000 epochs with batch size 50 and a constant learning rate of 10\u22123 in Tensorflow [1].\n3.3. DNN Prediction and Validation. After satisfactory network training, we obtain a predictive model (2.7) for the unknown system which can be marched forward in time from any new initial condition. To validate the network prediction, testing data is generated in the same manner as training data was above. In particular, a new initial condition generates a reference solution from t = 0 to t = 100 seconds (10, 000 time steps) using the true governing equations. For prediction, the DNN model is marched forward starting with the first nM+1 time steps of the test trajectory until t = 100 and is compared against the reference. Note that we march forward significantly longer than K\u2206t (the length of each training sequence which is typically t = 0.1) to examine the long-term system behavior.\nFor chaotic systems, it is practically impossible to approximate the flow map with a model that achieves low pointwise error in the long term due to the fact that even machine epsilon changes in the initial condition or other system parameters will drastically change the evolution of the system. However, these changes will not alter the physics of the system, i.e. the nature of the behavior of the system. Hence, in this study, we recognize the challenge of low long-term pointwise error and focus on learning the physics of the system to match the chaotic behavior. In particular, we look at a myriad of qualitative and quantitative metrics in order to demonstrate a strong match of physics including: pointwise error, phase plot, histogram, autocorrelation function,\ncorrelation dimension, approximate entropy, and Lyapunov dimension. These tools have been used to classify chaotic behavior in Lorenz and other chaotic systems, e.g. in [3, 9, 18, 44]. We briefly review each of the evaluation tools below. All metrics are computed using MATLAB [28].\n\u2022 Pointwise error: In the following examples, we look at the reference test trajectories versus the trajectories predicted by the network model. However, the predictions will quickly deviate from the reference trajectories since the dynamics we learn are in fact an approximation and at best some small perturbation away from the true dynamics.1 For a chaotic system, this small perturbation causes drastic changes, which stay bounded, in the system later in time. Hence, we can also look at the log absolute error between the trajectories, and check that this quantity remains bounded. Bounded pointwise error demonstrates stability of the predictive model. \u2022 Phase plot: We also qualitatively compare the phase plots of reference and predicted test trajectories when multiple state variables are observed. This can serve as a qualitative measure of the behavior of the system, e.g. if both systems exhibit two attractors centered at the same particular points. \u2022 Histogram: We compare the approximate densities of reference and predicted values for the trajectories as well, where a match in the distribution indicates similar behavior in the long term. The histograms are computed over all time steps starting with the initial condition. \u2022 The autocorrelation function ([5]) measures the correlation between the time series xt and its lagged counterpart xt+k, where k = 0, . . . ,K and xt is a stochastic process. The autocorrelation for lag k is defined as rk = ck/c0 where\nck = 1\nT\nT\u2212k \u2211\nt=1\n(xt \u2212 x\u0304)(xt+k \u2212 x\u0304) (3.3)\nand c0 is the sample variance of the time series, with x\u0304 the mean of the time series and T the length of the time series. In our implementation, the sample autocorrelation is computed by MATLAB\u2019s Econometrics Toolbox [51] using the default settings. See this link2 for further computational details. \u2022 The correlation dimension ([53]) is a measure of chaotic signal complexity in multidimensional phase space. Specifically, it is a measure of the dimensionality of the space occupied by a set of random points, whereby a higher correlation dimension represents a higher level of chaotic complexity in the system. It is computed by first generating a delayed reconstruction Y1:N with embedding dimension m (in our implementation m is set to be the dimension of the full system regardless of whether the system is partially- or fully-observed) and lag \u03c4 of reference or predicted trajectories assembled in a matrix X . The number of with-in range points, at point i, is calculated by\nNi(R) =\nN \u2211\ni=1,i6=k\n1(\u2016Yi \u2212 Yk\u2016\u221e < R) (3.4)\n1For that matter, even the reference trajectories themselves are just an approximation of the unknown true trajectories as they are generated with a high-order numerical solver rather than analytically solving the system.\n2https://www.mathworks.com/help/econ/autocorr.html\nwhere 1 is the indicator function, R is the radius of similarity, and N is the number of points used to compute R. The correlation dimension is the slope of C(R) vs. R, where the correlation integral is defined as\nC(R) = 2\nN(N \u2212 1)\nN \u2211\ni=1\nNi(R). (3.5)\nIn our implementation, the correlation dimension is computed by MATLAB\u2019s Predictive Maintenance Toolbox [52] using default settings (e.g. for N and R). See this link3 for further computational details. \u2022 The approximate entropy ([33]) is a measure used to quantify the amount of regularity and unpredictability of fluctuations over a nonlinear time series. It is computed as \u03a6m \u2212 \u03a6m+1, where\n\u03a6m = (N \u2212m+ 1) \u22121\nN\u2212m+1 \u2211\ni=1\nlog(Ni(R)), (3.6)\nwhere m, N , R, and Ni(R), are defined as above when discussing correlation dimension using the same delayed reconstruction. In our implementation, the approximate entropy is computed by MATLAB\u2019s Predictive Maintenance Toolbox [52] using default settings. See this link4, which contains a working example of the Lorenz 63 system, for further computational details. \u2022 The Lyapunov exponent ([41]) characterizes the rate of separation of infinitesimally close trajectories in phase space to distinguish different attractors, which can be useful in quantifying the level of chaos in a system. A positive Lyapunov exponent indicates divergence and chaos, with the magnitude indicating the rate of divergence. For some point i, the Lyapunov exponent is computed using the same delayed reconstruction Y1:N as the correlation dimension and approximate entropy as\n\u03bb(i) = 1\n(Kmax \u2212Kmin + 1)dt\nKmax \u2211\nK=Kmin\n1\nK ln\n\u2016Yi+K \u2212 Yi\u2217+K\u2016\n\u2016Yi \u2212 Yi\u2217 , (3.7)\nwhere Kmin and Kmax represent the expansion range, dt is the sample time (equal to \u2206t = 0.01 in our case), and i\u2217 is the nearest neighbor point to i satisfying a minimum separation. A single scalar for the Lyapunov exponent is then computed as the slope of a linear fit to the \u03bb(i) values. In our implementation, the Lyapunov exponent is computed by MATLAB\u2019s Predictive Maintenance Toolbox [52] using sampling frequency of 100 (corresponding to \u2206t = 0.01 and otherwise default settings. See this link5, which also contains a working example of the Lorenz 63 system, for computational details. We note that while the computational specifics of each of these evaluation tools are in fact tunable and indeed changing them would yield different values, the comparisons that follow are not confined to accuracy for these particular default implementation choices and we present them simply as an example of one configuration.\n3https://www.mathworks.com/help/predmaint/ref/correlationdimension.html 4https://www.mathworks.com/help/predmaint/ref/approximateentropy.html 5https://www.mathworks.com/help/predmaint/ref/lyapunovexponent.html\n4. Computational Results. In this section we provide numerical examples of DNN modeling of chaotic systems. We focus on two well-known systems: the 3- dimensional Lorenz 63 system and the 40-dimensional Lorenz 96 system. In each system, we start with the learning of fully-observed variables to demonstrate the effectiveness of ResNet learning. We then focus on partially-observed cases. For the 3-dimensional Lorenz -63 system, we examine the DNN learning using training data of (different combinations of) only 2 variables, as well as of only 1 variables. For the 40-dimensional Lorenz-96 system, we examine the DNN learning when only 3 variables are observed in the training data.\n4.1. Low-dimensional system: Lorenz 63. The Lorenz 63 system is a nonlinear deterministic chaotic 3-dimensional system\ndx dt = \u03c3(y \u2212 x), dy dt = x(\u03c1\u2212 z)\u2212 y, dz dt = xy \u2212 \u03b2z,\n(4.1)\nwhere \u03c3, \u03c1, and \u03b2 are parameters. It was proposed in [22] as a simplified model for atmospheric convection. When \u03c3 = 10, \u03c1 = 28, and \u03b2 = 8/3, the system exhibits chaotic behavior and is a widely studied case.\n4.1.1. Example 1: Full 3-dimensional system. In this example all three state variables x, y, and z are observed and stored in the training data set. In particular, a trajectory is generated by a high-order numerical solver and observed at every \u2206t = 0.01 starting from initial condition (x0, y0, z0) = (1, 1, 1) until T = 10, 000 seconds. From this long trajectory, 10, 000 data sequences of length 0.1 seconds (11 time steps) are taken as training data, which allows 10 steps to be used for recurrent loss. A standard ResNet with 3 hidden layers with 20 neurons each is used, and the mean squared recurrent loss function is minimized using Adam with a constant learning rate of 10\u22123 for 10, 000 epochs.\nPrediction is carried out to T = 100 seconds (10, 000 time steps) from a new initial condition (10, 10, 20). Figure 4.1 shows the trajectory prediction as well as the log absolute error. We see that despite how the prediction trajectories quickly deviate from the reference trajectories (as should be expected for a chaotic system), the pointwise error in all three variables remains bounded over the long term. In addition, Figures 4.2, 4.3, and 4.4, show qualitatively similar phase plots, histograms, and autocorrelation functions, with at worst the quantitative chaos statistics of 9% in relative error. See Table 4.1 for details.\n4.1.2. Example 2: Reduced 1- or 2-dimensional systems. In this example we consider the six possible combinations of partially-observed systems arising from Lorenz 63. Specifically, we consider observing two variables of only x and y, only x and z, and only y and z, as well as one variable of only x, only y, and only z. When variables are missing, the Mori-Zwanzig formulation informs us that memory is required to learn the reduced system dynamics. Hence, in all of the following experiments a trajectory of the full Lorenz system is generated by a high-order numerical solver and the observed variables are observed at every \u2206t = 0.01 starting from initial condition (1, 1, 1) until T = 10, 000 seconds. (The unobserved variables are discarded from the true system solutions.) From this long trajectory, 10, 000 data sequences of length 0.2 seconds (21 time steps) are taken from only the observed variable(s) as training data. This allows 0.1 seconds (10 steps) for memory and 0.1 seconds (10 steps) for recurrent loss. A standard ResNet with 3 hidden layers and 20 neurons each is used, and the mean squared recurrent loss function is minimized using Adam with a constant learning rate of 10\u22123 for 10, 000 epochs.\nPrediction for the observed variables is carried out to T = 100 seconds (10, 000 time steps) from a new initial condition (10, 10, 20), with the corresponding observables in different cases. The two-variable x and y system is shown in Figures 4.5, 4.6, 4.7, and 4.8. We see bounded pointwise error indicating stability, qualitatively similar phase plots indicating similar behavior, similar histogram indicating the appropriate density, and chaos statistics very close to the reference.\nThe one-variable z system is shown in Figures 4.9 and 4.10. Once again we see bounded pointwise error, similar histograms and autocorrelation functions, as well as very accurate chaos statistics. The correlation dimension, approximate entropy, and Lyapunov exponent values for all of the combinations of observed variables are reported in Table 4.2. The predicted statistics are at worst 17.5% off from the reference statistics in relative error, but are typically significantly lower, and we note that in the few cases where relative error exceeds 10% it is only in one of the three statistics with the other two being much more accurate. E.g., we see that the reduced system of x and z has a relative error in Lyapunov exponent of 17.5%, but matches correlation dimension and approximate entropy at relative errors of 0.1% and 3.0%.\n4.2. High-dimensional System: Lorenz 96. The Lorenz 96 system [23] is an N -dimensional dynamical system given by\ndxi dt = (xi+1 \u2212 xi\u22122)xi\u22121 + F (4.2)\nfor i = 1, . . . , N , for N \u2265 4 where x\u22121 = xN\u22121, x0 = xN , and xN+1 = x1 and F is a forcing parameter. If F = 8, the system exhibits chaotic behavior. In the examples below, we choose N = 40 as in [24].\n4.2.1. Example 3: Full 40-dimensional system. In this example all 40 state variables are observed. In particular, a trajectory is generated by a high-order numerical solver with \u2206t = 0.01 from initial condition (8.0081, 8, 8, . . . , 8) until T = 10, 000 seconds (1, 000, 000 time steps). From this long trajectory, 100, 000 chunks of length 0.1 seconds (11 time steps) are taken as training data. This allows 10 steps to be used for recurrent loss. A standard ResNet with 3 hidden layers with 200 neurons each is used, and the mean squared recurrent loss function is minimized using Adam with a constant learning rate of 10\u22123 for 2, 000 epochs.\nPrediction is carried out to T = 500 (50, 000 time steps) from new initial condition (8.01, 8, 8, . . . , 8). Figure 4.11 shows the prediction results for t = 450 through t = 500 with the ith row corresponding to the trajectory of the variable xi. We see the same type of qualitative behavior in the reference and prediction, with wave-like forms flowing through the 40 variables. Table 4.3 show the correlation dimension, approximate entropy, and Lyapunov exponent for this experiment. As in Ex. 2, we note that while the relative error for approximate entropy is very high at 46.3%, we consider that it may be an anomaly as the other two statistics are significantly more accurate at 3.1% and 3.5%.\n4.2.2. Example 4: Reduced 3-dimensional system. In this example we consider observing only x1, x2, and x3 from the 40-dimensional Lorenz 96 system. Again, Mori-Zwanzig informs us that memory is required to learn the reduced system dynamics. Hence, a trajectory of the full system is generated by a high-order numerical\nsolver with \u2206t = 0.01 from initial condition (8.0081, 8, 8, . . . , 8) until T = 10, 000 seconds. From this long trajectory, 10, 000 chunks of length 1.1 seconds (111 time steps) are taken from only the observed variables as training data. This allows 1 second (100 time steps) for memory and 0.1 seconds (10 time steps) for recurrent loss. A standard ResNet with 10 hidden layers and 20 neurons each is used, and the mean squared recurrent loss function is minimized using Adam with a constant learning rate of 10\u22123 for 10, 000 epochs.\nPrediction is carried out to T = 100 (10, 000 time steps) from new initial condition (8.01, 8, 8, . . . , 8). Figure 4.12 shows the individual trajectories between t = 80 and t = 100 seconds along with bounded pointwise error, which again indicates stability of the prediction. Figure 4.13 shows the phase plots and quantitative measures, while Figures 4.14 and 4.15 show the histogram and autocorrelation function comparisons. Due to the significantly more chaotic and complex nature of the Lorenz 96 system, it is now more difficult to pick out identifying characteristics in the trajectories and phase plots to compare. Nevertheless, the chaos statistics reported in Table 4.4 indicate a strong match with only one of three statistics exceeding 10% relative error.\n5. Conclusion. We have presented a systematic and rigorous examination of learning the flow maps of fully- and partially-observed chaotic systems using an approachable DNN framework. While in most papers that examine this topic, pointwise error or a visual comparison of phase plots are used to assess the accuracy of network prediction, here we use these tools as well as a variety of other measures to demonstrate accurate long-term chaotic behavior prediction. Our numerical examples show that this DNN framework is able to learn long-term chaotic behavior even when systems are severely under-observed and training data are collected from a single initial\ncondition."
        }
    ],
    "year": 2022
}