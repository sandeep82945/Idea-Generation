{
    "abstractText": "We propose an omnidirectional measurement method without blind spots by using a convex mirror, which in principle does not cause chromatic aberration, and by using vertical disparity by installing cameras at the top and bottom of the image. In recent years, there has been significant research in the fields of autonomous cars and robots. In these fields, three-dimensional measurements of the surrounding environment have become indispensable. Depth sensing with cameras is one of the most important sensors for recognizing the surrounding environment. Previous studies have attempted to measure a wide range of areas using fisheye and full spherical panoramic cameras. However, these approaches have limitations such as blind spots and the need for multiple cameras to measure all directions. Therefore, this paper describes a stereo camera system that uses a device capable of taking an omnidirectional image with a single shot, enabling omnidirectional measurement with only two cameras. This achievement was challenging to attain with conventional stereo cameras. The results of experiments confirmed an improvement in accuracy of up to 37.4% compared to previous studies. In addition, the system succeeded in generating depth image that can recognize distances in all directions in a single frame, demonstrating the possibility of omnidirectional measurement with two cameras.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuki Ozawa"
        },
        {
            "affiliations": [],
            "name": "Yue Bao"
        }
    ],
    "id": "SP:02cf73b428b15fa322f8fbd8489f1c08e954b34e",
    "references": [
        {
            "authors": [
                "D. Scharstein",
                "R. Szeliski"
            ],
            "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms",
            "venue": "Int. J. Comput. Vis. 2002,",
            "year": 2002
        },
        {
            "authors": [
                "H. Iida",
                "Y. Ji",
                "K. Umeda",
                "A. Ohashi",
                "D. Fukuda",
                "S. Kaneko",
                "J. Murayama",
                "Y. Uchida"
            ],
            "title": "High-accuracy Range Image Generation by Fusing Binocular and Motion Stereo Using Fisheye Stereo Camera",
            "venue": "In Proceedings of the 2020 IEEE/SICE International Symposium on System Integration (SII),",
            "year": 2020
        },
        {
            "authors": [
                "C. Won",
                "J. Ryu",
                "J. Lim"
            ],
            "title": "End-to-End Learning for Omnidirectional Stereo Matching With Uncertainty Prior",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2021
        },
        {
            "authors": [
                "M. Sch\u00f6nbein",
                "B. Kitt",
                "M. Lauer"
            ],
            "title": "Environmental Perception for Intelligent Vehicles Using Catadioptric Stereo Vision Systems",
            "venue": "In Proceedings of the ECMR, O\u0308rebro, Sweden,",
            "year": 2011
        },
        {
            "authors": [
                "M. Sch\u00f6nbein",
                "A. Geiger"
            ],
            "title": "Omnidirectional 3d reconstruction in augmented manhattan worlds",
            "venue": "In Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, Chicago, IL,",
            "year": 2014
        },
        {
            "authors": [
                "T. Aoki",
                "S. Mengcheng",
                "H. Watanabe"
            ],
            "title": "Position Estimation and Distance Measurement from Omnidirectional Cameras",
            "venue": "80th Inf. Process. Soc. Jpn",
            "year": 2018
        },
        {
            "authors": [
                "S. Tanaka",
                "Y. Inoue"
            ],
            "title": "Outdoor Human Detection with Stereo Omnidirectional Cameras",
            "venue": "J. Robot. Mechatron",
            "year": 2020
        },
        {
            "authors": [
                "S. Aghayari",
                "M. Saadatseresht",
                "M. Omidalizarandi",
                "I. Neumann"
            ],
            "title": "Geometric calibration of full spherical panoramic Ricoh-Theta camera",
            "venue": "In Proceedings of the ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
            "year": 2017
        },
        {
            "authors": [
                "K. Yamazawa",
                "Y. Yagi",
                "M. Yachida"
            ],
            "title": "Omnidirectional imaging with hyperboloidal projection",
            "venue": "In Proceedings of the 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\u201993),",
            "year": 1993
        },
        {
            "authors": [
                "H. Hirschmuller"
            ],
            "title": "Accurate and efficient stereo processing by semi-global matching and mutual information",
            "venue": "In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),",
            "year": 2005
        },
        {
            "authors": [
                "X. Cheng",
                "Y. Zhong",
                "M. Harandi",
                "Y. Dai",
                "X. Chang",
                "H. Li",
                "T. Drummond",
                "Z. Ge"
            ],
            "title": "Hierarchical neural architecture search for deep stereo matching",
            "venue": "Adv. Neural Inf. Process. Syst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "L. Lipson",
                "Z. Teed",
                "J. Deng"
            ],
            "title": "Raft-stereo: Multilevel recurrent field transforms for stereo matching",
            "venue": "In Proceedings of the 2021 International Conference on 3D Vision (3DV),",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "P. Wang",
                "P. Xiong",
                "T. Cai",
                "Z. Yan",
                "L. Yang",
                "J. Liu",
                "H. Fan",
                "S. Liu"
            ],
            "title": "Practical stereo matching via cascaded recurrent network with adaptive correlation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Citation: Ozawa, Y.; Kimura, S.; Zhu,\nY.; Kurihara, A.; Bao, Y. Research on\nOmnidirectional Stereo Measurement\nUsing Convex Mirrors and Vertical\nDisparity. Sensors 2023, 23, 3243.\nhttps://doi.org/10.3390/s23063243\nAcademic Editor: Adrian Barbu\nReceived: 21 February 2023\nRevised: 10 March 2023\nAccepted: 16 March 2023\nPublished: 19 March 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: distance measurement; depth sensing; stereo camera; omnidirectional image; omnidirectional measurement"
        },
        {
            "heading": "1. Introduction",
            "text": ""
        },
        {
            "heading": "1.1. Backgrounds",
            "text": "In recent years, there has been a great deal of research and development on driver assistance systems for automobiles, fully automated driving, and unmanned search robots for disaster areas. As representative examples of autonomous driving technology, we will introduce the unmanned vehicle \u201cR2\u201d [1], the serving robot \u201cBellaBot\u201d [2], and the electric vehicle \u201ce-palette\u201d [3]. The \u201cR2\u201d [1] is an unmanned vehicle developed by Nuro, an American self-driving technology company. They launched a service to transport medical equipment, medicines, food, and beverages to assist medical personnel responding to a new type of coronavirus infection (COVID-19). The BellaBot [2] is a cat-shaped meal delivery robot invented by Pudu. It can provide a stable, non-contact food delivery service and has already been introduced in many restaurants in Japan. The \u201ce-Palette\u201d [3] is an electric vehicle developed by Toyota Motor Corporation exclusively for Autono-MaaS (autonomous mobility as a service). As a Worldwide Mobility Partner of the Olympic and Paralympic Games, Toyota provided more than a dozen ePalettes (Tokyo 2020 specifications), the first Toyota electric vehicles dedicated for AutonoMaaS, to support the transportation of athletes and Games officials as buses that traveled\nSensors 2023, 23, 3243. https://doi.org/10.3390/s23063243 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 3243 2 of 14\naround the athlete village. The e-Palettes were used as buses to transport athletes and Games officials around the village. In these fields, forward distance measurement and three-dimensional measurement of the surrounding environment are indispensable. There are various types of sensors, but depth sensing using cameras is the most widely used and one of the most important sensors for recognizing the surrounding environment. Depth sensing with cameras has been studied for a long time and is widely used because of its high measurement density, wide measurement range, and diversity of information obtained."
        },
        {
            "heading": "1.2. Existing Technology",
            "text": "Depth sensing using cameras is often based on a technique called stereo matching, which uses two cameras to calculate the disparity between two corresponding pixel regions from a pair of images to obtain the distance [4]. There have been many studies on distance measurement by stereo matching, especially for automatic guidance. In such cases, a wider range of measurements is important. In the study of Hirotaka Iida et al. [5], a fisheye lens was used as a stereo camera for distance measurement, to enable a wider range of measurements that are impossible with ordinary cameras. However, the angle of view of a fisheye lens is generally about 180\u25e6, which restricts the measurement range. In the study by Changhee Won et al. [6], the team addressed the limitation of Iida et al. [5] by utilizing four fisheye cameras and measuring the distance in all directions. Although this method allows for stable measurement in all directions, it requires double the number of cameras compared to the method of Iida et al. [5] Sch\u00f6nbein, Miriam et al. proposed a method [7,8] in which omnidirectional visual sensors are installed in front of the vehicle roof and on the left and right sides. In this method, two cameras are used for omnidirectional measurement, but the accuracy of the measurement is not sufficient because one of the sensors is reflected in the image, and the large difference between the images makes matching from the image development difficult. Aoki et al. [9] used a stereo omnidirectional camera consisting of two omnidirectional cameras to estimate depth. In estimating depth, they extract feature points from the least distorted portion of the fisheye image captured by the omnidirectional camera. However, their method has the following problems: depth is estimated only from the least distorted part of the image, and it is difficult to estimate the depth of the entire circumference at the same time due to the distortion of the fisheye lens. Moreover, extracting feature points is difficult in their method. In the study by Tanaka Shunya et al. [10], a commercially available all-sky camera [11] was utilized to perform object detection through machine learning and azimuth angle calculation to achieve omnidirectional measurement using two cameras. However, this method faced a drawback as it resulted in a blind spot in the camera\u2019s extended baseline (Figure 1) that could not be detected, so it was not a perfect omnidirectional measurement. Despite capturing an omnidirectional image by the back-to-back method, the fisheye camera lens faced the issue of chromatic aberration. To eliminate this chromatic aberration, multiple lenses must be stacked. Since it is impossible to mount multiple lenses on a small all-sky camera, it is impossible to eliminate chromatic aberration. Therefore, we considered that other omnidirectional imaging methods should be used to achieve more accurate measurements. This research uses a method with no blind spots by using an omnidirectional camera with a convex mirror, which in principle does not cause chromatic aberration. The mirror used is a hyperbolic mirror. This technique was adopted because it is the most superior in terms of visibility in measurement among various omnidirectional imaging techniques and matches the conditions of our method. The measurement using hyperbolic mirrors is a new method using vertical disparity by installing cameras at the top and bottom of the image. This method solves the problem of conventional methods, where a complete omnidirectional measurement is not possible because the other camera is also included in\nSensors 2023, 23, 3243 3 of 14\nthe image. The purpose of this research is to detect obstacles and to create a sensor that can be used for obstacle detection with automatic guidance, similar to the conventional stereo method. To demonstrate the usefulness of this method, we conducted a measurement experiment to verify the measurement accuracy and a depth image generation experiment to verify whether measurement in all directions is possible. In the measurement experiment, the results were compared with those of the conventional method to verify the accuracy. The conventional method compared was Tanaka\u2019s method [10], which was selected as the one with the best measurement accuracy among the conventional studies that have performed measurements under similar conditions. While the conventional method had a relative error of up to 40%, the new method succeeded in reducing the error to a maximum of 2.6%. In addition, when comparing the relative error for each distance, the accuracy was better than that of the conventional method at all points. Sensors 2022, 22, x FOR PEER REVIEW 3 of 15"
        },
        {
            "heading": "2. Materials and Methods",
            "text": ""
        },
        {
            "heading": "2.1. Proposed System Configuration",
            "text": "Figure 2. Conceptual diagram of the proposed system.\n2.2. Omnidirectional Visual Sensor\nThe omnidirectional visual sensor is described below. As shown in Figure 2, the\nomnidirectional visual sensor consists of a convex mirror and a camera facing vertically\nupward and can capture all directions in a single shot. Three types of possible convex\nmirrors were considered: conical, spherical, and hyperbolic. As shown in Figure 3, the\nmethod using a conical mirror provided high lateral resolution, but reflected light rays\nfrom below did not enter the lens, making it difficult to capture the feet.\nFigure 2. Conceptual diagram of the proposed system.\nSensors 2023, 23, 3243 4 of 14"
        },
        {
            "heading": "2.2. Omnidirectional Visual Sensor",
            "text": "The omnidirectional visual sensor is described below. As shown in Figure 2, the omnidirectional visual sensor consists of a convex mirror and a camera facing vertically upward and can capture all directions in a single shot. Three types of possible convex mirrors were considered: conical, spherical, and hyperbolic. As shown in Figure 3, the method using a conical mirror provided high lateral resolution, but reflected light rays from below did not enter the lens, making it difficult to capture the feet.\nSensors 2022, 22, x FOR PEER REVIEW 4 of 15"
        },
        {
            "heading": "2.2. Omnidirectional Visual Sensor",
            "text": "omnidirectional visual sensor consists of a convex mirror and a camera facing vertically\nupward and can capture all directions in a single shot. Three types of possible convex\nmirrors were considered: conical, spherical, and hyperbolic. As shown in Figure 3, the\nmethod using a conical mirror provided high lateral resolution, but reflected light rays\nfrom below did not enter the lens, making it difficult to capture the feet.\nFigure 3. Conical mirror.\nAs shown in Figure 4, in the method using a spherical mirror, the closer one gets to\nthe outer edge of the mirror, the larger the area to be imaged concerning the area to be\nprojected, resulting in good resolution of the feet, but the camera itself contains a large\nAs shown in Figure 4, in the method using a spherical mirror, the closer one gets to the outer edge of the mirror, the larger the area to be imaged concerning the area to be projected, resulting in good resolution of the feet, but the camera itself contains a large area, and objects in the distance or on the side are not well captured as if compressed. Sensors 2022, 22, x FOR PEER REVIEW 5 of 15\nFigure 4. Spherical mirror.\nIn contrast, hyperbolic surfaces offer a distinct advantage, as shown in Figure 5, as\nthe upper field of view has the same high resolution as the method using a conical mirror\nwhile avoiding the limitations of the lower field of view found in spherical mirrors.\nTherefore, the hyperbolic surface has the advantage in the field of view of both methods\nusing conical and spherical mirrors in that it is side-centered and also provides a foot\nview. Therefore, hyperbolic mirrors are used in the present system.\nIn contrast, hyperbolic surfaces offer a distinct advantage, as shown in Figure 5, as the upper field of view has the same high resolution as the method using a conical mirror while avoiding the limitations of the lower field of view found in spherical mirrors. Therefore, the hyperbolic surface has the advantage in the field of view of both methods using conical and spherical mirrors in that it is side-centered and also provides a foot view. Therefore, hyperbolic mirrors are used in the present system.\nSensors 2023, 23, 3243 5 of 14\nSensors 2022, 22, x FOR PEER REVIEW 5 of 15 Figure 4. Spherical mirror.\nIn contrast, hyperbolic surfaces offer a distinct advantage, as shown in Figure 5, as\nthe upper field of view has the same high resolution as the method using a conical mirror\nwhile avoiding the limitations of the lower field of view found in spherical mirrors.\nTherefore, the hyperbolic surface has the advantage in the field of view of both methods\nusing conical and spherical mirrors in that it is side-centered and also provides a foot\nview. Therefore, hyperbolic mirrors are used in the present system.\nFigure 5. Conceptual diagram of the omnidirectional visual sensor."
        },
        {
            "heading": "2.3. Panoramic Expansion",
            "text": "Since a hyperbolic mirror is used, distortion occurs during imaging. In addition, since\nwe want to use vertical epipolar lines for measurement, this method performs panoramic\nexpansion on an omnidirectional image. First, the optics of the hyperbolic mirror is\ndescribed. A 2-leaf hyperbolic surface is used for the hyperbolic surface in this method."
        },
        {
            "heading": "As shown in Figure 6, a 2-leaf hyperbolic surface is a surface obtained by rotating a hyperbola around (Z-axis). The characteristic of the hyperbola, which has two foci (\ud835\udc50 =",
            "text": "\u221a\ud835\udc4e2 + \ud835\udc4f2), (0, 0, +\ud835\udc50) and (0, 0, \u2212\ud835\udc50), is also retained in the hyperbolic surface. Moreover,\nas shown in Figure 5, consider 3-dimensional coordinates O-XYZ with the Z-axis as the\nvertical axis. In this case, the 2-leaf hyperbolic surface can be expressed by the following\nequation.\n\ud835\udc4b2 + \ud835\udc4c2\n\ud835\udc4e2 \u2212\n\ud835\udc4d2 \ud835\udc4f2 = \u22121 (1)\nNote that \ud835\udc4e and \ud835\udc4f are constants that define the shape of the hyperbola. In this\nmethod, the hyperbolic surface in the region of \ud835\udc4d > 0 among the two leaves is used as a\nmirror."
        },
        {
            "heading": "2.3. Panoramic Expansion",
            "text": "Si ce a hyperbolic mirror is used, distortion occurs during imaging. In addition, since we want to use vertical epipolar lines for measurement, this method performs panoramic expansion on an omnidirectional image. First, the optics of the hyperbolic mirror is described. A 2-leaf hyperbolic surface is used for the hyperbolic surface in this method. As shown in Figure 6, a 2-leaf hyperbolic surface is a surface obtained by rotating a hyperbola around (Z-axis). The characteristic of the hyperbola, which has two foci ( c = \u221a a2 + b2 )\n, (0, 0,+c) and (0, 0,\u2212c), is also retained in the hyperbolic surface. Moreover, as shown in Figure 5, consider 3-dimensional coordinates O-XYZ with the Z-axis as the vertical axis. In this case, the 2-leaf hyperbolic surface can be expressed by the following equation.\nX2 + Y2\na2 \u2212 Z\n2\nb2 = \u22121 (1)\nSensors 2022, 22, x FOR PEER REVIEW 6 of 15\nFigure 6. Bilobed hyperbolic surface.\nNext, the method of developing an omnidirectional image used in this method into\na panoramic image is explained using an actual photograph as an example. Figure 7 is an\nomnidirectional image taken.\nF gur 6. Bilobed hyperbolic surface.\nNote that a and b are constants that define the shape of the hyperbola. In this method, the hyperbolic surface in the region of Z > 0 among the two leaves is used as a mirror. Next, the method of developing an omnidirectional image used in this method into a panoramic image is explained using an actual photograph as an example. Figure 7 is an omnidirectional image taken.\nSensors 2023, 23, 3243 6 of 14\nSensors 2022, 22, x FOR PEER REVIEW 6 of 15\nFigure 6. Bilobed hyperbolic surface.\nNext, the method of developing an omnidirectional image used in this method into a panoramic image is explained using an actual photograph as an example. Figure 7 is an omnidirectional image taken.\nFigure 7. Omnidirectional image.\nFirst, to extract only the area to be used for measurement, an omnidirectional image taken is cropped in the area surrounded by red as shown in Figure 8a,b. The area to be cropped is determined according to the shooting range of the camera used for the measurement.\nFigure 7. Omnidirectional image.\nFirst, to extract only the area to be used for measurement, an omnidirectional image taken is cropped in the area surrounded by red as shown in Figure 8a,b. The area to be cropped is determined according to the shooting range of the camera used for the measurement. Sensors 2022, 22, x FOR PEER REVIEW 7 of 15\nFig . rop image: (a) before cropping; (b) after cropping.\nSensors 2023, 23, 3243 7 of 14\nNext, a perspective projection transformation is performed on each quadratically divided image using the following equation. The panoramic expansion of an omnidirectional image using a hyperbolic mirror can be expanded using the following equations [12].\nx = \u2212 a 2 f X\n(b2+c2)Z\u22122bc \u221a X2+Y2+Z2 + xc (2)\ny = \u2212 a 2 f Y\n(b2+c2)Z\u22122bc \u221a X2+Y2+Z2 + yc (3)\nwhere X, Y, Z are points in three-dimensional coordinates; x, y are points in the image coordinate system; xc, yc are the image center coordinates; a, b, c are the mirror parameters of the hyperbolic surface; and f is the focal length. Since the mirror parameters and focal length are known, expansion is possible. The results of the expansion of the quadratically divided image are shown in Figure 10a,b. Sensors 2022, 22, x FOR PEER REVIEW 8 of 15\n(a) (b)\nFigure 10. Cropping image: (a) image before expansion; (b) image after expansion.\nFinally, as shown in Figure 11, the panoramic image is completed by connecting the\nfour developed images.\nFigure 11. Panoramic image.\n2.4. Vertical Disparity Stereo Matching\nThe following is an explanation regarding the principle of stereo matching using\nvertical disparity. First, a diagram of Figure 2 considering a certain vertical cut plane is\nshown in Figure 12.\nFigure 12. Conceptual diagram of a certain longitudinal cut plane.\nThe focal distance f is between the upper and lower virtual panoramic cameras, the\ndistance (baseline) \ud835\udc4f is between the virtual panoramic cameras, the object to be measured\nFigure 10. Cropping image: (a) image before expansion; (b) image after expansion.\nFinally, as shown in Figure 11, the pa oramic image is completed by connecting the four developed images.\nSensors 2022, 22, x FOR PEER REVIEW 8 of 15\n(a) (b)\nFigure 10. Cropping image: (a) image before expansion; (b) image after expansion.\nFinally, as shown i Figure 11, the panoramic image is completed by connecting the\nfour developed images.\nFigure 11. Panoramic image.\n2.4. Vertical Disparity Stereo Matching\nThe following is an explanation regarding the principle of stereo matching using\nvertical disparity. First, a diagram of Figure 2 considering a certain vertical cut plane is\nshown in Figure 12.\nFigure 12. Conceptual diagram of a certain longitudinal cut plane.\nThe focal distance f is between the upper and lower virtual panoramic cameras, the\ndistance (baseline) \ud835\udc4f is between the virtual panoramic cameras, the object to be measured\nFigure 11. Panoramic image."
        },
        {
            "heading": "2.4. Vertical Disparity Stereo Matching",
            "text": "The following is an explanation regarding the principle of stereo matching using vertical disparity. First, a diagram of Figure 2 considering a c rtain vertical cut plane is shown in Figure 12. The focal distance f is between the upper and lower virtual panoramic cameras, the distance (baseline) b is between the virtual panoramic cameras, the object to be measured is at (Y, Z), the centers of the left and right panoramic images are O1 and O2, respectively, and the deviations from these positions are expressed as u1 and u2, respectively. From the similarity condition of the triangles, using the ratio\nb : u2\u2212 u1 = Y : f (4)\nSensors 2023, 23, 3243 8 of 14\nand solving for Y, we obtain\nY = b f\nu2\u2212 u1 (5)\nThis allows us to find the distance Y to the object.\nSensors 2022, 22, x FOR PEER REVIEW 8 of 15 (a) (b) Figure 10. Cropping image: (a) image before expansion; (b) image after expansion. Finally, as shown in Figure 11, the panoramic image is completed by connecting the four developed images.\nFigure 11. Panoramic image.\n2.4. Vertical Disparity Stereo Matching\nThe following is an explanation regarding the principle of stereo matching using\nvertical disparity. First, a diagram of Figure 2 considering a certain vertical cut plane is\nshown in Figure 12.\nFigure 12. Conceptual diagram of a certain longitudinal cut plane.\nThe focal distance f is between the upper and lower virtual panoramic cameras, the\ndistance (baseline) \ud835\udc4f is between the virtual panoramic cameras, the object to be measured\nFigure 12. Conceptual diagram of a certain longitudinal cut plane."
        },
        {
            "heading": "3. Results",
            "text": ""
        },
        {
            "heading": "3.1. Purpose of the Experiment",
            "text": "Measurement experiments were conducted to verify the accuracy of this method. In addition, to verify the possibility of measuring in all directions, a depth image generation experiment of a panoramic image was conducted.\n3.2. Measurement Experiment 3.2.1. Experimental Method\nIn this experiment, the same experiment as in the previous study [10] was conducted for comparison with the previous study [10]. Target persons were photographed at 1.0 m intervals in the distance range of 1.0 m to 5.0 m from the omnidirectional stereo camera. Distance measurements were taken 20 times, and the average distance between the camera and the object was used as the experimental result. Relative error was used to evaluate accuracy; relative error is calculated by the following equation.\nRE = |AV \u2212MV|\nAV \u00d7 100 (6)\nRE: relative error AV: actual value MV: measured value To confirm the measurement accuracy, we took measurements at five different points\n(1.00 m, 2.00 m, 3.00 m, 4.00 m, and 5.00 m) with 0.00 m directly underneath the device, and recorded the results. The measurement range was determined based on the maximum speed of Toyota Motor Corporation\u2019s fully automated vehicle, the e-Palette [3], as well as the stopping distance. The stopping distance was calculated as the sum of the empty run distance due to program processing time and the e-Palette\u2019s braking distance.\nSensors 2023, 23, 3243 9 of 14\nThe empty run distance is\nEmpty run distance = reaction time [s]\u00d7 car speed [m/s] (7)\nand the braking distance is determined by\nBreaking distance = (car speed [km/h])2 \u00f7 (254\u00d7 coe f f icient o f f riction) (8)\nIn this case, a coefficient of friction of 0.5 is assumed to account for bad weather conditions. Since the maximum speed of the e-Palette is 5.28 (m/s) and the processing time of the\ncurrent program is approximately 0.033 (s), the empty run distance is\nEmpty run distance = 5.28\u00d7 0.033 = 0.17 [m] (9)\nSince the maximum speed of the e-Palette is 19 (km/h) and the coefficient of friction of the road in rain is 0.5, the braking distance is\nBreaking distance = 192 \u00f7 (254\u00d7 0.5) = 2.84 [m] (10)\nTherefore, the total stopping distance including the empty run distance and the braking distance is approximately 3.01 (m). In addition, considering the size of the vehicle itself, the measurement accuracy of the distance up to 5 m is recorded in this case.\nDetails of the equipment used in the experiment are shown in Table 1.\nand 5.00 m from the camera. The object of measurement was the cardboard shown in the Figure 14. The correct distance was measured with a tape measure from directly under the camera.\nSensors 2023, 23, 3243 10 of 14\nSensors 2022, 22, x FOR PEER REVIEW 10 of 15 \ud835\udc35\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc58\ud835\udc56\ud835\udc5b\ud835\udc54 \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 = 192 \u00f7 (254 \u00d7 0.5) = 2.84 [m] (10) Therefore, the total stopping distance including the empty run distance and the braking distance is approximately 3.01 (m). In addition, considering the size of the vehicle itself, the measurement accuracy of the distance up to 5 m is recorded in this case. Details of the equipment used in the experiment are shown in Table 1. Table 1. Details of the equipment used in the experiment. Name of Equipment Specification Camera Name of maker The Imaging Source (New Taipei City, Taiwan) Name of product DFK 33 UX 183 Name of sensor Sony CMOS Exmor IMX 183 CQ Resolution 5472 \u00d7 3648 Lens Name of maker Shodensha Co., Ltd. (Osaka, Japan) Name of product SM 1226\u2013MP 20 Focal length 12 mm Camera aperture range F 2.6\u2013F 16 Convex mirror Name of maker Vstone Co., Ltd. (Osaka, Japan) Name of product VS\u2013C 450 MR Mirror parameter \ud835\udc4e 29 mm\nMirror parameter \ud835\udc4f 40 mm\nMirror parameter \ud835\udc50 49.4 mm\nDiameter of mirror 45 mm\nThe distance between cameras (baseline) b was set to 24.6 cm.\nThe experimental environment is shown in Figure 13.\nFigure 13. The experimental environment.\nMeasurements were taken at five points at distances of 1.00 m, 2.00 m, 3.00 m, 4.00\nm, and 5.00 m from the camera. The object of measurement was the cardboard shown in\nthe camera.\nFigure 14. The object of measurement cardboard.\n3.2.2. Experimental Results\nThe proposed method\u2019s results are graphically depicted in Figure 15. Meanwhile, Table 2 summarizes the outcomes of the proposed method and the conventional method.\n6\nFigure 14. The object of measurement cardboard.\n3.2.2. Experimental Results\nThe proposed method\u2019s results are graphically depicted in Figure 15. Meanwhile, Table 2 summarizes the outcomes of the proposed method and the conventional method.\nSensors 2023, 23, 3243 11 of 14\nSensors 2022, 22, x FOR PEER REVIEW 11 of 15 the Figure 14. The correct distance was measured with a tape measure from directly under the camera.\nFigure 14. The object of measurement cardboard.\n3.2.2. Experimental Results\nThe proposed method\u2019s results are graphically depicted in Figure 15. Meanwhile,\nTable 2 summarizes the outcomes of the proposed method and the conventional method.\n0\n1\n2\n3\n4\n5\n6\n0 5 10 15 20 25\nD is\nta n\nce [\nm ]\nNumber of trials [times]\n1.00m 2.00m 3.00m 4.00m 5.00m\nFigure 15. graphical representation of the results.\nTo validate the accuracy, the data were compared with data from a previous study. While the conventional method had a relative error of up to 40%, the new method succeeded in reducing the error to a maximum of 2.6%. In addition, the relative error for each distance was compared, and the accuracy was higher at all locations. However, as shown in Figure 15, the stability of the measurements decreases as the distance from the camera increases. This is due to the characteristics of stereo measurement, where the accuracy is determined by the actual length of one pixel in the image, which is influenced by the lens, camera resolution, shooting distance, and baseline distance between cameras. In addition, since the stereo correspondence point pixel in the camera image plate has a size, the point to be measured can only be identified within a certain range in the actual space, which results in measurement errors. Typically, errors are larger in the depth direction from the camera. While lengthening the baseline can reduce the depth error, it can result in larger errors for shorter distances or even make measurements impossible. Therefore, stereo image measurement systems require appropriate baseline settings for the specific conditions in which they will be used. Thus, this issue can be resolved by adjusting the baseline according to the device being mounted.\nThese results demonstrate the effectiveness of this study in terms of accuracy.\n3.3. Depth Image Generation Experiment 3.3.1. Experimental Method\nSince the accuracy was verified in Section 3.2, this experiment verifies whether omnidirectional measurement is possible. In order to confirm whether this method is capable of omnidirectional measurement, an experiment was conducted to generate a depth image of a panoramic image. The method used for depth image generation is called the SGM method [13].\n3.3.2. Experimental Results\nFigure 16 shows the original image and Figure 17 shows the depth image created.\nSensors 2023, 23, 3243 12 of 14\nSensors 2022, 22, x FOR PEER REVIEW 12 of 15 Table 2. Experimental results of the proposed method and conventional method. Distance between Omnidirectional Stereo Camera and Object (m) Distance Measurement Results (m) Mean Absolute Error (m) Average Relative Error (%) Proposed Method Conventional Method Proposed Method Conventional Method Proposed Method Conventional Method Proposed Method Convention al Method 1.00 1.00 1.20 0.00 0.20 0.0 20.0 2.00 2.02 1.60 0.02 0.40 1.0 20.0 3.00 3.06 2.90 0.06 0.10 2.0 3.3 4.00 4.10 5.60 0.10 1.60 2.5 40.0 5.00 4.87 6.30 0.13 1.30 2.6 26.0 3.2.3. Consideration To validate the accuracy, the data were compared with data from a previous study. While the conventional method had a relative error of up to 40%, the new method succeeded in reducing the error to a maximum of 2.6%. In addition, the relative error for each distance was compared, and the accuracy was higher at all locations. However, as shown in Figure 15, the stability of the measurements decreases as the distance from the camera increases. This is due to the characteristics of stereo measurement, where the accuracy is determined by the actual length of one pixel in the image, which is influenced by the lens, camera resolution, shooting distance, and baseline distance between cameras. In addition, since the stereo correspondence point pixel in the camera image plate has a size, the point to be measured can only be identified within a certain range in the actual space, which results in measurement errors. Typically, errors are larger in the depth direction from the camera. While lengthening the baseline can reduce the depth error, it can result in larger errors for shorter distances or even make measurements impossible. Therefore, stereo image measurement systems require appropriate baseline settings for the specific conditions in which they will be used. Thus, this issue can be resolved by adjusting the baseline according to the device being mounted. These results demonstrate the effectiveness of this study in terms of accuracy. 3.3. Depth Image Generation Experiment 3.3.1. Experimental Method Since the accuracy was verified in Section 3.2, this experiment verifies whether omnidirectional measurement is possible. In order to confirm whether this method is\ncapable of omnidirectional measurement, an experiment was conducted to generate a\ndepth image of a panoramic image. The method used for depth image generation is called\nthe SGM method [13].\n.3.2. Experimental Results\nFigure 16 shows the original image and Figure 17 shows the depth image created.\nSensors 2022, 22, x FOR PEER REVIEW 13 of 15\nFigure 17. Depth image.\nFor visual clarity, depth is replaced by hue. Undetected areas are output in black.\n3.3.3. Consideration\nThrough experiments, our proposed method was able to output an omnidirectional\ndepth image that enabled confirmation of distances in all directions. It was confirmed that\nthe system was able to detect obstacles such as humans, desks, chairs, boxes, and walls,\nindicating that it can be applied to autonomous driving. However, false detections were\nobserved for featureless objects and areas, as well as for areas where the same features\nwere repeatedly seen due to the reliance on block matching, which requires image features\nfor detection. Despite this limitation, the primary goal of this project is obstacle detection\nand textured objects can be detected effectively. It can be used for obstacle detection with\nautomatic guidance in the same way as the conventional stereo method. In the\nconventional method, it is impossible to measure in the complete omnidirectional\ndirection as it includes the other camera in the image.\n4. Discussion\nWe conducted two experiments to discuss the issues and potential solutions of the\nproposed system. From the measurement experiment of Measurement Experiment, we\nfound that the system improved up to 37.5% in Relative error value compared to the\nconventional method. However, as mentioned in the Consideration, there was a problem\nwith the lower measurement accuracy the further away the camera was from the object.\nTo address this problem, we proposed changing the camera resolution, shooting distance,\nand baseline. By setting the baseline appropriately for the defined measurement range,\nwe could expect stable measurements even in distant areas. However, a larger baseline\nwould require a bigger device, which could result in a loss of mobility or design quality.\nAlternatively, increasing the camera resolution could improve depth direction accuracy\nbut at a higher cost. Therefore, we concluded that the system should be flexible enough to\nchange the baseline and camera parameters according to the machine\u2019s size, maximum\nspeed, and required measurement distance for autonomous driving. By doing so, we can\nensure the system\u2019s adaptability to different situations while maintaining accurate and\nstable measurements.\nIn Depth Image Generation Experiment, the proposed method successfully\ngenerated an omnidirectional depth image, allowing for distance checking in all\ndirections. Additionally, the method detected various obstacles, including people, desks,\nchairs, boxes and walls, suggesting its potential use for autonomous driving. However,\nfalse detections were observed in featureless and repetitive areas, which is a common\nlimitation of block matching that relies on image features. While the study did not\nconsider this false detection as a significant issue since the main focus was obstacle\nrecognition, further improvements are required if more precise detection is necessary. In\nthis study, only feature points were used for detection, but we believe that the detection\nrate can be improved by using deep learning. Depth estimation methods using stereo\nvision with prior learning have been studied in recent years. Advances in image\nrecognition technology, especially deep learning, have expanded research on depth\nestimation from images and videos. In depth estimation by stereo viewing using machine\nFigure 17. Depth image.\nFor visual clarity, depth is replaced by hue. Undetected areas are output in black.\n3.3.3. Consideration\nThrough experiments, our proposed method was able to output an omnidirectional depth image that enabled confirmation of distances in all directions. It was confirmed that the system was able to detect obstacles such as humans, desk , chairs, boxes, nd walls, indicating that it can be applied to autonomous driving. However, false detections were observed for featureless objects and areas, as well as for areas where the same features were repeatedly seen due to the reliance on block matching, which requires image features for detection. Despite this limitation, the primary goal of this project is obstacle detection and textured objects can be detected effectively. It can be used for obstacle detection with automatic gu d nce in t same wa as the conventional stereo me hod. In the conventional method, it is impossible to measure in the compl te omnidirectional direction as it includes the other camera in the image."
        },
        {
            "heading": "4. Discussion",
            "text": "We conducted two experiments to discuss the issues and potential solutions of the proposed system. From the measurement experiment of Measurement Experiment, we found that the system improved up to 37.5% in Relative error value compared to the conventional method. However, as mentioned in the Consideration, there was a problem with the lower measurement accuracy the further away the camera was from the object. To address this problem, we proposed changing the camera resolution, shooting distance, and baseline. By setting the baseline appropriately for the defined measurement range, we could expect stable measurements even in distant areas. However, a larger baseline would require a bigger device, which could result in a loss of m bili y or design quality. Alternatively, increasing the camera resolution could improve depth direction accuracy but at a higher cost. Therefore, we concluded that the system should be flexible enough to change the baseline and camera parameters according to the machine\u2019s size, maximum speed, and required measurement distance for autonomous driving. By doing so, we can ensure the system\u2019s adaptability to different situations while maintaining accurate and stable measure ents.\nSensors 2023, 23, 3243 13 of 14\nIn Depth Image Generation Experiment, the proposed method successfully generated an omnidirectional depth image, allowing for distance checking in all directions. Additionally, the method detected various obstacles, including people, desks, chairs, boxes and walls, suggesting its potential use for autonomous driving. However, false detections were observed in featureless and repetitive areas, which is a common limitation of block matching that relies on image features. While the study did not consider this false detection as a significant issue since the main focus was obstacle recognition, further improvements are required if more precise detection is necessary. In this study, only feature points were used for detection, but we believe that the detection rate can be improved by using deep learning. Depth estimation methods using stereo vision with prior learning have been studied in recent years. Advances in image recognition technology, especially deep learning, have expanded research on depth estimation from images and videos. In depth estimation by stereo viewing using machine learning [14\u201316], stereo images captured by left and right cameras and correct depth maps (ground truth) acquired by LiDAR and millimeter wave radar are pretrained as teacher data. Stereo images captured under similar conditions are used as test data. By inputting the correct depth map (ground truth) obtained by LiDAR or millimeter wave radar as the teacher data, it is now possible to detect featureless areas and areas where the same features appear repeatedly, which has been difficult to achieve in the past. Therefore, in the future, the proposed method can be improved by using such a model to enhance the accuracy of depth images."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this study, we proposed an omnidirectional measurement method with no blind spots by using a convex mirror, which in principle does not cause chromatic aberration, and by using vertical disparity by placing cameras above and below the image. Two experiments were conducted to demonstrate the effectiveness of this research, which achieved a wider measurement range and a smaller number of cameras while still allowing obstacle detection similar to the conventional stereo method. Compared to the previous study [5], we achieved about twice the measurement range. Compared to the previous study [6], omnidirectional measurement is now possible with half the number of cameras. Moreover, compared to the previous studies [7\u201310], the new system enables complete omnidirectional measurement without any blind spots. Furthermore, the accuracy was improved by 37.4% compared to the previous study [10], which had the best accuracy among the studies that enabled measurement with two cameras. We consider this result to be a remarkable achievement in stereo measurement.\nAuthor Contributions: Conceptualization and manuscript preparation, Y.O.; manuscript review S.K. and Y.Z.; project administration, A.K. supervision, Y.B. All authors have read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. BellaBot. Pudu Robotics. Available online: https://www.pudurobotics.com/jp/product/detail/bellabot (accessed on 19 February 2023). 2. Nuro. Helping the Heroes during COVID-19. Available online: https://medium.com/nuro/helping-the-heroes-during-covid-19 -49c189f216a2 (accessed on 19 February 2023). 3. TOYOTA. e-Palette. Available online: https://global.toyota/jp/newsroom/corporate/29933339.html (accessed on 19 February 2023). 4. Scharstein, D.; Szeliski, R. A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms. Int. J. Comput.\nVis. 2002, 47, 7\u201342. [CrossRef]\nSensors 2023, 23, 3243 14 of 14\n5. Iida, H.; Ji, Y.; Umeda, K.; Ohashi, A.; Fukuda, D.; Kaneko, S.; Murayama, J.; Uchida, Y. High-accuracy Range Image Generation by Fusing Binocular and Motion Stereo Using Fisheye Stereo Camera. In Proceedings of the 2020 IEEE/SICE International Symposium on System Integration (SII), Honolulu, HI, USA, 12\u201315 January 2020. 6. Won, C.; Ryu, J.; Lim, J. End-to-End Learning for Omnidirectional Stereo Matching With Uncertainty Prior. IEEE Trans. Pattern Anal. Mach. Intell. 2021, 43, 3850\u20133862. [CrossRef] [PubMed] 7. Sch\u00f6nbein, M.; Kitt, B.; Lauer, M. Environmental Perception for Intelligent Vehicles Using Catadioptric Stereo Vision Systems. In Proceedings of the ECMR, \u00d6rebro, Sweden, 7\u20139 September 2011. 8. Sch\u00f6nbein, M.; Geiger, A. Omnidirectional 3d reconstruction in augmented manhattan worlds. In Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems, Chicago, IL, USA, 14\u201318 September 2014. 9. Aoki, T.; Mengcheng, S.; Watanabe, H. Position Estimation and Distance Measurement from Omnidirectional Cameras. 80th Inf. Process. Soc. Jpn. 2018, 2018, 265\u2013266. 10. Tanaka, S.; Inoue, Y. Outdoor Human Detection with Stereo Omnidirectional Cameras. J. Robot. Mechatron. 2020, 32, 1193\u20131199. [CrossRef] 11. Aghayari, S.; Saadatseresht, M.; Omidalizarandi, M.; Neumann, I. Geometric calibration of full spherical panoramic Ricoh-Theta camera. In Proceedings of the ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences IV-1/W1 (2017), Hannover, Germany, 6\u20139 June 2017; Volume 4, pp. 237\u2013245. 12. Yamazawa, K.; Yagi, Y.; Yachida, M. Omnidirectional imaging with hyperboloidal projection. In Proceedings of the 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\u201993), Yokohama, Japan, 26\u201330 July 1993; Volume 2. 13. Hirschmuller, H. Accurate and efficient stereo processing by semi-global matching and mutual information. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), San Diego, CA, USA, 20\u201325 June 2005; Volume 2. 14. Cheng, X.; Zhong, Y.; Harandi, M.; Dai, Y.; Chang, X.; Li, H.; Drummond, T.; Ge, Z. Hierarchical neural architecture search for deep stereo matching. Adv. Neural Inf. Process. Syst. 2020, 33, 22158\u201322169. 15. Lipson, L.; Teed, Z.; Deng, J. Raft-stereo: Multilevel recurrent field transforms for stereo matching. In Proceedings of the 2021 International Conference on 3D Vision (3DV), London, UK, 1\u20133 December 2021. 16. Li, J.; Wang, P.; Xiong, P.; Cai, T.; Yan, Z.; Yang, L.; Liu, J.; Fan, H.; Liu, S. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 18\u201324 June 2022.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Research on Omnidirectional Stereo Measurement Using Convex Mirrors and Vertical Disparity",
    "year": 2023
}