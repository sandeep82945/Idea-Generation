{
    "abstractText": "A brain\u2013computer interface (BCI) is a computer-based system that allows for communication between the brain and the outer world, enabling users to interact with computers using neural activity. This brain signal is obtained from electroencephalogram (EEG) signals. A significant obstacle to the development of BCIs based on EEG is the classification of subject-independent motor imagery data since EEG data are very individualized. Deep learning techniques such as the convolutional neural network (CNN) have illustrated their influence on feature extraction to increase classification accuracy. In this paper, we present a multi-branch (five branches) 2D convolutional neural network that employs several hyperparameters for every branch. The proposed model achieved promising results for cross-subject classification and outperformed EEGNet, ShallowConvNet, DeepConvNet, MMCNN, and EEGNet_Fusion on three public datasets. Our proposed model, EEGNet Fusion V2, achieves 89.6% and 87.8% accuracy for the actual and imagined motor activity of the eegmmidb dataset and scores of 74.3% and 84.1% for the BCI IV-2a and IV-2b datasets, respectively. However, the proposed model has a bit higher computational cost, i.e., it takes around 3.5 times more computational time per sample than EEGNet_Fusion.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chang-Hwan Im"
        },
        {
            "affiliations": [],
            "name": "Markus Vincze"
        },
        {
            "affiliations": [],
            "name": "Diego R. Faria"
        },
        {
            "affiliations": [],
            "name": "Radia Rayan Chowdhury"
        },
        {
            "affiliations": [],
            "name": "Usman Adeel"
        }
    ],
    "id": "SP:812173993a62c74f5232c0a2fbec227451a1e11e",
    "references": [
        {
            "authors": [
                "E. Niedermeyer",
                "F.L. da Silva"
            ],
            "title": "Electroencephalography: Basic Principles, Clinical Applications, and Related Fields",
            "year": 2005
        },
        {
            "authors": [
                "J.D.R. Mill\u00e1n",
                "R. Rupp",
                "G. Mueller-Putz",
                "R. Murray-Smith",
                "C. Giugliemma",
                "M. Tangermann",
                "C. Vidaurre",
                "F. Cincotti",
                "A. Kubler",
                "R Leeb"
            ],
            "title": "Combining brain\u2013computer interfaces and assistive technologies: State-of-the-art and challenges",
            "venue": "Front. Neurosci",
            "year": 2010
        },
        {
            "authors": [
                "M. Teplan"
            ],
            "title": "Fundamentals of EEG measurement",
            "venue": "Meas. Sci. Rev. 2002,",
            "year": 2002
        },
        {
            "authors": [
                "V. Jurcak",
                "D. Tsuzuki",
                "I. 10/20 Dan"
            ],
            "title": "10/10, and 10/5 systems revisited: Their validity as relative head-surface-based positioning systems",
            "year": 2007
        },
        {
            "authors": [
                "L. Koessler",
                "L. Maillard",
                "A. Benhadid",
                "J.P. Vignal",
                "J. Felblinger",
                "H. Vespignani",
                "M. Braun"
            ],
            "title": "Automated cortical projection of EEG sensors: Anatomical correlation via the international 10\u201310 system",
            "year": 2009
        },
        {
            "authors": [
                "L. Bougrain",
                "M. Clerc",
                "F. Lotte"
            ],
            "title": "Brain Computer Interfaces: Methods, Applications and Perspectives",
            "year": 2016
        },
        {
            "authors": [
                "J.R. Wolpaw"
            ],
            "title": "Brain-computer interfaces (BCIs) for communication and control",
            "venue": "In Proceedings of the 9th international ACM SIGACCESS Conference on Computers and Accessibility, Tempe, AZ, USA,",
            "year": 2007
        },
        {
            "authors": [
                "M.A. Khan",
                "R. Das",
                "H.K. Iversen",
                "S. Puthusserypady"
            ],
            "title": "Review on motor imagery based BCI systems for upper limb post-stroke neurorehabilitation: From designing to application",
            "venue": "Comput. Biol. Med",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shen",
                "H. Lu",
                "J. Jia"
            ],
            "title": "Classification of motor imagery EEG signals with deep learning models",
            "venue": "In Proceedings of the Intelligence Science and Big Data Engineering: 7th International Conference,",
            "year": 2017
        },
        {
            "authors": [
                "H. Altaheri",
                "G. Muhammad",
                "M. Alsulaiman",
                "S.U. Amin",
                "G.A. Altuwaijri",
                "W. Abdul",
                "M.A. Bencherif",
                "M. Faisal"
            ],
            "title": "Deep learning techniques for classification of electroencephalogram (EEG) motor imagery (MI) signals: A review",
            "venue": "Neural Comput. Appl. 2023,",
            "year": 2023
        },
        {
            "authors": [
                "R. Sharma",
                "M. Kim",
                "A. Gupta"
            ],
            "title": "Motor imagery classification in brain-machine interface with machine learning algorithms: Classical approach to multi-layer perceptron model",
            "venue": "Biomed. Signal Process. Control. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "T.I. Voznenko",
                "E.V. Chepin",
                "G.A. Urvanov"
            ],
            "title": "The control system based on extended BCI for a robotic wheelchair",
            "venue": "Procedia Comput. Sci",
            "year": 2018
        },
        {
            "authors": [
                "M. Eidel",
                "A. K\u00fcbler"
            ],
            "title": "Wheelchair control in a virtual environment by healthy participants using a P300-BCI based on tactile stimulation: Training effects and usability",
            "venue": "Front. Hum. Neurosci",
            "year": 2020
        },
        {
            "authors": [
                "A. Palumbo",
                "V. Gramigna",
                "B. Calabrese",
                "N. Ielpo"
            ],
            "title": "Motor-imagery EEG-based BCIs in wheelchair movement and control: A systematic literature review",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Muhammad",
                "D. Vaino"
            ],
            "title": "Controlling Electronic Devices with brain rhythms/electrical activity using artificial neural network (ANN)",
            "venue": "Bioengineering 2019,",
            "year": 2019
        },
        {
            "authors": [
                "R. Bousseta",
                "I. El Ouakouak",
                "M. Gharbi",
                "F. Regragui"
            ],
            "title": "EEG based brain computer interface for controlling a robot arm movement through thought",
            "venue": "Irbm",
            "year": 2018
        },
        {
            "authors": [
                "G. Gillini",
                "P. Di Lillo",
                "F. Arrichiello"
            ],
            "title": "An assistive shared control architecture for a robotic arm using eeg-based bci with motor imagery",
            "venue": "In Proceedings of the 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Prague, Czech Republic,",
            "year": 2021
        },
        {
            "authors": [
                "M. Tariq",
                "P.M. Trivailo",
                "M. Simic"
            ],
            "title": "EEG-based BCI control schemes for lower-limb assistive-robots",
            "venue": "Front. Hum. Neurosci. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "K.K. Ang",
                "C. Guan",
                "K.S.G. Chua",
                "B.T. Ang",
                "C. Kuah",
                "C. Wang",
                "K.S. Phua",
                "Z.Y. Chin",
                "H. Zhang"
            ],
            "title": "Clinical study of neurorehabilitation in stroke using EEG-based motor imagery brain-computer interface with robotic feedback",
            "venue": "In Proceedings of the 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology, Buenos Aires, Argentina,",
            "year": 2010
        },
        {
            "authors": [
                "G. Onose",
                "C. Grozea",
                "A. Anghelescu",
                "C. Daia",
                "C.J. Sinescu",
                "A.V. Ciurea",
                "T. Spircu",
                "A. Mirea",
                "I. Andone",
                "A Sp\u00e2nu"
            ],
            "title": "On the feasibility of using motor imagery EEG-based brain\u2013computer interface in chronic tetraplegics for assistive robotic arm control: A clinical test and long-term post-trial follow-up",
            "venue": "Spinal Cord",
            "year": 2012
        },
        {
            "authors": [
                "C. Jeunet",
                "B. Glize",
                "A. McGonigal",
                "J.M. Batail",
                "J.A. Micoulaud-Franchi"
            ],
            "title": "Using EEG-based brain computer interface and neurofeedback targeting sensorimotor rhythms to improve motor skills: Theoretical background, applications and prospects",
            "year": 2019
        },
        {
            "authors": [
                "F. Molina-Rueda",
                "C. Navarro-Fern\u00e1ndez",
                "A. Cuesta-G\u00f3mez",
                "I.M. Alguacil-Diego",
                "A. Molero-S\u00e1nchez",
                "M. Carratal\u00e1-Tejada"
            ],
            "title": "Neuroplasticity modifications following a lower-limb amputation: A systematic review",
            "venue": "PM&R 2019,",
            "year": 2019
        },
        {
            "authors": [
                "N. Padfield",
                "J. Zabalza",
                "H. Zhao",
                "V. Masero",
                "J. Ren"
            ],
            "title": "EEG-based brain-computer interfaces using motor-imagery: Techniques and challenges",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "F. Nijboer",
                "N. Birbaumer",
                "A. K\u00fcbler"
            ],
            "title": "The influence of psychological state and motivation on brain\u2013computer interface performance in patients with amyotrophic lateral sclerosis\u2013a longitudinal study",
            "venue": "Front. Neuropharmacol",
            "year": 2010
        },
        {
            "authors": [
                "R. Abiri",
                "S. Borhani",
                "E.W. Sellers",
                "Y. Jiang",
                "X. Zhao"
            ],
            "title": "A comprehensive review of EEG-based brain\u2013computer interface paradigms",
            "venue": "J. Neural Eng",
            "year": 2019
        },
        {
            "authors": [
                "A. Miladinovi\u0107",
                "M. Aj\u010devi\u0107",
                "P. Busan",
                "J. Jarmolowska",
                "G. Silveri",
                "M. Deodato",
                "S. Mezzarobba",
                "P.P. Battaglini",
                "A. Accardo"
            ],
            "title": "Evaluation of Motor Imagery-Based BCI methods in neurorehabilitation of Parkinson\u2019s Disease patients",
            "venue": "In Proceedings of the 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), Montreal, QC, Canada,",
            "year": 2020
        },
        {
            "authors": [
                "M.O. Tamm",
                "Y. Muhammad",
                "N. Muhammad"
            ],
            "title": "Classification of vowels from imagined speech with convolutional neural networks",
            "venue": "Computers 2020,",
            "year": 2020
        },
        {
            "authors": [
                "G. Udovi\u010di\u0107",
                "A. Topi\u0107",
                "M. Russo"
            ],
            "title": "Wearable technologies for smart environments: A review with emphasis on BCI",
            "venue": "In Proceedings of the 2016 24th International Conference on Software, Telecommunications and Computer Networks (SoftCOM), Split, Croatia,",
            "year": 2016
        },
        {
            "authors": [
                "B. Kerous",
                "F. Skola",
                "F. Liarokapis"
            ],
            "title": "EEG-based BCI and video games: A progress report",
            "venue": "Virtual Real",
            "year": 2018
        },
        {
            "authors": [
                "M.A. Lebedev",
                "M.A. Nicolelis"
            ],
            "title": "Brain-machine interfaces: From basic science to neuroprostheses and neurorehabilitation",
            "venue": "Physiol. Rev",
            "year": 2017
        },
        {
            "authors": [
                "P.A. Abhang",
                "B.W. Gawali",
                "S.C. Mehrotra"
            ],
            "title": "Technological basics of EEG recording and operation of apparatus. In Introduction to EEG-and Speech-Based Emotion Recognition",
            "year": 2016
        },
        {
            "authors": [
                "R.T. Schirrmeister",
                "J.T. Springenberg",
                "L.D.J. Fiederer",
                "M. Glasstetter",
                "K. Eggensperger",
                "M. Tangermann",
                "F. Hutter",
                "W. Burgard",
                "T. Ball"
            ],
            "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
            "venue": "Hum. Brain Mapp",
            "year": 2017
        },
        {
            "authors": [
                "V.J. Lawhern",
                "A.J. Solon",
                "N.R. Waytowich",
                "S.M. Gordon",
                "C.P. Hung",
                "B.J. Lance"
            ],
            "title": "EEGNet: A compact convolutional neural network for EEG-based brain\u2013computer interfaces",
            "venue": "J. Neural Eng",
            "year": 2018
        },
        {
            "authors": [
                "K. Roots",
                "Y. Muhammad",
                "N. Muhammad"
            ],
            "title": "Fusion convolutional neural network for cross-subject EEG motor imagery classification",
            "venue": "Computers 2020,",
            "year": 2020
        },
        {
            "authors": [
                "K. V\u00e4rbu",
                "N. Muhammad",
                "Y. Muhammad"
            ],
            "title": "Past, Present, and Future of EEG-Based BCI Applications",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhao",
                "H. Zhang",
                "G. Zhu",
                "F. You",
                "S. Kuang",
                "L. Sun"
            ],
            "title": "A multi-branch 3D convolutional neural network for EEG-based motor imagery classification",
            "venue": "IEEE Trans. Neural Syst. Rehabil. Eng",
            "year": 2019
        },
        {
            "authors": [
                "S.U. Amin",
                "M. Alsulaiman",
                "G. Muhammad",
                "M.A. Bencherif",
                "M.S. Hossain"
            ],
            "title": "Multilevel weighted feature fusion using convolutional neural networks for EEG motor imagery classification",
            "venue": "IEEE Access 2019,",
            "year": 2019
        },
        {
            "authors": [
                "G. Schalk",
                "D.J. McFarland",
                "T. Hinterberger",
                "N. Birbaumer",
                "J.R. Wolpaw"
            ],
            "title": "BCI2000: A general-purpose brain-computer interface (BCI) system",
            "venue": "IEEE Trans. Biomed. Eng",
            "year": 2004
        },
        {
            "authors": [
                "J. Sleight",
                "P. Pillai",
                "S. Mohan"
            ],
            "title": "Classification of executed and imagined motor movement",
            "venue": "EEG signals. Ann. Arbor. Univ. Mich",
            "year": 2009
        },
        {
            "authors": [
                "Z. Tayeb",
                "N. Waniek",
                "J. Fedjaev",
                "N. Ghaboosi",
                "L. Rychly",
                "C. Widderich",
                "C. Richter",
                "J. Braun",
                "M. Saveriano",
                "G Cheng"
            ],
            "title": "Gumpy: A Python toolbox suitable for hybrid brain\u2013computer interfaces",
            "venue": "J. Neural Eng",
            "year": 2018
        },
        {
            "authors": [
                "C. Brunner",
                "R. Leeb",
                "G. M\u00fcller-Putz",
                "A. Schl\u00f6gl",
                "G. Pfurtscheller"
            ],
            "title": "BCI Competition 2008\u2013Graz data set A",
            "venue": "Inst. Knowl. Discov. (Lab. Brain Comput. Interfaces) Graz Univ. Technol",
            "year": 2008
        },
        {
            "authors": [
                "T.M. Ingolfsson",
                "M. Hersche",
                "X. Wang",
                "N. Kobayashi",
                "L. Cavigelli",
                "L. Benini"
            ],
            "title": "EEG-TCNet: An accurate temporal convolutional network for embedded motor-imagery brain\u2013machine interfaces",
            "venue": "In Proceedings of the 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Toronto, ON,",
            "year": 2020
        },
        {
            "authors": [
                "R. Leeb",
                "C. Brunner",
                "G. M\u00fcller-Putz",
                "A. Schl\u00f6gl",
                "G. Pfurtscheller"
            ],
            "title": "BCI Competition 2008\u2013Graz Data Set B; Graz University of Technology: Graz, Austria",
            "year": 2008
        },
        {
            "authors": [
                "D.A. Clevert",
                "T. Unterthiner",
                "S. Hochreiter"
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "year": 2015
        },
        {
            "authors": [
                "Z. Jia",
                "Y. Lin",
                "J. Wang",
                "K. Yang",
                "T. Liu",
                "X. Zhang"
            ],
            "title": "MMCNN: A multi-branch multi-scale convolutional neural network for motor imagery classification",
            "venue": "In Proceedings of the Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium,",
            "year": 2020
        },
        {
            "authors": [
                "V. Jayaram",
                "M. Alamgir",
                "Y. Altun",
                "B. Scholkopf",
                "M. Grosse-Wentrup"
            ],
            "title": "Transfer learning in brain-computer interfaces",
            "venue": "IEEE Comput. Intell. Mag",
            "year": 2016
        },
        {
            "authors": [
                "M. Dehghani",
                "A. Mobaien",
                "R. Boostani"
            ],
            "title": "A deep neural network-based transfer learning to enhance the performance and learning speed of BCI systems",
            "venue": "Brain-Comput. Interfaces 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wan",
                "R. Yang",
                "M. Huang",
                "N. Zeng",
                "X. Liu"
            ],
            "title": "A review on transfer learning in EEG signal analysis",
            "venue": "Neurocomputing",
            "year": 2021
        },
        {
            "authors": [
                "I. Shashkov",
                "A. Zaytsev",
                "N. Balabin",
                "E. Burnaev"
            ],
            "title": "Transfer learning for ensembles: Reducing computation time and keeping the diversity",
            "venue": "In Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition, Xiamen, China,",
            "year": 2022
        },
        {
            "authors": [
                "A. Miladinovi\u0107",
                "M. Aj\u010devi\u0107",
                "J. Jarmolowska",
                "U. Marusic",
                "M. Colussi",
                "G. Silveri",
                "P.P. Battaglini",
                "A. Accardo"
            ],
            "title": "Effect of power feature covariance shift on BCI spatial-filtering techniques: A comparative study",
            "venue": "Comput. Methods Programs Biomed",
            "year": 2021
        },
        {
            "authors": [
                "C.A. Loza",
                "J.C. Principe"
            ],
            "title": "The embedding transform. A novel analysis of non-stationarity in the EEG",
            "venue": "In Proceedings of the 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Honolulu, HI, USA,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Citation: Chowdhury, R.R.;\nMuhammad, Y.; Adeel, U. Enhancing\nCross-Subject Motor Imagery\nClassification in EEG-Based\nBrain\u2013Computer Interfaces by Using\nMulti-Branch CNN. Sensors 2023, 23,\n7908. https://doi.org/10.3390/\ns23187908\nAcademic Editors: Chang-Hwan Im,\nMarkus Vincze, Diego R. Faria,\nTimothy Patten and Alessandro Carf\u00ec\nReceived: 11 July 2023\nRevised: 23 August 2023\nAccepted: 11 September 2023\nPublished: 15 September 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: convolutional neural network (CNN); brain\u2013computer interface (BCI); deep learning; fusion network; motor imagery (MI); electroencephalography (EEG)"
        },
        {
            "heading": "1. Introduction",
            "text": "The brain\u2013computer interface (BCI), which decodes brain signals, is a mode of communication between the human brain and electronic devices. BCIs may enable people with low motor strength to connect and interact with their environment without taking physical action. Electroencephalography (EEG) is one of the most popular methods of gathering input from the brain by reading brain waves using a device capable of electrophysiological monitoring of the brain\u2019s electrical activity [1]. The brain\u2019s neural activity is usually recorded using a non-invasive scalp EEG [2]. The global 10\u201320 standard approach for electrode implantation is used to record brain signals by applying electrodes to the scalp. The terms 10 and 20 denote that the specific gaps between adjacent electrodes account for either 10% or 20% of the overall distance from the front to the back or from the right to the left side of the skull [3\u20135] (from Wikipedia, the free encyclopedia. Available online: https://en.wikipedia.org/wiki/10-20_system_(EEG), accessed date: 5 June 2023). EEG generates readings of electrical potential as time signals for each electrode positioned on the participant\u2019s scalp. These data reflect the brain\u2019s activity and provide vital information that can be used in numerous fields, including cognitive science and medical diagnosis [6]. By utilizing EEG data, researchers can analyze brain functions, explore cognitive processes, identify irregularities or disorders, and potentially assist in diagnosing certain conditions.\nSensors 2023, 23, 7908. https://doi.org/10.3390/s23187908 https://www.mdpi.com/journal/sensors\nMultiple studies have illustrated the connections between EEG signals and both actual and imagined movements [7]. Motor imagery (MI) involves a dynamic mental state where an individual mentally simulates a physical action. MI has gained considerable research attention due to its unique ability to generate brain signals without reliance on external triggers [8]. Recent studies have investigated the application of deep learning to EEG signals. Deep learning (DL) is a type of machine learning that has become popular in recent years. DL techniques have a high learning capacity and can retrieve complicated and significant features from different data sorts. DL models achieved impressive results in EEG motor imagery classification [9\u201311]. EEG-based BCI development has seen widespread use in medical applications. BCI in neurorehabilitation offers promising avenues to enhance recovery and regain function for individuals with neurological conditions and injuries. BCI applications support disabled people to communicate and use assistive technologies like wheelchairs [12\u201315], robotic arms [16,17], and wearable lower-limb exoskeletons [18]. BCI has been explored for improving cognitive functions such as stroke neurorehabilitation [19], spinal cord injury, and traumatic brain injury [20]. BCI systems have also been used to provide neurofeedback and cognitive training exercises to enhance attention, memory, and executive functions [21]. Moreover, BCI can contribute to the improvement of neuroplasticity [22], which is the brain\u2019s capacity to modify and adapt its structure and function based on experience [23]. Patients with paralysis, amputations, or central nervous system dysfunction are receiving support through BCI-based prosthetics [24]. Additionally, BCI has been used to assist individuals with locked-in syndrome or amyotrophic lateral sclerosis (ALS) [25,26] and Parkinson\u2019s disease [27]. Imagined speech also has potential in the BCI domain, which allows people with physical disabilities to communicate and use devices by imagining specific commands that the device can then recognize and carry out [28]. This innovative approach holds promise for enhancing the quality of life for patients with physical disabilities. BCI implementations have also expanded into the entertainment industry, enabling the development of smart environments [29], biofeedback-based games [30], and other forms of entertainment that offer engaging and immersive experiences. In the realm of BCI technology, a study highlights four areas where advancements can bring substantial benefits to disabled individuals: communication and control, motor substitution, entertainment, and motor recovery [2]. BCI applications have become popular due to their affordability, user-friendliness, and adequate temporal resolution [31]. However, the highly discrete pattern of brain signals makes building a cross-subject classifier challenging [32]. Convolutional neural networks (CNN), a type of deep network, have been used in modern studies to explore the possibility of automatic feature extraction. Deep and shallow CNN architectures with distinctive design choices were researched in previous work [33]. EEGNet, a neural network architecture designed for cross-paradigm BCI issues, was first presented in [34]. It can correctly identify EEG signals obtained from diverse BCI systems. Fusion structures have shown promise in addressing the issue of poor cross-subject data generalization in EEG-based categorization. Fusion networks can extract information from many branches with diverse designs or hyperparameters, combine the features in a fusion layer, and improve the classification accuracy of the architecture [35]. This method is based on the premise that the brain signal patterns of the subjects are unique and that the high-performing networks and hyperparameters can change depending on the subject. Overcoming the challenges of cross-subject classification development will enable the widespread implementation of BCI [36]. A 3D multi-branch CNN with three branches and various strides and kernel sizes in the convolutional layers was investigated in the study reported in [37]. After implementing feature fusion, the softmax classification layer was applied to the final connected layer. On the BCI Competition IV 2a dataset, the model secured 75% accuracy for within-subject classification. Another study [38] applied each pooling layer prior to feature extraction\nto conduct multi-layer feature fusion. In this study, the features gathered from the four branches were combined, and then the softmax classification layer was added. This method\u2019s accuracy rating on the BCI Competition IV 2a dataset is 74.5%. MMCNN, another multi-branch, multi-scale CNN model, utilized parallel processing with five EEG Inception Networks (EINs) to address subject and time variability issues. A multi-branch 2D convolutional neural network (CNN) named EEGNet Fusion was introduced in the literature [35] with varying hyperparameters, kernels, and filter sizes for three branches. The primary architecture of each branch follows the EEGNet model [34]. On the eegmmidb dataset\u2019s executed movement and imagined movement tasks, this network scored 84.1% and 83.8% accuracy, respectively. The EEGNet Fusion network allowed more freedom in hyperparameter selection and reduced the complexity of subject-independent EEG classification. As a result, this model improved cross-subject classification accuracy. In the literature, it has been observed that multi-branch fusion networks can improve the accuracy of cross-subject classification in EEG-based BCIs. These networks provide a flexible and adaptable structure that can capture diverse patterns and characteristics across different subjects. Hence, our objective was to examine the performance of higherbranch models and propose an optimal multi-branch model for EEGNet architecture [34]. We aimed to evaluate how the CNN network operates with higher branches and diverse parameter configurations. In this paper, we propose a novel technique for categorizing motor imagery tasks using a multi-branch feature fusion convolutional neural network model. We improved the EEGNet Fusion model and presented EEGNet Fusion V2, a five-branch convolutional neural network model. We employed various hyperparameters to observe their impact on the model\u2019s performance. Additionally, we implemented four-branch, six-branch, and seven-branch EEGNet Fusion models to evaluate the proposed model. Furthermore, we compared the proposed model, EEGNet Fusion V2, with state-of-the-art models, including DeepConvNet, ShallowConvNet, MMCNN, EEGNet, and EEGNet Fusion."
        },
        {
            "heading": "2. Materials and Methods",
            "text": "The dataset used for evaluation, pre-processing steps, proposed 5-branch fusion CNN model architecture EEGNet Fusion V2, and training and testing approach are discussed in this section."
        },
        {
            "heading": "2.1. Dataset and Preprocessing",
            "text": "2.1.1. Dataset 1: EEG Motor Movement/Imagery Dataset\nExperimental tests of the proposed model was performed using the PhysioNet EEG Motor Movement/Imagery dataset (Available online: https://physionet.org/content/ eegmmidb/1.0.0/, accessed date: 22 September 2022) which contains about 1500 one- and two-minute EEG recordings from 109 subjects. The BCI2000 [39] system was used to record EEG data while the experiment\u2019s subjects engaged in various motor imagining activities. Each participant completed 14 experimental runs, which included two baseline runs with their eyes opened and closed as well as three two-minute runs with the tasks of executing or imagining the opening and closing of the left or right hand, both fists or both feet. For the validation of the model, two subgroups of the dataset were used. Left-hand or right-hand movement tasks are included in the first subgroup. Imaginary left-hand or right-hand movement tasks are included in the second subgroup. The motor movement or imagery tasks were recorded as EEG signals on 64 channels placed on subject\u2019s scalps. Each channel is annotated with three codes: T0, T1, and T2. T0 refers to the rest period, while T1 refers to the motion of the left hand in some tasks and both fists in others. T2 denotes the movement of the right hand for some tasks and both feet for others. Each experimental run was partitioned based on these annotations. But according to the literature [35,40], the EEG channels of six subjects (subject 38, 88, 89, 82, 100, and 104) were not annotated as specified in the experiment. As a result, partitioning each experimental run based on\nthese annotations carries the potential risk of making incorrect decisions. Due to incorrect annotations, these six subjects were eliminated. Hence, 103 subjects out of 109 were used. Each trial\u2019s input data were divided into (C, W) dimensions, where C stands for the number of channels and W is the temporal dimension. All trials contained 4 to 4.1 s sustained and continuous movements for executed and imagined tasks. Therefore, to keep the dataset consistent, 4 s of data were clipped on each trial, sampled at 160 Hz, for a total of 640 samples. The sliding window approach was used to divide the 640 samples into eight non-overlapping windows of 80 samples each. The target label of each window was identical to the initial trial. The eight-time windows can provide more discriminatory information on the motor imagery data. After that, the signal processing module in the Gumpy BCI library [41] was applied to process the EEG signals. To eliminate the alternating current (AC) noise at the 60 Hz frequency, a notch filter was applied to the data. Then, Butterworth band-pass filtering was performed on the data in the 2 Hz to 60 Hz range with an order value of 5.\n2.1.2. Dataset 2: BCI Competition IV 2a\nBCI Competition IV-2a dataset (BCI Competition IV dataset. Available online: https://www.bbci.de/competition/iv/, accessed date: 15 October 2022) was used to evaluate the proposed model [42]. The Graz University of Technology generated famous public MI-EEG dataset in 2008 known as BCI-2a. The dataset\u2019s small number of samples taken in uncontrolled conditions with many artifacts makes decoding MI tasks difficult. The dataset contains 5184 trials (samples) of MI-EEG data collected from 9 participants applying 22 EEG electrodes (576 trials per participant). Moreover, 3 extra electrooculography (EOG) channels give information about eye movements. MI trials are 4 s long, captured at 250 Hz, and filtered between 0.5 and 100 Hz. Two sessions were captured for each subject on different days. There were 288 trials per subject. Four MI tasks corresponds to each trial: imagined movement of the left hand, right hand, both feet, and tongue. The time frame length selected for this dataset is 4.5 s (from 1.5 s to 6 s), producing 1125 samples [33]. Standardization was applied in the pre-processing step [43].\n2.1.3. Dataset 3: BCI Competition IV 2b\nThis publicly available dataset [44] was collected from 9 subjects using 3 bipolar electrodes at a sampling rate of 250 Hz. A bandpass filter between 0.5 and 100 Hz was then used for filtering. Moreover, 3 extra EOG channels were employed to collect data on eye movement. The dataset consisted of two classes, called the motor imagery of left hand and right hand. Each subject participated in 2 screening sessions without feedback and 3 screening sessions with feedback. Each session consisted of six runs with ten trials each and two classes of imagery. This resulted in 20 trials per run and 120 trials per session. The pre-processing step was similar to the BCI IV-2a dataset."
        },
        {
            "heading": "2.2. Model Architecture",
            "text": "The foundations for the proposed model architecture are the EEG-based CNN model, EEGNet [34], and EEGNet Fusion [35]. The authors in EEGNet [34] demonstrated that any EEG-based classification task may be successfully solved using EEGNet\u2019s original design. The proposed EEGNet Fusion V2 model included five distinct branches, each of the branches received identical input. The architecture of each branch matched with the EEGNet architecture. EEGNet Fusion V2 used different kernel sizes and convolutional filters in the depth-wise and separable layers for all five branches. The fusion method helps to reduce variance and enhance accuracy by aggregating the diverse predictions from different branches. Figure 1 illustrates the proposed EEGNet Fusion V2 architecture.\nFirst, 2-D input with temporal (datapoints per sample) and spatial (number of channels) dimensions are transmitted to the input layer for each sample. The model has five branches, and each branch has a 2-D convolutional layers with kernel sizes 8, 16, 32, 64, and 128, and filters with sizes (1, 64), (1, 80), (1, 96), (1, 112), and (1, 128), respectively. By using the convolutional layers in each branch, the model initially learns frequency filters with temporal convolutions. After the initial temporal convolutional layer, a depth-wise convolutional layer with (C, 1) filters for all branches was added, where C refers to the number of channels. The depth-wise convolutions help to decrease the number of trainable parameters and extract frequency specific spatial filters for each temporal filter [35]. Batch normalization, a useful technique for minimizing overfitting and enhancing the network\u2019s training pace, was used after the convolutional layer. After that, a separable convolutional layer was added to each branch with kernel size 16, 32, 64, 128, and 256 and filter size (1, 8), (1, 16), (1, 32), (1, 64), and (1, 128), respectively. To acquire temporal summaries of each feature map independently and to integrate the feature maps, the separable convolution merges pointwise and depth-wise convolutions [34]. Batch normalization was also used after the separable convolutional layer. An exponential linear unit (ELU) activation function followed each convolution layer. In CNN-based classification, this activation function is more computationally effective [45]. Furthermore, an input representation\u2019s dimensions were down-sampled by using average pooling layers following the depth-wise and separable convolutional layers. As a result, the computational cost was diminished because there were fewer parameters to\nlearn. Each pooling layer was additionally followed by a dropout function with a value of 0.5 to minimize overfitting. After the final pooling layer, the five CNN branches\u2019 weights were combined in a feature fusion layer, and the output of this layer was used as input to the softmax classification layer to create the output classes. The output classes were left-hand or righthand movement in one test and left-hand or right-hand imagined movement in another test on eegmmidb dataset. For the BCI Competition IV 2a dataset, the output classes were left hand, right hand, feet, and tongue imagined movement. For the BCI Competition IV 2b dataset, the output classes were left-hand and right-hand imagined movement."
        },
        {
            "heading": "2.3. Implementation of the Models",
            "text": "The Python TensorFlow framework was used to implement the neural networks. Google Colab GPU was used for testing and training the model. The implementation of this model is publicly available at GitHub (Available online: https://github.com/radiarayan-chowdhury/EEGNet-Fusion-V2, accessed date: 11 July 2023)."
        },
        {
            "heading": "2.4. Training and Testing Strategy",
            "text": "The PhysioNet EEG Motor Movement/Imagery dataset contains 45 trials per participant and 360 labeled samples per subject after preprocessing. A total of 37,080 samples from the executed and imagined task subsets for all 103 individuals are labeled. In total, 70% random data were used for training, 10% for validation, and 20% for testing. Each experiment used the same data split because a fixed seed value was applied. The BCI IV-2a dataset consists of 288 trials and 2 sessions per subject. In this dataset, one session was used for training. The other session was used for testing. In the BCI Competition 2b dataset, there are 5 sessions. Three sessions were used for training. The other 2 sessions were used for testing. The validation loss was calculated and the model weights with the best validation accuracies were saved during the training period. The model weights were loaded, and during the testing phase, the model was evaluated on the testing dataset by predicting target labels and determining the accuracy. Also, the precision, recall, f1-score and computational time per sample were calculated for the model. The Adam algorithm, a binary cross-entropy loss function, and a learning rate of 0.00001 were applied for optimization. All dropout layers have a dropout probability value of 0.5."
        },
        {
            "heading": "2.5. Performance Measure",
            "text": "Accuracy, precision, recall, and f1-score score were measured to examine the model. No negative trials were found in the datasets since all movements were treated as positive trials. The two-class classification required distinct computations for each hand. In one set of assessments, the left-hand (L) target label served as positive, and the right-hand (R) label served as negative. This was used to assess the model\u2019s capacity to differentiate between left- and right-hand movements. The right-hand target label served as a positive value in another set of assessments. If the prediction is L (or R) and the actual label is L (or R), then the value is true positive (TP). If the prediction is L (or R) but the actual label is R (or L), then it is false positive (FP). If the prediction is R (or L), and the actual label is also R (or L), then it is true negative (TN). If the prediction is R (or L), but the actual label is L (or R), then the value is false negative (FN). Moreover, there are four classes in the BCI IV-2a dataset, and the performance of each class (left-hand (L), right-hand (R), feet (F), and tongue (T)) was calculated separately. At first, the left-hand (L) target label was used as a positive, and the rest of the labels were used as negatives. Similarly, for each time, one class target label was used as positive and the others were used as negative to calculate the accuracy, precision, recall, and F1-score. More comprehensive information can be found in Appendix A."
        },
        {
            "heading": "3. Results",
            "text": "This section describes the experimental setup, results, and performance of the proposed model and four benchmark models used for comparison."
        },
        {
            "heading": "3.1. Experimental Setup",
            "text": "The PhysioNet EEG Motor Movement/Imagery dataset was tested on our proposed EEGNet Fusion V2, EEGNet Fusion [35], EEGNet [34], MMCNN [46], ShallowConvNet, and DeepConvNet [33]. The dataset was used to evaluate different parameters on EEGNet Fusion V2 model. The dataset also used to evaluate four-branch, six-branch, and sevenbranch EEGNet Fusion models. The results were recorded for comparison. For each experiment, 20% of samples were selected at random for testing, 10% for validation, and 70% for training. Furthermore, the BCI Competition IV-2a and the BCI Competition IV-2b dataset were implemented to measure the performance. The sessions with class labels were used to train the models, and the other sessions were used to test the models for both dataset."
        },
        {
            "heading": "3.2. Experimental Results: Evaluation of Different Parameters on EEGNet Fusion V2",
            "text": "In this study, we explored the effect of different filter sizes on the model\u2019s performance. We varied the filter sizes of the convolutional layers in each branch and assessed their impact on the accuracy and computation time of the EEGNet Fusion V2 model. Our results showed that certain filter sizes were more effective in capturing temporal patterns in the EEG signals. Each time the model was evaluated with different filter sizes of the convolutional layers, the kernel sizes and filter sizes of the separable convolutional layers remained unchanged. Each branch has a 2-D convolutional layers with kernel sizes 8, 16, 32, 64, and 128. A separable convolutional layer was added to each branch with kernel size 16, 32, 64, 128, and 256 and filter size (1, 8), (1, 16), (1, 32), (1, 64), and (1, 128). The kernel sizes and filter sizes of the separable convolutional layers remained unchanged during each evaluation of the model. For each branch, the kernel sizes were set to 16, 32, 64, 128, and 256, along with filter sizes of (1, 8), (1, 16), (1, 32), (1, 64), and (1, 128). Each time the model was evaluated with different filter sizes for 2-D convolutional layers, the kernel sizes remained unchanged at 8, 16, 32, 64, and 128. To evaluate the model, we conducted five tests, with each test utilizing different filter sizes for the convolutional layers in each branch. The specific filter sizes employed in each test were as follows:\n\u2022 Test 1: (1, 64), (1, 128), (1, 256), (1, 512), (1, 1024); \u2022 Test 2: (1, 64), (1, 256), (1, 544), (1, 512), (1, 1024); \u2022 Test 3: (1, 64), (1, 304), (1, 544), (1, 784), (1, 1024); \u2022 Test 4: (1, 64), (1, 80), (1, 96), (1, 112), (1, 128); \u2022 Test 5: (1, 64), (1, 96), (1, 128), (1, 192), (1, 256).\nThe PhysioNet EEG Motor Movement/Imagery dataset was used to test these models. Based on the obtained results from Tables 1 and 2, it is evident that the filter sizes\nhave minimal impact on accuracy but significantly affect the computation time. Therefore, we have made the decision to select the smallest filter sizes (Test 4) for our model. By choosing smaller filter sizes, we aim to optimize the computational efficiency of our model while maintaining a comparable level of accuracy. This allows us to reduce the computational burden and improve the overall efficiency of the model without compromising its performance."
        },
        {
            "heading": "3.3. Experimental Results: Evaluation of Different Numbers of EEGNets",
            "text": "To evaluate the proposed five-branch EEGNet Fusion model, we implemented fourbranch, six-branch, and seven-branch EEGNet Fusion models. These models share a similar structure to the proposed five-branch model, but with some variations in the number of branches and the kernel and filter sizes. By implementing these alternative fusion models, we aim to compare their performance with the proposed five-branch model. This comparative analysis enables us to assess the impact of the number of branches and the specific kernel and filter sizes on the overall performance of the EEGNet Fusion architecture. Appendix B provides a comprehensive overview of the filter and kernel sizes utilized in the four-branch, five-branch, six-branch, and seven-branch models.\nThe PhysioNet EEG Motor Movement/Imagery dataset was used to test these models. Due to limitations with the available GPU resources in Google Colab, we were unable\nto implement the higher branch fusion networks to evaluate our proposed model. However, despite this limitation, we proceeded with evaluating our proposed five-branch EEGNet Fusion model and comparing it to the implemented four-branch, six-branch and seven-branch models, while the evaluation could have been more comprehensive with the inclusion of the higher branch models, the results obtained from the four-branch, six-branch, and seven-branch models still provide valuable insights into the performance and effectiveness of the proposed model. Future work could involve conducting evaluations with higher branch fusion networks when more substantial computational resources are available to further investigate the impact of increasing the number of branches on the performance of the EEGNet Fusion architecture. According to the observations from Tables 3 and 4, the accuracy generally increases as the number of branches increases. Specifically, there is a noticeable improvement in accuracy when going from three branches to four branches and from four branches to five branches. However, the accuracy of the five branch, six branch and seven-branch models are very close, despite the six branch and seven branch model requiring a significantly higher computation time per sample. Based on these findings, we have decided to consider the five branch model as the proposed model. The five branch model provides a good balance between the accuracy and computation time compared to the six and seven branch models."
        },
        {
            "heading": "3.4. Experimental Results: Evaluation with Benchmark Model",
            "text": "3.4.1. Results of PhysioNet EEG Motor Movement/Imagery Dataset\nAs we improved the EEGNet Fusion model [35], the EEGNet Fusion model was replicated and validated the results on the EEG Motor Movement/Imagery dataset. The paper reported that the model achieved 84.1% and 83.8% accuracy in motor movement and imagined movement task, respectively. Similarly, the replicated model of EEGNet Fusion achieved 84.1% and 84.3% accuracy in motor movement and imagined movement task, respectively, which validated the results of the model. Table 5 contains the testing accuracy, precision (left and right hand), recall (left and right hand), f1-score (left and right hand), and computing time (per sample) for executed movement tasks from the EEG Motor Movement/Imagery dataset for all examined models. Our proposed EEGNet Fusion V2 model outperformed the other classifiers, and achieved 89.6% accuracy, 89.7% left-hand F1-score, and 89.6% right-hand F1-score. However, the average computation time of the EEGNet Fusion V2 model was 361 ms, which was the slowest and 3.4 times higher than EEGNet Fusion model.\nThe bold texts represent the results of the proposed model.\nFor all evaluated models, Table 6 shows the testing accuracy, precision, recall, f1-score, and computing time (per sample) for imagined movement tasks from the EEG Motor Movement/Imagery dataset. The EEGNet Fusion V2 model performed better than the benchmark models and scored 87.8% accuracy, 87.8% f1-score for left-hand and right-hand, though the computational time per sample was higher than other models, which was 354 ms.\nThe bold texts represent the results of the proposed model.\n3.4.2. Results of BCI IV-2a Dataset\nAccording to Table 7, the proposed EEGNet Fusion V2 model achieved higher accuracy on BCI IV-2a dataset, which confirmed that the multi-branch architecture outperformed on cross-subject classification. The proposed model scored 74.3% accuracy and took 815 ms computational time per sample. The fastest model, ShallowConvNet, achieved 69.8% accuracy with 44 ms computational time per sample.\nThe bold texts represent the results of the proposed model.\n3.4.3. Results of BCI IV-2b Dataset\nTable 8 demonstrates the accuracy, precision (left and right hand), recall (left and right hand), and f1-score (left and right hand) of all the models on BCI IV-2b dataset. Our EEGNet Fusion V2, outperformed using this dataset and achieved 84.1% with 326 ms computational time per sample.\nThe bold texts represent the results of the proposed model.\nThe comparison between the EEGNet Fusion V2 model and the benchmark models based on EEG Motor Movement/Imagery, BCI Competition IV-2a and IV-2b dataset were plotted clearly in Figure 2.\nBased on these findings, it can be concluded that for cross-subject EEG motor imagery categorization, the EEGNet Fusion V2 model outperforms the other architectures."
        },
        {
            "heading": "4. Discussion",
            "text": "The purpose of this research was to increase the accuracy of motor imagery classification to assist people with insufficient motor abilities in communicating with their surroundings through a brain\u2013computer interface. We proposed a multi-branch (fivebranch) composite two-dimensional neural network in this study, named the EEGNet Fusion V2 model, to classify the EEG-based motor imagery data. The proposed model was trained and tested on the PhysioNet EEG Motor Movement/Imagery dataset, the BCI Competition IV-2a, and the BCI Competition IV-2b datasets. The Motor Movement/Imagery dataset was implemented to categorize actual and imagined movements made with the left and right hands. Band-pass signal filtering and a moving window technique were used to preprocess the 103-subject data in this dataset. The EEGNet Fusion V2 model achieved 89.6% and 87.8% accuracy for executed and imagined movements, respectively. In comparison, the EEGNet Fusion model achieved lower accuracy rates of 84.1% for executed and 83.8% for imagined movements. The accuracy for executed movements increased by 5.5%, while the accuracy for imagined movements increased by 4% in the proposed model. The other benchmark models achieved less than 82% accuracy for both executed and imagined movement tasks. However, the proposed model was slower than the other tested models. It took 361 ms and 354 ms of computational time per sample for the real and imagined movement tasks, respectively. The DeepConvNet, ShallowConvNet, MMCNN, EEGNet, and EEGNet Fusion models had computation times of 44 ms, 23 ms, 102 ms, 34 ms, and 107 ms, respectively, for the executed task and 36 ms, 24 ms, 102 ms, 36 ms, and 110 ms, respectively, for the imagined task. These values indicate that the proposed model\u2019s computation time is higher than the other models. Although this computing time was more than three times longer than the EEGNet Fusion model, the computational cost was moderated by the notably greater accuracy of the EEGNet Fusion V2 model. The BCI Competition IV-2a dataset was implemented to classify four classes: left hand, right hand, feet, and tongue. The BCI Competition IV-2b dataset was also implemented to classify two classes of left and right hands. The proposed model achieved higher accuracy on both datasets, with 74.3% and 84.1% accuracy, respectively. In comparison, the EEGNet fusion model scored 73.8% and 83.8% on the BCI Competition IV-2a and IV-2b datasets,\nwhich is less than the proposed model\u2019s performance. The performance of the other benchmark models is below 71% for the IV-2a dataset and 83.5% for the IV-2b dataset. In our study, we investigated the performance of EEGNet Fusion models with different numbers of branches, specifically four, five, six, and seven. We also explored different filter sizes within the architecture to assess the models. Our findings demonstrate that the five-branch EEGNet Fusion model achieved the highest accuracy with the lowest computational time, outperforming the other models with four, six, and seven branches. As a result, we proposed the five-branch EEGNet Fusion V2 model as the optimal choice due to its performance and less complex structure than the higher-branch models. Our study provides valuable insights into the design of EEGNet Fusion models and offers guidance for future research in this area. The performance indicates that this composite network outperforms EEGNet Fusion, EEGNet, MMCNN, ShallowConvNet, and DeepConvNet. The results also show that the higher number of multi-branch fusion models obtain higher accuracy and require longer computational time. The key contribution of this study is the examination of multibranch (five-branch) composite networks using EEG motor imagery data, which represents an enhanced version of EEGNet Fusion. The adaptability of the architecture\u2019s multiple branches improved the model\u2019s performance. The ideal hyperparameter can be set for each branch, and more sophisticated feature maps are created by combining features in the fusion layer. The EEGNet Fusion V2 network can mitigate the complexity of cross-subject EEG classification by providing more flexibility in choosing the hyperparameters. When developing real-time applications such as brain\u2013computer interfaces (BCIs), it is crucial to prioritize computational efficiency to ensure rapid response following user actions. Otherwise, delays in feedback could lead to a sense of loss of control for the user. As the current model has demonstrated relatively longer computational time when implementing multiple branches, a potential avenue for future research could be to explore strategies for reducing computation time in such scenarios. The exploration of diverse transfer learning approaches can lead to decreased computation time and superior model performance [47\u201350]. Furthermore, the computation time can vary significantly depending on the hardware and software used. In this study, the computation time per sample is provided as a reference. This model can be applied to various EEG-based areas, for example, driver fatigue evaluation, disorder detection, sleep stage, etc. BCI-based motor movement classification can be used to control prosthetic limbs or assistive devices for individuals with limb loss or disabilities. By analyzing brain signals related to motor imagery, therapists can design personalized rehabilitation programs and provide real-time feedback to patients, facilitating motor recovery. Real-time motor movement classification can enable more immersive and interactive gaming experiences where users can perform actions or navigate through virtual worlds by simply imagining the movements. The proposed model shows strong results when applied to cross-subject classification across the mentioned datasets. However, using the model in real-time scenarios presents challenges due to the dynamic nature of brain signals. The model\u2019s performance might deteriorate when it encounters new subject data that it has not been exposed to before, even though it performs well in controlled experiments. Adapting the model to these changes requires strategies like transfer learning [47\u201350], where the model builds upon its existing knowledge to handle new scenarios. There may be further opportunities to explore more multi-branch designs. As EEG data is non-stationary, which has implications for the feature extraction process, different approaches can be explored to address the issue of non-stationarity [51\u201353]. Transfer learning approaches can be implemented to resolve the issue of less data per subject and inter-subject variations of the EEG-based dataset.\nAuthor Contributions: Conceptualization, all authors; methodology, R.R.C.; software, R.R.C.; validation, R.R.C.; formal analysis, all authors; writing\u2014original draft preparation, R.R.C.; writing\u2014review and editing, all authors; visualization, R.R.C.; project administration, Y.M. All authors have read and agreed to the published version of the manuscript.\nFunding: This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The datasets are publicly available online: https://www.bbci.de/ competition/iv/ (accessed date: 15 October 2022) and https://physionet.org/content/eegmmidb/1. 0.0/ (accessed date: 22 September 2022).\nConflicts of Interest: The authors declare no conflict of interest.\nAbbreviations The following abbreviations are used in this manuscript:\nMDPI Multidisciplinary Digital Publishing Institute DOAJ Directory of open access journals TLA Three letter acronym LD Linear dichroism"
        },
        {
            "heading": "Appendix A",
            "text": "The BCI IV-2a dataset includes four classes: left-hand (L), right-hand (R), feet (F), and tongue (T). Each class was assessed separately using metrics like precision, recall, and F1-score. Positive target labels were assigned to one class at a time, while the rest were treated as negative.\nFor the left-hand (L) class:\n\u2022 TP: Prediction is L and actual label is L. \u2022 FP: Prediction is L but actual label is R, F, or T. \u2022 TN: Prediction is R, F, or T, and actual label is also R, F, or T. \u2022 FN: Prediction is R, F, or T, but actual label is L.\nFor the right-hand (R) class:\n\u2022 TP: Prediction is R and actual label is R. \u2022 FP: Prediction is R but actual label is L, F, or T. \u2022 TN: Prediction is L, F, or T, and actual label is also L, F, or T. \u2022 FN: Prediction is L, F, or T, but actual label is R.\nFor the feet (F) class:\n\u2022 TP: Prediction is F and actual label is F. \u2022 FP: Prediction is F but actual label is L, R, or T. \u2022 TN: Prediction is L, R, or T, and actual label is also L, R, or T. \u2022 FN: Prediction is L, R, or T, but actual label is F.\nFor the tongue (T) class:\n\u2022 TP: Prediction is T and actual label is T. \u2022 FP: Prediction is T but actual label is L, R, or F. \u2022 TN: Prediction is L, R, or F, and actual label is also L, R, or F. \u2022 FN: Prediction is L, R, or F, but actual label is T."
        },
        {
            "heading": "Appendix B",
            "text": "To assess the proposed five-branch EEGNet Fusion V2 model, we conducted experiments using four-branch, six-branch, and seven-branch EEGNet Fusion models. The filter and kernel sizes implemented in these models are listed below.\nThe four-branch EEGNet:\n\u2022 Convolutional Filter sizes for each branch: (1, 64), (1, 80), (1, 96), and (1, 112) \u2022 Convolutional Kernel sizes for each branch: 8, 16, 32, and 64 \u2022 Separable Filter sizes for each branch: (1, 8), (1, 16), (1, 32), and (1, 64) \u2022 Separable Kernel sizes for each branch: 16, 32, 64, and 128\nThe proposed five-branch EEGNet:\n\u2022 Convolutional Filter sizes for each branch: (1, 64), (1, 80), (1, 96), (1, 112), and (1, 128) \u2022 Convolutional Kernel sizes for each branch: 8, 16, 32, 64, and 128 \u2022 Separable Filter sizes for each branch: (1, 8), (1, 16), (1, 32), (1, 64), and (1, 128) \u2022 Separable Kernel sizes for each branch: 16, 32, 64, 128, and 256\nThe six-branch EEGNet:\n\u2022 Convolutional Filter sizes for each branch: (1, 32), (1, 64), (1, 80), (1, 96), (1, 112), and (1, 128) \u2022 Convolutional Kernel sizes for each branch: 4, 8, 16, 32, 64, and 128 \u2022 Separable Filter sizes for each branch: (1, 8), (1, 16), (1, 32), (1, 64), (1, 128) and (1, 256) \u2022 Separable Kernel sizes for each branch: 8, 16, 32, 64, 128, and 256\nThe seven-branch EEGNet:\n\u2022 Convolutional Filter sizes for each branch: (1, 16), (1, 32), (1, 64), (1, 80), (1, 96), (1, 112), and (1, 128) \u2022 Convolutional Kernel sizes for each branch: 2, 4, 8, 16, 32, 64, and 128 \u2022 Separable Filter sizes for each branch: (1, 4), (1, 8), (1, 16), (1, 32), (1, 64), (1, 128) and (1, 256) \u2022 Separable Kernel sizes for each branch: 4, 8, 16, 32, 64, 128, and 256"
        }
    ],
    "title": "Enhancing Cross-Subject Motor Imagery Classification in EEG-Based Brain\u2013Computer Interfaces by UsingMulti-Branch CNN",
    "year": 2023
}