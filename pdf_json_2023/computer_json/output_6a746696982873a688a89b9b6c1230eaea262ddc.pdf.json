{
    "abstractText": "Facial landmark detection is an essential task in face-processing techniques. Traditional methods however require expensive pixel-level labels. Semi-supervised facial landmark detection has been explored as an alternative but previous approaches only focus on training-oriented issues (e.g., noisy pseudolabels in the semi-supervised learning), neglecting task-oriented issues (i.e., the quantization error in the landmark detection). We argue that semi-supervised landmark detectors should resolve the two technical issues simultaneously. Through a simple experiment, we found that taskand training-oriented solutions may negatively influence each other, thus eliminating their negative interactions is important. To this end, we devise a new heatmap regression framework via hybrid representation, namely HybridMatch. We utilize both 1-D and 2-D heatmap representations. Here, the 1-D and 2-D heatmap help alleviate the task-oriented and the training-oriented issues, respectively. To exploit the advantages of our hybrid representation, we introduce curriculum learning; relying more on the 2-D heatmap at the early training stage and gradually increasing the effects of the 1-D heatmap. By resolving the two issues simultaneously, we can capture more precise landmark points than existing methods with only a few annotated data. Extensive experiments show that HybridMatch achieves state-of-the-art performance on three benchmark datasets, especially showing 26.3% NME improvement over the existing method in the 300-W full set at 5% data ratio. Surprisingly, our method records a comparable performance, 5.04 (challenging set in the 300-W) to the fully-supervised facial landmark detector 5.03. The remarkable performance of HybridMatch shows its potential as a practical alternative to the fully-supervised model. INDEX TERMS Facial landmark detection, facial key-points, landmark detection, semi-supervised facial landmark detection, heatmap-based landmark detection.",
    "authors": [
        {
            "affiliations": [],
            "name": "SEOUNGYOON KANG"
        },
        {
            "affiliations": [],
            "name": "MINHYUN LEE"
        },
        {
            "affiliations": [],
            "name": "MINJAE KIM"
        },
        {
            "affiliations": [],
            "name": "HYUNJUNG SHIM"
        }
    ],
    "id": "SP:c95bae25838b81b6e799eadf472a6a082be39c9b",
    "references": [
        {
            "authors": [
                "Aiden Nibali",
                "Zhen He",
                "Stuart Morgan",
                "Luke Prendergast"
            ],
            "title": "Numerical coordinate regression with convolutional neural networks",
            "venue": "arXiv preprint arXiv:1801.07372,",
            "year": 2018
        },
        {
            "authors": [
                "Xiao Sun",
                "Bin Xiao",
                "Fangyin Wei",
                "Shuang Liang",
                "Yichen Wei"
            ],
            "title": "Integral human pose regression",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Ying Tai",
                "Yicong Liang",
                "Xiaoming Liu",
                "Lei Duan",
                "Jilin Li",
                "Chengjie Wang",
                "Feiyue Huang",
                "Yu Chen"
            ],
            "title": "Towards highly accurate and stable face alignment for high-resolution videos",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Feng Zhang",
                "Xiatian Zhu",
                "Hanbin Dai",
                "Mao Ye",
                "Ce Zhu"
            ],
            "title": "Distribution-aware coordinate representation for human pose estimation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Shi Yin",
                "Shangfei Wang",
                "Xiaoping Chen",
                "Enhong Chen",
                "Cong Liang"
            ],
            "title": "Attentive one-dimensional heatmap regression for facial landmark detection and tracking",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Bjorn Browatzki",
                "Christian Wallraven"
            ],
            "title": "3fabrec: Fast few-shot face alignment by reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Xuanyi Dong",
                "Yi Yang"
            ],
            "title": "Teacher supervises students how to learn from partially labeled images for facial landmark detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Sina Honari",
                "Pavlo Molchanov",
                "Stephen Tyree",
                "Pascal Vincent",
                "Christopher Pal",
                "Jan Kautz"
            ],
            "title": "Improving landmark localization with semi-supervised learning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Shengju Qian",
                "Keqiang Sun",
                "Wayne Wu",
                "Chen Qian",
                "Jiaya Jia"
            ],
            "title": "Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jingdong Wang",
                "Ke Sun",
                "Tianheng Cheng",
                "Borui Jiang",
                "Chaorui Deng",
                "Yang Zhao",
                "Dong Liu",
                "Yadong Mu",
                "Mingkui Tan",
                "Xinggang Wang"
            ],
            "title": "Deep highresolution representation learning for visual recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Liu",
                "Jiwen Lu",
                "Jianjiang Feng",
                "Jie Zhou"
            ],
            "title": "Two-stream transformer networks for video-based face alignment",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Zhanpeng Zhang",
                "Ping Luo",
                "Chen Change Loy",
                "Xiaoou Tang"
            ],
            "title": "Learning deep representation for face alignment with auxiliary attributes",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Adrian Bulat",
                "Georgios Tzimiropoulos"
            ],
            "title": "How far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Lisha Chen",
                "Hui Su",
                "Qiang Ji"
            ],
            "title": "Deep structured prediction for facial landmark detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xiao Chu",
                "Wei Yang",
                "Wanli Ouyang",
                "Cheng Ma",
                "Alan L Yuille",
                "Xiaogang Wang"
            ],
            "title": "Multi-context attention for human pose estimation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Alejandro Newell",
                "Kaiyu Yang",
                "Jia Deng"
            ],
            "title": "Stacked hourglass networks for human pose estimation",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Ying Tai",
                "Yicong Liang",
                "Xiaoming Liu",
                "Lei Duan",
                "Jilin Li",
                "Chengjie Wang",
                "Feiyue Huang",
                "Yu Chen"
            ],
            "title": "Towards highly accurate and stable face alignment for high-resolution videos",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Wayne Wu",
                "Chen Qian",
                "Shuo Yang",
                "Quan Wang",
                "Yici Cai",
                "Qiang Zhou"
            ],
            "title": "Look at boundary: A boundaryaware face alignment algorithm",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Merget",
                "Matthias Rock",
                "Gerhard Rigoll"
            ],
            "title": "Robust facial landmark detection via a fully-convolutional local-global context network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Wayne Wu",
                "Chen Qian",
                "Shuo Yang",
                "Quan Wang",
                "Yici Cai",
                "Qiang Zhou"
            ],
            "title": "Look at boundary: A boundaryaware face alignment algorithm",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Meilu Zhu",
                "Daming Shi",
                "Mingjie Zheng",
                "Muhammad Sadiq"
            ],
            "title": "Robust facial landmark detection via occlusion-adaptive deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and 10 VOLUME 4, 2016 This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3257180 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License",
            "venue": "For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ Kang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Xinyao Wang",
                "Liefeng Bo",
                "Li Fuxin"
            ],
            "title": "Adaptive wing loss for robust face alignment via heatmap regression",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jiahao Xia",
                "Weiwei Qu",
                "Wenjian Huang",
                "Jianguo Zhang",
                "Xi Wang",
                "Min Xu"
            ],
            "title": "Sparse local patch transformer for robust face alignment and landmarks inherent relation learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xing Lan",
                "Qinghao Hu",
                "Jian Cheng"
            ],
            "title": "Revisting quantization error in face alignment",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Liu",
                "Jonathan Niles-Weed",
                "Narges Razavian",
                "Carlos Fernandez-Granda"
            ],
            "title": "Early-learning regularization prevents memorization of noisy labels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Paola Cascante-Bonilla",
                "Fuwen Tan",
                "Yanjun Qi",
                "Vicente Ordonez"
            ],
            "title": "Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning",
            "venue": "arXiv preprint arXiv:2001.06001,",
            "year": 2020
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Dandelion Mane",
                "Vijay Vasudevan",
                "Quoc V Le"
            ],
            "title": "Autoaugment: Learning augmentation strategies from data",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552,",
            "year": 2017
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Geoffrey French",
                "Michal Mackiewicz",
                "Mark Fisher"
            ],
            "title": "Self-ensembling for visual domain adaptation",
            "venue": "arXiv preprint arXiv:1706.05208,",
            "year": 2017
        },
        {
            "authors": [
                "Peter N Belhumeur",
                "David W Jacobs",
                "David J Kriegman",
                "Neeraj Kumar"
            ],
            "title": "Localizing parts of faces using a consensus of exemplars",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Vuong Le",
                "Jonathan Brandt",
                "Zhe Lin",
                "Lubomir Bourdev",
                "Thomas S Huang"
            ],
            "title": "Interactive facial feature localization",
            "venue": "In European conference on computer vision,",
            "year": 2012
        },
        {
            "authors": [
                "Xiangxin Zhu",
                "Deva Ramanan"
            ],
            "title": "Face detection, pose estimation, and landmark localization in the wild",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Kieron Messer",
                "Jiri Matas",
                "Josef Kittler",
                "Juergen Luettin",
                "Gilbert Maitre"
            ],
            "title": "Xm2vtsdb: The extended m2vts database",
            "venue": "In Second international conference on audio and video-based biometric person authentication,",
            "year": 1999
        },
        {
            "authors": [
                "Christos Sagonas",
                "Epameinondas Antonakos",
                "Georgios Tzimiropoulos",
                "Stefanos Zafeiriou",
                "Maja Pantic"
            ],
            "title": "300 faces in-the-wild challenge: Database and results",
            "venue": "Image and vision computing,",
            "year": 2016
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Xudong Cao",
                "Yichen Wei",
                "Jian Sun"
            ],
            "title": "Face alignment via regressing local binary features",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Martin Koestinger",
                "Paul Wohlhart",
                "Peter M Roth",
                "Horst Bischof"
            ],
            "title": "Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization",
            "venue": "IEEE international conference on computer vision workshops (ICCV workshops),",
            "year": 2011
        },
        {
            "authors": [
                "Jiangjing Lv",
                "Xiaohu Shao",
                "Junliang Xing",
                "Cheng Cheng",
                "Xi Zhou"
            ],
            "title": "A deep regression architecture with two-stage re-initialization for high performance facial landmark detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Wei Yang",
                "Shuang Li",
                "Wanli Ouyang",
                "Hongsheng Li",
                "Xiaogang Wang"
            ],
            "title": "Learning feature pyramids for human pose estimation",
            "venue": "In proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Jing Huo",
                "Wenbin Li",
                "Yinghuan Shi",
                "Yang Gao",
                "Hujun Yin"
            ],
            "title": "Webcaricature: a benchmark for caricature recognition",
            "venue": "In British Machine Vision Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Shizhan Zhu",
                "Cheng Li",
                "Chen Change Loy",
                "Xiaoou Tang"
            ],
            "title": "Face alignment by coarse-to-fine shape searching",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Jiankang Deng",
                "Qingshan Liu",
                "Jing Yang",
                "Dacheng Tao. M"
            ],
            "title": "csr: Multi-view, multi-scale and multicomponent cascade shape regression",
            "venue": "Image and Vision Computing,",
            "year": 2016
        },
        {
            "authors": [
                "Riza Alp Guler",
                "George Trigeorgis",
                "Epameinondas Antonakos",
                "Patrick Snape",
                "Stefanos Zafeiriou",
                "Iasonas Kokkinos"
            ],
            "title": "Densereg: Fully convolutional dense shape regression in-the-wild",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Facial landmark detection, facial key-points, landmark detection, semi-supervised facial landmark detection, heatmap-based landmark detection.\nI. INTRODUCTION Facial landmark detection aims to identify predefined key points of the face image, including eyes, nose, mouth, and facial contour. It has been widely utilized in various applications, such as face morphing, tracking, expression analysis, and face identification. Existing landmark detection methods can be divided into coordinate-based or heatmap-based approach, depending on the representation of a landmark point set. Since the coordinate-based approach does not fully utilize spatial and contextual information of landmark points, it shows relatively lower performance than the heatmap-based\napproach. For this reason, recent studies tend to develop heatmap-based methods.\nThe heatmap-based methods mostly utilize 2-D heatmap as the landmark representation. Since a 2-D heatmap requires O(N2) memory complexity to express N\u00d7N heatmap resolution, N cannot be large in practical applications. However, various studies [1, 2, 3, 4] have pointed out that a small N yields the large quantization error of the heatmap and thus is a performance bottleneck of facial landmark detection. To alleviate this issue, recent models have been actively studied to restore the residual parts (fractional components after\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nquantization). However, restoring residual parts is sensitive to the size of the original 2-D heatmap; a poor performance with considerably small N . Recently, Yin et al. [5] addresses this issue by representing the coordinate (x, y) of a heatmap with only O(2N) memory complexity. That is, they replace a 2-D heatmap with two 1-D heatmaps by assuming the separability of the 2-D heatmap. However, a 2-D heatmap is often not separable and the correlation between x and y coordinate is informative for landmark regression. To compensate for the loss of correlation information, they introduce a co-attention module between two 1-D heatmaps. Overall, the method by Yin et al. [5]. effectively reduces the quantization error by increasing N . Finally, it significantly improves the accuracy of fully-supervised facial landmark detection over the 2-D heatmap-based methods (17% NME improvement in 300-W common dataset).\nDespite the significant performance advantages of the fully-supervised methods, these models heavily rely on a large number of clean annotations. In particular, the landmark detection scenario requires pixel-level labels, which involves an expensive annotation cost. Besides, it easily suffers from noisy labels as creating precise pixel-level labels is challenging even for human annotators. To reduce the labeling budget and the sensitivity to data quality, recent studies have investigated the semi-supervised learning regime for facial landmark detection. The semi-supervised models utilize a mixture of a small amount of labeled data and a large amount of unlabeled data for model training, which is a reasonable setting for practical applications.\nTo this end, existing semi-supervised landmark detection methods focus only on training-oriented issue, such as effectively handling unlabeled data. They implicitly learn facial shapes via unsupervised training [6], developing a selective pseudo-labeling scheme by assessing pseudo-label quality [7], formulating multi-task learning [8], or utilizing style transfer to increase training dataset [9]. However, we argue that semi-supervised landmark detection should resolve two\ntechnical challenges at the same time; (i) the task-oriented issue such as quantization errors caused by low-resolution heatmap representation, and (ii) the training-oriented issue such as noisy pseudo-labels caused by the semi-supervised learning scenario.\nAssuming that the task-oriented solution and trainingoriented solution independently affect the performance, it is natural to combine the state-of-the-art of each side and then constitute the framework. Therefore, we attempt to combine the 1-D heatmap-based method by Yin et al. [5] for handling the quantization error and the high-performance semi-supervised framework (FixMatch [10]). Interestingly, through a simple experiment, we found that a 1-D heatmap is no longer effective than a 2-D heatmap in the semisupervised setting. Figure 1 compares facial landmark detection performances using a 1-D and 2-D heatmap, respectively. As expected, 1-D heatmap (FSL 1-D [5]) outperforms 2-D heatmap (FSL 2-D [11]) in the fully-supervised setting as reported in [5]. Counter-intuitively, under the semisupervised scenario, we found that using the 1-D heatmap (SSL 1-D) and 2-D heatmap (SSL 2-D) reported similar NME values. It means that the semi-supervised training is facilitated better with a 2-D heatmap than a 1-D heatmap when observing the performance gain of each method over its fully-supervised counterpart; 24.2% and 11.4% NME improvements at 5% data ratio for 2-D and 1-D representation, respectively. From these results, we conclude that taskoriented and training-oriented solutions may negatively influence each other, thus eliminating their negative interactions is an important issue to bridging the performance gap.\nTo understand what causes the negative feedback, we further investigate the learning profiles using 1-D and 2-D heatmaps, and confirm the positive role of the 2-D heatmap in the semi-supervised setting. The estimated 2-D heatmap is more accurate than the estimated 1-D heatmap at the early training stage. Then, we propose a new training strategy that uses 1-D and 2-D heatmap representations simultaneously to enjoy the advantages of both sides, namely HybridMatch. The proposed model is built upon FixMatch [10]; it learns unlabeled data via consistency regularization between weaklyaugmented and strongly-augmented samples and labeled data via conventional cross-entropy loss. Then, it utilizes the high-resolution 1-D representation to reduce the quantization errors in the heatmap. The low-resolution 2-D representation plays a central role in facilitating semi-supervised learning. To enjoy the advantages of both 1-D and 2-D representation, we employ the curriculum learning strategy. It focuses only on the feedback from the 2-D heatmap at the beginning of training, and gradually increases the feedback from the 1-D heatmap.\nThe motivation behind our training scheme is two-fold. First, it increases the quality of the pseudo-label at the early training stage with 2-D heatmaps. Second, it exploits higher performance with high-resolution 1-D heatmap when it is available (i.e., at the end of training). Through this coarseto-fine approach, our HybridMatch significantly outperforms"
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nthe existing semi-supervised models on three datasets (26.3% NME improvement in 300-W full set at 5% data ratio), even comparable to those of the fully-supervised models.\nII. RELATED WORKS Supervised facial landmark detection. Supervised facial landmark detection techniques can be categorized into two groups; (i) coordinate regression methods [12, 13] and (ii) heatmap regression methods [2, 14, 15, 16, 17, 18, 19]. The coordinate regression predicts a normalized landmark coordinate, and the heatmap regression estimates a heatmap per landmark coordinate. Owing to the performance advantages, recent methods adopt heatmap regression for facial landmark detection. Several heatmap-based methods utilize additional geometric constraints to improve performance; Merget et al. [20] using a PCA-based 2-D shape model, LAB [21] exploiting face boundary information, and ODN [22] learning additional weighting from occlusion probabilities. Awing [23] resolves the imbalance between foreground and background on the heatmap and significantly improves the performance. HRNet [11] develops a new model architecture by exploiting high-resolution representation. SLPT [24] proposes a sparse local patch transformer for learning inherent relation between facial landmarks.\nHowever, heatmap-based methods commonly suffer from large memory complexity because they estimate a 2-D heatmap per landmark coordinate value. Consequently, the heatmap resolution is usually lower than the resolution of the input image in practice. Then, the fractional part of the landmark coordinates is neglected, resulting in severe quantization errors. HIH [25] observes that NME caused by quantization error is even larger than 1/3 of the state-ofthe-art item. To tackle the quantization error, Sun et al. [2] exploits the probabilities of all landmarks to estimate the landmark coordinates with fractional parts. DSNT [1] converts discrete 2-D heatmaps into continuous coordinates by adding a differential layer. FHR [3] estimates fractional parts of landmarks by fitting the 2-D Gaussian distribution from samples of a 2-D heatmap. DARK [4] estimates landmarks by approximating the distribution using Taylor-expansion. However, when the resolution of a 2-D heatmap is considerably low, it no longer carries the informative spatial distribution. Then, various strategies for estimating the fractional part are often not effective. Recently, Yin et al. [5] introduces an idea of representing a 2-D heatmap via two 1-D heatmaps with a co-attention mechanism and shows that significant performance gain can be achieved by effectively handling the quantization error. Semi-supervised facial landmark detection. RCN [8] proposes a multi-task framework, performing both attribute classification and landmark detection. SA [9] employs a data augmentation method by generating style-translated examples to secure more training data. TS3 [7] utilizes a teacherstudents framework. Here, the teacher criticizes the quality of the student-generated samples, and the students are re-trained with the refined pseudo samples via quality filtering. 3Fab-\nRec [6] shows that unsupervised generative training captures implicit facial shape information. Then, it sufficiently trains a facial landmark detector with supervised follow-up training, only using small supervised samples.\nWhile our HybridMatch uses pseudo-labels like TS3, we do not require multiple trainable networks with multi-stage training. More importantly, existing semi-supervised facial landmark detectors focus only on training-oriented issue (i.e., how to use unlabeled data). That is, they do not consider task-oriented issue such as quantization errors in a semisupervised setting, leading to sub-optimal performance. To the best of our knowledge, we are the first to argue that semi-supervised landmark detection should resolve the taskoriented issue as well as the training-oriented issue. In this work, we propose an integrated 1-D and 2-D heatmap representation and an effective training strategy for semisupervised facial landmark detection, which effectively utilizes unlabeled data and tackles quantization errors at the same time."
        },
        {
            "heading": "III. METHODS",
            "text": ""
        },
        {
            "heading": "A. PRELIMINARY: FIXMATCH",
            "text": "According to the semi-supervised learning scenario, the training dataset can be partitioned into a labeled set {xs, ys} \u223c Ds and an unlabeled set {xu} \u223c Du. Here, xs and xu are image data and ys is a corresponding label of xs. Following the convention, the model learns Ds in the same way as a fully-supervised model. For unlabeled data Du, one of the common approaches is to extract the pseudo-label y\u0302u = PL(xu) via the pseudo-label extractor PL(\u00b7) and then guide the model training using {xu, y\u0302u}. Recently, FixMatch [10] shows impressive performance improvement in the semi-supervised image classification task. The method selectively uses pseudo-labels and exploits consistency regularization to handle unlabeled data. Specifically, FixMatch generates high-confidence one-hot pseudo-labels from weakly-augmented unlabeled data y\u0302u = PL(Tw(xu)). Then, it trains the model f\u03b8(\u00b7) parameterized by \u03b8 by mapping strongly-augmented unlabeled data Ts(xu) to y\u0302u, where Tw(\u00b7) and Ts(\u00b7) are weak and strong data augmentation polices, respectively. Finally, the supervised data loss Ls and the unsupervised data loss Lu of FixMatch are expressed as follows:\nLs = 1 |Ds| \u2211\n{xs,ys}\u223cDs\nd (f\u03b8(xs), ys), (1)\nLu = 1 |Du| \u2211\nxu\u223cDu\nd (f\u03b8(Ts(xu)), PL(Tw(xu))), (2)\nwhere d(\u00b7) is an error metric. The final objective L is,\nL = Ls + \u03bbu \u00b7 Lu, (3)\nwhere \u03bbu(\u2265 0) is a weighting parameter of the unsupervised loss. In the following, we develop the proposed model based on the semi-supervised learning framework of FixMatch for\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nhandling unlabeled data. (For fair comparison, the same FixMatch framework is employed for semi-supervised competitors.)\nB. MOTIVATION Semi-supervised facial landmark detection inherits two performance bottlenecks; (i) limited representation power due to low-resolution heatmap (i.e., task-oriented bottleneck), and (ii) noisy pseudo-labels in semi-supervised learning [7, 26] (i.e., training-oriented bottleneck). Presuming the solution for each issue does not affect the other, one may introduce a 1-D heatmap representation into the FixMatch framework, collectively choosing the state-of-the-art methods for each side. As shown in Figure 1, we observe that the 1-D heatmapbased method in the semi-supervised setting has less performance gain than the 2-D heatmap-based method, unlike the fully-supervised setting.\nFrom the counter-intuitive observation, we further investigate why the 2-D heatmap-based method is more suitable for the semi-supervised setting. For that, we compare the convergence trends under different fully-supervised settings based on HRNet [11], and semi-supervised settings based on FixMatch [10] using the same feature extractor [11]. Figure 2 shows that the 2-D heatmap-based method converges faster than the 1-D-based method. We conjecture that the different convergences are induced by the different levels of representation power. By definition, the 2-D heatmap inherently encodes the relationship between x-y coordinates and thus can reveal the relationship naturally in the heatmap output. On the other hand, the 1-D heatmap-based method ignores the dependency along x-y coordinates to simplify the representation. Although a co-attention module is used to restore their relationships, gaps in convergence speed are inevitable. That is, the 1-D heatmap-based method shows a slower convergence speed than that of the 2-D heatmap-based method.\nSlow convergence is critically negative in a semi-\nsupervised setting. Note that, at the early stage of the semisupervised training, the 2-D heatmap-based method provides more accurate pseudo-labels than those of the 1-D heatmapbased method. This large gap in the early training stage affects even the final performance, especially in the semisupervised training [27]. The importance of the early training stage is also discussed in Liu et al. [26]. Based on our experiments on convergence trends, we confirm that the stateof-the-art method for reducing the quantization error can provide negative feedback to the state-of-the-art semi-supervised framework. Our method eliminates negative feedback by enjoying fast convergence by 2-D representation and accurate performance by 1-D representation simultaneously. Details of the method will be discussed in the next section."
        },
        {
            "heading": "C. HYBRIDMATCH",
            "text": "We propose HybridMatch, utilizing both high-resolution 1- D and low-resolution 2-D heatmap representations. Our key motivation is to eliminate the negative effects between taskand training-oriented solutions in semi-supervised landmark detection, thus enjoying the advantages of both sides. Specifically, the high-resolution 1-D heatmap enables the model to reduce quantization errors. Meanwhile, the low-resolution 2- D heatmap provides more accurate pseudo-labels at the early stage of semi-supervised training. Figure 3 depicts the overall architecture for training unlabeled data. Our model is built upon the HRNet architecture. (i) It first regresses the 2-D heatmap supervised by 2-D pseudo-labels like HRNet. (ii) It estimates the 1-D heatmap via a 1-D heatmap regressor using the estimated 2-D heatmap. For the parameter updates, both the 1-D and 2-D pseudo-labels are used to update all parameters except for the 1-D heatmap regressor. Here, the parameters for the 1-D regressor are updated only with 1-D pseudo-labels. In this way, we enjoy the advantages of both 1-D and 2-D heatmaps because both heatmaps participate in the training loss.\nMotivated by our analysis of convergence trends in Section III-B, we rely more on feedback from the 2-D heatmap at the early training stage. For that, we have gradually increased the importance of 1-D heatmap feedback by controlling \u03bb1Du in Eqn. 5 as the training evolves. Finally, the total loss for the labeled data Ls and the unlabeled data Lu are written as follows:\nLs = L2Ds + \u03bb1Ds \u00b7 L1Ds , (4)\nLu = L2Du + \u03bb1Du \u00b7 L1Du , (5)\nwhere \u03bb1Ds is a constant weighting factor for L1Ds and \u03bb1Du is a varying weighting factor for L1Du . We use linear scheduling for \u03bb1Du in Eqn. 5, which is defined as follows:\n\u03bb1Du = { i/K \u00b7 \u03bb1D if i < K. \u03bb1D if K \u2264 i \u2264 I,\n(6)\nwhere i is the index of the current training iteration, I is the total iterations, and K is the end iteration when the linear"
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nInput image\nWeakly-augmented image\nStrongly-augmented image Backbone\nBackbone (Momentum net.)\n2-D heatmap prediction\n1-D heatmap prediction 1-D heatmap regressor (Momentum net.)\n1-D heatmap regressor2-D heatmap prediction\n\ud835\udcdb\ud835\udcdb\ud835\udc96\ud835\udc96\ud835\udfd0\ud835\udfd0\ud835\udfd0\ud835\udfd0 2-D pseudo label \ud835\udcdb\ud835\udcdb\ud835\udc96\ud835\udc96\ud835\udfcf\ud835\udfcf\ud835\udfd0\ud835\udfd0 1-D pseudo label\nMomentum updates\nMomentum updates\nFIGURE 3. Overall framework of HybridMatch for training the unlabeled data. L2Du and L 1D u are pseudo-labeling-based 2-D and 1-D heatmap regression feedback, respectively (see Eqn. 5).\nramp-up ends. With our adaptive training strategy, HybridMatch can receive high-quality feedback at the beginning of the training from the 2-D heatmap representation. Then, it finally has the advantage of high-resolution 1-D heatmap representation that helps reduce quantization errors.\nIn the following, we describe three training skills to further improve our model. They are data augmentation, a mean teacher framework, and confidence regularized hard pseudolabeling. Data augmentation. We adopt a data augmentation strategy from Sohn et al. [10] and make several modifications for facial landmark detection. Firstly, to ensure the pixel alignment between pseudo-labels from the weakly and strongly augmented image, we set Ts(\u00b7) = T \u2032s(Tw(\u00b7)), where T \u2032s only includes photometric transformation. By doing so, the geometric alignment between Tw(\u00b7) and Ts(\u00b7) is guaranteed. Specifically, we choose AutoAugment [28] followed by Cutout [29] except for rotation, shear, and translation for T \u2032s(\u00b7). Mean teacher framework. We employ a mean teacher framework [30], widely used in the semi-supervised scenario to improve the training stability and prediction quality. Instead of generating the pseudo-label y\u0302u = PL(Tw(xu)) using the model PL(\u00b7) = f\u03b8(\u00b7), we use the model PL(\u00b7) = f\u03d5(\u00b7), where \u03d5 is an exponential moving average of the previous values in \u03b8 throughout the optimization. The mean teacher framework is regarded as a temporal ensemble and leads to stable prediction without an expensive computing cost. Following the convention, the model f\u03d5 is used to obtain the pseudo-label of Tw(xu). The model f\u03b8 provides the predicted heatmap for Ts(xu). The parameters \u03b8 are updated using the loss between the predicted heatmaps and\nthe pseudo-labels. Pseudo-labeling. How to use the prediction f\u03d5(Tw(xu)) as the pseudo-label (i.e., whether to use soft- or hardlabel) remains an open question in semi-supervised learning. Many semi-supervised methods use a hard-label with confidence-based thresholding for entropy minimization [10, 31]. Confidence-based thresholding can also be applied to facial landmark detection. It selects a point of the highest intensity on the heatmap as the confidence score for the heatmap and then uses it if the confidence is greater than the threshold. However, it is non-trivial to choose an appropriate threshold value for different datasets. Furthermore, we observe that the optimal threshold value should change upon each landmark point. To bypass the reliability issue of thresholding, we use a hard-label with confidence regularization as follows:\nLmu = 1 |Du| \u2211\nxu\u223cDu\nAmconf \u00b7 d (fm\u03b8 (Ts(xu)), y\u0302mu )),\nLu = 1\nM M\u2211 m=1 Lmu , (7)\nwhere fm\u03b8 (\u00b7), Amconf and y\u0302mu are the mth heatmap prediction, its highest intensity value, and its pseudo-label. Here, xu is the unlabeled input and M is the number of landmarks. Since Aconf \u2208 [0, 1] is the highest value in the heatmap, it naturally penalizes the low-confidence heatmap (i.e., generally having low intensities) but promotes the high-confidence heatmap. In this way, we can focus on the high-confidence heatmap without additional parameters for thresholding.\nBy conducting experiments on various benchmark datasets, we confirmed that both soft-labels and hard-labels with confidence regularization are successful on datasets with\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\n\ud835\udc3a\ud835\udc3a\ud835\udc3a\ud835\udc3a 20% 10% 5% 50 10 \ud835\udc3a\ud835\udc3a\ud835\udc3a\ud835\udc3a 20% 10% 5% 50 10\n30 0- W W FL W A FL W\nFIGURE 4. Qualitative results on 300-W, AFLW, and WFLW datasets. The second to sixth columns depict predicted landmark points overlaid input images along with the labeled image ratio (%) or the number of images. GT indicates ground-truth landmark points overlaid images. Better viewed when zoomed in.\nsmall variations, such as human faces. Meanwhile, hardlabels with confidence regularization are more effective on datasets with large variations, such as caricature faces (see Section IV-F2). To achieve the generalized performances on various datasets, we use hard-labels with confidence regularization and then generate pseudo-labels from the prediction f\u03d5(Tw(xu)) in all experiments. To render hard-labels, we specifically apply the argmax operation on the heatmap to obtain coordinate information. Then, we transform the coordinate information to the heatmap by fitting the Gaussian distribution; it is a common protocol, thus identical to all other methods of producing the heatmap label from the coordinate label. We use the generated pseudo-labels as guidance for unlabeled data.\nIV. EXPERIMENTAL RESULTS A. DATASET 300-W is a semi-automatically annotated facial landmark dataset with 68 landmark points, including LFPW [32], AFW [33], HELEN [34], XM2VTS [35], and additional data [36]. We use the same data split as Ren et al. [37], which composes 3,148 training and 689 testing images (full). The test split consists of 135 images for a challenging subset and 554 images for a common subset. For the experiments, we report performances on challenging, common, and full testing sets. AFLW is a large-scale collection of annotated face images from Flickr, exhibiting a large variety of appearance (e.g., pose, expression, ethnicity, age, and gender). AFLW [38] contains 24,386 images, and we use splits of 20,000 images for training and 4,386 images for testing (full). The test split includes 1,165 images for a frontal subset. Following the convention as in [39], we use only 19 out of 21 annotated landmarks. WFLW [19] is a manually annotated facial landmark dataset with 98 landmark points, whose images are sourced from the WIDER FACE dataset [40]. WFLW contains 10,000 faces with 7,500 training images and 2,500 test images. The test split consists of several different test subsets, where each subset varies in the pose, illumination, expression, occlusion,\nmake-up, or blur. WebCari [41] is a large photograph-caricature dataset with 252 identities collected from the web. We composed the WebCari dataset for our work with only caricature images, which vary in artistic styles with 17 landmark points. WebCari dataset includes a total of 6,042 caricature images with 246 identities and we divide them into 3,942 training images and 2,100 test images. Unlike a real face dataset, the structural information and style information of each image vary significantly in this dataset."
        },
        {
            "heading": "B. EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "1) Network architecture",
            "text": "Our model is based on HRNetV2-W18 [11], which performs 2-D heatmap regression on the input. We follow the same model configuration as in Wang et al. [11]. In order to perform 1-D heatmap regression, we add 1-D heatmap regressor as suggested in [5] at the end of the last layer."
        },
        {
            "heading": "2) Implementation details",
            "text": "We follow the configuration from HRNetV2-W18 [11], which is widely used in facial landmark detection. All images are cropped and resized to 256\u00d7256. We choose random horizontal flipping (p = 0.5), random rotation (\u00b130\u25e6), and random scaling (\u00b125%) for weak data augmentation Tw(\u00b7). The total training epoch is 60. We use an Adam optimizer with a linear warmup. The learning rate is 0.0001, where the rate decreases by 0.1 times in the 30th and 50th epochs. The output resolution is 64 \u00d7 64 and 256 \u00d7 2 for the 2-D and 1-D heatmap, respectively. We randomly sample r% of data using a fixed seed value and report the best performance out of 3 runs in all experiments. Our experiments are carried out on NVIDIA Titan Xp GPUs. Curriculum learning. In Eqn. 6, we use linear scheduling for \u03bb1Du . For the hyper-parameters in the equation, we use K = 0.1I and \u03bb1D = \u03bb1Ds = 0.05 for the rest of the paper."
        },
        {
            "heading": "C. EVALUATION METRIC",
            "text": ""
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nMethod Setting 300-W AFLW WFLWCom. Chall. Full Full Frontal Full Pose Exp. Ill. Mk. Up Occ. Blur SA \u2020\nFSL (100%)\n3.21 6.49 3.86 - - 4.39 8.24 4.68 4.24 4.27 5.60 4.86 TS3 \u2020 3.17 6.41 3.78 - - - - - - - - - 3FabRec\u2020 3.36 5.74 3.82 1.84 1.59 5.62 10.23 6.09 5.55 5.68 6.92 6.38 HRNet \u2021 2.87 5.15 3.32 1.57 1.46 4.60 7.94 4.85 4.55 4.29 5.44 5.42 SAAT \u2021 2.87 5.03 3.29 - - - - - - - - - HybridMatch \u2020 SSL (20%) 2.99 5.04 3.40 1.61 1.50 4.73 7.86 5.01 4.59 4.33 5.46 5.44\nTABLE 1. Upper-bound performance on 300-W, AFLW, and WFLW datasets. \u201c\u2020\u201d and \u201c\u2021\u201d denote semi-supervised method and fully-supervised method, respectively. \u201c-\u201d denotes unavailable, {100%, 20%} indicate the labeled data ratio, and the results are NME\u2193 (%). We highlight that our HybridMatch achieves comparable performance to the HRNet using full labeled dataset."
        },
        {
            "heading": "1) NME",
            "text": "For facial landmark detection, the mean squared error has a significant limitation in that it neglects the scale of the face. Thus, the normalized mean squared error (NME) is widely adopted as an evaluation metric in the literature. NME includes a normalization factor of L, which is often defined as the distance between eyes and is defined as follows:\nNME = 1\nM M\u2211 m=1 ||pm1 \u2212 pm2 ||2 L , (8)\nwhere pm1 and p m 2 are landmark coordinate points and M is the number of landmarks. We use Inter-ocular norm for 300-W and WFLW datasets, which defines L as the outereye-corner distance for quantitative evaluations as following [6, 7]. For AFLW, we define L as the width of the square bounding-box following Zhu et al. [42]."
        },
        {
            "heading": "2) Failure rate and area under curve",
            "text": "Failure Rate (FRr) indicates the ratio of failed predictions out of the given images. We consider an image that has NME larger than the threshold r as a failed prediction. We use r = 10% as a threshold.\nArea Under Curve (AUC) computes area of the cumulative error distribution curve (CED(x) = 1\u2212 FRx). A larger AUC means higher accuracy and lower sensitivity to the threshold. We evaluate CED(x) for x \u2208 [0%, 10%].\nD. COMPARISON WITH STATE-OF-THE-ART Table 1 compares the upper-bound performances of existing semi-supervised methods and HRNet. The upperbound performances of SSL methods are computed with the model trained with the entire training data (100% supervision). From these comparisons, HRNet shows comparable or superior upper-bound performances over existing semisupervised models, thus our HybridMatch is implemented based on HRNetV2-W18. HRNet is denoted as FSL 2-D throughout this paper, and Figure 1 clearly shows that HybridMatch significantly improves the HRNet baseline.\nIn Table 2, we compare our method with state-of-the-art semi-supervised models on the 300-W dataset. Our HybridMatch outperforms the existing methods on all data ratios. Semi-supervised models are evaluated under 20%, 10%, 5%,\nand even extreme ratios such as 1.4% (50 samples) and 0.3% (10 samples). Our method is more robust against data ratio variation than the other methods; the effects of HybridMatch is more pronounced under harsh conditions. For example, 3FabRec shows a performance degradation of 0.93%p (full testing set) when it only uses 5% labeled data ratio, compared to the same model using full supervision. On the other hand, HybridMatch shows a performance degradation of 0.18%p under the same setting. It indicates that our method is less sensitive to the size of training set. Notably, given only 50 labeled samples, HybridMatch outperforms 3FabRec with 100% training data.\nTable 3 summarizes NME scores on the AFLW dataset. HybridMatch records the robust performances across varying data ratios on the AFLW dataset as it is on the 300-W. More importantly, even with only 1% annotation labels, we achieve 1.77/1.62 (full and frontal testing set), which is comparable to the accuracy of 3FabRec using full supervision (1.84/1.59).\nTable 4 shows NME scores on the WFLW, which is considered the most challenging dataset. Our HybridMatch still outperforms all existing methods with large gaps. Especially, SA reports the outstanding performance on the WFLW full testing set, even outperforming HRNetV2-W18 in the fully-supervised setting of Table 1. However, in the semisupervised setting, our method performs remarkably better than SA with a 2.35%p gain on 10% labeled data ratio.\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nMethod 20% 10% 5% 50 (1.4%) 10 (0.3%) RCN - 6.12 4.15 - 6.63 4.47 - 9.95 5.11 - - - - - - SA 3.85 - - 4.27 - - 6.32 - - - - - - - - TS3 4.31 7.97 5.03 4.67 9.26 5.64 - - - - - - - - -\n3FabRec 3.76 6.53 4.31 3.88 6.88 4.47 4.22 6.95 4.75 4.55 7.39 5.10 4.96 8.29 5.61 HybridMatch 2.99 5.04 3.40 3.03 5.09 3.44 3.10 5.16 3.50 3.30 5.64 3.76 3.70 6.37 4.22\nTABLE 2. Quantitative results of semi-supervised methods on 300-W dataset. Each column represents {common, challenging, full} testing set. The results are NME\u2193 (%).\nMethod 20% 10% 5% 1% 50 (0.25%) 10 (0.05%) RCN - - - - 2.17 - 2.88 - - - - - TS3 1.99 1.86 2.14 1.94 2.19 2.03 - - - - - -\n3FabRec 1.96 1.74 2.03 1.74 2.13 1.86 2.38 2.03 2.74 2.23 3.05 2.56 HybridMatch 1.61 1.50 1.66 1.56 1.68 1.57 1.77 1.62 1.92 1.73 2.29 2.18\nTABLE 3. Quantitative results of semi-supervised methods on AFLW dataset. Each column represents {full, frontal} testing set, respectively. The results are NME\u2193 (%).\nTo investigate the effectiveness of our HybridMatch, we compare the proposed method with other fully-supervised models using FR and AUC metrics. We evaluate the methods on the 300-W testset. Table 5 shows that our HybridMatch with only 20% of labeled data achieves the best results in both measurement. Our FR indicates that only one image out of the full 300-W testset has larger NME than the threshold. Furthermore, our HybridMatch outperforms 3FabRec in AUC at a high margin. This results indicates that our model shows accurate results with low deviation. Note that our HybridMatch depicts FR10% = 0.67 and AUC = 56.33 only with 50 labeled images training.\nE. QUALITATIVE RESULTS Figure 4 visualizes our landmark prediction results on the 300-W, AFLW, and WFLW datasets. Although NME in-\ncreases by reducing the labeled data, we observe that our predicted landmark points are sufficiently close to groundtruth landmark points. Besides, the predictions are generally robust against facial orientation, expression, and occlusion. In the second row on the left image in the WFLW dataset, we find that the predicted facial contour is imprecise when only 10 samples are labeled. This is because the labeled samples are extremely few, thus the model is incapable of learning challenging cases, such as occluded or rotated faces. In general, our model achieves compelling quality at 20% labeled data ratio, which is on par with the fully supervised model. Considering the impressive results and the computational efficiency, HybridMatch can serve as a good alternative to fully-supervised facial landmark detection.\nFigure 5 shows landmark prediction results on the WebCari dataset. Although WebCari is a challenging dataset with high variation (i.e., diverse shapes and textures), the proposed model successfully provides accurate predictions. In the image of the first row on the left, we observe that the predicted forehead point is inaccurate. However, since the corresponding subject is bald, the human also suffers from pinpointing the accurate forehead point. Considering the difficulty of the task, we conclude that our model is fairly robust against the highly varying dataset."
        },
        {
            "heading": "F. ABLATION STUDY",
            "text": ""
        },
        {
            "heading": "1) Effects of heatmap representation",
            "text": "Figure 1 and Table 6 exhibit the effects of the heatmap representations on the 300-W dataset. In a fully-supervised setting based on [11], the 1-D heatmap representation [5] (FSL 1-D) outperforms the 2-D heatmap representation [11] (FSL 2-D). This is expected because the high-resolution 1- D heatmap helps reduce quantization errors. However, in the semi-supervised setting based on [10], the 2-D heatmap provides more accurate pseudo-labels from the early training"
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nstage. Thus, the performance gap between the 2-D heatmapbased method (SSL 2-D) and the 1-D heatmap-based method (SSL 1-D) is significantly reduced. On the other hand, our method utilizes 1-D and 2-D heatmap representations simultaneously to have the advantages of both sides, improving the final performance.\n2) Effects of pseudo-labeling method\nTable 7 compares the accuracy of the pseudo-labeling methods. We observe that both the hard-labeling and softlabeling methods as PL(\u00b7) show no significant difference on the dataset with low geometric variation such as 300-W. However, we observe a considerable performance gap in high geometric variation datasets such as WebCari. Based on our observation, we conjecture that the soft pseudo-label under the dataset with high geometric variation tends to produce a low-confidence heatmap, which is blurry and distorted. Then, the key point no longer obeys the Gaussian distribution. This misleads the feedback from unlabeled data, thus resulting in performance degradation.\nG. MEMORY USAGE AND MODEL PARAMETERS Since HybridMatch uses both 1-D and 2-D heatmap, one might consider the increase in model parameters as a negative side effect. However, compared to the 1-D based model [5], our model only adds two convolutional layers with batch normalization (for regressing a 2-D heatmap), thus the increase in parameters is negligible. Furthermore, as our model provides feedback from the 2-D representation, we reduce the network size of the 1-D heatmap branch without much performance drop. As a result, the number of total weights of our model is 11.23M while the 1-D based model [5] is 16.44M. Our memory cost is much lower than that of the state-of-the-art semi-supervised model [6] (25.97M). Another computational factor is the memory capacity during\ntraining. Since the 2-D heatmap requires much more capacity, increasing the 2-D resolution can incur out-of-memory issue. Although our hybrid representation inherits the same memory capacity issue, adding the 1-D heatmap is marginal in terms of memory capacity. Overall, our hybrid representation does not consume many model parameters over the 1-D heatmap model [5] and memory capacity compared to the 2- D heatmap model [11]."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "We propose an effective semi-supervised facial landmark detection framework via hybrid representation, namely HybridMatch. This paper first identifies that we should consider both task-oriented (i.e., quantization error) and training-oriented (i.e., noisy pseudo-labels) issues simultaneously when tackling a semi-supervised landmark detection problem. To this end, we propose HybridMatch for simultaneously mitigating the performance bottlenecks caused by quantization error and noisy pseudo-labels. Specifically, our HybridMatch utilizes the high-resolution 1-D heatmap representation for reducing quantization error and the low-resolution 2-D heatmap for facilitating the fast convergence of semi-supervised learning. Extensive evaluations demonstrate the effectiveness of our HybridMatch and the outstanding performances, the new state-of-the-art accuracies for semi-supervised facial landmark detection on 300-W, AFLW, and WFLW datasets. Concretely, our method achieves 26.3% NME improvement over the existing method in 300-W full set at 5% data ratio. Our HybridMatch can capture more precise facial landmark points than existing methods with only a few annotated data (e.g., even when training with only 10 annotation labels). More importantly, HybridMatch achieves comparable performance, 2.99/5.04/3.40 (common, challenging and full testing set in 300-W) to the fully-supervised facial landmark detector (2.87/5.03/3.29) even using 20% labeled data.\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nREFERENCES [1] Aiden Nibali, Zhen He, Stuart Morgan, and Luke\nPrendergast. Numerical coordinate regression with convolutional neural networks. arXiv preprint arXiv:1801.07372, 2018. [2] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei. Integral human pose regression. In Proceedings of the European Conference on Computer Vision (ECCV), pages 529\u2013545, 2018. [3] Ying Tai, Yicong Liang, Xiaoming Liu, Lei Duan, Jilin Li, Chengjie Wang, Feiyue Huang, and Yu Chen. Towards highly accurate and stable face alignment for high-resolution videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8893\u20138900, 2019. [4] Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu. Distribution-aware coordinate representation for human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7093\u20137102, 2020. [5] Shi Yin, Shangfei Wang, Xiaoping Chen, Enhong Chen, and Cong Liang. Attentive one-dimensional heatmap regression for facial landmark detection and tracking. In Proceedings of the 28th ACM International Conference on Multimedia, pages 538\u2013546, 2020. [6] Bjorn Browatzki and Christian Wallraven. 3fabrec: Fast few-shot face alignment by reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6110\u20136120, 2020. [7] Xuanyi Dong and Yi Yang. Teacher supervises students how to learn from partially labeled images for facial landmark detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 783\u2013792, 2019. [8] Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent, Christopher Pal, and Jan Kautz. Improving landmark localization with semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1546\u20131555, 2018. [9] Shengju Qian, Keqiang Sun, Wayne Wu, Chen Qian, and Jiaya Jia. Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10153\u201310163, 2019. [10] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 596\u2013608. Curran Associates, Inc., 2020. [11] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu,\nMingkui Tan, Xinggang Wang, et al. Deep highresolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 2020.\n[12] Hao Liu, Jiwen Lu, Jianjiang Feng, and Jie Zhou. Two-stream transformer networks for video-based face alignment. IEEE transactions on pattern analysis and machine intelligence, 40(11):2546\u20132554, 2017. [13] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Learning deep representation for face alignment with auxiliary attributes. IEEE transactions on pattern analysis and machine intelligence, 38(5):918\u2013 930, 2015. [14] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks). In Proceedings of the IEEE International Conference on Computer Vision, pages 1021\u20131030, 2017. [15] Lisha Chen, Hui Su, and Qiang Ji. Deep structured prediction for facial landmark detection. Advances in Neural Information Processing Systems, 32:2450\u2013 2460, 2019. [16] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L Yuille, and Xiaogang Wang. Multi-context attention for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1831\u20131840, 2017. [17] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In European conference on computer vision, pages 483\u2013 499. Springer, 2016. [18] Ying Tai, Yicong Liang, Xiaoming Liu, Lei Duan, Jilin Li, Chengjie Wang, Feiyue Huang, and Yu Chen. Towards highly accurate and stable face alignment for high-resolution videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8893\u20138900, 2019. [19] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou. Look at boundary: A boundaryaware face alignment algorithm. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2129\u20132138, 2018. [20] Daniel Merget, Matthias Rock, and Gerhard Rigoll. Robust facial landmark detection via a fully-convolutional local-global context network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 781\u2013790, 2018. [21] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, and Qiang Zhou. Look at boundary: A boundaryaware face alignment algorithm. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2129\u20132138, 2018. [22] Meilu Zhu, Daming Shi, Mingjie Zheng, and Muhammad Sadiq. Robust facial landmark detection via occlusion-adaptive deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and"
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nPattern Recognition, pages 3486\u20133496, 2019. [23] Xinyao Wang, Liefeng Bo, and Li Fuxin. Adaptive\nwing loss for robust face alignment via heatmap regression. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6971\u2013 6981, 2019.\n[24] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, and Min Xu. Sparse local patch transformer for robust face alignment and landmarks inherent relation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4052\u20134061, 2022. [25] Xing Lan, Qinghao Hu, and Jian Cheng. Revisting quantization error in face alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1521\u20131530, 2021. [26] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20331\u201320342. Curran Associates, Inc., 2020. [27] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. arXiv preprint arXiv:2001.06001, 2020. [28] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 113\u2013123, 2019. [29] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. [30] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [31] Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. arXiv preprint arXiv:1706.05208, 2017. [32] Peter N Belhumeur, David W Jacobs, David J Kriegman, and Neeraj Kumar. Localizing parts of faces using a consensus of exemplars. IEEE transactions on pattern analysis and machine intelligence, 35(12):2930\u20132940, 2013. [33] Vuong Le, Jonathan Brandt, Zhe Lin, Lubomir Bourdev, and Thomas S Huang. Interactive facial feature localization. In European conference on computer vision, pages 679\u2013692. Springer, 2012. [34] Xiangxin Zhu and Deva Ramanan. Face detection, pose estimation, and landmark localization in the wild. In\n2012 IEEE conference on computer vision and pattern recognition, pages 2879\u20132886. IEEE, 2012.\n[35] Kieron Messer, Jiri Matas, Josef Kittler, Juergen Luettin, Gilbert Maitre, et al. Xm2vtsdb: The extended m2vts database. In Second international conference on audio and video-based biometric person authentication, volume 964, pages 965\u2013966. Citeseer, 1999. [36] Christos Sagonas, Epameinondas Antonakos, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: Database and results. Image and vision computing, 47:3\u201318, 2016. [37] Shaoqing Ren, Xudong Cao, Yichen Wei, and Jian Sun. Face alignment via regressing local binary features. IEEE Transactions on Image Processing, 25(3):1233\u2013 1245, 2016. [38] Martin Koestinger, Paul Wohlhart, Peter M Roth, and Horst Bischof. Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization. In 2011 IEEE international conference on computer vision workshops (ICCV workshops), pages 2144\u20132151. IEEE, 2011. [39] Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, and Xi Zhou. A deep regression architecture with two-stage re-initialization for high performance facial landmark detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3317\u20133326, 2017. [40] Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. Learning feature pyramids for human pose estimation. In proceedings of the IEEE international conference on computer vision, pages 1281\u2013 1290, 2017. [41] Jing Huo, Wenbin Li, Yinghuan Shi, Yang Gao, and Hujun Yin. Webcaricature: a benchmark for caricature recognition. In British Machine Vision Conference, 2018. [42] Shizhan Zhu, Cheng Li, Chen Change Loy, and Xiaoou Tang. Face alignment by coarse-to-fine shape searching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4998\u2013 5006, 2015. [43] Jiankang Deng, Qingshan Liu, Jing Yang, and Dacheng Tao. M3 csr: Multi-view, multi-scale and multicomponent cascade shape regression. Image and Vision Computing, 47:19\u201326, 2016. [44] Riza Alp Guler, George Trigeorgis, Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou, and Iasonas Kokkinos. Densereg: Fully convolutional dense shape regression in-the-wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6799\u20136808, 2017. [45] Jiankang Deng, George Trigeorgis, Yuxiang Zhou, and Stefanos Zafeiriou. Joint multi-view face alignment in the wild. IEEE Transactions on Image Processing, 28(7):3636\u20133648, 2019.\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKang et al.: HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations\nSEOUNGYOON KANG is currently a Ph.D. student at the School of Integrated Technology, Yonsei University, Korea and a contract researcher at the Kim Jaechul Graduate School of Artificial Intelligence, KAISTm Korea. He received his B.S. degree in electrical engineering from Yonsei University, Seoul, Korea, in 2017. His research interests are in computer vision and machine learning. In particular, he is interested in manipulating images using generative models, such as generative\nadversarial networks.\nMINHYUN LEE is currently a Ph.D. student at the School of Integrated Technology, Yonsei University, Korea. He received his B.S. degree in computer science from Yonsei University, Seoul, Korea, in 2017. His research interests are in computer vision and machine learning. In particular, he is interested in weakly-supervised semantic segmentation and active learning.\nMINJAE KIM is received the B.S. degree and Ph.D. degree from Korea University, South Korea, in 2007 and 2015, all in electrical engineering. His research interests are in image processing (super-resolution, multimodal image fusion) and computer vision. He was a Senior Researcher for autonomous driving technology at LG Electronics in 2015 to 2017. Dr. Kim joined NCSOFT Vision AI Lab in 2017 to engage research on machine learning and generative models.\nHYUNJUNG SHIM received her B.S. degree in electrical engineering from Yonsei University, Seoul, Korea, in 2002, and her M.S. and Ph.D. degrees in electrical and computer engineering from Carnegie Mellon University, Pittsburgh, PA, USA, in 2004 and 2008, respectively. She was with Samsung Advanced Institute of Technology, Samsung Electronics Company, Ltd., Suwon, Korea, from 2008 to 2013. She was with School of Integrated Technology, Yonsei University, Korea, from 2013\nto 2022. She is currently an associate Professor with the Kim Jaechul Graduate School of Artificial Intelligence, KAIST. Her research interests include generative models, deep neural networks, classification/recognition algorithms, 3-D vision, inverse rendering, face modeling, and medical image analysis."
        },
        {
            "heading": "12 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "HybridMatch: Semi-supervised Facial Landmark Detection via Hybrid Heatmap Representations",
    "year": 2023
}