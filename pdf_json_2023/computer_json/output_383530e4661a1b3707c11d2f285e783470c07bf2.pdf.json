{
    "abstractText": "Human-robot interaction (HRI) is a rapidly growing field that encompasses social and industrial applications. Machine learning plays a vital role in industrial HRI by enhancing the adaptability and autonomy of robots in complex environments. However, data privacy is a crucial concern in the interaction between humans and robots, as companies need to protect sensitive data while machine learning algorithms require access to large datasets. Federated Learning (FL) offers a solution by enabling the distributed training of models without sharing raw data. Despite extensive research on Federated learning (FL) for tasks such as natural language processing (NLP) and image classification, the question of how to use FL for HRI remains an open research problem. The traditional FL approach involves transmitting large neural network parameter matrices between the server and clients, which can lead to high communication costs and often becomes a bottleneck in FL. This paper proposes a communication-efficient FL framework for human-robot interaction (CEFHRI) to address the challenges of data heterogeneity and communication costs. The framework leverages pre-trained models and introduces a trainable spatiotemporal adapter for video understanding tasks in HRI. Experimental results on three human-robot interaction benchmark datasets: HRI30, InHARD, and COIN demonstrate the superiority of CEFHRI over full fine-tuning in terms of communication costs. The proposed methodology provides a secure and efficient approach to HRI federated learning, particularly in industrial environments with data privacy concerns and limited communication bandwidth. Our code is available at https://github.com/umarkhalidAI/ CEFHRI-Efficient-Federated-Learning.",
    "authors": [
        {
            "affiliations": [],
            "name": "Umar Khalid"
        },
        {
            "affiliations": [],
            "name": "Hasan Iqbal"
        },
        {
            "affiliations": [],
            "name": "Saeed Vahidian"
        },
        {
            "affiliations": [],
            "name": "Jing Hua"
        },
        {
            "affiliations": [],
            "name": "Chen Chen"
        }
    ],
    "id": "SP:8d3b7b6e8adca7e018cb8c57536025b62547be69",
    "references": [
        {
            "authors": [
                "C. Breazeal"
            ],
            "title": "Designing sociable robots",
            "year": 2004
        },
        {
            "authors": [
                "H. He",
                "S. Li",
                "X. Chen"
            ],
            "title": "Industrial robots: A survey on the recent progress and future directions",
            "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 6, no. 4, pp. 847\u2013863, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Kumar",
                "J. Kini",
                "A. Mian",
                "M. Shah"
            ],
            "title": "Self supervised learning for multiple object tracking in 3d point clouds",
            "venue": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2022, pp. 3754\u20133761.",
            "year": 2022
        },
        {
            "authors": [
                "H. Mahdi",
                "S.A. Akgun",
                "S. Saleh",
                "K. Dautenhahn"
            ],
            "title": "A survey on the design and evolution of social robots\u2014past, present and future",
            "venue": "Robotics and Autonomous Systems, p. 104193, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Choudhury",
                "G. Swamy",
                "D. Hadfield-Menell",
                "A.D. Dragan"
            ],
            "title": "On the utility of model learning in hri",
            "venue": "2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2019, pp. 317\u2013325.",
            "year": 2019
        },
        {
            "authors": [
                "N. Karim",
                "U. Khalid",
                "A. Esmaeili",
                "N. Rahnavard"
            ],
            "title": "Cnll: A semi-supervised approach for continual noisy label learning",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2022, pp. 3877\u20133887.",
            "year": 2022
        },
        {
            "authors": [
                "M. Mendieta",
                "T. Yang",
                "P. Wang",
                "M. Lee",
                "Z. Ding",
                "C. Chen"
            ],
            "title": "Local learning matters: Rethinking data heterogeneity in federated learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8397\u20138406, June 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Vahidian",
                "M. Morafah",
                "B. Lin"
            ],
            "title": "Personalized federated learning by structured and unstructured pruning under data heterogeneity",
            "venue": "IEEE ICDCS, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Li",
                "Y. Zhang",
                "L. Wang",
                "X. Zhang"
            ],
            "title": "Pre-trained federated learning: A comprehensive survey",
            "venue": "IEEE Communications Surveys & Tutorials, vol. 24, no. 1, pp. 702\u2013729, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Roitberg",
                "A. Perzylo",
                "N. Somani",
                "M. Giuliani",
                "M. Rickert",
                "A. Knoll"
            ],
            "title": "Human activity recognition in the context of industrial human-robot interaction",
            "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific. IEEE, 2014, pp. 1\u201310.",
            "year": 2014
        },
        {
            "authors": [
                "A. Hentout",
                "M. Aouache",
                "A. Maoudj",
                "I. Akli"
            ],
            "title": "Human\u2013robot interaction in industrial collaborative robotics: a literature review of the decade 2008\u20132017",
            "venue": "Advanced Robotics, vol. 33, no. 15-16, pp. 764\u2013799, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Iodice",
                "E. De Momi",
                "A. Ajoudani"
            ],
            "title": "Hri30: An action recognition dataset for industrial human-robot interaction",
            "venue": "2022 26th International Conference on Pattern Recognition (ICPR). IEEE, 2022, pp. 4941\u20134947.",
            "year": 2022
        },
        {
            "authors": [
                "J. Huang",
                "S. Guo",
                "J. Chen",
                "Y. Yang"
            ],
            "title": "Convolutional neural networks for hand gesture recognition in industrial human-robot interaction",
            "venue": "Journal of Intelligent Manufacturing, vol. 32, no. 5, pp. 1175\u20131187, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Rizzi",
                "E. Battaglia",
                "A. Marino",
                "A. Nava",
                "M. Matteucci",
                "V. Caglioti"
            ],
            "title": "Learning grasping affordances from visual cues for industrial robotics",
            "venue": "Robotics and Autonomous Systems, vol. 112, pp. 14\u201324, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "G. Spina",
                "P. Rocco",
                "S. Aldegheri",
                "D. Cattin",
                "M. Bicego",
                "E. Menegatti"
            ],
            "title": "Deep learning for visual perception of robotic manipulators in industrial environments",
            "venue": "Robotics and Computer-Integrated Manufacturing, vol. 64, p. 101927, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Liu",
                "J. Li",
                "Z. Shen",
                "G. Huang",
                "S. Yan",
                "C. Zhang"
            ],
            "title": "Structured pruning for efficient neural network inference",
            "venue": "International Conference on Learning Representations, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "N. Houlsby",
                "A. Giurgiu",
                "S. Jastrzebski",
                "B. Morrone",
                "Q. de Laroussilhe",
                "A. Gesmundo",
                "M. Attariyan",
                "S. Gelly"
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Machine, vol. abs/1902.00751, 2019.",
            "year": 1902
        },
        {
            "authors": [
                "T. Yang",
                "Y. Zhu",
                "Y. Xie",
                "A. Zhang",
                "C. Chen",
                "M. Li"
            ],
            "title": "Aim: Adapting image models for efficient video action recognition",
            "venue": "arXiv preprint arXiv:2302.03024, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Pan",
                "Z. Lin",
                "X. Zhu",
                "J. Shao",
                "H. Li"
            ],
            "title": "Parameter-Efficient Image-to-Video Transfer Learning",
            "venue": "June 2022, arXiv:2206.13559 [cs]. [Online]. Available: http://arxiv.org/abs/2206.13559",
            "year": 2022
        },
        {
            "authors": [
                "S. Chen",
                "C. Ge",
                "Z. Tong",
                "J. Wang",
                "Y. Song",
                "J. Wang",
                "P. Luo"
            ],
            "title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
            "venue": "arXiv preprint arXiv:2205.13535, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Lester",
                "R. Al-Rfou",
                "N. Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics, 2021, pp. 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "S. Vahidian",
                "M. Kuo",
                "C. Li",
                "R. Zhang",
                "G. Wang",
                "Y. Chen"
            ],
            "title": "Towards building the federated GPT: federated instruction tuning",
            "venue": "CoRR, vol. abs/2305.05644, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.05644",
            "year": 2023
        },
        {
            "authors": [
                "M. Jia",
                "L. Tang",
                "B.-C. Chen",
                "C. Cardie",
                "S. Belongie",
                "B. Hariharan",
                "S.-N. Lim"
            ],
            "title": "Visual Prompt Tuning",
            "venue": "July 2022, arXiv:2203.12119 [cs]. [Online]. Available: http://arxiv.org/abs/2203.12119",
            "year": 2022
        },
        {
            "authors": [
                "H. Cai",
                "C. Gan",
                "L. Zhu",
                "S. Han"
            ],
            "title": "TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning",
            "venue": "Advances in Neural Information Processing Systems, vol. 33. Curran Associates, Inc., 2020, pp. 11 285\u201311 297. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ 81f7acabd411274fcf65ce2070ed568a-Abstract.html",
            "year": 2020
        },
        {
            "authors": [
                "E.B. Zaken",
                "S. Ravfogel",
                "Y. Goldberg"
            ],
            "title": "Bitfit: Simple parameterefficient fine-tuning for transformer-based masked language-models",
            "venue": "arXiv preprint arXiv:2106.10199, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "Artificial intelligence and statistics. PMLR, 2017, pp. 1273\u20131282.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhao",
                "M. Li",
                "L. Lai",
                "N. Suda",
                "D. Civin",
                "V. Chandra"
            ],
            "title": "Federated learning with non-iid data",
            "venue": "arXiv preprint arXiv:1806.00582, 2018.",
            "year": 1806
        },
        {
            "authors": [
                "T. Li",
                "A.K. Sahu",
                "M. Zaheer",
                "M. Sanjabi",
                "A. Talwalkar",
                "V. Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine learning and systems, vol. 2, pp. 429\u2013450, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Li",
                "T. Wang",
                "K. Zhou",
                "Y. Niu",
                "J. Yang",
                "S. Hu"
            ],
            "title": "Fedadapt: Overcoming model degradation for federated learning with non-iid data",
            "venue": "Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2020, pp. 377\u2013392.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhao",
                "M. Li",
                "L. Lai",
                "N. Suda",
                "D. Civin",
                "V. Chandra"
            ],
            "title": "Federated learning with non-iid data",
            "venue": "arXiv preprint arXiv:1806.00582, 2018.",
            "year": 1806
        },
        {
            "authors": [
                "J. Wang",
                "Q. Liu",
                "H. Liang",
                "G. Joshi",
                "H.V. Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "Advances in Neural Information Processing Systems, vol. 33. Curran Associates, Inc., 2020, pp. 7611\u20137623.",
            "year": 2020
        },
        {
            "authors": [
                "S.P. Karimireddy",
                "S. Kale",
                "M. Mohri",
                "S. Reddi",
                "S. Stich",
                "A.T. Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "International Conference on Machine Learning. PMLR, 2020, pp. 5132\u20135143.",
            "year": 2020
        },
        {
            "authors": [
                "J.J. Gamboa-Montero",
                "F. Alonso-Martin",
                "S. Marques-Villarroya",
                "J. Sequeira",
                "M.A. Salichs"
            ],
            "title": "Asynchronous federated learning system for human\u2013robot touch interaction",
            "venue": "Expert Systems with Applications, vol. 211, p. 118510, 2023. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S0957417422015901",
            "year": 2023
        },
        {
            "authors": [
                "X. Su",
                "F. Yuan",
                "R. Zhang",
                "J. Liu",
                "M. Boltz",
                "X. Zhao"
            ],
            "title": "Deploying a human robot interaction model for dementia care in federated learning",
            "venue": "2022 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE), 2022, pp. 184\u2013185.",
            "year": 2022
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "June 2021, arXiv:2010.11929 [cs]. [Online]. Available: http://arxiv.org/abs/2010. 11929",
            "year": 2021
        },
        {
            "authors": [
                "S. Chen",
                "C. Ge",
                "Z. Tong",
                "J. Wang",
                "Y. Song",
                "J. Wang",
                "P. Luo"
            ],
            "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
            "venue": "May 2022, arXiv:2205.13535 [cs]. [Online]. Available: http://arxiv.org/abs/2205.13535",
            "year": 2022
        },
        {
            "authors": [
                "G. Xiao",
                "J. Lin",
                "S. Han"
            ],
            "title": "Offsite-tuning: Transfer learning without full model",
            "venue": "arXiv preprint arXiv:2302.04870, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "M.M. Naseer",
                "K. Ranasinghe",
                "S.H. Khan",
                "M. Hayat",
                "F. Shahbaz Khan",
                "M.-H. Yang"
            ],
            "title": "Intriguing properties of vision transformers",
            "venue": "Advances in Neural Information Processing Systems, vol. 34, pp. 23 296\u201323 308, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Kay",
                "J. Carreira",
                "K. Simonyan",
                "B. Zhang",
                "C. Hillier",
                "S. Vijayanarasimhan",
                "F. Viola",
                "T. Green",
                "T. Back",
                "P. Natsev"
            ],
            "title": "The kinetics human action video dataset",
            "venue": "arXiv preprint arXiv:1705.06950, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Dallel",
                "V. Havard",
                "D. Baudry",
                "X. Savatier"
            ],
            "title": "Inhard-industrial human action recognition dataset in the context of industrial collaborative robotics",
            "venue": "2020 IEEE International Conference on Human-Machine Systems (ICHMS). IEEE, 2020, pp. 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tang",
                "D. Ding",
                "Y. Rao",
                "Y. Zheng",
                "D. Zhang",
                "L. Zhao",
                "J. Lu",
                "J. Zhou"
            ],
            "title": "Coin: A large-scale dataset for comprehensive instructional video analysis",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1207\u20131216.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Tong",
                "Y. Song",
                "J. Wang",
                "L. Wang"
            ],
            "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pretraining",
            "venue": "arXiv preprint arXiv:2203.12602, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Ioffe",
                "C. Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "International conference on machine learning. PMLR, 2015, pp. 448\u2013456.",
            "year": 2015
        },
        {
            "authors": [
                "E.B. Zaken",
                "Y. Goldberg",
                "S. Ravfogel"
            ],
            "title": "Bitfit: Simple parameterefficient fine-tuning for transformer-based masked language-models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. 1\u20139.",
            "year": 2022
        },
        {
            "authors": [
                "S. Vahidian",
                "S. Kadaveru",
                "W. Baek",
                "W. Wang",
                "V. Kungurtsev",
                "C. Chen",
                "M. Shah",
                "B. Lin"
            ],
            "title": "When do curricula work in federated learning?",
            "venue": "CoRR, vol. abs/2212.12712,",
            "year": 2022
        },
        {
            "authors": [
                "H.-Y. Chen",
                "C.-H. Tu",
                "Z. Li",
                "H.-W. Shen",
                "W.-L. Chao"
            ],
            "title": "On Pre-Training for Federated Learning",
            "venue": "June 2022, arXiv:2206.11488 [cs]. [Online]. Available: http://arxiv.org/abs/2206.11488",
            "year": 2022
        },
        {
            "authors": [
                "J. Nguyen",
                "K. Malik",
                "M. Sanjabi",
                "M. Rabbat"
            ],
            "title": "Where to Begin? Exploring the Impact of Pre-Training and Initialization in Federated Learning",
            "venue": "June 2022, arXiv:2206.15387 [cs]. [Online]. Available: http://arxiv.org/abs/2206.15387",
            "year": 2022
        },
        {
            "authors": [
                "C. Feichtenhofer"
            ],
            "title": "X3d: Expanding architectures for efficient video recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 203\u2013213.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nHuman-robot interaction (HRI) is a rapidly growing field encompassing social [1] and industrial applications [2], [3]. In social settings, HRI involves interactions for entertainment, education, therapy, and personal assistance [4]. Conversely, Industrial HRI focuses on collaboration between humans and robots in industrial environments. Comprehending Industrial HRI is vital for the design and utilization of secure and efficient robotic systems in industrial environments, ultimately leading to increased productivity and optimized interaction between humans and robots. In the context of Industrial HRI, machine learning plays a crucial role in\n*This work is supported by the NSF/Intel Partnership on MLWiNS under Grant No. 2003198.\n1Center For Research in Computer Vision, University of Central Florida, Orlando, FL, USA umarkhalid@knights.ucf.edu and chen.chen@crcv.ucf.edu\n2Dept. of Computer Science, Wayne State University, Detroit, MI, USA hasan.iqbal.cs@wayne.edu and jinghua@wayne.edu\n3Dept. of Electrical and Computer Engineering, Duke University, Durham, NC, USA saeed.vahidian@duke.edu.\nfacilitating the adaptability of robots to ever-changing and intricate industrial environments [5]. This enhances their capability to execute tasks precisely and autonomously, while also reducing the potential for accidents and harm to human workers.\nRegarding the interaction between humans and robots in industrial settings, data privacy is an essential concern as companies need to protect sensitive data while machine learning algorithms often require access to large datasets to make accurate predictions [6]. Federated Learning (FL) [7] offers a solution by enabling distributed training without sharing raw data. FL ensures privacy while improving human-robot interaction and productivity. Despite its potential, FL faces challenges in remote locations with limited bandwidth, hindering communication and data transfer, impacting its widespread deployment and effective utilization [8]. The substantial communication costs in FL due to parameter/data transmission between clients and servers also present a bottleneck.\nOur Approach. To address the FL communication cost challenge, we propose a communication-efficient Federated Learning (FL) framework for Human-Robot Interaction (HRI) action recognition, named CEFHRI. We leverage pretrained video models and fine-tune a carefully designed adapter specifically for video understanding on HRI datasets. The CEFHRI framework addresses challenges related to data heterogeneity [9] and large communication costs in HRI. The study is the first to investigate communication efficiency in FL for video understanding in the context of HRI. The key contributions of this study can be summarized as follows:\n\u2022 This paper offers a pioneering study that systematically explores the effects of pre-training in the context of human-robot interaction (HRI) through FL. \u2022 The study introduces a parameter-efficient fine-tuning framework, CEFHRI, within the Vision Transformer architecture. CEFHRI addresses the challenges of data heterogeneity and large communication costs by using a lightweight trainable spatiotemporal adapter. \u2022 Furthermore, the proposed CEFHRI framework is evaluated for preserving model privacy on the server while achieving efficient transfer learning. \u2022 The proposed methodology is suitable for industrial environments prioritizing data privacy and facing communication and bandwidth constraints, providing a communication-efficient alternative for HRI federated learning. It can also be extended to other FL scenarios\nar X\niv :2\n30 8.\n14 96\n5v 1\n[ cs\n.C V\n] 2\n9 A\nug 2\n02 3\ninvolving video understanding tasks."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "a) Industrial Human-Robot Interaction: In recent years, industrial human-robot interaction (HRI) has emerged as a prominent research field, attracting considerable interest [10], [11]. However, there are a limited number of studies that have explored the computer vision domain specifically action recognition in industrial HRI settings [12]. One study by Huang et al. [13] proposes a model for gesture recognition that uses Convolutional Neural Networks (CNNs) to recognize hand gestures performed by human workers in industrial environments. Rizzi et al. [14] explores the use of deep learning to improve the performance of robotic grasping in industrial environments, by training a model to predict the optimal grasp point for objects based on visual cues. [15] applied deep learning using a combination of convolutional and recurrent neural networks to improve the perception of robots in an industrial setting, enabling them to recognize and locate objects on a conveyor belt.\nb) Efficient Fine-Tuning: [16] proposes a novel structured pruning method for parameter-efficient fine-tuning that preserves important network structures while discarding unimportant connections. [17] realizes transfer learning in NLP tasks with adapter modules, which adds a few trainable parameters per task while keeping the backbone frozen. Some recent works [18]\u2013[20] extend the adapter design to use the image foundation models for video understanding. [21], [22] propose prompt tuning for adapting language models. In the visual domain, [23] introduces visual prompt tuning as a highly efficient and effective approach for largescale Transformer models in vision. Another efficient finetuning alternative is bias-tuning [24], [25] which is a sparse fine-tuning method that only fine-tunes a subset of the bias terms of the model during training.\nc) Federated Learning: Federated learning is an innovative machine learning technique enabling model training\nacross numerous decentralized devices or servers, all while avoiding the need to transfer data to a central server [26], [27]. Until now, FedAvg [26] has been widely considered the primary benchmark for federated learning. In FedAvg, the weight aggregation is performed by averaging the model weights obtained from different devices. FedAvg works well when the data is homogenous. However, in heterogeneous environments, the global model that is suitable for all clients faces convergence challenges. FedProx[28], FedAdapt [29], [30], FedNova [31] and SCAFFOLD [32] are some enhancements proposed to FedAvg which have attempted to develop modified versions of the algorithm that can handle non-IID data. In our evaluation, we establish that FedAvg has the tendency to achieve satisfactory performance in the heterogeneous environment given a pre-trained foundation model.\nAlthough there are some recent works on FL for HRI [33], [34], no study has yet explored HRI for action recognition. Our research constitutes the initial investigation into the video understanding of HRI within Federated Learning (FL) settings. Additionally, this study serves as a fundamental basis for the utilization of parametric-efficient fine-tuning in conjunction with pre-trained models for the purpose of video comprehension within FL."
        },
        {
            "heading": "III. EFFICIENT FEDERATED LEARNING FOR HUMAN",
            "text": "ROBOT INTERACTION RECOGNITION"
        },
        {
            "heading": "A. Federated Learning in HRI Action Recognition",
            "text": "Industrial HRI action recognition is a multi-class classification task where, given T training samples of a video dataset XT = {(xt, yt)Tt=1}, with y \u2208 {0, 1, .., C\u22121} for C classes, the goal is to achieve accurate classification on a set of unseen videos Xu = {x1, ..., xM}, where M is the number of videos in the test set. To set up the HRI recognition task in the FL setting, we assume a system of N clients that can coordinate with the centralized server without sharing their\nlocal data. Further, let X be a subset of Rp representing the instance space, Z a subset of Rd denoting the latent feature space and Y a subset of R representing the output space. The server model F , parameterized by \u0398 := [\u03b8f ;\u03b8p], comprises two components: a feature extractor f : X \u2192 Z parameterized by \u03b8f , and a predictor p : Z \u2192 \u2206Y parameterized by \u03b8p, where \u2206Y is the simplex over Y .\nIn each communication round, the server randomly selects a subset of available clients Sr \u2208 [N] with |Sr| = n and broadcasts the model to all clients. The clients, upon receiving the model from the server, perform several steps of stochastic gradient descent (SGD) updates on their local training data Tk \u2282 XT , obtain the updated model, and send their model parameters \u0398 := [\u03b8fk ;\u03b8 p k] n k=1 back to the server.\nAssuming the global model is initialized by F (\u0398), the clients minimize their loss in each round r using SGD training as follows:\nmin \u0398\nF (r) k (\u0398) =\n1\n|Tk| |Tk|\u2211 t=1 \u2113k(\u0398, xt), (1)\nwhere \u2113 is the loss function, r is the communication round, and |Tk| represents the number of the training samples of the kth client. The server collects all the parameter updates from clients and conducts model averaging as [26]\n\u0398(r+1) = n\u2211 k=1 |Tk|\u2211n i=1 |Ti| \u0398 (r) k . (2)\nThis parameter exchange between the clients and server goes on from r = 0 to r = R \u2212 1 until the convergence of the global model."
        },
        {
            "heading": "B. Problem definition",
            "text": "As stated in Section III-A, the global model convergence in FL is accomplished after several rounds of parameter exchange between the local nodes and the global server. The procedure is repeated for r rounds. In this paper, we let C represent the communication cost associated with the FedAvg baseline. This cost is directly related to the number of parameters shared by the clients as C \u221d |\u03b8| \u00b7 |Sr|, where \u03b8 \u2286 \u0398 the parameters that need to be transmitted. We aim to minimize C without an accuracy drop for the HRI recognition task. As the communication cost is directly related to the number of trainable parameters, one intuitive method is to freeze the backbone \u03b8f and only train the penultimate linear layer \u03b8p. However, the drawbacks of linear probing in terms of performance, which will be discussed in Section IV-B, have prompted us to introduce our federated learning framework for human-robot interaction recognition (CEFHRI) which keeps tunable parameters, \u03b8 << \u0398 using Vision Tansformers [35]."
        },
        {
            "heading": "C. Proposed Framework",
            "text": "In this section, we provide a succinct overview of the Vision Transformer (ViT) and its application in the video\nunderstanding task of FL followed by an introduction to spatial-temporal adaptation. We further discuss the utility of such adaptation to preserve server privacy.\na) Overview: The transformer architecture [35] has been extensively utilized in vision applications such as video surveillance and action recognition. In this work, we investigate the role of FL in recognizing human-robot interaction which is another video understanding task. To this end, we introduce CEFHRI, an FL framework that employs the vision transformer for efficient decentralized HRI learning. Within the CEFHRI framework, the local and global models are customized by inserting an adapter module. Taking inspiration from the adapter design in [19], [36], we propose a variant of spatio-temporal adapter [19] architecture, called the ST-Adapter specifically designed for a video pre-trained model.\nb) Adapter Design: The designed adapter architecture allows for the preservation of both the spatial and temporal characteristics of videos, while simultaneously facilitating efficient fine-tuning. The present discourse will concentrate on creating adapters for deep transformer backbones. In particular, we have introduced a depth-wise 3D convolu-\ntion layer in a standard configuration, placed between the bottlenecks of the vanilla adapter [36] and parallel to the MLP block. The visual representation of this arrangement is depicted in the accompanying Fig. 2.\nSince the ST-Adapter branch is inserted alongside the MLP block, we term the adapted MLP block as the STadapted MLP and its output as X3D. The 3D branch will transform the features in the following manner:\nX\u0303i = \u03b3 \u00b7 ((DWConv3D(NL(X\u0302i) \u00b7 WDOWN )) \u00b7WUP ) (3)\nwhere DWconv3D represents the depth-wise 3Dconvolution. Here, \u03b3 is a tuneable parameter to scale the adapter output as in [36]. Before applying DWConv3D, the features are reshaped from X\u2032i \u2208 RN\u00d7d\u0302 to X\u2032\u2032i \u2208 RD\u00d7H\u00d7W\u00d7d\u0302, where X \u2032 = NL(X\u0302i). X\u0303i features are then fused with the MLP branch output features to generate X3D,\nX3Di = MLP(NL(X\u0302i)) + X\u0303i (4) Further, X\u0302 features are fused with X3D using a residual connection as following,\nXi = X3Di + X\u0302i (5)\nWe provide a detailed comparison of the proposed adapter design with other parametric-efficient finetuning techniques in Section IV-B under various FL settings. We further evaluate the existing adapter designs from the literature in Section V.\nAlgorithm 1 The CEFHRI Framework\nServer: initialize the foundation model F parametrized by \u0398, and the compressed model F c with \u0398c Clients: fetch the compressed model weights \u0398c, and initialize the tunable adapter weights, \u03b8 Require: N clients, sampling rate R \u2208 (0, 1], communication round budget R, shared weight \u03b8 \u2286 \u0398. for each round r = 0, 1, 2, ..., R\u2212 1 do\nSr \u2190 Server samples |S| clients from N clients\nClient update (k,\u0398c,\u03b8(r)): for each client k \u2208 Sr do\nFk \u2190 {\u0398c,\u03b8(r)k } training Fk by SGD on Dk for E epochs \u03b8 (r+1) k \u2190 \u03b8 (r) k \u2212 \u03b7\u2207Fk return \u03b8(r+1)k to server\nServer executes: for each client k \u2208 Sr do\n\u03b8 (r+1) k \u2190 ClientUpdate(k,\u0398 c,\u03b8(r)) \u03b8(r+1) \u2190 aggregate updated parameters \u03b8(r+1)k by clients as in Eq. 2\nc) Server-Side Privacy: We have additionally expanded the applicability of the proposed adapter design to a situation wherein the foundation model owner, i.e., the server is precluded from disclosing their model to the data owner [37]. To overcome this challenge, we devise a strategy to fine-tune\nthe proposed adapter on the data owner\u2019s dataset without accessing the complete pre-trained model weights. In order to safeguard model ownership while simultaneously enhancing efficiency, we have implemented a technique known as lossy compression on the frozen backbone model as shown in Fig. 3. We leverage the findings of [38] which reveal that the discriminative information crucial for accurate classification is predominantly captured within the class tokens of the final few blocks. Specifically, we have selectively dropped a few layers from the model to produce a compressed variant, an adapter counselor, F c with parameters \u0398c such that \u0398c < \u0398. The purpose of utilizing F c is to furnish approximate gradient directions to update the adapters, while simultaneously maintaining similarity to the original frozen component weights, \u0398. Nonetheless, it is imperative that the F c precision be restrained, as a higher degree of accuracy could potentially divulge information regarding the original model. Furthermore, a smaller F c size facilitates a more efficient fine-tuning process for downstream users. We report our results for various compression ratios in Section V. The outline of the proposed framework with client data privacy and server model privacy has been described in Algorithm 1."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "We evaluate CEFHRI for downstream human-robot interaction tasks across a wide range of FL settings. The experimental setup is described in Section IV-A, and the effectiveness of CEFHRI is demonstrated in Section IV-B."
        },
        {
            "heading": "A. Experiment Setup",
            "text": "a) Architecture and Datasets: In our experiments, we use Kinetics-400 [39] dataset for pre-training. For the downstream FL task, we select three datasets with varying degrees of domain gap as compared to Kinetics-400: (i) InHARD [40], (ii) HRI30 [12] and (iii) COIN Dataset [41]. We adopted experimental settings from [36], [42]. We use plain ViT-B/16 [35] with supervised pre-train weights from VideoMAE [42] official repository, where pre-trained checkpoints on Kinetics-400 [39] dataset are publicly available. An extra BatchNorm layer [43] without affine transformation is inserted before the penultimate layer as in [36]. For all datasets, we use 8 frames with a temporal stride of 4. The tubelet size is set to 2 as in VideoMAE\u2019s default settings [42]. The VideoMAE codebase includes a downsampling layer that converts original frames based on the tubelet size ratio. Hence, the final number of tokens for the transformer block is 4 \u00d7 14 \u00d7 14, where 4 is the downsampled number of frames while the original input video has 8 frames.\nb) Evaluation Metrics: We compare the proposed CEFHRI with four commonly used fine-tuning baselines: (1) Full Fine-tuning, (2) Linear Probing (fine-tuning classification head only), (3) Bias-Tuning, and (4) Prompt-Tuning. Here full fine-tuning indicates that all the parameters of the foundation model are fine-tuned on the downstream dataset. Linear probing indicates that\nonly the penultimate linear layer is fine-tuned. Biastuning [24], [44] aims to fine-tune only the bias parameters for the downstream task while prompt-tuning [21], [23] concatenates prompt tokens to the input embedding, where each prompt token is a learnable d-dimensional vector. The baselines are compared with CEFHRI with respect to two evaluation criteria (i) Communication Efficiency, (ii) Human-Robot Interaction recognition.\nc) Training Settings: We distribute the training data between clients based on the Dirichlet distribution (with \u03b1 \u2208 {0.1, 0.5, 1.0}) to achieve heterogeneous data partitioning across clients. Lower \u03b1 indicates a higher degree of data heterogeneity [45]. In all experiments, we assume N = 16 clients are available, and we set the sampling rate to 25%, and 100% which means either |Sr| = 4 or |Sr| = 16 in each round. Each client performs E = 8 local epochs with a batch size of 8 before global aggregation is performed for R = 40 communication rounds. We keep the model weights frozen during the fine-tuning of CEFHRI techniques. We set \u03b3 = 2.5, while the bottleneck dimension, d\u0302 = 64 in our default settings for the adapter. We followed the training settings of [20] except the learning rate which we selected as 0.001. For prompt-tuning, the number of introduced tokens is set to 8 based on optimal performance as mentioned in [36]. Any modification to the default settings will be clearly stated."
        },
        {
            "heading": "B. Main Results",
            "text": "The results reported in this section ensure the client\u2019s data privacy with the assumption that the client has access to the non-compressed version of the pre-trained model.\na) Human-Robot Interaction Recognition: Table I displays the interaction recognition results of the CEFHRI framework. Our assessment shows that the CEFHRI framework surpasses all other approaches, except for full finetuning, across all datasets and FL configurations. As reported in Table I, the CEFHRI framework achieves satisfactory adaptation performance despite having significantly fewer trainable parameters than full fine-tuning. Moreover, the results presented in Table I indicate that irrespective of the value of \u03b1, there is no substantial difference in the performance gap for CEFHRI. This highlights the crucial role of the pre-trained model in mitigating the impact of\nheterogeneous data. These findings are in agreement with recent studies conducted on image classification tasks [46], [47].\nb) Communication Cost: In this paper, we only consider the uploading (i.e., from clients to server) communication cost, C, for FedAvg [26] baseline defined as C = R \u00d7 |\u03b8| \u00d7 |Sr|\u00d7 4B, where R stands for the total number of communication rounds; |Sr| denotes the number of participating clients, and |\u03b8| \u2286 |\u0398| represents the subset of the total number of model parameters1 that are exchanged between each client and server in each round, with each parameter occupying 4 bytes (4B) of storage. With this in mind, we report the communication efficiency results against the pre-specified target accuracy for each dataset in Table II. Upon comparison to full fine-tuning, it becomes evident that the CEFHRI framework can achieve the target accuracy with a significantly reduced communication cost of at least \u2248 35\u00d7 lower. Despite the linear probing approach having the fewest tunable parameters, it falls short of attaining the target accuracy for any of the datasets being examined. Fig. 4 further illustrates the performance vs cost tradeoff between the baselines and CEFHRI. This suggests that the proposed parameter-efficient fine-tuning prototype of the CEFHRI framework is efficacious in mitigating communication overhead, without compromising recognition performance.\n1Each dataset has different # of classes, so the # of model parameters are different owing to varying linear classification head parameters.\nc) CEFHRI vs Parametric Efficient Model: One straightforward solution to reduce the communication cost is to use tiny video networks such as the X3D-S model [48]. Nevertheless, the architectures with low capacity are incapable of achieving satisfactory performance even with pre-training, as is evidenced in Table III. In particular, X3D-S [48] could only achieve 30.8% HRI recognition performance on HRI30 [12], while CEFHRI yields 84.6% accuracy for \u03b1 = 0.5. Our comprehension is that models with low capacity are not effective in producing desirable performance under data heterogeneity scenarios. Consequently, the CEFHRI approach offers a practical remedy for mitigating the decrease in performance while still keeping the communication cost as minimal as that of smaller models, such as X3D-S, for the purpose of HRI recognition."
        },
        {
            "heading": "V. ABLATION STUDIES",
            "text": "a) Impact of Model Initialization: To demonstrate the applicability of the pre-trained model in the CEFHRI framework, we undertake a more in-depth examination of two fundamental questions: (1) How does the model initialization impact the industrial HRI action recognition performance in FL settings? (2) To what extent can pre-training alleviate the accuracy drop caused by the heterogeneity of data from clients in the context of industrial HRI action recognition?\nOur findings show that using a pre-trained model can drastically improve the performance of industrial HRI action recognition in FL. Fig. 5 demonstrates the significant advantage of using a pre-trained model over training from scratch for the InHARD [40], and HRI30 [12] datasets. It is evident that the server achieves notable performance within a fewer number of rounds. This is in contrast to random initialization, which is unable to narrow the performance gap even after 100 rounds for heterogeneous FL settings. Our results indicate that using pre-trained models not only outperforms\nrandom initialization but also effectively mitigates the data heterogeneity effect in the industrial HRI action recognition task.\nb) Performance Gain of the Proposed Adapter Design: CEFHRI adapter is different in design from [20] and distinct in application from [19]. Firstly, the ST-adapter proposed by [19] is specifically designed for frozen CLIP models, and its effectiveness in FL has not been explored. Secondly, the sequential implementation doesn\u2019t maintain the original features achieved by the frozen backbone. We argue that the parallel design of the adapter maintains the original features as the adapter branch is added parallel to the original MLP block and only scaled feature aggregation is performed. As shown in Table IV, our analysis indicates that the parallel adapter structure outperforms the sequential design. As a result, we use the parallel ST-adapter design by default for the CEFHRI prototype, which distinguishes it structurally from that of [36].\nc) Performance while Preserving Server Privacy: In order to protect the privacy of the foundation model with l layers, we implement a model compression strategy by\nremoving a specified number of layers, n from the ViTB/16 [35] model. The resulting compressed model is finetuned using knowledge distillation, with mean square error, under the supervision of the original model using Kinetics400 [39] dataset for 20 epochs. The fine-tuned compressed model is then distributed to the clients as a point of reference. The clients then use this reference model to train the l \u2212 n adapters, which are later inserted into the server model\u2019s last l \u2212 n layers. The results of our experiments indicate that knowledge distillation was essential in attaining the best performance. We also find that dropping the last n layers produced superior results compared to dropping the first n layers of the transformer model, which is consistent with [38]. Therefore, the reported results in Table V are generated using the strategy of dropping the last n layers for model compression. Here, we can observe that as the # of removed layers is more than 3, the performance drop is significant."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In this research, we present a new FL framework called CEFHRI, which aims to improve the performance of humanrobot interaction recognition tasks in industrial settings through the utilization of pre-trained video models. To mitigate the communication overhead that is commonly encountered in FL systems, the CEFHRI framework proposes a parameter-efficient fine-tuning prototype. We conduct a comprehensive evaluation of the CEFHRI framework by comparing its performance against other baselines with regard to both recognition for human-robot interaction and communication cost. Our findings indicate that the CEFHRI framework not only effectively addresses the communication bottleneck issue but also outperforms other FL fine-tuning techniques, such as linear probing and bias-tuning. Additionally, our results show that the proposed CEFHRI adapter performs satisfactorily in scenarios where the downstream dataset has a major domain shift compared to the pre-trained dataset. In conclusion, our work sheds new light on the potential for improving communication efficiency in FL for video understanding tasks and lays the groundwork for future advancements in this area."
        }
    ],
    "title": "CEFHRI: A Communication Efficient Federated Learning Framework for Recognizing Industrial Human-Robot Interaction",
    "year": 2023
}