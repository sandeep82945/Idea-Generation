{
    "abstractText": "Recent years have seen a shift from a pattern mining process that has users define constraints before-hand, and sift through the results afterwards, to an interactive one. This new framework depends on exploiting user feedback to learn a quality function for patterns. Existing approaches have a weakness in that they use static pre-defined low-level features, and attempt to learn independent weights representing their importance to the user. As an alternative, we propose to work with more complex features that are derived directly from the pattern ranking imposed by the user. Learned weights are then aggregated onto lower-level features and help to drive the quality function in the right direction. We explore the effect of different parameter choices experimentally and find that using higher-complexity features leads to the selection of patterns that are better aligned with a hidden quality function while not adding significantly to the run times of the method. Getting good user feedback requires to quickly present diverse patterns, something that we achieve but pushing an existing diversity constraint into the sampling component of the 1 ar X iv :2 20 4. 04 24 2v 1 [ cs .A I] 8 A pr 2 02 2 Springer Nature 2021 LTEX template 2 Exploiting complex pattern features for interactive pattern mining interactive mining system LetSIP. Resulting patterns allow in most cases to converge to a good solution more quickly. Combining the two improvements, finally, leads to an algorithm showing clear advantages over the existing state-of-the-art.",
    "authors": [
        {
            "affiliations": [],
            "name": "Arnold Hien"
        },
        {
            "affiliations": [],
            "name": "Samir Loudni"
        },
        {
            "affiliations": [],
            "name": "Noureddine Aribi"
        },
        {
            "affiliations": [],
            "name": "Abdelkader Ouali"
        },
        {
            "affiliations": [],
            "name": "Albrecht Zimmermann"
        }
    ],
    "id": "SP:842770744a4ad7e2ea88ae9ca2323ab5a701f7ec",
    "references": [
        {
            "authors": [
                "R. Agrawal",
                "R. Srikant"
            ],
            "title": "Fast algorithms for mining association rules in large databases",
            "venue": "Proceedings of the 20th International Conference on Very Large Data Bases. VLDB",
            "year": 1994
        },
        {
            "authors": [
                "Tan",
                "P.-N",
                "V. Kumar",
                "J. Srivastava"
            ],
            "title": "Selecting the right interestingness measure for association patterns",
            "venue": "KDD, pp",
            "year": 2002
        },
        {
            "authors": [
                "N. Pasquier",
                "Y. Bastide",
                "R. Taouil",
                "L. Lakhal"
            ],
            "title": "Discovering frequent closed itemsets for association",
            "venue": "rules. In: ICDT,",
            "year": 1999
        },
        {
            "authors": [
                "L. De Raedt",
                "A. Zimmermann"
            ],
            "title": "Constraint-based pattern set mining",
            "year": 2007
        },
        {
            "authors": [
                "S. R\u00fcping"
            ],
            "title": "Ranking interesting subgroups",
            "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,",
            "year": 2009
        },
        {
            "authors": [
                "V. Dzyuba",
                "M. van Leeuwen"
            ],
            "title": "Learning what matters - sampling interesting patterns",
            "venue": "PAKDD 2017,",
            "year": 2017
        },
        {
            "authors": [
                "M. Bhuiyan",
                "M.A. Hasan"
            ],
            "title": "Interactive knowledge discovery from hidden data through sampling of frequent patterns",
            "venue": "Stat. Anal. Data Min. 9(4),",
            "year": 2016
        },
        {
            "authors": [
                "V. Dzyuba",
                "M. van Leeuwen",
                "L.D. Raedt"
            ],
            "title": "Flexible constrained sampling with guarantees for pattern mining",
            "venue": "Data Min. Knowl. Discov. 31(5),",
            "year": 2017
        },
        {
            "authors": [
                "M. Belaid",
                "C. Bessiere",
                "N. Lazaar"
            ],
            "title": "Constraint programming for mining borders of frequent itemsets",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "N. Lazaar",
                "Y. Lebbah",
                "S. Loudni",
                "M. Maamar",
                "V. Lemi\u00e8re",
                "C. Bessiere",
                "P. Boizumault"
            ],
            "title": "A global constraint for closed frequent pattern mining",
            "venue": "Proceedings. Lecture Notes in Computer Science,",
            "year": 2016
        },
        {
            "authors": [
                "L.D. Raedt",
                "T. Guns",
                "S. Nijssen"
            ],
            "title": "Constraint programming for itemset mining",
            "venue": "Proceedings of the 14th ACM SIGKDD",
            "year": 2008
        },
        {
            "authors": [
                "P. Schaus",
                "J.O.R. Aoga",
                "T. Guns"
            ],
            "title": "Coversize: A global constraint for frequency-based itemset mining",
            "venue": "Proceedings of the 23rd CP 2017,",
            "year": 2017
        },
        {
            "authors": [
                "M. Khiari",
                "P. Boizumault",
                "B. Cr\u00e9milleux"
            ],
            "title": "Constraint programming for mining n-ary patterns",
            "venue": "Principles and Practice of Constraint Programming - CP 2010 - 16th International Conference,",
            "year": 2010
        },
        {
            "authors": [
                "A. Hien",
                "S. Loudni",
                "N. Aribi",
                "Y. Lebbah",
                "M.E.A. Laghzaoui",
                "A. Ouali",
                "A. Zimmermann"
            ],
            "title": "A relaxation-based approach for mining diverse closed patterns",
            "venue": "European Conference, ECML PKDD 2020,",
            "year": 2020
        },
        {
            "authors": [
                "K.S. Meel",
                "M.Y. Vardi",
                "S. Chakraborty",
                "D.J. Fremont",
                "S.A. Seshia",
                "D. Fried",
                "A. Ivrii",
                "S. Malik"
            ],
            "title": "Constrained sampling and counting: Universal hashing meets SAT solving",
            "venue": "Beyond NP, Papers from the 2016 AAAI Workshop,",
            "year": 2016
        },
        {
            "authors": [
                "S. Chakraborty",
                "D.J. Fremont",
                "K.S. Meel",
                "S.A. Seshia",
                "M.Y. Vardi"
            ],
            "title": "Distribution-aware sampling and weighted model counting for SAT",
            "venue": "CoRR abs/1404.2984",
            "year": 2014
        },
        {
            "authors": [
                "C.P. Gomes",
                "W.J. van Hoeve",
                "A. Sabharwal",
                "B. Selman"
            ],
            "title": "Counting CSP solutions using generalized XOR constraints",
            "venue": "Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-",
            "year": 2007
        },
        {
            "authors": [
                "S. Morishita",
                "J. Sese"
            ],
            "title": "Traversing itemset lattices with statistical metric pruning",
            "venue": "Proceedings of the Nineteenth ACM SIGACT-SIGMOD- SIGART Symposium,",
            "year": 2000
        },
        {
            "authors": [
                "S. Arora",
                "E. Hazan",
                "S. Kale"
            ],
            "title": "The multiplicative weights update method: Springer Nature 2021 LTEX template 32 Exploiting complex pattern features for interactive pattern mining a meta-algorithm and applications",
            "venue": "Theory Comput",
            "year": 2012
        },
        {
            "authors": [
                "V. Dzyuba",
                "M. van Leeuwen",
                "S. Nijssen",
                "L.D. Raedt"
            ],
            "title": "Interactive learning of pattern rankings",
            "venue": "International Journal on Artificial Intelligence Tools",
            "year": 2014
        },
        {
            "authors": [
                "Boulicaut",
                "J.-F",
                "B. Jeudy"
            ],
            "title": "Mining free itemsets under constraints",
            "venue": "Proceedings 2001 International Database Engineering and Applications Symposium,",
            "year": 2001
        },
        {
            "authors": [
                "T. Calders",
                "B. Goethals"
            ],
            "title": "Mining all non-derivable frequent itemsets",
            "venue": "Proceedings of the 6th European Conference on Principles of Data Mining and Knowledge Discovery",
            "year": 2002
        },
        {
            "authors": [
                "J. Wang",
                "J. Han",
                "Y. Lu",
                "P. Tzvetkov"
            ],
            "title": "TFP: an efficient algorithm for mining top-k frequent closed itemsets",
            "venue": "IEEE Trans. Knowl. Data Eng. 17(5),",
            "year": 2005
        },
        {
            "authors": [
                "B. Bringmann",
                "A. Zimmermann"
            ],
            "title": "The chosen few: On identifying valuable patterns",
            "venue": "Proceedings of the 7th IEEE International Conference on Data Mining (ICDM",
            "year": 2007
        },
        {
            "authors": [
                "L.D. Raedt",
                "T. Guns",
                "S. Nijssen"
            ],
            "title": "Constraint programming for itemset mining",
            "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data",
            "year": 2008
        },
        {
            "authors": [
                "P. Schaus",
                "J.O. Aoga",
                "T. Guns"
            ],
            "title": "Coversize: A global constraint for frequency-based itemset mining",
            "venue": "In: International Conference on Principles and Practice of Constraint Programming,",
            "year": 2017
        },
        {
            "authors": [
                "M. Bhuiyan",
                "M.A. Hasan"
            ],
            "title": "Interactive knowledge discovery from hidden data through sampling of frequent patterns",
            "venue": "Stat. Anal. Data Min",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "1\nar X\niv :2\ninteractive mining system LetSIP. Resulting patterns allow in most cases to converge to a good solution more quickly. Combining the two improvements, finally, leads to an algorithm showing clear advantages over the existing state-of-the-art.\nKeywords: Pattern mining, interactive mining, preference learning, Constraint programming"
        },
        {
            "heading": "1 Introduction",
            "text": "Constraint-based pattern mining is a fundamental data mining task, extracting locally interesting patterns to be either interpreted directly by domain experts, or to be used as descriptors in downstream tasks, such as classification or clustering. Since the publication of the seminal paper [1], two problems have limited the usability of this approach: 1) how to translate user preferences and background knowledge into constraints, and 2) how to deal with the large result sets that often number in the thousands or even millions of patterns. Replacing the original support-confidence framework with other quality measures [2] does not address the pattern explosion. Post-processing results via condensed representations [3] still typically leaves many patterns, while pattern set mining [4] just pushes the problem further down the line.\nIn recent years, research on interactive pattern mining has proposed to alter the mining process itself: instead of specifying constraints once, mining a result set, and then post-processing it, interactive pattern mining performs an iterative loop [5]. This loop involves three repeating main steps: (1) pattern extraction in which a relatively small set of patterns is extracted; (2) interaction in which the user expresses his preferences w.r.t. those patterns; (3) preference learning in which the expressed preferences are translated into a quality assessment function for mining patterns in future iterations.\nExisting approaches [5\u20137] have a short-coming, however: to enable preference learning, they represent patterns by independent descriptors, such as included items or covered transactions, and expect the learned function, usually a regression or multiplicative weight model, to handle relations. Furthermore, to get quality feedback, it is important to present diverse patterns to the user, an aspect that is at best indirectly handled by existing work.\nThe contributions of this paper are the following:\n1. We introduce cdFlexics, which improves the pattern sampling method proposed in [8], Flexics, by integrating closedDiversity into it to extract patterns. This will allow us to exploit both the sampling and the XOR constraints of Flexics which has the advantage that it is anytime, as well as the filtering of ClosedDiv to maximize patterns diversity. 2. We extend a recent interactive pattern mining approach (LetSIP [6]) by introducing new, more complex, descriptors for explainable ranking, thereby leading to a more concise and diverse pattern sets. These descriptors exploit\nAlgorithm 1 The general CP solving algorithm\n1: function Solve(S = \u3008X,D, C\u3009) 2: Begin 3: Propagate(S) 4: if \u2203xi \u2208 X s.t. |dom(xi)| = 0 then 5: return \u2205 6: else if \u2203xi \u2208 X s.t. |dom(xi)| > 1 then 7: d\u2190MakeDecision(P) 8: return Solve(P \u2227 d) \u222a Solve(P \u2227 \u00acd) 9: else\n10: return solution D 11: end if 12: End 13: end function\nthe concept of discriminating sub-patterns, which separate patterns that are given low rank by the user from those with high rank. By temporarily adding those descriptors, we can learn weights for them, which are then apportioned to involved items without blowing up the feature space. 3. We combine those two improvements to LetSIP, i.e. the improved sampling of cdFlexics and the more complex features learned dynamically during the iterative mining process. This allows the resulting algorithm to both return more diverse patterns more efficiently, and to leverage user feedback more effectively to improve later iterations.\nIn Section 3, we discuss how we integrate closedDiversityinto Flexics. In Section 4, we explain how to derive complex pattern features, followed by a discussion of how to map user feedback back into primitive features. Section 6 containts extensive experiments evaluating our contributions, before we discuss related work in Section 7. We conclude in Section 8."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Constraint Programming (CP)",
            "text": "Constraint programming [9] is a powerful paradigm which offers a generic and modular approach to model and solve combinatorial problems. A CP model consists of a set of variables X = {x1, . . . , xn}, a set of domains D mapping each variable xi \u2208 X to a finite set of possible values dom(xi), and a set of constraints C on X. A constraint c \u2208 C is a relation that specifies the allowed combinations of values for its variables X(c). An assignment on a set Y \u2286 X of variables is a mapping from variables in Y to values in their domains. A solution is an assignment on X satisfying all constraints.\nTo find solutions, CP solvers usually work in two steps. First, the search space is reduced by adding constraints (called decisions) in a depth-first search. A decision is, for example, an assignment of a variable to a value in its domain.\nThen comes a phase of propagation that checks the satisfiability of the constraints of the CP model. This phase can also filter (i.e. remove) values in the domains of variables that do not lead to solutions . At each decision, constraint filtering algorithms prune the search space by enforcing local consistency properties like domain consistency. A constraint c on X(c) is domain consistent, if and only if, for every xi \u2208 X(c) and every v \u2208 dom(xi), there is an assignment satisfying c such that (xi = v). The baseline recursive algorithm for solving is presented in Algorithm 1. It relies on the two functions Propagate and MakeDecision to respectively filter values by using constraints and reduce the search space by making decisions."
        },
        {
            "heading": "2.2 Pattern mining",
            "text": "Given a database D, a language L defining subsets of the data and a selection predicate q that determines whether an element \u03c6 \u2208 L (a pattern) describes an interesting subset of D or not, the task is to find all interesting patterns, that is,\nT (L,D, q) = {\u03c6 \u2208 L | q(D, \u03c6) is true} A well-known pattern mining task is frequent itemset mining [1]. Let I = {1, ..., n} be a set of n items, an itemset (or pattern) P is a non-empty subset of I. The language of itemsets corresponds to LI = 2I\\\u2205. A transactional dataset D is a bag (or multiset) of transactions over I, where each transaction t is a subset of I, i.e., t \u2286 I; T = {1, ...,m} a set of m transaction indices. An itemset p occurs in a transaction t, iff p \u2286 t. The cover of p in D is the bag of transactions in which it occurs: VD(p) = {t \u2208 D | p \u2286 t}. The support of p in D is the size of its cover: supD(p) = |VD(p)|. A well-known interestingness predicate is a threshold on the support of the itemsets, the minimal support : \u03b8. The task in frequent set mining is to compute frequent itemsets: {p \u2208 LI | supD(p) \u2265 \u03b8}.,\nAdditional constraints on the individual itemsets to reduce the redundancy between patterns, such as closed frequent itemsets[3], have been proposed. An itemset p is said to be closed if there is no q \u2287 p such that supD(p) = supD(q), that is, p is maximal with respect to set inclusion among those itemsets having the same support.\nConstraint Programming for Itemset Mining. Several proposals have investigated relationships between itemset mining and constraint programming (CP) to revisit data mining tasks in a declarative and generic way [10\u201314]. The declarative aspect represents the key advantage of the proposed CP approaches. This allows modeling of complex user queries without revising the solving process but at the cost of efficiency.\nA CP model for mining frequent closed itemsets was proposed in [11] which has successfully encoded both the closeness relation as well as the antimonotonicity of frequency into one global constraint called closedPattern. It uses a vector of Boolean variables (x1, . . . , x|I|) for representing itemsets, where xi represents the presence of the item i \u2208 I in the itemset. We will use\nthe notation x+ = {i \u2208 I | xi = 1} to represent the pattern associated to these variables.\nDefinition 1 (closedPattern constraint) Let (x1, . . . , x|I|) be a vector of Boolean variables, \u03b8 a support threshold and T a dataset. The constraint closedPatternD,\u03b8(x1, . . . , x|I|) holds if and only if x\n+ is a closed frequent itemset w.r.t. the threshold \u03b8."
        },
        {
            "heading": "2.3 Pattern Diversity",
            "text": "The discovery of a set of diverse itemsets from binary data is an important data mining task. Different measures have been proposed to measure the diversity of itemsets. In this paper, we consider the Jaccard index as a measure of similarity on sets. We use it to quantify the overlap of the covers of itemsets.\nDefinition 2 (Jaccard index) Let p and q be two itemsets. The Jaccard index is defined as\nJac(p, q) = |VDp \u2229 VDq| |VDp \u222a VDq|\nA lower Jaccard indicates low similarity between itemset covers, and can thus be used as a measure of diversity between pairs of itemsets.\nDefinition 3 (Diversity/Jaccard constraint) Let p and q be two itemsets. Given the Jac measure and a diversity threshold Jmax, we say that p and q are pairwise diverse iff Jac(p, q) \u2264 Jmax.\nWe now formalize the diversity problem for itemset mining. Diversity is controlled through a threshold on the Jaccard similarity of pattern occurrences.\nDefinition 4 (Diverse frequent itemsets) Let H be a history of pairwise diverse frequent itemsets, and Jmax a bound on the maximum allowed Jaccard value. The task is to mine new itemsets p such that p is diverse according to H:\ndivJ (p,H, Jmax)\u21d4 \u2200h \u2208 H, Jac(p, h) \u2264 Jmax\nIn [15], we showed that the Jaccard index has no monotonicity property, which prevents usual pruning techniques and makes classical pattern mining unworkable. To cope with the non-monotonicity of the Jaccard similarity, we proposed an anti-monotonic lower bound relaxation of the Jaccard index, which allow effective pruning.\nProposition 1 (Lower bound) Let p and h be two patterns, and \u03b8 a bound on the minimal frequency of the patterns, then\nLB(p, h) = max(0, \u03b8 \u2212 |VD(p)\\VD(h)| |VD(h)|+ |VD(p)\\VD(h)| \u2264 Jac(p, h)\nThis Approximate bound is then exploited within the closedPattern constraint to mine pairwise diverse frequent closed itemsets.\nDefinition 5 (closedDiversity) Let x be a vector of Boolean item variables, H a history of pairwise diverse frequent closed itemsets (initially empty), \u03b8 a support threshold, Jmax a bound on the maximum allowed Jaccard, and D a dataset. The closedDiversityD,\u03b8(x,H, Jmax) constraint holds if and only if: (1) x+ is closed; (2) x+ is frequent, supD(x +) \u2265 \u03b8; (3) x+ is diverse, \u2200h \u2208 H, LB(x+, h) \u2264 Jmax.\nBy using this lower bound, it is easier to filter items that would give patterns too close to the ones in the history (see [15] for more details)."
        },
        {
            "heading": "2.4 LetSIP: Weighted constrained sampling",
            "text": "Our work is based on LetSIP [6], which in turn uses Flexics [8]. In this and the next section, we will therefore sketch the pattern sampling and feedback + preference learning steps of LetSIP (see Algorithm 3) before describing our own proposals in Sections 3 and 4. The main idea behind Flexics consists of partitioning the search space into a number of cells, and then sampling a solution from a random cell. This technique was inspired by recent advances in weighted solution sampling for SAT problems [16]. It exploits Weightgen [17] to sample SAT problem solutions. Weightgen partitions the search space into cells using random XOR constraints and then extracts a pattern from a randomly selected cell. An individual XOR constraint over variables X has the form \u2297 bi.xi = b0, where b0|i \u2208 {0, 1}. The coefficients bi determine the variables involved in the constraint, whereas the parity bit b0 determines whether an even or an odd number of variables must be set to 1. Together, m XOR constraints identify one cell belonging to a partitioning of the overall solution space into 2m cells. Weightgen uses a two-step procedure consisting of 1) estimating the number of XOR required for the partitioning and 2) generating these random constraints and sample the solutions. The sampling step needs the use of an efficient oracle to enumerate the solutions in the different cells. Dzyuba et al. evaluated two oracles: EFlexics which employs the Eclat Algorithm for the enumeration and GFlexics, a CP-based method based on CP4IM [12]. In this paper, we propose to use an efficient CP-based method, called cdFlexics, based the closedDiversityglobal constraint."
        },
        {
            "heading": "2.5 LetSIP: Preference based pattern mining",
            "text": "User feedback w.r.t. patterns takes the form of providing a total order over a (small) set of patterns, called a query Q. User feedback {p1 p3 p2 p4},\nfor instance, indicates that the user prefers p1 over p3, which they prefer over p2 in turn, and so on. This total order is translated into pairwise rankings {(p1 p3), (p1 p2), (p1 p4), . . .}. Patterns are represented using a feature representation F = {f1, . . . , fd} \u2208 Rd. Examples of features include Len(p) = |p|/|I|, Freq(p) = supD(p)/|D|, in which case the corresponding fi is truly in R, or Items(i, p) = [i \u2208 p]; and Trans(ti, p) = [p \u2286 ti], where [.] denotes the Iverson bracket, leading to (partial) feature vectors \u2208 {0, 1}|I| and \u2208 {0, 1}|D|, respectively. The total number of features depends on the dimensions of the data, and is equal to |I| + |D| + 2. Given this feature representation, each ranked pair pi pj corresponds to a classification example (Fi \u2212 Fj ,+).\nLetSIP uses the parameterized logistic function\n\u03c6logistic(p; w,A) = A+ 1\u2212A\n1 + e\u2212w\u00b7F\nas a learnable quality function for patterns with F the aforementioned feature representation of a pattern p, w the weight vector to be learned, and A is a parameter that controls the range of the interestingness measures, i.e. \u03c6logistic \u2208 [A, 1]. For details, we direct the reader to the original publication [6]. Finally, to select the k patterns Q to be presented to the user, LetSIP uses Flexics to sample k patterns proportional to \u03c6logistic. These patterns are selected according to a Top(m) strategy, which picks the m highest-quality patterns from a cell (Algorithm 3, line 29). Moreover, to help users to relate the queries to each other, LetSIP propose to retain the top ` patterns from the previous query and only sample k \u2212 ` new patterns (Algorithm 3, lines 8-10).\nExample 1 Let I = {0, . . . , 6} be a set of items and Q = {p1, p2, p3, p4} a sample of four patterns, where p1 = {4, 6}, p2 = {1, 6}, p3 = {0} and p4 = {3}. Let\u2019s consider a user ranking U = {p2 p1 p3 p4} as feedback and suppose that supD(p1) = 0.54, supD(p2) = 0.18 and supD(p3) = 0.36. Using items and frequency as features, we have F1 = (0, 0, 0, 0, 1, 0, 1, 0.54), F2 = (0, 1, 0, 0, 0, 0, 1, 0.18), . . . To learn the vector of weights w, U is translated into pairwise rankings {(p2 p1), (p2 p3), . . .} and distances between feature vectors for each pair are calculated. So, given the preference p2 p1, we have F2\u2212F1 = (0, 1, 0, 0,\u22121, 0, 0,\u22120.36). After the first iteration of LetSIP, the learned weight vector is w = (\u22120.33, 0.99, 0,\u22120.99, 0.33, 0, 1.33, 0.15)."
        },
        {
            "heading": "3 Sampling Diverse Frequent Patterns",
            "text": "LetSIP\u2019s sampling component is based on Flexics. An interactive system seeks to ensure faster learning of accurate models by targeted selection of patterns to show to the user. An important aspect is that the user be quickly presented with diverse results. If patterns are too similar to each other, deciding which one to prefer can become challenging, and if they appear in several successive iterations, it eventually becomes a slog.\nWe propose in this section a modification to Flexics, called cdFlexics, to explicitly control the diversity of sampled patterns, thus ensuring pattern diversity within each query and thus sufficient exploration. Our key idea is to exploit the diversity constraint closedDiversity as an oracle to select the patterns in the different cells (partitions). We start by presenting our strategy for integrating closedDiversity into Flexicsin section 3.1, then we detail the associated filtering algorithm in section 3.2."
        },
        {
            "heading": "3.1 Integrating the diversity constraint into Flexics",
            "text": "In order to turn closedDiversity into a suitable oracle, we propose to control the diversity of patterns sampled from each cell as follows. We maintain a local history Hloc for each cell. Initially, Hloc is initialized to empty. Each time a new diverse pattern is mined from the current cell, it is added to Hloc and the exploration of the remaining search space of the cell continues using the new updated history. Finally, the pattern sampled for that cell is picked among the diverse patterns in Hloc, with a probability proportional to a given quality measure, e.g., frequency. This process is repeated for each new cell until the required number of samples is reached. With a such strategy, the exploration of the different cells become more efficient and faster, as their size is greatly reduced thank to the filtering of closedDiversity which avoids the discovery of non diverse patterns within each cell."
        },
        {
            "heading": "3.2 Filtering of cdFlexics",
            "text": "The filtering of cdFlexics combines two propagators, that of closedDiversity (see [15] for more details) and the propagator for XOR constraints. In CP, constraints interact through shared variables, i.e., as soon as a constraint has been propagated (no more values can be pruned), and at least a value has been eliminated from the domain of a variable, say v, then the propagation algorithms of all other constraints involving v are triggered. For instance, when the propagator of closedDiversity modifies the domain of item variables, these modifications must be propagated to XOR constraints. Now, we detail the filtering algorithm of the XOR constraints. This propagator is based on the method of Gaussian elimination [18], a classical algorithm for solving systems of linear equations. All coefficients bi \u2208 {0, 1} of the XOR constraints form a binary matrixM of size (n,m+1), where n is the number of Individual XOR constraints and m is the number of items in the dataset, the last column\nb0 \u2208 {0, 1} represents the parity bit. Thus, for each constraint XOR k, if a variable xi is involved in this constraint, then M[k, i] = 1, otherwise M[k, i] = 0.\nFigure 1.2 shows the augmented matrix representation of the XOR constraints system of Figure 1.1 on a dataset of 5 items.\nAlgorithm 2 Filtering for XOR constraints\nIn: Coefficient matrixM InOut: Vector of Boolean variables (x1 . . . xn) 1: Begin 2: x+ \u2190 {i|xi = 1}, x\u2212 \u2190 {i|xi = 0}, x\u2217 \u2190 {i|i /\u2208 x+ \u222a x\u2212} 3: foreach (i \u2208 x+ \u2227 row r \u2208 M) do 4: if (M[r][i] = 1) then 5: br0 \u2190 1\u2212 b r 0\n6: end if 7: M[r][i]\u2190 0 8: end for 9: if CheckInconsistency() then return fail\n10: else if (x\u2217 = \u2205) then return true 11: else 12: Echlonize(M) 13: if CheckInconsistency() then return fail 14: else FixVariables() 15: end if 16: end if 17: return true 18: End 19: function CheckInconsistency( ) 20: foreach row r \u2208 M do 21: if (br0 = 1 \u2227 \u2211n i=1M[r][i] = 0) then 22: return true 23: end if 24: end for 25: return false 26: end function 27: procedure FixVariables( ) 28: foreach row r \u2208 M do 29: if ( \u2211n i=1M[r][i] = 1) then 30: dom(xi)\u2190 dom(xi)\u2212 {1\u2212 br0}; 31: x+ \u2190 x+ \u222a {i}; \u2228 x\u2212 \u2190 x\u2212 \u222a {i} 32: end if 33: end for 34: end procedure\nAt each step (see Algorithm 2), the matrix M is updated with the latest variable assignments x+: for each variable xi set to 1, the parity bit of the constraint where the variable appears is inverted and the coefficient of this variable in the matrix M is set to 0. Figure 1.5 shows this transformation where the parity bit of line 1 becomes 1 and the coefficient of variable x5 is set 0. Then, we check if the updated matrix does not lead to an inconsistency, i.e., if a row becomes empty while its right hand side is equal to 1. In this case, the XOR constraints system is unsatisfiable and the current search branch terminates (Figure 1.5). Otherwise, an echelonization1 operation (line 12) is performed with the Gauss method to obtain an echelonized matrix : all ones are on or above the main diagonal and all non-zero rows are above any rows of all zero (Figure 1.3). During echelonization, two situations allow propagation.\n1This operation is performed using the m4ri library.\nAlgorithm 3 LetSIP extension (new steps are in bold font)\nIn: Dataset D, minimal frequency threshold \u03b8 Parameters: Query of size k, number of iterations T , query retention `, range A, Flexics error\ntolerance \u03ba, Patterns feature representations F 1: Begin 2: Weight vector w0 \u2190 0, 3: Feedback U \u2190 \u2205, 4: Ranked patterns Q\u22170 \u2190 \u2205 5: . Initialization 6: Ranking function h0 \u2190 Logistic(w0, A) 7: for t = 1, 2 . . . T do . Mine, Interact, Learn, Repeat loop 8: R\u2190 TakeFirst(Q\u2217t\u22121, `) 9: . Retain the top ` patterns from the previous iteration 10: Query Qt \u2190 R \u222a SamplePatterns(D, ht\u22121)\u00d7 (k \u2212 |R|) 11: . Call SamplePatterns (k \u2212 |R|) times 12: Pt \u2190 Rank(Qt); U \u2190 U \u222a Pt 13: . Ask user to rank patterns in Qt 14: F\u2190 ConvertToFeaturesVectors(Pt,D) 15: . Convert patterns to feature representations 16: pdisc \u2190 LearnDiscriminatingPattern(Pt) 17: . Extract discriminating pattern (Algorithm 4) 18: F\u2217 \u2190 AddDiscrPatternFeature(F, pdisc,D) 19: . Update pattern features 20: wt \u2190 Aggregate(LearnWeights(U, \u03bb,wt\u22121,F\u2217)) 21: . Update pattern features weights 22: F\u2190 RemoveDiscrPatternFeature(F\u2217, pdisc,D) 23: . remove discr. pattern feature 24: ht \u2190 Logistic(wt, A) 25: end for 26: End 27: function SamplePatterns(D, Sampling weight w : L\u2192 [A, 1]) 28: C \u2190 cdFlexics(D, freq(.) \u2265 \u03b8,w; \u03ba) 29: return Top(m) highest-wheighted patterns from C 30: end function\nIf a row becomes empty while its right hand side is equal to 1, the system is unsatisfiable. If a row contains only one free variable, it is assigned the right hand side of the row (Figure 1.3)."
        },
        {
            "heading": "4 Preference learning based on explainable ranking",
            "text": ""
        },
        {
            "heading": "4.1 Towards more expressive and learnable pattern representations",
            "text": "Features for pattern representation involve indicator variables for included items or sub-graphs, or for covered transactions [6, 7], or pattern length or frequency [6]. The issue is that such features are treated as if they were independent, whether in the logistic function mentioned above, or multiplicative functions [5, 7]. While this allows to identify pattern components that are globally interesting for the user, it is impossible to learn relationships such as \u201dthe user is interested in item i1 if item i3 is present but item i4 is absent\u201d. In addition, the pattern elements whose inclusion is indicated are defined before-hand, and the user feedback has no influence on them. We therefore propose to learn more expressive features in order to improve the learning of user preferences.\nIn this work, we propose to consider discriminating sub-patterns that better capture (or explain) these preferences. Those features exploit rankingcorrelated patterns, i.e., patterns that influence the user ranking either by allowing some patterns to be well ranked or the opposite."
        },
        {
            "heading": "4.2 Interclass Variance",
            "text": "As explained above, our goal is to mine sub-patterns that discriminate between patterns that have been given a high user ranking and those that received a low one. An intuitive way of modelling this problem consists of considering the numerical ranks given to individual patterns as numerical labels and the mining setting as akin to regression. We are not aiming to build a full regression model but only to mine an individual pattern that correlates with the numerical label. For this purpose, we use the interclass variance measure as proposed by [19].\nDefinition 6 Let P be a set of ranked patterns of size k according to user ranking U , and P\u2217 be a subset of P. We define by \u00b5(P\u2217) the average rank of patterns p \u2208 P\u2217:\n\u00b5(P\u2217) = 1|P\u2217| \u00b7 \u2211 p\u2208P\u2217 r(p), (1)\nwhere r(p) is the rank of p according to a user feedback.\nLet Py = {p \u2208 P | p \u2287 y}: the subset of patterns p in P in which the subpattern y is present, and Py = P\\Py. The interclass variance of the sub-pattern y is defined by:\nICV (y) = |Py| \u00b7 (\u00b5(P)\u2212 \u00b5(Py))2 + |Py| \u00b7 (\u00b5(P)\u2212 \u00b5(Py))2 (2) The sub-pattern y having the largest ICV is then the one which is the\nmost strongly correlated with the user ranking of the patterns.\nExample 2 Consider again our example with a query Q = {p1, p2, p3, p4} and the user feedback U = {p2 p1 p3 p4}. Let P the corresponding ranked patterns w.r.t. U . For y = {1}, we get Py = {p2} (since {1, 6} \u2287 {1}), Py = {p1, p3, p4}, \u00b5(Py) = 1; \u00b5(Py) = r(p1)+r(p3)+r(p4)3 = (2+3+4) 3 = 3 and \u00b5(P) = 2.5. Applying equation (2), we get ICV ({1}) = (2.5\u2212 1)2 + 3 \u00b7 (2.5\u2212 3)2 = 3. Similarly, ICV ({0}) = 0.33."
        },
        {
            "heading": "4.3 Learning Discriminating Sub-Patterns",
            "text": "To find the sub-pattern y \u2286 p with the greatest interclass variance ICV to be used as a new feature (p \u2208 P, st. P being the set of ranked patterns), we systematically search the pattern space spanned by the items involved in patterns in P. Semantically, this is the sub-pattern whose presence in one or more itemsets p has influenced their ranking. So, if y \u2286 p, we can say that the ranking of p at the ith position in P is more likely to be explained by the presence of sub-pattern y.\nAlgorithm 4 The LearnDiscriminatingPattern algorithm\n1: function LearnDiscriminatingPattern(set of ranked patterns P) 2: ICVmax \u2190 0, pdisc \u2190 \u2205 3: allItems\u2190 items(\u222ap \u2208 P) 4: subPatterns\u2190 allItems 5: foreach item \u2208 allItems do 6: if ICV (item) \u2265 ICVmax then 7: ICVmax \u2190 ICV (item) 8: pdisc \u2190 {item} 9: end if\n10: end for 11: foreach elt \u2208 subPatterns do 12: while ((item \u2208 allItems\\elt) \u2227 (\u2203p \u2208 P st. {elt \u222a item} \u2286 p)) do 13: if ICV (elt \u222a item) \u2265 ICVmax then 14: ICVmax \u2190 ICV (elt \u222a {item}) 15: pdisc \u2190 elt \u222a {item} 16: subPatterns\u2190 subPatternss \u222a pdisc 17: end if 18: end while 19: end for 20: return pdisc 21: end function\nAlgorithm 4 implements the function LearnDiscriminatingPattern (see Algorithm 3, line 16), which learns the best discriminating pattern as a feature. Its accepts as input a set of ranked patterns P, and returns the sub-itemset with the highest ICV. Its starts by calculating the ICV of all items of the patterns p \u2208 P (loop 5\u221210). Then, it iteratively combines the items to form a larger and finer-grained discriminating pattern (loop 11-19) . Obviously, before combining sub-itemsets, we should ensure that the resulting sub-pattern belongs to an existing pattern p \u2208 P (line 12). If such a pattern exists (line 13), we update the ICVmax, we save the best discriminating pattern computed so far (i.e. pdisc, line 15) and we extend the set of sub-itemsets with the pdisc pattern for further improvements (line 16). Finally, the best discriminating pattern is returned at line 20.\nProposition 2 (Time complexity) In the worst case, Algorithm 4 runs in a quadratic time O(n2), where n = |items(\u222ap \u2208 P)|.\nProof The first loop runs in O(n). For the ith iteration of the outer loop (6-14), we have, for a fixed size subitemsets, at-most (n\u2212 i) iterations for the inner loop. Thus, giving O(n 2\n2 ) for both loops. However, subPatterns evolves with at-most |pmax| elements, since |pdisc| \u2264 |pmax|, which gives additional complexity of O(n\u00d7|pmax|). Since |pmax| \u2264 n, the worst case complexity is O(n2).\nExample 3 Consider again the set of ranked patterns P = {p2, p1, p3, p4} w.r.t. U = {p2 p1 p3 p4}. After the first loop of Algorithm 4 we get subPatterns = {0, 1, 3, 4, 6}, ICV (1) = ICV (3) = 3, ICV (0) = ICV (4) = 0.33 and ICV (6) = 4. Thus, ICVmax = 4 and pdisc = {6}. The second loop (lines 11-19) seeks to find the best combination of subitemsets that maximizes the interclass variance value ICVmax.\nTo illustrate this reasoning, let us look at the second iteration of the outer loop, where elt = {1} and allItems = {0, 1, 3, 4, 6}. In this case, the inner while loop will only consider items item \u2208 allItems\\{1}, such that elt \u222a {item} belongs to an existing pattern p \u2208 P. Thus, we find item = {6} for which elt \u222a {item} \u2286 p2. As ICV ({1, 6}) = 3 < ICVmax, pdisc remains unchanged. Note that for item = {3} or {4}, @p \u2208 P st. ({1}\u222a{item}) \u2286 p. For our example, Algorithm 4 returns pdisc = {6}."
        },
        {
            "heading": "5 Learning weights for \u03c6logistic from discriminating sub-patterns",
            "text": ""
        },
        {
            "heading": "5.1 Exploiting the ICV-based Features",
            "text": "Learning the discriminating sub-pattern as a new feature in LetSIP brings meaningful knowledge to consider during an interactive preference learning. In fact, this sub-pattern correlated with the user\u2019s ranking emphasizes the items of interest related to his ranking. Now, we describe how these discriminating patterns can be used in order to improve the learning function \u03d5logistic for patterns.\nA direct way of exploiting discriminating sub-patterns pdisc (explaining user rankings) consists of adding them to the feature vector representing patterns. However, this increases the size of feature vectors, introduces additional cost and most probably leads to over-fitting and generalization issues of the learning function \u03c6logistic.\nInstead, we temporarily add new features representing the discriminating sub-pattern pdisc, denoted F\ndisc, to the initial feature representation F of individual patterns only to learn and update the weights of F (see Algrithm 3, line 18):\n\u2022 discPatt: a binary feature for each pattern p \u2208 P; equals 1 iff the corresponding discriminating sub-pattern pdisc belongs to p;\ndiscPatt(p, pdisc) = [pdisc \u2286 p]\n, \u2022 discFreq: numerical feature; the frequency of the discriminating sub-pattern\nbelonging to p;\ndiscFreq(p, pdisc) = { supD(pdisc)/|D| if pdisc \u2286 p 0 otherwise\n\u2022 discSize: numerical feature; the relative size of the discriminating subpattern belonging to p;\ndiscSize(p, pdisc) = { |pdisc|/|I| if pdisc \u2286 p 0 otherwise\nLet F\u2217 = F \u2016 Fdisc the concatenated feature representation st. Fdisc = (discPatt, discFreq, discSize). As the size of F is |I| + |D| + 2 and the size of Fdisc is 3, thus we have in total at most |I|+ |D|+ 5 features.\nNow, similar to LetSIP, we need to learn a vector of feature weights defining the ranking function \u03c6logistic. We proceed in three steps:\n(i) First, we use F\u2217 to learn the weight vector wF associated to F (as in LetSIP) and the weight vector wFdisc associated to F\ndisc. (ii) Second, using wFdisc , we update the weight wf \u2208 wF of each feature f \u2208 F\nby mean of a multiplicative aggregation function (see Algrithm 3, line 20). (iii) Finally, to learn from new discriminating sub-patterns, we remove the\nfeatures Fdisc from F\u2217 (see Algrithm 3, line 22).\nThe next section shows how to update the feature weights wF using the learned feature weights wFdisc .\nExample 4 Let us consider example 3 and the discriminating sub-pattern pdisc = {6} computed using Algorithm 4. Let us assume that F represents items and frequency features, while Fdisc = (discPatt, discFreq). According to example 1, pattern p1 = {4, 6} is represented by the feature vector F1 = (0, 0, 0, 0, 1, 0, 1, 0.54), while pattern p3 is represented by F3 = (1, 0, 0, 0, 0, 0, 0, 0.36). Now, using the two features discPatt and discFreq, we obtain for p1 the full feature vector F \u2217 1 = (0, 0, 0, 0, 1, 0, 1, 0.54,1,0.63), since F1 disc = (1, 0.63), i.e., discPatt(p1, {6}) = 1 and discFreq(p1, {6}) = 0.63. Similarly, for p3 = {0} we get F\u22173 = (1, 0, 0, 0, 0, 0, 0, 0.36,0,0.0), since F3\ndisc = (0, 0.0), i.e., discPatt(p3, {6}) = 0, and discF (p3, {6}) = 0.0."
        },
        {
            "heading": "5.2 Updating feature weights of patterns",
            "text": "Given a complete features representation F\u2217 = F \u2016Fdisc for which we learned two weight vectors: wF for features in F and wFdisc for new features in F\ndisc, i.e., discPatt, discSup, and discSize. Here, we show how to combine the weight vector wFdisc , so as to reveal features (items, transactions, ...) in F that are seen as making better ranking prediction than others.\nUnlike LetSIP, our approach aims to identify those features in terms of discriminating sub-pattern pdisc and seeks to reward (or penalize) features in F by updating their current weight wf (f \u2208 F) using a multiplicative aggregation function. Let pdisc be the discriminating sub-pattern and let p \u2208 P. For a given feature fdisc \u2208 Fdisc and its associated learned weight wfdisc , the weight wf is updated as follows:\n\u2022 (fd \u2261 discPatt) \u2013 for each item feature fi \u2208 F s.t. i \u2208 pdisc \u2227 pdisc \u2286 p, wfi = wfi \u00b7\u03a6(wfd) \u2013 for each transaction feature ft \u2208 F s.t. t \u2208 VD(pdisc) \u2229 VD(p), wft =\nwft \u00b7 \u03a6(wfd) \u2022 (fd \u2261 discSup \u2228 fd \u2261 discSize)\n\u2013 for the frequency feature fsup \u2208 F s.t. pdisc \u2286 p, wfsup = wfsup \u00b7 \u03a6(wfd)\n\u2013 for the size feature fsize \u2208 F s.t. pdisc \u2286 p, wfsize = wfsize \u00b7 \u03a6(wfd)\nIntuitively, this updating procedure works since it assigns higher weights to the features that better explain the user ranking over time, thus increasing the probability of being picked in the next iteration.\nAggregation functions Our approach is inspired by [20], which proposes two multiplicative functions for aggregating the learned weights. The updating scheme described above is performed after each ranking. At iteration t > 0, for each feature f \u2208 F, its weight w(t)f is updated using a multiplicative aggregation function \u03a6:\nw (t) f = w (t) f \u00b7 \u03a6(m, \u03b7)\nwhere m is a reward (or a loss) applied to f and \u03b7 \u2208]0, 12 ] is parameter. We use two multiplicative factors [20]: (1) a linear factor \u03a61(m, \u03b7) = 1 + \u03b7m; and (2) an exponential factor \u03a62(m, \u03b7) = exp\n\u03b7m, where m stands for wtfd , the\nlearned feature weight in Fdisc. Thus, we have\nw (t) f =  w (t) f \u00b7 (1 + \u03b7w (t) fd )\n\u2228 w\n(t) f \u00b7 exp\n\u03b7w (t) fd\n(3)\nIn our experiments (see Section 6), we compare both aggregation functions to update features\u2019 weights.\nExample 5 Let us consider example 4. At iteration 1, we get pdisc = {6}, the learned weight vector w\n(1) F = (\u22120.33, 0.99, 0,\u22120.99, 0.33, 0, 1.33, 0.15) for items and\nfrequency features of patterns (see example 1), and w (1) Fdisc = (w (1) discPatt ,w (1) discSup\n) = (1.33, 0.84). For our example, we consider the linear factor and we set \u03b7 to 0.2. For the items feature, since, pdisc = {6} \u2286 p1, we have w (1) 6 = w (1) 6 \u00b7 (1 + \u03b7w (1) discPatt ). As w (1) 6 = 1.33 and w 1 discPatt = 1.33, after the updating step, we get a new value for w (1) 6 = 1.68. For the frequency feature, we have w (1) fsup = w (1) fsup \u00b7 (1 + \u03b7w(1) discF ). As w (1) fsup = 0.15 and w (1) discSup = 0.84, we get w (1) fsup = 0.175. The resulting weights are fed into the \u03c6logistic function to extract new patterns for the next iteration."
        },
        {
            "heading": "6 Experiments",
            "text": "In this section, we experimentally evaluate our approach for interactive learning user models from feedback. The evaluation focuses on the effectiveness of learning using discriminating sub-patterns and cdFlexics. We denote by\n\u2022 LetSIP-disc our extension of LetSIP using discriminating sub-patterns; \u2022 LetSIP-cdf our extension of LetSIP using cdFlexics; \u2022 LetSIP-disc-cdf our extension of LetSIP using both discriminating sub-\npatterns and cdFlexics.\nWe address the following research questions:\nQ1 : What effect do LetSIP\u2019s parameters have on the quality of patterns learned by LetSIP-disc? Q2 : What effect do LetSIP\u2019s parameters have on the quality of sampled patterns using cdFlexics? Q3 : How do our different extensions compare to LetSIP?"
        },
        {
            "heading": "6.1 Evaluation methodology",
            "text": "Evaluating interactive data mining algorithms is hard, for domain experts are scarce and it is impossible to collect enough data for drawing reliable conclusions. To perform an extensive evaluation, we emulate user feedback using a (hidden) quality measure \u03d5, which is not known to the learning algorithm. We follow the same protocol used in [6]: for each dataset, a set S of frequent patterns is mined without prior user knowledge.We assume that there exists a user ranking R\u2217 on the set S, derived from \u03d5, i.e. p q \u21d4 \u03d5(p) > \u03d5(q). Thus, the task is to learn to sample frequent patterns proportional to \u03d5. As \u03d5, we use frequency supD, and surprisingness surp, where surp(p) =\nmax{supD(p)\u2212 \u220f|p| i=1 supD({i}), 0}.\nTo compare the performance of our approaches (LetSIP-disc, LetSIPcdf and LetSIP-disc-cdf) with LetSIP, we use cumulative regret, which is the difference between the ideal value of a certain measure M and its observed value, summed over iterations for each dataset. At each iteration, we evaluate the regret of ranking pattern pi by \u03d5. For that, we compute the percentile rank pct.rank(pi) by \u03d5 of each pattern pi (1 \u2264 i \u2264 k) of the query as follows:\npct.rank(pi) = |q \u2208 S, \u03d5(q) \u2264 \u03d5(pi)|\n|S|\nThus, the ideal value is 1 (e.g., the highest possible value of \u03d5 over all frequent patterns has the percentile rank of 1). The regret is then defined as\n1\u2212M(1\u2264i\u2264k)(pct.rank(pi))\nwhere M \u2208 {max, Avg}. We repeat each experiment 10 times with different random seeds; the average obtained cumulative regret is reported. To ensure that all methods are sampled on the same pattern bases, at each iteration of LetSIP, the same sets of XOR constraints are generated in both approaches.\nFor the evaluation we used UCI data-sets, available at the CP4IM repository2. For each dataset, we set the minimal support threshold such that the size of S is approximately 1,400,000 frequent patterns. Table 1 shows the data set statistics. Each experiment involves 100 iterations (queries). We use the default values suggested by [6] for the parameters of LetSIP: \u03bb = 0.001, \u03ba = 0.9, A = 0.1 and Top(1).\nThe implementation used in this article is available online3. All experiments were conducted as single-threaded runs on AMD Opteron 6174 processors (2.2GHz) with a RAM of 256 GB and a time limit of 24 hours."
        },
        {
            "heading": "6.2 Experimental results",
            "text": ""
        },
        {
            "heading": "6.2.1 Evaluating Aspects of LetSIP-disc",
            "text": "In the first part of the experimental evaluation, we take a look at the effect of different parameter settings on LetSIP-disc, the influence of different pattern features, and how it compares to LetSIP itself. Evaluating the influence of parameter settings We evaluate the effects of the choice of parameters values on the performance of LetSIP-disc, in particular query size k \u2208 {5, 10}, the aggregation function \u03a6 and the range \u03b74. We use the following feature combination: Items (I); Items + Transactions (IT); and Items+Length+Frequency+ Transactions (ILFT). We consider two settings for parameter `: ` = 0 and ` = 1.\nFigure 2 shows the effect of LetSIP-disc\u2019s parameters on regret w.r.t. two performance measures (max .\u03d5 and Avg.\u03d5) and query retention ` = 0. Figures 2a and 2b show that both aggregation functions ensure the lowest quality regrets with k = 10 w.r.t. the maximal aggregator. This indicates that our approach is able to identify the properties of the target ranking from ordered lists of patterns even when the query size increases. Additionally, the exponential function yields lower regret and \u03b7 = 0.13 seems the best value. Regarding the average quality regret (see Figures 2c and 2d), k = 10 continues to be the better query size for the exponential aggregation function but for the linear function, this depends on the value of \u03b7. In addition, as Figures 2e and 2f show, while the exponential function lead to lower maximal regret with\n2https://dtai.cs.kuleuven.be/CP4IM/datasets/ 3The link will be available in the final submission. 4\u03b7 \u2208 {0.13, 0.15, 0.17, 0.18, 0.2, 0.23, 0.25, 0.33}\n\u03b7 = 0.13, the linear function can lead to lower average regret for higher values of \u03b7.\nFigure 3 shows the effect of LetSIP-disc\u2019s parameters on regret for query retention ` = 1. Retaining one highest-ranked pattern from the previous query w.r.t. maximal quality does not affect the conclusions drawn previously: k = 10 being the better query size and exponential function results in the lowest regret with \u03b7 = 0.13. However, we can see the opposite behaviour w.r.t. average quality (see Figures 3c and 3d): querying 5 patterns allows attaining low regret values for both functions, linear and exponential. Interestingly, as Figure 3e shows, the exponential function outperforms linear function on\nalmost all values of \u03b7. Based on these findings, we use the following parameters in the next experiments: k = 10, aggregation = exponential and \u03b7 = 0.13.\nEvaluating the importance of pattern features. In order to evaluate the importance of various feature sets we performed the following procedure. We construct feature representations of patterns incrementally, i.e. we start with an empty representation and add feature sets one by one, based on the improvement of learning accurate pattern rankings that they enable. Table 2 shows the results for two settings of query retention `. As we can observe, additional features provide valuable information to learn more accurate pattern rankings, particularly for LetSIP-disc-Exp where the regret values all\ndecrease. These results are consistent with those observed in [6]. However, the importance of features depends on the pattern type and the target measure \u03d5 [21]. For surprising pattern mining, Length is the most likely to be included in the best feature set, because long patterns tend to have higher values of Surprisingness. Items are important as well, because individual item frequencies are directly included in the formula of Surprisingness. Transactions are important because this feature set helps capture interactions between other features, albeit indirectly.\nComparing quality regrets of LetSIP-disc with LetSIP. [6] showed that LetSIP outperforms two state-of-the-art interactive methods, APLe [21], another approach based on active preference learning to learn a linear ranking function using RankSVM, and IPM [7], an MCMC-based interactive sampling framework. Thus, we compare LetSIP-disc-Exp with LetSIP. We use query size k and feature representations identical to LetSIP-disc-Exp and we vary the query retention `. Table 2 reports the regret values. When considering Items as feature representation for patterns, LetSIP performs the best w.r.t. maximal quality. However, selecting queries uniformly at random allows LetSIP-disc-Exp to ensure the lowest quality regrets w.r.t. average quality. Regarding the other features (IT and ITLF), LetSIP-disc-Exp always outperforms LetSIP whatever the value of the query retention ` and the performance measure considered. On the other hand, we observe that the improvements are more pronounced w.r.t. average regret. These results indicate that the learned ranks by LetSIP-disc of the k selected patterns of each query in the target ranking are more accurate compared to those learned by LetSIP; average values of surprisingness increase substantially compared to LetSIP rankings.\nFinally, retaining one highest-ranked pattern (` = 1) results in the lowest regret whatever the feature representations of patterns. As explained in [6], fully random queries (` = 0) do not enable sufficient exploitation for learning accurate weights.\nFigure 4 presents a detailed view of the comparison between LetSIPdisc-cdf-exp and LetSIP on Anneal dataset (other results are given in the Appendix A). Curves show the evolution of the regret (cumulative and non cumulative) in the resulting frequent patterns over 100 iterations of learning. The results confirm that the learned ranking function of LetSIP-disc-Exp have the capacity to identify frequent patterns with higher quality (i.e., of lowest regrets).\nCPU-times analysis. Finally, regarding the CPU-times, Figure 5 compares the performance of LetSIP-disc-Exp and LetSIP on German-credit, Chess, Heart-Cleveland and Zoo-1 datasets. Overall, learning more complex descriptors from the user-defined pattern ranking does not add significantly to the runtimes of our approach. LetSIP-disc-Exp clearly beats LetSIP on four datasets (i.e., Chess, Heart-cleveland, Kr-vs-kp, and Soybean) and performs similarly on three other ones (see complementary results in\nthe Appendix C), while LetSIP performs better on three datasets (i.e., German-credit, Hepatitis, Zoo).\nOn average, one iteration of LetSIP-disc-Exp takes 8.84s (8.81s for LetSIP) on AMD Opteron 6174 with 2.2 GHz. The largest proportion of LetSIP\u2019s runtime costs is associated with the sampling component (costs of weight learning are low). As it will be shown in the next Section, exploiting cdFlexics within LetSIP enables to improve the running times."
        },
        {
            "heading": "6.2.2 Evaluating Aspects of LetSIP-cdf",
            "text": "We investigate the effects of the choice of features and parameter values on the performance of LetSIP-cdf and we compare it to the original LetSIP. We use the same protocol as in Section 6.2.1. We consider two settings for parameter Jmax \u2208 {0.05, 0.1}. Tables 3 and 4 show detailed results.\nAs observed previously, in almost of the cases, adding new features allow to decrease the value of the regret. Interestingly, increasing the query size\ndecreases the maximal quality regret about twofold, while for the average quality regret k = 5 seems to be the best setting. Regarding the parameter Jmax, the performance depends on the target measure \u03d5: Jmax = 0.1 results in the lowest regret with respect to the maximal quality, while Jmax = 0.05 ensures the lowest average quality regrets. Finally, for the choice of values for query retention `, we observe the same conclusions drawn previously: ` = 1 being the better query retention value.\nComparing quality regrets of LetSIP-cdf with LetSIP. LetSIP-cdf exhibits two different behaviours. On 11 datasets (see Table 3), the regret of LetSIP-cdf is substantially lower than that of LetSIP, particularly for the maximal quality and for ` = 1 where the decrease is very impressive (an average gain of 94%). However, it is also results in the highest quality regrets on four datasets (see Table 4), which negates the previous results.\nAn analysis of the results on these four data sets shows us that closedDiversity mines very few patterns for each query. This is due to the use of a rather low diversity threshold (Jmax =0.05) which considerably reduces the many redundancies in these data sets. The trade-off is, however, that it becomes extremely difficult to improve the regret given the few patterns.\nFigure 6 presents a detailed view of the comparison between LetSIPcdf and LetSIP (cumulative and non-cumulative regret) on Hepatitis dataset\n(other results are given in the Appendix B). These results confirm once again the interest of ensuring higher diversity for learning accurate weights.\nCPU-times analysis. Figure 5 also compares the performance of LetSIPcdf and LetSIP on German-credit, Chess, Heart-Cleveland and Zoo-1 data sets. Other results are given in the Appendix C. Clearly, LetSIP-cdf is more efficient than LetSIP. This is explainable by the large reduction of the size of each cell performed by closedDiversity to avoid the mining of non diverse patterns, which speeds exploration up. The only exception are the two data sets Zoo-1 and Vote where LetSIP is faster. When comparing to LetSIPdisc-cdf-exp, we can observe that LetSIP-cdf remains better, except again for the two data sets Zoo-1 and Vote."
        },
        {
            "heading": "6.2.3 Evaluating Aspects of LetSIP-disc-cdf",
            "text": "Our last experiments evaluates the combination of our two improvements to LetSIP, i.e., the improved sampling of cdFlexics and the more complex features learned dynamically during the iterative mining process. We report detailed results for different features combination, and different parameter settings (query size k, query retention `, and \u03b7). We only show the results for Jmax = 0.05. As previously, LetSIP-disc-cdf clearly beats LetSIP-cdf on 11 datasets (see Tables 5) whatever the feature representations of patterns and the value of parameters k and \u03b7. When comparing to LetSIP-cdf, the choice of values for query retention ` clearly affects LetSIP-disc-cdf. Fully random queries (` = 0) seems to favour LetSIP-disc-cdf, yielding small improvements. However, both aggregation functions EXP and Linear get similar results with a slight advantage for EXP. The results of LetSIP-disc-cdf with ` = 1 are comparable with those of LetSIP-cdf, but we can notice a very slight advantage to LetSIP-disc-cdf with ILFT as feature representation for patterns. As for LetSIP-cdf, LetSIP-disc-cdf performs badly on four datasets (see Table 5).\nCPU-times analysis. When analysing the running times (see Figure 5), we can see that LetSIP-cdf and LetSIP-disc-cdf perform very similarly on most of the instances, except on two datasets (Kr-vr-kp and Zoo-1) where LetSIP-disc-cdf is faster.\nT a b le\n5 :\nE va\nlu a ti\no n\no f\nth e\nim p\no rt\na n ce\no f\np a tt\ner n\nfe a tu\nre s\na n d\nco m\np a ri\nso n\no f\nL e t S IP\n-d is c -c d f v s L e t S IP\n-c d f\nv s. L e t S IP\nw .r\n.t . d iff\ner en\nt va\nlu es\no f J m a x . R\nes u lt s a re a g g re g a te d ov er 1 1 d a ta se ts : L y m p h , H ep a ti ti s, H ea rt -c le v el a n d , Z o o , G er m a n - cr ed it , S oy b ea n , V o te , H ea rt , H y p o , V eh ic le a n d W in e. J m a x = 0 .0 5 \u2227 ` = 1 k = 5 k = 1 0 R eg re t: m a x .\u03d5 R eg re t: A v g .\u03d5 R eg re t: m a x .\u03d5\nR eg\nre t: A v g .\u03d5\nF ea\ntu re s\n\u03b7 L e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nI 0 .2 5\n2 0 4 .5 9 5\n1 3 .8 6 0\n1 3 .9 3 4\n1 3 .8 0 6\n4 2 4 .4 7 2\n3 3 7 .0 6 9\n3 4 1 .5 7 5\n3 4 0 .6 2 3\n1 9 6 .4 4 4\n1 0 .9 8 6\n1 1 .1 2 7\n1 1 .1 6 7\n4 3 3 .1 4 2\n3 7 6 .3 8 0\n3 7 8 .9 4 8\n3 7 8 .6 9 0\n0 .2\n1 4 .0 1 8\n1 4 .0 1 7\n3 4 0 .3 2 2\n3 4 1 .2 8 3\n1 1 .2 2 9\n1 1 .1 2 2\n3 7 9 .4 3 8\n3 7 9 .0 7 0\n0 .1 3\n1 3 .9 5 7\n1 3 .9 8 2\n3 3 9 .7 7 3\n3 4 0 .1 3 1\n1 1 .1 9 5\n1 1 .2 8 3\n3 7 7 .1 4 2\n3 7 7 .7 0 0\nIT 0 .2 5\n2 0 2 .9 2 9\n1 5 .3 3 4\n1 5 .5 3 3\n1 5 .4 2 4\n3 8 6 .0 5 3\n3 1 9 .2 6 2\n3 2 0 .5 8 9\n3 2 0 .1 9 5\n1 9 5 .4 9 7\n1 1 .4 0 1\n1 1 .6 5 4\n1 1 .6 6 0\n3 9 4 .8 0 4\n3 5 6 .8 9 7\n3 5 8 .1 6 4\n3 5 7 .4 6 0\n0 .2\n1 5 .8 5 7\n1 5 .7 9 8\n3 2 0 .9 7 5\n3 2 1 .0 3 6\n1 1 .5 9 6\n1 1 .5 1 0\n3 5 7 .2 1 1\n3 5 7 .2 5 7\n0 .1 3\n1 5 .6 9 8\n1 5 .4 0 4\n3 1 9 .3 6 6\n3 1 8 .6 9 0\n1 1 .6 9 4\n1 1 .6 7 8\n3 5 8 .4 0 1\n3 5 8 .0 6 5\nIT L F\n0 .2 5\n2 0 6 .0 6 4\n1 5 .5 1 7\n1 5 .5 5 2\n1 5 .5 3 9\n3 8 3 .7 9 9\n3 2 0 .8 3 8\n3 2 0 .5 7 2\n3 2 0 .7 9 2\n1 9 7 .4 2 2\n1 1 .5 2 1\n1 1 .4 7 8\n1 1 .4 7 5\n3 9 3 .1 4 7\n3 5 6 .3 9 5\n3 5 9 .1 6 3\n3 5 7 .5 1 8\n0 .2\n1 5 .3 7 3\n1 5 .5 6 8\n3 1 9 .6 9 4\n3 2 0 .7 8 2\n1 1 .5 6 2\n1 1 .5 7 7\n3 5 8 .2 6 7\n3 5 7 .7 7 0\n0 .1 3\n1 5 .5 0 4\n1 5 .4 3 2\n3 2 0 .2 5 0\n3 2 0 .4 3 5\n1 1 .5 6 8\n1 1 .6 0 3\n3 5 7 .9 1 7\n3 5 8 .8 4 5\nJ m\na x =\n0 .0 5\n\u2227 ` =\n0\nk =\n5 k =\n1 0\nR eg\nre t:\nm a x .\u03d5\nR eg\nre t: A v g .\u03d5\nR eg\nre t:\nm a x .\u03d5\nR eg\nre t: A v g .\u03d5\nF ea\ntu re s\n\u03b7 L e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nI 0 .2 5\n2 7 8 .6 5 4\n1 0 6 .8 2 5\n1 0 6 .5 8 4\n1 0 7 .3 6 6\n4 7 5 .5 0 4\n4 1 8 .0 9 6\n4 2 0 .3 7 8\n4 2 1 .0 5 7\n2 3 6 .0 5 8\n5 2 .9 5 1\n5 2 .1 3 1\n5 2 .6 7 8\n4 5 0 .7 5 7\n4 1 8 .3 9 5\n4 1 9 .8 4 0\n4 2 0 .1 2 1\n0 .2\n1 0 6 .8 6 5\n1 0 5 .9 3 9\n4 2 0 .4 3 2\n4 1 9 .4 4 3\n5 2 .8 1 1\n5 3 .0 5 3\n4 1 9 .5 0 7\n4 2 0 .4 5 5\n0 .1 3\n1 0 6 .3 7 8\n1 0 6 .8 3 4\n4 1 8 .8 1 7\n4 1 9 .6 8 6\n5 2 .7 6 8\n5 2 .9 8 0\n4 1 9 .3 4 4\n4 1 9 .5 1 4\nIT 0 .2 5\n2 6 4 .4 2 5\n1 0 3 .5 1 6\n1 0 2 .4 7 6\n1 0 2 .6 4 1\n4 2 8 .1 6 0\n3 9 2 .5 2 2\n3 8 9 .4 9 4\n3 9 0 .3 3 7\n2 3 5 .3 9 7\n5 3 .9 3 9\n5 4 .2 3 4\n5 4 .0 7 2\n4 1 4 .6 0 6\n3 9 5 .6 7 1\n3 9 4 .8 7 1\n3 9 5 .2 4 4\n0 .2\n1 0 0 .7 6 0\n1 0 0 .9 9 6\n3 8 8 .6 8 1\n3 8 8 .8 0 8\n5 3 .8 7 4\n5 3 .4 5 9\n3 9 5 .6 5 2\n3 9 5 .0 7 5\n0 .1 3\n1 0 2 .9 4 6\n1 0 1 .4 2 0\n3 9 1 .1 1 7\n3 9 0 .8 3 8\n5 3 .4 6 6\n5 3 .4 5 0\n3 9 5 .3 4 6\n3 9 4 .4 2 3\nIT L F\n0 .2 5\n2 6 5 .0 6 4\n1 0 2 .0 7 6\n1 0 2 .8 3 8\n1 0 2 .1 9 7\n4 2 7 .8 9 1\n3 9 1 .8 0 5\n3 9 1 .1 1 9\n2 3 3 .1 5 9\n5 4 .0 9 9\n5 4 .0 0 4\n5 4 .2 7 1\n4 1 1 .1 1 5\n3 9 4 .6 9 5\n3 9 5 .0 3 4\n3 9 6 .0 4 6\n0 .2\n1 0 1 .8 2 5\n1 0 2 .1 5 2\n3 9 0 .3 4 1\n3 8 9 .8 3 8\n3 9 0 .4 3 2\n5 3 .5 9 9\n5 3 .3 1 2\n3 9 4 .8 8 7\n3 9 4 .2 1 9\n0 .1 3\n1 0 1 .5 3 1\n1 0 1 .6 3 8\n3 8 8 .7 8 5\n3 9 0 .1 7 8\n5 3 .7 6 4\n5 3 .7 3 0\n3 9 4 .8 5 4\n3 9 5 .1 2 1\nT a b le\n6 :\nE va\nlu a ti\no n\no f\nth e\nim p\no rt\na n ce\no f\np a tt\ner n\nfe a tu\nre s\na n d\nco m\np a ri\nso n\no f\nL e t S IP\n-d is c -c d f v s L e t S IP\n-c d f\nv s. L e t S IP\nw .r\n.t . d iff\ner en\nt va\nlu es\no f J m a x . R\nes u lt s a re a g g re g a te d ov er 4 d a ta se ts : K rv sk p , C h es s, M u sh ro o m a n d A n n ea l. J m a x = 0 .0 5 \u2227 ` = 1 k = 5 k = 1 0 R eg re t: m a x .\u03d5 R eg re t: A v g .\u03d5 R eg re t: m a x .\u03d5\nR eg\nre t: A v g .\u03d5\nF ea\ntu re s\n\u03b7 L e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nI 0 .2 5\n4 .8 4 6\n2 7 7 .3 5 1\n2 7 7 .3 6 9\n2 7 7 .3 6 9\n1 5 3 .2 3 0\n3 3 8 .8 4 1\n3 3 9 .4 3 9\n3 3 9 .2 9 3\n2 .0 5 2\n2 7 6 .5 8 7\n2 7 6 .6 2 4\n2 7 6 .6 2 6\n1 7 3 .4 8 2\n3 4 6 .3 3 6\n3 4 6 .2 4 1\n3 4 6 .1 6 7\n0 .2\n2 7 7 .2 8 9\n2 7 7 .2 8 9\n3 3 8 .7 7 6\n3 3 8 .6 3 5\n2 7 6 .6 5 3\n2 7 6 .6 5 3\n3 4 6 .4 4 2\n3 4 6 .5 2 1\n0 .1 3\n2 7 7 .2 9 6\n2 7 7 .2 9 6\n3 3 8 .9 6 3\n3 3 9 .0 0 4\n2 7 6 .7 6 7\n2 7 6 .7 6 7\n3 4 6 .3 1 1\n3 4 6 .2 9 6\nIT 0 .2 5\n3 .7 2 1\n2 7 7 .0 8 3\n2 7 7 .1 1 4\n2 7 7 .1 1 4\n1 3 6 .0 9 1\n3 3 7 .7 4 4\n3 3 7 .9 7 6\n3 3 7 .9 7 6\n1 .8 0 5\n2 7 6 .3 8 9\n2 7 6 .5 9 9\n2 7 6 .5 9 9\n1 5 9 .6 4 6\n3 4 4 .8 7 6\n3 4 4 .3 2 4\n3 4 4 .2 9 4\n0 .2\n2 7 7 .1 1 6\n2 7 7 .1 1 3\n3 3 7 .8 3 4\n3 3 7 .7 5 6\n2 7 6 .5 5 9\n2 7 6 .5 5 9\n3 4 4 .6 0 5\n3 4 4 .6 9 0\n0 .1 3\n2 7 7 .1 1 6\n2 7 7 .1 1 6\n3 3 8 .1 6 0\n3 3 8 .1 6 0\n2 7 6 .5 9 3\n2 7 6 .5 9 3\n3 4 4 .5 5 7\n3 4 4 .5 5 0\nIT L F\n0 .2 5\n3 .8 1 3\n2 7 7 .0 8 3\n2 7 7 .1 1 4\n2 7 7 .1 1 4\n1 3 9 .1 8 4\n3 3 7 .7 4 4\n3 3 8 .2 5 9\n3 3 7 .9 7 6\n1 .7 6 5\n2 7 6 .3 9 5\n2 7 6 .5 9 6\n2 7 6 .5 9 6\n1 6 5 .9 8 2\n3 4 4 .7 0 2\n3 4 4 .4 7 9\n3 4 4 .4 5 4\n0 .2\n2 7 7 .1 1 6\n2 7 7 .1 1 6\n3 3 7 .8 3 4\n3 3 7 .8 3 4\n2 7 6 .5 6 0\n2 7 6 .5 5 9\n3 4 4 .4 9 7\n3 4 4 .6 4 6\n0 .1 3\n2 7 7 .1 1 6\n2 7 7 .1 1 5\n3 3 8 .1 6 0\n3 3 8 .0 7 8\n2 7 6 .5 9 4\n2 7 6 .5 9 4\n3 4 4 .4 0 9\n3 4 4 .4 0 9\nJ m\na x =\n0 .0 5\n\u2227 ` =\n0\nk =\n5 k =\n1 0\nR eg\nre t:\nm a x .\u03d5\nR eg\nre t: A v g .\u03d5\nR eg\nre t:\nm a x .\u03d5\nR eg\nre t: A v g .\u03d5\nF ea\ntu re s\n\u03b7 L e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nL e t S IP\nL e t S IP\n-c d f\nL e t S IP\n-d is c -c d f\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nE X P\nL in e a r\nI 0 .2 5\n6 3 .4 7 1\n3 0 9 .1 3 7\n3 1 0 .5 9 5\n3 1 1 .0 3 0\n1 9 3 .5 1 6\n3 5 3 .5 7 3\n3 5 4 .3 5 0\n3 5 4 .3 5 0\n3 4 .4 8 0\n2 9 1 .0 9 9\n2 9 1 .6 4 0\n2 9 1 .3 9 3\n1 9 2 .3 3 2\n3 5 3 .9 1 8\n3 5 3 .9 1 9\n3 5 3 .9 3 8\n0 .2\n3 1 1 .7 4 6\n3 1 1 .7 3 6\n3 5 4 .4 4 6\n3 5 4 .4 6 3\n2 9 0 .9 7 2\n2 9 0 .6 3 7\n3 5 3 .6 5 9\n3 5 3 .3 9 6\n0 .1 3\n3 0 8 .8 1 7\n3 0 8 .9 7 8\n3 5 3 .4 5 0\n3 5 3 .4 8 2\n2 9 1 .0 2 0\n2 9 0 .8 8 9\n3 5 3 .5 6 2\n3 5 3 .5 0 7\nIT 0 .2 5\n5 2 .4 3 8\n3 0 7 .7 8 3\n3 0 8 .2 5 6\n3 0 8 .0 4 1\n1 7 9 .8 6 7\n3 5 2 .2 5 5\n3 5 2 .3 3 8\n3 5 2 .4 2 6\n2 8 .0 2 1\n2 9 1 .1 2 4\n2 9 1 .4 2 8\n2 9 1 .4 5 4\n1 8 4 .7 9 3\n3 5 2 .4 1 8\n3 5 2 .7 1 6\n3 5 2 .6 9 3\n0 .2\n3 0 8 .6 6 1\n3 0 8 .2 9 4\n3 5 2 .5 1 6\n3 5 2 .4 0 5\n2 9 0 .2 0 8\n2 9 0 .6 4 6\n3 5 2 .4 8 2\n3 5 2 .4 1 3\n0 .1 3\n3 0 7 .1 1 1\n3 0 7 .1 1 1\n1 7 9 .8 6 7\n3 5 2 .2 5 5\n3 5 1 .9 5 9\n3 5 1 .9 5 9\n2 9 0 .9 0 9\n2 9 0 .9 0 9\n3 5 2 .0 9 2\n3 5 2 .0 9 2\nIT L F\n0 .2 5\n5 2 .5 5 4\n3 0 7 .7 0 7\n3 0 8 .2 5 6\n3 0 8 .2 5 6\n1 7 9 .2 1 4\n3 5 2 .2 3 8\n3 5 2 .3 3 8\n3 5 2 .3 3 8\n2 7 .7 5 1\n2 9 0 .2 6 5\n2 9 1 .2 2 1\n2 9 1 .2 7 7\n1 8 3 .8 6 1\n3 5 1 .8 5 0\n3 5 2 .5 7 8\n3 5 2 .5 8 2\n0 .2\n3 0 8 .4 5 3\n3 0 8 .4 5 3\n3 5 2 .3 9 5\n3 5 2 .3 9 5\n2 9 0 .3 0 0\n2 9 0 .3 1 9\n3 5 2 .3 4 2\n3 5 2 .3 5 2\n0 .1 3\n3 0 7 .1 1 1\n3 0 7 .1 1 1\n3 5 1 .9 5 9\n3 5 1 .9 5 9\n2 9 0 .7 0 0\n2 9 0 .7 0 0\n3 5 2 .0 5 7\n3 5 2 .0 5 7"
        },
        {
            "heading": "7 Related Work",
            "text": "Pattern mining has been faced with the problem of returning too large, and potentially uninteresting result sets since early on.\nThe first proposed solution consisted of condensed representations [3, 22, 23] by reducing the redudancy of the covers of patterns. Top-k mining [19, 24] is efficient but results in strongly related, redundant patterns showing a lack of diversity. Pattern set mining [4, 25\u201327] takes into account the relationships between the patterns, which allows to control redundancy and lead to small sets. It requires that one mines an ideally large pattern set to select from, and typically doesn\u2019t involve the user.\nConstraint-based pattern mining has been leveraged as a general, flexible solution to pattern mining for a while [28\u201330], and has since seen numerous improvements, including closed itemset mining [31] and, eventually mining of diverse itemsets [15].\nThe most recent proposal to dealing with the question of finding interesting patterns involves the user, via interactive pattern mining [5] often involving sampling [7], with LetSIP [6] one of the end points of this development."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we have proposed an improvement to the state-of-the art of iterative pattern mining: instead of using static low-level features that have been pre-defined before the process starts, our approach learns more complex descriptors from the user-defined pattern ranking. These features allow to capture the importance of item interactions, and, as shown experimentally, lead to lower cumulative and individual regret than using low-level features. We have explored two multiplicative aggregation functions for mapping weights learned for complex features back to their component items, and find that the exponential multiplicative factor gives better results on most of the data sets we worked with. Furthermore, we have proposed to use cdFlexics as the sampling component in LetSIP to maximize the pattern diversity of each query and showed a straightforward combination of both improvements in LetSIP. cdFlexics takes the XOR-constraint based data partitioning of Flexics and extends it with the diversity constraint closedDiversity, introduced in [15]. The results show convincing improvements both in terms of learning more accurate pattern rankings and CPU-times. We have evaluated our proposal only on itemset data so far since the majority of existing work, including the method that we extended, is defined for this kind of data. But the importance of using complex dynamic features can be expected to be even higher when interactively mining complex, i.e. sequential, tree-, or graph-structured data. We will explore this direction in future work.\nDeclarations\n\u2022 Conflict of interest/Competing interests: all authors declare not having any financial or non-financial interests that are directly or indirectly related to the work submitted for publication. \u2022 Availability of data, materials and code: They will be available in the final submission."
        },
        {
            "heading": "Appendix A Detailed view of results for other",
            "text": "datasets (LetSIP-disc vs. LetSIP)\nFigures A1-A5 plot a detailed view of comparison between LetSIP-disc-Exp and LetSIP (cumulative and non-cumulative regret) for different datasets w.r.t. maximal quality, k = 10 and ` = 1."
        },
        {
            "heading": "Appendix B Detailed view of results for other",
            "text": "datasets (LetSIP-cdf vs. LetSIP)\nFigures B6-B9 plot a detailed view of comparison between LetSIP-cdf and LetSIP (cumulative and non-cumulative regret) for different datasets w.r.t. maximal quality, k = 10 and ` = 1."
        },
        {
            "heading": "Appendix C Complementary results for CPU-times comparison",
            "text": "Figure C11 compares the CPU-times of LetSIP, LetSIP-cdf, LetSIP-discExp and LetSIP-disc-cdfw.r.t. the ILFT feature combination, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\n6\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(a) Chess: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(b) Chess: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et s\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(c) German-credit: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m ul at iv e re gr et s\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(d) German-credit: non cumulative regret.\nFig. A1: A detailed view of comparison between LetSIP-disc-Exp and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et s\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(a) Heart-cleveland: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\nheart-cleveland - \u03b8=0.388 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22124\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\nheart-cleveland - \u03b8=0.388 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nheart-cleveland - \u03b8=0.388 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(b) Heart-cleveland: non cumulative regret.\n0 20 40 60 80 100 Iterations\n100\n101\n102\nCu m\nul at\niv e\nre gr\net s\nhepatitis - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n100\n101\n102\nCu m\nul at\niv e\nre gr\net s\nhepatitis - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n100\n101\n102\nCu m\nul at\niv e\nre gr\net s\nhepatitis - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(c) Hepatitis: cumulative regret.\n0 20 40 60 80 100 Iterations\n9.965\u00d710\u22121\n9.97\u00d710\u22121\n9.975\u00d710\u22121\n9.98\u00d710\u22121\n9.985\u00d710\u22121\n9.99\u00d710\u22121\nRe gr\net s p\ner it\ner at\nio ns\nhepatitis - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n9.97\u00d710\u22121\n9.975\u00d710\u22121\n9.98\u00d710\u22121\n9.985\u00d710\u22121\nRe gr\net s p\ner it\ner at\nio ns\nhepatitis - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n9.965\u00d710\u22121\n9.97\u00d710\u22121\n9.975\u00d710\u22121\n9.98\u00d710\u22121\n9.985\u00d710\u22121\nRe gr\net s p\ner it\ner at\nio ns\nhepatitis - \u03b8=0.35 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(d) Hepatitis: non cumulative regret.\nFig. A2: A detailed view of comparison between LetSIP-disc-Exp and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n2\u00d710\u22121\n3\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(a) Kr-vs-kp: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(b) Kr-vs-kp: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nmushroom - \u03b8=0.1 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\nCu m\nul at\niv e\nre gr\net s\nmushroom - \u03b8=0.1 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\nCu m\nul at\niv e\nre gr\net s\nmushroom - \u03b8=0.1 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(c) Mushroom: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nmushroom - \u03b8=0.1 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nmushroom - \u03b8=0.1 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nmushroom - \u03b8=0.1 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(d) Mushroom: non cumulative regret.\nFig. A3: A detailed view of comparison between LetSIP-disc-Exp and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nsoybean - \u03b8=0.044 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\n6\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nsoybean - \u03b8=0.044 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\n6\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nsoybean - \u03b8=0.044 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(a) Soybean: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\nso bean - \u03b8=0.044 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nso bean - \u03b8=0.044 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nso bean - \u03b8=0.044 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(b) Soybean: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et s\nvote - \u03b8=0.057 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et\nvote - \u03b8=0.057 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\n6\u00d710\u22121\nCu m ul at iv e re gr et\nvote - \u03b8=0.057 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(c) Vote: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nvote - \u03b8=0.057 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nvote - \u03b8=0.057 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nvote - \u03b8=0.057 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(d) Vote: non cumulative regret.\nFig. A4: A detailed view of comparison between LetSIP-disc-Exp and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nzoo-1 - \u03b8=0.09 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nzoo-1 - \u03b8=0.09 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nzoo-1 - \u03b8=0.09 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(a) Zoo-1: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\noo-1 - \u03b8=0.09 - l=1 - k=10 - \u03b7=0.13- Items\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\noo-1 - \u03b8=0.09 - l=1 - k=10 - \u03b7=0.13- Items\u2212 Transactions\nLetSIP LetSIP-Disc-EXP\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\noo-1 - \u03b8=0.09 - l=1 - k=10 - \u03b7=0.13\nLetSIP LetSIP-Disc-EXP\n(b) Zoo-1: non cumulative regret.\nFig. A5: A detailed view of comparison between LetSIP-disc-Exp and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101 102 Cu m ul at iv e re gr et s chess - \u03b8=0.63 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m\nul at\niv e\nre gr\net s\nchess - \u03b8=0.63 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(a) Chess: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr\net s p\ner it\ner at\nio ns\nchess - \u03b8=0.63 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr\net s p\ner it\ner at\nio ns\nchess - \u03b8=0.63 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr\net s p\ner it\ner at\nio ns\nchess - \u03b8=0.63 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(b) Chess: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22122\n10\u22121 Cu m ul at iv e re gr et s\ngerman-credit - \u03b8=0.35 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22122\n10\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22122\n10\u22121\nCu m ul at iv e re gr et\ngerman-credit - \u03b8=0.35 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(c) German-credit: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\ngerman-credit - \u03b8=0.35 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\ngerman-credit - \u03b8=0.35 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\ngerman-credit - \u03b8=0.35 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(d) German-credit: non cumulative regret.\nFig. B6: A detailed view of comparison between LetSIP-cdf and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m ul at iv e re gr et s\nheart-cleveland - \u03b8=0.388 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m ul at iv e re gr et\nheart-cleveland - \u03b8=0.388 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nCu m ul at iv e re gr et\nheart-cleveland - \u03b8=0.388 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(a) Heart-cleveland: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nheart-cleveland - \u03b8=0.388 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\nRe gr\net s p\ner it\ner at\nio ns\nheart-cleveland - \u03b8=0.388 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nheart-cleveland - \u03b8=0.388 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(b) Heart-cleveland: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m ul at iv e re gr et s\nhepatitis - \u03b8=0.35 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m ul at iv e re gr et\nhepatiti - \u03b8=0.35 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m ul at iv e re gr et\nhepatiti - \u03b8=0.35 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(c) Hepatitis: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22122\n10\u22121\n100\nRe gr\net s p\ner it\ner at\nio ns\nhepatitis - \u03b8=0.35 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22122\n10\u22121\n100\nRe gr\net s p\ner it\ner at\nio ns\nhepatitis - \u03b8=0.35 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22122\n10\u22121\n100\nRe gr\net s p\ner it\ner at\nio ns\nhepatitis - \u03b8=0.35 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(d) Hepatitis: non cumulative regret.\nFig. B7: A detailed view of of comparison between LetSIP-cdf and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n100\n101 102 Cu m ul at iv e re gr et s\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n100\n101\n102\nCu m ul at iv e re gr et s\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m ul at iv e re gr et s\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(a) Kr-vs-kp: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr et s p\ner it er at io ns\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr et s p\ner it er at io ns\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr et s p\ner it er at io ns\nkr-vs-kp - \u03b8=0.63 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(b) Kr-vs-kp: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n100\nCu m ul at iv e re gr et s\nmushroom - \u03b8=0.1 - l=1 - k=10- features=I LetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\nCu m ul at iv e re gr et s\nmushroom - \u03b8=0.1 - l=1 - k=10- features=IT LetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\nCu m ul at iv e re gr et s\nmushroom - \u03b8=0.1 - l=1 - k=10- features=ITFL LetSIP LetSIP-CDF\n(c) Mushroom: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nmushroom - \u03b8=0.1 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nmushroom - \u03b8=0.1 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nmushroom - \u03b8=0.1 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(d) Mushroom: non cumulative regret.\nFig. B8: A detailed view of of comparison between LetSIP-cdf and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n100\nCu m ul at iv e re gr et s\nsoybean - \u03b8=0.044 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\nCu m ul at iv e re gr et s\nsoybean - \u03b8=0.044 - l=1 - k=10- features=IT LetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\nCu m ul at iv e re gr et s\nsoybean - \u03b8=0.044 - l=1 - k=10- features=ITFL LetSIP LetSIP-CDF\n(a) Soybean: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22122\nRe gr et s p\ner it er at io ns\nsoybean - \u03b8=0.044 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nsoybean - \u03b8=0.044 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nsoybean - \u03b8=0.044 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(b) Soybean: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nvote - \u03b8=0.057 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nvote - \u03b8=0.057 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m\nul at\niv e\nre gr\net s\nvote - \u03b8=0.057 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(c) Vote: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nvote - \u03b8=0.057 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nvote - \u03b8=0.057 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr\net s p\ner it\ner at\nio ns\nvote - \u03b8=0.057 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(d) Vote: non cumulative regret.\nFig. B9: A detailed view of of comparison between LetSIP-cdf and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1.\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101 102 Cu m ul at iv e re gr et s\nanneal - \u03b8=0.81 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m ul at iv e re gr et s\nanneal - \u03b8=0.81 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n100\n101\n102\nCu m ul at iv e re gr et s\nanneal - \u03b8=0.81 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(a) Anneal: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr et s p\ner it er at io ns\nanneal - \u03b8=0.81 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr et s p\ner it er at io ns\nanneal - \u03b8=0.81 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\n100\nRe gr et s p\ner it er at io ns\nanneal - \u03b8=0.81 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(b) Anneal: non cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\nCu m ul at iv e re gr et s\nzoo-1 - \u03b8=0.09 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et s\nzoo-1 - \u03b8=0.09 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22121\n2\u00d710\u22121\n3\u00d710\u22121\n4\u00d710\u22121\nCu m ul at iv e re gr et s\nzoo-1 - \u03b8=0.09 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(c) Zoo-1: cumulative regret.\n0 20 40 60 80 100 Iterations\n10\u22124\n10\u22123\n10\u22122\nRe gr et s p\ner it er at io ns\nzoo-1 - \u03b8=0.09 - l=1 - k=10- features=I\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nzoo-1 - \u03b8=0.09 - l=1 - k=10- features=IT\nLetSIP LetSIP-CDF\n0 20 40 60 80 100 Iterations\n10\u22123\n10\u22122\n10\u22121\nRe gr et s p\ner it er at io ns\nzoo-1 - \u03b8=0.09 - l=1 - k=10- features=ITFL\nLetSIP LetSIP-CDF\n(d) Zoo-1: non cumulative regret.\nFig. B10: A detailed view of comparison between LetSIP-cdf and LetSIP and LetSIP (cumulative and non-cumulative regret) for different pattern features w.r.t. maximal quality, k = 10 and ` = 1."
        }
    ],
    "title": "Exploiting complex pattern features for interactive pattern mining",
    "year": 2022
}