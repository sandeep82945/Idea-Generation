{
    "abstractText": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoning Wu"
        },
        {
            "affiliations": [],
            "name": "Zicheng Zhang"
        },
        {
            "affiliations": [],
            "name": "Erli Zhang"
        },
        {
            "affiliations": [],
            "name": "Chaofeng Chen"
        },
        {
            "affiliations": [],
            "name": "Liang Liao"
        },
        {
            "affiliations": [],
            "name": "Annan Wang"
        },
        {
            "affiliations": [],
            "name": "Chunyi Li"
        },
        {
            "affiliations": [],
            "name": "Wenxiu Sun"
        },
        {
            "affiliations": [],
            "name": "Qiong Yan"
        },
        {
            "affiliations": [],
            "name": "Guangtao Zhai"
        },
        {
            "affiliations": [],
            "name": "Weisi Lin"
        }
    ],
    "id": "SP:85d97aa45c9c2afca3584f3288793c87e089d670",
    "references": [
        {
            "authors": [
                "Harsh Agrawal",
                "Karan Desai",
                "Yufei Wang",
                "Xinlei Chen",
                "Rishabh Jain",
                "Mark Johnson",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee",
                "Peter Anderson"
            ],
            "title": "nocaps: novel object captioning at scale",
            "year": 2019
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Jinze Bai",
                "Shuai Bai",
                "Shusheng Yang",
                "Shijie Wang",
                "Sinan Tan",
                "Peng Wang",
                "Junyang Lin",
                "Chang Zhou",
                "Jingren Zhou"
            ],
            "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
            "venue": "arXiv preprint arXiv:2308.12966,",
            "year": 2023
        },
        {
            "authors": [
                "Keqin Chen",
                "Zhao Zhang",
                "Weili Zeng",
                "Richong Zhang",
                "Feng Zhu",
                "Rui Zhao"
            ],
            "title": "Shikra: Unleashing multimodal llm\u2019s referential dialogue magic",
            "venue": "arXiv preprint arXiv:2306.15195,",
            "year": 2023
        },
        {
            "authors": [
                "Xinlei Chen",
                "Hao Fang",
                "Tsung-Yi Lin",
                "Ramakrishna Vedantam",
                "Saurabh Gupta",
                "Piotr Dollar",
                "C. Lawrence Zitnick"
            ],
            "title": "Microsoft coco captions: Data collection and evaluation",
            "year": 2015
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi"
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Glm: General language model pretraining with autoregressive blank infilling",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Yuming Fang",
                "Hanwei Zhu",
                "Yan Zeng",
                "Kede Ma",
                "Zhou Wang"
            ],
            "title": "Perceptual quality assessment of smartphone photography",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Peng Gao",
                "Jiaming Han",
                "Renrui Zhang",
                "Ziyi Lin",
                "Shijie Geng",
                "Aojun Zhou",
                "Wei Zhang",
                "Pan Lu",
                "Conghui He",
                "Xiangyu Yue",
                "Hongsheng Li",
                "Yu Qiao"
            ],
            "title": "Llama-adapter v2: Parameter-efficient visual instruction model",
            "venue": "arXiv preprint arXiv:2304.15010,",
            "year": 2023
        },
        {
            "authors": [
                "Deepti Ghadiyaram",
                "Alan C. Bovik"
            ],
            "title": "Massive online crowdsourced study of subjective and objective picture quality",
            "year": 2016
        },
        {
            "authors": [
                "Tanaya Guha",
                "Vlad Hosu",
                "Dietmar Saupe",
                "Bastian Goldl\u00fccke",
                "Naveen Kumar",
                "Weisi Lin",
                "Victor Martinez",
                "Krishna Somandepalli",
                "Shrikanth Narayanan",
                "Wen-Huang Cheng",
                "Kree McLaughlin",
                "Hartwig Adam",
                "John See",
                "Lai-Kuan Wong"
            ],
            "title": "Atqam/mast\u201920: Joint workshop on aesthetic and technical quality assessment of multimedia and media analytics for societal trends",
            "venue": "In ACM MM,",
            "year": 2020
        },
        {
            "authors": [
                "Vlad Hosu",
                "Hanhe Lin",
                "Tamas Sziranyi",
                "Dietmar Saupe"
            ],
            "title": "Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment",
            "venue": "IEEE TIP,",
            "year": 2020
        },
        {
            "authors": [
                "Jingwen Hou",
                "Weisi Lin",
                "Yuming Fang",
                "Haoning Wu",
                "Chaofeng Chen",
                "Liang Liao",
                "Weide Liu"
            ],
            "title": "Towards transparent deep image aesthetics assessment with tag-based content descriptors",
            "venue": "IEEE TIP,",
            "year": 2023
        },
        {
            "authors": [
                "Dinesh Jayaraman",
                "Anish Mittal",
                "Anush K. Moorthy",
                "Alan C. Bovik"
            ],
            "title": "Objective quality assessment of multiply distorted images",
            "venue": "In ASILOMAR,",
            "year": 2012
        },
        {
            "authors": [
                "Shu Kong",
                "Xiaohui Shen",
                "Zhe Lin",
                "Radomir Mech",
                "Charless Fowlkes"
            ],
            "title": "Photo aesthetics ranking network with attributes and content adaptation",
            "year": 2016
        },
        {
            "authors": [
                "Xin Lai",
                "Zhuotao Tian",
                "Yukang Chen",
                "Yanwei Li",
                "Yuhui Yuan",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "Lisa: Reasoning segmentation via large language model",
            "venue": "arXiv preprint arXiv:2308.00692,",
            "year": 2023
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu"
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2305.03726,",
            "year": 2023
        },
        {
            "authors": [
                "Bohao Li",
                "Rui Wang",
                "Guangzhi Wang",
                "Yuying Ge",
                "Yixiao Ge",
                "Ying Shan"
            ],
            "title": "Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Chunyi Li",
                "Zicheng Zhang",
                "Haoning Wu",
                "Wei Sun",
                "Xiongkuo Min",
                "Xiaohong Liu",
                "Guangtao Zhai",
                "Weisi Lin"
            ],
            "title": "Agiqa-3k: An open database for ai-generated image quality assessment, 2023c",
            "year": 2023
        },
        {
            "authors": [
                "Dingquan Li",
                "Tingting Jiang",
                "Weisi Lin",
                "Ming Jiang"
            ],
            "title": "Which has better visual quality: The clear blue sky or a blurry animal",
            "venue": "IEEE TMM,",
            "year": 2019
        },
        {
            "authors": [
                "Hanhe Lin",
                "Vlad Hosu",
                "Dietmar Saupe"
            ],
            "title": "Kadid-10k: A large-scale artificially distorted iqa database",
            "venue": "In QoMEX,",
            "year": 2019
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Yuheng Li",
                "Yong Jae Lee"
            ],
            "title": "Improved baselines with visual instruction tuning, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Liu",
                "Haodong Duan",
                "Yuanhan Zhang",
                "Bo Li",
                "Songyang Zhang",
                "Wangbo Zhao",
                "Yike Yuan",
                "Jiaqi Wang",
                "Conghui He",
                "Ziwei Liu",
                "Kai Chen",
                "Dahua Lin"
            ],
            "title": "Mmbench: Is your multi-modal model an all-around player?, 2023c",
            "year": 2023
        },
        {
            "authors": [
                "Jiaying Lu",
                "Jinmeng Rao",
                "Kezhen Chen",
                "Xiaoyuan Guo",
                "Yawen Zhang",
                "Baochen Sun",
                "Carl Yang",
                "Jie Yang"
            ],
            "title": "Evaluation and mitigation of agnosia in multimodal large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Kenneth Marino",
                "Mohammad Rastegari",
                "Ali Farhadi",
                "Roozbeh Mottaghi"
            ],
            "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge",
            "venue": "In Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "Claudio Michaelis",
                "Benjamin Mitzkus",
                "Robert Geirhos",
                "Evgenia Rusak",
                "Oliver Bringmann",
                "Alexander S. Ecker",
                "Matthias Bethge",
                "Wieland Brendel"
            ],
            "title": "Benchmarking robustness in object detection: Autonomous driving when winter is coming",
            "year": 1907
        },
        {
            "authors": [
                "Anish Mittal",
                "Rajiv Soundararajan",
                "Alan C. Bovik"
            ],
            "title": "Making a \u201ccompletely blind\u201d image quality analyzer",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2013
        },
        {
            "authors": [
                "Naila Murray",
                "Luca Marchesotti",
                "Florent Perronnin"
            ],
            "title": "Ava: A large-scale database for aesthetic visual analysis",
            "venue": "In CVPR, pp",
            "year": 2012
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wenhui Wang",
                "Li Dong",
                "Yaru Hao",
                "Shaohan Huang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "title": "Kosmos-2: Grounding multimodal large language models to the world",
            "venue": "ArXiv, abs/2306,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Shaolin Su",
                "Vlad Hosu",
                "Hanhe Lin",
                "Yanning Zhang",
                "Dietmar Saupe"
            ],
            "title": "Koniq++ : Boosting noreference image quality assessment in the wild by jointly predicting image quality and defects",
            "venue": "In The British Machine Vision Conference (BMVC),",
            "year": 2021
        },
        {
            "authors": [
                "Hossein Talebi",
                "Peyman Milanfar. Nima"
            ],
            "title": "Neural image assessment",
            "venue": "IEEE TIP,",
            "year": 2018
        },
        {
            "authors": [
                "MosaicML NLP Team"
            ],
            "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023",
            "venue": "URL www.mosaicml.com/blog/mpt-7b",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jianyi Wang",
                "Kelvin C.K. Chan",
                "Chen Change Loy"
            ],
            "title": "Exploring clip for assessing the look and feel of images, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Yilin Wang",
                "Junjie Ke",
                "Hossein Talebi",
                "Joong Gon Yim",
                "Neil Birkbeck",
                "Balu Adsumilli",
                "Peyman Milanfar",
                "Feng Yang"
            ],
            "title": "Rich features for perceptual quality assessment of ugc videos",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Haoning Wu",
                "Chaofeng Chen",
                "Jingwen Hou",
                "Liang Liao",
                "Annan Wang",
                "Wenxiu Sun",
                "Qiong Yan",
                "Weisi Lin"
            ],
            "title": "Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling",
            "year": 2022
        },
        {
            "authors": [
                "Haoning Wu",
                "Chaofeng Chen",
                "Liang Liao",
                "Jingwen Hou",
                "Wenxiu Sun",
                "Qiong Yan",
                "Jinwei Gu",
                "Weisi Lin"
            ],
            "title": "Neighbourhood representative sampling for efficient end-to-end video quality assessment, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Haoning Wu",
                "Liang Liao",
                "Chaofeng Chen",
                "Jingwen Hou Hou",
                "Erli Zhang",
                "Annan Wang",
                "Wenxiu Sun",
                "Qiong Yan",
                "Weisi Lin"
            ],
            "title": "Exploring opinion-unaware video quality assessment with semantic affinity criterion",
            "venue": "In International Conference on Multimedia and Expo (ICME),",
            "year": 2023
        },
        {
            "authors": [
                "Haoning Wu",
                "Erli Zhang",
                "Liang Liao",
                "Chaofeng Chen",
                "Jingwen Hou",
                "Annan Wang",
                "Wenxiu Sun",
                "Qiong Yan",
                "Weisi Lin"
            ],
            "title": "Exploring video quality assessment on user generated contents from aesthetic and technical perspectives",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Haoning Wu",
                "Erli Zhang",
                "Liang Liao",
                "Chaofeng Chen",
                "Jingwen Hou",
                "Annan Wang",
                "Wenxiu Sun",
                "Qiong Yan",
                "Weisi Lin"
            ],
            "title": "Towards explainable video quality assessment: A database and a language-prompted approach",
            "venue": "In ACM MM,",
            "year": 2023
        },
        {
            "authors": [
                "Jiazheng Xu",
                "Xiao Liu",
                "Yuchen Wu",
                "Yuxuan Tong",
                "Qinkai Li",
                "Ming Ding",
                "Jie Tang",
                "Yuxiao Dong"
            ],
            "title": "Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Qinghao Ye",
                "Haiyang Xu",
                "Guohai Xu",
                "Jiabo Ye",
                "Ming Yan",
                "Yiyang Zhou",
                "Junyang Wang",
                "Anwen Hu",
                "Pengcheng Shi",
                "Yaya Shi",
                "Chaoya Jiang",
                "Chenliang Li",
                "Yuanhong Xu",
                "Hehong Chen",
                "Junfeng Tian",
                "Qian Qi",
                "Ji Zhang",
                "Fei Huang"
            ],
            "title": "mplug-owl: Modularization empowers large language models with multimodality, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Zhenqiang Ying",
                "Haoran Niu",
                "Praful Gupta",
                "Dhruv Mahajan",
                "Deepti Ghadiyaram",
                "Alan Bovik"
            ],
            "title": "From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality",
            "year": 2020
        },
        {
            "authors": [
                "Zhenqiang Ying",
                "Maniratnam Mandal",
                "Deepti Ghadiyaram",
                "Alan Bovik"
            ],
            "title": "Patch-vq: \u2019patching up\u2019 the video quality problem",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier"
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2014
        },
        {
            "authors": [
                "Cheng Zhang",
                "Shaolin Su",
                "Yu Zhu",
                "Qingsen Yan",
                "Jinqiu Sun",
                "Yanning Zhang"
            ],
            "title": "Exploring and evaluating image restoration potential in dynamic scenes",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Pan Zhang",
                "Xiaoyi Dong",
                "Bin Wang",
                "Yuhang Cao",
                "Chao Xu",
                "Linke Ouyang",
                "Zhiyuan Zhao",
                "Shuangrui Ding",
                "Songyang Zhang",
                "Haodong Duan",
                "Wenwei Zhang",
                "Hang Yan",
                "Xinyue Zhang",
                "Wei Li",
                "Jingwen Li",
                "Kai Chen",
                "Conghui He",
                "Xingcheng Zhang",
                "Yu Qiao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "title": "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A. Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Zicheng Zhang",
                "Wei Sun",
                "Tao Wang",
                "Wei Lu",
                "Quan Zhou",
                "Qiyuan Wang",
                "Xiongkuo Min",
                "Guangtao Zhai"
            ],
            "title": "Subjective and objective quality assessment for in-the-wild computer graphics images",
            "venue": "ACM TOMM,",
            "year": 2023
        },
        {
            "authors": [
                "Zicheng Zhang",
                "Wei Sun",
                "Yingjie Zhou",
                "Haoning Wu",
                "Chunyi Li",
                "Xiongkuo Min",
                "Xiaohong Liu"
            ],
            "title": "Advancing zero-shot digital human quality assessment through text-prompted evaluation, 2023c",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The emergent large language models (LLMs) such as ChatGPT and Bard, as well as their excellent open-source counterparts (e.g., LLaMA (Touvron et al., 2023), MPT (Team, 2023)), have served as powerful general-purpose assistants, which opens a new era for artificial intelligence (AI) from targeting specific tasks towards general intelligence. Following the advancements of LLMs, multimodality large language models (MLLMs), as represented by LLaVA (Liu et al., 2023b), MiniGPT4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023), and Otter (Li et al., 2023a), have brought exciting progresses on the vision field as well. They are capable of providing robust general-level abilities on visual perception/understanding and can even seamlessly dialog and interact with humans through natural language. While such abilities of MLLMs have been explored and validated on several vision-language tasks such as image captioning (Chen et al., 2015), visual question answering (Antol et al., 2015), cross-modality grounding (Peng et al., 2023), and traditional vision tasks such as image classification or segmentation (Lai et al., 2023), most attention is paid to the high-level perception and understanding of visual contents. Meanwhile, the ability of MLLMs remains not clear on low-level visual perception and understanding, which play significant roles in image quality assessment (IQA) (Hosu et al., 2020; Fang et al., 2020) and its associated tasks on perceiving visual distortions (noises, blurs) (Su et al., 2021; Wu et al., 2023d) and other low-level attributes (color, lighting, composition, style, etc) (Kong et al., 2016) that may relate to aesthetics and emotions of natural photos (Murray et al., 2012) and human preferences on emerging computer-\nar X\niv :2\n30 9.\n14 18\n1v 3\n[ cs\n.C V\n] 1\nJ an\n2 02\n4\ngraphics generated (Zhang et al., 2023b) or AI-generated images (Li et al., 2023c; Xu et al., 2023). These low-level visual abilities are strongly associated with a wide range of applications, such as recommendation (Wu et al., 2023c), guidance on camera systems (Zhang et al., 2022), or visual quality enhancement (Zhang et al., 2018). Henceforth, it is crucial to evaluate the current abilities of these general-purpose foundation models in low-level visual perception and understanding, to ideally relieve extensive human resources to give feedback on every specific low-level task.\nIn our work, we propose the first systematic benchmark to measure the low-level visual perception and understanding abilities of MLLMs. Our benchmark is constructed around a key question:\nHow do MLLMs emulate human ability related to low-level visual perception and understanding?\nA simple answer is language, which is the fundamental property of MLLMs. Specifically, we define two emerging language abilities of MLLMs on low-level vision as follows:\n\u2022 Ability 1 (A1): Perception of Low-level Attributes. As shown in Fig. 1(a), like a human, an MLLM should be able to respond accurately to simple questions related to low-level attributes, e.g answering \u2018No\u2019 for a blurry image when queried with \u2018Is this image clear?\u2019\n\u2022 Ability 2 (A2): Description via Natural Language. As shown in Fig. 1(b), like a human, an MLLM should be able to describe the quality and other low-level information for an image with natural language. The descriptions should be both complete and accurate.\nTo systematically evaluate the low-level perception ability (A1) on various low-level attributes under diverse circumstances, we construct the LLVisionQA dataset, including 2,990 images from 10 diverse sources. Aligned with existing practices (Liu et al., 2023c; Lu et al., 2023), each image in LLVisionQA is equipped with a question, alongside a correct answer and false candidate answers. In LLVisionQA, we design three diverse types of questions: Yes-or-No questions, What questions, and How questions. Moreover, we divide low-level concerns into four quadrants, via two axes: (1) distortions (blur, noises, etc) vs other low-level attributes (color, lighting, composition, etc) (Guha et al.,\n2020). (2) global perception (e.g., sharpness of the whole picture) vs local content-related in-context perception (e.g., whether the red flower is in focus) (Li et al., 2019). With three types of questions and four quadrants of concerns, the proposed LLVisionQA dataset provides a holistic, diverse, and balanced benchmark for the perception ability on low-level visual attributes of MLLMs.\nFor the description ability (A2), given that the output description is expected to be complex (without fixed formats), we propose the LLDescribe dataset by inviting experts to write long golden low-level descriptions (average 58 words per description) for 499 images, which serve as the reference texts for the single-modal GPT to evaluate MLLM output descriptions. The quality of MLLM descriptions is evaluated through three dimensions: completeness (punish missing information), preciseness (punish outputs controversial with reference), as well as relevance (punish outputs irrelevant to lowlevel attributes). With golden descriptions and the multi-dimensional evaluation process participated by GPT, we comprehensively evaluate the low-level description ability of MLLMs.\nBesides the two emerging language abilities, we also evaluate MLLMs on the traditional IQA task, a more abstract task that requires understanding on human opinions of low-level attributes, as follows:\n\u2022 Ability 3 (A3): Precise Assessment Aligned with Human Opinions. As depicted in Fig. 1(c), an MLLM should be able to predict quantifiable quality scores for images, which can be aligned with the human-rated mean opinion scores (MOS) on low-level visual appearances.\nFor the assessment ability (A3), we utilize plenty of existing IQA databases (Hosu et al., 2020; Lin et al., 2019; Li et al., 2023c) that focus on various low-level appearances of images, to benchmark MLLMs within conventional IQA settings. Specifically, we notice that MLLMs encounter difficulties in providing sufficiently quantifiable outputs, whether instructed to directly rate with texts or provide numerical outputs. To solve this challenge, we propose to extract the softmax pooling result on the logits of the two most frequent tokens (good and poor) under the response template of MLLMs (Fig 1(c)) as their quality predictions. Our studies prove that the proposed softmax-based strategy is generally better correlated with human perception than direct token outputs of MLLMs (via argmax), which bridges between these emergent MLLMs and the traditional IQA task settings. Under this strategy, we evaluate all MLLMs on their precise assessment ability by measuring the correlations between their predictions and human opinion scores in various IQA databases.\nIn summary, we systematically explore the potential of MLLMs on three low-level visual abilities: perception, description, and assessment. The three realms compose into the proposed Q-Bench, a MLLM benchmark on low-level visual tasks. Our contributions can be summarized as three-fold:\n\u2022 We build a benchmark for MLLMs on low-level perception ability. To achieve this, we construct a first-of-its-kind balanced and comprehensive LLVisionQA dataset with 2,990 images with one low-level-related question-answer pair for each image. The LLVisionQA includes three question types and four quadrants of low-level concerns to ensure diversity.\n\u2022 We define a benchmark process to evaluate the low-level description ability of MLLMs, including an LLDescription dataset of 499 images with expert-labelled long golden quality descriptions, and a GPT-assisted evaluation to rate MLLM-descriptions in terms of completeness, preciseness, and relevance compared with golden descriptions.\n\u2022 To evaluate precise quality assessment ability, we propose a unified softmax-based quality prediction strategy for all MLLMs based on their probability outputs. With its effectiveness validated in our experiments, the proposed strategy sets up a bridge between generalpurpose MLLMs and traditional IQA tasks that requires quantifiable scores as outputs."
        },
        {
            "heading": "2 CONSTRUCTING THE Q-BENCH",
            "text": ""
        },
        {
            "heading": "2.1 GENERAL PRINCIPLES",
            "text": "Focusing on Low-level Visual Abilities of MLLMs. Unlike existing MLLM benchmarks (Li et al., 2023b; Liu et al., 2023c; Lu et al., 2023) that aim at all-round abilities, the tasks in Q-Bench are constrained with two basic principles: (1) Requiring perception and/or understanding on lowlevel attributes of images; (2) Not requiring reasoning (i.e. why) or outside knowledge (Marino et al., 2019). We adhere to the principles in designing the perception, description, and assessment tasks, making the proposed Q-bench a focused reflection on the low-level visual abilities of MLLMs.\nCovering Diverse Low-level Appearances. To cover diverse low-level appearances, we collect multi-sourced images for each task, as depicted in Tab. 1. Among all images in the perception and description tasks, two-thirds are in-the-wild images directly collected from social media posts, smartphones or professional photography. The rest one-third images are collected after various artificial distortions, or via generative processes (CGI, AIGC). Furthermore, we employ k-means clustering for the low-level attribute indicators to certify that the sub-sampled images retain high diversity. In the assessment task, full images of 7 IQA datasets within all three source types are evaluated through traditional IQA metrics. The diverse and multiple sources of images morph the Q-bench into a holistic and balanced benchmark to fairly evaluate low-level-related abilities.\n2.2 BENCHMARK ON LOW-LEVEL PERCEPTION ABILITY\nIn the first task of Q-Bench, we evaluate the low-level perception ability of MLLMs to examine whether they can answer simple natural queries related to low-level attributes. For this purpose, we first collect 2,990 images (I) from multiple sources (see Table 1) with diverse low-level concerns. Then, we collect one low-level-related question (Q), one correct answer to the question (C), and 1-3 candidate false answers (F) for each image. The 2,990 (I,Q,C,F) tuples compose into the LLVisionQA dataset (as illustrated in Fig. 2), the first visual question answering (VQA) dataset in the low-level computer vision field. Specifically, the questions in LLVisionQA cover four quadrants of distinct low-level concerns (in Sec. 2.2.1) and three question types (in Sec. 2.2.2). After constructing the dataset, the (I,Q,C,F) are together fed into MLLMs for evaluation, while their outputs are further examined by GPT to judge correctness (in Sec. 2.2.3). The details are elaborated as follows."
        },
        {
            "heading": "2.2.1 QUADRANTS FOR LOW-LEVEL VISUAL CONCERNS",
            "text": "Axis 1: Distortions vs Other Low-level Attributes. The primary axis differentiates two categories of low-level perceptual attributes: 1) technical distortions (Su et al., 2021), seen as the low-level characteristics that directly degrade the quality of images (Ying et al., 2020), and 2) aesthetic-related other low-level attributes (Kong et al., 2016; Hou et al., 2023) which are discernible to human perception and evoke varied emotions. Several studies (Talebi & Milanfar, 2018; Ying et al., 2020; Guha et al., 2020) follow this paradigm and categorize them through a relative golden standard, that whether the attributes directly improve or degrade picture quality (Yes\u2192Distortions; No\u2192Others). Despite this standard, we also enumerate common types of distortions vs other low-level attributes as extra guidance for constructing the LLVisionQA dataset, as listed in Sec. A.1.2.\nAxis 2: Global Perception vs Local In-context Perception. In recent research on low-level vision, it is observed that human perceptions of low-level visuals often intertwine with higher-level contextual comprehension (Li et al., 2019; Wang et al., 2021; Wu et al., 2023a). For instance, a clear sky might lack complex textures yet display exceptional clarity. Furthermore, localized lowlevel appearances can deviate from their overall counterparts, as observed by Wu et al. (2022); Ying et al. (2021). Acknowledging these differences, we curate local in-context perception (Fig. 2 right) questions, that require MLLMs to grasp the content or other context to answer correctly, while other questions are categorized as global perception (Fig. 2 left). (More analysis in Sec. A.1.2.)"
        },
        {
            "heading": "2.2.2 QUESTION TYPES",
            "text": "In the LLVisionQA dataset, we curate three question types, Yes-or-No, What, and How to simulate multiple query forms from humans. The details of the three question types are defined as follows.\nType 1: Yes-or-No Questions. The fundamental type of questions is Yes-or-No, i.e., judgments. Specifically, we notice that some MLLMs especially prefer to respond with yes rather than no. To reduce such biases in our benchmark, though designing questions with answers as yes is easier, we ensure that around 40% of all judgments are with correct answers as no, via querying on contrastive low-level attributes or non-existing low-level attributes. We further measure the bias levels of different MLLMs and present a further de-biased evaluation among them, as discussed in Sec. A.3.2.\nType 2: What Questions. Despite Yes-or-No judgments, the what questions are also a common type of queries in recent MLLM benchmarks such as Lu et al. (2023). In Q-bench, they classify low-level attributes in pictures (e.g., What distortion occurs in the image?), or associated context given specific low-level appearances (for in-context perception questions, e.g., Which object in the image is underexposed?). Unlike Yes-or-No questions, the What questions examine more comprehensive low-level attribute understanding of MLLMs, by requiring correct perception on multiple attributes.\nType 3: How Questions. Despite the two common types, we also include a special type, the How questions, to cover non-extreme appearances (Wu et al., 2023d) of low-level attribute dimensions into our benchmark, as an extension to Yes-or-No questions. As shown in Fig. 2, we can query How is the clarity of the image? for the image with both clear and blurry areas, and answer with Medium. With this special question type, we broaden the Q-bench into finer-grained low-level perception."
        },
        {
            "heading": "2.2.3 GPT-ASSISTED EVALUATION PROCESS",
            "text": "After constructing the LLVisionQA dataset, we feed it to multiple MLLMs to evaluate their abilities on low-level visual perception. The input format to query MLLMs is exemplified as follows:\n#User: How is the clarity of the image? (Question) [IMAGE TOKEN] (Image) Choose between one of the following options: A. High (Correct) B. Medium(Wrong) C. Low(Wrong)\nThe correct and wrong answers are shuffled during the actual evaluation. Moreover, while traditional visual question answering (Antol et al., 2015; Marino et al., 2019) tasks typically employ traditional language metrics (BLEU-4, CIDEr) to compare performance, as observed by recent studies (Ye et al., 2023) and validated by us, most MLLMs cannot consistently provide outputs on instructed formats. Given the question above, different MLLMs may reply \u201cA.\u201d, \u201cHigh\u201d, \u201cThe clarity of the image is high.\u201d, \u201cThe image is of high clarity.\u201d (all correct), which are difficult to be exhaustively-\nincluded under traditional metrics. To solve this problem, we design, validate, and employ a 5-round GPT-assisted evaluation process inspired by Liu et al. (2023c). Under this process, the question, correct answers, and MLLM replies are fed into GPT for evaluation (See Sec. A.2.1 for its details).\n2.3 BENCHMARK ON LOW-LEVEL DESCRIPTION ABILITY\nIn the second task of Q-Bench, we evaluate the language description ability of MLLMs on low-level information. This task is a sibling task of image captioning (Chen et al., 2015; Young et al., 2014; Agrawal et al., 2019) that describes image content with natural language, with a specific concern on the low-level appearance of images. To evaluate this ability automatically, we first derive a golden low-level description dataset, denoted as LLDescribe (Sec. 2.3.1), including one long (average 40 words) golden description provided by experts for each of 499 images. With these golden text descriptions, we are able to measure the quality of output low-level descriptions from MLLMs with a single-modal GPT, under the three dimensions: completeness, preciseness, as well as relevance (Sec 2.3.2). The discussions of the golden descriptions and the evaluation process are as follows.\n2.3.1 DEFINING Golden LOW-LEVEL DESCRIPTIONS FOR IMAGES\nFor the description ability, MLLMs should accurately and completely describe low-level visual information of images. Thus, the ground truths for these MLLMs are also built within a basic principle to cover as many low-level concerns as possible, so long as they are enumerated in Sec. 2.2.1 and occur in images. The resulting golden descriptions in LLDescribe have an average duration of 58 words, notably longer than common high-level image caption datasets (11 for Agrawal et al. (2019), 10 for Chen et al. (2015)). Similar to the LLVisionQA dataset for the perception task, the 499 images in LLDescribe dataset also include all 10 sources (as in Tab. 1) to cover images with diverse low-level appearances. The golden descriptions on different sources of images are depicted in Fig. 3."
        },
        {
            "heading": "2.3.2 EVALUATION WITH SINGLE-MODAL GPT",
            "text": "Recent studies (Zheng et al., 2023) have proved single-modal GPT (OpenAI, 2023) to be a reliable evaluation tool for pure language tasks. Via the LLDescribe dataset, we convert the multi-modality problem into a text-only setting, by matching the MLLM outputs with the golden descriptions with single-modal GPT under three dimensions: (1) Completeness. More matched information with the golden description is encouraged. (2) Preciseness. The controversial information with the golden description is punished. (3) Relevance. More proportions of MLLM outputs should be related to low-level information, instead of others. Each dimension is scored among [0,1,2]. Similar as Sec. 2.2.3, we repeat 5 rounds for each single evaluation and collect the weighted average as the final score. The detailed settings for GPT to evaluate the three dimensions are in Sec. A.2.2."
        },
        {
            "heading": "Large Language Models",
            "text": "2.4 BENCHMARK ON PRECISE QUALITY ASSESSMENT ABILITY\nIn the third task, we benchmark the ability of MLLMs to provide quantitative assessment on the overall low-level appearance of images. Unlike the two tasks above, we utilize existing IQA datasets that are collected across a variety of low-level appearances to evaluate how MLLMs can predict quantitative quality scores aligned with human opinions. All the three types of IQA datasets (inthe-wild, generated, artificially-distorted) as mentioned in Sec. 2.1 are evaluated, to provide a broad range measurement of the assessment ability of MLLMs. Nevertheless, how to collect quantifiable quality scores from MLLMs remains challenging as their outputs only have weak measurability (Sec. 2.4.1). Noticing that MLLMs can provide probabilities of tokens, we employ softmax pooling on the logits of good and poor under a simple and direct prompt template, deriving into quantifiable quality score predicted by MLLMs (Sec. 2.4.2), as illustrated in Fig. 4. Details are as follows."
        },
        {
            "heading": "2.4.1 WEAK MEASURABILITY OF MLLM OUTPUTS",
            "text": "In Q-Bench, we aim to fairly compare the assessment ability between different MLLMs on diverse low-level appearances. Henceforth, our principle is to define a unified, simplest instruction that is applicable for all MLLMs on all IQA datasets. Under this principle, we conduct toy experiments on LLVisionQA on Shikra and LLaVA-v1, with two simple instruction strategies: (A) Direct Instruction, in which the prompt is designed as simple as \u201cRate the quality of the image\u201d. The top-frequency answers are good (78%), and poor (20%), with other outputs almost negligible. (B) Numerical Instruction, in which we specifically instruct numerical ratings, with the prompt: \u201cScore the quality of the image from 1 to 5, with 1 as lowest and 5 as highest.\u201d. Under the numerical strategy, the top-frequency answers are 5 (84%), 1 (9%), and 3 (5%); though within the score range, the frequencies of scores 2 and 4 are both less than 1%. The toy experiments imply the weak measurability of MLLM outputs, given that the answers are statistically 1) biased towards positive, 2) biased towards extreme, and 3) with only two effective scales. Therefore, it is necessary to explore extended strategies for MLLMs to provide truly quantifiable outputs for low-level assessment."
        },
        {
            "heading": "2.4.2 A SOFTMAX-BASED EVALUATION STRATEGY",
            "text": "Given the above observations, we design the softmax-based evaluation strategy (Fig. 4) to reduce the negative impacts of the biases and lack of scales. To start with, we design our strategy within the Direct Instruction, which is more general and less biased than the Numerical Instruction. The strategy is based on the observation that two top-frequency outputs, good and poor, can be considered as anchors for better and worse human perception, and the Direct Strategy can be approximated into a binary classification problem on the [SCORE TOKEN] position, or technically, an argmax between the logits of good (xgoodSCORE TOKEN) and poor (x poor SCORE TOKEN) on this position. In our revised strategy, we modify the argmax into softmax to collect better quantifiable scores:\nqpred = ex\ngood SCORE TOKEN\nex good SCORE TOKEN + ex poor SCORE TOKEN\n(1)\nThis simple and generally-applicable strategy enables us to collect quantifiable outputs (qpred) from MLLMs with higher correlation to human ratings, as verified in our experimental analysis (Tab. 9)."
        },
        {
            "heading": "3 RESULTS ON Q-BENCH",
            "text": "In Q-Bench, we evaluate 15 variants on 13 up-to-date popular and competitive open-source MLLMs, together with GPT-4V, under zero-shot settings. More results and analyses are appended in Sec. A.3.\n3.1 RESULTS AND OBSERVATIONS ON PERCEPTION\nOpen-Source MLLMs. For a holistic examination on the perception ability of MLLMs, we evaluate the multi-choice correctness of MLLMs on different sub-categories of the LLVision dataset, which is equally divided as dev (Tab. 7, will be released) and test (Tab. 2, will keep private) subsets. We are glad that the majority of MLLMs can significantly outperform random guess on all sub-categories. Considering that all participating MLLMs are without any explicit training on lowlevel visual attributes, these results show strong potentials for these general-purpose models when further fine-tuned with respective low-level datasets. Among all MLLMs, the recently-released InternLM-XComposer-VL reaches the best accuracy on this question-answering task, followed by LLaVA-v1.5, QWen-VL and InstructBLIP (Flan-T5), which show rather close results. By achieving more than 60% accuracy on both subsets, these models show exciting potentials as robust low-level visual assistants in the future. Another key observation is that almost all methods perceive worse on distortions than other low-level attributes. One exception is LLaMA-Adapter-V2, which is the only MLLM that adopts multi-scale features as visual inputs. We also notice that all MLLMs prefer yes than no among Yes-or-No questions, as analyzed in Tab. 8; qualitative comparisons are illustrated in Fig. 10. For Kosmos-2, we specially adopt close-set inference for it, as discussed in Sec. A.2.1.\nGPT-4V vs Human. To evaluate the low-level perception abilities of the commercial MLLM, GPT-4V, we gauge its accuracy against human using the test subset of LLVision dataset. GPT-4V exhibits competitive performance and outperforms open-source MLLMs by a large margin (+9%), and on par accuracy with the Junior-level Human. Despite its prowess, there is still a way to go for GPT-4V before it can match the overall proficiency of the Senior-level Human (with experiences on low-level visual tasks, 8% better than GPT-4V). Furthermore, across all categories, the results show that GPT-4V, much like its open-source counterparts, faces challenges in recognizing distortions.\n3.2 RESULTS AND OBSERVATIONS ON DESCRIPTION\nFor the description ability, InternLM-XComposer-VL reaches best proficiency again, especially in terms of the relevance dimension. Nevertheless, in the perspective of the completeness and precision of the descriptions, even the best of all MLLMs cannot obtain an excellent score; on the contrary, almost all MLLMs reach an acceptable standard (0.8/2.0). In general, all MLLMs at present are only with relatively limited and primary ability to provide low-level visual descriptions. We also conduct a qualitative comparison for MLLM descriptions in Sec. A.3.3.\n3.3 RESULTS AND OBSERVATIONS ON ASSESSMENT\nTo measure the assessment ability, we evaluate the performance of 15 MLLMs on 7 IQA datasets that are with at least 1,000 images and 15 human ratings per image (itu, 2000). Primarily, we notice that the majority of MLLMs are notably better than NIQE on non-natural circumstances (CGI, AIGC, artificial distortions), showing their potential towards general-purpose evaluators on a broader range of low-level appearances. We also notice that without explicit alignment with human opinions during training, the most excellent MLLM, which is again InternLM-XComposer-VL, can already outperform CLIP-ViT-Large-14 by a large margin (20%), marking the dawn of MLLMs as robust quality evaluators. Furthermore, we also design a synonym ensemble (see Sec. A.2.3) strategy which can further generally improve IQA accuracy of MLLMs, whose results are analyzed in Sec. A.3.5. Despite their proficiency, current MLLMs are still less accurate in finer-grained situations (LIVE-FB, CGIQA-6K) for the assessment task, which could be enhanced in the future."
        },
        {
            "heading": "4 CONCLUSION",
            "text": "In this study, we construct the Q-Bench, a benchmark to examine the progresses of MLLMs on lowlevel visual abilities. Anticipating these large foundation models to be general-purpose intelligence that can ultimately relieve human efforts, we propose that MLLMs should achieve three important and distinct abilities: accurate perception on low-level visual attributes, precise and complete language description on low-level visual information, as well as quantitative assessment on image quality. To evaluate the abilities, we collect two multi-modality benchmark datasets for low-level vision, and propose a unified softmax-based quantitative IQA strategy on MLLMs. Our evaluation proves that even without any low-level-specific training, several extraordinary MLLMs still have decent low-level abilities. Nevertheless, there is still a long way to go for MLLMs to be truly-reliable general low-level visual assistants. We sincerely hope that the observations found in the Q-Bench can inspire future MLLMs to enhance the low-level perception and understanding abilities."
        },
        {
            "heading": "AUTHOR CONTRIBUTIONS",
            "text": "We will reveal the contributions of authors in the final edition."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "100% of the annotated labels in the LLVisionQA and LLDescribe datasets (question-answers and long golden descriptions) are conducted by human experts. We sincerely thank their efforts.\nWe thank the anonymous reviewers on ICLR2024 Conference on providing valuable and constructive suggestions for us to improve this paper."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 MORE INFORMATION ON BENCHMARK DATASETS",
            "text": ""
        },
        {
            "heading": "A.1.1 SUBJECTIVE EXPERIMENT",
            "text": "A total of eleven experts, each with professional skills and extensive experience in photography, are invited to participate in the subjective labeling experiment of Q-Bench. The subjective experiment takes place in a laboratory environment with standard indoor lighting. A Dell-4K monitor, which supports a resolution of 3840 \u00d7 2160, is used for displaying the interfaces. The screenshots of interfaces can be referred to in Fig. 5. Each expert annotates up to 30 images a day to avoid fatigue, and every annotation is carefully reviewed by at least three other experts before acceptance. In this way, we ensure the accuracy and rigor of the Q-Bench labels to the greatest extent possible. This, in turn, makes the performance testing capability of Q-Bench more precise and meaningful."
        },
        {
            "heading": "A.1.2 MORE DETAILS ON LLVISIONQA",
            "text": ""
        },
        {
            "heading": "The Enumeration on Distortions and Other Low-level Attributes:",
            "text": "Distortions: Blurs [lens blur (out-of-focus), motion blur, zoom blur, gaussian blur, glass blur], Noises [gaussian noise, speckle noise, pepper noise], Artifacts [compression artifact, transmission error], Exposure Issues [under-exposure, over-exposure], Miscellaneous Artificial Distortions [pixelate, color-diffusion, jitter, etc] Other low-level attributes: Color [color style, color vividity], Lighting [bright, dim], Composition [Symmetrical, Rule-of-Thirds], Visual Styles [animation, realism, computer-generated, AI-generated], Photographic Methods [background bokeh (shallow DOF), high contrast, motion blur (on fast-moving objects), etc]"
        },
        {
            "heading": "Relationship between In-context Questions and Global Questions:",
            "text": "Distortions: Is this image blurred? \u2192 In-context Distortions: Is the tree in the image blurred? Distortions: How is the clarity of the image? \u2192 In-context Distortions: How is the clarity of the man\u2019s face? Other low-level: Is this image colorful? \u2192 In-context Other: Is the house colorful? Other low-level: How is brightness of the image? \u2192 In-context Other: Which is the darkest object?"
        },
        {
            "heading": "A.2 DETAILS ON BENCHMARK EVALUATION SETTINGS",
            "text": "A.2.1 EVALUATION DETAILS FOR PERCEPTION ABILITY\n[Special Note] Multi-choice Question vs Close-Set Inference for Kosmos-2:\nWhile Kosmos-2 performs generally well on the description and assessment tasks, we notice that it is hardly capable of answering a multi-choice question with the general prompt form applicable for other methods, as follows:\nHow is the clarity of the image? (Question) [IMAGE TOKEN] (Image) Choose between one of the following options: A. High (Correct) B. Medium(Wrong) C. Low(Wrong)\nFor most situations (86%) in our primary sample test with the prompts above, Kosmos-2 will directly append a new candidate (e.g., D. Excellent or D. Very Low) answer instead of choosing one option among them, denoted as prompt failure. This might be because the language model of Kosmos-2 has smaller capacity (1B) than other MLLMs that are based on LLaMA/MPT (7B/13B).\nConsidering that the prompt failure is actually not directly related with low-level perception, we try different prompt engineering techniques to reduce the prompt failure rate, and finalize with a simple modification which can limit the prompt failure to less than 10% in our sample set, as follows:\nHow is the clarity of the image? (Question) [IMAGE TOKEN] (Image) Choose between one of the following options: A. High (Correct) B. Medium(Wrong) C. Low(Wrong) #Answer:\nNevertheless, we are still not able to eliminate the prompt failures for Kosmos-2. Henceforth, to systematically remove the negative effect of prompt failures on multi-choice questions for Kosmos-\n2, we conduct a choice-free special setting for it, i.e. close-set inference, via ranking the perplexity of different answers and choose the answer with minimum generative loss:\nHow is the clarity of the image? [IMAGE TOKEN] #Answer: High \u2192 loss:7.43\u2192 \u2713 Choose this. How is the clarity of the image? [IMAGE TOKEN] #Answer: Medium \u2192 loss:7.56\u2192 \u2717 How is the clarity of the image? [IMAGE TOKEN] #Answer: Low \u2192 loss:7.92\u2192 \u2717 As shown in Tab. 5, perplexity-based close-set inference can notably improve results of Kosmos-2. Considering that it is still the MLLM with fewest parameters among the ten models, its results are decent at its model size. More importantly, they validate that our observation on the prompt failure is reasonable, and we will further delve deeper into this problem of MLLMs in our extended works."
        },
        {
            "heading": "Settings for GPT Evaluation:",
            "text": "Given GPT\u2019s inherent variability, identical prompts can yield non-definitive responses. To address the impact of such situations on our evaluation, we\u2019ve implemented a 5-round voting strategy. Under this approach, we pose the same prompt as defined in the following templates five times, taking the popular votes of GPT\u2019s answers to determine the final outcome. Our human analysis on a sample set confirms that the 5-round voting strategy improves GPT evaluation accuracy from 93.2% to 98.4%, reducing errors to only 1/4 compared with the single-round evaluation."
        },
        {
            "heading": "Prompt Templates for GPT Evaluation:",
            "text": "#System: You are a helpful assistant that grades answers related to image quality and aesthetics. There are a lot of special terms or keywords related to image processing and photography. You will pay attention to the context of \u2019quality evaluation\u2019 when grading.\n#User: Assuming you are a grader, you will now be provided with a question [question] and a set of options [options] with option [options[0]] being the correct answer. Additionally, there will be an answer [answer] provided by a respondent. Please determine whether the respondents\u0301 answer is correct considering the context of the question. Even if the word choice is not completely the same, you can decide based on the given options and see whether the one in the answer is close enough to the given correct answer, The result is 1 if the answer is correct and else the result is 0. Please only provide the result in the following format: Result:"
        },
        {
            "heading": "Examples for GPT Evaluation:",
            "text": "(1) \u201cRephrased\u201d Answers. (Fig. 6)\nQuestion: Which is the brightest part in this image?\nChoices: [\u2018Capital letters E and S\u2019, \u2018ST\u2019, \u201818\u2019, \u201856\u2019]\nMLLM Answer:\nThe brightest part in this image is the neon sign with the letters \u201cE\u201d and \u201cS\u201d in capital letters.\n5-Round GPT Answers: [\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d] \u2192 Final Correctness after Voting: \u2713 (2) \u201cDistracted\u201d Answers. (Fig. 7)\nQuestion: How\u2019s the level of blur in the image?\nChoices: [\u2018Some blur\u2019, \u2018Not blurry at all\u2019, \u2018Very blurry\u2019]"
        },
        {
            "heading": "MLLM Answer:",
            "text": "The image has some blur, but it is not very blurry.\n5-Round GPT Answers: [\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d] \u2192 Final Correctness after Voting: \u2713 (3) \u201cBoth\u201d Answers. (Fig. 8)\nQuestion: Is the little boy emphasized in the center of the composition of the image?\nChoices: [\u2018No\u2019, \u2018Yes\u2019]"
        },
        {
            "heading": "MLLM Answer:",
            "text": "A. No, the little boy is not emphasized in the center of the composition of the image. B. Yes,\n5-Round GPT Answers: [\u201cScore: 1\u201d,\u201cScore: 0\u201d,\u201cScore: 0\u201d,\u201cScore: 1\u201d,\u201cScore: 0\u201d] \u2192 Final Correctness after Voting: \u2717\nA.2.2 EVALUATION DETAILS FOR DESCRIPTION ABILITY"
        },
        {
            "heading": "General Description Prompt for MLLMs:",
            "text": "#User: Describe the quality, aesthetics and other low-level appearance of the image in details."
        },
        {
            "heading": "Settings for GPT Evaluation:",
            "text": "Given GPT\u2019s inherent variability, identical prompts can yield non-definitive responses. To address the impact of such situations on our evaluation, we\u2019ve implemented a 5-round average pooling strategy. Under this approach, we pose the same prompt as defined in the following templates five times, taking the mean result of GPT\u2019s answers to determine the final outcome. This method effectively mitigates the unpredictability associated with GPT, ensuring a more accurate score."
        },
        {
            "heading": "Prompt Templates for GPT Evaluation:",
            "text": "#System: You are a helpful assistant.\nCompleteness. #User: Evaluate whether the description [MLLM DESC] completely includes the low-level visual information in the reference description [GOLDEN DESC]. Please rate score 2 for completely or almost completely including reference information, 0 for not including at all, 1 for including part of the information or similar description. Please only provide the result in the following format: Score: Preciseness. #The precision metric punishes controversial low-level descriptions that output description contrasts with the referencce, e.g., blur for clear, high quality for low quality, colorful for monotonous, noisy for clean, bright for dark. Evaluate whether output [b] precisely reflects reference [a]. Please rate score 2 for totally no controversial low-level description, 1 for less controversial low-level description than matched descrpition, and 0 for more controversial low-level description than matched description. Please only provide the result in the following format: Score: Relevance. #User: Evaluate whether the description [MLLM DESC] is relevant to the low-level visual information, which may include blur, noise, exposure, artifact, color, lighting, focus, composition, etc. Please rate score 2 for completely relevant, 1 for partly relevant, and 0 for totally irrelevant.\nPlease only provide the result in the following format: Score:\nIn the prompt template, the [MLLM DESC] denotes the output description from MLLMs, and [GOLDEN DESC] denotes the golden description in the LLDescribe dataset."
        },
        {
            "heading": "Examples for GPT Evaluation:",
            "text": "(A) Completeness.\nUser Input: #User: Evaluate whether the description [ The image is a large, clear, and detailed picture of a white airplane flying in the sky. The airplane appears to be a JAL (Japan Airlines) passenger jet, and it is flying at a relatively low altitude. The plane\u2019s white color and its distinctive red logo are clearly visible, making it easily discernable as a JAL aircraft. Overall, the image captures a clear, beautiful scene of the airplane in flight, with a blue sky as the backdrop.] completely includes the low-level visual information in the reference description [This image has poor lighting. The blue sky has vivid colors, but the overall tone is dark. The focus is on the main subject, the aircraft. Apart from the backlighting on the wings, the details of the aircraft are clear. The composition is slightly incomplete, and the clarity of the background sky is average. Overall, the image quality is good.]. Please rate score 2 for completely or almost completely including reference information, 0 for not including at all, 1 for including part of the information or similar description. Please only provide the result in the following format: Score:\n5-Round GPT Answers: [\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d] \u2192 Final Score: 1.0 (B) Preciseness.\nUser Input: #User: The precision metric punishes controversial low-level descriptions that output description outputs contrast with the referencce, e.g., blur for clear, high quality for low quality, colorful for monotonous, noisy for clean, bright for dark. Evaluate whether output [The image quality is good, with a clear and well-exposed scene. The lighting appears to be natural, as the sun is shining on the scene, which adds a warm and inviting atmosphere to the image. The composition is well-balanced, with the focus on the trees and the pathway, which creates a sense of depth and perspective. The aesthetics of the image are pleasing, with the use of natural light and the greenery of the trees contributing to a visually appealing scene. The overall feeling of the image is peaceful and serene, as the viewer is invited to walk down the pathway surrounded by the trees and the cityscape in the background.] precisely reflect reference [The overall clarity of this image is very low, with serious focusing issues and significant noise. The lighting is adequate, but the colors are monotone. The main subjects are trees and distant buildings, which are extremely blurry and lack any texture details. The composition is unbalanced, and the background is very blurry. Therefore, the quality of this image is very poor.], Please rate score 2 for no controversial low-level description, 1 for less controversial low-level description than matched descrpition, and 0 for more controversial low-level description than matched description. Please only provide the result in the following format: Score:\n5-Round GPT Answers: [\u201cScore: 0\u201d,\u201cScore: 0\u201d,\u201cScore: 0\u201d,\u201cScore: 0\u201d,\u201cScore: 0\u201d] \u2192 Final Score: 0.0 (C) Relevance.\nUser Input: #User: Evaluate whether the description [ The image is a low-level shot of a white dog walking through a dark forest. The dog appears to be somewhat blurry, suggesting a level of motion in the picture. The photo is not very detailed, and the colors in the image might be somewhat muted due to the darkness of the forest. Overall, the picture has a somewhat mysterious and moody atmosphere.] is relevant to the low-level visual information, which may include blur, noise, exposure, artifact, color, lighting, focus, composition, etc.\nPlease rate score 2 for completely relevant, 1 for partly relevant, and 0 for totally irrelevant. Please only provide the result in the following format: Score:\n5-Round GPT Answers: [\u201cScore: 2\u201d,\u201cScore: 1\u201d,\u201cScore: 1\u201d,\u201cScore: 2\u201d,\u201cScore: 1\u201d] \u2192 Final Score: 1.4\nAlgorithm 1 Pytorch-style Pseudo Code for Softmax-based Strategy for IQA with MLLMs\nfrom PIL import Image from my_mllm_model import Model, Tokenizer, embed_image_and_text\nmodel, tokenizer = Model(), Tokenizer()\nprompt = \"##User: Rate the quality of the image.\\n\" \\ \"##Assistant: The quality of the image is\"\ngood_idx, poor_idx = tokenizer([\"good\",\"poor\"]).tolist()\nimage = Image.open(\"image_for_iqa.jpg\") input_embeds = embed_image_and_text(image, prompt) output_logits = model(input_embeds=input_embeds).logits[0,-1] q_pred = (output_logits[[good_idx, poor_idx]] / 100).softmax(0)[0]\nA.2.3 EVALUATION DETAILS FOR ASSESSMENT ABILITY"
        },
        {
            "heading": "Example Pseudo Code for MLLMs on IQA:",
            "text": "In Algo. 1, we provide an example on how to evaluate image quality with MLLMs. The algorithm is simple with only 9 lines, and could be easily integrated with any new MLLMs (based on causal LLMs), so as to allow these models to quantitatively predict the quality of images."
        },
        {
            "heading": "IQA Evaluation Strategy for CLIP-ViT-Large-14:",
            "text": "In Tab. 4, we compare the IQA performance of MLLMs with CLIP-ViT-Large-14, the visual backbone of the majority of MLLMs. Attempting to understand whether the new language part (LLM) can do better than the original language part of CLIP, we try to compare between CLIP and MLLMs in a relatively aligned setting. Firstly, noticing that most MLLMs will resize images into 224\u00d7 224 as their input sizes, we align this setting on CLIP, and ignore the strategies as proposed by (Wang et al., 2022). Secondly, same as the strategy on MLLMs, we also apply softmax pooling between good and poor, as in the CLIP\u2019s zero-shot classification format: a photo of good quality and a photo of poor quality. Besides the two alignments, similar as existing practices (Wang et al., 2022; Wu et al., 2023b; Zhang et al., 2023c), the quality scores of CLIP-ViT-Large-14 are obtained as follows:\nqpred,CLIP = eCosineSimilarity(f[IMAGE],fa photo of good quality)\neCosineSimilarity(f[IMAGE],fa photo of good quality) + eCosineSimilarity(f[IMAGE],fa photo of poor quality) (2)\nSpecial IQA Settings for Flan-T5-based InstructBLIP:\nFor InstructBLIP (Dai et al., 2023) (Flan-T5-XL), different from the majority of LLaMA-based (or MPT-based Otter-v1) MLLMs, the two top-frequency tokens are high (89%) and low (8%) instead of the common good\u2194poor. Henceforth, based on our motivation to only modify the argmax into softmax and follow the default top-frequency output tokens of MLLMs, we replace the probabilities of good\u2194poor into those of high\u2194low in Eq. 1 for T5, defined as follows:\nqpred,T5 = ex\nhigh SCORE TOKEN\nex high SCORE TOKEN + ex low SCORE TOKEN\n(3)\nOverall\nYes-or-No Questions"
        },
        {
            "heading": "What Questions",
            "text": "How Questions\nDistortion\nOther\nIn-context Distortion\nIn-context Other\n0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4"
        },
        {
            "heading": "VisualGLM-6B MPLUG-Owl LLaMA-Adapter-V2 LLaVA-v1 MiniGPT-4",
            "text": "As validated in our experiments (Tab. 10, the high\u2194low pair generally predicts better than good\u2194poor on majority of databases. The better performance on MLLM-specific top-frequency tokens by side validates the effectiveness of our methodology for MLLMs on IQA.\nFurther Improving IQA Abilities of MLLMs with Synonym Ensemble:\nThe quality assessment scores for the synonym ensemble strategy can be derived as:\nqpred = e \u2211t\u2208P t x t SCORE TOKEN\ne \u2211t\u2208P t x t SCORE TOKEN + e \u2211t\u2208N t x t SCORE TOKEN\n(4)\nwhere P indicates the positive token set (from good, fine, high, etc.), while N represents the negative token set (from poor, bad, low, etc.). The results of different P and N are listed in Tab. 11. Special Validation Protocol for CGIQA-6K:\nThe CGIQA-6K (Zhang et al., 2023b) dataset contains two separate sub-sets which consist of 3,000 game images and 3,000 movie images respectively, with different instructions for human annotators during its subjective experiments. Therefore, we validate the MLLMs\u2019 assessment performance on the two sub-sets individually and average the results for the final exhibition. The results of NIQE and CLIP-ViT-Large-14 are also obtained under the same protocol for a fair comparison."
        },
        {
            "heading": "A.3 EXTENDED EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "A.3.1 ARCHITECTURES OF DIFFERENT MLLMS",
            "text": "As compared in Tab. 6, the 15 variants of MLLMs as evaluated in the Q-Bench are with varying vision and language architectures, as well as the alignment strategies between the two modalities. It can be noticed that all MLLMs are combined with a version of CLIP Radford et al. (2021) and a large language model, which are generally connected under one among three strategies: direct project layers (MLP or linear layer), Q-Former (a transformer to abstract visual features into LLM tokens), or cross-attention (use visual features as conditions for text generation).\nA.3.2 EXTENDED RESULTS FOR PERCEPTION\nResults on the dev subset:\nIn Tab. 7, we list the results on the dev subset of the LLVisionQA benchmark set for the lowlevel perception task. This subset is planned to be opened to public in the future. Therefore, the performance in it will only be taken as a reference. At present, all MLLMs as evaluated have not yet seen this subset, so it can be taken as a cross-validation with the test subset. From Tab. 7 and Tab. 2, we validate that MLLMs perform pretty similar between the two subsets, suggesting that LLVisionQA is a reliable and stable benchmark set for question answering on low-level vision."
        },
        {
            "heading": "Radar Chart for Different MLLMs:",
            "text": "In Fig. 9, we show the radar chart to compare the low-level perception abiliies among different MLLMs. Despite the observations as revealed in Sec. 3.1, we also notice two extra fun facts: 1) Adding the content context does not degrade the performance of MLLMs. On the contrary, MLLMs can answer better on in-context questions. This result validates the aforementioned conjectures that appropriate higher-level contexts as prompts may help improve the preciseness of low-level visual perception; 2) MLLMs have strong capabilities of answering what questions, suggesting potential reasoning abilities. In the future, we will excavate more interesting characteristics of MLLMs and try to improve their perception accuracy through better guidance based on these characteristics.\n\u201cYes or No?\u201d: How Biased are MLLMs?\nIn this section, we take a deeper analysis on the Yes-or-No judgment ability of MLLMs, that whether these models can get similar accuracy on questions that should be answered with Yes, as those should be replied as No. Sadly, we notice that all MLLMs have higher prediction accuracy on Yes-questions than No-questions, while some MLLMs are more very severe biased (e.g., IDEFICSInstruct). Considering that our LLVisionQA dataset contains more (62%) Yes-questions than Noquestions (38%) and may introduce biases while comparing different MLLMs, we further compute a de-biased accuracy for all these methods, as the mean value of the accuracies on two types of questions, and present the respective de-biased rank for all participating MLLMs, as listed in Tab 8.\nWe hope this study on the biases and the de-biased results can provide a fairer comparison among them, as well as bring insights on the future improvements of MLLMs for low-level perception.\nQualitative examples of MLLM responses:\nIn Fig. 10, we show qualitative examples of MLLM responses on questions in the LLVisionQA dataset, that MLLMs are still unstable on basic low-level attributes such as blurs (Fig. 10(a)), and may fail on in-context questions that are easy to human (Fig. 10(b)). These unsatisfactory results suggest that we still need to improve the basic low-level perception ability of these models.\nA.3.3 EXTENDED RESULTS FOR DESCRIPTION"
        },
        {
            "heading": "Bar Chart for Different MLLMs:",
            "text": "In Fig. 11, we show the bar chart to visualize MLLM capabilities on the three dimensions of lowlevel visual description. From the figure, we notice that current MLLMs still struggle on describing complete and accurate low-level information. As the relevance scores are generally higher (showing that most MLLMs can follow this abstract instruction well), the results suggest that the main bottleneck of MLLMs on enhancing their description ability is still the perception on low-level attributes.\nA Qualitative Comparison on the Descriptions:\nIn Fig. 12 and Fig 12 we qualitatively compare among different MLLM low-level descriptions on an AI-generated image and a natural photograph. While most MLLMs can precisely describe their contents (which are actually not instructed in our user prompt), different MLLMs may have several divergences on their quality and related low-level attributes, especially on the AI-generated image. Some MLLMs describe it as clear, colorful, or aesthetically pleasing, which are typically incorrect; on the contrary, some correct descriptions are also seen, such as colors are subdued, not as clear as it could be, or not very detailed. This qualitative study validates our quantitative conclusion that current MLLMs general cannot provide noise-free low-level visual descriptions of images in a stable manner. Moreover, we notice that even given the same prompt, different MLLMs tend to describe the image with diverse output styles and lengths, while the longer descriptions usually come with a larger percentage of descriptions on irrelevant information.\nA.3.4 EXTENDED RESULTS FOR ASSESSMENT"
        },
        {
            "heading": "Radar Chart for Different MLLMs:",
            "text": "In Fig. 14, we visualize the IQA performance of different MLLMs on seven IQA datasets. The visualization proves that MLLMs can notably positively correlate with human ratings, and could very highly align with human perception on relatively coarse situations (AGIQA-3K, SPAQ). As\nsuch results are obtained via no direct alignment with human perception and from the simplest prompts (\u201cRate the quality of the image.\u201d), they suggest the exciting underlying abilities of general intelligence to naturally understand \u201cquality\u201d via their vast training data. However, the performance are still yet to be accurate on finer-grained situations, such as LIVE-FB, which is constructed by more than 95% high-quality images (i.e. quality score > 50/100), or CGIQA-6K, made up entirely by relatively high-quality images collected from video games or movies. This suggests that MLLMs still need to improve the measurability on their predictions through well-designed fine-tuning."
        },
        {
            "heading": "A Deep Dive Into the Probabilities:",
            "text": "(A) Softmax vs Argmax:\nIn the first part of the deep dive, we quantitatively evaluate the correlation with human perception on a simple argmax strategy between good\u2194bad and our proposed softmax strategy. In Tab. 9, we confirm that for all MLLMs on all IQA datasets, the more measurable softmax strategy predicts better than the argmax strategy, which degenerates into only two scores, 0 and 1. Though the result is generally expected, the experiments validate that MLLMs have quantitative assessment ability hidden behind their word outputs, and prove the effectiveness of our softmax-based IQA strategy.\n(B) [For T5-based InstructBLIP] high\u2194low vs good\u2194poor: We further conduct a special study for InstructBLIP (Flan-T5-XL). With a different LLM as language backbone, even pre-trained with the same settings, the T5-version of InstructBLIP tends to predict more high\u2194low than good\u2194poor, different from its Vicuna-7B-based counterpart. The experimental results in Tab 10 validate that the more probable high\u2194low tokens are more competitive in IQA than good\u2194bad tokens, suggesting that top-frequency tokens are more quality-distinctive.\nA.3.5 Synonym Ensemble: FURTHER IMPROVING IQA ABILITY (A3) FOR MLLMS\nAs shown in Table 11, the synonym ensemble strategy (as proposed in Eq. 4) on top-5 methods (i.e. InternLM-XComposer-VL, QWen-VL, LLaVA-v1.5 (13B), mPLUG-Owl, and LLaVA-v1.5 (7B)) can in average lead to up to 2% accuracy improvement (in average 1.3%). We believe it is a useful boost to improve the performance of MLLMs on IQA task.\nNevertheless, we also notice that different MLLMs perform best with different specific prompt combos. For example, the good+fine\u2194poor+bad performs best on InternLM-XComposer-VL, but comes with reduced accuracy on QWen-VL compared with only good\u2194poor. While good\u2194poor is proved overall best single word pair for the evaluation and shows stable results across MLLMs, we decide to keep the current strategy in Q-Bench to evaluate MLLMs."
        },
        {
            "heading": "B STATEMENT ON DATA CONTAMINATION",
            "text": "The Q-bench contains three tasks, where the first two tasks, (A1) perception and (A2) description, are evaluated with our own datasets proposed with the paper. For these two tasks, the questions, answers, or low-level descriptions in the two datasets are not seen by any existing MLLMs. Half of LLVisionQA (i.e. the test subset) and full of LLDescribe labels are kept private, to avoid being added to the training sets of any MLLMs. We hope that this measure will allow Q-Bench to have long-term significance as an indicator of low-level visual abilities.\nFor the third task, (A3) assessment, the situation is a bit more complicated. For open-source models as tested, almost all of them have provided their technical reports, and as far as we know, no image quality assessment (IQA) dataset has participated in the multi-modality training stages of them.\nWhile text knowledge about image quality assessment should have been injected to them (e.g. a blurry image is a low quality image) during their pure-language training stages, we think this should not be regarded as data contamination for IQA, because the images cannot be seen by a language model. Instead, they are important knowledge for MLLMs to better link particular visual attributes (blur) to human opinions (quality), which motivates us to explore MLLMs for these tasks."
        },
        {
            "heading": "C LIMITATIONS AND DISCUSSIONS",
            "text": "In Section A.3.2, we observed that MLLMs frequently respond with \u2018yes\u2019 to Yes-or-No questions. It\u2019s worth noting that the current LLVisionQA dataset is skewed, with 62% of its questions being Yes-questions and only 38% being No-questions. This imbalance could introduce biases when comparing various MLLMs. To fully address this, we aim to balance the dataset by preparing a reversed version for each question in our subsequent work, ensuring a less biased evaluation.\nFor the description task, we acknowledge that judging whether a description matches the gold description is a subjective process, which may not have an absolute standard. Even when evaluated by humans, the scores rated for the MLLM descriptions are subject to individual differences. Though we have employed the 5-round GPT-assisted evaluation protocol, which could be the most reliable and reproducible way at present, it may still unavoidably contain hallucinations (from GPT). We will continue to explore how to design a more reliable evaluation protocol for the low-level visual description task in our follow-up works.\nWhile the proposed Q-Bench has offered a comprehensive evaluation on the low-level visual capabilities of MLLMs, it does not provide direct guidance on enhancing these capabilities. As our next steps, we intend to progressively scale up the LLDescribe and LLVisionQA datasets to eventually allow a reliable low-level visual instruction tuning process that can further improve the low-level abilities for MLLMs."
        }
    ],
    "title": "Q-BENCH: A BENCHMARK FOR GENERAL-PURPOSE FOUNDATION MODELS ON LOW-LEVEL VISION",
    "year": 2024
}